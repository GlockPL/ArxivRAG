{"title": "RLTHF: Targeted Human Feedback for LLM Alignment", "authors": ["Yifei Xu", "Tusher Chakraborty", "Emre K\u0131c\u0131man", "Bibek Aryal", "Eduardo Rodrigues", "Srinagesh Sharma", "Roberto Estevao", "Maria Angels de Luis Balaguer", "Jessica Wolk", "Rafael Padilha", "Leonardo Nunes", "Shobana Balakrishnan", "Songwu Lu", "Ranveer Chandra"], "abstract": "Fine-tuning large language models (LLMs) to align with user preferences is challenging due to the high cost of quality human annotations in Reinforcement Learning from Human Feedback (RLHF) and the generalizability limitations of AI Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid framework that combines LLM-based initial alignment with selective human annotations to achieve full-human annotation alignment with minimal effort. RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward model's reward distribution and iteratively enhances alignment by integrating strategic human corrections while leveraging LLM's correctly labeled samples. Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human annotation-level alignment with only 6-7% of the human annotation effort. Furthermore, models trained on RLTHF's curated datasets for downstream tasks outperform those trained on fully human-annotated datasets, underscoring the effectiveness of RLTHF's strategic data curation.", "sections": [{"title": "1. Introduction", "content": "In recent years, large language models (LLMs) have demonstrated remarkable advancements, unlocking new possibilities across a wide range of applications (Touvron et al., 2023; Jiang et al., 2024; Achiam et al., 2023; Team et al., 2023). As these models become more powerful, the focus has shifted toward customization, i.e., fine-tuning base models to better serve specific tasks and user needs (Wei et al., 2021; Li et al., 2023a). Companies are increasingly investing in solutions built upon fine-tuned models, recognizing the value of adapting LLMs to align with end-user preferences, including intent, style, grounding, and compliance requirements (Atreya, 2024; Microsoft, 2024; Sharma,\n2024; \u0391\u0399, 2024). A key approach to achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which has emerged as a widely adopted technique in the literature for refining model behavior based on human feedback (Bai et al., 2022a; Stiennon et al., 2020; Rafailov et al., 2024; Wang et al., 2024a; Ouyang et al., 2022).\nThe effectiveness of RLHF techniques heavily depends on high-quality human annotations, which are both costly and time-consuming to obtain (Pang et al., 2023; Lee et al., 2023; Wang et al., 2024a). To mitigate this challenge, Reinforcement Learning from AI Feedback (RLAIF) has been introduced, leveraging LLMs to replace human annotators in the feedback loop (Lee et al., 2023; Lee et al.; Bai et al., 2022b). While RLAIF can approximate human judgment to some extent, it is sensitive to factors such as prompt optimization, task complexity, model bias, generator-discriminator gap, and the capability of the judge model, limiting its ability to fully replicate human annotations (Huang et al., 2024; Sharma et al., 2024; Lee et al., 2023; Zeng et al., 2024; Huang et al., 2023). Our evaluation also provides evidence of these limitations. Furthermore, the samples that challenge a judge model are often the ones most critical for adapting base models to specialized fine-tuning tasks (Ethayarajh et al., 2024; Yuan et al., 2024; Huang et al., 2023).\nThe cost of human annotation is further exacerbated by privacy and security constraints that restrict fine-tuning service providers' access to an entire customer data corpus. In such cases, only subject matter experts (SMEs) within the customer organization have full visibility into the data, making it particularly difficult to optimize prompts effectively across the entire corpus, especially for hard-to-annotate samples.\nTo address these challenges, we propose Reinforcement Learning from Targeted Human Feedback (RLTHF), a human-AI hybrid solution that combines coarse initial alignment using general-purpose LLMs with the progressive integration of strategically selected human annotations to achieve annotation quality comparable to fully human-supervised approaches. RLTHF begins with an initial alignment stage, where a general-purpose LLM labels unlabeled data based on high-level instructions. While this approach effectively captures broader human alignment for easier data points, it often struggles with fine-grained nuances, leading to incorrect labeling. RLTHF automatically identifies these hard-to-annotate data points and directs human effort exclusively toward them. This targeted approach enables RLTHF to achieve the quality of fully human-annotated data while reducing the majority of human annotation effort.\nTo enable this efficient human-in-the-loop approach for achieving comprehensive human alignment, RLTHF introduces the following key technical contributions:\nFirst, we develop a concept that leverages the reward distribution of a reward model over its training dataset to capture the relative arrangement of samples based on rewarded features. This property allows us to detect plausible inaccuracies in annotations across the dataset. Specifically, we train a reward model on the LLM-labeled dataset to identify clusters of hard-to-annotate samples that are highly likely to be either mislabeled or correctly labeled by the LLM.\nBuilding on this concept, we propose an innovative iterative reward model training technique to achieve oracle-level human alignment in the dataset. In each iteration, RLTHF identifies and rectifies highly probable mislabeled data points using human annotations. Simultaneously, it detects clusters of samples that are very likely to be correctly labeled by the LLM and incorporates them with human-annotated data to construct a high-quality training set for the next iteration of reward model training. Throughout this process, RLTHF preserves data richness and maximizes the efficiency of human annotation investment through carefully controlled hyperparameters.\nFinally, we implement and evaluate RLTHF on two distinct preference datasets: HH-RLHF and TL;DR. Our results demonstrate that RLTHF achieves accuracy comparable to a fully human-annotated dataset while requiring only 6-7%"}, {"title": "2. Background and Related Work", "content": "LLMs have demonstrated impressive performance across a wide spectrum of tasks (Achiam et al., 2023; Dubey et al., 2024; Team et al., 2023). Despite the progress, their performance on customized downstream tasks can be significantly optimized by supervised fine-tuning (SFT) with instruction and human-written responses pairs (Chung et al., 2024; Thoppilan et al., 2022). Reinforcement learning with preference data has further shown success due to the easier-to-collect data form (Ouyang et al., 2022; Stiennon et al., 2020; Lee et al., 2023). Representative methods include Proximal Policy Optimization (PPO) (Schulman et al., 2017), which optimizes the LLM with a separate reward model, and Direct Preference Optimization (DPO) (Rafailov et al., 2024), which directly learns from the preference data. Although an easier data collection is available, these methods still largely rely on the richness and quality of the preference data (Xu et al., 2024; Zheng et al., 2023b; Wang et al., 2024a)."}, {"title": "2.1. Alignment with External Feedback", "content": "Human feedback is regarded as the golden standard in LLM alignment. However, reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020; K\u00f6pf et al., 2024) typically incorporates heavy and expensive crowdsourcing efforts or expert annotations to guarantee data diversity and richness. To relieve the reliance on human effort, reinforcement learning with AI feedback (RLAIF) (Lee et al., 2023; Bai et al., 2022b) provides an alternative that collects feedback from stronger LLMs instead of humans. On the other hand, this method is limited by the capability of the stronger LLM annotators (Huang et al., 2024; Sharma et al., 2024; Lee et al., 2023) especially for customized tasks, and suffers from their intrinsic biases (Zheng et al., 2023a). In this paper, we take advantage of RLAIF to establish an initial alignment and strategically incorporate human feedback to efficiently bring LLMs to the true alignment."}, {"title": "2.2. LLM Self-Improvement", "content": "To break the upper bound of LLMs, recent efforts have been devoted to enabling LLMs to self-improve. Self-Rewarding LMs (Yuan et al., 2024) and Math-shepherd (Wang et al., 2024b) demonstrate the possibility of LLM self-improvement with reward signals from itself. SELF-ALIGN (Sun et al., 2024b) uses a carefully written set of principles to guide LLMs through self-improvement. SER (Huang et al., 2024) starts with only a fraction of human annotations to achieve full-annotation performance by progressively generating additional training data for itself. However, these methods still suffer from the intrinsic upper bound of LLMs and self-improvement is not guaranteed for customized tasks. RLTHF, on the other hand, efficiently introduces new human intelligence into the improvement process, thereby ensuring that the improvement is not bounded by LLMs' initial lack of domain understanding."}, {"title": "3. Improving Human Alignment with RLTHF", "content": "RLTHF enhances human alignment in preference datasets used for training preference optimization techniques like DPO and PPO. It facilitates LLM training for various downstream tasks, including summarization, compliance, and grounding. Starting with an unlabeled preference dataset, RLTHF strategically integrates AI-generated labels with selective human feedback to maximize alignment while minimizing annotation effort. As illustrated in Figure 1, RLTHF operates in three stages: 1) Initial alignment, where an off-the-shelf LLM provides dataset labeling to establish a coarse task understanding, 2) Iterative alignment improvement, which leverages reward distribution by an RM to locate and rectify hard-to-annotate samples mislabeled by the LLM with selective human feedback while investing the correct LLM labels, 3) Transferring knowledge for downstream task, where the curated preference dataset is fed into the DPO pipeline or the trained RLTHF reward model is integrated into the PPO pipeline."}, {"title": "3.1. Initial Alignment", "content": "This stage aims to establish an initial coarse alignment in the unlabeled dataset using a general-purpose LLM, which provides preference annotations for each unannotated sample. Prior research suggests that model selection here depends on task complexity relative to the model's capability (Snell et al., 2024). While RLTHF is not found to be sensitive to the choice of model at this stage, a well-suited model can accelerate alignment convergence. The only assumption is that the general-purpose LLM possesses a basic understanding of the downstream task, enabling it to provide a rough initial alignment that serves as a seed for RLTHF.\nOur prompt for obtaining preference judgments from the LLM consists of three key components: 1) task description, 2) preference judgment principles provided by the end user, and 3) few-shot examples with optional chain-of-thought (CoT) reasoning. The prompt templates are detailed in Appendix A. We do not perform explicit fine-grained prompt tuning, as full visibility into the data may be restricted when offering fine-tuning services to third-party customers. However, to ensure that the selected LLM with our prompt attains a reasonable level of alignment, we perform an eyes-off validation using strategic human feedback, as detailed in Section 3.2.3.\nAs mentioned earlier, AI-generated feedback is prone to errors due to factors such as model biases from pre-training data, task complexity, and prompt optimization, which is also evident in our evaluation. When our ultimate goal is to customize an existing model through fine-tuning to align with end-user preferences, we inherently assume that an off-the-shelf LLM lacks comprehensive alignment with the end-user. However, RLTHF builds upon the initial AI-provided alignment and systematically refines it in subsequent stages to achieve oracle-level human alignment."}, {"title": "3.2. Iterative Alignment Improvement", "content": "In this stage, we refine the LLM-labeled preference dataset by iteratively training a reward model (RM) with selective human annotations to enhance alignment. Before diving into the details of this process, we first establish the premise for RM."}, {"title": "3.2.1. REWARD MODEL", "content": "Given a labeled preference dataset $D_A = \\{x_i, Y_{i,c}, Y_{i,r}\\}$, where i \u2208 [N], $x_i$ is the prompt, $Y_{i,c}$ and $Y_{i,r}$ denote the chosen and rejected completions, respectively, as labeled according to the annotator's preference, A. Here, if we represent the relative preference orientation of ith completion pair with x = [-1, +1], A is a N-dimensional vector consists of $[A]_i$, meaning that flipping the preferences of all completion pairs results in $D_{-A}$. To train an RM on this dataset, we can formulate the probability distribution of $Y_{i,c}$ being preferred over yi,r given $x_i$ as an input, following the Bradley-Terry (BT) model (David, 1963).\n$P(x > y) = \\sigma(r(x_i, Y_{i,c}) \u2013 r(x_i, Y_{i,r}))$\nwhere \u03c3(\u00b7) denotes the sigmoid function and r(\u00b7) denotes the reward function. Assuming the existence of a true deterministic reward function, the goal is to train the RM to learn this function and predict the reward, f(x, y). The RM training can be framed as a binary classification problem (Sun et al., 2024a), where a labeled pair of $p_{i,c} := (x_i, Y_{i,c})$ and $P_{i,r}:= (X_i, Y_{i,r})$ is passed to the model to predict the conditional class probability according to Eq. 1. This leads to the negative log-likelihood loss function for training.\n$L(\\hat{r}) = -E_{(x,y)~D}[log\\sigma(\\hat{r}(p_{i,c}) \u2013 \\hat{r}(p_{i,r}))]$\nIn essence, during the RM training, we pass a preference pair {$p_{i,c}, p_{i,r}$} labeled as $p_{i,c}$ winning over $p_{i,r}$ according to the annotator's preference A. Provided sufficient preference samples in a dataset, the RM learns the winning preference features of the data that determine the winner in a pair, captured in the reward function A."}, {"title": "3.2.2. LOOKING AT REWARD DISTRIBUTION", "content": "At this stage, we analyze the distribution of the predicted reward function (A) within the training preference dataset $D_A$. For each labeled preference pair {$p_{i,c}, P_{i,r}$}, we compute the reward score difference as \u0394\u06f8\u06f8 = ((pc) - A(pr)). It is important to note that A quantifies the relative preference score of a given pair in alignment with the annotator's preference orientation A, satisfying the property \u0394^r = -\u0394-Ar. By ranking all preference pairs in Da based on AAA, a monotonic reward distribution curve, denoted as v(\u0394\u039b^^), emerges. This distribution, as depicted in Figure 2a, provides insight into the model's reward assignment across the dataset, though for the moment, the legend in the graph can be disregarded.\nThe reward distribution curve (\u0394), derived from the training preference dataset DA, reflects the degree of alignment the RM (trained with optimal validation loss) has achieved during training across DA. The upper left region of the curve consists of samples with high positive \u0394\u039b\u039b, indicating strong agreement between the RM and the training preference labels A. This suggests that the RM effectively identifies and reinforces strong winning preference features in these samples, implying that these features were dominant in DA. Conversely, the bottom right region of the curve contains samples with very low or even negative AAA, signaling disagreement between the trained reward function and the training preference label for these samples. This misalignment arises from two primary factors. (1) Absence of strong features, where RM is not able to find any"}, {"title": "3.2.3. RLTHF LEVERAGING REWARD DISTRIBUTION", "content": "RLTHF trains the initial RM using a preference dataset filtered by a general-purpose LLM in the previous stage. We denote this dataset as DALLM where ALLM represents the preference labeling performed by the LLM. Since the RM training includes a validation set derived from DALLM, this ensures that the trained RM is broadly aligned with the LLM's preferences. We assume that the LLM has a coarse but reasonable understanding of preference judgments, particularly for relatively easy-to-annotate samples. As a result, the features of these samples dominate in ALLM. Based on our earlier discussion, the upper left region of the reward density curve, (\u2206ALLMALLM) contains high density of samples with prominent preference features, i.e., those that are easier for the LLM to annotate accurately. Before proceeding, we further validate that the LLM is at least roughly aligned with the end-user's preferences in terms of these easy-to-annotate samples. This step mitigates the risk of significant misalignment due to prompt curation or model selection. To achieve this, RLTHF automatically (details in the following section) samples a small subset (<0.1%) of preference data from the upper left region and gathers eyes-off user feedback. If human agreement on these samples is low, it signals a major misalignment between the user's preferences and the LLM's outputs. While we did not observe such cases in our experiments, this issue can be addressed by refining the judgment principles in the prompt. Updates can be made directly by the user, by incorporating verbose user feedback, or even through automated prompt optimization techniques (Kepel & Valogianni, 2024; Li et al., 2024).\nAt this stage, we can identify regions with a high density of correctly labeled samples by the LLM, i.e., those that are relatively easy for the LLM to annotate in alignment with human preference. Now, we turn our attention to two critical types of samples necessary for achieving fine-grained alignment: (1) hard-to-annotate samples and (2) samples mislabeled by the LLM w.r.t. the human preference Ah. Since the LLM was unable to correctly label these samples initially, the reward function ALLM cannot accurately capture their preference features. Consequently, these samples are expected to cluster around the bottom right region of the reward distribution curve 9 (\u2206ALLMY ALLM). To illustrate this, we refer to Figure 2a and 2b. Figure 2a shows (ALLMALLM) from one of our experiments. In this figure, we classify each sample pi \u2208 DALLM as either correctly or incorrectly labeled w.r.t. the human preference An, i.e., whether the preference assigned by ALLM is matching"}, {"title": "3.2.4. SELECTIVE HUMAN ANNOTATION", "content": "To enhance alignment from this stage, human annotation is necessary, but it must be done efficiently to maximize its impact. A straightforward approach is to refer to the accuracy density curve-annotations in the lowest accuracy region will yield the highest benefit. Thus, we could start annotating from the very bottom of the curve. However, as previously discussed, some samples in this region may exhibit preference features that are largely opposite to the dominant features captured by ALLM. These samples are highly likely to be mislabeled in ALLM (see Appendix B). Instead of requiring human annotation, we can simply flip the preference of these samples to correct the mislabeling. To estimate the positions of such samples, we take the reflection of the \"elbow\" point across the x-axis, as the elbow marks the region containing strong preference features. This reflected point, known as the \"reflection point\", always lies"}, {"title": "3.2.5. ITERATIVE APPROACH", "content": "The current reward function ALLM, trained on DALLM, exhibits an alignment gap with respect to An due to the presence of hard-to-annotate samples for the LLM and mislabeling by the LLM. Since we have identified ways to rectify these issues, we can refine D to improve alignment and train a new RM that better aligns with Ah. Now, the question is how to prepare the dataset for the next iteration of RM training. Suppose we are currently in iteration 0 (Itr-0) with DALLM and ALLM. For the iteration 1 (Itr-1) training dataset, DTT our primary goal is to include high-confidence samples that are well-aligned with An. The first choice is definitely human annotated samples from Itr-0. Additionally, another set of candidates can be drawn from the high-accuracy density region of v(\u2206ALLMALLM), specifically the region to the left of the \"elbow\", where the RM has learned strong preference features in alignment with Ah.\nAlthough these two sets of samples offer high precision, DAT will still face a data coverage issue. These two candidate sets represent samples with the longest feature distance, leaving gaps in intermediate regions. However, expanding the dataset by including samples from the middle region, i.e., right of the \"elbow\" and left of the \"knee\" risks introducing misaligned samples. Since the accuracy in this region is likely to be just above 50%, obtaining human annotations for these samples would be inefficient. Furthermore, as the number of samples annotated from the right of the knee is relatively small, their preference features are likely to be overshadowed by the dominant preference features of the high numbers of left-side samples. As a result, their features may not be effectively captured in AT1. To balance these trade-offs, we introduce two hyperparameters specific to RLTHF, allowing for a more controlled and effective"}, {"title": "3.3. Reward Knowledge Transfer", "content": "RLTHF progressively converges toward the comprehensive human preference through iterative RM training and strategic human annotation investment. As shown in Figure 2c and 2d, after five iterations, the reward distribution and accuracy curves closely align with the full-human annotation. Intermediate iteration curves can be found in Appendix B. The required number of iterations depends on the available human annotation and RM training budget. Notably, full-human alignment can sometimes be achieved before exhausting the annotation budget. In such cases, the samples selected for human annotation would largely lack distinct preference features, indicating that the model has effectively captured the human preference. Once desired alignment is achieved or the annotation budget is fully utilized, we proceed with fine-tuning the model for the downstream task. This can be done in two ways: 1) incorporating the final iteration RM into the PPO loop, or 2) labeling the whole dataset with the final RM and feeding the labeled dataset to a DPO pipeline."}, {"title": "4. Results", "content": "In this section, we present the results of our main experiments, conducted on two datasets: HH-RLHF (Bai et al., 2022a) and TL;DR (V\u00f6lske et al., 2017). Specifically, we compare RLTHF against two baselines: (1) Random, where samples are randomly selected for human annotation (matching the number of human-annotated samples in RLTHF), with the rest relying on AI feedback, and (2) Human, where all samples are annotated by humans. A detailed description of our experimental setup is provided in Appendix C."}, {"title": "4.1. Reward Modeling", "content": ""}, {"title": "4.1.1. OVERALL ALIGNMENT IMPROVEMENT", "content": "We here use GPT-40 for the initial alignment and evaluate RLTHF's iterative alignment improvements by measuring the preference accuracy of RMs trained with varying proportions of human annotations relative to the full dataset. We employ RLTHF on both the complete dataset and multiple down-sampled shards as described in \u00a7 3.2.5. For a given shard, we run RLTHF in an iterative manner infusing targeted human annotations in each iteration. We evaluate the trained RMs on a separate test dataset, ensuring a clear distinction from the training data, and report their preference accuracy in Figure 3.\nIn Figure 3, each data point for a shard corresponds to an iteration of RLTHF. The results show a consistent improvement in test preference accuracy across iterations, with significant early gains that gradually diminish as accuracy approaches the upper bound. Additionally, down-sampling enhances the efficiency of human annotations: RLTHF running on 1/2 and 1/4 shards outperforms its full-dataset counterpart when using the same number of human annotations. However, excessive down-sampling (e.g., 1/8 shard) may limit the achievable accuracy due to reduced data richness. For downstream task fine-tuning, we identify 1/4 shard as the optimal choice. Under this setting, RLTHF enhances preference accuracy on HH-RLHF from GPT-40's baseline of 74.7% to 89.6% with only 6% human annotations, and on TL;DR, from 78.8% to 88.0% with just 7% human annotations. We"}, {"title": "4.1.2. COMPARISON AGAINST THE BASELINES", "content": "We begin by using two different LLMs\u2014GPT-40 and GPT-40 mini\u2014for the initial AI labeling. We then employ two separate RLTHF pipelines, RLTHF (40) and RLTHF (40 mini), to improve alignment. To evaluate their effectiveness, we compare these pipelines against two baselines: (1) AI-only labeling (GPT-40 and GPT-4o mini), (2) Random human annotation, and (3) fully Human-annotated datasets (baseline setup details are provided in Appendix C.3).\nThe results of this experimental setup on two datasets are shown in Figure 4. RLTHF (40) consistently outperforms Random (40), as random human annotation proves ineffective in correcting AI mislabeling, resulting in only marginal improvements in test accuracy. Of particular interest is the \"Return on Investment (ROI)\"\u2014measured as the increase in test accuracy per unit of human annotation. With just 6% human annotation, RLTHF (40) achieves a 15.9\u00d7 and 5.3\u00d7"}, {"title": "4.1.3. EFFECTS OF HYPERPARAMETERS", "content": "Amplification Ratio. To investigate how the human amplification ratio a contributes to RLTHF, we fix the back-off ratio \u1e9e at 60% and conduct a controlled study on different amplification ratios.\nThe results for each dataset are shown in Figures 5a and 5d. We observe that both no amplification (a = 1) and excessive amplification (a = 8) of human annotations lead to suboptimal RM improvements. Specifically, lower amplification results in smaller improvements in the initial iterations, while in later iterations, this trend reverses. This is expected, as no or low amplification weakens the impact of human annotations, particularly in the early iterations when the total number of annotations remains low, while over-amplification skews the training data distribution and increases the risk of overfitting, especially in later iterations when the base number of annotations is already large.\nAlthough we use a fixed amplification ratio (Appendix C.4), we recommend starting with a higher amplification ratio and gradually reducing it as the number of accumulated annotations grows to achieve the best results.\nBack-off Ratio. To investigate how the back-off ratio B"}, {"title": "4.1.4. ABLATION STUDY", "content": "We conduct an ablation study to evaluate the necessity of each component in RLTHF and understand their contributions. All experiments in this study use a 4\u00d7 amplification ratio, a 60% back-off ratio, and 4% human annotations relative to a 1/4 shard per iteration. Our primary goal is to answer two key questions.\nDoes self-improvement alone work? To assess whether RLTHF can function effectively without human annotations, we set the annotation batch size to 0%, reducing RLTHF to a pure self-improvement scheme. The results in Table 1 indicate that self-improvement alone has inherent limitations and struggles to surpass GPT's baseline preference accuracy. Further evaluation on downstream tasks also confirms that this GPT-level preference accuracy is insufficient for aligning with human preferences. These findings suggest that self-improvement alone is inadequate for RLTHF's intended use case.\nAre amplification and back-off necessary? To examine the importance of RLTHF's human amplification and sanitization back-off mechanisms, we disable both simultaneously (since the individual ablation of amplification was already tested in \u00a7 4.1.3 with a = 1). The results in Table 1 show that RLTHF achieves only marginal improvement without these mechanisms, even with accumulating annotations. This result highlights the strong contributions of both amplification and back-off. Without sanitization back-off, incorrect labels persist in the training data, making it harder to identify valuable samples and further reducing data cleanliness. Without human amplification, an even greater number of correct samples is required to overturn incorrect ones, exacerbating the issue."}, {"title": "4.2. Downstream Tasks", "content": "To assess RLTHF's effectiveness on final downstream tasks, we conduct DPO training on RLTHF-prepared data and compare it against other baselines w.r.t. an SFT model. For RLTHF, we leverage the RM trained on a 1/4 shard, refined with targeted 6% human annotation for HH-RLHF and 7% for TL;DR, to regenerate preferences for the full dataset. We then train an LLM with DPO on this refined dataset. For the Random baseline, we introduce the same amount of randomly selected human annotations on top of AI labels into the DPO training dataset. Performance is evaluated using the pairwise win rate against SFT model outputs, calculated with AlpacaEval (Li et al., 2023b) running Claude 3.5 Sonnet (Anthropic, 2024) as a judge on test sets from HH-RLHF and TL;DR datasets (see Appendix D for details)."}, {"title": "5. Conclusion", "content": "In this work, we introduce RLTHF, an iterative reward model training approach that enhances alignment in preference datasets by strategically infusing human annotations, complemented by sanitized AI labeling. Through reward distribution analysis, we identify key samples for targeted human intervention, optimizing annotation efficiency. Our experiments demonstrate that RLTHF progressively improves alignment, converging toward comprehensive human alignment."}, {"title": "A. Prompt Templates for Initial Alignment", "content": "We structure the prompt using the following components to obtain the initial preference judgments from off-the-shelf LLMS (e.g., GPT-40, Gemini, etc.).\n1. Task description: User-provided short description and instructions for the task.\n2. Preference criteria: User-provided detailed criteria defining what is a good/bad response.\n3. Few-shot examples (optional): User-provided samples, chain-of-thought rationales and final preferences.\n4. Sample to annotate: The target sample to be annotated.\nBelow we provide our unified system prompt and user prompts used for HH-RLHF and TL;DR. Note that these prompts are not carefully tuned, as RLTHF should not assume ideal prompt engineering."}, {"title": "A.1. System Prompt", "content": "You are an AI assistant designed to compare and select the better response to a\ngiven task and input based on the provided criteria."}, {"title": "A.2. User Prompt for HH-RLHF", "content": "## TASK\nYou are a helpful and harmless Assistant. Given the context, provide the most\nhelpful and harmless response.\n## CRITERIA\n**Helpfulness**: The response should be relevant, clear, and effective in\naddressing the human user's task or question within the given context.\n**Honesty**: The response should be accurate, truthful, and reliable.\n**Harmlessness**: The response should be safe, considerate, and free from\npotential harm or negativity.\n## INPUT\n{input}\n## CANDIDATE RESPONSES\n**Response A**:\n**Response B**:\n## INSTRUCTIONS\n{response_a}\n{response_b}\n1. Review the task, criteria, and input thoroughly.\n2. Evaluate each response based on how well it aligns with the criteria.\n3. Decide which response (A or B) is better aligned with the criteria.\n4. If both responses seem equal in quality, choose the one that you judge to be\nthe best.\n5. Provide your answer in the following format:\nrationale: preference: \"Response A\" or \"Response B\""}, {"title": "A.3. User Prompt for TL;DR", "content": "## TASK\nSummarize the given reddit post.\n## CRITERIA\nWhat makes for a good summary? Roughly speaking, a good summary is a shorter\npiece of text that has the essence of the original { tries to accomplish the same\npurpose and conveys the same information as the original post. We would like you\nto consider these different dimensions of summaries:\n**Essence:** is the summary a good representation of the post?\n**Clarity:** is the summary reader-friendly? Does it express ideas clearly?\n**Accuracy:** does the summary contain the same information as the longer post?\n**Purpose:** does the summary serve the same purpose as the original post?\n**Concise:** is the summary short and to-the-point?\n**Style:** is the summary written in the same style as the original post?\nGenerally speaking, we give higher weight to the dimensions at the top of the\nlist. Things are complicated though - none of these dimensions are simple\nyes/no matters, and there aren't hard and fast rules for trading off different\ndimensions.\n## INPUT\n{input}\n## CANDIDATE RESPONSES\n**Response A**:\n**Response B**:\n## INSTRUCTIONS\n{response_a}\n{response_b}\n1. Review the task, criteria, and input thoroughly.\n2. Evaluate each response based on how well it aligns with the criteria.\n3. Decide which response (A or B) is better aligned with the criteria.\n4. If both responses seem equal in quality, choose the one that you judge to be\nthe best.\n5. Provide your answer in the following format:\nrationale:  preference: \"Response A\" or \"Response B\""}, {"title": "B. Iterative Alignment Improvement", "content": "In Figure 6, we show all the reward distribution curves and accuracy density curves from all the iterations that we ran on the HH-RLHF dataset."}, {"title": "C. Experimental Setup", "content": ""}, {"title": "C.1. Data Preparation", "content": ""}, {"title": "C.1.1. DATASETS", "content": "We use the following datasets in our experiments:\n\u2022 HH-RLHF: We use Anthropic's helpful and harmless human preference dataset (Bai et al., 2022a), which includes 161K training samples. Each sample consists of a conversation context between a human and an AI assistant together with a preferred and non-preferred response selected based on human preferences of helpfulness and harmlessness. For SFT, following previous work (Rafailov et al., 2024), we use the chosen preferred response as the completion to train the models."}, {"title": "C.1.2. FLIPPING HUMAN PREFERENCES", "content": "It has been observed that both datasets contain a significant number of incorrect preferences due"}]}