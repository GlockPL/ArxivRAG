{"title": "AVG-LLAVA: A MULTIMODAL LARGE MODEL WITH ADAPTIVE VISUAL GRANULARITY", "authors": ["Zhibin Lan", "Liqiang Niu", "Fandong Meng", "Wenbo Li", "Jie Zhou", "Jinsong Su"], "abstract": "Recently, when dealing with high-resolution images, dominant LMMs usually divide them into multiple local images and one global image, which will lead to a large number of visual tokens. In this work, we introduce AVG-LLaVA, an LMM that can adaptively select the appropriate visual granularity based on the input image and instruction. This approach not only reduces the number of visual tokens and speeds up inference, but also improves the overall model performance. Specifically, we introduce the following modules based on LLaVA-NeXT: (a) a visual granularity scaler that includes multiple pooling layers to obtain visual tokens with different granularities; (b) a visual granularity router, which includes a Transformer layer, an MLP layer, and a voter layer, used to select the appropriate visual granularity based on the image and instruction. Furthermore, we propose RGLF, a novel training paradigm that aims at aligning the granularity predicted by the router with the preferences of the LMM, without the need for additional manually annotated data. Extensive experiments and analysis show that AVG-LLaVA achieves superior performance across 11 benchmarks, as well as significantly reduces the number of visual tokens and speeds up inference (e.g., an 85.3% reduction in visual tokens and a 2.53x increase in inference speed on the AI2D benchmark). The code will be available at https://github.com/DeepLearnXMU/AVG-LLAVA.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, the field of artificial intelligence (AI) has witnessed a significant advancement in Large Multimodal Models (LMMs) (OpenAI, 2023b; Zhu et al., 2023; Dai et al., 2023; Liu et al., 2023a; 2024a), marking a further step toward artificial general intelligence (AGI). Most existing LMMs follow the structure of LLaVA (Liu et al., 2023a; 2024a), which includes a vision encoder to embed images into visual tokens and a connector to map them into the LLM's word embedding space. Subsequently, these visual tokens are fed into a Large Language Model (LLM) (Touvron et al., 2023; OpenAI, 2023a; Chiang et al., 2023) for multimodal understanding and reasoning, alongside the word embeddings.\nDue to the limitations imposed by the fixed aspect ratio (e.g., 1:1) and low resolution (e.g., 336\u00d7336) used by visual encoders (e.g., CLIP-ViT (Radford et al., 2021)), earlier LMMs face challenges in processing high-resolution images with different aspect ratios. To deal with this limitation, dominant models, such as LLaVA-NeXT (Liu et al., 2024b), dynamically divide each input high-resolution image into multiple local images. These local images are encoded separately, and their tokens are then concatenated with the tokens of the original global image. This approach will lead to longer visual token sequences, such as 2880 visual tokens for a 672\u00d7672 image. However, in practice, such fine-grained visual information is not always necessary, and in some cases, coarse-grained visual information can even be more beneficial for model predictions. For instance, as shown in Figure 1, when the model is asked to recognize the number on the jersey, it requires relatively fine-grained visual information. In contrast, determining the color of the jersey only necessitates coarse-grained visual information.\nIn this paper, we propose Adaptive Visual Granularity LLaVA (AVG-LLaVA), an LMM that can adaptively select the appropriate visual granularity based on the input image and instruction. The basic intuition behind our model is that humans only scrutinize images carefully when answering difficult questions; otherwise, a brief glance is sufficient. As displayed in Figure 2, AVG-LLaVA extends LLaVA-NeXT with a visual granularity scaler and a visual granularity router. The visual granularity scaler performs multiple rounds of pooling on visual tokens without training, each time halving the number of visual tokens, thus obtaining a series of visual features with different granularities. The visual granularity router adaptively selects the appropriate visual granularity features based on the input multi-granularity visual features and text features. By doing so, for images and instructions that do not require fine-grained details, the number of visual tokens can be reduced, which not only speeds up inference but also improves performance. This performance enhancement likely stems from the reduction of redundant information, as selecting the appropriate visual granularity makes it easier for the model to answer questions based on images effectively.\nBesides, we observe that it is challenging to train the visual granularity router directly through visual instruction tuning (Liu et al., 2023a). This may be because the router cannot learn the distinctions between different visual granularities from visual instruction tuning, making it difficult to learn how to select the most appropriate visual granularity based on the image and instruction. To deal with this issue, we propose a novel training paradigm called Ranking Granularity to align LMM Feedback (RGLF). This paradigm aligns router probabilities of multiple granularities with LMM preferences by a ranking loss (Hadsell et al., 2006; Hopkins & May, 2011; Liu et al., 2022), effectively aiding the router in distinguishing between different visual granularities and selecting the appropriate one.\nWe further evaluate AVG-LLaVA on 11 benchmarks including tasks from various types (e.g., general VQA and text-oriented VQA, etc.). Extensive experimental results show that AVG-LLaVA can effectively reduce the number of visual tokens and improve inference speed (e.g., an 85.3% reduction in visual tokens and a 2.53\u00d7 increase in inference speed on the AI2D benchmark) while achieving better performance under the same base LLM."}, {"title": "2 RELATED WORK", "content": "Large Multimodal Models. LLMs such as GPT-4 (OpenAI, 2023a), LLaMA (Touvron et al., 2023), and Gemini (Team et al., 2023) have achieved significant success in language understanding and generation. Benefiting from this, multimodal large models (LMMs) have garnered widespread attention. Flamingo (Alayrac et al., 2022), BLIP-2 (Li et al., 2023b), and LLaMA-adapter (Zhang et al., 2023) integrate a frozen visual encoder and trainable modules into a LLM, extending it into a LMMs. These models are then fine-tuned using plain image-text pairs, enabling them to process and perceive visual content. To further improve instruction-following abilities and response quality, LLaVA (Liu et al., 2023a) fine-tunes the entire model using visual instruction data generated by GPT-4. However, since these LMMs rely on CLIP-ViT to process images at a fixed resolution (e.g. 336x336), it hinders the LMMs from perceiving image details at higher resolutions.\nHigh-Resolution LMMs. To perceive images with higher resolutions, Qwen-VL (Bai et al., 2023) increases the input resolution of the visual encoder to 448\u00d7448 and introduces an additional training stage. Along this line, both Vary (Wei et al., 2023) and Mini-Gemini (Li et al., 2024a) include two vision encoders: one is an additional introduced high-resolution vision encoder, and the other is the original low-resolution vision encoder. Unlike the methods mentioned above, SPHINX (Lin et al.,"}, {"title": "3 OUR MODEL", "content": null}, {"title": "3.1 MODEL ARCHITECTURE", "content": "As shown in Figure 2, in addition to the visual encoder, visual-language connector, and LLM, AVG-LLAVA introduces two additional modules on top of LLaVA-NeXT: the visual granularity scaler and the visual granularity router. The key components will be elaborated in the following.\nHigh-Resolution Image Encoding. Given an input image $I \\in \\mathbb{R}^{H \\times W \\times 3}$, we follow common practice (Liu et al., 2024b) to divide it into multiple smaller local images $I_{local} \\in \\mathbb{R}^{H_p \\times W_p \\times 3}$, where $H_p$ and $W_p$ are the resolution that the vision encoder is originally trained for. Then, these"}, {"title": "3.2 MULTI-STAGE TRAINING", "content": "To effectively train our model, we carefully design a multi-stage training strategy, which consists of four stages, as illustrated in Figure 3."}, {"title": "Stage 1: Pretraining", "content": "During this stage, we only pretrain the vision-language connector on a plain image-caption dataset. Formally, we define the following cross-entropy loss for the next token prediction:\n$\\mathcal{L}_{1}=-\\sum_{t=1}^{T} \\log P(x_{t} | X_{v}, X_{c,<t}),$"}, {"title": "Stage 2: Visual Instruction Tuning", "content": "In the second stage, we jointly train the visual encoder, vision-language connector, and LLM on high-quality visual instruction data. In this way, the LLM can be converted into an LMM, which is able to complete various multimodal tasks. Specifically, we perform next-token predictions with the following cross-entropy loss only on the answering part\n$\\mathcal{L}_{2}=-\\sum_{t=1}^{T} \\log P(x_{t} | X_{v}, X_{instruct}, X_{a,<t}),$"}, {"title": "Stage 3: Multi-Granularity Visual Instruction Tuning", "content": "Following the previous stages, we introduce the visual granularity scaler. As described in Section 3.1, this module does not contain trainable parameters and thus does not need to be trained. Therefore, we use the same data as in the stage 2 to train the visual encoder, vision-language connector, and LLM, enabling them to perceive and process visual features of N different granularities. Formally, the loss at this stage is formulated as\n$\\mathcal{L}_{3}=\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T}-\\log P(x_{t} | X_{v}^{i}, X_{instruct}, X_{a,<t}).$"}, {"title": "Stage 4: Ranking Granularity to Align LMM Feedback", "content": "Lastly, we introduce the visual granularity router into the model training, where all other modules are frozen, and only the router is trained. This stage allows the model to select the appropriate visual granularity based on the input image and instruction. Intuitively, a straightforward approach to training the router is to use the same visual instruction fine-tuning method as in previous stages. However, we find that the router trained with this method performs poorly. This could be due to the difficulty of visual instruction fine-tuning in effectively enabling the router to learn the differences between different visual granularities.\nTo address the above issue, we propose RGLF, as illustrated in Figure 4, where the router is trained with a ranking loss, utilizing the feedback from the LMM fine-tuned with multi-granularity visual instructions as the ranking criterion. Concretely, for the given image and instructions, we let the LMM predict answers using visual tokens of different granularity $X$ and calculate their respective log probabilities. Then, based on these log probabilities, we sort $X^{1}, X^{2}, \\ldots, X^{N}$ in a descending order to obtain $X^{\\overline{1}}, X^{\\overline{2}}, \\ldots, X^{\\overline{N}}$. Given the visual tokens $X^{\\overline{i}}$ of the $\\overline{i}$-th granularity, we directly consider those tokens $\\left(X^{\\overline{1}}, X^{\\overline{2}}, \\ldots, X^{\\overline{i-1}}\\right)$ ranked above it as positive examples and the remaining tokens $\\left(X^{\\overline{i+1}}, X^{\\overline{i+2}}, \\ldots, X^{\\overline{N}}\\right)$ as negative ones. Afterwards, we use the router to give scores (log probability) $s_{i}$ for each $X$:\n$s_{i}=\\log P\\left(g_{i} | X_{v}, X_{\\text {instruct }}\\right),$\nwhere $g_{i}$ denotes the $i$-th granularity predicted by the router based on multi-granularity visual tokens $X$ and filtered instruction tokens $X_{\\text {instruct }}$. Since we expect the router to assign higher probabilities to more appropriate visual granularities, the ranking loss is defined as follows:\n$\\mathcal{L}_{\\text {rank }}=\\sum_{i=1}^{N} \\sum_{j>i} \\max \\left(0, s_{j}-s_{i}+\\Delta_{i j}\\right),$\nwhere $\\Delta_{i j}$ is the margin calculated as the difference in log probabilities between the answers predicted by the LLM using visual tokens of the $i$-th and $j$-th granularities:\n$\\Delta_{i j}=\\frac{j-i}{|T|} \\sum_{t=1}^{T}\\left(\\log P\\left(x_{t} | X_{v}^{i}, X_{\\text {instruct }}, X_{a,<t}\\right)-\\log P\\left(x_{t} | X_{v}^{j}, X_{\\text {instruct }}, X_{a,<t}\\right)\\right).$\nWhen the preference of $X^{i}$ is only slightly worse than $X^{j}$, the margin will be small. Conversely, when $X^{i}$ is significantly worse than $X^{j}$, the margin will correspondingly increase. In this way, we can dynamically adjust the margin to obtain adaptively penalty degrees between different pairs.\nIn addition to aligning with the LMM preference ranking, it is also desirable for the router to select the optimal visual granularity. Therefore, we add a cross-entropy loss to let the router learn the prediction of granularity with the highest log probability from the LMM, defined as follows:\n$k=\\arg \\max _{i} \\sum_{t=1}^{T} \\log P\\left(X_{t} | X_{v}^{i}, X_{\\text {instruct }}, X_{a,<t}\\right),$\n$\\mathcal{L}_{c e}=-\\log P\\left(g_{k} | X_{v}, X_{\\text {instruct }}\\right).$\nIn summary, the total loss is defined as the weighted sum of two losses:\n$\\mathcal{L}_{4}=\\mathcal{L}_{\\text {rank }}+\\alpha \\mathcal{L}_{c e},$\nwhere $\\alpha$ is the hyperparameters used to maintain the balance between the ranking loss $\\mathcal{L}_{\\text {rank }}$ and cross-entropy loss $\\mathcal{L}_{c e}$."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 SETTINGS", "content": "Training Datasets. Note that in this work, we mainly focus on investigating the effectiveness of automatic visual granularity selection in reducing the number of visual tokens and improving model performance. Therefore, during the first stage, we also use CC-595K image-text pairs for model training, as implemented in LLaVA-NeXT (Liu et al., 2024b). In the subsequent training stages, we also hope to use the same data as LLaVA-NeXT. However, the real user interaction data used for visual instruction fine-tuning in LLaVA-NeXT are not open-sourced, so we opt to extract 200K samples from ALLaVA (Chen et al., 2024a) dataset as a substitute. Although LLaVA-NeXT replaces TextVQA (Singh et al., 2019) with DocVQA (Mathew et al., 2021) and SynDog-EN (Kim et al., 2022), the TextVQA has already been included in the training data of most existing LMMs. Consequently, we choose to retain it to ensure a fair comparison with other models\u00b2. In total, the visual instruction fine-tuning data we use contains 1M image-text pairs."}, {"title": "4.2 MAIN RESULTS", "content": "General VQA Benchmarks. The results in Table 1 show that AVG-LLaVA outperforms all standard-resolution LMMs on the General VQA benchmarks and other high-resolution LMMs on VizWiz. Although it does not achieve the best results on GQA and ScienceQA, it is important to note that AVG-LLaVA uses fewer visual tokens compared to other high-resolution models, and this comparison will be detailed in Section 4.3.\nText-oriented VQA Benchmarks. In this category of benchmarks, as illustrated in Table 1, except for TextVQA, AVG-LLaVA outperforms all other comparison models. Back to TextVQA, AVG-LLaVA achieves the second-best result, only trailing behind Mini-Gemini-HD. Notably, Mini-Gemini-HD utilizes more than twice the amount of data during the pretraining and approximately 1.5 times the amount of data during the visual instruction fine-tuning compared to AVG-LLaVA.\nGeneral Multimodal Benchmarks. Compared to traditional VQA datasets, this type of benchmark covers a broader range of evaluation aspects, requiring models to possess more complex perception and reasoning capabilities. As summarized in Table 2, AVG-LLaVA surpasses all other models, exhibiting superior overall performance and highlighting its adaptability and effectiveness across various disciplines. Specifically, AVG-LLaVA outperforms the second best model by 9.4 and 6.1 on MME and MMEC, respectively, and by 1.9 and 1.2 on MMB and MMBCN, respectively. Moreover, AVG-LLaVA's performance on the POPE and MMMU benchmarks demonstrates its ability to reduce hallucinations and perform complex reasoning."}, {"title": "4.3 COMPUTATIONAL EFFICIENCY", "content": "To validate the effectiveness of dynamic visual granularity selection, we compare AVG-LLaVA with LLaVA-NeXT in terms of visual token number and inference speed across multiple benchmarks. Specifically, for each type of benchmark, we select three benchmarks for comparison, and report the reduction in the number of visual tokens per grid and the actual speedup during inference. As shown in Table 3, except for text-intensive VQA benchmarks that require very fine-grained visual information, such as TextVQA and ChartVQA, AVG-LLaVA significantly reduces the number of visual tokens and improves inference speed across other benchmarks. Particularly, on the AI2D benchmark, AVG-LLaVA achieves better performance than LLaVA-NeXT while using only 14.7% of the visual tokens, and the inference speed increased by 2.53 \u00d7.3 Notably, even with the addition of two extra modules, there is no significant slowdown in inference speed on the ChartVQA benchmark when using a comparable number of visual tokens. Moreover, AVG-LLaVA only increases the number of parameters by 1.66% compared to LLaVA-NeXT."}, {"title": "4.4 ROUTING VISUALIZATION", "content": "To further understand the differences in the granularity selection of AVG-LLaVA across different benchmarks, we visualize the proportion of visual tokens selected at each granularity level for all benchmarks. Figure 5 shows the visualization results, it is evident that different tasks tend to favor different visual granularity, which is consistent with our expectations. In the case of text-intensive benchmarks like TextVQA, ChartQA, and DocVQA, the model requires fine-grained visual information, so the router predominantly selects the finest visual granularity. On the other hand, for benchmarks involving object-level questions, such as AI2D and MMMU, the model may find it easier to answer correctly by utilizing coarse-grained visual information. Additionally, we observe that the granularities with 72 and 288 visual tokens are rarely selected. However, we find that incorporating these granularities of visual tokens helps the model progressively learn to utilize visual tokens"}, {"title": "4.5 ABLATION STUDY", "content": "In order to validate the effectiveness of our designed modules and training paradigm, we conduct the following ablation analysis.\nAdaptive Visual Granularity vs. Fixed Visual Granularity. We first delve into the proposed adaptive visual granularity router and report results in Table 4(a). It is clear that, compared to fixed visual granularity, adaptive visual granularity shows significant improvement on ScienceQA, MME, and MMB. It is worth noting that, in addition to performance improvement, adaptive visual granularity can also significantly reduce the number of visual tokens and increase the model's inference speed, as reported in Section 4.3.\nRouter Granularity Selection vs. Random Granularity Selection. In Table 4(b), we replace the granularity selected by the router with randomly-selected granularity. The results show that visual granularity router can indeed select a relatively appropriate granularity based on the input image and instruction, thereby significantly enhancing model performance.\nImpact of Router Input. The instruction plays a crucial role in granularity selection. To validate this, we remove the instruction from the router input. As shown in Table 4(c), a clear performance degradation rises when solely using image as input (e.g, -12.4 on ChartQA), illustrating the importance of choosing granularity based on input image and instruction.\nImpact of Granularity Range. In Section 4.4, we observe that granularities with 72 and 288 visual tokens are rarely selected, therefore we remove the visual tokens of these two granularities. As shown in Table 4(d), this change leads to a decrease in model performance, proving that introducing these granularities benefits the model's progressive learning to utilize features of different visual granularities and distinguish among various visual granularities.\nImpact of Router Training Methods. We directly train the router using visual instructions fine-tuning, applying the same loss function as in Stage 2. Unlike our original approach where the router is directly supervised by LMM feedback, this variant computes the loss on the LMM and backpropagates the gradient to the router using the Gumbel-Softmax technique (Jang et al., 2017). The results in Table 4(e) show that direct feedback from the LMM allows the router to better distinguish the advantages and disadvantages of different granularities, thereby enabling it to select an appropriate granularity.\nImportance of Ranking Granularity. In Table 4(f) and Table 4(g), we remove the cross-entropy loss and ranking loss during the fourth stage, respectively. The results indicate that both types of loss are beneficial to model training and are complementary to each other, between which the ranking loss is more crucial. This underscores the necessity to train the router by ranking granularity to align LMM feedback."}, {"title": "4.6 HYPERPARAMETER ANALYSIS", "content": "We experimentally explore the influence of the filtered instruction token number k and the cross-entropy loss weight \u03b1 on model performance. As shown in Figure 6, the model performance is significantly affected when k is too small or too large. This may be due to the fact that too few instruction tokens provide insufficient text information, while too many tokens will introduce more noise. Figure 7 indicates that our approach is relatively robust to \u03b1 and setting a smaller \u03b1 is able to consistently enhance model performance, making our training method easy to apply."}, {"title": "5 CONCLUSION", "content": "In this work, we propose AVG-LLaVA, an LMM that can adaptively select appropriate visual granularity based on input image and instruction. AVG-LLaVA builds upon LLaVA-NeXT by introducing a visual granularity scaler and a visual granularity router, which are used to obtain multi-granularity visual features and select the appropriate visual granularity based on image and instruction, respectively. Besides, we introduce RGLF, which aligns router-predicted probabilities of multiple granularities with LMM preferences by a ranking loss, effectively helping the model learn to distinguish between different granularities based on image and instruction. Experimental results show that AVG-LLaVA not only exhibits superior performance across 11 benchmarks, but also significantly reduce the number of visual tokens and speed up inference in tasks that do not require fine-grained information. Particularly, on the AI2D benchmark, it reduces the number of visual tokens by 85.3% and speeds the inference by 2.53x. We hope our work can inspire more attention to the visual granularity in LMMs.\nWhile AVG-LLaVA has achieved good results, there is still considerable potential to be further explored. On text-intensive benchmarks, the model tends to select the finest-grained visual tokens, which may be due to the pooling directly reducing half of the tokens, resulting in significant differences in granularity size. In the future, we plan to design a more suitable granularity scaling network to provide richer visual granularities."}]}