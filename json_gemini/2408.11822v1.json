{"title": "State-of-the-art in Robot Learning for Multi-Robot Collaboration: A\nComprehensive Survey", "authors": ["Bin Wu", "C. Steve Suh"], "abstract": "With the continuous breakthroughs in core tech-\nnology, the dawn of large-scale integration of robotic systems\ninto daily human life is on the horizon. Multi-robot systems\n(MRS) built on this foundation are undergoing drastic evo-\nlution. The fusion of artificial intelligence technology with\nrobot hardware is seeing broad application possibilities for\nMRS. This article surveys the state-of-the-art of robot learning\nin the context of Multi-Robot Cooperation (MRC) of recent.\nCommonly adopted robot learning methods (or frameworks)\nthat are inspired by humans and animals are reviewed and\ntheir advantages and disadvantages are discussed along with\nthe associated technical challenges. The potential trends of\nrobot learning and MRS integration exploiting the merging of\nthese methods with real-world applications is also discussed at\nlength. Specifically statistical methods are used to quantitatively\ncorroborate the ideas elaborated in the article.", "sections": [{"title": "I. INTRODUCTION", "content": "With the advancement of technology and the development\nof artificial intelligence [1], [2], [3], robot learning has\nbecome one of the key factors driving the progress of\nrobotic technology. Especially in the field of MRS [4], robot\nlearning has shown great potential and application value.\nMRS complete complex tasks by coordinating the actions\nof multiple robots, offering higher efficiency, reliability, and\nflexibility compared to single robot systems (SRS) [5], [6].\nHowever, with the diversification of application scenarios and\nthe increase in task requirements, the learning mechanisms in\nMRS face many challenges, including collaborative learning,\ncommunication constraint, environmental adaptability, and\nalgorithm's ability to generalize [7], [8].\nThis article reviews the latest research in robot learning\nwithin MRS, including theoretical foundations, key technolo-\ngies, applications, challenges faced and paths being explored.\nAn extensive review of existing literature allows the core\nissues and proper solution strategies to be identified along\nwith the performance and limitations of different learning\nmethods for practical applications.\nFirst, the basic concepts of MRS and robot learning are\nintroduced and their importance in current technological\ncontext is stated. Next, the design principles of learning\nmechanisms in MRS is explored in-depth, including, but\nnot limited to, technologies such as Reinforcement Learning\n(RL), Transfer Learning (TL), and Imitation Learning (IL).\nMuch insight is gained from evaluating the advantages and\ndisadvantages of different learning strategies essential to\ninspiring future research and charting the path moving for-\nward. Moreover, successful case studies of MRS in specific\napplications are examined, such as automated warehousing,\nsearch and rescue, environmental monitoring, and precision\nagriculture, to showcase the effectiveness and applicability of\nrobot learning technologies in solving real-world problems.\nThe main challenges encountered in implementing multi-\nrobot learning systems, including resource allocation, task\ndecomposition, learning efficiency, and system scalability\nare also discussed. Finally, the article looks forward to the\nvarious directions of future development of robot learning in\nMRS, emphasizing the importance of interdisciplinary col-\nlaboration and how the integration of artificial intelligence,\nmachine learning, control theory, and cognitive science will\nelevate the field to a greater level of development. The survey\npaper should provide researchers in the related fields with a\ncomprehensive reference framework, inspire more innovative\nresearch and application, and jointly promote the progress of\nMRS and robot learning."}, {"title": "II. DEFINITION AND SCOPE", "content": "In this section, more detailed definitions of the key\nconcepts addressed in this article are provided along with\ndelineation of the scope of current issues. This is done to\nhelp establish a unified cognitive basis and also facilitate a\ndeeper discussion of the issues based on the basis.\nFrom a system-level perspective, the advantages of MRS\nover SRS are evident [4], [9]. An MRS refers\nto a system composed of two or more robots that can\ncomplete specific tasks or objectives through collaboration\nor competition. In such systems, each robot may have\nunique capabilities and limitations, and they achieve col-\nlective intelligence and action through communication and\ncoordination. Research on MRS mainly focuses on how to ef-\nfectively design and implement interaction, communication,\nand collaboration mechanisms among robots to improve the\nefficiency and effectiveness of the entire system [9].\nAn MRS can be represented as a tuple\n$(N, S, A, T, R,C,G)$.\n\u2022 Robot Set: Let $N = {1, 2, ..., n}$ be the set of robots in\nan MRS, where each $i$ represents an independent robot\nentity.\n\u2022 State Space: For each robot $i$, its state can be repre-\nsented by an element in the state space $S_i$. The state"}, {"title": "B. Multi-robot Cooperation", "content": "The concept and mathematical definition of MRC prob-\nlems can be further developed based on the foundational\nframework of MRS in the last section [5], [6]. MRC prob-\nlems involve a group of robots that\nshare information, coordinate actions, and make decisions\ntogether to achieve a common task or goal. The primary objective of\ncooperation is to utilize the collective capabilities of multiple\nrobots to accomplish tasks that are not possible or inefficient\nfor a single robot. This cooperation may include aspects such\nas task allocation, joint decision-making, resource sharing,\ncoordinated actions, and goal sharing. To achieve this, a\ncommon goal function $(G: S\\rightarrow \\mathbb{R})$ can be defined to\nmeasure and observe the performance of the entire system\nin achieving the common goal.\n\u2022 Collaboration Strategy: Define a set of strategies $\\Pi =$\n${\\pi_1, \\pi_2,..., \\pi_n}$, where each $\\pi_i : S \\rightarrow A_i$ is a decision\nrule guiding robot $i$ in choosing actions in a given state.\n\u2022 Communication Model Extension: Extend the commu-\nnication model $C$ to $C_e : S\\times N \\rightarrow P(N)$, where $P(N)$\nis the power set of the robot set $N$, indicating which\ngroups of robots can communicate in a given state.\n\u2022 Joint Action: Define a joint action mapping $a: S\\times\\Pi \\rightarrow$\n$A$, combining the strategies of all the robots and current\nstate to produce the action of the entire system.\n\u2022 Collaborative Benefit: Introduce a utility function $U :$\n$S\\times A\\rightarrow \\mathbb{R}$ to evaluate the performance of the system\nas a whole under a given state and action.\n\u2022 Constraints: Consider the constraints of interactions\nand cooperation among robots, such as communication\nrange, resource limitation, and task dependency."}, {"title": "C. Robot Learning", "content": "When discussing robot learning, researchers often first\nconsider it as an interdisciplinary field bringing together\nmachine learning and robotics, which is undoubtedly correct.\nHowever, before clarifying the essence of robot learning,\nwhat learning is must be clarified. In sociology [10], learning\nis usually defined as the process by which individuals or\ngroups acquire social behavior patterns through the process\nof socialization. This includes the internalization of social\nnorms, values, language, skills, and behavior patterns. Soci-\nologists emphasize the impact of social structures and culture\non the individual learning process. The definition of learning\nin anthropology [11] emphasizes the process of cultural\ntransmission and adaptation. Learning is seen as the way\nindividuals acquire knowledge, skills, beliefs, and behavior\npatterns from their cultural environment. Anthropologists\noften focus on the differences in learning across cultures\nor ethnic groups and how these differences affect the adap-\ntation and development of individuals and communities. In\npsychology [12], learning is typically defined as a relatively\npermanent change in behavior or thought patterns produced\nby experience. This definition emphasizes an individual's\nresponse and adaptation to environmental stimuli, including\ncognitive learning, emotional learning, and behavioral learn-\ning.\nIn traditional disciplines, there are far more definitions of"}, {"title": "III. LEARNING METHOD", "content": "The concept of machine learning was inspired by the\nways human and animal learn [16], [17]. Researchers design\nalgorithms and machines by simulating how the human brain\nworks, hoping that machines can acquire knowledge and\nskill through observation and experience. The classification\nof learning methods discussed in this section is also based on\na logical division, mirroring human or animal learning meth-\nods. Afterward various robot learning methods is discussed\nin the context of MRS.\nFirst, one can draw from psychology and neuroscience\nthe following classifications of human and animal learning\nmethods:\n\u2022 Classical Conditioning: Conducted by the Russian phys-\niologist Pavlov [18], who discovered how animals learn\nthrough associating stimuli with responses via experi-\nments.\n\u2022 Operant Conditioning: Introduced by B.F. Skinner in his\nbook [19], the concept involves increasing or decreasing\nthe frequency of specific behaviors through rewards and\npunishments.\n\u2022 Observational Learning: Individuals learn by observing\nothers' behaviors and their consequences [12].\n\u2022 Cognitive Learning: Emphasizes the role of exploration\nand problem-solving in the learning process [20].\n\u2022 Sociocultural Learning: Discusses how social interac-\ntions influence cognitive development [21].\n\u2022 Affective Learning: Explores how emotions affect learn-\ning and memory, especially the neural mechanisms of\nfear responses [22].\nAlthough these do not cover all known ways of learning\nin human and animal, however, they do provide a brief\nclassification basis for robot learning in MRS. Mapping the\nlearning methods of human and animal to those of machine\nlearning provides an interesting perspective on how robots\nemulate natural learning processes. The classification of learning methods based on carbon-based\nlife forms such as human and animal can be mapped onto\nthe principles of the learning processes of existing machine\nlearning silicon \u2013 based methods.\n1) Classical Conditioning: Classical conditioning in-\nvolves learning through the association between stimuli. In\nthe field of machine learning, the mechanism most similar\nto this associative learning is supervised learning [23], [24].\nThis type of machine learning involves mapping between\ninputs (similar to conditioned stimuli) and outputs (similar\nto conditioned responses). Training data includes inputs and\ntheir corresponding outputs, and the model learns from these\ndata to predict the output of new inputs. For example, when\nusing neural networks for image recognition, the model\nlearns the relationship between patterns in images and labels,\nsimilar to the associative learning between stimuli in classical\nconditioning.\n2) Operant Conditioning: Operant conditioning focuses\non the relationship between behaviors and consequences,\nwhich is very similar to the concept of RL [25], [26]. In\nRL, an agent learns behavior strategies through interaction\nwith the environment to maximize certain cumulative re-\nwards. This learning process involves exploration (trying new\nbehaviors to discover effective strategies) and exploitation\n(using known strategies to obtain rewards). The mechanism\nof RL is similar to operant conditioning, relying on the\nconsequences of behaviors (rewards or punishments) to form\nor change behaviors.\n3) Observational Learning: In machine learning methods\nanalogous to observational learning, unsupervised learning\n[23], [27] and imitation learning [15], [28] stand out as\nparticularly representative. Observational learning involves\nobserving the behaviors and outcomes of others and learning\nfrom them. If one likens certain aspects of observational\nlearning to feature learning or clustering in the field of\nmachine learning, then the process in unsupervised learning,\nwhich involves identifying patterns and structures from data\nwithout explicit labels or feedback, shares strong similarities\nwith observational learning. Inference learning (IL) is a\nmethod that allows robots or software agents to observe\nand mimic the behaviors of human experts or other agents.\nThe key components of IL include: 1. Demonstrations\nBehavioral demonstrations observed by the learner, usually\nprovided by human experts or other advanced agents. 2.\nBehavior Cloning - A method of learning behavior acquired\ndirectly from demonstration, without the need for explicit\nmodeling of the environment. 3. Inverse RL - Inferring a\nreward function through observation of demonstrations and\nthen using this reward function to guide the learning process.\nIt is evident that IL and observational learning of animals\nshare a notable connection, with their principles being sim-\nilar in many ways. Both learning methods involve learning\nfrom the observation of others' behaviors and imitating or\nreplicating these observed behaviors in future actions.\n4) Cognitive Learning: In machine learning, there are\nmany methods that correspond to cognitive learning. For\nexample, Transfer Learning (TL) [29], [30] allows a model\nto apply existing knowledge (usually learned on one task) to\nanother related but different task. This method is particularly\nuseful in situations with limited data, as it can reduce\nthe amount of data and computational resources needed to\ntrain on a new task. TL typically involves learning feature\nrepresentations from a source task and then adapting these\nrepresentations to a target task.\nCausal inference learning (CIL) [31], [32] focuses on\nlearning the causal relationships between variables from data,\nrather than just correlations. This involves using statistical\nmethods, experimental design, and computational models to\ndetermine whether one event (the cause) directly leads to\nanother event (the effect) and how to quantify this impact.\nBoth cognitive learning and CIL focus on understanding\ncausal relationship. In cognitive learning, individuals under-\nstand causal relationships through observation, experience,\nand reasoning, while in CIL, algorithms identify and validate\ncausal relationships through data analysis.\nML [33] is defined as the process of \"learning how\nto learn.\" The goal of ML is to enable machine learning\nmodels to optimize and improve the learning process through\nprevious experiences, allowing them to adapt and learn more\nquickly when faced with new tasks. Both cognitive learning\nand ML involve higher-level learning on top of the basic\nlearning process. In cognitive learning, this is manifested as\nmetacognitive abilities [34], i.e., understanding and manag-\ning one's own learning process. In ML, this is manifested as\nthe ability to learn how to learn more effectively.\nEnsemble learning (EL) [35], [36] improves the accuracy\nand stability of predictions by combining multiple learning\nmodels. The basic idea behind this method is that individual\nmodels may have their limitations, but when the predictions\nof multiple models (such as decision trees, neural networks,\netc.) are combined, the overall predictive performance is en-\nhanced through diversity and complementarity. In cognitive\nlearning, individuals may use multiple sources of information"}, {"title": "B. Reinforcement Learning", "content": "The application of RL in MRS has been a research hotspot\nin the last decade. It is defined as a framework where each\nagent learns its behavior strategy through interaction with\nthe environment to maximize certain cumulative rewards.\nSpecifically, this learning process can be mathematically\ndescribed as an extension of the Markov Decision Process\n(MDP), commonly known as the Multi-Agent Markov Deci-\nsion Process (MAMDP) [45], [46]. A standard MAMDP can\nbe defined as a tuple $(S, A, P, R, \\gamma)$, where: S represents the\nstate space, encompassing all possible environmental states.\n$A = A^1 \\times A^2 \\times ... \\times A^n$ represents the joint action space, with\neach $A^i$ being the action space for agent $i$. $P : S\\times A\\times S \\rightarrow$\n$[0,1]$ is the state transition probability function, indicating\nthe probability of transitioning to the next state given the\ncurrent state and joint actions. $R: S\\times A\\times S \\rightarrow \\mathbb{R}$ is the\nreward function, which could be the sum of rewards for each\nagent or some other form of aggregation. $\\gamma$ is the discount\nfactor used to calculate the present value of future rewards,\nwith its value ranging from $0 \\leq \\gamma < 1$. In a multi-agent\nenvironment, the goal of each agent is to learn a policy\n$\\pi^i : S \\rightarrow A^i$, aiming to maximize its expected cumulative\ndiscounted reward. Unlike single-agent RL, in multi-agent\nRL, each agent must consider the impact of the behaviors of\nother agents on the environment and on its own rewards.\nFor an agent, the objective can be mathematically defined\nas maximizing the expected cumulative discounted reward\n$V^{\\pi}(s)$ or $Q^{\\pi}(s,a)$, where s represents the state and a\nrepresents the action. State-value function $V^{\\pi}(s)$ represents\nthe expected return under policy in state s. It is defined\nas the sum of expected rewards for all possible paths, where\neach reward is multiplied by the power of the discount factor\n$\\gamma$, indicating the proximity in time:\n$V^{\\pi}(s) = \\mathbb{E}^{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s] \\qquad (1)$\nAction-value function $Q^{\\pi}(s, a)$ represents the expected\nreturn of taking action a in state s, and then following policy\n$\\pi$:\n$Q^{\\pi}(s, a) = \\mathbb{E}^{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s, A_0 = a] \\qquad (2)$\nThe goal of maximizing the reward function can be\nachieved by optimizing the aforementioned value functions.\nIn a multi-agent environment, this goal is more complex\nbecause the optimal strategy of each agent may depend\non the strategies of other agents. Therefore, agents need to\nlearn a policy $\\pi^*$ that maximizes their expected cumulative\ndiscounted reward while considering the possible strategies\nof other agents. For single-agent environment, this can be\nsimplified to finding an optimal policy $\\pi^*$ such that for all\nstates s, $V^{\\pi^*}(s) > V^{\\pi}(s)$ for all $\\pi$. Thus, the mathematical\ndefinition of maximizing rewards involves finding an optimal\npolicy $\\pi^*$ such that: For all states s, $V^{\\pi^*}(s) = \\max_{\\pi} V^{\\pi}(s)$,\nor for all states s and actions a, $Q^{\\pi^*}(s, a) = \\max_{\\pi} Q^{\\pi}(s,a)$.\nThis is usually achieved through iterative algorithms such as\ndynamic programming, the Monte Carlo method, Temporal\nDifference learning, or deep learning, which gradually ap-\nproximates the optimal policy $\\pi^*$.\nThe basic idea of RL is to learn through interaction, which\nmeans that compared to traditional model-based methods,\nRL does not require detailed prior knowledge about the\nenvironment. Robots can learn effective strategies through\ntrial and error, which is particularly useful in unknown or\nuncertain environments. For example, in [47], [48], RL is\nused to solve the navigation problems of robot teams in\ncomplex dynamic environment. At the same time, RL can\nimprove the adaptability and flexibility of MRS, such as\nhelping aerial robot swarms deal with complex turbulent\nflows [47], or MRS tracking of moving targets [49]. RL can\nalso handle multi-objective optimization problems, allowing\neach robot in an MRS to consider the overall system's\noptimal performance while pursuing individual goals. A\ncontinuous RL method introduced in [50] can adapt to multi-\nobjective optimization functions to guide robots' movement\nin dynamic environment. In the previous MAMDP definition,\nit is assumed that each robot is fully observable, but in\nreality, partially observable situations are more common.\nFortunately, MAMDP can be easily extended to MAPOMDP,\nand in [51], [52], [53], [54], all are based on the assumption\nof partial observability to apply RL to MRS. As a hot re-\nsearch topic in recent years, there are a rich set of references\navailable for multi-robot reinforcement learning [40], [55],\n[56], [57], [58].\nOn the other hand, RL also has unique disadvantages\n[7], [52], [59], [60]. RL usually requires a large number\nof interaction samples to learn effective strategies, which\nmay be impractical in real-world MRS due to the high"}, {"title": "C. Imitation Learning", "content": "In the context of MRC, IL, as an effective learning\nstrategy, aims to accelerate the learning process by observing\nand imitating the behavior of experts or other robots [64],\n[65], [66], [67]. While mathematical definition may vary de-\npending on the specific method (such as Behavioral Cloning,\nInverse RL, etc.) and the application scenario considered, a\ngeneral mathematical framework can be stated: in the context\nof MRS where the followings are assumed, a state space S,\nan action space A, and a transition function $T : S \\times A \\rightarrow S$,\nrepresenting the probability of system state transition under a\ngiven state and action. The goal is to learn a policy $\\pi : S \\rightarrow$\nA, that is, given the current state, to determine the action\nto be taken, in order to imitate the behavior of an expert\n(another robot or human). The expert's behavior is given by\na set of trajectories $D = \\{(s_1,a_1), (s_2, a_2), ..., (s_n,a_n)\\}$,\nwhere $s_i$ represents the state and $a_i$ represents the action\ntaken by the expert in that state. The goal of IL is to minimize\nthe difference between the learning policy and the expert\npolicy. This can be quantified in different ways, for example,\nby minimizing the distance between the policy output action\nand the expert action, or by maximizing the similarity of the\ntrajectories generated by the learning policy to the expert\ntrajectories.\nIL has clear advantages and disadvantages in MRS. A\nunique advantage is that by observing and imitating robots\nor humans who have effectively performed specific tasks,\nother robots can quickly learn new skills, reducing the time\nneeded to learn from scratch through trial-and-error. In [66],\na new active IL framework is proposed, where a teacher-\nstudent interaction model is utilized to significantly leverage\nthe advantages of IL. Experiments on the MetaDrive bench-\nmark and Atari 2600 games demonstrate that this method\nis more efficient in achieving performance close to that of\nexperts compared to previous methods. In MRS, individuals\ncan learn the importance of cooperation and cooperative\nstrategies more quickly by observing the behavior of other\ncooperating robots, thereby promoting collaborative work\nthroughout the group. In [68] it is pointed out that the\nemergence of cooperative behavior can be explained through\nunderstanding the co-evolution process of cooperators' core"}, {"title": "D. Transfer Learning", "content": "In the context of MRS, the concept of TL typically\ninvolves transferring knowledge learned on one robot or a\ngroup of robots to other robots or robot groups, in order to\nimprove learning efficiency, reduce the amount of training\ndata needed, or enhance performance in new tasks [73],\n[74], [75], [76], [77]. While there are detailed mathematical\nmodels and definitions for specific applications, a general\nmathematical framework for TL may involve the following\nkey elements: 1. Source Task $T_s$ and Target Task $T_t$, which\nare defined through task-related data distributions, objective\nfunctions, etc. 2. Source Domain $D_s$ and Target Domain\n$D_t$, each domain consists of a feature space and a marginal\nprobability distribution (i.e., $P(X)$). In MRS, different robots\nmay encounter different environment (domain). 3. Transfer\nFunction f, which is the mapping from the source task to\nthe target task. The purpose of this function is to enable the\neffective application of the knowledge learned on the source\ntask to the target task. [78] introduces a framework for multi-robot TL from a dynamical system perspective.\nFrom the perspective of multi-robot TL, TL can assist in\nthe learning process of robots, reducing the training time and\ndata needed to achieve excellent performance, while also\npromoting the sharing of knowledge and experience. This\nenables robots to learn not only from their own experiences\nbut also from the successes and failures of other robots which\nis similar to IL. Especially under resource-constrained condi-\ntions (such as computing power, storage space, etc.), TL can\nreuse existing knowledge. As for shortcomings, if the source"}, {"title": "E. Causal Inference Learning", "content": "Causal inference learning refers to the process of using\ndata to infer the causal relationships between variables. In\nthe context of MRS, causal inference can help comprehend\nand predict the interactions between different robot behaviors\nand environmental factors [81], [82], [83], [84]. Although\nthe concept of causal inference has a precise mathematical\ndefinition in statistics, its application in MRS remains an\nactive research area, involving complex dynamic systems\nand interactions. In causal inference research, a core concept\nis the Potential Outcomes Framework, also known as the\nRubin Causal Model (RCM). Additionally, methods based\non Graphical Models play an important role in causal infer-\nence, especially in describing complex causal relationships\nbetween variables. Under the Potential Outcomes Frame-\nwork, for each individual and each possible treatment (or\nintervention), there is a potential outcome. Causal effect is\ndefined as the difference in potential outcomes under dif-\nferent interventions. Mathematically, if $Y_i(t)$ represents the\npotential outcome of individual i under intervention t, then\nthe causal effect for individual i can be expressed as $Y_i(t_1)$ -\n$Y_i(t_0)$, where $t_1$ and $t_0$ represent different intervention states.\nGraphical models use graphs (typically Directed Acyclic\nGraphs, DAGs) to represent the causal relationships between\nvariables. In these models, nodes represent variables, and\ndirected edges represent causal relationships. Through the\nanalysis of the graph, one can identify conditional inde-\npendency, causal pathway, and possible intervention effect.\nIn MRS, CIL usually focuses on how to infer the causal\nrelationships between robot behaviors based on observed data\n(e.g., the behavior of robots and changes in the environment)\nor the causal effects of robot behaviors on environmental\nchanges. This includes understanding how the behavior of\none robot might affect the behavior of other robots or the\noverall state of the system.\nMRS typically involve multiple autonomous robots inter-\nacting in a shared environment to complete various tasks,\nsuch as collaborative transport, search and rescue, and auto-\nmated monitoring. CIL has a unique advantage in explainable\nrobot learning methods, as it can help one understand how\nthe behavior of one robot affects the behavior of other robots\nor the overall state of the system. This is crucial for design-\ning highly coordinated and efficient MRS. Furthermore, by\nidentifying and understanding causal relationships, system\ndesigner can better formulate intervention measures (such as\nadjusting task assignments, communication strategies, etc.) to\noptimize the performance of the entire system. When robots"}, {"title": "F. Ensemble Learning", "content": "In the context of MRC, Ensemble Learning (EL) can\nbe seen as multiple robots (or agents) working together to\nimprove the effect of overall task execution, where each robot\ncan be considered as a base learner. The mathematical defini-\ntion of EL usually relates to a specific ensemble method, such\nas Bagging, Boosting, or Stacking. Generally, the goal of EL\nis to combine the predictions of multiple models to reduce\ngeneralization error. Mathematically, EL can be defined as\nsuppose there is a set of base learners ${h_1,h_2, ..., h_T}$, each\nlearner $h_i$ can give an output $h_i(x)$ for a given input x. The\ngoal of EL is to combine these predictions through a certain\nstrategy (such as simple averaging, weighted averaging, or\nvoting) to form the final ensemble prediction $H(x)$:\n$H(x) = f(h_1(x), h_2(x), ..., h_T(x)) \\qquad (3)$\nwhere f is the combining strategy, and T is the total number\nof base learners. In the context of MRC, the decision or\nprediction of each robot can be considered as the output of a\nbase learner, and the ensemble method is used to coordinate\nthe decisions among these robots, in order to improve the\noverall efficiency or accuracy of task execution.\nIn MRS, the greatest advantage of EL stems from its\nfundamental characteristic of improving prediction accuracy,\nrobustness, and generalization ability by combining multiple\nmodels. By adjusting EL strategies, MRS can flexibly adapt\nto changes in task requirements or robot capabilities [92],\n[90], [100], [91], [93]. Supriyo Ghosh [90] combined the\nstrengths of both kernel-based and deep multi-agent RL\npolicies. This combination allows the system to leverage fine-\ngrained local policies and more global policies efficiently,\nimproving the decision-making process in air traffic con-\ntrol scenarios. [100] presents a decentralized EL approach\nthat leverages sample exchange among multiple agents to\nimprove performance in multi-agent systems. The method\nallows for decentralized data handling by having agents\nexchange data samples to enhance their collective predic-\ntive abilities. The benefits of using this ensemble method\ninclude increased accuracy through collaborative learning,\ncompetitive performance with state-of-the-art methods while\nmaintaining data decentralization, and efficient utilization of\nlocal data resources by each agent. [91] introduces a novel\nframework for EL through differentiable model selection,\nintegrating machine learning with combinatorial optimiza-\ntion. Despite the many advantages that EL offers in MRS,\nrealizing these advantages also requires addressing a series\nof challenges, including how to effectively integrate data and\ndecisions from different robots [101], [102]."}, {"title": "G. Meta Learning", "content": "Meta learning (ML), in the field of machine learning,\nrefers to the process of building models that learn how\nto learn. It enables models to use past experiences to\naccelerate the learning process for new tasks, or to im-\nprove performance on new tasks [103], [94], [95], [104],\n[96]. Although the concept of ML is relatively intuitive,\nits mathematical definition may vary depending on different\nresearch background and application scenario. In the context\nof MRC, the goal of ML is to enable robots to quickly\nadapt to new cooperative tasks or environments drawing from\nprevious cooperative experiences. In this scenario, ML often\nfocuses on learning how to effectively share information,\nmake decisions, and adapt to the behaviors of cooperative\npartners. A summary of all the methods mentioned above\nand their corresponding references can be found in Table I.\nMathematically, ML in MRS can be defined as finding a\nlearning algorithm A, which use the learning experiences\nfrom past tasks ${T_1, T_2, ..., T_n}$ to improve learning effi-\nciency and performance in a new task $T_{new}$. Specifically,\nconsider a system containing multiple robots, each robot i\nhas a parameter vector $\\theta_i$ for task T, and the system's goal\nis to minimize a common loss function $L(T, {\\theta})$, which\nmeasures the performance of the entire robot team in task\nT. The process of ML can be seen as finding an optimization\nalgorithm A, which adjusts the learning strategy of each\nrobot so that the team can adapt more quickly to new tasks.\nThis can be formalized by minimizing the expected loss over\nall tasks, as below:\n$\\min_A \\mathbb{E}_{T\\sim \\mathcal{T}}[L(T, {\\theta})]  \\qquad (4)$\nwhere {$\\theta$} is the set of robot parameter vectors obtained by\napplying algorithm A to task T, and T is the distribution of\ntasks.\nApplying ML in MRS offers a series of unique advantages.\nFor instance, MRS can quickly adjust their strategies to\nadapt to new environments or tasks through ML, reducing\nthe exploration time in unknown environments [105], [106],\n[97]. Moreover, once a single robot learns a new skill or strat-\negy, this knowledge can be rapidly disseminated throughout\nthe entire robot group via ML mechanisms, improving the\noverall learning efficiency. This also facilitates the sharing\nof strategy and experience among robots, enabling the entire\nsystem to perform complex tasks cohesively. In summary,\nML allows robots to learn from past experiences to adapt\nquickly to new task or environment, which is especially\nimportant for MRS, as these systems often need to operate\ncollaboratively in dynamically changing environment. [107],\n[108]"}, {"title": "IV. DISCUSSION", "content": "Technical challenges of robot learning in the context of\nMRS mainly come from three aspects: 1. The complexity\nof MRS themselves [5", "8": [52], "109": 2.0, "13": [69], "7": [59], "60": ".", "9": "n[110", "problem\n[84": [111], "112": "compared to SRS,\nMRS face more difficult challenges in dealing with the\nheterogeneity, uncertainty, and temporality of different sensor\ndata, as well as the challenge of processing large volumes\nof data in real-time with limited computing resources. In\ncertain complex environments, robots in an MRS need to\nunderstand the intentions and behaviors of other robots to\nwork effectively together. This requires not only advanced\ncommunication capability but also the ability to engage\nin complex social interaction and collaborative decision-\nmaking. A lot of works remain required to address other\nissues such as safety, reliability, and scalability still.\nIn the previous section on learning methods, the respective\nweakness and challenge of each robot learning method were\ndiscussed. Challenges that are common across these robot\nlearning methods [59", "62": "are considered in the present\nsection. First is data and sample efficiency. Robot learning\nis often limited by the amount of available training data.\nCollecting a large volume of labeled data in the real-world\nis both expensive and time-consuming. Therefore, improv-\ning learning algorithms' data efficiency, such as through\ntransfer learning, learning from simulation, and sample-\nefficient"}]}