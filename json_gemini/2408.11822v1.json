{"title": "State-of-the-art in Robot Learning for Multi-Robot Collaboration: A Comprehensive Survey", "authors": ["Bin Wu", "C. Steve Suh"], "abstract": "With the continuous breakthroughs in core tech- nology, the dawn of large-scale integration of robotic systems into daily human life is on the horizon. Multi-robot systems (MRS) built on this foundation are undergoing drastic evo- lution. The fusion of artificial intelligence technology with robot hardware is seeing broad application possibilities for MRS. This article surveys the state-of-the-art of robot learning in the context of Multi-Robot Cooperation (MRC) of recent. Commonly adopted robot learning methods (or frameworks) that are inspired by humans and animals are reviewed and their advantages and disadvantages are discussed along with the associated technical challenges. The potential trends of robot learning and MRS integration exploiting the merging of these methods with real-world applications is also discussed at length. Specifically statistical methods are used to quantitatively corroborate the ideas elaborated in the article.", "sections": [{"title": "I. INTRODUCTION", "content": "With the advancement of technology and the development of artificial intelligence [1], [2], [3], robot learning has become one of the key factors driving the progress of robotic technology. Especially in the field of MRS [4], robot learning has shown great potential and application value. MRS complete complex tasks by coordinating the actions of multiple robots, offering higher efficiency, reliability, and flexibility compared to single robot systems (SRS) [5], [6]. However, with the diversification of application scenarios and the increase in task requirements, the learning mechanisms in MRS face many challenges, including collaborative learning, communication constraint, environmental adaptability, and algorithm's ability to generalize [7], [8].\nThis article reviews the latest research in robot learning within MRS, including theoretical foundations, key technolo- gies, applications, challenges faced and paths being explored. An extensive review of existing literature allows the core issues and proper solution strategies to be identified along with the performance and limitations of different learning methods for practical applications.\nFirst, the basic concepts of MRS and robot learning are introduced and their importance in current technological context is stated. Next, the design principles of learning mechanisms in MRS is explored in-depth, including, but not limited to, technologies such as Reinforcement Learning (RL), Transfer Learning (TL), and Imitation Learning (IL). Much insight is gained from evaluating the advantages and disadvantages of different learning strategies essential to inspiring future research and charting the path moving for- ward. Moreover, successful case studies of MRS in specific applications are examined, such as automated warehousing, search and rescue, environmental monitoring, and precision agriculture, to showcase the effectiveness and applicability of robot learning technologies in solving real-world problems. The main challenges encountered in implementing multi- robot learning systems, including resource allocation, task decomposition, learning efficiency, and system scalability are also discussed. Finally, the article looks forward to the various directions of future development of robot learning in MRS, emphasizing the importance of interdisciplinary col- laboration and how the integration of artificial intelligence, machine learning, control theory, and cognitive science will elevate the field to a greater level of development. The survey paper should provide researchers in the related fields with a comprehensive reference framework, inspire more innovative research and application, and jointly promote the progress of MRS and robot learning."}, {"title": "II. DEFINITION AND SCOPE", "content": "In this section, more detailed definitions of the key concepts addressed in this article are provided along with delineation of the scope of current issues. This is done to help establish a unified cognitive basis and also facilitate a deeper discussion of the issues based on the basis."}, {"title": "A. Multi-robot Systems", "content": "From a system-level perspective, the advantages of MRS over SRS are evident [4], [9]. An MRS refers to a system composed of two or more robots that can complete specific tasks or objectives through collaboration or competition. In such systems, each robot may have unique capabilities and limitations, and they achieve col- lective intelligence and action through communication and coordination. Research on MRS mainly focuses on how to ef- fectively design and implement interaction, communication, and collaboration mechanisms among robots to improve the efficiency and effectiveness of the entire system [9].\nAn MRS can be represented as a tuple $(N, S, A, T, R,C,G)$.\n\u2022 Robot Set: Let $N = {1, 2, ..., n}$ be the set of robots in an MRS, where each $i$ represents an independent robot entity.\n\u2022 State Space: For each robot $i$, its state can be repre- sented by an element in the state space $S_i$. The state"}, {"title": "B. Multi-robot Cooperation", "content": "The concept and mathematical definition of MRC prob- lems can be further developed based on the foundational framework of MRS in the last section [5], [6]. MRC prob- lems involve a group of robots that share information, coordinate actions, and make decisions together to achieve a common task or goal. The primary objective of cooperation is to utilize the collective capabilities of multiple robots to accomplish tasks that are not possible or inefficient for a single robot. This cooperation may include aspects such as task allocation, joint decision-making, resource sharing, coordinated actions, and goal sharing. To achieve this, a common goal function $(G: S\\rightarrow R)$ can be defined to measure and observe the performance of the entire system in achieving the common goal.\n\u2022 Collaboration Strategy: Define a set of strategies $\\Pi = {\\pi_1, \\pi_2,..., \\pi_n}$, where each $\\pi_i: S\\rightarrow A_i$ is a decision rule guiding robot i in choosing actions in a given state.\n\u2022 Communication Model Extension: Extend the commu- nication model $C$ to $Ce : S\\times N \\rightarrow P(N)$, where $P(N)$ is the power set of the robot set N, indicating which groups of robots can communicate in a given state.\n\u2022 Joint Action: Define a joint action mapping $a : S\\times \\Pi \\rightarrow A$, combining the strategies of all the robots and current state to produce the action of the entire system.\n\u2022 Collaborative Benefit: Introduce a utility function $U : S \\times A \\rightarrow R$ to evaluate the performance of the system as a whole under a given state and action.\n\u2022 Constraints: Consider the constraints of interactions and cooperation among robots, such as communication range, resource limitation, and task dependency."}, {"title": "C. Robot Learning", "content": "When discussing robot learning, researchers often first consider it as an interdisciplinary field bringing together machine learning and robotics, which is undoubtedly correct. However, before clarifying the essence of robot learning, what learning is must be clarified. In sociology [10], learning is usually defined as the process by which individuals or groups acquire social behavior patterns through the process of socialization. This includes the internalization of social norms, values, language, skills, and behavior patterns. Soci- ologists emphasize the impact of social structures and culture on the individual learning process. The definition of learning in anthropology [11] emphasizes the process of cultural transmission and adaptation. Learning is seen as the way individuals acquire knowledge, skills, beliefs, and behavior patterns from their cultural environment. Anthropologists often focus on the differences in learning across cultures or ethnic groups and how these differences affect the adap- tation and development of individuals and communities. In psychology [12], learning is typically defined as a relatively permanent change in behavior or thought patterns produced by experience. This definition emphasizes an individual's response and adaptation to environmental stimuli, including cognitive learning, emotional learning, and behavioral learn- ing.\nIn traditional disciplines, there are far more definitions of"}, {"title": "D. Robot Learning in MRC", "content": "learning than those listed above, but this does not prevent one from finding some commonalities. In this article, the concept of learning from four dimensions is discussed:\n\u2022 Utilization of Experience: Learning is a process of acquiring knowledge or skills through experience.\n\u2022 Behavioral and Cognitive Changes: Learning leads to changes in behavior or ways of thinking.\n\u2022 Environmental Adaptation: Learning is considered a way for individuals or groups to adapt to their envi- ronment.\n\u2022 Process Nature: Learning is viewed as a continuous process, rather than a single event. This process involves the constant acquisition, processing, and application of information, as well as gradual adaptation and change over time.\nReturning to the definition of robot learning, it refers to the process where a robot merges hardware (such as sens- ing, processing, and executing components) with software (specific learning algorithms) to utilize data, experience, or interactions with the environment to acquire new knowledge or skills, thereby improving its performance [13], [14], [15]. This process involves perception (acquiring information through sensors), decision-making (making decisions based on learned knowledge), and action (executing tasks). The goal of robot learning is to enable robots to autonomously adapt to new tasks and environments, enhancing their flexi- bility and efficiency. Suppose a robot's strategy (\u03c0 : S \u2192 A) is a mapping from state s to action a, guiding the robot in choosing actions in a given state, then, the learning algorithm is used to extract patterns and knowledge from the data to improve the strategy (\u03c0).\nIn the section above, the concepts of MRC and robot learning were separately introduced. Building on this foun- dation, it naturally leads to another question: what is the difference between robot learning in MRC and single robot learning? Before answering this question, an assumption needs to be made that, regardless if MRS or SRS is being considered, each robot makes decisions based on the local information it acquires. Below, the question is addressed from two aspects: 1. Utilization of information, where robots in a MRS exchange information. 2. Joint decision-making, whether it's competitive or cooperative decision-making, there is a coordination mechanism among the robots in a MRS to promote more efficient operation of the overall system. These two basic points are both the advantage and challenge of multi-robot learning. Exchange of information (experience) provides the robots with more learning materials but also increases learning burden. Joint decision-making can create a synergy effect, while also imposing more restrictions on each robot's decision-making, increasing the difficulty of learning."}, {"title": "III. LEARNING METHOD", "content": "The concept of machine learning was inspired by the ways human and animal learn [16], [17]. Researchers design algorithms and machines by simulating how the human brain works, hoping that machines can acquire knowledge and skill through observation and experience. The classification of learning methods discussed in this section is also based on a logical division, mirroring human or animal learning meth- ods. Afterward various robot learning methods is discussed in the context of MRS."}, {"title": "A. Carbon-based vs. Silicon-based", "content": "First, one can draw from psychology and neuroscience the following classifications of human and animal learning methods:\n\u2022 Classical Conditioning: Conducted by the Russian phys- iologist Pavlov [18], who discovered how animals learn through associating stimuli with responses via experi- ments.\n\u2022 Operant Conditioning: Introduced by B.F. Skinner in his book [19], the concept involves increasing or decreasing the frequency of specific behaviors through rewards and punishments.\n\u2022 Observational Learning: Individuals learn by observing others' behaviors and their consequences [12].\n\u2022 Cognitive Learning: Emphasizes the role of exploration and problem-solving in the learning process [20].\n\u2022 Sociocultural Learning: Discusses how social interac- tions influence cognitive development [21].\n\u2022 Affective Learning: Explores how emotions affect learn- ing and memory, especially the neural mechanisms of fear responses [22].\nAlthough these do not cover all known ways of learning in human and animal, however, they do provide a brief classification basis for robot learning in MRS. Mapping the learning methods of human and animal to those of machine learning provides an interesting perspective on how robots emulate natural learning processes. The classification of learning methods based on carbon-based life forms such as human and animal can be mapped onto the principles of the learning processes of existing machine learning silicon \u2013 based methods.\n1) Classical Conditioning: Classical conditioning in- volves learning through the association between stimuli. In the field of machine learning, the mechanism most similar to this associative learning is supervised learning [23], [24]. This type of machine learning involves mapping between inputs (similar to conditioned stimuli) and outputs (similar to conditioned responses). Training data includes inputs and their corresponding outputs, and the model learns from these data to predict the output of new inputs. For example, when using neural networks for image recognition, the model learns the relationship between patterns in images and labels, similar to the associative learning between stimuli in classical conditioning.\n2) Operant Conditioning: Operant conditioning focuses on the relationship between behaviors and consequences, which is very similar to the concept of RL [25], [26]. In RL, an agent learns behavior strategies through interaction with the environment to maximize certain cumulative re- wards. This learning process involves exploration (trying new behaviors to discover effective strategies) and exploitation (using known strategies to obtain rewards). The mechanism of RL is similar to operant conditioning, relying on the consequences of behaviors (rewards or punishments) to form or change behaviors.\n3) Observational Learning: In machine learning methods analogous to observational learning, unsupervised learning [23], [27] and imitation learning [15], [28] stand out as particularly representative. Observational learning involves observing the behaviors and outcomes of others and learning from them. If one likens certain aspects of observational learning to feature learning or clustering in the field of machine learning, then the process in unsupervised learning, which involves identifying patterns and structures from data without explicit labels or feedback, shares strong similarities with observational learning. Inference learning (IL) is a method that allows robots or software agents to observe and mimic the behaviors of human experts or other agents. The key components of IL include: 1. Demonstrations - Behavioral demonstrations observed by the learner, usually provided by human experts or other advanced agents. 2. Behavior Cloning - A method of learning behavior acquired directly from demonstration, without the need for explicit modeling of the environment. 3. Inverse RL - Inferring a reward function through observation of demonstrations and then using this reward function to guide the learning process. It is evident that IL and observational learning of animals share a notable connection, with their principles being sim- ilar in many ways. Both learning methods involve learning from the observation of others' behaviors and imitating or replicating these observed behaviors in future actions.\n4) Cognitive Learning: In machine learning, there are many methods that correspond to cognitive learning. For example, Transfer Learning (TL) [29], [30] allows a model to apply existing knowledge (usually learned on one task) to another related but different task. This method is particularly useful in situations with limited data, as it can reduce the amount of data and computational resources needed to train on a new task. TL typically involves learning feature representations from a source task and then adapting these representations to a target task.\nCausal inference learning (CIL) [31], [32] focuses on learning the causal relationships between variables from data, rather than just correlations. This involves using statistical methods, experimental design, and computational models to determine whether one event (the cause) directly leads to another event (the effect) and how to quantify this impact. Both cognitive learning and CIL focus on understanding causal relationship. In cognitive learning, individuals under- stand causal relationships through observation, experience, and reasoning, while in CIL, algorithms identify and validate causal relationships through data analysis.\nML [33] is defined as the process of \"learning how to learn.\" The goal of ML is to enable machine learning models to optimize and improve the learning process through previous experiences, allowing them to adapt and learn more quickly when faced with new tasks. Both cognitive learning and ML involve higher-level learning on top of the basic learning process. In cognitive learning, this is manifested as metacognitive abilities [34], i.e., understanding and manag- ing one's own learning process. In ML, this is manifested as the ability to learn how to learn more effectively.\nEnsemble learning (EL) [35], [36] improves the accuracy and stability of predictions by combining multiple learning models. The basic idea behind this method is that individual models may have their limitations, but when the predictions of multiple models (such as decision trees, neural networks, etc.) are combined, the overall predictive performance is en- hanced through diversity and complementarity. In cognitive learning, individuals may use multiple sources of information"}, {"title": "B. Reinforcement Learning", "content": "and strategies to enhance learning effectiveness. EL improves predictive performance by combining multiple models. Both can potentially utilize a diversified perspective and methods to optimize results.\nMultitask learning [37], [38] enhances the model's gener- alization ability by learning multiple related tasks simulta- neously during the training process. The core idea behind this method is that multiple tasks share some common representations or features so that while learning one task, the model can also learn useful information from other tasks. Both cognitive learning and multi-task learning may involve simultaneously dealing with multiple related tasks to improve efficiency and performance.\n5) Socialcultural Learning: Sociocultural learning theory emphasizes that individuals learn and develop within the context of social interaction and cultural background. This type of learning naturally adapts to robot learning in MRS [39], [40], especially in decentralized systems where each robot can make independent decision. Multi-agent learning focuses on how multiple intelligent agents learn and interact in the same or interdependent environment. These agents learn the best strategies by observing the behavior of the environment and other agents to achieve their goals, which may involve cooperation or competition. Key issues in multi- agent learning include coordination, competition, communi- cation, and the sharing of learning strategies. The similarity between sociocultural learning and multi-robot learning lies in the emphasis on interactions with other agents and the influence of the environment on learning.\n6) Affective Learning: Affective learning in robots is a profound and complex research direction [41], but this article will not delve into it extensively. However, when there is a human in the loop in MRS [42], [43], the system may face issues related to recognizing, interpreting, processing, and simulating human emotions. These types of issues can be categorized under affective computing [44]. How to properly handle this information to improve the naturalness and efficiency of human-computer interaction is a promising research direction.\nThe application of RL in MRS has been a research hotspot in the last decade. It is defined as a framework where each agent learns its behavior strategy through interaction with the environment to maximize certain cumulative rewards. Specifically, this learning process can be mathematically described as an extension of the Markov Decision Process (MDP), commonly known as the Multi-Agent Markov Deci- sion Process (MAMDP) [45], [46]. A standard MAMDP can be defined as a tuple $(S, A, P, R, \\gamma)$, where: S represents the state space, encompassing all possible environmental states. $A = A^1 x A^2 x ... \\times A^n$ represents the joint action space, with each $A^i$ being the action space for agent i. $P : S \\times A \\times S \\rightarrow [0,1]$ is the state transition probability function, indicating the probability of transitioning to the next state given the current state and joint actions. $R : S \\times A \\times S \\rightarrow R$ is the reward function, which could be the sum of rewards for each agent or some other form of aggregation. $\\gamma$ is the discount factor used to calculate the present value of future rewards, with its value ranging from $0 \\leq \\gamma < 1$. In a multi-agent environment, the goal of each agent is to learn a policy $\\pi^i : S \\rightarrow A^i$, aiming to maximize its expected cumulative discounted reward. Unlike single-agent RL, in multi-agent RL, each agent must consider the impact of the behaviors of other agents on the environment and on its own rewards.\nFor an agent, the objective can be mathematically defined as maximizing the expected cumulative discounted reward $V^{\\pi}(s)$ or $Q^{\\pi}(s,a)$, where s represents the state and a represents the action. State-value function $V^{\\pi}(s)$ represents the expected return under policy $\\pi$ in state s. It is defined as the sum of expected rewards for all possible paths, where each reward is multiplied by the power of the discount factor $\\gamma$, indicating the proximity in time:"}, {"title": "", "content": "$V^{\\pi} (s) = E_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s \\right]$ (1)\nAction-value function $Q^{\\pi}(s, a)$ represents the expected return of taking action a in state s, and then following policy $\\pi$:\n$Q^{\\pi} (s, a) = E_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s, A_0 = a \\right]$ (2)\nThe goal of maximizing the reward function can be achieved by optimizing the aforementioned value functions. In a multi-agent environment, this goal is more complex because the optimal strategy of each agent may depend on the strategies of other agents. Therefore, agents need to learn a policy $\\pi^*$ that maximizes their expected cumulative discounted reward while considering the possible strategies of other agents. For single-agent environment, this can be simplified to finding an optimal policy $\\pi^*$ such that for all states s, $V^{\\pi^*}(s) > V^{\\pi}(s)$ for all $\\pi$. Thus, the mathematical definition of maximizing rewards involves finding an optimal policy $\\pi^*$ such that: For all states s, $V^{\\pi^*}(s) = max_{\\pi} V^{\\pi}(s)$, or for all states s and actions a, $Q^{\\pi^*}(s, a) = max_{\\pi} Q^{\\pi}(s,a)$. This is usually achieved through iterative algorithms such as dynamic programming, the Monte Carlo method, Temporal Difference learning, or deep learning, which gradually ap- proximates the optimal policy $\\pi^*$.\nThe basic idea of RL is to learn through interaction, which means that compared to traditional model-based methods, RL does not require detailed prior knowledge about the environment. Robots can learn effective strategies through trial and error, which is particularly useful in unknown or uncertain environments. For example, in [47], [48], RL is used to solve the navigation problems of robot teams in complex dynamic environment. At the same time, RL can improve the adaptability and flexibility of MRS, such as helping aerial robot swarms deal with complex turbulent flows [47], or MRS tracking of moving targets [49]. RL can also handle multi-objective optimization problems, allowing each robot in an MRS to consider the overall system's optimal performance while pursuing individual goals. A continuous RL method introduced in [50] can adapt to multi- objective optimization functions to guide robots' movement in dynamic environment. In the previous MAMDP definition, it is assumed that each robot is fully observable, but in reality, partially observable situations are more common. Fortunately, MAMDP can be easily extended to MAPOMDP, and in [51], [52], [53], [54], all are based on the assumption of partial observability to apply RL to MRS. As a hot re- search topic in recent years, there are a rich set of references available for multi-robot reinforcement learning [40], [55], [56], [57], [58].\nOn the other hand, RL also has unique disadvantages [7], [52], [59], [60]. RL usually requires a large number of interaction samples to learn effective strategies, which may be impractical in real-world MRS due to the high cost and time consumption of physical experiments. For this reason, [61] discusses various aspects of the simulation to reality (Sim-to-Real) transfer problem in robotics Deep Rein- forcement Learning (DRL). In [62], an actor-critic algorithm combined with experience replay is introduced to improve sample utilization. By reusing past experiences, this method can learn effective strategies with fewer samples required. In complex multi-robot environment, ensuring the stability and convergence of learning algorithms is a challenge, es- pecially in the case of continuous action spaces or multi- agent interactions. [63] introduces the Deep Deterministic Policy Gradient (DDPG) algorithm, a method that combines deep learning and reinforcement learning to solve control problems in continuous action space."}, {"title": "C. Imitation Learning", "content": "In the context of MRC, IL, as an effective learning strategy, aims to accelerate the learning process by observing and imitating the behavior of experts or other robots [64], [65], [66], [67]. While mathematical definition may vary de- pending on the specific method (such as Behavioral Cloning, Inverse RL, etc.) and the application scenario considered, a general mathematical framework can be stated: in the context of MRS where the followings are assumed, a state space S, an action space A, and a transition function $T : S \\times A \\rightarrow S$, representing the probability of system state transition under a given state and action. The goal is to learn a policy $\\pi : S \\rightarrow A$, that is, given the current state, to determine the action to be taken, in order to imitate the behavior of an expert (another robot or human). The expert's behavior is given by a set of trajectories $D = {(s_1,a_1), (s_2, A_2), ..., (s_n,a_n)}$, where $s_i$ represents the state and $a_i$ represents the action taken by the expert in that state. The goal of IL is to minimize the difference between the learning policy and the expert policy. This can be quantified in different ways, for example, by minimizing the distance between the policy output action and the expert action, or by maximizing the similarity of the trajectories generated by the learning policy to the expert trajectories.\nIL has clear advantages and disadvantages in MRS. A unique advantage is that by observing and imitating robots or humans who have effectively performed specific tasks, other robots can quickly learn new skills, reducing the time needed to learn from scratch through trial-and-error. In [66], a new active IL framework is proposed, where a teacher- student interaction model is utilized to significantly leverage the advantages of IL. Experiments on the MetaDrive bench- mark and Atari 2600 games demonstrate that this method is more efficient in achieving performance close to that of experts compared to previous methods. In MRS, individuals can learn the importance of cooperation and cooperative strategies more quickly by observing the behavior of other cooperating robots, thereby promoting collaborative work throughout the group. In [68] it is pointed out that the emergence of cooperative behavior can be explained through understanding the co-evolution process of cooperators' core and betrayers' periphery, emphasizing the role of partner"}, {"title": "D. Transfer Learning", "content": "selection and imitation strategies in promoting cooperative behaviors, without assuming the presence of underlying communication or reputation mechanism. In this way, the article provides a unified framework to study imitation-based cooperation in dynamic social networks.\nThe disadvantages of IL [69] are also obvious. They can be summarized into the followings: 1. Dependence on high-quality demonstrations: If the quality of expert demon- strations is not high, or if they do not match the current task environment, the learned strategy may lead to poor performance or even incorrect behavior. 2. Limitations in generalization capability: Since IL relies on specific demon- strations, the learned strategy may exhibit insufficient gener- alization capability when encountering unseen environment or task. 3. Lack of self-exploration: Over-reliance on imita- tion may lead to a lack of self-exploration and innovation capability in robots, preventing them from autonomously discovering solutions that are superior to the demonstrations. 4. Lack of diversity: In an MRS, if all robots imitate the same demonstration, it may lead to homogenization of behaviors, reducing the system's adaptability and robustness. With the development of IL, methods to address flawed demonstrations have been proposed in [70], [71]. A novel IL framework introduced in [72] expands the applicability of IL by incorporating the concepts of hindsight information embedding and contextual strategies, demonstrating superior performance across multiple tasks and settings.\nIn the context of MRS, the concept of TL typically involves transferring knowledge learned on one robot or a group of robots to other robots or robot groups, in order to improve learning efficiency, reduce the amount of training data needed, or enhance performance in new tasks [73], [74], [75], [76], [77]. While there are detailed mathematical models and definitions for specific applications, a general mathematical framework for TL may involve the following key elements: 1. Source Task Ts and Target Task Tr, which are defined through task-related data distributions, objective functions, etc. 2. Source Domain Ds and Target Domain $D_T$, each domain consists of a feature space and a marginal probability distribution (i.e., $P(X)$). In MRS, different robots may encounter different environment (domain). 3. Transfer Function $f$, which is the mapping from the source task to the target task. The purpose of this function is to enable the effective application of the knowledge learned on the source task to the target task. [78] introduces a framework for multi- robot TL from a dynamical system perspective.\nFrom the perspective of multi-robot TL, TL can assist in the learning process of robots, reducing the training time and data needed to achieve excellent performance, while also promoting the sharing of knowledge and experience. This enables robots to learn not only from their own experiences but also from the successes and failures of other robots which is similar to IL. Especially under resource-constrained condi- tions (such as computing power, storage space, etc.), TL can reuse existing knowledge. As for shortcomings, if the source task and target task are not sufficiently similar, or if the method of TL is not properly implemented, negative transfer may occur [79], [80]. This means the knowledge learned from other robots could actually decrease the performance of the robot on the target task. Moreover, to achieve effective knowledge transfer, communication and coordination among robots are necessary, which might increase the complexity and overhead of the system."}, {"title": "E. Causal Inference Learning", "content": "Causal inference learning refers to the process of using data to infer the causal relationships between variables. In the context of MRS, causal inference can help comprehend and predict the interactions between different robot behaviors and environmental factors [81], [82], [83], [84]. Although the concept of causal inference has a precise mathematical definition in statistics, its application in MRS remains an active research area, involving complex dynamic systems and interactions. In causal inference research, a core concept is the Potential Outcomes Framework, also known as the Rubin Causal Model (RCM). Additionally, methods based on Graphical Models play an important role in causal infer- ence, especially in describing complex causal relationships between variables. Under the Potential Outcomes Frame- work, for each individual and each possible treatment (or intervention), there is a potential outcome. Causal effect is defined as the difference in potential outcomes under dif- ferent interventions. Mathematically, if $Y_i(t)$ represents the potential outcome of individual i under intervention t, then the causal effect for individual i can be expressed as $Y_i(t_1) - Y_i(t_0)$, where $t_1$ and $t_0$ represent different intervention states. Graphical models use graphs (typically Directed Acyclic Graphs, DAGs) to represent the causal relationships between variables. In these models, nodes represent variables, and directed edges represent causal relationships. Through the analysis of the graph, one can identify conditional inde- pendency, causal pathway, and possible intervention effect. In MRS, CIL usually focuses on how to infer the causal relationships between robot behaviors based on observed data (e.g., the behavior of robots and changes in the environment) or the causal effects of robot behaviors on environmental changes. This includes understanding how the behavior of one robot might affect the behavior of other robots or the overall state of the system.\nMRS typically involve multiple autonomous robots inter- acting in a shared environment to complete various tasks, such as collaborative transport, search and rescue, and auto- mated monitoring. CIL has a unique advantage in explainable robot learning methods, as it can help one understand how the behavior of one robot affects the behavior of other robots or the overall state of the system. This is crucial for design- ing highly coordinated and efficient MRS. Furthermore, by identifying and understanding causal relationships, system designer can better formulate intervention measures (such as adjusting task assignments, communication strategies, etc.) to optimize the performance of the entire system. When robots clarify the various causal structures between themselves and"}, {"title": "F. Ensemble Learning", "content": "the environment, they can greatly reduce the dependency on large-scale data, which is significant in situation where data is scarce or the cost of data acquisition is high. On the other hand, the dynamism and interaction complexity of MRS make constructing accurate causal models very challenging. Therefore, the identification and verification of causal relationships require precise model design and complex data analysis [88], [98]. Although causal inference can reduce the dependency on large amounts of data, it still requires high-quality data to identify true causal relation- ships. Collecting and organizing such data in MRS is also very challenging. Conducting experiments in MRS (such as randomized controlled trials) to verify causal relationships may be impractical, especially in real-world applications, such as operating in unstable or uncontrollable environment [89], [99].\nIn the context of MRC, Ensemble Learning (EL) can be seen as multiple robots (or agents) working together to improve the effect of overall task execution, where each robot can be considered as a base learner. The mathematical defini- tion of EL usually relates to a specific ensemble method, such as Bagging, Boosting, or Stacking. Generally, the goal of EL is to combine the predictions of multiple models to reduce generalization error. Mathematically, EL can be defined as suppose there is a set of base learners {h1,h2, ..., hT}, each learner $h_i$ can give an output $h_i(x)$ for a given input x. The goal of EL is to combine these predictions through a certain strategy (such as simple averaging, weighted averaging, or voting) to form the final ensemble prediction H(x):\n$H(x) = f(h_1(x), h_2(x), ..., h_T(x))$ (3)\nwhere f is the combining strategy, and T is the total number of base learners. In the context of MRC, the decision or prediction of each robot can be considered as the output of a base learner, and the ensemble method is used to coordinate the decisions among these robots, in order to improve the overall efficiency or accuracy of task execution."}, {"title": "G. Meta Learning", "content": "In MRS, the greatest advantage of EL stems from its fundamental characteristic of improving prediction accuracy, robustness, and generalization ability by combining multiple models. By adjusting EL strategies, MRS can flexibly adapt to changes in task requirements or robot capabilities [92", "90": [100], "91": [93]}, {"90": "combined the strengths of both kernel-based and deep multi-agent RL policies. This combination allows the system to leverage fine- grained local policies and more global policies efficiently, improving the decision-making process in air traffic con- trol scenarios. [100", "91": "introduces a novel framework for EL through differentiable model selection, integrating machine learning with combinatorial optimiza- tion. Despite the many advantages that EL offers in MRS, realizing these advantages also requires addressing a series of challenges, including how to effectively integrate data and decisions from different robots [101", "102": ".", "103": [94], "95": [104], "96": "."}]}