{"title": "A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning in Large Language Models", "authors": ["Rong Wang", "Kun Sun", "Jonas Kuhn"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across various tasks. However, LLMs often struggle with spatial reasoning which is one essential part of reasoning and inference and requires understanding complex relationships between objects in space. This paper proposes a novel neural-symbolic framework that enhances LLMs' spatial reasoning abilities. We evaluate our approach on two benchmark datasets: StepGame and SparQA, implementing three distinct strategies: (1) ASP (Answer Set Programming)-based symbolic reasoning, (2) LLM + ASP pipeline using DSPy, and (3) Fact + Logical rules. Our experiments demonstrate significant improvements over the baseline prompting methods, with accuracy increases of 40-50% on StepGame dataset and 3-13% on the more complex SparQA dataset. The \u201cLLM + ASP\" pipeline achieves particularly strong results on the tasks of Finding Relations (FR) and Finding Block (FB) questions, though performance varies across different question types. Our impressive results suggest that while neural-symbolic approaches offer promising directions for enhancing spatial reasoning in LLMs, their effectiveness depends heavily on the specific task characteristics and implementation strategies. We propose an integrated, simple yet effective set of strategies using a neural-symbolic pipeline to boost spatial reasoning abilities in LLMs. This pipeline and its strategies demonstrate strong generalizability and broader applicability to other reasoning domains in LLMs, such as temporal reasoning, deductive inference etc.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are known for their impressive performance across a range of tasks, demonstrating certain commonsense reasoning abilities. However, since LLMs are trained to predict subsequent words in a sequence, they seem to lack sufficient grounding to excel at tasks requiring spatial, physical, and embodied reasoning. Recent studies (Bang et al., 2023; Cohn, 2023), highlighted the limitations of models like ChatGPT in deductive logical reasoning, spatial reasoning and non-textual semantic reasoning, underlining the need for further improvements in spatial reasoning.\nSpatial reasoning is a crucial cognitive function that enables humans to conceptualize and predict the movement and interactions of objects in two or three-dimensional spaces (Byrne and Johnson-Laird, 1989). Spatial reasoning is one essential part of reasoning and inference. Reasoning and inference are essential building blocks in advancing Artificial General Intelligence (AGI), which aims to create AI systems that can perform at or beyond human levels across a wide range of cognitive tasks (Kumar et al., 2023). If LLMs are to approach AGI, they must be equipped with spatial reasoning abilities comparable to those of humans. These capabilities are not only valuable in themselves but also essential for LLM-based applications in robotics, task planning, path planning, and navigation (Sharma, 2023; Chen et al., 2024).\nSpatial reasoning includes two primary categories: quantitative and qualitative reasoning. Quantitative spatial reasoning involves precise mathematical computations of spatial properties such as distances, angles, coordinates, and dimensions. In contrast, qualitative spatial reasoning (QSR) deals with more abstract, symbolic relationships and relative positions between objects (Cohn and Renz, 2008). QSR comprises directional relations describing how objects are oriented relative to each other (e.g., \u201cnorth\u201d,\u201cleft\u201d), distance relations capturing relative proximity (e.g., \"near\u201d, \u201cfar\u201d), and topological relations characterizing how spatial regions connect and contain each other (e.g., \u201cinside\u201d, \u201coverlapping\u201d, \u201cdisconnected\"). LLMs face significant challenges in processing these spatial relationships, as they must not only comprehend semantic descriptions of scenes but also perform complex multi-hop reasoning about how objects relate to one another in space (Li et al., 2024a; Yang et al., 2024).\nTraditional approaches in LLMs mainly rely on free-form prompting in a single call to LLMs for facilitating spatial reasoning. However, these methods have demonstrated notable limitations, particularly on challenging datasets like StepGame (Shi et al., 2022) and SparQA (Mirzaee and Kordjamshidi, 2022), which require multi-step planning. In these scenarios, LLMs often struggle with maintaining coherence, frequently hallucinating or losing sight"}, {"title": "2 Related Work", "content": "Recent advancements have shown promise in augmenting LLMs with external tools for tasks requiring arithmetic, navigation, and knowledge base lookups (Fang et al., 2024). However, these efforts are insufficient for enhancing spatial reasoning in LLMs significantly. Recent studies have introduced neural-symbolic strategies, such as ASP (Answer Set Programming) (Yang et al., 2023), to address the limitations of prompting-based approaches. Neural-symbolic methods typically follow a two-step process: extracting facts using LLMs and then applying logical reasoning. As is known, LLMs excel at extracting facts but struggle with reasoning. However, integrating logical reasoning can effectively bridge this gap in their reasoning capabilities. This approach therefore mitigates the weaknesses of reasoning in LLMs and has been shown to significantly outperform prompting methods (Mirzaee and Kordjamshidi, 2023). However, despite their advantages, current research on employing neural-symbolic approaches to enhance LLMs\u2019spatial reasoning capabilities still faces several challenges. For example, some research merely tested performance on specific datasets without comprehensive evaluation (Yang et al., 2023). Some researchers have not properly applied neural-symbolic methods (Li et al., 2024a). Furthermore, some studies fail to fully explore LLMs' spatial reasoning by implementing incomplete feedback loops via multiple LLMs (Mirzaee and Kordjamshidi, 2023). A detailed analysis is presented in the related work section. Such limitations require the need for more rigorous, integrated and effective approaches.\nTo address these limitations, we propose a systematic pipeline that effectively enhances LLMs' spatial reasoning capabilities. Our approach combines strategic prompting with symbolic reasoning to create a robust framework that significantly improves performance of spatial reasoning across different LLM architectures. By integrating feedback loops and ASP-based verification, our methodology demonstrates strong generalizability when tackling complex spatial reasoning tasks. Specifically, building on these insights, we propose a novel neural-symbolic pipeline that integrates LLMs with ASP, aimed at enhancing the LLMs' capabilities of spatial reasoning in SparQA and StepGame datasets. We investigate the potential benefits of integrating symbolic reasoning components into LLMs to further boost their spatial reasoning capabilities. Within the broader field of neural-symbolic AI, the combination of LLMs as parsers with ASP solvers has emerged as a particularly effective approach for complex reasoning tasks. Additionally, the present study is one of the pioneering projects which uses DSPy (Khattab et al., 2023) to pipeline and program LLM.\nOur pipeline's effectiveness is evident across diverse datasets, ranging from simple directional relationships to intricate spatial configurations in"}, {"title": "2.1 Prompting LLMs for spatial reasoning", "content": "Research in spatial reasoning includes both visual and textual domains, with Visual Spatial Reasoning (VSR) facing challenges even in advanced models like CLIP (Radford et al., 2021). On the other hand, text-based reasoning adds complexity due to linguistic ambiguity (Landau and Jackendoff, 1993). Recent studies by Rozanova et al. (2021) and Mirzaee and Kordjamshidi (2022) have explored transformer models and introduced SparQA, addressing challenges in bridging symbolic language and spatial concepts (Tenbrink and Kuhn, 2011), with (Geibinger, 2023) advancing neural-symbolic approaches.\nThe field has evolved through various traditional prompting strategies to enhance LLM reasoning capabilities, including chain of thought (CoT) (Wei et al., 2022; Chu et al., 2023), few-shot prompting (Schick and Sch\u00fctze, 2021), least-to-most prompting (Zhou et al., 2022), and self-consistency (Wang et al., 2022b), with newer techniques like tree of thoughts, visualization of thought (Wang et al., 2023), and program-aided language models (Gao et al., 2022) further advancing structured, interpretable approaches for complex reasoning tasks."}, {"title": "2.2 Neural-symbolic approach to reasoning", "content": "Neural-symbolic AI has a long history. However, it remained a rather new topic until recently, driven largely by breakthrough advances in deep learning and transformer architectures. Despite deep learning's success, its limitations in reasoning, interpretability, and generalization have driven interest in combining neural and symbolic approaches to employ their complementary strengths.\nDifferent from the prompting strategies, the core idea behind neural-symbolic approaches is to integrate the strengths of neural networks, such as learning from data and handling uncertainty, with the strengths of symbolic systems, such as logical reasoning and knowledge representation. This integration aims to address the limitations of each approach when used in isolation. As Garcez and Lamb (2023) point out, neural networks often struggle with explicit reasoning and generalization beyond their training distribution, while symbolic systems can be brittle and struggle with real-world data. Hamilton et al. (2022) emphasize that successful Al systems must incorporate rich representations that facilitate precise, human-interpretable inference. By combining neural architectures with symbolic reasoning, researchers aim to create systems capable of learning from experience but applying logical rules to reason about new situations, thereby addressing some shortcomings of purely neural models (Besold et al., 2021).\nBased on how neural and symbolic components interact and complement each other, these approaches can be categorized into five distinct patterns: Symbolic[neural], where symbolic systems are enhanced with neural capabilities; neural Symbolic, implementing hybrid pipelines; neural:Symbolic \u2192 neural, compiling symbolic rules into neural structures; neural Symbolic, combining logic rules through embeddings; and neural[Symbolic], enhancing neural systems with symbolic reasoning. Although this taxonomy provides valuable insights, there is significant overlap between these patterns, particularly in how they incorporate symbolic reasoning into neural networks. To provide a more practical framework, we can consolidate these approaches into four fundamental integration architectures, each capturing a distinct way of combining neural and symbolic capabilities: a) Neural| symbolic Sequential Integration (Weber et al., 2019); b) Neural-symbolic Iterative Integration (Evans and Stanovich, 2013; Bellini-Leite, 2022); c) Symbolic Embedded NN architecture (Riegel et al., 2020); d) LLM+Tools (Parisi et al., 2022) (The details are seen in Appendix A). These approaches have inspired us to develop more effective integration neural-symbolic strategies and a generalized integrated pipeline to enhance the performance of spatial reasoning in LLMs.\nWe take specific examples to demonstrate the effectiveness of neural-symbolic approaches to boosting various systems. Neural-symbolic systems excel in complex applications by blending data-driven learning with rule-based inference across domains like computer vision, NLP, and robotics. They integrate visual recognition with semantic understanding (Mao et al., 2019a), language with logical inference (Yu et al., 2024), and perception with"}, {"title": "3 Methods", "content": "decision-making (Wang et al., 2022a). Spatial-temporal reasoning, crucial in neural-symbolic AI, enhances problem-solving in physical contexts (Lee et al., 2023). In Qualitative Spatial Reasoning (QSR), combining LLMs with spatial calculi (e.g., RCC-8, CDC) strengthens spatial reasoning (Liu et al., 2009; Katerinenko, 2015), while neural-symbolic integration improves geospatial segmentation (Alirezaie et al., 2019). Multi-hop question answering benefits from systems like NLProlog, which merge neural networks with symbolic reasoning for multi-step inference (Weber et al., 2019; Chen et al., 2019). Language contextualization is advanced by systems like PIGLeT, grounding understanding in a 3D world (Zellers et al., 2021), and neural-symbolic VQA, combining visual perception and logical reasoning (Lu et al., 2024). Overall, neural-symbolic approaches enhance explainability through interpretable reasoning paths, improve robustness by handling edge cases and ensuring consistency, and allow flexible integration of domain-specific knowledge.\nNext we focus on neural-symbolic approach on spatial reasoning in LLMs.\nYang et al. (2023) proposed a novel strategy to convert language into logic serving as ASP(Answer Set Programs, a logic-based declarative knowledge representation formalism), and claimed to boost LLMs' spatial reasoning capabilities. However, they merely used LLMs to extract some facts. Instead they tested performance on specific datasets without comprehensive evaluations on spatial reasoning on LLMs. In other words, their ASP method is novel but the method was not actually tested on spatial reasoning in the simplistic dataset (i.e., StepGame. Li et al. (2024a) focus on simplistic datasets (i.e., StepGame) by applying some method which is not neural-symbolic. The method in Li et al. (2024a) is essentially language-based prompting strategy. Furthermore, some studies proposed using mutiple LLMs to form multi-agent to facilitate spatial reasoning, attempting to fully explore LLMs' spatial reasoning by implementing via multiple LLMs (Mirzaee and Kordjamshidi, 2023) on another complex test data (i.e., SparQA). However, it seems that Mirzaee and Kordjamshidi (2023) did not realize loop via mutiple LLM agents. These limitations show the need for better and more comprehensive neural-symbolic strategies. In order to overcome these limitations, we propose a systematic pipeline that enhances LLMs' spatial reasoning by combining strategic prompting with symbolic reasoning. Incorporating feedback loops and ASP-based verification, our method generalizes effectively to complex tasks across diverse LLM architectures.\nOur strategies are grounded in two key observations from prior research. First, LLMs have demonstrated strong capabilities in understanding and generating spatial descriptions, as well as representing abstract spatial relationships and environments through extensive training on diverse textual data, suggesting their ability to learn and reason about spatial concepts from"}, {"title": "3.1 Datasets", "content": "The two test benchmark datasets are taken to help evaluate the effectivenss of our strategies comprehensively: StepGame (Shi et al., 2022), and SparQA (Mirzaee and Kordjamshidi, 2022). The following provides a detailed account of the two datasets.\nStepGame (Shi et al., 2022) is a synthetic spatial question answering dataset featuring Finding Relations questions that require between 1 to 10 reasoning steps to answer. It employs eight spatial relations (top, down, left, right, top-left, top-right, down-left, and down-right) for story generation. The specific details and samples of StepGame are seen in Appendix B1.\nMirzaee and Kordjamshidi (2022) introduced SparQA, a textual question answering benchmark for spatial reasoning, highlighting the unique challenges posed by text-based spatial tasks. Their work emphasized the need for models to understand complex spatial relationships described in natural language and to perform multi-hop reasoning to answer questions accurately. The details and samples of SparQA are seen in Appendix B2. The SparQA dataset advances benchmarking for spatial reasoning in AI, offering a more sophisticated and realistic scenario than predecessors like StepGame. Key features include:\nBroader Language Use: SparQA includes longer, more complex sentences (approximately 2.5 times the length of StepGame). Each scenario typically consists of three blocks arranged vertically or horizontally, with around four objects per block, characterized by attributes like size, color, and shape. In a typical context, there are twelve objects across three blocks, with roughly ten explicitly defined relationships.\nDiverse Question Types: SparQA includes Yes/No, FR, CO, and"}, {"title": "3.2 Answer set programming (ASP)", "content": "FB question types, often involving multiple objects and relations. For instance, \"What is the relation between the blue circle touching the top edge of block B and the small square?\u201d\nComplex Relational Dynamics: SparQA incorporates 3D spatial reasoning, topological relations, and distance relations, with candidate choices such as left, above, near to, and additional synonym terms.\nQuantifier Questions: These questions test quantification abilities, e.g., \u201cAre all of the squares in B?\u201d or \u201cWhich block has only small black things inside?\u201d demanding higher-level reasoning.\nBoth datasets are employed in the present study to test the effectiveness of our proposed strategies. The following sections introduces these strategies applied in enhancing spatial reasoning abilities in LLMs: 1) Answer Set Programming (ASP); 2) Proposed LLM + ASP Pipeline with DSPy. 3) Fact + Logical rules. We applied the first strategy to StepGame and the second and third strategies to both SparQA and StepGame to evaluate their effectiveness through experiments.\nASP is a powerful declarative programming paradigm tailored for complex reasoning tasks, particularly those that involve knowledge representation and combinatorial search problems (Yang et al., 2023). ASP is declarative, meaning problems are defined through logical relationships between entities, while ASP solvers automatically determine solutions that satisfy given conditions. ASP consists of the following elements: facts, rules, constraints and queries. The following briefly describes these elements.\nFacts in ASP form the foundation of the problem domain, representing fundamental truths or atoms. These unconditional statements consist of predicates with arguments. For example, in spatial reasoning, predicates might include:\nblock/1: Represents a block with one argument.\nobject/5: Represents an object with five attributes (name, size, color, shape, block).\nis/3: Describes spatial relationships between objects.\nExample facts can be expressed as:"}, {"title": "3.3 Proposed LLM + ASP pipeline with DSPy", "content": "ASP is a fundamental neural-symbolic strategy in improving LLMs' spatial reasoning. However, some weaknesses are obvious. For instance, recent"}, {"title": "3.4 Fact + logical rules", "content": "studies have demonstrated LLMs' effectiveness as semantic parsers, often surpassing traditional parsing tools. While Geibinger (2023) and Eiter et al. (2022) showed promising results integrating LLMs with ASP, challenges remain. Ishay et al. (2023) found LLMs could generate complex ASP programs but often with errors, while Yang et al. (2023) achieved 90% accuracy on StepGame using GPT-3 and ASP, though scalability remains uncertain.\nIn order to overcome these weaknesses, inspired by Pan et al. (2023)'s LOGIC-LM framework and integration neural-symbolic strategies, we propose a novel neural-symbolic pipeline employing ASP using DSPy that treats the LLM as an agent capable of feedback and iteration. DSPy (Declarative Self-improving Language Programs, pythonically) is a Python framework that uses a declarative and self-improving approach to simplify working with LLMS (Khattab et al., 2023). It automates the optimization of prompts and model tuning, enhancing reliability and scalability in AI applications. By defining tasks and metrics rather than manual prompts, DSPy streamlines the development of various NLP tasks and complex AI systems. The framework of this pipeline is shown in Fig.1.\nThe pipeline consists of four main stages: a) Facts Generation Stage: LLM converts natural language descriptions into symbolic formulations and formal queries. b) ASP Refining Stage: LLM iteratively refines the ASP representation over three iterations, adding rules, checking consistency, and incorporating feedback from error messages. c) Symbolic Reasoning Stage: The refined ASP undergoes inference using the Clingo solver, ensuring accurate and explainable reasoning by combining LLM capabilities with logical inference. d) Result Interpretation and Evaluation: This stage involves mapping the Clingo solver's outputs to candidate answers. For certain question types, like Yes/No and Finding-Block questions, the solver's output can directly serve as the correct answer. However, for Finding Relations and Choose Object questions, additional processing is necessary to filter relevant solutions. In the StepGame context, outputs from the ASP solver are evaluated against a synonym dictionary to determine accuracy.\nOverall, this pipeline requires multiple interactions with LLMs during ASP generation and refinement. We employ the DSPy framework to manage these complex workflows (e.g., interfacing with Llama3 60B DeepSeek and GPT 4.0 mini models via their APIs). DSPy's modular features enhance memory retention between modules, enabling adjustments and optimizations while maintaining workflow integrity.\nAdditionally, DSPy optimizes LLM prompts and weights, reducing the need for manual prompt engineering and ensuring consistent performance across datasets. Its optimization compiler iteratively generates and refines prompts, enhancing task performance. To support transparency and debugging, outputs from all modules are logged, capturing errors and providing insights for prompt engineering and system optimization, facilitating continuous improvement of the system and enhancing the integration of neural and symbolic components. In this way, this integrated neural-symbolic pipeline could greatly facilitate spatial reasoning in LLMs.\nThe \u201cLLM + ASP\" approach could have some fundamental limitations in applying neural-symbolic integration to complex spatial reasoning tasks. Despite their strong in-context learning capabilities, LLMs may consistently struggle to generate precise logical programs from few-shot examples, particularly when dealing with the formal syntax requirements of ASP or Prolog. This limitation could become especially pronounced in testing in some complex tasks such as the SparQA, where the iterative refinement process necessitates multiple LLM calls for a single ASP program generation, creating significant computational overhead even with a modest three-iteration limit.\nTo address these challenges, we propose an alternative neural-symbolic approach that preserves the advantages of structured knowledge representation while reducing the complexity of formal logical programming. Specifically, inspired by Symbolic Embedded Neural Network architectures, our alter"}, {"title": "3.5 Tools, LLMs and evaluation criteria", "content": "native approach embeds logical rules directly into natural language prompts, enabling LLMs to perform reasoning within structured knowledge representations. Instead of relying on the external logical solver to do the reasoning, this approach explicitly prompts LLMs of which rules should be used for certain scenarios. That is why the approach is termed as \u201cfact +logical rules\". It is expected that direct rule application in natural language may offer a more reliable path to spatial reasoning than attempting to generate and execute formal logical programs.\nThe core principle of our alternative approach aligns with a fundamental aspect of neural-symbolic AI: the translation of raw data into structured, symbolic representations that serve as meaning ASP. By using predicates with precise argument structures, we instruct LLMs to create consistent knowledge representations that serve as an intermediate basis for question answering. This structured approach simplifies the previous complex process of generating and refining ASP code while maintaining the benefits of formal reasoning.\nIn our experiments, we used DSPy framework to optimize multi-stage prompting workflows, moving from manual prompt engineering to a structured, automated process. DSPy was key in defining and refining each stage of our pipeline from converting natural language to logical expressions, to generating Python code, and running ASP tasks with Clingo. This setup allows us to streamline complex reasoning processes and achieve high-quality, multi-step outputs. While LangChain could support similar workflows, DSPy's focus on multi-stage LLM optimization made it a better fit for our needs.\nTo evaluate the effectiveness of our approaches in spatial reasoning comprehensively, we selected three representative LLMs with diverse architectures and capabilities: DeepSeek, Llama3, and GPT4.0 Mini. These LLMs were chosen to ensure a comprehensive assessment across different types of language representations, ranging from lightweight and specialized models like DeepSeek to more advanced general-purpose systems like GPT4.0 Mini. Llama3, known for its balance between performance and computational efficiency, provides an intermediate perspective. By testing our methods on these distinct models, we want to demonstrate the adaptability and robustness of our approach across a variety of LLM architectures and reasoning capacities.\nMoreover, to effectively evaluate the impact of our proposed integration methods, we used \u201cdirect prompting\" as the baseline for comparison. In the following sections, \"direct\" is used to refer to this baseline method"}, {"title": "4 Experiment 1: Proposed LLM + ASP", "content": "LLM performance evaluation includes a range of metrics depending on the task, such as objectivity, truthfulness, human alignment, fluency, coherence, relevance, and task-specific indicators like BLEU, ROUGE, F1, and exact match. For creative or reasoning-intensive tasks, human assessment is often essential to complement automated measures. Frameworks like LangChain and DSPy offer evaluation tools for LLMs. LangChain includes string-based and semantic metrics, while DSPy provides core metrics for NLP tasks, although it lacks direct support for evaluating Answer Set Programming, where no reference answer exists. Considering these, our study adopts micro-F1 score as the main evaluation metric.\nDue to the widespread application of ASP in spatial reasoning tasks and the relative simplicity of the StepGame dataset, we have included the detailed implementation and results in Appendix C. These results serve as a baseline for comparisons with other datasets and methodologies in the spatial reasoning domain.\nIn this section, we assess the effectiveness and generalizability of the \u201cLLM + ASP\" approach on SparQA, a more challenging benchmark than StepGame, with its longer sentences, varied question types, and complex reasoning requirements."}, {"title": "4.1 Implementation", "content": "Due to the task complexity and need for high-quality ASP fact generation, we focused on powerful LLMs: GPT4.0 mini, llama3 70B, and Deepseek 76B. We built a subset of the SparQA dataset with 220 examples (55 from each question type) for model inference.\nWe adopted the same pipeline as StepGame to SparQA: (1) Converting Natural Language Context and Question to ASP Facts; (2) Adding Rules"}, {"title": "4.2 Results and discussion", "content": "and Refining ASP Program; (3) Symbolic Reasoning; (4) Result Mapping and Evaluation.\nThe first module prompts LLMs to identify blocks, objects, and relation facts using three predicates: block/1, object/5, and is/3. The second module involves rule adoption and ASP refinement, with manually designed rules for inverse, transitive, and symmetric relations. The specific code and samples are seen in Appendix D.\nOur neural-symbolic pipeline showed mixed results across different models and question types, as shown in Table ??. Finding Relation (FR) questions demonstrated significant improvement with accuracy increasing by approximately 20% across all models (from 26.92% to 53.12% on Llama3, 38.18% to 58.94% on Deepseek, and 45.45% to 65.32% on GPT 4.0). Finding Block (FB) questions benefited from structured block/5 predicate representation, showing substantial gains particularly in GPT 4.0 (from 60.91% to 80.49%). Choose Object (CO) questions showed varied results, with GPT-4.0 achieving a notable 15% improvement while other models showed minimal changes. Interestingly, Yes/No (YN) questions performed better with direct prompting across all models, suggesting that simpler question types may not benefit from the additional complexity of neural-symbolic methods."}, {"title": "4.3 Error analysis", "content": "The ASP solver outputs reveal four main types of errors when inconsistent with ground truth. Grounding Errors occur due to inconsistent variable naming or undefined objects/relations in the ASP code, highlighting the importance of maintaining consistency between facts and queries. Parsing Errors are caused by unqualified relations, punctuation problems, irrelevant comments, or undefined variables. For example, Deepseek often failed to properly comment code with \u201c%\u201d, while GPT-4.0 mini frequently mixed argument orders in block/5 facts.\nSatisfiability without Query Results arise when facts and rules are insufficient to solve queries, often due to implied relations not being explicitly coded. Consider this example:\n\u2018\u2018The medium triangle is touching the bottom edge of the block.\nThe circle is below and to the left of the small triangle.\nIt is above the medium triangle.\u2019'\nHere, the query query(R):-is(medium_triangle,circle) becomes unsatisfiable without explicit relation definitions.\nWrong choices occurred when the ASP solver produced logically consistent but incorrect results compared to ground truth. Most parsing and grounding errors occurred during query parsing, particularly with complex spatial descriptions involving multiple objects and nested relations. Different models showed distinct error patterns, suggesting the need for model-specific optimization strategies and prompting approaches."}, {"title": "5 Experiment 2: Fact + Logical rules", "content": "The implementation consists of two primary stages. The first stage maintains the initial semantic parsing component from the \u201cLLM + ASP\" pipeline, where models convert natural language into structured facts using predefined predicates. This conversion provides a clear and organized foundation for subsequent reasoning steps. In the second stage, rather than generating formal logical programs, LLMs directly apply logical rules (inverse, symmetric, transitive, and inter-block relationships for SparQA dataset, and chain linking based on offset for StepGame) through natural language reasoning to derive new knowledge and answer queries. The specific prompt and code samples are seen in Appendix E."}, {"title": "5.1 Implementation", "content": "Our experiments demonstrated the potential of neural-symbolic integration, achieving consistent accuracy above 80% across different models. This success can be attributed to three key factors: (1) the effective separation of semantic parsing and logical reasoning, enabling precise control over each component; (2) the well-defined spatial relationships in a 2D environment, allowing for unambiguous predicate representation; and (3) the efficient handling of multi-hop reasoning chains through explicit logical rules.\nWe summarize our contributions on using neural-symbloic methods to systematically improve the performance of spatial reasoning in LLMs. The five contributions could be described as follows.\nFirst, boost spatial reasoning: our integration methods significantly enhance spatial reasoning in complex tasks across various LLMs. While"}, {"title": "5.2 Result", "content": "The implementation consists of two primary stages. The first stage maintains the initial semantic parsing component from the \u201cLLM + ASP\" pipeline, where models convert natural language into structured facts using predefined predicates. This conversion provides a clear and organized foundation for subsequent reasoning steps. In the second stage, rather than generating formal logical programs, LLMs directly apply logical rules (inverse, symmetric, transitive, and inter-block relationships for SparQA dataset, and chain linking based on offset for StepGame) through natural language reasoning to derive new knowledge and answer queries. The specific prompt and code samples are seen in Appendix E.\nThe evaluation demonstrates the effectiveness of the \"Facts + Rules\" approach across both SparQA and StepGame datasets. To make the comparison easy to observe, we only report the overall F1 score on the dataset, leaving out the question type accuracy.\nThe \"Facts + Rules\" method demonstrates competitive performance with the \"LLM + ASP\" approach on the SparQA dataset while significantly outperforming direct prompting (>5%). This success can be attributed to the elimination of parsing errors and inconsistent naming issues that plagued the ASP-based approach. When the facts are correctly represented and logical rules properly applied, the reasoning process becomes more transparent and reliable."}, {"title": "5.3 Extension", "content": "We extended this approach to the StepGame dataset by translating its two-dimensional offset-based rules into natural language prompts. While the performance (66.7-77.2%) does not match the exceptional results of the ASP-based approach (82.4-90.9%), it represents a substantial improvement over baseline direct prompting (29.8-33.3%). Surprisingly, the \"Facts + Rule\" natural language prompts work especially best for Llama3 70B. This improvement is particularly notable in handling extended reasoning chains (k \u2265 5), where the structured approach provides clear guidance for step-by-step inference.\nThe success of this simplified approach illustrates an important principle in neural-symbolic integration: effective reasoning can be achieved"}, {"title": "6 Discussion", "content": "LLMs can struggle with maintaining consistency over long, complex reasoning chains (like navigating through a series of spatial steps), symbolic modules can enforce strict rules of inference and logic, which enhances accuracy and coherence in spatial tasks. Second, advance neural-symbolic approaches: we provide a cohesive pipeline that strengthens neural-symbolic methods for improving LLM reasoning. Third, structured knowledge representation: our approach facilitates efficient knowledge representation and inference through advanced encoding techniques. Our symbolic modules enable LLMs to handle structured, explicit representations of spatial information, such as geometric shapes, positions, and spatial relationships. This allows LLMs to reason in a more organized and interpretable manner compared to raw, unstructured data processing alone. Forth, robust and generalizable: our approach is robust and highly generalizable, making it suitable for other complex reasoning tasks where LLMs typically struggle, such as temporal and deductive reasoning. Our symbolic reasoning systems are designed to generalize better across a wide range of situations by applying logical rules. Integrating this with the probabilistic nature of LLMs provides the system with a more robust way to infer spatial relationships, even in new or unseen scenarios. Moreover, LLMs are strong in natural language processing, while symbolic systems excel at tasks requiring formal logic. By combining both, LLMs can leverage the flexibility of language generation and the rigor of symbolic reasoning, making spatial tasks more efficient and accurate."}, {"title": "6.1 Strengths", "content": "This section focuses on the potential problems and limitations on our approaches and experiments. Our experiments with StepGame and SparQA datasets reveal excellent performance of neural-symbolic integration for spatial reasoning tasks. Through systematic evaluation across different question types and reasoning complexities, we observed distinct performance patterns that illuminate the practical considerations for deploying such systems.\nSparQA saw averaging 60% accuracy across four question types. However, the SparQA experiments perform differently across the four questions. First, though the over F1 accuracy is higher than the baseline direct prompting strategy, the performance is not consistent among the four types of questions, particularly in YN questions with quantifiers and implicit spatial relationships. Second, the system struggled with query generation for sophisticated question types, often failing to capture the full semantic complexity of natural language descriptions. Third, the refinement process introduced substantial computational overhead, raising questions about scalability in real-world ap"}, {"title": "6.2 Limitations", "content": "Compared with the previous related work (Yang et al., 2023; Li et al., 2024a; Mirzaee and Kordjamshidi, 2023), our results shown in Tables ?? and 3 have made substantial improvements in using neural-symbolic methods to enhance spatial reasoning in LLMs. For example, our neural-symbolic approaches achieved over 80% accuracy on StepGame, and averaged 60% on the more complex SparQA dataset. We first confirm that our integration methods are much more effective. Second, our methods greatly outperform the existing methods. For instance, our experiments demonstrate significant improvements over the baseline prompting methods, with accuracy increases of 40-50% on StepGame dataset and 3-13% on the more complex SparQA dataset. The \u201cLLM + ASP\" pipeline achieves particularly strong results on the tasks of Finding Relations (FR) and Finding Block (FB) questions, though performance varies across different question types. Additionally, our methods have formed an integrated pipeline compared with previous related work, and further could be easily applied in other domains. We have addressed our first research question. The following will resolve the second research question.\nOur experiments demonstrated the potential of neural-symbolic integration, achieving consistent accuracy above 80% across different models. This success can be attributed to three key factors: (1) the effective separation of semantic parsing and logical reasoning, enabling precise control over each component; (2) the well-defined spatial relationships in a 2D environment, allowing for unambiguous predicate representation; and (3) the efficient handling of multi-hop reasoning chains through explicit logical rules."}, {"title": "6.3 Future directions", "content": "plications.\nThis performance disparity between datasets highlights a crucial challenge in neural-symbolic integration: domain sensitivity. While StepGame's structured environment enabled efficient predicate-based representation, SparQA's naturalistic descriptions exposed the difficulties in maintaining consistent mappings between neural and symbolic components. This observation aligns with Yang et al. (2023) findings regarding the domain-specific nature of knowledge modules in symbolic reasoning. Their work demonstrated that even relatively simple reasoning tasks, such as those in the babI dataset, require multiple specialized knowledge modules for different types of reasoning (e.g., temporal reasoning, spatial relations, and basic inference). Creating and maintaining such modules can be time-consuming and requires domain expertise.\nThe neural-symbolic pipeline allows for more explainable and traceable reasoning processes, addressing a major criticism of pure neural network approaches. Nevertheless, this advantage comes at the cost of increased system complexity and the need for careful design of the interaction between neural and symbolic components. The error-prone nature of converting natural language to logic programs, as observed in our experiments, remains a bottleneck of the neural-symbol"}]}