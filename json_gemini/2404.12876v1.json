{"title": "A Large-scale Medical Visual Task Adaptation Benchmark", "authors": ["Shentong Mo", "Xufang Luo", "Yansen Wang", "Dongsheng Li"], "abstract": "Visual task adaptation has been demonstrated to be effective in adapting pre-trained Vision Transformers (ViTs) to general downstream visual tasks using specialized learnable layers or tokens. However, there is yet a large-scale benchmark to fully explore the effect of visual task adaptation on the realistic and important medical domain, particularly across diverse medical visual modalities, such as color images, X-ray, and CT. To close this gap, we present Med-VTAB, a large-scale Medical Visual Task Adaptation Benchmark consisting of 1.68 million medical images for diverse organs, modalities, and adaptation approaches. Based on Med-VTAB, we explore the scaling law of medical prompt tuning concerning tunable parameters and the generalizability of medical visual adaptation using non-medical/medical pre-train weights. Besides, we study the impact of patient ID out-of-distribution on medical visual adaptation, which is a real and challenging scenario. Furthermore, results from Med-VTAB indicate that a single pre-trained model falls short in medical task adaptation. Therefore, we introduce GMoE-Adapter, a novel method that combines medical and general pre-training weights through a gated mixture-of-experts adapter, achieving state-of-the-art results in medical visual task adaptation.", "sections": [{"title": "Introduction", "content": "Recent advancements in deep learning have significantly propelled the field of computer vision, especially with the introduction of Vision Transformers (ViTs) [8, 19, 35, 42]. These models [6, 11, 28], once pre-trained on large-scale datasets, have demonstrated remarkable capabilities across a wide range of visual tasks. The adaptability of ViTs to specific downstream tasks through mechanisms such as specialized learnable layers or tokens [13, 41] has opened new avenues for task-specific model optimization. This adaptability, known as visual task adaptation, allows for the fine-tuning of pre-trained models to cater to the nuances of particular tasks, enhancing model performance and applicability.\nDespite these advancements [43], the application of visual task adaptation in the medical domain remains underexplored, particularly across diverse medical"}, {"title": "Related Work", "content": "This section delves into the landscape of existing benchmarks and adaptation methodologies, positioning our work within the broader context of medical task adaptation and highlighting the novel contributions of the Med-VTAB benchmark and the Gated Mixture-of-Experts Adapter (GMoE-Adapter).\nAdaptation Benchmark. Benchmarks play a crucial role in assessing the performance and generalizability of machine learning models across a wide array of tasks. In the domain of visual task adaptation, benchmarks like VTAB (Visual Task Adaptation Benchmark) [43] have set the standard for evaluating the adaptability of models to various visual domains and tasks. However, the specific challenges and requirements of medical imaging-ranging from the diversity of imaging modalities to the critical need for high accuracy in diagnostic tasks-necessitate a dedicated benchmark. Our Med-VTAB fills this gap by providing a comprehensive suite of medical imaging datasets that encompass a"}, {"title": "Med-VTAB: Medical Visual Task Adaptation Benchmark", "content": "To comprehensively evaluate the efficacy of visual task adaptation in the medical domain, we developed the Medical Visual Task Adaptation Benchmark (Med-VTAB). This benchmark is meticulously designed to cover a broad spectrum of medical images (1.68 million), encompassing diverse organs and modalities, and to test various adaptation approaches. Below, we detail the benchmark's structure in terms of organ diversity, modality diversity, and the range of adaptation methods explored."}, {"title": "Organ Diversity", "content": "Med-VTAB includes a wide array of organ-specific datasets, ensuring comprehensive coverage of the human body and a variety of diseases and conditions. The inclusion of diverse organs enables the evaluation of adaptation techniques across different anatomical structures and pathologies. The organs covered in the benchmark are as follows: Lung, Breast, Chest, Eye, Shoulder, Skin, Brain, Bone, and Gastrointestinal, as shown in Fig. 2 (Left). This diversity ensures that"}, {"title": "Modality Diversity", "content": "Medical imaging employs a variety of modalities, each offering unique insights into the body's internal structures. Med-VTAB reflects this diversity by including datasets from the following imaging modalities: Color Images, X-ray, Optical Coherence Tomography (OCT), Computed Tomography (CT), and Magnetic Resonance Imaging (MRI), as shown in Fig. 2 (Right). This range allows for the assessment of adaptation methods across different imaging techniques, highlighting their versatility and performance in handling the specific challenges posed by each modality, such as varying image resolutions, contrasts, and dimensions. Certain modalities, such as MRI and CT, benefit from additional preprocessing steps like windowing, which adjusts the range of pixel intensities to highlight specific tissues or structures. These modality-specific adjustments are critical for maximizing the diagnostic value of the images."}, {"title": "Adaptation Diversity", "content": "A core aspect of Med-VTAB is its exploration of various adaptation strategies. These strategies are categorized into full fine-tune, head-oriented, backbone-oriented, and prompt-oriented adaptations, each targeting different components of the pre-trained models for optimization. The specific approaches explored under each category are as follows:\nFull fine-tune. This approach involves the comprehensive fine-tuning of all model parameters, serving as a baseline for adaptation performance.\nHead-oriented. This category focuses on adapting the final layers (heads) of the model, with strategies including: 1) Linear prob: finetune a single linear layer as a classification head; 2) MLP-3: use a three-layer Multi-Layer Perceptron adaptation as the classification head; 3) Partial-1 [27]: only fine-tune the last block of the backbone and freeze other blocks.\nBackbone-oriented. Targeting the model's backbone, these methods aim to adjust the core representations: 1) Sidetune [44]: introducing parallel networks that are fine-tuned while the main network remains frozen; 2) Bias [3]: adjusting only the bias terms in the model. 3) Adapter [29]: inserting small, trainable layers within the model's architecture.\nPrompt-oriented. Leveraging the concept of prompts, these methods introduce small modifications or additional inputs to guide the model's focus: 1) VPT [13]-Shallow & -Deep: Variants of Visual Prompt Tuning that introduce prompts at different depths of the model; 2) GaPT [41]: a gated prompt tuning approach that dynamically adjusts prompts based on the input. 3) LSPT [23]: a strong prompt tuning method with long-term gated prompts on both temporal and spatial coding.\nBy encompassing this broad range of adaptation strategies, Med-VTAB enables a thorough investigation into the scalability, efficiency, and effectiveness of various adaptation methods in the context of medical imaging. Through this benchmark, we aim to identify optimal strategies for leveraging pre-trained models across diverse medical visual tasks, thereby advancing the field of medical image analysis and improving diagnostic capabilities."}, {"title": "Method", "content": "The divergence between the medical and general domains presents a significant challenge for models trained exclusively within one domain, as those focused solely on general domain lack the specificity required for medical applications, whereas models dedicated to the medical domain suffer from limitations in training data volume and model capacity. To bridge this gap, we consider leveraging multiple Vision Transformers (ViTs) for adapation, and introduce the Gated Mixture-of-Experts Adapter (GMoE-Adapter), a novel approach aimed at enhancing the adaptation of ViTs for a broad spectrum of medical imaging tasks. In this section, we first outline the notations used and revisit the concept of adapters in ViTs. Then, we present methods on leveraging multiple ViTs, including an intuitive baseline and the proposed GMoE-Adapter."}, {"title": "Notations", "content": "Let X denote the input image, and Y the corresponding label in a medical imaging dataset. We use f(.) to represent the pre-trained ViT model, which transforms X into a set of feature representations. The adapter module, denoted as A(.), is applied to these representations to produce task-specific features. The final output, Y, represents the model's prediction. Parameters of f(.) are denoted by \u0398, while I represents the parameters of the adapter module."}, {"title": "Adapter", "content": "Adapters [29] are small neural network modules inserted between the layers of a pre-trained model. They allow for fine-tuning on specific tasks without altering the original model parameters significantly. This approach is particularly appealing for medical imaging, where the amount of available training data may be limited, and preserving the knowledge in pre-trained models is crucial. Adapters typically consist of a few fully connected layers and non-linear activation functions, enabling the model to learn task-specific features efficiently."}, {"title": "Leveraging Multiple Expert Pre-Trained Models", "content": "Mixture-of-Experts Adapter The Mixture-of-Experts (MoE) Adapter extends the adapter concept by incorporating multiple expert networks within the adapter module. Each expert for general Ag(\u00b7) and medical adapter Am() is designed to specialize in different aspects or features of the input data. The MoE approach allows for a more nuanced adaptation of the model to the task at hand by combining the outputs of these experts based on the input data, that is, $Y_{mo} = A_g(\u00b7) + A_m (\u00b7)$ .\nGated Mixture-of-Experts Adapter Building upon the MoE Adapter, the Gated Mixture-of-Experts Adapter (GMoE-Adapter) introduces a gating mechanism to control the contribution of each expert to the final adapted feature representation, as shown in Fig. 3. The gating mechanism is a trainable component that determines the relevance of each expert for a given input, enabling"}, {"title": "", "content": "the model to dynamically allocate computational resources and attention to the most pertinent experts.\nThe GMoE-Adapter comprises two key components: 1) Experts: pre-trained backbones from two different domains, each designed to capture different aspects of the medical imaging data using general Ag(\u00b7) and medical Am(\u00b7) adapters. 2) Gating: a learnable parameter a that computes weights for each expert based on the input features, effectively determining which experts are most relevant for the current task. The final output for GMoE-Adapter is defined with respect to the gated interpolation of each expert adapter output as $Y_{GMOE} = \\alpha \\cdot A_g(\u00b7) + (1-\\alpha) \\cdot A_m(\u00b7)$, where $\\alpha \\in \\mathbb{R}^{1\\times D}$ and D is the dimension of embeddings.\nThe integration of the GMoE-Adapter into pre-trained ViTs involves inserting the adapter module at strategic points within the model, typically after each transformer block. This placement allows the adapted model to refine the pre-trained features iteratively, ensuring that the final feature representations are highly tailored to the specific requirements of medical imaging tasks. Through the GMoE-Adapter, we aim to harness the rich representational power of pre-trained ViTs and enhance their adaptability to the nuanced and diverse challenges of medical image analysis."}, {"title": "Benchmarking Medical Visual Task Adaptation", "content": ""}, {"title": "Experimental Setup", "content": "We detail our experimental setup, including the datasets used, evaluation metrics, implementation details, benchmarking results on medical visual task adaptation, and comparisons with prior work.\nDatasets. Our experiments span a comprehensive set of medical image datasets to ensure a broad evaluation of our GMoE-Adapter across various organs and"}, {"title": "Main Results", "content": "To comprehensively assess the capabilities of existing adaptation methods in medical visual task adaptation, we performed extensive benchmarking across various medical imaging modalities. This section reports on the performance of existing visual task adaptation methods across color images, X-ray, \u041e\u0421\u0422, \u0421\u0422, and MRI modalities.\nFor color image datasets, including those for skin lesion analysis and retinal disease diagnosis, our GMoE-Adapter demonstrated superior performance in adapting pre-trained ViTs to these specific tasks. The results, detailed in Table 1, highlight the adapter's effectiveness in capturing the nuanced features required for accurate diagnosis from color medical images by inserting small and trainable parameters into the backbone.\nThe X-ray modality, crucial for diagnosing a wide range of conditions from bone fractures to lung diseases, posed unique challenges due to its high variance in image density and structure. As shown in Table 2, the Adapter still outperforms linear probing and previous prompt tuning approaches with limited trainable parameters, showcasing its ability to enhance model sensitivity to relevant pathological features in X-ray images. For other modalities such as"}, {"title": "Evaluation of GMoE-Adapter", "content": "In comparing our GMoE-Adapter to prior works, including the original Mixture-of-Experts Adapter and other state-of-the-art adaptation techniques, we further establish the superiority of our approach. The comparative analysis, summarized in Table 4, illustrates the enhanced performance of the GMoE-Adapter across all evaluated datasets and modalities. In particular, the proposed GMoE-Adapter significantly outperforms the vanilla Adapter based on DINO v2 [28] by 0.37,"}, {"title": "In-Depth Analysis for Medical Visual Task Adaptation", "content": "In this section, we introduce the experiments conducted by us to discuss the following research insights:"}, {"title": "Scaling Law of Medical Prompt Tuning", "content": "Inspired by the Scaling Law study [9] by OpenAI, we investigate the scaling law of medical prompt tuning in the context of tunable parameters. Our analysis involves adjusting the number of tunable parameters and observing the corresponding performance changes on various medical imaging tasks. The results, detailed in Table 5, illustrate a clear trend (e.g., 62.21\u2192 64.97 on Polyp) where increasing the number of tunable parameters from 1.01X to 1,39X enhances the model's performance. When it comes to other organs, a similar increasing trend of performance can be observed with the increase in the number of tunable parameters. This finding suggests a desirable scaling law between the number of prompt tuning parameters and the effectiveness of adaptation, highlighting the efficiency of prompt tuning for medical applications."}, {"title": "Generalizability of Medical Visual Adaptation", "content": "The generalizability of medical visual adaptation is examined by applying visual prompt tuning [13] to models pre-trained on both medical and non-medical datasets. Specifically, we use DINO v2 [28] pre-trained ViT-B/16 models on 1.28 million general images and medical pre-trained models [25] on 1.6 million cell images. This experiment conducted on all color image modalities aims to understand how the source of pre-training influences the adaptation process and"}, {"title": "Patient ID Out-of-Distribution", "content": "To assess the impact of patient ID out-of-distribution on medical visual adaptation, we conducted experiments using visual prompt tuning [13] on 160 infected patients in LHNCBC Malaria [18] dataset with different patient ID splits, including seen and unseen distributions. Specifically, we utilize three settings for a comprehensive evaluation: 1) use the same total number of seen and unseen patients, 2) use the same number of seen patients and a varied number of unseen patients from {80,60,40, 20}; 3) use the same number of unseen patients and a varied number of seen patients from {140, 120, 100, 80, 60}. For the first setting, we vary the number of seen patients from {160,100,80,60} and the number of unseen patients from {0,60,80, 100}. The results in Tables 7, 8, and 9, demonstrate the robustness of prompt tuning to patient ID variations. We can observe that with the increase in the number of tunable parameters from 1.01X to 1,39X, the model's performance improves regarding all settings. When using the same number of unseen patients for evaluation, increasing the number of seen patients"}, {"title": "", "content": "from 60 to 80 enhances the results. However, increasing 80 to 140 deteriorates the results, which might be caused by the overfitting of the training sets. While performance naturally dips in unseen patient ID scenarios, the adapter maintains a commendable level of accuracy, suggesting its potential to handle real-world variations and generalize across patient cohorts effectively."}, {"title": "Conclusion", "content": "In this work, we introduced Med-VTAB, a comprehensive benchmark for medical visual task adaptation, addressing a significant gap in the evaluation of pre-trained Vision Transformers (ViTs) across diverse medical imaging modalities and tasks. Through the deployment of Med-VTAB, we have explored the landscape of medical visual adaptation with an emphasis on organ diversity, modality diversity, and adaptation diversity, encompassing a vast dataset of 1.68 million medical images. Our methodological contribution, the Gated Mixture-of-Experts Adapter (GMoE-Adapter), represents a novel approach in leveraging both medical and general pre-trained weights for enhancing the performance of ViTs on medical visual tasks. The experimental analysis provided deep insights into three critical aspects of medical visual adaptation: the scaling law of medical prompt tuning concerning tunable parameters, the generalizability of adaptation using both non-medical and medical pre-train weights, and the impact of patient ID out-of-distribution scenarios on model performance. Our findings underscore the importance of scalable, generalizable, and robust adaptation mechanisms in medical imaging, where variability across patients and tasks is immense.\nBroader Impact. Our work on Med-VTAB and the GMoE-Adapter not only advances the state of the art in medical visual task adaptation but also opens up new avenues for research. Our contributions pave the way for more personalized, accurate, and accessible diagnostic tools, ultimately contributing to better healthcare outcomes through the power of artificial intelligence. While our work has the potential to improve medical imaging analysis significantly and show robustness against out-of-distribution data, it also raises important ethical considerations. The adaptation of AI models in healthcare must be approached with caution to ensure patient privacy, data security, and bias mitigation."}, {"title": "Dataset Details", "content": "In this section, we provide detailed information on the datasets incorporated into the Med-VTAB benchmark. These datasets span across different imaging modalities such as Color Images, X-ray, Optical Coherence Tomography (OCT), Computed Tomography (CT), and Magnetic Resonance Imaging (MRI), reflecting the diversity and complexity of medical visual tasks, as shown in Table 10. The comprehensive details including the dataset names, primary medical applications, number of images, unique characteristics, and data sizes are detailed as follows."}, {"title": "Color Images", "content": "HyperKvasir [4]: Polyp, Color images. The dataset contains 110,079 images where it captures anatomical landmarks and pathological/normal findings across 23 different classes. 3.\nMESAD Prostatectomy [2]: Prostatectomy Procedures, Action classification in Prostatectomy Surgery. The dataset includes 29,454 color images with 21 different action classes in the dataset. 4.\nAMLC [22]: Cell, Peripheral blood smears, Color images. The dataset consists of 18,365 images across 15 different morphological classes. 5.\nAPTOS [1]: Eye, Severity of diabetic retinopathy, color images. The dataset contains 3,662 images, rated on a scale of 0 to 4 for the severity of diabetic retinopathy across 5 different classes. 6.\nISIC [34]: Skin Cancer, Color images. This dataset includes 25,331 images for the classification of dermoscopic images across 9 different diagnostic categories. 7.\nKvasir [16]: Gastrointestinal cancer, Color images. The dataset includes 6,000 images showing 8 different classes of findings in the GI tract. 8.\nLHNCBC Malaria [18]: Cells, Malaria, Color images. The dataset contains 27,560 images with 12 different classes of annotations from 160 patients. 9.\nMLLBone [21]: Cells, Blood cell, Color images. This dataset features 171,374 images across 21 different classes of tissues from 945 patients. 10.\nEyePACS [14]: Eye, Diabetic Retinopathy, Color images. The dataset includes 88,702 images categorized into 5 classes based on the severity of Diabetic Retinopathy (DR). 11."}, {"title": "X-ray", "content": "Vindr [26]: Abnormal Disease. This dataset contains 18,000 CXR scans with 14 critical findings. 12"}, {"title": "", "content": "CBIS-DDSM [17]: Breast Cancer. This dataset contains 10,239 images for breast cancer screening, including conditions such as normal, benign, and malignant. 13\nCOVIDX [37]: Lung Covid-19. This dataset includes 194,922 images for COVID-19 detection, categorized into 4 classes.14\nSYMH [33]: Shoulder. This dataset provides 1,049 shoulder X-ray images across 4 categories. 15\nRSNA Bone [10]: Bone age. This dataset features 12,611 images for bone age assessment, spanning 228 age classes. 16\nCheXpert [12]: Chest radiographs. This dataset includes 224,316 chest radiographs with 5 categories (atelectasis, cardiomegaly, consolidation, edema, and pleural effusion) from 65,240 patients. 17\nRSNA [32]: This dataset contains 29,684 chest radiographs with 2 classes (normal and pneumothorax positive). 18."}, {"title": "Optical Coherence Tomography (OCT)", "content": "Heidelberg OCT [15]: contains 84,495 OCT images across 4 categories (CNV, DME, DRUSEN, and NORMAL) related to eye diseases. 19."}, {"title": "Computed Tomography (CT)", "content": "CC-CCII [45]: focuses on COVID-19 pneumonia with a large dataset of 617,775 CT images.20\nMosmed [24]: documents COVID-19 pneumonia cases in a collection of 1,110 CT scans.21\nCOVID-C [30]: contains 349 CT images for COVID-19 pneumonia detection.22\nRICORD [36]: offers a dataset of 120 CT images also for COVID-19 pneumonia analysis.23"}, {"title": "Magnetic Resonance Imaging (MRI)", "content": "PPMI [20]: A dataset containing 480 MRI scans related to Parkinson's disease. 24\nBrain-Tumor [5]: consists of 7,023 MRI images for brain tumor detection and segmentation.25"}, {"title": "Additional Analysis on Scaling Law", "content": "Table 5 presents a detailed examination of the scaling law of medical visual prompt tuning with DINO v2 [28] pre-trained vision transformers on color images. We observe that the performance improvement is directly correlated with the increase in the total number of parameters (Total Params) which encompasses the backbone encoder ViT-B [8], prompt tokens, and the task heads. An analysis across different organs and medical conditions such as cell, eye, skin, and polyp-reveals quantitative differences in the adaptation efficiency of tunable parameters. For example, the HyperKvasir Polyp [4] dataset demonstrates substantial performance improvements as Total Params increases, whereas the AMLC [22] Cell dataset shows relatively modest gains.\nBased on the observed data, we propose the following suggestions for tuning parameters tailored to each organ type. For polyp detection tasks (HyperKvasir [4] and Kvasir [16] Polyp), a higher number of tunable parameters (>1.1X) can significantly boost performance, likely due to the complexity and variability within the image data. Cell-related tasks (AMLC [22] and MLLBone [21] Cell) may require a moderate increase in parameters, as excessive tuning does not correspond to a significant performance leap, suggesting a point of diminishing returns. Eye conditions (APTOS [1] and EyePACS [14]) show a favorable response to parameter scaling, indicating that a careful increment in parameters could be beneficial, particularly for nuanced features in retinal images. For skin lesion analysis (ISIC Skin [34]), a moderate to high parameter scaling appears to offer the best performance gains, potentially due to the visual similarity among different lesion types which requires more complex model tuning to distinguish."}, {"title": "Additional Analysis on Patient ID Out-of-Distribution", "content": "To delve deeper into the influence of Patient ID Out-of-Distribution (OOD) on medical visual adaptation, we extended our experiments with visual prompt tuning as proposed by Jia et al. [13] using the LHNCBC Malaria dataset [18], focusing on 160 infected patients and analyzing the effects under various patient ID splits. Our analysis not only considers the variation in the number of seen versus unseen patients but also investigates the impact of tuning parameter"}]}