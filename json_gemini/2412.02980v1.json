{"title": "Surveying THE EFFECTS OF QUALITY, DIVERSITY, AND\nCOMPLEXITY in SynNTHETIC Data From Large LAN-\nGUAGE MODELS", "authors": ["Alex Havrilla", "Andrew Dai", "Laura O'Mahony", "Koen Oostermeijer", "Vera Zisler", "Alon Albalak", "Fabrizio Milo", "Sharath Chandra Raparthy", "Kanishk Gandhi", "Baber Abbasi", "Duy Phung", "Maia Iyer", "Dakota Mahan", "Chase Blagden", "Srishti Gureja", "Mohammed Hamdy", "Wen-Ding Li", "Giovanni Paolini", "Pawan Sasanka Ammanamanchi", "Elliot Meyerson"], "abstract": "Synthetic data generation with Large Language Models (LLMs) has emerged as a promis-\ning paradigm for augmenting natural data over a nearly infinite range of tasks. However,\nmost existing methods are fairly ad-hoc, utilizing a wide range of seed-datasets, LLMs,\nprompts, filters, and task-specific generation strategies. Given this variety, direct compar-\nisons among synthetic data generation algorithms are scarce, making it difficult to under-\nstand where improvement comes from and what bottlenecks exist. To address this, we\npropose to evaluate algorithms via the makeup of synthetic data generated by each algo-\nrithm. In particular, we propose to examine the quality, diversity, and complexity (QDC)\nof resulting synthetic data. We choose these three data characteristics due to their signifi-\ncance in open-ended processes and the impact each has on the capabilities of downstream\nmodels. We find quality to be essential for in-distribution model generalization, diversity\nto be essential for out-of-distribution generalization, and complexity to be beneficial for\nboth. Further, we emphasize the existence of Quality-Diversity trade-offs in training data\nand the downstream effects on model performance. We then examine the effect of various\ncomponents in the synthetic data pipeline on each data characteristic. This examination\nallows us to taxonomize and compare synthetic data generation algorithms through the\ncomponents they utilize and the resulting effects on data QDC composition. This anal-\nsis extends into a discussion on the importance of balancing QDC in synthetic data for\nefficient reinforcement learning and self-improvement algorithms. Analogous to the QD\ntrade-offs in training data, often there exist trade-offs between model output quality and\noutput diversity which impact the composition of synthetic data. We observe that many\nmodels are currently evaluated and optimized only for output quality, thereby limiting\noutput diversity and the potential for self-improvement. We argue that balancing these\ntrade-offs is essential to the development of future self-improvement algorithms and high-\nlight a number of works making progress in this direction.", "sections": [{"title": "INTRODUCTION", "content": "Synthetic data generation has emerged as a promising approach to enhance the capabilities of large language\nmodels beyond traditional supervised fine-tuning datasets. This development has led to the creation of\na diverse set of synthetic data generation algorithms for a variety of tasks and domains. The majority\nof these algorithms follow a two-step process: First, leverage existing large language models to gather a\nlarge set of task prompts and sample continuations. Second, filter the generated dataset to eliminate \"low-\nquality\" samples. Their main goal is to maximize the \"quality\" and quantity of synthetically generated data.\nRelatively little effort is spent seeking to carefully understand what intrinsic characteristics of the data most\nimpact downstream generalization. While these algorithms are a natural place to start, this type of approach\nis inefficient, leading to most synthetically generated data being discarded (Zhou et al., 2023a).\nThis survey aims to clarify the impact of synthetic data generation on downstream model generalization\nby analyzing three key data characteristics: quality, diversity, and complexity. Informally, quality mea-\nsures the \u201cnoisiness\u201d, \u201ccorrectness\u201d, or \u201calignment\u201d of data with a desired target distribution Q. Diversity\nmeasures the \"self-similarity\u201d or \u201ccoverage\" of data. Complexity intuitively captures some notion of the\n\"difficulty\u201d or \u201ccompositionality\" of data. We choose these characteristics for their importance so far in\nassessing and building artificial open-ended systems, an emerging paradigm that can be applied to iterative\nself-improvement of models (Hughes et al., 2024). The field of Quality-Diversity has established quality and\ndiversity measures as effective proxies in encouraging increasingly novel and interesting/learnable/valuable\nsynthetic artifacts, often of increasing complexity, and synthetic data generation is natural application of this\nframework (Pugh et al., 2016; Cully & Demiris, 2017; Chatzilygeroudis et al., 2021). The importance of\ndata quality, diversity, and complexity is also reflected in many prominent synthetic data generation methods,\nwhich explicitly or implicitly aim to maximize at least one of the above (though rarely all three together)\n(Xu et al., 2023; Gunasekar et al., 2023; Wang et al., 2023c).\nThrough this quality-diversity-complexity (QDC) lens we investigate three closely related research ques-\ntions:\n\u2022 RQ1: How should quality, diversity, and complexity be defined? How are these quantities measured\nin LLM literature?\n\u2022 RQ2: How do quality, diversity, and complexity in training data impact model generalization?\n\u2022 RQ3: How do existing synthetic data generation algorithms promote quality, diversity, and com-\nplexity?\nThe answers to these questions can enable the design of more sample-efficient synthetic data generation\nalgorithms with better model generalization and self-improvement abilities.\nIn Section 2 we investigate RQ1. We begin by providing abstract, high-level definitions of quality, diversity\nand complexity in data. Informally, each characteristic is fairly intuitive: quality measures the \u201cnoisiness\u201d\nor \"correctness\u201d of data, diversity measures the \u201ccoverage\" and \"self-similarity\" of data, and complexity\nmeasures the \"difficulty\u201d or \u201ccompositionality\" of data. Yet, despite these intuitive informal definitions,\nmany different practical measures for each characteristic exist in the literature, and these practical measures\nvary in their utility. Some are generally applicable, others domain-specific. Some correlate with downstream\nmetrics of interest, while others do not (depending on the task).\nArmed with a better understanding of how data quality, diversity, and complexity are measured in practice,\nin Section 3 we survey the effects of each characteristic on model performance. We come away with three\nkey takeaways in answer to RQ2:\n\u2022 Data quality is essential for in-distribution generalization.\n\u2022 Data diversity is essential for out-of-distribution (OOD) generalization."}, {"title": "Defining Data Quality, Diversity, and Complexity", "content": "Suppose we have some sample space \u03a9 = X \u00d7 Y where each w = (x, y) \u2208 \u03a9 is an input-output sample\npair. Further suppose we have a set of tasks T1, ..., Tk defined as probability measures on \u03a9. Finally, suppose\nwe are given a large n-sample training dataset D \u2208 \u03a9\" and a model Me. Note that D need not necessarily\nbe sampled depending on the tasks T1, ..., Tk. Given some objective/loss l, it is often of interest to find\ncharacteristics of D which can be used to predict the downstream performance of Me on tasks T1, ..., Tk. In\nthis survey we are interested in understanding the impact of three intuitive characteristics: dataset quality\nQ(D), dataset diversity D(D), and dataset complexity C(D).\nDespite being intuitive, widely used terms in ML, defining exactly what is meant by dataset quality, diver-\nsity, and complexity can be a surprisingly difficult task. Numerous implementations of proxy measures exist\nwhich attempt to capture our intuitive notions of these characteristics. To further complicate matters, differ-\nent measures correlate better with downstream metrics of interest (such as model performance) in different\nsettings. This makes choosing a single definitive measure practically impossible. Instead, we attempt to de-\nfine abstract notions of quality, diversity, and complexity that line up with our intuitions as best as possible."}, {"title": "DEFINING DATASET QUALITY", "content": "Fix a target task 7 as a probability distribution on the sample space \u03a9. Informally, we say the quality Q(D)\nof dataset D aims to measure the \"noisiness\" or \"correctness\" of samples in D with respect to T. High-\nquality datasets with respect to 7 should be entirely contained inside the support of T in the sample space\n\u03a9. Low-quality datasets will contain many samples far out of distribution of 7. Often, quality measures are\ndefined at the sample level Q\u2229 : \u03a9 \u2192 R. For example, the \"quality\" of a piece of code could be determined\nby how many unit tests it passes. In these common cases we could regard the corresponding task distribution\nT which concentrates on the set of code samples passing all unit tests. A quality measure for the entire\ndataset can then be extracted by averaging the sample level quality: Q(D) = = \u03a3wed Qn(\u03c9).\nImplementations of Q can be categorized into four groups: ground truth measures, neural measures, attribute\nmeasures, and hybrid / other.\nGround truth reference measures: Most often the data quality of a sample w is measured by comparing\nto a corresponding ground truth sample w*. The sample level quality Qn(w) is then the similarity of w to w*.\nIn domains where there is an intended final answer A*, e.g., math (Yu et al., 2024; Toshniwal et al., 2024;\ncoding (HumanEval, Pourcel et al. (2024)), and other reasoning settings, the quality of a\nsample w can be easily measured as\nQ(\u03c9) = {1 A\u03c9 = A\u2217\n  {otherwise."}, {"title": null, "content": "However, this comes with the obvious drawback of ignoring any intermediate steps in the chain of thought\nused to produce the final answer Aw. Further, more complex, open-ended tasks such as theorem proving\nand creative writing do not have a final answer. In these cases, another measure must be used to assess\nquality. When a ground truth sample w* is available, lexical measures of similarity to w*, such as Rouge or\nBleu/SacreBleu (Samvelyan et al., 2024b) can be used. While these measures are popular for less complex\ntasks such as paragraph-level summarization (Stiennon et al., 2022), lexical similarity is often insufficient\nfor evaluating correctness on more complex tasks such as instruction following (Liu et al., 2024b). In such\ncases, alternative metrics have been proposed. MAUVE (Pillutla et al., 2021; Ye et al., 2022a) proposes\nto approximate the area under the divergence curve between the given data and a reference dataset. Other\nworks simply define quality as performance on a downstream benchmark: Zhou et al. (2023a); Viswanathan\net al. (2023); Gandhi et al. (2024a).\nNeural measures: When training data is available, reward models can be trained and generalize to unseen\nsamples. The simplest example of a reward model is a classifier trained using labeled data (e.g., a toxicity\nclassifier (Chakrabarty, 2019)). A number of works have trained binary classification models using data that\nis known to be high-quality as the positive class examples, and unfiltered data as the negative class examples\n(Brown et al., 2020b; Gao et al., 2020; Du et al., 2022; Xie et al., 2023; Li et al., 2024b). In domains such as\nreasoning, specialized reward models such as Outcome-based reward models (ORMs) (Uesato et al., 2022)\nare used, which are trained with a classification objective at the step level supervised by the the final answer\n(Pang et al., 2024; Tian et al., 2024; Havrilla et al., 2024b). The resulting reward model outperforms final\nanswer classifiers and (somewhat) generalizes to intermediate steps. Process-based reward models (PRMs)\n(Lightman et al., 2023) further improve on ORMs by training a step-level classifier with supervised labels at\neach step. The PRM can then be used as a quality measure of each step of a solution w in addition to the full\ntrace. However, collecting human annotations for training PRMs is expensive. Recent work (Havrilla et al.,\n2024b; Wang et al., 2024d) has investigated automating this process with synthetic data. Ye et al. (2024b)\nuse synthetic natural language critiques generated by LLMs to provide additional feedback and then train a\nreward model that predicts a scalar reward on top of them.\nA related line of work trains so called Bradley-Terry reward models contrastively on ordered preference\ndata of the form (w-,w+) where w+ is the accepted sample and w_ is the rejected sample (Ziegler et al.,\n2020; Ouyang et al., 2022; Bai et al., 2022a; Kirk et al., 2024; Bukharin & Zhao, 2024; Havrilla et al.,\n2023; Bai et al., 2022a; Dubey et al., 2024b). Classifiers can also be used in this setting by conditioning\non a ground truth reference, as in the Stanford Human Preferences Dataset (Ethayarajh et al., 2022). A\nquickly growing line of work (Ankner et al., 2024; Zhang et al., 2024c; Mahan et al., 2024) trains reward\nmodels with the ubiquitous next-token prediction objective instead of a discriminative objective to generate\ncritiques or explanations before providing a quality score. Such approaches appear to generalize further\nout of distribution than their purely discriminative counterparts due to their CoT reasoning capability and\nability to effectively utilize more inference-time compute. Also related is the zero-shot or few-shot use of\nLLMs-as-a-judge (Zheng et al., 2023) to perform in-context preference selection over multiple candidates.\nAttribute measures: When ground truth data is not available to assess data quality another option is to\nrely on data annotators to assess sample attributes relevant to quality. Abstractly, a sample attribute is a\ngeneric function T : \u03a9 \u2192 [0, 1] measuring some semantic property of the sample. T(w) = 1 signifies that w\nhas the semantic property, T(w) = 0 signifies that w does not have the semantic property, and 0 < T(\u03c9) < 1\nsignifies that w somewhat has the property. A quality score Q for a sample can then be recovered via some\ncombination f of the attributes Q(w) = f (T\u2081(w), ..., \u03a4k(\u03c9)).\nEach attribute function T is often implemented as a discrete Likert score (Likert, 1932) over the integers\n1-N (which can then be normalized into the range [0, 1]). For example, Bai et al. (2022a) recruit annotators\nto measure helpfulness, harmlessness, and honesty of AI assistant responses on a scale of 1-5, and Stiennon\net al. (2022) recruit annotators to measure the coverage, clarity, and fidelity of a summary. LLMs themselves\nare increasingly being used as annotators to judge how well a sample adheres to a written constitution (Bai"}, {"title": "DEFINING DATASET DIVERSITY", "content": "A diversity measure D(D) aims to capture the \u201cself-similarity\u201d and \u201ccoverage\u201d of D over the entire sample\nspace \u03a9. A highly diverse dataset should be uniformly distributed on \u03a9. This motivates us to consider\nformally defining the diversity of D as the distance between D and the uniform measure on \u03a9 under some\nprobability metric. Importantly, this means the diversity of D is not necessarily dependent on a target\ntask T. However, in some settings where we have a good prior over the sample space \u03a9 it may make more\nsense to adapt our notions of diversity to be uniform with respect to the prior instead of uniform over the\nentire sample space. For example, in the language setting a more sensible uniform prior may be the uniform\ndistribution over all natural language sentences instead of uniform over all possible strings up to a fixed finite\nlength.\nUnlike quality, it is difficult to define diversity at the sample level. We categorize practical diversity measures\ninto four groups: lexical, attribute, embedding, and miscellaneous as summarized in Figure 3.\nGround truth measures: Lexical similarity measures can be used to capture a relatively simple type\nof diversity in text data by comparing n-grams between source and target datasets. Scores like ROUGE\n(Lin, 2004; Gandhi et al., 2024a), Bleu (Papineni et al., 2002a; Samvelyan et al., 2024b; Ye et al., 2022b;\nZhu et al., 2018; O'Mahony et al., 2024), and INGF (Yu et al., 2023b; Kirk et al., 2024) measure pairwise\nsimilarity between samples in a dataset D by pairing n-gram overlap between samples. A diversity measure\nfor D is then assigned by averaging all pairwise similarity measures. We refer to these kinds of diversity\nmeasures as similarity measures. Dataset vocabulary size, i.e., the number of unique words in a dataset, can\nalso be used as a simple diversity measure which does not rely on pairwise lexical comparisons between\nsamples (Yu et al., 2023b). We refer these types of diversity measures as coverage measures. Measure\nof Textual Lexicial Diversity (MTLD) (McCarthy & Jarvis, 2009; Cao et al., 2024) is another measure of\nlexical diversity which analyzes the average number of words in a row that maintain a certain Type-Token\nRatio (TTR). The \u201cdistinct\u201d metric (Li et al., 2016a) measures diversity of a collection of texts as the ratio\nof distinct tokens to the total number of tokens in the text. The expectation adjusted distinct metrict (EAD)\nadditionally normalizes by the expected number of distinct tokens under some prior.\nLexical diversity measures are general-purpose, requiring no special domain knowledge for application.\nHowever, for the same reason they often do not capture more relevant/interesting notions of diversity in\ndomain specific settings: e.g., the type of skill used to solve a programming problem (Pourcel et al., 2024).\nAttribute diversity: Similar to attribute-based measures of quality, attribute-based measures of diversity\nalso require domain-specific knowledge to define attribute functions T\u2081 : \u03a9 \u2192 [0, 1]. Each attribute function\nT\u2081 measures where a given sample w falls in the semantic attribute T\u2081. The collection of attribute measures\n\u03a4(\u03c9) = (\u03a4\u2081(\u03c9), ..., \u03a4k(w)) defines an attribute description for each sample w. Concretely, the attributes\ncomputed for a dataset vary from task to task. Many instruction tuning papers (Wang et al., 2022; Li et al.,\n2024d; Zhang et al., 2024a; Lu et al., 2023b) categorize instruction by task type (summarization, question\nanswering, translation, etc.) to measure instruction diversity. Some papers (Zhou et al., 2023a; Wang et al.,\n2022) also annotate the topic, e.g., politics, health, sports, etc., of each instruction. Wang et al. (2022)\nadditionally classifies instructions by language to measure instruction diversity along three axes: type, topic,\nand language. Other works describe attributes using behavioral descriptors. Samvelyan et al. (2024b)\nclassifies red-teaming prompts by what constitutional rules are violated. Fontaine et al. (2021b) counts the\nnumber of attributes contained in a description of a tile-based Mario level. Pourcel et al. (2024) categorizes\nsolutions to programming puzzles by tracking what skills are used in the solution (e.g., arrays, graphs,\ndynamic programming, etc.). Tian et al. (2024); Havrilla et al. (2024a) identify solutions to math problems\nby their orders of operations. Gandhi et al. (2024a) measures the diversity of learned search algorithms\nplaying the Game of 24 by the number of unique states they visit while finding a solution.\nThe main difficulty with measuring attribute-based diversity is in accurately measuring the desired attributes\nof an unstructured text sample w. Implementations of these measures fall largely into two approaches: (1)\nrelying on human annotators or (2) using LLMs as judges to automatically annotate samples. Earlier works\n(Zhou et al., 2023a; Wang et al., 2022) utilize attribute labels coming exclusively from human annotators.\nThe marked improvement in LLM capability made automatic annotation of attributes only recently possible.\nSeveral works have already experimented with this approach (Samvelyan et al., 2024b; Pourcel et al., 2024;"}, {"title": "DEFINING DATASET COMPLEXITY", "content": "Complexity is a third essential characteristic of data that intuitively measures some notion of the \"difficulty\"\nor \"compositionality\u201d of a sample. While less commonly considered than quality and diversity, improving\noutput complexity is essential for any algorithm for recursive self-improvement through synthetic data. This\nis because the output complexity of a model reflects its capability to compose distinct concepts across do-\nmains. A model with high-quality and highly-diverse output may reliably recall the data it was trained on\nacross a variety of tasks. However, if the model has low output complexity, it may completely fail to learn\nhow to combine pieces of knowledge it has already learned. Learning how to do this type of composition is\nessential for intelligence.\nDespite its intuitiveness, complexity is a notoriously difficult concept to formalize (Mitchell, 2009). Kol-\nmogorov complexity (Li & Vit\u00e1nyi, 1997) proposes to measure complexity of a sample as the length of\nthe shortest program generating the sample w with respect to a fixed programming language. Logical depth\n(Bennett, 1988) is another notion of complexity which instead measures the minimum amount of time needed\nto compute the sample w when minimizing over all programs in a fixed programming language. While such\nnotions are quite general, they are intractable to compute in common situations, necessitating practical alter-\nnatives. However, they do point to the common theme that most measures of complexity implicitly depend\non a fixed background set of primitives used to construct the sample w.\nTo help express the foundations of the nature of complexity that could be studied under different measures,\nwe give a more actionable definition for complexity. We define complexity C'(w) of a sample w \u2208 \u03a9 as\nits size under a fixed representation scheme. This also aims to align closely with the broader definition of\ncomplex systems as described in Mitchell (2009):\nA system in which large networks of components with no central control and simple rules\nof operation give rise to complex collective behavior, sophisticated information process-\ning, and adaptation via learning or evolution.\nThis notion of complex systems could be important for future developments in dataset creation: learning\nincreasingly complex knowledge is important for the development of more powerful models, and the evolu-\ntion of more complex data is important for models to be able to have more learning opportunities in the first\nplace.\nConcretely, we denote an n-samples complexity measure as function C : \u03a9\" \u2192 R, which intuitively\nmeasures data \u201cdifficulty\u201d. Similarly to quality, complexity can also be defined on a sample level as Co \u2192 R\""}, {"title": null, "content": "with C being recovered as the average over samples. We now survey commonly used measures of complexity\nin text data and summarize them in Figure 4.\nBasic measures of complexity: We start by reviewing some simple measures of textual data complexity.\nToken length (Liu et al., 2024b) is broadly applicable across all text but lacks any notions of semantic com-\nplexity. Sometimes human generated labels can also be used to form complexity hierarchies as in Hendryck's\nMATH which groups problems into increasing levels of difficulty (Hendrycks et al., 2021b). Some other\nworks measure the complexity of data as its compressibility (Pandey, 2024) or intrinsic dimension (Sharma\n& Kaplan, 2020; Havrilla & Liao, 2024).\nAttribute complexity: Similar to attribute based measures of quality and diversity, one can define an\nattribute function T, which measures for each sample its complexity in this case. One such attribute com-\nplexity metric is Tree instruct Zhao et al. (2024), which is defined as the number of nodes in the semantic\ntree of an instruction. The authors show empirically that for this specific complexity metric, the number of\nadded tree nodes in the prompt, with increasing complexity performance increases. Moreover using less,\nbut more complex data over performs more, but less complex data. In this case, it is shown that the better\nperformance does not come due to more tokens. Another complexity metric falling into this category is\nInsTag (Lu et al., 2023b), where T(w) = # semantic tags (x) assigned to sample w. Each set of tags is as-\nsigned using a prompted LLM.(Sharma et al., 2024) also propose to measure complexity based on the depth\nand structure of a parse tree. Other works directly prompt GPT-4 to score the complexity of samples on a\n1-5 scale (Chen et al., 2024b). EvolComplexity (Liu et al., 2024b) computes complexity scores by evolving\ninstruction responses in a manner similar to WizardLM (Luo et al., 2023b) and then applying GPT-4 as a\njudge to assess the complexity of each sequence of evolved samples in-context.\nOther model based measures: Model perplexity (Liu et al., 2024a) can also be regarded as a measure of\ncomplexity, with higher perplexity samples being more complex. Note however there is some overlap with\nusing perplexity as a measure of data quality. The Instruction Following Difficulty (IFD) score (Li et al.,\n2024c) applies perplexity to measuring complexity of instruction following samples as\nIFD(x, y) = s(xy)\ns(x) ,\nwhere s denotes the cross entropy\ns(x) =1\nN \u03a3N\ni=1 [\u2212log P(xi| x1, ..., Xi\u22121; \u03b8)],\nand @ denotes the weights of the pretrained base LLM. Albalak et al. (2023a) similarly measure complexity\nas the information gain of a sample.\nPsychometric measures: Psychometric approaches like item response theory (IRT)(Rasch, 1993; DeMars,\n2010), traditionally used to assess human cognitive abilities and the difficulty of questions, could be adapted\nto measure the difficulty or complexity of datasets for LLMs. IRT allows the difficulty of datasets to be\ncaptured through parameters in a probabilistic model aimed at explaining model performance by sampling\nfrom a diverse population of LLMs as respondents. This method could involve presenting samples from\nthe dataset to a variety of LLMs with different sizes and strengths. Measures such as the perplexity of data\npoints could then be analyzed using IRT (Thissen, 1982) to generate standardized difficulty scores for each\nsample. Such measures have been used to adaptively evaluate models with fewer examples (Polo et al., 2024;\nZhuang et al., 2023), and to compare model responses to a population of humans (He-Yueya et al., 2024)\nbut could be extended to measure the complexity of the training data. This would provide a nuanced\nmeasure tailored specifically to LLMs, which could be valuable for curating datasets of varying difficulty\nlevels for training and testing."}, {"title": "THE EFFECTS OF DATA QDC ON MODEL PERFORMANCE", "content": "In Section 2 we defined notions of quality, diversity, and complexity and surveyed numerous practical imple-\nmentations used in the literature. In this section we now seek to understand the effects of quality, diversity,\nand complexity in training data on model performance and generalization. Overall, we find that quality\ntends to most benefit in-distribution generalization, diversity most benefits OOD generalization, and appro-\npriate levels of complexity can benefit both. Additionally, we examine often occurring trade-offs between\nquality and diversity in training data. As a result, this often produces corresponding trade-offs between\nin-distribution and OOD generalization.\nA note on in-distribution vs. OOD generalization Let P be probability measure on task space (X, Y)\nand Dtrain = {(Xi, Yi)=1} ~ P a set of training data independently identically sampled (i.i.d) from P. In\nthis survey, when we say a model Mo has good in-distribution generalization, we mean that Me trained on\nDtrain generalizes well to test data {(Xj, Yj)=1} sampled from P. We say Me has good OOD generalization\nif it generalizes well to a related but new distribution Q on (X, Y). See Miller et al. (2021) for more\ndiscussion on in-distribution versus OOD generalization."}, {"title": "THE EFFECT OF QUALITY", "content": "Data quality on its own has been a huge topic of interest recently for both model pre-training and post-\ntraining (Albalak et al., 2024; Zhou et al., 2023a; Chen et al., 2024b; Cao et al., 2024; Sharma et al., 2024).\nFor this reason we break our discussion into two parts: one for pre-training and one for post-training.\nData quality in pre-training SOTA pre-training data pipelines include several iterative rounds of qual-\nity filtering (Albalak et al., 2024). The initial round applies of set of heuristic filters aimed at removing"}, {"title": "THE EFFECT OF DIVERSITY", "content": "Of equal importance to the quality of training data is its diversity during both pre-training and post-training.\nIn contrast to quality, post-training approaches often do not see a huge improvement in in-distribution per-\nformance with more diverse training data. Instead, diversity most benefits OOD performance.\nData diversity in pre-training Data diversity plays a crucial role in pre-training (Raffel et al., 2020;\nBrown et al., 2020a; Li et al., 2023b). Many pre-training methods assume that a post-training process will\noccur, thereby making the goal of pre-training for the model to see as much data as possible. However, sim-\nply training on all available data can lead to highly duplicated content, and training inefficiency, motivating\nsignificant efforts to improve data deduplication. For example, Wenzek et al. (2020) and Ortiz Su\u00e1rez et al.\n(2019) utilize hashing based deduplication methods when developing CCNet and OSCAR, respectively. Ad-\nditionally, Lee et al. (2022) propose the EXACTSUBSTR algorithm, which detects duplicates based on shared\nsubstrings. More recently, model-based deduplication has become popular, with SEMDEDUP (Abbas et al.,\n2023) and D4 (Tirumala et al., 2023) being common examples. Each of these methods utilize a neural net-\nwork to embed the data, followed by some clustering and a removal of data within each cluster according to\nsome similarity metric. In particular for pre-training where the downstream goals are not known, data dedu-\nplication reduces the risk of models overfitting to anything in particular, and has been shown to sometimes\nimprove accuracy on downstream tasks (Tirumala et al., 2023).\nAdditionally, a number of works have found that training on more diverse data can lead to improved per-\nformance (Lee et al., 2022; Tirumala et al., 2023). For example, Ye et al. (2024a) found that pretraining"}, {"title": "THE EFFECT OF COMPLEXITY", "content": "Finally, we consider the effect of data complexity on model generalization during both pre-training and\npost-training. Unlike quality and diversity, complexity appears to often benefit both in-distribution and\nOOD generalization. However, this is only true to a certain point, with excessively complex data instead\nharming generalization. For example, recent work by Kallini et al. (2024) demonstrate that while language\nmodels struggle with completely random patterns, they can learn unnatural but structured patterns, albeit\nless efficiently than natural ones. Hence an effective training data should maintain learnable structure while\nintroducing sufficient challenge.\nData complexity in pre-training Complexity measures can be used to filter pre-training datasets. Albalak\net al. (2023a) propose an approach that adapts data-mixing proportions in an online fashion by training a\nmulti-armed bandit algorithm on domain perplexity. The higher the perplexity, the more complex the data\nis for the model and the larger the information gained by learning from it. This approach achieves lower\ntext perplexity compared to DoReMi (Xie et al., 2024) and an unweighted baseline. However, it requires a\nrelatively clean dataset to ensure that high perplexity is not correlated with low-quality data. For instance,\nWenzek et al. (2019) train a language model on Wikipedia data to compute perplexity as a measure of\ndistance from high-quality text, removing documents that deviate too much.\nAlternative approaches focus on gradually increasing complexity during training. Dubey et al. (2024b)\nshow that using an annealing phase where the learning rate is lowered to zero while incorporating more\ncomplex domain data like code and mathematics enhances performance on key benchmarks. Others\n(Pandey, 2024; Sharma & Kaplan, 2020; Havrilla & Liao, 2024) investigate how data complexity impacts\npre-training, using measures like intrinsic dimension or gzip compression. They demonstrate that more\ncomplex data takes longer to learn.\nData complexity in fine-tuning Zhao et al. (2024) measures complexity of instruction following datasets\nby parsing instructions into a semantic tree. They find fine-tuning on more difficult instructions outperforms\nfine-tuning on more less-difficult instructions where the number of training tokens is equalized. INSTAG\nLu et al. (2023b) measures complexity as the number of attribute tags assigned to a sample by a classifier\nLLM. They find models fine-tuned on more complex instruction samples perform better on MT-Bench. The\nWizardLM model series (Xu et al., 2023; Luo et al., 2023b;a; Zeng et al., 2024) employ LLMs to auto-\nmatically evolve the complexity of seed data. They see significant improvement in models fine-tuned on\nthe resulting synthetic data. Li et al. (2024c) measures the complexity of instruction following data using\na novel Instruction Following Difficulty metric based on the ratio of the instruction's perplexity given the\nresponse to instruction's unconditional perplexity. They also find higher complexity datasets yield better\ninstruction following results. Liu et al. (2024b"}]}