{"title": "A Fair Post-Processing Method based on the MADD Metric for Predictive Student Models", "authors": ["M\u00e9lina Verger", "Chunyang Fan", "S\u00e9bastien Lall\u00e9", "Fran\u00e7ois Bouchet", "Vanda Luengo"], "abstract": "Predictive student models are increasingly used in learning environments. However, due to the rising social impact of their usage, it is now all the more important for these models to be both sufficiently accurate and fair in their predictions. To evaluate algorithmic fairness, a new metric has been developed in education, namely the Model Absolute Density Distance (MADD). This metric enables us to measure how different a predictive model behaves regarding two groups of students, in order to quantify its algorithmic unfairness. In this paper, we thus develop a post-processing method based on this metric, that aims at improving the fairness while preserving the accuracy of relevant predictive models' results. We experiment with our approach on the task of predicting student success in an online course, using both simulated and real-world educational data, and obtain successful results. Our source code and data are in open access at https://github.com/melinaverger/MADD.", "sections": [{"title": "1 Introduction", "content": "Due to their ability to enhance educational outcomes and support stakeholders in making informed decisions, predictive student models are increasingly used in learning environments [8]. However, due to the rising social impact of their usage, it is thus now all the more important for these models to not only be sufficiently accurate, but also fair in their predictions. Indeed, the quality of the predictions could influence the adoption of real-world interventions and possibly produce long-term implications for students [4].\nTo evaluate algorithmic fairness, which is broadly defined in [7], a new metric has been recently developed, namely the Model Absolute Density Distance (MADD) [9]. The interest of this metric is to measure how different a predictive model behaves regarding two groups, independently from its predictive performance, which will be further explained in part 2.1. However, precisely because it is independent from predictive performance, [9] recommend to use it for fairness evaluation on models that already demonstrate satisfying predictive performance to be used in real-world applications. Therefore, we would like to extend the use"}, {"title": "2 The Model Absolute Density Distance (MADD) metric", "content": "of this metric to unfairness mitigation as well, that is to say to improve the fairness of accurate but unfair results.\nThat is why, in this paper, we develop a fair post-processing method based on the MADD metric that improves the fairness of the results outputted by a model without compromising accuracy. It is important to note that this method improves the fairness of the results regarding a single chosen attribute only, since it is based on the MADD metric that evaluates fairness regarding a single attribute as well (see part 2.3). We then experiment with our approach on the task of predicting student success in an online course, using both simulated and real-world educational data, and for which our source code and data are in open access at https://github.com/melinaverger/MADD. Our results show that our method successfully improves the fairness without losing the accuracy of the results.\nThe remainder of this paper is organized as follows. We first describe the MADD metric in Section 2 in order to explain our MADD post-processing approach in Section 3. Then, we describe how we conducted our experiments in Section 4 and we report our results in Section 5. Finally, we discuss our approach in Section 6, and we conclude our paper in Section 7 with future work."}, {"title": "2.1 Motivation of its choice", "content": "Among the plethora of fairness metrics that have emerged from recent literature, we chose the MADD for our fair post-processing approach. The first reason is that it is a statistical metric. Indeed, fairness metrics mostly fall into three main classes, counterfactual (or causality-based), similarity-based (or individual), and statistical (or group) criteria [10,1], but so far the metrics from the first two classes are seldom used in practice [10]. Using the MADD ensures an easy integration for fairness improvement in existing applications.\nThe second reason is that, contrary to the other statistical metrics, the MADD is able to capture the severity of the predictive errors between the different groups of students in the data (further explained in part 2.3). Indeed, the principle of the existing metrics consists in making comparisons of any predictive performance of a model across groups (either by independence, separation or sufficiency [5]). However, two models producing any similar predictive error proportions across groups can still exhibit very different and possibly harmful errors themselves, which is not captured by these metrics. That is why, since the MADD is precisely not based on predictive performance (then a comparison) but on the intrinsic difference with which a models behaves regarding the groups, this metric is particularly relevant for improving fairness on this distinct aspect."}, {"title": "2.2 Preliminaries", "content": "To provide a framework for the whole paper, let consider a binary classifier C that aims at predicting student success at a course. C is trained on a dataset {X,Y}=1 with n the number of unique students, X the attributes characterizing the students, and Y the binary target variable whose values \\(y_i\\) can take 0 for failure and 1 for success. To apply the MADD, C should output both its predictions \u0176 and the predicted probability \\(p_i\\) associated to each prediction: C \u2192 {\\(\\hat{y}_i\\) = {0,1}, \\(p_i \\in [0,1]\\)}. In the rest of the paper, we will focus on the probability related to the success prediction for every student i, i.e. \\(p_i(\\hat{y}_i = 1)\\), but as it is also equal to 1 - \\(p_i(\\hat{y}_i = 0)\\), it is simpler to use the notation \\(p_i\\) indifferently.\nLet also consider one attribute of X, simply noted as a (instead of \\(x^{(a)}\\) conventionally), which will be our attribute of interest, that is to say the attribute regarding which we will evaluate algorithmic fairness. This attribute a should be binary, i.e. composed of two distinct groups of students, Go and G1. As an example, if a corresponds to having declared a disability, a student could not belong to the group of those who have not (e.g. Go) and the group of those who have (e.g. G1) declared a disability."}, {"title": "2.3 Definition", "content": "To introduce the metric, the Model Absolute Density Distance (MADD) [9] relies on the comparison between how a model C distributes its probabilities of success predictions \\(p_i\\) between the students of a group Go and those of a group G\u2081 from an attribute a. To do so, the calculation of the metric needs two one-dimensional vectors of the same length, noted \\(D^G_0\\), and \\(D^G_1\\) and called density vectors. They both contain the proportions of students of Go and G1 respectively who receive the same predicted probabilities \\(p_i\\) (see Figures la and 1b). These proportions are noted \\(d^G_{0,k}\\) and \\(d^G_{1,k}\\) with k the index of the distinct value of \\(p_i\\). The MADD is thus defined as follows [9]:\n$$\nMADD (D^G_0, D^G_1) = \\sum_{k=0}^{m}|d^G_{0,k} - d^G_{1,k}| \\in [0,2]\n$$\nTo better understand what the MADD represents, a visual approximation is given in Figure 1c through the red zone, i.e. the zone where the two continuous density estimates (represented by continuous curves) of discrete \\(d^G_{0,k}\\) and \\(d^G_{1,k}\\) do not intersect. We highlight one advantage of the MADD that is to be bounded, making it objective and comparable for any data-models applications. Indeed, it is bounded between 0 and 2, and the closer the MADD is to 0, the fairer the outcome of the model is (regarding the two groups). More precisely, in the case where the MADD is equal to 0, the model produces the same probability outcomes for both groups so that \\(D^G_0 = D^G_1\\) and MADD(\\(D^G_0\\), \\(D^G_0\\)) = 0. Conversely, in the most unfair case, where the model produces totally distinct probability outcomes for both groups, the MADD is equal to 2 because we directly sum over the total proportion of both groups, that is to say 1 and 1. This"}, {"title": "3 The MADD Post-Processing Approach", "content": "will give some intuition about how to improve fairness based on the MADD in the next section."}, {"title": "3.1 Purpose", "content": "As introduced in the previous section, the closer the MADD is to 0, the fairer the outcome of the model is (w.r.t to attribute a), since the distributions of predicted probabilities are no longer distinguishable between the two groups Go and G1. Thus, to illustrate how a post-processing with the MADD would work, let consider that a model tends to give higher predicted probabilities (i.e. probabilities of success predictions) to a group, e.g. Go, than to the other, as shown in Figure 2a. Therefore, the goal of the MADD post-processing is to reduce the"}, {"title": "3.2 Approach", "content": "Following up on the previous part, a question can be raised: where should the two distributions coincide? Indeed, should the distribution of G\u2081 move to the one of Go or is there a better location between the two? To solve this issue, let first note as D the distribution related to all students, composed of students from both groups Go and G\u2081 (see the black histogram in Figure 3a). In machine learning, the goal for a model is to approximate the \"true\" relationship (or prediction function X \u2192 Y) between the attributes X in input and the target variable Y in output. As a consequence, we assume that a model that shows satisfying predictive performance outputs a discrete distribution D which should be really close to D, its \"true\" distribution (see the black line in Figure 3a). Therefore, assuming having such a model, our goal is to make the distributions \\(D^G_0\\) and \\(D^G_1\\) coincide at the place of D, which should best approximate D (see proofs in Appendix A). Indeed, this allows both to reduce the gaps between the two groups hence improve fairness and to prevent a loss in predictive performance. Therefore, the MADD post-processing is based on the following theoretical considerations.\nSince D, \\(D^G_0\\) and \\(D^G_1\\) correspond to histograms, then they can be mathematically considered as discrete estimators of the \"true\" probability density functions (PDFs) they describe (see Figure 3b) [3], noted as f, \\(f^{(G_0)}\\) and \\(f^{(G_1)}\\) respectively in the following. We thus want \\(f^{(G_0)}\\) and \\(f^{(G_1)}\\) to move towards the target f, as the intuition was given in the previous paragraph, but in a linear way because we want to keep the proportionality of their relative distance (see Figure 3c), otherwise it will improve fairness more for one group than for the other. We can now define the new theoretical PDFs f, \\(f^{(G_0)}\\) and \\(f^{(G_1)}\\) that we will estimate after the post-processing, by introducing a \u03bb parameter, that we call fairness coefficient of distributions convergence, such that:\n$$\n\\bar{f^{(G_0)}} = (1 - \\lambda) f^{(G_0)} + \\lambda f\n$$\n$$\n\\bar{f^{(G_1)}} = (1 - \\lambda) f^{(G_1)} + \\lambda f\n$$\nA can be seen as a distance ratio (see Figure 3c) so that \\(\\lambda\\in [0,1]\\), with \u03bb = 0 when the PDFs of Go and G\u2081 are at their initial state and \u03bb = 1 when they both coincide. A between 0 and 1 means that the distributions are getting closer (see discrete examples of distribution convergence in Figure 5). The challenge is to find the highest A possible that best improves the fairness without affecting the accuracy of the results. However, in practice, as we do not know the true f, \\(f^{(G_0)}\\) and \\(f^{(G_1)}\\), we cannot directly compute f, \\(f^{(G_0)}\\) and \\(f^{(G_1)}\\) as written in equations 2 and 3 with different values of A. That is why we introduce fip in the next part."}, {"title": "3.3 Implementation", "content": "We will generate a mapping function\u00b9, fairness_improved_prediction or in short fip, between the discrete estimates of f, \\(f^{(G_0)}\\), \\(f^{(G_1)}\\) (i.e. D, \\(D^G_0\\), \\(D^G_1\\)) and the discrete estimates of f, \\(f^{(G_0)}\\), \\(f^{(G_1)}\\) that we will note as \\(\\bar{D}\\), \\(\\bar{D^{G_0}}\\), \\(\\bar{D^{G_1}}\\). The purpose of fip is more precisely to take as inputs the \\(p_i\\) available at the output of a trained model and a value of A, and to output the new fairer predicted probabilities that we note as \\(p_i^{(\\lambda)}\\) (fip: (\\(p_i\\), \\(\\lambda\\)) \u2194 \\(p_i^{(\\lambda)}\\)). Consequently, \\(p_i^{(\\lambda)}\\) will allow to reconstruct the new \\(\\bar{D^{G_0}}\\), and \\(\\bar{D^{G_1}}\\), as shown in Figure 2b.\nfip will be generated as follows. Let focus on the group Go first. As we want the proportions of students having the same predicted probabilities to be kept even if the predicted probabilities values are changing with the post-processing, we will seek to make the cumulative density function (CDF) of the initial \\(p_i\\) of group Go being equal to the CDF of the new \\(p_i^{(\\lambda)}\\) of group Go. Thus, it comes that:\n$$CDF_{(G_0)}(p_i(G_0)) = CDF_{\\bar{(G_0)}}(\\bar{p_i^{(G_0)}})$$\n$$\n\\bar{p_i^{(G_0)}} = CDF_{\\bar{(G_0)}}^{-1}(CDF_{\\bar{(G_0)}}(p_i(G_0)))\n$$\nwhere \\(CDF_{\\bar{(G_0)}}(C) = (1-\\lambda) CDF_{(G_0)} +\\lambda CDF\\), and \\(CDF_{\\bar{(G_0)}}^{-1}\\) is the general inverse function of \\(CDF_{(G_0)}\\). We will have the same equations for the group G\u2081. In the end, what we do is to compute the different CDFs and CDFs thanks to interp1d and cumtrapz Python functions from scipy library that estimate their \"true\" equivalents based on the discrete values of \\(p_i\\) we have access to, which gives us the core of our fip mapping function. Now we have the ability to compute the \\(p_i^{(\\lambda)}\\), let define an objective function based both on the accuracy and the fairness of the new fairer predicted probability results which depend on A, to evaluate the outcome of our MADD post-processing method."}, {"title": "3.4 Objective Function", "content": "Similarly to existing balancing methods between accuracy and penalty values, we define the objective function as follows:\n$$\nL = (1 - \\theta) AccuracyLoss(\\lambda) + \\theta FairnessLoss(\\lambda)\n$$\nwhere \\(\\theta\\in [0, 1]\\) represents the importance of the accuracy and the fairness in the objective function. Indeed, a larger \\(\\theta\\) puts more emphasis on fairness, while a smaller \\(\\theta\\) favors accuracy. The value of \\(\\theta\\) could be set by an expert depending of what one wants to put more emphasis on, or experimentally determined like what we do with A in part 5.1 for instance. The AccuracyLoss(\\(\\lambda\\)), compatible with any common loss functions l (e.g. binary cross-entropy loss), and the FairnessLoss(\\(\\lambda\\)) could be defined as follows:\n$$\nAccuracyLoss(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} l(\\bar{p_i^{(\\lambda)}}, y_i)\n$$\n$$\nFairnessLoss(\\lambda) = MADD (\\bar{D^G_0}, \\bar{D^G_1})\n$$\nSince the two losses may vary across different scales of values, one should pay particular attention to the choice of l and the way of rescaling both losses to balance them effectively. We will show an example in part 4.2."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Process", "content": "Our MADD post-processing method, illustrated in Figure 4, can be applied for a fixed \\(\\theta\\) as follows. Let us have a training, a validation and a test sets. We first train a classifier. Then, we use this trained model on the validation set to output the predictions \\(\\hat{y}_{i,validation}\\) and predicted probabilities \\(p_{i,validation}\\). Next, we apply our fip mapping function with various values of A to obtain different corresponding \\(\\bar{p_i^{(\\lambda)}}_{,validation}\\). We will thus deduce the new \\(\\bar{y_i^{(\\lambda)}}_{,validation}\\) thanks to the classification threshold t. Now, with the new \\(\\bar{p_i^{(\\lambda)}}_{,validation}\\), \\(\\bar{y_i^{(\\lambda)}}_{,validation}\\) and the true labels \\(y_{i,validation}\\), we can plot the results of our objective function depending on the As to find the optimal \\(\\lambda^*\\) that will best improve the results of the classifier. Finally, we evaluate the accuracy and the fairness of the results with the chosen \\(\\lambda^*\\) on the test set (i.e. with \\(\\bar{p_i^{(\\lambda^*)}}_{,test}\\), \\(\\bar{y_i^{(\\lambda^*)}}_{,test}\\) and the true labels \\(y_{i,test}\\)). For the sake of simplification, in the experiments we omit training, validation and test subscripts from the notations but they will be easily deduced from the context."}, {"title": "4.2 Rescaled Objective Function", "content": "$$\nAccuracy Lossexp(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} l(\\bar{p_i^{(\\lambda)}}, y_i)\n$$\n$$\nFairness Lossexp(\\lambda) = \\frac{1}{2} MADD (\\bar{D^G_0}, \\bar{D^G_1})\n$$\nFor our experiments, we use the objective function \\(L_{exp}\\) composed of the rescaled terms we define as above (Equations 92 and 10). Both losses have thus a range of [0, 100%]. Indeed, the AccuracyLossexp(\\(\\lambda\\)) is the percentage of incorrect pre-dictions, and the FairnessLossexp(\\(\\lambda\\)) now represents a percentage of dissimilarity between the two distributions. Therefore, the resulting objective function \\(L_{exp}\\) is a weighted average of these two losses based on their importance. However, as a case study, we choose to give in all our experiments the same importance both to the accuracy and the fairness in the post-processing and we fix \\(\\theta\\) = 0.5. Additionally, it is important to note that in the case of this AccuracyLossexp(\\(\\lambda\\)), it exactly corresponds to 1 minus the standard accuracy score, which we will exploit in our results in section 5. Our goal will be to experimentally find the optimal parameter \\(\\lambda^*\\) that minimizes this objective function \\(L_{exp}\\), with \\(\\theta\\) = 0.5."}, {"title": "4.3 Simulated Data", "content": "To demonstrate the validity of our approach, we first experiment our MADD post-processing method on simulated data of \\(p_i\\) for which we know the real distributions. Simulated \\(p_i^{(G_0)}\\) and \\(p_i^{(G_1)}\\) are thus the predicted probabilities that we would have obtained at the output of a classifier. Let \\(p_i^{(G_0)}\\) and \\(p_i^{(G_1)}\\) be represented by some respective PDFs \\(f^{(G_0)}\\) and \\(f^{(G_1)}\\). \\(f^{(G_0)}\\) and \\(f^{(G_1)}\\) are respectively parts of the gamma distribution \u0393(4, 1) and the normal distribution N(0.55, 1), properly scaled along the x-axis and normalized within the interval [0, 1]:\n$$\nf^{(G_0)}(x) := \\frac{1}{C_0} f_{\\Gamma(4,1)} (11x)1_{[0,1]}(x)\n$$\n$$\nf^{(G_1)}(x) := \\frac{1}{C_1} f_{\\mathcal{N}(0.55,1)} (10x)1_{[0,1]}(x)\n$$\nBased on the above PDFs, we generate 10,000 samples of \\(p_i^{(G_0)}\\) and 10,000 samples of \\(p_i^{(G_1)}\\), whose density vectors \\(D^G_0\\) and \\(D^G_1\\) are displayed in Figure 6a. Moreover, to simulate the true label \\(y_i\\) for each student i, we arbitrarily choose to pass the simulated \\(p_i\\) value as a parameter of a Bernoulli law: Bernoulli(\\(p_i\\)) \u2208 {0,1}. This will enable us to simulate how a classifier would perform before the post-processing, to compare its results with those obtained after the post-processing. The latter are thus deduced from the classification threshold parameter t that we also arbitrarily set to 0.5 in this paper, and we refer the reader to the footnote 2."}, {"title": "4.4 Real-world educational data", "content": "As a second testbed for our approach, we use real-world educational data. This data, also used in [9] for fairness evaluation with the MADD, come from the Open University Learning Analytics Dataset (OULAD) [6] corresponding to courses offered by The Open University, a distance learning university from the United Kingdom, between 2013 and 2014. The attributes we use to predict whether a student will pass or fail a course are displayed in Table 1. The sum_click attribute was the only one that was not immediately available as is, and we computed it from inner joints and aggregation on the original data. Moreover, we primarily decide in this paper to limit the data to a specific course, tagged \"BBB\" in [6] and composed of 4,740 unique students, because it demonstrates high correlation and high imbalance w.r.t. the gender, which makes it a good candidate to analyze and improve possible algorithmic unfairness regarding this attribute. We learn a logistic classifier on this data, following a 70-15-15% split ratio between the training, validation and test sets."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Simulated Data", "content": "We generate 1,000 values of A with a constant step in its interval [0,1], and we compute all the corresponding \\(\\bar{p_i^{(\\lambda)}}\\), to obtain the relationships between the next three \\(L_{exp}\\), AccuracyLossexp(\\(\\lambda\\)), FairnessLossexp(\\(\\lambda\\)) and \u03bb. In Figure 5, we present how the new predicted probabilities progress with some increasing values of A. In Figure 6c, we display for all values of A the evolution of \\(L_{exp}\\), AccuracyLossexp(\\(\\lambda\\)) and FairnessLossexp(\\(\\lambda\\)). We remind that we set \\(\\theta\\) = 0.5 as we decided to give an equal importance to both the accuracy and the fairness in the post-processing. As we can see (Fig. 6c), on the one hand, when A increases the accuracy loss increases too (while we want to minimize it), but only slightly (0.361 to 0.390 i.e. about +8%). On the other hand, the fairness loss, which corresponds to half of the MADD, significantly drops as what we look for (0.598 to its lowest at 0.063, i.e. about -90%). In addition, the objective function \\(L_{exp}\\) reaches its minimum value 0.226 at \\(\\lambda^*\\) = 0.970, almost 1. Therefore, if we accept to loose about 8% of accuracy (we can make this interpretation because of how we defined our AccuracyLossexp(\\(\\lambda\\))), then by choosing \\(\\lambda^*\\) = 0.970 we would"}, {"title": "5.2 OULAD Data", "content": "Again, we display in Figure 7c the evolution of \\(L_{exp}\\), AccuracyLossexp(\\(\\lambda\\)) and FairnessLossexp(\\(\\lambda\\)), for the values of A we generated in part 5.1. When A increases, the accuracy loss remains almost constant (0.332 to 0.336 i.e. about +1%), and the fairness loss significantly drops again (0.431 to its lowest at 0.158, i.e. about -63%). However, the Figure 7c shows a lot more variability than in the previous case study, which comes from the much lower number of samples in the validation set (about 700). This loss in precision makes more challeng-ing to find an optimal \\(\\lambda^*\\). Indeed, we see that the minimum of the objective"}, {"title": "6 Discussion", "content": "Both experiments on simulated data and real-world data show that a post-processing based on the MADD metric successfully improves fairness while pre-serving a reasonably constant accuracy. The strength of our approach lies pre-cisely on having found, for every possible value of A, i.e. the fairness coefficient of distributions convergence, how to compute the two new predicted probability distributions to make them fairer and to keep the accuracy preserved.\nIndeed, the slight change in accuracy can be explained as follows. Since the model predicts success (i.e. 1) for all students whose predicted probabilities ex-ceed the threshold t, then after the post-processing only those whose new pre-dicted probabilities pass the threshold t affect the accuracy (i.e. passing from 0 to 1 in their prediction). More precisely, we found that the accuracy loss is"}, {"title": "7 Conclusion", "content": "In this paper, we present a fair post-processing method based on the MADD metric. We apply this method to the task of predicting student success at the course level, with the code and data in open access at https://github.com/melinaverger/MADD. This post-processing method allows to improve the fairness of models' results deemed sufficiently accurate but unfair. It does not require to have access to the original data nor the trained model itself. Finally, experiment-ing with various values of \\(\\theta\\) is part of our future work, as well as extending our approach to the consideration of multiple attributes for a more global fairness improvement."}, {"title": "A Proofs of 3.2", "content": "We set C to be a random variable with probability density function D, repre-senting the predicted probability value of the output of the model C, and S to be a random variable subject to Bernoulli distribution, representing the value of the sensitive parameter a. Thus, \\(D^{G_0}\\) and \\(D^{G_1}\\) are the probability density functions of the conditional distributions C|S = 0 and C|S = 1, respectively. According to the law of total probability, we have:\nP (C \u2264 t) = P (C \u2264 t | S = 0) P (S = 0) + P (C \u2264 t | S = 1) P (S = 1)\nF(t) = \\(F^{G_0}\\) (t) P (S = 0) + \\(F^{G_1}\\) (t) P (S = 1)\nD(t) = \\(D^{G_0}\\) (t) P (S = 0) + \\(D^{G_1}\\) (t) P (S = 1)"}, {"title": "B Proof of 3.3", "content": "According to Inverse transform sampling [2], we have the following two theorems:\nTheorem 1. Let A be a distribution and \\(F_A\\) be the cumulative distribution function of that distribution. If X obeys the distribution A i.e. X ~ A, then \\(F_A(X)\\) ~ U[0,1], where U[0,1] is a uniform distribution over [0, 1]."}]}