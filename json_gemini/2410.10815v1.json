{"title": "DEPTH ANY VIDEO WITH SCALABLE SYNTHETIC DATA", "authors": ["Honghui Yang", "Di Huang", "Wei Yin", "Chunhua Shen", "Haifeng Liu", "Xiaofei He", "Binbin Lin", "Wanli Ouyang", "Tong He"], "abstract": "Video depth estimation has long been hindered by the scarcity of consistent and scalable ground truth data, leading to inconsistent and unreliable results. In this paper, we introduce Depth Any Video, a model that tackles the challenge through two key innovations. First, we develop a scalable synthetic data pipeline, capturing real-time video depth data from diverse synthetic environments, yielding 40,000 video clips of 5-second duration, each with precise depth annotations. Second, we leverage the powerful priors of generative video diffusion models to handle real-world videos effectively, integrating advanced techniques such as rotary position encoding and flow matching to further enhance flexibility and efficiency. Unlike previous models, which are limited to fixed-length video sequences, our approach introduces a novel mixed-duration training strategy that handles videos of varying lengths and performs robustly across different frame rates\u2014even on single frames. At inference, we propose a depth interpolation method that enables our model to infer high-resolution video depth across sequences of up to 150 frames. Our model outperforms all previous generative depth models in terms of spatial accuracy and temporal consistency.", "sections": [{"title": "1 INTRODUCTION", "content": "Depth estimation is a foundational problem in understanding the 3D structure of the real world. The ability to accurately perceive and represent depth in video sequences is crucial for a broad range of applications, including autonomous navigation (Borghi et al., 2017), augmented reality (Holynski & Kopf, 2018), and advanced video editing (Zhang et al., 2024; Peng et al., 2024). Although recent advancements in single-image depth estimation (Ke et al., 2024; Yang et al., 2024a; Fu et al., 2024; Ranftl et al., 2021) have led to significant improvements in spatial accuracy, ensuring temporal consistency across video frames remains a substantial challenge.\nA major bottleneck in existing video depth estimation (Wang et al., 2023; Shao et al., 2024) is the lack of diverse and large-scale video depth data that capture the complexity of real-world environments. Existing datasets (Geiger et al., 2012; Li et al., 2023; Karaev et al., 2023) are often limited in terms of scale, diversity, and scene variation, making it difficult for models to generalize effectively across different scenarios. From a hardware perspective, depth sensors like LiDAR, structured light systems, and time-of-flight cameras can provide accurate depth measurements but are often costly, limited in range or resolution, and struggle under specific lighting conditions or when dealing with reflective surfaces. Another common approach (Wang et al., 2023; Hu et al., 2024b) is to rely on unlabeled stereo video datasets (Alahari et al., 2013) and state-of-the-art stereo-matching methods (Jing et al., 2024; Xu et al., 2023); however, such methods are complex, computationally intensive, and often fail in areas with weak textures. These limitations hinder the development of robust models that can ensure both spatial precision and temporal consistency in dynamic scenes.\nTo tackle the challenge, in this paper, we propose a solution from two complementary perspectives: (1) constructing a large-scale synthetic video depth dataset and (2) designing a novel framework that leverages powerful visual priors of generative models to effectively handle various real-world videos.\nSynthetic Data: Modern video games offer highly realistic graphics and simulate diverse real-world scenarios. For example, car racing games realistically replicate driving environments, while open-world games simulate various complex scenes. Given that modern rendering pipelines often include depth buffers, it becomes possible to extract large-scale, highly accurate video depth data from synthetic environments, which is scalable and cost-effective. In light of this, we construct DA-V, a synthetic dataset comprising 40,000 video clips. DA-V captures a wide range of scenarios, covering various lighting conditions, dynamic camera movements, and intricate object interactions in both indoor and outdoor environments, providing the opportunity for models to generalize effectively to real-world environments.\nFramework: To complement the dataset, we propose a novel framework for video depth estimation that leverages the rich prior knowledge embedded in video generation models. Drawing from recent advancements (Ke et al., 2024), we build upon SVD (Blattmann et al., 2023a) and introduce two key innovations to enhance generalization and efficiency. First, a mixed-duration training strategy is introduced to simulate videos with varying frame rates and lengths by randomly dropping frames. To handle videos of different lengths, those with the same duration are grouped into the same batch, and batch sizes are adjusted accordingly, thus optimizing memory usage and improving training efficiency. Second, a depth interpolation module is proposed, generating intermediate frames conditioned on globally consistent depth estimates from key frames, allowing for high-resolution and coherent inference of long videos under limited computational constraints. Additionally, we refine the pipeline"}, {"title": "2 SYNTHETIC DATA WORKFLOW", "content": "Real-time Data Collection. To address the challenges of depth data, we collect a large-scale synthetic dataset comprising approximately 40,000 video clips. A significant portion of this dataset is derived from state-of-the-art synthetic engines, leveraging their ability to generate photorealistic environments with accurate depth information. We extract depth data from a diverse set of virtual environments, carefully selected to encompass a wide range of scene conditions. \nData Filtering. After collecting initial synthetic video data, occasional misalignments between the image and depth are observed. To filter these frames, we first employ a scene cut method to detect scene transitions based on significant color changes. Then, the depth model (detailed in Sec. 3.1), trained on a hand-picked subset of the data, is used to filter out splited video sequences with low depth metric scores. However, this straightforward approach can lead to excessive filtering of unseen data. Therefore, we further use a CLIP (Radford et al., 2021) model to compute semantic similarity between the actual and predicted depth, both colorized from a single channel to three. Finally, we uniformly sample 10 frames from each video segment. If both the median semantic and depth metric scores fall below predefined thresholds, the segment is removed."}, {"title": "3 GENERATIVE VIDEO DEPTH MODEL", "content": "In this section, we introduce Depth Any Video, a generative model designed for robust and consistent video depth estimation. The model builds upon prior video foundation models, framing video depth estimation as a conditional denoising process (Sec.3.1). A mixed-duration training strategy is then presented to improve model generalization and training efficiency (Sec.3.2). Finally, we extend the model to estimate high-resolution depth in long videos (Sec. 3.3)."}, {"title": "3.1 MODEL DESIGN", "content": "Our approach builds upon the video foundation model, Stable Video Diffusion (SVD) (Blattmann et al., 2023a), and reformulates monocular video depth estimation as a generative denoising process. The overall framework is illustrated in Figure 2. The training flow consists of a forward process that gradually corrupts the ground truth video depth $x_d$ by adding Gaussian noise $ \\epsilon \\sim N(0, I)$ and a reverse process that uses a denoising model $v_\\theta$, conditioned on the input video $x_c$, to remove the noise. Once $v_\\theta$ is trained, the inference flow begins with pure noise $ \\epsilon$ and progressively denoises it, moving towards a cleaner result with each step."}, {"title": "Latent Video Condition", "content": "Following prior latent diffusion models (Rombach et al., 2022; Esser et al., 2024), the generation process operates within the latent space of a pre-trained variational autoencoder (VAE), allowing the model to handle high-resolution input without sacrificing computational efficiency. Specifically, given a video depth $x_d$, we first apply a normalization as in Ke et al. (2024) to ensure that depth values fall primarily within the VAE's input range of $[-1,1]$:\n$x_d = (\\frac{x_d - d_2}{d_{98} - d_2} - 0.5) \\times 2$,\nwhere $d_2$ and $d_{98}$ represent the 2% and 98% percentiles of $x_d$, respectively. Then, the corresponding latent code is obtained using the encoder $E$: $z_d = E(x_d)$. From this latent code, the normalized video depth can then be recovered by the decoder $D$: $\\hat{x}_d = D(z_d)$. Unlike the recent advanced 3D VAE (Yang et al., 2024c; OpenAI, 2024), which compresses the input across both temporal and spatial dimensions into the latent code, we focus on compressing only the spatial dimension, as in Blattmann et al. (2023a). This is because temporal compression potentially causes motion blur artifacts when decoding latent depth codes, especially in videos with fast motion (detailed in Sec. 4.4).\nTo condition the denoiser $v_\\theta$ on the input video, we first transform the video $x_c$ into latent space as $z_c = E(x_c)$. Then, $z_c$ is concatenated with the latent depth code $z_d$ frame by frame to form the input for the denoiser. Unlike SVD, we remove the CLIP embedding condition and replace it with a zero embedding, as we find it has minimal impact on performance."}, {"title": "Conditional Flow Matching", "content": "To accelerate the denoising process, we replace the original EDM framework (Karras et al., 2022) in SVD with conditional flow matching (Lipman et al., 2023), which achieves satisfactory results in just 1 step, compared to the original 25 steps. Concretely, the data corruption in our framework is formulated as a linear interpolation between Gaussian noise $ \\epsilon \\sim N(0, I)$ and data $x \\sim p(x)$ along a straight line:\n$\\phi_t(x) = tx + (1 - t) \\epsilon$,\nwhere $\\phi_t(x)$ represents the corrupted data, with $t \\in [0, 1]$ as the time-dependent interpolation factor. This formulation implies a uniform transformation with constant velocity between data and noise. The corresponding time-dependent velocity field, moving from noise to data, is given by:\n$v_t(x) = x - \\epsilon$.\nThe velocity field $v_t : [0, 1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ defines an ordinary differential equation (ODE):\n$\\frac{d}{dt} \\phi_t(x) = v_t(\\phi_t(x)) dt$.\nBy solving this ODE from $t = 0$ to $t = 1$, we can transform noise into a data sample using the approximated velocity field $v_\\theta$. During training, the flow matching objective directly predicts the target velocity to generate the desired probability trajectory:\n$\\mathcal{L}_\\theta = \\mathbb{E}_{t} ||V_\\theta (\\phi_t (z_d), z_c, t) - v_t (z_d) ||^2$,\nwhere $z_d$ and $z_c$ represent the latent depth code and video code, respectively."}, {"title": "3.2 MIXED-DURATION TRAINING STRATEGY", "content": "Real-world applications often encounter data in various formats, including images and variable-length videos. To enhance the model's generalization across tasks like image and video depth estimation, we implement a mixed-duration training strategy to ensure robustness across various inputs. This strategy includes frame dropout augmentation, which preserves training efficiency when handling long video sequences, and a video packing technique that optimizes memory usage for variable-length videos, enabling our model to scale efficiently across different input formats.\nFrame Dropout. Directly training long-frame videos is computationally expensive, requiring substantial training time and GPU resources. Inspired by context extension techniques (Chen et al., 2024; 2023; Liu et al., 2024) in large language models, we propose frame dropout augmentation with rotary position encoding (RoPE) (Su et al., 2021) to enhance training efficiency while maintaining adaptability for long videos. Concretely, in each temporal transformer block of the 3D UNet used in SVD, we replace the original sinusoidal absolute position encoding for fixed-frame videos with RoPE to support variable frames. However, training on a short video with RoPE still struggles to generalize to longer ones with unlearned frame positions, as shown in Figure 2(a). To mitigate this, we retain the original frame position indices $i = [0,......, T - 1]$ of the long video with T frames, and randomly sample K frames with their original indices for training. This simple strategy helps the temporal layer generalize effectively across variable frame lengths.\nVideo Packing. To train videos of varying lengths, an intuitive way is to use only one sample per batch, as all data within a batch must maintain a consistent shape. However, this leads to inefficient memory usage for shorter videos. To solve this, we first group videos by similar resolution and crop them to a fixed size. For each batch, we then sample examples from the same group and apply the same frame dropout parameter K. The process is illustrated in Figure 2(b). In particular, we increase the batch size for small-resolution and short-duration videos to improve training efficiency."}, {"title": "3.3 LONG VIDEO INFERENCE", "content": "Using the trained model, we can process up to 32 frames at 960 \u00d7 540 resolution in one forward pass on a single 80GB A100 GPU. To handle longer high-resolution videos, Wang et al. (2023) applies a sliding window to process short segments independently and concatenate the results. However, this would lead to temporal inconsistencies and flickering artifacts between windows. Thus, we first predict consistent key frames, and then each window generates intermediate frames using a frame interpolation network conditioned on these key frames to align the scale and shift of the depth distributions, as shown in Figure 3. Specifically, the interpolation network is finetuned from the video depth model $v_\\theta$ in Sec. 3.1. Instead of conditioning solely on the video, the first and last key frames of each window are also used, with a masking map indicating which frames are known. The frame interpolation is formulated as follows:\n$z_d = v_\\theta (\\phi_t (z_d), z_c, z_d, m, t)$,\nwhere $z_d$ represents the predicted key frames, with non-key frames padded with zeros. The masking map $m$ is used to indicate known key frames, which are set to 1, while other frames are set to 0. The masking map is replicated four times to align with the latent feature dimensions. To preserve the pre-trained structure and accommodate the expanded input, we duplicate input channels of $v_\\theta$ and halve the input layer's weight tensor as initialization."}, {"title": "4 EXPERIMENTS", "content": "4.1 DATASETS AND EVALUATION METRICS\nTraining Datasets. In addition to the collected DA-V dataset, we follow Ke et al. (2024) by incorporating two single-frame synthetic datasets, Hypersim (Roberts et al., 2021) and Virtual KITTI 2 (Cabon et al., 2020). Hypersim is a photorealistic synthetic dataset featuring 461 indoor scenes,"}, {"title": "4.2 IMPLEMENTATION DETAILS", "content": "Our implementation is based on SVD (Blattmann et al., 2023a), using the diffusers library (von Platen et al., 2022). We employ the AdamW optimizer (Loshchilov & Hutter, 2019) with a learning rate of $6.4 \\times 10^{-5}$. The model is trained at various resolutions: 512 \u00d7 512, 480 \u00d7 640, 707 \u00d7 707,"}, {"title": "4.3 ZERO-SHOT DEPTH ESTIMATION", "content": "Our model demonstrates exceptional zero-shot generalization in depth estimation across both indoor and outdoor datasets, as well as single-frame and multi-frame datasets.\nQuantitative Comparisons. Table 2 presents our model's performance in comparison to state-of-the-art depth estimation models using single-frame inputs. Our model significantly surpasses all previous generative models across various datasets and achieves results that are comparable to, and in some cases better than, those of top-performing discriminative models. For example, compared to GeoWizard (Fu et al., 2024), our model shows improvements of 0.4 in 81 and 0.1 in AbsRel on the NYUv2 dataset, 3.0 in 81 and 2.4 in AbsRel on KITTI, 1.8 in 81 and 1.7 in AbsRel on ETH3D, and 1.3 in 81 and 0.8 in AbsRel on the ScanNet dataset. When compared to Depth Anything (Yang et al., 2024a), we achieve gains of 0.5 in 81 and 0.7 in AbsRel on KITTI, along with a 1.5 improvement in the AbsRel metric on the ETH3D dataset. The impressive results are primarily attributed to the large-scale synthetic data we collected. Table 3 presents a comprehensive comparison of our model against previous video depth models. All generative models process multi-frame inputs in a single forward pass. Notably, our model demonstrates improved temporal consistency and spatial accuracy on the ScanNet++ dataset, highlighting its effectiveness in video depth estimation. Table 4 presents detailed comparisons with previous generative methods without ensemble techniques. Our model has fewer parameters than ChronoDepth (Shao et al., 2024) because we utilize a parameter-free RoPE instead of learnable absolute positional embeddings. It also reduces complexity compared to DepthCrafter (Hu et al., 2024b) by removing the clip embedding condition and classifier-free guidance. Additionally, we achieve lower inference time and fewer denoising steps while attaining better spatial accuracy on the ScanNet dataset compared to Marigold, ChronoDepth, and DepthCrafter."}, {"title": "4.4 ABLATION STUDIES", "content": "In this section, we evaluate the effectiveness of each component in Depth Any Video. For training efficiency, unless otherwise specified, the model is trained for only 10 epochs during ablation studies.\nGenerative Visual Prior. We investigate the impact of prior visual knowledge from the stable video diffusion model, as shown in Table 5. The first two rows clearly demonstrate that incorporating this prior significantly boosts the model's overall performance. Additionally, Figure 6(c) illustrates that this prior provides a strong initialization, leading to fast training convergence and enabling the model"}, {"title": "5 RELATED WORK", "content": "Monocular Depth Estimation. Existing models for monocular depth estimation can be roughly divided into two categories: discriminative and generative. Discriminative models are trained end-to-end to predict depth from images. For example, MiDaS (Lasinger et al., 2019) focuses on relative depth estimation by factoring out scale, enabling robust training on mixed datasets. Depth Anything (Yang et al., 2024a;b) builds on this concept, leveraging both labeled and unlabeled images to further enhance generalization. ZoeDepth (Bhat et al., 2023) and Metric3D (Yin et al., 2023; Hu et al., 2024a) aim to directly estimate metric depth. Generative models (Saxena et al., 2023), such as Marigold (Ke et al., 2024) and GeoWizard (Fu et al., 2024), leverage powerful priors learned from large-scale real-world data, allowing them to generate depth estimates in a zero-shot manner, even on unseen datasets. Our work falls into the second category, but focuses on video depth estimation.\nVideo Depth Estimation. Unlike single-image depth estimation, video depth estimation requires maintaining temporal consistency between frames. To eliminate flickering effects between consecutive frames, some works (Luo et al., 2020; Chen et al., 2019; Zhang et al., 2021) use an optimization procedure to overfit each video during inference. Other approaches (Guizilini et al., 2022; Zhang et al., 2019; Teed & Deng, 2020) directly predict depth sequences from videos. For instance, NVDS (Wang et al., 2023) proposes a refinement network to optimize temporal consistency from off-the-shelf depth predictors. Some concurrent works (Hu et al., 2024b; Shao et al., 2024) have focused on leveraging video diffusion models to produce coherent predictions. However, they often face challenges due to a lack of sufficiently high-quality and realistic depth data.\nVideo Generation. Diffusion models (Ho et al., 2020; Song et al., 2021) have achieved high-fidelity image generation from text descriptions, benefiting from large-scale aligned image-text datasets. Building on this success, VDM (Ho et al., 2022b) first introduces unconditional video generation in pixel space. Imagen Video (Ho et al., 2022a) and Make-a-Video (Singer et al., 2023) are cascade models designed for text-to-video generation. Align Your Latent (Blattmann et al., 2023b) and SVD (Blattmann et al., 2023a) extend Rombach et al. (2022) by modeling videos in the latent space of an autoencoder. Our model builds upon the generative visual prior of SVD, which is trained on diverse real video data, to maintain robust generalization in real-world scenarios."}, {"title": "6 CONCLUSION", "content": "We present Depth Any Video, a novel approach for versatile image and video depth estimation, powered by generative video diffusion models. Leveraging diverse and high-quality depth data collected from diverse synthetic environments, our model could generate temporally consistent depth sequences with fine-grained details across a broad spectrum of unseen scenarios. Equipped with a mixed-duration training strategy and frame interpolation, it generalizes effectively to videos of various lengths and resolutions. Compared to previous generative depth estimation models, our approach sets a new state-of-the-art in performance while significantly enhancing efficiency.\nLimitations. There are still certain issues in our model, such as difficulties in estimating depth for mirror-like reflections on water surfaces and challenges with extremely long videos. Future work will focus on collecting data for these challenging scenarios and improving model efficiency."}]}