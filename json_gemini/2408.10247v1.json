{"title": "MetaEnzyme: Meta Pan-Enzyme Learning for Task-Adaptive Redesign", "authors": ["Jiangbin Zheng", "Han Zhang", "Qianqing Xu", "An-Ping Zeng", "Stan Z. Li"], "abstract": "Enzyme design plays a crucial role in both industrial production and biology. However, this field faces challenges due to the lack of comprehensive benchmarks and the complexity of enzyme design tasks, leading to a dearth of systematic research. Consequently, computational enzyme design is relatively overlooked within the broader protein domain and remains in its early stages. In this work, we address these challenges by introducing MetaEnzyme, a staged and unified enzyme design framework. We begin by employing a cross-modal structure-to-sequence transformation architecture, as the feature-driven starting point to obtain initial robust protein representation. Subsequently, we leverage domain adaptive techniques to generalize specific enzyme design tasks under low-resource conditions. MetaEnzyme focuses on three fundamental low-resource enzyme redesign tasks: functional design (FuncDesign), mutation design (MutDesign), and sequence generation design (SeqDesign). Through novel unified paradigm and enhanced representation capabilities, MetaEnzyme demonstrates adaptability to diverse enzyme design tasks, yielding outstanding results. Wet lab experiments further validate these findings, reinforcing the efficacy of the redesign process.", "sections": [{"title": "1 INTRODUCTION", "content": "Enzymes, distinguished as specialized proteins, serve as biological catalysts, expediting chemical reactions. Their capacity to catalyze reactions ensures specificity and enables operation under mild conditions.  Positioned at the forefront of industrial production and the biological domain, enzyme design involves the deliberate creation of modified variants through functional design, commonly known as protein redesign , based on known structures or sequences.\nDespite its critical role, computational enzyme design is still in its early stages within the broader protein field. The scarcity of comprehensive enzyme data, coupled with the diversity of enzyme tasks and models , has resulted in a lack of systematic research and oversight in computational enzyme design. The inherent complexity of tasks and the vast diversity of data pose challenges for widespread adoption, contributing to relatively low attention in the enzyme field.\nTo address the challenges inherent in enzyme design, it is imperative to confront issues stemming from data scarcity and model generalization. Enzyme datasets often suffer from smaller scales compared to the broader protein domain due to specific functional categorizations, leading to inadequate model training. To mitigate this challenge, we propose leveraging pretrained universal protein models as intermediaries, replacing direct training of task-specific enzyme functional models. These universal models benefit from more extensive datasets in the general domain, resulting in stronger representation capabilities in both sequence and structural modalities. This could facilitates better domain adaptation to downstream tasks through transfer learning manner.\nFurthermore, functional enzyme tasks exhibit complex diversities, necessitating different modalities and data requirements, which may require multi-modality pretrained model architectures as drivers. However, this approach is not the more efficient or unified solution. Given our concentration on essential low-resource enzyme redesign tasks-functional design (FuncDesign), mutation design (MutDesign), and sequence generation design (SeqDesign)- encompassing the primary tasks and paradigms of contemporary AI research in enzymes, we underscore the significance of incorporating structural modality. This emphasis stems from the recognized principle that structures dictate functions. Taking these considerations into account, we aim to pursue both robust representation and generalization while simplifying the enzyme design framework.\nThrough observing commonalities across all tasks, we propose a novel MetaEnzyme framework, as illustrated in Figure 1. MetaEnzyme consists of a foundational universal protein design network (UniProt-Net) and downstream enzyme redesign modules. In the first stage, UniProt-Net undergoes pretraining to acquire robust representation capabilities. We instantiate UniProt-Net with a cross-modal structure-to-sequence network, as illustrated in Figure 2(a), to bridge the gaps between various redesign tasks. To augment structural and contextual representations, we introduce geometry-equivariant modules, implicit energy-motivated data augmentation, and multi-modal fusion, significantly enhancing generalization capabilities for low-resource scenarios. In the second stage, all enzyme redesign modes are driven by the cross-modal UniProt-Net. Based on characteristics such as autoregressive (AR) vs. non-autoregressive (NAR), non-parametric vs. parametric, etc., we rationalize different tasks and adopt meta-learning and domain-adaptive techniques in low-cost and high-efficiency manners.\nThe main contributions are outlined as follows:\n\u2022 The innovative unified enzyme design framework, MetaEnzyme, consolidates patterns across mainstream tasks, effectively leveraging commonalities for enhanced adaptability. The framework facilitates seamless transitions between universal protein design network and in-domain design evaluation tasks.\n\u2022 The proposed geometry-enhanced module, combined with techniques such as data augmentation, multi-modal fusion, significantly enhances representation capabilities for low-resource settings, along with the extension of multiple enzyme tasks.\n\u2022 A novel decoupled mutation scoring method is introduced for evaluating mutation effects, greatly improving efficiency.\n\u2022 Additional wet lab experiments for computational validation."}, {"title": "2 UNIVERSAL PROTEIN DESIGN NETWORK", "content": "As shown in Figure 2(a), UniProt-Net aims to to generate amino acid sequences conditioned on the protein backbones. UniProt-Net comprises a Geometric-invariant Structure Encoder (GeoStruc-Encoder), a Structure-Sequence Adapter (StrucSeq-Adapter), and a Self-attention Sequence Decoder (SaSeq-Decoder). The GeoStruc-Encoder and StrucSeq-Adapter collectively constitute the Encoder-Adapter Module (EnAd-Module), responsible for ensuring transformation equivariance. The StrucSeq-Adapter and SaSeq-Decoder collectively constitute the Context-Module incorporating fully self-attention layers.\nFormally, given a protein backbone \\(X = \\{X_1, X_2,\\cdots, X_n\\}\\) with a length of n, where \\(X_i \\in \\mathbb{R}^{3 \\times 3}\\) represents the atomic coordinates of the i-th amino acid residue composed of N, Ca, and C atoms. The corresponding generated protein sequence is denoted as \\(Y = \\{Y_1, Y_2,\\ldots, Y_n\\} \\in \\mathbb{R}^n\\). And \\(\\hat{Y}\\) denotes the native sequence \\(\\hat{Y} = \\{\\hat{Y}_1, \\hat{Y}_2,\\ldots,\\hat{I}_n\\} \\in \\mathbb{R}^n\\). The function \\(P_{uniprot}\\) represents the underlying UniProt-Net."}, {"title": "2.2 The Underlying Structure-to-Sequence Network", "content": "Definition 1. For any transformation \\(T \\in E(3)\\), a geometric network \\(\\varphi\\) is E(3)-equivariant if \\(\\varphi(T \\cdot X) = T \\cdot \\varphi(X)\\), and \\(\\varphi\\) is E(3)-invariant if \\(\\varphi(T \\cdot X) = \\varphi(X)\\).\nThe core component of the GeoStruc-Encoder is the lightweight GVP module [23], in which the vanilla GVP layers are overall rotation-invariant for rigid bodies since GVP outputs a rotation-equivariant vector feature v through an equivariant function f, and a rotation-invariant scalar feature s through an invariant function g for each amino acid, thus for any arbitrary rotation R: \\(R f(v) = f(R v), g(s) = g(R s)\\).\nThen to make the entire EnAd-Module rotation-invariant, a local reference frame \\(v_l\\) is further introduced to fuse with the rotating vector feature \\(v'\\), resulting in enhanced rotation-invariant features, which is rotation-invariant for any rotation R: \\(f(v' \\oplus v_l) = f(R(v' \\theta v_l))\\). Additionally, the input features are translation-invariant[24], making the overall EnAd-Module also translation-invariant. Therefore, the concatenated features are invariant to translations and rotations on the input coordinates. Thus for any rotation/translation T of the input, the output of the entire EnAd-Module \\(P_{enad}\\) can be invariant: \\(P_{enad}(v, s) = P_{enad}(T(v, s))\\). To obtain a better structural representation, we initialize the EnAd-Module parameters as [17] which is trained on millions of proteins."}, {"title": "2.2.2 Energy-motivated Data Augmentation", "content": "Guided by biochemistry principles, atomic interactions are regulated by forces and energy. This insight inspires the incorporation of energy and force concepts into data augmentation[21, 22]. To achieve this, we introduce an energy-driven Riemann-Gaussian geometric technique, illustrated in Figure 2(c), which ensures that structural variations maintain the energy dynamics of proteins, preserving pairing relationships and geometric characteristics. In essence, the augmented structures guarantee structural invariance for the spatial distribution of energy or force. In a nutshell, we obtain a noisy sample \\(X'\\) from X according to a certain conditional distribution, i.e., \\(X' \\sim P_{noise} (X'|X)\\). Unlike conventional denoising methods applied to images or other Euclidean data, the introduced noise in our 3D geometry is tailored to be geometry-aware rather than conformation-aware, i.e., \\(p(X'|X)\\) should possess doubly E(3)-invariant:\n\\[p(T_1 X'|T_2 X) = p(X'|X), \\forall T_1, T_2 \\in E(3).\\]\nThis is consistent with the observation that the behavior of proteins with the same geometry should be independent of different conformations. A conventional choice of \\(p_{noise} (X'|X)\\) is utilizing the standard Gaussian with noise scale \\(\\sigma\\) as \\(p_{noise} (X'|X) = N(X, \\sigma^2 I)\\). But this naive form fails to meet the doubly E(3)-invariant property in Eq. 1. Specifically, considering the derived force target \\(\\nabla_{X,logp(X'|X)} = -\\frac{X}{\\sigma^2}\\) where \\(X' = R \\cdot X\\) s.t. rotation \\(R \\neq I\\), then \\(\\nabla_{X,logp(X'|X)} = -\\frac{1}{\\sigma^2}(R - I)X \\neq 0\\), which imply that the force between \\(X' = R \\cdot X\\) and X is not equal although the same geometry is shared. Hence, to devise the form with the symmetry in Eq. 1, we instead resort to Riemann-Gaussian [7] as:\n\\[P_{noise} (X'|X) = Rie(X'|X) := \\frac{1}{\\zeta(\\sigma)}exp(-\\frac{\\delta^2(X', X)}{4\\sigma^2}),\\]\nwhere \\(\\zeta(\\sigma)\\) is the normalization term, and \\(\\delta\\) is the metric that calculates the difference between \\(X'\\) and X. Riemann-Gaussian is a generalization version of typical Gaussian, by choosing various distances \\(\\delta\\) beyond the Euclidean metric. To pursue the constraint in Eq. 1:\n\\[\\delta(X', X) = \\sqrt{\\mid\\mid X' - \\mu(X') - (X - \\mu(X))\\mid\\mid_2},\\]\nwhere \\(X = X - \\mu(X)\\) shifts X to zero mean (\\(\\mu(X)\\) is the mean of X's columns). The same transformation applies to X. The distance function \\(\\delta\\) adheres to the doubly E(3)-invariance as in Eq. 1. Notably, \\(\\delta\\) is permutation-invariant concerning the order of the columns in X' and X."}, {"title": "2.2.3 Context Module Initialization", "content": "The initialization of the Context-Module, aims to incorporate prior language knowledge into the modules . The entire Context-Module functions as an encoder-decoder Transformer[51], denoted as \\(P_{context}\\) with linear output layers. To initialize the Context-Module exclusively based on sequence data from the training set, we employ a sequence-to-sequence recovery task using an autoencoder (AE) mode. This allows the Context-Module to acquire contextual semantic knowledge, utilizing cross-entropy (CE) loss for learning. Formally, given an input protein sequence for the encoder \\(S_{in} = \\{s_1, s_2,..., s_n\\}\\) with n amino acids, and the reference native sequence as \\(S_{native} = S_{in} = \\{s_1, s_2,..., s_n\\}\\), the AE-based objective is to generate a sequence \\(\\hat{S}\\) to recover \\(S_{native}\\) as accurately as possible:\n\\[L_{AE} = CE(p_{context}(logits_{\\hat{S}_j}|S_{in}), S_{native}).\\]"}, {"title": "2.3 Training Pipeline in Initial Phase", "content": "We initialize the parameters of UniProt-Net based on prior knowledge. To enhance the dataset, we incorporate Riemann-Gaussian data augmentation, combining the original protein dataset \\(D_{ori}\\) with the augmented dataset denoted as \\(D_{rie}\\), resulting in a new dataset \\(D_{aug} = D_{ori} \\cup D_{rie}\\). In the processing flow, the protein backbone \\(X_{D_{aug}}\\) is initially input into the EnAd-Module, producing geometric context features. These features are then fed into the decoder to generate the sequence distribution \\(logits_Y\\) and derive the generated sequence Y as:\n\\[Y = argmax(p_{uniprot} (logits_Y |X)), X \\sim D_{aug},\\]\nand cross-entropy loss is used for training:\n\\[L_{primary} = CE(P_{uniprot} (logits_Y |X), \\hat{Y}).\\]"}, {"title": "3 DOMAIN ADAPTION FOR LOW-RESOURCE ENZYME DESIGN TASKS", "content": null}, {"title": "3.1 Few-shot Learning for Functional Design (FuncDesign)", "content": "We introduce a task-specific meta-learning framework for the task distribution T, outlined in Algorithm 1. The UniProt-Net here is regarded as a function \\(f_{\\theta}\\) with an initial pretrained parameter \\(\\theta_{meta}\\) involving linear classification layers. The primary objective of the meta-learner is to acquire updated parameters of \\(\\theta_{meta}\\) containing high-order meta-information, facilitating swift adaptation to a new task drawn from T. Upon adapting the meta-learner to a new task \\(T_i \\sim T\\), the meta undergoes updates, transforming into the task-specific parameter \\(\\theta_i\\) through a few gradient descent iterations. These parameter updates, executed based on the support set \\(S_i\\) for \\(T_i\\), are referred to as inner loops. The number of inner loops is denoted as \\(L_{in}\\). And loss function \\(I_{meta}\\) is utilized in the inner-loop updates as:\n\\[\\theta_i^{(L_{in})} = \\theta_{meta} - \\sum_{t=0}^{L_{in}-1} (\\nabla_{\\theta_i^{(t)}} L_i^{(t)}) s.t.\\]\n\\[L_i^{(t)} = \\mathbb{E}_{x, y, \\hat{y} \\in S_i} (I_{meta}(f_{\\theta_i^{(t)}}(c_i|Y_i; X_i), \\hat{c}_i),\\]\nwhere \\(\\theta_i^{(t=0)}\\) equals to \\(\\theta_{meta}\\), and \\(\\theta_i^{(t)}\\) denotes the fold-specific parameters after t inner loops. l is the learning rate. \\(X_i, Y_i\\) represents the native structure-sequence pairs, with \\(\\hat{c}_i\\) as labels in \\(S_i\\). The optimization of \\(\\theta_{meta}\\) occurs on the query set \\(Q_i \\sim Q\\) across tasks T during outer loops. Consequently, the update for \\(\\theta_{meta}\\) at one step as:\n\\[\\theta_{meta} \\leftarrow \\theta_{meta} - (l'\\nabla_{\\theta_{meta}} L_{Q_i}) s.t.\\]\n\\[L_{Q_i} = \\mathbb{E}_{X,Y, \\hat{c} \\in Q_i} I_{meta}(f_{\\theta_i^{(L_{in})}}(c_i|Y_i; X_i), \\hat{c}_i),\\]\nwhere l' is the learning rate of the outer loop. \\(X_i, Y_i\\) represent protein pairs with \\(\\hat{c}_i\\) labels in \\(Q_i\\), similarly.\nThe meta-objective function \\(I_{meta}\\) varies across functional tasks. For instance, in binary categorization, it can be expressed as:\n\\[I_{meta} = -y(1-p)log(p) \u2013 (1 \u2013 y)plog(1 \u2013 p)\\]\n\\[=\\begin{cases}\n-(1-p)log(p), & y = 1 \\\\n-plog(1-p), & y = 0\n\\end{cases}\\]\nwhere y > 0 denotes an adjustable factor. While in a multi-classification task, it can be expressed as:\n\\[I_{meta} = -\\sum_{i=1}^{K} y_i log(p_i),\\]\nwhere K is the number of classification categories."}, {"title": "3.2 Zero-shot Learning for Mutant Effect Prediction (MutDesign)", "content": "Mutation effect prediction is a non-parametric zero-shot learning in the NAR manner. Given the unique architecture of our MetaEnzyme, we introduce a novel mutation effect scoring method that decouples mutation scoring approaches, as illustrated in Figure 3. Formally, considering a wild-type enzyme structure \\(X_{wild} \\in \\mathbb{R}^{n \\times 3 \\times 3}\\) with its corresponding wild-type sequence \\(Y_{wild} \\in \\mathbb{R}^n\\), the non-parametric mutation effect prediction task aims to query the mutation-related ranking for the mutation sequence \\(Z_{mut} \\in \\mathbb{R}^n\\), which is scored based on the marginal probabilities \\(logits_{wild}\\) and the reference distribution \\(logits_{mut}\\) as:\n\\[p(Y_{wild}||Z_{mut}) = \\delta_{mut}(logits_{wild}, LS(logits_{mut})),\\]\ns.t. \\(logits_{wild} = P_{uniprot} (X_{wild}; Y_{wild}) \\in \\mathbb{R}^{n \\times M},\\)\n\\[= \\{Y_0, Y_1, , Y_n\\}\\]\n\\[logits_{mut} = OneHot(Z_{mut}) \\in \\mathbb{R}^{n \\times M},\\)\n\\[= \\{Z_0, Z_1,..., Z_n\\}\\]\nwhere \\(\\delta_{mut}\\) is a distance measure between distributions of the wild-type and mutant. M=20 corresponds to 20 commonly used amino acid types. OneHot is one-hot encoding, with \\(z_{ik} \\in \\{0,1\\}\\) s.t. \\(Z_{ik} \\sim z_i, 0 \\le k < M\\). LS denotes the label smoothing function with a constant \\(\\epsilon = 0.1\\) as:\n\\[LS(logits_{mut}) = logits_{mut} \\times (1 - \\epsilon)+\\]\n\\[(1 - logits_{mut}) \\times \\frac{\\epsilon}{M-1} = \\{z_0, z_1, ..., z\\}.\\]\nInstantiating \\(\\delta_{mut}\\) with a weighted cross-entropy function:\n\\[p(Y_{wild}||Z_{mut}) = -\\sum_{i=1}^n W_i z_i log(y_i),\\]\nwhere \\(W = \\{W_1, W_2, , W_n\\} \\in \\mathbb{R}^{n \\times 1}\\) is a weight matrix:\n\\[W_i = \\begin{cases}\n\\frac{1-\\alpha}{\\sum_{j=1}^n MutSet_j}, & MutSet_i = 1 \\\\\n\\frac{\\alpha}{\\sum_{j=1}^n (1-MutSet_j)}, & MutSet_i = 0\n\\end{cases}\\]\nwhere \\(MutSet \\leftarrow (Y_{wild} == Z_{mut}) \\in \\mathbb{R}^n, \\forall MutSet_i \\in \\{0, 1\\}\\), and MutSet implies all mutant positions. \\(\\alpha = 0.5\\).\nFrom Eq. 11, we observe that: 1) the computation of wild-type and mutant proteins is independent; 2) deep network flow (such as UniProt-Net) predicts only the wild-type proteins, requiring a single inference due to the insight that mutant sequences stem from unique wild-type proteins through evolution or modification; 3) large-scale mutant sequences do not necessitate a network model, and they can be converted to a distribution representation based on one-hot encoding simply. This decoupling method provides a significant advantage in terms of speed, particularly for sequence datasets exceeding a million entries and for scenarios involving multi-site/higher-order mutations. Additionally, as the computation process relies entirely on the pre-trained parameters of UniProt-Net and makes no distribution assumptions about the query mutant enzymes, it is non-parametric. Section 4.3.2 presents detailed analysis."}, {"title": "3.3 Autoregressive Generalization for Sequence Generation (SeqDesign)", "content": "For the conditional sequence generation, we shift the entire framework to an AR mode. Specifically, when provided with a candidate structure backbone \\(X_c\\), the objective is to generate an unknown protein sequence \\(Y_{sample}\\) as:\n\\[Y_{sample} = \\prod_{i=0}^{n} P_{uniprot}(Y_i|Y_{i-1}, \u00b7\u00b7\u00b7, Y_0; X_c).\\]\nThis density is represented using a Vanilla Transformer decoding paradigm. AR decoding typically conducts SeqDesign tasks for general proteins and functional enzymes."}, {"title": "4 IN SILICO EXPERIMENTS", "content": null}, {"title": "4.1 Setups", "content": "Training Set. CATH dataset [20] is divided into training, validation, and testing sets, containing 18,204, 608, and 1,120 structure-sequence pairs, respectively. The training set is utilized to train the UniProt-Net in the pretraining stage.\nEvaluation Sets. FuncDBbi: Approximately 700 enzymes with experimentally determined structure-sequence pairs across 10 folds from RCSB are collected. Each fold is crafted with an equal number of enzymes and balanced with non-enzyme negative samples. This dataset is used for enzyme function prediction and design. FuncDBmulti is a subset of FuncDBbi dataset, comprising solely positive samples, for multi-class functional prediction tasks-precisely predicting the fold level to which an enzyme belongs.\nMutDBProtGym [40] is intricately designed for protein fitness, encompassing an expansive collection of over 217 deep mutational scanning assays and millions of mutant sequences.\nSeqDesignDBPet comprises 178 proteins identified for the capability to degrade plastics [12]. SeqDesignDBHybrid is the alias of FuncDBmulti for differentiation. SeqDesignDBALL is extracted from the testing set of CATH. SeqDesignDBTs50 and SeqDesignDBTs500 correspond to Ts50 and Ts500 [30] for validating generalization.\nImplementation. The AdamW optimizer with a batch size of 5 and a learning rate of 1e-3 to train the UniProt-Net. The GeoStruc-Encoder consists of 4 layers with a dropout of 0.1. The node hidden dimensions of scalars and vectors are 1024 and 256. The adapter and decoder have 8 multi-head self-attention layers, with an embedding dimension of 512 and an attention dropout of 0.1. For meta-learning configures, the number of inner loops is set to 5, and the batch sizes of the support set and query set each are both 10. The updated learning rates l' and I are set to 1e-3, and the Adam optimizer is used. The parameters of EnAd-Module are fixed. And 1 NVIDIA A100 80GB GPU is used. The code is available at https://github.com/binbinjiang/MetaEnzyme."}, {"title": "4.2 Few-shot Learning for Function Prediction (FuncDesign)", "content": "Meta Learning for Fold independent prediction aims to predict unseen enzyme fold classes. Based on FuncDBi, data from 9 folds are used as the training set, leaving one fold as the evaluation. Due to the limited samples in each fold and their mutual independence, we treat each fold as an independent task, leading to the application of meta-learning for task-based learning. 'Fold' here denotes the three-dimensional configuration of secondary structural elements, such as alpha helices and beta sheets, that define a specific protein or protein group. Proteins sharing similar folds usually exhibit substantial structural resemblances, despite variations in their sequences and functions. Classifying folds can provide valuable insights into the evolutionary connections between proteins. As shown in the table 1, Ours(M) involves meta-leaner as a base, followed by meta-finetuning for further refinement. And Ours(Z) directly employs zero-shot learning for trained meta-learner. Ours(M) yields superior results, highlighting the significant improvement brought about by meta-finetuning."}, {"title": "4.2.2 Fold-agnostic Prediction", "content": "Ours(F) serves as a control group, primarily comparing few-shot learning with conventional finetuning methods. It represents fold-agnostic prediction, excluding meta-learning learning. We combine positive and negative samples from 9 folds, randomly allocating 80% as the training set and 20% as the evaluation set, treating the remaining 1 fold as an out-of-distribution generalization. For unseen folds, we observed that task-specific learning based on meta-learning is more effective with the average value of Ours(M) at 0.903 exceeding that of Ours(F) at 0.857. Additionally, ESM-IF, acting as a baseline due to its outstanding representation and similar modalities, clearly demonstrates the superiority of our model in terms of generalization."}, {"title": "4.2.3 Enzyme Fold Recognition", "content": "Furthermore, we conduct predictions on more challenging fold types based on FuncDBmulti, essentially involving a less data-intensive multi-task enzyme function prediction, as shown in Figure 4. This setup aims to predict, within enzymes with mixed folds, the specific fold to which an enzyme belongs. We select ESM-IF and GVP as structure-to-sequence models as baselines, as they also utilize N, Ca, and C as standard inputs. These comparisons validate the superior structural representation capabilities by a large margin (Ours: 17.3%P@1,31.7%P@2,41.0%P@3). We employ ESM-2 as a language modality input for comparison to confirm its sequence-only representation capabilities. Although ESM-2 (11.5%P@1,24.5%P@2,36.5%P@3) is a little better than the earlier GVP, but is far below the performance of our models, which demonstrates enhanced generalization ability due to the incorporation of structural information.\nOverall, while we have undertaken initial explorations in the few-shot setting, the overall accuracy remains relatively low. This is attributed to limitations in the quantity of available data and the diversity of fold types. This compels us to collect data encompassing a broader range of categories."}, {"title": "4.3 Non-parametric Zero-shot Learning for Mutation Effects (MutDesign)", "content": "To ensure a comprehensive and unbiased comparison, we selected prominent protein language models and inverse folding models as benchmarks, as illustrated in Figure 5 (a). All baseline models employ zero-shot learning for mutation effects evaluation through the MutDBprotGym, which encompasses millions of mutations. Note that our model consists of two configurations: Ours(w/o finetune) directly infers based on UniProt-Net, yielding an average \\(\\rho\\) of 43.9%; And Ours(w/ finetune) utilizes the meta-finetuned UniProt-Net, resulting in an average \\(\\rho\\) of 44.5%, which consistently achieves the best matching rank, implying that the in-domain finetuning on enzyme data contributes to enhanced overall performance in mutation prediction. In contrast, the optimal protein language model (VESPA)[35] achieved an average \\(\\rho\\) of 43.7%, while the optimal inverse folding model (ESM-IF) scored 42.2%. This performance superiority can be attributed to our model's dual advantage, encompassing both contextual and structural transfer learning within the proposed training paradigm."}, {"title": "4.3.2 Complexity of Decoupled Scoring Mechanism", "content": "The complexity analysis was carried out utilizing a single NVIDIA A100 80GB GPU, as outlined in Table 2. We utilize the widely accepted fitness scoring metric, as referenced in ESM-1v [36], as our baseline. By maintaining a fixed batch size of 1 and an average protein length of 500 amino acids, we vary the number of mutant proteins from 500 to 5000. Space complexity is governed by the number of parameters, where both the baseline and our decoupling method exhibit relatively small parameter counts (124.82M). Regarding time complexity, reflected through Throughputs and FLOPs, both the baseline and our method showcase nearly identical FLOPs (52.7k). As we increment the average number of mutant proteins from 500 to 5000, our decoupling method's advantage further amplifies, although this ascent in advantage slows down due to hardware memory limitations. Our approach requires only one inference step for the wild-type protein to acquire its probability distribution, subsequently facilitating swift calculation of mutant differences between mutant sequences and the inferred wild-type distributions. This significantly expedites the process. In contrast, the baseline necessitates model inference for each mutant protein, resulting in reduced speed. Overall, our model demonstrates markedly enhanced throughput performance, especially noticeable under conditions involving a substantial number of mutations. Compared to the mainstream baseline scoring method, ours boasts a speed advantage ranging from 4k to 30k times faster. This presents a promising avenue for future research, particularly in multi-site/high-order mutation inference, thereby expanding the possibilities for exhaustive exploration."}, {"title": "4.4 Parametric Conditional Protein Generation (SeqDesign)", "content": "Switching to the NAR mode and coordinating with the lower triangular mechanism, MetaEnzyme is employed for SeqDesign. The amino acid recovery (AAR) metric is adopted."}, {"title": "4.4.1 Internal Validation of General Proteins", "content": "Figure 6 shows SeqDesign comparisons at general proteins, with detailed benchmarks in Appendix."}, {"title": "4.4.2 Generalization on Functional Enzymes", "content": "Conditional SeqDesign for PET Enzymes. Polyethylene terephthalate (PET) is a widely used synthetic plastic polymer globally, known for its chemical inertness due to ester bonds and aromatic nuclei. This makes PET resistant to degradation, raising environmental concerns. Enzymatic degradation offers a promising and eco-friendly solution to address the ecological challenge of plastic waste, particularly polyester waste recycling. Despite the potential, our understanding of PET-degrading enzymes is limited. Machine learning-aid techniques might accelerate the discovery of PET hydrolases. To support this effort, we curated the SeqDesignDBpet dataset for SeqDesign analysis, serving as a valuable resource. Figure 7(PET) illustrates AAR scores for PET hydrolases, showing performance improvements up to 64.35% over baselines (62.56% ESM-IF, 45.40% GVP). This suggests strong generalization capabilities for unseen functional enzymes, paving the way for future analysis and PET-enzyme redesign."}, {"title": "4.5 Initialization Analysis and Ablation Study", "content": "To underscore the significance of pretrained modules, we conducted an ablation study to assess the impact of the pretrained EnAd-Module (PEM) and the pretrained Context-Module (PCM). The results, outlined in Table 3, highlight their substantial contributions to overall improvement. Specifically, when comparing #1 vs. #3 and #1 vs. #2, it is evident that the pretrained structural features play a pivotal role in boosting performance. This aligns seamlessly with our rationale for incorporating prior structural knowledge. Moreover, given that the PCM module operates downstream of PSM, it is likely to directly benefit from the representation capabilities of PSM. Surprisingly, in the isolated comparison of #1 vs. #2, significant performance is enhanced even when utilizing only the PCM without the initialization parameters of the PEM. This intriguing finding, previously unexplored in SeqDesign studies, suggests that the PCM has acquired substantial knowledge of the protein language, thereby contributing significantly to improved SeqDesign performance. This observation gains further support in the comparison between #3 and #4, solidifying the conclusion that the PCM's acquired knowledge plays a vital role, diminishing the reliance on initialization parameters. The comprehensive analysis presented here highlights the logical and noteworthy advancements brought about by these pretrained modules in protein design."}, {"title": "5 IN VITRO WET EXPERIMENTAL VALIDATION", "content": "To further validate the reliability, we selected a commercially relevant enzyme with potential applications in production, the reversible glycine cleavage system (rGCS) [32, 43, 55], for wet lab experimentation. rGCS, known for efficiently fixing carbon to produce glycine in vitro, currently exhibits relatively low yields in glycine production. Traditional rational enzyme design for rGCS has shown limited improvements in catalytic activity and other attributes. The rGCS, consisting of three proteins (P-protein decarboxylase, T-protein aminomethyltransferase, and H-protein shuttle), can fix two different one-carbon carbon sources to synthesize the two-carbon compound glycine. We focus on P-protein redesign in this work. For details on the structure and mutation preparation processes, please refer to Appendix.\nUtilizing rational design principles, we strategically chose mutation sites with anticipated substantial influence on mutation effects. Wet lab experiments assessing carbon fixation were conducted for approximately 40 guided mutations, and their effects are illustrated in Figure 8. The wet lab outcomes exhibit close alignment with in silico predictions, demonstrating a notable Spearman's \\(\\rho\\) ranking correlation of 70.1%. These results reinforce the practical efficacy of MetaEnzyme, pinpointing crucial sites for subsequent iterations in enhancing the P-protein's carbon-fixing capabilities."}, {"title": "6 RELATED WORK", "content": "In the dynamic landscape of protein design [18, 19, 53, 59, 60", "57": "have made significant strides. Tools such as Structured Transformer [20", "23": "have pioneered conditional protein generation, while recent models like ProteinMPNN [8", "8": "and ESM-IF [17", "24": "RosettaFold [3", "52": "helixfold [10", "31": "stand out as influential structure prediction models. In the niche domain of enzyme engineering [4, 16, 25-29, 44, 47", "48": "ESM-1v [36, 45, 46", "39": "ESM-2 [31"}]}