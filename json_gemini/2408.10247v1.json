{"title": "MetaEnzyme: Meta Pan-Enzyme Learning for Task-Adaptive Redesign", "authors": ["Jiangbin Zheng", "Han Zhang", "Qianqing Xu", "An-Ping Zeng", "Stan Z. Li*"], "abstract": "Enzyme design plays a crucial role in both industrial production and biology. However, this field faces challenges due to the lack of comprehensive benchmarks and the complexity of enzyme design tasks, leading to a dearth of systematic research. Consequently, computational enzyme design is relatively overlooked within the broader protein domain and remains in its early stages. In this work, we address these challenges by introducing MetaEnzyme, a staged and unified enzyme design framework. We begin by employing a cross-modal structure-to-sequence transformation architecture, as the feature-driven starting point to obtain initial robust protein representation. Subsequently, we leverage domain adaptive techniques to generalize specific enzyme design tasks under low-resource conditions. MetaEnzyme focuses on three fundamental low-resource enzyme redesign tasks: functional design (FuncDesign), mutation design (MutDesign), and sequence generation design (SeqDesign). Through novel unified paradigm and enhanced representation capabilities, MetaEnzyme demonstrates adaptability to diverse enzyme design tasks, yielding outstanding results. Wet lab experiments further validate these findings, reinforcing the efficacy of the redesign process.", "sections": [{"title": "1 INTRODUCTION", "content": "Enzymes, distinguished as specialized proteins, serve as biological catalysts, expediting chemical reactions. Their capacity to catalyze reactions ensures specificity and enables operation under mild conditions, thus playing a crucial role in various industries. [1, 38, 54, 56]. Positioned at the forefront of industrial production and the biological domain, enzyme design involves the deliberate creation of modified variants through functional design, commonly known as protein redesign [6, 25, 41], based on known structures or sequences.\nDespite its critical role, computational enzyme design is still in its early stages within the broader protein field. The scarcity of comprehensive enzyme data, coupled with the diversity of enzyme tasks and models [34, 64], has resulted in a lack of systematic research and oversight in computational enzyme design. The inherent complexity of tasks and the vast diversity of data pose challenges for widespread adoption, contributing to relatively low attention in the enzyme field.\nTo address the challenges inherent in enzyme design, it is imperative to confront issues stemming from data scarcity and model generalization. Enzyme datasets often suffer from smaller scales compared to the broader protein domain due to specific functional categorizations, leading to inadequate model training. To mitigate this challenge, we propose leveraging pretrained universal protein models as intermediaries, replacing direct training of task-specific enzyme functional models. These universal models benefit from more extensive datasets in the general domain, resulting in stronger representation capabilities in both sequence and structural modalities. This could facilitates better domain adaptation to downstream tasks through transfer learning manner.\nFurthermore, functional enzyme tasks exhibit complex diversities, necessitating different modalities and data requirements, which may require multi-modality pretrained model architectures as drivers. However, this approach is not the more efficient or unified solution. Given our concentration on essential low-resource enzyme redesign tasks-functional design (FuncDesign), mutation design (MutDesign), and sequence generation design (SeqDesign)-encompassing the primary tasks and paradigms of contemporary AI research in enzymes, we underscore the significance of incorporating structural modality. This emphasis stems from the recognized principle that structures dictate functions. Taking these considerations into account, we aim to pursue both robust representation and generalization while simplifying the enzyme design framework.\nThrough observing commonalities across all tasks, we propose a novel MetaEnzyme framework, as illustrated in Figure 1. MetaEnzyme consists of a foundational universal protein design network (UniProt-Net) and downstream enzyme redesign modules. In the first stage, UniProt-Net undergoes pretraining to acquire robust representation capabilities. We instantiate UniProt-Net with a cross-modal structure-to-sequence network, as illustrated in Figure 2(a), to bridge the gaps between various redesign tasks. To augment structural and contextual representations, we introduce geometry-equivariant modules, implicit energy-motivated data augmentation, and multi-modal fusion, significantly enhancing generalization capabilities for low-resource scenarios. In the second stage, all enzyme redesign modes are driven by the cross-modal UniProt-Net. Based on characteristics such as autoregressive (AR) vs. non-autoregressive (NAR), non-parametric vs. parametric, etc., we rationalize different tasks and adopt meta-learning and domain-adaptive techniques in low-cost and high-efficiency manners.\nThe main contributions are outlined as follows:\n\u2022 The innovative unified enzyme design framework, MetaEnzyme, consolidates patterns across mainstream tasks, effectively leveraging commonalities for enhanced adaptability. The framework facilitates seamless transitions between universal protein design network and in-domain design evaluation tasks.\n\u2022 The proposed geometry-enhanced module, combined with techniques such as data augmentation, multi-modal fusion, significantly enhances representation capabilities for low-resource settings, along with the extension of multiple enzyme tasks.\n\u2022 A novel decoupled mutation scoring method is introduced for evaluating mutation effects, greatly improving efficiency.\n\u2022 Additional wet lab experiments for computational validation."}, {"title": "2 UNIVERSAL PROTEIN DESIGN NETWORK", "content": "2.1 Problem Statement\nAs shown in Figure 2(a), UniProt-Net aims to to generate amino acid sequences conditioned on the protein backbones. UniProt-Net comprises a Geometric-invariant Structure Encoder (GeoStruc-Encoder), a Structure-Sequence Adapter (StrucSeq-Adapter), and a Self-attention Sequence Decoder (SaSeq-Decoder). The GeoStruc-Encoder and StrucSeq-Adapter collectively constitute the Encoder-Adapter Module (EnAd-Module), responsible for ensuring transformation equivariance. The StrucSeq-Adapter and SaSeq-Decoder collectively constitute the Context-Module incorporating fully self-attention layers.\nFormally, given a protein backbone X = {X1, X2,\uff65\uff65\uff65, Xn} with a length of n, where Xi \u2208 R3\u00d73 represents the atomic coordinates of the i-th amino acid residue composed of N, Ca, and C atoms. The corresponding generated protein sequence is denoted as Y = {Y1, Y2,\u2026, Yn} \u2208 R\u201d. And \u0176 denotes the native sequence \u0176 = {\u01761, \u01762,\u2026, \u00cen} \u2208 R\u201d. The function Puniprot represents the underlying UniProt-Net.\n2.2 The Underlying Structure-to-Sequence Network\n2.2.1 Geometry-enhanced Structural Encoder. Definition 1. For any transformationT \u2208 E(3), a geometric network \u03c6 is E(3)-equivariant if \u03c6(T \u00b7 X) = T\u00b7 \u03c6(X), and \u03c6 is E(3)-invariant if \u03c6(T \u00b7 X) = \u03c6(\u03a7).\nThe core component of the GeoStruc-Encoder is the lightweight GVP module [23], in which the vanilla GVP layers are overall rotation-invariant for rigid bodies since GVP outputs a rotation-equivariant vector feature v through an equivariant function f, and a rotation-invariant scalar feature s through an invariant function g for each amino acid, thus for any arbitrary rotation R: Rf (v) = f(Ru), g(s) = g(Rs), as shown in Figure 2(b).\nThen to make the entire EnAd-Module rotation-invariant, a local reference frame v\u03b9 [17] is further introduced to fuse with the rotating vector feature v', resulting in enhanced rotation-invariant features, which is rotation-invariant for any rotation R: f (v' \u2295v\u2081) = f(R(\u03c5' \u03b8 \u03c5\u03b9)). Additionally, the input features are translation-invariant[24], making the overall EnAd-Module also translation-invariant. Therefore, the concatenated features are invariant to translations and rotations on the input coordinates. Thus for any rotation/translation T of the input, the output of the entire EnAd-Module Penad can be invariant: Penad(v, s) = Penad(T(v, s)). To obtain a better structural representation, we initialize the EnAd-Module parameters as [17] which is trained on millions of proteins.\n2.2.2 Energy-motivated Data Augmentation. Guided by biochemistry principles, atomic interactions are regulated by forces and energy. This insight inspires the incorporation of energy and force concepts into data augmentation[21, 22]. To achieve this, we introduce an energy-driven Riemann-Gaussian geometric technique, illustrated in Figure 2(c), which ensures that structural variations maintain the energy dynamics of proteins, preserving pairing relationships and geometric characteristics. In essence, the augmented structures guarantee structural invariance for the spatial distribution of energy or force. In a nutshell, we obtain a noisy sample X' from X according to a certain conditional distribution, i.e., X' ~ Pnoise (X'|X). Unlike conventional denoising methods applied to images or other Euclidean data, the introduced noise in our 3D geometry is tailored to be geometry-aware rather than conformation-aware, i.e., p(X'|X) should possess doubly E(3)-invariant:\np(T\u2081X'|T2X) = p(X'|X), VT1, T2 \u2208 E(3). (1)\nThis is consistent with the observation that the behavior of proteins with the same geometry should be independent of different conformations. A conventional choice of pnoise (X'|X) is utilizing the standard Gaussian with noise scale \u03c3 as pnoise (X'|X) = N(\u03a7, \u03c32\u0399). But this naive form fails to meet the doubly E(3)-invariant property in Eq. 1. Specifically, considering the derived force target Vx,logp(X'\\X) = \u2212XX where X' = R \u00b7 X s.t. rotation R \u2260 I, then \u2207x,logp(X'|X) = \u2212+(R \u2013 I)X \u2260 0, which imply that the force between X' = R. X and X is not equal although the same geometry is shared. Hence, to devise the form with the symmetry in Eq. 1, we instead resort to Riemann-Gaussian [7] as:\nPnoise (X'|X) = Rie(X'|X) := \\frac{1}{\u03b6(\u03c3)}exp(-\\frac{\u03b4\u00b2 (\u03a7', \u03a7)}{4\u03c32}), (2)\nwhere \u03b6(\u03c3) is the normalization term, and 8 is the metric that calculates the difference between X' and X. Riemann-Gaussian is a generalization version of typical Gaussian, by choosing various distances & beyond the Euclidean metric. To pursue the constraint in Eq. 1:\n\u03b4(X', X) = \\X \u2212 \u03bc(X) \u2212 (X' \u2212 \u03bc(X'))r\\2, (3)\nwhere X = X \u2013 \u03bc(X) shifts X to zero mean (\u03bc(X) is the mean of X's columns). The same transformation applies to X. The distance function & adheres to the doubly E(3)-invariance as in Eq. 1. Notably, 8 is permutation-invariant concerning the order of the columns in X' and X.\n2.2.3 Context Module Initialization. The initialization of the Context-Module, as shown in Figure 2(d), aims to incorporate prior language knowledge into the modules [58, 61?-63]. The entire Context-Module functions as an encoder-decoder Transformer[51], denoted as Pcontext with linear output layers. To initialize the Context-Module exclusively based on sequence data from the training set, we employ a sequence-to-sequence recovery task using an autoencoder (AE) mode. This allows the Context-Module to acquire contextual semantic knowledge, utilizing cross-entropy (CE) loss for learning. Formally, given an input protein sequence for the encoder Sin = {$1, $2,..., n} with n amino acids, and the reference native sequence as Snative = Sin = {$1, $2,..., n}, the AE-based objective is to generate a sequence S, to recover Snative as accurately as possible:\nLAE = CE(pcontext (logitss, |Sin), Snative). (4)\n2.3 Training Pipeline in Initial Phase\nWe initialize the parameters of UniProt-Net based on prior knowledge. To enhance the dataset, we incorporate Riemann-Gaussian data augmentation, combining the original protein dataset Dori with the augmented dataset denoted as Drie, resulting in a new dataset Daug = Dori \u016aDrie. In the processing flow, the protein backbone X Daug is initially input into the EnAd-Module, producing geometric context features. These features are then fed into the decoder to generate the sequence distribution logitsy and derive the generated sequence Y as:\nY = argmax(puniprot (logitsy|X)), X ~ Daug, (5)\nand cross-entropy loss is used for training:\nLprimary = CE(Puniprot (logitsy|X), \u0176). (6)"}, {"title": "3 DOMAIN ADAPTION FOR LOW-RESOURCE ENZYME DESIGN TASKS", "content": "Algorithm 1 Task-specific Meta Enzyme Learning\nRequire: T: task distributions; Ti: individual enzyme task s.t. i \u2208 [0, N); Lout and Lin: Update steps for outer and inner loop; bt instances in each batch of tasks; l, l': learning rates; Randomly initialize 0;\nrepeat\nif Lin > 0 then\nSample batch of tasks Ti ~ T.\nfor all i \u2208 [0, N) do\nEvaluate VOLT; (fo) with respect to bt examples;\nCompute adapted parameters with gradient descent 0\u2081 = 0-l. VoLT; (fo);\nend for\nUpdate 0 - 0 - l'. \u2207\u04e9 \u03a3\u03c4\u2081~T LT; (fo);\nLin -= 1;\nend if\nLout -= 1;\nuntil Lout == 0;\n3.1 Few-shot Learning for Functional Design (FuncDesign)\nWe introduce a task-specific meta-learning framework [5, 11]for the task distribution T, outlined in Algorithm 1. The UniProt-Net here is regarded as a function fo with an initial pretrained parameter Ometa involving linear classification layers. The primary objective of the meta-learner is to acquire updated parameters of Ometa containing high-order meta-information, facilitating swift adaptation to a new task drawn from T. Upon adapting the meta-learner to a new task Ti ~ T, the meta undergoes updates, transforming into the task-specific parameter \u03b8; through a few gradient descent iterations. These parameter updates, executed based on the support set Si for Ti, are referred to as inner loops. The number of inner loops is denoted as Lin. And loss function Imeta is utilized in the inner-loop updates as:\nLin-1\n(t)) s.t.\n(in) = Ometa \u2013 \u03a3 (.(t) Lt))\nt=0\nSi\n(t))\nL(t) L) = Bx Y\u00ea-S\u2081 (Imeta (foto) (ci|Yi; Xi), \u0109i),\n\u03a3 (l'ometa LQi) s.t.\nOmeta - Ometa - (l' \nabla_{\theta} \\sum_{\\langle X, Y \\hat{c}\\rangle \\in Q_i} I_{meta} (f_{\\theta^{(in)}} (c_i|Y_i; X_i), \\hat{c}_i)\n\u03a3yilog(pi),\n(8)\nwhere l' is the learning rate of the outer loop. X, Y represent protein pairs with \u0109 labels in Qi, similarly.\nThe meta-objective function Imeta varies across functional tasks. For instance, in binary categorization, it can be expressed as:\nImeta = -y(1-p)log(p) \u2013 (1 \u2013 y)plog(1 \u2013 p) ={-(1-p)log(p),  y = 1  -plog(1-p),  y = 0 (9)\nwhere y > 0 denotes an adjustable factor. While in a multi-classification task, it can be expressed as:\n\u2211yilog(pi), (10)\ni=1\nwhere K is the number of classification categories.\n3.2 Zero-shot Learning for Mutant Effect Prediction (MutDesign)\nMutation effect prediction is a non-parametric zero-shot learning in the NAR manner. Given the unique architecture of our MetaEnzyme, we introduce a novel mutation effect scoring method that decouples mutation scoring approaches, as illustrated in Figure 3. Formally, considering a wild-type enzyme structure Xwild \u2208 Rn\u00d73\u00d73 with its corresponding wild-type sequence Ywild \u2208 Rn, the non-parametric mutation effect prediction task aims to query the mutation-related ranking for the mutation sequence Zmut \u2208 R\", which is scored based on the marginal probabilities logits wild and the reference\""}, {"title": "Meta Pan-Enzyme Learning for Task-Adaptive Redesign", "content": "distribution logitsmut as:\np(Ywild||Zmut) = dmut (logits wild, LS(logitsmut)),\ns.t. logits wild = Puniprot (Xwild; Ywild) \u2208 Rn\u00d7M,\n= {Yo, Y1,, Yn}\nlogitsmut = OneHot(Zmut) \u2208 Rn\u00d7M,\n= {20, Z1,..., Zn} (11)\nwhere Smut is a distance measure between distributions of the wild-type and mutant. M=20 corresponds to 20 commonly used amino acid types. OneHot is one-hot encoding, with zik \u2208 {0,1} s.t. Zik ~ zi, 0 \u2264 k  14) where MutSet \u2190 (Ywild == Zmut) \u2208 Rn, \u2200MutSeti \u2208 {0, 1}, and MutSet implies all mutant positions. a = 0.5.\nFrom Eq. 11, we observe that: 1) the computation of wild-type and mutant proteins is independent; 2) deep network flow (such as UniProt-Net) predicts only the wild-type proteins, requiring a single inference due to the insight that mutant sequences stem from unique wild-type proteins through evolution or modification; 3) large-scale mutant sequences do not necessitate a network model, and they can be converted to a distribution representation based on one-hot encoding simply. This decoupling method provides a significant advantage in terms of speed, particularly for sequence datasets exceeding a million entries and for scenarios involving multi-site/higher-order mutations. Additionally, as the computation process relies entirely on the pre-trained parameters of UniProt-Net and makes no distribution assumptions about the query mutant enzymes, it is non-parametric. Section 4.3.2 presents detailed analysis.\n3.3 Autoregressive Generalization for Sequence Generation (SeqDesign)\nFor the conditional sequence generation, we shift the entire framework to an AR mode. Specifically, when provided with a candidate structure backbone Xc, the objective is to generate an unknown protein sequence Ysample as:\nn\u2211Puniprot(Yi Yi\u22121, \u00b7\u00b7\u00b7, Yo; Xc). (15)\ni=0\nThis density is represented using a Vanilla Transformer decoding paradigm. AR decoding typically conducts SeqDesign tasks for general proteins and functional enzymes."}, {"title": "4 IN SILICO EXPERIMENTS", "content": "4.1 Setups\nTraining Set. CATH dataset [20] is divided into training, validation, and testing sets, containing 18,204, 608, and 1,120 structure-sequence pairs, respectively. The training set is utilized to train the UniProt-Net in the pretraining stage.\nEvaluation Sets. FuncDBbi: Approximately 700 enzymes with experimentally determined structure-sequence pairs across 10 folds from RCSB are collected. Each fold is crafted with an equal number of enzymes and balanced with non-enzyme negative samples. This dataset is used for enzyme function prediction and design.\nFuncDBmulti is a subset of FuncDBbi dataset, comprising solely positive samples, for multi-class functional prediction tasks-precisely predicting the fold level to which an enzyme belongs.\nMutDBProtGym [40] is intricately designed for protein fitness, encompassing an expansive collection of over 217 deep mutational scanning assays and millions of mutant sequences.\nSeqDesignDBPet comprises 178 proteins identified for the capability to degrade plastics [12]. SeqDesignDBHybrid is the alias of FuncDBmulti for differentiation. SeqDesignDBALL is extracted from the testing set of CATH. SeqDesignDBTs50 and SeqDesignDBTs500 correspond to Ts50 and Ts500 [30] for validating generalization.\nImplementation. The AdamW optimizer with a batch size of 5 and a learning rate of 1e-3 to train the UniProt-Net. The GeoStruc-Encoder consists of 4 layers with a dropout of 0.1. The node hidden dimensions of scalars and vectors are 1024 and 256. The adapter and decoder have 8 multi-head self-attention layers, with an embedding dimension of 512 and an attention dropout of 0.1. For meta-learning configures, the number of inner loops is set to 5, and the batch sizes of the support set and query set each are both 10. The updated learning rates l' and I are set to 1e-3, and the Adam optimizer is used. The parameters of EnAd-Module are fixed. And 1 NVIDIA A100 80GB GPU is used. The code is available at https://github.com/binbinjiang/MetaEnzyme.\n4.2 Few-shot Learning for Function Prediction (FuncDesign)\n4.2.1 Fold-independent Prediction. Meta Learning for Fold independent prediction aims to predict unseen enzyme fold classes. Based on FuncDBi, data from 9 folds are used as the training set, leaving one fold as the evaluation. Due to the limited samples in each fold and their mutual independence, we treat each fold as an independent task, leading to the application of meta-learning for task-based learning. 'Fold' here denotes the three-dimensional configuration of secondary structural elements, such as alpha helices and beta sheets, that define a specific protein or protein group. Proteins sharing similar folds usually exhibit substantial structural resemblances, despite variations in their sequences and functions. Classifying folds can provide valuable insights into the evolutionary connections between proteins. As shown in the table 1, Ours(M) involves meta-leaner as a base, followed by meta-finetuning for further refinement. And Ours(Z) directly employs zero-shot learning for trained meta-learner. Ours(M) yields superior results, highlighting the significant improvement brought about by meta-finetuning.\n4.2.2 Fold-agnostic Prediction. Ours(F) serves as a control group, primarily comparing few-shot learning with conventional finetuning methods. It represents fold-agnostic prediction, excluding meta-learning learning. We combine positive and negative samples from 9 folds, randomly allocating 80% as the training set and 20% as the evaluation set, treating the remaining 1 fold as an out-of-distribution generalization. For unseen folds, we observed that task-specific learning based on meta-learning is more effective with the average value of Ours(M) at 0.903 exceeding that of Ours(F) at 0.857. Additionally, ESM-IF, acting as a baseline due to its outstanding representation and similar modalities, clearly demonstrates the superiority of our model in terms of generalization.\n4.2.3 Enzyme Fold Recognition. Furthermore, we conduct predictions on more challenging fold types based on FuncDBmulti, essentially involving a less data-intensive multi-task enzyme function prediction, as shown in Figure 4. This setup aims to predict, within enzymes with mixed folds, the specific fold to which an enzyme belongs. We select ESM-IF and GVP as structure-to-sequence models as baselines, as they also utilize N, Ca, and C as standard inputs. These comparisons validate the superior structural representation capabilities by a large margin (Ours: 17.3%P@1,31.7%P@2,41.0%P@3). We employ ESM-2 as a language modality input for comparison to confirm its sequence-only representation capabilities. Although ESM-2 (11.5%P@1,24.5%P@2,36.5%P@3) is a little better than the earlier GVP, but is far below the performance of our models, which demonstrates enhanced generalization ability due to the incorporation of structural information.\nOverall, while we have undertaken initial explorations in the few-shot setting, the overall accuracy remains relatively low. This is attributed to limitations in the quantity of available data and the diversity of fold types. This compels us to collect data encompassing a broader range of categories.\n4.3 Non-parametric Zero-shot Learning for Mutation Effects (MutDesign)\n4.3.1 Mutant Effect Prediction. To ensure a comprehensive and unbiased comparison, we selected prominent protein language models and inverse folding models as benchmarks, as illustrated in Figure 5 (a). All baseline models employ zero-shot learning for mutation effects evaluation through the MutDBprotGym, which encompasses millions of mutations. Note that our model consists of two configurations: Ours(w/o finetune) directly infers based on UniProt-Net, yielding an average p of 43.9%; And Ours(w/ finetune) utilizes the meta-finetuned UniProt-Net, resulting in an average p of 44.5%, which consistently achieves the best matching rank, implying that the in-domain finetuning on enzyme data contributes to enhanced overall performance in mutation prediction. In contrast, the optimal protein language model (VESPA)[35] achieved an average p of 43.7%, while the optimal inverse folding model (ESM-IF) scored 42.2%. This performance superiority can be attributed to our model's dual advantage, encompassing both contextual and structural transfer learning within the proposed training paradigm.\n4.3.2 Complexity of Decoupled Scoring Mechanism. The complexity analysis was carried out utilizing a single NVIDIA A100 80GB GPU, as outlined in Table 2. We utilize the widely accepted fitness scoring metric, as referenced in ESM-1v [36], as our baseline. By maintaining a fixed batch size of 1 and an average protein length of 500 amino acids, we vary the number of mutant proteins from 500 to 5000. Space complexity is governed by the number of parameters, where both the baseline and our decoupling method exhibit relatively small parameter counts (124.82M). Regarding time complexity, reflected through Throughputs and FLOPs, both the baseline and our method showcase nearly identical FLOPs (52.7k). As we increment the average number of mutant proteins from 500 to 5000, our decoupling method's advantage further amplifies, although this ascent in advantage slows down due to hardware memory limitations. Our approach requires only one inference step for the wild-type protein to acquire its probability distribution, subsequently facilitating swift calculation of mutant differences between mutant sequences and the inferred wild-type distributions. This significantly expedites the process. In contrast, the baseline necessitates model inference for each mutant protein, resulting in reduced speed. Overall, our model demonstrates markedly enhanced throughput performance, especially noticeable under conditions involving a substantial number of mutations. Compared to the mainstream baseline scoring method, ours boasts a speed advantage ranging from 4k to 30k times faster. This presents a promising avenue for future research, particularly in multi-site/high-order mutation inference, thereby expanding the possibilities for exhaustive exploration.\n4.4 Parametric Conditional Protein Generation (SeqDesign)\nSwitching to the NAR mode and coordinating with the lower triangular mechanism, MetaEnzyme is employed for SeqDesign. The amino acid recovery (AAR) metric is adopted.\n4.4.1 Internal Validation of General Proteins. Figure 6 shows SeqDesign comparisons at general proteins, with detailed benchmarks in Appendix.\nIn-domain Protein Assessment. In the evaluation of the comprehensive SeqDesignDBALL (\u2018ALL'), our MetaEnzyme demonstrates superior performances, achieving an impressive AAR score of 54.94%. This surpasses the mainstream ProteinMPNN, PiFold, and ESM-IF consistently by a substantial margin. In alignment with [20], we further assess specific subsets within 'ALL', namely the 'Short' dataset (comprising protein sequences with a length \u2264 100 residues) and the 'Single-chain' dataset (consisting of single-chain proteins cataloged in the Protein Data Bank). Notably, MetaEnzyme exhibits outstanding performance on both 'Single-chain' (39.17%) and 'Short' datasets (40.92%) compared to alternative methods.\n4.4.2 Generalization on Functional Enzymes. Conditional SeqDesign for PET Enzymes. Polyethylene terephthalate (PET) is a widely used synthetic plastic polymer globally, known for its chemical inertness due to ester bonds and aromatic nuclei. This makes PET resistant to degradation, raising environmental concerns. Enzymatic degradation offers a promising and eco-friendly solution to address the ecological challenge of plastic waste, particularly polyester waste recycling. Despite the potential, our understanding of PET-degrading enzymes is limited. Machine learning-aid techniques [9, 33, 37] might accelerate the discovery of PET hydrolases. To support this effort, we curated the SeqDesignDBpet dataset for SeqDesign analysis, serving as a valuable resource. Figure 7(PET) illustrates AAR scores for PET hydrolases, showing performance improvements up to 64.35% over baselines (62.56% ESM-IF, 45.40% GVP). This suggests strong generalization capabilities for unseen functional enzymes, paving the way for future analysis and PET-enzyme redesign.\nConditional SeqDesign for Fold-aware Enzymes. In the examination of enzyme performance across diverse fold levels within the SeqDesign task, we systematically categorized them into 10 distinct classes utilizing SeqDesignDBHybrid, as presented in Figure 7(HybridEnz). When compared to alternative SeqDesign models, our attained AAR performance of 61.83% consistently outperforms the baseline scores (60.31% and 44.68%). This noteworthy result underscores a persistently elevated overall performance across a spectrum of hybrid fold types, emphasizing the robustness and effectiveness of our SeqDesign model.\n4.5 Initialization Analysis and Ablation Study\nTo underscore the significance of pretrained modules, we conducted an ablation study to assess the impact of the pretrained EnAd-Module (PEM) and the pretrained Context-Module (PCM). The results, outlined in Table 3, highlight their substantial contributions to overall improvement. Specifically, when comparing #1 vs. #3 and #1 vs. #2, it is evident that the pretrained structural features play a pivotal role in boosting performance. This aligns seamlessly with our rationale for incorporating prior structural knowledge. Moreover, given that the PCM module operates downstream of the PSM, it is likely to directly benefit from the representation capabilities of PSM. Surprisingly, in the isolated comparison of #1 vs. #2, significant performance is enhanced even when utilizing only the PCM without the initialization parameters of the PEM. This intriguing finding, previously unexplored in SeqDesign studies, suggests that the PCM has acquired substantial knowledge of the protein language, thereby contributing significantly to improved SeqDesign performance. This observation gains further support in the comparison between #3 and #4, solidifying the conclusion that the PCM's acquired knowledge plays a vital role, diminishing the reliance on initialization parameters. The comprehensive analysis presented here highlights the logical and noteworthy advancements brought about by these pretrained modules in protein design."}, {"title": "5 IN VITRO WET EXPERIMENTAL VALIDATION", "content": "To further validate the reliability, we selected a commercially relevant enzyme with potential applications in production, the reversible glycine cleavage system (rGCS) [32, 43, 55], for wet lab experimentation. rGCS, known for efficiently fixing carbon to produce glycine in vitro, currently exhibits relatively low yields in glycine production. Traditional rational enzyme design for rGCS has shown limited improvements in catalytic activity and other attributes. The rGCS, consisting of three proteins (P-protein decarboxylase, T-protein aminomethyltransferase, and H-protein shuttle), can fix two different one-carbon carbon sources to synthesize the two-carbon compound glycine. We focus on P-protein redesign in this work. For details on the structure and mutation preparation processes, please refer to Appendix.\nUtilizing rational design principles, we strategically chose mutation sites with anticipated substantial influence on mutation effects. Wet lab experiments assessing carbon fixation were conducted for approximately 40 guided mutations, and their effects are illustrated in Figure 8. The wet lab outcomes exhibit close alignment with in silico predictions, demonstrating a notable Spearman's p ranking correlation of 70.1%. These results reinforce the practical efficacy of MetaEnzyme, pinpointing crucial sites for subsequent iterations in enhancing the P-protein's carbon-fixing capabilities."}, {"title": "6 RELATED WORK", "content": "In the dynamic landscape of protein design [18, 19, 53, 59, 60], AI-driven approaches, exemplified by inverse folding [2, 15, 42, 57], have made significant strides. Tools such as Structured Transformer [20] and GVP-GNN [23] have pioneered conditional protein generation, while recent models like ProteinMPNN [8], PiFold [8], and ESM-IF [17] showcase advancements in sequence recovery. AlphaFold2 [24], RosettaFold [3], OmegaFold [52], helixfold [10], ESMFold [31] stand out as influential structure prediction models. In the niche domain of enzyme engineering [4, 16, 25-29, 44, 47], language and structure models such as DeepSequence [48],ESM-1v [36, 45, 46], Tranception [39], ESM-2 [31] employ few/zero-shot learning for mutation fitness. Our work extends these innovations, leveraging learned structural and linguistic insights, to encompass comprehensive design tasks, addressing challenges in general proteins and functional enzymes. Notably, the field grapples with the need for broader generalization across diverse proteins and presents opportunities for advancing functionality prediction."}, {"title": "7 LIMITATIONS AND CONCLUSIONS", "content": "This work presents a pioneering MetaEnzyme framework tailored for crucial enzyme tasks, unveiling an architectural breakthrough with far-reaching implications across diverse protein engineering applications. MetaEnzyme underscores the prospect of transitioning from a universal to a unified enzyme design, enabling seamless adaptation across various functionalities through straightforward architectural modifications or lightweight adjustments. Despite the resource-intensive wet lab validation in this study, certain limitations are acknowledged: 1) restricted datasets of functional enzymes; 2) the necessity for more robust models to enhance generalization; and 3) a desire for additional wet lab experiments. Overcoming these challenges is crucial for advancing the field, and bolstering the applicability of enzyme design models."}, {"title": "A GENERAL PROTEINS FOR SEQDESIGN TASK", "content": "A.1 Baselines\nThe histogram depicted in Figure 5 within the main body, along with the comprehensive information provided in Table 4, showcases a selection of noteworthy works in the field. These works, predominantly open-source and easily reproducible, offer valuable insights into computational enzyme design. Notably, studies falling into Group 1 and Group 3 of Table 4 have been trained on a relatively modest-scale CATH training set, comprising approximately 18k training pairs. Despite the modest training data, earlier benchmarks such as Structured Transformer [20] and GVP-GNN [23] remain influential, boasting competitive performances and lightweight architectures. Additionally, ProteinMPNN [8] by Baker's group has garnered attention for its advancements in performance and speed, backed by impressive biological validation experiments. PiFold [14] follows suit with further efficiency and effectiveness enhancements. Noteworthy among the selections in Group 2 is ESM-IF [17], distinguished by its robust open-source nature and robust data augmentation strategies, trained on a vast dataset of approximately 12M training pairs sourced from AlphaFoldDB [50]. It's worth mentioning that our MetaEnzyme architectures share similarities with ESM-IF, hence we choose ESM-IF as a primary baseline for comparison in our study.\nA.2 Detailed Comparison for General Protein Sequence Design\nComparison of the CATH, Ts50, and Ts500 datasets using perplexity and AAR metrics, as shown in Table 4."}, {"title": "B DETAILS OF IN VITRO WET EXPERIMENTS", "content": "A schematic representation of the structure of the reversible glycine cleavage system (rGCS) [32, 43, 55"}]}