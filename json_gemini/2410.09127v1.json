{"title": "CYCLE: Cross-Year Contrastive Learning in Entity-Linking", "authors": ["Pengyu Zhang", "Congfeng Cao", "Klim Zaporojets", "Paul Groth"], "abstract": "Knowledge graphs constantly evolve with new entities emerging, existing definitions being revised, and entity relationships changing. These changes lead to temporal degradation in entity linking models, characterized as a decline in model performance over time. To address this issue, we propose leveraging graph relationships to aggregate information from neighboring entities across different time periods. This approach enhances the ability to distinguish similar entities over time, thereby minimizing the impact of temporal degradation. We introduce CYCLE: Cross-Year Contrastive Learning for Entity-Linking. This model employs a novel graph contrastive learning method to tackle temporal performance degradation in entity linking tasks. Our contrastive learning method treats newly added graph relationships as positive samples and newly removed ones as negative samples. This approach helps our model effectively prevent temporal degradation, achieving a 13.90% performance improvement over the state-of-the-art from 2023 when the time gap is one year, and a 17.79% improvement as the gap expands to three years. Further analysis shows that CYCLE is particularly robust for low-degree entities, which are less resistant to temporal degradation due to their sparse connectivity, making them particularly suitable for our method. The code and data are made available at https://github.com/pengyu-zhang/CYCLE-Cross-Year-Contrastive-Learning-in-Entity-Linking.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) are multi-relational graphs representing a wide range of real-world entities and knowledge structured as facts [5]. In KGs, facts are represented as triples, denoted as (head entity, relation, tail entity). KGs like ICEWS [3], GDELT [10], and YAGO3 [15] not only contain numerous entities and their relations but also incorporate timestamps, allowing to track the evolution of the knowledge in these KGs. Each of these timestamps is part of a quadruplet (head entity, relation, tail entity, timestamp), reflecting the point in time a particular relation between head and tail entities was created. Such temporal features enable KG users to track historical data trends, understand entity behavior over time, and predict future events or facts, highly relevant in domains such as medical and risk analysis systems [6], question-answering systems [21], and recommendation systems [2]. However, the continuous emergence of new entities and changes to existing ones, poses challenges in adapting entity representations, which in turn affects the performance on tasks such as Entity Linking (EL) [17].\nEL involves mapping mentions in text to entities in a KG. In the example in Figure 1, EL maps the mention 'Amazon' in the text, with its correct entity Amazon (company) in the KG. The term 'Amazon' could refer to Amazon (company), known for e-commerce, or Amazon rainforest. The correct entity is decided by analyzing the content of the context. If the context includes cloud computing, it likely refers to Amazon (company). Beyond the challenge of disambiguating the correct entity given the mention and its context, this task becomes even more difficult in the face of the constant addition of new entities in a KG and the evolving meanings of existing entities (continual entities). This change leads to temporal degradation a decline in model performance as the KG moves from the state where the EL model was initially trained.\nIn this paper, we develop a new model for EL that deals with temporal change. For instance, as shown in Figure 2, in 2019, the primary sources of income for Amazon (company) were e-book reader and bookstore. By 2022, however, Amazon (company) had established new connections, and recently added entities like digital streaming and cloud computing had become major contributors to its income. Concentrating on these new connections allows for a deeper understanding of the current state of the entity and its context. Although these changes might appear minor, they substantially affect the model's ability to represent entities accurately.\nTo address this challenge, a benchmark introduced by [8] leverages the differences between consecutive snapshots of Wikipedia and Wikidata to provide a platform for testing and training language models, enabling them to adapt to and update continually changing knowledge information. Furthermore, recent work has focused on reducing bias through graph contrastive learning [27]. In addition, recent studies like [38] on self-supervised biomedical EL and [26] on multimodal EL with contrastive learning have advanced the field by improving accuracy. However, existing methods do not leverage the temporal evolution of structured relations between entities in a KG across different years. To tackle this research gap, we introduce an expanded version of the TempEL [36] dataset named GCL-TempEL (see Section 4). The original TempEL dataset consists of 10 yearly snapshots, evenly distributed, from English Wikipedia entities, spanning from January 1, 2013, to January 1, 2022. Building upon this, we have incorporated the relationships between entities for each year from the KG introduced in the Wikidata5M [28] dataset, and the changes in these relationships over time.\nWe hypothesize that the evolution of KG relationships across different years in our newly introduced dataset, can provide crucial information about the changes in the entities (see Figure 2). To support this, we introduce CYCLE: Cross-Year Contrastive Learning in Entity-Linking. CYCLE is a novel approach to solve EL task in temporally evolving setting based on graph contrastive learning, leveraging the features of temporal data to construct a cross-year contrastive mechanism. Doing so ensures that similar entity representations remain distinct over time. Our contrastive learning approach uses KG relationships to obtain structurally enhanced entity representations. We define such positive and negative samples as follows:\n1 Positive samples: sampled from a pool of newly added relationships between the target entity and its neighbors.\n2 Negative samples: sampled from a pool of newly deleted relationships between the target entity and its neighbors.\nOur experimental results demonstrate that our approach can be particularly advantageous in updating representations of less frequent, long tail, entities. Such entities have lower degree connectivity in a KG and, as a result, are inherently more vulnerable to the effects of temporal changes in their neighbors. Concretely, a change in the meaning of even one neighboring node can significantly impact the low-degree node's representation. This phenomenon has been also observed in other large-scale KGs such as ICEWS and GDELT [25, 30, 31].\nOur contributions are summarized as follows:\n\u2022 A dataset, GCL-TempEL, that incorporate cross-year KG temporal tracking of entity changes. Concretely, we define positive and negative samples with respect to a specific temporal snapshot.\n\u2022 CYCLE: a novel model employing graph contrastive learning to mitigate temporal degradation in EL. CYCLE enhances"}, {"title": "2 Related Work", "content": "2.1 Entity Linking\nGenerally, the Entity Linking (EL) task is categorized into three main phases: mention detection, candidate generation, and candidate ranking. Recent studies have developed end-to-end models that integrate all three phases into a single process [4, 16, 19]. Specifically, the CHOLAN model [19] proposed two transformer-based models integrated sequentially to tackle the EL task. The first transformer identifies entity mentions in the text, while the second assigns each mention to a predefined candidate, enhanced by contextual data from the sentence and Wikipedia. As the field evolved, addressing the linking of mentions to previously unseen entities a scenario termed zero-shot EL - remained a significant challenge [20]. To tackle this, [32] introduce BLINK, a highly effective two-stage BERT-based architecture for zero-shot entity linking. This model and others [1, 20, 33] rely on traditional candidate retrieval methods and employ a cross-encoder for candidate ranking. Building on the BLINK model, KG-ZESHEL [20] aims to combine graph vectors with textual content to address the zero-shot problem. Their method enhances the model's ability to clarify ambiguities and improve EL accuracy.\nHowever, the above work did not capture the impact of changes in entity relationships on future predictions when faced with dynamic KGs. To tackle this issue, our study introduces a cross-year contrastive mechanism to capture KG relationship changes across the years, thereby improving the performance on EL task in a temporally evolving setting.\n2.2 Temporal Knowledge Graphs\nTemporal Knowledge Graphs (TKGs), capturing the dynamic evolution of entities and their relationships over time, are gaining increased attention [25]. While most of the existing graph datasets are designed for static graphs, TKGs stand out for their ability to chronicle the continuous evolution of both entities and relations.\nThe application of TKGs spans various sectors, each benefiting from its capacity to track changes over time. For e-commerce, TKGs enhance understanding of consumer behavior patterns over time [39]. Similarly, the Internet of Things (IoT) provides a dynamic framework for interpreting evolving data from interconnected devices [12]. Healthcare applications of TKGs are particularly noteworthy, as they aid in tracking the progression of diseases and patient health trends [6]. In industrial settings, TKGs play a pivotal role in monitoring and predicting machinery's lifecycle and maintenance needs. Recent advances in large language models (LLMs) have further expanded the potential of TKGs, particularly in forecasting applications [13]. These architectures offer new ways to comprehend structured temporal data, potentially revolutionizing traditional embedding-based and rule-based applications using TKGs. Moreover, integrating temporal information into KG embedding has significantly improved model performance, underscoring the importance of time-aware approaches in knowledge representation [7, 11].\nHowever, these studies overlook the challenges of low-degree nodes with very sparse connectivity, which can significantly impact the accuracy and robustness of temporal predictions. To address this, our model employs graph contrastive learning on positive and negative samples (see Figure 2) to enhance the neighbor information for low-degree nodes, thereby improving their representation in the graph.\n2.3 Graph Contrastive Learning\nRecent Graph Contrastive Learning (GCL) advancements highlight its growing significance in graph representation learning. PyGCL [40] emphasizes the importance of design elements like augmentation functions and contrasting modes. GraphCL [34] stands out for its ability to learn robust representations from unlabeled graphs, though its effectiveness relies on specific data augmentation strategies. POT-GCL [35] addresses the need to maximize similarity between positive node pairs and minimize it for negative pairs, while recognizing unresolved issues due to complex graph structures. In contrast, SGCL [24] demonstrates the low impact of negative samples for achieving top performance. Finally, EdgePruner [9] exposes GCL's vulnerability to poisoning attacks, suggesting a need for better defense mechanisms in graph learning models. These studies mark a significant evolution in GCL, indicating its potential and challenges in graph representation learning.\nOur work expands the application of contrastive learning techniques within KGs to include temporal evolution. Specifically, we have developed a new method that considers the impact of both newly added and removed node relationships at each timestamp."}, {"title": "3 Task Formulation and Definition", "content": "Entity Linking (EL). The EL task takes a given text document D as input. This document is represented as a list of tokens $[w_1,..., w_r]$, where $r$ indicates the length of the document. Each document contains a list of entity mentions $M_p = [m_1,..., m_n]$, where each mention $m_i$ corresponds to a span of continuous tokens in D, represented as $m_i = D[x, y]$. An EL model subsequently yields a list of mention-entity pairs $\\{(m_i, e_i)\\}_{i \\in [1,n]}$. Every entity $e_i$ is mapped to the corresponding entity in a Knowledge Base, such as Wikipedia. It is assumed that both the title and description of these entities are available, a standard premise in the EL task [14].\nGraph. A graph is defined as $G = (V, E)$, where $V$ is the set of N nodes (i.e., entities) $\\{v_1, v_2, ..., v_N\\}$. $E$ is the set of M edges (i.e., relationships) represented as $\\{e_1, e_2, ..., e_M\\}$, where each $e_i$ is a pair of nodes from V, such as $e_i = (v_a, v_b)$."}, {"title": "4 Dataset Construction", "content": "We extend the TempEL dataset, a benchmark for temporal Entity Linking (EL), with Wikidata5M. The resulting GCL-TempEL dataset comprises two text-based components inherited from TempEL, namely entity description and mention context, extended with three graph-based components: relation graph, feature graph, and feature matrix. Additionally, it includes both positive and negative samples. Both the relation and feature graphs depict yearly entity relationships. The construction process is illustrated in Figure 3, and it introduces the following components:\nEntity descriptions and mention contexts. First, we categorized each year of data from the TempEL dataset into entity descriptions and mention context parts based on the year. The entity description comprises the title, text, document ID, and the unique ID of the entity (its QID). The mention context consists of the text surrounding the entity mention (to the left and to the right), the mention itself, target entity as label, QID, and category. Furthermore, we categorized the mentions into two groups: those linked to continual entities, which exist across all the years in GCL-TempEL, and those linked to new entities, which are created in a specific year and do not exist in previous years.\nRelation graph. We create a relation graph based on the Knowledge Graph (KG) relationships in the Wikidata5M dataset and the entity IDs in the TempEL dataset. We matched the entities involved in the relationships in Wikidata5M with the ones that exist in the TempEL dataset. Concretely, we keep a relationship if both entities are involved in a relationship in the Wikidata5M data and are also present in TempEL. The relation graph is an $n \\times n$ adjacency matrix, where n represents the total number of entities in the GCL-TempEL dataset. Each row indicates whether an entity has a connection with another entity. Concretely, the adjacency matrix is made up of 0s and 1s. If entity i and entity j are connected, the value in the ith row and jth column of the matrix is 1; otherwise, it is 0. After we construct the relation graph, we use the differences in node relationships across the years (cross-year changes in Figure 3) to construct positive and negative samples, which are defined as follows:\n1 Positive samples: sampled from a pool of newly added relationships between the target entity and its neighbors.\n2 Negative samples: sampled from a pool of newly deleted relationships between the target entity and its neighbors.\nFeature graph. KGs are inherently incomplete and sparse [18]. As a result, we consider that relation graph derived from Wikidata5M KG above does not contain all the possible relations between entities, which limits its expressiveness. To address this, we introduce feature graph which extends the edges in the relation graph with additional edges given by k-nearest neighbors (k-NN) with other entities. In order to create this graph, we use pre-trained bert-base-uncased model to embed the textual description of each of the entities. We use the resulting description embeddings to identify the k-NN entities for each of the target entities using cosine similarity. The resulting feature graph highlights the connections between entities based on their entity descriptions. We hypothesize that such connections will provide additional information that would allow to generate more representative entity embeddings for a given temporal snapshot. Similarly to the relation graph, the feature graph is represented as $n \\times n$ adjacency matrix, where n is the total number of entities in the dataset. Each row indicates whether an entity has a connection with other entities. If entity i and entity j are connected, the value in the ith row and jth column of the matrix is 1; otherwise, it is 0.\nFeature matrix. Building on previous research [29], we developed a Feature matrix to derive more representative entity embeddings based on their descriptions in the dataset. This matrix not only provides a robust representation of entities but also improves the graph aggregation process, enabling the generation of more nuanced embeddings. The goal of the Feature matrix is to represent each entity based on the tokens from entity descriptions in the dataset. After obtaining the token IDs for each entity using the pre-trained bert-base-uncased model, we filtered all token IDs based on their frequency of occurrence (see token frequency list in Figure 3). We retained those token IDs that appeared between 46 and 200 times. We discarded highly frequent token IDs since these tokens, such as 'is,' 'an,' 'the, and other common words, do not offer meaningful differentiation among entities. Also, the less frequent token IDs were removed due to the possibility of them being meaningless noise or random codes, and including an excess of these rare tokens would make the matrix too sparse, slowing down computation. The resulting feature matrix is an $n \\times m$ dimensional adjacency matrix, where n is the total number of entities in the dataset, and m is the number of retained token IDs. This matrix is composed of 1s and 0s, if the data in the ith row and jth column of the matrix is 1, it indicates that entity i contains the jth token."}, {"title": "5 Approach", "content": "Figure 4 illustrates the framework of our model. The core idea is to exploit the relationships in graph-based input between entities across different years in the dataset. These relationships are contained in relation graph ($G_r$), feature graph ($G_f$) and feature matrix (X) (see Section 4 for details). When Knowledge Graph (KG) entities appear or disappear at a specific point in time, they can be used as positive and negative samples, respectively. Once these positive and negative samples are identified across different years in our GCL-TempEL dataset, we employ them in graph contrastive learning module. This module employs contrastive learning loss to efficiently adapt entity embeddings to temporal changes. Besides using the graph-based input X, $G_r$, $G_f$, CYCLE also leverages textual information (see left part of Figure 4) composed of mention contexts and entity descriptions.\nFirst, the bi-encoder module in Section 5.1 employs two separate BERT transformers to transform mention context and entity description into dense vectors $y_m$ and $y_e$. Entity candidates are scored via the dot product of these vectors. We introduce $L_e$ to maximize the correct entity's score against randomly sampled entities. Second, we input the pre-constructed relation graph $G_r$, feature graph $G_f$, feature matrix X, and cross-year positive and negative samples into the Graph Embedding Module in Section 5.2. Our model encodes entities from relation and feature graphs and uses graph contrastive learning losses $L_f$ and $L_r$ across different years. Lastly, all the loss functions are unified for joint optimization.\n5.1 Bi-encoder Module\nMention Representation. Following [32], the mention representation $t_m$ is constructed from tokens of the surrounding context and the mention:\n$t_m = [CLS] \\; ctxt_l \\; [M_s] \\; mention \\; [M_e] \\; ctxt_r \\; [SEP]$, (1)\nwhere $ctxt_l$, $ctxt_r$ denote tokens before and after the mention, and $[M_s]$, $[M_e]$ tag the mention. The input's maximum length is set to 128, consistent with the baseline.\nEntity Representation. The representation $t_e$ consists of tokens of the entity title and its description:\n$t_e = [CLS] \\; title \\; [ENT] \\; description \\; [SEP]$, (2)\nwhere [ENT] separates the title and description.\nEncoding. We use the bi-encoder architecture from [32] to encode descriptions into the vectors $y_e$ and $y_m$:\n$y_m = red(T_1(t_m))$, (3)\n$y_e = red(T_2(t_e))$, (4)\nwhere, $T_1$ and $T_2$ are transformers, and $red(.)$ takes the last layer of the output of the [CLS] token to reduce the sequence of vectors into a single vector.\nScoring. Entity candidate scores are computed via dot-product:\n$s(m, e_i) = y_m \\cdot y_{e_i}$ (5)\n5.2 Graph Embedding Module\nWe aim to enable the learning connections between nodes from the relation graph $G_r$. By inputting the relation graph $G_r$ and node features X, we obtain specific embeddings, denoted as $z_r$.\nIn the relation graph, we consider the target node $e_i$ that is connected to $s$ other nodes $\\{n_1, n_2,..., n_s\\}$. Thus, the set of neighbors for node $e_i$ can be defined as N. For node $e_i$, each neighbor contributes differently to its embedding. To effectively integrate these contributions, we employ an attention mechanism to aggregate messages from the neighbors to the target node $e_i$:\n$x'_i = \\sigma (\\sum_{j \\in N_i} a_{i,j} x_j)$, (6)\nwhere $\\sigma$ is a nonlinear activation, $x_j$ is the feature of node $e_j$, and $a_{i,j}$ denotes the attention value of node j to node $e_i$. It is calculated as follows:\n$a_{i,j} = \\frac{exp(LeakyReLU(\\overline{a} \\cdot [x_i||x_j]))}{\\sum_{l \\in N_i} exp(LeakyReLU(\\overline{a} \\cdot [x_i||x_l]))}$, (7)\nwhere $\\overline{a}$ is the attention vector and $||$ denotes concatenate operation. Following [29], we randomly sample a subset of neighbors in $N_i$ during each epoch. If the number of neighbors exceeds a predefined threshold, we sample neighbors as $\\overline{N}_i$. If the predefined threshold number of positive samples cannot be found, all the positive samples will be selected. In this case, the number of positive samples will be less than the predefined threshold. This way, we ensure that every node aggregates the same amount of information from neighbors, and promote diversity of embeddings in each epoch.\nWhen using the feature graph $G_f$ and node features X as inputs, the embeddings are denoted as $\\overline{z}_r$.\n5.3 Cross-year Contrastive Module\nThe input feature graph embedding $\\overline{z}_r$ and relation graph embedding $z_r$ are passed through a multi-layer perceptron with a single hidden layer. This process maps the features into a representation space where the contrastive loss is calculated:\n$z_i^{f^P} = W^{(2)} (\\sigma (W^{(1)} z + b^{(1)}) + b^{(2)}$, (8)\n$z_i^{r^P} = W^{(2)} (\\sigma (W^{(1)} z + b^{(1)}) + b^{(2)}$, (9)\nwhere, $\\sigma$ represents exponential linear unit, a type of non-linear activation function. The parameter sets $W^{(2)}$, $W^{(1)}$, $b^{(2)}$, $b^{(1)}$ are shared across the input graphs for embedding consistency.\nTo define the positive $P_i$ and negative $N_i$ samples for node $e_i$ in relation graph $G_r$ at time $t_2$:\n$P(t_2) = \\{j | (i, j) \\notin E_{t_1} \\land (i, j) \\in E_{t_2}\\}$, (10)\n$N(t_2) = \\{j | (i, j) \\in E_{t_1} \\land (i, j) \\notin E_{t_2}\\}$, (11)\nhere, $E_{t_1}$ and $E_{t_2}$ denote the edge sets at times $t_1$ and $t_2$, respectively. $(i, j) \\notin E_{t_1}$ indicates that there was no edge between nodes $e_i$ and $e_j$ at $t_1$, whereas $(i, j) \\in E_{t_2}$ indicates that an edge between nodes $e_i$ and $e_j$ exists at time $t_2$. The symbol $\\land$ is used to denote the logical AND operation.\nIn a feature graph $G_f$, positive samples $P_i^f$ includes all neighboring nodes directly connected to $e_i$, while the set of negative samples $N_i^f$ includes all nodes that are not connected to $e_i$.\n5.4 Objective Function\nThe objective function is calculated based on three components: two contrastive loss functions $L_f$ and $L_r$, and a task-specific EL loss function $L_e$ defined below.\nEntity Linking Loss Function $L_e$. The objective is to train the network such that it maximizes the score of the correct entity compared to the other entities from the same batch. Formally, for each training pair $(m_i, e_i)$ within a batch of N pairs, the loss $L_e$ is defined as:\n$L_e (m_i, e_i) = -s(m_i, e_i) + log \\sum_{j=1}^{N} exp(s(m_i, e_j))$, (12)\nContrastive Learning Loss Functions $L_f$ and $L_r$. The contrastive loss function $L_f$ and $L_r$ for a given set of positive ($P_i$) and negative ($N_i$) samples. The purpose of $L_f$ is to compute the contrastive loss where the embedding of node $e_i$ is from graph $G_f$, while the positive and negative samples are from graph $G_r$. Conversely, $L_r$ computes the contrastive loss where the embedding of node $e_i$ is from graph $G_r$, and the positive and negative samples are from graph $G_f$. This approach maximizes the advantages of contrastive learning by comparing node embeddings from different graphs and different sets of positive and negative samples, thereby capturing complex structural and semantic information from the graphs:\n$L_f = -log \\frac{exp(\\frac{sim(z_i^f, z_k^{r^P})}{\\tau})}{\\sum_{k \\in \\{P_i \\cup N_i\\}} exp(\\frac{sim(z_i^f, z_k^{r^P})}{\\tau})}$, (13)\n$L_r = -log \\frac{exp(\\frac{sim(z_i^r, z_k^{f^P})}{\\tau})}{\\sum_{k \\in \\{P_i^f \\cup N_i^f\\}} exp(\\frac{sim(z_i^r, z_k^{f^P})}{\\tau})}$, (14)\nwhere the function sim computes cosine similarity between two vectors, and the temperature parameter $\\tau$ helps prevent the model from getting stuck in local optima during training. $P_i$ and $P_i^f$ are the sets of positive samples for node $e_i$, and $N_i$ and $N_i^f$ are the sets of negative samples for node $e_i$.\nOverall Objective Function. The final objective function is a weighted sum of the individual $L_e$, $L_r$, and $L_f$ loss functions calculated above:\n$L = aL_e + bL_f + cL_r$, (15)\nwhere a, b and c are weights for the three losses defined above."}, {"title": "6 Evaluation", "content": "In this section, we evaluate the performance of the proposed model across three Entity Linking (EL) datasets. The implementation of our approach is based on the original codebase BLINK and HeCo. We selected BLINK and SpEL because of their relevance and performance benchmarks in the field. BLINK has excellent scalability and serves as part of our model's codebase. Furthermore, SpEL is the latest state-of-the-art as of 2023. Comparison with these models highlights the influence of integrating additional structured graph-based input and conducting graph contrastive learning on node embeddings in CYCLE to mitigate temporal performance degradation. Experimental details can be found in [37].\n6.1 Datasets\nOur proposed model is evaluated on three datasets which are summarized in Table 1.\n6.2 Training Details\nWe compare our approach to the zero-shot EL BLINK model [32]. Concretely, we reuse the same hyperparameter settings and the same bert_uncased_L-8_H-512_A-8 pre-trained model to train the bi-encoder.\nParameter Settings. We utilize recall@N as our evaluation metric, with N being 1, 2, 4, 8, 16, 32, and 64. A prediction is correct if the true answer is within the model's top N predictions. The bi-encoder model is trained on the ZESHEL dataset for 5 epochs, using mention context and entity description tokens at a learning rate 1e-05. On the GCL-TempEL dataset, training is performed for 1 epoch under similar conditions. The model undergoes annual training and testing on test sets from 2019 to 2022, with each year's model being trained and validated on that year's data before testing across other years.\nTraining Environment and Inference Time. Software versions: Python 3.11.5; PyTorch 2.1.0_cuda 12.1_cudnn 8.9.2; Faiss-gpu 1.6.5; Numpy 1.26; SciPy 1.11.3; scikit-learn 1.3.2. All the experiments were run on a single A100 GPU with 40GB RAM.\n6.3 Main Results\nTable 2 showcases the effectiveness of our model in addressing temporal degradation on the GCL-TempEL dataset. We evaluate the performance on continual and new entities sets. Each column in the table represents the years' gap between the training and testing datasets, as denoted by the digits from 0 to 3. For instance, 0 indicates that training and testing datasets come from the same year, while 3 indicates that the model was trained in 2019 and tested in 2022. The rows are divided based on various metrics: @1 to @64. 'Boost' displays a comparison between our model CYCLE and SpEL model, calculated as $Boost = \\frac{Our \\; Model's \\; Result - SpEL \\; Model's \\; Result}{SpEL \\; Model's \\; Result}$.\nTable 2 demonstrates that our model consistently outperforms the BLINK baseline as well as SpEL, with its superiority becoming increasingly clear as the temporal gap between training and testing datasets grows. Specifically, when the temporal gap is one year, and the evaluation metric is @1, our model exhibits a 23.78% (continual entities) and 12.86% (new entities) improvement. This improvement boosts to 30.24% (continual entities) and 14.68% (new entities) with a two-year gap and jumps to 36.26% (continual entities) and 15.25% (new entities) when the gap extends to three years. The observed performance improvement can be attributed to the baseline model's limited exposure to previously unseen new entities, resulting in a lack of samples for effective learning. In our model, graph contrastive learning enhances the distinction of each node's unique characteristics. This approach effectively enables the model to identify and accurately represent new entities, even without similar historical data.\nFigure 5 displays recall@1 results from the continual and new entities datasets. We compare our proposed model against the baseline models. The x-axis indicates the year gap between training and testing sets. Overall, our model consistently outperforms the baselines. Two different evaluation approaches are examined: 1) forward and backward (f & b), referring to training on the past and testing on the future data, and vice versa, and 2) only forward (f) where the models are only trained on the past and evaluated on the future data. For example, when the year gap is 3, the forward and backward (f & b) setting includes two scenarios: the model trains in 2019 and tests in 2022, and trains in 2022 and tests in 2019. However, the only forward (f) setting only includes one scenario: the model trains in 2019 and tests in 2022. Notably, a zero-year gap implies the same years for training and testing are used, leading to equal recall values in both forward and backward (f & b) and only forward (f) setting. Our model's only forward (f) setting consistently outperforms its forward and backward (f & b) counterpart. We believe this demonstrates that when using previously non-existent entities as the training set, our model is more adept at capturing the evolving trends of KGs, thereby enhancing its ability to predict future developments.\nFurthermore, our model's improvement margin diminishes as the recall metric threshold N is increased, as illustrated in Figure 6. The x-axis represents the year gap between training and testing datasets, while the y-axis shows our model's performance improvement over the SpEL model. Solid bars denote performance on the new entities dataset and hollow bars on continual entities. This diminishing effect is likely because at the @64 threshold, the model needs only to correctly predict one of the top 64 answers, thus allowing a higher error tolerance. Additionally, our model's performance enhancement grows with the year gap, as shown by the regression lines for both continual and new entities.\n6.4 Results on Low-degree Entities and Fairness Analysis\nIn Figure 7, we analyze the improvement of our model's performance on entities with different degrees. The model is trained on the 2019 new entities training set and tested on the test sets from 2019 to 2022. The figure shows the entity's degree on the x-axis and the corresponding improvement in model performance compared to the BLINK model on the y-axis. We observe that our model has a more pronounced improvement for lower-degree entities. When facing entities with a relatively high degree and entities with more neighbors, our model still demonstrates an improvement, although it is less noticeable.\nWe hypothesize that this trend is partly due to sparse information of low-degree nodes. Such nodes are highly sensitive to new connections, making graph contrastive learning module especially effective at integrating new data and adapting to dynamic changes. This sensitivity is critical, as it allows low-degree nodes to more effectively capture and utilize new information, improving their embeddings and adaptability over time.\n6.5 Results on Non-temporal datasets\nOur model achieves state-of-the-art performance on temporally evolving GCL-TempEL dataset (see Table 2). In this section, we evaluate the performance of our model in a traditional setting, characterized by static (i.e., lacking temporal evolution) EL datasets without graph-based input. Concretely, Table 3 describes EL results using recall@N metric, for our CYCLE model compared to BLINK and SpEL models on the ZESHEL and WikilinksNED static datasets. Even without graph-based input and temporal evolution in these datasets, our model maintains consistent performance on par with BLINK, underscoring its adaptability."}, {"title": "7 Conclusion and Future Work", "content": "This paper introduces CYCLE", "research": "nMultimodal Temporal KGs with Contrastive Learning. In addition to temporal information, TKGs can also contain multimodal data.This multimodal data can be used further to improve the performance of contrastive learning for TKGs. For example, a pair of entities with a relationship in multiple timestamps that are also mentioned in the audio can be considered a more informative positive sample than a pair of entities with a relationship in multiple timestamps but not in any other type of data.\nTemporal KGs with Large Language Models. Considering the challenge of aligning similar concepts across languages where direct translations often fail to convey identical meanings, using Large Language Models (LLMs) for aligning conceptually similar entities in multilingual KGscan improve the coherence"}]}