{"title": "In-situ GRAPH REASONING AND KNOWLEDGE EXPANSION USING GRAPH-PREFLEXOR", "authors": ["Markus J. Buehler"], "abstract": "The pursuit of automated scientific discovery has fueled progress from symbolic logic to modern AI, forging new frontiers in reasoning and pattern recognition. Transformers function as potential systems, where every possible relationship remains latent potentiality until tasks impose constraints, akin to measurement. Yet, refining their sampling requires more than probabilistic selection: solutions must conform to specific structures or rules, ensuring consistency and the invocation of general principles. We present Graph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning), a framework that combines graph reasoning with symbolic abstraction to dynamically expand domain knowledge. Inspired by reinforcement learning, Graph-PREFLexOR defines reasoning as a structured mapping $M : T \\rightarrow (G,P, A)$, where tasks yield knowledge graphs G, abstract patterns P, and final answers A. Inspired by category theory, it encodes concepts as nodes and their relationships as edges, supporting hierarchical inference and adaptive learning through isomorphic representations. Demonstrations include hypothesis generation, materials design, and creative reasoning, such as discovering relationships between mythological concepts like \"thin places\" with materials science. We propose a \u201cknowledge garden growth\" strategy that integrates insights across domains, promoting interdisciplinary connections. Results with a 3-billion-parameter Graph-PReFLexOR model show superior reasoning depth and adaptability, underscoring the potential for transparent, multidisciplinary AI-driven discovery. It lays the groundwork for general autonomous reasoning solutions.", "sections": [{"title": "1 Introduction", "content": "Discovery is driven by the ability to think deeply, reflect, and iteratively refine ideas before arriving at conclusions. Emergent artificial intelligence (AI) methods have achieved success in domains such as natural language processing [1, 2, 3, 4, 5, 6, 7], materials science, and molecular biology, including protein folding [8], and others. However, much of these advances rely on models that produce outputs directly, without an explicit intermediate reasoning process akin to scientific thinking, and hence, generally lack self-awareness and capacity to reflect. For AI to ultimately accelerate discovery, we must develop systems capable of reasoning in relational and symbolical manners [9, 10, 11, 12, 13], reflecting critically, and explaining their answers in an iterative process that mirrors scientific methods of exploration.\nWhen we solve a problem by identifying how things are connected and flow together, we often discover a template that works far beyond your original situation, serving as a model that can be understood in a different context or applied to a new scenario to extrapolate. For example, the hierarchical structure of bone, combining stiffness and toughness"}, {"title": "1.1 Modeling isomorphisms for generalization", "content": "To model isomorphisms computationally, we require a neural network architecture capable of capturing the structural equivalences between relational systems. Graph Isomorphism Networks (GINs) are particularly well-suited for this purpose. GINs are designed to operate on graph-structured data, where nodes represent objects and edges capture relationships or interactions. Unlike traditional graph neural networks (GNNs), which may struggle to distinguish between non-isomorphic graphs, GINs achieve maximum expressiveness for distinguishing graph structures by leveraging a theoretically grounded update rule that closely resembles the Weisfeiler-Lehman graph isomorphism test [21]. In a GIN, the node update function aggregates features from neighboring nodes using a weighted sum and applies a learned multi-layer perceptron (MLP) to the result, ensuring the network can capture higher-order structural properties. For example, to model the isomorphism between Newton's second law, $F = m \\times a$, and Ohm's law, $V = I \\times R$, a GIN can represent the relationships between variables and operators (e.g., force, mass, acceleration, =, \u00d7) as nodes and their interactions as edges. By learning the graph embeddings, the GIN can identify the shared relational structure underlying these systems, demonstrating their equivalence. After processing, the embeddings for both equations would be nearly identical in the latent space, reflecting their structural similarity and isomorphic relationship as they can be mapped to the same representation (see, Section S1 and Figure S3, for an example). This capability to recognize and model isomorphisms makes GINs a powerful tool for tasks that involve reasoning over relational data, such as predicting properties of molecules, designing robust materials, or uncovering universal scientific laws.\nHow does this relate to Transformer-based LLMs? Recent work has shown that Transformers implicitly function as graph isomorphism neural networks [13], which provides a powerful lens for explicitly integrating graph-based reasoning into these architectures. Whereas [13] focused on theoretically grounded advancements in the Transformer architecture itself to strengthen its expressive capacity, in this paper we focus on utilizing a standard Transformer interpreted with the expressive power of a GIN, and endow it with explicit capabilities to conduct in-situ graph reasoning and symbolic deduction. We hypothesize that by explicitly leveraging graph theory within Transformers can unlock new levels of performance, generalization, and interpretability, accelerating progress in various scientific domains. Moreover, modeling complex principles can be inspired from biology, specifically the use of hierarchically organized systems that utilize some of the core ideas of category theory in materials and scientific exploration, where natural systems are found to often reuse relational patterns and mechanisms such as amino acids, DNA, and others. Building on these ideas, it has been suggested that inspiration for such advances may come from biology, such as emergent hierarchical intelligence based on agentic systems and other strategies that mimic biological materials and design principles [22, 23, 24, 25, 26]. Related principles include concerted problem solving as illustrated in a recent study of ant dynamics [27], providing ample examples for latent opportunities to discover mechanisms that can be translated across domains."}, {"title": "Large Language Models (LLMs)", "content": "Large Language Models (LLMs) have demonstrated capabilities in generating human-like text and extracting insights from scientific literature. A grand challenge in the application of AI models in science in particular remains to develop algorithms that yield more sophisticated reasoning mechanisms [25, 28, 29, 30, 31, 32, 33, 34, 35]. These models often excel in fluency and pattern recognition, yet typically lack the ability to engage in structured reasoning. Addressing this limitation, earlier work like PReFLexOR (abbreviation of: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning) [36] introduced a framework where models were developed that innately develop a thinking and reflection strategy before answering, leveraging reinforcement learning (RL) to refine reasoning through iterative reflection and dynamic feedback loops, inspired by earlier research [37, 38, 39, 40]. A key goal of the work reported in this paper is to develop a strategy that allows AI models to not simply retrieve information but to learn underlying abstractions and relational motives, and then use these insights to generalize across new domains and ultimately to expand its understanding and capabilities to generalize.\nBuilding on this foundation, we expand the concept of \u201cthinking before answering\u201d to incorporate in situ graph-based reasoning. Graphs provide a powerful framework for representing relationships, causality, and structures inherent in scientific problems (Figure 2). From molecular interactions, dynamics, to material properties, many scientific phenomena are naturally represented as graphs. By enabling models to autonomously construct and manipulate these symbolic graph representations, we create a model that mimics the reflective, iterative reasoning processes integral to scientific inquiry. In Transformers, knowledge is represented as a superposition of potentialities that collapse into specific outputs when conditioned on tasks. This interpretation offers a framework for enhancing both reasoning and creativity. We show how, by balancing structured coherence with divergent exploration, models can traverse new ideas, generating insights while maintaining rigorous, task-aligned solutions."}, {"title": "1.2 Foundations of in-situ graph reasoning", "content": "In this work, we introduce a novel approach that unifies the linguistic fluency of LLMs with the relational reasoning capabilities that have been quite successful in architectures such as Graph Neural Networks (GNNs). Our model extends the reflective capabilities introduced in the original PReFLexOR model, enabling the construction of graph-based intermediate reasoning mechanisms, by taking advantage of the flexible capabilities of the transformer architecture. Through these symbolic representations via special tokens, the model engages in a \u201cthinking phase,\" reasoning over the graph to refine its understanding before generating an answer. This approach not only enhances interpretability but also improves the model's ability to solve complex scientific problems requiring relational and structural reasoning. We achieve this by formalizing reasoning as a structured mapping:\n$M : T \\rightarrow (G,P,A),$\n(1)\nwhere a task T produces a knowledge graph G, abstract patterns P, and final answers A. The knowledge graph\n$G = (V, E)$\n(2)\nencodes concepts as nodes V and relationships as directed edges E.\nBy explicitly constructing and abstracting relational graphs, the model can encode structural information that standard next-token prediction often overlooks or treats only implicitly. In this setup, each entity and its relations become first-class objects in the learned representation, enabling the network to detect and exploit common subgraph motifs and isomorphisms that recur across different inputs. Unlike pure sequence-based transformers, which must infer latent structure solely from token order and distributional cues, a graph-driven approach anchors learning in explicitly linked entities, preserving both local and global connectivity. As a result, it is not only easier for the model to discover universal features (such as repeated subgraphs or underlying algebraic forms) but also more straightforward to apply symbolic abstractions, since the graph representation makes these recurring structures and higher-level patterns more salient and amenable to consistent transformations.\nOur model thereby offers opportunities for diverse use cases in AI for science and beyond. Tasks such as multi-step reasoning, hypothesis generation, and causal inference become more robust and explainable. This work bridges connectionist and symbolic paradigms, pushing the boundaries of what AI can achieve in scientific domains [41, 42, 43]. By creating models that have a more explicit process of relational deliberation, both symbolically and structurally, before answering, we take an important step toward AI systems capable of true scientific reasoning and discovery."}, {"title": "1.3 Outline of this paper", "content": "The plan of the paper is as follows. First, we briefly review the PReFLexOR architecture published in earlier work [36], then describe the training process of developing Graph-PReFLexOR, and then review several case studies of how the method can be applied. We conclude with final thoughts and a discussion on future opportunities."}, {"title": "2 Results and Discussion", "content": "We report the results of a series of experiments and analyses conducted based on the model. We start with a review of the PREFLEXOR framework [36], how we adapted it for graph reasoning capability, and then move on to the results of the experiments conducted. We conclude with a discussion of the results, interpretations, and an outlook to future opportunities."}, {"title": "2.1 Review of PReFLEXOR", "content": "PRefLexOR, Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning [36], is a framework that enhances the reasoning capabilities of language models by integrating preference optimization and recursive reasoning (Figure 3). The approach relies on the introduction of special tokens such as <|thinking|>..<|/thinking|> and <|reflect|>..<|/reflect|>, which explicitly mark distinct phases of reasoning, enabling structured generation and iterative refinement. During training, the model undergoes two stages: Structured Thought Integration, where reasoning processes are guided using special tokens and optimized through Odds Ratio Preference Optimization (ORPO), and Independent Reasoning Development, which leverages masking of thinking tokens and applies Efficient Exact Optimization (EXO) [44] to align final outputs with preferred reasoning patterns without prescribing the specific reasoning steps. Thinking tokens allow the model to simulate intermediate reasoning steps, while masking ensures that the model infers the best reasoning pathways without direct supervision. We note that EXO's reverse Kullback-Leibler divergence [45] objective promotes mode-seeking behavior, emphasizing"}, {"title": "Recursive Reasoning Algorithm", "content": "As was shown in the original paper [36], with a trained model the reasoning process can be iteratively refined at inference time, where the model generates intermediate reasoning (thinking phase), the critic evaluates it (reflection phase), and then produces improved responses. Formally, the reasoning at step i, $R_i$, is represented as:\n$R_{i+1} = f_{critic} (R_i, F_i),$\n(3)\nwhere $f_{critic}$ applies feedback $F_i$ to refine the intermediate reasoning. At each step, reflection is guided by feedback on gaps or inconsistencies in $R_i$, enabling the next iteration to improve alignment with the desired output. The final response A is derived after N refinement steps as:\n$A = g(R_N),$\n(4)\nwhere g extracts the synthesized final answer. During training, masking is applied to tokens embedded within <|thinking|>..<|/thinking|>, requiring the model to infer reasoning paths indirectly while optimizing the final output. This iterative combination of thinking and reflection, structured via tokens and recursive refinement, ensures that the model autonomously enhances its reasoning capabilities, providing accurate and well-aligned outputs."}, {"title": "2.2 Graph-PReFLexOR algorithm design and training", "content": "Here we use the original PReFLexOR framework but create a different function to develop the structured training data by teaching the model to explicitly construct graphs and symbolic representations in its thinking phase. Details are included in the Materials and Methods section, but we review highlights here. All training is done on top of the meta-llama/Llama-3.2-3B-Instruct model, a small but performant base LLM that serves as the foundational platform for development.\nStructured reasoning is generated on-the-fly during training as described in the Materials and Methods section. At its core, the method constructs dynamic knowledge graphs in-situ $G = (V, E)$, where nodes V represent key concepts extracted from enriched context, and edges E encode relationships such as IS-A, RELATES-TO, and INFLUENCES, common and shared relational descriptions. These relationships are quantified using a semantic scoring function $f(C_i, C_j; r_k)$, which evaluates the significance of relationships $r_k$ between concepts $c_i$ and $c_j$. Retrieval-Augmented Generation (RAG) is employed to enrich the input context and identify abstract patterns and dependencies within the graph, enabling higher-order reasoning.\nThe thinking section organizes reasoning into structured headings, including 'Core Concepts and Relationships', 'Reasoning Steps', and 'Abstract Patterns', providing a framework for logical progression and shared token representation to encourage generation (see, Figure 1 and the discussion in Section S1). For example, the \u2018Core Concepts and Relationships' heading defines the key nodes and their connections in the knowledge graph, while \u2018Reasoning Steps' outlines step-by-step deductions using the generalized formalisms. The \u2018Abstract Patterns' section summarizes these relationships into symbolic transformations, such as $\\alpha \\rightarrow \\beta \\rightarrow \\gamma \\rightarrow \\delta \\rightarrow \\epsilon$, with proportional dependencies $\\alpha \\chi \\epsilon$ indicating critical links. These structured headings not only guide the model's reasoning but also ensure interpretability and coherence in outputs and aid to boost extrapolation capacity in recursive reasoning steps.\nSpecial tokens delineate the reasoning phases, facilitating recursive refinement of the knowledge graph and symbolic abstractions during training and inference. By uniting graph-based reasoning, symbolic methods, and structured thinking sections, Graph-PReFLexOR achieves robust, interpretable performance in generating high-quality responses for reasoning-intensive tasks.\nWe train the model using the same multi-stage training process as done in the original model [36], featuring first ORPO and then DPO-EXO. The training dataset is the same as in the original paper, consisting of raw text of around 1,000 papers in the realm of biological and bio-inspired materials [30]."}, {"title": "2.3 Detailed analysis of example responses: Single task-response pair", "content": "In the first example, we task the model as follows, challenging the model to answer a question that partially involves a materials-specific task but also invokes a domain that was not included in the training data, to test how well the model can extrapolate its structured reasoning to other domains that were not included in the training set. The task is:"}, {"title": "2.4 In-situ graph generation and recursive reasoning through multi-agent modeling", "content": "The earlier example showed that the model was able to expand its capabilities beyond the materials-science focused training data and was able to successfully integrate different domains into the structured reasoning paradigm. In the next experiment, we task the model as follows:"}, {"title": "2.5 Growing a knowledge garden via in-situ graph generation and knowledge expansion", "content": "In this next experiment we use the capability of the graph reasoning model to generate graphs that describe phenomena in a relational manner and apply it to explore ever-expanding graph structures by repeatedly prompting the model. This allows us to recursively grow a graph, starting from an initial concept or response to a task, and to examine whether the model can blend creative and analytical reasoning. The initial task is:"}, {"title": "Abstract Patterns", "content": "The abstract patterns are:\n**Abstract Pattern: **\n$\\alpha \\rightarrow \\beta$\n$\\beta \\rightarrow \\gamma$\n$\\gamma \\rightarrow \\delta$\n$\\beta \\rightarrow \\varepsilon$\n$\\alpha \\rightarrow \\zeta$\n$\\beta \\rightarrow \\zeta$\n**Key Transformation Rule:**\nIf $\\alpha \\rightarrow \\delta$ and $\\B \\rightarrow \\delta$ then $\\delta \\rightarrow \\varepsilon$\n**Essential Condition:**\n$\\alpha \\neq \\beta$\nPattern Context:\nThe song incorporates snowflakes and flower petals to represent sent the contrasting yet harmonious aspects of nature's beauty and seasonal changes. Snowflakes symbolize the serenity and uniqueness of winter, while flower petals signify the delicacy and renewal of spring. Together, they highlight the dynamic interplay between these elements, creating a balanced and evocative musical composition."}, {"title": "Reasoning steps", "content": "**Reasoning Steps**:\n1. Snowflakes and flower petals are both natural phenomena.\n2. Snowflakes are associated with winter, while flower petals are associated with spring.\n3. Snowflakes evoke a sense of serenity and uniqueness, while flower petals symbolize delicacy and renewal.\n4. Combining snowflakes and flower petals in a song can create a harmonious representation of nature's beauty and seasonal changes.\n5. This combination can inspire a balanced and evocative musical composition.\n**Relevant Materials or Concepts**:\nSnowflakes\nFlower Petals\nNature's Beauty\nSeasonal Changes\nSerenity\nUniqueness\nDelicacy\nRenewal\n**Design Principles**:\n**Contrast**: Snowflakes and flower petals represent opposing seasons, creating a striking contrast.\n**Harmony**: Together, they create a harmonious representation of nature's beauty and seasonal changes.\n**Evocative**: Each element evokes distinct emotions and sensations, inspiring a balanced and evocative musical composition.\n**Material Properties**:\n**Snowflakes**:\n**Appearance**: Delicate, intricate, crystalline structure\n**Sound**: Soft, gentle, repetitive\n**Emotional Connotation**: Serene, calming, unique\n**Flower Petals**:\n**Appearance**: Delicate, colorful, soft\n**Sound**: Light, airy, whimsical\n**Emotional Connotation**: Delicate, whimsical, renewal\n**Hypothesis**:\n\"Incorporating snowflakes and flower petals in a song will create a harmonious representation of nature's beauty and seasonal changes, evoking distinct emotions and sensations.\"\n**Additional Background**:\nTo incorporate snowflakes and flower petals into a song, one could consider the following elements:\n1. **Imagery**: Use vivid and evocative language to describe the intricate patterns and beauty of snowflakes and flower petals. This can be achieved through lyrics or even instrumental descriptions.\n2. **Metaphors**: Draw parallels between the delicate nature of snowflakes and flower petals and the human experience. For instance, snowflakes could represent individuality and uniqueness, while flower petals could symbolize growth and renewal.\n3. **Seasonal themes**: Explore the contrast between winter (snowflakes) and spring (flower petals) to create a narrative that highlights the cyclical nature of life and the beauty in each season.\n4. **Musical motifs**: Use specific musical motifs or melodies to evoke the feeling of snowflakes (e.g., soft, repetitive patterns) and flower petals (e.g., light, airy, whimsical).\n5. **Collaboration**: Consider co-writing with a poet or lyricist to craft lyrics that incorporate these natural elements in a meaningful and impactful way."}, {"title": "Final response", "content": "The primary recognition pattern can be formalized as $P(\\alpha, \\beta) = \\{(a \\rightarrow x) \\Leftrightarrow (\\beta \\rightarrow x) | x \\in Influences\\}$, where a represents snowflakes and \u03b2 represents flower petals. This symmetrical mapping demonstrates the model's ability to identify and preserve parallel structures while maintaining essential distinctions.\nWe can see that the model's pattern recognition operated on multiple levels: $L_1$ : Categorical $\\rightarrow$ {Natural Phenomenon}, $L_2$ : Temporal $\\rightarrow$ {Winter, Spring}, and $L_3$ : Influence $\\rightarrow$ {Mood, Nature's Beauty, Seasonal Changes}. The abstraction process generated a formal framework where $\\forall x \\in L_3 : (a \\rightarrow x) \\land (\\beta \\rightarrow x)$ while maintaining the essential condition $a \\neq \\beta$. This demonstrates sophisticated categorical reasoning while preserving compositional possibilities.\nThe response reveals remarkable symmetrical structures between winter and spring phenomena, characterized by parallel relationships that can be formalized as $S = \\{(S_w, S_p) \\in Phenomena \\times Seasons\\}$, where snowflakes ($s_w$)"}, {"title": "2.5.1 Knowledge expansion growth phase", "content": "An interesting aspect of the capability of our graph reasoning model is its ability to grow graphs dynamically to extract, generate and design new knowledge. To do this we iteratively task the model to generate graphs, where new prompts are developed based on earlier ones to expand the answer in new directions. Since the model produces a graph at every iteration, all these graphs can ultimately be integrated into one large graph that spans diverse topics. We use the following series of prompts as visualized in Figure 8.\nFigures 9 and S1 present complementary visualizations of the concatenated knowledge graphs derived from the series of prompts outlined in Figure 8. Figure 9 organizes the knowledge graphs by individual prompts, emphasizing the structure and interconnections within each sub-graph. This visualization highlights the local relationships and the specific thematic focus of each prompt while illustrating how these ideas are interconnected across different sub-graphs. In contrast, Figure S1 integrates the same data into a unified knowledge graph, providing a holistic view of the interplay among all the prompts. By merging the sub-graphs into a single structure, this representation reveals overarching connections and emergent patterns that may not be immediately apparent when examining the sub-graphs in isolation. Together, these figures offer a layered perspective on the relationships within the dataset, from prompt-specific insights to a system-level understanding.\nFigure 10 depicts the graph in a more abstract representations, visualized per distinct layouts and different node properties. Data is shown in node degree, PageRank [46], and bridging coefficient displays. These are fundamental measures in network analysis, each capturing different aspects of a node's role within a graph, where the analyses point to the powerful insights that can be generated based on the model output. Node degree represents the number of connections a node has, providing a simple yet powerful indicator of local importance; in directed graphs, this can be further divided into in-degree (incoming connections) and out-degree (outgoing connections).\nAs an alternative, PageRank offers a more global perspective, measuring a node's importance based on the significance of its neighbors. Originally developed for ranking web pages, PageRank emphasizes the quality of connections, assigning higher scores to nodes linked by other highly-ranked nodes. PageRank is a measure of a node's global importance in a network, originally designed to rank web pages in search engines. It assigns a score to each node based on the principle that connections from important nodes contribute more to a node's rank than connections from less important nodes. The algorithm works iteratively, where a node's PageRank is proportional to the sum of the PageRank scores of its neighbors, weighted by the number of links those neighbors have. This approach captures both the quantity and quality of connections, emphasizing nodes that are linked by highly influential entities. A damping factor is typically introduced to account for random jumps, ensuring the scores converge and the network can handle dangling nodes (nodes with no outgoing links). In the context of knowledge graphs, PageRank is particularly relevant for identifying the most semantically important entities or concepts. For instance, in a knowledge graph where nodes represent entities and edges represent relationships, PageRank can highlight nodes that are central to the graph's structure, such as frequently referenced concepts or pivotal entities connecting different subdomains. This can aid in prioritizing entities for tasks"}, {"title": "Symmetrical structures in nature", "content": "The knowledge expansion process showcased in these repeated prompts demonstrates the ability of Graph-PReFLexOR to iteratively generate and connect knowledge graphs based on a series of related prompts. Beginning with a creative task (writing a song incorporating snowflakes and flower petals), the model progressively expands its knowledge base by addressing scientific and philosophical inquiries stemming from the initial prompt. Some unique insights/ideas generated during the recursive knowledge expansion process include:\n\u2022 Symmetrical structures in nature: The model identifies symmetrical influence patterns between snowflakes (associated with winter) and flower petals (associated with spring). Despite their temporal opposition, both exhibit isomorphic influence on mood, nature's beauty, and seasonal changes. This reveals a fundamental balance in nature's organization across different seasons.\n\u2022 Abstraction of natural phenomena: The model constructs an abstract pattern framework representing the relationships between distinct natural phenomena. This framework highlights both first-order and second-order conditional transformations, demonstrating the model's capacity for high-level reasoning and abstraction.\n\u2022 Integration of diverse concepts: The knowledge graph expands to incorporate concepts like molecular symmetry principles, nanopatterning in flower petals, and the philosophical idea of impermanence and renewal, showcasing the ability to connect ideas across diverse disciplines from materials science to music to literature and philosophy [49, 50] .\n\u2022 Emergent patterns: By visualizing the integrated knowledge graph, overarching connections and patterns emerge that might not be apparent when examining individual sub-graphs. This highlights the system's ability to synthesize knowledge and reveal hidden relationships."}, {"title": "2.6 Autonomously growing knowledge garden", "content": "For a fully autonomous generation of graphs without human input for prompt generation, the system can be modified easily by using an LLM to develop new prompts autonomously based on previous results, to create an ever-expanding abstraction of relationships. Figure 13 shows the results of an autonomously grown knowledge graph. The upper panel shows the graph, and the lower panel depicts a selection of interesting paths identified in the integrated graph. The starting prompt, the only one generated by a human, was chosen to be:"}, {"title": "3 Conclusion", "content": "This work introduced Graph-PReFLexOR, a framework that integrates in-situ graph reasoning, symbolic abstraction, and recursive reflection into the generative modeling paradigm (Figures 1 and 2). By embedding graph-based intermediate representations within LLMs, Graph-PReFLexOR advances beyond the limitations of purely linguistic systems to tackle complex tasks requiring relational reasoning, multi-step deduction, and adaptive knowledge synthesis. Our experiments were designed to assess particularly how well the model could generalize, as the training data consisted of 1,000 papers on biological materials [30], whereas the tasks were constructed at the interface of science and other disciplines including design, music, and philosophy to probe the model's generalization capability beyond the technical-focused training data. We find exceptional performance throughout and see that the model could very well generalize, follow the structured reasoning it learned during training (see, Figure 1), and even construct highly complex graphs that interface a myriad of disciplines (see, e.g. Figure 9 and S1), and other results.\nA particularly compelling application was the development of the knowledge garden concept, building on the ability of the model to grow knowledge graphs dynamically and iteratively, by adding new relational insights and abstractions. This resulted in a series of experiments where we expanded graphs upon an initial simple task, yielding complex graph structures that themselves could be the basis of further research, inquiry and reasoning (e.g., Figures 10 and 11). Much future work can be conducted based on this method. Some initial experimentation on growing graphs autonomously yielded interesting results, such as shown in Figures 13 and 14, where we identified a capability of the model to search and expand topics and connect complex ideas.\nBy explicitly constructing and abstracting relational graphs, the Graph-PReFLexOR approach provides a more structured foundation than standard sequence-oriented Transformer training. In particular, preserving connectivity among entities and relations makes it easier to detect and exploit universal features such as isomorphisms and recurrent subgraph patterns. Unlike pure next-token objectives, which largely rely on distributional cues hidden in token sequences, graph abstractions bring structural commonalities to the forefront, enabling more systematic identification of shared algebraic forms, relational templates, and higher-order symmetries. As a result, symbolic rewriting and generalization become more direct, since the underlying topology is explicitly represented rather than merely inferred. Consequently, the"}, {"title": "3.1 Mathematical and Logical Framework", "content": "Graph-PReFLexOR establishes a pipeline of thinking and a unified framework that bridges symbolic reasoning and dynamic graph-based abstraction to tackle the complexity of scientific inquiry. At its core, the framework formalizes reasoning as a multi-layered mapping as introduced in equation 1 where a task T produces a knowledge graph G, abstract patterns P, and final answers A. The knowledge graph $G = (V,E)$ encodes concepts as nodes V and relationships as directed edges E, such as IS-A, RELATES-TO, and INFLUENCES. The system derives abstract patterns P by identifying higher-order dependencies, structured as transformations:\n$\\alpha \\rightarrow \\beta \\gamma \\rightarrow \\delta \\rightarrow \\epsilon,$\nwith proportional relationships such as:\n$\\alpha \\chi \\epsilon,$\nindicating how the initial state a contributes to the final transformation e. Recursive reflection as shown in Figure 5 refines these outputs iteratively, producing an optimized answer A.\nInspired by category theory, Graph-PReFLexOR emphasizes relational over intrinsic properties, allowing the abstraction of domain-specific patterns into reusable, transferable frameworks. For example, graph symmetries enable translational reasoning across materials science, bioengineering, and philosophy, breaking traditional siloed boundaries. The recursive refinement process ensures these abstractions remain grounded and interpretable, aligning outputs with task-specific goals while introducing new hierarchies of understanding, specifically creating shared embedding representations with multidimensional mappings."}, {"title": "Iterative, knowledge-expanding capability", "content": "Another key aspect of the framework lies in its iterative, knowledge-expanding capability. By incorporating feedback-driven graph updates and dynamically integrating symbolic patterns, Graph-PReFLexOR demonstrates the potential for in-situ knowledge growth. This positions it uniquely to address challenges in hypothesis generation, interdisciplinary exploration, and adaptive learning tasks that require not just computational power but conceptual flexibility. Ultimately, Graph-PReFLexOR presents opportunities to expand the paradigm of AI-driven reasoning by fostering transparent, interpretable models of discovery, laying the groundwork for a new era of autonomous scientific and creative inquiry."}, {"title": "3.2 Advances Over Existing Approaches", "content": "Compared to traditional LLMs that rely on unstructured token-level generation, Graph-PReFLexOR introduces:\n\u2022 Structured Intermediate Representations: Unlike models that output responses directly, Graph-PReFLexOR generates a knowledge graph G to explicitly represent relationships and dependencies. This improves interpretability and ensures reasoning consistency, which can be adapted easily to other scientific, technical or other domains, and formalizes shared representations.\n\u2022 Symbolic and Connectionist Integration: While most transformer-based models do not focus specifically on symbolic reasoning capabilities, Graph-PReFLexOR bridges the gap by combining linguistic fluency with graph-based reasoning. This aligns with hybrid approaches but demonstrates superior adaptability to novel tasks.\n\u2022 Recursive Refinement: Recursive reasoning enables multi-step improvement by iteratively refining knowledge graphs and abstract patterns. This dynamic process enhances response quality and ensures alignment with task objectives.\n\u2022 Scalability and Adaptability: Graph-PReFLexOR extends traditional methods by dynamically expanding its knowledge graph to adapt to evolving prompts, facilitating knowledge transfer across domains.\nOur experimental results demonstrate that Graph-PReFLexOR outperforms baseline models in reasoning depth, knowledge transfer, and adaptability. For instance, the recursive reasoning mechanism (equation 3) achieves higher reasoning depth and adaptability compared to static methods, as we demonstrated in Table 1. For scientific applications, and special cases like mathematics, the critic function $f_{critic}$ could involve specific fact-checking or consistency assessments, or even executing simulations to inject new data or physical insights. These can be incorporated during training or inference and ensure that such a recursive reasoning system produces accurate and consistent results. This flexibility offers significant potential for scientific applications."}, {"title": "3.3 Quantum-Inspired Metaphor for Transformers and Graph-Based Reasoning", "content": "The Transformer architecture", "embeddings": "n$|\\Psi_0\\rangle = \\sum_k C_k |k\\rangle", "product": "n$|\\Psi\\rangle = |\\psi_1\\rangle |\\psi_2\\rangle  \\cdot\\cdot |\\psi_n\\rangle", "Hamiltonian": "n$|\\Psi_{i+1"}, "rangle = \\hat{H}|\\psi_i\\rangle,$\nwhere $|\\Psi_i\\rangle$ represents the graph's state at step i, and $\\hat{H}$ encodes the principles of relational optimization (e.g., maximizing coherence or minimizing contradictions). The final knowledge graph, after sufficient refinement, reflects a collapsed eigenstate, optimized for the given question.\nPrincipally, this analogy extends to the entire Transformer framework. The recursive refinement within Graph-PREFLexOR (e.g., Figures 1, 2 and 5) exemplifies how transformers resolve complex queries by dynamically collapsing potential answers into structured outputs through an iterative process. The abstraction mechanisms used here are just one specific instantiation of this broader capability, where relationships between nodes represent entangled dependencies that become more expressive, and accessible, through shared representations that encourage the model think in isomorphisms. In more abstract terms, the system applies interference patterns via attention weights to amplify relevant paths and suppress contradictory ones:\n$C_k^{(i+1)} = \\sum_j A_{kj} C_j^{(i)},$\nwhere $A_{kj}$ are attention weights that act analogously to quantum amplitudes, modulating the influence of each path.\nBy connecting graph reasoning to these foundational quantum-like behaviors, Graph-PReFLexOR illustrates how Transformers can model tasks requiring multi-step abstraction, recursive refinement, and contextual reasoning. The graph serves as a symbolic representation of the underlying quantum-inspired mechanisms, offering interpretability and structure while adhering to the general principles governing the model's operation.\nThe reasoning process can be viewed as evolving from a state of superposition $|\\Psi_0\\rangle = \\sum_k C_k|k\\rangle$, where potential reasoning paths are explored. Through recursive refinement, the framework updates the reasoning state iteratively, $R_{i+1} = f_{critic}(R_i, F_i)$, analogous to quantum state evolution governed by external feedback $F_i$. The integration of reasoning paths into a unified representation aligns with the mapping defined in equation 1, where a task T produces a knowledge graph G, abstract patterns P, and final answers A. Just as the quantum system collapses into a measurable eigenstate, Graph-PReFLexOR resolves these intermediate abstractions into a final response"]}