{"title": "A mathematical framework of intelligence and consciousness based on Riemannian Geometry", "authors": ["Meng Lu"], "abstract": "Understanding intelligence is a central pursuit in neuroscience, cognitive science, and artificial intelligence. Intelligence encompasses learning, problem-solving, creativity, and even consciousness. Recent advancements in geometric analysis have revealed new insights into high-dimensional information representation and organisation, exposing intrinsic data structures and dynamic processes within neural and artificial systems. However, a comprehensive framework that unifies the static and dynamic aspects of intelligence is still lacking. This manuscript proposes a mathematical framework based on Riemannian geometry to describe the structure and dynamics of intelligence and consciousness. Intelligence elements are conceptualised as tokens embedded in a high-dimensional space. The learned token embeddings capture the interconnections of tokens across various scenarios and tasks, forming manifolds in the intelligence space. Thought flow is depicted as the sequential activation of tokens along geodesics within these manifolds. During the navigation of geodesics, consciousness, as a self-referential process, perceives the thought flow, evaluates it against predictions, and provides feedback through prediction errors, adjusting the geodesic: non-zero prediction errors, such as learning, lead to the restructuring of the curved manifolds, thus changing the geodesic of thought flow. This dynamic interac-tion integrates new information, evolves the geometry and facilitates learning. The geometry of intelligence guides consciousness, and consciousness structures the geometry of intelligence. By integrating geometric concepts, this proposed theory offers a unified, mathematically framework for describing the structure and dynamics of intelligence and consciousness. Applicable to biological and artificial intelligence, this framework may pave the way for future research and empirical validation.", "sections": [{"title": "Introduction", "content": "Understanding intelligence, both in humans and artificial systems, has been a central pursuit in various fields including neuroscience, cognitive science, and ar-tificial intelligence. Intelligence encompasses a wide range of cognitive abilities such as learning, problem-solving, creativity, and adaptation. These abilities are fundamental to how organisms interact with their environment, process in-formation, and make decisions. In recent years, the geometric and topological tools have been developed and utilised to analyse the geometry of the high di-mensional representation. These advancements have led to new insights into how information is processed and represented in neural and artificial systems (Hensel et al., 2021; Chung and Abbott 2021).\nResearchers have used latent spaces to create low-dimensional representa-tions of data manifolds, revealing their underlying geometrical structures (Reif et al., 2019; Marks and Tegmark 2023). Deep generative models like VAEs (Variational Autoencoders, Kingma and Welling, 2013) and GANs (Genera-tive Adversarial Networks) have shown that these latent spaces can capture the curvature of learned manifolds, providing insights into intrinsic data struc-tures (Arvanitidis et al., 2021; Chadebec and Allassonniere 2022). Additionally, algorithms for computing geodesic curves and parallel translation of tangent vectors allow for an intrinsic notion of distance and efficient navigation within these manifolds (Acosta et al., 2022). However, these models primarily describe the static data structure and lack mechanisms to account for the dynamics of how the latent space evolves over time. Another set of generative models, such as GPTs (Generative Pre-trained Transformers), utilise attention mechanisms to accumulate contextual information and autoregression to generate token se-quences, which can be regarded as thought flow. This approach represents the dynamic aspect of intelligence. However, these models do not have an explicitly defined latent space in the same way that VAEs (Variational Autoencoders) and GANS (Generative Adversarial Networks) do. Consequently, the organisation and connections of knowledge or features within these models remain unclear.\nIn parallel with the advancement of geometric analysis in AI, the geomet-ric representation of neural activities has been observed to efficiently encode behavioural variables and predict outcomes (DiCarlo and Cox 2007; Gao and Ganguli 2015; Vyas et al., 2020) by a variety of new tools and methods based on geometry (Jolliffe 1986; Tenenbaum et al., 2000; Roweis and Saul, 2000; Hinton and Roweis 2002; Mclnnes et al., 2018; Chaudhuri et al., 2019). For example, the hippocampus has been shown to encode both spatial and abstract variables within neural manifolds, serving as a common organising principle for storing declarative memory and generating cognitive maps (Dmitriy et al., 2017; Nieh et al., 2020). Apart from this, geometric representation has also been used in sensory recognition (Kobak et al., 2019; Okazawa et al., 2021; Stringer et al., 2019), motor control (Gallego et al., 2017) and decision making such as bayesian inference (Sohn et al., 2019). These studies highlight how geometric structures"}, {"title": "", "content": "in the brain can facilitate decision-making and predictive coding through effi-cient integration of spatial and abstract information.\nBuilding on the advancements in geometric analysis for both artificial and human intelligence, it is compelling to explore a general theory based on Rie-mannian geometry (Bronstein et al., 2021; Ma et al., 2021) to describe the structure and dynamics of intelligence representation. This raises a few key questions: 1) If such a theory exists, what is the general form for rep-resenting information or features as elements within this framework,\nand what is their structure? 2) Under this representation and struc-ture, what are the dynamics of the thought flow navigating within the structures formed by these representations? 3) Additionally, how does the structure of feature representation interact with the dynamic thought flow? These questions highlight the need for a unified framework that encompasses both the static and dynamic aspects of intelligence, bridging the gap between information representation and cognitive processes.\nA general theory of intelligence should encapsulate a range of properties emergent from intelligence, including learning, imagination, creative thinking, problem-solving, and the pinnacle of the intelligence hierarchy-consciousness. Consciousness, as an emergent property of complex cognitive processes (Seth and Bayne 2022), rests on the brain's ability to sustain complex dynamics of constantly changing activity and connectivity between brain regions (Dehaene and Changeux, 2011; Hutchison et al., 2013; Barttfeld et al., 2015; Demertzi et al., 2019), indicating that consciousness influences and is influenced by the un-derlying structure of intelligence. Therefore, it is a necessary component of this general theory and can only be adequately explained and formulated within the comprehensive framework of intelligence. Understanding consciousness within this framework would provide a holistic view of how thought processes evolve and adapt, while explaining intelligence in the context of consciousness high-lights the dynamic and self-referential nature of cognitive functions.\nThe author here proposes a theory of the geometry of intelligence that aims at addressing these questions via a comprehensive mathematical framework that describes the structure and dynamics of intelligence. This theory conceptualises elements of intelligence as tokens embedded in a high-dimensional space of intel-ligence, forming manifolds with geometric properties such as curvature. These manifolds capture both the static feature distributions and dynamic sequences of activation, reflecting the complexity and interconnections within cognitive processes. In the dynamics of intelligence and consciousness, sequential token activation forms thought flow, moving along manifold geodesics. The direc-tion of this sequence, the tangent vector of this geodesic, is determined by the manifold's intrinsic geometry in terms of Riemannian geometry. Cognitively, the direction of thought flow is guided by contextual information from past to-kens. Consciousness perceives and evaluates thought flow, providing feedback via prediction errors. When prediction error is zero, navigation follows the"}, {"title": "", "content": "manifold's structure as a geodesic, representing free thought flow state with no consciousness perturbation. Typically, non-zero prediction errors and external input reflect the process of accepting new information and integrating it into the existing manifold, thereby evolving the intelligence space, representing process such as learning.\nIn summary, the geometry of intelligence guides how the consciousness nav-igate, the consciousness dictates how the geometry of intelligence evolves. By integrating geometric and topological concepts, this theory offers a novel and mathematically rigorous framework to describe the structure and dynamics of intelligence and consciousness, for both biological and machinery intelligence, paving the way for future research and empirical validation."}, {"title": "Background of Riemannian Geometry", "content": "Riemannian geometry is a branch of differential geometry that studies smooth manifolds equipped with a Riemannian metric. This metric allows for the def-inition of various geometric concepts such as distances, angles, and curvatures on the manifold."}, {"title": "Manifold and Curvature", "content": "A manifold M is a topological space that locally resembles Euclidean space and is equipped with a smooth structure. The curvature of a Riemannian manifold is a measure of how much the manifold deviates from being flat. It is quantified using the Riemann curvature tensor $R_{\\varrho \\mu\\nu}$."}, {"title": "Metric Tensor", "content": "The local geometry of a Riemannian manifold is defined by the metric tensor $g_{\\mu \\nu}$. This tensor provides a way to measure distances and angles on the manifold. The components of the metric tensor in local coordinates are given by:\n$g_{\\mu\\nu} = \\frac{\\partial x^{\\alpha}}{\\partial y^{\\mu}}\\frac{\\partial x^{\\beta}}{\\partial x^{\\nu}}g_{\\alpha\\beta}$\nwhere $x^{\\mu}$ and $x^{\\nu}$ are local coordinates on the manifold, and $g_{\\alpha\\beta}$ is the metric tensor in a different coordinate system."}, {"title": "Christoffel Symbols", "content": "The Christoffel symbols $\\Gamma^{\\nu}_{\\mu\\lambda}$, are derived from the metric tensor and represent the connection coefficients:\n$\\Gamma^{\\nu}_{\\mu\\lambda} = \\frac{1}{2} g^{\\nu\\rho} \\left( \\frac{\\partial g_{\\rho \\mu}}{\\partial x^{\\lambda}} + \\frac{\\partial g_{\\rho\\lambda}}{\\partial x^{\\mu}} - \\frac{\\partial g_{\\mu\\lambda}}{\\partial x^{\\rho}} \\right)$"}, {"title": "Curvature Tensor", "content": "The Riemann curvature tensor $R^{\\kappa}_{\\varrho \\mu\\nu}$ is a measure of the manifold's curvature:\n$R^{\\kappa}_{\\varrho \\mu\\nu} = \\partial_{\\mu} \\Gamma^{\\kappa}_{\\nu \\varrho} - \\partial_{\\nu} \\Gamma^{\\kappa}_{\\mu \\varrho} + \\Gamma^{\\sigma}_{\\mu \\kappa} \\Gamma^{\\kappa}_{\\nu \\sigma} - \\Gamma^{\\sigma}_{\\nu \\kappa} \\Gamma^{\\kappa}_{\\mu \\sigma}$"}, {"title": "Geodesic Equation", "content": "The geodesic equation describes the evolution of a point moving along the man-ifold. The coordinates $\\gamma(t)$ describe the position of the point on the manifold as a function of the parameter t, which could represent time or another affine parameter. The geodesic equation is given by:\n$\\frac{d^2 \\gamma^{\\kappa}(t)}{dt^2} + \\Gamma^{\\kappa}_{\\mu\\lambda} \\frac{d\\gamma^{\\mu}(t)}{dt} \\frac{d\\gamma^{\\lambda}(t)}{dt} = 0$\nThis equation ensures that the path $\\gamma(t)$ is locally the shortest path between points on the manifold, accounting for the curvature defined by the Christoffel symbols."}, {"title": "Geometry of Intelligence", "content": ""}, {"title": "1 Tokens, embedding and manifold", "content": "Tokens, as discrete units, effectively represent various types of information, such as words in a sentence or pixels in an image, capturing complex data in a man-ageable form. High-dimensional spaces capture intricate relationships between data points, with each dimension representing a different attribute, enabling rich and detailed representations. These spaces facilitate learning and representing underlying data manifolds, crucial for understanding data structure and tasks like interpolation and extrapolation. High-dimensional spaces also separate data points that are close in lower dimensions, aiding in classification, clustering, and retrieval.\nAdvances in multi-modal models like Visual-Language Models (VLMs) such as CLIP (Contrastive Language\u2013Image Pre-training) (Radford et al., 2021) inte-grate visual and textual information into a unified embedding space, showcasing the feasibility of a common space for embedding diverse data types. This in-tegration highlights the power of high-dimensional token spaces in capturing complex, multi-modal relationships, enhancing understanding and generation tasks across various domains, and advancing both artificial and human-like in-telligence.\nTokens are transformed into high-dimensional embeddings, which reside on manifolds capturing the data's underlying structure. The curvature of these manifolds reveals important characteristics about the data distribution, influ-encing model performance. This geometric perspective provides deep insights"}, {"title": "", "content": "into data relationships and guides the development of robust and efficient ma-chine learning models.\nMathematically, let a token $t_i$ be represented as a point in a high-dimensional space:\n$t_i \\in \\mathbb{R}^d$\nwhere d is the dimensionality of the space.\nThe embedding of token $t_i$ is given by:\n$v_i = \\phi(t_i)$\nwhere $\\phi$ is a function mapping the token to its embedding in the high-dimensional space.\nThe collection of all embeddings forms a manifold $\\mathcal{M}$ within this high-dimensional space. The dimension of the manifold, denoted as dim($\\mathcal{M}$), is typically much lower than d, capturing the intrinsic structure of the data:\ndim($\\mathcal{M}$) < d"}, {"title": "2 Thought flow and Geodesic", "content": "Concept: Concept: In cognitive science, we propose that without perturbation or external stimulation, the thought flow navigates naturally on curved mani-folds, following the geometry of the space. This unperturbed path satisfies the definition of a geodesic. In this context, the geodesic path is represented as a sequence of tokens activated along this path, reflecting the natural trajectory of thought flow in the high-dimensional token space. The perturbed thought flow that is \"forced\" by stimuli or external input to deviate from its geodesic, and is the most common scenario, will be analysed in the later section of the Consciousness. The geodesic is the path that minimises the distance between points, analogous to the shortest path on a curved surface. In this theory, the geodesic $\\gamma(t)$ represents the state of intelligence at time t.\nMathematical Representation in cognitive aspect:\n$\\gamma(t) = \\{v_1, v_2, ..., v_n\\}$\n$\\overline{v_i} \\sim \\mathcal{N}(\\mathcal{V}_i, \\Sigma_i)$(1)\n$\\mathcal{V}_i \\sim \\mathcal{N}(V_i, \\Sigma_i)$(2)\n$\\mathcal{V}_i$ is the mean embedding. - $\\Sigma_i$ is the covariance matrix representing the vari-ability around $v_i$."}, {"title": "", "content": "Sampling from the distribution $\\mathcal{N}(v_i, \\Sigma_i)$ rather than using a specific vector $v_i$ allows the model to capture the inherent variability and uncertainty associ-ated with each token. This approach ensures that the model generalises better to new, unseen data by reflecting the natural noise and variations present in real-world information. It also makes the geodesic path more robust to out-liers, providing a more stable and reliable thought flow. Random sampling promotes exploration within the high-dimensional space, which can lead to dis-covering new and potentially better paths, similar to the use of randomness in transformers through techniques like top-k sampling and temperature scaling to introduce variability and flexibility in generating sequences.\nBy incorporating randomness through sampling, the model better mimics human cognitive processes, which are inherently probabilistic and uncertain. This stochastic approach allows the application of probabilistic methods for analysing and optimising the geodesic paths, leading to more accurate, robust, and adaptable representations of intelligence. Integrating these elements into the geometric framework of intelligence provides a comprehensive understanding of how complex cognitive processes are structured and evolve over time."}, {"title": "3 Tangent Vector and State Transition Function", "content": "Concept: The tangent vector at the moving front of the geodesic represents the direction and rate of change of the thought flow at that point. The differentia-tion of the geodesic function represents the state transition function, describing how the state of intelligence evolves over time.\nMathematical Representation:\n$v(t) = \\frac{d\\gamma(t)}{dt}$ (3)\nwhere $\\gamma(t)$ is now a sequence of sampled embeddings $\\{v_1, v_2, ..., v_n\\}$.\nHere, the geodesic $\\gamma(t) = \\{v_1, v_2, ..., v_n\\}$ represents a continuous and smooth path composed of connected points, or tokens, within a smooth manifold. This smooth structure ensures that functions defined on it, including $\\gamma(t)$, are con-tinuous and differentiable.\nThe geodesic $\\gamma(t)$ models the trajectory of thought flow, implying grad-ual transitions between tokens without abrupt jumps. The tangent vector $v(t) = \\frac{d\\gamma(t)}{dt}$ represents the derivative of this path with respect to time, pro-viding a rigorous mathematical description of the rate of change of the thought flow. This differentiation is grounded in the manifold's smoothness, allowing for well-defined tangent vectors that capture the instantaneous direction and speed of movement along the geodesic. Thus, $v(t)$ characterises the dynamic state of"}, {"title": "", "content": "the system at any given time.\nBy treating the geodesic as a smooth path on a smooth manifold, we en-sure mathematical rigor, applying differential geometry to model the continuous evolution of intelligence. This framework enables a precise description of token evolution over time, with $v(t)$ encapsulating both the state and transition dy-namics within the high-dimensional space of intelligence."}, {"title": "4 Attention Mechanism", "content": "Concept: The attention mechanism computes the relevance of each token in the sequence with respect to the current token, determining how contextual in-formation influences the next token.\nMathematical Representation:\n$a_{ij} = A(\\upsilon'_i, \\upsilon'_j)$ (4)\nwhere $a_{ij}$ are the attention weights, and A is a general attention function that measures the relevance of token j to token i.\nThe curvature and structure of the geodesic are results of the training pro-cess, capturing the intrinsic links, organisation, and distribution of tokens. The attention mechanism measures the contextual significance and tokens' correla-tion, which is geometrically represented by the manifold's curvature and struc-ture. This creates a fundamental link between the attention mechanism and the tangent vector. In mathematical terms, while the tangent vector provides a local derivative (instantaneous change), the attention mechanism offers a global perspective by integrating the influence of prior tokens. This integrated influ-ence aligns with the geodesic's learned curvatures, as both are products of the underlying manifold's geometry."}, {"title": "5 Context Vector and Contextual Embedding", "content": "Concept: The context vector is a weighted sum of the value vectors, determined by the attention weights. It represents the aggregated contextual information for a given token. The contextual embedding $h_{t,j}$ for the token at position t in the j-th thought flow is derived from the attention mechanism, representing the aggregated contextual information.\nMathematical Representation:\n$h_{t,j} = C_t = \\Sigma_{j} \\alpha_{ij} (W_{v}\\upsilon'_i)$(5)"}, {"title": "6 Predicted Token", "content": "Concept: The predicted token concept is derived from the context vector. It represents the next token in the thought flow, influenced by the contextual information.\nThe predicted token concept is derived from the context vector, representing the next token in the thought flow influenced by contextual information. This concept can be expressed mathematically by two different equations:\nI. Contextual Representation:\n$g(t) = \\sigma(W_{v}c(t) + b_v)$ (6)\nwhere $\\sigma$ is the activation function, $W_p$ is a weight matrix, $c(t)$ is the context vector at time t, and $b_v$ is a bias term.\nII. Geometric Representation\nThe geometric representation of the predicted token is given by:\n$g(t) = \\int_{t-\\Delta t}^{t} \\upsilon(t) dt + g(t - \\Delta t)$\nwhere $v(t)$ represents the incremental change over time, and $g(t - \\Delta t)$ is the previous state of the token.\nIn these equations, $g(t)$ captures the essence of the predicted token by com-bining the immediate context with the temporal evolution of the thought flow. The integral component in the second equation signifies the accumulation of changes over time, reflecting the dynamic nature of thought processes."}, {"title": "Weight Matrix $W_v$", "content": "The weight matrix $W_v$ and bias vector $b_v$ perform a linear transformation on the context vector $c(t)$. This transformation adjusts the integrated information in $c(t)$ to a new representation that can be used to predict the next token. It helps in mapping the high-dimensional context vector to the appropriate dimensional space of the next token. In human cognition, this can be likened to how the brain integrates various pieces of information and then transforms this integrated information into a specific thought or action. The weights and biases represent how different aspects of the accumulated knowledge are emphasized or de-emphasized in forming the next step in the thought process."}, {"title": "Non-linear Activation Function ($\\sigma$)", "content": "The activation function $\\sigma$ (such as ReLU or tanh) introduces non-linearity into the model. Non-linearity is crucial for capturing complex relationships between tokens. Without non-linearity, the model would be limited to linear mappings, which cannot effectively represent the intricacies of thought processes. In hu-man cognition, non-linear processing allows for the flexibility and complexity required in thinking, decision-making, and creativity. It allows the brain to process information in a way that is not simply additive or subtractive but can involve more complex interactions.\nWhile the specific formulation provided is inspired by machine learning mod-els like Transformers, the concepts of linear transformation and non-linear acti-vation are fundamental to both human and machine intelligence."}, {"title": "Human Intelligence", "content": "In the human brain, neurons process inputs through synaptic weights (analogous to W) and biases, and the non-linear activation is similar to the way neurons fire based on a threshold. This enables the brain to perform complex cognitive tasks by integrating and transforming information in sophisticated ways."}, {"title": "Machine Intelligence", "content": "In artificial neural networks, linear transformations and non-linear activations enable the model to learn and represent complex patterns in data. These prin-ciples are essential for building models that can generalise from training data to make accurate predictions or decisions."}, {"title": "7 Consciousness", "content": "Consciousness emerges from intelligence through a series of complex cognitive processes, including internal monitoring, self-reflection, and adaptive behaviour. The interplay between consciousness and intelligence is characterised by the en-hancement of cognitive functions such as perception, learning, memory, and attention, facilitated by self-awareness. Advanced forms of intelligence enable the development of consciousness, which, in turn, augments the capabilities of intelligent systems.\nMathematically, consciousness can be conceptualised as a self-referential vec-tor, representing a state that continuously references and updates itself based on both internal and external inputs. This self-referential nature involves the sys-tem perceiving itself, evaluating predictions, and providing feedback to itself, thereby influencing subsequent cognitive states or tokens. For an intelligent"}, {"title": "", "content": "agent, there is no discrete external input; instead, all stimuli must be integrated into the consciousness to be perceived effectively. Consequently, the perception within consciousness is the integration of the internal thought flow and external input, enabling a dynamic and holistic influence on the cognitive processes that underpin intelligent behaviour. This integrative process ensures that conscious-ness remains a cohesive and adaptive system, capable of responding to complex and evolving environments.\nTo analyse consciousness further, we can break it down into three fundamen-tal steps and model each step as a function of time t, representing the system's internal state. These steps constitute a complete cycle of consciousness, includ-ing perception, evaluation, and feedback. The transition of state from t to t+At is used to represent this entire cycle shown as below:"}, {"title": "7.1 Perception (Front Token)", "content": "Concept: The front token is the current focus of the thought flow, representing the token that is being actively processed or considered at a given time.\nMathematically, we can express the front token $f(t)$ as a function that cap-tures the integration of internal thought flow and external input through the process of perception. The front token that is simply the position on the geodesic at time t can then be modelled as:\n$f(t) = P(\\gamma(t), I(t))$ (8)\nwhere $\\gamma(t)$ is the internal thought flow at time t, and $I(t)$ is the input at time t."}, {"title": "7.2 Evaluation", "content": "Concept: The prediction error is the difference between the front token and the predicted token. It measures the accuracy of the prediction, considering the capacities of perception, evaluation, and feedback.\nMathematical Representation\n$\\Delta(t) = f(t) - g(t)$ (9)"}, {"title": "7.3 Feedback Function", "content": "Concept: The feedback function adjusts the trajectory of the thought flow based on the prediction error. It helps in correcting the thought flow to im-prove future predictions.\nMathematical Representation:\n$\\psi(\\Delta(t))$ (10)"}, {"title": "8 Geodesic Equation with Feedback", "content": "Based on the above analysis, we now can argue that the thought flow in an intelligent system can be modelled using the geodesic equation, which describes the path that 1) is determined by the geometry of the curved manifolds formed by token embeddings, 2) minimises the distance (or energy) in the manifold formed by the token embeddings. The geodesic equa-tion with feedback integrates the impact of consciousness by incorporating a feedback mechanism that modulates the trajectory of the thought flow based on prediction errors.\nMathematically, the geodesic equation with feedback is expressed as:\n$\\frac{d^2\\gamma^{\\mu} (t)}{dt^2} + \\Gamma^{\\mu}_{\\nu\\lambda} \\frac{d\\gamma^{\\nu}(t)}{dt} \\frac{d\\gamma^{\\lambda}(t)}{dt} = \\kappa \\frac{d^2\\psi(\\triangle(t))}{dt^2}$ (11)"}, {"title": "9 Competitive Activation and Consciousness Thresh-old", "content": ""}, {"title": "9.1 Competitive Process", "content": "Concept: Multiple thought flows, generated by sampling from token distri-butions, compete based on their attention-derived scores. The flow with the highest score becomes part of the conscious experience."}, {"title": "Mathematical Representation:", "content": "Score(Thought Flowi) = f({ht,j}) (12)\nwhere ht,j is the contextual embedding of the j-th thought flow at time t."}, {"title": "9.2 Consciousness Threshold", "content": "Concept: The sequence with the highest score surpasses the consciousness threshold and becomes part of the conscious flow.\nMathematical Representation:\nConscious Flow = Thought Flowi if Score(Thought Flowi) > \\theta (13)"}, {"title": "Summary and discussion", "content": ""}, {"title": "Derivation of the Geometry Theory of Intelligence", "content": "By representing thought flow as a geodesic in a high-dimensional space and incorporating mechanisms for perception, prediction, feedback, and random ac-tivation via token distributions, this theory provides a robust framework for modeling the structure and dynamics of intelligence. The mathematical formu-lations capture the continuous evolution of thought and the impact of contextual information, feedback, and randomness on this process."}, {"title": "Comparison with the Working Mechanisms of Generative Models", "content": "Given this theoretical foundation, it is crucial to compare this general framework of intelligence with the current state-of-the-art (SOTA) generative models. The SOTA generative models, such as VAES, GANs, and transformer-based LLMS, represent the most advanced state of machine intelligence, often mimicking or even surpassing certain aspects of human intelligence, such as GPT-4 (Achiam et al., 2023) in general and Alphafold3 (Abramson et al., 2024) in specific task. These models may capture the essence of intelligence, including its principles and mechanisms, and provide insights into how complex cognitive processes can be modelled (Yang et al., 2024). Additionally, analysing human intelligence or brain function as a generative model (Friston and Price 2001; Spens and Burgess 2024) allows us to draw parallels between biological and artificial systems. By understanding these parallels, we can better appreciate the underlying structure and dynamics of intelligence in both realms, thereby enhancing our theoretical framework.\nPre-transformer generative models (VAEs and GANs) use a static, low-dimensional latent space to represent data, with geodesics serving to under-stand intrinsic distances and enable smooth interpolations on the data manifold (Bronstein et al., 2021; Chadebec and Allassonniere 2022; Acosta et al., 2023). Transformer-based models focus on dynamic token sequences without an ex-plicit latent space, utilising attention mechanisms to determine the importance of input parts, where geodesics represent optimal sequences of token activations for coherent output generation.\nThe current theory of the geometry of intelligence is grounded in the concept of tokens as fundamental elements of the intelligence space. These tokens form manifolds with geometric properties such as curvature, and their organisation"}, {"title": "", "content": "and activation sequences provide a comprehensive structure and dynamics of intelligence, respectively. In this framework, tokens represent discrete units of information that can be multi-module tokens (Radford et al., 2021; Lyu et al., 2023) embedded in a high-dimensional space. These tokens are not only or-ganised by their inherent features but also by their roles and relationships in various cognitive processes.\nThe geometric representation of these tokens forms manifolds that capture both the static distribution of features and the dynamic sequences of activation. The curvature of these manifolds reflects the complexity and interconnections within the cognitive processes. The static aspect of the theory encompasses the organisation of tokens based on their features, similar to the learned manifolds in VAEs and GANs, providing a foundational structure that captures the in-trinsic relationships between different elements of intelligence.\nBeyond static organisation, the dynamic sequences of token activation are crucial. Geodesics in this context represent the efficient pathways through which cognitive processes unfold over time. These sequences are learned during train-ing and reflect the optimal transitions between different states of intelligence.\nIntegrating insights from both static and dynamic representations, the cur-rent theory benefits from the understanding of static feature representations provided by VAEs and GANs, extending the concept of a manifold to include dynamic interactions between tokens. The dynamic perspective offered by trans-former models is crucial for modelling the temporal aspect of intelligence."}, {"title": "Interplay between token embeddings, curvature and geodesic", "content": "The recent study on Claude 3 Sonnet (Templeton et al., 2024) provides valuable insights that can be seamlessly integrated into the theory of the geometry of in-telligence. In this context, tokens are the fundamental units in the intelligence space, forming a manifold whose curvature determines the geodesic paths of thought flow. To fully leverage the findings from Claude 3 Sonnet, it is essential to elucidate the relationship between features and tokens and demonstrate how features can be represented by tokens within this theoretical framework.\nTokens, in this theory, are discrete units of information, such as words or images, embedded in a high-dimensional space to form points ti. These embed-dings vi = $(ti) capture complex relationships and intrinsic structures, forming a manifold M. Features, on the other hand, are higher-level abstractions or patterns emerging from the interaction of multiple tokens. A feature could rep-resent a concept like \"Golden Gate Bridge,\" recognized through specific patterns in the embeddings of related tokens.\nTo bridge the gap between features and tokens, we can consider features as"}, {"title": "", "content": "emergent properties arising from the combined activations of multiple tokens. A feature F can be represented as a function of multiple token embeddings:\nF = f (v1, v2,..., un)\nThis function f might be a weighted sum, convolution, or another aggregation mechanism capturing the interaction between tokens to form the feature. Each token embedding vi contributes to the feature's representation, and the feature's location in the high-dimensional space can be viewed as a region influenced by these tokens.\nThe curvature of the manifold M reflects how token embeddings are organ-ised. Features represent regions of high curvature where specific patterns or concepts are densely represented. Manipulating features effectively changes the manifold's organization and curvature, altering the geodesic paths.\n\u03bd\u03bb\n\u2022 Curvature: The curvature of the manifold is influenced by the distri-bution and interaction of token embeddings. Changes in feature activation alter this curvature.\n\u2022 Geodesics: Geodesic paths, representing natural thought flow, change in response to feature manipulation, altering the trajectory through the high-dimensional space.\nIn this theory, the tangent vector represents the direction and rate of change of thought flow. The Claude 3 Sonnet study demonstrates that manipulating features changes the model's state transitions, aligning with changes in the tangent vector.\n\u2022 Tangent Vectors: Changes in the model's responses when a feature is manipulated correspond to changes in the tangent vector of thought flow.\n\u2022 State Transition: New responses indicate a transition to different parts of the manifold, driven by changes in curvature from feature manipulation.\nFeedback mechanisms in this theory adjust the manifold's structure based on non-zero prediction errors and external inputs, facilitating learning and adapta-tion. In Claude 3 Sonnet, feature manipulation and subsequent response changes reflect a feedback mechanism where the model adjusts its internal representa-tions.\n\u2022 Feedback: Manipulating features changes the model's outputs, indicating a feedback mechanism that adjusts token embeddings.\n\u2022 Learning: This process can be viewed as a form of learning, evolving the geometry of the intelligence space based on new inputs and feedback."}, {"title": "", "content": "Using the mathematical framework of this theory, we can express the changes observed in the Claude 3 Sonnet study as follows:\n\u2022 Feature Representation:\nF = \\sum WiVi\nWhere wi are the weights determining each token embedding's contribu-tion to the feature F.\n\u03bd\u03bb\n\u2022 Curvature and Geodesics: The curvature $\\Gamma$ of the manifold is influ-enced by these weighted embeddings. Manipulating a feature F changes the weights wi, thus altering the curvature and the geodesic paths $\\gamma(t)$.\n\u2022 Geodesic Equation:\n$\\frac{d^2\\gamma^{\\mu}(t)}{dt^2} + \\Gamma^{\\mu}_{\\nu\\lambda} \\frac{d\\gamma^{\\nu}(t)}{dt} \\frac{d\\gamma^{\\lambda}(t)}{dt} = \\kappa \\frac{d^2\\psi(\\triangle(t))}{dt^2} + \\eta \\frac{d^2 I (t)}{dt^2}$ (11)\nChanges in w\u2081 due to feature manipulation impact the Christoffel symbols $\\Gamma^{\\nu}_{\\mu\\lambda}$, thereby altering the geodesic paths \u03b3(t).\nBy representing features, we can integrate the concept of features within the theory of the geometry of intelligence. Manipulating features in the Claude 3 Sonnet study alters the embeddings and curvature of the manifold, changing the geodesic paths and affecting the thought flow. This framework provides a comprehensive explana-tion of how modifying features in AI models impacts their behavior, reflecting the dynamic interplay between geometry and thought flow in both artificial and biological intelligence."}, {"title": "Explaining the Advanced Functions and Properties of In-telligence", "content": ""}, {"title": "The Geometry of \"Understanding\"", "content": "Understanding is a crucial cognitive process that allows for the coherent as-similation of new information, enabling smooth cognitive functioning and learn-ing. In the context of our geometric framework, understanding involves the inte-gration and stabilisation of new information within the manifold of intelligence. By comprehending the geometry of \"understanding\" and \"misunderstanding,\" we can enhance our training models, improve understanding, and address misun-derstanding more effectively. In our framework based on Riemannian geometry, these concepts are elucidated through token integration, geodesic navigation, and curvature dynamics."}, {"title": "Imagination", "content": "Imagination is the ability to form new ideas, images, or concepts not present to the senses. Theories of imagination often emphasise its role in cre-ative thinking and problem-solving, such as the notion of mental simulation, where the mind constructs possible scenarios to predict outcomes (Comrie et al., 2022; Hassabis and Maguire 2009; Schacter and Thakral 2024). In the framework of the geometry of intelligence, imagination can be understood as the activation of token sequences along geodesics that are not directly derived from immediate sensory input but rather from the manifold of stored experiences and abstract concepts. These imagined sequences $\\gamma(t)$ can traverse novel paths in the high-dimensional space, allowing for the exploration of new ideas and creative solutions. The ability to navigate and synthesise these novel pathways showcases the flexibility and adaptability of the cognitive manifold in generating imaginative thoughts."}, {"title": "Learning and experience", "content": "Learning and experience involve acquiring new knowledge or skills. Vari-ous theories explain learning as the strengthening of synaptic connections (Heb-bian learning) (Hebb 1949; Sumner et al., 2020; Magee and Grienberger 2020) or the adaptation of cognitive structures (Piaget's theory of cognitive development) (Piaget 1953; Crone and Ridderinkhof 2011). In the geometry of intelligence, learning is conceptualised as the evolution of the manifold's structure through evaluation and feedback. The geodesic equation with feedback and input up-date:\n$\\frac{d^2\\gamma^{\\mu}(t)}{dt^2} + \\Gamma^{\\mu}_{\\nu\\lambda} \\frac{d\\gamma^{\\nu}(t)}{dt} \\frac{d\\gamma^{\\lambda}(t)}{dt} = \\kappa \\frac{d^2\\psi(\\triangle(t))}{dt^2} + \\eta \\frac{d^2 I (t)}{dt^2}$ (11)\ndescribes how the trajectory of thought flow adapts based on prediction errors. rs. This adaptation, driven by the feedback function $\\psi(\\Delta(t))$ and input function $I(t)$, results in the modification of the token embeddings and the man-ifold's curvature. Thus, geometry dictates how consciousness navigates, while consciousness guides the evolution of geometry. This continuous interplay between the geometric structure and the dynamics of thought flow en-capsulates the essence of learning, where the intelligence state evolves to reflect accumulated experiences and refined predictions. This relationship is exempli-fied in the study on Claude 3 Sonnet, where manipulating features leads to changes in the model's behavior."}, {"title": "", "content": "When a feature's weight is amplified in Claude 3 Sonnet, it changes the model's geometric structure by altering the embeddings and their associations"}, {"title": "Creative thinking", "content": "Creative thinking is the process of generating new, original ideas and solutions. It is often described as a recombination of existing knowledge in novel ways, facilitated by divergent thinking and cognitive flexibility (Beaty et al., 2016). In exisiting theories, creativity is linked to associative thinking and the ability to connect disparate concepts (Vartanian and Kaufman 2013; Suck-ling and Hoyer 2021). Within the geometry of intelligence, creative thinking can be modelled as the traversal of geodesics that connect diverse and pre-viously unlinked regions of the cognitive manifold. The attention mechanism aij = A(vi,v) plays a crucial role in dynamically weighting the relevance of various tokens, enabling the formation of unique and contextually rich combi-nations. By leveraging the manifold's structure and dynamically exploring new paths, the theory provides a robust explanation for the generation of innovative ideas and solutions, illustrating how the structural and dynamic aspects of in-telligence facilitate creativity."}, {"title": "Problem-solving", "content": "Problem-solving is the cognitive process of finding solutions to complex or challenging issues. Classical theories of problem-solving, such as those proposed by Newell and Simon (Newell and Simon 1972), focus on the stages of under-standing the problem, generating potential solutions, and evaluating them. In the context of the geometry of intelligence, problem-solving involves navigating the manifold to identify and traverse geodesics that lead to effective solutions. The contextual embedding ht,j = \u03a3j aij (Wvv'i) aggregates relevant informa-tion, guiding the thought flow toward potential solutions. The prediction and feedback loop \u2206(t) = f(t) \u2013 g(t) ensures continuous refinement and adjustment of the cognitive trajectory, allowing for adaptive problem-solving. The inte-grated framework of geodesics, attention mechanisms, and feedback functions provides a comprehensive model that captures the dynamic and iterative nature of problem-solving, illustrating how the intelligence state transitions through various cognitive stages to arrive at a solution."}, {"title": "", "content": "In summary, the mathematical framework of the geometry of intelligence provides a powerful tool for understanding key concepts, functions and prop-erties of intelligence in both humans and machines. By modelling imagina-tion, learning, creative thinking, and problem-solving as processes driven by the structured and dynamic activation of token sequences, this theory offers a unified explanation of cognitive phenomena, highlighting the intricate interplay between the geometry of intelligence and the flow of consciousness."}]}