{"title": "Breaking Through the Spike: Spike Window Decoding for Accelerated and Precise Automatic Speech Recognition", "authors": ["Wei Zhang", "Tian-Hao Zhang", "Chao Luo", "Hui Zhou", "Chao Yang", "Xinyuan Qian", "Xu-Cheng Yin"], "abstract": "Recently, end-to-end automatic speech recognition has become the mainstream approach in both industry and academia. To optimize system performance in specific scenarios, the Weighted Finite-State Transducer (WFST) is extensively used to integrate acoustic and language models, leveraging its capacity to implicitly fuse language models within static graphs, thereby ensuring robust recognition while also facilitating rapid error correction. However, WFST necessitates a frame-by-frame search of CTC posterior probabilities through autoregression, which significantly hampers inference speed. In this work, we thoroughly investigate the spike property of CTC outputs and further propose the conjecture that adjacent frames to non-blank spikes carry semantic information beneficial to the model. Building on this, we propose the Spike Window Decoding algorithm, which greatly improves the inference speed by making the number of frames decoded in WFST linearly related to the number of spiking frames in the CTC output, while guaranteeing the recognition performance. Our method achieves SOTA recognition accuracy with significantly accelerates decoding speed, proven across both AISHELL-1 and large-scale In-House datasets, establishing a pioneering approach for integrating CTC output with WFST.", "sections": [{"title": "I. INTRODUCTION", "content": "Automatic speech recognition (ASR) technology has seen remarkable growth in recent years [1]\u2013[5]. Recently, contemporary end-to-end (E2E) ASR systems [6] have gained popularity, surpassing traditional methods [7], [8] by eliminating the need for iterative alignment and requiring only a single model to achieve superior accuracy, speed, and training simplicity. E2E systems are typically categorized into CTC-based [9], [10], RNN-T-based [11], [12], and AED-based models [13]\u2013[15], as well as hybrid approaches that couple these methods, such as CTC/RNN-T and CTC/AED ASR systems [16]\u2013[18].\nWhile E2E ASR systems excel in general-purpose applications, relying solely on an acoustic model often proves inadequate in specialized or vertical scenarios. In these scenarios, an additional language model is typically used to refine the final recognition output. A common approach involves generating N-best results from the acoustic model and applying shallow fusion with the language model to determine the highest-scoring sequence [19], [20]. Previous studies also investigated generating N-best results from the CTC encoder, followed by non-autoregressive combination in the decoder to achieve optimal outcomes [21]. WFST [22]\u2013[24] is extensively utilized in industrial ASR systems due to its high interpretability and scalability. EESEN [25] pioneers the integration of E2E ASR with WFST, introducing the CTC-TLG paradigm, which using the CTC model to predict phone posteriors at each frame, followed by Viterbi beam search on a modified WFST network, achieving comparable performance with traditional HMM systems.\nHowever, it is impractical to directly decode the complete CTC posterior output with the WFST due to the significant increasing of decoding latency. Extensive prior researches have been conducted to address this issue. Chen et al [26] proposed the CTC Lattices based Phone Synchronous Decoding (LSD) algorithm which removed the redundancy caused by blank frames in the CTC output. Speech-LLaMA [27] indicated that the blank frames in the CTC output still encode pertinent information, and an averaging technique was applied to these blank frames. Poly Voice [3] employed a method that systematically discards all blank frames. In this paper, we posit that blank frames in close proximity to non-blank spikes harbor the most essential semantic information. We assert that selectively incorporating a limited subset of these neighboring blank frames into the WFST decoding represents an optimal strategy for enhancing both performance and decoding efficiency. Building on this hypothesis, we introduce the Spike Window Decoding (SWD) algorithm, which leverages our novel spike window function to construct sequence of windowed neighborhoods centered around the non-blank spikes. Compared to the LSD and Discarding algorithm, our method not only acknowledges but also empirically validates the beneficial quality of the information encoded in blank frames adjacent to CTC spikes, thereby substantially elevating the system's performance ceiling. Additionally, unlike the vanilla density strategy and the averaging strategy, which utilize all blank frames, the SWD algorithm adeptly mitigates the adverse effects of redundant blank frames that contain non-contributory information. In summary, we present a novel and state-of-the-art (SOTA) paradigm for utilizing CTC outputs in the WFST decoding, which strategically harnesses the intrinsic properties of CTC spikes to attain exceptional performance while preserving decoding efficiency.\nWe present a comprehensive evaluation of our proposed SWD algorithm through extensive experiments conducted on the widely utilized AISHELL-1 [28] Mandarin dataset, as well as a large-scale In-House dataset comprising 43000 hours of speech data. To establish a rigorous ASR baseline, we first develop a CTC/AED-based acoustic model and a GPU-optimized WFST [29], achieving results that set a new benchmark for SOTA performance. Subsequently, we thoroughly explore the SWD algorithm, investigating its application to speech frames with left-neighboring, right-neighboring, and window-neighboring configurations. As a result, our method achieves Character Error Rates (CER) of 3.89% and 2.09% on the AISHELL-1 and In-House datasets, respectively, surpassing both baseline models and previous SOTA approaches in recognition accuracy. Concurrently, the proposed SWD algorithm significantly enhances inference speed, with improvements of 1.76 and 2.17 times over the baseline method for the respective datasets. This illustrates that our method achieves a remarkable balance between superior recognition performance and enhanced decoding speed across datasets of different scales.\nIt robustly establishes our approach as a pioneering paradigm for decoding WFST with CTC outputs, advancing the SOTA in this domain."}, {"title": "II. RELATED WORK", "content": "A. Hybrid CTC/AED Model\nGiven an input speech feature X, we use Y to represent the label sequence corresponding to X, which a length of L. The jointly loss of hybrid CTC/AED E2E ASR system are provided as:\n$\\begin{aligned}\nH_{encoder} &= Encoder(X) \\\\\nH_{decoder} &= Decoder(H_{encoder}, Y) \\\\\nL_{CTC} &= CTC(H_{encoder}, Y) \\\\\nL_{AED} &= Cross Entropy (H_{decoder}, Y) \\\\\nL &= \\alpha * L_{CTC} + (1 - \\alpha) * L_{AED}\n\\end{aligned}$\nwhere we use T to denote the length of $H_{encoder}$, and $\\alpha$ is a hyper-parameter used to adjust the weight ratio between the encoder and decoder, with its value ranging from [0, 1], which is routinely configured to 0.1 for the rest of the study.\nB. WFST based decoding algorithm\nIn this work, the WFST system is integrated on the encoder side, culminating in the construction of the TLG graph. The process of constructing the static graphs is as follows:\n$TLG = T \\circ min(det(LG))$\nwhere L and G denotes Lexicon.fst and Grammar.fst, respectively; min, det and $\\circ$ are short for minimization, determinization and composition WFST operations. It must be spcified that we adopt GPU-based WFST decoding algorithm and the compact Token.fst (denoted as T) from [29] which ultimately reduces the construction of state complexity from second-order exponential level to linear level while maintaining comparable accuracy."}, {"title": "III. METHODOLOGY", "content": "Our proposed SWD algorithm first retrieves the indexes of the frames with the highest probabilities in the CTC posteriori probability matrix through the index function argmax, and then obtain the indexes corresponding to the non-blank spike frames through the comparison function. Subsequently, the window sequence of neighboring frames is obtained via the innovative Spike Window function. The final decoded sequence is derived from the processed window sequence. Additionally, we employ a weight pushing strategy between det and min during the TLG WFST graph construction. The following sections will provide a detailed explanation of the SWD process.\nA. Spike window decoding\nDrawing on the insight that the frames adjacent to CTC spikes encapsulate crucial information, we introduce the SWD algorithm. Initially, $H_{logits}$ is derived by applying log_softmax to $H_{encoder}$ of Eq. 1. The index function argmax is then employed to identify the index of the maximum value within the posterior probability matrix, followed by calculating the non-zero maximum sequence. Here, 0 denotes the index corresponding to the modeling unit blank, while the remaining indices correspond to the non-blank modeling units represented by $Y_{logits_spike}$. The calculation process for $Y_{logits_spike}$ is detailed as follows:\n$Y_{logits_spike} = argmax(H_{logits}) != 0$\nthereafter, to obtain the sequence of window neighboring frames at spike positions, we introduce the spike window function $F_{sw}$. The calculation principles underlying this function are presented as:\n$H_{logits_sw} = F_{sw} (Y_{logits_spike})$\n$F_{sw} = Concat(H_s, H_{s\\pm1}, ..., H_{s\\pm w}), s \\in Y_{logits_spike}$\nwhere $H_{s\\pm w}$ represents the w-th frame to the left and right of the spliced s-frame, and w denotes the range of the spliced frames. In order to get the final input $H_{logits_final}$ to WFST, we propose the $F_{post}$ function, which operates as follows: Initially, remove the frames in $H_{logits_sw}$ which exceed the upper and lower limits of the numbering range of $Y_{logits_spike}$; Subsequently, duplicate elements are eliminated and the remaining elements are sorted; Finally, mapping the sequence of frames after window neighboring to the corresponding $H_{logits}$ and obtaining $H_{logits_final}$. Fig. 1 illustrates the schematic of the SWD algorithm, with black blocks representing the window sequence in the final SWD output. The algorithm effectively leverages the blank frames surrounding non-blank spikes, optimizing their contribution to the decoding process.\nConventional algorithms for WFST decoding precedure on CTC outputs require iterating over all frames in an autoregressive mode, wherein the states involved in WFST computation span the entire frame sequence, which leads to slow decoding speeds. However, SWD algorithm reduces the number of frames involved in WFST decoding procedure to be linearly proportional to the number of non-blank spikes in the CTC outputs. The theoretical proof proceeds as follows: First, we assume that all non-blank spikes are non-overlapping and that any two non-blank spikes are separated by more than K \u00d7 W frames. Second, we assume that the distance of the first and last non-blank spike frames from the 0-th and T-th frames of the CTC outputs is less than the W frames. So that we can get:\n$L_{SWD} = N * (K * W + 1)$\nwhere $L_{SWD}$ represents the total number of frames after applying the SWD algorithm, N denotes the total number of non-blank spikes,\nand K denotes the window coefficient, where K = 1 accounts for either the left or right frame, and K = 2 accounts for both left and right frames. Given that K \u00d7 W is set to a small value and N is much smaller than T, the output length of SWD is significantly reduced compared to the original CTC output. It is true that in practice there are cases where SWD does not fulfill the above two assumptions, such as in Fig. 1 where the two non-blank spikes are adjacent to each other, but in this case $L_{SWD}$ will only be smaller, since this means that fewer blank frames are considered. This explains why the inference efficiency is enhanced by the proposed SWD algorithm.\nB. TLG graph optimization\nThe major method of constructing CTC TLG graphs in existing work is depicted as Eq. 6. However, when dealing with large-scale language models, the constructed TLG graph will become significantly large, leading to increased memory usage, storage space, and a more complex inference process. To mitigate these issues, this work introduces the weight pushing operation to shift the weights of the corresponding sequence forward, which is applied between the det and min operation. This method allows for the pruning of low-probability paths in advance without compromising accuracy, thereby improving both the search space and speed. The construction process of static graphs is as follows:\n$TLG = T \\circ min(push(det(LG)))$"}, {"title": "IV. EXPERIMENT", "content": "A. Dataset\nWe conduct experiments on AISHELL-1 and a large-scale In-House dataset, on which the AISHELL-1 dataset is a 178-hour Mandarin dataset with a sampling rate of 16000 Hz. Our In-House dataset is also a Mandarin dataset which sampling rate is 8000 Hz, on which the training set contains about 43000 hours of labeled speech data, and the test set includes about 20 hours of labeled data for hundreds of vertical scenes.\nB. Model architecture\n1) Acoustic model settings: The acoustic model constructed in this work is a multi-task Hybrid CTC/AED structure, in which the encoder and decoder models are based on the Zipformer and transformer models, respectively. On the AISHELL-1 dataset, the encoder follows the medium size Zipformer in [30], the decoder side is based on the transformer standard model, which has 6 transformer blocks. For experiments on the In-Hourse dataset, the decoder is as the same as the one set in the AISHELL-1 dataset; for the encoder side, we use a configuration with a larger number of parameters. The final sizes of the acoustic models constructed on the AISHELL-1 and In-House datasets are 155.9M and 316.7M, respectively, and we will refer to the larger model as -XL in the following expriments.\n2) Language model settings: For the AISHELL-1 dataset, the language model is constructed using only its 120098 training sets, and its 3-gram, 5-gram, and 7-gram language models are trained using the Srilm tool \u00b9. On the In-House dataset, we use about 1.5 billion pieces of Mandarin dialog text data to construct the language model. The operations composition, det, min and weight pushing introduced in Section III are implemented using the Openfst tool 2.\n3) Training, inference and evaluation: During the training stage, 80-dimensional filter banks are extracted as speech features, with a frame length of 25 ms and a frame shift of 10 ms. To augment the data, a speech speed perturbation [36] is used, using perturbation coefficients of 0.9, 1.0, and 1.1. Furthermore, the SpecAugment [37] strategy is also used to enhance the robustness of the model. All models are trained on 16 NVIDIA Tesla A800 GPUs (80G) with mixed precision training. For inference, all our computations are consistently performed on a Tesla T4 GPU (16GB) with a 16-core CPU and 32GB of RAM. In order to achieve competitive results, AISHELL-1 and In-House dataset are trained for 200 epochs and 20 epochs, respectively. For the inference stage, we evaluate the recognition performance using the standard CER which is calculated by determining the Levenshtein distance [38] between the labeled sequence and the predicted sequence.\nC. Experimental results\n1) Benchmark modeling: To rigorously validate the efficacy of the proposed SWD algorithm in both recognition accuracy and inference speed, we first establish a baseline model, referred to as A1, using a CTC/AED architecture based on zipformer. As shown in Table 1, we perform experiments on the AISHELL-1 dataset and when decoding with greedy search by CTC only, our implemented model achieves SOTA results of 3.98% and 4.19% CER on the validation and test sets, respectively. The subsequent experiments are builded on our SOTA model, further substantiate the effectiveness and persuasiveness of the proposed method.\nExperiments F1 through F9 present the results of applying the SWD algorithm proposed in this work. The notation {-1,0} in F1 represents a scenario where only the left neighbor of the CTC spike is spliced, while F1 through F3 denote the progressively increasing in the range of neighboring frames on the left side of the splice. Experiments F4 through F6 focus on splicing the right neighbor, and experiments F7 through F9 explore the effects of simultaneously splicing both the left and right neighbor spikes. The experimental findings reveal that there are no substantial differences in recognition accuracy and inference speed between splicing only the left neighbor and splicing only the right neighbor. However, compared to El, the discarding strategy will result in a huge accuracy degradation, this suggests that the left and right neighbors of non-blank spikes carry similar semantic information which is essential to maintain the system performance. Meanwhile, simultaneous splicing of both left and right neighbors yields a 7.29% and 7.16% relative reduction in CER in the validation and test sets compared to A1, respectively, along with a 1.76-times improvement in inference speed compared to the B2 system. These results further corroborate the effectiveness of the SWD algorithm in improving both the accuracy and inference speed of the model, demonstrating its potential for practical applications.\n3) Experiment of the In-House dataset.: To further substantiate the effectiveness of the SWD algorithm on large-scale datasets, we train a 5-gram TLG model with 1.5 billion internal textual dialog samples, coupled with a 316.7 million parameter acoustic model. Remarkably, as the volume of training data and the model's parameter count increase, the SWD algorithm demonstrates that only one frame adjacent to both the left and right of each spike is sufficient to surpass the performance of the baseline model, even surpassing the performance of adding more frames to both the left and right sides. Furthermore, this approach contributes to a significant enhancement in inference speed, achieving a 2.17-fold improvement over vanilla dense methods.\nOur findings suggest that frames adjacent to non-blank spikes encapsulate rich contextual information. By harnessing this neighboring information, SWD not only captures the most critical spikes but also leverages the additional context, leading to a significant boost in performance. This approach, which we describe as Breaking Through the Spike, surpasses traditional dense frame selection methods and simply utilizing CTC spikes strategies. Building on this, we further hypothesize that if arbitrary inserting a blank unit near non-blank spikes might achieve accuracy comparable to dense frame selection. These findings introduce a novel and effective paradigm for leveraging CTC outputs, offering new insights for further optimization."}, {"title": "V. CONCLUSION", "content": "In this paper, we thoroughly explore the spiking behavior of CTC outputs and propose the conjecture that frames adjacent to non-blank spikes contain semantic information beneficial to the model. Building on this, we present a novel approach to enhance both inference speed and recognition accuracy of CTC-based end-to-end ASR systems named Spike Window Decoding algorithm. By capitalizing on the semantic richness of frames adjacent to non-blank spikes, our method enables a more efficient integration with WFSTs, drastically reducing the number of frames involved in decoding. Additionally, we introduce a weight pushing optimization between the det and min steps, improving TLG search efficiency through early pruning. The experimental results on both AISHELL-1 dataset and large-scale In-House evaluations confirm that the SWD algorithm can significantly enhance inference speed while even improving recognition accuracy.\nIn future work, we aim to extend the SWD algorithm by formalizing the semantic implications of blank frames, providing a theoretical foundation for speech recognition systems that utilize CTC as the objective function during decoding."}]}