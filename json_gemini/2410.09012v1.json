{"title": "Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models", "authors": ["Hao Li", "Cor-Paul Bezemer", "Ahmed E. Hassan"], "abstract": "Foundation models (FMs) such as large language models (LLMs) have significantly impacted many fields, including software engineering (SE). The interaction between SE and FMs has led to the integration of FMs into SE practices (FM4SE) and the application of SE methodologies to FMs (SE4FM). While several literature surveys exist on academic contributions to these trends, we are the first to provide a practitioner's view. We analyze 155 FM4SE and 997 SE4FM blog posts from leading technology companies, leveraging an FM-powered surveying approach to systematically label and summarize the discussed activities and tasks. We observed that while code generation is the most prominent FM4SE task, FMs are leveraged for many other SE activities such as code understanding, summarization, and API recommendation. The majority of blog posts on SE4FM are about model deployment & operation, and system architecture & orchestration. Although the emphasis is on cloud deployments, there is a growing interest in compressing FMs and deploying them on smaller devices such as edge or mobile devices. We outline eight future research directions inspired by our gained insights, aiming to bridge the gap between academic findings and real-world applications. Our study not only enriches the body of knowledge on practical applications of FM4SE and SE4FM but also demonstrates the utility of FMs as a powerful and efficient approach in conducting literature surveys within technical and grey literature domains. Our dataset, results, code and used prompts can be found in our online replication package at https://github.com/SAILResearch/fmse-blogs.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the rapid advancements in machine learning (ML) have fundamentally transformed various fields, including software engineering (SE). Among these developments, foun-dation models (FMs) such as large language models (LLMs) have emerged as a major force, reshaping how software is developed, tested, and maintained [17]. The interaction between SE and FMs has led to the emergence of two key trends: (1) FMs for SE (FM4SE), where FMs are leveraged to automate or enhance various SE tasks, such as code generation and testing, and (2) SE for FMs (SE4FM), where SE practices are adapted to the development and deployment of FMs.\nAcademic research has made significant strides in exploring these trends, but literature surveys have only focused on published, peer-reviewed literature [17], [28], mostly leaving out the perspectives and experiences of industry practitioners. The input of practitioners, who work at the intersection of SE and FMs in real-world settings, is a crucial yet under-explored source of insights. While the research community has recognized the value of user-generated contents such as Q&A websites [2] and issue reports [32], less attention has been paid to grey literature such as technical blog posts from industry leaders. Tech companies publish blog posts for several reasons, including positioning themselves as innovation leaders and establishing thought leadership [4]. As a result, these blog posts often provide in-depth discussions on cutting-edge challenges and solutions in SE and FM integration.\nTo bridge the gap between academic findings and industry practices, we analyze blog posts from leading technology com-panies, focusing on how practitioners discuss the challenges and approaches related to FM4SE and SE4FM. By system-atically labelling and examining these blog posts, we seek to provide a clearer picture of how FMs are being integrated into the SE domain (i.e., FM4SE), and how SE principles are being applied to FMs (i.e., SE4FM) in industry. This study stands out by offering a synthesized industry voice, derived directly from real-world, practitioner-driven insights. We employ an ensemble of FMs as judges [50] into an FM/LLM Jury [42] (see Figure 1) to assist with the labelling and synthesis of knowledge within 155 FM4SE and 997 SE4FM blog posts."}, {"title": "Our study focuses on these research questions (RQs):", "content": "RQ1. Which FM4SE activities are discussed in industry blog posts? Software development tasks, particularly code generation, are the most frequently discussed across FM4SE blogs. FMs are increasingly integrated as code assistants, providing developers with multifunc-tional tools to boost productivity. Vulnerability detec-tion is the dominant software quality assurance task, while software maintenance activities primarily focus on refactoring and transforming existing codebases.\nRQ2. Which SE4FM activities are discussed in industry blog posts? The most discussed activities in SE4FM blog posts are model deployment & operation, with a focus on cloud hosting and model serving & scaling. Other trends include prompt chaining, workflow or-chestration, and building AI agents. Data management activities focus on RAG and vector databases to sup-port unstructured data and information retrieval. Model customization relies on fine-tuning methods such as full fine-tuning, LoRA, and RLHF."}, {"title": "The main contributions of this paper are:", "content": "\u2022\n\u2022 The first study of industry blog posts on FM4SE and SE4FM to provide the practitioner's view on these emerg-ing and crucial topics in today's software industry.\n\u2022 A dataset of 1,152 blog posts from top technology companies related to FM4SE or SE4FM.\n\u2022 A list of eight research directions that are driven by the findings of our survey on blog posts.\n\u2022 A demonstration of an efficient approach that leverages a jury of FMs to assist with grey literature surveys on SE-related topics."}, {"title": "Paper Organization.", "content": "The rest of this paper is organized as follows. Section II presents background information and related work. Section III details the proposed FM/LLM Jury framework. Section IV presents our methodology. Sections V and VI present the findings of our research questions. Sec-tion VII discusses promising future research directions that follow from our survey. Section VIII discusses the threats to the validity our study. Section IX concludes this paper."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": ""}, {"title": "A. LLM-as-a-judge", "content": "Leveraging FMs/LLMs as evaluators, or LLM-as-a-judge, has emerged as a scalable alternative to traditional human evaluations to assess the quality of outputs from LLMs [21]. This assessment is not straightforward, as LLMs generate natural language, which needs to be compared with a ground truth semantically. LLM-as-a-judge builds on the idea that state-of-the-art models, especially those trained with Rein-forcement Learning from Human Feedback (RLHF) [36] (e.g., GPT-4) are well-aligned with human judgments, making them promising substitutes [50]. While the use of FMs like GPT-4 as evaluators has become more common, these models often exhibit biases, such as favouring their own outputs over those from other models [38]. To mitigate these biases, researchers have proposed the use of a panel of FM evaluators instead of relying on a single model [42]. Instead of using a single FM/LLM to evaluate FM/LLM outputs, in this paper, we propose using a jury of FMs/LLMs to assist with the labelling and summarization of industry blog posts."}, {"title": "B. Related work", "content": "The work that is closest related to our work consists of other literature surveys on FM4SE and SE4FM. Recent comprehensive surveys on FM4SE have examined the rapidly growing field of FMs/LLMs applied to SE activities and tasks. Hou et al. [17] conducted a systematic survey of 395 studies covering the application of LLMs to 84 specific SE tasks across 6 SE activities. In addition, Wang et al. [44] surveyed 102 studies that have used LLMs for software testing. These applications have shown promise, but several challenges remain. For example, Fan et al. [12] emphasized technical challenges like hallucinations when applying LLMs for SE, and highlighted the importance of hybrid techniques that combine traditional SE with LLMs.\nOther surveys (on SE4FM) focus on how established SE practices can be adapted to support building, testing, deploying and maintaining FMs. Chang et al. [5] reviewed evaluation methods and benchmarks for LLMs in different areas such as education and social sciences, highlighting the importance of robust benchmarks to assess the performance of LLMs. Most prior surveys on SE for models have focused on SE4ML rather than SE4FM (i.e., SE for machine learning models that are not foundation models). Mart\u00ednez-Fern\u00e1ndez et al. [28] reviewed 248 studies and classified them based on the Software En-gineering Body of Knowledge (SWEBOK), identifying gaps in areas like maintenance and data handling. Villamizar et al. [43] discussed gaps in requirements engineering for ML, while Masuda et al. [29] surveyed quality assurance approaches, highlighting the need for specialized testing techniques in verifying an ML system's output.\nAll prior surveys on FM4SE and SE4FM focused on aca-demic efforts. Our work is the first to provide an overview of FM4SE and SE4FM activities in top technology companies."}, {"title": "III. USING AN FM/LLM JURY FOR LABELLING BLOG POSTS", "content": "Labelling blog posts using a single frontier FM (e.g., GPT-40) in the LLM-as-a-judge approach [50] can be both expen-sive and potentially biased. To address these limitations, we propose FM/LLM Jury, a methodology that leverages multiple FMs to collaboratively label blog posts. In this framework, each model provides a label along with a confidence score, and these outputs are merged using a majority vote to determine a final label. This framework is inspired by Verga et al. [42]."}, {"title": "A. Constructing the prompt", "content": "The prompt construction process is iterative and consists of the following key steps:\nStep 1 - Create the golden dataset. To evaluate the performance of the prompts that we send to the FM and the quality of our FM/LLM Jury, we begin by constructing a golden dataset. We randomly sample a subset of blog posts and manually label them to serve as ground truth.\nStep 2 \u2013 Design the prompt. We design prompts following best practices in prompt engineering [35] and techniques outlined by Liu et al. [26]. The prompt should instruct the FMs on how to label the blog posts, specifying the labelling criteria and providing the necessary context. To improve labelling accuracy, we incorporate advanced techniques such as Chain-of-Thought prompting [46] and few-shot in-context learning [3]. We used a predefined set of labels to ensure a common vocabulary for the classification. Because FMs generate natural language text, a common vocabulary is nec-essary to (1) group many different blog posts that may use different terms to describe the same aspects and (2) facilitate a comparison with prior work which also uses that vocabulary. We also ask FMs to provide new labels if the predefined ones do not fit the content. Our replication package [23] includes all used prompts. We encourage researchers to refine and rerun the process with new blog posts to ensure relevance in this rapidly moving field.\nStep 3 Run the prompt on the golden dataset. Each FM in the jury is prompted to label the blog posts in the golden dataset. For each blog post, the FM outputs both a label and an associated confidence score. As FMs may exhibit overconfidence, directly using the raw confidence scores is likely to introduce bias [25]. While calibration methods exist to address this issue, they typically require access to the model's internal information or fine-tuning [31], which is not feasible with closed-source FMs. Therefore, we apply a z-score stan-dardization based on the confidence score distribution across the dataset to normalize the confidence values.\nStep 4 Compare FM labels with human labels. The produced labels are compared with the human-provided labels from the golden dataset. We assess inter-rater reliability using Cohen's \u043a coefficient [11], which measures the degree of agreement between the FM-generated labels and human labels. We set a threshold of $\\kappa$ > 0.78 (indicating excellent agree-ment) for at least one FM in the jury. Additionally, all FMs must achieve $\\kappa$ \u2265 0.63 (indicating substantial agreement). If these thresholds are not met, we return to Step 2 and refine the prompt iteratively. The refinement process involves adding clarifications, improving instructions, or reordering prompt components to resolve ambiguities [14]. The iterative loop continues until the desired agreement is reached.\nStep 5 Freeze the prompt for full dataset labelling. Once the prompt achieves the required level of agreement in Step 4, it is finalized and used to label the entire dataset."}, {"title": "B. Merging the FM outputs", "content": "After the individual FMs in the jury provide their labels, we aggregate the results using a majority vote, where the final label is determined by the label that receives the most votes from the individual FMs. In case of a tie, we use the normalized confidence scores to break the tie."}, {"title": "C. Human-in-the-loop", "content": "A human-in-the-loop process was employed to decide whether to accept or reject new labels proposed by FMs. However, the process is only required when the FM/LLM Jury cannot resolve the final label. In our study, the FM/LLM Jury successfully handled all cases without the need for human involvement in the labelling process."}, {"title": "D. Selected FMs", "content": "The jury FMs are selected based on their performance and ability to follow complex instructions. We used the LLM"}, {"title": "IV. METHODOLOGY", "content": "Figure 2 gives an overview of our methodology. In this section, we discuss every step."}, {"title": "A. Identifying blogs related to SE and FM from technology companies", "content": "We employed a systematic approach using search queries based on keywords related to FM4SE and SE4FM to gather blog posts. The data collection process consists of these steps:\nStep 1 - Collect a list of blogs from technology com-panies. We began by collecting a list of companies from the \"Technology\" sector with a market capitalization greater than $200 billion (\u201cMega\u201d) based on data from NASDAQ.\u00b9 In addition, we manually went through the top 100 companies from Forbes' Global 2,000 list [33] and included companies that are categorized under the \u201cIT Software & Services\" industry. In total, we included 20 companies. For each of these companies, we searched for relevant blogs using this query:"}, {"title": "B. Identifying the FM4SE and SE4FM area, activities and discussed tasks in the blog posts", "content": "We first label blog posts as FM4SE or SE4FM-related. Second, we label the FM4SE or SE4FM activities discussed in the blog posts. Finally, we summarize the activity-specific tasks that are discussed, to facilitate our manual review of the posts. Tables III and IV show the identified activities and tasks."}, {"title": "V. RQ1: WHICH FM4SE ACTIVITIES ARE DISCUSSED IN INDUSTRY BLOG POSTS?", "content": "Motivation. Industry practitioners are at the forefront of apply-ing FMs to SE, sharing practical insights and real-world expe-riences through blogs. While academic research has explored many aspects of FM4SE, the industry's perspective remains underexplored. This study analyzes industry blogs to uncover key FM4SE activities and tasks discussed by practitioners, providing insights into real-world applications of FMs in SE.\nApproach. We used the FM/LLM Jury to label the FM4SE activities and tasks in the 155 FM4SE blog posts. To avoid overrepresentation, we counted each activity and task uniquely per company, even if they were mentioned in multiple blog posts from the same company. To gain insights into how these activities and tasks are discussed, we manually reviewed the selected blog posts. We also compared our findings with those reported by Hou et al. [17]."}, {"title": "A. Software development", "content": "Although code generation is the most prominent task, FMs are used for many other tasks in the software develop-ment process. As shown in Table III, code generation emerges as the most prominent task. Practitioners report leveraging FMs to generate code in modern languages [B140] such as Python and Java, but there is also growing attention for legacy systems, with FMs being used to generate COBOL code [B830]. Additionally, FMs are applied to specialized domains such as SQL query generation [B183], and domain-specific languages (DSLs) tailored to industry-specific needs such as semiconductor design [B8]. The flexibility of FMs to adapt across various programming languages and domains highlights their versatility in software development."}, {"title": "B. Software quality assurance", "content": "Vulnerability detection is the most frequently discussed software quality assurance (QA) task. Practitioners employ FMs to automate common vulnerabilities and exposures (CVE) detection and analysis [B101]. Other tasks under this category include test generation and automation, where FMs are used to generate test cases based on the functionality of a given code. For instance, the FM can suggest test cases for invalid inputs, edge cases, and error handling [B5518]. For debugging tasks, FMs can suggest where to insert logging and exception handling to track code execution and errors, and FMs can also be used to detect anomalies and fix common issues such as syntax and logical errors [B5465]."}, {"title": "C. Software maintenance", "content": "The use of FMs in software maintenance focuses on the refactoring, translation, and transformation of exist-ing codebases. Practitioners often discuss these tasks in the context of modernizing legacy systems. For instance, FMs are employed to refactor and translate legacy COBOL code into Java [B769]. In addition, upgrading Java applications to newer Long-Term Support (LTS) versions [B369] or migrating Java codebases to cloud-based infrastructures [B6378] are tasks frequently associated with code transformation. Using FMs for these tasks helps industries transition from older systems to modern architectures more efficiently."}, {"title": "D. Software management", "content": "Software management receives the least attention in FM4SE blogs, and no discussions were found regarding requirements engineering or software design. Practitioners use FMs to generate software tool configurations for managing cloud infrastructure components [B5625]. We did not find discussions on requirements engineering or software design in FM4SE blogs. One blog, which was initially misclassified as covering requirements analysis [B1182], was actually about using FMs to extract developer intent from code comments or function documentation for generating formal postconditions. Overall, FM-based requirements engineering and software design remain an underreported area in the industry."}, {"title": "E. Comparison with Academic Research", "content": "Software development is the most discussed activity in both industry blog posts and SE research papers [17]. Both prac-titioners and researchers frequently highlight code generation as a key task. However, there are some notable differences. Regarding software maintenance, for example, industry blogs focus more on code refactoring and revision, while academic research papers focus more on program repair. Additionally, while Hou et al. [17] reported that 4.3% of surveyed papers (17 out of 395) cover requirements engineering, we found no substantial discussion on this topic in the industry blogs we analyzed (except the one misclassified)."}, {"title": "RQ1 Summary:", "content": "Software development tasks, par-ticularly code generation, are the most frequently discussed across FM4SE blogs. FMs are increasingly integrated as code assistants, providing developers with multifunctional tools to boost productivity. In the domain of software quality assurance, vulnera-bility detection is the dominant task, while software maintenance activities are primarily focused on refac-toring and transforming existing codebases. Software management, requirements engineering, and software design receive less attention in these industry blogs."}, {"title": "VI. RQ2: WHICH SE4FM ACTIVITIES ARE DISCUSSED IN INDUSTRY BLOG POSTS?", "content": "Motivation. As FMs are integrated into production systems, applying SE principles to the development cycle of FM-based systems becomes increasingly important. While academic research studied SE for AI-based systems [28], FM-based systems present unique challenges [15], such as the resource-intensive nature of FMs, and the complexities involved in fine-tuning, deployment, and monitoring at scale. Industry blog posts offer valuable insights into how practitioners adapt SE principles for developing, deploying, managing, and scaling FMs in real-world settings. This study gives an overview of key SE4FM activities and tasks discussed in these blog posts.\nApproach. We used the FM/LLM Jury to label the SE4FM activities and tasks in the 997 SE4FM blog posts. Similar to Section V, we counted each activity and task uniquely per company and manually reviewed selected blog posts for each task. We did not compare the discussion frequency with previous research like we did in Section V, because there exists no prior survey on SE4FM, and SE4FM activities and tasks are quite different from those for SE4ML [28]."}, {"title": "A. Model deployment & operation", "content": "Model deployment & operation is the most frequently discussed activity in SE4FM industry blog posts. Model deployment on cloud is the dominant task (see Table IV), re-flecting the industry's reliance on cloud environments for host-ing foundation models. Foundation models are very resource intensive. For example, a large model such as Meta Llama 3 (with 405 billion parameters [9]) [B1749,B5439,B6691] requires ~810GB of GPU VRAM for inference, ~3.25TB for fine-tuning [40] and much more for training from scratch. The cloud facilitates using such large models on-demand without the need for buying very expensive hardware.\nFor model serving & scaling [B111,B627, B6028], tech-niques such as speculative decoding [7] accelerate model infer-ence by using draft models for faster response times [B5308]. In addition, automatic model scaling ensures that resources au-tomatically adjust to workload needs [B701, B4518]. Model monitoring [B6659] is another key task, involving tracking token usage [B406, B920] and monitoring system metrics such as memory and GPU load [B5192].\nWhile cloud deployment dominates, there is increasing interest in model deployment on local devices such as edge or mobile devices, and PCs. For example, practitioners deploy FM-based medical chatbots for healthcare applic-ations on edge devices, to facilitate data privacy [B139]. Another reason to deploy FMs on smaller devices is to overcome the GPU supply-and-demand problem, which makes it hard for many companies to integrate FMs into their products [B1355]. To enable running the resource-intensive FMs on relatively small devices, companies frequently use model compression techniques [51] to reduce the required resources [B1744,B1993,B5463,B5714]. For example, quantization techniques (e.g., 4-bit precision) enable running models on CPUs, avoiding the need for GPUs [B669]. Com-pression techniques are not limited to text models: model quantization is also applied to image generative models such as Stable Diffusion [39] [B2254]. Several libraries are used by practitioners that assist with running FMs on CPU, such as LLaMA.cpp\u00b3 and ExLlama\u2074 [B1355]. Also, several practition-ers describe how the use of Neural Processing Units (NPUs) can facilitate running FMs locally [B914, B1750]."}, {"title": "B. System architecture & orchestration", "content": "System architecture & orchestration activities, including building AI agents and model & prompt chaining, have be-come a popular topic in SE4FM industry blog posts. One of the most frequently discussed tasks is model & prompt chain-ing, which is used to manage complex workflows by breaking them into smaller, more manageable steps, each handled by different models or prompts [B881]. For example, a task such as responding to customer reviews might be divided into steps like filtering harmful content, performing sentiment analysis, and generating an appropriate response [B4881]. Tools and frameworks like LangChain [6] and PromptFlow [30] are commonly mentioned as practical solutions for implementing these chained workflows [B1357].\nA closely related discussion topic is building Al agents, which extend the functionality of FMs by integrating external tools and orchestrating workflows. In multi-agent systems, multiple AI agents collaborate using a complex workflow to handle different parts of a task [B1166]. These AI agents can autonomously decide which tools to use, retrieve necessary data, and execute predefined plans based on user input or real-time data [B414]. Another key feature of AI agents is their ability to leverage working memory, allowing them to retain information from previous interactions or external tool outputs, which can be critical for managing long-term tasks [B442].\nEnterprise development platforms & studios provide support for building FM-based systems based on chaining or Al agents [B414,B1166,B4881]. These platforms, such as FMArts [15], simplify the orchestration of workflows [B358],"}, {"title": "C. Data management", "content": "Data management has evolved in the FM era, with new techniques supporting the vast amounts of both struc-tured and unstructured data. At the core of this evolution is the shift to specialized, more dynamic data management techniques. The most frequently discussed task is Retrieval-Augmented Generation (RAG) [22], which combines the use of private datasets (i.e., data on which the FM was not trained before such as proprietary data) with FMs. With RAG, the FM generates its response based on the prompt and infor-mation in the private dataset [B4270]. Several practitioners discuss how they build datasets for RAG [B6492,B6688] using techniques including document chunking, embedding, and vector storage. Another example of usage of RAG by practitioners is GraphRAG [10] which enhances RAG by generating knowledge graphs from the private data which can then be used for prompt augmentation [B1155,B1156].\nThere is a great amount of discussion of special-ized databases, particularly vector databases. Specialized databases are key to enabling RAG. These databases sup-port semantic search by indexing unstructured data like text and images, making FM-based retrieval faster and more accurate [B1571]. Advanced features include multimodal search, where users retrieve image or video content using text queries [B5035]. This shift from traditional keyword-based search to semantic search also integrates with SQL queries for managing both structured and unstructured data [B1426]. As data management moves beyond traditional data types (e.g., rows, columns, JSON), vector-based storage and retrieval systems are becoming more important [B5370]. Likewise, embedding as feature engineering is becoming increasingly important in FM-based systems, enabling text, images, and structured data to be converted into numerical vectors that FMs can process [B1982]. Multimodal embeddings, which map both text and images into a shared vector space, are particularly useful in cross-modal applications such as text-to-image search or video retrieval [B5815].\nSynthetic data generation provides scalable, domain-specific data without privacy risks, reducing reliance on real-world datasets. As FM data requirements grow, syn-thetic data is increasingly used to address the challenges of high-quality data collection [B35,B4210]. In parallel, automated data labelling is being transformed by model-assisted approaches. For example, the Recognize Anything Model (RAM) [49] can automatically label visual datasets, enabling users to search for images or videos using natural language queries [B3649]. Additionally, human-in-the-loop is applied for combining model-generated annotations with man-ual oversight to ensure accuracy while reducing the time and cost associated with traditional labelling methods [B5181]."}, {"title": "D. Model customization", "content": "Model customization is achieved through fine-tuning techniques for adapting FMs to specific application needs. Fine-tuning methods such as supervised fine-tuning (SFT) tune the entire model on domain-specific data, rather than train it from scratch. Enterprise platforms now support no-code fine-tuning, simplifying the process and accelerating development [B4777]. Open source libraries such as Hugging Face's PEFT [27] support both full fine-tuning and Low-Rank Adaptation (LoRA) [B353]. LoRA is an efficient approach where original model parameters are frozen and injected with trainable matrices. LoRA reduces the number of trainable parameters and lowers GPU requirements, making it cost-effective [B109]. With different LoRA adapters, a single FM can adapt to handle different tasks. Platforms support dynamic loading and caching of LoRA adapters, offering flexibility and optimizing performance [B2462]. RLHF is used to align mod-els with user preferences to improve their experience [B656] and can also be applied to image models [B2367]."}, {"title": "E. Evaluation & quality assurance", "content": "SE4FM blog posts outline practical strategies for en-suring the safety, fairness, and trustworthiness of FMs through systematic evaluation & QA processes. With the diverse applications of FMs, establishing robust model evaluation frameworks is essential to ensure models meet operational requirements [B898]. For example, for FM-based systems with RAG integration, an evaluation framework in-cludes metrics such as answer relevance, context precision, and recall to assess the effectiveness of model outputs [B1305]. Ensuring model safety & compliance is particularly critical in high-stakes industries. Industry blogs highlight the use of adversarial testing to identify model vulnerabilities, while automated raters are often deployed to perform consistent safety assessments [B437]. Standardized benchmarks are also leveraged to ensure models meet security and compliance standards, especially in regulated industries [B845].\nPractitioners are increasingly adopting automated testing strategies for FMs, which often use academic benchmarks like BIG-bench [B158]. However, custom datasets tailored to specific domain requirements are also vital for evaluating FMs in domain-specific applications [B845]. One emerging approach in this area is the use of LLM-as-a-judge techniques, which leverage LLMs to provide scalable and consistent evaluations [B158]. Additionally, adversarial testing is used to strengthen models against potential threats by uncovering weaknesses that may not surface under traditional testing methods [B845]. Another emerging task is model explain-ability & interpretability, which is important particularly in sensitive industries. Tools that generate natural language ex-planations for model outputs are becoming common, help-ing developers understand why certain test cases pass or fail [B4776]. This increases transparency and aligns FMs with best practices for software engineering.\nSome practitioners discuss model fairness & bias, as bi-ased outputs from FMs can lead to ethical concerns or operational risks. To reduce bias in FM-generated outputs, techniques such as prompt engineering and scenario testing are employed [B1295]. Adversarial testing and diverse rater systems are also leveraged to ensure fairness and prevent harmful outputs [B437]. Additionally, practitioners highlight the importance of human oversight in critical decision-making processes to mitigate risks associated with harmful or biased outputs [B1064]. To enhance trust in FM-based systems, practitioners conclude and follow a set of best practices for AI security, such as the Secure AI Framework (SAIF) [B697]."}, {"title": "F. Prompt construction", "content": "Prompt construction receives the least attention, and no discussions were found regarding requirements en-gineering. Prompt engineering techniques are discussed in the blog posts, such as structured prompts [B374] and multi-shot prompts [B4401]. In database contexts, prompts consider schema, query history, and user-specific factors to generate SQL queries [B3601]. In addition, automated prompt generation techniques are explored through dynamic metaprompts which are optimized for greater control and adaptability [B1172]. For tasks like text-to-image generation, prompts are improved based on semantic search and user context [B3754]. In addition, prompt compression is proposed to automatically reduce prompt length without sacrificing essential information [B1173]."}, {"title": "RQ2 Summary:", "content": "The most discussed activities in SE4FM blog posts are model deployment & opera-tion, with a focus on cloud hosting and model serving & scaling. With regards to system architecture, trends include prompt chaining, workflow orchestration for FMs, and AI agents. Data management emphasizes RAG and vector databases to support unstructured data and information retrieval. Model customization relies on fine-tuning methods such as full fine-tuning, LORA, and RLHF. Evaluation & quality assurance practices focus on automated testing, safety, trustwor-thiness, and bias mitigation, while prompt engineering receives limited attention. Discussion about require-ments engineering is not found in SE4FM blog posts."}, {"title": "VII. DISCUSSION OF FUTURE RESEARCH DIRECTIONS", "content": ""}, {"title": "A. Research Directions for FM4SE", "content": "Research Direction 1: Using FMs for modernization and transformation of legacy code. Practitioners applied FMs to translate legacy systems into modern languages such as Java [B769], upgrade to newer language versions [B369], and migrate to cloud-based infrastructures [B6378]. This process mostly relies on FMs for code translation, however, Pan et al. [37] highlight challenges in applying FMs to translate code in real-world projects. To address these challenges, researchers should explore methodologies that enhance FM performance for automating complex system migrations, including the transformation of entire legacy codebases and architectures.\nResearch Direction 2: Evaluating code assistants in software development workflows for tasks other than code generation. Liang et al. [24] conducted a survey on the usability of code assistants focusing on the code generation task. However, industry discussions highlight that code as-sistants are employed for other tasks such as code under-standing [B5668], code summarization [B3357], and API recommendation [B3177] as well. This broader integration, including their potential role as AI teammates [16], suggests opportunities for researchers to expand studies on code assis-tants beyond code generation.\nResearch Direction 3: Real-world validation of research on applying FMs in software management, requirements engineering, and software design. Our analysis of industry blog posts shows that discussions around FM applications in these activities are rare (see Table III). Likewise, though software engineering researchers have explored using FMs for requirements engineering tasks such as requirements clas-sification and traceability automation, such studies remain relatively rare as well [17]. Considering the capabilities of FMs in handling natural languages and programming languages, they should be well-suited for these tasks, hence the lower number of (reported) applications in this area is surprising. Researchers should aim to bridge the gap by identifying the barriers to and developing tools that apply FM techniques in these underreported activities. Such efforts could help bring research advancements in these areas closer to practical industry applications, showcasing the value of FM4SE for a broader range of software engineering activities."}, {"title": "B. Research Directions for SE4FM", "content": "Research Direction 4: Researchers should expand re-search on SE4FM. Our findings in Sections V and VI show that practitioners discussed SE4FM much more often than FM4SE, with 997 blog posts focused on SE4FM compared to 155 blog posts on FM4SE. Activities such as model deploy-ment & operation, system architecture, data management, and model customization are frequently mentioned by more than half of the companies (Table IV). Although the lower number of posts on FM4SE does"}]}