{"title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment", "authors": ["Hantao Lou", "Changye Li", "Jiaming Ji", "Yaodong Yang"], "abstract": "With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V's ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms.", "sections": [{"title": "1. Introduction", "content": "With the development and success of large language models (LLMs) (Dubey et al., 2024; Achiam et al., 2023), researchers have begun to introduce visual understanding to these models, thereby extending their operational scope from language to a mix of vision and language, resulting in the creation of powerful multimodal large language models (MLLMs) (Alayrac et al., 2022; Liu et al., 2024; Team et al., 2024; Team, 2024). To enhance the multimodal understanding capabilities of MLLMs, the research community has explored various architectures, including using individual image/text encoders to encode cross-modal information into a joint representation space (Zhang et al., 2023; Liu et al., 2024; Zhu et al., 2024; Wu et al., 2024b) and leveraging image tokenizers to transform all inputs into a unified token sequence (Team, 2024; Xie et al., 2024; Wu et al., 2024a; Wang et al., 2024). Despite the difference in the architectures of these models, their essential goal is the same: Fuse the text and image representation space into a joint multimodal semantic space.\nAs MLLMs continue to scale up in both size and capability, their interpretability and controllability remain a significant challenge (Zhang & Zhu, 2018; Stan et al., 2024). Currently, mechanistic interpretability techniques such as circuit analysis (Olsson et al., 2022) and dictionary learning with sparse autoencoders (Cunningham et al., 2023) are the most widely recognized approaches to interpreting LLMs. However, their application to MLLMs, especially in the context of cross-modal integration, has been limited. There is a pressing need for specialized tools and frameworks that can unravel the intricate workings of MLLMs.\nMoreover, current interpretability efforts are focused mainly on interpreting models, rather than applying this interpretability to real alignment situations, which also makes it difficult to evaluate these methods effectively. Top-down approaches, such as Representation Engineering (Zou et al., 2023) and activation steering (Turner et al., 2023; Panickssery et al., 2023), can directly evaluate the control effects of interpretability methods through control or unlearning techniques. However, for bottom-up methods like circuit analysis (Wang et al., 2022), sparse autoencoders (SAEs) (Cunningham et al., 2023), and cross-coders, effective evaluation methods beyond loss reduction are limited. Based on the previous discussion, can we propose a bottom-up multimodal interpretability approach that can directly enhance the alignment process?\nIn this work, we developed SAE-V, a mechanistic interpretability framework for MLLMs that extends the SAE paradigm to MLLMs. These tools are then applied to interpret the training process of transitioning from LLMs to MLLMs, as well as the process of enhancing the multimodal capabilities of MLLMs. Furthermore, utilizing the interpretable features of SAE-V models and their relationship to MLLM capabilities, we designed a data filtering metric based on SAE-V. This metric can filter out data that hinder the development of multimodal understanding, achieving stronger alignment with a smaller dataset. Overall, our work makes the following contributions:\n\u2022 Multimodal interpretability tool We developed mechanistic interpretability tools for MLLMs based on previous attempts on LLMs and trained corresponding SAE-V models. We demonstrated that SAE-V models trained on MLLMs can effectively extract interpretable features, and SAE-V models can be transferred to the corresponding LLMs. Specifically, the reconstruction loss of our SAE-V models trained on MLLMs is 38.3% and 50.6% compared to the SAE model when applied to MLLMs and LLMs, respectively.\n\u2022 Interpreting Multimodal Alignment Process We utilized SAE-V to study the feature distribution throughout the alignment process. We discovered that the feature distribution of SAE-V corresponds to the MLLM's performance on multimodal understanding tasks.\n\u2022 Filtering metric to improve alignment Based on the previous investigation with SAE-V, we developed a metric to filter multimodal datasets and acquire high-quality data, therefore improving alignment quality and efficiency. Experiments demonstrate that our filtering tool achieves more than 110% performance compared to the full dataset while using 50% less data, underscoring the efficiency and effectiveness of SAE-V."}, {"title": "2. Methodology", "content": "In this section, we present our method to train, evaluate, and apply SAE-V to interpret MLLMs and multimodal data."}, {"title": "2.1. Preliminary: Sparse Autoencoder Paradigm", "content": "We adopt SAE-V (denoted as $S_{\\theta}$) architecture from the methodology proposed in (Bricken et al., 2023), which comprises an encoder and a feature dictionary $F_{\\theta}: \\{f_k\\}_{k=1}^{m}$ as a decoder. Let the input be denoted as $H \\in \\mathbb{R}^{l\\times m}$, the hidden state of a specific layer of a MLLM $M_{\\theta}$. The SAE-V encoding operation $S_{\\theta}()$ is\n$Z = ReLU(W_{enc}H + b_{enc}),$ (1)\nwhere $Z \\in \\mathbb{R}^{l\\times n}$ is the feature activation of the input. The reconstruction loss of $S_{\\theta}$ donates as\n$L_R = ||H - Z \\times (f_1, f_2,..., f_m)||_2^2.$ (2)\nThe training loss is defined by\n$L = L_R + \\lambda L_1,$ (3)\nwhere $L_1 = ||Z||_1$ adds a sparsity constraint to the learned features and $\\lambda$ is a hyperparameter controlling the level of sparsity. The training results could also quantized by incorporating an additional sparsity constraint via $1L_0 = ||Z||_0$, which counts the number of nonzero elements in the learned features $Z$."}, {"title": "2.2. Interpreting Multimodal Data with SAE-V", "content": "It has been previously demonstrated (Gao et al., 2024; Cunningham et al., 2023) that SAE can be employed to interpret how LLMs encode semantic information from these models. This feature motivates us to apply SAE-V to assess the quality of the data and thus facilitate data filter for alignment.\nWe adopt a cosine similarity score ranking algorithm for data filtering (shown in Algorithm 1). Let the multimodal training dataset be donated as $D = \\{d_i\\}_{i=1}^{n}$, where $d_i = \\{u_j:u_j \\in T \\vee u_j \\in V\\}_{j=1}^{m}$ is a set of tokens from text vocabulary $T$ and tokens from vision vocabulary $V$. We acquire feature activation token $z_j$ by MLLM forward and Equation 1, i.e.\n$H_i = M_{\\theta}(d_i)$ (4)\n$Z_i = S_{\\theta}(H_i),$ (5)\n$z_j = e_j Z_i,$ (6)\nwhere $e_j = (0,0,..., 1,...,0)$.\n$\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad j-th \\, position$\nWe define a SAE-V feature $f_k$ is activated on $z_j$ if $z_j f_k^T > \\delta$, where $\\delta$ is activation bound. Correspondingly, we state that $f_k$ is activated on $d_i$ if $\\exists z_j \\in Z_i$ activating $f_k$.\nOur algorithm 1 consists of three stages: (1) Collecting feature activation tokens from dataset, (2) Computing cross-modal weight of SAE-V features, and (3) Ranking dataset by cross-modal weight."}, {"title": "1. Feature Activation Token Collecting", "content": "We first sample a small subset $D_s$ of the training dataset $D$ and input these samples into the MLLM to obtain hidden states $H$. These hidden states are then fed into the SAE-V encoder to extract feature activations defined as Equation 4. For each feature, we collect its hidden state tokens thereby obtaining a sample of feature activation tokens across the dataset."}, {"title": "2. SAE-V Feature Weighting", "content": "For each feature $f_k$, we identify its top-K hidden state text tokens $t = TopK(A_k \\cap T)$ and top-K hidden state vision tokens $v = TopK(A_k \\cap V)$, where the top-K is ranked by the activation value $z_j f_k$ of the token. We then compute the cosine similarity between the two lists of tokens, donating the cross-modal weight of feature $f_k$ as\n$Cosine(t, v) = \\sum_{i=1}^K \\frac{t_iV_i}{||t_i|||| V_i||}.$ (7)"}, {"title": "3. Data Ranking", "content": "Using the weighted features of SAE-V model, we score the entire training dataset. The cosine similarity score of each piece of data is defined as the sum of the cosine similarity scores of its activating features. We rank the data set by the score and the resulting cosine similarity score order allows us to filter data that are better aligned with the structures of multimodal semantic information.\nWe present our experiments and results in Section 4, demonstrating that our cosine similarity score ranking method can effectively filter high-quality data from the training data set."}, {"title": "3. Interpretability Analysis with SAE-V", "content": "In this section, we conduct experiments on the SAE-V paradigm, aiming to demonstrate the capability and transferability of SAE-V model. We also performed experiment to prove the effectiveness of our SAE-V-based data interpreting tool from the inference side."}, {"title": "3.1. Training and Evaluating SAE-V Model", "content": "We trained a series of SAE and SAE-V models on MLLMs and their base LLMs. We evaluated the performance of these models, and the results demonstrated that SAE-V model is capable of interpreting MLLMs and that SAE-V model trained on MLLM can be effectively transferred to its original LLM."}, {"title": "3.1.1. EXPERIMENT SETUP", "content": "Datasets For text-only and multimodal situations, we selected the Pile (Gao et al., 2020) and Obelics (Lauren\u00e7on et al., 2023) datasets separately. Specifically, we sampled 100K data from each dataset as the train set and 10K data as the test set. The Pile is a diverse language modeling dataset for LLM pretraining, and Obelics is a massive interleaved image-text dataset for MLLM pretraining. These two datasets are widely recognized in various pretraining and interpretability works (Black et al., 2022; Biderman et al., 2023; Cunningham et al., 2023; Team, 2024).\nModels We selected two generic MLLMs, LLaVA-NeXT-7B (Liu et al., 2024) and Chameleon-7B (Team, 2024), as our target models. These two models represent two distinct architectures, and testing our method on them can prove that our method is applicable to different architectures.\nAdditionally, we also studied Anole-7B (Chern et al., 2024) and Mistral-7B (Jiang et al., 2023) to compare the behavior of SAE and SAE-V models before and after finetuning, specifically the transitioning fine-tuning from LLM to MLLM. Anole-7B is a variant of Chameleon-7B, with its image generation capability unlocked, while Mistral-7B is the base LLM of LLaVA-NeXT-7B. 1\nEvaluation Metrics To evaluate the performance of SAE-V models, we use two key metrics: $L_0 = ||z||_0$ where z is defined in Equation 6 and reconstruction loss $L_R$ in Equation 2. $L_0$ quantifies the number of activated features, reflecting the method's ability to extract interpretable features, while reconstruction loss measures the method's activation reconstruction capability compared with the model output, indicating the method's accuracy in giving interpretations."}, {"title": "3.1.2. EXPERIMENT RESULT", "content": ""}, {"title": "Capability of SAE-V Model", "content": "We compare the performance of SAE-V and SAE on different multimodal models. The $L_0$ (shown in Table 1) varies significantly across the three models. For LLaVA-NeXT-7B, the $L_0$ of SAE-V is much higher than that of SAE. For Chameleon-7B, SAE-V performs normally, whereas the $L_0$ of SAE is abnormally high, indicating that SAE fails to extract sparse features. We suppose that the failure is attributed to a large number of unseen vision tokens for SAE during the inference stage. For Anole-7B, the $L_0$ of SAE and SAE-V are nearly identical. The reconstruction loss (shown in Figure 3) of SAE-V is lower than SAE and is closer to the original activation, demonstrating that SAE-V behaves much better at reconstructing original activation than SAE across all three models. The results indicate that SAE-V outperforms SAE in terms of capability."}, {"title": "Transferability of SAE-V Model", "content": "We compared the reconstruction performance of SAE-V model trained on LLaVA-NeXT-7B and SAE model to prove that SAE-V model trained on MLLMs can generalize to its base LLM. The findings (shown in Figure 4) indicate that across different settings, SAE-V model consistently achieves the best performance. Moreover, when trained on both MLLM and LLM, SAE model exhibits nearly identical reconstruction loss values, showing its robust transferability.\nThese results highlight that training SAE-V model for MLLMs with multimodal data is effective for interpreting MLLMs, and even LLMs, as SAE model trained solely on textual data fail to extract and disentangle the hidden representations of MLLMs effectively. Moreover, SAE-V model demonstrates superior capability in reconstructing the reasoning features of MLLMs compared to the standard SAE model."}, {"title": "3.2. Apply SAE-V Model on Multimodal Data", "content": "In this section, we conduct an image classification task on the ImageNet dataset (Russakovsky et al., 2015) to investigate whether SAE-V can capture the key information within images and to validate the effectiveness of the methods proposed in Section 2.2 on multimodal data. We apply 4 methods, namely $L_0$, $L_1$, co-occurring $L_0$, and cosine similarity score, where co-occurring $L_0$ is defined as the number of features activated on at least one text and image token. The cosine similarity score is defined as the sum of cross-modal weights of features, consistent with Algorithm 1. We adopt these metrics to filter image patches, thus obtaining images that preserve 75%, 50%, and 25% patches, respectively. 2"}, {"title": "Quantized Results", "content": "The quantized results are presented in Figure 6, where we observe that, for all methods, preserving 75% or 50% of the patches achieves an accuracy close to that obtained using the full image. In the challenging scenario where only 25% of the patches are retained, $L_0$ method, $L_1$ method, and cosine similarity score method maintain accuracy levels close to 70%, and all methods significantly surpass the accuracy obtained by the random preservation method. These results demonstrate that SAE-V can accurately capture critical information in images and that the methods proposed in Section 2.2 effectively utilized SAE-V features during inference."}, {"title": "4. Alignment Experiment", "content": "In this section, we adopt cosine similarity score rank- ing algorithm as a data filter (as shown in Algorithm 1) to acquire high-quality data for model alignment."}, {"title": "4.1. Experiment Setup", "content": "Dataset and Model Consistent with Section 3.1.1, we selected LLaVA-NeXT-7B (Liu et al., 2024) and Chameleon-7B (Team, 2024) for our alignment experiment. Since the LLaVA-NeXT-7B model is rather powerful in multimodal capabilities, we selected the Align-Anything (Ji et al., 2024) text-image-to-text dataset for our experiment. Align-Anything is a 400K multimodal preference dataset containing fine-grained annotated multimodal input-output preference data, and we used the 40K subset of text-image input and text output in our experiment.\nAlgorithm We adopt the cosine similarity score ranking algorithm (shown in Algorithm 1) as a filter to exclude data with low scores. In addition, we also adopt two algorithms, the $L_0$ ranking, and the co-occurrence ranking. 4\nEvaluation To evaluate the efficiency of our methods, we applied Direct Preference Optimization (DPO) to the model using the filtered datasets. 5 We then evaluate the multimodal capabilities of the model using LLaVA-Bench (Liu et al., 2024) benchmarks."}, {"title": "4.2. Experiment Results", "content": "We performed SAE-V-based data filter with different filtering ratios on the LLaVA-NeXT-7B model and Align-Anything dataset. The filtered datasets were then used to fine-tune MLLMs, which were evaluated on LLaVA-Bench. The results (shown in Figure 7) demonstrate that our SAE-V-based filtering method effectively enhances the alignment of LLaVA-NeXT-7B, even with reduced data. Since most of the data in Align-Anything contribute positively to model alignment, the performance of the model is higher than the base model without any fine-tuning in most cases. At any data filter proportion, the SAE-V-based data filtering method outperforms the random selection baseline, with the best result being 108.17 (115% of the full dataset's performance) achieved using 50% filtered data from the cooccurrence filter, and 104.20 (108% of the full dataset's performance) achieved using 20% filtered data from the cosine similarity filter. However, as the dataset inevitably contains some low-quality data, the performance is optimal with a moderate data proportion and shows a downward trend as the data proportion increases."}, {"title": "4.3. Relationship between MLLM Capability and SAE-V Features", "content": "In the previous section, we demonstrated the effectiveness of utilizing the cosine similarity score for data filters in model training. To further investigate the relationship between model performance and cross-modal similarity, as measured by cosine similarity of SAE-V features, we further measure the average cosine similarity score of these models. Given a dataset, we apply the cosine similarity score ranking algorithm (shown in Algorithm 1) to the MLLM, and we define the MLLM's average cosine similarity score as the mean score of all non-zero cross-modal weight SAE-V features.\nWe calculated the average cosine similarity scores for the models discussed in Section 4.2. The result (shown in Figure 8) revealed a positive correlation between the average cosine similarity score of SAE-V feature and the performance of MLLM, suggesting that higher similarity scores of SAE-V features correspond to enhanced MLLM performance."}, {"title": "4.4. Ablation Study", "content": "Ablation on Models To prove that SAE-V-based data filter method could generalize to distinct model architectures, we replicate SAE-V-based data filter on Chameleon-7B model and Align-Anything dataset. The result (shown in Figure 9) demonstrates that although Chameleon-7B performs worse than LLaVA-NeXT-7B on LLaVA-Bench, the SAE-V-based filter method still shows its effectiveness. When using a smaller data proportion, the performance is strongly correlated with the data quantity, and thus the differences between methods are minimal. However, with a larger data proportion, the SAE-V-based filter method significantly surpasses the random filter, achieving a peak of 52.13 (120% of the full dataset's performance) with 70% of the data. The largest performance gap is observed in the 50-70% data range, while the differences converge again as the data proportion approaches 100%. This proves that SAE-V-based data filter is effective on architectures other than CLIP-based MLLM, and shows its potential to generalize across a wide range of models.\nAblation on Datasets We also performed an ablation study on the datasets to be filtered. Since the LLaVA-NeXT-7B model is highly capable, most datasets fail to further enhance its multimodal abilities. Therefore, we selected the RLAIF-V dataset and the relatively weaker Chameleon-7B model for the dataset ablation study. The results (shown in Figure 10) further confirm that SAE-V-based data filter is working across different datasets. Moreover, on RLAIF-V, the cosine similarity filter could achieve a score of 54.23 (125% of the full dataset's performance) by using only 10% of the data, demonstrating exceptional efficiency.\nComparation with Other Filtering Methods To validate the effectiveness of our SAE-V-based data filtering method, we conducted an ablation study comparing it with other similar data filtering approaches. Since there are currently no widely recognized data filtering methods specifically designed for multimodal data, we adapted the IFD metric (Li et al., 2023b) method to the multimodal setting. The result (shown in Figure 11) suggests that our data filter method achieves a performance comparable to the IFD metric. However, considering that the IFD metric needs to train an additional cherry model, our SAE-V-based data filter could directly fit various datasets, demonstrating greater generalizability and efficiency."}, {"title": "5. Related Work", "content": "Multimodal Large Language Model MLLM is a type of LLM integrated with multimodal modules that incorporate multimodal information to deal with multimodal tasks. Based on the method of integrating vision features into the model, most MLLMs can be categorized into three types:\n\u2022 CLIP-based MLLMs: These models encode images with CLIP (Radford et al., 2021) and use MLP to project visual features. Examples include LLaVA (Liu et al., 2024) series and NEXT-GPT (Wu et al., 2023).\n\u2022 Early-Fusion MLLMs: These models directly tokenize visual features for input. Examples include Chameleon (Team, 2024) and Janus (Wu et al., 2024a) series.\n\u2022 Q-Former-based MLLMs: These models use a structure similar to Q-Former (Li et al., 2023a) to extract visual representations, represented by Qwen-VL (Bai et al., 2023) and MiniGPT-4 (Zhu et al., 2024).\nOur study focuses on the CLIP-based and early-fusion MLLMs. Specifically, we select LLaVA-NeXT-7B and Chameleon-7B as the target models.\nMechanistic interpretability with Sparse Autoencoder Mechanistic interpretability seeks to uncover and explain the internal mechanisms that enable models to understand input data and generate responses (Rai et al., 2024). Specifically, most current mechanistic interpretability methods focus on analyzing features, smaller units that contribute to performing explainable semantic tasks, within models (Olah et al., 2020).\nSparse Autoencoder (SAE) aims to learn sparse and interpretable features from polysemantic model representations (Yun et al., 2021; Bricken et al., 2023; Sharkey et al., 2022; Peign\u00e9, 2023; Elhage et al., 2022). By introducing sparsity constraints, the activation values in the hidden layers of SAE are mostly zero, allowing SAE to encode polysemantic features in LLM to monosemantic ones.\nIn this paper, we extended the scope of SAE to MLLMS, thereby building SAE-V. We further demonstrated SAE-V's capability and transferability on MLLMs, and built a data filter tool based on SAE-V to enhance multimodal alignment.\nData Filter in Alignment Data filtering ensures that only relevant high-quality data are used during the alignment of LLM or MLLM s, thus reducing the quantity of data while achieving greater performance (Zhou et al., 2023; Chen et al., 2023; Du et al., 2023; Li et al., 2023c;b; Tu et al., 2024). For example, LIMA (Zhou et al., 2023), ALPAGA-SUS (Chen et al., 2023), and IFD (Li et al., 2023b) use human annotation, API annotation and train a new model for annotation to score data separately. Our method, SAE-V-based data filter, provides a self-guided and interpretable metric to evaluate the similarity of multimodal data, which indicates their qualities. The method is stable and efficient for models of different architectures."}, {"title": "6. Conclusion", "content": "This work introduced SAE-V, a framework that extends SAE to MLLMs and improves their alignment. Through experiments on LLaVA-NeXT-7B and Chameleon-7B, we demonstrated that SAE-V model demonstrates excellent capability and transferability in interpreting MLLM and multimodal data, and SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. These results highlight SAE-V's potential to enhance multimodal model interpretability and alignment efficiently.\nLimitation While SAE-V introduces significant advancements in interpreting multimodal models and enhancing alignment through mechanistic analysis, several limitations remain unaddressed and warrant further exploration: (1) Although SAE-V demonstrates superior interpretability and data filtering efficiency compared to SAE, the theory behind SAE-V, especially the mathematical relationship between image-text similarity metrics, cross-modal co-occurrence features, and model performance, is not fully revealed. (2) Due to resource constraints, SAE-V is primarily evaluated on text and vision modalities, leaving its effectiveness on other modalities such as audio, video, and embodied AI systems unexplored. Our future work will focus on establishing a comprehensive theoretical foundation for SAE-V and extending its application to additional modalities, such as audio, video, and embodied AI systems, to broaden its utility and impact."}, {"title": "Impact Statement", "content": "The source code and checkpoints of SAE-V mentioned in this paper will be released under the CC BY-NC 4.0 license. This research has several potential risks that must be considered. The interpretability tools introduced in this work, while beneficial for alignment, could also be leveraged to manipulate or reverse-engineer model behaviors in unintended ways. Additionally, while SAE-V provides a self-guided filtering mechanism, it remains dependent on the initial dataset quality, meaning biases in the dataset could still propagate into the final model. We strongly condemn any malicious use of the SAE-V code and checkpoints and advocate for its responsible and ethical use."}, {"title": "A. Details of Interpretability Experiment", "content": ""}, {"title": "A.1. Hyperparameter of SAE and SAE-V Models Training", "content": "eration by only counting features that are activated across both modalities. Building upon these algorithms, we further developed the cosine similarity score Ranking approach."}, {"title": "Algorithm 2 Lo-based Ranking", "content": "Require: multimodal dataset D = $\\{d_i\\}_{i=1}^{n}$; MLLM $M_{\\theta}$; SAE-V $S_{\\theta}$; features of SAE-V; activation bound: $\\delta$; $F_{\\theta}: \\{f_k\\}_{k=1}^{m}$;\nEnsure: Ranked data $D_R$\nfor each $d_i \\in D$ do\n$Z_i\\leftarrow S_{\\theta}(M_{\\theta}(d_i))$\n$F_i\\leftarrow\\{f_k:||\\,f_kZ_i\\,||_{\\infty} > \\delta\\}$\n$L_{o,i}\\leftarrow \\, |F_i|$\nend for\n$D_R \\leftarrow \\, Sort(D, \\{L_{o,i}\\}_{i=1}^{n})$"}, {"title": "Algorithm 3 Co-ocurring Lo-based Ranking", "content": "Require: Text token vocabulary: $T$; vision token vocabulary: $V$; multimodal dataset D = $\\{d_i\\}_{i=1}^{n}$; MLLM $M_{\\theta}$; SAE-V $S_{\\theta}$; features of SAE-V $F_{\\theta}: \\{f_k\\}_{k=1}^{m}$; activation bound: $\\delta$;\nEnsure: Ranked data $D_R$\nInitialize coocurrence feature set of data $F_i\\leftarrow \\varnothing$\nfor each $d_i \\in D$ do\nInitialize activated token set of features $A_k\\leftarrow \\varnothing$\n$H_i\\leftarrow M_{\\theta}(d_i)$\n$Z_i\\leftarrow S_{\\theta}(H_i)$\nfor each $f_k \\in F_{\\theta}$ do\n$A_k\\leftarrow A_k\\bigcup \\{h_i: h_i \\in H_i, \\, z_i = e_j Z_i,\\, z_i^T f_k > \\delta\\}$\nif $A_k \\cap T \\neq \\varnothing > A_k \\cap V \\neq \\varnothing$ then\n$F_i\\leftarrow F_i\\bigcup \\{f_k\\}$\nend if\nend for\nend for\n$D_R \\leftarrow \\, Sort(D, \\{|F_i|\\}_{i=1}^{n})$"}, {"title": "B. Details of alignment experiment", "content": "We present details of alignment experiment in this section, including algorithms and hyperparameters of algorithms and model training."}, {"title": "B.1. Algorithms in alignment experiment", "content": "The complete algorithm of Lo-based Ranking and co-occurring Lo-based Ranking are shown in Algorithm 2 and Algorithm 3. These two algorithms serve as ablation variants of the cosine similarity score Ranking (shown in Algorithm 1). The Lo-based Ranking represents a straightforward algorithm that selects data by directly computing the sum of $L_o$ for each data point. The co-occurring Lo-based Ranking takes an initial step toward cross-modal consid-"}, {"title": "B.2. Hyperparameter of Model Training", "content": "In this section, we list out the hyperparameters used for model training through SFT and DPO (shown in Table B.2). All SAE training is performed on 8\u00d7A800 GPUs. To ensure fair comparison between algorithms, we maintained consistent parameter settings across all experiments."}, {"title": "C. Details of Applying SAE-V on Multimodal Data", "content": "In this section, we present implementation details of the SAE-V application experiments. We enumerate 4 image patch selection algorithms employed in this study and provide additional case analyses. These comprehensive results further demonstrate the robust inference capabilities of SAE-V."}, {"title": "C.1. Algorithm", "content": "The complete algorithms of Lo, L1, co-occurring Lo, and cosine similarity score methods are shown in Algorithm 4, Algorithm 5, Algorithm 6 and Algorithm 7."}, {"title": "Algorithm 4 Lo patch filter", "content": "Require: Vision token vocabulary: V; image V; fixed Prompt T; MLLM M; SAE-V S; features of SAE-V F:{f;}=1; activation bound d; mask rate g;\nEnsure: Filtered image V'\nInitialize score of each patch pi \u2190 0\nH\u2190 M(T, V)\nZ\u2190 S(H)\nfor each hi \u2208 H do\nif hi\u2208V then\nPi = \u2211j 1(zij > \u03b4)\nend if\nend for\nK\u2190[I]\nV' \u2190 TopK(vi \u2208 V) sorted by pi"}, {"title": "Algorithm 5 L\u2081 patch filter", "content": "Require: Vision token vocabulary: V; image V; fixed Prompt T; MLLM M; SAE-V S; features of SAE-V F:{f;}=1; activation bound \u03b4; mask rate \u03b3;\nEnsure: Filtered image V'\nInitialize score of each patch pi \u2190 0\nH \u2190 M(T, V)\nZ\u2190 S(H)\nfor each hi\u2208 H do\nif hi \u2208 V then\nPi = \u03a3; (Zij)\nend if\nend for\nK\u2190[I]\nV' \u2190 TopK(vi \u2208 V) sorted by Pi"}, {"title": "Algorithm 6 Co-occuring Lo patch filter", "content": "Require: Text token vocabulary T; vision token vocabulary: V; image V; fixed Prompt T; MLLM M; SAE-V S; features of SAE-V F:{fj}", "\u03b3;\nEnsure": "Filtered image V'\nInitialize score of each patch pi \u2190 0", "hi": "hi \u2208 H, zi = eiZ, zif >"}]}