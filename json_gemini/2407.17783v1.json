{"title": "How Lightweight Can a Vision Transformer Be", "authors": ["Jen Hong Tan"], "abstract": "In this paper, we explore a strategy that uses Mixture-of-Experts (MoE) to streamline, rather than augment, vision transformers. Each expert in an MoE layer is a SwiGLU feedforward network, where V and W2 are shared across the layer. No complex attention or convolutional mechanisms are employed. Depth-wise scaling is applied to progressively reduce the size of the hidden layer and the number of experts is increased in stages. Grouped query attention is used. We studied the proposed approach with and without pre-training on small datasets and investigated whether transfer learning works at this scale. We found that the architecture is competitive even at a size of 0.67M parameters.", "sections": [{"title": "1. INTRODUCTION", "content": "In real-world applications of computer vision, such as edge intelligence, small and performant models are still preferred to overcome computational challenges [1]. Vision Transformers (ViTs) [2] have achieved remarkable results, but their performance drops significantly when the model size and dataset are small [3]. Consequently, there are studies investigating lightweight vision transformers that perform well on mid-size datasets. Almost all of these studies either use new types of attention blocks [4, 5] or integrate convolutional mechanisms [6] into their architectures.\nOn the other hand, Tan [7] has shown that by employing Masked Auto-Encoder (MAE) [8] as a pre-training strategy, it is possible to get ViT to learn effectively from small datasets. In that work, the ViT consists of 12 transformer encoder layers, each containing a multi-head attention component and a feedforward network. The feedforward network consists of two linear layers: the first expands the output to twice, rather than four times, the embedding size, and the second reduces the output back to the embedding size. To further lighten the model, reducing the expanded output size in the middle of the feedforward network can help, but excessive reduction can negatively affect model performance.\nWith these considerations in mind, we designed an architecture that uses Mixture-of-Experts (MoE) [9] to streamline vision transformers. In our architecture, each expert in a MoE layer is formed by a SwiGLU [10] feedforward network. By design, SwiGLU is heavier in terms of parameters compared to a typical multi-layer perceptron. However, with several experts in a MoE layer, we are able to make the hidden size in SwiGLU smaller than the embedding size without negatively affecting model performance. Furthermore, we share 2 out of the 3 linear transformations in each SwiGLU across the layer. This helps to significantly lower the parameter count while maintaining the strength of MoE. Beyond that, to further reduce the number of parameters, we progressively increase the number of experts in the MoE in stages, while linearly reducing the hidden size by depth, borrowing the idea from depth-wise scaling [11]. Lastly, we use grouped query attention [12] to keep the parameter count low. Source code will be provided in near future."}, {"title": "2. METHOD", "content": "Our proposed approach consists of two parts: mLiT and mm-LiT. mLiT is a mixture-of-experts based Lightweight vision Transformer, and mmLiT is a mLiT pre-trained and fine-tuned using masked auto-encoder pretraining strategy."}, {"title": "2.1. mLiT", "content": "Similar to the original ViT [2], we start by dividing each image into non-overlapping patches. Each of these patches is linearly transformed into a set of embeddings, augmented by learnable positional embeddings. The processed embeddings go through a series of MoE-based transformer encoder layers. Figure 1 shows the overall structure of our transformer encoder layer."}, {"title": "2.1.1. Grouped Query Attention (GQA)", "content": "Grouped query attention divides query heads into G groups, with each group sharing a single key head and value head. For instance, GQA-1, with a single group, is equivalent to multi-query attention"}, {"title": "2.1.2. Mixture of Experts (MoE)", "content": "A single Mixture-of-Experts layer comprises t number of expert networks E1, E2,..., Et, with a gating network G that directs each input vector/embedding to the relevant expert(s) in the layer. For each vector or embedding, the output of G is a sparse t-dimensional vector. Figure 2 shows an overview of the working of MoE when the input is a vector.\nLet's denote a vector/embedding by x, the output of the i-th expert by Ei(x), and the i-th component of the output of the gating network G(x) by G(x)i. The output y of the MoE is given by:\n$y = \\sum_{i=1}^{t} G(x)_iE_i(x)$                                                              (1)\nAs the output G(x) is sparse, in MoE, no computation is performed on the j-th expert when G(x)j = 0. In many applications, however, the input to the layer is generally of shape (b, n, m),"}, {"title": "Noisy Top-K Gating Network.", "content": "We follow the proposal by Shazeer et. al. [9] that adds sparsity and noise to a softmax gating mechanism. Assume a reshaped input tensor x with a shape (b\u22c5 n, m) and two trainable weight matrices W, and Wnoise, both of shape (m, t), where t is the total number of experts in the layer and k is the number of expert(s) to be selected. The output of the gating network is given by\nG(x) = Softmaxk (H(x))                                                                 (2)\nwhere\nH(x) = xWg+randn_like(xWg)Softplus (xWnoise)                                                                 (3)\n denotes the element-wise product. randn_like generates a tensor filled with random numbers from a normal distribution with mean 0 and variance 1, and the shape of the tensor is equal to the shape of the output from xWg. Softmaxk applies softmax (row-wise) only on the top k elements, with the rest set to 0 in the output.\nThe equation below illustrates how Softmaxk works when k is 2 and t (the number of experts) is 4:"}, {"title": "Losses for MoE.", "content": "To encourage all experts to have equal importance, two loss functions are introduced: load balancing loss and importance loss. To calculate load balancing loss, we first determine the following probability by\nP(x) = \u03a6\\left(\\frac{xW_g - \u03a8(H(x))}{Softplus(xW_{noise})}\\right)                                                             (5)\nwhere \u03a6 is the cumulative distribution function of a standard normal distribution. To calculate the output of \u03a6, we let H denote the output of H(x), which has a shape of (bn, t), with each element in H denoted by hr,c. Furthermore, we let Hr denote a row by index r in H. Similarly, \u03a8r,c denotes each element in the output of \u03a8(H(x)). If we let lk and lk+1 denote the kth and k + 1th largest values respectively for a row r in H, lk > lk+1, then\n\u03c8r,c = \\begin{cases}\n-1 & \\text{if } h_{r,c} < l_k\\\\\nl_{k+1} & \\text{if } l_k < h_{r,c} \\leq l_k\\\\\n\\end{cases}                                                             (6)\nwhere k is the amount of expert(s) to be selected in the layer. As an example, assume we have"}, {"title": "2.1.3. Depth-wise Scaling", "content": "Our vision transformer does not have a fixed hidden size dh across the transformer encoder layers. Instead, the hidden size is linearly reduced from the first layer to the last layer. Let dfirst and dlast denote the hidden size of the first and the last layer respectively, the hidden size of any layer is given by\nd_i = \\left\\lfloor \\frac{(L_E-1)-i}{L_E-1}(d_{\\text{first}} - d_{\\text{last}}) \\right\\rfloor + d_{\\text{last}}                                                                (15)\nwhere Le is the total number of the transformer encoder layers, \u230a\u230b is floor operator, and i is a layer index starting from 0.\nFurthermore, the number of experts in mLiT is increased at different stages, specifically from 3 to 5 after every 3, 4, or 5 layers, depending on the model size. See Figure 4 for more detail."}, {"title": "2.2. mmLIT", "content": "In mmLiT, we apply masked auto-encoder on mLiT. We follow strictly the process outlined in [7], which is based on the original MAE paper [8]. The masking ratio is set to 0.75. However, unlike in [7], we did not incorporate a separable learnable positional embeddings at the decoder's input (See Figure 5). Similar to [7], we compute the loss of the auto-encoder as a sum of the loss on masked patches and an additional, discounted loss on unmasked patches. The total loss is given by\nLoss = MSEmasked + \u03b1 MSEunmasked + \u03b2L                                                                   (16)\nwhere \u03b1 denotes the discounting factor, \u03b2 is the loss coefficient for MoE. L is calculated from Equation 12."}, {"title": "3. EXPERIMENTAL SETUP", "content": "We investigate the performance of mLiT and mmLiT at three different sizes: S, XS and XXS. Table 1 shows the details of the encoder of each model, and Table 2 for decoder. For mmLiT, we perform self-supervised pre-training on the Cifar100 [14] training datasets and fine-tune on Cifar100, Cifar10[14], Flowers102 [15] and Svhn [16]. Additionally, we conduct supervised learning (without any pre-training) on the aforementioned four datasets.\nThe input image to the models has a size of 36 x 36, slightly larger than the original 32 x 32 dimensions of these datasets (except Flowers102, which is much larger and varied). This results in each image being divided into 144 patches, given the patch size of 3 x 3. Similar to [7], an auxiliary dummy patch is added to each image during both the pre-training and fine-tuning phases.\nThis dummy patch, which contains zero values for all elements, is appended as the 145th patch for classification purposes.\nAll linear layers within the MoE based transformer encoder layers include bias. However, we exclude bias from other linear projection layers in both the encoder and decoder. For initializing weights and biases across all layer types, we rely on the default methods provided by Pytorch. The same applies to our approach to layer normalization [17], where we use Pytorch's default setup."}, {"title": "3.1. Pre-training", "content": "mmLiT-S, mmLiT-XS, and mmLiT-XXS were pre-trained on Cifar100 for 4000, 6000 and 8000 epochs, respectively. We employed the AdamW optimizer [18] with a weight decay set at 0.05. The initial 5% of the total epochs were designated for warm-up [19]. We followed this with a cosine decay schedule [20] for the learning rate and adhered to the linear learning rate scaling rule with a base learning rate of 3e - 4 [19, 8]:\nlr = base_lr \u00d7 batch_size/256                                                            (17)\nSee Table 3 for more details."}, {"title": "3.2. Fine-tuning", "content": "For each pre-trained model, we conducted two sets of fine-tuning experiments. First, we fine-tuned the models pre-trained on Cifar100 for Cifar100 classification over 300 epochs. Second, we evaluated the transfer learning capabilities of the models by fine-tuning each one on Cifar100, Cifar10, Flowers102, and SVHN for 100 epochs. Table 4 shows the configuration for the first set of fine-tuning, and Table 5 shows the deviations in configurations with respect to Table 4 for the second set of fine-tuning.\nAdditionally, we evaluated the performance of various sizes of mLiT models (trained from scratch with only supervised learning). Table 6 shows the deviations in configurations for supervised learning on mLiT on the four datasets. We adhere to the linear learning rate scaling rule (Eq. 16) for all fine-tunings and supervised learning. All pre-trainings and fine-tunings were done using mixed precision (torch.float16)."}, {"title": "4. RESULT", "content": ""}, {"title": "4.1. On Pre-Training", "content": "Figure 6 presents the pre-training losses for various sizes of mm-LiT. The reductions in training losses exhibit approximately a log-linear relationship when plotted on logarithmic scales for both the x-axis (epochs) and y-axis (loss values). This indicates an exponential decrease in losses across epochs."}, {"title": "4.2. On Fine-Tuning", "content": "Table 7 compares the performances of models with similar sizes, all trained or fine-tuned over 300 epochs. mmLiT-S achieves an accuracy nearly 1% higher than Mae-ViT-C100, despite having only two-thirds the parameters. mmLiT-XS, which is one-third the size of Mae-ViT-C100, exhibits a performance only 1.5% lower. More interestingly, mmLiT-XXS, with just 18% of the parameters of Mae-ViT-C100, remains competitive, trailing by only 3.3% in accuracy. Furthermore, mmLiT-XXS slightly outperforms ResNet56, even though ResNet56 has 18% more parameters.\nTable 8 illustrates the transfer learning capabilities of the models pre-trained only on Cifar100, with reference to the results reported from [21]. It can be seen that mmLiT is competitive even at a scale of 0.67M parameters, where ViT-T+SSAT is almost 9 times larger, and the rests are at least 30 times larger than mmLiT-XXS. Furthermore, with the exception of CVT-13, which has convolutions in the architecture, mLiT always performs better than vanilla ViT and Swin-T even at the smallest scale on Cifar100, Cifar10 and Svhn. Flowers102 is a fine-grained classification dataset, so it is no surprise that mLiT is not competitive at an image size of 36 x 36."}, {"title": "5. DISCUSSION", "content": "The results from Table 8 show that ViT can learn better with streamlined MoE at a much smaller scale. In our current proposal, V and W2 are shared across an MoE layer. The main consideration behind this arrangement is the non-linear silu applied on xW. Nevertheless, we have also explored two other possible sharing arrangements on mmLiT-S (see Table 9), revealing that retaining W gives a slight advantage.\nIn our design, we have progressively reduced the hidden size and increased the number of experts at several stages. This arrangement is inspired by convolutional neural networks [27], where the size of the feature maps is reduced successively across layers, and the number of feature maps increases as the network gets deeper. We found that with the arrangement of MoE, a reduction in hidden size leads to little deterioration in performance if the reduction is not significant at the early layers and not more than 75% in the last layers. Further experiments are required in future to confirm this.\nWhen we performed fine-tuning on the four datasets, we started by applying the fine-tuning setup we used for Cifar100 to the other three datasets. But overfitting at an early stage became a consistent issue, and because of that, we had to modify the setup to accommodate each dataset. We encountered similar problem when we tried to perform supervised learning with mLiT from scratch. Therefore, we reported setups in Table 4 and Table 5 for clarity. It is worth nothing that we did not perform a thorough search on the configurations; we simply adjusted the hyperparameters to avoid overfitting during training. Furthermore, unlike in [21], we did not use advanced augmentation techniques in all our fine-tunings.\nDuring pre-training of mmLiT, we used different amount of epochs for the different sizes of models. From Figure 6, it is clear that more epochs should have been allocated for mmLiT-XS and mmLiT-XXS from the perspective of loss. However one of the downsides of masked autoencoders is the unclear relationship between the loss of a model and its subsequent performance in downstream tasks. In our experience, we have seen cases where further pre-training for a few hundreds epochs at a later stage leads to similar or poorer performance in classification. This might be a specific issue when the dataset is small. Despite the above, we believe that with such datasets, further pre-training of thousands of epochs should lead to a better model.\nIn this study, we found that mLiT pre-trained on only 50,000 images can serve as a sort of foundation model [28] even at the smallest size. On both Cifar10 and Flowers102, models pre-trained on Cifar100 improve by at least 10%. On Cifar10, mmLiT-XXS can reach 90% accuracy in less 40 epochs. On Svhn, the improvements are modest. This is partly because mLiT is already performing well without pre-training. It is also possibly due to the lack of similar images in Cifar100. We believe a model pre-trained on a slightly larger and a more diverse dataset can perform competitively on various simpler tasks at these tiny scales.\nIn literature, it is common to integrate convolutional layers [29, 23] or employ a convolutional 'teacher' [24] for imparting inductive bias. It seems the use of streamlined MoE can help alleviate the lack of inductive bias. However, as demonstrated in [7, 21], with a masked auto-encoder setup, the problem of inductive bias can be overcome."}, {"title": "6. CONCLUSION", "content": "In this paper, we demonstrated the potential of streamlined MoE architecture to create very lightweight vision transformers. By sharing parameters across MoE layers and adopting depth-wise scaling, we achieved a balance between model complexity and performance. Furthermore, our findings suggest that pre-training on a slightly larger and more diverse dataset could enhance the model's versatility and efficacy across various tasks. The streamlined MoE approach appears promising in mitigating the lack of inductive bias, particularly when used in conjunction with masked autoencoder setups."}]}