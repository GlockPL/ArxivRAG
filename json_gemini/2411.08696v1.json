{"title": "SCHOLARLY WIKIDATA: POPULATION AND EXPLORATION OF CONFERENCE DATA IN WIKIDATA USING LLMS", "authors": ["Nandana Mihindukulasooriya", "Sanju Tiwari", "Finn \u00c5rup Nielsen", "Tek Raj Chhetri", "Axel Polleres", "Daniil Dobriy"], "abstract": "Several initiatives have been undertaken to conceptually model the domain of scholarly data using ontologies and to create respective Knowledge Graphs. Yet, the full potential seems unleashed, as automated means for automatic population of said ontologies are lacking, and respective initiatives from the Semantic Web community are not necessarily connected: we propose to make scholarly data more sustainably accessible by leveraging Wikidata's infrastructure and automating its population in a sustainable manner through LLMs by tapping into unstructured sources like conference Web sites and proceedings texts as well as already existing structured conference datasets. While an initial analysis shows that Semantic Web conferences are only minimally represented in Wikidata, we argue that our methodology can help to populate, evolve and maintain scholarly data as a community within Wikidata.\nOur main contributions include (a) an analysis of ontologies for representing scholarly data to identify gaps and relevant entities/properties in Wikidata, (b) semi-automated extraction \u2013 requiring (minimal) manual validation - of conference metadata (e.g., acceptance rates, organizer roles, programme committee members, best paper awards, keynotes, and sponsors) from websites and proceedings texts using LLMs. Finally, we discuss (c) extensions to visualization tools in the Wikidata context for data exploration of the generated scholarly data. Our study focuses on data from 105 Semantic Web-related conferences and extends/adds more than 6000 entities in Wikidata. It is important to note that the method can be more generally applicable beyond Semantic Web-related conferences for enhancing Wikidata's utility as a comprehensive scholarly resource.", "sections": [{"title": "1 Introduction", "content": "Scientific conferences are vital for researchers to share their research findings and advancements. It offers an opportunity to discuss research problems or limitations, a platform for networking with peers, and a platform for promoting collaboration, which is essential for learning, innovation, and problem-solving. Because of the importance of scientific conferences, we have seen tremendous growth in the number of conferences over the years [1]. For example, IEEE (Institute of Electrical and Electronics Engineers) sponsors more than 2,000\u00b9 conferences and events annually. Similarly, ACM (Association for Computing Machinery) hosts more than 1702 conferences annually worldwide.\nTherefore, efforts have been made to capture metadata about scientific events [2, 3, 1, 4] in a linked-data format as they provide valuable information. Such data can be used for (i) better understanding the progress of science overall, (ii) the evolution of particular research topics (or fields), (iii) understanding research impact (e.g. by sponsors' interest) over time, etc. The availability of scholarly metadata enables scientometrics [5], or practical tools such as recommending relevant conferences or papers to readers [2] for navigating through the fastly growing scientific output which is becoming time-consuming and almost impractical.\nHowever, as much as the benefits these metadata about scientific events provide, there exist challenges. The primary obstacle is the collection of large-scale metadata, which is nontrivial in nature [2]. Similarly, the sustainability, which is also the focus of this paper, of the accumulated metadata constitutes the second and most significant obstacle. If the data collected is not sustainable, it may be lost over time, resulting in the loss of valuable information and efforts put into data collection. For instance, the Microsoft Academic Graph, which contained over 8 billion triples [2] with information about scientific publications and related data, was retired in December 20213. While the effort was somewhat continued shortly later in OpenAlex\u2074, the case demonstrates sustainability issues in individual or commercial scholarly KG offerings.\nWe argue that collaborative, general purpuse, community-driven platforms, such as Wikipedia, are generally more sustainable than such fragmented efforts: community participation is motivated by intrinsic factors, fostering a sense of belonging to the group [6]. Notably, commercial initiaves seem to recognize this, as shown by Google's declaration that it will cease operations on Freebase and transfer its content to Wikidata [7]. Wikidata, which focuses on knowledge graphs (KGs), is a sister project of Wikipedia and another example of a community-driven platform [8, 9]. Wikidata currently has more 110M entities and 25K active contributors. By bringing Scholarly data about scientific conferences into Wikidata, they can be seamlessly integrated with existing background knowledge through SPARQL queries. Furthermore, Wikidata benefits from a robust tooling ecosystem and widely used libraries, including entity linkers, search tools, SPARQL endpoint with high-availability, easy-to-use query editor, visualization tools, and more [10, 11]. Wikidata also allows non-expert users to directly access the KGs through search and Web UI (user interface). Therefore, the primary objective of our work is to integrate scientific conference metadata into Wikidata, a community-led platform.\nAfter conducting an analysis of Wikidata entities related to Semantic Web conferences such as International Semantic Web Conference (ISWC), Extended/European Semantic Web Conference (ESWC), International Conference Knowledge Engineering and Knowledge Management (EKAW), International Conference on Knowledge Capture (K-CAP), SEMANTICS, and Knowledge Graph and Semantic Web Conference (KGSWC), it was noticed that some conferences were missing and the ones that were present had only minimal information. In this project, we have extended Wikidata to include a more comprehensive set of information (e.g. see ISWC 20236 (Q119153957)). Within the scope of this work, we focused on the Semantic Web conferences but our method is more generally applicable and can be extended to other conference series. We note that 105 conferences we added to, updated in Wikidata is higher than the comparable related work such as Scholarly Data (35 confs)7, ORKG (5 confs) 8 as of July, 2024.\nLarge language models (LLMs) have proven their language understanding capabilities with many NLP benchmarks [12]. In recent years, approaches such as in-context learning with a few-shot example have allowed them to perform many tasks such as relation or fact extraction [13, 14]. Such models can be used to easily extract information from sources with natural language text, such as conference proceedings, websites, or call for papers. Nevertheless, their output can be prone to errors. In our work, LLMs are used to extract data, which is then verified by a human-in-the-loop validation to eliminate any noisy extraction and ensure accuracy.\nIn particular, this paper makes the following contributions.\n\u2022 We analysed existing ontologies for representing scholarly data and mapped them to Wikidata to identify relevant Wikidata entities/properties as well as gaps.\n\u2022 We present a method for utilizing large language models to efficiently extract conference metadata from various sources, curating them through a human-in-the-loop validation process using OpenRefine, and populating the data in Wikidata via Wikidata QuickStatements and provide an evaluation for LLM-based extractions.\n\u2022 As a result of this project, we have extended over 1000 existing entities and created more than 5,000 new entities, including conferences, scientific articles, and people. These entities are now available on Wikidata and can be accessed via the Web UI or SPARQL endpoint.\n\u2022 We extend visualization tools Scholia and Synia10 to better visualize the information we added to Wikidata."}, {"title": "2 Related Work", "content": "Scientific events have emerged as a crucial element in scholarly communication across various scientific fields. They serve as central hubs for fostering scientific connections among various elements such as individuals (e.g., organizers and attendees), locations, activities (e.g., participant roles), and materials (e.g., conference proceedings) within the realm of scholarly discourse [1].\nThis section will explore the existing work in the related topic.\nDifferent works have been done to capture and (re-)use the metadata about the scholarly events. The first work is by Fathalla et al. [1] who developed Scientific Events Ontology (OR-SEO) to capture the information of scientific events. OR-SEO is currently utilized as the framework for event pages on the OpenResearch11 (OR) platform, which serves as a semantic wiki platform aimed at crowdsourcing metadata and supporting scholarly metadata management and utilization. Similarly, Fathalla et al. [3], in their other work, introduce a 5-star Linked Dataset containing dereferenceable IRIS (Internationalized Resource Identifier), which is an updated version of the EVENTSKG dataset. The dataset contains information about 73 different conferences in the field of computer science, such as Artificial Intelligence (AI), World Wide Web (WEB), and Software Engineering (SE). Nuzzolese et al. [15] examine the Semantic Web Dog Food (SWDF) dataset and explore its quality and sustainability challenges. The SWDF employs the Semantic Web Conference (SWC) ontology as the foundational ontology for representing data related to academic conferences. Proposed approach uses cLODg3 (conference Linked Open Data generator) [16] to regenerate the SWDF dataset based on the conference ontology, offering a sustainable solution. Scholarly data [4] initiative refactors SWDF to continue the growth of the dataset. It introduces the conference ontology, which improves SWC. The Open Research Knowledge Graph (ORKG) is one of the main knowledge graphs designed to enable the structured representation and sharing of scholarly knowledge. ORKG aims to transform scholarly publications into a structured, interconnected knowledge graph, allowing for easier data access, analysis, and reuse. Currently, ORKG has 5 conference records covering 3 conference series under the event.class12.\nIn addition to scholarly ontologies, work has been done on scholarly information tooling, such as visualization and scraping. Angioni et al. [17], for example, developed an AIDA dashboard for analyzing and comparing scientific conferences. Their work uses Computer Science Ontology and the AIDA knowledge graph which enable the construction of visualization, such as top authors and organizations. Similarly, Kruger et al. [18] developed an early system named DEADLINER to extract information, such as deadline, topic, title, and program committee, from conference and workshop announcements (call for papers). The other similar work is by Fahl et al. [19] who implemented a system for scraping the CEUR Workshop Proceedings site. Fahl et al.'s [19] work also includes entity linking of event locations and proceeding editor disambiguation. Kirrane et. al. [5] has presented a qualitative analysis of the main seminal papers by adopting a top-down approach and produced results with three bottom-up data-driven approaches (Saffron, Rexplore, PoolParty). The analysis has been conducted on the corpus of Semantic Web papers acquired from 2006 to 2015. Several other efforts already done in earlier time and proposed Bibster, a Peer-to-Peer system [20] for transforming the bibliographic data among research community. Scholia, which our work re-uses and extends, is another application by Nielsen et al. [21] that allows visualization of scholarly profiles for topics, people, organizations, species, chemicals, etc., using bibliographic and other information in Wikidata. In addition to visualization, Scholia also has functionality for scraping data from the DOI (Digital Object Identifier), NeurIPS conferences, CEUR workshop series, and Open"}, {"title": "3 Overview of Scholarly Wikidata Process", "content": "The objective of our work is to extract information from structured and unstructured data sources and add it to Wikidata, specifically related to Semantic Web conferences. We leverage existing structured data and tools thereby promoting the reusability principles of the Semantic Web and further enhance them with additional information extracted from text using LLMs."}, {"title": "3.1 Data Sources", "content": "We considered both structured and unstructured data as input to our process. We wanted to extract information from authoritative sources, so we used the official conference proceedings and the conference website as input to our process. Table 1 illustrates the different sources we used in our process. These sources contained most of the information we wanted to extract. If the data has already been extracted and exposed in structured formats, especially in RDF (Resource Description Framework), such as DBLP and Scholarly Data, we reuse them."}, {"title": "3.2 Data population with human-in-the-loop validation", "content": "Figure 1 shows the process we followed to extract the data from different sources and populate data in Wikidata.\n1. semi-automatic extraction from unstructured sources In order to extract the necessary information from unstructured sources such as conference proceedings front matter or conference websites, we have LLMs using the"}, {"title": "3.3 Exploration and visualizations", "content": "We have used Scholia and Synia as visualization tools to allow the community to explore conference-related information with visual summaries, including line charts (e.g, acceptance rates, number of participants), area charts (e.g, topics through time), maps (e.g, conference locations), graphs (e.g., co-author graphs), and timelines (e.g., important dates, deadlines). While Scholia's visualizations of Wikidata content are defined in the Scholia Web application with Jinja2 templates, Synia's visualizations are defined on wiki pages, making it easier to add new visualizations. The visualizations are generated reusing the standard graph plotting of the Wikidata Query Service and embedded on the Scholia (or Synia) webpage via Iframes. To visualize the new conference data we created new Synia template wiki pages."}, {"title": "4 Mapping Conference Ontologies to Wikidata", "content": "Several ontologies representing conference metadata exist in literature as discussed in Section 2. Our objective was to use existing ontologies to understand the important aspects that should be represented in Wikidata. We utilized those conceptual models to identify the relevant Wikidata entities, properties, and qualifiers to represent scholarly data. We identified gaps in Wikidata and proceeded to add the missing elements to the platform to extend its capabilities."}, {"title": "4.1 Overview of Mappings to Wikidata", "content": "This section provides an overview of how existing conference ontology properties maps to Wikidata properties. Table 3 shows a comparison summary of several conference-related ontologies and Wikidata. More detailed mappings are documented in the Wiki\u00b2\u00b9 of the project."}, {"title": "4.2 Extensions to qualifier values", "content": "Wikidata data model has a flexible way to include fine-grained information through a primary relation and a set of qualifiers. Fig. 2 illustrated how the property \u201chas programme committee member\u201d property is used to link \u201cIrene Celino\" to \"ISWC 2023\" and qualifiers \u201capplies to\u201d and \u201cobject has role\" are used to provide additional information that she was an SPC in the research track. We used Reference URL to refer to the source where the fact was extracted. Based on our analysis of the existing ontologies in literature as discussed before, we have added several qualifier values to populate richer information. Table 4 illustrates some examples of such additions. For instance, more than 60 organizer roles were added to represent the conferences we analyzed. In addition to adding new qualifiers, we also modified the property constraints appropriately considering the use case of conference metadata."}, {"title": "5 Data exploration", "content": "Fig. 3 shows a screenshot of Scholia displaying the topics though time for a scientific event series, the ESWC conference series. The SPARQL query behind this visualization assembles topics for each year based on the chain event series - event - proceedings - article - topic and the publication year of the article. This visualization was inspired by a note in [25]: \"One drawback of [Scholia] is that the topics are associated to venues as a whole and cannot be used to evaluate their temporal evolution\". Any interpretation of the plots should be aware that the data behind the plot is not complete, i.e., not all conference articles are added and annotated in Wikidata. In Fig. 4 another Scholia screenshot shows the co-author graph of people associated with the ISWC conference series."}, {"title": "6 Evaluation", "content": "In this section, we present evaluations for four extraction tasks using two LLMs to extract facts from preface proceedings and conference websites.\nSetup For this evaluation, we used text from 42 prefaces and websites (22 ISWC, 20 ESWC) as a test dataset. For each task, we created a human extraction ground truth by manually extracting the facts by reading the preface. Then after performing the LLM extractions, we compared system generations with the human ground truth to calculate micro precision, recall, and F1 metrics on extraction. The system is not penalized for not extracting the information that is not available in the text.\nDiscussion Table 5 shows that both LLMs we tested performed well in extracting submitted/accepted papers, organizational roles, and PC members from the proceeding front matter. The proceedings clearly list the organization"}, {"title": "7 Conclusions and Future Work", "content": "In this paper, we present our work on scholarly data, with a focus on Semantic Web-related conferences. The paper addresses the sustainability challenges (see Section 1) of metadata by bringing the metadata of scholarly events, particularly conferences, to Wikidata. Our work uses LLMs to efficiently extract data from authoritative sources such as conference proceedings and websites and performs a human-in-the-loop validation to ensure any noisy extractions are eliminated. Our work has resulted in the creation of more than 6K entities or more than 30k statements about scientific conferences in Wikidata, providing valuable information about Semantic Web-related conferences in a machine-processable format aka KGs representation.\nImpact and Reusability By populating scholarly data in Wikidata, we make them available to a large community of both experts and non-experts using SPARQL endpoints, Web UI, and other tools such as visualizations or entity linkers. Wikidata is supported by the Wikimedia Foundation and a large community of active contributers, ensuring its sustainability. The availability of this scholarly data in Wikidata enables the community to easily access information such as acceptance rates by making simple queries. Organizers and track chairs can easily access the programme committee and other information from past years. By combining with existing data, interested parties can run ad-hoc queries, for example, the gender balance in ISWC organization committees throughout the previous years (see query25). The data we added to Wikidata, along with their sources, can potentially be used to generate benchmarks for tasks in scholarly domain, such as information extraction or question answering.\nThe data in Wikidata follows Linked Data and Findable, Accessible, Interoperable, and Reusable (FAIR) principles. In addition to the visualizations in Scholia and Synia, we also provide a Wiki page with a list of example queries26.\nDesign & Technical Quality We analyzed the existing ontologies in the scholarly data domain and used them to identify appropriate Wikidata entities, properties, and qualifiers. We followed the best practices of Wikidata by using appropirate qualifiers. For the properties we identified as gaps in Wikidata, for example, the number of submitted/accepted papers, we created property proposals following Wikidata guidelines as discussed in Section 4.1. We used authoritative sources such as conference proceedings or official website as our source. Furthermore, we included the reference URLs of sources where we extracted the facts to ensure their provenance is maintained and the community members can verify those. We used tools such as OpenRefine to perform entity reconciliation to avoid creation of duplicates when some entities already existed in Wikidata.\nAvailability The data we generated is added to Wikidata and is already accessible via the Wikidata web user interface and SPARQL endpoint. Data in Wikidata is distributed under a Creative Commons CC0 license. Additionally, the source code for the extraction and population process is the GitHub repository under MIT license. A snapshot of the proceeding front matter links, website crawls, SPARQL query results, LLM extractions, and evaluation benchmarks are available on Zenodo, with a Creative Commons CC0 license, a persistent URI (DOI) and a canonical citation.\nAs future work, we are planning on creating a comprehensive benchmark for extracting triples from scholarly data communications with manually curated ground truth and making it available for the community. We believe such benchmark can help the community to evaluate new approaches and strategies for information extraction from sources using novel technologies such as LLMs.\nFurthermore, based on our empirical evaluations based on the human-created ground truth, we plan to recommend a subset of extraction tasks that can be fully automated with LLMs with minimal noise so that they can be integrated to Wikidata as bots to perform large-scale extractions on all conference series.\nThe fact that Wikidata can be edited by anyone is quite useful for crowd-sourcing information, but also brings some challenges. Even though our experience with scholarly data on Wikidata since 2016 does not show its a common occurrence, Wikidata can be prone to vandalism and inclusion of falsified information by malicious parties [27]. Wikidata tracks the provenance of all edits with username / IP addresses, and analysis of these edits using ML models can be used to detect vandalism. Ensuring the veracity (i.e., accuracy) of crowd-sourced conference information within Wikidata poses an open challenge and we plan to formulate different strategies as part of our future work."}]}