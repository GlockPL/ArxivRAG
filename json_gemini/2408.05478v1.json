{"title": "Multi-agent Planning using Visual Language Models", "authors": ["Michele Brienza", "Francesco Argenziano", "Vincenzo Suriani", "Domenico D. Bloisi", "Daniele Nardi"], "abstract": "Large Language Models (LLMs) and Visual Language\nModels (VLMs) are attracting increasing interest due to their improv-\ning performance and applications across various domains and tasks.\nHowever, LLMs and VLMs can produce erroneous results, especially\nwhen a deep understanding of the problem domain is required. For\ninstance, when planning and perception are needed simultaneously,\nthese models often struggle because of difficulties in merging multi-\nmodal information. To address this issue, fine-tuned models are typ-\nically employed and trained on specialized data structures represent-\ning the environment. This approach has limited effectiveness, as it\ncan overly complicate the context for processing. In this paper, we\npropose a multi-agent architecture for embodied task planning that\noperates without the need for specific data structures as input. In-\nstead, it uses a single image of the environment, handling free-form\ndomains by leveraging commonsense knowledge. We also introduce\na novel, fully automatic evaluation procedure, PG2S, designed to bet-\nter assess the quality of a plan. We validated our approach using the\nwidely recognized ALFRED dataset, comparing PG2S to the exist-\ning KAS metric to further evaluate the quality of the generated plans.", "sections": [{"title": "Introduction", "content": "Foundation Models (FMs) are machine learning models that are\ntrained on a broad (Internet-scale) amount of data and can be re-\nfined to be used in a wide range of downstream applications [6]. Ini-\ntial examples of these models, i.e., Large Language Models (LLMs)\n[9, 7, 1, 31], were inherently of the Natural Language Processing\n(NLP) field. Nevertheless, in the last years, we have witnessed the\nemergence of multi-modal LLMs, which can handle non-textual in-\nputs and outputs. Visual Language Models (VLMs) [16, 22] have\nparticular relevance in this category since they can take as input im-\nages and/or textual queries and generate contextual high-quality out-\nputs. Additionally, the birth of many toolkits like HuggingFace [35]\nor LangChain [5] have contributed to the outburst and the distribution\nof such models, widening their domain of applications.\nIt has been demonstrated that LLMs can be used as zero-shot [12]\nand few-shot [28] planners. This is due to the fact that these models\nhave been trained on huge amounts of data, therefore they incorpo-\nrate the commonsense knowledge proper of humans [14]."}, {"title": "Related Work", "content": "In this section, we discuss existing solutions about both using LLMs\nfor planning and adopting a multi-agent architecture for prompting."}, {"title": "LLMs as Planners", "content": "A pioneering work that exploits the use of LLMs for embodied\nagents is SayCan [2], where a robot can behave as \"hands and eyes\"\nfor an LLM when grounding tasks in real-world scenarios, taking ad-\nvantage of the semantic knowledge of the model when performing\ncomplex instructions. Following this research, several approaches\nstarted to emerge that tried to use LLMs as the planning component\nin many different use cases.\nHuang et al. [12] demonstrate that LLMs behave like zero-shot\nplanners when they are correctly prompted. In contrast, Song et al.\n[28] show that tuning these models in a few-shot setting, allows them\nto surpass state-of-the-art Vision Language Navigation (VLN) mod-\nels even if they are trained on a broader amount of data, thanks to\nLLMs' embedded commonsense knowledge.\nLLMs' capabilities change when the query in input is not com-\npletely textual, but can assume a more structured form, e.g., a tabular\nstructure [15], a graph-like structure [23] (such as 3D Scene Graphs\n4), or even LTL formulas [8]. Incorporating this additional infor-\nmation is useful to improve the overall performance in the desired\ntasks. However, the biggest drawback of these techniques is that they\nrequire a very high computational cost when applied to real-world\nscenarios, where the environment is unstructured."}, {"title": "Multi-agent Prompting", "content": "As LLMs became more and more diffused, it was discovered that\nspecific prompting patterns produced better results than free-form\nprompts (prompt engineering) [39]. In planning applications, chain-\nof-thought reasoning [34] has marked a notable advance, with multi-\nstep reasoning.\nAnother important step in prompt engineering with LLMs is\nachieved by leveraging the power of multi-agent systems. In [30], a\ncollaborative environment where multiple agents with different roles\nhad to work together to accomplish a task, is demonstrated to have\nbetter performance w.r.t. a single-agent. Moreover, results improved\nnot only in settings with many role-specific agents but also in settings\nwith multi-persona self-collaborating agents[33].\nSeveral frameworks started to emerge, simplifying the develop-\nment of multi-agent applications [36, 24, 26]. As a drawback, these\nframeworks intrinsically increase the complexity of the systems that\nadopt them."}, {"title": "Methodology", "content": "The typical interaction between an LLM and a user consists of a\ntrial-and-error process to obtain the desired result by refining the\nprompt. The accuracy of the environmental information is crucial\nto obtain a correct plan. Usually, this information comes from tables\nor structured data. Our method is based on relaxing the structured\ninformation known a priori from a previous labeling process. In our\narchitecture, we use a multi-agent pipeline that takes as input only an\nimage of the environment, along with the task to execute. Then, we\nshow how this strategy allows us to have a correct plan, even in free-\nform domains. To assess the correctness, we use our PG2S metric by\ncomparing the plans devised from images and those from tables by\nreferring to ALFRED's annotations."}, {"title": "Multi-agent Planning", "content": "Our solution employs three agents, each representing a phase in the\nplanning generation process: the Semantic-Knowledge Miner Agent\n(SKM), the Grounded-Knowledge Miner Agent (GKM), and the\nPlanner Agent (P). GPT-4V is used for agents that process images,\nwhile GPT-4 is used for the planning agent [1].\nThe SKM Agent identifies object classes within the image and es-\ntablishes the scene's ontology. It also determines relationships be-\ntween objects, creating a knowledge graph. The GKM Agent grounds\nthese objects, providing short descriptions that include their relation-\nships with surrounding objects, resulting in a high-level yet struc-\nturally sound scene description. The P Agent then generates a plan\nusing the information from the SKM and GKM Agents. This method\nminimizes hallucinations and focuses the plan on the relevant objects\nin the scene.\nUsing a Visual Language Model (VLM), we achieve better results\nwith a multi-agent strategy compared to a single-agent approach. In a\nsingle-agent setup, the prompt directs the VLM to create a plan from\nthe input image. In contrast, the multi-agent setup allows the Miner\nAgents to enrich the Planning Agent's knowledge with detailed en-\nvironmental information, as illustrated in Fig. 2.\nThe multi-agent strategy enhances plan quality by distributing the\nworkload among agents, each handling specific tasks. This division\nreduces the risk of hallucinations by maintaining smaller, more fo-\ncused prompts within each agent's context window [17]. By splitting\nthe task into simpler sub-tasks, our pipeline ensures more accurate\nand coherent responses, following the \"divide and conquer\" princi-\nple."}, {"title": "Evaluation", "content": "Choosing an adequate metric to evaluate the quality of produced\nplans is not trivial. Usually, only the Success Rate (SR) or the SR\nweighted by the inverse path length (SRL) are used to evaluate the\nplan correctness [28, 10]. However, these metrics are not very conve-nient to compute, and researchers often rely on Amazon Mechanical\nTurk to check the correctness using human experts. Moreover, they\ndo not evaluate the quality of the plan: they state how many times the\ngoal is achieved and how the length of the plan influences the result.\nG-PlanET [15] tries to define a new metric to cope with this prob-\nlem: inspired by metrics used for semantic captioning like CIDEr\n32] and SPLICE [3], it proposes KeyActionScore (KAS). KAS\nbuilds a set of key action phrases obtained from every step of the gen-\nerated plan $\\hat{S}_i$, and from the reference plan of the dataset $S_i$. Then,\nby checking how many action phrases in $\\hat{S}_i$ are covered by $S_i$, and\nby computing this precision, it is possible to evaluate the matching\nquality of the two sets for the i-th step of the plan.\nThis metric present two main limitations. The first is that it always\nassumes that the reference plan is correct, which is not always true\nas we found some examples in the ALFRED dataset of plans that are\nnot completely correct: e.g. the reference plan for the goal \"Put a hot\nbread in the refrigerator\" has as one of the steps the action \"put the\nknife in the microwave\" which is extremely dangerous and globally\nincorrect for the desired goal. The second is that in definition of KAS,\na mapping is considered correct if and only if it follows the order of\nactions given by the step. This is a strong assumption, since there are\nmany plans in which the order of actions is not necessary to reach a\ngoal [19], so it can penalize plans that are actually correct.\nTo this end, we propose a new metric, PG2S, that copes with this\nproblem. As an example, we show a reference plan that can be used\nas a ground truth plan and a possible predicted plan (see Table 1). The\npredicted plan to reach the goal \"Wear a pair of shoes\" is correct for\na human evaluator. Despite this, the plan is different from the ground\ntruth in the order of the actions, and the evaluation should be able to\ntake into account this possibility. Using the KAS metric the similar-"}, {"title": "PG2S Evaluation Procedure", "content": "Require: $P_{gt}$ ground truth plan, $P_{pred}$ predicted plan\nEnsure: PG2S\n1: MaxSimilPlan, MaxSimilGoal $\\leftarrow$ []\n2: for $s_i \\in P_{gt}$ do\n3:   find the most similar sentence $s_j$ in $P_{pred}$\n4:   if exists: add 1 to MaxSimilPlan; otherwise add 0\n5:   $P_{pred}.pop(s_j)$\n6: Splan $\\leftarrow$ mean(MaxSimilPlan)\n7: Agt, Apred $\\leftarrow$ []\n8: for $s_i, s_j \\in P_{gt}, P_{pred}$ do\n9:   add actions in Agt and Apred with Framing()\n10: for $a_i \\in Agt$ do\n11:   find the most similar action $a_j$ in Apred\n12:   if exists: add 1 to MaxSimilGoal; otherwise add 0\n13:   Apred.pop($a_j$)\n14: Sgoal $\\leftarrow$ mean(MaxSimilGoal)\n15: PG2S $\\leftarrow \\alpha * S_{plan} + (1 - \\alpha) * S_{goal}$\nity score is equal to 0.33; while for PG2S (ours) the similarity score\nobtained is equal to 0.83. Algorithm 1 presents the procedure used to\ncompute such an evaluation score. More in detail, given two sets of\nplanning descriptions, $P_{gt}$ and $P_{pred}$, respectively the ground truth\nplan and the predicted plan, we aim at quantifying their similarity, us-\ning two levels of evaluation, namely a sentence-wise and a goal-wise,\nboth based on the semantic values. To determine if two embeddings\nare similar we use a threshold mechanism. In particular, we adopt\nthe approach presented in [25], where the authors obtain thresholds\nthat vary according to the dimensionality of the embedding vector\nand verify that their use allows to obtain only semantically similar\nelements.\nSentence-wise similarity. To compute the sentence similarity, we\ndeploy embedding vector representations for each sentence using\na Sentence Transformer. In particular, we use MPNet [29], which\nachieves better results in semantic evaluation tasks compared with\nprevious state-of-the-art pre-trained models [29] (e.g., BERT, XL-\nNet, and ROBERTa). For each sentence $s_i \\in P_{gt}$ and $s_j \\in P_{pred}$, we\nobtain the similarity between their embeddings ($v_i$ and $v_j$) using the\ncosine similarity $cos(v_i, v_j)$. For each $s_i$, we identify the most simi-\nlar sentence in $P_{pred}$ (line 3) and remove it from the set (line 5). The\nvalue of each similarity yields a list of maximum similarity scores.\nThe sentence-wise similarity is the average of these scores (line 6).\n$S_{plan} (P_{gt}, P_{pred}) = \\frac{1}{N} \\sum_{i=1}^{N} MaxSimilPlan_i$ (1)\nGoal-wise similarity. To compute the goal similarity, we first per-\nform a POS tagging pre-processing stage using spaCy [11], and then,\nfor each sentence we extract the main action using a Framing() pro-\ncedure (line 9). This procedure works as follows: for each word in a\nsentence, we add it in the action set if it is either i. a central ('root')\nverb (VERB), or ii. if it is a noun (NOUN) and its dependency tag\nis either a 'direct object' (DOBJ) or the 'nominal subject' (NSUBJ).\nIn this way, for each step we obtain the main action and the involved\nobjects. For each action $a_i \\in Agt$ and $a_j \\in Apred, we obtain a sim-\nilarity value from the product between the mean of nouns similarity\nand the verbs similarity, obtained from a WordEmbeddingSimilar-\nity() tool (Word2Vec [18]). We consider two nouns and two verbs to\nbe similar if their similarity value exceeds a threshold T = 0.708\naccording to [25].\nFor each action $a_i \\in Agt$ we identify the most similar action in\n$Apred$ and remove it from the set. The most similar action is found\nusing the combined similarity computed with the product of both\nvalues (line 11) and removed from Apred (line 13). The value of\neach action similarity yields a list of maximum similarity scores. The\naverage of these scores gives us the goal-wise similarity of the sets\n(line 14).\n$S_{goal}(A_{gt}, A_{pred}) = \\frac{1}{N} \\sum_{i=1}^{N} MaxSimilGoal_i$ (2)\nPG2S. The final similarity score is our metric PG2S, which is a\nweighted average of the sentence-level and action-state similarities,\nwhere $\\alpha$ is a weighting factor, set to 0.5 to equally balance the con-\ntributions of the two scores:\nPG2S = $(1 - \\alpha) * S_{plan}(P_{gt}, P_{pred}) + \\alpha* S_{goal} (A_{gt}, A_{pred})$ (3)\nAnother issue arises because KAS employs a set intersection,\nwhereby terms that are not equal are not considered for the similarity\ncalculation. This can result in the problem of having the same action\nwith a subject that is not appropriate for use in the case of goal simi-\nlarity. To illustrate this aspect, consider the action \"Walk to the desk\"\nin comparison to \"Walk to the moon\". In the case of KAS, the re-\nsulting similarity score is 0.67 because two out of three elements are\nequal, whereas in PG2S, the similarity score is 0. This discrepancy\ncan be attributed to the fact that KAS does not consider the nuances\nof natural language, whereas PG2S does."}, {"title": "Experimental Results", "content": "This section presents the outcomes of the conducted experiments,\nwhich were designed to test the proposed architecture's validity. The\nresults obtained using a single image are presented and then com-\npared with a structured perception of the environment, as seen in\nstate-of-the-art works. The output plans regarding home scenarios\ntasks are taken from the ALFRED dataset using the AI2Thor envi-\nronment. Chosen the image and the environment, for each of those\nwe have found the plan associated with the scene and saved the\nground truth plans that we have used to compare our results. The\nenvironment scenarios are chosen by selecting several different situ-\nations in order to have various complexity and domains of application\naccording to the chosen fields of ALFRED such as: picking up ob-\njects and placing them; picking up objects, heating or cooling them,\nand place them somewhere else; cleaning objects and examining un-\nder the light; and more."}, {"title": "Evaluation of our PG2S Metric", "content": "During the experimental phase of PG2S development, a series of tests\nwere conducted to ensure the correctness of the metric. Specifically,\nwe compared ALFRED plans with those predicted by our architec-\nture, together with their corrupted version. During the test phase, sev-\neral examples were selected from the ALFRED dataset. The plans\nobtained were checked qualitatively and it was possible to verify\nthat the plans generated by the multi-agent architecture are correct"}, {"title": "Evaluation of our Architecture", "content": "To evaluate the presented methodology, we have chosen ten differ-\nent rooms of an apartment, such as a living room, a kitchen, and a\ntoilet. Frames were captured for each room as in the example in Fig.\n3 which depicts a kitchen. The complexity of generating a plan is\nevident, given that an entire scene is represented by a single image\nand that some of the objects needed can be quite small. Our tests\ndemonstrated that even in complex situations, the VLM is capable of\nidentifying objects and perceiving their relationships, allowing it to\ndefine a correct plan. The ten environments chosen allow us to obtain\nthirty tasks to perform and, for each of these plans, we have obtained\nthe plan using four approaches: two using a single-agent architecture\nand two using a multi-agent architecture. In both single-agent and\nmulti-agent evaluations, the plan was obtained using a table describ-\ning the environment rather than a single image.\nTable 3 presents the results, highlighting instances where the KAS\nmetric fails, resulting in None values. This failure occurs because the\nKAS metric cannot evaluate plans of different lengths, which was\ncommon in the \"with table\" setups.\nThe results show how using a single image the architecture gener-\nates a plan similar to the ground truth plan. Furthermore, we demon-\nstrated to obtain improved results in multi-agent architecture using a\nsingle image."}, {"title": "Discussion", "content": "The current state of the art involves the use of traditional Success\nRate metrics to evaluate a plan, where the plan is considered cor-\nrect in cases where execution leads to the desired outcome. How-\never, this metric is not sufficient or suitable for all cases where the\ncorrectness of a task execution plan is to be analyzed. In particular,\nin cases where the plan is complicated, it should be evaluated before\nexecution to avoid damage to the environment or simply unsuccess-\nful executions and ensure that time and resources are not wasted in\na new execution. The advent of LLMs has made it possible to eas-\nily generate plans that previously required model training or other\nmore complex techniques. Given that these models can' hallucinate'\nor generate incorrect responses, there could be errors present. There-\nfore, these inaccuracies could lead to failures when evaluating them\nbased on success rates. Our work seeks to define a new PG2S met-\nric for plan evaluation based only on natural language processing"}, {"title": "Conclusion", "content": "In this paper, we, first, introduced a multi-agent planning framework\nthat leverages the capabilities of Visual Language Models (VLMs)\nto improve planning for embodied agents without the need for pre-\nencoded environmental data structures. Our approach simplifies the\ninput requirements by utilizing a single environmental image and\nalso enhances the adaptability and effectiveness of the planning pro-\ncess through a multi-agent system. This innovation addresses the\nlimitations of traditional models that rely heavily on structured data,\nproviding a more flexible and dynamic planning mechanism that is\nparticularly effective in unstructured, real-world scenarios.\nThe empirical results, validated using the ALFRED dataset,\ndemonstrate the efficacy of our approach, especially when compared\nto existing metrics like the KAS metric. We, then, introduce a new\nmetric for the plan evaluation. The newly proposed PG2S metric,\nwhich assesses planning quality based on semantic understanding\nrather than strict action order, has shown superior performance in\ncapturing the variations of plan execution.\nThe presented approach can address some of the current limita-\ntions in embodied agent planning and can open future research in\nthe application of VLMs and multi-agent systems. Future studies\nmight explore the scalability of our approach to more complex multi-\nagent environments and the integration of more diverse modalities\nto enhance the agents' understanding of their operational contexts.\nPG2S explores novel possibilities in the plan evaluation, focusing on\nsemantic integrity rather than strict action sequencing. We believe\nthat the research community can take advantage of the proposed ap-\nproach, considering semantic coherence as a critical component of\nplan success, especially in applications requiring high reliability and\nsafety."}]}