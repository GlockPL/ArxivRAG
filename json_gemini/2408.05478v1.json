{"title": "Multi-agent Planning using Visual Language Models", "authors": ["Michele Brienza", "Francesco Argenziano", "Vincenzo Suriani", "Domenico D. Bloisi", "Daniele Nardi"], "abstract": "Large Language Models (LLMs) and Visual Language Models (VLMs) are attracting increasing interest due to their improving performance and applications across various domains and tasks. However, LLMs and VLMs can produce erroneous results, especially when a deep understanding of the problem domain is required. For instance, when planning and perception are needed simultaneously, these models often struggle because of difficulties in merging multi-modal information. To address this issue, fine-tuned models are typically employed and trained on specialized data structures representing the environment. This approach has limited effectiveness, as it can overly complicate the context for processing. In this paper, we propose a multi-agent architecture for embodied task planning that operates without the need for specific data structures as input. Instead, it uses a single image of the environment, handling free-form domains by leveraging commonsense knowledge. We also introduce a novel, fully automatic evaluation procedure, PG2S, designed to better assess the quality of a plan. We validated our approach using the widely recognized ALFRED dataset, comparing PG2S to the existing KAS metric to further evaluate the quality of the generated plans.", "sections": [{"title": "Introduction", "content": "Foundation Models (FMs) are machine learning models that are trained on a broad (Internet-scale) amount of data and can be refined to be used in a wide range of downstream applications [6]. Initial examples of these models, i.e., Large Language Models (LLMs) [9, 7, 1, 31], were inherently of the Natural Language Processing (NLP) field. Nevertheless, in the last years, we have witnessed the emergence of multi-modal LLMs, which can handle non-textual inputs and outputs. Visual Language Models (VLMs) [16, 22] have particular relevance in this category since they can take as input images and/or textual queries and generate contextual high-quality outputs. Additionally, the birth of many toolkits like HuggingFace [35] or LangChain [5] have contributed to the outburst and the distribution of such models, widening their domain of applications.\nIt has been demonstrated that LLMs can be used as zero-shot [12] and few-shot [28] planners. This is due to the fact that these models have been trained on huge amounts of data, therefore they incorporate the commonsense knowledge proper of humans [14].\nAn agent with commonsense knowledge acquires complex reasoning capabilities via chain-of-thought [34] and it becomes able to correctly generate a plan to achieve the desired goal. The generated plans are grounded in the sense that actions, objects, and states all refer to the specific environment the embodied agent is deliberating in, thanks to the information incorporated in the queries. Existing solutions for grounding concern encoding the environment in a structured manner, i.e., using tables or graphs [15, 23], since they are easily promptable to the model once converted in some sort of streamable format. However, these kinds of representations grow very quickly as the environment grows in size, thus it becomes difficult to incorporate them in a language model query prompt. The context window, namely the amount of text the model can handle as input when generating language, becomes very big as the prompt increases, and the output could be affected by several hallucinations [17]. This is a problem in any LLMs application, but particularly when we are trying to plan a specific procedure to achieve a certain goal. For this reason, it is important to keep the input to the model as small as possible, including only the necessary information to carry out the desired task. In fact, decomposing the goal into several sub-goals for multiple independent agents can drastically improve the final output, thus performing better w.r.t. single-agent architectures [33, 30, 20].\nIn this paper, we analyze the capabilities of FMs when used as reasoning components. In particular, we use FMs to deploy grounded plans for embodied agents in free-form domains, i.e. domains that come without a structured representation. All we need is a picture of the scene that captures the most relevant aspects of the environment, and a textual query on the goal we want to achieve. Our approach exploits a hierarchical multi-agent structure, in the sense that every agent is a different VLM/LLM instance which addresses to solve only one aspect of the whole planning procedure, according to the definition given by [30]. In this way, every agent has a limited context window, thus being less prone to hallucinations.\nWe conduct a comparative study to demonstrate that our single-image multi-agent scheme outperforms both an architecture utilizing a structured environment representation (such as tabular) and a single-agent architecture where all input is fed to the VLM simultaneously.\nWe also propose Planning Goal Semantic Score (PG2S), a new metric that does not rely on user validations to evaluate the results. PG2S does not consider the partial ordering of actions needed to carry out the plan to achieve a goal, and it is semantically sound in the sense that it deals with synonyms without losing the meaning of the plan. In Fig. 1 it is possible to see an overall view of our system. In summary, the contribution of this work is three-fold:\n\u2022 We demonstrate that it is possible to soundly plan to achieve a task with a VLM and a query input, relaxing the assumptions about complex structured query;\n\u2022 We propose a multi-agent framework to decompose the final task into different sub-tasks, thus reducing the risk of hallucinations\nand other harmful phenomena;\n\u2022 We introduce a new metric, PG2S, to autonomously evaluate the correctness of a plan expressed in natural language that is partial-ordering agnostic and semantically aware.\nWe validate our approach on ALFRED [27], a benchmark purposefully designed to evaluate natural language instructions mapping in household environments, built upon the AI2-THOR framework [13]. For comparison, we use G-PlanET [15], which contains all the simulation environments of ALFRED but represented in a tabular form. We release the code, the prompts used and the results obtained on our project website.2"}, {"title": "Related Work", "content": "In this section, we discuss existing solutions about both using LLMs for planning and adopting a multi-agent architecture for prompting."}, {"title": "LLMs as Planners", "content": "A pioneering work that exploits the use of LLMs for embodied agents is SayCan [2], where a robot can behave as \"hands and eyes\" for an LLM when grounding tasks in real-world scenarios, taking advantage of the semantic knowledge of the model when performing complex instructions. Following this research, several approaches started to emerge that tried to use LLMs as the planning component in many different use cases.\nHuang et al. [12] demonstrate that LLMs behave like zero-shot planners when they are correctly prompted. In contrast, Song et al. [28] show that tuning these models in a few-shot setting, allows them to surpass state-of-the-art Vision Language Navigation (VLN) models even if they are trained on a broader amount of data, thanks to LLMs' embedded commonsense knowledge.\nLLMs' capabilities change when the query in input is not completely textual, but can assume a more structured form, e.g., a tabular structure [15], a graph-like structure [23] (such as 3D Scene Graphs [4]), or even LTL formulas [8]. Incorporating this additional information is useful to improve the overall performance in the desired tasks. However, the biggest drawback of these techniques is that they require a very high computational cost when applied to real-world scenarios, where the environment is unstructured."}, {"title": "Multi-agent Prompting", "content": "As LLMs became more and more diffused, it was discovered that specific prompting patterns produced better results than free-form prompts (prompt engineering) [39]. In planning applications, chain-of-thought reasoning [34] has marked a notable advance, with multi-step reasoning.\nAnother important step in prompt engineering with LLMs is achieved by leveraging the power of multi-agent systems. In [30], a collaborative environment where multiple agents with different roles had to work together to accomplish a task, is demonstrated to have better performance w.r.t. a single-agent. Moreover, results improved not only in settings with many role-specific agents but also in settings with multi-persona self-collaborating agents[33].\nSeveral frameworks started to emerge, simplifying the development of multi-agent applications [36, 24, 26]. As a drawback, these frameworks intrinsically increase the complexity of the systems that adopt them."}, {"title": "Methodology", "content": "The typical interaction between an LLM and a user consists of a trial-and-error process to obtain the desired result by refining the prompt. The accuracy of the environmental information is crucial to obtain a correct plan. Usually, this information comes from tables or structured data. Our method is based on relaxing the structured information known a priori from a previous labeling process. In our architecture, we use a multi-agent pipeline that takes as input only an image of the environment, along with the task to execute. Then, we show how this strategy allows us to have a correct plan, even in free-form domains. To assess the correctness, we use our PG2S metric by comparing the plans devised from images and those from tables by referring to ALFRED's annotations."}, {"title": "Multi-agent Planning", "content": "Our solution employs three agents, each representing a phase in the planning generation process: the Semantic-Knowledge Miner Agent (SKM), the Grounded-Knowledge Miner Agent (GKM), and the Planner Agent (P). GPT-4V is used for agents that process images, while GPT-4 is used for the planning agent [1].\nThe SKM Agent identifies object classes within the image and establishes the scene's ontology. It also determines relationships between objects, creating a knowledge graph. The GKM Agent grounds these objects, providing short descriptions that include their relationships with surrounding objects, resulting in a high-level yet structurally sound scene description. The P Agent then generates a plan using the information from the SKM and GKM Agents. This method minimizes hallucinations and focuses the plan on the relevant objects in the scene.\nUsing a Visual Language Model (VLM), we achieve better results with a multi-agent strategy compared to a single-agent approach. In a single-agent setup, the prompt directs the VLM to create a plan from the input image. In contrast, the multi-agent setup allows the Miner Agents to enrich the Planning Agent's knowledge with detailed environmental information, as illustrated in Fig. 2.\nThe multi-agent strategy enhances plan quality by distributing the workload among agents, each handling specific tasks. This division reduces the risk of hallucinations by maintaining smaller, more focused prompts within each agent's context window [17]. By splitting the task into simpler sub-tasks, our pipeline ensures more accurate and coherent responses, following the \"divide and conquer\" principle."}, {"title": "Evaluation", "content": "Choosing an adequate metric to evaluate the quality of produced plans is not trivial. Usually, only the Success Rate (SR) or the SR weighted by the inverse path length (SRL) are used to evaluate the plan correctness [28, 10]. However, these metrics are not very convenient to compute, and researchers often rely on Amazon Mechanical Turk to check the correctness using human experts. Moreover, they do not evaluate the quality of the plan: they state how many times the goal is achieved and how the length of the plan influences the result.\nG-PlanET [15] tries to define a new metric to cope with this problem: inspired by metrics used for semantic captioning like CIDEr [32] and SPLICE [3], it proposes KeyActionScore (KAS). KAS builds a set of key action phrases obtained from every step of the generated plan $\\hat{S}_i$, and from the reference plan of the dataset $S_i$. Then, by checking how many action phrases in $\\hat{S}_i$ are covered by $S_i$, and by computing this precision, it is possible to evaluate the matching quality of the two sets for the i-th step of the plan.\nThis metric present two main limitations. The first is that it always assumes that the reference plan is correct, which is not always true as we found some examples in the ALFRED dataset of plans that are not completely correct: e.g. the reference plan for the goal \"Put a hot bread in the refrigerator\" has as one of the steps the action \"put the knife in the microwave\" which is extremely dangerous and globally incorrect for the desired goal. The second is that in definition of KAS, a mapping is considered correct if and only if it follows the order of actions given by the step. This is a strong assumption, since there are many plans in which the order of actions is not necessary to reach a goal [19], so it can penalize plans that are actually correct.\nTo this end, we propose a new metric, PG2S, that copes with this problem. As an example, we show a reference plan that can be used as a ground truth plan and a possible predicted plan (see Table 1). The predicted plan to reach the goal \"Wear a pair of shoes\" is correct for a human evaluator. Despite this, the plan is different from the ground truth in the order of the actions, and the evaluation should be able to take into account this possibility. Using the KAS metric the similar-"}, {"title": "PG2S Evaluation Procedure", "content": "Require: $P_{gt}$ ground truth plan, $P_{pred}$ predicted plan\nEnsure: PG2S\n1: $MaxSimilPlan, MaxSimilGoal \\leftarrow []$\n2: for $s_i \\in P_{gt}$ do\n3: $\\quad$find the most similar sentence $s_j$ in $P_{pred}$\n4: $\\quad$if exists: add 1 to $MaxSimilPlan$; otherwise add 0\n5: $\\quad P_{pred}.pop(s_j)$\n6: $S_{plan} \\leftarrow mean(MaxSimilPlan)$\n7: $A_{gt}, A_{pred} \\leftarrow []$\n8: for $s_i, s_j \\in P_{gt}, P_{pred}$ do\n9: $\\quad$add actions in $A_{gt}$ and $A_{pred}$ with Framing()\n10: for $a_i \\in A_{gt}$ do\n11: $\\quad$find the most similar action $a_j$ in $A_{pred}$\n12: $\\quad$if exists: add 1 to $MaxSimilGoal$; otherwise add 0\n13: $\\quad A_{pred}.pop(a_j)$\n14: $S_{goal} \\leftarrow mean(MaxSimilGoal)$\n15: $PG2S \\leftarrow \\alpha * S_{plan} + (1 - \\alpha) * S_{goal}$\nity score is equal to 0.33; while for PG2S (ours) the similarity score obtained is equal to 0.83. Algorithm 1 presents the procedure used to compute such an evaluation score. More in detail, given two sets of planning descriptions, $P_{gt}$ and $P_{pred}$, respectively the ground truth plan and the predicted plan, we aim at quantifying their similarity, using two levels of evaluation, namely a sentence-wise and a goal-wise, both based on the semantic values. To determine if two embeddings are similar we use a threshold mechanism. In particular, we adopt the approach presented in [25], where the authors obtain thresholds that vary according to the dimensionality of the embedding vector and verify that their use allows to obtain only semantically similar elements.\nSentence-wise similarity. To compute the sentence similarity, we deploy embedding vector representations for each sentence using a Sentence Transformer. In particular, we use MPNet [29], which achieves better results in semantic evaluation tasks compared with previous state-of-the-art pre-trained models [29] (e.g., BERT, XL-Net, and ROBERTa). For each sentence $s_i \\in P_{gt}$ and $s_j \\in P_{pred}$, we obtain the similarity between their embeddings ($v_i$ and $v_j$) using the cosine similarity $cos (v_i, v_j)$. For each $s_i$, we identify the most similar sentence in $P_{pred}$ (line 3) and remove it from the set (line 5). The value of each similarity yields a list of maximum similarity scores. The sentence-wise similarity is the average of these scores (line 6).\n$S_{plan} (P_{gt}, P_{pred}) = \\frac{1}{N} \\sum_{i=1}^{N} MaxSimilPlan_i$\nGoal-wise similarity. To compute the goal similarity, we first perform a POS tagging pre-processing stage using spaCy [11], and then, for each sentence we extract the main action using a Framing() procedure (line 9). This procedure works as follows: for each word in a sentence, we add it in the action set if it is either i. a central ('root') verb (VERB), or ii. if it is a noun (NOUN) and its dependency tag is either a 'direct object' (DOBJ) or the 'nominal subject' (NSUBJ). In this way, for each step we obtain the main action and the involved objects. For each action $a_i \\in A_{gt}$ and $a_j \\in A_{pred}$, we obtain a similarity value from the product between the mean of nouns similarity and the verbs similarity, obtained from a WordEmbeddingSimilarity() tool (Word2Vec [18]). We consider two nouns and two verbs to be similar if their similarity value exceeds a threshold $T = 0.708$, according to [25].\nFor each action $a_i \\in A_{gt}$ we identify the most similar action in $A_{pred}$ and remove it from the set. The most similar action is found using the combined similarity computed with the product of both values (line 11) and removed from $A_{pred}$ (line 13). The value of each action similarity yields a list of maximum similarity scores. The average of these scores gives us the goal-wise similarity of the sets (line 14).\n$S_{goal} (A_{gt}, A_{pred}) = \\frac{1}{N} \\sum_{i=1}^{N} MaxSimilGoal_i$\nPG2S. The final similarity score is our metric PG2S, which is a weighted average of the sentence-level and action-state similarities, where a is a weighting factor, set to 0.5 to equally balance the contributions of the two scores:\n$PG2S = (1 - \\alpha) * S_{plan}(P_{gt}, P_{pred}) + \\alpha* S_{goal} (A_{gt}, A_{pred})$\nAnother issue arises because KAS employs a set intersection, whereby terms that are not equal are not considered for the similarity calculation. This can result in the problem of having the same action with a subject that is not appropriate for use in the case of goal similarity. To illustrate this aspect, consider the action \"Walk to the desk\" in comparison to \"Walk to the moon\". In the case of KAS, the resulting similarity score is 0.67 because two out of three elements are equal, whereas in PG2S, the similarity score is 0. This discrepancy can be attributed to the fact that KAS does not consider the nuances of natural language, whereas PG2S does."}, {"title": "Experimental Results", "content": "This section presents the outcomes of the conducted experiments, which were designed to test the proposed architecture's validity. The results obtained using a single image are presented and then compared with a structured perception of the environment, as seen in state-of-the-art works. The output plans regarding home scenarios tasks are taken from the ALFRED dataset using the AI2Thor environment. Chosen the image and the environment, for each of those we have found the plan associated with the scene and saved the ground truth plans that we have used to compare our results. The environment scenarios are chosen by selecting several different situations in order to have various complexity and domains of application according to the chosen fields of ALFRED such as: picking up objects and placing them; picking up objects, heating or cooling them, and place them somewhere else; cleaning objects and examining under the light; and more."}, {"title": "Evaluation of our PG2S Metric", "content": "During the experimental phase of PG2S development, a series of tests were conducted to ensure the correctness of the metric. Specifically, we compared ALFRED plans with those predicted by our architecture, together with their corrupted version. During the test phase, several examples were selected from the ALFRED dataset. The plans obtained were checked qualitatively and it was possible to verify that the plans generated by the multi-agent architecture are correct"}, {"title": "Evaluation of our Architecture", "content": "To evaluate the presented methodology, we have chosen ten different rooms of an apartment, such as a living room, a kitchen, and a toilet. Frames were captured for each room as in the example in Fig. 3 which depicts a kitchen. The complexity of generating a plan is evident, given that an entire scene is represented by a single image and that some of the objects needed can be quite small. Our tests demonstrated that even in complex situations, the VLM is capable of identifying objects and perceiving their relationships, allowing it to define a correct plan. The ten environments chosen allow us to obtain thirty tasks to perform and, for each of these plans, we have obtained the plan using four approaches: two using a single-agent architecture and two using a multi-agent architecture. In both single-agent and multi-agent evaluations, the plan was obtained using a table describing the environment rather than a single image.\nTable 3 presents the results, highlighting instances where the KAS metric fails, resulting in None values. This failure occurs because the KAS metric cannot evaluate plans of different lengths, which was common in the \"with table\" setups.\nThe results show how using a single image the architecture generates a plan similar to the ground truth plan. Furthermore, we demonstrated to obtain improved results in multi-agent architecture using a single image."}, {"title": "Discussion", "content": "The current state of the art involves the use of traditional Success Rate metrics to evaluate a plan, where the plan is considered correct in cases where execution leads to the desired outcome. However, this metric is not sufficient or suitable for all cases where the correctness of a task execution plan is to be analyzed. In particular, in cases where the plan is complicated, it should be evaluated before execution to avoid damage to the environment or simply unsuccessful executions and ensure that time and resources are not wasted in a new execution. The advent of LLMs has made it possible to easily generate plans that previously required model training or other more complex techniques. Given that these models can' hallucinate' or generate incorrect responses, there could be errors present. Therefore, these inaccuracies could lead to failures when evaluating them based on success rates. Our work seeks to define a new PG2S metric for plan evaluation based only on natural language processing"}, {"title": "Conclusion", "content": "In this paper, we, first, introduced a multi-agent planning framework that leverages the capabilities of Visual Language Models (VLMs) to improve planning for embodied agents without the need for pre-encoded environmental data structures. Our approach simplifies the input requirements by utilizing a single environmental image and also enhances the adaptability and effectiveness of the planning process through a multi-agent system. This innovation addresses the limitations of traditional models that rely heavily on structured data, providing a more flexible and dynamic planning mechanism that is particularly effective in unstructured, real-world scenarios.\nThe empirical results, validated using the ALFRED dataset, demonstrate the efficacy of our approach, especially when compared to existing metrics like the KAS metric. We, then, introduce a new metric for the plan evaluation. The newly proposed PG2S metric, which assesses planning quality based on semantic understanding rather than strict action order, has shown superior performance in capturing the variations of plan execution.\nThe presented approach can address some of the current limitations in embodied agent planning and can open future research in the application of VLMs and multi-agent systems. Future studies might explore the scalability of our approach to more complex multi-agent environments and the integration of more diverse modalities to enhance the agents' understanding of their operational contexts. PG2S explores novel possibilities in the plan evaluation, focusing on semantic integrity rather than strict action sequencing. We believe that the research community can take advantage of the proposed approach, considering semantic coherence as a critical component of plan success, especially in applications requiring high reliability and safety."}]}