{"title": "Sensor2Text: Enabling Natural Language Interactions for Daily Activity Tracking Using Wearable Sensors", "authors": ["WENQIANG CHEN", "JIAXUAN CHENG", "LEYAO WANG", "WEI ZHAO", "WOJCIECH MATUSIK"], "abstract": "Visual Question-Answering, a technology that generates textual responses from an image and natural language question, has progressed significantly. Notably, it can aid in tracking and inquiring about daily activities, crucial in healthcare monitoring, especially for elderly patients or those with memory disabilities. However, video poses privacy concerns and has a limited field of view. This paper presents Sensor2Text, a model proficient in tracking daily activities and engaging in conversations using wearable sensors. The approach outlined here tackles several challenges, including low information density in wearable sensor data, insufficiency of single wearable sensors in human activities recognition, and model's limited capacity for Question-Answering and interactive conversations. To resolve these obstacles, transfer learning and student-teacher networks are utilized to leverage knowledge from visual-language models. Additionally, an encoder-decoder neural network model is devised to jointly process language and sensor data for conversational purposes. Furthermore, Large Language Models are also utilized to enable interactive capabilities. The model showcases the ability to identify human activities and engage in Q&A dialogues using various wearable sensor modalities. It performs comparably to or better than existing visual-language models in both captioning and conversational tasks. To our knowledge, this represents the first model capable of conversing", "sections": [{"title": "1 INTRODUCTION", "content": "Visual Question Answering is an exciting task in computer vision and natural language processing that has gained much progress in recent years; it involves generating a text response given an image and natural language question [4]. Notably, visual question-answering can be applied in tracking personal daily activities, which shows great promise for healthcare monitoring, especially for elderly people or patients with Alzheimer's. While traditional monitoring systems can only send raw footage generated by monitoring cameras to caregivers, current question-answering systems enable conversational interactions with the users. This capability allows caregivers to inquire about various aspects of the target's well-being, such as medication adherence, dietary habits, sleep patterns, and more.\nHowever, using video as the input modality to question-answering has several issues. Video suffers from obstructions, restricted viewpoints, and vulnerability to inadequate lighting, all of which are common occurrences in daily activities monitoring. More importantly, using constant video surveillance has privacy concerns and may make the subject feel uncomfortable. Therefore, it is crucial to devise an innovative approach to replace the reliance on cameras, which enables the collection of data on subjects' daily activities while also preserving their privacy.\nIt is observed that sensor-based data, particularly those obtained from wearable sensors, can be a promising alternative. Wearable sensors offer several advantages over video surveillance. Firstly, they allow the subject to roam freely outside of designated areas, as opposed to video which requires the subjects to remain within the camera coverage zones. Additionally, compared with video cameras, wearable sensors can provide more information regarding one's health and physical activity, making it a better option for healthcare monitoring. Finally, they are less invasive in privacy compared to video cameras.\nIn this paper, we propose Sensor2Text, a sensor language model capable of conditioning itself on wearable sensor data to perform the aforementioned question-answering, illustrated in Figure 1. Sensor2Text is interactive and versatile for subjects and caregivers to track daily activities. Additionally, it functions without requiring any visual input, making it less obtrusive than video monitoring and applicable in dark or occluded settings. Furthermore, compared with existing video and sensor captioning models [30, 53, 73], Sensor2Text stands out for its chat capabilities and proficiency in question-answering tasks, enhancing its utility in tracking daily activities.\nHowever, developing a sensor-language model like Sensor2Text presents several challenges. Firstly, wearable sensors lack the density of information found in video data regarding human activities. To overcome this, we employ cross-modality supervision using teacher-student networks. In this approach, a model trained on visual camera data acts as a teacher, efficiently transferring its knowledge to models analyzing sensor data after relatively few sensor examples.\nIn addition, a single wearable sensor is often insufficient when differentiating between similar activities. To fix this, we use multiple wearable devices to track human activities. We jointly process the data modalities using a multi-modal encoder-decoder neural network architecture and align the temporal dynamics between the modalities with time-dependent output tokens."}, {"title": "2 RELATED WORK", "content": "Our study is closely connected to research within the domains of Human Activity Recognition, Video Captioning, and Multi-modal Large Language Models. Below, we provide an in-depth exploration of these areas, elucidating how Sensor2Text leverages insights from prior research to pioneer a novel approach enabling conversations and question-answering about content in wearable sensors. This innovative approach mitigates privacy concerns and viewpoint limitations in existing vision-based solutions, making it a promising alternative in applications such as healthcare monitoring."}, {"title": "2.1 Human Activity Recognition", "content": "The domain of Human Activity Recognition (HAR)[19], which focuses on identifying human actions through sensor data analysis, has witnessed notable progress in recent years, particularly in leveraging machine learning techniques for interpreting human activities from sensor data. Researchers have utlized machine learning techniques such as kernel discriminant analysis and Long Short-Term Memory (LSTM)-based Neural Structured Learning [35, 65], Decision Trees, Support Vector Machines, and Neural Networks [61] to recognize activities from smartphone data. Another area of interest involves the use of radio signals interacting with human bodies to comprehend movements and behaviors. Specifically, previous studies have demonstrated the use of radio signals for tasks such as location tracking [1, 30, 48], extraction of 3D skeletons of nearby individuals [78], and action classification [47]. Finally, some studies also employ wearable sensors and deep learning techniques for HAR and other related tasks [7, 11\u201318, 20, 22-25, 28, 34, 39, 55, 62, 70? ], including unsupervised domain adaptation [9], contrastive predictive coding [38], and deep unsupervised clustering [54]."}, {"title": "2.2 Video Captioning", "content": "Video captioning refers to the generation of descriptive text for video content. One strategy for video captioning involves applying image captioning models to uniformly sampled frames [32, 49]. Recent advancements employ recurrent neural networks (RNNs) [68], attention mechanisms [66], hierarchical architectures [69, 75] to capture more temporal intricacies in the video. Furthermore, recent work studies the integration of audio and video comprehension through advancements in multi-modal learning [10, 73].\nAnother development pertinent to video captioning is Vision-Language Pre-training (VLP), which aims to enhance the performance of multi-modal foundation models across various vision-language tasks through unsupervised learning. For instance, CLIP [60] utilizes conservative loss between image/text pairs to pre-train a model on internet-scraped data. These models often adopt architectures like dual-encoders [56, 60, 72], fusion- encoders [63], or a hybrid of both as encoder-decoder architectures [46]. VLP models can be applied across a range of vision-based tasks, including classification and captioning, among others.\nWhile great advancements have been made in video captioning, our work, in contrast, is intended to employ wearable sensor data for conversational interactions with the user. However, the sensor-language model continues to leverage advancements in video captioning and VLP by employing visual language models in transfer learning to distill the learned knowledge to wearable sensor data. After training, the user will be able to conduct Q&A conversations using wearable sensor data as input, without requiring any visual input."}, {"title": "2.3 Multi-modal Large Language Models", "content": "Large Language Models (LLMs) have garnered significant attention in recent years for their ability to comprehend and generate human-like text. There is a growing interest in extending their capabilities to process modalities of inputs other than language. One approach is to use LLMs as controllers, with existing multi-modal models serving as tools [8, 40, 41, 57, 74]. In this setup, the LLM decides which tools to call based on the user's text instruction and incorporates results from these multi-modal models. Other work focuses on developing fundamental multi-modal LLMs by aligning the representation space of pre-trained unimodal models with that of LLMs, allowing the language model to natively understand other data modalities rather than through a proxy. Models like BLIP2 [46] and LLaVA [51] exemplify this approach.\nDespite these advancements, the majority of multi-modal LLMs primarily accept images as input [76]. Some studies have explored processing other data modalities, such as 3D point clouds [36] or audio [77], but few have attempted to extend the capabilities of LLMs to comprehend wearable sensor data. Our proposed model, Sensor2Text, aims to bridge this gap by enabling the understanding of wearable sensor data and its integrate them withLLMs.\nIn summary, our research extends the existing body of literature on Human Activity Recognition, Video Captioning, Vision-Language Pre-training (VLP), and multi-modal Large Language Models. What sets our model apart is its utilization of wearable sensor data, rather than conventional visual data, to perform Q&A and chat about an individual's daily activities."}, {"title": "3 PRELIMINARY STUDY", "content": "This section first explores the capability of wearable sensor data in recognizing and distinguishing daily activities. It then assesses the feasibility of training such a sensor model due to the scarcity of comprehensive multi-modal wearable sensor datasets."}, {"title": "3.1 Sensor Characteristics for Activity Tracking", "content": "We observe that wearable sensor data can effectively be used to identify human activities for the following reasons. Firstly, activities that may appear similar through video cameras often yield distinct sensor readings. Illustrated in Figure 2, while actions like peeling or cutting cucumbers may seem alike in video images, the corresponding human skeleton representations derived from sensor data exhibit notable differences.\nAdditionally, wearable sensors reveal dynamic variations. Each wearable sensor gathers time series data rather than static snapshots, capturing a subject's movement patterns over time that can further aid in distinguishing similar activities.\nFurthermore, integrating multiple modalities from wearable sensors enhances the differentiation of human activities. While certain activities may exhibit similar body tracking readings, they may yield entirely different readings from other wearable sensors. For instance, although body tracking data excels in distinguishing activities with various movements, it may struggle to discern certain nuances, such as differentiating between cutting a cucumber and a potato. However, by integrating muscle activation sensors that monitor electromyography signals, it becomes possible to differentiate between the objects the subject is holding [21, 31], effectively distinguishing the action of cutting a potato from cutting a cucumber. By processing multiple modalities of sensor data through a multi-modal deep learning architecture, it becomes possible to make use of each modality in determining the details of the activity."}, {"title": "3.2 Feasibility", "content": "A significant concern regarding the feasibility of constructing a sensor-language model is the scarcity of large-scale wearable sensor datasets. In contrast, visual-language models benefit from abundant datasets, facilitating extensive training or fine-tuning for robust performance in captioning tasks. Consequently, an insightful approach emerges: we can transfer the knowledge acquired from training visual-language models to sensor data through transfer learning and teacher-student networks.\nIndeed, prior research [36, 37] demonstrates the feasibility of leveraging large visual-language datasets for downstream fine-tuning by aligning the encoder of new modality data with an existing visual encoder. This significantly mitigates the need for an extensive amount of wearable sensor data. Accordingly, employing wearable sensor data to train a sensor language model is feasible."}, {"title": "4 METHOD", "content": "Sensor2Text is a novel model designed to generate textual responses based on sensor data and textual prompts. The primary challenges in developing such a model are the low information density of single modal sensor data and the limited sizes of existing sensor datasets. To address these challenges, a unique architecture is proposed, illustrated in Figure 3. The model consists of two main subcomponents: a sensor encoder and a language decoder. The sensor encoder takes multiple modalities of sensor data as input and outputs a learned representation vector that incorporates information from all sensors. The language decoder then takes this representation along with the language prompt and generates the textual response.\nTo efficiently train the model despite the lack of sensor data, a vision encoder is introduced as a teacher network to train the sensor encoder. Additionally, transfer learning techniques are employed to train the language decoder. To facilitate the training process, a dataset is carefully selected to include videos, labels, and multiple sensor data modalities.\nThe following subsections provide a detailed description of the architecture, including the sensor encoder and language decoder, as well as the procedure used to train both components."}, {"title": "4.1 Sensor Encoder", "content": "This section introduces the sensor encoder, which extracts useful features from time series data collected by the various wearable sensors and integrates them into a single representation.\nWe utilize distinct encoders to extract features from each sensor data modality separately before combining them with late fusion. A transformer encoder [66] is applied to each data modality due to its excellent performance in capturing intricate dependencies within the sequential data. Each sequence of wearable sensor data, denoted as $x^{(i)} \\in \\mathbb{R}^{T_i \\times d_i}$ for the ith sensor modality, where $d_i$ denotes the number of features and $T_i$ represents the number of timesteps, is processed via a 1D convolutional tokenizer. This converts the raw sensor data into tokens that are more easily ingested by the downstream transformer. The sequential data is segmented into fixed-length windows and a learned 1D convolution is applied to each window. The resulting token for each window is represented as $u_j^{(i)} = O_{tok}^{(i)} \\ast x^{(i)}_{s*j,...,s*j+d-1}$, where d indicates the window size, s denotes the stride between consecutive windows, $O_{tok}^{(i)} \\in \\mathbb{R}^{d \\times d_{encoder}}$ represents the learnable weights of the tokenizer, and $u_j^{(i)} \\in \\mathbb{R}^{d_{encoder}}$ represents the output tokens.\nThe tokenizer then applies a sine/cosine positional encoding onto each token to inject temporal information into each token, rendering different positions distinguishable to the downstream transformer. The positional encoding for the jth position and dth dimension is given as PE(j, d), where:\n$PE(j, 2k) = sin(j/10000^{2k/d_{encoder}})$\n$PE(j, 2k + 1) = cos(j/10000^{2k/d_{encoder}})$\nEach positional encoding is directly added onto the token sequence for each input data modality, resulting in a modified sequence of position-aware tokens denoted as $u'_j = u_j + PE(j)$. A learnable classification token, denoted as [CLS], is prepended to this sequence. After processing in the downstream sequence-to-sequence transformer, the output of this token will serve as the single representation for the entire sequence. Consequently, we obtain a sequence of tokens u = ([CLS], u'\u2081, ..., Un).\nNext, the position-aware tokens are processed in a transformer encoder to obtain an intermediate representation for each sensor. The transformer encoder consists of N layers of alternating self-attention and feedforward layers, effectively capturing both long and short-term dependencies in the sensor tokens. The output vector of the [CLS] token is extracted to obtain an intermediate representation for each sensor modality, $y^{(i)} = Transformer(u^{(i)})$, where $y^{(i)} \\in \\mathbb{R}^{d_{encoder}}$.\nFinally, the late fusion technique is employed to obtain the ultimate representation vector. This process involves concatenating the representation vectors of each sensor and then applying a final feedforward layer to capture potential interdependencies among all sensor modalities. The outcome is a single representation vector, Ysens \u2208 $\\mathbb{R}^{d_{output}}$, which encapsulates all relevant information from the wearable sensor data, where $d_{output}$ is the dimension of the final representation vector."}, {"title": "4.2 Auxiliary Training Objective", "content": "We leverage a visual encoder as a teacher network to train the sensor encoder. Through this method, the sensor encoder can effectively distill valuable information from the visual encoder while avoiding the instability, high computational cost, and susceptibility to overfitting that comes from training the encoder with an end-to-end text generation loss. For this purpose, the ImageBind model [33] is utilized as the designated teacher network. ImageBind has the capacity to encode diverse modalities such as visual, audio, IMU, thermal, depth, and text data into a unified embedding space. Its proficiency in learning representations that transcend modality-specific boundaries makes it an ideal candidate to act as a teacher network."}, {"title": "4.3 Language Decoder", "content": "After acquiring the feature representation for a segment of sensor data, it becomes essential to devise a method to process both the input textual prompt and the sensor data to generate a response. To accomplish this, the LLaMA Large Language Model is employed as the language decoder [64]. Furthermore, a bridging layer is incorporated to reconcile our sensor representation with the subsequent language decoder."}, {"title": "4.3.1 Primer on Querying Transformer", "content": "Prior to processing the encoded sensor representation with LLaMA, we utilize the Querying Transformer [46] to bridge the sensor encoder's outputs to LLaMA's embedding space. The Querying Transformer (Q-former) is a bottleneck architecture designed to distill textual information from the outputs of an upstream encoder. The Q-former is a transformer-based model that utilizes a set of learned queries as input and uses cross-attention to interact with the embeddings of the upstream encoder. Compared to traditional methods, such as employing a linear layer or feedforward network, the Q-former is able to generate any number of tokens for the downstream language decoder. Moreover, the use of attention and cross-attention allows the Q-former to learn and output complex interdependencies between the output tokens. As a result, the tokens produced are semantically coherent to the downstream LLM.\nWe initialize the Q-former with pre-trained weights to leverage the existing vision-language knowledge inherited from pre-training tasks [46] and further end-to-end text generation finetuning. Specifically, the pre- trained Q-former from VideoLLaMA's [77] omni-modal (AL) branch is utilized because it has undergone the aforementioned pre-training as well as further finetuning on vision-language tasks using ImageBind as the visual encoder, serving as the ideal base model to transfer knowledge to sensor data."}, {"title": "4.3.2 Language Decoder", "content": "The language decoder not only interprets data from the sensor encoder but also handles conversational tasks involving reasoning, leveraging world knowledge, and following instructions. Hence, we opt for a pre-trained Large Language Model (LLaMA-7B) [64] as our optimal choice. LLaMA-7B, the most lightweight model in the LLaMA family, is well-suited for our computationally intensive training process.\nSimilar to other transformer-based large language models [3, 58], LLaMA employs a text tokenizer to generate embeddings from an input text sequence, represented as Tokenizer(x) \u2208 $\u211d^{T\u00d7d_{LLaMA}}$, before processing it in the transformer layers. Here, T is the number of tokens in the text sequence and $d_{LLaMA}$ indicates the size of each token embedding. The output of the language model can then be written as y = LLaMA(Tokenizer(x)).\nTo incorporate the sensor data, we apply the Querying Transformer to the output of the sensor encoder to extract sensor embeddings and prepend them to the text embeddings. The resulting output is y = LLaMA(QFormer(usens)\u2295 Tokenizer(x)). Here, usens is the output from the sensor encoder, and QFormer(usens) \u2208 $\u211d^{K\u00d7d_{LLaMA}}$ is the output"}, {"title": "4.3.3 Temporal-Aware Tokens", "content": "The sensor representation encapsulates information regarding events within a sequence of sensor data but lacks temporal clarity for the LLM. However, comprehending daily activities necessitates differentiating the chronological sequence of events, which may be further inquired by the user in detail. To tackle this issue, n segments of equal length are sampled from a specified time interval of wearable sensor data. Each segment is then encoded to yield usens,t for t \u2208 [1, ..., n]. These encoded segments are concatenated with the output tokens from all n intervals when inputted into LLaMA. The final output is represented as y = LLaMA(QFormer(usens,1) \u2295 ... \u2295 QFormer(usens,n) \u2295 Embedding(x)). Given that each of the n sequences of tokens carries meaningful information, different values for n can be employed throughout training and inference. This adaptability enables Sensor2Text to handle variable lengths of wearable sensor data sequences.\nFinally, the outputs of all of the QFormer(usens,i) are encapsulated in natural language, enabling us to leverage more of LLaMA's inherent capabilities. An illustrative prompt is shown in Figure 4."}, {"title": "4.3.4 Cross-Modal Training", "content": "The language decoder is trained by employing a text generation loss over sen- sor/caption pairs. This loss is computed utilizing the logits generated by LLaMA for each token in its vocabulary and calculating a cross-entropy loss against the ground truth token. The total loss across a sequence of input tokens {xi}i=1,...,n and output tokens {yi}i=1,...,m is the cumulative sum of cross-entropy losses for each output token, conditioned on the preceding sequence of tokens. These tokens comprise a prompt, as illustrated in Figure"}, {"title": "4.3.5 Noise Injection for Robustness", "content": "Visual LLMs trained in a similar fashion [46, 51, 77] utilize much larger datasets than existing wearable sensor datasets. Consequently, the language decoder may be less robust, susceptible to overfitting, and exhibit significantly different behaviors with minor variations in the input distribution. To mitigate this issue, noise injection is incorporated into the outputs of the Q-former before feeding them into LLaMA. Noise injection involves adding a random Gaussian vector with a small variance to the inputs, enhancing the model's resilience to minor input alterations. Specifically, we implement\n$QFormer' (u_{sens})_i = QFormer(u_{sens})_i + X_i, X_i \\sim N(0, \u03c3^2I_n)$\nwhere QFormer(usens)i denotes the i-th token generated by the Q-former, \u03c3\u00b2 represents a small variance, and In is an identity matrix with n rows and n columns. Since adjacent points in LLaMA's embedding space share similar semantic meanings, this approach ensures robustness without substantially altering LLaMA's perception of its inputs in the training stage. Following the incorporation of noise injection, our model demonstrates improved performance on unseen data, as detailed in subsequent sections."}, {"title": "4.3.6 Instruct Finetuning", "content": "After completing the initial stage of cross-modal training, the model's performance in conversational contexts, such as responding to user inquiries, diminishes, as it has been trained solely to output activity labels. To address this issue, a second stage of fine-tuning is introduced to restore instruction-following capabilities. In this phase, the model is finetuned using question-answer pairs and example conversations. However, due to the lack of such a dataset with wearable sensor data, a visual instruction dataset is utilized instead. Since the sensor encoder has been trained with the ImageBind visual encoder as a teacher, the language decoder can be finetuned using ImageBind as the encoder on a vision instruct dataset. The Q-former is fine-tuned using text generation loss on image/conversation pairs, using ImageBind to extract visual embeddings. Through this process, the model retains its understanding of sensor data while also regaining instruction-following capabilities. After training is completed, the model can engage in conversations about sensor data without prior exposure to sensor-based discussions."}, {"title": "4.4 Training Procedure and Implementation Details", "content": "We select the datasets ActionSense [29] and MMAct [43] to evaluate our model. Each consists of subjects performing various daily activities while collecting wearable sensor data, corresponding video footage, and"}, {"title": "5 EVALUATION", "content": "In this section, a comprehensive evaluation of Sensor2Text's capabilities is provided. We showcase sample conversations with Sensor2Text, demonstrating its proficiency in daily activity captioning, question answering, and reasoning. We also systematically compare Sensor2Text against similar models using quantitative metrics. Additionally, we explore Sensor2Text's proficiency in leveraging multiple modalities of sensor data, its robust ability to generalize to unseen users, and ablation studies to assess the contribution of various components of the model. Finally, we assess Sensor2Text's performance when applied on low information-density IMU sensors."}, {"title": "5.1 Dataset", "content": "We select the ActionSense [29] dataset, a multi-modal wearable sensor dataset for human activities in a kitchen environment, for use in the primary qualitative and quantitative evaluations of the model. The ActionSense dataset consists of 10 subjects, each performing kitchen activities in episodes spanning 60-120 minutes, such as preparing food, cleaning tableware, and interacting with items. In total, there are 20 diverse kitchen activities distributed between these categories. The full set of activities is depicted in Figure 5.\nVarious wearable sensor data modalities are collected during each episode and are paired with corresponding visual footage and text labels. From the available data modalities, we select the body-tracking, muscle activation, and eye-tracking due to them being consistently present for all episodes. The data is gathered using the Xsens MTw Awinda body tracking system [71], the Pupil Core eye-tracking headset from Pupil Labs [45], EMG sensors from Thalmic Lab's Myo Gesture Control Armband. Each wearable sensor records readings stored as sequential data of different frequencies. Additionally, we use visual footage collected from on-head cameras."}, {"title": "5.1.1 Data Preprocessing", "content": "To ensure the strong performance and reliability of our model, we preprocess and normalize each modality of wearable sensor data. We sample each at a frequency of 50Hz. For eye-tracking gaze data, we remove outliers by clipping values outside the range of [0.05, 0.95], handle missing or faulty values through interpolation, and normalize the data to the range [\u22121, 1]. For Electromyography (EMG) data, we apply a low-pass filter with a cutoff frequency of 5Hz to smooth the signal and remove high-frequency noise, followed by normalizing each data value to the range [\u22121, 1]. Lastly, for the body tracking data, we preprocess the joint rotation values by normalizing them to the range [-1, 1], assuming that the joint rotation data falls within the range of [-180, 180] degrees. These preprocessing steps handle potential faulty sensor readings, thus enhancing the input data's quality and improving our deep learning architecture's performance.\nWe partition the dataset into train, validation, and test subsets following a 70/15/15 split ratio, respectively. The episodes in ActionSense are segmented into 2-second clips to train the sensor encoder, which significantly increases the number of training examples available. For the language decoder, labeled sections of each episode are chosen, segmented into 16-second clips, and then trained using text generation loss. In total, there are 16000 2-second segments for the cross-modality supervised training of the sensor encoder and 1100 16-second segments that are adequately labeled. After incorporating a curated list of training prompts for the latter, there are approximately 7700 training examples for end-to-end text generation training."}, {"title": "5.2 Qualitative Evaluation", "content": "This section presents examples of Sensor2Text in diverse conversational contexts. For each instance, wearable sensor data from an unseen subject in the ActionSense dataset is utilized, and multiple questions are posed to the chatbot to assess its responses. For qualitative analysis, ground truth of the tested human activities is illustrated using footage from video cameras. These recordings are subsequently inputted into vision-language models to facilitate comparison with other models of the same category. It's worth noting that these visual data were not provided to Sensor2Text; the sensor-language model only receives the aforementioned wearable sensor data"}, {"title": "5.2.1 Sensor Perception Ability", "content": "Figure 6 illustrates Sensor2Text's proficiency in identifying various activities from sensor data. Sensor2Text correctly identifies that the subject is slicing a cucumber and notes the subject's precise movements during the slicing process, showing its powerful sensor perception abilities.\nFurthermore, in Figure 7, Sensor2Text accurately identifies that the subject is retrieving items from a cabinet and correctly identifies that the cabinet is above the subject. In contrast, all three video models are unable to determine any details of the activities due to the inadequate lighting. In Figure 8, during intervals with bad camera angles, the sensor model is again able to determine the subject's activity while vision models fail. This shows the superior performance of the sensor-based model in diverse settings."}, {"title": "5.2.2 Conversational Capability", "content": "Sensor2Text excels at interactive conversations with users, seamlessly blending the conversational abilities of LLaMA with insights derived from provided sensor data. Illustrated in Figure 6, when prompted, Sensor2Text correctly identifies that the subject is slicing a cucumber. When asked if the subject appears good at cooking, the model responds affirmatively and cites the subject's precise movements during the slicing process. This demonstrates Sensor2Text's ability to integrate the information from the inputted sensor data together with the language model's reasoning capabilities for smooth conversations. This integration enables dynamic and contextually aware interactions."}, {"title": "5.2.3 External Knowledge Integration", "content": "Sensor2Text is capable of drawing upon world knowledge while integrating insights from provided sensor data. In Figure 8, Sensor2Text correctly assesses that it will take the subject about 30 seconds to peel the potato, showcasing its ability to utilize the information provided by the sensors and its inherent world knowledge. This enables dynamic and context-aware interactions."}, {"title": "5.3 Quantitative Evaluation", "content": "Providing a quantitative evaluation for Sensor2Text is challenging due to the unstruc- tured nature of conversation-based tasks. While visual language models have established benchmarks like VQA [4], LLaVA-Bench [51], and ScienceQA [52], which assess conversational models' performance via meticulously crafted tasks, prompts, and metrics, comparable benchmarks are lacking for sensor language tasks. For example, Visual Question-Answering (VQA) assesses a model's correctness by comparing its output against a large set of human annotations [4]. In the absence of such benchmarks for sensor language tasks, we employ captioning scores as a proxy for evaluating performance, leveraging the availability of ground truth captions in our training data.\nTo assess Sensor2Text's performance, the model is prompted with a generic prompt requesting a caption. Such a prompt is illustrated in Figure 4. Then, traditional image captioning benchmarks are used to score the model's response. Following Microsoft COCO [26], we utilize the BLEU [59], ROUGE-L [50], METEOR [6], CIDEr [67], and SPICE [2] evaluation metrics. BLEU considers the co-occurrences of words between the predicted"}, {"title": "5.3.2 Captioning", "content": "The LLMs are prompted with a randomly selected prompt from a predefined list to elicit a caption output. This approach encourages the language model to produce outputs closely aligned with the terse, one-sentence descriptions of scenes found in the ActionSense dataset annotations."}, {"title": "5.3.3 Generalization and Robustness", "content": "This section showcases Sensor2Text's generalization capacity to unseen users. Due to the slight variations in data collected from each user, additional fine-tuning is often necessary for each new user. This process can impede the practical deployment of Sensor2Text, as collecting such data is time-consuming and arduous, presenting a challenge for new users.\nA comparison is conducted between the model's performance when evaluated on data from users it has encountered before versus data from unseen users. Specifically, rather than splitting the train/validation/test sets by uniformly sampling across the entire dataset, partitioning by user is conducted instead. A subset of users is"}, {"title": "5.4 Multi-Modality", "content": "One crucial aspect of Sensor2Text lies in its usage of wearable sensor data across various modalities. This section examines the impact of multiple modalities on the model's training, contrasting it with the use of a single modality, accessed by conducting experiments where only a subset of data modalities is utilized. Specifically, several new models are trained using solely eye tracking, muscle activation, or body tracking data, following the same training procedure. The results are presented in the Table 3."}, {"title": "5.5 Ablation Study", "content": "To show the necessity of the various components, experiments are conducted on the model's architecture and training process. Specifically, we demonstrate the need for the 2-step finetuning process, noise injection, and usage time-aware embeddings in the language decoder through ablation studies. The following alternatives are considered:\n\u2022 Omitting language decoder fine-tuning\n\u2022 Using a single embedding instead of 8 time-ordered embeddings\n\u2022 Omitting noise injection"}, {"title": "5.6 Low Information Density", "content": "Although Sensor2Text shows strong performance on the ActionSense dataset, the high information-density sensors present in the dataset may be difficult to collect in practice for daily activity tracking. For instance, collecting full body-tracking data involves wearing bulky sensors attached to a full-body suit. Thus, further investigation is needed to assess the applicability of Sensor2Text to sensors with low information density, such as smartphone and smartwatch IMUs. In this section, we select another dataset, the MMAct dataset [43], and provide a qualitative and quantitative evaluation to assess whether Sensor2Text can perform accurate Q&A and have meaningful conversations when applied to the MMAct dataset."}, {"title": "5.6.1 MMAct Dataset", "content": "To this end, we select the MMAct dataset [44]. In this dataset, 20 subjects perform activities from a set of 37 activities such as sitting, jumping, and entering/exiting a room. Wearable sensor data is collected from a smartphone and smartwatch, and the corresponding video is captured via environment-mounted video cameras. The smartphone collects acceleration, gyroscope, and orientation data, while the smartwatch further collects a set of smartwatch acceleration data for 4 total sensor data modalities. In contrast to the wearable sensor data collected from the ActionSense dataset, the sensors used in MMAct are more feasible to collect during everyday life but have lower information density.\nTo preprocess the data, we sample each modality of sensor data at a frequency of 50Hz. Each acceleration, gyroscope, and orientation signal is given in the three orthogonal directions (x, y, z). We begin by computing the average magnitude across the dataset for each sensor modality and normalizing each to have an average magnitude of 1. The collected data may be sensitive to sensor placement; thus, following [44], we further compute $R_i = arcsin(\\frac{Z_i}{\\sqrt{x_i^2+y_i^2+z_i^2}})$ for each data point and concatenate this to the original data. In total, each sensor data modality consists of 4 data values, significantly lower than the 66 data values from ActionSense's body tracking data or 16 from ActionSense's EMG data, signifying the lower information density. We use 60% of the data for training, 20% for validation, and 20% for testing. The same training settings are applied as before, with for 200 epochs for the sensor encode, 20 epochs for the language decoder trained, and 5000 iterations for instruction finetuning."}, {}]}