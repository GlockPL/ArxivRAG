{"title": "Lodge++: High-quality and Long Dance Generation with Vivid Choreography Patterns", "authors": ["Ronghui Li", "Hongwen Zhang", "Yachao Zhang", "Yuxiang Zhang", "Youliang Zhang", "Jie Guo", "Yan Zhang", "Xiu Li", "Yebin Liu"], "abstract": "We propose Lodge++, a choreography framework to generate high-quality, ultra-long, and vivid dances given the music and desired genre. To handle the challenges in computational efficiency, the learning of complex and vivid global choreography patterns, and the physical quality of local dance movements, Lodge++ adopts a two-stage strategy to produce dances from coarse to fine. In the first stage, a global choreography network is designed to generate coarse-grained dance primitives that capture complex global choreography patterns. In the second stage, guided by these dance primitives, a primitive-based dance diffusion model is proposed to further generate high-quality, long-sequence dances in parallel, faithfully adhering to the complex choreography patterns. Additionally, to improve the physical plausibility, Lodge++ employs a penetration guidance module to resolve character self-penetration, a foot refinement module to optimize foot-ground contact, and a multi-genre discriminator to maintain genre consistency throughout the dance. Lodge++ is validated by extensive experiments, which show that our method can rapidly generate ultra-long dances suitable for various dance genres, ensuring well-organized global choreography patterns and high-quality local motion. The project page is https://li-ronghui.github.io/lodgepp.", "sections": [{"title": "1 INTRODUCTION", "content": "MUSIC and dance have been fundamental aspects of human culture for centuries. In the modern age of digital content, there is a high demand for 3D dance content in industries such as film, animation, virtual reality, and social media. However, traditional methods of obtaining 3D dance sequences require professional dancers and motion capture equipment, leading to high costs and low efficiency that cannot meet the industry's growing demands. Consequently, automatically generating dance sequences based on music has become an important research topic, which not only helps the film and television industry quickly produce 3D dance assets but also assists dancers in their choreography creation.\nIn recent years, the music-driven dance generation has made significant progress with the development of generative models [1], [2], [3], [4], [5], [6]. However, most efforts [7], [8], [9], [10], [11] have focused on producing high-quality dance sequences that last only a few seconds, leaving efficient methods for generating longer sequences largely unexplored. This limitation hampers the ability to quickly create high-quality, minute-long dance sequences, reducing their practical applicability.\nIn this paper, we identify the following challenges towards the goal of high-quality and long-sequence dance generation with appropriate choreography patterns: 1) Computational Efficiency: Generating long sequences significantly increases computational demands. Reducing the algorithm's computational load and improving training and inference efficiency are primary concerns. 2) Global Choreography Patterns: Long sequence dance contains complex global choreographic patterns. Dancers enhance the expressiveness and memorable moments of the dance by performing repeated or mirrored key movements. They also express heightened or shifting emotions through progressive or turning actions. Meaningless, chaotic movements or excessive repetition should be avoided. The challenge lies in the fact that different dance genres have unique choreographic patterns, which are complex and difficult to summarize with simple rules. 3) Local Dance Quality: Achieving graceful and expressive dance movements while maintaining physical realism is also a significant challenge. Meanwhile, the rhythm of the dance should align with the musical beats as closely as possible.\nPrevious works have made various efforts to address these challenges. To reduce the computational cost of the training process, many have adopted autoregressive architectures [7], [9], [10], training music-to-dance generation networks within small time windows, typically 2-5 seconds. During the generation phase, long dance sequences are produced autoregressively through a sliding window. However, these approaches fail to learn global choreography patterns because they focus on short-term dependencies. The autoregressive nature of these methods accumulates prediction errors, resulting in motion freezing or mean-"}, {"title": "2 RELATED WORK", "content": "2.1 Human Motion Synthesis\nHuman motion synthesis has been a prominent area of research in computer vision and computer graphics.\nMotion graph [21], [22], [23], [24], [25] have been extensively employed for human motion synthesis for decades. These methods construct a graph where nodes represent motion clips from a dataset, and edges represent possible transitions between motions. New motions are synthesized by traversing the graph according to specific rules or constraints. While motion graphs can produce high-quality and realistic motions by utilizing real motion capture data, they require maintaining complex graph structures and substantial preprocessing. Moreover, the diversity and flexibility of the synthesized motions are limited to combinations of existing motion clips, making it challenging to generate novel or highly varied motions.\nSimilarly, motion matching techniques [26], [27], [28], [29], [30], [31], [32] aim to match user inputs or constraints to the most suitable motion frames from a database. These methods are efficient and responsive, making them suitable for interactive applications like video games. However, their performance heavily depends on the richness of the motion database. They struggle to generate motions that are not present in the dataset and lack the capability to produce long, complex dance sequences automatically.\nIn recent years, with the development of deep neural network [33], [34] and generative models [4], [5], [6], [35], many works have utilized neural networks to generate human motion [36], [37], [38], [39], [40], [41], [42], [43]. HumanML3D [44] first created a high-quality text-motion paired dataset based on AMASS [45] and designed an effective text-driven motion generation network based on VAE [46] and Transformer [34]. Following this approach, works such as TM2T [47], T2M-GPT [48], HumanTomato [49] and MotionGPT [50] encode motions into a series of discrete motion tokens using VQ-VAE [51], and then employ a GPT [52], [53], [54] based sequence model to learn the mapping between text tokens and motion tokens, achieving text-driven motion generation. These framework encodes motions into latent space and then uses a sequence model to learn the dependencies between conditions and motions. The compressed latent space effectively reduces the complexity of sequence modeling, making it easier to generate logically coherent long-sequence motions. However, encoding motions into latent space reduces the diver-"}, {"title": "2.2 Music Driven Dance Generation", "content": "Research on generating dance sequences that are tightly synchronized with music input spans a variety of methods, including motion-graph approaches [78], sequence models [7], [9], [14], VQ-VAE models [14], [79], GAN-based methods [9], and diffusion-based techniques [10], [80].\nThe motion-graph based dance generation methods [18], [81], [82] typically utilize a cross-modal retrieval network to calculate matching scores between music and dance segments. Based on these matching scores and predefined choreography rules, a motion graph is constructed to synthesize long dance sequences. However, the predefined choreography rules are not adaptable to various dance genres, restricting these approaches to single-genre dances. Additionally, they are unable to achieve fine-grained beat alignment and lack diversity and creativity in the generated dances.\nDeep learning approaches have become more widely adopted for generating high-quality, visually appealing dance movements. The most straightforward approach [8], [83] to generating dance using neural networks is to employ a sequence model such as LSTM [84] or Transformer [34]. For example, FACT [7] employs a Transformer network to generate new dance frames conditioned on both music and initial motion frames. However, issues like error accumulation and motion freezing [85] still arise.\nSome methods use VQ-VAE [51] to convert dance motions into a sequence of dance tokens, then employ a sequence model [34] to learn dependencies between music and dance tokens. These methods [86], [87] can capture complex and vivid global choreography patterns. However, the two-stage design hinders the optimization of fine-grained dance quality, such as precise beat alignment and motion realism. Bailando [14] addresses the beat alignment issue by introducing reinforcement learning and a beat-align reward function, enabling the generated dances to balance global choreography patterns with fine-grained beat alignment. Nevertheless, there is still considerable room for improvement in the fine-grained dance quality of Bailando [14].\nGenerative Adversarial Networks (GANs) [1], [2], [3], which involve a generator and discriminator in an adversarial structure, have been used to create realistic dance animations. For instance, MNET [9] employs a transformer-based generator along with a multi-genre discriminator to control dance genres. However, GAN-based techniques often struggle with challenges like mode collapse and training instability.\nMore recently, diffusion models like FineDance [80] and EDGE [10] have been employed to produce diverse, high-quality dance clips. Although FineDance incorporates Motion Graph to generate long-sequence dances, and EDGE maintains temporal consistency during the generation of long sequences, neither establishes a robust network for learning choreography patterns. Consequently, both methods exhibit some discontinuities at the transitions between dance segments.\nIn summary, while significant progress has been made in dance generation, producing high-quality, long dance sequences that maintain robust global choreography structures remains challenging."}, {"title": "3 METHOD", "content": "3.1 Preliminaries\nMusic and Dance Representation. For music representation, we follow the approach in [7] and use Librosa [88] to extract the whole music feature $m_w \\in \\mathbb{R}^{L\\times35}$, where $L$ represents the number of frames and 35 denotes the music feature channels, including 1-dim envelope, 20-dim MFCC, 12-dim chroma, 1-dim one-hot peaks, and 1-dim one-hot beats. For dance representation, we follow EDGE [10] and define the whole dance sequence as $d_w \\in \\mathbb{R}^{L\\times139}$, where the motion format adheres to SMPL [89] and includes: (1) a 4-dimensional foot-ground contact binary label for left toe, left heel, right toe, and right heel, where 1 indicates contact with the ground and 0 indicates no contact; (2) 3-dim root translation; (3) 132-dim rotation data in a 6-dim rotation representation [90], with the first 6 dimensions as global rotation and the remaining 126 for relative rotations of 21 sub-joints along the kinematic chain.\nThe Diffusion Model. The diffusion model consists of a diffusing process and a denoising process. The diffusion process perturbs the ground truth dance data $d_0$ into $d_t$ over $t$ steps, we follow [5] to simplify this multi-step diffusion process into one step, which can be formulated as:\n$q (d_t | d_0) = \\mathcal{N}(\\sqrt{\\bar{a}_t}d_0, (1 -\\bar{a}_t)I),$ (1)\nwhere $\\bar{a}_t$ is within the range of (0, 1) and follows a monotonically decreasing schedule. $\\bar{a}_t$ converges to 0 as $t$ goes to infinity, making $d_t$ converging to a sample from the standard normal distribution. The denoising process employs a Transformer base-network $f_\\theta$ to gradually recover the motion, generating $d_0$ conditioned on given music $m$. Instead of predicting the noise [42], we directly predict the $d_0$ like [10]. Therefore, the training process can be formulated as:\n$\\mathcal{L}_{recon} = \\mathbb{E}_{d_0,t} [||d_0 - f_\\theta (d_t, t, m)||^2].$ (2)\n3.2 Overview of Lodge++\nGiven a long music feature $m_w \\in \\mathbb{R}^{L\\times35}, L = ln$, and dance genre $g$, Our goal is to learn a neural network Lodge++, $d_w = \\text{Lodge++}(m_w,g),d_w \\in \\mathbb{R}^{L\\times139}$. We first use the Global Choreography Network to autoregressively generate a coarse dance sequence with $L$ frames quickly, and then extract dance primitives. Next, we split $m_w$ into segments of length $n$ without overlaps, i.e. ${m^i \\in \\mathbb{R}^{n\\times35}}_{i=1}^\\frac{L}{n}$. PDDM can parallelly generate ${d^i \\in \\mathbb{R}^{n\\times139}}_{i=1}^\\frac{L}{n}$ based on the given music clips and corresponding dance primitives, and finally get $d_w = \\text{concatenate}([d^i], dim = 0)$.\nWe find that employing a two-stage network that separately focuses on global choreography patterns and local dance quality is highly effective [20]. However, the earlier version of this work, Lodge, modeled global choreography patterns by learning Characteristic Dance Primitives and applying basic choreographic rules. Due to the temporal sparsity of these Characteristic Dance Primitives and the oversimplified choreography rules, Lodge was unable to fully capture the richness and complexity of global choreography patterns.\nTherefore, in Lodge++, we learn global choreography patterns in a compact implicit space. Additionally, we need"}, {"title": "3.3 Global Choreography Network", "content": "Long-sequence dances involve complex choreography patterns, and the choreography rules vary across different dance genres. Previous work [7], [8], [11], [80], [91], [92], [93] overlooked the learning of global choreography patterns, resulting in long sequences of dance that are chaotic and disordered.\nTo alleviate the substantial computational load brought by long sequence data, we first encode and quantize the coarse-level dance of main joint rotation movements into a Choreographic Memory Codebook using VQ-VAE. Each segment of the dance can be encoded into a series of tokens. The coarse-level dance is robust enough to express rich choreography patterns. To learn the complex global choreography patterns between music and dance, we use a sequence model, specifically a Transformer, to learn the dependencies between the music and dance tokens.\n3.3.1 Dance VQ-VAE for Choreography Memory Encoding\nDance is composed of many basic movement elements. By arranging and combining these basic elements with appropriate modifications, a rich variety of long-term dances can be choreographed. Inspired by this idea, Bailando and Bailando++ use VQ-VAE to encode dances into a Choreographic Memory Codebook, which can encode a short segment of dance into discrete Dance Tokens. To further enhance the capacity of the Choreographic Memory Codebook, they decouple upper-body and lower-body movements. Despite showing significant progress, existing VQ-VAE networks still struggle to precisely compress and reconstruct full-body dance movements due to the inherent diversity and complexity of dance movements. This is especially true for challenging movements like fast turns and backflips, which often result in unrealistic reconstructed motions.\nWe lower the demands on VQ-VAE by encoding only the rotation angles of the main body joints: specifically, the rotations of the root, left shoulder, right shoulder, left arm, right arm, left leg, and right leg. Therefore, we extract the main body joints from each dance sequence $d\\in \\mathbb{R}^{N\\times139}$ in the dataset to obtain $d' \\in \\mathbb{R}^{N\\times42}$, where $N$ is the time length, 139 is the feature dimension of origin dance, 42 is the 6-dim rotation information of the above 7 main joints.\nThen, we use these coarse-grained dance $d'$ to train a VQ-VAE network. The VQ-VAE network maintains a learnable choreography memory codebook. The VQ-Encoder $\\mathcal{E}$ can map any sequence of coarse dance to a series of discrete"}, {"title": "3.3.2 Motion GPT for Global Choreography Learning", "content": "Thanks to the Dance VQ-VAE model, coarse-grained dance sequences $[d_1, d_2, ..., d_N']$ can be transformed into $[z_1, z_2, ..., z_{N'}]$. Due to the VQ-VAE being pre-trained, we can further rewrite $[z_1, z_2, ..., z_{N'}, \\text{End}]$ into a series of tokens $S = [s_1, s_2, ..., s_{T'}, \\text{End}]$, $s_i$ is the index of $z_i$ in the choreography memory codebook, $\\text{End}$ is a special token means that the sequence is ended.\nConsequently, global choreography can be formed as a sequence prediction problem. Given the music feature $m$ and dance genre $g$, the GPT autoregressively predicts the next dance token, formulated as:\n$P_\\theta (s_t|m, g) = p_\\theta (s_t|m, g, s_{<t}),$ (5)\n$p_\\theta (S|m, g) = \\prod_{i} p_\\theta (s_i|m, g, s_{<i})$ (6)\nThe optimization goal of dance GPT Network is:\n$\\mathcal{L}_{GPT} = \\mathbb{E}_{S\\sim P(S)}[-\\text{log } p_\\theta (S|m, g)]$ (6)\nBased on GPT and the choreography memory codebook, our proposed Global Choreography Network learns complex and vivid global choreography patterns, making it adaptable to a wide range of dance genres. However, the global choreography network faces challenges with beat alignment due to the token compression in the latent space of VQ-VAE. To address this issue, we first use the VQ-Decoder to reconstruct coarse-grained dance movements from the predicted sequence of dance tokens and then detect the motion beats within coarse dance. Next, we extract the 8 frames surrounding each dance beat and align them with the nearest music beats, resulting in $d_s$. Additionally, we extract the motion frames $[i_n \u2013 4, i_n + 4), i = 1,\u2026\u2026 ,l \u2212 1$ as $d_h$. Both the $d_s$ and $d_h$ consist of the dance primitives, where $d_s$ can convey the complex choreography patterns learned by the Global Choreography Network, guiding the local PDDM to generate more expressive dance movements, while $d_h$ can be used to support parallel generation."}, {"title": "3.4 Primitives-based Dance Diffusion Model", "content": "To facilitate the transfer of dance primitives from the Global Choreography Network to the local dance generation network and to minimize the gap between training and inference, we propose the Primitives-based Dance Diffusion Model (PDDM). The PDDM can generate high-quality local dance clips ${d^i}_{i=1}^\\frac{L}{n}$ based on dance primitives ${d'^{i}_{i=1}}^\\frac{L}{n-1}$, the given music ${m^i}_{i=1}^\\frac{L}{n}$, and the desired dance genre $g$. $d^i = \\text{PDDM}(m^i, d'^{i}, g)$. Based on the $d_h$ in dance primitives, the PDDM supports parallel generation to enhance the efficiency. For the sake of simplicity, we omit the superscript $i$ in section 3.4.1 and section 3.4.2.\n3.4.1 The forward and reverse process of PDDM\nThe forward process is to progressively shift the residual $d_{res} = d_p - d_0$ and inject noise. The forward process of the PDDM can be formulated as:\n$q(d_t | d_{t-1}, d_0, d_p) = \\mathcal{N}(d_t; d_{t-1} + a_td_{res}, k^2a_tI),$ (7)\n$q(d_{1:T} | d_0, d_p) = \\prod_{t=1}^{T} q(d_t | d_{t-1}, d_{res}),$ \n$q(d_t | d_0, d_p) = \\mathcal{N}(d_t; d_0 + \\eta_td_{res}, k^2\\eta_tI),$ (8)\n$q(d_T | d_0, d_p) = \\mathcal{N}(d_T; d_p, k^2I)$ (9)\nwhere $a_t$ is a hyper-parameter controlling the shifting trajectory. The smaller $a_t$ is, the smoother the shifting becomes. $k$ is a hyper-parameter controlling the noise variance. $a_t = \\frac{\\eta_t - \\eta_{t-1}}{1 - \\eta_{t-1}}$ for $t > 1, \\eta_1 = a_1 \\rightarrow 0, \\eta_T = 1 - \\frac{1 - a_T}{1 - a_{T-1}} \\rightarrow 1$.\nTherefore, $q(d_T | d_0, d_p)$ converges to $\\mathcal{N}(d_T; d_p, k^2I)$, which is approximate distribution for the dance primitives $d_p$.\nThe reverse denoising process is to recover $d_0$ from $d_p$ and $c$, where $c$ is the condition derived by concatenating music $m$ and genre $g$. In Lodge++, we utilize a denoising network $p_\\theta (d_{t-1} | d_t, d_p, c)$ to estimate the posterior distribution $p(d_0 | d_T, d_p, c)$:\n$p(d_0 | d_T, d_p, c) = \\int d_T p_\\theta (d_{t-1} | d_t, d_p, c) dd_{1:T},$ (10)\nwhere $d_T \\approx \\mathcal{N}(d_T; d_p, k^2I)$. Following most of previous diffusion relative papers [94], [95], [96], we adopt the following assumption:\n$p_\\theta (d_{t-1} | d_t, d_p, c) = \\mathcal{N} (d_{t-1}; \\mu_\\theta (d_t, d_p, c, t), \\Sigma_\\theta (d_t, d_p, c, t)) .$ (11)\n3.4.2 The Training Objective of PDDM\nTo optimize the neural network, we minimize the negative evidence lower bound [94], [95], specifically:\n$\\min \\mathbb{D}_{KL} ([q(d_{t-1} | d_t, d_0, d_p, c)||p_\\theta (d_{t-1} | d_t, d_p, c)]),$ (12)\nwhere $\\mathbb{D}_{KL} [||.]$ represents the Kullback-Leibler (KL) divergence [97]. The targeted distribution $q(d_{t-1} | d_t, d_0, d_p)$ in Eq.(11) can be made tractable and represented as:\n$q(d_{t-1} | d_t, d_0, d_p, c) = \\mathcal{N}(d_{t-1}| \\frac{\\eta_{t-1}}{\\eta_t} d_t + \\frac{a_t}{\\eta_t} d_0, \\frac{k^2 \\eta_{t-1} a_t}{\\eta_t}I),$ (13)"}, {"title": "3.4.3 The Auxiliary Losses", "content": "In addition to reconstruction loss $\\mathcal{L}_{recon}$, we follow previous works [10], [20], [98], [99] by adding extra losses to enhance training stability and reduce motion jitter and foot sliding issues:\n$\\mathcal{L}_{joint}^{(i)} = FK(d^{(i)}),$ (16)\n$\\mathcal{L}_{j-vel} = \\frac{1}{n} \\sum_{j=1}^{n} ||d^{(i)}_{joint}- d^{(i-1)}_{joint}||_2,$ (17)\n$\\mathcal{L}_{j-vel} = \\frac{1}{n-1} \\sum_{i=1}^{n-1} ||d^{(i)}_{j-vel}- d^{(i)}_{j-vel}||^2,$ (18)\n$\\mathcal{L}_{j-acc} = \\frac{1}{n-2} \\sum_{j=1}^{n-2} ||d^{(i)}_{j-acc}- d^{(i)}_{j-acc}||^2,$ (19)\n$\\mathcal{L}_{contact} = \\frac{1}{n-1} \\sum_{j=1}^{n-1} (\\mathbb{I}(b^i) + || f_{hv}^{(i)}, f_{dv}^{(i)}||_2), $ (20)\nwhere $j$ means the $j^{th}$ frame, $n$ is the number of motion frames. $b$ is the predicted foot contact label. $f_{hv}$ and $f_{dv}$ represent horizontal downward velocity and vertical velocity of feet, respectively. The overall training objective is determined by combining the losses with weighted factors:\n$\\mathcal{L}_{total} = \\mathcal{L}_{recon} + \\lambda_{joint}\\mathcal{L}_{joint} + \\lambda_{j-vel}\\mathcal{L}_{j-vel} + \\lambda_{j-acc}\\mathcal{L}_{j-acc} + \\lambda_{contact}\\mathcal{L}_{j-contact} + \\lambda_{genre} \\mathcal{L}_{genre}.$ (21)\nwhere $\\lambda$ represents the weights for the corresponding loss terms."}, {"title": "3.4.4 The Detailed Network Architecture of PDDM", "content": "Foot Refine Block. In the SMPL format, motion representation is defined by relative rotations propagated along the kinematic chain, which means that minor change near root joints can cause large shifts at distal joints, such as the knees and feet. This issue is particularly challenging in dance movements, which often involve intricate foot actions, making it difficult to resolve foot-ground contact issues (e.g., foot sliding, foot floating) by simply applying foot-specific loss functions.\nThe primary problem arises from the domain gap between the optimization target and the data representation. Foot-ground contact is measured in a linear space based on joint positions, whereas SMPL-based motion exists within a nonlinear rotational space. To address this, we introduce a Foot Refine Block, which first calculates the positions of foot keypoints $foot_p$ and foot velocity $foot_v$ through forward kinematics. Following [99], we then compute the foot-ground contact score $foot_c$. A Cross Attention mechanism is subsequently applied to further refine foot movements.\nMulti Genre Discriminator. The PDDM can produce high-quality, diverse dance segments. As shown in the Figure 5, to ensure consistency with the overall dance genre, we also concatenate the genre embedding $g$ with the music features for feature encoding. We then use a Multi-Genre Discriminator (MGD) to control the dance genre following MNET [9]. The training process of MGD is formulated as:\n$\\mathcal{L}_{genre} = \\mathbb{E}_a [\\text{log MGD } (\\hat{a},g,m^i)] + \\mathbb{E}_a[\\text{log } (1 \u2013 \\text{MGD } (L_D (d^i,g,m^i),g,m^i))],$ (22)\n3.4.5 The Penetration Guidance\nWe find that the hands in the generated dances often penetrate the body. To further enhance physical realism and address the penetration issue, we propose a Penetration Guidance method based on a diffusion denoising process. By using the COAP [100] model to compute the Signed Distance Field (SDF) between the hand vertices and other body vertices, we can obtain gradients to guide hand and arm movements to alleviate the penetration problem:\n$\\mathcal{G}_{pene}(d) = \\frac{1}{P_h} \\sum_{q \\in P} \\sigma(f_\\theta(q|S))f_\\theta(p|S)>0,$ (23)\nwhere $p$ is the point set of hands calculated by $d$ using SMPL-X Skinning Function [101]. $f_\\theta(q|S)$ is the COAP model used to compute the signed distance between point $q$ and the body mesh $S$, $\\sigma(\\cdot)$ is the sigmoid function.\n$d_{t+1} = d_t + \\alpha_{con}\\nabla \\mathcal{L}_{con}(d_t) + \\alpha_{pene} \\nabla \\mathcal{G}_{pene}(d_t),$ (24)\nwhere $\\alpha_{con}$ and $\\alpha_{pene}$ are scale factors and $d_t$ is the motion representation after guidance.\n3.4.6 Parallel Inference.\nThe PDDM can generate $d_0^{i=1} ^\\frac{L}{n}$ in parallel. However, directly concatenating $d_0$ along the temporal axis to form a long dance sequence results in abrupt transitions at every $(i \\times n)^{th}$ frames, where $i = 1,\u2026\u2026,\\frac{L}{n} \u2013 1$. To address this issue, we constrain $d^i$ using $d_h$ from the dance primitives. we divide each $d_h$ into the first four frames and the last four frames. The first four frames serve as the tail four frames of the previous $d^i$, and the last four frames of $d_s$ serve as the leading four frames for the next $d^{i+1}$.\nHowever, directly using diffusion inpainting [102] to control the first and last frames of each segment still results in incoherent motions. Therefore, we use a joint acceleration loss $\\mathcal{L}_{j-acc}$ and modified the $\\mathcal{L}_{recon}$ loss of the PDDM. At each diffusion time step, we mixture $d_t$ of and the ground truth $d_0$ by $d_t[:4] = d_0[:4]$, $d_t[\u22124:] = d_s[\u22124:], d_t[4 : \u22124] = \\hat{d_t}[4 : \u22124]$. The $\\mathcal{L}_{recon}$ loss is formulated as:\n$\\mathcal{L}_{recon} = \\mathbb{E}_{d_0,d_t} [|| d_0 - f_\\theta (d_t, g, t, m^i) ||^2].$ (25)"}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Setup\nDataset: We use the FineDance [80] dataset to train and validate our algorithm. Compared to the earlier publicly"}, {"title": "4.2 Evaluation Metrics", "content": "We evaluate our algorithm comprehensively from five perspectives: Motion Quality, Motion Diversity, Beat Align Score, Run Time, and the ratio of wins in user study.\nMotion Quality primarily measures the quality of dance movements, analyzing their natural smoothness and physical realism. Motion Quality includes $FID_k$, $FID_g$, and Foot Skating Ratio (FSR). $FID_k$ and $FID_g$ are the Fr\u00e9chet Inception Distances (FID) of generated dance and all dance in the dataset. The subscripts k and g represent the FID distances calculated using kinematic features and geometry features, respectively, and both kinematic and geometry features are extracted using the Fairmotion-tools [106]. Foot Skating Ratio (FSR) is used to measure the level of contact between the feet and the ground, which measures the proportion of frames in which either foot skids more than a certain distance while maintaining contact with the ground (foot height<5 cm). We follow ProHMR [107] to calculate the ratio of penetrated vertices to the total number of vertices, obtaining the Penetration Ratio to quantitatively evaluate the character self-penetration degree.\nMotion Diversity primarily focuses on the richness and diversity of movements, including $DIV_k$ and $DIV_g$. We follow the methods used in AIST++ [7] and Bailando [14] to calculate the average feature distances of the generated dances to obtain DIV. The subscripts k and g represent the calculations using kinematic features and geometry features, respectively.\nBeat Align Score (BAS) [7] measures the degree of rhythmic alignment between the music and dance. It evaluates how well the timing of dance movements corresponds to the beats of the music, providing insights into the synchronization of the two modalities.\nRun time measures the efficiency of long-sequence dance generation, and we calculate the average time required to generate each dance sequence in the test set, expressed in seconds.\nWins is a subjective test where we ask different participants, including those with and without a dance background, to watch two sets of videos: one set containing the results generated by our method and the other set consisting of randomly selected ground truth (GT) or other state-of-the-art (SOTA) methods. Participants are then asked to choose which dance segment they believe has better quality."}, {"title": "4.3 Comparison to Existing Methods", "content": "As shown in Table 1, we compare our method with advanced music-driven dance generation algorithms. FACT is a classic autoregressive generation algorithm, while MNET builds upon FACT by incorporating a multi-genre discriminator. Bailando is an outstanding dance generation network based on VQ-VAE and GPT, demonstrating excellent generation performance. EDGE is a diffusion-based dance generation method that supports dance editing operations such as inbetweening, continuation, and joint-conditioned generation, and it has shown high quality in short-sequence dance generation. Compared to these methods, Lodge has made significant improvements in motion quality, beat alignment, and run time. Lodge achieved an FID of 50.00 and a Foot Skating Ratio of 2.76%, representing improvements of 44.34 (47%) and 16 (85.28%), respectively, compared to the previous state-of-the-art methods. $FID_k$ mainly reflects the physical quality of the movements, while $FID_g$ evaluates the global choreography rules' rationality. As shown, Lodge's $FID_g$ is 35.52, which, although significantly better than FACT, MNET, and EDGE, falls short of Bailando's 28.17. We argue this is due to Lodge using Global Diffusion to learn overly sparse Characteristic Dance Primitives and relying on handcrafted choreography rules to enhance these Primitives, leading to weaker generalization of the learned choreography rules.\nTherefore, we draw inspiration from Bailando's choreography model, adopting the Choreography Memory Book and sequence models to learn vivid choreography patterns from data. Additionally, we incorporate the PDDM model and utilize the Foot Refine Block, Multi-Genre Discriminator, and Penetration Guidance, further enhancing motion quality. Ultimately, both $FID_k$ and $FID_g$ of Lodge++ achieve significant improvements, reaching 40.77 and 30.79, respectively. The Ground Truth penetration ratio is 0.6954%, as the current dance dataset is collected from dancers with varying heights and body proportions. However, all dancer motions are retargeted to a standard human model for training, which introduces errors during the retargeting process"}, {"title": "4.4 Ablation Study", "content": "The Global Choreography Network: The Global Choreography Network (GCN) can efficiently model long-term choreography patterns. On one hand, it ensures that the generated dances is better choreography structure; on the other hand, it guides the local diffusion model to generate more expressive dance movements. We compared the method without GCN (i.e., not using the global choreography network and instead generating long-sequence dances with an autoregressive approach) to the method that uses the GCN to generate Dance Primitives, which then guides the diffusion model to produce high-quality dances. As shown in Table"}]}