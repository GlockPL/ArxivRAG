{"title": "If You Can't Use Them, Recycle Them:\nOptimizing Merging at Scale Mitigates Performance Tradeoffs", "authors": ["Muhammad Khalifa", "Yi-Chern Tan", "Arash Ahmadian", "Tom Hosking", "Honglak Lee", "Lu Wang", "Ahmet \u00dcst\u00fcn", "Tom Sherborne", "Matthias Gall\u00e9"], "abstract": "Model merging has shown great promise at com-\nbining expert models, but the benefit of merging is\nunclear when merging \u201cgeneralist\u201d models trained\non many tasks. We explore merging in the context\nof large (~100B) models, by recycling check-\npoints that exhibit tradeoffs among different tasks.\nSuch checkpoints are often created in the process\nof developing a frontier model, and many subop-\ntimal ones are usually discarded. Given a pool of\nmodel checkpoints obtained from different train-\ning runs (e.g., different stages, objectives, hyper-\nparameters, and data mixtures), which naturally\nshow tradeoffs across different language capabili-\nties (e.g., instruction following vs. code genera-\ntion), we investigate whether merging can recycle\nsuch suboptimal models into a Pareto-optimal one.\nOur optimization algorithm tunes the weight of\neach checkpoint in a linear combination, result-\ning in a Pareto-optimal models that outperforms\nboth individual models and merge-based base-\nlines. Further analysis shows that good merges\ntend to include almost all checkpoints with with\nnon-zero weights, indicating that even seemingly\nbad initial checkpoints can contribute to good fi-\nnal merges.", "sections": [{"title": "1. Introduction", "content": "Model merging is gaining traction as a cost-effective alterna-\ntive to multi-task learning or model ensembling (Wortsman\net al., 2022; Yu et al., 2024). While the research on model\nmerging has rapidly advanced in the last few years, it re-\nmains limited in terms of both model scale and the type\nof considered checkpoints. On one hand, most work has\nstudied merging fairly small models (e.g., 7B) by today's\nstandards, and it remains unclear how much benefit merging\nwould bring with much larger models (e.g., 100B+). On the\nother hand, the setup where merging was mostly applied\ninvolved two or more expert models, where experts are inde-\npendently optimized for specialized tasks, merged to com-\nbine their capabilities (Yadav et al., 2024a; Yu et al., 2024;\nAkiba et al., 2024). The primary motivation for such expert\nmerging is to eliminate the cost of multi-task training-each\nexpert can be trained separately, and then later merged for\ncombined expertise (Li et al., 2022). However, expert merg-\ning is only reasonable when expert models are available.\nDeparting from that setting, modern large language model\n(LLM) development scenarios tend to create a large quantity\nof multi-task models.\nTraining general-purpose LLMs relies on training a sin-\ngle model on many tasks during supervised finetuning\n(SFT)/instruction tuning (Chung et al., 2024; Jiang et al.,\n2023; Achiam et al., 2023; Dubey et al., 2024; Team et al.,\n2024; 2023). A prevalent issue here is that different tasks\nmay conflict with each other (Lin et al., 2019), resulting\nin tradeoffs among different capabilities. Parameters well\nsuited to one task may conflict or combine poorly with pa-\nrameters specialized for another (Gueta et al., 2023). For\nexample, aligning a model with human preferences can\nhurt performance on other evaluations (Bai et al., 2022; Vi-\njini et al., 2024). Similarly, improving math capabilities\nmay come at the cost of other reasoning skills (Fu et al.,\n2023). One strategy to alleviate these tradeoffs is to care-\nfully tune different training choices until a good enough\n(e.g., a Pareto-optimal) model is obtained. This approach is\nnot only computationally expensive but also discards sub-\noptimal models under the assumption that they constitute\n\"failed\" experiments.\nOur goal is to investigate: Can model merging provide a\ntraining-free approach to reduce performance tradeoffs be-\ntween multiple tasks? We adopt a realistic setup, using 16\ncheckpoints obtained from Command R+\u00b9 training runs\nwhich naturally exhibit tradeoffs among different model\ncapabilities such as code, math, and instruction following.\nWe focus on linear merging for simplicity and apply itera-\ntive search to optimize the weighting of each model to the"}, {"title": "2. Related Work", "content": "Prior work on model merging has mainly considered two\naxes: merging methods (i.e., how to merge); and in which\ncontext merging is useful (i.e., when to merge).\nMerging methods Recent efforts on model merging meth-\nods aim to overcome the limitations of simple averaging\n(Utans, 1996; Wortsman et al., 2022) with more involved\ntechniques. For instance, Matena & Raffel (2022) use the\nFisher information matrix to approximate each model's pos-\nterior beyond the isotropic assumption made by averaging.\nIlharco et al. (2023) propose to merge so-called task vec-\ntors instead of full models. Yadav et al. (2024a) propose\na 3-step process to resolve parameter interference among\nmerged models. Daheim et al. (2023) propose to improve\naveraging by reducing mismatch in parameter gradients. A\nmajor downside of most of these methods is that they require\naccess to gradient information, which becomes expensive\nas the models scale. In addition, other techniques such as\nDARE (Yu et al., 2024) assume access to a shared base\nmodel from which experts are fine-tuned, which is limiting\nin cases with different heterogeneous models as in our setup.\nMerging context Model merging has been mainly ex-\nploited in the context of transfer learning (Matena & Raffel,\n2022; Jin et al., 2022; Yadav et al., 2024a; Hammoud et al.,\n2024; Akiba et al., 2024). In this setup, merging is done\nacross a set of experts, specialized models that are fine-\ntuned separately on different tasks or domains, and which\nare then merged to combine their capabilities into a single\nmulti-task model (Rame et al., 2024). Merging can also\nbe exploited to ablate on dataset mixtures across domains\nduring pre-training (Na et al., 2024). While intriguing, we\nargue that this setup may not fully capture the typical large-\nscale LLM training scenarios. In practice, LLMs are often\nsimultaneously trained on a multitude of tasks during a sin-\ngle training run-suggesting that training is generally not\napproached as a multiple single-task optimization problem\nbut rather as a single multi-task optimization problem, e.g.,\nduring the supervised fine-tuning or instruction tuning stage\n(Ouyang et al., 2022; Jiang et al., 2023; Chung et al., 2024;"}, {"title": "3. Optimizing LLM Merging", "content": "3.1. Task Conflict\nDifferent tasks might conflict with each other in required\nexpertise, resulting in performance tradeoffs where improve-\nment observed over some tasks incurs performance degrada-\ntion on other tasks (Lin et al., 2019; Wang et al., 2021). For\ninstance in language modeling, aligning a model with hu-\nman preferences could result in an alignment tax, where the\nmodel performance degrades on held-out tasks (Bai et al.,\n2022). Another example is that instruction tuning could hurt\nthe performance on tasks such as question answering and\nreasoning (Dou et al., 2023). It is well known in practice\nthat different decisions related to hyperparameters, train-\ning data mixtures, or long-context training could yield such\ntradeoffs.\nOne way to minimize these tradeoffs is by carefully tuning\nthe training choices. This involves running several training\nruns with different hyperparameters and choosing Pareto-\noptimal runs with minimal tradeoffs (Team et al., 2024).\nHowever, this is cumbersome and expensive, and becomes\nless tractable as model parameters increase. We investi-\ngate whether it is possible to recycle the models obtained\nfrom such \"failed\" runs, which are usually abandoned or\ndiscarded altogether. We propose to use training-free model\nmerging as an efficient and compute-minimal strategy to\ncombining models.\n3.2. The Optimization Problem\nGiven $T$ tasks ${t_1,\\ldots, t_T}$ and $N$ checkpoints\n${\\theta_1, \\theta_2,..., \\theta_N}$ which tradeoff across the tasks, we\naim to output a model $\\theta_{mrg}$ by merging the checkpoints\nsuch that all tradeoffs are minimized. We mainly focus\non weight interpolation of the model weights, or model\nsoups (Wortsman et al., 2022), which yield a merged\nmodel by linearly interpolating the model parameters\nwith non-negative weightings. Precisely, the resulting\nmodel is computed as a weighted sum of the individual\nmodel parameters i.e., $\\theta_{mrg} = \\sum_{i=1}^{N} \\alpha_i\\theta_i$, where $\\alpha_i$\nrepresents the weighting assigned to $\\theta_i$, subject to constraint\n$\\sum_{i=1}^{N} \\alpha_i = 1$.\nWe limit our study to linear merging (i.e., model soups) for\nthree reasons. First, linear merging is easy to implement and\nis generally considered a strong baseline (Wortsman et al.,\n2022; Ilharco et al., 2022b; Yadav et al., 2024a). Second, it\nmakes few assumptions about the merged models (e.g., as-\nsumes no shared base model (Ilharco et al., 2022a), or access\nto gradient information (Matena & Raffel, 2022)) enabling\neasy scaling to large models. Third, while we acknowledge\nthe effectiveness of other merging strategies, recent work\n(Yadav et al., 2024b) has shown that different merging meth-\nods, including linear merging, may converge approximately\nto the same performance at large model scales.\nLet $P_t(\\theta')$ represent the performance of the model $\\theta'$ on\ntask $t$. Given a candidate model $\\theta'$, we quantify the\nperformance tradeoffs using a fitness function $R(\\theta') =$\n$R(P_{t_1}(\\theta'),\\ldots, P_{t_T}(\\theta'))$ which assesses the balance across\ntasks to capture all tradeoffs. That is, higher fitness means\nless severe tradeoffs. Thus, our objective is to find the\noptimal weightings vector $\\alpha^* = {\\alpha_1^*, \\alpha_2^*,\\ldots,\\alpha_N^*}$ that\nminimizes the task tradeoffs and therefore maximizes $R$:"}, {"title": "3.3. Search Algorithm", "content": "There exists a plethora of techniques to solve such prob-\nlems, including Bayesian Optimization (Snoek et al., 2012),\nrandom search (Bergstra & Bengio, 2012), and genetic al-\ngorithms (Young et al., 2015; Alibrahim & Ludwig, 2021).\nFor the purpose of our study, we focus on Covariance Ma-\ntrix Adaptation Evolution Strategy (Hansen & Ostermeier,\n2001), which has proven its usefulness in hyperparameter\noptimization (Loshchilov & Hutter, 2016, CMA-ES), in-\ncluding in model merging literature (Akiba et al., 2024), as\nwell as requiring minimal hyperparameter configuration.\nCMA-ES is an optimization technique suited for continuous,\nnon-linear optimization. It improves solutions by iteratively\nsampling candidates from a multivariate normal distribution,\nwith a mean representing the instantaneous optimal solution.\nAt each step, CMA-ES adapts the covariance matrix of the\nnormal distribution over time, based on successful solutions,\nto assign higher probability to good solutions. CMA-ES is\nsuitable for cases where gradient information is not available\nor is expensive to obtain."}, {"title": "4. Experimental Setup", "content": "Merge candidates As candidate inputs to the merge, we\nselect 16 models from the Command R+ (100B) develop-\nment pipeline, where each model is the result of a separate\ntraining run. To ensure these models are representative of\ndifferent training stages, we select such that:\n\u2022 50% are sourced from the supervised fine-tuning\n(SFT) training stage, and the other 50% from prefer-\nence optimization (PO) stage."}, {"title": "5. Results and Discussion", "content": "Before merging any models, we first look at the performance\nof individual models across different tasks to get a sense\nof the existing tradeoffs. Figure 2 shows the performance\nof each of the 16 merge candidates over both held-in and\nheld-out tasks. Table 2 in Appendix A shows exact numbers.\nInteresting tradeoffs show up when looking at individual\nmodel performances. For instance, SFT models (1-8) ex-\nhibit better code performance (i.e., on MBPP and LBPP)\ncompared to PO models (9-16), while PO models seem to\nperform better than SFT ones on MT-Bench and IFEval.\nThis is a tradeoff likely caused by alignment training, which\ncould hurt some other model capabilities (Bai et al., 2022).\nNow we zoom in on pairwise tradeoffs between two tasks.\nIn this case, it is fairly straightforward to measure the sever-\nity of such tradeoffs by computing performance correla-\ntion across the models. Figure 4 shows Spearman's rank\ncorrelation $p$ between all task pairs over our 16 selected\nmodels. We observe a few strong pairwise tradeoffs be-\ntween task pairs, such as MBPP-IFEval ($p = -0.35$) and\nMBPP-MUSR ($p = -0.40$). Throughout this section, we\nwill refer to the tasks with tradeoffs as held-in tasks. The\nmain question we aim to answer here through our experi-\nments is: Can optimizing ${\\alpha_1, \\alpha_2,\\ldots, \\alpha_N}$ yield $\\theta_{mrg}$ with\nminimal tradeoffs over the held-in tasks without a hurting\nperformance over the held-out ones?\n5.1. Optimizing Pairwise Tradeoffs\nWe apply our merge optimization recipe over three task pairs\nwith relatively strong tradeoffs: MBPP-IFEval, MBPP-\nMUSR, and MMLU Pro-IFEval.\nFigure 3 shows a plot for each pair of tasks. Each plot in-\ncludes a best-fit line to the performances of each pair (shown\nin green), which exhibits a negative slope in all three cases\ndue to the respective tradeoff. We observe that baselines"}, {"title": "5.2. Optimizing Three-task Tradeoffs", "content": "In practice, production LLMs are expected to be perfor-\nmant at more than two tasks. We consider balancing per-\nformance across three tasks: code generation, instruction\nfollowing, and math reasoning, by using MBPP-IFEval-\nGSM8K as held-in tasks. We target this combination since\nIFEval correlates both negatively with MBPP, and positively\nwith GSM8K (as shown in Figure 4), making it a challeng-\ning combination to optimise. The goal is to identify how\nour approach scales with more than 2 tasks, due to the expo-\nnential growth in choices (and search space) with respect to\nthe number of tasks.\nLooking at Figure 5, it is evident that the best fitness single\nmodel (i.e., highest average performance) performs well\non IFEval and GSM8K, but comparably poor on MBPP.\nThe other two baselines, \u2018Merge-Best' and 'Uniform Soup',\nwere able to improve the tradeoffs by some degree but ex-\nhibit noticeable performance drop on IFEval. While the\nsearch-optimized merge is a Pareto-optimal model \u2013 main-"}, {"title": "5.3. Analysis", "content": "We perform further analysis into the dynamics of linear\nmerging:\nMost models contribute to the best merges. The first\nquestion we ask is: What fraction of the initial 16 models\ncontribute to the best solutions? Figure 6 shows a heatmap\nof the top 5 solutions on each task combination. We observe\nthat CMA-ES identifies good solutions which distribute the\nweightings among almost all checkpoints (dense solution),\ninstead of assigning high weights to a small subset of the\nmodels (sparse). For example, the top solution assigns very\nfew zero weightings (shown in black in Figure 6) for MBPP-\nMUSR and MBPP-IFEval. Also, while the top solutions for\nMBPP-IFEval-GSM8K are slightly sparser in the pairwise\ncase, at least 9/16 weightings are non-zero for the top 5\nsolutions. This indicates that almost all checkpoints have\ncontributed to the optimal merge.\nLow performing models may lead to optimal merges.\nSince the CMA-ES optimization process evaluates many\nsolutions, we can investigate the solutions found through\nsearch along with their quality, as measured by their stand-\nalone performance. Intuitively, one would expect that high\nfitness solutions found through search will assign higher"}, {"title": "Fitness improves with more iterations", "content": "We inspect\nwhether CMA-ES effectively optimizes the fitness function\nas the search progresses by looking at the fitness function\ndevelopment over the course of search. Figure 7 plots the\nfitness function vs the number of iterations. Each point in\nthe graph is a weightings vector proposed by CMA-ES, and\nthe fitness is the average of the held-in task performances of\nthe resulting merge. Over the three pairwise task combina-\ntions, it is clear that the average solution fitness improves\nwith more CMA-ES iterations.\nRecycling benefits from more checkpoints. Including\nmore initial checkpoints obviously extends the search space,\nwhich might include a better solution, at the expense of\na larger search space. To study how the merge quality\nchanges with the number of checkpoints, we run CMA-\nES over the $N$ checkpoints with best fitness scores, with\n$N\\in \\{2,4,8,16\\}$. The results over MBPP-MUSR, and\nMBPP-IFEval are shown in Figure 8 (Figure 9 in Ap-\npendix B plots the same for MBPP-IFEval), where we high-\nlight the centroid for each N. As can be seen, with larger N\nthe search space is explored more exhaustively and results in\ncentroid checkpoints with better fitness. Smaller N exploits\nsome sub-spaces deeper, also resulting in good merges (as\ncan be seen in Figure 9)."}, {"title": "Conclusion", "content": "In this paper, we present an approach to recycle checkpoints\nobtained during a typical training run of a frontier model.\nWhile the vast majority of those checkpoints are in general\ndiscarded, in this paper we show how to leverage them via\nsearch-optimized merging. We show that a simple search\nalgorithm focusing on linear merging can yield better, and\noften Pareto-optimal models with respect to the existing\ncheckpoints. Our research show that it is possible to lever-\nage merging when we have many multi-task trained check-\npoints, as opposed to the standard setup of merging experts.\nA surprising finding is that even checkpoints which perform\nrelatively bad on subtasks can contribute to an overall bet-\nter model. While we relied on a simple merging approach,\nwe hope future development will further investigate more\ninvolved merging techniques in a similar setup to ours via\nmerging as a cheaper and training-free approach."}]}