{"title": "Musical ethnocentrism in Large Language Models", "authors": ["Anna Kruspe"], "abstract": "Large Language Models (LLMs) reflect the bi-ases in their training data and, by extension,those of the people who created this trainingdata. Detecting, analyzing, and mitigating suchbiases is becoming a focus of research. Onetype of bias that has been understudied so farare geocultural biases. Those can be caused byan imbalance in the representation of differentgeographic regions and cultures in the trainingdata, but also by value judgments containedtherein.\nIn this paper, we make a first step towards an-alyzing musical biases in LLMs, particularlyChatGPT and Mixtral. We conduct two experi-ments. In the first, we prompt LLMs to providelists of the \"Top 100\" musical contributors ofvarious categories and analyze their countriesof origin. In the second experiment, we askthe LLMs to numerically rate various aspectsof the musical cultures of different countries.Our results indicate a strong preference of theLLMs for Western music cultures in both ex-periments.", "sections": [{"title": "1 Introduction", "content": "It has long been known that machine learning mod-els pick up and thus perpetuate human biases invarious ways, most prominently by learning themfrom their training data. For text-based models,even early embedding approaches exhibited e.g.gender bias (Bolukbasi et al., 2016). With the re-cent rise of Large Language Models (LLMs), gen-der and race biases were quickly discovered andanalyzed in various domains (Kotek et al., 2023;Sun et al., 2023; Omiye et al., 2023; Warr et al.,2023). Types of bias that have been consideredsomewhat less include those based on culture andgeography. However, (Manvi et al., 2024) recentlyshowed that LLMs also exhibit those, both implic-itly and explicitly. Their research demonstrated thatwhen prompted to rate random locations on Earthon various characteristics, LLMs generally yieldedlower ratings for certain regions, e.g. the globalSouth. There appear to be correlations with thecoverage of regions and their cultural and historicalsignificance in the training data, statistics acrossa range of aspects around the world, and possiblystructural biases of the institutions creating thesemodels, which are mainly based in North Americaand Europe.\nWe hypothesize that such biases are not onlypresent for purely geographic topics, but also forcultural developments in different regions of theworld. In this paper, we conduct first experimentsto detect such biases with regards to music culture.Those are based on two different types of measure-ments, translated into prompts: a) Asking modelsto give an overview of top musical artists, and b)asking models to rate aspects of musical culture indifferent regions of the world. The first experimentelicits model bias on an open-ended question withregards to presence of different cultural regions inthe models, while the second one employs a directcomparison to extract implicit judgments learnedby the models. To gain insights into the influence ofwhere and how the model was trained, we prompttwo models from different regions of the world infour languages.\nThe rest of the paper is structured as follows:Section 2 gives an overview of other work in thefield of geocultural biases in LLMs. Section 3provides details of our experimental design. Theresults are presented in section 4. Finally, sections5 and 6 discuss our findings and make suggestionsfor future work."}, {"title": "2 Related work", "content": "Initial studies, such as those discussed in (Tao et al.,2024) and (Naous et al., 2023), show LLMs oftenencode biases favoring Western, English-speakingnorms, impacting their fairness and representationof non-Western cultures as well as performanceon non-Western topics, like Traditional ChineseMedicine (Lingxuan Zhu and Luo, 2024).\nFurther research seeks methodologies to mea-sure and mitigate these biases more accurately. Theinterdisciplinary approach in (Biedma et al., 2024)and the survey on modeling culture in LLMs (Adi-lazuarda et al., 2024) propose new frameworks forunderstanding and adjusting the embedded culturalvalues in LLMs. The \u201cCulturePark\" (Li et al.,2024a) initiative and the \"NormAd\u201d (Rao et al.,2024) benchmark are notable in their attempts tosimulate cross-cultural communication scenariosand assess LLMs' adaptability to cultural contextsthrough synthetic story generation, providing novelapproaches to evaluating cultural sensitivity andadaptability in AI technologies. The \u201cCDEval\"(Wang et al., 2023) benchmark specifically ad-dresses the need to evaluate the cultural dimensionsof LLMs, integrating automated generation andhuman verification to assess cultural traits acrossmultiple domains. Similarly, the \u201cCultureLLM\"(?) project aims to fine-tune LLMs on culturallydiverse data.\nIn the music domain, (Li et al., 2024b) illustratesdomain-specific biases, arguing for more compre-hensive benchmarks in varied knowledge domains."}, {"title": "3 Methodology", "content": "In this section, we will describe our experimen-tal design, including used models, prompt design,prompted tasks, and postprocessing of the results.\n3.1 Models\nWe tested our bias prompts on two different modelsvia their online interfaces:\n\u2022 ChatGPT-4 (paid version) via its online inter-face (https://chatgpt.com/)\n\u2022 Mixtral-8x7B via the online interface un-der https://deepinfra.com/mistralai/Mixtral-8x7B-Instruct-v0.1, maximumnew token length set to 10,000\nChatGPT was created in the US by OpenAI, whileMixtral was released by the French company Mis-tral AI. We wanted to compare models from twodifferent regions of the world on this geocentrictask. A more geographically wide-ranging selec-tion of LLMs would be of high interest for futurecomparisons. Currently, Chinese institutions arealso intensifying their efforts in the LLM domain,but we were not able to obtain access to a freelyavailable Chinese model."}, {"title": "3.2 Prompt design", "content": "We conducted two experiments. In the first one, weasked open-ended questions about the \"Top 100\"musical performers of various types. Those in-cluded bands, solo musicians, singers, instrumen-talists, and composers. Prompts were simply of theform \"Name the Top 100 singers/instrumentalists/bands/...", "which country has the bestmusic\") due to content filters, but ratings workedwell. The aspects of music culture included agree-ableness, successfulness, musical creativity, globalinfluence, musical tradition, and musical complex-ity.\nPrompts were designed in English, and thentranslated to Spanish, Chinese, and French usingChatGPT-4. The list of countries was kept in En-glish. ChatGPT was prompted in English, Spanish,and Chinese, and Mixtral was prompted in Englishand French. Each experiment was repeated threetimes on each model and each language to accountfor different initializations.\"\n    },\n    {\n      \"title\": \"3.3 Postprocessing\",\n      \"content\": \"For the Top 100 experiment, we then calculatedthe frequency of each country's appearance acrossall runs. In cases where the model named multiplecountries of origin for one performer, we only keptthe first one for simplicity.\nThe rating results were normalized by mean andstandard deviation for each characteristic. Then,we averaged those normalized results across allthree runs for each characteristic.\nOur full results and analysis notebooksare available on https://github.com/annakaa/musical_ethnocentrism.\"\n    },\n    {\n      \"title\": \"4 Results\",\n      \"content\": \"4.1 \u201c\u0422\u043e\u0440 100": "esults\nAn example result for the \"Top 100\" experimentsis shown in Figure 1, with the full results in Fig-ure 3 (appendix). As hypothesized, the results arevery focused on Western countries, especially theU.S. South American representation varies a bit,whereas Asia and Africa are completely underrep-resented. The effect is particularly strong for bandsand singers. For the question about solo artists andinstrumentalists, results are a bit more diverse. Theprompt about composers has a stronger Europeanfocus, but also results in a surprisingly high num-ber of those from the U.S. (including some who arepossibly lesser-known in the rest of the world).\nWhen prompting with different models and dif-ferent languages, the results vary, but are somewhatinconclusive. Spanish-language prompts do seemto lead to a slightly stronger representation of Spainand South America, and Chinese-language ones toa stronger focus on China, but none of the changesare very pronounced. Compared to ChatGPT, Mix-tral appears to produce slightly more diverse re-sults, especially with regards to Africa (interest-ingly, though, more for the countries with Englishas their official language rather than French).\n4.2 Rating results\nAn example result of the experiments where weasked LLMs to rate aspects of music culture indifferent countries is presented in Figure 2, andthe full results are shown in Figure 4 (appendix).Once again, we see a strong tendency towards West-ern countries, especially the U.S. Correlating withthe results of the previous experiments, Asian andAfrican countries are rated much lower in compar-ison, while South America lies somewhere in themiddle. This is true for almost all prompted aspects.The outlier appears to be \"Tradition\u201d, where, forexample, India tends to be rated higher. This mayhappen due to training data sources that are morefocused on folkloristic (\"world\") music rather thanpop or classical music, which may become associ-ated with the \"Tradition\" keyword.\nOnce again, we do no see major effects betweenmodels and languages. Prompting in Chinese ap-pears to emphasize the U.S. and India, but not nec-essarily China itself, whereas prompting in Span-ish once again leads to slightly higher ratings forSpain and South America. When using Mixtral,we once again obtain somewhat more balanced re-sults. In particular, the \u201cTradition\" prompt yieldshigher ratings in Africa, and this time mainly forFrench-speaking countries. This may happen dueto a higher frequency of French-language sourcesin Mixtral training."}, {"title": "5 Discussion", "content": "As expected, we observed a strong dominance ofthe Western world, particularly the U.S., in bothtasks. South America was comparatively well-represented, whereas Asia and Africa were almostnever mentioned in the \"Top 100\" experiments, andrated consistently lower in the second experiment.\nBoth models produce slightly different results,with Mixtral appearing a bit more diverse. How-ever, there is some indication that state-of-the-artlanguage models are trained on most of the textdata currently available on the internet, meaningthat the cultural distribution of training data maynot vary too much between any current models\u00b9.\nThe language in which prompts are given tothe model does appear to play a role, but not ina very straightforward way (e.g. Chinese-languageprompting does not lead to China being mentionedsignificantly more often). Due to the cross-lingualabilities of LLMs, the language of the contextmay in fact play a smaller role than language inthe training data. CommonCrawl, often namedas the biggest source of LLM training data, con-tains around 46% English-language text, which thesecond-most frequent language being Russian atjust 6%\u00b2. Nevertheless, the language of a countrywill in all probability be implicitly somewhat morestrongly associated with its culture."}, {"title": "6 Future work", "content": "In this work, we presented a first step towards de-tecting cultural biases in LLMs with a focus on themusic domain. As mentioned above, it would bevery interesting to see whether LLMs from otherparts of the world (first and foremost China) per-petuate the same biases. Future work could alsoanalyze other music-related tasks around the world,and compare with other aspects of culture. Ona smaller note, the results were obtained via theonline interfaces of the models which may filteror change results; future work could employ themodels directly for more control.\nBeyond analyzing these biases, an important re-search goal lies in mitigating them. When consid-ering the \u201cTop 100\" task, this is very subjective.An interesting research direction may be aimedmore towards human-computer interaction: Whatdo users expect when prompting models for recom-mendations like these? From an ethical standpoint,should models then fulfil users' expectations, oraim for more diversity than what a human authoror the training data may provide? Answers maylie in integrating external knowledge sources (e.g.knowledge graphs) into LLMs, but also in adaptingthem towards individual users.\nFor the ratings task, possible solutions are muchharder to determine. In principle, the whole task ofrating musical cultures is not well-posed, but it re-veals underlying judgments learned by the model,which may influence downstream tasks (includ-ing the \"Top 100\" experiment). Removing thesejudgments may be impossible as they appear tobe implicit in the training data. A possible futuredirection may lie in making these influences moretransparent to users, allowing them to decide forthemselves whether the model's answer is basedon the correct assumptions (Kruspe, 2024)."}]}