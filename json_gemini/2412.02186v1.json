{"title": "VideoICL: Confidence-based Iterative In-context Learning\nfor Out-of-Distribution Video Understanding", "authors": ["Kangsan Kim", "Geon Park", "Youngwan Lee", "Woongyeong Yeo", "Sung Ju Hwang"], "abstract": "Recent advancements in video large multimodal mod-\nels (LMMs) have significantly improved their video under-\nstanding and reasoning capabilities. However, their per-\nformance drops on out-of-distribution (OOD) tasks that are\nunderrepresented in training data. Traditional methods like\nfine-tuning on OOD datasets are impractical due to high\ncomputational costs. While In-context learning (ICL) with\ndemonstration examples has shown promising generaliza-\ntion performance in language tasks and image-language\ntasks without fine-tuning, applying ICL to video-language\ntasks faces challenges due to the limited context length in\nVideo LMMs, as videos require longer token lengths. To ad-\ndress these issues, we propose VideoICL, a novel video in-\ncontext learning framework for OOD tasks that introduces\na similarity-based relevant example selection strategy and a\nconfidence-based iterative inference approach. This allows\nto select the most relevant examples and rank them based\non similarity, to be used for inference. If the generated\nresponse has low confidence, our framework selects new\nexamples and performs inference again, iteratively refin-\ning the results until a high-confidence response is obtained.\nThis approach improves OOD video understanding perfor-\nmance by extending effective context length without incur-\nring high costs. The experimental results on multiple bench-\nmarks demonstrate significant performance gains, espe-\ncially in domain-specific scenarios, laying the groundwork\nfor broader video comprehension applications.", "sections": [{"title": "1. Introduction", "content": "Recent video large multimodal models (LMMs) [11, 34, 47,\n57] have shown notable improvement in video understand-\ning and reasoning tasks, e.g., enabling these models to com-\nprehend natural scene videos and answer causal questions.\nHowever, as new types of data continue to emerge, these\nmodels are expected to handle previously unseen, out-of-\ndistribution (OOD) videos [5, 17, 28, 37, 38, 44]. Such\nOOD videos are rarely encountered during training due\nto their specialized nature or the need for domain-specific\nknowledge, such as gymnastic or surgical videos. Conse-\nquently, the performance of video LMMs on OOD videos\nis poor compared to in-distribution videos [23, 36], pri-\nmarily due to the limited representation of OOD content\nin training datasets. For instance, video LMMs can read-\nily distinguish between well-represented actions like \"danc-\ning\" and \"exercising\" but struggle to differentiate actions\nlike \"abuse\" from \"assault\", as crime videos are seldom in-\ncluded in training datasets. Given that the model faces nu-\nmerous unseen situations in real-world scenarios, improv-\ning OOD video understanding with video LMMs remains a\nsignificant challenge.\nFine-tuning the model on the target videos is a straight-\nforward method to enhance its performance. However, fine-\ntuning the model for each OOD scenario is time-consuming\nand often impractical, as it requires substantial amounts of\ntraining data to avoid overfitting and incurs significant train-\ning costs. In contrast, in the realm of large language mod-\nels (LLMs), in-context learning (ICL) [7]\u2014which involves\nproviding example inputs alongside a test sample during in-\nference-has been actively explored as an efficient alterna-\ntive for handling OOD tasks. ICL has shown strong gener-\nalization on unseen tasks in LLMs, making it advantageous\nas it bypasses the need for fine-tuning. Many studies have\ndemonstrated the effectiveness of ICL in language-only and\nimage-language tasks [2, 16, 27, 55, 56]; however, ICL's po-\ntential in video-language tasks has not been fully explored.\nA key challenge with ICL in the video domain is that\nvideo tokens are significantly longer than image or text to-\nkens, limiting the number of video examples in a single con-\ntext. For instance, LLaVA-Video [57] can process up to 32K\ntokens (i.e., context-length), but a 30-second, 384\u00d7384 res-\nolution video sampled in 32 frames is converted to approxi-\nmately 5.5K tokens through the vision encoder of the model\nwith a patch size of 14. This allows for a maximum of only\nfour video samples within the token limit. Considering that"}, {"title": "2. Related works", "content": "recent text-only and image-text ICL research is advancing\ntoward many-shot learning [1, 6, 21], often utilizing over\n1K examples, the number of video examples here is notably\nconstrained in comparison.\nThis challenge has been approached from two main di-\nrections in multimodal ICL research. One approach aims to\nreduce the token length of each example [14, 59], while the\nother focuses on selecting a minimal set of highly effective\nexamples [8, 48]. However, the first approach potentially\nmay lead to a loss of crucial information, while the second\nrelies on a limited number of in-context examples, making\nthe model performance highly sensitive to the relevance of\nthese examples to the given query prompt.\nTo address these issues, we propose VIDEOICL, a\ntraining-free video in-context learning framework for OOD\nvideo understanding, which efficiently handles multiple ex-\namples within a limited context length while preserving ex-\nample quality. Specifically, we first introduce a similarity-\nbased relevant example selection strategy, ranking demon-\nstrating examples based on both video and text feature sim-\nilarity to construct an ordered set of relevant examples for\ninference. Then, we present a confidence-based iterative in-\nference mechanism; VIDEOICL performs in-context learn-\ning iteratively, with each iteration leveraging a new subset\nof highly relevant examples from the top of the similarity\nranking. At each iteration, the model calculates a confi-\ndence score based on token probabilities and stops iterating\nonce it reaches a sufficient confidence level to generate an\naccurate answer. This iterative approach enables the model\nto effectively utilize a larger pool of examples, maintaining\ninformational richness without exceeding context limits.\nExtensive experiments on several OOD video under-\nstanding benchmarks demonstrate the superiority of our\nmethod. Specifically, we validate our approach across\nsix datasets, spanning four distinct video-language tasks:\nmultiple-choice question answering, open-ended question\nanswering, video classification, and video captioning. The\nresults show that our framework, tested with LLaVA-Video-\n7B [57], outperforms zero-shot baselines, achieving an av-"}, {"title": "2.1. Video large multimodal models", "content": "The impressive capabilities of LMMs have motivated sub-\nstantial research into video LMMs [11, 34, 47, 57], demon-"}, {"title": "2.2. Multimodal in-context learning", "content": "Beyond the success of ICL on text-only tasks, it has been\nextended to multimodal tasks involving images [4, 8, 9, 14,\n27, 43, 52, 58] and videos [2, 27, 54]. In video ICL, most\nstudies aim to equip LMMs with the ability to comprehend\nmultiple video examples by training them on video-text in-\nterleaved datasets. Otter [27] trains image-text ICL model\nusing ICL instruction tuning dataset with video-language\ndemonstrations. Yu et al. [54] propose a strategy to gen-\nerate a video-text ICL training dataset that induces small-\nscale language models. However, these studies do not ad-\ndress OOD videos, and fine-tuning is impractical for han-\ndling OOD tasks, as it is both costly and time-consuming to\ntrain the model for each task.\nA significant challenge in multimodal ICL is the in-\ncreased length of each demonstration. One approach to ad-\ndress this is to compress the tokens of context examples to\nfit within the model's limit, using fused virtual tokens [14]"}, {"title": "2.3. Iterative in-context learning", "content": "To enhance the robustness of ICL, some studies have em-\nployed iterative methods in language-only tasks. IDS [41]\nutilizes iterative refinement by selecting multiple sets of\ndemonstrations and performing majority voting among the\ngenerated answers to identify the best response. Se\u00b2 [33]\nintroduces an example selection approach that considers the\nsequence of examples, iteratively refining the selection to\nfind the optimal set based on previously chosen examples.\nIterative retrieval [10] enhances ICL by using a stateful,\npolicy-learning framework for iterative example selection,\nimproving performance in semantic parsing tasks. Inspired\nby these works, we adopt an iterative method to address the\nchallenge of limited example numbers in video ICL."}, {"title": "2.4. Factual confidence estimation", "content": "Estimating the factual confidence of LLMs has become an\nimportant research focus, as higher confidence levels are of-\nten associated with more accurate outputs and fewer hallu-\ncinations [12, 15, 19]. Xiong et al. [51] and Lin et al. [32]\nexplore verbalization, where models express confidence in\nnatural language. Kumar et al. [24] employ logit-based\nmethods to estimate token-level confidence by assessing to-\nken probabilities as indicators, while Liu et al. [33] apply\nthese techniques for selecting ICL examples. Sequence-\nlevel confidence is often derived by aggregating token-level\nconfidence, typically using the minimum probability across\nall tokens [20]. Azaria and Mitchell [3] and Li et al. [29]"}, {"title": "3. Method", "content": "In this section, we describe our method to overcome the\nchallenges of video ICL. First, we select a certain num-\nber (k) of in-context examples for a given query, construct-\ning a list of demonstrating examples based on similarity\n(Sec. 3.2). Next, we sequentially fetch a small number\n(mk) of examples from the list, iteratively refining the\nanswer based on model confidence (Sec. 3.3). The complete\nworkflow of VIDEOICL is shown in Fig. 2."}, {"title": "3.1. Notation", "content": "Throughout the paper, \u0177 = M(x; D) denotes the answer\na video LMM M generates for a query x using a set of\nin-context examples D. Conf(x, y; D) denotes the video\nLMM M's estimated confidence in a generated answer \u0177,\ngiven a question x and a set of in-context examples D."}, {"title": "3.2. Similarity-based example selection", "content": "At test time, we are given a query x = (t, v), where t and\nv are the text and video in the query prompt, respectively.\nBased on the relevance to the query, k in-context exam-\nples are selected from the set of target task-specific example\ndata. k is a fixed hyperparameter. For this, we use a linear\ncombination of the cosine similarities on the vector repre-\nsentation of the given query and example data:\n$S_Q ((t, v), (\\tilde{t}, \\tilde{v})) :=\\alpha S_c(r_t(t), r_t(\\tilde{t})) + (1 - \\alpha) S_c(r_v(v), r_v(\\tilde{v}))$,\nwhere $S_c$ denotes the cosine similarity between two vec-\ntors. $r_t(\\cdot)$ and $r_v(\\cdot)$ denote text and video encoders, respec-\ntively, which map arbitrary-length text and video inputs into\nfixed-length vectors. \u03b1 is a balancing coefficient between\nthe text and the video similarities. From the set of exam-\nple data $D = \\{(t_1, \\tilde{v}_1), ..., (t_n, \\tilde{v}_n)\\}$, we select the top-k\nexamples that maximize $S_Q$:\n$SelectRelevant_k(D, x) := Top-k_{x \\in D}[S_Q(x, \\tilde{x})]$.\nNote that this example selection has negligible cost over-\nhead at inference time, since all of the text and video vector\nrepresentations in the example set can be pre-processed and\nbe stored in a vector database."}, {"title": "3.3. Confidence-based iterative inference", "content": "Since all k in-context examples do not fit within the con-\ntext size of open-source video LMMs, we instead provide\nthe model m examples at each iteration. m \u2264 k is a fixed\nhyperparameter, chosen so that m examples, along with the"}, {"title": "3.4. Theoretical analysis", "content": "To substantiate our approach, we further present a theoreti-\ncal analysis illustrating how our confidence-based iteration\nmethod can enhance model accuracy. Let us assume that\nthe probability of a video LMM outputting the correct an-\nswer for a given task, given a query and a set of in-context\nexamples, is constant: $Pr(M(x; D_{cur}) = y) = p_c$, and inde-\npendent across iterations. For simplicity, we further assume\nthat confidence estimation operates independently at each\niteration and that the framework outputs the final iteration's\nanswer as the final result, rather than selecting the answer\nwith the highest confidence score."}, {"title": "4. Experiments", "content": "In this section, we validate our framework in four video-\nlanguage tasks on a wide range of out-of-distribution bench-\nmarks using six datasets: multiple-choice QA on Animal\nKingdom [38], open-ended QA on Sports-QA [28] and\nPitVQA [17], video classification on UCF-Crime [44] and\nDrive&Act [37], and video captioning task on CapERA [5]."}, {"title": "4.1. Experiment setup", "content": "We apply our framework to state-of-the-art open-source\nvideo LMMs, such as LLaVA-Video-7B [57], Qwen2-VL-\n7B [47], and Oryx-1.5-7B [34], selected for their strong\nperformance in general video understanding benchmarks.\nFor the similarity-based example selection, we use Sen-\ntenceBERT [42] and InternVideo2 [49] as text and video\nencoders, respectively. We sample each video at 1 frame\nper second across benchmarks, and for videos over 32 sec-\nonds, we uniformly select 32 frames from the entire se-\nquence. For the confidence estimation, we use minimum\nvalue among the probabilities of generated tokens, and the\nconfidence threshold is set to $C_{th} = 0.7$ in multiple-choice\nQA and $C_{th} = 0.5$ in other tasks. Our reasoning for this\nchoice and additional experiments with various $C_{th}$ settings\nare provided in Appendix A. The total number of demon-\nstrating examples is set to k = 8. Note that considering the\nlimited context length of the video LMMs, we set the num-\nber of in-context examples to m = 2 at each iteration across\nall benchmarks. Therefore, the maximum number of itera-\ntions per test sample is n := k/m = 4."}, {"title": "4.2. Baselines", "content": "We compare VIDEOICL with several strong baselines\nto rigorously assess our method's performance: GPT-40 [46], Gemini-1.5 Pro [45], Otter-7B [27], MMICES [8]"}, {"title": "4.5. Qualitative results", "content": "We illustrate some representative samples from our bench-\nmarks in Fig. 3. In the first row, we show an example from\nUCF-Crime [44]. While the ground truth label is Shoplift-\ning, the zero-shot model answers incorrectly as Normal\nEvent. In contrast, our model selects two relevant demon-\nstrations-a Normal Event and a Shoplifting example in\nthe first round. Using these two in-context examples, our\nmodel is able to answer the original question correctly. This\ndemonstrates that our similarity-based example selection al-\nlows the model to select relevant and helpful examples.\nIn the second row, we show an example from Sports-QA [28] where the vanilla model makes an incorrect predic-\ntion. In the first iteration, the initial response of our model\nis incorrect too, with low confidence. By the second iter-\nation, our model could answer correctly with high confi-\ndence. Without the iterative approach, the model would not\nhave reached the correct answer, highlighting the effective-\nness of iterative selection, especially when initial depen-\ndations are suboptimal.\nIn contrast, in the third row, even though the model pre-"}, {"title": "5. Analysis", "content": "In this section, we analyze the impact of each component\nand hyperparameter of our method on its performance. For\nour analysis, we select one representative dataset for each\nvideo-language task: Animal Kingdom [38], PitVQA [17],\nUCF-Crime [44], and CapERA [5]."}, {"title": "5.1. Demonstration selection method", "content": "The impact of similarity-based example selection is stud-\nied by analyzing which query features (text or video) most\ninfluence the effectiveness of example selection. We com-\npare our method with random demonstration selection,\ntext feature-only selection, and video feature-only selection\nacross three datasets. The results, shown in Tab. 2, indicate\nthat our similarity-based selection method using both text\nand video features outperforms the random selection base-\nline by a large margin. For instance, on PitVQA, similarity-\nbased selection leads to up to +53.0%p performance in-\ncrease over random selection. Furthermore, we observe that\nboth text and video features contribute to similarity-based\nselection, as using only one of the two yields lower scores."}, {"title": "5.2. Total number of available examples (k)", "content": "We investigate the impact of confidence-based iterative in-\nference by comparing our method, which uses multiple iter-\nations, with a single-iteration baseline across four datasets.\nThe results, shown in Tab. 3, indicate that increasing the\ntotal number of demonstrations generally helps the bench-\nmark performance. For example, compared to performing\nonly one iteration (k = 2), we observe up to +7.2%p per-\nformance increase in PitVQA when using eight iterations\n(k = 16). This is consistent with the general observation"}, {"title": "5.3. Confidence estimation method", "content": "We evaluate our confidence estimation method, token prob-\nability [20], against two alternative approaches: verbaliza-\ntion [32, 51] and trained probe [3, 22], across four bench-\nmark datasets. For the trained probe, following Azaria\nand Mitchell [3], we pre-train a 4-layer MLP as a confi-\ndence estimator using the hidden states of the last token\non a subset of diverse video-language datasets and bench-\nmarks [13, 25, 30, 57]. The confidence score from the\ntrained probe is used in the same way as our main method.\nFor verbalization, we ask the model if it is confident enough\nto respond before generating the answer. If it answers yes,\nwe proceed with the response; if not, we iterate with the\nnext set of examples. The results are shown in Tab. 4.\nThe token probability method outperforms other ap-\nproaches, with the trained probe following closely, while\nverbalization performs the worst. This somewhat contra-\ndicts the findings in [35], which report that the trained probe\nestimates model confidence most accurately. A possible ex-\nplanation is that, unlike other zero-shot methods, the trained\nprobe's reliance on pre-training makes it more dependent on\nits specific training data, limiting its ability to generalize,"}, {"title": "5.4. Most confident responses across iterations", "content": "We examine which iteration yields the highest confidence\nscores. Fig. 4 illustrates the confidence across four datasets\nand highlights the iteration, out of four, that produced the\nmost confident answers. Notably, despite using the most\nsimilar examples in the first iteration, only 29% of re-\nsponses in CapERA and 50% in PitVQA achieved enough\nconfidence to surpass the threshold in the first round. This\nfinding suggests that relying only on the first iteration may"}, {"title": "6. Conclusion", "content": "In this paper, we propose VIDEOICL, a novel frame-\nwork for in-context learning for out-of-distribution video-\nlanguage tasks with large multimodal models. We address\nthe challenge of multimodal in-context examples exceeding\nthe token length limit of LMMs by employing similarity-\nbased demonstration selection and confidence-based itera-\ntion. Extensive experimental results highlight the effective-\nness of our method for OOD videos. VIDEOICL is training-\nfree, enabling rapid adaptation to novel tasks while offering\na more efficient and feasible alternative to naive in-context\nlearning at inference time."}, {"title": "7. Acknowledgments", "content": "This work was supported by Institute of Information\n& communications Technology Planning & Evalu-\nation (IITP) grant funded by the Korea government\n(MSIT) (No. RS-2022-00187238, Development of\nLarge Korean Language Model Technology for Effi-\ncient Pre-training) and (No. RS-2019-II190075, Ar-\ntificial Intelligence Graduate School Program(KAIST))."}, {"title": "Appendices", "content": null}, {"title": "A. Discussion on Hyperparameter Choice", "content": "We compare the results on varying confidence threshold\n$C_{th}$ in Tab. 5. While the accuracy generally increases with\n$C_{th} = 0.9$, it also increases the cost of the entire process\nby performing more iterations per query on average. There-\nfore, we choose $C_{th} = 0.5$ and $0.7$ for the best trade-off\nbetween cost and accuracy."}, {"title": "B. Details on Datasets", "content": "Animal Kingdom We\nuse\nthe Animal Kingdom\ndataset [38] for our multiple choice question answering\ntask. This dataset includes videos of animals with action\nlabels such as Yawning and Struggling, covering 140 unique\nclasses. While it was originally built for action recognition\ntasks, we modified its format to suit a multiple-choice QA\ntask by pairing one true action label with four randomly\nchosen alternative labels. The dataset provides 24,004\nlabeled training examples and 6,096 test examples.\nSports-QA We employ the Sports-QA [28] dataset for\nopen-ended question answering task, which is designed for\nsports video question answering. This dataset includes vari-\nous sports, such as basketball, football, and gymnastics, and\nfeatures diverse question types like descriptions, timelines,\ncausalities, and hypothetical scenarios. The dataset includes\n56,385 training examples and 18,718 test examples.\nPitVQA We also use PitVQA [17], a dataset designed for\nVQA in endonasal pituitary surgery videos that requires\nspecific medical knowledge, for the open-ended question\nanswering task. PitVQA provides question-answer anno-\ntations at the frame level. For our experiments, we process\na sequence of 10 consecutive frames as the video input, with\nquestion-answer pairs drawn from the middle, fifth frame.\nThe dataset includes 75,010 training examples and 10,832\ntest examples.\nUCF-Crime UCF-Crime [44], which classifies the type\nof crime in security camera footage into 13 categories, is\nused for video classification task. We include all crime cat-\negories in the prompt, guiding the model to select the ap-\npropriate crime class for the given video. The dataset also\nincludes normal event videos as challenging negative exam-\nples. The official split of UCF-Crime provides four different\ntrain and test splits, with each split consisting of 532 train-\ning samples and 168 test samples. The result is reported\nas the average performance across the test sets of all four\nsplits.\nDrive&Act The Drive&Act dataset [37] is utilized for\nvideo classification tasks. This offers comprehensive labels\nfor driver behaviors inside vehicles, including action seg-\nmentation information captured in Kinect-IR videos. We\nextract each segment from the video and ask the model to\nrecognize the action. The official split of Drive&Act pro-\nvides three different train and test splits. Each split consists\nof around 2,000 labeled training examples and around 600\ntest examples. The result is reported as the average perfor-\nmance across the test sets of all three splits.\nCapERA For the video captioning task, we evaluate mod-\nels on the CapERA dataset [5], which is specifically curated\nfor describing scenes captured from an aerial perspective.\nCapERA provides concise captions for a range of scenarios\nviewed from above, including concerts, harvesting, and car\nracing, and consists of 1,473 labeled examples for training\nand 1,391 for testing."}, {"title": "C. Proof of Asymptotic Model Accuracy", "content": "Proposition (Asymptotic Model Accuracy). Let a(n) be\nthe expected accuracy of VIDEOICL with a maximum of\nn confidence-based iterations. Then,\n$lim_{n \\rightarrow \\infty} a(n) = \\frac{1}{1+\\frac{FPR}{TPR} \\frac{1-p_c}{p_c}}$,\nwhere TPR and FPR stand for the true positive rate (i.e.,\nrecall) and the false positive rate of the confidence estima-\ntion method, respectively.\nProof. At each iteration, there are three possibilities:\n\u2022 The model returns a correct response and is estimated to\nbe confident, with probability $p_c$ \u00b7 TPR.\n\u2022 The model returns an incorrect response, but is estimated\nto be confident, with a probability of $(1 \u2013 p_c)$ \u00b7 FPR.\n\u2022 The model returns a response, and is estimated to be un-\nconfident, occurring with probability $p_u := 1 \u2013 (p_c \u00b7\nTPR + (1-p_c) \u00b7 FPR)$."}, {"title": "D. Additional Results", "content": null}, {"title": "D.1. Additional Discussion on Main Results", "content": "For LoRA fine-tuning, we use a rank of 32 and train the\nmodel for 1 epoch on Animal Kingdom and PitVQA, 5\nepochs on UCF-Crime, and 2 epochs on CapERA. In Tab. 1,\nVIDEOICL outperforms the LoRA fine-tuned model on all\ndatasets except CapERA, showing that in-context examples\nare more effective than training in OOD video QA when\ndomain knowledge requires extensive data and training.\nInterestingly, the LLaVA-Video-72B model underper-\nforms compared to LLaVA-Video-7B model notably in\nvideo classification and captioning. For captioning, this is\nbecause 72B model often generates excessively long out-\nputs filled with irrelevant details. In video classification, we\nsuspect the limited capacity of 7B model may act as a form\nof regularization, helping it generalize better on OOD data,\nbut this needs further investigation.\nIn addition, VIDEOICL outperforms SIMRANK VOTE in\nTab. 1, highlighting the benefits of using confidence-based\naggregation instead of majority voting. VIDEOICL also\nachieves better results than SIMRANKONCE, showing that\nusing more examples leads to better performance. Lastly,\nSIMRANK VOTE outperforms RANDEXVOTE, demonstrat-\ning the effectiveness of selecting similar examples based on\nvideo and text features."}, {"title": "D.2. Additional Qualitative Results", "content": "In the following pages, we present qualitative results of\nVIDEOICL for each dataset. For each iteration, we use two\nexamples with maximum of 4 iterations, and the outputs of\nthe model are presented together with confidence scores."}, {"title": "E. Limitation", "content": "While VIDEOICL delivers remarkable performance, it does\nhave some limitations. First, VIDEOICL requires more\ntime compared to single-step in-context learning because\nit performs multiple rounds of inference. This additional\ncomputation may make it less suitable for applications that\ndemand low latency, such as real-time video analysis. How-\never, VIDEOICL mitigates this issue by using early ter-\nmination when the model confidence in its output is suffi-\nciently high, which significantly reduces computation time.\nIt is also much faster than training a model from scratch.\nSecond, VIDEOICL relies on having an example pool\nto select reference examples from. We demonstrate its ef-\nfectiveness on the UCF-Crime dataset, which contains only\n532 training samples, showing that VIDEOICL can per-\nform well even with a relatively small example pool. How-\never, we have not tested its performance with extremely\nsmall datasets. Considering the challenges of generating\nout-of-distribution video data, exploring the effectiveness\nof VIDEOICL with very limited examples is an important\ndirection for future research."}]}