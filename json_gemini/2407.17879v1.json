{"title": "HG-PIPE: Vision Transformer Acceleration with Hybrid-Grained Pipeline", "authors": ["Qingyu Guo", "Jiayong Wan", "Songqiang Xu", "Meng Li", "Yuan Wang"], "abstract": "Vision Transformer (ViT) acceleration with field programmable gate array (FPGA) is promising but challenging. Existing FPGA-based ViT accelerators mainly rely on temporal architectures, which process different operators by reusing the same hardware blocks and suffer from extensive memory access overhead. Pipelined architectures, either coarse-grained or fine-grained, unroll the ViT computation spatially for memory access efficiency. However, they usually suffer from significant hardware resource constraints and pipeline bubbles induced by the global computation dependency of ViT. In this paper, we introduce HG-PIPE, a pipelined FPGA accelerator for high-throughput and low-latency ViT processing. HG-PIPE features a hybrid-grained pipeline architecture to reduce on-chip buffer cost and couples the computation dataflow and parallelism design to eliminate the pipeline bubbles. HG-PIPE further introduces careful approximations to implement both linear and non-linear operators with abundant Lookup Tables (LUTs), thus alleviating resource constraints. On a ZCU102 FPGA, HG-PIPE achieves 2.78x better throughput and 2.52\u00d7 better resource efficiency than the prior-art accelerators, e.g., AutoViTAcc. With a VCK190 FPGA, HG-PIPE realizes end-to-end ViT acceleration on a single device and achieves 7118 images/s, which is 2.81\u00d7 faster than a V100 GPU.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have witnessed the wide adoption of Transformer models in the field of computer vision (CV)[6, 20, 31, 2, 32, 36]. While Vision Transformers (ViTs) achieve state-of-the-art (SOTA) performance compared to convolutional neural networks (CNNs), they suffer from a drastic increase in parameters and computation, which calls for more efficient acceleration.\nViT acceleration based on field-programmable gate array (FPGA) has been actively studied considering its efficiency and programmability [1]. FPGA-based ViT accelerators can be categorized into two architectural paradigms, i.e., temporal architecture [5, 4, 19, 37, 12] and pipelined architecture [42, 28, 39, 49]. Temporal architectures build general processing engines (PEs) for different operators and reuse the PEs temporally [4, 12]. Although temporal architectures simplify the design, they suffer from extensive off-chip memory access [24] and utilization problems. As shown in Figure 1, the temporal architecture (labeled \"GeMM\") is limited by bandwidth and can only achieve 1.1 TOP/s in our estimation. Pipelined architectures, in contrast, instantiate distinct PEs specialized for different operators and directly stream activations among PEs to reduce off-chip memory access [42, 26, 48, 49]. Since they allow concurrent processing of multiple layers, pipeline architectures hold the promise to enable efficient and low-latency ViT acceleration.\nHowever, to realize the efficiency promise of pipelined architectures faces major challenges. On the one hand, the multi-head attention (MHA) operation in ViTs involves computing the correlation of each token with all other tokens in an image. This introduces global computation dependency and causes either pipeline bubbles [7] or high buffer cost [42]. On the other hand, pipelined architectures compute multiple layers simultaneously and natively require more hardware resources, e.g., digital signal processing (DSP) blocks. The complex non-linear functions in ViT, including GeLU, Softmax, LayerNorm, etc, also require high-precision computation and DSP usage [33, 25, 47]. As shown in Figure 1, only 3.2 TOP/s can be achieved for a coarse-grained pipeline design due to the DSP limitation. If lookup tables (LUTs) are also utilized to construct the PEs, the roofline can be improved, but the design will be limited by bandwidth again and can only achieve 7.8 TOP/s.\nTo address the challenges, we introduce HG-PIPE in this paper. It combines the features of fine-grained and coarse-grained pipelines to eliminate the bubbles as well as reduce buffer costs. HG-PIPE keeps as many weights on-chip as possible to maximize the computational intensity. It further introduces careful approximations for activation functions to enable LUT-based processing to combat the DSP resource limits. As shown in Figure 1, HG-PIPE breaks through both the DSP roofline and bandwidth limitations, achieving a throughput of 17.8 TOP/s. The contribution of HG-PIPE can be summarized as follows:\n\u2022 HG-PIPE features a hybrid-grained pipelined architecture to simultaneously achieve low off-chip memory access, low buffer requirements, and negligible pipeline bubbles.\n\u2022 HG-PIPE leverages low bit-width quantization and introduces careful approximations for activation functions. The abundant LUT resources are utilized to process both linear and non-linear operators, achieving a higher roofline.\n\u2022 HG-PIPE demonstrates 2.72\u00d7 better throughput and 2.46\u00d7 better resource efficiency over prior-art accelerators on a ZCU102 FPGA. It realizes end-to-end ViT acceleration on a single VCK190 FPGA and achieves 7118 images/s."}, {"title": "2 BACKGROUND", "content": "2.1 Non-linear Functions in Transformers\nViTs involve various non-linear functions. Different from linear functions, it is challenging to implement them efficiently on FPGAs. GeLU is applied in the MLP block and can be computed as:\n$$GeLU(x) = \\frac{x}{2} \\left(1 + erf \\left(\\frac{x}{\\sqrt{2}}\\right)\\right)$$, where erf(x) = $\\frac{2}{\\sqrt{\\pi}} \\int_0^x exp(-t^2)dt$ (1)\nLayerNorm, as shown in Eq. 2, is applied in both the MHA block and the MLP block. The division and the square root operations are fused as the \"Rsqrt\" operator for hardware efficiency.\n$$LayerNorm(x) = \\frac{x - E[x]}{\\sqrt{Var(x)}} = (x - E[x]) \\cdot Rsqrt(Var(x))$$ (2)\nSoftmax, as shown in Eq. 3, is applied in the MHA block to normalize the attention scores. In Softmax, the exponential function (Exp) and the reciprocal function (Recip) are required.\n$$Softmax(x) = \\frac{Exp(x - x_{max})}{\\sum Exp(x - x_{max})}$$\nWe also regard the ReQuant operator in the quantized networks as a non-linear function. ReQuant can be computed as Eq. 4, where $a_{int}$ and $S_{fixed}$ denote the integer zero point and the fixed-point scaling factor, respectively. ReQuant requires high-precision multiplication and therefore consumes DSPs.\n$$ReQuant(x) = clamp \\left(\\lfloor(x - a_{int}) \\cdot S_{fixed}\\rceil, Q_{min}, Q_{max}\\right)$$\nTo implement these non-linear functions, three methods have been developed by prior works:\n\u2022 Floating point implementation employs 32-bit or 16-bit floating point computation. Despite its simplicity and precision, it demands substantial DSPs and LUTs.\n\u2022 Fixed-point polynomial approximation uses low-order polynomials to implement non-linear functions within specified ranges [14]. This method is a compromise between computational complexity and accuracy.\n\u2022 Lookup table method involves discretizing the function input range and recording the output. While it reduces DSP usage, it usually requires Block RAMS (BRAMs) for accurate sampling[34].\n2.2 FPGA-based ViT Acceleration\nThe FPGA-based ViT accelerators can be categorized into two architectures: temporal architecture and pipelined architecture. As shown in Figure 2(a), temporal architectures leverage unified PEs to"}, {"title": "3 CHALLENGES", "content": "Although ViT has outstanding performance, its complexity brings challenges for FPGA accelerator design as depicted in Figure 3. These challenges motivated us to propose HG-PIPE.\nChallenge 1: Dataflow Design Dilemma. The self-attention mechanism in ViTs introduces global computation dependencies, leading to challenges in data locality and dataflow design. As illustrated in Figure 3 (1a), the transpose operation within the self-attention block breaks the data access pattern for the V matrix, hampering the continuity needed for fine-grained pipelines and necessitating coarse-grained pipelines. However, as shown in Figure 3 (1b), the coarse-grained approach causes gigantic activation buffering costs, exacerbated by residual connections that require multiple Ping Pong buffers to avoid deadlock. Specifically, in the Deit-tiny network, buffering one residual tensor consumes 14 BRAMs. One attention block in a coarse-grained pipeline requires 6 PIPO stages (168 BRAMs) just for the residual path. This extensive buffering demand is impractical for FPGA platforms[42].\nChallenge 2: High DSP Usage. The extensive non-linear functions in ViT present challenges for efficient FPGA implementation. In our HLS synthesis experiments, naive floating-point implementations of functions like Exp, Rsqrt, and Recip are DSP-intensive, consuming 7, 8, and 9 DSPs respectively. The GeLU function is even more DSP-intensive, requiring 26 DSPs. The ReQuant function additionally uses 1 DSP. In the estimation, implementing these non-linear functions in a Deit-tiny model requires 3024 DSPs, exceeding the DSP capacity of a VCK190 FPGA. Therefore, reducing DSP usage in ViT accelerators is crucial for FPGA implementation."}, {"title": "4 MAIN METHODS", "content": "4.1 Overview of HG-PIPE\nThe overview of the HG-PIPE accelerator is depicted in Figure 4. It is integrated into an FPGA System-on-Chip (SoC) framework, with multiple peripheral components, including external memory, host system, and network-on-chip (NoC).\nHG-PIPE stands out with its fully pipelined architecture across all layers, eliminating external memory access for intermediate activations or weights. It directly processes incoming data from the Direct Memory Access (DMA) module, channeling the final output back to external memory. HG-PIPE utilizes dedicated modules for each component, avoiding time division multiplexing. This design principle extends from the Inter-Block to the Intra-Block Level.\nTo achieve sufficient throughput, HG-PIPE incorporates over 20,000 MAC units. Managing such a vast array of MAC units typically complicates control logic. Centralized control often leads to extended routing nets that hamper timing. As seen in FixyFPGA [26], the accelerator utilizes over 1336k LUTs and only reaches a 132MHz clock frequency. To mitigate this, HG-PIPE adopts an asynchronous, decentralized pipeline strategy, where each stage is controlled by its own FSM. With handshakes on the AXI-Stream interface, modules are completely decoupled. The design incorporates FIFOs within these connections to cover data generation fluctuations, avoiding deadlocks in the data flow. As illustrated, data is transferred in the form of tiled tensors.\n4.2 Hybrid-Grained Pipeline\nTo address the challenges outlined in Sec. 3, we developed a hybrid-grained pipeline to effectively manage computing and buffering"}, {"title": "4.3 Parallelism Design", "content": "Matrix Multiplication (MM) is the primary operator used in ViT computations. We implement tiled MM using an Output Stationary (OS) dataflow to minimize partial sum storage costs, as illustrated in Figure 8. MM involves three nested loops: Token, Output Channel, and Input Channel. All loops are tiled to enhance data locality. For clarity, notations are detailed in the Table. We meticulously design the parallelism of all the modules for two reasons: pipeline balance and BRAM utilization efficiency.\n4.3.1 Pipeline Balance. The pipeline balance is essential for the accelerator's performance. The Initiation Interval (II) of the whole pipeline is the maximum of the II of all the pipeline stages. As shown in Figure 9a(1), if the cycles are not balanced, the bubbles will be generated. Therefore, by allocating more computing resources to the Matmul1 module, the pipeline can be balanced in Figure 9a(2). By adjusting the parallelism, we try to make the II of the pipeline stages balanced. The calculation of II is detailed in Table 1 footnotes.\n4.3.2 BRAM Utilization Efficiency. BRAM is a scarce resource on FPGA for AI applications. It is also a critical resource for our design since HG-PIPE needs to freeze as much of the weights on-chip as possible to reduce external memory access. The parallelism design can directly decide the memory layout of the weights in BRAM (as shown in Figure 9b), and improper memory layout will lead to BRAM waste. In Layout1, the weight w requires two BRAMs to accommodate it. However, by scaling $C_{IP}$ to half of its original value in Layout2, there will only one BRAM be required. As illustrated in\n4.3.3 Parallelism Design Results. The parallelism design results are shown in Table 1, in which we tried to achieve the two goals at the same time. As demonstrated, most modules have closely matched IIs. To save DSP resources, we choose the non-linear operators to be the II bottleneck (57624 of the Softmax module). For the Residual Add module, we keep $T_P$ = 2 to simplify the design, leading to smaller II values. Although bubbles exist in Residual, this is not a big waste since it only requires 0.038 MOPs. For the BRAM utilization efficiency, we achieved 100% efficiency for all the modules that employ static weight (QKV Gen, Output Proj in MHA block and MatMul1, MatMul2 in MLP block)."}, {"title": "4.4 Efficient and Accurate LUT-based Processing", "content": "ViTs incorporate complex non-linear functions, previously discussed in Sec. 2.1 and Sec. 3. Accurately computing these functions is crucial for maintaining model performance, but full-precision"}, {"title": "4.4.1 LUT-based MAC unit", "content": "The implementation of multiplication operations using LUTs is a technique in FPGA-based computations. Consider a scenario where operands of multiplication are quantized to 3 bits. In this case, the multiplication operation can be decomposed into six boolean functions, with each function consuming 6 bits to produce a single bit of the multiplication. As a result, only 6 LUT-6 are required. The adoption of LUTs for MAC units can significantly enhance the computational capabilities of an FPGA, effectively raising its computation roofline. This approach has been successfully utilized in several accelerator works [19, 26, 41, 48]. We have incorporated this technique in our design as well."}, {"title": "4.4.2 Power-of-Two Index Approximation", "content": "In the LUT method, the index computation is essential. The process is similar to quantization in Eq. 4: discretizing continuously distributed input values. In the traditional table method, the index is computed as Eq. 5.\n$$index = (data - \\alpha) \\times \\frac{2^{n-1}}{\\beta - \\alpha}$$ (5)\nin which $(\\alpha, \\beta)$ is the data range of input tensor, and n is the address width of the table. However, this method will require a DSP for the multiplication. This is contradictory to the motivation of the table method: reduce DSP usage. The similarity between index computation and quantization inspired us to introduce a quantization technique into our LUT methods: Power-of-Two Quantization [27, 3, 48]. This method estimates the scaling factor with its nearest Power-of-Two (PoT) value, therefore simplifying the high-precision multiplication to a bit shifting. The estimation is shown in Eq. 6.\n$$index_{poT} = (data - \\alpha) >> S_{poT}, where S_{poT} = \\lceil log_2(\\frac{\\beta - \\alpha}{2^{n-1}})\\rceil$$ (6)\nOur estimation is a bit different from vanilla PoT Quantization. We apply a ceiling instead of rounding to avoid index overflowing. The whole process is shown in Figure 10a. The blue bars show the data distribution of the input tensor. Taking the min and max values, the normal scaling factor can be calculated, and the black dashed arrows show the normal index mapping, aligning the boundaries and mapping $\\beta$ to the highest index (2\u2033 \u2013 1). On the contrary, as shown by the red arrows, PoT estimation will not guarantee boundary alignment, but it will make sure the scaling happens as a static bit shifting."}, {"title": "4.4.3 GeLU-ReQuant Operator Fusion", "content": "In a quantized network, quantizers are inserted before all MMs to improve MAC efficiency. Fusing quantizers with preceding operators simplifies the logic and reduces LUT consumption. The fusion involves sampling a combined transfer curve, depicted in Figure 10b. The left sub-figure represents the GeLU function, the middle one the ReQuant function with 4-bit precision, and the right one the resulting fused curve. The sampling will happen on the combined curve."}, {"title": "4.4.4 Implement ReQuant as Table", "content": "Besides GeLU fusions, numerous ReQuant operators cannot be fused and also consume DSP"}, {"title": "4.4.5 Joint Table Range Calibration", "content": "In the implementation result of the ReQuant table and GeLU-ReQuant table, we discovered that there are many repeated entries generated from the clamping behavior (Eq. 4) on the two ends, causing wastes of representative ability. This is demonstrated in the middle sub-figure of Figure 10c, in which the repeated entries on the two ends are colored orange. We introduce Joint Table Range Calibration to reduce redundancy in the table. This method aligns the input data distribution with the table contents to optimize implementation. It iteratively identifies the Least Significant Index (LSI) and Most Significant Index (MSI) to recalculate the data range and updates the table until the data"}, {"title": "4.4.6 Segmented Table for High Dynamic Range Recip", "content": "The Recip function in the Softmax module exhibits a high dynamic range in our statistic experiments. To accurately sample the function results on the sheer range between 0 to 1, the table needs to be very large. Initially, storing the reciprocal function would have needed an entire BRAM bank (depth=1024, width=36) to maintain accuracy. To minimize BRAM usage, we exploited the function's inherent properties and segmented it into two parts, each owning an independent scaling factor. We empirically divide the input range at the first 1/8 for the steep part and the remainder for the flat. This approach is visualized in Figure 10d. The orange line is the abs error of the original LUT implementation of Recip, and the blue one is the segmented implementation with the segmentation pivot annotated to it. With more entries between 0 and 1, the sampling is more accurate, reducing Mean Squared Error (MSE) from 0.032 to 0.0034."}, {"title": "4.4.7 Inversed Exponential Table", "content": "In our experiment, we observed that the PoT approximation on Exp will cause huge accuracy degradation. The possible explanation is that in the calculation of Softmax, each element is subtracted by the maximum value in its group to maintain numerical stability. Therefore, the max value of the input range is anchored to 0. However, the approximation described in Sec. 4.4.2 takes the min value as the zero point. The moving of the anchor point may cause an inaccurate approximation of the sensitive anchor values. The solution is simple: we make \u03b2 the zero point and modify the index computation from Eq. 6 to Eq. 7:\n$$index_{poT} = (\\beta - data) >> S_{poT}, where S_{poT} = \\lceil log_2(\\frac{\\beta - \\alpha}{2^{n-1}})\\rceil$$ (7)"}, {"title": "5 EXPERIMENTS", "content": "5.1 Experiment Setup\nTo evaluate our design, we selected the Deit-tiny model and Deit-small model, following our baselines AutoViTAcc [19], HeatViT [5] and SSR[49]. We tested on two FPGA platforms: ZCU102 and VCK190. The ZCU102 allows direct comparisons with prior works, while the VCK190 supports full deployment of the whole network. Throughput was measured using the PYNQ framework, and power consumption was assessed with Xilinx's BEAM tool.\n5.2 Timing Diagram of the Pipeline\nWe simulated our design to generate a timing diagram of blocks, shown in Figure 12. The accelerator sequentially loads input tensor tiles. As Imagel's loading completes, Image2's begins, indicating overlapped execution. The MHA block exhibits coarse-grained buffering, causing a slight delay in outputting the first tile. For the third image, the stable II measured was 57,624 cycles as expected, validating the hybrid-grained pipeline's effectiveness. The trace also reveals that the total processing time for Image1 is 824,843 cycles or 1.94ms. Using the stable II, the computed latency is 0.136 ms, equating to an ideal frame rate of 7353 images/s."}, {"title": "5.3 Comparison with Related Works", "content": "We benchmarked HG-PIPE against leading works, as detailed in Table 2. Our hybrid-grained pipeline design demonstrated significant improvements in throughput, resource efficiency, and power efficiency. On the ZCU102 platform, compared to AutoViTAcc, HG-PIPE achieves a LUT efficiency of 18.55 GOPs/kLUT, which is 2.52 times higher under the same 4-bit quantization and platform conditions. On the VCK190 platform, HG-PIPE achieved a throughput of 7118 images/s and 17.8 TOPs/s, which is 96.8% of the ideal 7353 images/s throughput. Compared to the V100 GPU (2529 FPS), HG-PIPE outperforms it by 2.81 times. For DSP usage, HG-PIPE drastically"}, {"title": "5.4 LUT Optimization Techniques", "content": "We evaluated the impact of our LUT optimization techniques in reducing DSP usage and maintaining network accuracy in Figure 11a. Initially, the network had 74.5% accuracy with a prohibitive DSP cost of 14304 [38]. Implementing advanced quantization techniques [13] allowed us to implement MAC units with LUTs, reducing DSP usage from 14304 to 3024. Further applying Power-of-Two LUTs for non-linear functions cut DSP usage to 312, albeit with a significant accuracy loss. The crucial Inverted Exp Table technique significantly restored accuracy, with subsequent optimizations gradually recovering it while keeping DSP usage constant. We also conducted an ablation study to pinpoint the effects of optimizations detailed in Figure 11b. Also, Figure 11c demonstrates the table sizes and the resource reduction effects. As listed, LUT costs of non-linear functions are significantly reduced, while DSP usage is eliminated."}, {"title": "5.5 Test Environment and Device View", "content": "Our test environment is demonstrated in Figure 13. The VCK190 board is connected to the host PC via Ethernet. The screen of the host PC displays the BEAM and Jupyter Notebook GUI, highlighting real-time power consumption and frame rate. On the right side, the implementation device view is elaborated with each module annotated. The device view contains 26 neural network blocks, including PatchEmbed, Classification Head, 12 MHA blocks and 12 MLP blocks. The placement is automatically generated by Vivado, thus appearing in a random order. Other modules such as PS, PLL,"}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce HG-PIPE, a hybrid-grained pipelined ViT accelerator optimized for FPGAs. By combining the advantages of fine-grained and coarse-grained designs, our approach achieves low latency and high resource efficiency at the same time, reducing the on-chip activation buffering cost by 83.3%. We have implemented optimizations including PoT table index computation and LUT-based ReQuant, which collectively reduce DSP usage by 89.6% without sacrificing accuracy. On the VCK190, it delivers real-time ViT processing at 7118 FPS, equivalent to 17.8 TOP/s."}]}