{"title": "HG-PIPE: Vision Transformer Acceleration with Hybrid-Grained Pipeline", "authors": ["Qingyu Guo", "Jiayong Wan", "Songqiang Xu", "Meng Li", "Yuan Wang"], "abstract": "Vision Transformer (ViT) acceleration with field programmablegate array (FPGA) is promising but challenging. Existing FPGAbased ViT accelerators mainly rely on temporal architectures, whichprocess different operators by reusing the same hardware blocksand suffer from extensive memory access overhead. Pipelined ar-chitectures, either coarse-grained or fine-grained, unroll the ViTcomputation spatially for memory access efficiency. However, theyusually suffer from significant hardware resource constraints andpipeline bubbles induced by the global computation dependencyof ViT. In this paper, we introduce HG-PIPE, a pipelined FPGAaccelerator for high-throughput and low-latency ViT processing.HG-PIPE features a hybrid-grained pipeline architecture to reduceon-chip buffer cost and couples the computation dataflow and parallelism design to eliminate the pipeline bubbles. HG-PIPE furtherintroduces careful approximations to implement both linear andnon-linear operators with abundant Lookup Tables (LUTs), thus alleviating resource constraints. On a ZCU102 FPGA, HG-PIPE achieves2.78x better throughput and 2.52\u00d7 better resource efficiency thanthe prior-art accelerators, e.g., AutoViTAcc. With a VCK190 FPGA,HG-PIPE realizes end-to-end ViT acceleration on a single deviceand achieves 7118 images/s, which is 2.81\u00d7 faster than a V100 GPU.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have witnessed the wide adoption of Transformermodels in the field of computer vision (CV). While Vision Transformers (ViTs) achieve state-of-the-art (SOTA)performance compared to convolutional neural networks (CNNs),they suffer from a drastic increase in parameters and computation,which calls for more efficient acceleration.\nViT acceleration based on field-programmable gate array (FPGA)has been actively studied considering its efficiency and programmability [1]. FPGA-based ViT accelerators can be categorized into twoarchitectural paradigms, i.e., temporal architecture and pipelined architecture . Temporal architecturesbuild general processing engines (PEs) for different operators andreuse the PEs temporally [4, 12]. Although temporal architecturessimplify the design, they suffer from extensive off-chip memoryaccess and utilization problems. As shown in Figure 1, the temporal architecture (labeled \"GeMM\") is limited by bandwidth andcan only achieve 1.1 TOP/s in our estimation. Pipelined architectures, in contrast, instantiate distinct PEs specialized for differentoperators and directly stream activations among PEs to reduceoff-chip memory access . Since they allow concurrent processing of multiple layers, pipeline architectures hold thepromise to enable efficient and low-latency ViT acceleration.\nHowever, to realize the efficiency promise of pipelined architectures faces major challenges. On the one hand, the multi-headattention (MHA) operation in ViTs involves computing the correlation of each token with all other tokens in an image. This introduces global computation dependency and causes either pipelinebubbles [7] or high buffer cost [42]. On the other hand, pipelinedarchitectures compute multiple layers simultaneously and nativelyrequire more hardware resources, e.g., digital signal processing(DSP) blocks. The complex non-linear functions in ViT, includingGeLU, Softmax, LayerNorm, etc, also require high-precision computation and DSP usage . As shown in Figure 1, only 3.2TOP/s can be achieved for a coarse-grained pipeline design dueto the DSP limitation. If lookup tables (LUTs) are also utilized toconstruct the PEs, the roofline can be improved, but the design willbe limited by bandwidth again and can only achieve 7.8 TOP/s.\nTo address the challenges, we introduce HG-PIPE in this paper. It combines the features of fine-grained and coarse-grainedpipelines to eliminate the bubbles as well as reduce buffer costs.HG-PIPE keeps as many weights on-chip as possible to maximizethe computational intensity. It further introduces careful approximations for activation functions to enable LUT-based processingto combat the DSP resource limits. As shown in Figure 1, HG-PIPE breaks through both the DSP roofline and bandwidth limitations, achieving a throughput of 17.8 TOP/s. The contribution ofHG-PIPE can be summarized as follows:"}, {"title": "2 BACKGROUND", "content": "ViTs involve various non-linear functions. Different from linearfunctions, it is challenging to implement them efficiently on FPGAs.GeLU is applied in the MLP block and can be computed as:\n$GeLU(x) = \\frac{x}{2} (1 + erf(\\frac{x}{\\sqrt{2}}))$, where $erf(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x exp(-t^2)dt$\nLayerNorm, as shown in Eq. 2, is applied in both the MHA blockand the MLP block. The division and the square root operations arefused as the \"Rsqrt\" operator for hardware efficiency.\n$LayerNorm(x) = \\frac{x - E[x]}{\\sqrt{Var(x)}} = (x - E[x]) Rsqrt(Var(x))$\nSoftmax, as shown in Eq. 3, is applied in the MHA block tonormalize the attention scores. In Softmax, the exponential function(Exp) and the reciprocal function (Recip) are required.\n$Softmax(x) = \\frac{Exp(x - x_{max})}{\\sum Exp(x - x_{max})}$\nWe also regard the ReQuant operator in the quantized networksas a non-linear function. ReQuant can be computed as Eq. 4, where$a_{int}$ and $S_{fixed}$ denote the integer zero point and the fixed-pointscaling factor, respectively. ReQuant requires high-precision multiplication and therefore consumes DSPs.\n$ReQuant(x) = clamp([(x - a_{int})\\cdot S_{fixed}], Q_{min}, Q_{max})$\nTo implement these non-linear functions, three methods havebeen developed by prior works:\nFPGA-based ViT Acceleration\nThe FPGA-based ViT accelerators can be categorized into two architectures: temporal architecture and pipelined architecture. Asshown in Figure 2(a), temporal architectures leverage unified PEs to"}, {"title": "3 CHALLENGES", "content": "Although ViT has outstanding performance, its complexity bringschallenges for FPGA accelerator design as depicted in Figure 3.These challenges motivated us to propose HG-PIPE."}, {"title": "4 MAIN METHODS", "content": "The overview of the HG-PIPE accelerator is depicted in Figure4. It is integrated into an FPGA System-on-Chip (SoC) framework,with multiple peripheral components, including external memory,host system, and network-on-chip (NoC).\nHG-PIPE stands out with its fully pipelined architecture acrossall layers, eliminating external memory access for intermediateactivations or weights. It directly processes incoming data from theDirect Memory Access (DMA) module, channeling the final outputback to external memory. HG-PIPE utilizes dedicated modules foreach component, avoiding time division multiplexing. This designprinciple extends from the Inter-Block to the Intra-Block Level.\nTo achieve sufficient throughput, HG-PIPE incorporates over20,000 MAC units. Managing such a vast array of MAC units typically complicates control logic. Centralized control often leads to extended routing nets that hamper timing. As seen in FixyFPGA [26],the accelerator utilizes over 1336k LUTs and only reaches a 132MHzclock frequency. To mitigate this, HG-PIPE adopts an asynchronous,decentralized pipeline strategy, where each stage is controlled by itsown FSM. With handshakes on the AXI-Stream interface, modulesare completely decoupled. The design incorporates FIFOs within these connections to cover data generation fluctuations, avoidingdeadlocks in the data flow. As illustrated, data is transferred in theform of tiled tensors."}, {"title": "4.2 Hybrid-Grained Pipeline", "content": "To address the challenges outlined in Sec. 3, we developed a hybrid-"}, {"title": "4.3 Parallelism Design", "content": "Matrix Multiplication (MM) is the primary operator used in ViTcomputations. We implement tiled MM using an Output Stationary(OS) dataflow to minimize partial sum storage costs, as illustrated inFigure 8. MM involves three nested loops: Token, Output Channel,and Input Channel. All loops are tiled to enhance data locality. Forclarity, notations are detailed in the Table. We meticulously designthe parallelism of all the modules for two reasons: pipeline balanceand BRAM utilization efficiency. 1\nFPGA for AI applications. It is also a critical resource for our designsince HG-PIPE needs to freeze as much of the weights on-chip aspossible to reduce external memory access. The parallelism designcan directly decide the memory layout of the weights in BRAM(as shown in Figure 9b), and improper memory layout will lead toBRAM waste. In Layout1, the weight $w$ requires two BRAMs toaccommodate it. However, by scaling $C_{IP}$ to half of its original valuein Layout2, there will only one BRAM be required. As illustrated in"}, {"title": "4.4 Efficient and Accurate LUT-based Processing", "content": "ViTs incorporate complex non-linear functions, previously discussed in Sec. 2.1 and Sec. 3. Accurately computing these functionsis crucial for maintaining model performance, but full-precision"}, {"title": "4.4.1 LUT-based MAC unit", "content": "The implementation of multiplicationoperations using LUTs is a technique in FPGA-based computations.Consider a scenario where operands of multiplication are quantizedto 3 bits. In this case, the multiplication operation can be decom-posed into six boolean functions, with each function consuming 6bits to produce a single bit of the multiplication. As a result, only6 LUT-6 are required. The adoption of LUTs for MAC units cansignificantly enhance the computational capabilities of an FPGA,effectively raising its computation roofline. This approach has beensuccessfully utilized in several accelerator works .We have incorporated this technique in our design as well."}, {"title": "4.4.2 Power-of-Two Index Approximation", "content": "In the LUT method, theindex computation is essential. The process is similar to quantization in Eq. 4: discretizing continuously distributed input values. Inthe traditional table method, the index is computed as Eq. 5.\n$index = (data - \\alpha) \\times \\frac{2^{n-1}}{\\beta - \\alpha}$\nin which $(\\alpha, \\beta)$ is the data range of input tensor, and n is the addresswidth of the table. However, this method will require a DSP forthe multiplication. This is contradictory to the motivation of thetable method: reduce DSP usage. The similarity between index computation and quantization inspired us to introduce a quantizationtechnique into our LUT methods: Power-of-Two Quantization .This method estimates the scaling factor with its nearestPower-of-Two (PoT) value, therefore simplifying the high-precisionmultiplication to a bit shifting. The estimation is shown in Eq. 6.\n$index_{poT} = (data - \\alpha) >> S_{POT}$, where $S_{POT} = log_2(\\frac{\\beta - \\alpha}{2^{n-1}})$$\nOur estimation is a bit different from vanilla PoT Quantization.We apply a ceiling instead of rounding to avoid index overflowing.The whole process is shown in Figure 10a. The blue bars show thedata distribution of the input tensor. Taking the min and max values,the normal scaling factor can be calculated, and the black dashedarrows show the normal index mapping, aligning the boundariesand mapping $\\beta$ to the highest index ($2^n$ \u2013 1). On the contrary,as shown by the red arrows, PoT estimation will not guaranteeboundary alignment, but it will make sure the scaling happens as astatic bit shifting."}, {"title": "4.4.3 GeLU-ReQuant Operator Fusion", "content": "In a quantized network,quantizers are inserted before all MMs to improve MAC efficiency.Fusing quantizers with preceding operators simplifies the logicand reduces LUT consumption. The fusion involves sampling acombined transfer curve, depicted in Figure 10b. The left sub-figurerepresents the GeLU function, the middle one the ReQuant functionwith 4-bit precision, and the right one the resulting fused curve.The sampling will happen on the combined curve."}, {"title": "4.4.4 Implement ReQuant as Table", "content": "Besides GeLU fusions, numerous ReQuant operators cannot be fused and also consume DSP"}, {"title": "4.4.5 Joint Table Range Calibration", "content": "In the implementation resultof the ReQuant table and GeLU-ReQuant table, we discovered thatthere are many repeated entries generated from the clamping behavior (Eq. 4) on the two ends, causing wastes of representativeability. This is demonstrated in the middle sub-figure of Figure 10c,in which the repeated entries on the two ends are colored orange.We introduce Joint Table Range Calibration to reduce redundancyin the table. This method aligns the input data distribution with thetable contents to optimize implementation. It iteratively identifiesthe Least Significant Index (LSI) and Most Significant Index (MSI)to recalculate the data range and updates the table until the data"}, {"title": "4.4.6 Segmented Table for High Dynamic Range Recip", "content": "The Recipfunction in the Softmax module exhibits a high dynamic rangein our statistic experiments. To accurately sample the functionresults on the sheer range between 0 to 1, the table needs to bevery large. Initially, storing the reciprocal function would haveneeded an entire BRAM bank (depth=1024, width=36) to maintainaccuracy. To minimize BRAM usage, we exploited the function'sinherent properties and segmented it into two parts, each owningan independent scaling factor. We empirically divide the input rangeat the first 1/8 for the steep part and the remainder for the flat. Thisapproach is visualized in Figure 10d. The orange line is the abserror of the original LUT implementation of Recip, and the blueone is the segmented implementation with the segmentation pivotannotated to it. With more entries between 0 and 1, the samplingis more accurate, reducing Mean Squared Error (MSE) from 0.032to 0.0034."}, {"title": "4.4.7 Inversed Exponential Table", "content": "In our experiment, we observedthat the PoT approximation on Exp will cause huge accuracy degradation. The possible explanation is that in the calculation of Softmax,each element is subtracted by the maximum value in its group tomaintain numerical stability. Therefore, the max value of the inputrange is anchored to 0. However, the approximation described inSec. 4.4.2 takes the min value as the zero point. The moving ofthe anchor point may cause an inaccurate approximation of thesensitive anchor values. The solution is simple: we make $\\beta$ the zeropoint and modify the index computation from Eq. 6 to Eq. 7:\n$index_{poT} = (\\beta \u2013 data) >> S_{POT}$, where $S_{POT} = log_2(\\frac{\\beta-\\alpha}{2^{n-1}})$"}, {"title": "5 EXPERIMENTS", "content": "To evaluate our design, we selected the Deit-tiny model and Deitsmall model, following our baselines AutoViTAcc [19], HeatViT[5] and SSR[49]. We tested on two FPGA platforms: ZCU102 andVCK190. The ZCU102 allows direct comparisons with prior works,while the VCK190 supports full deployment of the whole network.Throughput was measured using the PYNQ framework, and powerconsumption was assessed with Xilinx's BEAM tool."}, {"title": "5.2 Timing Diagram of the Pipeline", "content": "We simulated our design to generate a timing diagram of blocks,shown in Figure 12. The accelerator sequentially loads input ten-sor tiles. As Imagel's loading completes, Image2's begins, indicating overlapped execution. The MHA block exhibits coarse-grainedbuffering, causing a slight delay in outputting the first tile. For thethird image, the stable II measured was 57,624 cycles as expected,validating the hybrid-grained pipeline's effectiveness. The tracealso reveals that the total processing time for Image1 is 824,843cycles or 1.94ms. Using the stable II, the computed latency is 0.136ms, equating to an ideal frame rate of 7353 images/s."}, {"title": "5.3 Comparison with Related Works", "content": "We benchmarked HG-PIPE against leading works, as detailed inTable 2. Our hybrid-grained pipeline design demonstrated significant improvements in throughput, resource efficiency, and powerefficiency. On the ZCU102 platform, compared to AutoViTAcc, HG-PIPE achieves a LUT efficiency of 18.55 GOPs/kLUT, which is 2.52times higher under the same 4-bit quantization and platform conditions. On the VCK190 platform, HG-PIPE achieved a throughput of7118 images/s and 17.8 TOPs/s, which is 96.8% of the ideal 7353 images/s throughput. Compared to the V100 GPU (2529 FPS), HG-PIPEoutperforms it by 2.81 times. For DSP usage, HG-PIPE drastically"}, {"title": "5.4 LUT Optimization Techniques", "content": "We evaluated the impact of our LUT optimization techniques in reducing DSP usage and maintaining network accuracy in Figure 11a.Initially, the network had 74.5% accuracy with a prohibitive DSPcost of 14304 [38]. Implementing advanced quantization techniques[13] allowed us to implement MAC units with LUTs, reducing DSPusage from 14304 to 3024. Further applying Power-of-Two LUTs fornon-linear functions cut DSP usage to 312, albeit with a significantaccuracy loss. The crucial Inverted Exp Table technique significantly restored accuracy, with subsequent optimizations graduallyrecovering it while keeping DSP usage constant. We also conducteda"}, {"title": "5.5 Test Environment and Device View", "content": "Our test environment is demonstrated in Figure 13. The VCK190board is connected to the host PC via Ethernet. The screen of thehost PC displays the BEAM and Jupyter Notebook GUI, highlightingreal-time power consumption and frame rate. On the right sidethe implementation device view is elaborated with each moduleannotated. The device view contains 26 neural network blocks,including PatchEmbed, Classification Head, 12 MHA blocks and 12MLP blocks. The placement is automatically generated by Vivado,thus appearing in a random order. Other modules such as PS, PLL,"}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce HG-PIPE, a hybrid-grained pipelinedViT accelerator optimized for FPGAs. By combining the advantagesof fine-grained and coarse-grained designs, our approach achieveslow latency and high resource efficiency at the same time, reducingthe on-chip activation buffering cost by 83.3%. We have implemented optimizations including PoT table index computation andLUT-based ReQuant, which collectively reduce DSP usage by 89.6%without sacrificing accuracy. On the VCK190, it delivers real-timeViT processing at 7118 FPS, equivalent to 17.8 TOP/s."}]}