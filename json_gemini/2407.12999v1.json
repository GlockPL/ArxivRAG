{"title": "Securing the Future of GenAI: Policy and Technology", "authors": ["Mihai Christodorescu", "Ryan Craven", "Soheil Feizi", "Neil Gong", "Mia Hoffmann", "Somesh Jha", "Zhengyuan Jiang", "Mehrdad Saberi Kamarposhti", "John Mitchell", "Jessica Newman", "Emelia Probasco", "Yanjun Qi", "Khawaja Shams", "Matthew Turek"], "abstract": "The rise of Generative AI (GenAI) brings about transformative potential across sectors, but its dual-use nature also\namplifies risks. Governments globally are grappling with the challenge of regulating GenAI, balancing innovation\nagainst safety. China, the United States (US), and the European Union (EU) are at the forefront with initiatives like\nthe Management of Algorithmic Recommendations, the Executive Order, and the AI Act, respectively. However, the\nrapid evolution of GenAI capabilities often outpaces the development of comprehensive safety measures, creating a\ngap between regulatory needs and technical advancements.\nA workshop co-organized by Google, University of Wisconsin, Madison (UW-Madison), and Stanford University\naimed to bridge this gap between GenAI policy and technology. The diverse stakeholders of the GenAI space-from\nthe public and governments to academia and industry-make any safety measures under consideration more complex,\nas both technical feasibility and regulatory guidance must be realized. This paper summarizes the discussions during\nthe workshop which addressed questions, such as: How regulation can be designed without hindering technological\nprogress? How technology can evolve to meet regulatory standards? The interplay between legislation and technology\nis a very vast topic, and we don't claim that this paper is a comprehensive treatment on this topic. This paper is meant\nto capture findings based on the workshop, and hopefully, can guide discussion on this topic.", "sections": [{"title": "1 Introduction", "content": "The Cambrian explosion of Generative-AI (GenAI) capabilities and applications highlights the promise of broad im-\npact GenAI holds in a variety of domains. At the same time, the dual-use characteristics of this technology introduce\nnew risks and enhance existing risks. Governments around the world are taking note of the rapid expansion in terms\nof capabilities, applications, and risks and have introduced regulatory frameworks for GenAI, with the United States'\nExecutive Order and the European Union's AI Act as some of the most recent such developments. Regulatory bodies\nare faced with a balancing act, where too-strict of regulation can stifle the technical development and economic growth\nof GenAI, while loose regulation can fail to provide sufficient guardrails around the impact of GenAI on society. Fur-\nther complicating the matter is the fast pace of research and development in GenAI, where new technical capabilities\nare launched seemingly every day but are not yet comprehensive in terms of safety guarantees.\nAs an example, the US Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial\nIntelligence, issued in October 2023 [76], highlights a number of areas at high risk due to the application of GenAI,\nranging from nuclear, biological, and chemical weapons, to critical infrastructure and energy security, and financial"}, {"title": "2 Regulatory-Policy Considerations for GenAI", "content": "GenAI's seemingly broad impact across the whole range of human activities has attracted the attention of regulatory\nbodies in many countries in order to mitigate present and future risks of developing and deploying GenAI. A primary\nquestion is what types of GenAI uses and risks are of interest to regulators, and, relatedly, whether different regulators\nfocus on different aspects of GenAI. In this section we summarize the workshop discussion on policies emerging\nin the European Union, the People's Republic of China, and the United States, as well as the governance efforts in\nmultilateral settings (e.g., G7). Understanding the scope of such policies provides insights into the types of GenAI\ntechnologies that can be applied (or need to be developed) to achieve the required policies.\nWhen GenAI falls short of perfect safety, it may be possible to design safeguards into the processes and procedures\naround using GenAI. The workshop participants found it useful to delve into how the (US) military addresses risk\nmanagement in their own domain, where lethal technologies are omnipresent and without many safety options. In the\nlast subsection we present a discussion of lessons from military risk management."}, {"title": "2.1 The Policy Landscape", "content": "Some governments around the world had made AI regulation a priority long before the release of OpenAI's ChatGPT\nin November 2022. For others, the sudden emergence of what seemed like a revolutionary technology reinvigorated\ninterest in regulatory oversight. Policy responses emerged rapidly, and varied widely in terms of scope and subject.\nThese differences are revealing. The approaches taken reflect how differently governments assess and prioritize the\nrisks associated with GenAI.\nUnderstanding the international policy landscape is important in its own right. In addition, insight into the un-\nderlying drivers of policy making by important geopolitical actors strengthens our ability to predict future action and\nidentify pathways for international coordination and collaboration. A brief summary of the policy landscape across\nthe European Union, People's Republic of China, and the United States is shown in Table 1.\nThe analysis focuses on GenAI policy from the European Union (EU), the People's Republic of China (PRC)\nand the United States (US), and is restricted to laws, regulations and proposals that directly pertain to GenAI. Broader\npolicies that also apply to AI, like privacy regulation, are beyond the scope of this analysis, as are regulations from\nother countries and policies of commercial, for-profit organizations.\nThe PRC's Interim Measures for the Management of GenAI. The Chinese government is an often overlooked\nfirst mover on AI regulation. It adopted rules for online recommender systems [78] already in 2021. Regulation for"}, {"title": "2.3 Learning from Military Risk Management", "content": "The ideal path to fulfilling the requirements of regulatory frameworks is to create GenAI models and build GenAI sys-\ntems that are completely safe and secure. As we argue in later sections, there are unfortunately technical impossibilities\nthat prevent us from creating such completely safe and secure GenAI offerings. Furthermore malicious actors may\nwant to hide unsafe behaviors inside GenAI models, for example through backdoors. Thus it is important to consider\nthe bigger picture of how GenAI will be used and how risks from these use cases must be mitigated. The experience\nof the (U.S.) military in handling, applying, and deploying potentially dangerous technologies may be instructive and\nmay lead to capable approaches to securing and safeguarding GenAI use cases.\nA security guard is bound to get bored and miss a few things on a TV monitor over the course of an 8-hour shift,\nbut when they're paying attention they can assess a security threat quickly. By contrast, a computer vision algorithm\nwill never be bored or inattentive, but it is unreliable at discerning a security threat from mere movement on the\nscreen. Both humans and AI systems have strengths and weaknesses, and in an ideal world, new technologies will\nemerge that harness the strengths and minimize the weaknesses of both. Current conversations have been focused on\nthe technical development of AI systems\u2014which is important and necessary work-but focus is also needed on ways\nto mitigate AI risks by improving the knowledge and performance of human operators. Thankfully, the challenge of\ndeveloping humans and organizations to employ advanced and safety-critical technologies is not new, and the tech-\npolicy community can look to the U.S. military for some useful lessons learned.\nThe military is not the only organization that must manage safety-impacting machinery, but they are perhaps the\nmost experienced and disciplined when it comes to creating and enforcing behavioral standards around powerful tech.\nCenturies of experience have refined the myriad ways in which professional militaries control lethal technologies.\nThese range from the longstanding tradition of badges, ribbons, and medals that officers wear to identify their experi-\nences and training, to organizational approaches like separately designating infantry from artillery units (an innovation\nin 1776), and later establishing armor units for tanks in World War I. In brief, the military is a leader in the art and\nscience of risk management for lethal technologies.\nThere is widespread awareness of how the military manages technical risks of weapons, such as the requirements\nprocess or operational test and evaluation, but it is harder to find a succinct description of all the efforts taken to\nmitigate risks through human intervention. These efforts are well known, but seldom thought of as a risk mitigation\nmethod for powerful technologies. These human, vice technological, interventions include qualifications regimes,\nthe delineation of roles and responsibilities, a continuous cycle of exercise and assessment, and the promulgation of\nstandardized doctrine, tactics, techniques, and procedures. None of these sorts of efforts are unique to the military,\nhospital systems also require qualifications and standard procedures, but the military has been a trailblazer that others\n(including healthcare) have imitated. Companies and governments thinking about AI governance today could imitate\nthis approach as well.\nAt the heart of human-factor risk mitigation in the military is the qualification. For service members, qualifying is\na process of demonstrating knowledge sufficient for a service member to be entrusted to operate a weapon. A qualified\nindividual is recognized in personnel records, through public ceremonies, and sometimes even by special pins and\nbadges worn on their uniforms. The difficulty of the qualification process and the type of recognition varies according\nto the risk of the technology.\nQualification processes are widespread in the private sector as well, and so too could they be used to address the\nrisks of AI systems. For example, just as teachers are qualified to instruct particular curricula in public schools, so\ntoo could they be qualified to use AI tools that would review student performance or provide instructional assistance.\nJust as doctors are qualified to perform certain specialized procedures and undergo a process to be granted privileges\nto perform these procedures at their respective medical centers, so too should they be qualified to use a large language\nmodel for medical record review properly. These qualifications acknowledge the potential utility of AI tools to improve\noutcomes, while simultaneously working to address the known weaknesses of AI tools through human intelligence.\nIn the first instance, qualifications simply prepare the operator to best use the system and understand its weaknesses.\nA thoughtful qualifications process that results in a verifiable designation for the individual can also communicate the\nseriousness of their responsibilities with an AI system to the operator themselves but also to members of the public who\nare affected by the AI system's decisions. Maintaining a qualification can also be made contingent on the successful\ncompletion of regular assessments to ensure qualified users are kept up to date with new AI developments\u2014which\nwill most certainly happen as the field continues to rapidly evolve.\nThe second benefit of qualifications as a governance mechanism is their role in accountability processes. While an\nunqualified individual may claim ignorance, a qualified individual will be recognized with a responsibility commen-"}, {"title": "3 Risk Mitigation through Model Alignment", "content": "GenAI model alignment is the process of training and tuning a model such that it always performs as desired. There\nare multiple definitions of alignment, but Wikipedia provides the following definition for AI alignment [86]:\nAI alignment research aims to steer AI systems towards humans' intended goals, preferences, or ethical\nprinciples. An Al system is considered aligned if it advances its intended objectives. A misaligned AI\nsystem pursues some objectives, but not the intended ones.\nMeanwhile, OpenAI describes the goal of their alignment efforts as follows\u2020"}, {"title": "3.1 Challenges in Achieving Alignment", "content": "Large language models (LLMs) are quickly becoming an integral part of the Internet infrastructure and software ap-\nplications. LLMs are being used to create more powerful online search, help software developers write code, and even\npower chatbots that help with customer service. LLMs are being integrated with corporate databases and documents\nto enable powerful Retrieval Augmented Generation (RAG) [51] scenarios when LLMs are adapted to specific do-\nmains and use-cases. However, these scenarios in effect expose a new attack surface to potentially confidential and\nproprietary enterprise data.\nAs the rapid evolution of AI-enabled chatbots continues and their deployment becomes more prevalent online\nand in business applications, the need to align them with human values and make them robust against adversarial\nattacks comes to the forefront. The identification and mitigation of a variety of risk factors, such as vulnerabilities, is\nthe goal of pre-deployment testing and evaluation of LLM's. Reinforcement learning from human feedback (RLHF)\ncombined with Red Teaming [20, 38] are the primary techniques today for alignment and vulnerability discovery and\nmitigation, aiming to make the chatbot more resilient against prompt injections [26]. These techniques include testing\nfor traditional cybersecurity vulnerabilities, bias, and discrimination, generation of harmful content, privacy violations,\nand emergent characteristics of LLM's, as well as evaluations of larger societal impacts [47, 72].\nAt the same time, the race between model developers and their adversaries has begun, and both sides are making\ngreat progress. There are no signs of abating in this race, which brings up the question about the long term equilibrium\nstate: is it going to be a state of safety and stability or a condition similar to cybersecurity?\nRecent theoretical results show that the modern technique of using guardrails to enforce alignment and resist\nprompt injections is inherently not robust - there are theoretical limits on rigorous LLM censorship [39]. Employing\nother means of mitigating such risks, e.g., setting up controlled model gateways and other cybersecurity mechanisms,\nare needed. In addition, adapting chatbots to downstream use-cases often involves the customization of the pre-trained\nLLM through further fine-tuning, which introduces new safety risks that may degrade the safety alignment of the\nLLM [67].\nAdversarial samples may be out-of-distribution (OOD) inputs. Thus, detecting OOD inputs is an important chal-\nlenge in adversarial machine learning, and might help with attacks on alignment. Fang et al. [32] established theoretical\nbounds on OOD detectability, i.e., an impossibility to detect when there is an overlap between the in-distribution and\nOOD data.\nAs models grow in size, the amount of training data grows proportionally. Very few of the LLMs in use today\npublish a detailed list of the data sources used in training. Those that do [58, 77] show the scale of the footprint and\nthe massive amounts of data consumed in training. The multi-modal generative AI systems exacerbate the demand\nfurther by requiring large amounts of data for each modality.\nData repositories are not monolithic data containers but a list of labels and data links to other servers that actually\ncontain the corresponding data samples. This creates new hard-to-mitigate risks [18]. In addition open source data\npoisoning tools [57] increase the risk of large scale attacks on image training data.\nAnother scale-related problem is the ability to generate synthetic content at scale on the internet. Although water-\nmarking may alleviate the situation, the existence of powerful open or ungoverned models creates realistic opportuni-\nties to generate massive amounts of unmarked synthetic content that can have a negative impact on the capabilities of\nsubsequently trained LLMs [73], leading to model collapse.\nBased on this, one may conclude we are likely to land in a state similar to where cybersecurity is today. Barrett\net al. [11] have developed detailed risk profiles for cutting-edge generative AI systems that map well to the NIST AI\nRMF [59] and should be used for assessing and mitigating potentially catastrophic risks to society that may arise from\nthis technology."}, {"title": "4 Risk Mitigation through Model Inspection", "content": "Model inspection is key to ensuring the effectiveness, fairness, reliability, and transparency of generative AI systems.\nModel inspection includes a wide range of tasks, from using model interpretation methods to discovering the biases\nin language and image models [52, 42, 88] to novel adversarial attacks and safety evaluation frameworks [71, 93],\noffering insights into the multi-faceted nature of generative AI development and application. This body of work serves\nas a crucial resource for anyone interested in the ethical and technical dimensions of AI and machine learning.\nBy inspecting models, developers can ensure that a model produces reliable and trustworthy outputs,\nwhich is crucial for applications where accuracy, precision and/or safety are critical. One recent study from [85]\npresents a framework for evaluating the safety of generative AI systems, highlighting the importance of integrating\nsociotechnical perspectives in AI safety evaluations. It outlines a three-layered approach: capability evaluation, human\ninteraction evaluation, and systemic impact evaluation. This framework emphasizes the need for comprehensive safety\nassessments that consider technical aspects, human interactions, and broader systemic impacts. The paper also reviews\nthe current state of safety evaluations for generative AI, identifying gaps and proposing steps to address them.\nModel inspection methods also allow for the identification of biases in a model's outputs. This is important for\nensuring fairness and preventing discrimination in AI-generated content. A recent study [52] addresses the issue of\nunder-representation in text-to-image stable-diffusion models. It introduces a method for identifying which words\nin input prompts contribute to biases in generated images. Their experiments show how specific words influence\nthe replication of societal stereotypes. The paper also proposes a word-influence metric to guide practitioners in\nmodifying prompts for more equitable representations, emphasizing the importance of addressing bias in AI models\nto prevent discrimination and stereotype perpetuation. Another related study [88] examines biases in text-to-image\nmodels, specifically Stable Diffusion. It introduces an evaluation protocol to analyze the impact of gender indicators\non the generated images. The study reveals how gender indicators influence not only gender representation in images\nbut also the depiction of objects and layouts. It also finds that neutral prompts tend to produce images more aligned\nwith masculine prompts than feminine ones, providing insights into the nuanced gender biases in Stable Diffusion."}, {"title": "Inner Interpretability and Outer Explainability", "content": "Model inspection consists of a number of techniques meant to\ngenerate explanations or interpretations of a GenAI model's operation and outputs. Borrowing from Doshi-Velez and\nKim [28], we use the following definition:\n[ML system] interpretability [is] the ability to explain or to present in understandable terms to a human\nDepending on what is being interpreted, we arrive at two classes of model-inspection techniques. If the goal is to relate\nthe outputs of the model to the relevant parts of its inputs, then outer explainability methods are applicable. If the goal\nis to relate the outputs of the model to the relevant inner structure (e.g., model weights, neurons, or subnetworks), then\ninner interpretability methods are applicable. We note that both classes of techniques may rely on model internals\n(weights, gradients, layers) to achieve their goals."}, {"title": "Outer Explainability", "content": "Deep-learning literature includes a large cohort of different model interpretability meth-\nods in deep learning, including methods from saliency maps, activation maximization, layer-wise relevance prop-\nagation, partial dependence plots, LIME, SHAP, and Integrated Gradients, and more (see, for example, survey in\nhttps://arxiv.org/abs/2011.07876). These methods help to demystify the decision-making processes of deep\nlearning models, making them more transparent and trustworthy. For instance, the method PRIME [68] proposes a\nnew method for analyzing failure modes in image classification models using human-understandable concepts (tags)\nfor images in the dataset and analyzing model behavior based on these tags. The method ensures that the tags de-\nscribing a failure mode form a minimal set, avoiding redundant and noisy descriptions. Experiments demonstrate that\nthis approach successfully identifies failure modes and generates high-quality text descriptions, emphasizing the im-\nportance of prioritizing interpretability in understanding model failures. In another study, authors of [87] investigate\nhow Chain-of-Thought (CoT) prompting affects LLMs. It examines whether CoT affects the importance given to spe-\ncific input tokens by LLMs. Using gradient-based feature attribution methods, this study analyzes several open-source\nLLMs to understand changes in token importance due to CoT prompting. The findings indicate that while CoT doesn't\nincrease the saliency scores of relevant tokens, it does enhance the robustness of these scores to variations in ques-\ntion phrasing and model outputs. This research provides insights into how CoT prompting influences LLM behavior,\nparticularly in question-answering tasks."}, {"title": "Inner Interpretability", "content": "Most studies on model interpretability focus on providing post-hoc explanations that identify\nfeatures or feature interactions that contribute most to a model's predictions. Recent literature has shown increasing\ninterest in treating interpretability as an inherent property of deep learning models or using interpretations as feedback\nto improve model performance and encourage explanation faithfulness.\nWe discuss here mechanistic interpretability, a particular flavor of interpretability that focuses on identifying spe-\ncific components of the neural network, such as individual neurons or groups of neurons or subnetworks/circuits,\nwhose operation is responsible for detecting particular properties of the input or guaranteeing particular properties of\nthe output. For example, analysis of the Inception-V1 vision model uncovered neurons that detect curves in images,\nor detect image patches at the boundary of high-frequency and low-frequency regions, or detect dog heads [63]. Per-\nforming such an analysis over the whole neural network of a model may be able to enumerate over all \"features\" of a\nmodel and then using that information to establish that a model is safe because all of its component features are safe\nand desirable for the task at hand, or, vice-versa, that a model is unsafe because one or more of its component features\nare known to be harmful or simply have unknown behavior. Anthropic's paper \u201cA Toy Model of Superposition", "explanations": "automatic (objective) evaluation and human\n(subjective) evaluation [19]. Human evaluation methods identify whether a generated interpretation is useful for\nhuman users to understand model predictions. Literature includes many automated strategies to quantify the quality\nof model explanations. For instance, [74] focuses on measuring the uncertainty in explanations provided by LLMs. It\nintroduces two new metrics, Verbalized Uncertainty, and Probing Uncertainty, to assess the reliability of explanations\ngenerated by LLMs. This study reveals that verbalized uncertainty is not a reliable estimate of explanation confidence\nwhile probing uncertainty correlates with the faithfulness of an explanation. The paper discusses the significance\nof understanding the uncertainty in LLM explanations to ensure trustworthiness and avoid plausible but inaccurate\nexplanations. This research contributes to enhancing the transparency and reliability of foundational models in natural\nlanguage processing.\nBroadly speaking, adversarial attack methods are also doing model inspection. An adversarial attack against a\nLLM like GPT-3 used carefully designed strategies to deliberately trick the model into making errors or producing\nunintended responses. These attacks exploit weaknesses or blind spots in a model's understanding of its underlying\ndata and algorithms. Understanding and defending against adversarial attacks is crucial for the responsible devel-\nopment and deployment of AI, especially in areas where accurate, safe and unbiased AI responses are critical. Our\nworkshop includes one presentation from a notable recent LLM attack from [93] that conducts adversarial attacks on\naligned LLMs. It focuses on generating objectionable content by appending a specially crafted suffix to various user\nqueries. This approach combines greedy and gradient-based optimization techniques to optimize these adversarial\nsuffixes, making them effective across different models and prompts. The study demonstrates that these attacks are"}, {"title": "5 Risk Mitigation through Provenance and Watermarking", "content": "One of the central problems in the era of GenAI is provenance tracking or the \u201cGenAI Turing Test (GTT)\u201d (e.g., was\ncontent x generated by a known GenAI system (Claude, GPT, DALL-E, Gemini) or a natural image). Recall that\ndeepfakes are synthetic media that have been digitally manipulated to replace one person's likeness convincingly with\nthat of another or to place a person convincingly in a fake setting (an example of a deepfake is shown in Figure 2).\nCurrently, deepfakes are mostly generated using GenAI techniques, so provenance tracking and deepfake detection are\nvery closely related problem, but not exactly the same (deepfakes can be generated without using GenAI techniques).\nGiven the importance of deepfake detection, attribution, and mitigating techniques, such as watermarking [45],\nseveral laws related to AI prominently mention these topics. For example, the executive order from the White House\nmentions the following\u2021:\nProtect Americans from AI-enabled fraud and deception by establishing standards and best practices\nfor detecting AI-generated content and authenticating official content. The Department of Commerce\nwill develop guidance for content authentication and watermarking to clearly label AI-generated content.\nFederal agencies will use these tools to make it easy for Americans to know that the communications they"}, {"title": "5.1 Detection of AI-Generated Output", "content": "Media falsification has existed since the beginning of media (e.g., photographs as shown in Figure 3). Early com-\nmercial photographers used darkroom techniques to create falsified images to monetize large collections of nega-\ntives [35]. Young photographers created falsified photographs that were published and fooled parts of the public using\ncardboard cutouts of fairies [43, 61]. Stalin used an army of retouchers to help falsify the history portrayed in pho-\ntographs [35, 14]. These examples illustrate the use of media manipulation for profit, entertainment and attention, and\nas a political weapon. In the context of GenAI, what's new is the lowering of the level of skills and resources necessary\nto create a compelling falsification. Lowering the bar to compelling falsification enables more potential adversaries\nand potentially a much larger scale of media-falsification attacks.\nSeveral potential attack vectors have been identified over the last several years, such as Jordan Peele's public\nservice announcement demonstrating a deepfake of President Obama [54]; Bricman's coining of the term Ransom-\nfake [17], a mashup of ransomware and deepfake, where generated media is used to put someone in a comprising\nposition unless a ransom is paid; and large-scale generated events [81]. As of 2023, many of these sorts of attacks\nhave been seen in the wild, particularly with deepfakes of President Zelenskyy and President Putin during the Russia-\nUkraine war [27, 22] and the generation of media purporting to be falsified historical events [64].\nGiven the gravity of this problem, the US Department of Defense (DoD) has invested significant resources in\ntackling this problem. In the context of these challenges, the Defense Advanced Research Projects Agency (DARPA)\nmade two significant investments. The first was the Media Forensics (MediFor) program, from 2016 to 2020. MediFor\nsought to produce quantitative measures of media integrity for images and video to enable integrity assessment at\nscale. The second is the Semantic Forensics (SemaFor) program that seeks to create rich semantic algorithms that"}, {"title": "5.2 Watermarking of GenAI Output", "content": "As described below, a watermarking method has two components.\n\u2022 An encoder Embedke (M, p, w) where ke is a secret key, M is the model, p is user supplied input to the model\n(e.g. a prompt or instructions for editing a message), and w is additional information (e.g. string to be embedded\nin a watermark). Some schemes might not use some parameters. For example, if a scheme does not use a secret\nkey, then Ke will not be used, and in some schemes w might not be used.\n\u2022 A decoder Detectkp (x, w) where kp is the key use for detection, x is content, and w is additional information.\nAs usual, some schemes might not use certain parameters, such as kp and w. This method returns 1 if x is\nwatermarked, and 0 otherwise.\nNote that in secret key schemes, such as [4, 50, 24], kE = kp = k and is kept secret. In publicly-verifiable schemes [31],\nke is the secret key and kD is the public key. As we already stated, the precise parameters of Embed and Detect\ndepend on the watermarking scheme.\nWatermarking methods can be categorized into non-learning-based [65, 15, 48] and learning-based [92, 53, 5].\nThe former manually design encoder and decoder; while the latter uses neural networks as encoder/decoder and trains\nthem using deep-learning techniques. In the image and audio domains, non-learning-based watermarking methods [65,\n15] have been studied for several decades, while learning-based watermarking methods [92, 53] were proposed in the\nlast several years. In the text domain, both non-learning-based [48] and learning-based [5] methods were proposed in\nrecent years.\nOne unique advantage of learning-based watermarking is that they can leverage adversarial training [41], a standard\ntechnique to build robust machine learning systems, to enhance robustness against post-processing that aims to remove\nthe watermark in watermarked content. The key idea of adversarial training in the context of watermarking is to\nintroduce a post-processing layer between the encoder and decoder [92]. When training the encoder and decoder, the\npost-processing layer manipulates the watermarked content produced by the encoder via post-processing operations\nsuch that the learnt decoder can still accurately decode the watermark from a post-processed watermarked content.\nWe note that some methods embed the watermark and encoder into the parameters of a GenAI model such that its\ngenerated content intrinsically has the watermark embedded [34]. The attacks on watermarking discussed below are\nalso applicable in such settings, but we will not discuss them in great detail.\nWatermarks are not perfect and not robust to all adversarial manipulations. There are also some impossibility\nresults that a \"perfectly robust\u201d watermark might not be possible [90]. In order to understand what are good use\ncases for watermarking, it is very important to understand the threat landscape for various watermarking schemes. We"}, {"title": "5.2.1 Current State-of-the-Art of Attacks on Watermarking", "content": "Common Post-processing. Watermarked content often undergoes various common post-processing operations in\nnon-adversarial settings. These operations, while possibly not malicious in intent, may inadvertently remove the\nwatermark. For instance, common image post-processing operations include compression, resizing, cropping,\nand color adjustments; typical text post-processing involves paraphrasing, word insertion, word deletion, and\nstructural modifications; and popular audio post-processing includes compression, filtering, and re-recording.\nNon-learning-based watermarking methods are often not even robust against common post-processing, e.g.,\nJPEG compression removes the image watermark inserted by non-learning-based image watermarking meth-\nods [46] and paraphrasing removes the text watermark inserted by non-learning-based text watermarking meth-\nods [49, 70]. However, prior studies [92, 46] showed that learning-based image watermarking methods can\nbe robust against common post-processing because they can leverage adversarial training. We expect learning-\nbased text watermarking to be also more robust against common post-processing than non-learning-based ones\ndue to adversarial training, though no prior studies have explored this.\nDiffusion Purification Attack. Diffusion purification involves the process of passing data through forward and back-\nward diffusion model steps for a specified number of diffusion steps (t). To elaborate, diffusion purification\nintroduces Gaussian noise to the content and then utilizes denoising diffusion models to undo the Gaussian\nnoise in order to get an output that is similar to the input. The parameter t determines the degree of similarity\nbetween the output and the input. This technique has been used as a defense against adversarial attacks (Nie\net al. [60]), and also to remove watermarks from images (Saberi et al. [69], Zhao et al. [91]). Saberi et al. [69]\nproposed a theoretical guarantee that diffusion purification can successfully attack watermarking techniques that\nintroduce small perturbations to the content in order to watermark it (i.e., imperceptible watermarks).\nAdversarial Post-processing. Adversarial post-processing represents a strategic manipulation by attackers aimed at\nremoving watermarks from content without compromising its quality. This section delves into the application of\nadversarial examples, originally introduced by Goodfellow et al. [41], to the domain of watermarking, focusing\non white-box, black-box, and no-box settings.\nIn the case where the attacker has white-box, black-box, or no-box access to the watermarking decoder, Jiang et\nal. [46] extended adversarial examples to image watermarks. Their research demonstrated that by introducing\na small, human-imperceptible perturbation to a watermarked image, an attacker can effectively remove the\nwatermark with theoretical guarantees. When the attacker lacks white-box access, they can find the perturbation\nby repeatedly querying the detection API. It's important to note that these white-box and black-box attacks do\nnot require training any surrogate model. Additionally, Jiang et al. [46] developed a no-box attack by training a\nsurrogate watermarking decoder. Hu et al. [44] further extended this approach with a transfer attack that involves\ntraining multiple surrogate watermarking decoders. This attack generates perturbations by aggregating outputs\nfrom multiple surrogate watermarking decoders.\nBesides, Zhang et al. [90] demonstrated the theoretical impossibility of creating robust watermarks against\nadversarial attacks that have black-box access to the model. Their analysis relies on two conceptual oracles: a\nquality oracle assessing output quality and similarity to the original data, and a perturbation oracle that can alter\ndata while maintaining acceptable quality.\nMoreover, Saberi et al. [69] have demonstrated that surrogate model adversarial attacks can effectively compro-\nmise image watermarking techniques, especially those employing high-perturbation watermarks (i.e., a family\nof watermarking techniques that significantly alter the original data, and usually have higher robustness to non-\nadversarial attacks [69]). These attacks involve training a surrogate model to mimic the watermark decoder using\na collection of watermarked images, eliminating the need for direct access to the actual watermark decoder. The\ntrained surrogate decoder can subsequently be employed to apply adversarial perturbations to the data. Further-\nmore, An et al. [9] have developed an extensive benchmark to evaluate the robustness of watermarks, offering\nvaluable insights into their effectiveness.\nThe literature on attacking watermarking schemes mostly tackle text and image modalities. Modalities, such as\naudio and video, have received scant attention. For example, existing attacks on text and audio watermarking are"}]}