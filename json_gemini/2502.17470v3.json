{"title": "MC2SleepNet: Multi-modal Cross-masking with Contrastive Learning for Sleep Stage Classification", "authors": ["Younghoon Na", "Hyun Keun Ahn", "Hyun-Kyung Lee", "Yoongeol Lee", "Seung Hun Oh", "Hongkwon Kim", "Jeong-Gun Lee"], "abstract": "Sleep profoundly affects our health, and sleep deficiency or disorders can cause physical and mental problems. Despite significant findings from previous studies, challenges persist in optimizing deep learning models, especially in multi-modal learning for high-accuracy sleep stage classification. Our research introduces MC2SleepNet (Multi-modal Cross-masking with Contrastive learning for Sleep stage classification Network). It aims to facilitate the effective collaboration between Convolutional Neural Networks (CNNs) and Transformer architectures for multi-modal training with the help of contrastive learning and cross-masking. Raw single channel EEG signals and corresponding spectrogram data provide differently characterized modalities for multi-modal learning. Our MC2SleepNet has achieved state-of-the-art performance with an accuracy of both 84.6% on the SleepEDF-78 and 88.6% accuracy on the Sleep Heart Health Study (SHHS). These results demonstrate the effective generalization of our proposed network across both small and large datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Sleep is crucial for human life, as it influences physical and mental health. Sleep disorders and deficiencies can profoundly affect an individual's overall well-being [37]. Automating sleep scoring commonly relies on Polysomnography (PSG), a widely implemented key diagnostic tool, enabling the analysis of sleep disorders through the recording of physiological signals such as the electroencephalogram (EEG) for brain activity, Electrooculogram (EOG) for eye movement, and Electromyogram (EMG) for muscle activity, etc. These signals are broken down into 30-second intervals, known as epochs, to categorize sleep stages.\nEvery epoch is systematically classified using criteria from the Rechtschaffen and Kales (R&K) [27] manual, and the more recent American Academy of Sleep Medicine (AASM) [2] standards. Both offer detailed guidelines for sleep stage classification.\nDespite the success of PSG-based manual sleep diagnostics, due to extensive recording time, a labor-intensive labeling process, and the inherent inconsistency in labeling [4], automating sleep scoring becomes essential to address these challenges effectively. [15] work highlights potential problems and demonstrates that automatic sleep stage classification achieves performance comparable to human experts, requiring only a few seconds for labeling.\nRecently, a series of studies, including DeepSleepNet [33], SeqSleepNet [22], XSleepNet [24] introduced deep learning models that demonstrate state-of-the-art single channel performance in PSG datasets. DeepSleepNet employs dual CNN models that capture temporal and frequency domain features with raw signal samples. SeqSleepNet introduces an RNN-based sequence-to-sequence network for processing spectrogram data. In contrast to the DeepSleepNet architecture, to implement multi-modality training (raw signal and the corresponding spectrogram data), XSleepNet adopts hierarchical CNNs as a backbone network for raw signals and SeqSleepNet as a backbone network for spectrogram. XSleepNet simultaneously optimizes two sub-models, leveraging a technique known as gradient blending [36] for updating their gradients individually. Even though XSleepNet pioneers tried to optimize multi-modal tasks for sleep stage classification, its effectiveness is constrained to specific deep-learning environments.\nDue to their ease of optimization, there are various single-modal training methods whose performance is on par with that of human experts. Within the context of multi-modal training, only XSleepNet holds a competitive status, still retaining its reliance on old-fashioned RNN-based architecture. This underscores inherent challenges in optimizing multi-modal tasks. Recently, START [32] and CoReSleepNet [11] have incorporated multi-channel inputs using distinct networks, highlighting the potential for integrating diverse information and reducing the modality gap. However, their contributions only display combined homogeneous information like same shape vectors consisting of different channels that use different models even though they show prominent results. They have yet to fully address the challenge of effectively combining heterogeneous information like signals and spectrograms.\nIn our approach, instead of utilizing gradient blending and old-fashioned RNN-based architectures, we develop a multi-modal model combining the features extracted from multi-modal data. Our network is designed with dual backbones, for the dual modalities of data samples: (1) \"Raw Signal View\" and (2) \"Spectrogram View\". A CNN-based backbone and a Transformer-based backbone are used as feature extractors for raw signal data and spectrograms, respectively.\nThe proposed model architecture, Multi-modal Cross-masking with Contrastive learning for Sleep stage classification Network (MC2SleepNet), is presented in Fig. 1. Training the MC2SleepNet model is conducted in two steps: (1) Pre-training both \"Epoch-Level\" and \"Sequence-Level\u201d simultaneously with the use of both the supervised and self-supervised learning and (2) Fine-tuning the model while freezing the CNN and Transformer backbones trained during the pre-training step.\nDuring the pre-training step at an epoch-level, InfoNCE loss [35] is employed to align and fine-tune the embedding from different network architectures, potentially improving the performance on downstream tasks. In the sequence-level, We introduce a novel concept, \"Cross-Masking\", for implementing cross-attention to unmasked and masked pairs referring to [29] work. It is noteworthy that two self-supervised learning techniques, \"Contrastive Learning\" and \"Masking Prediction\", are integrated within a pre-training process. This integration encourages two different CNN and Transformer backbones to cross-examine the features extracted from another modality. It enables the inference of information that cannot be perceived by one view alone.\nIn the fine-tuning stage, we freeze the backbone and update only the part of the model for sequence-level training. This process effectively leverages precise estimation by training inferring capability of the ground information from masked information [1], while maintaining the integrity of the core representations.\nOur proposed model shows state-of-the-art accuracy on the SleepEDF-78 (84.6%) and SHHS (88.6%) datasets, demonstrating"}, {"title": "2 PRELIMINARY AND RELATED WORK", "content": "A spectrogram is a two-dimensional representation that describes how the frequency spectrum of a signal changes over time. Recently, spectrograms have been widely adopted across numerous application fields, including sleep stage classification. Fig. 2 shows how we obtain a sample of spectrogram data from an epoch of raw EEG signal data. We use single-channel EEG signals, \"Fpz-Cz\" in a SleepEDF-78 dataset and \"C4-A1\" in a SHHS dataset. In this work, the employed signal is denoted by $S_{g_{i,j}} \\in \\mathbb{R}^{C \\times 3000}$, where C = 1. The signals are sampled at 100Hz and segmented into standard epoch samples of 30 seconds (100 data points \u00d7 30 sec = 3000 data points). This approach aligns with datasets like SHHS, where signals originally sampled at 125Hz are resampled to 100Hz.\nThen, we apply Fast Fourier Transforms (FFT) to the signals while adopting a window size of 200 data points (double the frequency) and an overlap of the equal size (100 data points), as recommended in studies like [11, 24] as shown in Fig. 2. So, we have 29 time segments for an epoch, and each segment is transformed into a frequency spectrum, which is then normalized between 0 and 128. This setup efficiently captures the spectral characteristics of the EEG data.\nAs a post-FFT processing, we implement log-magnitude transformations to normalize variance. Then we have a spectrogram data sample, $S_{p_{i,j}} \\in \\mathbb{R}^{C \\times T \\times Freq}$ where T is the number of time segments (it is set to 29) and Freq is set to 129."}, {"title": "2.2 Sleep stage classification", "content": "Sleep stage classification can be leveraged for diagnosing sleep deficiency or disorders. In this classification, 30-second epochs are classified into several classes. According to the AASM guidelines [2], there are five classes for the sleep stages: Wake, NREM1, NREM2, NREM3, and REM. The datasets used in this work, SleepEDF-78 and SHHS datasets are labeled following the AASM guidelines.\nAs an initial deep learning model for sleep stage classification, CNN-based works are used to recognize patterns in biological signals [21, 30] which effectively capture features of each epoch data. On the other hand, Recurrent Neural Networks (RNN) are used to extract sequential features from continuous sleep cycles and predict transitions between stages [16, 17]. To better exploit the sequential features within a consecutive sequence of epochs, variants such as Long-Short-Term Memory (LSTM) [8] and Gated Recurrent Units (GRU) [6] are particularly prevalent in sleep stage classification. Those methods, which employ a one-to-one approach [23], suffer from primary drawbacks such as increased training time.\nTo enhance the efficiency of training steps, a many-to-many framework, such as SleepNet [3], has been leveraged to predict the loss of multiple sequences. DeepSleepNet [33] combines the feature extraction capability of CNN with the sequential data interpretation power of BiLSTMs. While it excels in deciphering complex datasets, it has trade-offs such as a large footprint and extended training duration. However, the model demands intricate hyperparameter adjustments and incurs high computational expenses.\nIn [19], spectrogram was first adopted into various methods. Among them, SeqSleepNet [22] effectively navigated long-term dependencies within spectrograms and attention mechanisms. Sleep-Transformer [26] used self-attention to understand the temporal complexities of spectrograms. Its ability to process in parallel speeded up training, but the necessity for large datasets and challenges in model interpretability were its notable drawbacks.\nFor example, SleepTransformer had shown prominent results in large-size datasets such as SHHS, but relatively poor performance in small-size datasets such as SleepEDF. Consequently, numerous researchers have opted for RNN-based networks for spectrograms instead of Transformer-based models, as seen in works such as L-SeqSleepNet [25], XSleepNet, MVFSleepNet [14]. Although attempts have been made to utilize Transformer-based architectures, such as in CoReSleepNet [11], they have primarily been applied to large-scale datasets.\nTo date, no research has surpassed the XSleepNet architecture in multi-modal learning in sleep stage classification. We expect that incorporating multi-modal data, including spectrograms, into a model combined with a Transformer and self-supervised learning will robustly enhance performance. This approach could potentially address difficult optimization problems more effectively."}, {"title": "2.3 Self-supervised learning in sleep stage classification", "content": "Self-supervised learning was introduced with a specific focus on defining positive and negative samples [9] and contrastive loss"}, {"title": "3 METHODOLOGY", "content": "Given a training dataset $D_S = {SS_i}_{i=1}^N$ has N samples and each sample, $SS_i$ is an ordered tuple of sequential L epochs. Then $SS_i$ can be denoted by ${(S_{i,j})}_{j=1}^L$, in which $S_{i,j}$ corresponds to the j-th epoch of $SS_i$ and L is set to 21 in this work. The $S_{i,j}$ is a 3-element tuple and it is denoted by $(S_{g_{i,j}}, S_{p_{i,j}}, y_{i,j})$. The first two elements of a tuple correspond to multi-modality data of an epoch: a raw signal data ($S_{g_{i,j}}$) and a spectrogram data ($S_{p_{i,j}}$). The last element, $y_{i,j}$, is a ground-truth label for the epoch. Specifically, each sample of the training datasets consists of $SS_i = ((S_{g_{i,1}}, S_{p_{i,1}}, y_{i,1}), \u00b7\u00b7\u00b7 , (S_{g_{i,L}}, S_{p_{i,L}}, y_{i,L}))$.\nThe label $y_{i,j} \\in ${Wake, NREM1, NREM2, NREM3, REM} represents a stage class for $S_{i,j}$, $S_{g_{i,j}} \\in \\mathbb{R}^{C \\times 3000}$ represents 30-second information across C channels sampled at 100Hz frequency. In this work, we consider a single channel EEG so that C = 1.\nThe overall model architecture and its layer components are presented in Fig 1. We use a CNN backbone network denoted by $F_{sg}$ to draw out features from the raw signal data. Another backbone, $F_{sp}$, for the spectrogram data, is based on a Transformer network."}, {"title": "3.1 Epoch level training", "content": "In the backbone for raw signal data, following the [13, 20, 28], we adopt 5 CNN blocks with the following channel configurations: Each of the first two blocks has two convolutional layers, while the subsequent blocks each have three convolutional layers. Maxpool layers with a width of '5' are appended after all the blocks except the first block. In the whole CNN blocks, we adopt a kernel size of '3' and the same padding size with the stride of '1'. Finally, we implemented a maxpool layer with a width size of '2' to adjust the CNN output dimensions that match the Transformer output dimensions.\nThe backbone model function $F_{sg}$ produces the feature having the raw signal $Z_{g_{i,j}}$ as its domain, making dimension of the feature to $F_{sg}(S_{g_{i,j}}) \\in \\mathbb{R}^{batch \\times 5 \\times 128}$. On the other hand, as described in Section 2.1, a spectrogram data sample, $S_{p_{i,j}} \\in \\mathbb{R}^{C \\times 129 \\times 29}$, undergoes a simple CNN-based resizing layer and then the output of the resizing layer is further augmented with positional encoding vector (PE) to yield $S_{p_{i,j}}^{'} \\in \\mathbb{R}^{C \\times 29 \\times 128}$ as defined in Eq. 1. This resizing aims to exploit the potential benefits of dimensionality being a power of two and provide additional non-linearity.\n$S_{p_{i,j}}^{'} = proj_{cnn} (S_{p_{i,j}}) + PE$   (1)\nIn the backbone of the spectrogram, $F_{sp}$, a Transformer layer performs a sequence of the computations defined from Eq. 2 to Eq. 8. Specifically, $Attn(x)$ in Eq. 2 uses a set of typical learnable parameters for a Transformer: $W_k, W_q, W_v \\in \\mathbb{R}^{d_{model} \\times d_k}, W^{head} \\in \\mathbb{R}^{d_{head} \\times d_{FF}}, W \\in \\mathbb{R}^{d_{FF} \\times d_{model}}, b_1 \\in \\mathbb{R}^{d_{FF}}$, and $b_2 \\in \\mathbb{R}^{d_{model}}$. We set $d_k = 16$ and $d_{FF}$ is set to 1024 with a dropout rate of 0.1.\nIf $S_{p_{i,j}}^{'}$ is given as an input, $Z_{p_{i,j}}^{1}$, for the first Transformer layer as presented in Eq. 8 with l = 1. After \u2018iter\u2019 (4 is used as iter) iterations of the Transformer layer, the output, $Z_{p_{i,j}}^{iter+1}$, is produced as a shape of $\\mathbb{R}^{batch \\times 5 \\times 128}$ with $d_{head} = 8$. The final output of Eq. 8 can be re-written as $Z_{p_{i,j}}^{Sp}$\n$SA(x) = concat [Attn_1(x), ..., Attn_h(x)]$   (2)\n$Attn(x) = softmax(\\frac{Q W_k \\cdot (K W_q)^T}{\\sqrt{d_k}}) V W_v$   (3)\n$FF(x) = max(0, xW + b)W + b$   (4)\n$LN(F(x)) = layernorm(x + F(x))$   (5)\n$SA Block(x) = LN(SA(x))$   (6)\n$TF(x) = SA Block(x) + LN(FF(SA Block(x)))$   (7)\nfor l = 1 to iter : $Z^{l+1} = TF(Z^l)$   (8)\nBoth the raw signal and spectrogram inputs are fed into their corresponding backbone networks independently as presented in Fig. 1. After getting $Z_{g_{i,j}}^{Sg}, Z_{p_{i,j}}^{Sp}$, an epoch-wise attention (EW) defined in Eq. 11 is applied to enhance features further. Attention, $a_t$, for EW is defined by Eq. 9 and Eq. 10. In Eq. 9, $W \\in \\mathbb{R}^{A \\times F}$ and $b_a \\in \\mathbb{R}^{A}$ are learnable parameters and A is an attention size. The parameters, A and F, are set to 128.\nThe epoch-wise attention operation is also applied to the feature, $Z_{g_{i,j}}^{Sg}$, extracted from raw signal data to accentuate the important channel information in the feature. In Eq. 9, $z_t$ can be $Z_{g_{i,j}}^{Sg}$ or $Z_{p_{i,j}}^{Sp}$ according to the input. The index, t, of $z_t$ has it range, $1 \\leq t \\leq T$ where T is '5' for $Z_{g_{i,j}}^{Sg}$ and '29' for $Z_{p_{i,j}}^{Sp}$:\n$a_t = tanh(Wz_t + b_a)$   (9)\n$A_t = \\frac{exp(a_t)}{\\sum_{t=1}^T exp(a_t)}$   (10)\nThen, we employ global average pooling (GAP) to effectively condense the high-dimensional feature maps. This application of GAP results in compact representations of the feature vectors and finally we have $O_{g_{i,j}}^{Sg}$ and $O_{p_{i,j}}^{Sp} \\in \\mathbb{R}^{batch \\times d_{model}}$ as depicted in Eq. 12.\n$EW(Z_{i,j}) = \\sum_{t=1}^T A_t* z_t$   (11)\n$O_{g_{i,j}}^{Sg} = GAP(EW(Z_{g_{i,j}}^{Sg}))$   (12)\n$O_{p_{i,j}}^{Sp} = GAP(EW(Z_{p_{i,j}}^{Sp}))$"}, {"title": "3.2 Sequence level training", "content": "After passing through the global average pooling layer, InfoNCE loss is calculated for training while inducing an embedding feature space of similar semantic meanings. Before directly applying the InfoNCE loss, a projection layer is added to help a model be optimized by decoupling the layers before and after the projection layer. The projection layer takes $O_{g_{i,j}}^{Sg}$ and $O_{p_{i,j}}^{Sp}$, and it generates $zz_{g_{i,j}}^{Sg}$ and $zz_{p_{i,j}}^{Sp}$, respectively as presented in Eq. 13 ($zz_{g_{i,j}}^{Sg}$ and $zz_{p_{i,j}}^{Sp} \\in \\mathbb{R}^{d_{model} \\times d_{proj}}, d_{proj} = 128$).\nThe loss terms for the InfoNCE are described in Eq. 14, Eq. 15, and Eq. 16. We employ these loss equations to assess similarities between the features by contrasting the raw signal features with those of the spectrogram. For effective contrast, negative samples are collected from a batch with a batch size of B. Additionally, $L_{sig2spc}$ and $L_{spc2sig}$ are evaluated as the averages of the similarity ratios over the L epochs of a sequence sample. Finally, our epoch-level loss function can be defined in Eq. 16.\n$zz_{g_{i,j}}^{Sg} = Proj_{Sg}(O_{g_{i,j}}^{Sg}), zz_{p_{i,j}}^{Sp} = Proj_{Sp}(O_{p_{i,j}}^{Sp})$   (13)\n$L_{sig2spc} = - log \\frac{exp((zz_{g_{i,j}}^{Sg})^T zz_{p_{i,j}}^{Sp})}{\\sum_{k=1}^B exp((zz_{g_{i,j}}^{Sg})^T zz_{p_{k,j}}^{Sp})}$   (14)\n$L_{spc2sig} = - log \\frac{exp((zz_{p_{i,j}}^{Sp})^T zz_{g_{i,j}}^{Sg})}{\\sum_{k=1}^B exp((zz_{p_{i,j}}^{Sp})^T zz_{g_{k,j}}^{Sg})}$   (15)\n$L_{epoch-level} = (L_{sig2spc} + L_{spc2sig})/2$   (16)\nAlso, to mitigate unstable optimization and encourage learning useful features during the initial training steps, we employ pre-trained backbone networks for epoch-level training.\nIn our sequence-level training, we use a sequence of the features for each modality: ${ (O_{g_{i,j}}^{Sg})}_{j=1}^L and { (O_{p_{i,j}}^{Sp})}_{j=1}^L$ obtained from $SS_i$ with the epoch-level backbones, epoch-wise attention and GAP. The masking strategy is integrated into the sequence-level training process to leverage the complementary nature of multi-modal characteristic, enabling features from one modality to aid in restoring masked features of the other modality. For random masking, we use a masking probability of 50%. By selectively hiding half of the input, the training encourages a more robust learning process, intending to recover the masked features.\nOur proposed method, named \"Cross-Masking\", allows our model to extract the sequential features from the neighboring tokens for each modality while extensively referencing the feature information from the different modalities through cross-attention layers. For the Cross-Masking, two Transformer blocks are used for each modality and the Transformers exchange the feature information using a cross-attention mechanism. Each Transformer block is composed of four Transformer layers. In the Transformer block for the Cross-Masking, the primary distinction from the Transformer backbone given in Eq. 8 is that a pair of cross-attention layer and a layer normalization is added after a self-attention layer and a layer normalization. The cross-attention in the Transformer block for raw signal features can be described in Eq. 17 where $O_{g_{i,j}}^{Sg}$ is used as a query while $O_{p_{i,j}}^{Sp}$ is used as key and a value. Note that the order of arguments in the function of Eq. 17 is important.\n$Attn(O_{g_{i,j}}^{Sg}, O_{p_{i,j}}^{Sp}) = softmax(\\frac{(W_k O_{g_{i,j}}^{Sg}) \\cdot (W_q O_{p_{i,j}}^{Sp})^T}{\\sqrt{d_k}}) W_v O_{g_{i,j}}^{Sg}$   (17)\nFor the cross-attention for the spectrogram features, the roles of $O_{g_{i,j}}^{Sg}$ and $O_{p_{i,j}}^{Sp}$ are interchanged so that $O_{g_{i,j}}^{Sg}$ is used as a query while $O_{p_{i,j}}^{Sp}$ is used as key and a value.\nThe critical hyperparameter for the \"Cross-Masking\" is the masking ratio within the sequence of the features. Our random masking strategy is expected to enhance the model generalization by training a recovery process from incomplete or unseen data distributions as used in MAEEG [5] and BERT [7]. Even though the higher masking ratio is applied in paired samples, the model could compel to reference the features from different modalities through cross-attention layers to predict the label information. This results in more accurate predictions and facilitates the harmonization of different modalities. Our masked tokens are shared and learnable within an expanded batch.\nTo further improve our model, a fine-tuning step is conducted after obtaining the pre-trained model with the masking strategy. By freezing the epoch-level backbone models, we focus on refining the sequence-level predictions by training a sequence model part with the pairs of the unmasked feature sequences, fostering a more cohesive and effective dual encoder architecture. Note that the masking is not used for the fine-tuning step. This strategic approach can eliminate the need for redundant adjustments at the epoch-level, streamlining the model's optimization for peak performance.\nA loss function for the recovering process is presented in Eq. 18. In the equation, $y_{i,j}$ in a set, $Y$, are the true labels in the j-th epoch data in the i-th samples across a sequence length of L and a total of N samples. Similarly, $\\hat{y}_{i,j}$ in a set, $\\hat{Y}$, indicates the predicted label. The ground true label, $y_{i,j}$, and the predicted label, $\\hat{y}_{i,j}$, are paired with the indices, i and j.\n$L_{recover} (\\hat{Y}, Y) = \\frac{1}{NL} \\sum_{i=1}^N \\sum_{j=1}^L log(\\hat{y}_{i,j}) \\cdot y_{i,j}$   (18)\nwhere $y_{i,j} \\in Y$ and $\\hat{y}_{i,j} \\in \\hat{Y}$\nThe loss function for the sequence-level training can be expressed in Eq. 19 and it consists of three cross-entropy loss functions. The detailed model structure for the sequence-level training is presented in the right part of the proposed model in Fig. 1. The recovered features by our model are represented by $t_{i,j}^{Sg}$ or $t_{i,j}^{Sp}$. A cross-entropy loss function is used for evaluating the quality of the recovered features as shown in Eq. 20 and Eq. 21. In addition, we apply a cross-entropy function to the features obtained by concatenating the $t_{i,j}^{Sg}$ and $t_{i,j}^{Sp}$. The concatenate operator is denoted by $\\oplus$ in Eq. 22. Then, those features are fed forward through a multi-layer perceptron (MLP) to generate the class predictions.\n$L_{sequence-level} = w_1 \\cdot L_{CE}^{Sg} + w_2 L_{CE}^{Sp} + w_3 L_{CE}^{ST}$   (19)"}, {"title": "4 EXPERIMENTS", "content": "Two publicly available datasets are employed to assess the performance of the proposed model: SHHS and SleepEDF-78. The SHHS database contains massive PSG records from 5,793 subjects aged 39-90. We use the preprocessing guidelines of the previous study and we utilize data from 5,791 patients in the SHHS dataset (it is named 'SHHS 5791'). Additionally, we excluded the recordings that do not include all five sleep stages while following the approach detailed in [31] and the dataset is named 'SHHS 5463'. We validate the dataset under two conditions: one with 5,791 patients and another with 5,463 patients. Detailed information on the two datasets is presented in Table 1.\nWe use the SHHS dataset to test if our model works well with a large-size dataset. The PSG records of 100 patients are excluded from the dataset for validation and then the remaining dataset is divided into training data and testing data with a partitioning ratio of 70:30. On the other hand, the SleepEDF-78 contains 78 healthy Caucasian subjects aged 25-101, and it has smaller PSG records than those of the SHHS. We use the SleepEDF-78 as our small dataset to test if our model works well on a small dataset. We validate our model using the SleepEDF-78 through 10-fold cross-validation."}, {"title": "4.1.2 Training setup:", "content": "In all experiments, the networks were trained using an Adam optimizer [10] with the learning rate (lr) of 5\u00d710\u22124, \u03b2\u2081 = 0.9, \u03b22 = 0.999 and the weight decay of 1 \u00d7 10-5 to prevent over-fitting problems. We adopt B = 32 as mini-batch sizes in the training. Early-stopping is used in our training. We validate the model every 100-th iteration of the SleepEDF-78 and 500-th in SHHS datasets. We also used the early-stopping when the model didn't update during 1000 periods.\nWhen conducting contrastive learning, it is important to apply augmentation techniques properly. As augmentations for our contrastive learning, we adopt one of Amplitude Scaling, Amplitude Shift, Add Gaussian Noise, Band Stop Filter, Time Shift and Zero-Masking schemes for raw signals. We also employ an augmentation for the spectrogram data but only a simple Random Noise augmentation is used for the spectrogram since improperly used augmentations can introduce erroneous modifications that may degrade the quality of features."}, {"title": "4.2 Experiment result analysis", "content": "The XSleepNet is composed of a CNN and an RNN network while the proposed model consists of CNN and Transformer model resulting in a slightly larger model size (0.3% size increase) compared to the XSleepNet. On the other hand, the proposed model achieves a 1.2% improvement in accuracy compared to the SOTA performance. To further explore the scenario where RNN in XSleepNet is replaced by a Transformer, we modify the XSleepNet model and conduct a performance comparison between the two models. Although the model size increases by 8%, the modified XSleepNet (XSleepNet+TF) has 0.8% of accuracy improvement.\nTo evaluate the training efficiency of our proposed model, we compare the training time of three models: SleepTransformer, XSleepNet, and ours. Table 2 shows that the training time of the proposed model is 2.02 (1,005/496) times slower than that of SleepTransformer, whereas XSleepNet demonstrates 2.68 (828/308) times slower than SleepTransformer. We predict the runtime of our model on a V100 GPU based on the performance ratio observed when the SleepTransformer is executed on two GPUs (it is 496/308 and 1.61 speedup can be roughly approximated by changing a GPU from RTX3090 to V100). Our model is expected to run on a V100 GPU with an approximate runtime of 624 (1,005/1.61) seconds, which means 1.32 (828/624) times faster training speed than XSleepNet (828 seconds) while showing better accuracy in both the SleepEDF-78 and the SHHS datasets.\nTo investigate the detailed performance characteristics of our proposed methods, we conducted an ablation study and evaluated the performance of the different variants of our model. In Table 3, the 'TF only' model refers to a Transformer model, while the 'CNN only' model denotes a pure CNN model. These models lack a sequence-level training network and predict a stage class for each epoch solely based on the features extracted from that single epoch, without considering the sequential features between neighboring epochs. Since they only consider a single epoch for their predictions, their accuracy performances are relatively lower. The 'TF+CNN(multi)' model incorporates Transformer and CNN backbones with an additional Transformer-based sequence-level training network, allowing it to extract sequential features from multiple neighboring epochs. In this model, multi-modal training is utilized with both backbones and sequential features exploited to predict stages, resulting in higher accuracies. When compared to the single-epoch model, performance improvements are observed (83.5% for SleepEDF-78 and 87.8% for SHHS 5463).\n'TF+CNN+CL+FT' involves applying contrastive learning and fine-tuning. However, employing contrastive learning alone without the masking strategy results in only a 0.2% performance improvement (83.5% \u2192 83.7%) compared to that of 'TF+CNN(multi)'. Nevertheless, on larger datasets like the SHHS dataset, it demonstrates a 0.6% (87.8% \u2192 88.4%) improvement. 'TF+CNN+M+FT'"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced an MC2SleepNet for sleep stage classification which aimed to achieve effective collaborative learning of the features extracted from multi-modal data through CNN and Transformer architectures. By utilizing multi-modal data samples, and putting the data of each modal into different networks, our model overcame the limited inspection and exploration on the feature representation space that could be caused by limited views. A contrastive learning technique is employed to leverage the features extracted through CNN and Transformer backbones from these multi-modal data sources. Additionally, we have developed a 'Cross-Masking' scheme based on a cross-attention mechanism for sequence-level training, which enhances performance in classifying both minor and major classes. Our MC2SleepNet has achieved the state-of-the-art performance with an accuracy of both 84.6% on the SleepEDF-78 and 88.6% accuracy on the SHHS dataset. This demonstrates that our proposed network is generalized effectively across small and large datasets."}]}