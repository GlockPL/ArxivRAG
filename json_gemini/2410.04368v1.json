{"title": "Algorithmic Capabilities of Random Transformers", "authors": ["Ziqian Zhong", "Jacob Andreas"], "abstract": "Trained transformer models have been found to implement interpretable procedures\nfor tasks like arithmetic and associative recall, but little is understood about how\nthe circuits that implement these procedures originate during training. To what\nextent do they depend on the supervisory signal provided to models, and to what\nextent are they attributable to behavior already present in models at the beginning\nof training? To investigate these questions, we investigate what functions can be\nlearned by randomly initialized transformers in which only the embedding layers\nare optimized, so that the only input-output mappings learnable from data are\nthose already implemented (up to a choice of encoding scheme) by the randomly\ninitialized model. We find that these random transformers can perform a wide\nrange of meaningful algorithmic tasks, including modular arithmetic, in-weights\nand in-context associative recall, decimal addition, parenthesis balancing, and even\nsome aspects of natural language text generation. Our results indicate that some\nalgorithmic capabilities are present in transformers (and accessible via appropriately\nstructured inputs) even before these models are trained.", "sections": [{"title": "Introduction", "content": "A large body of recent work has demonstrated the effectiveness of transformer language models\n(LMs) [46] on general sequence-modeling tasks. Transformers seem to be especially well-suited\n(relative to other flexible neural models) at problems involving numerical reasoning [41, 24, 30],\nstring manipulation [28], and various forms of in-context learning [7, 17, 1, 25]. Why is this the case?\nOne possibility is that some aspect of the transformer architecture makes these behaviors easy to\nlearn. Under this hypothesis, transformer models do not implement any useful functionality when\ninitialized; however, their loss landscape is structured such that they can be (computation- and\nsample-) efficiently optimized for behaviors of interest. But another possibility is that because\nof intrinsic properties of the transformer architecture and parameter initialization schemes\u2014these\ncapabilities are already implemented in some fashion even in randomly initialized models.\nTo disentangle these possibilities, we investigate the behavior of randomly initialized transformer\nmodels in which only the embedding layers are optimized, leaving all other model-internal parameters\nfixed. If such embedding-only training is successful, it implies that the randomly initialized model's\nbehavior on some subspace already corresponds to the input-output mapping of interest, up to a\nchoice of encoding scheme-in other words, that the randomly initialized model can already perform\nthe target task, and it suffices to find an encoding of inputs and outputs that induces the target behavior.\nIn experiments on seven tasks, we find that embedding-only training yields accurate models for a\ndiverse set of problems spanning arithmetic, associative recall, and sequence generation-in some\ncases substantially outperforming similarly trained recurrent models. Remarkably, transformer\nlanguage models trained in this fashion can even produce grammatical (though largely nonsensical)"}, {"title": "Background and Related Work", "content": "Random deep convolutional networks are highly effective visual\nfeature extractors even without training. Jarrett et al. [23] first discovered that linearly combining fea-\ntures from a randomly initialized one-layer convolutional network achieved comparable performance\nto fully trained networks for downstream vision tasks. Saxe et al. [40] showed that performance\nimprovements from training is relatively minor comparing to architectural changes. In this work, we\nexpanded the discussion to language models and demonstrated that training embeddings alone is\nsufficient to succeed in many tasks, highlighting the strong inductive bias of transformer architectures.\nNeural reprogramming aims to repurpose existing neural networks for\nnovel tasks via simple transformation layers. This technique was first purposed by Elsayed et al.\n[15] as a way to exploit trained neural network served by existing providers, and it was later\nused as a resource-efficient domain-transfer technique [45, 49]. In our work, we showed that in\naddition to neural networks trained for other tasks, even randomly initialized neural networks can be\nreprogrammed to achieve non-trivial performance."}, {"title": "Setup", "content": "We study the behavior of decoder-only transformer language models. In these models,\ninputs \\(x\\) (represented as sequence of token IDs) are first assigned vector embeddings \\(h^{(0)} = E(x)\\)\nvia an embedding layer \\(E\\). These embeddings are then passed through a series of \\(m\\) intermediate\nlayers \\(F^{(1)}, F^{(2)}, ..., F^{(m)}\\) so that \\(h^{(i)} = F^{(i)}(h^{(i-1)})\\), with each \\(F^{(i)}(x)\\) computing a hidden\nrepresentation \\(h^{(i)}\\) via a transformation:\n\\[\nh^{(i)} = F^{(i)}(h^{(i-1)}) = FFN^{(i)}(SelfAtt^{(i)}(h^{(i-1)})),\n\\]\nwhere FFN, SelfAtt are feed-forward and causal self-attention modules as in Radford et al. [38]\n(layer norms are omitted for simplicity). The final activation \\(h^{(m)}\\) is mapped by a unembedding\nlayer \\(U\\) to a distribution over next tokens.\nTo encode information about the ordering of input tokens, we implement the embedding layer \\(E\\)\nusing two matrices: a token embedding matrix \\(E_{token}\\) and a positional embedding matrix \\(E_{pos}\\). For an\ninput \\(x = [x_1,x_2,..., x_n]\\), we first calculate the initial activations\n\\[\nh^{(0)} = E([x_1,x_2,...,x_n]) = [E_{token}[x_1] + E_{pos}[1], E_{token}[x_2] + E_{pos}[2], ..., E_{token}[x_n]+ E_{pos}[n]].\n\\]\nSimilarly, the unembedding layer is parameterized by a single matrix, and model predictions have the\nform:\n\\[\np(x_{n+1} | x_{1...n}; E, F,U) \\propto softmax (Uh^{(m)})_{[n+1]},\n\\]\nwhere (in a slight abuse of notation) \\(E, F\\) and \\(U\\) denote embedding, intermediate, and unembedding\nparameters respectively.\nIn terms of parameters, let the hidden dimension be \\(d\\), the number of layers be \\(m\\), the vocabulary size\nbe \\(v\\) and the maximum context length be \\(n\\), the embedding layers have \\(\\Omega((n + v)d)\\) parameters as\nmatrix \\(E_{token}\\) and \\(U\\) have shape \\(v \\times d\\) and matrix \\(E_{pos}\\) has shape \\(n \\times d\\), while the full network has an\nextra \\((\\Omega(md^2)\\) parameters."}, {"title": "Random Transformers Can Perform Simple Algorithmic Tasks", "content": "Can random transformers be steered to perform meaningful tasks by optimizing only input and output\ntokens' embeddings? We begin by evaluating four widely-studied tasks that serve as toy models of\nimportant behaviors in large-scale LMs."}, {"title": "Tasks", "content": "This task evaluates models' ability to perform integer addition under a fixed\nprime modulus \\(p = 199\\). Models receive a sequence of input tokens \\([a, b]\\) for \\(a, b \\in [0, p - 1]\\) and\nmust compute \\((a + b)\\) mod \\(p\\). When over-parameterized models are trained to perform this task,\ngrokking (a long period of memorization followed by an abrupt transition to generalization [36]) is\ntypically observed [26, 18]. Neural sequence models of different kinds have been found to implement\ntwo interpretable algorithms, sometimes referred to as the \u201cClock\u201d [31] and \u201cPizza\u201d [52], when\ntrained to perform this task.\nThis task evaluates models' abilities to process long input sequences [4]. In\nthe variant we study, models receive as input a sequence of form \\([m_1, c_1, m_2, c_2, \u2026\u2026\u2026, m_k, c_k, m_u]\\).\nHere, \\(m_1, m_2,..., m_k\\) are distinct markers \\((k \\le 30)\\) and \\(c_i\\)'s are corresponding values. The input\nends with a marker \\(m_u\\) \\((u \\in [1, k])\\), and models must search for the previous occurrence of that\nmarker in the input sequence and output the corresponding \\(c_u\\). Specific circuits like induction heads\nare often observed in models that perform this task [34].\nThis task evaluates models' ability to perform arithmetic operations distributed\nover sequences of multiple input tokens in this case, addition of two equal-length numbers repre-\nsented as digit sequences in base 10. The order of digits of both numbers and the results are reversed\nto simplify the task. For example, the task 39+71=110 is encoded as a pair with input 9 3 1 7 and\noutput 0 1 1. We use 10-digit numbers in our setup. Past work has found that fully trained models\ncan reliably learn some versions of this task [32, 51, 43].\nIn this task, models are presented with a sequence of\nparentheses, and must predict whether they are balanced-i.e., whether the sequence contain an equal\nnumber of opening and closing parentheses, and within every prefix, the number of closing parentheses\nis no greater than the opening parentheses. Such sequences are also called Dyck sequences, and have\nbeen widely studied in language models because of their connection to context-free models of natural\nlanguage syntax [50, 47]. Note that this task has a vocabulary of size 4 (two parentheses and two\nlabels), so only a very small number of parameters are optimized by embedding-only training. In our\nsetup, the input parenthesis sequences have lengths at most 60.\nFor the modular addition task, we partition the full set of well-formed input-output pairs into a\nfixed train/test split; for the other problems, we pre-generate a fixed test set but randomly generate\nnew pairs for each training batch. Additional details may be found in Appendix D.1."}, {"title": "Results", "content": "Results are shown in Table 1. Here we compare random transformers with a hidden size of 1024 to\nfully trained models with hidden sizes of 16 and 1024. For reference, we also compare to a (fully"}, {"title": "Random Transformers Can Memorize and Generate Structured Sequences", "content": "The preceding experiments evaluated the ability of random transformers to implement single, highly\nstructured input-output mappings. Can these models scale to more challenging tasks, involving\nmemorization of arbitrary associations or even free-form text generation?"}, {"title": "Memorization", "content": "Past work by Allen-Zhu and Li [2] has found that fully trained transformers can store roughly two bits\nof input per parameter. We investigate whether a similar scaling trend holds for random transformers.\nWe study a simple memorization task in which we generate a random mapping from a two-integer\nkey to a one-integer value, with all integers ranging from 1 to 512. Such a function requires 9 bits per\ninput-output mapping to represent (\\(\\log_2 512 = 9\\)), and may be defined for up to 262144 (= \\(512^2\\))\nvalues. Unlike the algorithmic tasks above, here the learned function must be fully specified by\nembeddings rather than the pre-trained model, and these experiments mainly evaluate how efficiently\ninformation can be stored in these embeddings.\nWe evaluate fully trained and random transformers of width 128 and 2 layers (Table 3). We mea-\nsure success using an exact-match metric\u2014an input-output pair is considered to be successfully\nmemorized if the output token assigned highest probability by the model matches the training data.\nFully trained transformers memorized 80% of training examples, stored 2.9 bits per parameter,\nwhile random transformers memorized only 5% of examples, corresponding to 0.4 bits per trainable\nparameter."}, {"title": "Language Modeling", "content": "Modeling natural language requires both memorization of arbitrary associations (e.g. between words\nand their parts of speech, as in Section 5.1), and structured sequence generation procedures (e.g. to\nenforce subject-verb agreement and close quotation marks, as in Section 4). Can random transformers\nmake any headway on this task?\nWe train models on the TinyStories dataset, a collection of easy-to-understand stories generated by\nGPT-3.5 and GPT-4 [14] which have been shown to induce fluent text generation when used to train"}, {"title": "Random Transformers Operate in Low-Dimensional Subspaces", "content": "Can we explain the success of random transformers in the tasks studied above? In this section,\nwe present evidence that embedding-only training steers the hidden computation in transformers\ninto low-dimensional subspaces in which target functions are already implemented. We term this\nphenomenon subspace selection, and show that it is distinct from sparsification, as these subspaces\nare distributed across neurons.\nIn Section 6.1, we measured fraction of activation variance explained by top principal components in\nvarious tasks. For algorithmic tasks, we show that both normal and random transformers work in\nlow-dimensional subspaces, which are sufficient for solving these tasks (Appendix F). However, for\nlanguage modeling and memorization, the random transformers displayed more subspace selection\ncompared to the fully trained ones, and as a result, they attained lower performances. In Section 6.2\nwe constructed a task that explicitly requires operating on high-dimensional spaces, circuit imitation,\nand indeed, the random transformers exhibit significant performance gap compared to normal\ntransformers."}, {"title": "Low-Dimensional Hidden Representations in Algorithmic and Memorization Tasks", "content": "To characterize the geometry of random transformers' internal representations, we present models\n(trained for all previously described tasks) with a set of randomly chosen inputs and collect their\nembeddings and hidden representations of these inputs at different layers. Using these representations,\nwe perform two analyses: (1) the fraction of variance explained by the top principal components of\nthese hidden representations (which will be large if representations lie in a low-dimensional subspace),\nand (2) the fraction of variance explained by the most variable entries in hidden state vectors, or\nneurons (which will be large if computation is confined to a sparse sub-network).\nBoth fully trained and random transformers exhibit subspace selection but not sparsification (Table 4\ntop) in the four algorithmic tasks. In Appendix F, we show that this behavior is expected, insofar\nas all four algorithmic tasks can be solved by shallow transformer-like circuits that operate in low-\ndimensional subspaces. On memorization and language modeling tasks, random transformers become\nmuch more concentrated on a small subspace than fully trained transformers, thus using a lower\neffective dimension (Table 4 bottom and Table 5). In the language modeling task, more than 30% of\nvariance in hidden representations is explained by 10 components."}, {"title": "Subspace Selection in Circuit Imitation", "content": "To provide another window into these results, we characterize how large the hidden representations\nof a random model must be for it to simulate a random circuit that operates in a low-dimensional\nsubspace.\nTo do so, we first generate a small, random target transformer associated with a distribution over\nstrings \\(p\\), then perform embedding-only training in a different, randomly initialized transformer to\nsimulate its behavior on some domain of interest by minimizing:\n\\[\narg \\underset{E,U}{min} E_x [KL(p(\\cdot | x) || p(\\cdot | x; E, F, U)]\n\\]\nTo construct target models, we begin with the same initialization scheme described in Section 3, then\nwe scale the query and key parameters in the attention mechanism by a factor of 10, and and the feed\nforward weights and biases by a factor of 20. We also scale the final projection layer by \\(100/\\sqrt{width}\\).\n(This initialization scheme increases variability of attention patterns and target model predictions\nacross random restarts; see Appendix D.2.2 for additional discussion.)"}, {"title": "Discussion", "content": "We have shown that transformer sequence models can accomplish a variety of meaningful tasks when\nonly their embedding layers are optimized. For tasks involving memorization, these results show that\nmuch of models \u201cknowledge\u201d can be encapsulated by input embeddings rather than models' internal\nparameters. For more algorithmic tasks like arithmetic and parenthesis balancing, which require\nrelatively sophisticated circuits to perform, our experiments show that versions of these circuits\ncan be accessed in random transformers (but not LSTM sequence models) simply by constructing\nappropriate input and output embeddings that confine models' internal states to low-dimensional\nsubspaces in which these tasks are performed."}, {"title": "Limitations", "content": "Even within the class of transformers, the space of architectural decisions (both around model size\nand implementation of attention mechanisms, normalization procedures, tokenization, etc.) is very\nlarge; our experiments in this paper generally characterize a small part of this phase space. It is thus\npossible that some of the described trends will change as models grow or differ in parameterization\ndetails. Outside Section 4, our experiments have focused on standard transformer models, and do\nnot answer whether these trends hold in other related linear attention [9, 35] or state-space model\n[19] families. Our discussion in Section 6 focused only on linear subspaces and used principal\ncomponent analysis as the primary tool, but it is also possible that subspaces or latent semantics\nappear non-linearly which is not addressed by our current analysis."}, {"title": "Impact statement", "content": "We do not anticipate any ethical concerns associated with these results and we believe understanding\nAl systems is a crucial step in harnessing their power for good."}, {"title": "Computational Resources", "content": "Roughly 154 GPU days of NVidia V100 were spent on this project."}, {"title": "Setup details", "content": ""}, {"title": "Data Curation and Tokenization Details", "content": ""}, {"title": "Modular Addition", "content": "Fix modulus \\(p = 199\\). We randomly shuffle all possible inputs (\\(p^2\\) of them) perform a 95%: 5% for\ntraining and test set. The split is the same (generated with the same seed and procedure) for all runs\nacross all architectures."}, {"title": "Dynamically Generated Tasks", "content": "For these tasks, we used a stream of training data and a heldout test set. The test set is fixed for all\nruns across all architectures. The training data is generated on the fly within the same distribution to\navoid overfitting."}, {"title": "Needle-in-a-Haystack", "content": "The number of entities (marker-value pairs) is first uniformly generated\nin \\([1,30]\\). The values are generated as integers in \\([1,127]\\). The markers are generated as distinct\nintegers in \\([127 + 1,128 + 30]\\). The final query token is the asked marker (uniformly chosen) plus\n30. For example, a 1 b 2 c 3d 4 b in token ids would be \\([128, 1, 129, 2, 130, 3, 131, 4, 159]\\) and\nthe expected output will be token 2."}, {"title": "Decimal Addition", "content": "Fix the number of digits \\(l = 10\\). The two numbers to be added are independently\nuniformly generated within \\([10^9, 10^{10} \u2013 1]\\). The added numbers and the results are reversed. The\naddition sign has token id 10 and the equal sign has id 11. For example, 1111111112+2222222223 =\nin token ids would be \\([2, 1, \u00b7\u00b7\u00b7, 1, 10, 3, 2, \u2026, 2, 11]\\). The result uses \\([20, 29]\\) to represent digits and\n30 to signal the end of output. For the example, the expected output will be 3333333335 encoded as\n\\([25, 23, 23, 30]\\)."}, {"title": "Parentheses Balancing", "content": "The opening parenthesis has token id 1, the closing parenthesis has token id 2, and the question mark\nhas token id 3. For the result, 2 signals balanced and 1 signals unbalanced. For example, (())()? in\ntoken ids will be \\([1, 1, 2, 2, 1, 2, 3]\\) and the expected output is balanced, token 2."}, {"title": "Memorization", "content": "For every integer pair \\(x \\in [0,511], y \\in [512,512 + 511]\\), we generate one data point with input\n\\([x, y]\\) and output \\(z\\) uniform in \\([0,511]\\). There is no \u201ctest split\u201d as the goal is to memorize all the\nassociations."}, {"title": "Language Modeling", "content": "We used the original train-test split of the TinyStories dataset [14]. We trained a 10000-token BPE\ntokenizer from the training split alone."}, {"title": "Model Details", "content": ""}, {"title": "Transformers", "content": "We use the GPT-2 [38] implementation of Huggingface [48]. Dropout and weight tying are disabled.\nThe activation function is kept as the default GeLU [20].\nSpecifically, all the weights of feed-forward layers are initialized by sampling from isotropic Gaus-\nsians with mean 0 and standard deviation \\(0.02/\\sqrt{2n}\\) where \\(n\\) in the number of layers. All the bias\nmatrices are initialized with zeroes. All the other weight matrices (including key, value, query,\nembedding, unembedding matrices) are initialized with 0-mean Gaussians with standard deviation\n0.02. The affine transformations in layer normalizations are initialized as identity.\nWe used two layer transformers except for the language modeling and circuit imitation task. The\nnumber of parameters for the algorithmic tasks can be found in Table 6."}, {"title": "Target Transformer in Circuit Imitation", "content": "Based on the initialization in Appendix D.2.1, we make the following modifications.\nFeed-forward Layers: Standard deviation scaled up by 20x: changed to \\(0.4/\\sqrt{2n}\\)."}, {"title": "Training Details", "content": "For synthetic experiments, we used AdamW optimizer [27] with a learning rate \\(10^{-3}\\) and weight\ndecay \\(10^{-3}\\). For LSTM a learning rate \\(5 \\times 10^{-3}\\) is used for faster convergence. For the language\nmodeling task, we used AdamW optimizer with a learning rate \\(6 \\times 10^{-4}\\) and weight decay 0.1. We\nclip all gradient norms at 1.\nFor random transformer experiments, the intermediate (query, embedding, unembedding, feed-\nforward) layers are kept as randomly initialized. We use a fixed number of training steps and no early\nstopping.\nModular Addition: 5000 epoches. Batch size 4000.\nNeedle-in-a-Haystack, Decimal Addition, Parentheses Balancing, Circuit Imitation: \\(10^4\\) steps\nof batch size 1000. Again, the training data is generated dynamically.\nMemorization: 21000 epoches. Batch size \\(2^{15}\\).\nLanguage Modeling: 5 epoches. Batch size 20 and context window 512."}, {"title": "Performance on synthetic tasks across model scales", "content": "In Fig. 7 we provide the test accuracy of needle-in-a-haystack, decimal addition and parenthese\nbalancing for fully trained and random transformers across different model widths. Note that the\n1024-width fully trained transformer had trouble reaching perfect accuracy in parentheses balancing,\nlikely due to imperfect hyperparameter choices.\nWe also include the explained variance measurements on 512-width models in these three tasks for\ncompleteness (Table 7). Generally more variances are explained from the top directions as the models\nare narrower."}, {"title": "Constant-dimensional subspaces are sufficient for many synthetic tasks", "content": "Definition F.1 (Intermediate complexity of tasks (informal)). For a family of neural networks with\nvarying intermediate width, we define the intermediate complexity of a task as the minimal width of\nintermediate layers required to succeed in the task.\nFor example, a 2-layer transformer with intermediate width 2 starts by embed the tokens into the\n2-dimensional hidden space, passes them through two transformer blocks of width 2, then linearly"}]}