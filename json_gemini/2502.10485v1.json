{"title": "Forecasting time series with constraints", "authors": ["Nathan Doum\u00e8che", "Francis Bach", "Eloi Bedek", "G\u00e9rard Biau", "Claire Boyer", "Yannig Goude"], "abstract": "Time series forecasting presents unique challenges that limit the effectiveness of traditional machine learning algorithms. To address these limitations, various approaches have incorporated linear constraints into learning algorithms, such as generalized additive models and hierarchical forecasting. In this paper, we propose a unified framework for integrating and combining linear constraints in time series forecasting. Within this framework, we show that the exact minimizer of the constrained empirical risk can be computed efficiently using linear algebra alone. This approach allows for highly scalable implementations optimized for GPUs. We validate the proposed methodology through extensive benchmarking on real-world tasks, including electricity demand forecasting and tourism forecasting, achieving state-of-the-art performance.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting. Time series data are used extensively in many contemporary applications, such as forecasting supply and demand, pricing, macroeconomic indicators, weather, air quality, traffic, migration, and epidemic trends (Petropoulos et al., 2022). However, regardless of the application domain, forecasting time series presents unique challenges due to inherent data characteristics such as observation correlations, non-stationarity, irregular sampling intervals, and missing values. These challenges limit the availability of relevant data and make it difficult for complex black-box or overparameterized learning architectures to perform effectively, even with rich historical data (Lim and Zohren, 2021).\nConstraints in time series. In this context, many modern frameworks incorporate physical constraints to improve the performance and interpretability of forecasting models. The strongest form of such constraints are typically derived from fundamental physical properties of the time series data and are represented by systems of differential equations. For example, weather forecasting often relies on solutions to the Navier-Stokes equations (Schultz et al., 2021). In addition to defin-"}, {"title": "2 Incorporating constraints in time series forecasting", "content": "Throughout the paper, we assume that n observations $(Xt_1, Yt_1), ..., (Xt_n, Yt_n)$ are drawn on $\\mathbb{R}^{d_1} \\times \\mathbb{R}^{d_2}$. The indices $t_1,...,t_n \\in T$ correspond to the times at which an unknown stochastic process $(X,Y) := (Xt, Yt)_{t\\in T}$ is sampled. Note that, all along the paper, the time steps need not be regularly sampled on the index set $T \\subset \\mathbb{R}$. We focus on supervised learning tasks that aim to estimate an unknown function $f^* : \\mathbb{R}^{d_1} \\rightarrow \\mathbb{R}^{d_2}$, under the assumption that $Yt = f^*(Xt) + \\mathcal{E}_t$, where $\\mathcal{E}$ is a random noise term. Without loss of generality, upon rescaling, we assume that $Xt := (X_{1,t},..., X_{d_1,t}) \\in [-\\pi,\\pi]^{d_1}$ and $-\\pi \\leq t_1 \\leq ... < t_{n+1} \\leq \\pi$. The goal is to construct an estimator f for f*.\nA simple example to to keep in mind is when Y is a stationary, regularly sampled time series with $t_j = j/n$, and the lagged value $X_j = Y_{t_{j-1}}$ serves as the only feature. In this specific case, where $d_1 = d_2$, the model simplifies to $Yt = f^*(Y_{t-1/n}) + \\mathcal{E}_t$. Thus, the regression setting reduces to an autoregressive model. Of course, we will consider more complex models that go beyond this simple case.\nModel parameterization. We consider parameterized models of the form\n$\\begin{equation}\n    f_\\theta(X_t) = (f^1_\\theta(X_t),..., f^{d_2}_\\theta(X_t)) = (\\langle \\phi_1(X_t), \\theta_1 \\rangle,..., \\langle \\phi_{d_2}(X_t), \\theta_{d_2} \\rangle),\n\\end{equation}$\nwhere each component $f^\\ell_\\theta(X_t)$ is computed as the inner product of a feature map $\\phi_\\ell(X_t) \\in \\mathbb{C}^{D_\\ell}$, with $D_\\ell \\in \\mathbb{N}^*$, and a vector $\\theta_\\ell \\in \\mathbb{C}^{D_\\ell}$. The parameter vector $\\theta \\in \\mathbb{C}^{D_1+\\dots+D_{d_2}}$ of the model is defined as the concatenation of $\\theta_1, ..., \\theta_{d_2}$. Note that $f_\\theta$ is uniquely determined by $\\theta$ and the maps $\\phi_\\ell$. To simplify the notation, we write dim($\\theta$) = $D_1 + \\dots + D_{d_2}$.\nOur goal is to learn a parameter $\\theta\\in \\mathbb{C}^{\\text{dim}(\\theta)}$ such that $f_\\theta(X_t)$ is an estimator of the target $Y_t$. Equivalently, $f_\\theta$ is an estimator of the target function $f^*$. To this end, the core principle of our approach is to consider $\\theta$ to be a minimizer over (dim($\\theta$)) of an empirical risk of the form\n$\\begin{equation}\n    L(\\theta) = \\frac{1}{n} \\sum_{j=1}^{n} ||A(f_\\theta(X_{t_j}) - Y_{t_j})||^2 + ||M \\theta||^2,\n\\end{equation}"}, {"title": "3 Shape constraints", "content": "3.1 Mathematical formulation\nIn this section, we introduce relevant feature maps $\\phi$ that incorporate prior knowledge about the shape of the function $f^* : [-\\pi,\\pi]^{d_1} \\rightarrow \\mathbb{C}^{d_2}$. To simplify the notation, we focus on the one-dimensional case where $d_2 = 1$ and $A = 1$. This simplification comes without loss of generality, since the feature maps developed in this section can be applied directly to (1).\nAs a result, the model reduces to $f_\\theta(X_t) = \\langle \\phi_1(X_t), \\theta_1 \\rangle$, and (3) simplifies to\n$\\begin{equation}\n    \\hat{\\theta} = (\\Phi^*\\Phi + nM^*M)^{-1} \\Phi^*Y,\n\\end{equation}$\nwhere $Y = (Y_{t_1}, ..., Y_{t_n})^\\top \\in \\mathbb{R}^n$ and the $n \\times \\text{dim}(\\theta)$ matrix $\\Phi$ takes the form\n$\\begin{equation}\n    \\Phi = (\\phi_1(X_{t_1}) | \\dots | \\phi_1(X_{t_n}))^*.\n\\end{equation}$\nNote that $\\Phi$ is the classical feature matrix, and that it is related to the matrix $\\Phi_t$ of (4) by $\\Phi^*\\Phi = \\sum_{j=1}^{n} \\Phi_{t_j} \\Phi_{t_j}^* = \\sum_{j=1}^{n} \\phi_1(X_{t_j}) \\phi_1(X_{t_j})^*$.\nAdditive model: Additive WeaKL. The additive model constraint assumes that $f^*(x_1,..., X_{d_1}) = \\sum_{\\ell=1}^{d_1} g^\\ell(x_\\ell)$, where $g^\\ell : \\mathbb{R} \\rightarrow \\mathbb{R}$ are univariate functions. This constraint is widely used in data science, both in classical statistical models (Hastie and Tibshirani, 1986) and in modern neural network architectures (Agarwal et al., 2021). Indeed, additive models are interpretable because the effect of each feature $x_\\ell$ is captured by its corresponding function $g^\\ell$. In addition, univariate effects are easier to estimate than multivariate effects (Ravikumar et al., 2009). These properties allow the development of efficient variable selection methods (see, for example, Marra and Wood, 2011), similar to those used in linear regression.\nIn our framework, the additivity constraint directly translates into the model as\n$f_\\theta(X_t) = \\langle \\phi_1(X_t), \\theta_1 \\rangle = \\langle \\phi_{1,1}(X_{1,t}), \\theta_{1,1} \\rangle + \\dots + \\langle \\phi_{1,d_1}(X_{d_1,t}), \\theta_{1,d_1} \\rangle$,\nwhere $\\phi_1$ is the concatenation of the maps $\\phi_{1,\\ell}$, and $\\theta_1$ is the concatenation of the vectors $\\theta_{1,\\ell}$. Note that the maps $\\phi_{1,\\ell}$ and the vectors $\\theta_{1,\\ell}$ can be multidimensional, depending on the model. In this formulation, the effect of each feature is modeled by the function $g_\\ell(x_\\ell) = \\langle \\phi_{1,\\ell}(x_\\ell), \\theta_{1,\\ell} \\rangle$, which can be either linear or nonlinear in $x_\\ell$. The empirical risk then takes the form\n$\\begin{equation}\n    L(\\theta) = \\frac{1}{n} \\sum_{j=1}^{n} |f_\\theta(X_{t_j}) - Y_{t_j}|^2 + \\sum_{\\ell=1}^{d_1} \\lambda_\\ell || M_\\ell \\theta_{1,\\ell}||^2,\n\\end{equation}$\nwhere $\\lambda_\\ell > 0$ are hyperparameters and $M_\\ell$ are regularization matrices. There are three types of effects that can be taken into account:\n(i) A linear effect is obtained by setting $\\phi_{1,\\ell}(x_\\ell) = x_\\ell \\in \\mathbb{R}$. To regularize the parameter $\\theta_{1,\\ell}$, we set $M_\\ell = 1$. This corresponds to a ridge penalty.\n(ii) A nonlinear effect can be modeled using the Fourier map $\\phi_{1,\\ell}(x_\\ell) = (\\exp(ikx_\\ell/2))_{-m \\leq k < m}^\\top$. To regularize the parameter $\\theta_{1,\\ell}$, we set $M_\\ell$ to be the $(2m + 1) \\times (2m + 1)$ diagonal matrix defined by $M_\\ell = \\text{Diag}((\\sqrt{1 + k^{2s}})_{-m \\leq k < m})$, penalizing the Sobolev norm. A common choice for the smoothing parameter s, as used in GAMs, is s = 2 (see, e.g., Wood, 2017)."}, {"title": "4 Learning constraints", "content": "4.1 Mathematical formulation\nSection 3 focused on imposing constraints on the shape of the regression function $f^*$. In contrast, the goal of the present section is to impose constraints on the parameter $\\theta$. We begin with a general method to enforce linear constraints on $\\theta$, and subsequently apply this framework to transfer learning, hierarchical forecasting, and differential constraints.\nLinear constraints. Here, we assume that $f^*$ satisfies a linear constraint. By construction of $f_\\theta$ in (1), such a linear constraint directly translates into a constraint on $\\theta$. For example, the linear constraint $f^*_1(X_t) = 2f^*_2(X_t)$ can be implemented by enforcing $\\theta_1 = 2\\theta_2$. Thus, in the following, we assume a prior on $\\theta$ in the form of a linear constraint. Formally, we want to enforce that $\\theta \\in S$, where S is a known linear subspace of $\\mathbb{C}^{\\text{dim}(\\theta)}$. Given an injective dim($\\theta$) $\\times$ dim(S) matrix P such that Im(P) = S, then, as shown in Lemma A.2, $||C\\theta||_3$ is the square of the Euclidean distance"}, {"title": "Appendix A. Proofs", "content": "The purpose of this appendix is to provide detailed proofs of the theoretical results presented in the main article. Appendix A.2 elaborates on the formula that characterizes the unique minimizer of the WeaKL empirical risks, while Appendix A.3 discusses the integration of linear constraints into the empirical risk framework.\nA.1 A useful lemma\nLemma A.1 (Full rank) The matrix\n$\\begin{equation}\n    M = (\\frac{1}{n} (\\sum_{j=1}^{n} \\Phi^* \\Phi) + M^*M\n\\end{equation}$\nis invertible. Moreover, for all $\\theta \\in \\mathbb{C}^{\\text{dim}\\theta}$, $\\theta^*M\\theta > \\lambda_{\\text{min}}(M)||||\\theta|||^2$, where $\\lambda_{\\text{min}}(M)$ is the minimum eigenvalue of M.\nProof First, we note that M is a positive Hermitian square matrix. Hence, the spectral theorem guarantees that M is diagonalizable in an orthogonal basis of $\\mathbb{C}^{\\text{dim}(\\theta)}$ with real eigenvalues. In particular, it admits a positive square root, and the min-max theorem states that $\\theta^*M\\theta = ||M^{1/2}\\theta||^2 \\geq \\lambda_{\\text{min}}(M^{1/2})^2||\theta||^2 = \\lambda_{\\text{min}}(M)||||\\theta|||^2$. This shows the second statement of the lemma.\nNext, for all $\\theta \\in \\mathbb{C}^{\\text{dim}\\theta}$, $\\theta^* M\\theta > \\theta^*M^*M\\theta$. Since M is full rank, rank(M) = dim($\\theta$). Therefore, $M\\theta = 0 \\Rightarrow \\theta^*M\\theta = 0 \\Rightarrow \\theta^*M^*M\\theta = 0 \\Rightarrow ||M\\theta||^2 = 0 \\Rightarrow M\\theta = 0 \\Rightarrow \\theta = 0$. Thus, M is injective and, in turn, invertible.\nA.2 Proof of Proposition 2.1\nThe function $L : \\mathbb{C}^{\\text{dim}(\\theta)} \\rightarrow \\mathbb{R}_+$ can be written as\n$\\begin{equation}\n    L(\\theta) = \\frac{1}{n} \\sum_{j=1}^{n} (\\Phi_{t_j}\\theta - Y_{t_j})^*((\\Phi_{t_j}\\theta - Y_{t_j})) + \\theta^* M^* M \\Theta.\n\\end{equation}$\nRecall that the matrices A and M are assumed to be injective. Observe that L can be expanded as\n$L(\\theta + \\delta\\theta) = L(\\theta) + 2\\text{Re}((M\\theta - \\tilde{Y}, \\delta\\theta)) + \\theta(||\\delta\\theta||^2)$,\nwhere $\\tilde{Y} = \\sum_{j=1}^{n} \\Phi_{t_j}A^*A Y_{t_j}$. This shows that L is differentiable and that its differential at $\\theta$ is the function $dL_\\theta : \\delta\\theta \\rightarrow 2\\text{Re}((M\\theta - \\tilde{Y}, \\delta\\theta))$. Thus, the critical points $\\theta$ such that $dL_\\theta = 0$ satisfy\n$\\forall \\delta\\theta \\in \\mathbb{C}^{\\text{dim}(\\theta)}, \\text{Re}((M\\theta - \\tilde{Y}, \\delta\\theta)) = 0$.\nTaking $\\delta\\theta = M\\theta - \\tilde{Y}$ shows that $||M\\theta - \\tilde{Y} ||^2 = 0$, i.e., $M\\theta = \\tilde{Y}$. From Lemma A.1, we deduce that $\\theta = M^{-1}\\tilde{Y}$, which is exactly the $\\hat{\\theta}$ in (3).\nFrom Lemma A.1, we also deduce that, for all $\\theta$ such that $||\theta||_2$ is large enough, one has $L(\\theta) \\geq \\lambda_{\\text{min}}(M)||||\\theta|||^2/2$. Since L is continuous, it has at least one global minimum. Since the unique critical point of L is $\\hat{\\theta}$, we conclude that $\\hat{\\theta}$ is the unique minimizer of L."}]}