{"title": "People who frequently use ChatGPT for writing tasks\nare accurate and robust detectors of AI-generated text", "authors": ["Jenna Russell", "Marzena Karpinska", "Mohit Iyyer"], "abstract": "In this paper, we study how well humans can de-\ntect text generated by commercial LLMs (GPT-\n40, CLAUDE-3.5-SONNET, 01-PRO). We hire\nannotators to read 300 non-fiction English ar-\nticles, label them as either human-written or\nAI-generated, and provide paragraph-length\nexplanations for their decisions. Our experi-\nments show that annotators who frequently use\nLLMs for writing tasks excel at detecting AI-\ngenerated text, even without any specialized\ntraining or feedback. In fact, the majority vote\namong five such \"expert\" annotators misclas-\nsifies only 1 of 300 articles, significantly out-\nperforming most commercial and open-source\ndetectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humaniza-\ntion. Qualitative analysis of the experts' free-\nform explanations shows that while they rely\nheavily on specific lexical clues (\"AI vocab-\nulary\"), they also pick up on more complex\nphenomena within the text (e.g., formality, orig-\ninality, clarity) that are challenging to assess\nfor automatic detectors. We release our anno-\ntated dataset and code to spur future research\ninto both human and automated detection of\nAI-generated text.", "sections": [{"title": "Introduction", "content": "Text generated by large language models (LLMs)\nis rampant in today\u2019s world.\u00b9 This state of affairs\nhas spurred increased research into automatic de-\ntection of AI-generated text, which helps combat\nacademic plagiarism (Zhu et al., 2024) and fake\ncontent creation (Gameiro et al., 2024). Unfortu-\nnately, automatic detectors suffer from low detec-\ntion rates, poor robustness to evasion attempts, and"}, {"title": "How good are humans at detecting\nAI-generated text?", "content": "Automatic detectors of AI-generated text suffer\nfrom low detection rates and poor robustness to\nsimple perturbations of the text (e.g., paraphras-\ning), rendering them unreliable for real-world de-\nployment (Sadasivan et al., 2024). How well do\nhuman detectors stack up against their automatic\ncounterparts? What features of a candidate text\ndo humans find most predictive of AI writing? To\nanswer these questions, we conducted five experi-\nments that ask humans to judge increasingly more\ndeceptive AI-generated texts (i.e., produced using\nmore powerful base LLMs and evasion tactics).\nTask setup: Each experiment consists of a batch\nof 60 articles, 30 of which are written by hu-\nmans and the remaining 30 generated by an LLM.\nEach human-written article has a corresponding\nAI-generated counterpart with the same title and\nsubtitle to control for high-level content and topics.\nThe 60 articles are shown to annotators one at a\ntime in a randomized order, and they are asked to\nprovide the following for each article, depicted in"}, {"title": "Experiment 1: What kinds of\nannotators can reliably detect\nAI-generated articles?", "content": "Is there a population of humans that is highly ac-\ncurate at our detection task, and if so, are there any\ncommonalities between them? To explore these\nquestions, we first ask five annotators with varying\nbackgrounds and levels of expertise with LLMs\nto attempt a batch of 60 articles (half generated\nby GPT-40 2024-08-06 and half human-written as\ndescribed above). We observe both ends of the\nspectrum in terms of performance: annotators who\nrarely or never use LLMs for writing tasks per-\nformed near random, while one annotator who uses\nLLMs (and edits LLM-generated text) daily per-\nformed almost perfectly. More details on our initial\nstudy can be found in \u00a7C. Inspired by this result,\nwe recruit four more annotators with similar back-\ngrounds as our top performer, all of whom perform\nsimilarly well on the same batch of articles.\nAnnotators with limited LLM experience are un-\nreliable detectors: The four annotators who self-\nreport either not using LLMs at all, or using LLMs\nonly for non-writing tasks, detect AI-generated text\nat a similar rate to random chance, achieving an av-\nerage TPR of 56.7% and FPR of 52.5% . While\nperformance among these annotators varies\ngreatly, no single annotator could reliably detect\nAI-generated text. We term this population nonex-\nperts for the rest of this paper.\nAnnotators who frequently use LLMs for writ-\ning tasks are highly accurate: In contrast, the\nfive annotators who have significant experience\nwith using LLMs for writing-related tasks are able\nto detect AI-generated text very reliably, achieving\na TPR of 92.7%. The average FPR for this popu-\nlation of annotators was 3.3%, meaning that they\nrarely mistake human-written text as AI-generated.\nThe majority vote out of these five annotators cor-\nrectly determined authorship of all 60 articles; the\n\"GPT-40\" column of contains more details.\nWe term this population experts for the rest of this\npaper; all subsequent experiments rely on only\nthese five expert annotators."}, {"title": "Experiment 2: Can experts detect\narticles generated by a different LLM?", "content": "While ChatGPT is the most widely-used LLM\n(both in general and by our expert annotators), com-\npetitors such as Anthropic's Claude (Anthropic,\n2023) also have a wide user base. Are our experts\noverfitting to artifacts specific to GPT-40, or do they\ndetect patterns that generalize to other LLMs? We\naddress this question in Experiment 2 by evaluating\nour experts on a second batch of 60 articles, this\ntime with 30 new human-written articles and 30\ncorresponding articles generated via CLAUDE-3.5-\nSONNET (Anthropic, 2024) using the same instruc-\ntions as in Experiment 1."}, {"title": "Experiment 3: How robust are experts\nto paraphrasing attacks?", "content": "Users of LLMs who hope to evade detection of-\nten resort to simple post-hoc modifications of LLM\noutputs such as paraphrasing. These tactics sig-\nnificantly drop detection rates of automatic meth-\nods (Krishna et al., 2023; Sadasivan et al., 2024),\nbut how do they affect our human experts? We\nevaluate the five expert annotators on a third set of\n60 articles, where the 30 AI-generated articles go\nthrough an additional round of paraphrasing, and\nwe show that they maintain high TPR and low FPR.\nCreating paraphrased AI-generated articles:\nWe follow the sentence-by-sentence paraphrasing\napproach with LLMs outlined by Chang et al.\n(2024) to create a more difficult set of AI articles\nfor this experiment. Additionally, since our experts"}, {"title": "Experiment 4: Can experts keep up\nwith advances in LLM reasoning\ncapabilities?", "content": "While we were in the middle of conducting ex-\nperiments, OpenAI released their 01 model (Ope-\nnAI, 2024), ushering in a new paradigm of test-time\nscaling. This release offered us a unique opportu-\nnity: since none of our experts had been previously\nexposed to a model with such advanced reasoning\ncapabilities (Zhong et al., 2024), how well would\nthey do at detecting its output? Interestingly, our ex-\nperts remain reliable detectors of articles generated\nby 01-PRO (using the same prompt as in Experi-\nments 1 and 2), although their comments show\nthat they often perform a more detailed analysis of\narticles to make their judgments.\nFour out of five experts are robust to 01-Pro:\nFor four of five experts, detection ability of 01-\ngenerated content remains largely consistent with\nprior experiments, with a majority vote TPR of\n96.7% and FPR of 0% Interestingly, Annotator 3's TPR drops"}, {"title": "Experiment 5: Are experts robust to\nhumanization?", "content": "As the deployment of automatic AI detectors in-\ncreases, many users are attempting to evade them\nby using humanization methods, which explicitly\nattempt to make AI-generated text more human-\nlike (Wang et al., 2024a,b) unlike more generic\nattacks such as paraphrasing. Are such methods\neffective at evading our human experts? Since no\nwell-established humanization methods exist, we\nfirst create our own humanizer by prompting 01-\nPRO with a detailed set of instructions derived from\nexpert comments written for Experiments 1-4. De-\nspite considerably degrading performance of many\nautomatic detectors, the majority of our experts re-\nmain robust to humanization (\u201c01-PRO HUMANIZED"}, {"title": "Fine-grained analysis of expert\nperformance", "content": "The previous section establishes that human experts\nare highly accurate detectors of AI-generated text\neven in the presence of evasion attempts. Here, we\nfirst provide in-depth comparisons between experts\nand automatic detectors. Next, we perform a fine-\ngrained coding of expert explanations into different\ncategories (e.g., vocabulary, originality, formality),\nwhich allows us to examine what they focus on dur-\ning the different experiments. Finally, we analyze\ndifferences between annotators and what they focus\non when they are incorrect, highlighting implica-\ntions for the future training of human annotators\nfor AI-generated text detection.\nHuman experts vs. automatic detectors: We\nbegin with a direct comparison between our human\nexperts and state-of-the-art AI detectors. Specifi-\ncally, we run five different detectors on our set of\n300 articles:"}, {"title": "Can LLMs be prompted to mimic\nhuman expert detectors?", "content": "Our experiments so far highlight the benefits of\nhiring humans to perform AI-generated text de-\ntection: in aggregate, they are more accurate and\nrobust than all automatic detectors we tried. Ad-\nditionally, they can provide detailed explanations\nof their decision-making process, unlike all of the\nautomatic detectors in our study. However, an ob-\nvious drawback is that hiring humans is expensive\nand slow: on average, we paid $2.82 per article\nincluding bonuses, and we gave annotators roughly\na week to complete a batch of 60 articles. In this\nsection, we prompt LLMs to imitate our expert\nannotators, providing the guidebook used in our\nhumanization experiments (\u00a72.5) to the model and\nasking it to produce an explanation and a label.\nWhile the approach shows promise, outperforming\ndetectors such as Binoculars and RADAR, it fails\nto reach the performance of our human experts as\nwell as advanced automatic detectors like Pangram.\nImplementing a prompt-based detector: Us-\ning our guidebook for AI detection, we\nprompt an off-the-shelf LLM to decide whether a\ncandidate text is human-written or AI-generated\nand explain its decision based on the criteria in\nthe guidebook (see Table 21 for prompt template).\nWhile we can control this detector's behavior by\nmodifying the prompt (e.g., provide explanations\nbefore producing a final label), it does not output a\nsingle score, which makes setting a false positive\nthreshold impossible.  We derive the final output\nof our detector by using the prediction from the\ndetection model in a deterministic configuration.\nPrompt-based detection shows promise but\nstruggles with humanization and high false posi-\ntive rates: We implement our prompt-based de-\ntector by zero-shot prompting GPT-40-2024-11-20\nand 01, comparing the effects of chain-of-thought\n(CoT) prompting and the inclu-\nsion of the detection guide on its performance. For\nGPT-40, the best detector configuration is achieved"}, {"title": "Related work", "content": "Our work builds on prior research centering around\nAI-generated text detection using both human and\nautomatic approaches, as well as recent work on\nevading such detectors.\nHuman detection of AI-generated text: Our\nstudy most closely resembles several papers pub-\nlished prior to ChatGPT's release. Existing work\nnotes that while na\u00efve annotators do not reliably de-\ntect AI-generated texts (Ippolito et al., 2020; Brown\net al., 2020; Clark et al., 2021; Karpinska et al.,\n2021), some annotators manage to perform very\nwell (Ippolito et al., 2020). More recently, these\nfindings have been reproduced in other domains\nsuch as Italian news articles (Puccetti et al., 2024)\nand poetry (Porter and Machery, 2024). Dugan\net al. (2020) and Dugan et al. (2022) address a\nmore complex task where humans and machines\nare tasked to identify boundary between human and\nAI-generated text in mixed documents.\nAutomatic detection of AI-generated text:\nSuccessful automatic detection methods are typ-\nically either perplexity-based (Mitchell et al., 2023;\nBao et al., 2023; Hans et al., 2024) or trained\nclassifiers (Solaiman et al., 2019; Emi and Spero,\n2024; Verma et al., 2023). Some detectors have\nfocused on sentence-level detection (Kushnareva\net al., 2024; Wang et al., 2023), emphasizing the\nneed for more detailed and explainable detection.\nDugan et al. (2024), Li et al. (2024), and (Zhang\net al., 2024) attempt to compare the performance\nof detectors in different domains and adversar-\nial attack settings. However, most automatic de-\ntectors are unreliable in the face of attacks such\nas paraphrasing (Krishna et al., 2023; Sadasivan"}, {"title": "Conclusion", "content": "Our paper demonstrates that a population of \u201cex-\npert\" annotators\u2014those who frequently use LLMs\nfor writing-related tasks are highly accurate and\nrobust detectors of AI-generated text without any\nadditional training. The majority vote of five such\nexperts performs near perfectly on a dataset of 300\narticles, outperforming all automatic detectors ex-\ncept the commercial Pangram model (which the ex-\nperts match). Analysis of explanations provided by\nour expert annotators reveals that they pick up on\nnot just vocabulary and sentence structure-related\nclues but also more complex properties like origi-\nnality, factuality, and tone. We observe that each"}, {"title": "Human Evaluation", "content": "In this section of the appendix we provide addi-\ntional details about our experts the data collection\npipeline.\nAnnotators: The annotations for the experiment\n1 were done by 5 annotators recruited on Upwork.\nAll annotators are native English speakers from the\nUS or South Africa. All annotators hold univer-\nsity degrees, worked in varying professions, and\nhad varying levels of familiarity with AI assistants\nlike ChatGPT. One had never used AI, 2 had little\nexperience, and 2 used Al every day. Our five ex-\npert annotators are native English speakers hailing\nfrom the US, UK, and South Africa. Most work\nas editors, writers, and proofreaders and have ex-\ntensively used AI assistants. See Table 5 for more\ninformation about annotators.\nCollecting Human Annotations All annotators\nwere required to read the guidelines (Figure 4)\nand sign a consent form (Figure 5) prior to the\nlabeling task. Collecting all labels usually required\nadditional communication with the annotators, re-\nsulting in about 20 hours of work from the author\ninvolved in this process. We estimate that the an-\nnotators were able to read and label between 8 and\n12 articles per hour based on self-reported time\nand records in the spreadsheets. Figure 6 shows\nthe interface annotators use to complete the anno-\ntation process. They read and highlight an article,\nthen complete the annotation by providing their\ndecision, confidence score, and explanation. Since\ncompleting annotations for 60 article batches takes\na long time (estimated 6-8 hours of work), we have\nimplemented an interface that made it possible for\nthe annotators to save their work and come back\nat any time, allowing annotators to allocate their\ntime as they saw best. Annotators were given 1"}, {"title": "Dataset", "content": "In this section of the appendix we provide more\ndetails on our article corpus (B.1), how our Al\narticles were generated (B.2).\nArticle Corpus\nHere we include more details about the articles col-\nlected for this study. Table 6 lists all publications\nof articles included in the corpus, with section\ndistribution presented in Figure 7. Table 7 provides\nthe statistics for articles by publication."}]}