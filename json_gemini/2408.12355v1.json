{"title": "Class-balanced Open-set Semi-supervised Object Detection for Medical Images", "authors": ["Zhanyun Lu", "Renshu Gu", "Huimin Cheng", "Siyu Pang", "Mingyu Xu", "Peifang Xu", "Yaqi Wang", "Yuichiro Kinoshita", "Juan Ye", "Gangyong Jia", "Qing Wu"], "abstract": "Medical image datasets in the real world are often unlabeled and imbalanced, and Semi-Supervised Object Detection (SSOD) can utilize unlabeled data to improve an object detector. However, existing approaches predominantly assumed that the unlabeled data and test data do not contain out-of-distribution (OOD) classes. The few open-set semi-supervised object detection methods have two weaknesses: first, the class imbalance is not considered; second, the OOD instances are distinguished and simply discarded during pseudo-labeling. In this paper, we consider the open-set semi-supervised object detection problem which leverages unlabeled data that contain OOD classes to improve object detection for medical images. Our study incorporates two key innovations: Category Control Embed (CCE) and out-of-distribution Detection Fusion Classifier (OODFC). CCE is designed to tackle dataset imbalance by constructing a Foreground information Library, while OODFC tackles open-set challenges by integrating the \"unknown\" information into basic pseudo-labels. Our method outperforms the state-of-the-art SSOD performance, achieving a 4.25 mAP improvement on the public Parasite dataset. The code will be released upon acceptance.", "sections": [{"title": "INTRODUCTION", "content": "Recent years have witnessed the rapid development of object detection research [1], [2], [3]. Fully supervised models require a substantial amount of annotated data, which is labor-intensive and time-consuming, especially in the realm of medical image analysis. Subsequently, semi-supervised object detection (SSOD) methods [4], [5], [6], [7], [8], [9], [10], which could effectively leverage unlabelled data, have emerged and gained increasing attention. Related research has also found wide application in medical image analysis [11], [12], [13], [14], [15], [16], [17]. These methods generally assume that the data used for training and testing share the same categories. However, in the medical imaging domain, rare diseases can appear unexpectedly in the testing set without being present in the training set. Due to the diversity and complexity of medical images, even experienced doctors may misdiagnose some rare and difficult-to-distinguish conditions, which coincides with the open-set problem in computer vision.\nFor natural images, Liu et al. [18] advocated using an offline DINO [19] model as a filter to eliminate information of unknown categories in unlabeled images. Wang et al.[20] proposes an online framework to distinguish and filter the out-of-distribution (OOD) instances from the in-distribution (ID) instances during pseudo-labeling. However, few works consider the open-set semi-supervised object detection (OSSOD) problem in medical imaging despite that the OSSOD problem is equally prominent and worth investigating in this area.\nIn medical imaging, class imbalance is a very common issue, with a significant disparity in the number of common and rare classes. Unknown and rare categories are often misclassified by existing models as the most numerous known categories in the training set, meaning that models exhibit severe bias, resulting in high confidence in wrong predictions. Our research aims to fill this gap by proposing an innovative pipeline that makes the trained data more balanced. Another issue is that the performance of SSOD methods deteriorates when unknown categories are present. [21] points out that the existence of unknown categories will in turn seriously affect the model's ability to judge known categories. Previous OSSOD methods [18] and [20] distinguish and simply discard the unknown categories. In contrast, we propose to mitigate the interference of unknown categories on known categories by distinguishing and fusing the unknown class predictions. In our framework, the Out-of-distribution Detection Fusion Classifier module (OODFC) incorporates the pseudo-labels generated by Opendet [22] which tags objects that it identifies as belonging to unknown categories with the label \"unknown\" as shown in Fig. 1. These pseudo-labels will be integrated into the basic pseudo-labels of SSOD framework as a hint to the model, aiding in the mitigation of the open-set problem.\nOn the other hand, we designed the Category Control Embed (CCE) module targeted at mitigating the class imbalance problem. It dynamically constructs a Foreground Information Library to regulate the frequency of occurrence of various category objects in unlabelled data, thereby mitigating the class imbalance phenomenon.\nOur contributions can be summarized as follows:\n\u2022 We propose the first open-set semi-supervised medical object detection framework, where a novel label fusion module is employed to reduce the interference of un- known categories on known ones.\n\u2022 Further, a novel category control embed module is inte- grated into the framework to alleviate the class imbalance issue in medical images.\n\u2022 Extensive experiments on private and public datasets show that the proposed method can significantly improve the performance in detecting known categories for med- ical images in an open-set semi-supervised setting."}, {"title": "METHOD", "content": "As shown in Fig. 2, our framework includes the Category Control Embed (CCE) module to address the issue of class imbalance, and the Out-Of-Distribution Detection Fusion Classifier (OODFC) to integrate the unknown class information into basic pseudo-labels to mitigate interference from unknown categories. These two modules will be integrated into a basic Semi-Supervised Object Detection (SSOD) paradigm named Mean-Teacher [7], [23], [9], [24] pipeline. It will serve as the baseline, which consists of a semi-supervised network that utilizes ResNet50 [25] as its backbone, FPN [26] as the neck, FCOS [27] as the detector and EMA [23] for updating the teacher network."}, {"title": "Semi-supervised Object Detection", "content": "Object detection is an essential task in computer vision that involves identifying and locating objects within images. Traditional methods for object detection require extensive annotated datasets, which are costly and time-intensive to generate. Semi-supervised object detection (SSOD) addresses this issue by utilizing both labeled and unlabeled data to enhance detection performance.\n1) Learning Framework: Semi-supervised learning aims to leverage a large corpus of unlabeled data in conjunction with a smaller labeled dataset to boost model performance. Initially, the model is trained on the labeled data, and then it is used to predict labels for the unlabeled data. These predicted labels, or pseudo-labels, are subsequently used to fine-tune the model.\n2) Objective Function: The objective function in SSOD typically combines a supervised loss on labeled data and an unsupervised loss on unlabeled data. The supervised loss \\(L_s\\) is calculated using standard object detection techniques like Faster R-CNN [1], YOLO [28], or SSD [3] on the labeled dataset \\(D_l\\). The unsupervised loss \\(L_u\\) leverages the pseudo-labels generated from the model's predictions on the unlabeled dataset \\(D_u\\).\nMathematically, the overall loss \\(L\\) can be expressed as:\n\\[L = L_s + \\lambda L_u\\]\nwhere \\(\\lambda\\) is a weight factor that balances the contributions of the supervised and unsupervised losses. \\(L_s\\) is computed from the labeled data and includes components such as classification loss \\(L_{cls}\\) and localization loss \\(L_{loc}\\)\n\\[L_s = L_{cls} + L_{loc}\\]\nUnsupervised Loss \\(L_u\\) is derived from the unlabeled data using pseudo-labels. It often includes consistency regulariza- tion and pseudo-labeling mechanisms. For instance, Mean- teacher [23] or self-training methods can be employed to generate reliable pseudo-labels \\(\\hat{y}\\) for the unlabeled data:\n\\[L_u = L_{consistency} + L_{pseudo}\\]\n3) Pseudo-Labeling: Pseudo-labeling is a key component of SSOD. It firstly Train the model using the labeled data \\(D_l\\) and then applies the trained model to the unlabeled data \\(D_u\\) to predict pseudo-labels. It will filter the pseudo-labels based on confidence thresholds to ensure quality and use the filtered pseudo-labeled data to further train the model, updating its parameters iterative.\n4) Consistency Regularization: Consistency regularization ensures that the model's predictions are stable and consistent under different perturbations (e.g., data augmentation). This technique encourages the model to produce similar predictions for the same image under various transformations.\n\\[L_{consistency} = \\sum_{i=1}^{N} ||f_\\theta(x_i) - f_\\theta(T(x_i))||^2\\]\nwhere \\(T(x_i)\\) is a transformed version of \\(x_i\\), and \\(f_\\theta\\) denotes the model's prediction function."}, {"title": "EMA Mechanism", "content": "In semi-supervised learning frameworks, we use Exponen- tial Moving Average (EMA) mechanism [23] to update model weights. This method stabilizes and improves the learning process of models, especially in scenarios with sparse or noisy labeled data.\nThe EMA [23] update rule at time step \\(t\\) for model parameters \\(\\theta_t\\) is defined as:\n\\[\\theta_{t+1} = \\alpha \\theta_t + (1 - \\alpha) \\theta'_{t+1}\\]\nHere, \\(\\alpha\\) is a decay coefficient between 0 and 1, controlling how much emphasis is placed on historical parameters. \\(\\theta'_{t+1}\\) represents the parameters of the model at the current time step. In semi-supervised learning, there are typically a large number of unlabeled data points and a smaller set of labeled data points. EMA [23] can smooth the update of model parameters by averaging over the entire dataset, particularly benefiting from the unlabeled data.\nEMA [23] reduces the variance in parameter updates, thereby stabilizing the model, which is crucial when training on a small amount of labeled data prone to noise. By smooth- ing parameter updates, it also improves the model's ability to generalize to unseen data, including unlabeled data. This is achieved by maintaining a stable model state throughout training. Furthermore, in semi-supervised settings, labeled data is often limited. EMA [23] efficiently incorporates information from both labeled and unlabeled data to update model parame- ters, improving performance across the entire data distribution."}, {"title": "OOD Detection Fusion Classifier", "content": "OODFC is proposed to address the challenge of open-set semi-supervised object detection. Our framework introduces an additional \"unknown\" category label. This implies that when the detector encounters objects of unknown categories, it does not discard these data outright; instead, it labels them as \"unknown\".\nThe advantage of this approach lies in its ability to preserve a greater amount of information, particularly for pseudo- labeled objects that may not have been correctly classified. Thus, the model can achieve a more comprehensive under- standing and interpretation of data.\nIn the semi-supervised learning pipeline, a fully supervised teacher network [27] is responsible for generating pseudo-labels for known categories, referred to as the original teacher. We use an additional detection module Opendet [22] to gener- ate pseudo-labels for unknown categories. These two types of labels are merged to form new hybrid labels as shown in Fig. 2 OODFC part. In the merging process, when two regions with IoU greater than 0.7 are predicted to be the known category \\(i\\) and the unknown category respectively, we will use a novel dynamic threshold to filter \"unknown\" labels:\n\\[T_i = max(0, min(1, e^{\\gamma(AP_i - 1)}))\\]\n\\(T_i\\) is the threshold used to determine whether to retain the label of the unknown category, which will be influenced by \\(AP_i\\) while \\(AP_i\\) represents the Average Precision for category \\(i\\) of the fully supervised teacher network in the SSOD pipeline. \\(\\gamma\\) is a positive coefficient that controls the rate of growth of the function, which is set to 1.5. The choice of the exponential function is because the closer \\(AP_i\\) is to 1, the thresholds need finer division.\nAlthough some areas may sometimes be marked as both a known and an unknown category, this method still plays a cru- cial role in guiding the model to recognize unknown objects, effectively reducing the interference of unknown categories on the known categories. The steps are shown as Algorithm 1"}, {"title": "Category Control Embed", "content": "Category Control Embed (CCE) is the second module we proposed. To address the common issue of class imbalance in object detection tasks, we propose a solution based on image fusion, inspired by the classic Mixup [29] method. In our consideration, class imbalance refers not only to the disparity in the number of images but more specifically to the imbalance in the number of foreground object categories within the images. To tackle this challenge, we create a Fore- ground Information Library. This library contains segments of each bounding box from labeled images along with their corresponding annotation information. In a dataset with class imbalance, there is a significant difference in the number of segments from different classes in the library. This library will then selectively dynamically enhance the segments to adjust the quantity of foreground information from various categories, aiming for a more balanced representation.\nIn detail, To balance the foreground information library, we use oversampling for categories below the average quantity and undersampling for those above .This balanced Foreground Information Library will then be integrated into unlabeled data, generating a new set of synthetic datasets. During the addition process, a parameter \\(\\beta\\) is used to control the fusion ratio between two images, achieving varying degrees of fusion effects as well as avoiding the loss of valuable information on the overlaid medical image:\n\\[x_{syn} = \\beta b_i + (1 - \\beta)x_i,\\]\n\\[y_{syn} = combine(y_i, y_j) = y_j,\\]\nWhere \\(b_i\\) is an augmented foreground segment. \\(x_i\\) is the part that is randomly selected in an unlabeled image, and this part will be fused with \\(b_i\\); \\(y_i\\) is the empty annotation of unlabeled data, so the \\(y^{syn}\\) finally becomes \\(y_j\\), which is the label of \\(b_i\\). \\(\\beta\\) is set to 0.5, which means the images are blended with equal weights.\nThe foreground parts' position is a random coordinate within the unlabeled image boundaries. The synthetic data label includes the foreground category and coordinates, with bounding box confidence set to 0.8. An augmentation factor \\(\\alpha_c\\) is used for the extent of augmentation required for each class to achieve balance. By selecting bounding box regions, this method specifically allevi- ates the imbalance in the number of foreground objects across classes. This entire procedure is named CCE, which will be integrated into Mean-Teacher [23] and executed automatically during runtime."}, {"title": "EXPERIMENTS AND RESULTS", "content": "In this study, we applied the framework to assess its per- formance on two different datasets: a private Ophthalmology dataset and a public Parasite dataset [30]. Both datasets inherently exhibit class imbalance, characterized by significant differences in the number of images across different categories. The two datasets used in our study contain quite distinct contents, verifying the model's ability to generalize across different types of medical imaging data.\nThe Slit Lamp Ophthalmology Dataset. The private Slit Lamp Ophthalmology Dataset includes 11 known categories and three unknown categories: \u201cConjunctival Congestion\", \"Limb Conjunctival Tumor\", and \"Keratitis\". These three categories are not involved in the training process at all and only appear in the validation set. From the annotated data of the 11 known categories, we selected 10% from each category as the labeled training data. This data, along with the unlabeled training data, forms a semi-supervised training set. The remaining 90% of the labeled data and all the data of the three unknown categories are used as the test set.\nThis private dataset comprises 136 labeled training images, 1224 unlabeled training images, and 1782 validation images, adopting a classic 10% labeled data and 90% unlabeled data split for the training set. The unlabeled data comes from an external validation set from a hospital. The external validation set and the internal training set come from different hospitals, and the lighting conditions may vary. This poses a greater challenge to the model's generalizability.\nThe Parasite dataset. Similarly, in the public Parasite dataset, we have 6 known categories plus two unknown parasite categories: \"Taenia Sp\" and \"Trichuris Trichiura\". These two unknown categories also do not appear in the training process and are only used for performance evaluation. From the 6 known categories, we also took 10% from each as the labeled training data, which, along with the unlabeled data, constitutes the semi-supervised training set. The original validation set of this dataset, along with all the data of the two unknown categories, is used as the test set.\nThe public Parasite dataset includes 152 labeled training images, 1332 unlabeled training images, and 411 validation images. The unlabeled and labeled data also comprise 10% and 90% in the training set, respectively. The detailed information about Categories and Splits is shown in Table I"}, {"title": "Metrics", "content": "We report the Average Precision (AP) for each class and mean Average Precision (mAP) for all classes, following object detection conventions [28]. AP (Average Precision) measures the accuracy of a model's predictions in object detec- tion by considering both precision and recall. It is calculated as the area under the precision-recall curve. mAP (Mean Average Precision) is the average of AP scores across all classes in a dataset, providing a single metric for evaluating overall model performance across multiple classes. In our experiment, we use AP50 as our evaluation metric, which means we measure the Average Precision with an Intersection over Union (IoU) threshold of 50%. This indicates that a predicted bounding box is considered a true positive if it overlaps with the ground truth bounding box by at least 50%."}, {"title": "Implementation Details", "content": "In our experiment, the baseline semi-supervised pipeline is Mean-Teacher [23]. It's worth noting that both the Opendet [22] detector and the Mean-Teacher [23] pipeline use the same batch of labeled training data. Therefore, no additional labeled data was utilized.\nThe model's training parameters are lr=0.001, momen- tum=0.9, weight-decay=0.0001, warmup-iters=4000. Weak augmentation includes random flip, while strong augmentation includes random flip, color jittering, and cutout. For other state-of-the-art (SOTA) models used for comparison we em- ploy their default parameters in the publicly available code."}, {"title": "Results", "content": "In our research, we adopted the following methods to ensure the accuracy and reliability of the results: In a single experiment, we recorded the highest mean Average Precision (mAP) and repeated the experiment five times to eliminate random errors. The experimental results are the average of these five experiments. To make effective comparisons, We compared our method with the state-of-the-art fully supervised object detection models and semi-supervised object detection models. It should be noted that the previous work by Liu et al. [18] has not been open-sourced. Hence, we were unable to make a comparison.\nIn Table. II, we presented the accuracy of different models on each class and mAP of all classes in the private dataset (Note: '0' means no disease). Meanwhile, Table. III shows the results of the same experimental setup using the public Parasite datasets. Percentage means the number of objects in this category as a percentage of the total and is used to show whether the current category belongs to the majority category or the minority category. The models in the upper half of the table are fully supervised models, and the lower half are semi-supervised models. In these tables, we indicated the highest AP value with bold, and the second highest AP value with an underline."}, {"title": "Discussion", "content": "In our experiments, the proposed model achieved the highest mean Average Precision (mAP) on both datasets. This perfor- mance surpasses the state-of-the-art semi-supervised models and high-performance fully supervised models with various architectures. This achievement demonstrates the effectiveness of our approach in leveraging the unique advantages of the proposed modules.\nA detailed examination of the AP for each category reveals that our model excels particularly in minority categories. While traditional methods tend to overlook these categories due to the inherent class imbalance problem in many datasets, our approach addresses this issue to a certain extent. By incorporating CCE that specifically enhances the number of minority classes, our model ensures that these categories receive adequate attention during the training process.\nAdditionally, although recent large models (such as SAM [32], ViT [33] and CLIP [34]) exhibit exceptional performance across different tasks, our model has several advantages com- pared to these powerful models:\n\u2022 Data efficiency: Specialized models adapt better to lim- ited or lower-quality data.\n\u2022 Real-time requirements: Lightweight models excel in scenarios requiring instant decisions due to lower com- putational overhead.\n\u2022 Compatibility: This component integrates with any pseudo-label-based semi-supervised object detection model to reduce interference from unknown classes."}, {"title": "Ablation study", "content": "As mentioned, we adopted a basic Semi-Supervised Object Detection (SSOD) paradigm as our baseline, named Mean-Teacher [23]. This serves as the baseline for comparison against the model that incorporates the CCE and OODFC modules respectively and that incorporates both two modules. Experimental results show the model with both CCE and OODFC consistently achieves the highest accuracy, thereby validating their effectiveness."}, {"title": "CONCLUSIONS AND FUTURE WORK", "content": "This study presents a pioneering approach in semi- supervised object detection for medical imaging, tackling the open-set problem and the class imbalance challenge. We introduce two novel modules: OODFC, which integrates un- known class label information to minimize unknown classes' interference on known category judgments; and CCE, which dynamically embeds foreground information to reduce bias towards majority classes. Our model outperforms state-of- the-art semi-supervised as well as fully-supervised object detection methods on two datasets (a public Parasite dataset and a private ophthalmology dataset), demonstrating excellent generalization ability.\nWe used Opendet [22] as the \"unknown\" class information producer, yet our framework design is adaptable to include any model capable of detecting unknown categories. This allows for flexible replacement with various external detectors to enhance the model's ability to recognize unknown categories, whether the detector is fully supervised or semi-supervised. Future research could evaluate the performance of different external detectors in our framework."}]}