{"title": "BROADENING DISCOVERY THROUGH STRUCTURAL MODELS: MULTIMODAL COMBINATION OF LOCAL AND STRUCTURAL PROPERTIES FOR PREDICTING CHEMICAL FEATURES", "authors": ["Nikolai Rekut", "Alexey Orlov", "Klea Ziu", "Elizaveta Starykh", "Martin Tak\u00e1\u010d", "Aleksandr Beznosikov"], "abstract": "In recent years, machine learning has profoundly reshaped the field of chemistry, facilitating significant advancements across various applications, including the prediction of molecular properties and the generation of molecular structures. Language models and graph-based models are extensively utilized within this domain, consistently achieving state-of-the-art results across an array of tasks. However, the prevailing practice of representing chemical compounds in the SMILES format -used by most datasets and many language models \u2013 presents notable limitations as a training data format. In contrast, chemical fingerprints offer a more physically informed representation of compounds, thereby enhancing their suitability for model training. This study aims to develop a language model that is specifically trained on fingerprints. Furthermore, we introduce a bimodal architecture that integrates this language model with a graph model. Our proposed methodology synthesizes these approaches, utilizing RoBERTa as the language model and employing Graph Isomorphism Networks (GIN), Graph Convolutional Networks (GCN) and Graphormer as graph models. This integration results in a significant improvement in predictive performance compared to conventional strategies for tasks such as Quantitative Structure-Activity Relationship (QSAR) and the prediction of nuclear magnetic resonance (NMR) spectra, among others.", "sections": [{"title": "1 Introduction", "content": "The integration of machine learning (ML) has emerged as a transformative force in the natural sciences, particularly in the discipline of chemistry [Chithrananda et al., 2020, Hu et al., 2016, Wang et al., 2022]. This integration encompasses various tasks, ranging from the regression of molecular properties, exemplified by quantitative structure-activity relationship (QSAR) models [Wu et al., 2021, R\u00e1cz et al., 2021], to complex challenges, such as predicting nuclear magnetic resonance (NMR) spectra from structure of chemical compound [Yao et al., 2023]. As an ever-evolving discipline, the latest advancements in machine learning are gradually being adapted for applications in chemistry, albeit with some delay. Molecular representations are fundamental to the application of machine learning in chemistry, and three primary types are typically employed: graph-based [Reiser et al., 2022], string-based [Heller et al., 2015, Weininger, 1988, Krenn et al., 2020], and vector representations [Rogers and Hahn, 2010, Durant et al., 2002].\nGraph-based representations conceptualize chemical compounds as molecular graphs, effectively capturing their structural properties [David et al., 2020, Kwon et al., 2020]. This format naturally aligns with graph neural networks, which have been successfully applied to numerous chemical problems, demonstrating their efficacy in molecular analysis. String representations, particularly Simplified Molecular Input Line Entry System (SMILES) [Weininger, 1988], are widely regarded as a standard method for the linear representation of molecular structures. SMILES is typically used for storing compounds in databases and, despite its limitations, effectively represents the structure of a molecule [Chithrananda et al., 2020, Ahmad et al., 2022, Wang et al., 2019, Shamshad et al., 2023, Cong et al., 2024]. However, it presents notable shortcomings. Initially designed for efficient storage and representation of molecular data, SMILES lacks comprehensive information regarding the physical and chemical properties of compounds. As machine learning progresses, the inadequacies of SMILES in facilitating in-depth semantic analysis have become increasingly evident. Recent methods have sought to combine graph models with SMILES-based natural language processing (NLP) transformers [Zhu et al., 2023], thereby integrating the strengths of both methodologies.\nThe other type of representation comprises vector representations, notably fingerprints, which were developed for substructure identification and similarity searching. Numerous studies, such as those presented by [Sabando et al., 2022] and [Wang et al., 2020], have highlighted the effectiveness of fingerprints as molecular representations in machine learning tasks, demonstrating promising results even with conventional algorithms and simple models. However, the potential of fingerprints (including ECFP) in the context of modern language models remains largely underexploited.\nWe believe this approach is particularly advantageous due to the transformer's inherent versatility and flexibility, allowing it to address a variety of physicochemical tasks that differ significantly in nature. For example, fingerprints-based architecture could potentially handle tasks such as predicting molecular property values [Wu et al., 2021, R\u00e1cz et al., 2021, Sabando et al., 2022], classifying compounds based on their biological activity, generating novel molecular structures, exploring molecular interactions [Hu et al., 2016] and complex challenges, such as predicting nuclear magnetic resonance (NMR) spectra from molecular structures [Kwon et al., 2020] or solving the co-crystallisation problem [Wang et al., 2020].\nGiven the diverse challenges presented by these tasks, we seek to create a unified framework that requires only minor modifications for each specific application. By leveraging the strengths of transformer models, we anticipate that our architecture will enable efficient and robust performance across multiple domains within the field of cheminformatics.\nThe questions we want to tackle with our research are the following:\n\u2022 To what extent can a language transformer model based on fingerprints enhance performance compared to conventional approaches?\n\u2022 How much results can be improved by combining such language transformer with graph model?\n\u2022 Furthermore, can transformers trained on fingerprint representations improve the quality of multitasking embeddings, thereby providing more robust and nuanced representations that capture the complexities across diverse tasks?"}, {"title": "2 Related Work", "content": "2.1 Extended-Connectivity Fingerprints\nExtended-Connectivity fingerprints [Rogers and Hahn, 2010], are so-called circular fingerprints that assign a two- dimensional hash array to each molecule. Each element of such an array is a hash corresponding to one atom. It encrypts a fixed set of physical and chemical properties of this atom, such as charge, as well as information about its neighbours.\nAs applied to our task, there are three significant particularities of ECFP. Firstly, the fingerprint of a single molecule consists of an array of hashes (i.e. in NLP terms, we can think of the array as a sentence and each individual hash as a word). Secondly, each hash is constructed based on a set of physical properties. Thus, each array element is based on physical and chemical data. And lastly, there is a so-called diameter, which shows neighbouring atoms in one iteration. That being said, we cover only those atoms, that are within the diameter's reach. This sensible of surrounding environment representation can be very useful for such tasks as molecular NMR spectroscopy, where chemical environment plays crucial role in spectrum definition.\nOver the last few years, a number of methods [Wang et al., 2020, Sturm et al., 2018] have pointed out the effectiveness of using ECFP as features for training quite simple models (such as SVM) to solve various chemical problems.\nSeveral approaches have employed ECFP as a representation for training data in natural language processing (NLP) algorithms; however, these methods predominantly rely on relatively conventional machine learning techniques. One notable example is Mol2Vec [Jaeger et al., 2018], which implements the Word2Vec algorithm utilizing ECFP data representation."}, {"title": "2.2 SMILES-based NLP models", "content": "Transformers [Vaswani, 2017] were initially introduced to facilitate the generation of vector representations for natural language processing tasks. Since their inception, they have found widespread application across a variety of domains, including speech recognition, medicine, and neuroscience [Shamshad et al., 2023, Cong et al., 2024].\nThere have been several efforts to adapt transformers for chemical applications, exemplified by models such as SmilesBERT [Wang et al., 2019], ChemBERTa [Chithrananda et al., 2020], and ChemBERTa-2 [Ahmad et al., 2022], and all of them were trained on SMILES.\nMany of these models have been trained on substantial datasets, including ZINC [Irwin et al., 2012] and PubChem [Kim et al., 2023], demonstrating commendable performance in classification and regression tasks across various established chemical benchmarks."}, {"title": "2.3 Graph models", "content": "Graph neural networks (GNNs) have been effectively utilized to address a variety of challenges within the field of chemistry [David et al., 2020, Kwon et al., 2020]. Many GNNs are highly specialized for specific tasks and are not inherently designed for generating vector representations of chemical compounds.\nSeveral methodologies have been proposed to enhance GNN-based embeddings. For instance, [Hu et al., 2016] introduced two primary concepts: the recovery of masked properties of a molecule, such as the type of a specific atom, and the application of contrastive learning to minimize discrepancies between two subgraphs within a molecule. Additionally, MolCLR [Wang et al., 2022] presents a framework based on the augmentation of molecular graphs through the removal of atoms, edges, and subgraphs, followed by the training of a model to reconstruct these components. However, many GNNs are specialized for specific tasks and are not inherently designed to generate vector representations of chemical compounds.\nIn the graph component of our model, we advocate for an approach that synthesizes these concepts and leverages state-of-the-art models. Specifically, we implement a mechanism to mask atom features and edge features in the case of Graphormer [Ying et al., 2021]. The model is trained not only to predict these masked features but also to align the embeddings of two augmented versions of the same molecule. This approach represents a modification of contrastive learning, a technique that remains underutilized in the chemistry domain.\nMoreover, [Zhu et al., 2023] introduced a bimodal architecture incorporating a BERT-based language model (LM) trained on SMILES alongside a GNN as the graphical representation model. In contrast, we propose a distinct language model that is trained on fingerprints, thus providing a more physically informed perspective and an advanced graph model. Additionally, our approach includes notable differences in the final projection and the processing of embeddings derived from both the language and graph models."}, {"title": "3 Our contributions", "content": "In this paper, we propose a novel methodology\u00b9 that integrates graph-based representations with language models based on fingerprints, effectively addressing these limitations. Our approach encompasses four distinct architectures.\nROBERTa with ECFP. The model implemented in this study is based on the RoBERTa framework and employs ECFP as a baseline for our investigations. As previously discussed, ECFP possesses a fixed radius, rendering it particularly effective for capturing the local properties of substructures or individual atoms within chemical compounds. Both ECFPs and language models effectively leverage their respective training data to extract local properties, showcasing their strength in identifying meaningful features within a localized context. This parallel emphasizes their utility, as noted by Iman Mirzadeh et al. [Mirzadeh et al., 2024], where language models similarly prioritize local information, while have some problems with reasoning. The incorporation of a specified radius in ECFPs allows for the adjustment of substructure sizes, which is especially pertinent for a range of chemical tasks, including the prediction of NMR spectra and the analysis of reaction centers.\nLM and Graph model. In contrast, a graph-based model offers a comprehensive representation of the molecular structure, which proves advantageous for analyzing extensive connections characterized by numerous substructures. Such scenarios are frequently encountered in the field of biochemistry, particularly when addressing pharmacological compounds or polymer-related challenges. In this study, we implement atom masking and graph augmentation"}, {"title": "4 Method", "content": "4.1 Architecture Overview\nThe proposed model comprises three primary components, as illustrated in Figure 1: the graph model, the language model, and the projection blocks. The language model is designed to accept ECFP connectives as input, whereas the graph model processes molecular graphs.\nThe function of the projection blocks is to transform the embeddings generated by the graph and language models from their respective latent spaces into a unified third latent space.\n4.2 Language model\nTokenizer. At first, we attempt to use hash values from the ECFP format as the direct input of the language model. This idea is not prosperous because the range of the hash function's outputs (approximately from -232 to 232) is too wide to utilize them as tokens for the model's input. In this case, we decide to include a tokenizer in the processing. This step allows us to narrow down the range of possible model vocabulary values. As we work with an array of integers interpreted as text, we cannot afford to use a normal tokenizer, which creates tokens from text. In this regard, we choose the Byte-Pair Encoding Tokenizer, as shown in Figure 2, which allows the production of tokens from raw bytes. We train this tokenizer on the largest dataset we have \u2013 PubChem [Kim et al., 2023], containing 10 million molecules. This pipeline modification decreases the vocab-size of the model to not more than 30, 522.\nROBERTa training. We utilize the ROBERTa architecture [Liu, 2019], which has been trained on ECFPs derived from the PubChem [Kim et al., 2023] and ZINC [Irwin and Shoichet, 2005] datasets, as our language model. Within this framework, the encoding of an individual atom in ECFP is interpreted as a \"word,\" while the encoding of an entire molecule is considered analogous to \"text.\" During the training process, the ECFP undergoes standard procedures including the masking of 15% of tokens (representing atom hashes), with the model subsequently predicting the probabilities of these masked tokens. The output embedding is derived from the CLS token located in the penultimate layer of the model.\n4.3 Graph model\nCreation and augmentation of graph. A graph is constructed from SMILES representations utilizing the RDKit package, wherein each atom is represented as a vertex. Two parameters \u2013 atom number and chirality \u2013 are designated as attributes of the vertices. In this framework, each bond is represented as an edge, with the bond multiplicity (single, double, triple, or aromatic) serving as the attribute for the edges.\nSubsequently, 20% of the atomic attributes are masked, replacing them with a designated mask token. In the case of graphomers, an equivalent approach is applied where 20% of the edge attributes are also masked, transforming these attributes into the mask token."}, {"title": "4.4 Connection Between Models", "content": "The projection blocks illustrated in Figure 4 of our proposed architecture comprise two linear layers accompanied by two batch normalization blocks. Prior to the application of the final batch normalization block, the ReLU activation function is employed on the embeddings."}, {"title": "4.5 Loss functions", "content": "The loss function used in our model is represented as\n$L = \\alpha \\cdot L_{lang} + \\beta \\cdot L_{graph} + \\gamma \\cdot L_{bimodal},$ (1)\nwhere $L_{lang}$ is the loss function of the language model, $L_{graph}$ is the loss function of the graph part of the model, and $L_{bimodal}$ is the embedding projection loss function from the graph and language models. Coefficients $\\alpha, \\beta$, and $\\gamma$ are some constants which can be considered hyperparameters.\nLanguage model loss. $L_{lang}$ is calculated as regular Cross-Entropy applied to labels and predicted tokens of the language model.\nGraph model loss. $L_{graph}$ is defined as NTXent-Loss [Sohn, 2016] applied to the batch of augmented graphs' embeddings and to the batch of original graphs' embeddings. It tries to minimize the distance between augmented and original ones of the same index and distances others with different indices. NTXent-Loss calculates the cosine distance between two vectors and uses the temperature parameter to balance positive and negative pairs. Let $sim(u, v)$ denotes the cosine similarity between vectors $u$ and $v$. Then the loss function for a positive pair of examples $(i, j)$ is as follows:\n$(L_{graph})_{i,j} = -log(\\frac{e^{sim(u_i,v_j)/T}}{\\sum_{k=1}^{N} e^{sim(u_i,v_k)/T}}),$ (2)\nwhere $N$ is the total number of examples and $\\tau$ (temperature) is a parameter that controls the contribution of positive and negative pairs.\nBimodal Loss. The bimodal loss, denoted as $L_{bimodal}$, is defined also as the NTXent-Loss applied to the output embeddings generated by both the language model and the graph model within a given batch. This loss function aims to minimize the distance between the embeddings of the same index from both models while maximizing the distance between embeddings corresponding to different indices.\nTo achieve this, we employ two distinct projection blocks to convert the embeddings from the graph and language models into a unified third latent space. Utilizing a single projection block to transfer the embeddings from one model into the latent space of the other could inadvertently lead to the training of one model to mimic the behavior of the other. Such an outcome is undesirable, as the distinct functionalities of the models enhance the universal applicability of the bimodal architecture."}, {"title": "5 Experiments", "content": "5.1 Pretraining datasets and data preparation\nWe pretrain our model on parts of two different datasets: PubChem [Kim et al., 2023] and ZINC [Irwin et al., 2012]. Initially, the compounds in them are stored in SMILES format. By leveraging a more physics-based input format, namely ECFP, and employing one of the most sophisticated language models, we achieved a significant milestone: a language model (LM) trained from scratch on two subsets of the PubChem dataset, comprising 2.5 million and 10 million entries, respectively. This model exhibits performance comparable to those trained on the largest datasets within the field. The data preparation process could be divided into two main parts (as shown in Figure 5).\n5.2 QSAR tasks\nFor the zero-shot evaluation of our proposed architecture, we select a set of widely recognized cheminformatics benchmarks based on the quantitative structure-activity relationship (QSAR). Although specially designed descriptors often outperform transformer-based models in these contexts, the simplicity of these benchmarks allows us to assess the quality and versatility of our architecture without the confounding influence of large-scale superstructures typically encountered in more complex problem-solving scenarios.\nWe evaluate four distinct models: ROBERTa (denoted as ECFP-BERT), which is trained on ECFP; ECFP-BERT in conjunction with a Graph Isomorphism Network (GIN); ECFP-BERT combined with a Graph Convolutional Network (GCN); and ECFP-BERT integrated with Graphormer. The first three models utilize a dataset comprising 10 million entries sourced from the PubChem database, while the fourth model is trained on a smaller dataset of 1 million entries from the same source."}, {"title": "6 Conclusion", "content": "Our proposed set of architectures, consisting of ECFP ROBERTa (ECFP-BERT) and bimodal configurations that integrate ECFP-BERT as the language branch alongside Graph Convolutional Network (GCN), Graph Isomorphism Network (GIN), or Graphormer as the graph branch, has demonstrated some of the most promising performance metrics compared to existing models in the domain for various quantitative structure-activity relationship (QSAR) problems across a range of well-established benchmarks.\nWhile specialized descriptors typically outperform transformer-based models for these challenges, these benchmarks serve as a simplified context, thereby allowing us to assess the quality and versatility of our architecture without the confounding influence of large-scale superstructures commonly encountered in more complex scenarios.\nTo further validate our model, we intend to explore additional challenges, including co-crystal prediction, the prediction of nuclear magnetic resonance (NMR) spectra from molecular structure, and other physicochemical tasks. For such demanding tasks, transformer-based architectures often yield significantly superior results compared to straightforward augmentations of task-specific descriptors.\nHowever, it is important to note that addressing these tasks will necessitate considerable modifications to the existing architecture. Consequently, the outcomes will be contingent not only upon the quality of the embeddings currently utilized but also on the enhancements made to the model architecture itself."}, {"title": "A ECFP construction algorithm", "content": "ECFP [Rogers and Hahn, 2010] is a so-called circular fingerprint that assigns a two-dimensional hash array to each molecule using the following algorithm:\n1. The initial step is assigning an integer identifier to each atom.\n2. The iterative update stage, in which the identifier of each atom is updated with the identifiers of its neighbours.\n3. Duplicate removal - a stage in which several occurrences of the same feature are reduced to a single represen- tation in the feature list.\nOne iteration for a single atom is as follows:\n1. An array of integers containing the iteration number and the ID of the given atom is initialized.\n2. The attached atoms are sorted in deterministic order using the bond order (single, double, triple, and aromatic) and the current ID of each attached atom. or each attachment, the attachment ID and bond order are added to the array.\n3. The array is hashed into a single 32-bit integer. This is the new atom identifier."}, {"title": "B Graph models", "content": "GCN. The Graph Convolutional Network (GCN), as introduced by Kipf and Welling [Kipf and Welling, 2016], constitutes a significant advancement in the field of graph neural networks, employing convolutional operations tailored specifically for graph data structures. Distinct from conventional neural networks that utilize linear transformations through a weight matrix W, represented mathematically as h = Wx, GCNs incorporate the inherent topological characteristics of the graph to update node representations. This approach is particularly advantageous given the phenomenon of network homophily, wherein connected nodes are more likely to exhibit similar attributes.\nGCNs operate through a principle known as neighborhood aggregation, which amalgamates the features of a target node with those of its neighboring nodes. For a given node i and its associated neighborhood $N_i$, this aggregation is formalized as follows:\n$h_i = \\sum_{j \\in N_i} Wx_j.$ (3)\nThis formulation enables GCNs to enhance the feature representation of each node by leveraging the attributes of its direct connections. However, given the variability in node degree, it is essential to normalize the aggregated features to ensure comparability across nodes. This normalization is achieved by factoring in the degree of the node, leading to the expression:\n$h_i = \\frac{1}{deg(i)} \\sum_{j \\in N_i} Wx_j.$ (4)\nKipf et al. further refined the GCN architecture by addressing the potential imbalance in feature propagation, whereby nodes with a greater number of neighbors may disproportionately influence the learning process. To mitigate this effect, they proposed a weighted aggregation mechanism that accounts for the degrees of both the target node and its neighbors. The updated formulation is expressed as:\n$h_i = \\sum_{j \\in N_i} \\frac{1}{\\sqrt{deg(i)deg(j)}} Wx_j.$ (5)\nThis enhancement promotes a more equitable distribution of influence among nodes, thereby ensuring that features from less-connected nodes are adequately considered.\nThe versatility of GCNs has led to their incorporation in various advanced frameworks, including Graph Attention Networks (GAT) [Velickovic et al., 2017] and Message Passing Neural Networks (MPNN). Their capacity to capture complex relational patterns and dependencies within graph structures renders GCNs particularly suited for applications spanning diverse domains, such as social network analysis, recommendation systems, and molecular property prediction in cheminformatics.\nAdditionally, GCNs can be further refined through modifications such as attention mechanisms that differentially weight the contributions of neighboring nodes based on learned significance or by integrating diverse edge types to enrich the"}, {"title": "Graph Isomorphism Network (GIN)", "content": "The Graph Isomorphism Network (GIN) is a neural network architecture introduced by Xu et al [Xu et al., 2018]. in 2019 that aims to improve the expressive capabilities of graph neural networks (GNNs). GIN is particularly significant due to its equivalence to the Weisfeiler-Lehman (WL) graph isomorphism test, which serves as a standard for assessing the ability of models to distinguish between different graph structures.\nThe update mechanism for GIN aggregates node features and those of their neighbors using the following formulation:\n$h_v^{(k)} = MLP^{(k)} ((1 + \\epsilon)h_v^{(k-1)} + \\sum_{u \\in N(v)} h_u^{(k-1)} )$ (6)\nIn this equation, $h_v^{(k)}$ denotes the representation of node v at the k-th layer, while $N(v)$ represents the set of neighboring nodes. The term $MLP^{(k)}$ indicates a multi-layer perceptron applied to the aggregated features. The parameter $\\epsilon$ is incorporated to preserve the unique identity of node features, thereby enhancing the model's ability to differentiate between nodes based on their characteristics.\nGIN operates using a two-step framework: initially performing aggregation of neighboring features, followed by the application of a multi-layer perceptron. This approach facilitates the learning of complex representations that capture both local and relational information within graph structures.\nEmpirical evaluations of GIN demonstrate its superior performance in graph classification tasks compared to other GNN variants, underscoring its robustness across various datasets. The architecture coalesces well with applications where fine distinctions in graph structures are essential, such as in the prediction of molecular properties.\nIn this study, the integration of GIN into our model is anticipated to enhance the ability to capture intricate relationships within molecular graphs. This choice aims to improve the predictive performance across diverse physicochemical tasks, contributing to a more accurate assessment of chemical compounds.\nGraphormer. Graphormer is an advanced architecture designed to enhance the capabilities of the Transformer model specifically for graph representation learning, as introduced by Ying et al. [Ying et al., 2021] This architecture effectively addresses the limitations encountered by traditional Transformer models, which often struggle to capture the inherent structural information present in graph data. To this end, Graphormer incorporates several innovative mechanisms, including centrality encoding, spatial encoding, and edge encoding, thereby improving the representation of graph data.\n1. Centrality Encoding: Graphormer enhances the feature representation of nodes by integrating degree centrality into the input features. For a node $v$, the encoded feature is defined as:\n$h_{centrality} = h_v + MLP(deg(v)),$ (7)\nwhere $h_v$ represents the original feature vector of node $v$, $deg(v)$ denotes the degree of node $v$, and $MLP$ denotes a multi-layer perceptron that transforms the centrality information into a vector space that aligns with the node features.\n2. Spatial Encoding: The architecture utilizes spatial encoding to represent the shortest path distance (SPD) between nodes. The SPD between nodes u and v is computed and expressed as:\n$spatial(u, v) = \\frac{1}{SPD(u, v) + 1},$ (8)\nwhere SPD(u, v) denotes the shortest path distance between nodes u and v.\n3. Edge Encoding: To effectively utilize the significance of edge features, Graphormer incorporates edge encoding by calculating the interaction between edge features and node embeddings. This edge encoding is defined as:\n$e(u, v) = \\frac{dot(h_u \\cdot W_Q, h_v \\cdot W_K)}{\\sqrt{d}},$ (9)\nwhere e(u, v) represents the embedded feature for the edge connecting nodes u and v, $W_Q$ and $W_K$ are query and key martices respectively, d corresponds to the hidden dimension. This interaction is integrated into the attention mechanism by modifying the attention score as follows:\n$Attention(u, v) = \\frac{exp(e(u, v) + spatial(u, v))}{\\sum_{w \\in N(u)} exp(e(u, w) + spatial(u, w))} \\cdot V,$ (10)\nwhere $N(u)$ represents the set of neighbors of node u and V is value matrix."}, {"title": "C Some training details", "content": "Weighted Cross-Entropy Loss. Weighted cross-entropy loss assigns different weights to different classes based on their frequency in the dataset. Such approach is useful when you have unbalanced data and you want the model to pay more attention to less represented classes. Class weights do compensate for the imbalance by increasing the contribution of rare classes to the total loss, according to the formulae:\n$L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} w_cY_{i,c}log(P_{i,c} + \\epsilon), $ (11)\nwhere\nN - the number of examples in the batches,\n- C - number of classes,\n$w_c$ - weight for class c,\n$Y_{i,c}$ - true label for example i and class c,\n$P_{i,c}$ - probability predicted by the model for example i and class c (after applying softmax),\n- e - a small value to prevent division by zero.\nThis formulae calculates the average of the weighted cross-entropy over all examples in the batches. We used this variation of Cross-Entropy Loss for the HIV, the Tox21, the ClinTox and the MUV datasets to improve the quality of our models."}, {"title": "D Testing datasets (QSAR)", "content": "QM7. The QM7 dataset is a curated subset of GDB-13, a comprehensive database containing nearly one billion stable and synthetically accessible organic molecules. Specifically, QM7 includes 7,165 molecules, each composed of up to 23 atoms, with a focus on seven heavy atoms: carbon (C), nitrogen (N), oxygen (O), and sulfur (S). This dataset not only provides a diverse array of molecular structures\u2014such as double and triple bonds, cyclic compounds, carboxylic acids, cyanides, amides, alcohols, and epoxides\u2014but also features the Coulomb matrix representation of these molecules. Additionally, the atomization energies for the QM7 molecules are computed using methods aligned with the FHI-AIMS implementation of the Perdew-Burke-Ernzerhof hybrid functional (PBE0).\nQM8. The QM8 dataset consists of 21,786 small organic molecules and serves as a critical resource for evaluating machine learning models in predicting quantum mechanical properties. Each molecule is characterized by quantum chemical properties, including total energies and electronic spectra derived from time-dependent density functional theory (TDDFT). Although TDDFT offers favorable computational efficiency for predicting electronic spectra across chemical space, its accuracy can be limited.dataset is used to validate machine learning models in a prediction of deviations between TDDFT predictions and reference second-order approximate coupled-cluster (CC2) singles and doubles spectra. This approach has successfully applied to the low-lying singlet-singlet vertical electronic spectra of over 20,000 synthetically feasible small organic molecules.\nQM9. The QM9 dataset is a prominent collection in computational chemistry, comprising 133,885 molecules with up to nine heavy atoms, including carbon (C), nitrogen (N), oxygen (O), and fluorine (F). This dataset is particularly valuable for evaluating machine learning models as it features a rich set of molecular structures representative of a wide chemical space.\nEach molecule is identified by a unique 'gdb9' tag facilitating data extraction and a consecutive integer identifier (i). Rotational constants (A, B, and C, in GHz) describe the molecule's rotational inertia. The dipole moment (\u03bc, in Debye) indicates the molecule's polarity, while isotropic polarizability (a, in a\u00b3) reflects its response to electric fields. The energies of the highest occupied molecular orbital (HOMO) and lowest unoccupied molecular orbital (LUMO), both in Hartree (Ha), are included, along with the energy gap (lumo \u2013 homo, also in Ha). Electronic spatial extent (R2, in a\u00b3) characterizes the molecule's size. Vibrational properties are represented by the zero-point vibrational energy (zpve, in"}]}