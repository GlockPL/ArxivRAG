{"title": "Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation", "authors": ["Wenfang Yao", "Chen Liu", "Kejing Yin", "William K. Cheung", "Jing Qin"], "abstract": "Integrating multi-modal clinical data, such as electronic health records (EHR) and chest X-ray images (CXR), is particularly beneficial for clinical prediction tasks. However, in a temporal setting, multi-modal data are often inherently asynchronous. EHR can be continuously collected but CXR is generally taken with a much longer interval due to its high cost and radiation dose. When clinical prediction is needed, the last available CXR image might have been outdated, leading to suboptimal predictions. To address this challenge, we propose DDL-CXR, a method that dynamically generates an up-to-date latent representation of the individualized CXR images. Our approach leverages latent diffusion models for patient-specific generation strategically conditioned on a previous CXR image and EHR time series, providing information regarding anatomical structures and disease progressions, respectively. In this way, the interaction across modalities could be better captured by the latent CXR generation process, ultimately improving the prediction performance. Experiments using MIMIC datasets show that the proposed model could effectively address asynchronicity in multimodal fusion and consistently outperform existing methods.", "sections": [{"title": "1 Introduction", "content": "Clinical data in modern healthcare is documented through various complementary modalities [1, 2]. Electronic health records (EHRs), for instance, systematically record the progression of diseases over time, including medical histories, laboratory test results, and treatment outcomes [3-6]. In parallel, medical imaging, such as chest X-rays (CXRs), is valuable for providing visual insights into the patient's internal anatomy, organ functions, and potential abnormalities [7]. Recent studies have shown that strategic integration of multimodal clinical data could lead to improved performance for clinical predictions compared to relying solely on uni-modal data [8-13].\nDespite the promising results obtained, the inherent asynchronicity of multimodal clinical data still hinders effective integration. Take the intensive care unit (ICU) setting as an example, patients are subject to continuous monitoring systems that capture vital signs, including heart rate, blood pressure, and oxygen saturation, with this information being routinely recorded in the EHR [14, 15]. On the other hand, CXRs are captured only on an as-needed basis and often as less as possible, due to limitations of radiation dose and resources [16]. However, patients admitted to ICU are in life-threatening conditions, which means their medical status is prone to rapid changes and highly time-sensitive [17]. In the MIMIC-CXR dataset [18], it is observed that among patients with positive disease findings in their CXR, over 70% of subsequent CXR images taken within a median interval"}, {"title": "Motivation", "content": "Existing works adopt the \"carry-forward\" approach, i.e., using the last CXR image available for downstream prediction tasks [20, 10]. This strategy ignores the potential rapid changes between the prediction time and the time of the last CXR image taken and thus inevitably leads to suboptimal prediction performance. On the contrary, we hypothesize that generating an updated CXR image at the prediction time could mitigate the asynchronicity problem and enhance the prediction accuracy. Nevertheless, generating patient-specific CXR images presents unique challenges. While multimodal generation has been explored extensively in various fields, these methods are not readily adaptable for generating individualized CXR images. In domains such as text-to-audio [21] or text-to-image generation [22], the attributes that need to be controlled (e.g., painting style) can be explicitly defined in input modalities (e.g., the text prompt). However, in the clinical context, explicit descriptions of a patient's anatomical structures, organ functions, and disease progression, which are highly specific to individual patients and critical for downstream prediction, are not directly available."}, {"title": "Contribution", "content": "To tackle the aforementioned challenge, we propose Diffusion-based Dynamic Latent Chest X-ray Image Generation (DDL-CXR), which utilizes a tailored latent diffusion model (LDM) [22] to generate individualized CXR images for clinical prediction. Specifically, DDL-CXR learns to generate representations in a latent space encoded by a variational auto-encoder (VAE). To incorporate detailed information about the patient's anatomical structure and organ specifics, we use a previous CXR image from the same patient as the reference image. To generate latent representations that align with the disease progression, we use a Transformer model [23] to encode the irregular EHR data spanning from the reference CXR to the prediction time. To further capture the implicit interactions between EHR and CXR, we use the encoded EHR representation to predict the labels of abnormality finding of the target image. To force the LDM to capture the disease course in the EHR data, we explore a contrastive learning approach for training the LDM. The generated up-to-date latent CXR is later fused with historical data for downstream clinical prediction.\nWe summarize our contributions as follows:\n\u2022 To our knowledge, DDL-CXR is the first work to generate an updated individual CXR image to improve clinical multimodal fusion, thereby alleviating the asynchronicity between EHR and CXR.\n\u2022 We propose a contrastive learning approach for the LDM training to enable the disease course in EHR to be captured and utilized by the LDM.\n\u2022 Experiments show that DDL-CXR outperforms existing methods in both multi-modal clinical prediction and individual CXR generation."}, {"title": "2 Related Work", "content": "Clinical multi-modal fusion Integrating multi-modal clinical data has shown beneficial for various clinical prediction tasks [24], including COVID-19 prediction [25], pulmonary embolism diagnosis [8, 26], AD diagnosis [9] and X-ray image abnormality detection [27].\nDifferent strategies have been proposed to facilitate the fusion of multi-modal clinical data [2, 28]. Hayat et al. [10] adopts feature-level fusion with an LSTM layer, while Zhang et al. [29] utilizes a modality-correlated encoder to capture long-range dependencies across modalities. Zhang et al. [30] and Lee et al. [12] incorporate modality type embedding into the self-attention to capture the interaction. Despite the effort, existing methods for multi-modal fusion are driven only by downstream predictions. How to capture the more fundamental interaction between different data modalities remains an open challenge.\nIn the temporal setting, asynchronicity presents another major challenge. Unlike the settings of medical images and radiology reports [31], which are naturally aligned in time, EHR and CXR are often highly asynchronous, bringing extra difficulties to information integration. \"Carry-forward\" is a common strategy adopted, where the last available data from different modalities are used [10, 13]. Lee et al. [12] and Zhang et al. [17] also adopt this approach while modeling the time information of the last available data.\nConditional latent diffusion models The diffusion model is one of the state-of-the-art generative models [32, 33] that has found important applications in areas such as image generation [34], sound generation [35], joint audio and video generation [36], and tabular data generation [37]. To reduce the computational cost, LDM [22] proposes to train diffusion models on a latent space encoded via pre-trained VAE, thus improving training and sampling efficiency as well as preserving generation quality. It also incorporates an attention mechanism into its underlying neural backbone to allow more flexible conditioning.\nBased on LDM, multi-modal generation models have been developed using priors obtained from large-scale contrastive pre-training, e.g., contrastive-image pairs for text-to-image generation [38] and contrastive language-audio pairs for text-to-audio generation [21]. However, it is infeasible to apply this method to clinical settings since many clinical data modalities, e.g., CXR and EHR, capture different aspects of patients and cannot be semantically aligned like the image and caption pairs as in CLIP.\nIn clinical settings, LDM-based models are developed for brain MRI image generation, conditioned on age, sex, brain structure volumes [39], and a subset of MRI slices [40]. For CXR image generation, Packh\u00e4user et al. [41] adopts a thoracic abnormality classifier-aided LDM to generate anonymous CXR images for privacy-protected data generation. Weber et al. [42] utilizes pathology labels, radiological reports, and radiologists' annotations for synthesizing customized CXR images. Gu et al. [43] explores counterfactual generation for CXR using information from imaging reports. Generating individual CXR images that reflect disease courses in EHR and applying them to medical predictions remains an open challenge."}, {"title": "3 DDL-CXR: The Proposed Method", "content": "In this work, we focus on improving multimodal clinical predictions by generating latent CXR images that are in line with patient conditions at prediction time. The generation process also works as a fusion module that captures the cross-modal interaction between EHR and CXR. The overview of DDL-CXR is depicted in Fig. 2. It consists of two stages: the LDM stage and the prediction stage. In the LDM stage, we use the consecutive image pairs to train an LDM that generates representations within a latent space encoded by a variational autoencoder (VAE). To generate patient-specific CXRs, an earlier CXR image of the same patient is used as a reference to capture the anatomical structure, and the EHR time series between the consecutive image pairs is used to capture the disease progression. In the prediction stage, conditioned on this composite information, DDL-CXR generates updated and informative CXR representations at prediction time, which are subsequently fused with available EHR data as well as the previous CXR image for downstream prediction tasks."}, {"title": "3.1 Notations and Preliminaries", "content": "EHR and CXR data Patient-wisely, we denote the EHR time series within the time interval between $t_i$ and $t_j$ as $X^{(t_i,t_j)}_{EHR} = [x_{t_i}, x_{t_i+1},...,x_{t_j}]$, where $x_t \\in R^K$ is the variables recorded at time $t$ and $K$ is the number of features. We denote the grayscale CXR images taken at the time $t_i$ as $X^{CXR}_{t_i} \\in R^{W\\times H}$, where $W$ and $H$ denote its width and height, respectively. For each CXR image, we extract the abnormality finding label, $y^{XR}$, from radiology reports using CheXpert [44].\nPredictive latent space for CXR Using diffusion models in a semantic latent space, rather than a high-dimensional data space, has shown a substantial decrease in computational expenses with minimal impact on synthesis quality [22]. To obtain an informative and expressive latent space, we first train a VAE [45] consisting of an encoder $\\varepsilon$ and a decoder $D$. The data used for training VAE are all available CXR images in the training set with corresponding abnormality finding labels, $(X^{CXR}, y^{XR})$. The primary objective of the VAE is to reconstruct the original CXR image $X^{CXR}$ with $D(\\varepsilon(X^{CXR}))$. We follow the VAE training process in [22], incorporating a pixel-wise reconstruction loss accompanied by a perceptual loss [46], an adversarial objective, and a lightly-penalized Kullback-Leibler loss towards a standard normal aiming at constraining the latent spaces from excessively high variance. Besides, to improve the encoder's ability to predict, we also include a prediction loss regarding the abnormality label $y^{CXR}$. We denote the encoded latent CXR by $Z_t = \\varepsilon(X^{CXR}) \\in R^{C\\times \\frac{W}{r}\\times\\frac{H}{r}}$, where $C$ represents the channel of the compressed representation and $r$ represents the compression ratio. We first pre-train the VAE model and then freeze it throughout the training and inference of DDL-CXR. More details on VAE training are presented in Appendix A.1."}, {"title": "3.2 LDM Stage: Dynamic Latent CXR Generation", "content": "As discussed previously, to generate an up-to-date, patient-specific latent CXR, it is important to incorporate the unique anatomical details of the individual patient. Furthermore, the generated image must accurately reflect the evolving pathology as documented in the irregular EHR time series. To this end, we extract all sequential image pairs and the EHR time series between them. We denote each sample as a quadruplet: $(X^{CXR}_{t_0}, X^{(t_0,t_1)}_{EHR}, y^{XR}_{t_1}, X^{CXR}_{t_1})$. CXR images are encoded using the pre-trained VAE as we aim to generate latent CXR images: $Z_{t_0} = \\varepsilon(X^{CXR}_{t_0}), Z_{t_1} = \\varepsilon(X^{CXR}_{t_1})$. We follow prior works on diffusion models to learn our LDM [22, 32]. It comes down"}, {"title": "Capturing disease course via EHR time series", "content": "To effectively capture useful information on disease progression for future CXR generation, we adopt a multi-task Transformer-based time series encoder [48] with the masked self-attention mechanism to handle the variable length of EHR time series [49]. The encoded representation of EHR, $E^{(t_0,t_1)}_{EHR}$, is given by\n\n$E^{(t_0,t_1)}_{EHR} = f_{cond}^{EHR}(X^{(t_0,t_1)}) = Transformer([h_{CLS}, \\phi(x_{t_0}), ..., \\phi(x_{t_{t_1}})]),$ (2)\n\nwhere $\\phi(x_t)$ projects the original EHR time series into an embedding space and applies the positional encoding at time step $t$. $h_{CLS}$ is the class token.\nTo further extract information that is relevant to CXR generation and facilitate modality fusion at the LDM stage, we incorporate an auxiliary prediction task: using the class token from the encoded EHR to predict the abnormality findings $y_{t_1}^{CXR}$, associated with the CXR image $X_{t_1}^{CXR}$, i.e., $\\hat{y}^{CXR}_{t_1} = g(h_{CLS})$, where $g$ denotes the prediction function, e.g., an MLP, which is trained by jointly minimize the loss function given by $L_{aux} := \\frac{1}{LM} \\sum_{m=1}^{M} \\sum_{l=1}^{L} y^{CXR}_{ml}log(\\hat{y}^{CXR}_{ml}) + (1 - y^{CXR}_{ml})log(1 - \\hat{y}^{CXR}_{ml})$, where $M$ is the number of training samples for LDM and $L$ is the number of classes of abnormality labels of CXR. The auxiliary task enables the EHR encoder to extract CXR-related information, which further encourages the interaction between EHR and CXR to be captured in the subsequent generation."}, {"title": "Enhancing semantic multimodal fusion via contrastive LDM learning", "content": "The generation conditioning on EHR data is challenging because the EHR and CXR data are highly heterogeneous and the interactions are implicit. To force the LDM to utilize EHR information during generation, we propose a contrastive way of learning the conditional LDM. Specifically, for each EHR time series, we obtain a perturbed version of its representation $\\widetilde{E}^{(t_0,t_1)}_{EHR} = (1 - \\beta)E^{(t_0,t_1)}_{EHR} + \\beta \\delta$, where $\\delta \\sim N(0,I)$ is randomly drawn from a standard normal distribution, $\\beta$ is a hyperparameter controlling the strength of the noise. When the perturbed EHR is given as input, we expect the generated image to be far away from the target image. This leads to the following training objective function:\n\n$L_{LDM} = \\mathbb{E}_{Z_{t_0}, X^{(t_0,t_1)}_{EHR}, \\epsilon \\sim N(0,I), \\eta} ||\\epsilon - \\epsilon_{\\theta}(Z^{(n)}_{t_1}, Z_{t_0}, f_{cond}^{EHR}(X^{(t_0,t_1)}), n)||^2_2 + \\lambda_1 max (0, || \\epsilon - \\epsilon_{\\theta}(Z^{(n)}_{t_1}, Z_{t_0}, f_{cond}^{EHR}(\\widetilde{X}^{(t_0,t_1)}), n) ||^2_2 + \\alpha, 0),$ (3)\n\nwhere $\\alpha$ is a hyperparameter controlling the tolerance of the noisy-conditional generation. $\\lambda_1$ is a coefficient controlling the strength of the contrastive term. To ensure stability during training, we set the initial value of $\\lambda_1$ to zero and linearly increase it to one during training."}, {"title": "3.3 Prediction Stage", "content": "In the prediction stage, we do not have access to an up-to-date CXR image. Therefore, we generate an updated latent CXR $\\hat{Z}_{t_1}$ at the prediction time $t_1$ using the last available CXR image $X^{CXR}_{t_0}$ as the reference image and the EHR time series in between $X^{(t_0,t_1)}_{EHR}$. To make predictions using available EHR, we adopt another time series encoder, $f_{pred}^{EHR}$, which has the same structure as $f_{cond}^{EHR}$ as in Eq. (2). Note that for prediction, the EHR data used, $X^{(t_0,t_1)}_{EHR}$, covers all EHR time series, with the observation time set as 48 hours, ensuring the available information is fully utilized. In other words, $X_{EHR}^{(t_0,t_1)} \\subset X_{EHR}^{<48h}$. In clinical practice, clinicians make predictions not only based on the latest CXR, but also on past CXR images as reference for disease basis. To this end, we employ all available data: $X^{CXR}_{t_0}, X^{EHR}, and the generated latent CXR $\\hat{Z}_{t_1}$ to make the final clinical prediction:\n\n$\\hat{y} = G_{\\Theta}(f_{pred}^{CXR}(X^{CXR}_{t_0}), f_{pred}^{EHR}(X^{EHR}), f_{pred}^{LAT}(\\hat{Z}_{t_1})),$ (4)\n\nHere $f_{pred, i}, i \\in \\{CXR, EHR, LAT\\}$ are encoders for CXR, EHR, and the generated latent CXR, accordingly. We parameterize $f_{pred}^{LAT}$ and $f_{pred}^{EHR}$ using Transformer models, and $f_{pred}^{CXR}$ using a ResNet model. The predicting model $G_{\\Theta}$ with $\\Theta$ denoting the model parameter, is parameterized by a self-attention layer. We learn it by minimizing the cross-entropy (CE) loss:\n\n$L_{task} := \\sum_{m=1}^{M'} \\sum_{l=1}^{L'} y_{ml} log(\\hat{y}_{ml}) + (1 - y_{ml}) log(1 - \\hat{y}_{ml}),$ (5)\n\nwhere $L'$ is the number of classes in the prediction task and $M'$ is the number of training samples in the prediction stage."}, {"title": "4 Experiments", "content": "4.1 Experiment Settings\nDatasets We empirically evaluate the clinical predictive performance of DDL-CXR using MIMIC-IV [50] and MIMIC-CXR [18]. MIMIC-IV comprises de-identified critical care data from adult patients admitted to either ICUs or the emergency department (EDs) of Beth Israel Deaconess Medical Center (BIDMC) between 2008 and 2019, and MIMIC-CXR contains chest X-rays and reports collected from BIDMC, with a subset of patients matched with those in MIMIC-IV. For EHR data, we follow a preprocessing pipeline similar to that described in [10]. 17 clinical time series variables as well as age and gender are extracted. The details can be found in Appendix A.2.\nDataset construction and partition The inclusion criteria for this study involve ICU stays from the matched subset of MIMIC-IV and MIMIC-CXR that contain at least one CXR image (with Anterior-Posterior (AP) projection) during the ICU stay or within 24 hours before ICU admission. We exclude ICU stays with lengths shorter than 48 hours. The dataset is randomly split by the patient identifier with a ratio of 24:4:7 for training, validation, and testing, which avoids patient overlapping between subsets.\nFrom the training patients, we further extract data for training the VAE, the LDM, and the prediction model. We extract all images from the training patients for training VAE and extract all CXR image pairs of the same patient taken at any interval greater than 12 hours for training the LDM, i.e.,\n\n$D_{LDM} = \\{(X^{CXR}_{t_0}, X^{(t_0,t_1)}_{EHR}, y^{XR}_{t_1}), X^{CXR}_{t_1})|(t_1-t_0)>12h,\\}$ \n\nwhere a single ICU stay may contain multiple data pairs for LDM training. This greatly enlarges the training subset for the LDM stage.\nFor the prediction stage, we extract the last available CXR image and the EHR time series in the first 48 hours and the label for the prediction task of each ICU stay, i.e., the triplet $(X^{CXR}_{t_0}, X^{(t_0,t_1)}_{EHR}, y_{task,t_1})$. Note that the EHR time series used in the prediction stage differs from that in the LDM stage in their time interval since they serve for different purposes."}, {"title": "4.2 Prediction Performance", "content": "DDL-CXR obtains the best overall performance. We summarize the overall performance of the phenotype classification and in-hospital mortality prediction in Table 1, where DDL-CXR outperforms all baselines. This shows that generating an updated CXR during test time is beneficial for downstream tasks. On the contrary, DrFuse, MedFuse, DAFT, and MMTM use the last available CXR for prediction, which might have been outdated.\nThe performance gain of DDL-CXR in terms of AUPRC is particularly noteworthy as the AUPRC metric is especially relevant in the context as it underscores the effectiveness of our approach in identifying the positive class in imbalanced medical datasets. DDL-CXR achieves relative improvements of 2.4% and 3.56% over the best baselines in terms of AUPRC for phenotype classification and mortality prediction, respectively.\nMortality prediction with varying time interval We define the time interval (by hour) between the prediction time and the time of the last available CXR as \u03b4 and compute the evaluation metrics in patient groups with different ranges of \u03b4. The results are presented in Table 2. Since the label prevalence varies significantly between groups, making the comparison of AUPRC between groups less meaningful, we report AUROC in the paper and AUPRC in the appendix. DDL-CXR consistently outperforms the baseline models for most groups of \u03b4 for the mortality prediction task. As the \u03b4 increases, the last CXR becomes more \"outdated\", and we observe a noticeable increase in the"}, {"title": "4.3 Quality of Generated Chest X-ray Images", "content": "Quantitative Evaluation We evaluate the quality of generated CXRs using the test set of the LDM stage, where the ground-truth target CXR is available. The Fr\u00e9chet Inception Distance (FID) score [54] evaluates the similarity between the distributions of generated and ground-truth target CXRs by computing Fr\u00e9chet distance on the representation obtained from a pre-trained Inception-v3 network. Besides, we directly measure the Fr\u00e9chet distance (FD) and Wasserstein distance (WD) in the latent space of the VAE between the generated and the target CXRs. Results are shown in Table 4. Results of \"Last-CXR\" are obtained between reference images $X_{t0}$ and target images $X_{t1}$, both are directly from the dataset without generation. Thus, this provides a reference to the lower bound of the metrics used. DDL-CXR surpasses GAN-based methods across all metrics and obtains the lowest FD and WD. \"w/o $Z_{t0}$\" and \"w/o $E^{(t_0,t_1)}$\" are obtained by removing the condition of the last available CXR and EHR data, respectively. Notably, excluding EHR data from the generation conditions resulted in lower FID scores, which is natural since the generation becomes less restrictive.\nQualitative Evaluation To further visually examine the generated CXR images, we decode the latent CXR and visualize seven examples in Fig. 3. The first row shows the last CXR images used as reference, the second row displays the ground-truth CXR images, and the last row showcases the generated CXR images. The comparison between the first and third rows indicates that the generated CXR could well capture anatomical structure, while the comparison between the second and third rows demonstrates that the generated CXRs are in line with the latest imaging manifestations, implying that the disease progression embedded in EHR could be captured and utilized in the generation process. We further retrieve the radiology reports and the discharge summary of the corresponding patients from the database for case studies. Fig. 1 shows one example of the case study (Sample #4), where the patient rapidly turned from normal CXR to severe pulmonary consolidation. The discharge summary shows that the patient experienced transfusion-related acute lung injury and sepsis. Evidently, the generated CXR could more accurately reflect the progressed condition of the patient. Due to space limits, we present more case studies in Appendix B.5."}, {"title": "4.4 Ablation Study", "content": "To better understand the factors contributing to the improved performance, we conducted an ablation study by removing the conditioning components in the LDM stage. The results are summarized"}, {"title": "5 Broader Impacts and Limitations", "content": "DDL-CXR holds promise for societal benefits, such as more precise and timely medical interventions, and offers an alternative for patients with limited access to X-ray imaging. Nonetheless, the potential for generating fake profiles necessitates stringent safeguards, including expert validation of synthesized images, to prevent misuse and protect patient confidentiality, especially when applied to private datasets. Despite its promise, DDL-CXR has some limitations like the need for meticulous hyperparameter tuning and a performance gap across different time intervals, as indicated in Table 2. Addressing such potential biases is a priority for future research. Furthermore, while various metrics have been employed to assess generation quality, expert evaluation by radiologists would provide a more insightful measure of the model's efficacy."}, {"title": "6 Conclusion", "content": "In this paper, we introduce DDL-CXR, which utilizes a powerful LDM to dynamically generate up-to-date latent chest X-rays to tackle the asynchronicity of multi-modal clinical data for predictions. Our approach involves leveraging various conditions for patient-specific generation: the most recent available chest X-ray to incorporate detailed patient-specific anatomical structure, as well as the EHR data with variable durations for disease progression information. To improve multi-modal fusion in the generation, we develop a contrastive-learning-based LDM to capture and utilize disease courses in EHR. Through quantitative and qualitative validations, we demonstrate the superior performance of DDL-CXR in both image generation and enhancing multi-modal fusion via conditional generation for clinical prediction."}, {"title": "A Experiment Details", "content": "A.1 Details of Architectures and Training Procedures of DDL-CXR\nThe training and validation processes are executed on a server equipped with a RTX 4090-24GB GPU card and a 16 vCPU Intel Xeon Processor. The method is implemented using PyTorch 1.9.1 and PyTorch-Lightning 1.4.2. DDIM [33] sampling with 200 steps is employed to accelerate the sampling process, and AdamW optimizer is used for all model training. Our implementation is partially based on the repository of the latent diffusion model [22].\nVariational autoencoder (VAE) The VAE training process, as outlined in [22], includes a pixel-wise reconstruction loss, a perceptual loss [46], an adversarial objective, and a lightly-penalized Kullback-Leibler loss toward a standard normal to constrain latent spaces, given by:\n\n$L_{VAE} \\triangleq \\min_{\\varepsilon,D,\\Phi} \\max_{\\psi} (L_{rec}^{L_{rec}}(X^{CXR}, D(\\varepsilon(X^{CXR}))) - L_{adv}(D(\\varepsilon(X^{CXR}))) + log D_w(D_w(X^{CXR})) + L_{reg}(X^{CXR}; \\varepsilon, D) + L_{CE}(\\Phi(\\varepsilon(X^{CXR})), y)),$ (6)\n\nwhere $\\Phi$ is a classifier that predicts the CheXpert labels associated with the image and we parameterize it with an MLP. All CXRs in the training subset of the LDM stage are gathered for VAE training. A compression rate $r = 8$ is adopted, and the training continues for a maximum of 50 epochs. The model with the minimum validation error, as measured using CXRs from the validation subset, is selected. The resulting latent representation has a dimension of $4 \\times 28 \\times 28 = 3136$. To restrict the normal prior in the latent space and prioritize reconstruction quality, a KL-divergence weighting of $1 \\times 10^{-6}$ is set. We use the base learning rate of $4.5 \\times 10^{-6}$, which is scaled by the number of GPU cards and batch size."}, {"title": "Latent diffusion model (LDM) stage in DDL-CXR", "content": "In the LDM stage of our DDL-CXR model, we employ the UNet architecture [47] as the neural backbone, denoted by $\\epsilon_{\\theta}$. Meanwhile, we utilize a multivariate time series Transformer [48] for the EHR conditioning encoder $f_{cond}^{EHR}$. The Transformer $f_{cond}^{EHR}$ is designed with one layer, a model dimension $d$ set to 128, and a maximum EHR data length of 70. The UNet model $\\epsilon_{\\theta}$ features an input channel of 8 and an output channel of 4. The encoding section comprises three blocks, with model channels set at 224, 448, and 672, consisting of a ResBlock module followed by a spatial transformer. The bottleneck consists of two ResBlock modules with a spatial transformer in between. The decoder mirrors the encoder architecture. As discussed in Section 3.2, we introduce the encoded EHR information through multi-head cross-attention to the spatial transformer module of $\\epsilon_{\\theta}$. The context dimension is set to 128, and the number of attention heads is 8. The model is trained for 200 epochs with a batch size of 32, and the model with the smallest composite loss on the validation set is selected for subsequent latent Chest X-ray (CXR) generation. We set the hyperparameters $\\alpha$ to 0.2, and $\\beta$ to 0.5, empirically."}, {"title": "Latent CXR generation via LDM", "content": "In the LDM stage, we aim to generate latent CXR images at time $t_1$, conditioned on $X_{t_0}^{CXR}$ and $X_{EHR}^{(t_0,t_1)}$. We first encode the CXR images using the pre-trained VAE, given by:\n\n$Z_{t_0} = \\varepsilon(X_{t_0}^{CXR}), Z_{t_1} = \\varepsilon(X_{t_1}^{CXR}).$ (7)\n\nEssentially, the latent CXR generation requires us to estimate the underlying data distribution $q(Z_{t_1}| Z_{t_0}, X_{EHR}^{(t_0,t_1)})$. The LDM approximates this distribution via a model distribution $p_{\\theta}(Z_{t_1}| Z_{t_0}, X_{EHR}^{(t_0,t_1)})$, where $Z_{t_1}^{(0)}$ represents the prior of a CXR in the latent space encoded by the VAE.\nWe follow prior work on diffusion models to learn our LDM, which involves two processes [32, 22]. In the diffusion process, we gradually add Gaussian noise to $Z_{t_1}^{(0)}$ in $N$ steps, producing a sequence of noisy representations $Z_{t_1}^{(1)}, Z_{t_1}^{(2)}, ..., Z_{t_1}^{(N)}$, with the transition probability given by:\n\n$q(Z_{t_1}^{(n)}| Z_{t_1}^{(n-1)}) = N(Z_{t_1}^{(n)}; \\sqrt{1 - \\beta_n} Z_{t_1}^{(n-1)}, \\beta_n I),$ (8)\n$q(Z_{t_1}^{(n)}| Z_{t_1}^{(0)}) = N(Z_{t_1}^{(n)}; \\sqrt{\\bar{a}_n} Z_{t_1}^{(0)}, (1 - \\bar{a}_n)\\epsilon),$\n\nwhere"}, {"title": "Prediction stage in DDL-CXR", "content": "In the prediction stage, the EHR data is encoded using a one-layer Transformer with a model dimension of 128. We set the dimension of the feedforward layers to 512. The context dimension is also set to 128, and the number of attention heads is 8. We use another Transformer with the same architecture to encode the generated latent CXR $\\hat{Z}_{t_1}$. We use a ResNet-34 model to encode the last available CXR image $X_{t_0}$. The encoded EHR, the encoded latent CXR $\\hat{Z}_{t_1}$, as well as the encoded $X_{t_0}$ are fed into a self-attention layer for final prediction."}, {"title": "A.2 Details of Data Preprocessing", "content": "EHR data preprocess We follow a similar EHR data extraction and processing pipeline as [10] but change the sampling frequency from 2h to 1h and introduce two static variables, age, and gender. We extract 17 clinical time series variables (12 continuous and 5 categorical) with discretization and standardization processes exactly the same as in [10]. In addition to the 17 clinical time series variables mentioned in the paper [10], e.g. five categorical (capillary refill rate, Glasgow Coma Scale eye-opening, Glasgow Coma Scale motor response, Glasgow Coma Scale verbal response, and Glasgow Coma Scale total) and 12 continuous (diastolic blood pressure, fraction of inspired oxygen, glucose, heart rate, height, mean blood pressure, oxygen saturation, respiratory rate, systolic blood pressure, temperature, weight, and pH), we introduce two crucial static variables (age and gender) to represent the demographic information of a patient, as patient demographic information is vital for achieving accurate predictions [55]. To construct our dataset, we sampled time series data at hourly intervals, followed by discretization and standardization processes. We adopt masks to handle missing values in time series to capture the missing pattern, acknowledging that the absence of medical data might be intentional and non-random, driven by caregivers [55]."}]}