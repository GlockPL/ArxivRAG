{"title": "Can LLMs Understand Social Norms in Autonomous Driving Games?", "authors": ["Boxuan Wang", "Haonan Duan", "Yanhao Feng", "Xu Chen", "Yongjie Fu", "Zhaobin Mo", "Xuan Di"], "abstract": "Social norm is defined as a shared standard of acceptable behavior in a society. The emergence of social norms fosters coordination among agents without any hard-coded rules, which is crucial for the large-scale deployment of AVs in an intelligent transportation system. This paper explores the application of LLMs in understanding and modeling social norms in autonomous driving games. We introduce LLMs into autonomous driving games as intelligent agents who make decisions according to text prompts. These agents are referred to as LLM-based agents. Our framework involves LLM-based agents playing Markov games in a multi-agent system (MAS), allowing us to investigate the emergence of social norms among individual agents. We aim to identify social norms by designing prompts and utilizing LLMs on textual information related to the environment setup and the observations of LLM-based agents. Using the OpenAI Chat API powered by GPT-4.0, we conduct experiments to simulate interactions and evaluate the performance of LLM-based agents in two driving scenarios: unsignalized intersection and highway platoon. The results show that LLM-based agents can handle dynamically changing environments in Markov games, and social norms evolve among LLM-based agents in both scenarios. In the intersection game, LLM-based agents tend to adopt a conservative driving policy when facing a potential car crash. The advantage of LLM-based agents in games lies in their strong operability and analyzability, which facilitate experimental design.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated re- markable capabilities in natural language understanding, en- abling them to process and generate human-like text across a wide range of domains [1]\u2013[5]. The increasing popularity of LLMs facilitates the design of numerous applications, allow- ing LLMs to frequently interact with us in our daily lives. With enhanced capabilities, LLMs hold considerable promise for applications in traffic planning and autonomous driving. This paper aims to study how LLM-guided agents behave and interact with each other in autonomous driving scenarios. Specifically, we utilize LLMs to analyze autonomous driving games in which LLM-guided agents are tasked with making decisions in simulated driving environments. Our goal is to investigate whether these agents can understand social norms in autonomous driving games.\nIn recent years, autonomous driving technology has wit- nessed significant advancements in recent years, promising safer and more efficient transportation systems. However, the integration of autonomous vehicles into society [6] raises concerns about their interaction with human drivers, pedestrians, and other road users. One critical issue of this interaction is the adherence to social norms, the informal rules that guide vehicles to navigate the road. Understanding and effectively modeling these social norms are essential for the development of autonomous driving systems capable of navigating real-world scenarios effectively.\nIn this paper, we explore the potential of leveraging Large Language Models (LLMs) to understand social norms in autonomous driving games. By utilizing LLMs on textual information related to autonomous driving scenarios, we can teach them to identify the social norms among vehicles on the road. By observing and analyzing the choices made by players in these games, we can extract implicit social norms that regulate their behavior. Our research aims to contribute to the development of socially aware autonomous driving systems that can effectively navigate complex driving envi- ronments. Leveraging LLMs to understand and model social norms can empower safer interactions between autonomous vehicles and other road users."}, {"title": "A. Related work", "content": "To explore LLMs' capability of understanding human be- haviors [7]-[9], there has been a growing trend of analyzing LLMs in the context of game theory [10], [11], including fairness [12], dilemma [13] and rationality [14]\u2013[16]. Many studies employ LLMs to replace humans as research subjects [17], [18]. To study complex interactions among players in game theory problems, LLMs are introduced into game- theoretic frameworks as intelligent agents, referred to as LLM-based agents [19], [20]. These LLM-based agents empower the development of sophisticated systems [21]-[24] where players' behaviors can be simulated via LLMs. For example, LLM-based agents are utilized to study coopera- tion and coordination behaviors in the prisoner's dilemma [13]. This paper further explores the capability of LLM- based agents regarding social outcomes in games. More specifically, we investigate whether LLM-based agents can form desired social norms in autonomous driving games to improve driving efficiency and road safety.\nSocial norms have been widely studied in games. Most lit- erature on social norms primarily focuses on stateless matrix games [25]\u2013[29]. To capture dynamic environments, Markov games are developed to study the emergence of social norms [30], [31] in sequential decision making. A learning-based framework is proposed to study how social norms evolve in autonomous driving games [32], where agents are guided by a deep reinforcement learning (DRL) algorithm. Compared to DRL-based agents, LLM-based agents have the following advantages: strong operability, relatively simple experimen- tal design in game theory, and strong analyzability [22]. Therefore, we introduce LLM-based agents into autonomous driving games to handle dynamically changing driving envi- ronments."}, {"title": "B. Contributions of this paper", "content": "This work focuses on how LLMs can guide agents in autonomous driving games. Our study utilizes the OpenAI Chat API, powered by GPT-4.0, to conduct experiments in simulated driving environments. The contributions include:\n1) We propose a framework where LLM-guided agents play Markov games in a multi-agent system (MAS) in order to investigate whether social norms emerge among individual agents.\n2) We design prompts to simulate interactions in an autonomous driving game. The prompts consist of \"system\" messages outlining the general set-up (e.g., the road layout) and \u201cuser\u201d messages regarding the ob- servations and decision making of LLM-guided agents.\n3) We apply the proposed framework to two traffic sce- narios: the unsignalized intersection and highway pla- toon, to examine social norms formed by autonomoud vehicles (AVs)."}, {"title": "II. PRELIMINARY", "content": ""}, {"title": "A. Markov Game", "content": "Markov games, also known as stochastic games, are a mathematical framework that generalizes both Markov deci- sion processes (MDPs) and game theory to model dynamic interactions among rational players in uncertain environ- ments. In its general form, it is defined by a states set $S$ and action sets $A_1, ..., A_n$ for each corresponding agent in the environment. Each agent also has a corresponding reward function $R_i: S \\times A_i \\rightarrow \\mathbb{R}$ for agent $i$. The goal of the Markov Game is to find an optimal policy for each agent.\nIn this paper, we adopt the general form of Markov Game with a multi-agent system. We want to observe the behavior and decision-making of LLMs when playing Markov Game as agents. We use $S$ to denote the environment set-up which is fed to system agent, $A_i$ to denote the action set of agent $i$, $O_i$ to denote the observation set of agent $i$, $R_i$ to denote the current reward of agent $i$. The actions of $A_i$ are made by LLM(agent i) with the input of $O_i$ and part of $S$. The reward $R_i$ of agent $i$ is output by LLM.\nThe optimal policy of agent $i$ is decided solely by LLMs, so we can investigate whether social norms are formed within the context of the above Markov Game."}, {"title": "B. Social Norms", "content": "Social norms represent the collective expectations regard- ing appropriate conduct within groups. They encompass both the unwritten agreements that dictate how society members should act, and the formalized regulations and laws that are established. Social norms in car driving are the unwritten rules that guide driver behavior on the road, focusing on safety, courtesy, and communication. These include obey- ing traffic laws, signaling turns, maintaining safe distances, yielding when necessary, using headlights properly, acknowl- edging other drivers, and staying in the correct lane.\nIn this paper, our primary focus is on examining whether the agents could develop social norms, including avoiding crashes and yielding, in two specific scenarios."}, {"title": "C. Large Language Models", "content": "A large language model (LLM) is a type of artificial intelligence algorithm designed to understand, generate, and interact with human language at a large scale. These models are trained on vast amounts of text data, learning patterns, vocabulary, grammar, and even nuances of language such as context, tone, and implications. They use a specific architecture known as Transformer, which allows them to efficiently process sequences of words and predict the next word in a sentence, understand the meaning of a text, or generate new text that follows a given prompt.\nLLMs are highly versatile AI tools with applications spanning multiple industries and fields. They excel in tasks such as content creation, customer support, education, lan- guage translation, software development, legal documenta- tion, healthcare research, optimization, sentiment analysis, and accessibility technologies. In this paper, we examine the application of LLM in Markov games and autonomous vehicles to see if they will make appropriate decisions and form the social norms."}, {"title": "D. Scenario 1: Unsignalized Intersection", "content": "The environment set $S$ of a unsignalized intersection is characterized by two intersecting thoroughfares, with Road 1 extending from the West to the East and Road 2 extending from the North to the South. Green and red cars symbolize the agents traveling on Road 1 and Road 2, respectively. Agents aim to cross the intersection with action set $A_i$ for agent $i$. Meanwhile, there will be some background vehicles that have predetermined driving strategies and do not develop any driving policy in Markov games. The background vehi- cles are included in $S$. Since there is no traffic signals, agents need to observe the position of other vehicles and make appropriate decisions to avoid accidents. The observation is included in $O_i$ for agent $i$.\nTable I shows the set-up of agent $i$ at the beginning of time $t$. In our set-up, agent $i$'s location is which cell it is current at. The observation $o_i$ is a subset of current state space $S$. Each agent can observe its own state $s_i$ in its entirety - both the location and the cumulative reward, but could only see the locations (not the cumulative rewards) of all other cars. The effect(i.e. the reward and state update) caused by the action $a_i[t]$ will not manifest until the beginning of the next time step $t + 1$. In other words, the reward $r_i[t]$ reflects on the cumulative reward of $t + 1$. The action $a_i[t]$ reflects on the location at $t + 1$."}, {"title": "E. Scenario 2: Highway Platoon", "content": "The environment set $S$ of a highway platoon consists of two lanes, going from South to North. We randomize initial locations of agents on two lanes. Agents aim to complete the trip quickly with action set $A_i$ for agent $i$. Meanwhile, there will be some background vehicles that have no driving strategies. The background vehicles are included in $S$. Also, agents need to observe the position of other vehicles and make appropriate decisions to avoid accidents. The observation is included in $O_i$ for agent $i$.\nTable II shows the set-up of agent $i$ at time $t$. In additional to the intersection set-up, the agent now can also choose to switch to the other lane without moving forward. Accord- ingly, we assigned a platoon reward of -2."}, {"title": "III. METHODOLOGY", "content": "Our study employs the OpenAI Chat API, specifically the GPT-4.0 model, to conduct our experiments. Notably, GPT- 4.0 has demonstrated significant improvement over its prede- cessor, GPT-3.5, by at least 20%, as per OpenAI's assertions, on an array of benchmarks such as the SuperGLUE language comprehension and the LAMBADA linguistic modeling tests. [33] As an advanced version of the transformer-based language model, GPT-4.0 holds considerable potential for traffic planning and autonomous driving."}, {"title": "A. Prompt Design", "content": "The chat model facilitates conversations through three distinct roles, including 'user', 'assistant', and 'system'. 'User' represents the human participant interacting with the AI. The 'assistant' role embodies the AI providing responses to the user's prompts. Finally, the 'system' role is primarily responsible for defining the assistant's behavior this can involve setting a specific language, character, or viewpoint for the assistant. The chat model prompt is structured as a series of messages, each identified by the role of the sender and the message content. By concatenating past interactions from both the user and assistant, the model can more effectively engage in multi-turn conversations.\nIn our research, we tailored our experiment without con- sidering any past states. This led us to use only one \"system\" message and one \"user\" message to construct our prompt. The \"system\" message outlines the general set-up, defining parameters like the environment space $S$ (i.e., the road layout), action space $A$ (i.e., all the permitted moves), and the reward function $R$. Meanwhile, the \"user\" message takes the perspective of a specific car. This message first identifies agent $i$ (i.e., its color) and its present observation $o_i$ (i.e., the locations of all the cars and its own cumulative reward). After that, the prompt asks the agent about the action $a_i$ it should take and the resulting location due to the action of choice."}, {"title": "B. Programming", "content": "We selected Python as our programming language and utilized the chat model to choose the move and calculate the location. Our programming was responsible for managing the progression of the game and computing the ground truth reward."}, {"title": "IV. EXPERIMENTS", "content": "We modeled the environments of both the intersection and platoon scenarios using a grid consisting of multiple cells, with each cell assigned a pair of coordinates. In our experiments, we only consider one green car, one red car, and one white car. The red and green cars are participating agents, while the white car serves as a background vehicle. The goal of the participating agents is to complete the trip and maximize their cumulative reward by choosing various actions. The background vehicle's goal is simply to complete the trip, with its policy dictating that it will choose \"Go\" at each time step.\nThe set-up is stored in a configuration file for ease of adjustment. This file contains all prompts, a list of cars, and their initial positions. Given that the general set-up is consistent for all agents and is unaffected by time, the system message portion of the prompt is stored as a static string. On the other hand, each car's private observation, influenced by time and specific agent conditions, is stored as templates. Each time these templates are accessed, they are customized in accordance with present conditions.\nAs previously mentioned, each car could observe both the location and the cumulative reward of itself, but could only see the locations (not the cumulative rewards) of all other cars. To easily concatenate the observation about all other cars, we stored the user message as two separate templates: one for the observing itself, and one for all other cars. We assembled all these components of the prompt together in our programming.\nIn both scenarios, we utilized the OpenAI API with the specified parameters: model = gpt-4, temperature = 0.1, max_token = 10. Increasing the temperature enhances the randomness of the response. [34] Our testing revealed that raising the temperature beyond 1.0 sometimes resulted in responses that did not conform to the desired format. Further increasing the temperature to 2.0 led to responses that were essentially unintelligible, as exemplified in Figure 5. The max_tokens parameter restricts the maximum length of the response. As per OpenAI's tokenizer, the language model can consistently provide responses in the preferred format within 10 tokens. [35]"}, {"title": "A. Scenario 1: Unsignalized Intersection", "content": "We modeled the environment of intersection using a 9 by 9 grid. We adjust the prompt according to the number and placement of the background vehicles.\nIn order to inspect the behavior of LLMs, we decided to simulate the game multiple times. We simulated the Markov Game 50 times without BVs(short for background vehicles), 10 times with 1 to 4 BVs respectively. The starting positions of 4 BVs are (5,2),(5,3),(4,5),(5,5). Since we set background vehicles as non-stopping white cars, we have to allocate them this way to avoid crashing on background vehicles.\nWe want to explicitly investigate the behavior of Green car and Red car when there is no interference of background vehicles. We found out that Red car tends to stop before several grids of the intersection, so we define the stops at positions other than (4,5) or (5,4) as early stops. From the plot, we know that Green car never stops earlier until it reaches the intersection and Red car stops when it is more than 1 grid away from the intersection. We can possibly infer that the LLM of Red car tends to make cautious decisions and behave courteously. Now, we can take a look at the regular stops meaning that they stop at (4,5) or (5,4). From the plot, we know that both car tends to make multiple stop before the intersection regardless the other car's position. We can possibly infer that the both cars wait long enough and one of them makes an action $a_i$=Go at timestamp i to finish the game. Now, we can investigate what happens in the crashed simulations which are 6-th and 37-th simulation. In 6-th simulation, Red car didn't stop earlier and decided to Go at the same time as Green car so that crash happened. In 37-th simulation, Red car stopped earlier and Green car stopped regularly before the intersection. After Red car arrived at intersection, they both began to wait and again decided to Go at the same time.\nGiven the context of Markov Game, they have a 96% success rate. When they stop simultaneously, we believe that it mimics the social norm in human society. The random Go action mimics the driver who breaks the dilemma. However, human drivers could always stop if they notice the other driver chose to Go. That is the scenario that we can't simulate by Markov Game."}, {"title": "B. Scenario 2: Highway Platoon", "content": "We modeled the environment of intersection using a 2 by 9 grid. We adjust the prompt according to the number and placement of the background vehicles.\nIn this scenario, we did similar thing from previous scenario. We simulated the Markov Game 50 times without BVs, 10 times with 1 to 4 BVs respectively. The starting positions of 4 BVs are (1,2), (2,2), (1,4), (2,4). With our parameters set up, we find that in this scenario, the LLM performed excellently, and there were no car crashes in the simulations.\nIn this scenario, our focus is primarily on the frequency and timing of lane changes, which are crucial for platoon for- mation. Additionally, we perform an analysis of the scenario in the absence of interference from background vehicles. From the plot, we can see that there is at most one lane change per simulation, with the green car executing most of the lane changes. This is reasonable, as it suggests that agents are striving to minimize lane changes, which are a significant factor in car crashes on highways. Furthermore, the LLM of the green car may exhibit a more aggressive or consistent lane-changing behavior.\nFigure 11 is the plot of the step time when the agents choose to change the lane, and figure 12 is the percentage of time of platoon in each game. From the plots, we observe that in most cases, agents opt to change lanes before time step 5. Moreover, in the majority of simulations, there is more than a 60 percent likelihood of two agents forming a platoon on the highway. From this, we can infer that LLM agents are inclined to change lanes and form platoons at an early stage, demonstrating an awareness that platoon formation is an efficient driving strategy on highways.\nThe behavior of changing lanes and forming platoons early is believed to mimic the social norms found in human society. Additionally, the varying behaviors of the agents may represent the diverse driving habits encountered in the real world. [argument of social norm in conclusion part?]"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This paper has demonstrated the potential of large lan- guage models (LLMs) in understanding and modeling social norms in autonomous driving scenarios. By introducing LLM-based agents into autonomous driving games, we have observed the emergence of social norms among agents navigating complex driving environments. Our experiments have shown that LLM-based agents can adapt and conform to social norms, thereby contributing to safer and more efficient driving behaviors. Overall, the advantage of em- ploying LLM-based agents in autonomous driving games lies in their strong operability and analyzability, which facilitate experimental design and provide valuable insights into the dynamics of social norms in driving environments. By further investigating the behavior of LLM-based agents in various driving scenarios and refining our experimental framework, we can contribute to the development of more socially aware autonomous driving systems.\nThis work can be extended in the following ways: (1) We will utilize LLM-based agents in more complex real- world scenarios, such as sequential social dilemmas. (2) We aim to explore how to construct a unified, controllable, and efficient framework for simulating strategic interactions and facilitating game design using LLMs. (3) We will compare the behaviors of human players and LLM-based agents to determine the extent to which LLMs can achieve strategic reasoning."}]}