{"title": "Can LLMs Understand Social Norms in Autonomous Driving Games?", "authors": ["Boxuan Wang", "Haonan Duan", "Yanhao Feng", "Xu Chen", "Yongjie Fu", "Zhaobin Mo", "Xuan Di"], "abstract": "Social norm is defined as a shared standard of\nacceptable behavior in a society. The emergence of social norms\nfosters coordination among agents without any hard-coded\nrules, which is crucial for the large-scale deployment of AVs\nin an intelligent transportation system. This paper explores\nthe application of LLMs in understanding and modeling social\nnorms in autonomous driving games. We introduce LLMs into\nautonomous driving games as intelligent agents who make\ndecisions according to text prompts. These agents are referred\nto as LLM-based agents. Our framework involves LLM-based\nagents playing Markov games in a multi-agent system (MAS),\nallowing us to investigate the emergence of social norms among\nindividual agents. We aim to identify social norms by designing\nprompts and utilizing LLMs on textual information related\nto the environment setup and the observations of LLM-based\nagents. Using the OpenAI Chat API powered by GPT-4.0, we\nconduct experiments to simulate interactions and evaluate the\nperformance of LLM-based agents in two driving scenarios:\nunsignalized intersection and highway platoon. The results\nshow that LLM-based agents can handle dynamically changing\nenvironments in Markov games, and social norms evolve among\nLLM-based agents in both scenarios. In the intersection game,\nLLM-based agents tend to adopt a conservative driving policy\nwhen facing a potential car crash. The advantage of LLM-based\nagents in games lies in their strong operability and analyzability,\nwhich facilitate experimental design.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated re-\nmarkable capabilities in natural language understanding, en-\nabling them to process and generate human-like text across a\nwide range of domains [1]\u2013[5]. The increasing popularity of\nLLMs facilitates the design of numerous applications, allow-\ning LLMs to frequently interact with us in our daily lives.\nWith enhanced capabilities, LLMs hold considerable promise\nfor applications in traffic planning and autonomous driving.\nThis paper aims to study how LLM-guided agents behave\nand interact with each other in autonomous driving scenarios.\nSpecifically, we utilize LLMs to analyze autonomous driving\ngames in which LLM-guided agents are tasked with making\ndecisions in simulated driving environments. Our goal is to\ninvestigate whether these agents can understand social norms\nin autonomous driving games.\nIn recent years, autonomous driving technology has wit-\nnessed significant advancements in recent years, promising\nsafer and more efficient transportation systems. However,\nthe integration of autonomous vehicles into society [6]\nraises concerns about their interaction with human drivers,\npedestrians, and other road users. One critical issue of this\ninteraction is the adherence to social norms, the informal\nrules that guide vehicles to navigate the road. Understanding\nand effectively modeling these social norms are essential for\nthe development of autonomous driving systems capable of\nnavigating real-world scenarios effectively.\nIn this paper, we explore the potential of leveraging Large\nLanguage Models (LLMs) to understand social norms in\nautonomous driving games. By utilizing LLMs on textual\ninformation related to autonomous driving scenarios, we can\nteach them to identify the social norms among vehicles on\nthe road. By observing and analyzing the choices made by\nplayers in these games, we can extract implicit social norms\nthat regulate their behavior. Our research aims to contribute\nto the development of socially aware autonomous driving\nsystems that can effectively navigate complex driving envi-\nronments. Leveraging LLMs to understand and model social\nnorms can empower safer interactions between autonomous\nvehicles and other road users."}, {"title": "A. Related work", "content": "To explore LLMs' capability of understanding human be-\nhaviors [7]\u2013[9], there has been a growing trend of analyzing\nLLMs in the context of game theory [10], [11], including\nfairness [12], dilemma [13] and rationality [14]\u2013[16]. Many\nstudies employ LLMs to replace humans as research subjects\n[17], [18]. To study complex interactions among players in\ngame theory problems, LLMs are introduced into game-\ntheoretic frameworks as intelligent agents, referred to as\nLLM-based agents [19], [20]. These LLM-based agents\nempower the development of sophisticated systems [21]\u2013[24]\nwhere players' behaviors can be simulated via LLMs. For\nexample, LLM-based agents are utilized to study coopera-\ntion and coordination behaviors in the prisoner's dilemma\n[13]. This paper further explores the capability of LLM-"}, {"title": "II. PRELIMINARY", "content": "Markov games, also known as stochastic games, are a\nmathematical framework that generalizes both Markov deci-\nsion processes (MDPs) and game theory to model dynamic\ninteractions among rational players in uncertain environ-\nments. In its general form, it is defined by a states set S\nand action sets A1, ..., An for each corresponding agent in\nthe environment. Each agent also has a corresponding reward\nfunction Ri: S\u00d7A; \u2192 R for agent i. The goal of the Markov\nGame is to find an optimal policy for each agent.\nIn this paper, we adopt the general form of Markov Game\nwith a multi-agent system. We want to observe the behavior\nand decision-making of LLMs when playing Markov Game\nas agents. We use S to denote the environment set-up which\nis fed to system agent, A; to denote the action set of agent\ni, O\u00a1 to denote the observation set of agent i, Ri to denote\nthe current reward of agent i. The actions of A; are made\nby LLM(agent i) with the input of Oi and part of S. The\nreward Ri of agent i is output by LLM.\nThe optimal policy of agent i is decided solely by LLMs,\nso we can investigate whether social norms are formed within\nthe context of the above Markov Game."}, {"title": "B. Social Norms", "content": "Social norms represent the collective expectations regard-\ning appropriate conduct within groups. They encompass both\nthe unwritten agreements that dictate how society members\nshould act, and the formalized regulations and laws that are\nestablished. Social norms in car driving are the unwritten\nrules that guide driver behavior on the road, focusing on\nsafety, courtesy, and communication. These include obey-\ning traffic laws, signaling turns, maintaining safe distances,\nyielding when necessary, using headlights properly, acknowl-\nedging other drivers, and staying in the correct lane.\nIn this paper, our primary focus is on examining whether\nthe agents could develop social norms, including avoiding\ncrashes and yielding, in two specific scenarios."}, {"title": "C. Large Language Models", "content": "A large language model (LLM) is a type of artificial\nintelligence algorithm designed to understand, generate, and\ninteract with human language at a large scale. These models\nare trained on vast amounts of text data, learning patterns,\nvocabulary, grammar, and even nuances of language such\nas context, tone, and implications. They use a specific\narchitecture known as Transformer, which allows them to\nefficiently process sequences of words and predict the next\nword in a sentence, understand the meaning of a text, or\ngenerate new text that follows a given prompt.\nLLMs are highly versatile AI tools with applications\nspanning multiple industries and fields. They excel in tasks\nsuch as content creation, customer support, education, lan-\nguage translation, software development, legal documenta-\ntion, healthcare research, optimization, sentiment analysis,\nand accessibility technologies. In this paper, we examine\nthe application of LLM in Markov games and autonomous\nvehicles to see if they will make appropriate decisions and\nform the social norms."}, {"title": "D. Scenario 1: Unsignalized Intersection", "content": "The environment set S of a unsignalized intersection is\ncharacterized by two intersecting thoroughfares, with Road\n1 extending from the West to the East and Road 2 extending\nfrom the North to the South. Green and red cars symbolize\nthe agents traveling on Road 1 and Road 2, respectively.\nAgents aim to cross the intersection with action set A\u00a1 for\nagent i. Meanwhile, there will be some background vehicles\nthat have predetermined driving strategies and do not develop\nany driving policy in Markov games. The background vehi-\ncles are included in S. Since there is no traffic signals, agents\nneed to observe the position of other vehicles and make\nappropriate decisions to avoid accidents. The observation is\nincluded in O; for agent i.\nTable I shows the set-up of agent i at the beginning of time\nt. In our set-up, agent i's location is which cell it is current"}, {"title": "E. Scenario 2: Highway Platoon", "content": "The environment set S of a highway platoon consists\nof two lanes, going from South to North. We randomize\ninitial locations of agents on two lanes. Agents aim to\ncomplete the trip quickly with action set A\u00a1 for agent i.\nMeanwhile, there will be some background vehicles that have\nno driving strategies. The background vehicles are included\nin S. Also, agents need to observe the position of other\nvehicles and make appropriate decisions to avoid accidents.\nThe observation is included in Oi for agent i.\nTable II shows the set-up of agent i at time t. In additional\nto the intersection set-up, the agent now can also choose to\nswitch to the other lane without moving forward. Accord-\ningly, we assigned a platoon reward of -2."}, {"title": "III. METHODOLOGY", "content": "Our study employs the OpenAI Chat API, specifically the\nGPT-4.0 model, to conduct our experiments. Notably, GPT-\n4.0 has demonstrated significant improvement over its prede-"}, {"title": "A. Prompt Design", "content": "The chat model facilitates conversations through three\ndistinct roles, including 'user', 'assistant', and 'system'.\n'User' represents the human participant interacting with the\nAI. The 'assistant' role embodies the AI providing responses\nto the user's prompts. Finally, the 'system' role is primarily\nresponsible for defining the assistant's behavior this can\ninvolve setting a specific language, character, or viewpoint\nfor the assistant. The chat model prompt is structured as\na series of messages, each identified by the role of the\nsender and the message content. By concatenating past\ninteractions from both the user and assistant, the model can\nmore effectively engage in multi-turn conversations.\nIn our research, we tailored our experiment without con-\nsidering any past states. This led us to use only one \"system\"\nmessage and one \"user\" message to construct our prompt.\nThe \"system\" message outlines the general set-up, defining\nparameters like the environment space S (i.e., the road\nlayout), action space A (i.e., all the permitted moves), and\nthe reward function R. Meanwhile, the \"user\" message takes\nthe perspective of a specific car. This message first identifies\nagent i (i.e., its color) and its present observation oi (i.e.,\nthe locations of all the cars and its own cumulative reward).\nAfter that, the prompt asks the agent about the action a\u00a1 it\nshould take and the resulting location due to the action of\nchoice."}, {"title": "B. Programming", "content": "We selected Python as our programming language and\nutilized the chat model to choose the move and calculate the\nlocation. Our programming was responsible for managing\nthe progression of the game and computing the ground truth\nreward."}, {"title": "IV. EXPERIMENTS", "content": "We modeled the environments of both the intersection\nand platoon scenarios using a grid consisting of multiple\ncells, with each cell assigned a pair of coordinates. In our\nexperiments, we only consider one green car, one red car,\nand one white car. The red and green cars are participating\nagents, while the white car serves as a background vehicle.\nThe goal of the participating agents is to complete the trip\nand maximize their cumulative reward by choosing various\nactions. The background vehicle's goal is simply to complete\nthe trip, with its policy dictating that it will choose \"Go\" at\neach time step.\nThe set-up is stored in a configuration file for ease of\nadjustment. This file contains all prompts, a list of cars,\nand their initial positions. Given that the general set-up is\nconsistent for all agents and is unaffected by time, the system\nmessage portion of the prompt is stored as a static string. On\nthe other hand, each car's private observation, influenced by\ntime and specific agent conditions, is stored as templates.\nEach time these templates are accessed, they are customized\nin accordance with present conditions.\nAs previously mentioned, each car could observe both the\nlocation and the cumulative reward of itself, but could only\nsee the locations (not the cumulative rewards) of all other\ncars. To easily concatenate the observation about all other\ncars, we stored the user message as two separate templates:\none for the observing itself, and one for all other cars. We\nassembled all these components of the prompt together in\nour programming.\nIn both scenarios, we utilized the OpenAI API with the\nspecified parameters: model = gpt-4, temperature = 0.1,\nmax_token = 10. Increasing the temperature enhances the\nrandomness of the response. [34] Our testing revealed that\nraising the temperature beyond 1.0 sometimes resulted in\nresponses that did not conform to the desired format. Further\nincreasing the temperature to 2.0 led to responses that were\nessentially unintelligible, as exemplified in Figure 5. The\nmax_tokens parameter restricts the maximum length of the\nresponse. As per OpenAI's tokenizer, the language model\ncan consistently provide responses in the preferred format\nwithin 10 tokens. [35]"}, {"title": "A. Scenario 1: Unsignalized Intersection", "content": "We modeled the environment of intersection using a 9 by 9\ngrid. Figure 6 shows an example of a prompt and a response.\nWe adjust the prompt according to the number and placement\nof the background vehicles.\nIn order to inspect the behavior of LLMs, we decided to\nsimulate the game multiple times. We simulated the Markov\nGame 50 times without BVs(short for background vehicles),\n10 times with 1 to 4 BVs respectively. The starting positions\nof 4 BVs are (5,2),(5,3),(4,5),(5,5). Since we set background\nvehicles as non-stopping white cars, we have to allocate them\nthis way to avoid crashing on background vehicles.\nWe want to explicitly investigate the behavior of Green\ncar and Red car when there is no interference of background\nvehicles. We found out that Red car tends to stop before\nseveral grids of the intersection, so we define the stops at\npositions other than (4,5) or (5,4) as early stops. Figure\n7 is the line plot of the frequency of early stops among\neach simulation with crashed simulations represented by blue\ndotted line.\nFrom the plot, we know that Green car never stops earlier\nuntil it reaches the intersection and Red car stops when it is\nmore than 1 grid away from the intersection. We can possibly\ninfer that the LLM of Red car tends to make cautious\ndecisions and behave courteously. Now, we can take a look\nat the regular stops meaning that they stop at (4,5) or (5,4).\nFigure 8 is the line plot of the frequency of regular stops\namong each simulation with crashed simulations represented\nby blue dotted line."}, {"title": "B. Scenario 2: Highway Platoon", "content": "We modeled the environment of intersection using a 2 by 9\ngrid. Figure 9 shows an example of a prompt and a response.\nWe adjust the prompt according to the number and placement\nof the background vehicles.\nIn this scenario, we did similar thing from previous\nscenario. We simulated the Markov Game 50 times without\nBVs, 10 times with 1 to 4 BVs respectively. The starting\npositions of 4 BVs are (1,2), (2,2), (1,4), (2,4). With our\nparameters set up, we find that in this scenario, the LLM\nperformed excellently, and there were no car crashes in the\nsimulations.\nIn this scenario, our focus is primarily on the frequency\nand timing of lane changes, which are crucial for platoon for-\nmation. Additionally, we perform an analysis of the scenario\nin the absence of interference from background vehicles.\nFigure 10 is the line plot of the frequency of lane changes\nin each simulation.\nFrom the plot, we can see that there is at most one lane\nchange per simulation, with the green car executing most\nof the lane changes. This is reasonable, as it suggests that\nagents are striving to minimize lane changes, which are a\nsignificant factor in car crashes on highways. Furthermore,\nthe LLM of the green car may exhibit a more aggressive or\nconsistent lane-changing behavior."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This paper has demonstrated the potential of large lan-\nguage models (LLMs) in understanding and modeling social\nnorms in autonomous driving scenarios. By introducing\nLLM-based agents into autonomous driving games, we have\nobserved the emergence of social norms among agents\nnavigating complex driving environments. Our experiments\nhave shown that LLM-based agents can adapt and conform\nto social norms, thereby contributing to safer and more\nefficient driving behaviors. Overall, the advantage of em-\nploying LLM-based agents in autonomous driving games lies\nin their strong operability and analyzability, which facilitate\nexperimental design and provide valuable insights into the\ndynamics of social norms in driving environments. By further\ninvestigating the behavior of LLM-based agents in various\ndriving scenarios and refining our experimental framework,\nwe can contribute to the development of more socially aware\nautonomous driving systems.\nThis work can be extended in the following ways: (1)\nWe will utilize LLM-based agents in more complex real-\nworld scenarios, such as sequential social dilemmas. (2) We\naim to explore how to construct a unified, controllable, and\nefficient framework for simulating strategic interactions and\nfacilitating game design using LLMs. (3) We will compare\nthe behaviors of human players and LLM-based agents to\ndetermine the extent to which LLMs can achieve strategic\nreasoning."}]}