{"title": "No-regret Exploration in Shuffle Private Reinforcement Learning", "authors": ["Shaojie Bai", "Mohammad Sadegh Talebi", "Chengcheng Zhao", "Peng Cheng", "Jiming Chen"], "abstract": "Differential privacy (DP) has recently been intro-duced into episodic reinforcement learning (RL) to formallyaddress user privacy concerns in personalized services. Previouswork mainly focuses on two trust models of DP: the centralmodel, where a central agent is responsible for protectingusers' sensitive data, and the (stronger) local model, where theprotection occurs directly on the user side. However, they eitherrequire a trusted central agent or incur a significantly higherprivacy cost, making it unsuitable for many scenarios. Thiswork introduces a trust model stronger than the central modelbut with a lower privacy cost than the local model, leveragingthe emerging shuffle model of privacy. We present the firstgeneric algorithm for episodic RL under the shuffle model,where a trusted shuffler randomly permutes a batch of users'data before sending it to the central agent. We then instantiatethe algorithm using our proposed shuffle Privatizer, relying ona shuffle private binary summation mechanism. Our analysisshows that the algorithm achieves a near-optimal regret boundcomparable to that of the centralized model and significantlyoutperforms the local model in terms of privacy cost.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning is a prominent sequential decision-making framework that has gained remarkable attraction inreal-world applications across several fields such as healthcare [1], online recommendation [2], and language models[3]. In these applications, the learning agent continuously improves its performance by learning from users' personal feedback, which usually contains sensitive information. Withoutprivacy protection mechanisms in place, the learning agentcan memorize information about users' interaction history[4], which makes the learning agent vulnerable to variousprivacy attacks [5].\nOver the past decade, differential privacy [6] has beenextensively applied in various decision-making settings in-cluding multi-armed bandits [7], linear control systems [8],and network systems [9]. In the case of RL, there is agrowing literature dealing with privacy issue of the rewardfunction [10], the environment transition [11], and policies[12]. Regarding privacy issues in interaction history, priorwork focused on episodic RL in the regret setting underJoint DP (JDP) [13], [14] and Local DP (LDP) constraints[15], [16]. In these settings, each episode is regarded as aninteraction with one user, and the aim is to ensure that theuser's states and generated rewards will not be inferred by anadversary during the learning process. Specifically speaking,JDP guarantees that all other users' decisions will not leak"}, {"title": "II. PRELIMINARY", "content": "An episodic Markov decision process (MDP) is definedby a tuple $(\\mathcal{X}, \\mathcal{A}, H, \\{P_h\\}_{h=1}^H, \\{R_h\\}_{h=1}^H, d_1)$, where $\\mathcal{X}, \\mathcal{A}$are state and action spaces with respective cardinalities $X$and $A$, and $H$ is the episode length. At step $h$, the transitionfunction $P_h(x, a)$ takes a state-action pair and returns adistribution over states, the reward distribution $R_h(x, a)$ is adistribution over $\\{0,1\\}$ with expectation $r_h(x,a)$, and $d_1$is the distribution of initial state.2 A deterministic policyis defined as a collection $\\pi = (\\pi_1,\\dots,\\pi_H)$ of policies$\\pi_h : \\mathcal{X} \\to \\mathcal{A}$. The value function $V^\\pi$ and $Q$ function $Q^\\pi$are defined as: $V^\\pi(x) = \\mathbb{E}_\\pi[\\sum_{h=1}^H r_h|x_h = x]$, $Q^\\pi(x, a) =\\mathbb{E}_\\pi[\\sum_{h=1}^H r_h|x_h, a_h = x,a]$, $\\forall x, a \\in \\mathcal{X} \\times \\mathcal{A}$. There existsan optimal deterministic policy $\\pi^*$ such that $V^{\\pi^*}(x) =\\max_\\pi V_\\pi(x) = \\max_\\pi V^\\pi(x)$ for all $x, h \\in \\mathcal{X} \\times [H]$. 3 The Bell-man (optimality) equation follows $\\forall h \\in [H] : Q^*(x,a) =r_h(x, a) + \\max_{a'} \\mathbb{E}_{x'\\sim P_h(x,a)}[V_{h+1}(x')]$. The optimal policyis the greedy policy: $\\pi^*(x) = \\operatorname{argmax}_a Q^*(x, a), \\forall x \\in \\mathcal{X}$.For generalization, we define the value function of underMDP transition $p$ and reward function $r'$ as $V^\\pi(r', p)$.\nWe assume the learning agent (e.g., a personalized service)interacts with an unknown MDP for $K$ episodes. Eachepisode $k \\in [K]$ is regarded as the interaction with a uniqueuser $u_k \\in \\mathcal{U}$, where $\\mathcal{U}$ is the user space. Following [13],a user $u_k$ can be seen as a tree of depth $H$ encoding thestate and reward responses they would reply to all $A^H$possible sequences of actions from the agent. For eachepisode $k \\in [K]$, the learner determines policy $\\pi_k$ and sendsto user $u_k$ for execution. The output of the execution, atrajectory $S_k = (x_h, a_h)_{h\\in [H]}$, is sent back to the learnerfor updating the policy. We measure the performance of alearning algorithm by its cumulative regret after $K$ episodes,\n$\\operatorname{Regret}(K) := \\sum_{k=1}^K \\big[V_1^{\\pi^*}(x_1^k) - V_1^{\\pi_k}(x_1^k)\\big],$ (1)\nwhere $x_1^k$ is the initial state sampled from $d_1$.\nConsider a general case where $\\mathcal{S}$ is the data universe, andwe have $n$ unique users. We say $D, D' \\in \\mathcal{S}^n$ are neighboringbatched datasets if they only differ in one user's data forsome $i \\in [n]$. Then, we have the standard definition ofdifferential privacy [6]:\nDefinition 1 (Differential Privacy (DP)): For $\\epsilon, \\beta > 0$, arandomized mechanism $\\mathcal{M}$ is $(\\epsilon, \\beta)$-DP if for all neighboringdatasets $D, D'$ and any event $E$ in the range of $\\mathcal{M}$, we have\n$\\mathbb{P}[\\mathcal{M}(D) \\in E] < \\exp(\\epsilon)\\cdot \\mathbb{P}[\\mathcal{M}(D') \\in E] + \\beta$.\nThe special case of $(\\epsilon, 0)$-DP is also called pure DP, whereas,for $\\beta > 0$, $(\\epsilon, \\beta)$-DP is referred to as approximate DP."}, {"title": "III. ALGORITHM", "content": "In this section, we introduce a generic algorithmic framework, Shuffle Differentially Private Policy Elimination (SDP-PE, Algorithm 1). Unlike existing private RL algorithms thatrely on the optimism in the face of uncertainty principle [13],[15], SDP-PE builds on the policy elimination idea from [26],which extends action elimination from bandits to RL.\nAt a high level, SDP-PE divides the exploration processinto stages, with exponentially increasing batch sizes. In eachstage, it maintains an active policy set $\\phi$, and refines valueestimates and confidence intervals for all active policies usingthe private model estimate from sufficient shuffle-privatestatistics. These value estimates are then used to eliminatepolicies that are likely sub-optimal. At the last stage, theremaining policies are guaranteed to be near-optimal. Animportant aspect of SDP-PE is the concept of forgetting,where only the data of the current stage is used to estimatethe model, preventing noise accumulation from earlier stages.To estimate the value for all active policies with uni-form convergence, we address this problem by obtainingan accurate estimate of the transition and reward functions.This requires sufficient exploration for each state-action pair,and injecting appropriate noise into the visitation countsand cumulative rewards of these pairs to preserve privacy.However, some states are rarely visited due to their verylow transition probabilities. To remedy this, we construct anabsorbing MDP that replaces these infrequent-visited stateswith an absorbing state $x^{\\dagger}$. For the remaining states, weensure they are visited sufficiently often by some policy in $\\phi$,allowing the estimated value to uniformly approximate thetrue value under the original MDP.\nInspired by the APEVE algorithm in [26], we design SDP-PE shown in Algorithm 1. It divides the $K$ episodes (users)into a sequence of stages. Each stage $b$ consists of threesteps, and the batch length for sub-steps grows exponentiallyas $L_b := 2^b$ for $b = 1, ..., B$, with $B = O(\\log K)$.\n1) Crude Exploration: For each pair $(x, a)$, explore themlayer by layer from scratch using the policy in the cur-rent policy set $\\phi$ that has the highest visitation prob-ability. We apply shuffle Privatizer to the explorationdata of each layer to obtain the private counts. Thesecounts help identify the infrequently-visited tuples $\\mathcal{W}$,and construct a private crude estimate $P_{\\operatorname{cru},b}$ of thecorresponding absorbing MDP $P'$.\n2) Fine Exploration: Using the crude transition estimatefrom the previous step, we explore each $h, x, a$ withthe policy that has the highest visitation probabilityunder the crude transition model. We then apply theshuffle Privatizer to the entire exploration data fromthis step and construct a refined private estimate $P_{\\operatorname{ref},b}$of $P'$ and reward estimate $r$.\n3) Policy Elimination: We evaluate all policies in $\\phi_b$ usingthe refined estimates $P_{\\operatorname{ref},b}$ and $r_b$. The active policyset is updated by eliminating all policies whose valueupper confidence bound (UCB) is less than the lowerconfidence bound (LCB) of any other active policy.\nAs the algorithm proceeds and more data is collected infurther stages, our confidence intervals shrink, along withthe policy elimination step, ensuring that the optimal policystays in the active policy set with high probability, whichguarantees that the algorithm eventually converges to theoptimal policy."}, {"title": "A. Counts in Algorithm 1", "content": "The algorithm introduced in the previous section employsa model-based approach for solving the private RL problemand uses the forgetting idea to estimate the MDP model. Inthis approach, only the data from the current step is used toconstruct the batch dataset. However, the dataset constructiondiffers between the Crude Exploration and Fine Explorationsteps, as will be detailed later. Once a batch dataset is formed,the counts for model estimation are established as follows.\nConsider the general case where a batch dataset $\\mathcal{D}$ from $n$users is sent to the Privatizer. We aim to collect the visitationcounts and cumulative rewards for each pair $(x, a, x')$ at $h$-th step. $N_h(x,a, x') := \\sum_{i=1}^n \\mathbb{1}\\{x_h^i, a_h^i, x_{h+1}^i = x,a,x'\\}$,similarly $N_h(x, a):=\\sum_{x'\\in \\mathcal{X}}N_h(x, a, x')$, and $R_h(x,a) :={$sum}_{i=1}^n \\mathbb{1}\\{x_h^i, a_h^i = x,a\\} \\cdot r_i$. The shuffle Privatizer thenreleases privatized versions of these counts, denoted as$\\widetilde{N}_h(x, a, x'), \\widetilde{N}_h(x, a)$, and $\\widetilde{R}_h(x, a)$. Assumption 4 belowguarantees that the private counts closely approximate thetrue counts, as justified in Section V.\nAssumption 4 (Private counts): For any privacy budget$\\epsilon > 0$ and failure probability $\\delta \\in (0,1)$, the privatecounts returned by Privatizer satisfy, for some $E_{\\epsilon,\\delta} >0$, with probability at least $1 - 3\\delta$, over all $(h,x, a, x')$,$|\\widetilde{N}_h(x,a) - N_h(x,a)| \\leq E_{\\epsilon,\\delta}, |\\widetilde{N}_h(x, a, x') - N_h(x, a, x')|\\leqE_{\\epsilon,\\delta}, |\\widetilde{R}_h(x, a) - R_h(x, a)| \\leq E_{\\epsilon,\\delta}$ and $\\widetilde{N}_h(x,a) = \\sum_{a'\\in \\mathcal{X}}\\widetilde{N}_h(x, a, x') \\geq N_h(x,a), \\widetilde{N}_h(x, a, x') > 0$.\nBased on Assumption 4, we define the private estimationof $P$ and $r$ built using counts from current batch:\n$\\widehat{P}_h (x'|x, a) := \\frac{\\widetilde{N}_h(x, a, x')}{\\widetilde{N}_h(x, a)}, \\widehat{r}_h (x, a) := \\frac{\\widetilde{R}_h(x, a)}{\\widetilde{N}_h(x, a)}$\nBy construction, $\\widehat{P}_h (\\cdot|x, a)$ is a valid probability distribution."}, {"title": "B. Crude Exploration in Algorithm 2", "content": "To learn an accurate private estimate of $P_h (x'|a, x)$ for anytuple $(h, x, a, x')$, it is necessary to collect private counts thatoccur at least $O(E_{\\epsilon,\\delta}H^2\\iota)$ times for each tuple. Thus, we tryto visit each tuple as frequently as possible by using policiesfrom active policy set $\\phi$. We define the set of infrequentlyvisited tuples $(h,x,a,x')$ as $\\mathcal{W}$, which consists of all thetuples $(h, x, a, x')$ that are visited less than $O(E_{\\epsilon,\\delta}H^2\\iota)$times in current exploration. For the tuples not in $\\mathcal{W}$, wecan get accurate estimates, and for tuples in $\\mathcal{W}$, they havelittle influence on the value estimate.\nIn crude exploration step, we perform layer-wise explo-ration. During the exploration of the $h$-th layer, we construct$\\pi_{h,x,a}$ that has the highest visit probability for $(h, x, a)$ underthe private crude estimate $P_{\\operatorname{cru},b}$, and then run each $\\pi_{h,x,a}$ forthe same number of episodes. At the same time, we collectthe interaction history for the $h$-th layer as a batch dataset$D_h$, and send it to the Privatizer to generate the private countsof this batch. The private counts are then used to updatethe infrequent tuple set $\\mathcal{W}$ and crude estimate of the $h$-th layer.\nSimilar to [26], we construct an absorbing MDP transitionfunction $P'$ by letting $P'_{h}(x'|x, a) = P_{h}(x'|x, a)$ first and then move theprobability $P_h(x'|x, a)$ to $P_h(x^{\\dagger}|x, a)$ for all $(h,x, a, x') \\in\\mathcal{W}$ to help an accurate estimate for $P$.\nDefinition 5 (Abosorbing MDP $P'$): Given $\\mathcal{W}$ and $P$,$\\forall (h,x,a,x') \\notin \\mathcal{W}$, let $P'_{h}(x'|x, a) = P_h(x'|x, a)$,$\\forall (h,x,a,x') \\in \\mathcal{W}, P'_{h}(x'|x,a) = 0$. For any $(h,x,a) \\in[H] \\times \\mathcal{X} \\times \\mathcal{A}$, define $P'_{h}(x^{\\dagger}|x^{\\dagger}, a) = 1$ and $P'_{h}(x^{\\dagger}|x, a) =1 - \\sum_{x'\\in \\mathcal{X}:(h,x,a,x')\\notin \\mathcal{W}} P_h(x'|x, a)$."}, {"title": "C. Fine Exploration in Algorithm 3", "content": "The idea of fine exploration is to use $P_{\\operatorname{cru},b}$ to con-struct policies that ensure efficient visitation of eachtuple. Specially, this is done with the guarantee that$\\sup_{\\pi \\in \\phi_b} \\frac{V^{\\pi}(1_{h,x,a}, P')}{\\mu_h (x,a)}< \\frac{1}{12HXA}$, where $\\mu$ is the truedistribution of our data. In this way, we can get a refinedprivate estimate $P_{\\operatorname{ref},b}$ for $P'$, which allows $V^{\\pi}(r', P_{\\operatorname{ref},b})$ tobe an accurate estimate of $V^{\\pi}(r', P')$ simultaneously acrossall policies $\\pi\\in\\phi_b$ and any reward function $r'$.\nDuring the fine exploration, for each $(h,x, a)$, we findpolicy $\\pi_{h,x,a}$ that visits $(h, x, a)$ with highest probability.After running $\\pi_{h,x,a}$ of all $(h, x, a)$ for the same number ofepisodes, we compile the entire history into a batch dataset$\\mathcal{D}$ and pass it to the Privatizer. This process along withthe same clipping process in crude exploration yields a refinedprivate estimate $P_{\\operatorname{ref},b}$ for $P'$ and reward function $\\widetilde{r}$."}, {"title": "IV. REGRET GUARANTEE", "content": "The following theorem shows a regret bound of SDP-PE.Theorem 6 (Regret bound of SDP-PE): For any privacybudget $\\epsilon > 0$ and failure probability $\\delta\\in (0,1)$, and anyPrivatizer that satisfies Assumptions 4, with probability atleast $1 - 9\\delta$, the regret of SDP-PE (Algorithm 1) is\n$\\operatorname{Regret}(K) \\leq \\widetilde{O} \\Big(\\sqrt{H^5X^2AK} + \\frac{X^3A^2H^6}{\\epsilon}E_{\\epsilon,\\delta}\\Big).$\nThe proof parallels the arguments in [26] for the analysisof APEVE. The key difference lies in adjusting the uniformvalue confidence bound for all active deterministic policiesto account for the noise introduced by the private counts.\nFor each stage $b$, assume that for any $\\pi\\in\\phi_b$, thevalue function $V^\\pi(r, P)$ can be estimated up to an error $\\xi$with high probability. With this estimation, policies that areat least $2\\xi$ sub-optimal can be eliminated based on theirestimated value. Therefore, the optimal policy will neverbe eliminated, as its value is always within the confidenceinterval. All remaining policies will be at most $4\\xi$ sub-optimal. By summing the regret across all stages, we havewith high probability that\n$\\operatorname{Regret}(K) \\leq 2HL_1 + \\sum_{b=2}^B 2L_b \\cdot 4\\xi_{b-1}.$\nThe following lemma gives a bound of $\\xi$ using our privateestimate $P_{\\operatorname{ref},b}$ and $r_b$ of the absorbing MDP.\nLemma 7: With probability $1 - \\frac{7}{9}\\delta$, it holds that for anystage $b$ and $\\pi\\in\\phi_b$,\n$|V^{\\pi^*}(r,P)-V^{\\pi^*}(r_b, P_{\\operatorname{ref},b})|\\leq O\\Big(\\frac{\\sqrt{H^5}}{L_b} \\frac{X^2AH^5}{\\epsilon} + \\frac{X^3A^2H^5}{L_b}E_{\\epsilon,\\delta}\\Big).$\nAs shown in [27], if each $(h, x, a)$ tuple is visited frequentlyenough, the empirical transition is sufficient for a uniformapproximation to $V^\\pi(r, P)$. In our setting, we leverage theabsorbing MDP $P'$ as the key intermediate step to ensure thisguarantee. Thus, we can decompose this confidence boundinto three components, \"Model Bias\" $|V^{\\pi^*}(r, P)-V^{\\pi^*}(r, P')|$,\"Reward Error\" $|V^{\\pi^*}(r, P') - V^{\\pi^*}(r_b, P')|$ and \"Model Variance\" $|V^{\\pi^*}(r_b, P') - V^{\\pi^*}(r_b, P_{\\operatorname{ref},b})|$. In this sketch, we focuson the \"Model Bias\" and \"Model Variance\" terms, leaving adetailed analysis of the lower-order term \"Reward Error\"and the complete proof for the full paper.\nTo analyze the difference between the true MDP with$P$ and the absorbing MDP with $P'$, we first clarify theproperties of the crude transition estimate $P_{\\operatorname{cru},b}$.\nIf $N_h(x,a,x')$ exceeds$\\mathcal{O}(E_{\\epsilon,\\delta}H^2\\iota)$, the following holds with high probability, $(1 -\\frac{\\delta}{4H}). \\frac{P_{\\operatorname{cru},b}(x'|x, a)P_h(x'|x, a)P_{\\operatorname{cru},b}(x'|x, a)P_h(x'|x, a)V^{\\pi^*}(1_{h,x,a}, P') \\leq \\frac{1}{12}\\frac{E_{\\epsilon,\\delta} \\sqrt{X^2AH^5}}{L_b}.$With the desirable"}, {"title": "V. PRIVACY GUARANTEE", "content": "In this section, we introduce a shuffle Privatizer based onthe shuffle binary summation mechanism introduced by [18],and show that Algorithm 1 satisfies $(\\epsilon, \\beta)$-SDP.\nTo protect the information of the batch users ($\\mathcal{D}_h$ in CrudeExploration and $\\mathcal{D}$ in Fine Exploration), we privatize thevisitation counts and rewards using the shuffling binary sum-mation mechanism. Following Section III-A, with $n$ usersin the current batch, we outline the process for generatingthe privatized visitation counts $\\widetilde{N}_h(x, a, x')$ for a specific"}, {"title": "B. Discussion", "content": "SDP-PE achieves an improvedregret-privacy trade-off under shuffle DP, with a regret boundthat is optimal regarding the episode number $K$ and privacybudget $\\epsilon$. It outperforms the optimal LDP result [17] by mak-ing the dependency on $1/\\epsilon$ additive rather than multiplicative,and matches the dependence on $K$ and $\\epsilon$ in the JDP case [17].This also addresses the concerns outlined in [15], where aburn-in phase was required for their algorithm.\nDependence on $H, X, A$ in the regret. Although ourregret bound is optimal with respect to $K$, a gap remainsin dependence on $X$ and $H$ compared to the lower bound$\\Omega(\\sqrt{H^3XAK})$ in the non-private setting. It is still openwhether this can be improved under SDP constraints.\nComputational efficiency. Our algorithm's efficiency islimited by the need to evaluate all policies in the active set,which may be exponential in size. Computational efficiencycould potentially be improved by using an external optimizer,as in [25], to optimize over the full policy space withoutexplicitly maintaining the active set."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we presented the first generic algorithmfor shuffle private RL, Shuffle Differentially Private PolicyElimination, which achieves a refined privacy-regret trade-off under shuffle differential privacy constraints. Unlike priorapproaches, our approach relies on novel strategies, i.e.,batch data collection and private updating, and the policyelimination principle. By instantiating the proposed Priva-tizer, SDP-PE attains optimal regret over episode number$K$ and privacy budget $\\epsilon$. Notably, the regret bound underSDP matches that under JDP while providing a strongerprivacy guarantee, and improves the results under LDP whichexpands possibilities for private RL.\nDespite these, our algorithm is sub-optimal in its depen-dence on MDP parameters, $X, A, H$, which requires somerefined ideas for dealing with the trade-off between batch-based exploration and exploitation under privacy. Meanwhile,improving the computation efficiency is also one interestingproblem. Our generic algorithm can also be extended to otheradvanced privacy notions, e.g., Renyi DP [29]."}, {"title": "VII. PROOFS FOR SECTION IV", "content": "The proof is similar to the arguments for the non-private algorithm APEVE in [26]. The core of the regret analysis is toconstruct a uniform policy evaluation bound that covers all active policies. The active policy set at the beginning of stageb is $\\phi_b$. Assume we can estimate $V^\\pi(r, P)(\\pi\\in \\phi_b)$ with an error up to $\\xi$ with high probability, then we can eliminate allpolicies that are at least $2\\xi$ suboptimal in the sense of estimated value function. Therefore, the optimal policy will not beeliminated and all the policies remaining will be at most $4\\xi$ sub-optimal with high probability. Summing up the regret ofall stages, we have with high probability,\n$\\operatorname{Regret}(K) \\leq 2HL_1 + \\sum_{b=2}^B 2L_b \\cdot 4\\xi_{b-1}.$\nThe following lemma gives a bound of $\\xi$ using the model-based plug-in estimator with our private transition estimate$P_{\\operatorname{ref},b}$ and reward estimate $r_b$ of the absorbing MDP $P'$.\nLemma 12 (Restatement of Lemma 7): With probability $1 - \\frac{7}{9}\\delta$, it holds that for any $b$ and $\\pi\\in\\phi_b$,\n$|V^{\\pi^*}(r, P) - V^{\\pi^*}(r_b, P_{\\operatorname{ref},b})| \\leq O\\Big(\\sqrt{\\frac{XAHXAHE}{L_b}}\\Big).$\nWe decompose this confidence interval by the following three terms.\n$|V^{\\pi^*}(r, P) - V^{\\pi^*}(r_{\\operatorname{ref},b}, P_{\\operatorname{ref},b})| < \\underbrace{[V^{\\pi^*}(r, P) - V^{\\pi^*}(r, P')]}_{Model Bias} + \\underbrace{[V^{\\pi^*}(r, P') - V^{\\pi^*}(r_{\\operatorname{ref},b}, P')]}_{Reward Error} + \\underbrace{V^{\\pi^*}(r_{\\operatorname{ref},b}, P') - V^{\\pi^*}(r_{\\operatorname{ref},b}, P_{\\operatorname{ref},b})}_{Model Variance}.$\nWe then provide the proof of bounding all these terms.\nWe construct the absorbing MDP $P'$ in the Crude Exploration step by using the private visitation counts. Let us denote theprivate visitation counts in the Crude Exploration step of stage $b$ as $N_{\\operatorname{cru},b}(x, a)$ and $N_{\\operatorname{cru},b}(x, a, x')$. As shown in Algorithm2, we construct an absorbing MDP $P'$ for $(h, x, a, x')$ tuples that their $N_{\\operatorname{cru},b}(x, a, x')$ is larger than $O(E_{\\epsilon,\\delta}H^2\\iota)$, and designan absorbing state for the tuples that are not visited often. Obviously, the probability of visiting a tuple $(h, x, a) \\in [H]\\times\\mathcal{X}\\times\\mathcal{A}$under $P$ is always greater than that under $P'$.\nLemma 13: For any policy $\\pi$, and any $(h, x, a) \\in [H] \\times \\mathcal{X} \\times \\mathcal{A}$, we have,\n$V^{\\pi^*}(1_{h,x,a}, P) \\geq V^{\\pi^*}(1_{h,x,a}, P').$\nConcerning the upper bound, we rely on the private estimate $P_{\\operatorname{cru},b}$ as an intermediate variable, which is accurate withhigh probability using Bernstein inequality by Lemma 38.\nLemma 14: With probability at least $1 - \\frac{6}{9}\\delta$, for all stage $b \\in [B]$ and $(h, x, a, x') \\in \\times [H] \\times \\mathcal{X} \\times \\mathcal{A} \\times \\mathcal{X}$ such that forthese tuples $(h, x, a, x') \\notin \\mathcal{W}$,\n$|P_h (x'|x, a) - P_{\\operatorname{cru},b} (x'|x, a)| \\leq \\beta_h^{\\operatorname{cru},b} (x'|x, a),$\nand $\\beta_h^{\\operatorname{cru},b} (x'|x, a)$ is defined as\n$\\beta_h^{\\operatorname{cru},b} (x'|x, a) = \\min \\bigg\\{1, \\sqrt{\\frac{2P_h (x'|x, a)}{N_{\\operatorname{cru},b}(x, a)} \\iota} + \\frac{14 \\iota}{3N_{\\operatorname{cru},b}(x, a)} \\bigg\\}.$\nIn addition, we have that for all $(h, x, a, x') \\in \\mathcal{W}, \n$P_h (x'|x, a) = P_{\\operatorname{cru},b} (x'|x, a) = 0.$\nWith Lemma 14, our crude estimate $P_{\\operatorname{cru},b}$ is also multiplicatively accurate as follows.\nLemma 15: Conditioned on the event in Lemma 14, for all stage $b \\in [B]$ and $(h, x, a, x') \\in [H] \\times \\mathcal{X} \\times \\mathcal{A} \\times \\mathcal{X}$ such that$(h, x, a, x') \\notin \\mathcal{W}$, it holds that\n$\\big(1 - \\frac{\\delta}{4H} \\big) \\cdot P_{\\operatorname{cru},b}(x'|x, a) \\leq P_h(x'|x, a) \\leq \\big(1 + \\frac{\\delta}{4H} \\big) \\cdot P_{\\operatorname{cru},b}(x'|x, a).$"}, {"title": "VIII. SUPPLEMENTARY LEMMAS", "content": "Consider $x$ and $y$ satisfying $|x - y| \\leq \\alpha \\sqrt{y(1 - y)} + \\beta$. Then$\\sqrt{y(1 - y)} \\leq \\sqrt{x(1 - x)} + 1.9\\alpha + 1.5\\beta.$"}, {"title": "IX. PROOF OF PRIVACY GUARANTEES", "content": "In this section, we present the proof of privacy guarantees in Section V. Recall the privacy mechanism in Section V, givena batch dataset $\\mathcal{D}$ from $n$ users, for visitation counters, $N_h(x, a)$ is the original count, $\\widetilde{N}_h(x, a)$ is the noisyc"}]}