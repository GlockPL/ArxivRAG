{"title": "WHAT MATTERS FOR IN-CONTEXT LEARNING: A BALANCING ACT OF LOOK-UP AND IN-WEIGHT LEARNING", "authors": ["Jelena Bratuli\u0107", "Sudhanshu Mittal", "Christian Rupprecht", "Thomas Brox"], "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in various tasks, including In-Context Learning (ICL), where the model performs new tasks by conditioning solely on the examples provided in the context, without updating the model's weights. While prior research has explored the roles of pretraining data and model architecture, the key mechanism behind ICL remains unclear. In this work, we systematically uncover properties present in LLMs that support the emergence of ICL. To disambiguate these factors, we conduct a study with a controlled dataset and data sequences using a deep autoregressive model. We show that conceptual repetitions in the data sequences are crucial for ICL, more so than previously indicated training data properties like burstiness or long-tail distribution. Conceptual repetitions could refer to n-gram repetitions in textual data or exact image copies in image sequence data. Such repetitions also offer other previously overlooked benefits such as reduced transiency in ICL performance. Furthermore, we show that the emergence of ICL depends on balancing the in-weight learning objective with the in-context solving ability during training.", "sections": [{"title": "1 INTRODUCTION", "content": "In-context learning (ICL) is a remarkable feature of Large Language Models (LLMs) (Radford et al., 2019; Brown et al., 2020) since it enables the model to adapt and solve tasks never seen during training, conditioned solely on the context provided during inference (Brown et al., 2020) without demanding any retraining or task-specific fine-tuning. ICL contrasts with standard in-weight learning (IWL), where the knowledge needed for inference tasks is embedded within the model weights during training. Models showing ICL are trained autoregressively to predict the next token as the in-weight learning objective. However, since no explicit training objective is tailored for ICL, it is challenging to identify the underlying factors for its emergence.\nPrevious research (Han et al., 2023; Chan et al., 2022) has attributed the emergence of ICL to pre-training data properties such as long-tail token distribution and high burstiness in data sequences. Here, burstiness refers to clustered occurrences of data points within a sequence. Following up on the idea of reoccurring concepts in data sequences, Chen et al. (2024) analyzed the impact of parallel structures in the pretraining data of LLMs, showing that the pairs of phrases following similar templates in the pretraining corpora lead to ICL. This paper shows that while these properties improve ICL performance, they are not the predominant factors for ICL emergence.\nWe distinguish key driving factors of ICL operating at the data sequence level and the in-weight learning objective level. At the data sequence level, we demonstrate that it is important for the training data to provide opportunities to solve training tasks in context, which we refer to as an in-context look-up mechanism. Specifically, in this work, we show that the look-up mechanism can be strongly driven by conceptual repetitions in data sequences, which are also commonly present in the training sequences of LLMs. In Figure 1, we illustrate that the pretraining text corpora used to train LLMs naturally contain a high frequency of repetitive n-grams in the context, indicating high concept-level repetitions. It shows that an input sequence of the context window of 2048 tokens contains more than ten 10-gram repetitions across different corpora on average. In this work, we conduct a controlled study to showcase the contribution of similar repetitions for ICL. At the in-weight learning objective level, we hypothesize that the high complexity of the in-weight learning objective is crucial for consistent ICL performance throughout the training process. In contrast, a simple IWL objective can result in subdued or transient ICL performance. This aspect is often overlooked because the training objective in LLMs is naturally quite complex, which demonstrate strong and stable ICL performance. To support this argument, we study the impact of different IWL objectives on ICL performance."}, {"title": "2 RELATED WORK", "content": "ICL research directions. Plenty of research has been dedicated to understanding and optimizing the model response to obtain the best ICL performance. Prior works have analyzed the important concepts that enable ICL in LLMs such as pretraining data (Liu et al. (2022); Rubin et al. (2022); Levine et al. (2022)), in-context prompt design (Voronov et al., 2024), impact of the in-context samples and length (Agarwal et al., 2024), and more. Studies (Liu et al., 2022; Rubin et al., 2022) have also focused on designing demonstrations, also known as prompt engineering, to get better responses from pretrained language models. Other works (Aky\u00fcrek et al., 2023; Li et al., 2023; Wu et al., 2024) tested the ability of ICL to solve novel tasks and its robustness (Ravent\u00f3s et al., 2024; Min et al., 2022) by evaluating on out-of-distribution tasks. Furthermore, (Xing et al., 2024; Chan et al., 2022) aimed to understand the underlying components driving ICL, such as model architecture, training data distribution, and optimization objectives.\nSimultaneously, ICL has been studied in the context of specific tasks. Many works have attempted to explain the emergence of ICL by considering it as a regression task. (Aky\u00fcrek et al., 2023; Dai et al., 2023; Von Oswald et al., 2023) showed that self-attention architectures with linear attention implement a gradient descent with in-context examples, while Dai et al. (2023) theoretically identified that the transformer attention mechanism has a dual form of SGD, where in-context learners implicitly perform fine-tuning. Other works showed that transformers can implement simple functions in-context like least squares, ridge regression, and gradient descent in two-layer neural networks (Bai et al., 2023; Garg et al., 2022). Alternatively, ICL for classification tasks relies on an in-context solving ability to match and retrieve the label from the context. This demonstrates a clear difference in the underlying mechanism between these two tasks. Our work focuses on understanding the relationship between different components for the classification task setup. Given the different underlying ICL mechanisms, our findings may not directly transfer to regression tasks.\nMechanistic perspective on ICL. Prior work has studied ICL from different angles through simplified experiments and model probing. Many works have analyzed the transition phases during the training process, discovering the sequence of operations and circuitry leading to ICL. Olsson et al. (2022) provided the initial evidence that induction heads may be pivotal for in-context learning in transformer-based models. Singh et al. (2024) conducted another mechanistic study using a causal approach to understand the abrupt emergence of induction heads and identified three interacting sub-circuits leading to their formation. Reddy (2024) demonstrated with a simple two-parameter model that ICL is driven by the formation of an induction head, which emerges due to nested non-linearities in a multi-layer attention network. Building on previous work in mechanistic interpretability, we analyze the emergence of induction heads alongside ICL in our setup using a small GPT-2 model with three layers and one head. Singh et al. (2023) found that ICL becomes transient as training progresses and discussed different ways to reduce transiency. In this work, we also observe similar transiency and analyze how induction heads appear and disappear as ICL becomes transient.\nTraining data properties for ICL. Numerous works indicate that the training data distribution both in the corpus and sequences plays a role in the emergence of ICL. Shin et al. (2022) investigated the ICL behaviour w.r.t. different pretraining data source and size, confirming that the source data properties can make or break ICL. Han et al. (2023) conducted an empirical study to show that challenging examples and long-tail tokens promote ICL by making the long-range information gain difficult. On the same lines, Chan et al. (2022) demonstrated that a large number of rarely occurring classes facilitate ICL. These works align with our generalized argument that a complex enough IWL task promotes ICL. Similarly, Razeghi et al. (2022) found a correlation between the term frequency of the input data in the pretraining corpus and ICL performance, further underscoring the influence of data characteristics. For local data patterns, studies by Olsson et al. (2022), and Chan et al. (2022) demonstrated that burstiness and repetitive structures within training sequences are vital for ICL. In a similar direction, Chen et al. (2024) have argued that parallel structures in the pretraining textual data, which follow a similar semantic or syntactic template, facilitate ICL in language models. They include repetitions in the parallel structures however their independent significance is not well disambiguated. Shi et al. (2024) studied the impact of combining the documents within the context window for language modeling and different textual in-context tasks. They showed a clear improvement in the ICL performance once the model was trained with a sequence of related documents showing the importance of data patterns in the pretraining. Our work studies local data patterns present in the training sequences, showing that simple copy-based data repetition, which can be seen as a special case of parallel structures is a major factor for stable ICL performance."}, {"title": "Meta-learning vs ICL", "content": "Meta-learning and In-context learning aim to enhance rapid adaptation to new tasks but operate through distinct mechanisms. Meta-learning often utilizes episodic pretraining where the model is pretrained with randomly sampled classes and label mappings in each episode, which enables fast adaptation to new unseen tasks during the testing phase with only a few parameters updates (Finn et al., 2017; Santoro et al., 2016; Snell et al., 2017). In this way, the model learns how to learn the task but does not learn the input label mapping directly due to the episodic permutations, which present an almost impossible learning task. On the other hand, there is no explicit pretraining objective for in-context learning. The input label mapping is crucial for the in-weight learning task and must not be harmed by the ICL abilities. Thus, while meta-learning explicitly optimizes for adaptability and robustness during training, ICL leverages pretraining knowledge like the original input-label mapping for adaption through information provided by input context. In this paper, we focus solely on concepts relevant to in-context learning but draw connections to the meta-learning pretraining via episodic pretraining and task difficulty when analyzing the choice of the in-weight learning task."}, {"title": "3 EXPERIMENTAL SETUP", "content": "We conducted an analysis to show how different properties impact ICL. We train an autoregressive model GPT-2 (Radford et al., 2019) on different sequences containing image-label pairs from image classification datasets widely popular in the FSL literature (Lake et al., 2015; Bertinetto et al., 2019; Fei-Fei et al., 2004; Cimpoi et al., 2014)."}, {"title": "3.1 DATA SEQUENCING", "content": "The autoregressive model in this work is trained with a sequence length of 2\ud835\udc3f + 1 with \ud835\udc3f image-label pairs in the context followed by a query image, as shown in Figure 2. The in-weight learning objective is to predict the label of the last image, which is the (2\ud835\udc3f + 1)-th token. Training sequences consist of a mixture of (1) standard sequences, in which sample-label pairs are randomly selected from the training dataset, and (2) in-context sequences, where the query image-label information is enforced to be present in the sequence by using a pair similar to the query image-label pair. The proportion of each sequence type in the total amount of training sequences is treated as a hyper-parameter. Using in-context sequences, the model is implicitly regularized to attend to the similar context tokens without completely relying on the model weights."}, {"title": "Sequence notations", "content": "Throughout the paper, we represent sequences using strings of \ud835\udc3f characters, where each character corresponds to an image-label pair from a specific class. This is followed by the query image token of class (Q). As an example, we denote sequences of length \ud835\udc3f = 8 as follows: we compare models trained with in-context sequences of different formats from (3xQ-3xA-B-C) to (Q-A-B-C-D-E-F-G), where (3xQ-3xA-B-C) means three image-label pairs are from the query(Q) class, three other image-label pairs from a non-query(A) class and other two image-label pairs are from two other non-query(B, C) classes. (Q-A-B-C-D-E-F-G) means only 1 sample is from the query class and the other 7 samples are from different non-query classes. The sequence order is shuffled during training. Standard sequences have the (A-B-C-D-E-F-G-H) format, where all image-label instances are from different non-query classes."}, {"title": "3.2 TRAINING AND EVALUATION DETAILS", "content": "We train a causal GPT-2 (Radford et al., 2019) transformer to predict the label \ud835\udc66\ud835\udc5e of the query image \ud835\udc65\ud835\udc5e given a sequence of \ud835\udc3f interleaved image-label pairs: (\ud835\udc651, \ud835\udc661, \ud835\udc652, \ud835\udc662, \u2026, \ud835\udc65\ud835\udc3f, \ud835\udc66\ud835\udc3f, \ud835\udc65\ud835\udc5e,?) as illustrated in Figure 2. Each image-label pair is converted into token embeddings separately. The model is trained to maximize the likelihood of the next token, with the loss applied to the final query output, thus using last-token prediction as the IWL training objective.\nDataset construction. We conduct our controlled experiments and analysis on the Omniglot dataset (Lake et al., 2015) and scale to more realistic visual datasets. Omniglot contains 1623 classes with 20 images each. Following previous work (Chan et al., 2022), we use 1600 classes for training and the remaining 23 as novel classes for ICL evaluation. More experiments (in Section 4) are performed on datasets including CIFAR-100 (Bertinetto et al., 2019), Caltech-101 (Fei-Fei et al., 2004), and DTD texture datasets (Cimpoi et al., 2014). ICL evaluation is performed using 20, 10, and 10 novel classes for CIFAR-100, Caltech-101, and DTD datasets, respectively. More experiment details are included in the Appendix B.\nData sequencing details. In all supervised image classification experiments, the models are trained with a mix of in-context and standard sequences. Models with instance discrimination (self-supervised learning) tasks are trained with 100% in-context sequences. ICL is evaluated in a few-shot classification setting for 2-way-4-shot and 4-way-2-shot tasks. We show results in the main paper for the more challenging 4-way-2-shot setting, while other results are included in the Appendix B.1 and Appendix C.3. This evaluation is performed on held-out novel classes. The trained classifier output is used for the few-shot evaluation using label mapping from 0-1 or 0-3 corresponding to both evaluation tasks. IWL is evaluated for the multi-class classification task on the held-out samples from the trained classes. The standard sequences with (A-B-C-D-E-F-G-H) format are used for IWL evaluation (see Figure 2). The same pre-set 10K and 3.2K sequences are used for ICL and IWL evaluation, respectively."}, {"title": "3.3 BASELINE", "content": "Our proposed baseline model is trained on the Omniglot dataset with a mix of standard and in-context sequences with 10% and 90% probability respectively. The standard sequences follow (A-B-C-D-E-F-G-H) format, and in-context sequences have a high burstiness with (3xQ-3xA-B-C) format. The baseline model with in-context sequence format (3xQ-3xA-B-C), achieves strong ICL and IWL performance (shown in Figure 3). Prior work observed poor IWL performance with strong ICL performance. We think this happens because they utilize standard sequences in (Q-A-B-C-D-E-F-G) format with at least one repetition of the query in context, which undermines the in-weight learning task. However, similar to prior work, we observe that the ICL performance is transient as training proceeds for this baseline model."}, {"title": "4 WHAT PROMOTES IN-CONTEXT SOLVING ABILITY?", "content": "The term \"in-context solving ability\u201d or \u201clook-up mechanism\" refers to the ability of the transformer-based model, where the model can prioritize the usage of information present within its current input context to generate responses. The model is not explicitly trained to learn this mechanism, however, it is crucial for the model to solve the task based on the in-context information. This work examines a previously proposed in-context sequence strategy, which are motivated by the trends observed in the pretraining corpora of LLMs and further analyzes the impact of repetitions in training data sequences."}, {"title": "Burstiness", "content": "Burstiness is an inherent feature of natural sequential data. Chan et al. (2022) showed that the burstiness property in training sequences is crucial for ICL and demonstrates its effectiveness. Burstiness, in a classification setup like ours, is defined as the number of occurrence of samples from the same class as the query sample in the context. Models with this strategy are trained with a mix of highly bursty (3xQ-3xA-B-C) and non-bursty (A-B-C-D-E-F-G-H) sequences to obtain both ICL and IWL. High-bursty and non-bursty sequences are types of in-context and standard sequences, respectively."}, {"title": "Repetitions", "content": "Although burstiness is typically a natural feature of sequential data, repetitions are also a specific aspect of burstiness. As motivated in the introduction, the analysis of pretraining corpora reveals frequent repetitions of n-grams in different pretraining corpora, indicating that pre-training text exhibits burstiness not only via clustered phrases of synonyms or similar topics but also through exact repetitions of different lengths. Therefore, we conducted a study to examine the impact of repetitions on ICL. We find that simply copying the query-label pair into the input sequence during training develops a strong look-up mechanism."}, {"title": "iCopy", "content": "In light of previously introduced ways of fostering stronger in-context learning, we refer to the sequences with conceptual repetitions for image classification tasks as iCopy. iCopy utilizes an instance-based copying mechanism where the query-label pair is copy-pasted within the sequence of defined burstiness. This simplifies the matching process between a query token and its duplicates across different token positions in the context. Since the positions of the copied token are shuffled and sequences with repetitions are mixed with standard sequences, the model learns a generalized look-up mechanism across the whole context. The sequences with repetitions can also be considered as a special case of an in-context sequence with low-burstiness, denoted as (Q-A-B-C-D-E-F-G iCopy) in this work. This copied version in the context can be an exact copy or an augmented version of the original query sample-label pair.\nIn the previously proposed high-burstiness strategy, samples from the same class as the query appear multiple times, whereas the sequences with repetitions may only contain one occurrence of the same instance as the query sample. However, repetitions show enhanced results when combined with the high-burstiness strategy (3xQ-3xA-B-C iCopy), comprising multiple instance copies in the context.\nWe conducted ablations and analysis to study how repetitions perform against previously introduced data distributional properties on ICL and IWL tasks."}, {"title": "4.1 ICOPY PROMOTES LOOK-UP", "content": "Repetitions are sufficient for ICL. We compare in-context sequences with high-burstiness (3xQ-3xA-B-C) and sequences with one repetition (Q-A-B-C-D-E-F-G iCopy), observing similar ICL peak performance for both types of in-context sequences but with reduced transiency for the sequences with repetition. Combining the repetitions and high-burstiness strategies (3xQ-3xA-B-C iCopy) further reduces ICL transiency. Using low-burstiness without iCopy (Q-A-B-C-D-E-F-G) shows no ICL ability. ICL and IWL performance curves are shown in Figure 3 a,b, where we can also observe that the repetitions in iCopy sequences do not harm the IWL performance.\nSkewness is not necessary. We compare models trained with skewed and uniform label distribution, using 7200 samples randomly selected from 992 classes. The sequences with repetitions and uniform distribution outperform the high-burstiness strategy with Zipfian distribution (Figure 3c). Thus showing that a skewed distribution improves ICL over the baseline, it is not necessary for ICL."}, {"title": "4.2 ICOPY REDUCES ICL TRANSIENCY", "content": "We observe that the ICL performance is transient across all look-up strategies studied in this work (see Figure 3), a behavior also observed by prior work (Singh et al., 2023). We observe using iCopy results in reduced ICL transiency as shown in Figure 3 (a). Previous work (Singh et al., 2023) speculate that the competition between IWL and ICL circuits is responsible for the transient behavior of ICL. We think this likely happens because, as the training progresses, the IWL task becomes easier and can be solved purely using in-weight knowledge, thus weakening the in-context look-up mechanism. To support this argument, we show that by using a harder IWL task of instance discrimination with iCopy, transiency is nearly eliminated (Figure 4a). In an instance discrimination task, each sample is treated as a separate class, also referred to as a self-supervised learning objective in the literature. This might also explain why LLMs can retain IWL and ICL performance even after training: language modeling is a complex IWL task due to the complexity of natural language, including ambiguity, long-range dependencies, and large vocabulary. More on this in Section 5."}, {"title": "4.3 ICOPY PROMOTES THE INDUCTION HEADS", "content": "Prior works show that the formation of an induction head leads to in-context learning (Olsson et al., 2022; Reddy, 2024). Induction heads are a result of the two-layer circuit that performs match and copy operations from the context. For a bigram sequence containing image-label pairs, first, the label tokens attend to the previous image tokens and copy the information into their representation. Then the query image token attends to the matching label tokens and copies the label information."}, {"title": "4.4 ICOPY PROMOTES ICL ON OTHER DATASETS", "content": "Repetitions in combination with high-burstiness scales well to more realistic datasets like CIFAR-100, Caltech-101, and DTD. We observed strong ICL performance on all three datasets. Using only high burstiness in the in-context sequences (Baseline) does not show any in-context learning for these datasets as shown in Figure 4b."}, {"title": "5 DOES IWL OBJECTIVE MATTER FOR ICL?", "content": "As described in Section 4.2, we believe the look-up mechanism on its own is not sufficient for a stable ICL performance and a choice of the appropriate IWL objective plays an important role. In particular, we hypothesize that if the in-weight task is too simplistic, the look-up mechanism does not emerge or results in a transient ICL performance since the model can optimize for the IWL objective without needing to attend to the context. Therefore, the in-weight task must have a minimum level of complexity to give rise to the look-up mechanism. A related study (Chan et al., 2022) showed that a long-tail distribution and an increase in the number of classes improve ICL, connecting these properties to natural language data. In this work, we additionally provide an explanation of why these properties enhance ICL - by increasing the complexity of the IWL task.\nWe propose four different ways of regulating the IWL task difficulty - changing the number of training classes, changing the number of samples used for training, training with noisy labels, and switching to the instance discrimination task. Here, we show how each of the proposed techniques influences the ICL and IWL performance."}, {"title": "Number of classes", "content": "We observe that increasing the number of classes monotonically improves ICL performance, as illustrated in Figure 6a. This finding follows similar insights in prior work (Chan et al., 2022; Reddy, 2024). However, these works explain the improved ICL capabilities due to the large number of rarely occurring classes. We interpret it as just one out of many ways to make the IWL task harder. Please refer to Appendix D for more details."}, {"title": "Skewed distribution", "content": "Keeping the total number of samples the same, we compare the ICL performance of a model trained with balanced and imbalanced (Zipfian) distributions. We observe improved ICL performance with skewed distribution, as illustrated in Figure 6b. This experiment is a confirmation of prior work (Chan et al., 2022), which also shows improved ICL with the increased long-tail distribution. Imbalancing the label distribution is a known way to make the in-weight learning task harder. Please refer to Appendix D for more details."}, {"title": "Learning with noisy labels", "content": "We increase the complexity of the IWL task by adding label noise, where labels of certain samples in the sequence are randomly assigned to another training class. Label noise is only applied to the standard sequences. We train a supervised model with 600 classes with three increasing levels of label noise percentage. Figure 7 (a, b) shows that as the label-noise ratio increases, the ICL performance improves while the IWL performance reduces, showing how the task is harder with more noise."}, {"title": "Instance discrimination task", "content": "Using the copy-based look-up strategy with repetitions in the sequence, we devise a more complex IWL task by moving from supervised to self-supervised learning. We design a task based on the instance discrimination (Wu et al., 2018) objective, where the model is trained to classify each sample as its class.  We train the baseline model in the supervised high burstiness setting with 3600 samples from 200 classes where the ICL does not emerge, illustrated in Figure 7 c. Whereas, when we train with the instance discrimination objective on the same number of samples, we obtain very strong and stable ICL performance, which is the result of hard IWL task and strong in-context look-up."}, {"title": "6 DISCUSSION", "content": "Key insights. We identify that the balance between training data that provides opportunities to use the in-context look-up mechanism and the complexity of the model training objective influences the emergence and stability of ICL. We show that the conceptual repetitions, which are also commonly present in textual data sequences, induce a strong in-context look-up mechanism. Repetitions combined with high-burstiness in the training sequences result in peak ICL performance. Models with such in-context look-up mechanisms ensure strong ICL performance with no harm to the IWL component, while also demonstrating reduced transiency in ICL performance. This supports our hypothesis that the conceptual repetitions in the data are strong drivers for the ICL ability. As shown in Section 5, training with a complex in-weight learning objective leads to improved and non-transient ICL performance. This conclusion aligns with the complex training objective and observed nature of ICL performance in LLMs."}, {"title": "Limitations", "content": "Although the ranking between different compared methods is clear, we observed a large variance in the ICL performance curves w.r.t. random seeds where the IWL task is simple. We believe one of the reasons for this is the sensitivity of ICL towards training sequences (Press et al., 2023). While our approach transfers well to other visual datasets and different training objectives, showing a similar analysis on real-world sequential data is out of scope of this work. We also found that ICL is sensitive to certain model design choices such as model weight initialization and image embedding architecture. For instance, using a truncated normal distribution for initialization improves the stability of ICL performance across different seeds. Addressing the robustness of ICL is a goal for future work."}, {"title": "Broader implications", "content": "Models with in-context learning ability allow users to leverage large models for various downstream tasks without the need to adapt the model weights by simply defining the task during inference. We provide an analysis of the training sequences showing that conceptual repetitions are influential and they reduce the need for specific properties in the data distribution, giving more flexibility to obtain ICL for different domains and applications. From a research perspective, our findings allow for analyzing internal workings in a controlled setup, facilitating a better understanding of ICL. Our work is based on mainly synthetic training sequences and thus has no direct societal impact."}, {"title": "Future work", "content": "We believe that our findings about the essential properties of the data sequence and design of in-weight learning objectives could be crucial for future large model training, where in-context solving ability is important. Our findings are agnostic of the data domain and should be applicable to LLMS, VLMs or other modalities."}, {"title": "A MODEL DETAILS", "content": "Architecture details. We train the GPT-2 model Radford et al. (2019) with 12 layers and 8 heads with an embedding dimension of 64. We use a smaller model for the induction head analysis experiments with 3 layers, a single head, and an embedding dimension of 64. GPT-2 expects a sequence-like format with aligned embedding size so we transformed our image-label pairs into separate image and label tokens, using a ResNet-like embedder for images and an embedding layer for labels. We initialized the model with a truncated normal distribution, which is important for training stability. We use a 3-block ResNet model He et al. (2016) as the image embedder with output channel dimensions [64, 128, 256]. After that, a projection layer is added to match the embedding dimension of 64. We train the image embedding model and GPT model together from scratch. We notice that the emergence of ICL is sensitive to the input image embedder architecture. We also report that pretrained embedders result in fast convergence of in-weight task and ICL failure cases, as shown in Figure 8.\nHyperparameters. We trained the model for different numbers of steps varying from 250k to 2M iterations using optimizer Adam Kingma & Ba (2014) with betas (0.9, 0.99) and epsilon 1e-08. We use learning rate warm-up for 15K iterations with a square root decay scheduler with a maximum learning rate value of 6e-4. We find that ICL performance is enhanced with longer warm-up periods. We perform gradient clipping to value 1.0. We trained the model with a batch size of 16 on a single Nvidia RTX 3090 where 500k iterations took around 12 hours. For all experiments, we run the approach for 3 random seeds, except for label-noise experiments in Section 5, where we report performance over 5 random seeds."}, {"title": "B EXPERIMENT SETUP DETAILS", "content": "Training details. The model is trained with a mix of two types of sequences with different burstiness forms: in-context sequences and standard sequences. In-context sequences can have multiple reoccurring samples from the same class as the query, whereas standard sequences have all samples selected from random classes. The high-burstiness strategy uses in-context sequences with three re-occurrences and iCopy strategy uses only one re-occurrence. All the supervised models in this work are trained with a burstiness probability of 90%, which means 90% in-context sequences and 10% standard sequences. All self-supervised models use 100% in-context sequences.\nEvaluation details. We evaluate separately for both ICL and IWL. ICL evaluation is performed in a few-shot manner with 2-way 4-shots and 4-way 2-shots tasks. The evaluation is performed using the pretrained softmax classifier without any model update. We use label mappings 0 to \ud835\udc58 with \ud835\udc58 being the number of classes in the few-shot setting. We notice that the ICL results are agnostic to label mappings. ICL is always performed on the hold-out classes not seen during training. ICL is performed over 10K presampled sequences to ensure a fair comparison. IWL is evaluated on the hold-out samples from the training classes where the sequence format must be (A-B-C-D-E-F-G-H) so that the model does not perform look-up, but relies only on the model weights."}, {"title": "B.1 DATASETS", "content": "All analysis experiments are performed on the Omniglot dataset Lake et al. (2015). We further show ICL results on other visual datasets Cifar-100 Bertinetto et al. (2019)", "below": "nOmniglot consists of 1623 handwritten characters from 50 alphabets with 20 exemplars for each character. Unless stated otherwise, we use 1600 classes as the base classes and the remaining 23 classes (sampled from the official evaluation subset with seed 42) for the ICL evaluation. We create a train-validation split as 18-2. During the supervised training, we apply no augmentations except for resizing to 64x64. However, self-supervised setup benefits from mild augmentations (random crop resize to 64x64 with scale (0.5, 1.5) and horizontal flip). For the self-supervised experiments, we used a batch size of 216 and a learning rate of le-3.\nCIFAR-100 is a natural dataset consisting of 60000, 32x32 colored images divided into 100 categories with 600 examples from each one. We use 80 classes for supervised training and 20 classes for the ICL evaluation as it is given by the Cifar-100FS (Few-Shot) version of the dataset. We used 10% of the data for the validation. We do not apply any augmentations, but we resize the image to 64x64 for training and evaluation.\nCaltech-101 is a natural, imbalanced dataset with 101 classes with 40-800 images per class while most classes have"}]}