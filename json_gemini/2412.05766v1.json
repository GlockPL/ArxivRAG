{"title": "Policy-shaped prediction: avoiding distractions in model-based reinforcement learning", "authors": ["Miles Hutson", "Isaac Kauvar", "Nick Haber"], "abstract": "Model-based reinforcement learning (MBRL) is a promising route to sample- efficient policy optimization. However, a known vulnerability of reconstruction- based MBRL consists of scenarios in which detailed aspects of the world are highly predictable, but irrelevant to learning a good policy. Such scenarios can lead the model to exhaust its capacity on meaningless content, at the cost of ne- glecting important environment dynamics. While existing approaches attempt to solve this problem, we highlight its continuing impact on leading MBRL meth- ods -including DreamerV3 and DreamerPro\u2014 with a novel environment where background distractions are intricate, predictable, and useless for planning future actions. To address this challenge we develop a method for focusing the capacity of the world model through synergy of a pretrained segmentation model, a task-aware reconstruction loss, and adversarial learning. Our method outperforms a variety of other approaches designed to reduce the impact of distractors, and is an advance towards robust model-based reinforcement learning.", "sections": [{"title": "1 Introduction", "content": "Model-based reinforcement learning (MBRL) is a promising path to data-efficient policy learning, and recent advances show impressive performance with high dimensional sensory data [Hafner et al., 2023]. A central component of MBRL is a world model, which is trained to predict how an agent's actions impact future world states. However, the world is highly complex while the capacity of a world model is finite, and ultimately only a subset of the components and dynamics of the environment can be accurately modeled. In this setting, distracting stimuli can be particularly problematic, as they waste the capacity of the world model on useless details.\nTo address the challenge of distractors, a number of MBRL methods seek to isolate the most important components of an environment, including structural regularizations [Deng et al., 2022, Fu et al., 2021, Wang et al., 2022], pretraining the agent's visual encoder [Seo et al., 2022, Wu et al., 2023], and value-equivalent world modeling [Schrittwieser et al., 2020], while environments such as Distracting Control Suite have been developed to assess distractor suppression [Stone et al., 2021].\nIn this paper we introduce a new method, Policy-Shaped Prediction (PSP), for identifying and focusing on the important parts of an image-based environment. Rather than relying on pre-imposed structural regularizations, PSP learns to prioritize information that is important to the policy. We synergize task-informed gradient-based loss weighting, use of a pre-trained segmentation model [Kirillov et al., 2023] and adversarial learning to create a distraction-suppressing agent that outperforms leading image-based MBRL agents. In addition to exhibiting similar performance in distraction-free settings and on a standard benchmark of robustness to distractions, our method markedly improves performance in the face of particularly challenging distractors that are intricate but entirely learnable. Because learnable distractors can be accurately modeled, they straightforwardly contribute to reducing the world model's error, but needlessly exhaust the capacity of the world model."}, {"title": "2 Policy-Shaped Prediction", "content": "We introduce PSP, a method to reduce an agent's sensitivity to useless distractions by focusing on sensory stimuli that are most relevant to its policy, rather than seeking to model everything in the environment. Our guiding intuition was that we can use the gradient from the policy to the input image to identify important pixels in the environment, and that we can aggregate these pixelwise salience signals to identify important objects by using image segmentation. Specifically, we extend the principles of VaGraM [Voelcker et al., 2022] to a high-dimensional vision model by using explainability-related notions of salience and aggregating an otherwise noisy gradient-based salience signal within objects. Additionally, inspired by the biological concept of efference copies [Crapse and Sommer, 2008], which are neural signals used to cancel out sensory consequences of an animal's actions, we incorporated a way to explicitly mitigate distractions caused by actions of the agent itself.\nPSP employs (1) gradients of the policy with respect to image inputs to identify task-relevant elements of the image, (2) a segmentation model to aggregate gradients within each object in the image, and (3) an adversarial objective to the image encoder of the world model that discourages encoding of duplicate information about the previous action. Figure 1 illustrates the training modifications made by this method to the underlying DreamerV3 [Hafner et al., 2023] architecture. Notably, since these modifications only affect the training stage of the world model, the DreamerV3 agent remains unaltered during inference. Below, we describe each of the three key components in detail."}, {"title": "2.1 Task-informed image reconstruction with policy-gradient weighting", "content": "Our approach builds upon the core idea that signals most important to the actor and/or critic should be given special importance in the world model. The concept of using the critic to inform model loss was applied in Value-Gradient weighted Model loss (VaGraM) [Voelcker et al., 2022], which weights the model loss according to the gradient of the value function with respect to the state. We extend this concept to high-dimensional image inputs, which previous work did not demonstrate. This extension to the image domain is inspired by gradient-based interpretability methods such as saliency maps [Simonyan et al., 2013, Shrikumar et al., 2017, Ancona et al., 2019].\nBy upweighting the reconstruction loss for parts of the image that inform value estimation, we might expect to improve the performance of a downstream policy that aims to maximize value. Going one step further, we propose using the gradient of the policy for weighting the model loss. While VaGraM focused solely on the value function, we hypothesize that the gradient of the policy may provide an even more informative signal \u2013 because ultimately, the state representation must support effective action selection. We hypothesize that the set of signals informing action selection may be richer than those that inform value estimation, which might rely primarily on simple cues such as whether an agent has flipped over. In contrast, the signals needed to select actions can be more subtle, such as the distance of an agent's leg from the platform it pushes off of in order to run.\nTo compute the policy-gradient weighting, we first sum across the dimensions of the action vector $a = E(\\pi(s))$, where $s$ is the latent state of the world model, to produce a scalar $a = \\sum_j a_j$, and then take the gradient with respect to the pixels of the input image $x$. To apply this weighting in the context of DreamerV3 [Hafner et al., 2023], we scale the image reconstruction loss term at each pixel i, for reconstructed image $\\hat{x}$.\n$L_{image}(\\theta) = \\sum_i \\frac{\\partial a}{\\partial x_i} (x_i - \\hat{x}_i)^2$"}, {"title": "2.2 Object-based aggregation of gradient weights", "content": "Gradient-based weighting of the world model's reconstruction ultimately is a form of applying model explainability methods, which attempt to highlight the most important elements of a model's input for its outputs. From model explainability literature, a known challenge with gradient-based weighting is its noisiness, which is likely caused by the presence of sharp but meaningless fluctuations in the derivative at small scales [Smilkov et al., 2017]. While this has been combated by more computationally demanding explainability approaches such as Integrated Gradients [Sundararajan et al., 2017] and SmoothGrad [Smilkov et al., 2017], we found these to be infeasible to run within the train loop, since this would require taking the derivative of the function with respect to multiple varying inputs for every example in the original input batch. Instead, to combat this problem we introduce a second novel contribution: object-based aggregation of an explainability signal using a segmentation model (SEG). In principle many models should work, but to ensure we had high quality segmentations, we used the Segment Anything Model (SAM) [Kirillov et al., 2023], a pre-trained and broadly applicable segmentation model. Nothing prevents a different model from being utilized, so long as it is of sufficient quality. This idea follows from the observation that the saliency map tends to show a broad, if noisy, correlation with relevant regions in the image (e.g. Figure 1). During data collection, we segment each image into object masks using the existing method laid out by the authors of SAM. We prompt the model with a grid of 256 points, filter the resulting masks with metrics for the SAM algorithm (Intersection-Over-Union and a \"stability score\"), followed by non-maximum suppression. We also include a mask to capture pixels that are not otherwise assigned to an object. Then, instead of weighting the image reconstruction loss with the raw gradient-based salience, we use a salience score that is aggregated within each segmentation mask. The weight of a pixel $x_i$ that has been assigned to segment SEG($x_i$) is the mean absolute value weight of the pixels in SEG($x_i$).\n$W_i = \\frac{1}{||SEG(x_i)||} \\sum_{j \\in SEG(x_i)} |\\frac{\\partial a}{\\partial x_j}|$"}, {"title": "2.3 Adversarial action prediction head", "content": "We additionally sought to explicitly reduce the distracting sensory impact of an agent's own actions. As animals move, they experience sensory signals generated by their actions and the external environment, and they have evolved the ability to distinguish these signals using efference copies [Crapse and Sommer, 2008]. We hypothesized that we could separate information about an agent's actions and its encoding of external stimuli through domain-adversarial training [Ganin et al., 2016]. To this end, we introduce an adversarial action prediction head that prevents the model from wasting capacity on irrelevant stimuli that are created by the agent's own actions.\nThe DreamerV3 world model consists of three main components: a convolutional neural network (CNN) image encoder $z_t \\sim q_{\\phi}(z_t|h_t, e_t)$ with $e_t = CNN_{\\rho}(x_t)$, which processes the input image, serves as a prior during training, and encodes the environment state during inference; a recurrent state space machine (RSSM) consisting of $h_t = f_{\\theta}(h_{t-1}, z_{t-1}, a_{t-1})$ and $z_t \\sim P(z_t|h_t)$ that is trained to simulate the progression of latent states given actions; and an image decoder, $\\hat{x}_t \\sim P(x_t|h_t, z_t)$ which reconstructs the image from the latent state. Problematically, the encoder can capture information about previous actions from the image, despite this information already being provided directly to the RSSM through the action input. In other words, $z_t$ may source information about $a_{t-1}$ directly through $x_t$, despite $a_{t-1}$ being an argument to $f_{\\theta}$ during the computation of $h_t$. Unfortunately, our reconstruction loss weighting may not solve this problem, since during backpropagation from the actor-critic functions, we do not distinguish information about previous actions that comes from the image versus the action input to the RSSM.\nTo prevent the CNN encoder from wasting capacity on encoding duplicate information about an agent's actions, we add a small multilayer perceptron (MLP) head that is optimized to predict the previous action from the image embedding.\n$\\hat{a}_{t-1} = MLP_\\omega (stop\\_grad(CNN(x_t)))$\n$L_{AdvHead}(\\hat{a}_{t-1}, a_{t-1}) = (\\hat{a}_{t-1} - a_{t-1})^2$\nWhen updating $\\theta$ during world model training, we subtract the scaled gradient $\\epsilon \\cdot \\nabla_{\\theta} L(\\hat{a}_{t-1}, a_{t-1})$ from the overall world model gradient, with $\\epsilon = 1e3$. This forces the latent state's previous action information to come solely from the provided action vector.\nOur training procedure for a DreamerV3 agent is shown in Algorithm 1. We note that it should be possible to apply these concepts of gradient-based weighting, segmentation-based aggregation, and adversarial action prediction to world models other than our chosen DreamerV3 architecture."}, {"title": "3 Experiments", "content": "To evaluate the model's performance we design our experiments around the following questions:\nQ1. Is our agent robust against distractors which are learnable by the world model, but of no utility for the actor-critic?\nQ2. What aspects of the environment are assigned importance by our method?\nQ3. Is our agent robust against distractors that are unrelated to the agent's actions?\nQ4. Does our agent maintain performance in standard, lower-distraction environments?\nQ5. What are the contributions of each component of our method?"}, {"title": "3.1 Experimental details", "content": "Baselines We test four Model-Based RL approaches as baselines: DreamerV3 [Hafner et al., 2023], and three methods specifically designed to handle distractions \u2013 Task Informed Abstractions [Fu et al., 2021], Denoised MDP (method in their Figure 2b) [Wang et al., 2022], and DreamerPro [Deng et al., 2022]. Additionally, we choose DrQv2 [Yarats et al., 2021a] as a representative baseline Model-Free approach. For all agents, we use 3 random seeds per task, and default hyperparameters.\nEnvironment details Visual observations are 64 x 64 x 3 pixel renderings. We test performance in three environments: DeepMind Control Suite (DMC) [Tassa et al., 2018], Reafferent DMC (described below), and Distracting Control Suite [Stone et al., 2021] (with background video initialized to a random frame each episode, 2,000 grayscale frames from the \"driving car\" Kinetics dataset [Kay et al., 2017]). For each environment, we test two tasks: Cheetah Run and Hopper Stand. We selected these tasks because they present different levels of difficulty, allowing us to assess how distraction-sensitivity depends on task difficulty. For ablation experiments, we test on Cheetah Run."}, {"title": "3.2 Reafferent Deepmind Control Suite", "content": "In the natural world, distractions can be highly complex, but in many cases are also highly pre- dictable. For instance, the creaking sound a rusty bicycle makes as you pedal, or the movement of your own shadow as you dance outside. We wanted an environment which would allow in- vestigation of how well existing methods would perform in scenarios where the representational complexity of the distractions are very high, but they cannot simply be ignored as 'unlearnable' noise. Distinguishing this type of partly self- generated distraction requires identifying which parts of the world are relevant to taking action, not just those affected and unaffected by our ac- tion. To achieve this, we devised the Reafferent Deepmind Control environment, in which the distracting background images have substantial content, but they depend deterministically on the agent's previous action and the elapsed time in the episode \u2013 and are thus completely predictable (Figure 2). We build on the Distracting Control Suite [Stone et al., 2021], using a background"}, {"title": "3.3 Performance on unaltered DMC and Distracting Control Suite", "content": "On Distracting Control tasks, in which the background distractor is uncoupled from the agent's actions, PSP produced consistently improved performance relative to baseline DreamerV3, in contrast to the more variable performance of DreamerPro, TIA, and Denoised MDP (Table 1, Figure 6). This addresses Q3.\nImportantly, PSP also shows comparable performance to other methods (including DreamerV3) on the unaltered Deepmind Control Suite, demonstrating that we have not introduced a tradeoff between performance on distracting and non-distracting environments (Table 1, Figure A5), resolving Q4.\nIn sum, PSP exhibits similar performance to baseline methods in commonly used tests of distractor-suppression and in non-distracting environments, while also demonstrating unmatched performance on particularly challenging distractors that are complex but learnable."}, {"title": "3.4 Ablation study", "content": "To understand the contributions of each sub-component of the method (i.e. address Q5), we conduct ablations on the reafferent and unaltered Cheetah Run (Table 2). We find that some ablations trade off performance between the environments, while our complete model has good performance on both.\nFor instance, the top-performing method on the reafferent environment does not incorporate the segmentation or adversarial components, and uses the value gradient rather than the policy gradient. However, the variance of its scores is higher than any other approach, and more problematically, it shows the worst performance of all experiments in the unaltered environment. We believe this occurs"}, {"title": "3.5 Additional segmentation models", "content": "We additionally tested the sensitivity of PSP to the segmentation model. Given that segmentation models are likely to continue improving over time, we won- dered 1) whether PSP could be compati- ble with other models besides SAM, and 2) how PSP performance might be mod- ulated by the performance of the segmen- tation model. To investigate, we used the recently released SAM2 [Ravi et al., 2024], which has multiple model sizes that allow for trading off performance for segmentation speed, with as high as 6x faster segmentation speeds than the orig- inal SAM. We updated PSP to use the 'tiny' SAM2 model, the smallest and low- est accuracy of the provided model sizes. Our basic implementation with SAM2-"}, {"title": "4 Related Work", "content": "Distraction-sensitivity of model-based RL Recent advances in Model Based RL (MBRL) includ- ing World Models [Ha and Schmidhuber, 2018], SimPLe [Kaiser et al., 2019], MuZero [Schrittwieser et al., 2020], EfficientZero [Ye et al., 2021], DreamerV1 [Hafner et al., 2019], DreamerV2 [Hafner et al., 2020], and most recently DreamerV3 [Hafner et al., 2023] have surpassed model-free RL in settings such as Atari, Minecraft, and Deepmind Control Suite. One deficiency of current MBRL algorithms is a susceptibility of the world model to become overwhelmed by easily predictable distractors, in part due to mismatch between the objectives of the policy (maximizing reward) and the world model (accurately predicting future states) [Lambert et al., 2020].\nOne line of work attempts to address the distractability of MBRL through structural regularizations. Deng et al. [2022] uses contrastive learning of prototypes instead of image reconstruction. Lamb et al. [2022] introduces the Agent Control-Endogenous State Discovery algorithm, which discards information not relevant to elements of the environment within the agent's control. Task Informed Abstractions (TIA) identifies task-relevant and task-irrelevant features via an adversarial loss on reward-relevant information [Fu et al., 2021]. Denoised MDPs extends TIA's factorization to include notions of controllability [Wang et al., 2022]. Clavera et al. [2018] use meta-learning and an ensemble of dynamics models. These works form a strong body of solutions, given prior knowledge of likely distractors, but they can struggle if a distractor does not fall into the designed regularizations.\nA different approach instead learns what is important by using the actor-critic functions to scale the importance of various learned dynamics. VaGraM uses value gradients to reweight state reconstruction loss [Voelcker et al., 2022], building on Lambert et al. [2020] and IterVAML [Farahmand, 2018], but VaGram does not operate on visual tasks. Eysenbach et al. [2022] propose a single objective for jointly training the model and policy. Goal-Aware Prediction learns a joint representation of the dynamics and a goal, by predicting a goal-state residual, although they describe this approach as likely still susceptible to distractions [Nair et al., 2020]. Seo et al. [2023] decouples visual representations and dynamics via an autoencoder, improving the performance of Dreamer on tasks involving small objects. Value-equivalent agents [Grimm et al., 2020], such as MuZero [Schrittwieser et al., 2020] or Value Prediction Networks [Oh et al., 2017], construct a world model that only aims to represent dynamics relevant to predicting the value function, in contrast to methods such as Dreamer that aim to learn the broader dynamics of the environment. MuZero is very effective in settings with discrete actions such as Atari, Go, and chess. Adaptation to domains with complex action spaces such as Deepmind Control Suite [Hubert et al., 2021] have shown some success, however Dreamer- based agents that include image reconstruction for world model learning can still exhibit superior performance, and the image-related signals have been shown to be essential to their performance [Hafner et al., 2020]. Building on these methods, our work investigates how to combine the benefits of both image reconstruction and task-aware modeling, through policy-shaped image-based world modeling, by applying concepts from VaGraM to the image-based MBRL setting.\nDistraction-sensitivity of model-free RL A parallel track of Model Free RL (MFRL) has its own body of literature, with a leading method DrQv2 [Yarats et al., 2021a] used in this paper"}, {"title": "5 Discussion", "content": "PSP combines three ideas to focus the capacity of an agent's world model on aspects of the environ- ment that are useful to its policy. First, the gradient of the policy with respect to the input image is used to identify pixels that influence the policy. Second, the importance of individual pixels are aggregated by object, using a segmentation model to identify objects. Third, wasteful encoding of the preceding action (which is known and does not need to be predicted) in the image embedding is removed using an adversarial prediction head. Together, these allow an agent to construct a world model that best informs its policy, and in doing so, use the policy to shape what information is prioritized by its world model. The outcome of this process is an agent that is selective about what parts of the world it models, and that becomes resilient against enticingly learnable, but ultimately empty, distractions.\nOur work draws a connection between the use of the value function gradient in VaGraM and related concepts from the vision model explainability literature. The value gradient can be seen as analogous to saliency maps [Simonyan et al., 2013]. Other gradient-based attribution methods, such as those that multiply saliency maps by input intensities [Shrikumar et al., 2017] or Integrated Gradients [Sundararajan et al., 2017, Ancona et al., 2019] offer additional ways to perform attribution. Some gradient-based attribution methods, such as Integrated Gradients, can be computationally expensive due to the need to approximate an integral over the input space. Future work may investigate incorporation of more advanced explainability methods such as these into PSP, and the concept of an agent 'interpreting itself' may exhibit broader utility. Finally, recent work that uses SAM combined with human supervision to improve the generalizability of model-free RL [Wang et al., 2023], together with our work, point towards the potential value of incorporating powerful object segmentation models into reinforcement learning systems.\nLimitations Limitations of PSP include its fundamentally object-centric view, which assumes that pixels belong to single objects, and that the objects can be ranked by their importance. Additionally, the SAM segmentation model requires significant compute, but these models will likely improve over time and can also be application-tailored. Notably, however, segmentation is not necessary during inference of the world model and policy, only training. Finally, it is not yet clear how well PSP will adapt in environments where the reward structure or salient features change across time. PSP may make the world model more task-specific than other approaches, although it does keep some reconstruction weight on non-task-relevant features and we observed initial evidence of resiliency (Figures A7, A8).\nOutlook Our work finds headroom to improve the robustness of MBRL to distractions by linking the actor-critic functions and the reconstruction loss and leveraging useful priors from pre-trained foundation models. The findings here open other lines of inquiry such as using better model explanation techniques or more explainable architectures, utilizing faster segmentation models, and utilizing segmentation models designed for videos, in order to do temporal aggregation. Substantial work likely remains to improve the speed of this technique and find extensions that allow it to reliably work for harder problems, such as applied robotics. The adversarial action prediction head's inspiration from the biological concept of efference copies also suggests there is still space in MBRL to consider biological metaphors as helpful design principles for learning algorithms. In sum, we present PSP, a method for avoiding distractors by focusing the world model on the parts of the environment that are important for selecting actions."}, {"title": "B Computational Overhead", "content": "We characterize the computational cost of the PSP algorithm by ablating various components and measuring its training speed (Table A1). There are four major components of PSP that affect performance. In order of decreasing computational cost: (1) Policy gradient-based weighting, (2) image segmentation (e.g. with SAM) (3) action adversarial head, and (4) segmentation-based aggregation of the gradient weighting. These are all costs that apply only during training, not during inference.\nPolicy gradient based weighting: For each latent state produced by the encoder RSSM in each step of a rollout, we take the gradient of the policy with regard to the image pixel inputs. In our implementation, this auto differentiation yields a complexity of $O((E + R + P) \\cdot S^2 \\cdot W \\cdot H)$, where E is the number of encoder parameters, R is the number of parameters of the RSSM, P is the number of parameters of the policy, S is the number of steps in a rollout, W is the width of the input image, and H is the image height.\nThere are a couple of major opportunities for optimization in future implementations. First, for debugging and experimental reasons, we have been computing the gradient of the policy with respect to all rollout steps, instead of just the current step. This reveals an opportunity to reduce the computational burden by a factor of S, where S is 64 in our implementation. Reducing this contribution could induce a significant speedup in the performance of future implementations. Second, we take the gradient of the policy with regard to the image during rollouts only to enable visualization"}, {"title": "C Experiments Compute Resources", "content": "Each trial of the PSP method used 4 Nvidia A40 GPUs to train the modified DreamerV3 model, and 4 A40 GPUs to run the Segment Anything model in parallel. Given an estimated 17 unique experiments for the final paper, 3 trials per experiment with our method, and about 1.5 days per training run, we used about 17 * 3 * 1.5 * 8 GPUs = 612 GPU days on A40 accelerators. Early experiments with this methodology likely used an additional 300. Baseline trials could be run on only a single A40 GPU or a desktop NVIDIA 2070 SUPER, usually in less than a day, and accounted for a comparably negligible level of resources.\nWe believe this level of resource consumption could be easily reduced. The modifications to the DreamerV3 model do not attempt to benchmark the most costly components. We suspect our method of parallelizing the new backpropagation from the policy to the image could be optimized further from its naive Jax implementation. Additionally, SAM could be supplanted by a more efficient segmentation model. We focused on establishing the basic technique with SAM, and replacing it with more efficient methods should be the subject of future work."}, {"title": "D Code", "content": "The repository with code and instructions for reproducing these experiments is available at this GitHub Repository."}]}