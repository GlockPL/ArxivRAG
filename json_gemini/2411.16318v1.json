{"title": "One Diffusion to Generate Them All", "authors": ["Duong H. Le", "Tuan Pham", "Sangho Lee", "Christopher Clark", "Aniruddha Kembhavi", "Stephan Mandt", "Ranjay Krishna", "Jiasen Lu"], "abstract": "We introduce OneDiffusion, a versatile, large-scale diffusion model that seamlessly supports bidirectional image synthesis and understanding across diverse tasks. It enables conditional generation from inputs such as text, depth, pose, layout, and semantic maps, while also handling tasks like image deblurring, upscaling, and reverse processes such as depth estimation and segmentation. Additionally, OneDiffusion allows for multi-view generation, camera pose estimation, and instant personalization using sequential image inputs. Our model takes a straightforward yet effective approach by treating all tasks as frame sequences with varying noise scales during training, allowing any frame to act as a conditioning image at inference time. Our unified training framework removes the need for specialized architectures, supports scalable multi-task training, and adapts smoothly to any resolution, enhancing both generalization and scalability. Experimental results demonstrate competitive performance across tasks in both generation and prediction such as text-to-image, multiview generation, ID preservation, depth estimation and camera pose estimation despite relatively small training dataset. Our code and checkpoint are freely available at https://github.com/lehduong/OneDiffusion.", "sections": [{"title": "1. Introduction", "content": "Diffusion models, particularly in text-to-image (T2I) generation, have recently shown remarkable results. Models such as DALL-E [43], Imagen [43], and Stable Diffusion [15, 41, 47] have established new benchmarks for generating high-quality, photorealistic images from text prompts. Additionally, recent studies have demonstrated the effectiveness of diffusion models in various other computer vision tasks, such as depth estimation [22] or optical flow estimation [37, 49], etc. However, despite these advancements, diffusion models are typically trained individually for either T2I generation or specific tasks.\nIn contrast, large language models (LLMs) (e.g. GPT-4 [1]) have demonstrated their ability to function as universal models. They can perform a wide range of tasks across different domains without the need for task-specific modules, and can effectively handle tasks they haven't been explicitly trained on zero-shot. This universality has been immensely valuable; it has dramatically simplified using, training and scaling these models, and ultimately lead to better performance. This incentivizes us to ask whether diffusion models can become universal in a similar way.\nDesigning a unified architecture for diverse image synthesis tasks presents significant challenges. Current methods often depend on external add-ons to handle new tasks. For example, ControlNet [68] or T2I-Adapter [39] require a specialized module to encode the conditional inputs, and personalization models typically require encoding the identity through a pretrained facial recognition network and adding auxiliary losses to preserve identity [21, 58, 63]. Additionally, tasks vary widely in input requirements. For instance, multi-view generation alone requires handling arbitrary input-output view combinations, posed or unposed images, and camera pose conditioning [18, 24, 34, 50, 57], while image understanding tasks require diverse outputs such as depth, pose, or segmentation. Finally, existing training recipes are often tightly tuned to particular tasks and therefore cannot be relied on to generalize between tasks.\nIn this work, we present OneDiffusion a unified diffusion model that seamlessly supports bidirectional image synthesis and understanding across diverse tasks. Our approach enables a single model to perform multiple tasks without the need for external losses and add-ons. Inspired by recent advances in diffusion models for sequential data [7, 48, 69], we model all conditions and target images as a sequence of \"views\" with varying noise levels during training. At inference, any of the views can be used as a conditional input, or set to noise and then used to generate an output image. Conditioning text can also be changed to define the task, and specify additional conditioning details (e.g. camera pose). The simple, but flexible, framework allows our model to support many kinds image generation and image understanding tasks with a unified architecture and training objective.\nTo train our model we create the One-Gen dataset, which integrates high-quality data across a variety of sources. This includes standard T2I data along with synthetic outputs from state-of-the-art models to support a range of tasks such as depth estimation, segmentation, pose estimation, etc. Our dataset also incorporates data for ID customization and multiview generation, providing diverse conditioning setups. The One-Gen dataset enables scalable joint training across multiple tasks, flexible conditioning options, improved generalization, and serves as a solid foundation for our model.\nTo demonstrate how general-purpose our training algorithm is, we train OneDiffusion completely from scratch. First, we train on text-to-image to equip the model with general image synthesis abilities, then on One-Gen to learn the full set of tasks. Our final model has 2.8 billion parameters and is equipped with a diverse set of skill, shown in Figure 1. The model also adapts naturally to various resolutions, enabling zero-shot high-resolution generation for sequence tasks even when such resolutions were not encountered during training.\nWe evaluate OneDiffusion on a diverse set of tasks for both generative and predictive tasks. On T2I, OneDiffusion efficiently generates high-quality images while utilizing fewer number of parameters. In"}, {"title": "2. Related work", "content": "Diffusion models for generative task. Recent advancements in diffusion models have greatly improved image generation capabilities, with models like Stable Diffusion [3, 4, 15, 41, 47, 72] setting new standards in text-to-image synthesis. Building on this foundation, controllable diffusion models such as ControlNet [68] and T2I-Adapter [39] extend functionality by enabling fine-grained control through auxiliary inputs like edge maps, depth maps, or human poses. Meanwhile, instruct-Pix2Pix [5] introduces natural language-guided image editing, making these tools more user-friendly. For personalized applications, identity-focused models, including IP-Adapter [63], InstantID [58], PhotoMaker [27], and PuLiD [21], personalize generation by conditioning on reference images. Meanwhile, in multiview generation, recent methods [18, 34, 50, 57], employ camera ray embeddings or 3D geometry to achieve consistent viewpoints. Together, these innovations showcase the versatility of diffusion models in delivering controllable, personalized, and multi-perspective image synthesis.\nDiffusion models for predictive tasks. Beyond image generation and manipulation, diffusion models have also proven effective for predictive tasks within computer vision. Marigold [22] fine-tunes the Stable Diffusion model [47] to perform monocular depth estimation, demonstrating the adaptability of diffusion models for prediction-based applications. Furthermore, diffusion models have been utilized for optical flow estimation, as shown in the works of Saxena et al. [49] and Luo et al. [37], where the models predict pixel-level motion between consecutive frames. Additionally, Li et al. [26] trained a diffusion model for open-vocabulary semantic segmentation, showcasing the potential of these models for more complex vision tasks. Prior works have attempt to unify diffusion model for predictive tasks [17, 19]. These studies show that diffusion models are not only useful for generating images but also highly effective for various predictive tasks in computer vision.\nUnified diffusion models. Several attempts have been made to unify diffusion model for different type of controls [42, 60, 70]. However, they are limited to utilization of multiple image conditions. These models usually requires to design complicated adapters for different conditions. [35, 36, 55, 71] propose unified models for language and images. Concurrently, [59] propose finetuning multimodal large language model with diffusion objective on diverse tasks like text-to-image, editing, and subject-driven generation etc. In contrast, our model distinguishes itself by leveraging bidirectional capabilities of diffusion models and addressing a wide range of diverse tasks."}, {"title": "3. Methodology", "content": "Flow matching [2, 30, 33] is a framework for training\ncontinuous-time generative models by learning a time-\ndependent vector field that transports between two probabil-\nity distributions. More specifically, a time-dependent vec-\ntor field $u_t : [0, 1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ governs the transformation\nfrom a base distribution $p_0$ to the target distribution $p_1 \\approx q$\nthrough an ODE $dx = u_t(x)dt$.\nThe solution of this ODE is a flow $\\Phi_t : [0, 1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\nwith initial condition $\\Phi_0(x) = x$, and this flow character-\nizes a push-forward operation $p_t = [\\Phi_t]_*p_0$, in which $p_t$ is\nthe density of samples $x \\sim p_0$ transported by $u$ from time 0\nto time t. The goal is approximate this ODE using a learned\ntime-dependent vector field parameterized as a neural net-\nwork $v_\\theta(t, x)$. Due to the intractable nature of $u_t$, [30] pro-\nposed to learn $v_\\theta(t, x)$ using the conditional flow matching\n(CFM) objective:\n$L_{CFM}(\\theta) := \\mathbb{E}_{t, q(z), p_t(x|z)} [||v_\\theta(t, x) - u_t(x|z)||^2]$                                                                                                                   (1)\nThis objective is equivalent to the original flow matching\nobjective, and only requires the samples from the target dis-\ntribution and a suitable conditional probability path."}, {"title": "3.2. Proposed Approach", "content": "Objective. We cast the problem of image generation with\nmultimodal conditions as sequential modeling. Inspired\nby previous work on diffusion model for sequential data\n[7, 48, 69], we jointly model all conditions and target im-\nages as a sequence of \"views\". Note that the number of\nviews N is determined by tasks. Particularly, N = 1 for\ntext-to-image tasks, N = 2 for image-to-image translation\nsuch depth/pose/image editing etc, N > 2 for multiview\ngeneration or ID customization.\nMathematically, let $\\{x_i\\}_{i=1}^N \\in \\mathbb{R}^{H \\times W \\times D}$\nbe sampled from a training dataset $q(x_1, ..., x_N)$. Given\ntime variables $t_i$, our goal is to learn a function\n$v_\\theta(t_1, ..., t_N, x_1, ..., x_N) : [0,1]^N \\times \\mathbb{R}^{N \\times H \\times W \\times D} \\rightarrow$"}, {"title": "3.3. Implementation Details", "content": "Model architecture. We adopt the Next-DiT architecture [72] in our model. By leveraging a full transformer-based architecture, our model can work with different numbers of views N. We independently encode each frame i.e. images and conditions as latent $z \\in \\mathbb{R}^{N \\times H \\times W \\times C}$ with a VAE tokenizer [15] and concatenate them in N dimension. With"}, {"title": "4. One-Gen Datasets", "content": "Text-to-Image We leverage both public and internal (synthetic) datasets. The public datasets including: PixelProse [52], Unsplash, Coyo [6], JourneyDB [40]. Additionally, we use a 10M internal synthetic dataset consisting of images re-captioned with LLaVA-NeXT [31] and Molmo [11]. The length of the text description for each image varies from 100 to 150 words. When an original prompt is available, we use both the LLM-generated caption and the original caption.\nImage-to-Image For simpler tasks like deblurring, in-painting, image generation from canny edge, or upscaling, we use a 1M-sample subset of our synthetic data and apply the relevant pre-processor for each image to create an input condition for it. For more complex tasks, we create a synthetic dataset from outputs generated by Midjourney, Stable Diffusion [15], and Flux-dev following the process outlined below:\n\\bullet Semantic Map and Detection For each image, we use the LLaVA-NeXT [31] model to identify entities or subjects (e.g., person, shirt, dog, building), with a maximum"}, {"title": "5. Experiments", "content": "In this section, we evaluate our OneDiffusion model on broad range of image generation and understanding tasks. We do not perform task-specific finetuning in any results. Details about additional qualitive exampels are in Appendix.\n5.1. Text-to-Image\nQualitative results of OneDiffusion for text-to-image task is illustrated in Figure 3. Thanks to the diversity of our One-Gen dataset, the model can handle various art styles, spanning both artistic and photorealistic designs.\nFollowing previous works [15], we evaluated the text-to-image capabilities of our model on GenEval benchmark [20]. For each prompt, we generate 4 images using Euler solver with 100 steps and guidance scale of 5. The results for OneDiffusion, along with those of baseline models, are presented in Table 1. Our model demonstrates strong performance compared to similarly sized baselines, excelling in multitasking capabilities despite being trained on a relatively smaller dataset.This performance is largely attributed to the diversity of the dataset and the comprehensive captions provided for each sample.\n5.2. Controllable Image generation\nWe show the experiment with image-to-image translation using various source domains, including HED, depth map,"}, {"title": "5.5. Depth Estimation", "content": "For image understanding tasks, we evaluate our model's performance on monocular depth estimation using standard benchmarks: NYUv2 [51] and DIODE [56]. We report the quantitative results in Table 4.Quantitative results are presented in Table 4, where our model achieves competitive performance compared to baselines that leverage pretrained text-to-image diffusion models, such as Marigold [22]. However, as shown in Figure 7, our model demonstrates greater robustness than diffusion-based depth estimators like Marigold. Specifically, it excels in handling open-world images, including paintings, hazy weather, and unconventional textures. For further qualitative comparisons, refer to Figures 15 and 16."}, {"title": "5.6. Camera Pose Estimation", "content": "We evaluate our model on camera pose estimation using the Google Scanned Object dataset [13]. For this task, we use six rendered images of each synthetic object and estimate the camera poses by denoising the corresponding ray embeddings. Following RayDiffusion [67], we apply least squares optimization to estimate the camera centers and rotations. The camera center accuracy, measured with a threshold of 0.3, is reported in Table 5."}, {"title": "5.7. Other Tasks", "content": "Since directly extracting masks, bounding boxes, and key-points from raw output images is not straightforward, we provide several qualitative results for human pose estimation and semantic segmentation on COCO dataset in Figures 17 and 18 in the appendix, respectively. Since our model does not distinguish tasks of conditions and images during training, its performance on understanding tasks serves as an additional indicator of the model. We leave further exploration of this aspect for future work."}, {"title": "6. Conclusion", "content": "Our experiments demonstrate that OneDiffusion achieves impressive results across a variety of tasks, including conditional T2I generation, depth estimation, open vocabulary semantic segmentation, pose estimation, multi-view generation, ID customization and camera pose estimation. We believe this work advances the capabilities of diffusion models, providing a versatile and scalable solution comparable to the flexibility offered by large language models. This represents a significant step toward developing a general-purpose vision model that can serve as the backbone for a wide variety of applications."}]}