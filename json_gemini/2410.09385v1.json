{"title": "Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models", "authors": ["Sathya Kamesh Bhethanabhotla", "Omar Swelam", "Julien Siems", "David Salinas", "Frank Hutter"], "abstract": "This paper introduces Mamba4Cast, a zero-shot foundation model for time series forecasting. Based on the Mamba architecture and inspired by Prior-data Fitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time series tasks without the need for dataset specific fine-tuning. Mamba4Cast's key innovation lies in its ability to achieve strong zero-shot performance on real-world datasets while having much lower inference times than time series foundation models based on the transformer architecture. Trained solely on synthetic data, the model generates forecasts for entire horizons in a single pass, outpacing traditional auto-regressive approaches. Our experiments show that Mamba4Cast performs competitively against other state-of-the-art foundation models in various data sets while scaling significantly better with the prediction length. The source code can be accessed at https://github.com/automl/Mamba4Cast.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting is a critical task in numerous domains, from finance (He et al., 2023) to healthcare (Jung et al., 2021), and has been approached through various deep learning methods in recent years (Chen et al., 2023; Liu & Wang, 2024). Time-series data often exhibits complex temporal patterns, varying distributions with many confounding variables, and long-range dependencies, making it more challenging to model than other data paradigms. Although the recent Cambrian explosion in deep learning, especially foundation models (Touvron et al., 2023; Yu et al., 2022), can be attributed in part to the availability of large amounts of data for training, the same cannot be said about forecasting in some domains (Wang et al., 2023; Sivaroopan et al., 2024).\nForecasting models (Salinas et al., 2020; Zeng et al., 2023; Oreshkin et al., 2019) have traditionally employed non-zero-shot methods, which typically require customized training or fine-tuning for each specific task. While effective, this approach can be resource-intensive and time-consuming. Transformer-based time series foundation models (Ansari et al., 2024; Rasul et al., 2023; Dooley et al., 2023) have demonstrated significant potential to address these limitations. However, their application to long sequences during inference is constrained by their quadratic sample complexity.\nIn an effort to address both of these problems, we present Mamba4Cast, a time series foundation model based on two concepts: Prior-data Fitted Networks (PFNs) (Hollmann et al., 2023; Dooley et al., 2023) and the Mamba (Gu & Dao, 2024; Dao & Gu, 2024) architecture. Our contributions are twofold:"}, {"title": "2 Related Work", "content": "Time series forecasting with Transformers In the last few years, transformer-based models have significantly improved the state of the art in time series forecasting. Works like the Informer (Zhou et al., 2021) and PatchTST (Nie et al., 2023) address the issue of long-term forecasting with trans-formers.\nZero-shot forecasting There have also been several advancements in zero-shot time series forecast-ing (Woo et al., 2024; Gruver et al., 2023). Gao et al. (2024) proposed UNITS, a unified multi-task model handling various predictive and generative tasks, and Oreshkin et al. (2021) proposed a meta-learning framework for zero-shot forecasting. These works highlight the growing trend towards more adaptable generalized time series models.\nForecasting LLMs In the wake of the recent success of Large Language Models (LLMs), a novel direction in time series analysis has emerged, focusing on adapting LLM-based architectures for forecasting. Studies such as Liu et al. (2024); Jin et al. (2024); Rasul et al. (2023) have demonstrated the effectiveness of re-purposing LLMs for time series tasks. These approaches involve techniques to align time series data with the text-based input expected by LLMs, such as using text prototypes or encoding time series as strings of numerical digits. Notably, Gruver et al. (2023) showed that LLMs can perform zero-shot time series forecasting at levels comparable to or exceeding purpose-built models. These developments suggest that LLMs are promising candidates for general-purpose time series analysis, which can offer advantages in flexibility and performance in various forecasting tasks.\nTraining on Synthetic Data While pre-training has enhanced the generalization capabilities of many models, their inductive biases often remain constrained to the distributions of their training corpus, potentially necessitating fine-tuning for niche applications. ForecastPFN (Dooley et al., 2023), inspired by PFNs (Hollmann et al., 2023; M\u00fcller et al., 2022), addressed this limitation by training on synthetic data, enabling zero-shot generalization to real-world time series. More recently, Chronos (Ansari et al., 2024) demonstrated state-of-the-art results by training on both synthetic and real-world time series, introducing a transformer-based foundation model that follows the next-token prediction paradigm of large language models.\nState Space Models Despite the success of transformer-based methods, they face scalability challenges due to their quadratic complexity. In contrast, state-space models, such as Mamba (Gu & Dao, 2024; Dao & Gu, 2024) or Linear Attention (Katharopoulos et al., 2020; Yang et al., 2024a,b), have emerged as more efficient architectures, adapting state space models / linear RNNs (P\u00f6ppel et al., 2024) for sequence modeling with linear scaling properties. This efficiency has proven crucial for modeling dense, long-sequence data in vision and time series forecasting (Behrouz et al., 2024; Patro & Agneeswaran, 2024). Subsequent works have further demonstrated Mamba's capacity in multivariate time-series forecasting; e.g., Wang et al. (2024) and Liang et al. (2024) proposed bi-directional Mamba architectures to capture inter- and intra-series dependencies, with the latter introducing a forget gate for enhancing selective performance on longer ranges. With recent"}, {"title": "3 Methodology", "content": "3.1 Background: State Space Models\nMamba4Cast builds upon the Mamba2 state-space model introduced by Dao & Gu (2024). Mamba2 is a linear Recurrent Neural Network described by the following recurrence:\n$h_t = A_t h_{t-1} + B_t x_t$;  $y_t = C_t h_t$\nwhere $h_t$, $x_t$, and $y_t$ represent the hidden state, input token embedding, and output at index t, respectively. In contrast to Mamba (Gu & Dao, 2024), which uses a fully parameterized diagonal state transition matrix $A_t$, Mamba2 employs a scalar multiple of the identity matrix allowing for more efficient computation. The recurrence can be computed in chunks of linear attention blocks that can be pieced together later, leveraging tensor cores through matrix multiplication. This approach differs from Mamba's evaluation through an associative scan, which is also performed in parallel across the sequence but cannot leverage GPUs as well.\n3.2 Mamba4Cast Architecture\nOur proposed architecture, illustrated in Figure 1, consists of four primary components:\n(1) Pre-processing: we scale the input series using a Min-Max Scaler and extract time features for positional embeddings. (2) Embedding: we embed the scaled input values and their temporal information using convolutions with different dilations, ensuring a large receptive field for the representation used by future layers. For more details about data pre-processing and embedding, refer to Appendix B. (3) Encoder: comprises of Mamba2 blocks with LayerNorm to avoid noisy learning signals followed by another dilated convolution layer. (4) Decoder: the final component is a linear projection layer that transforms the embedded token representations into point forecasts.\nWe perform an ablation study, detailed in Appendix D, investigating the role of convolutions, the efficacy of synthetic data generation methods, and the performance of alternative inference strategies.\n3.3 Synthetic Data Generation\nThe quality and diversity of the data generation process are crucial for Mamba4Cast's performance on real-world data, as it is trained exclusively on synthetic data. We employ two types of data-generating priors: ForecastPFN (FPFN) and Gaussian Process (GP) based. The FPFN prior, based on Dooley et al. (2023), decomposes a time series into trend, seasonality, and noise components reflecting real-life patterns. The GP prior, inspired by Ansari et al. (2024), complements the FPFN priors by accounting for patterns not captured therein. Each series is sampled from a GP with either a zero or a linear mean function and a composite kernel drawn from our Kernel bank. This allows for generating diverse and realistic synthetic time series that exhibit a wide range of temporal behaviors. Detailed descriptions of these data priors are provided in Appendix A."}, {"title": "4 Experiments", "content": "4.1 Training Details\nArchitectural choices The Mamba4Cast model is designed with approximately 27M parameters, positioning it between Chronos-Mini (20M) and Chronos-Small (46M) in size. As demonstrated in Figure 1, Mamba4Cast is built on Mamba2 (Dao & Gu, 2024) with a state expansion factor (N) of 128 and a block expansion factor (E) of 2. It features 2 encoder layers following an input projection to an embedding dimension of 1024. The final layer of the encoder is defined similarly to the stacked convolution layer illustrated in Appendix B with the difference in the input channels being 1024 for the embedding size. We minimize the mean squared error over the prediction horizon using AdamW (Loshchilov & Hutter, 2019).\nTraining setup The model is trained for 420K batches of size 64, using data sampled from the priors in Section 3.3, via a parallelized data loader that ensures the same sample is not seen twice. We train on sequence lengths uniformly sampled between 30 and 512 and minimize the mean squared error over a prediction length uniformly sampled between 10 and 60 per batch. 50% of the time we train to predict a contiguous chunk from the middle of the prediction length to improve predictability over the sequence by reducing reliance on previous states and encouraging emphasis on temporal information. The learning rate is cosine annealed (Loshchilov & Hutter, 2017) from le-5 to le-7 throughout the training.\nThe model is trained exclusively on synthetic data generated using two methods outlined in Section 3.3. The data composition is 70% sampled from GP priors and 30% sampled from FPFN priors, leveraging the GP kernels' flexibility in capturing diverse patterns. Training was conducted over 3 days on a single Nvidia RTX2080Ti GPU, for 360k training rounds consisting of 64 independently generated samples each. As stated in Appendix A.2, we continue training for another 60K rounds with a changed kernel composition and a learning rate of 1e-6.\n4.2 Performance Comparison with Baseline Models\nWe evaluate on 17 publicly available time series datasets from a wide range of domains from the dataset repository of the GluonTS (Alexan-drov et al., 2020) library with a 512 context length. A detailed list of the datasets used is in-cluded in Appendix C. Our evaluations involve comparisons with zero-shot baselines trained on synthetic data (Chronos and ForecastPFN), a deep learning baseline (DeepAR), and statistical methods (AutoARIMA and Seasonal Naive).\nFor our evaluations, we use Auto-Gluon-TimeSeries (Shchur et al., 2023) to evaluate the baselines, with the exception of ForecastPFN, whose results are sourced from the Chronos paper (Ansari et al., 2024). To ensure fair comparison across datasets with varying scales, we use the MASE metric (Hyndman & Koehler, 2006), which is scale-invariant.\nThe results, as illustrated in Figures 2 and 3, demonstrate that Mamba4Cast achieves competitive performance with Chronos-Base(200M) and surpasses other baselines. Notably, this performance is achieved without fine-tuning on real-world datasets. Figure 3 shows a critical difference diagram, visualizing the mean model rankings based on MASE (Mean Absolute Scaled Error) over the datasets. In this diagram, models are arranged from best (left) to worst (right), with statistically insignificant"}, {"title": "4.3 Qualitative Analysis on Synthetic and Real Data", "content": "We conduct a qualitative inspection of Mamba4Cast to evaluate its ability to extrapolate over the forecasting horizon. Figure 4 illustrates Mamba4Cast's improvement with increasing context length and its ability to capture real-life patterns. We also visualize the model's forecasting capability on additional real-world data in Appendix E."}, {"title": "5 Conclusion and Future Work", "content": "Our experiments demonstrate Mamba's capability in creating a reliable zero-shot time-series foun-dation model. After training solely on synthetic data, Mamba4Cast achieves near state-of-the-art results while also maintaining scalability and efficient inference. However, Mamba4Cast is limited to the univariate domain, which only forms a small portion of real time series problems, and is heavily reliant on the diversity of its priors. Nevertheless, we believe our work serves as a significant step towards developing highly performant and scalable multivariate zero-shot forecasting models, setting the stage for future advancements in this domain."}, {"title": "A Synthetic Data Generation", "content": "A.1 ForecastPFN Prior\nWe adopted the prior generation process from Dooley et al. (2023) that decomposes the time series into three components as outlined in Section 3.3. The trend incorporates linear and exponential growth factors, while seasonal components capture periodic variations at multiple time scales (minutely, hourly, daily, weekly, and monthly), reflecting natural cycles in the data. Noise is modeled using a Weibull distribution to maintain a constant expected value. We introduced some modifications to the original procedure that are mentioned below.\nTrend In our experiments, we found that training Mamba4Cast on long sequence time series with exponential trends results in suboptimal performance. Therefore we limited the non-linear growth behavior to be polynomial ones represented in the GP priors, while the FPFN prior only models linearly growing signals.\nSeasonal Seasonal patterns are generated according to the granularity of the timestamps. For each granularity, we sample sine-wave signals, referred to as harmonics, with periodicities corresponding to that granularity: 60 for minutely, 24 for hourly, 7 for daily, and 12 for monthly data. For each time series, we sample harmonics from both its granularity and the immediate higher granularity. As an example, for minutely data, we sample seasonal signals with both minutely and hourly periodicities. In the original design, 10 or 6 harmonics were sampled for each granularity, but in our optimal setup, we used 8 and 5 harmonics, respectively. As the number of harmonics increases, their periodicity is scaled down by the harmonic index, allowing the model to capture finer fluctuations in the data.\nA.2 GP priors\nGP model We use GPyTorch (Gardner et al., 2018) for sampling our time series from composite kernels, with a sampled zero or linear mean, and a Gaussian likelihood. We add the noise using Cholesky jitter, with the jitter level being sampled among 0.1, 0.01, and 0.001, with probabilities of 0.1, 0.2, and 0.7 respectively. This design choice is to generalize Mamba4Cast for different noise levels in the real-life datasets.\nKernels The kernel bank comprises Linear, Polynomial, Matern, and Periodic kernels. To ensure complex time series patterns, we combine up to 6 kernels using sampled binary operations (addition or multiplication). The best training pipeline involved sampling a number of kernels from 1 to 6. In the first 360K training rounds, for each kernel, we sampled from Periodic, Matern, or Linear kernels with weights of 5, 1.5, and 1 respectively. This prior was inspired by the KernelSynth method outlined by Chronos (Ansari et al., 2024).\nWe also observed that training followed by a fine-tuning phase of 60K rounds with changed weights of Periodic, Matern, and Polynomial kernels to 5, 2 and 1 respectively, resulted in better generalization.\nA.3 Signal level noises\nIn addition to the white noise signal incorporated in our priors, we introduce two types of multiplicative noise signals: spikes and step noise. Spikes are designed to introduce regular peaks at every interval, l. To simulate peaks that occur regularly but are irregularly spaced, we apply a masking window m, which masks up to 40% of the spikes within the window. Similarly, multiplicative step functions are applied in an alternating high-low-high-low pattern to enable Mamba4Cast to capture seasonal level shifts."}, {"title": "B Dataset Preprocessing", "content": "We adopt a preprocessing approach similar to e.g. ForecastPFN (Dooley et al., 2023). Time-steps are decomposed into minutely, hourly, day of week, day of month, day of year, monthly and yearly components, encoded using sinusoidal embeddings. These encodings, along with the series value, are linearly projected and concatenated to represent each time-point in a 112 embedding vector. The"}, {"title": "C Benchmark Datasets", "content": "We use 17 datasets from Chronos zero-shot benchmark while removing datasets with very small context and prediction length, datasets that are very large, and datasets with sub-hourly frequencies. We will extend to support those datasets in future work. We used GluonTS as an interface for these datasets to have a comparable evaluation pipeline to Chronos. The context length (input sequence length) was restricted to be at most 512, while the prediction length varied according to the evaluated dataset as shown in Table 1."}, {"title": "D Ablation studies", "content": "We investigate the robustness of Mamba4Cast in different configurations, which fall into three main categories:\n\u2022 Architectural Changes: We look into the effectiveness of a stacked causal convolutions layer (CNN) against a linear layer (Linear) in the input embedding and as the encoder-block's last layer. While adding the CNN layer as the final layer of the encoder block (baseline) provides superior performance with a significant overhead in model size, the key advantage stems from the CNN layer in the input embedding without overhead in model size.\nThe model sizes of the three setups listed in the corresponding section of Table 2 are 27M, 17M, and 15M, in the same order as in the table.\n\u2022 Prior Mixing Ratios: Given the importance of the distribution of synthetic data, we conducted experiments to explore the impact of each of the two approaches mentioned in Section 3.3. The ablation indicates the effectiveness of the GP prior over the FPFN prior, leading to our choice of a GP favoured mixture of data for training."}, {"title": "E Evaluations on real datasets", "content": "E.1 Evaluation metric\nAs part of our evaluation, we tested the performance of our model on real-world time series datasets alongside the synthetic data. The primary metric used was the seasonal Mean Absolute Scaled Error (MASE), which scales the forecast error by the mean absolute error of a seasonal na\u00efve forecast on the training data. The evaluation of Mamba4Cast on real-world datasets demonstrates the model's capability to generalize and perform well in diverse, real-world forecasting scenarios. Detailed evaluations per dataset can be found in Table 3. We witnessed inconsistencies between the evaluations performed by AutoGluon in our setups and the ones reported in Chronos paper on datasets with daily frequency, specifically on \"Covid Deaths.\" This resulted in the large gap witnessed on ForecastPFN's results reported here, since the model's MASE evaluations are sourced from the Chronos paper. The results reported for Mamba4Cast per dataset are evaluated with the best model trained according to the procedures in Section 4.1."}]}