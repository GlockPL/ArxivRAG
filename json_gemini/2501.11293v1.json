{"title": "A Machine Learning Framework for Handling Unreliable Absence Label and Class Imbalance for Marine Stinger Beaching Prediction", "authors": ["Amuche Ibenegbu", "Amandine Schaeffer", "Pierre Lafaye de Micheaux", "Rohitash Chandra"], "abstract": "Bluebottles (Physalia spp.) are marine stingers resembling jellyfish, whose presence on Australian beaches poses a significant\npublic risk due to their venomous nature. Understanding the environmental factors driving bluebottles ashore is crucial for mitigat-\ning their impact, and machine learning tools are to date relatively unexplored. We use bluebottle marine stinger presence/absence\ndata from beaches in Eastern Sydney, Australia, and compare machine learning models (Multilayer Perceptron, Random Forest,\nand XGBoost) to identify factors influencing their presence. We address challenges such as class imbalance, class overlap, and\nunreliable absence data by employing data augmentation techniques, including the Synthetic Minority Oversampling Technique\n(SMOTE), Random Undersampling, and Synthetic Negative Approach pproach that excludes the negative class. Our results show\nthat SMOTE failed to resolve class overlap, but the presence-focused approach effectively handled imbalance, class overlap, and\nambiguous absence data. The data attributes such as wind direction, which is a circular variable, emerged as a key factor influ-\nencing bluebottle presence, confirming previous inference studies. However, in the absence of population dynamics, biological\nbehaviours, and life cycles, the best predictive model appear to be Random Forests combined with Synthetic Negative Approach.\nThis research contributes to mitigating the risks posed by bluebottles to beachgoers and provides insights into handling class overlap\nand unreliable negative class in environmental modelling.", "sections": [{"title": "1. Introduction", "content": "Physalia spp. are marine stingers that inhabit the ocean surface\nin tropical and subtropical regions, with occasional occurrences\nin temperate zones [86]. Known as Portuguese Man-of-War in\nthe Atlantic Ocean [33] and bluebottles in the Indian and Pa-\ncific Oceans [11], their presence along coastlines is frequently\nreported.\nThere is limited understanding of the factors influencing the dis-\ntribution and movements of Physalia [11]. Similar to jellyfish,\nthese organisms are known for rapid and widespread blooms\nthat can disrupt food webs, biochemical processes, and socioe-\nconomic activities, including tourism, fishing, aquaculture, and\nindustrial operations [100, 74]. Their abundance, long tenta-\ncles, and painful stings pose threats to humans and marine in-\ndustries, with stings causing severe pain and sometimes serious\nhealth issues, impacting gastrointestinal, muscular, neurologi-\ncal, and cardio-respiratory systems [17, 111].\nLimited attention has been given to the occurrence of marine\nstingers due to these risks, which extend to beachgoers and\nmarine-related sectors [15, 100, 86]. However, ultimately, the\nrisks are higher when these organisms reach the coast. Hence,\nthe term \"beaching\", commonly associated with stranded whales\nand dolphins [26], is also used to describe the occurrence of\nbluebottles and jellyfish on shorelines.\nTo mitigate the negative impacts of marine stingers, it is crucial\nto understand the environmental factors driving their arrival on\nbeaches [9]. Furthermore, a deeper understanding of the re-\nlationship between their migration, beaching events, and en-\nvironmental conditions is essential for managing their hazards\neffectively [16].\nWhile research on marine stinger behavior and movements is\nongoing, there is limited work on modeling jellyfish beaching\nevents using machine learning tools, and even less for the blue-\nbottle (or Physalia). Several studies attempted the task, high-\nlighting critical challenges rising from the type of data. [16] ap-\nplied a MultiLayer Perceptron (MLP) model to predict jellyfish\npresence along the Spanish coast. While the model performed\nwell in identifying absence, its precision for detecting presence\nwas low, likely due to class imbalance in the dataset. Simi-\nlarly, [98] used an MLP to predict Physalia's presence in New\nZealand, emphasizing challenges with class imbalance and noisy\ndata. To improve predictions, they incorporated time-lagged in-\nformation into the input features. [2] used remote sensing data\nto predict jellyfish occurrences but reported highly unbalanced\ndata, with only 9% of observations in the positive (i.e. pres-\nence) class. The model achieved 91% accuracy, matching the\nmajority class ratio, indicating that it predominantly classified\nall observations as the negative class (i.e. absence).\n[74] investigated the distribution of Physalia physalis (bluebot-"}, {"title": "2. Literature review", "content": "2.1. Class Imbalance in environmental problems\nImbalanced classification datasets are common in scenarios in-\nvolving rare event (or anomaly) detection, such as identify-\ning uncommon diseases, detecting fraud, or addressing envi-\nronmental issues [55]. Class imbalance frequently occurs in\nmachine learning classification tasks, and here we list several\nexamples related to environmental challenges. [58], while pre-\ndicting cyanobacteria occurrence using climatological and en-\nvironmental controls in South Korea, encountered class imbal-\nance in the occurrence dataset. To prevent model bias, they\nproposed a parameter estimation method that assigns equal im-\nportance to cases with very few occurrences compared to the\nmore frequent non-occurrence cases. [77] highlighted the in-\nherent imbalance in their dataset while applying various ma-\nchine learning classification algorithms to predict the health\nrisks of cyanobacteria blooms in Northern European lakes. De-\nspite achieving high accuracy, they did not consider accuracy\nas the primary evaluation metric, focusing instead on the con-\nfusion matrix. For early warning systems to predict cyanobac-\nteria bloom outbreaks in freshwater reservoirs using machine\nlearning models, [95] faced an imbalance in the number of ob-\nservations for each class level. They evaluated model perfor-\nmance similarly to [77], placing emphasis on metrics derived\nfrom the confusion matrix. [57] addressed the class imbal-\nance in their dataset by applying oversampling techniques to\npredict cyanobacteria blooms in Chilgok, South Korea, using\na classification-based machine learning model. Similarly, [25]\ndealt with a binary class imbalance problem, where normal cases\noutnumbered algae or wastewater cases in detecting de facto\nreuse and cyanobacteria in drinking water intake. They em-\nployed Cohen's Kappa, boosting, and bagging methods to mit-\nigate the imbalance and used accuracy as a key performance\nmetric. [29] tackled the class imbalance issue in the automatic\nimage classification of coastal marine litter items from orthopho-\ntos. They tested various techniques such as class weighting,\noversampling, and classifier thresholding, with oversampling\nyielding the best results based on the F1-score. Finally, to ad-\ndress the challenge of imbalanced datasets,[46] applied over-\nsampling techniques to environmental complaints related to con-\nstruction projects. Their model outperformed existing mod-"}, {"title": "2.2. Data augmentation with SMOTE and GANS", "content": "Data augmentation involves techniques for generating synthetic\ndata from existing datasets, particularly for minority classes, to\naddress class imbalance problems [115, 106]. This approach\nhas proven effective in mitigating the issue of insufficient train-\ning data in real-world scenarios. The primary objective of data\naugmentation is to increase the size, quality, and diversity of the\ntraining dataset [85]. It has been widely applied across various\nfields facing class imbalance issues, including both tabular and\nimage data domains. Examples include its use in education[102],\ndata mining [68], healthcare [50], finance [120], and environ-\nmental studies [51]. The application of SMOTE for addressing\nclass imbalance problems has expanded across various fields,\nresulting in the development of several SMOTE variants over\nthe years [31]. Notable variants include SMOTE-ENN, K-Means\nSMOTE, SMOTE-SVM, Borderline SMOTE, Geometric SMOTE,\nand Weighted SMOTE [55]. [102] applied SMOTE to tackle\nclass imbalance in predicting student performance. [68] em-\nployed Random-SMOTE (R-S) on five imbalanced datasets for\ndata mining tasks, reporting significant improvements in per-\nformance. [109] introduced a selective oversampling technique\nby adapting SMOTE to focus on the most challenging minor-\nity class samples rather than uniformly sampling all minority\ninstances.\n[50] integrated SMOTE with Convolutional Neural Networks\n(CNNs) to address binary classification problems, observing\nthat SMOTE-CNN outperformed other comparable methods.\n[48] highlighted two major challenges associated with SMOTE:\nnoise and borderline data samples. To address this, they pro-\nposed a method that first generated synthetic instances for the\nminority class, followed by removing minority class samples\nnear the majority class. This approach consistently outperformed\ntraditional methods across 44 datasets.\nIn a Generative Adversarial Network (GAN), the generator net-\nwork is responsible for creating realistic samples, while the dis-\ncriminator network distinguishes these generated samples from\nthe original data [37]. [65] employed GAN to address class\nimbalance in an intrusion detection system, reporting improved\nclassifier performance. Similarly, [56] utilized a GAN-based\nanomaly detection model to tackle class imbalance in defect\ndetection, achieving enhanced predictive performance. [106]\nintroduced SMOTified-GAN, which combines synthetic data\ngenerated by SMOTE with GAN to create more realistic sam-\nples, significantly improving performance in addressing class\nimbalance problems. [30] applied SMOTified-GAN to model\ncritical mineral deposits using a Random Forest classifier, achiev-\ning better prediction accuracy. [21] proposed SMOTE+GAN\nand GANified-SMOTE for financial fraud detection. They demon-\nstrated improved performance across minority samples gener-\nated by these methods, effectively enhancing model outcomes\nfor imbalanced datasets."}, {"title": "2.3. Physalia, the bluebottle", "content": "Jellyfishes, box jellies, and the Physalia genus which includes\nthe bluebottle, are of the phylum Cnidaria but through different\nclasses [17], and can thrive in both low- and high-productivity\nenvironments [80]. Physalia has been reported with differences\nin size, colors, and habitat [97, 86], and recently classified in\nfour different species [24]. They are colonies of floating organ-\nisms that inhabit the water-air interface (pleustonic organisms),\ncomposed of highly specialized and interdependent zooids that\nrely on one-another for survival. To travel long distances, Physalia\nuses an inflated gas-filled float for buoyancy, while its long ten-\ntacles, equipped with venomous cells, can deliver stings as it\nmoves [86]. These aquatic predators are highly effective hunters,\nusing their venomous cells to immobilize and consume fish and\nfish larvae.\nPhysalia colonies exhibit left- or right-handed asymmetry, de-\ntermined by the position of the underwater zooids and the at-\ntachment point of the tentacle relative to the float and sail.[86].\n[44] highlighted that Physalia and other epleustonic organisms\nare not uniformly distributed in the ocean and face environmen-\ntal challenges such as climate change, oil spills, pollution, and\nfishing. Increased understanding of their habitats and distribu-\ntions is crucial for their conservation and prediction.\nA few studies have already attempted to understand the arrival\nof Physalia to the shore. [98], in predicting factors influencing\nthe presence of Physalia on New Zealand beaches, found that\nwave and wind directions from the northwest significantly in-\ncrease their presence. Conversely, wave directions to the north\none day prior lead to their absence. [74], modeling Physalia\noccurrence of Physalia in the North Atlantic, identified macro-\necological drivers like primary productivity, temperature, and\ncurrent direction. At regional scales, primary productivity and\nwind patterns were the key determinants. The study warned\nthat future climate change and reduced primary productivity\ncould cause significant shift in the distribution of this species.\n[11] studied Physalia beaching events and stings in relation to\nenvironmental factors. Their findings revealed a seasonal pat-\ntern, with greater occurrences during the Austral summer on\nthe East coast of Australia. The temperature seasonal cycle,\nwith a phase lag of 3-4 months, also influences their distri-\nbution. Wind direction emerged as the primary driver of their\npresence on beaches. Similarly, [16] modeled all jellyfish pres-\nence together and included Physalia, emphasizing the substan-\ntial impact of environmental factors on their occurrence. They\nrecommended regular monitoring of sea surface temperature\n(SST) and wind conditions to improve prediction accuracy. [75]\ninvestigated the occurrence of Physalia in the North Atlantic\n(Azores, Portugal) and the southeast pacific (Australian East\nCoast) using a machine learning approach based on boosted re-\ngression trees. Despite the class imbalance and class overlap\nin the dataset, their model demonstrated excellent predictive\nperformance, highlighting that Physalia occurrences are influ-\nenced by region-specific wind patterns and enhanced produc-\ntivity. They also modelled the Australian East Coast data on a\nmonthly time-scale rather than the daily scale reported by the"}, {"title": "3. Methodology", "content": "3.1. Data\nThis study utilizes data collected from the southeastern region\nof Sydney, Australia, focusing on three beaches: Maroubra,\nClovelly, and Coogee used in [11] and [75]. Maroubra, the\nlongest and most exposed of the three, is located to the South.\nFurther north, Coogee, is somehow protected by Wedding Cake\nIsland, and Clovelly is nestled in a narrow bay (Figure 1).\nThe dataset is based on Bluebottle beaching events, which serve\nas indicators of Bluebottle occurrences (daily presence and ab-\nsence). Beaching data were extracted from lifeguard reports\nprovided by the respective local councils. The dataset spans\nfour years, from March 2016 to June 2020, and includes daily\nobservations. Maroubra is monitored year-round, with data avail-\nable for all seasons from January to December. Similarly, Coogee\nhas year-round data coverage, whereas Clovelly only includes\nobservations from the warmer months (September to April) due\nto limited activity at the beach during winter [11].\nFigure 1 illustrates the locations of the three beaches in Sydney.\nThe daily observational bluebottle presence and absence dataset\nused in this study is available in the supplementary material\nfrom [11].\nIn addition, environmental variables were extracted from pub-\nlicly available gridded reanalysis products, corresponding to the\nmodel grid cell nearest to the beach where an observation was\nmade. This includes wind speed (m s\u00af\u00b9) and direction (\u00b0 , where\n0 = north and 180 = south) at 10 m height from the Australian\nBureau of Meteorology Atmospheric high-resolution Regional\nReanalysis for Australia (BARRA2; [112]; 12 km resolution,\naveraged daily) and surface ocean current speed (m s\u00af\u00b9) and\ndirection (\u00b0 , where 0 = north and 180 = south) and sea surface\ntemperature (SST, \u00b0C) from the Bluelink ReANalysis (BRAN2020;\n[18]; \u2248 10 km, daily).\n presents the list of features in the raw and processed\ndataset, along with associated features and data type."}, {"title": "3.2. Data Augmentation", "content": "3.2.1. SMOTE\nSMOTE operates in the feature space rather than the data space,\ngenerating synthetic samples by oversampling each minority\nclass instance. These synthetic samples are created along the\nline segments connecting the minority instance to one or more\nof its k-nearest neighbors, which are randomly selected based\non the required level of oversampling [20].\nSMOTE is an iterative process that starts by randomly select-\ning a minority class instance from the training dataset. For the\nselected instance, the algorithm identifies its k-nearest neigh-\nbors, and N of these k neighbors are randomly chosen to gen-\nerate new synthetic samples. The generation process involves\ninterpolation, which is achieved by calculating the difference"}, {"title": "3.2.2. Generative Adversarial Networks (GAN)", "content": "GANS [38] consist of two components: a generator network\n(GG) and a discriminator network (DD), which are trained si-\nmultaneously to produce synthetic data samples. The genera-\ntor GG learns to approximate the data distribution, while the\ndiscriminator DD estimates the probability that a given sample\noriginates from the real training data rather than from GG. Es-\nsentially, GG aims to generate samples indistinguishable from\nreal data, while DD works adversarially to distinguish between\ngenerated samples and true data.\nGANs can produce diverse variations of the original dataset,\nwhich can be leveraged to enhance model performance [55].\nThey have demonstrated exceptional performance in applica-\ntions such as computer vision (image generation) [41] and text\ngeneration tasks [69].\nUnlike image and text data, tabular data often contains a mix-\nture of discrete and continuous features, making it more com-\nplex and challenging to model and generate realistic synthetic\ndata. Additional difficulties arise when discrete columns are\nimbalanced, and continuous columns exhibit multiple modes.\nTo address these issues, [117] developed the Conditional Tabu-\nlar Generative Adversarial Network (CT-GAN), which employs\na conditional generator to tackle these challenges effectively.\n[105] applied CT-GAN to address class imbalance in tabular\ndata and reported significant success in improving the balance\nof datasets. However, [55] evaluated the combination of CT-\nGANs with ensemble learning and concluded that while ef-\nfective, this combination does not outperform established ap-\nproaches such as SMOTE, which are computationally more ef-\nficient.\nIn this study, we employ CT-GAN as a data augmentation method\ndue to its suitability for tabular data."}, {"title": "3.3. Models", "content": "We employed three different supervised machine learning mod-\nels: Random Forest, XGBoost, and Multilayer Perceptron. Ran-\ndom Forest and XGBoost, both ensemble learning methods,\nhave proven effective in addressing imbalanced datasets [82,\n119]. Additionally, we included Multilayer Perceptron, a neu-\nral network model capable of handling non-linear data, selected\nbased on studies by [98] and [16], which demonstrated the ef-\nfectiveness of artificial neural networks in predicting factors in-\nfluencing the presence of jellyfish.\nAlongside the supervised models, we also utilized an unsuper-\nvised learning model, the one-class Support Vector Machine\n(One-class SVM) [84]. One-class SVM was selected to address\nthe issue of unreliable negative data. To handle unreliable ab-\nsence data effectively, we incorporated techniques proposed by\n[42] which involves training with one class while generating\nsynthetic negative class for the model evaluation and [120] uti-\nlizing CT-GAN to create a new set of negative class (absence)\nsamples, which are then combined with the original positive\nclass (presence)."}, {"title": "3.3.1. Simple neural network", "content": "A simple neural network, also known as a Multilayer Percep-\ntron (MLP), is inspired by the biological neurons in the brain\nand is one of the most widely used machine learning models\n[122, 98]. MLP has demonstrated its versatility and flexibility\nas a function approximator for modeling diverse types of data\n[66], showing robustness in handling noisy and incomplete data"}, {"title": "3.3.2. Random Forest", "content": "Random Forest is a supervised ensemble learning model de-\nsigned for both regression and classification tasks [13, 82]. It\noperates by constructing an ensemble of decision trees, each\nbuilt using a random subset of data samples and features. For\nclassification tasks, the model predicts the class based on the\nmajority vote of the trees. The generalization error of a Ran-\ndom Forest is influenced by the strength of the individual trees\nand the correlation among them, while internal estimates allow\nfor calculating feature importance [13].\nRandom Forest is robust in handling missing values, noisy data,\nand high-dimensional datasets, making it effective for dimen-\nsionality reduction [82, 13]. It has been shown to outperform"}, {"title": "3.3.3. Boosting using XGBoost", "content": "Boosting is a family of ensemble learning models designed to\ntransform weak learners into strong learners by iteratively im-\nproving model performance [32, 121]. In Boosting, models are\nadded sequentially to the ensemble, with each new model cor-\nrecting the errors of its predecessors. The final prediction is a\nweighted combination of the outputs from all models [55].\nGradient Boosting improves predictions by creating new mod-\nels that focus on the residuals (errors) of previous models. These\nmodels are then combined to form the final output and can be\napplied to both regression and classification tasks. Extreme\nGradient Boosting (XGBoost) builds on this framework by in-\ncorporating regularization in individual learners (trees) to pre-\nvent overfitting, offering a more advanced and versatile imple-\nmentation of Gradient Boosting Machines [89, 22].\nXGBoost achieves scalability through innovative methods such\nas an efficient tree learning algorithm designed for sparse data\nand a theoretically sound weighted quantile sketch method for\nhandling instance weights in approximate tree learning [22].\nHowever, XGBoost has limitations, including sensitivity to pa-\nrameter selection and the need for careful fine-tuning to achieve\noptimal performance [55]. Additionally, it provides insights\ninto feature importance, allowing users to evaluate the contri-\nbution of each input feature to the model.\nXGBoost has been successfully applied to various environmen-\ntal and ecological challenges, including predicting particulate\nmatter levels, flood risk, pollutant concentrations, and more"}, {"title": "3.3.4. Random Undersampling", "content": "Random Sampling is a data augmentation method that selects\nkk unique items from a population of nn items, ensuring that\nevery possible combination of kk items has an equal probabil-\nity of selection [78]. Random Sampling Without Replacement\nselects a subset of elements from the population, giving each\nitem an equal chance of inclusion while disallowing duplicates\n[92].\nRandom Undersampling is a technique used to balance target\ndistributions by randomly eliminating instances from the ma-\njority class. Several undersampling strategies have been pro-\nposed, based on differing noise model theories. One theory"}, {"title": "3.4. One-Class SVM", "content": "The Support Vector Machine (SVM) [84] is a highly effective\nnon-parametric technique rooted in statistical learning theory,\nwidely used in machine learning for classification and regres-\nsion tasks [43]. SVM is typically a two-class model that incor-\nporates both negative and positive samples to achieve high pre-\ndictive accuracy while minimizing generalization error in de-\ntection and classification tasks [73, 43].\nThe One-Class SVM, an unsupervised learning extension of\nSVM, was proposed by Muller et al. [84] and is frequently ap-\nplied for anomaly and outlier detection. It operates by mapping\ntraining data from the input space into a higher-dimensional\nfeature space using a kernel function. In this feature space, it\nidentifies a hyperplane that maximizes the margin between the\nmapped data points and the origin [43].\nThere are two primary approaches for determining the decision\nboundary in One-Class SVM:\n1. Hypersphere Approach: This defines a hypersphere that\nseparates outliers from the positive class. The shape of\nthe decision boundary is controlled by the v-parameter,\nwhich balances the trade-off between the proportion of\ndata points classified as positive and as outliers [43].\n2. Hyperplane Approach: This approach calculates a hy-\nperplane in the feature space that ensures a specified frac-\ntion of training instances fall beyond it, while simultane-\nously maximizing the margin from the origin [84]. Due\nto its simplicity, this approach has become the most com-\nmonly used [43].\nOne-Class SVM has been successfully applied in various fields,\nincluding medical and environmental studies, among others [42,\n23]. In this study, we will employ the One-Class SVM to ad-\ndress the no-negative (absence) data challenge."}, {"title": "3.5. Dimensionality reduction for visualisation", "content": "Principal Component Analysis (PCA) [96] is a statistical tech-\nnique for dimensionality reduction that identifies a hyperplane\nminimizing the sum of squared perpendicular distances from a\nset of points in a multidimensional space. PCA determines the\ndirections (principal axes) that capture the maximum variance\nin the data, allowing for dimensionality reduction while retain-\ning the essential structure of the data [96].\nThis technique linearly transforms a multivariate dataset into\na set of uncorrelated variables (principal components), ordered"}, {"title": "3.6. Framework", "content": "We now present a machine learning framework addressing blue-\nbottle data as a case of unreliable negative data and class imbal-\nance, focusing on environmental factors influencing beaching\nevents (Figure 4).\nIn Step 1, data is extracted from council lifeguard reports of the\nselected beaches. This is followed by data cleaning and prepro-\ncessing, including removing outliers and duplicates, handling\nmissing values, and addressing data skewness.\nIn Step 2, exploratory data analysis is conducted using plots\nsuch as correlation matrices, density plots, seasonality trends,\nand line plots to examine class imbalance and data patterns.\nFeature selection is performed using a correlation matrix to\nidentify the most relevant predictors for modeling.\nLocation-specific variables (e.g., beach key, embaymentisation,\norientation, latitude, longitude, and surf club) are excluded to\nensure that the findings are generalizable to the region and not\nrestricted to the dataset's specific locations.\nIn Step 3, we focus on modeling the data and identifying\nthe most suitable machine learning models for handling class im-\nbalance. The dataset is split into training and testing sets, and\nmodels such as Multilayer Perceptron (MLP), Random Forest,\nand XGBoost are evaluated. Performance metrics, including\nprecision, recall, and F1-score, are reported for both the major-\nity and minority class labels to ensure a comprehensive assess-"}, {"title": "3.6.1. Evaluation Metrics", "content": "Evaluation metrics are essential for designing and assessing\nmachine learning models [39], particularly when addressing\nclass imbalance problems. A comprehensive set of metrics is\nrequired to ensure the model's performance is evaluated fairly\nacross both majority and minority classes, capturing its effec-\ntiveness in imbalanced scenarios.\nThe most commonly used metrics for binary classification in-\nclude classification accuracy, precision, recall, F1-score, and\nAUC. Classification accuracy evaluates the model's effective-\nness by measuring the percentage of correctly classified sam-\nples. While simple to implement, it is not suitable for highly\nimbalanced datasets [8]. This is because classification accuracy\noften fails to reflect the model's true performance, particularly\nfor the minority class, where it can be misleading due to the\ndominance of the majority class in the dataset.\nTo accurately evaluate model performance, we must consider\nfour key metrics: True Positives (TP): The number of positive\nsamples correctly identified as positive; True Negatives (TN):\nThe number of negative samples correctly identified as nega-\ntive; False Positives (FP): The number of negative samples in-\ncorrectly identified as positive; False Negatives (FN): The num-\nber of positive samples misclassified as negative.\nA confusion matrix provides a structured way to report these\ncounts, offering a clearer understanding of the relationship be-\ntween the actual and predicted classes [110]. It is a fundamental\ntool for assessing classification performance, especially in im-\nbalanced datasets.\nOther metrics are more effective for addressing class imbalance\nproblems. Precision reflects the correctness of a model by mea-\nsuring the proportion of predicted positive instances that are\ntrue positives. It is defined as:\nPrecision = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)+False Positives (FP)}}\nRecall, on the other hand, evaluates the model's completeness\nby assessing the proportion of actual positive instances that are\ncorrectly identified. It is defined as:\nRecall = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)+False Negatives (FN)}}\nUnlike classification accuracy, Precision and Recall are less\nsensitive to data distribution changes, making them more robust\nfor imbalanced datasets. They are particularly useful for de-\ntecting imbalances between two classes and providing a clearer\nunderstanding of a model's performance in such scenarios.\nThe F1 score is the harmonic mean of Precision and Recall,\nproviding a single metric that balances both correctness (Pre-\ncision) and completeness (Recall). It is particularly useful in\ncases where there is class imbalance, as it ensures that both\nmetrics are considered equally. A high F1 score indicates that\nthe model performs well in identifying the positive class while\nmaintaining a balance between Precision and Recall."}, {"title": "3.6.2. Technical and implementation details", "content": "We utilize Python libraries such as NumPy, Pandas, and scikit-\nlearn for data processing, analysis, and modeling, with the code\navailable in our GitHub repository 1. For all machine learning\nmodels used in this study (MLP, Random Forest, XGBoost),\nwe conduct 30 independent experimental runs. Each run in-\nvolves randomly assigned parameters (e.g., weights and biases\nin MLP) and employs a randomly shuffled training/test data\nsplit, with 60% of the data allocated for training and 40% for\ntesting.\nOur data was further processed where we subgrounded certain\nfeatures so that we get a better understanding of the cause of\nbluebottle beaching.  presents detail of the original fea-\nture and the subgroups of the feature.\nWe selected model hyperparameters from the literature and trial\nexperimental runs for the respective models. In the case of\nMLP, we utilised 100 hidden neurons and two output neurons,\nwith 6 input neurons for each feature. We used the Adam op-\ntimiser with a maximum of 400 epochs and a learning rate ini-\ntialisation of 0.1 and alpha of 0.0001.\nWe configured the XGBoost classifier with a learning rate of\n0.1, 100 trees, and a scale positive weight of 3. The model was\ntrained without label encoding, using logarithmic loss (logloss)\nas the evaluation metric and a maximum tree depth of 2. This\nconfiguration was chosen to balance model complexity and per-\nformance while addressing class imbalance effectively.\nThe Random Forest model was used with its default parame-\nters for modeling without applying any data augmentation tech-\nniques. This baseline configuration provides a reference for as-\nsessing the impact of augmentation and other parameter adjust-\nments on model performance.\nIn the data augmentation scenario, specific hyperparameter con-\nfigurations were applied to optimize the models. For SMOTE,\nwe used SMOTENC [83], which is suitable for datasets with a\nmix of categorical and continuous features. The sampling strat-\negy was set to \"minority\u201d, and the random state was specified\nas \"run\".\nFor the One-Class SVM, the gamma parameter was set to \u201cauto\u201d,\nand the kernel was configured as \"rbf\" to effectively capture\""}, {"title": "4. Results", "content": "4.1. Data Analysis\nThis stage focuses on exploratory data analysis (EDA) and fea-\nture selection. The dataset comprises sixteen features repre-\nsented in the first column of . The beach name, coun-\ncil report, beach key-identification, surf club, SLSA, state, lat-\nitude, longitude, length, orientation, embaymentisation were\nremoved from the modelling analysis as explained in Step 2\nof the framework diagram. After preprocessing the data, we\nhave six input features for the modelling stage, detailed in the\nsecond column. The final column presents the transformation\nof the features into categorized or processed variables named\nsub-features. The target variable is the presence/absence of the\nBluebottle (physalia)found at a particular location."}, {"title": "4.2. Modelling and Evaluation", "content": "We first modelled the dataset with three different classifiers,\nwhere we used the presence and absence data from the origi-"}, {"title": "4.2.1. Results without Data Augmentation", "content": "presents the results for the effect of class imbalance\nand class overlap within the dataset, showing the accuracy, F1\nscore and AUC. We observe that MLP achieved high accuracy\non both training (0.936) and test (0.935) sets. The Random\nForest and XGBoost models demonstrate higher accuracy and\nAUC scores for both training and test sets when compared to\nMLP. The problem is visible only when we review the F1 score,\nwhich is the main metric for class imbalanced problems and ap-\nplies to this problem. However, the F1 scores are almost negli-\ngible for MLP on both training (0.008) and test (0.003), indicat-\ning poor performance in capturing the minority class (bluebottle\npresence). This indicates that MLP has not been effectively cap-\nturing the minority class, possibly due to class imbalance and\nclass overlap. We notice that the F1 score for Random Forest\nis much higher than XGBoost for training data but close when"}, {"title": "4.2.2. Results with Data Augmentation", "content": "We next present the results of data augmentation techniques for\nenhancing the model performance, including methods for han-\ndling class imbalance (SMOTE and Random Undersampling)\nand the no negative approach for unreliable negative class (CT-\nGAN) to mitigate class imbalance and class overlap. Further-\nmore, we use the One-Class SVM to take an alternative ap-\nproach by assuming there is no negative data.\n compares the performance of SMOTE and Random Un-\ndersampling for addressing the class imbalance in bluebottle\nprediction, using MLP, Random Forests, XGBoost and One-\nClass SVM. We report selected performance metrics including,\nclassification accuracy, F1 score, and AUC for both training and\ntest sets, based on 30 independent model training runs. We ob-\nserve that the classification models with SMOTE improved in\ntraining performance across all three metrics (F1, AUC and ac-\ncuracy), with MLP achieving the highest accuracy, followed by\nXGBoost and then Random Forest. However, the test perfor-\nmance dropped drastically across all metrics, particularly in the"}, {"title": "4."}]}