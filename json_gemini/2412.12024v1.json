{"title": "Learning to Navigate in Mazes with Novel Layouts using Abstract Top-down Maps", "authors": ["Linfeng Zhao", "Lawson L.S. Wong"], "abstract": "Learning navigation capabilities in different environments has long been one of the major challenges in decision-making. In this work, we focus on zero-shot navigation ability using given abstract 2-D top-down maps. Like human navigation by reading a paper map, the agent reads the map as an image when navigating in a novel layout, after learning to navigate on a set of training maps. We propose a model-based reinforcement learning approach for this multi-task learning problem, where it jointly learns a hypermodel that takes top-down maps as input and predicts the weights of the transition network. We use the DeepMind Lab environment and customize layouts using generated maps. Our method can adapt better to novel environments in zero-shot and is more robust to noise.", "sections": [{"title": "Introduction", "content": "If we provide a rough solution of a problem to a robot, can the robot learn to follow the solution effectively? In this paper, we study this question within the context of maze navigation, where an agent is situated within a maze whose layout has never been seen before, and the agent is expected to navigate to a goal without first training on or even exploring this novel maze. This task may appear impossible without further guidance, but we will provide the agent with additional information: an abstract 2-D top-down map, treated as an image, that illustrates the rough layout of the 3-D environment, as well as indicators of its start and goal locations (\u201cabstract map\u201d in Figure 1). This is akin to a tourist attempting to find a landmark in a new city: without any further help, this would be very challenging; but when equipped with a 2-D map of environment layout, the tourist can easily plan a path to reach the goal without needing to explore or train excessively.\nIn our case, we are most concerned with zero-shot navigation in novel environments, where the agent cannot perform further training or even exploration of the new environment; all that is needed to accomplish the task is technically provided by the abstract 2-D map. This differs from the vast set of approaches based on simultaneous localization and mapping (SLAM) typically used in robot navigation (Thrun et al., 2005), where the agent can explore and build an accurate but specific occupancy map of each environment prior to navigation. Recently, navigation approaches based on deep reinforcement learning (RL) approaches have also emerged, although they often require extensive training in the same environment (Mirowski et al., 2017; 2018). Some deep RL approaches are even capable of navigating novel environments with new goals or layouts without further training; however, these approaches typically learn the strategy of efficiently exploring the new environment to understand the layout and find the goal, then exploiting that knowledge for the remainder of the episode to repeatedly reach that goal quickly (Jaderberg et al., 2017). In contrast, since the solution is essentially provided to the agent via the abstract 2-D map, we require a more stringent version of zero-shot navigation, where it should not explore the new environment; instead, we expect the agent to produce a near-optimal path in its first (and only) approach to the goal."}, {"title": "Related work", "content": "Navigation is widely studied in robotics, vision, RL, and beyond; to limit the scope, we focus on zero-shot navigation in novel environments, which is most relevant to this work. This excludes"}, {"title": "Problem statement", "content": "We consider a distribution of navigation tasks $p(T)$. Each task is different in two aspects: map layout and goal location. (1) Abstract map. The layout of each navigation task is specified by an abstract map. Specifically, an abstract map $m \\in R^{N \\times N}$ is a 2-D occupancy grid, where cell with 1s (black) indicate walls and 0s (white) indicate nagivable spaces. A cell does not correspond to the agent\u2019s world, so the agent needs to learn to localize itself on an abstract 2-D map (i.e., to know which part of map it is currently at). We generate a set of maps and guarantee that any valid positions are reachable, i.e., there is only one connected component in a map. (2) Goal position. Given a map, we can then specify a pair of start and goal position. Both start and goal are represented as a \u201cone-hot\u201d occupancy grid $g \\in R^{2 \\times N \\times N}$ provided to the agent. For simplicity, we use $g$ to refer to both start and goal, and we denote the provided map and start-goal positions $c = (m,g)$ as the task context."}, {"title": "Learning to navigate using abstract maps", "content": "This section presents an approach that can effectively use abstract maps (in image form) by end-to-end model-based planning based on MuZero (Schrittwieser et al., 2020). We expect the agent to be able to efficiently train on multiple maps as well as generalize to new maps.\nThis poses several technical challenges. (i) A local change in map may introduce entirely different environment structure, so we need the model and planner to adapt to the task context in a different way than conditioning on state, and not directly condition on the entire task context. (ii) During training, we can only rely on a very small proportion of training tasks (e.g., 20 of 13 \u00d7 13 maps). This requires compositional generalization from existing map patches to novel combinations of patches. (iii) The reward signal is sparse, but model learning is done jointly and purely relies on reward signal. To this end, we first introduce the idea of using a hypermodel that learns to predict weights of transition model, instead of state output directly, to tackle (i) and (ii). For challenge (iii), we use the idea from Hindsight Experience Replay (HER) (Andrychowicz et al., 2017) to reuse failure experience and also add an auxiliary loss of predicting transitions (described in Appendix A)."}, {"title": "Task-conditioned hypermodel", "content": "Our goal is to create a transition model that accurately handles various map inputs, enabling plan-ning in 3D environments with arbitrary layouts. In a single-task training schema, a straightforward approach would be to learn a parameterized transition function $f_{i}(s, a)$ for each individual map. However, we aim to leverage shared knowledge between navigation tasks, where maps often exhibit common local patterns and require the ability to generalize to recombination of known patterns."}, {"title": "Planning using a learned hypermodel", "content": "Equipped with a map-conditioned model, we use it to search for actions according to the map layout and goal location: $(a^{1},...,a^{k}) = Plan({s_{i}}, c, f_{\\phi})$. We follow MuZero Schrittwieser et al. (2020) to use Monte-Carlo tree search (MCTS) to search with the learned hypermodel $f_{\\phi}$. The planner needs to act based on different task inputs, which necessitates a task-dependent value function that"}, {"title": "Experiments", "content": "In the experiments, we assess our method and analyze its performance on DeepMind Lab (Beattie et al., 2016) maze navigation environment. We focus on zero-shot evaluation results."}, {"title": "Experimental setup", "content": "We perform experiments on DeepMind Lab (Beattie et al., 2016), an RL environment suite support-ing customizing 2-D map layout. As shown in Figure 1, we generate a set of abstract 2-D maps, and use them to generate 3-D environments in DeepMind Lab. Each cell on the abstract map cor-responds to 100 units in the agent world. In each generated map, all valid positions are reachable, i.e., there is only one connected component in the map. Given a sampled map, we then generate a start-goal position within a given distance range. Throughout each task, the agent receives the abstract map and start/goal location indicators, the joint state vector $o \\in R^{12}$ (consisting of position $R^{3}$, orientation $R^{3}$, translational and rotational velocity $R^{6}$), and reward signal $r$. The action space is {forward, backward, strafe left, strafe right, look left, look right}, with an action repeat of 10. This means that, at maximum forward velocity, the agent can traverse a 100 \u00d7 100 block in two steps, but typically takes longer because the agent may slow down for rotations.\nTraining settings We train a set of agents on a variety of training settings, which have several key options: (1) Map size. We mainly train on sets of 13 \u00d7 13, 15 \u00d7 15, 17 \u00d7 17, 19 \u00d7 19, 21 \u00d7 21 maps. One cell in the abstract map is equivalent to a 100 \u00d7 100 block in the agent's world. (2) Goal distance. During training, we generate local start-goal pairs with distance between 1 and 5 in the abstract map. (3) Map availability. For each map size, we train all agents on the same set of 20 generated maps, with different randomly sampled start-goal pairs in each episode.\nEvaluation settings We have several settings for evaluation: (1) Zero-shot transfer. We mainly study this type of generalization, where the agent is presented with 20 unseen evaluation maps, and has to navigate between randomly generated start-goal pairs of varying distances. (2) Goal distance on abstract map. We consider both local navigation and hierarchical navigation. In the local case, we evaluate on a range of distances ([1,15]) on a set of maps, while in the hierarchical case, we"}, {"title": "Zero-shot local navigation in novel layouts", "content": "For zero-shot generalization of locally trained agents, we train all four agents on 20 of 13 \u00d7 13 maps with randomly generated local start-goal pairs with distance [1,5] in each episode. We train the agents until convergence; MAH typically takes 3\u00d7 more training episodes and steps. We evaluate all agents on 20 unseen 13 \u00d7 13 maps and generate 5 start-goal pairs for each distance from 1 to 15 on each map. The results are shown in Figure 4 left. MMN and MAH generally outperforms the other two baselines. MMN has better performance especially over longer distances, both in success rate and successful-trajectory length (not shown), even though it was only trained on distances < 5. Since we compare fully trained agents, we found MMN performs asymptotically better than MAH. Additionally, as shown in Figure 4 right, we also train and evaluate MMN on larger maps from 15 x 15 to 21 \u00d7 21. Observed with similar trend to 13 \u00d7 13 maps, when trained with start-goal distance 5, the agent will find distant goals and larger maps more difficult."}, {"title": "Hierarchical navigation in novel layouts", "content": "We also performed a hierarchical navigation experiment, which requires an additional landmark oracle to generate sequences of subgoals between long-distance start-goal pairs, and evaluate the performance of hierarchical navigation. The agent is trained on 13 \u00d7 13 maps, and evaluate on 20 unseen 13 \u00d7 13 maps. On each map, we use the top-right corner as the global start position and the bottom-left corner as the global goal position, then plan a shortest path in the abstract 2-D map, and generate a sequence of subgoals with distance 5 between them; this typically results in 3 to 6 intermediate subgoals. The choice of distance 5 is motivated by our previous experiment, and"}, {"title": "Conclusion", "content": "In this work, we have presented an end-to-end model-based approach, MMN, for enabling agents to navigate in environments with novel layouts. By using provided abstract 2-D maps and start/goal information, MMN does not require further training or exploration (zero-shot). Compared to the map-conditioned model-free counterpart MAH, both approaches performed well in zero-shot navi-gation for short distances; for longer distances (with access to a landmark oracle), our model-based"}, {"title": "Further details on our approach", "content": "A.1 n-step goal relabelling: Denser reward\nJointly training a planner with learned model can suffer from lack of reward signal, especially when the model training entirely relies on reward from multiple tasks, which is common in model-based agents based on value gradients (Schrittwieser et al., 2020; Oh et al., 2017). Motivated by this, we introduce a straightforward strategy to enhance the reward signal by implicitly defining a learning curriculum, named n-step hindsight goal relabelling. This generalizes the single-step version of Hindsight Experience Replay (HER) (Andrychowicz et al., 2017) to n-step return relabeling.\nMotivation.\nAs shown in Figure 3 (right), we sample a trajectory of experience $(c_{T}, \\{s_{t}, a_{t}, r_{t}, s_{t+1}\\}_{t})$ on a specific map and goal $c_{T} = (m_{T},g_{T})$ from the replay buffer. Observe that, if the agent does not reach the goal area $S_{g}$ (a 100 \u00d7 100 cell in the agent's 3-D environment, denoted by a 2-D position $g_{T}$ on the abstract 2-D map), it will only receive reward $r_{t} = -1$ during the entire episode until timeout. In large maps, this hinders the agent to learn effectively from the current map $m_{T}$. Even if the agent partially understands a map, it would rarely experiences a spe-cific goal area on the map again. This is more frequent on larger maps in which possible navigable space is larger.\nRelabelling n-step returns.\nMotivated by single-step HER, we relabel failed goals to randomly sampled future states (visited area) from the trajectory, and associating states with the relabelled n-step return. Concretely, the task-conditioned bootstrapped n-step return is\n$G_{t}^{n} = r_{t+1} + \\gamma r_{t+2} + \u00b7\u00b7\u00b7 + \\gamma^{n}v_{T},\\quad [\\upsilon_{T}, \\pi_{T}] = g_{\\theta}(s_{t}, c_{T})$\nwhere $v_{T}$ is the state-value function bootstrapping $n$ steps into the future from the search value and conditional on task context $c_{T}$. This task-conditioned value function is asymmetric since $R^{12} = S \\neq S_{g} = R^{2}$.\nSteps. To relabel the task-conditioned bootstrapped n-step return, there are three steps, demon-strated by the blue lines from \"N-step Relabel\" box. (1) Goal (red boxes). Randomly select a reached state $s_{t} \\in R^{12}$ from the trajectories, then take the 2-D position $(x, y) \\in R^{2}$ in agent world and convert it to a 2-D goal support grid $g_{Ts}$. Then, relabel the goal in task context $c_{Ts} = (m_{T},g_{Ts})$, keeping the abstracted map and start position unchanged. (2) Reward (orange boxes). Recompute the rewards along the n-step segment. In episodic case, we need to terminate the episode if the agent can reach the relabelled goal area $g_{Ts}$, by marking \"done\" at the certain timestep or assigning zero discount after that step $t=0$ to mask the remaining segment. (3) Value (purple circles). Finally, we need to recompute the bootstrapping task-conditioned value $\\upsilon_{\\eta s}, \\pi_{\\eta s} = g_{\\theta}(s_{t}, c_{Ts})$.\nEmpirically, this strategy significantly increases the efficiency of our multi-task training by providing smoothing gradients when sampling a mini-batch of n-step targets from successful or failed tasks. It can also be applied to other multi-task agents based on n-step return."}, {"title": "Joint optimization: Multi-task value learning", "content": "Our training target has two components. The first component is based on the value gradient objective in MuZero (Schrittwieser et al., 2020; Oh et al., 2017), using relabelled experiences from proposed n-step HER. It is denoted by $L_{task}^{k}$ for step $k = 1,..., K$. However, this loss is only suitable for single-task RL.\nThus, we propose an auxiliary model prediction loss, denoted by $L_{model}$ in Figure 3 (right). The motivation is to regularize that the hypermodel $f_{\\theta}(s, a, h_{\\psi}(c_{T}))$ should predict trajectory based on the information of given abstract map and goal $c_{T}$. The objective corresponds to maximizing the mutual information between task context $c_{t}$ and predicted trajectories $\u00ee_{T}$ from the hypermodel on"}, {"title": "Robustness to map and localization errors", "content": "To further study the robustness of our method and the importance of each component, we consid-ered breaking three components in closed-loop map-based navigation: Map - (1) \u2192 Path - (2) \u2192 Environment - (3) \u2192 Map (repeat). In general, our learning-based agent is robust to these changes. To illustrate the difficulty of the problem, we considered a hard-coded strategy (hand-crafted de-terministic planner) based on perfect information of the environment (e.g., can plan on map) for comparison correspondingly: (1) known perfect maps and intermediate landmarks, (2) scaling factor (unavailable to MMN), and (3) world position on map. Since we assume that it has perfect local-ization and landmarks, the key step is to reach a landmark given current location, which consists of several procedures: (a) change the orientation to the target landmark, (b) move forward along the direction, and (c) stop at the target cell as soon as possible.\nPerturbing planning We try to break the implicit assumption of requiring perfect abstract map information. We adopt the hierarchical setting, but generating subgoals on perturbed maps, where some proportion of the map's occupancy information is flipped. In Figure 6 (left), as the perturbation level increases, MMN's performance gradually decreases, but it still navigates successfully with significant noise levels.\nPerturbing action mapping We break the implicit requirement of known scaling between map and environment. We provide the agent with randomly transformed maps with random perspective transformation, where the ratio (in both x and y directions) is different. As shown in Table 2,"}, {"title": "Perturbing location", "content": "We break the identifiability of agent position (a part of its joint state) by applying random noise to given position. We aim to show that our agent does not rely on the position to understand the map, since providing position in the agent world has no relation with localizing on abstract maps and our learning-based method can adapt to the noise. In Figure 6 (right), even though MMN is trained without noise, it tolerates some amount of noise and maintains relatively high SPL even at 50 units of noise (corresponding to 0.5 cell width). In Figure 7, we visualize the trajectories of MMN and the deterministic planner to qualitatively demonstrate MMN's robustness to noise."}]}