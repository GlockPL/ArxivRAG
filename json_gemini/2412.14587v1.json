{"title": "Spike2Former: Efficient Spiking Transformer for High-performance Image Segmentation", "authors": ["Zhenxin Lei", "Man Yao", "Jiakui Hu", "Xinhao Luo", "Yanye Lu", "Bo Xu", "Guoqi Li"], "abstract": "Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly in image segmentation tasks. The reason is that directly converting neural networks with complex architectural designs for segmentation tasks into spiking versions leads to performance degradation and non-convergence. To address this challenge, we first identify the modules in the architecture design that lead to the severe reduction in spike firing, make targeted improvements, and propose Spike2Former architecture. Second, we propose normalized integer spiking neurons to solve the training stability problem of SNNs with complex architectures. We set a new state-of-the-art for SNNS in various semantic segmentation datasets, with a significant improvement of +12.7% mIoU and 5.0\u00d7 efficiency on ADE20K, +14.3% mIoU and 5.2\u00d7 efficiency on VOC2012, and +9.1% mIoU and 6.6\u00d7 efficiency on CityScapes. Our code is available at https://github.com/BICLab/Spike2Former", "sections": [{"title": "Introduction", "content": "Spiking Neural Networks (SNNs) emulate the spatiotemporal dynamics and spike-based communication of biological neurons. The former ensures the network's representation (Maass 1997), while the spike-driven paradigm introduced by the latter allows SNNs to perform sparse computing when deployed on neuromorphic chips (Merolla et al. 2014; Davies et al. 2018), thereby benefiting from low power consumption (Roy, Jaiswal, and Panda 2019; Schuman et al. 2022). A notable example is the sensing-computing neuromorphic chip Speck, which operates at a power level as low as 0.7mW in typical visual scenes (Yao et al. 2024d).\nThe complex neuronal dynamics and binary activations make it challenging to train large-scale SNNs. It took a long time for the SNN field to effectively address this issue through surrogate gradient training (Wu et al. 2018; Neftci, Mostafa, and Zenke 2019a) and residual learning design (Fang et al. 2021; Hu et al. 2024b). Currently, SNNs have achieved commendable performance on simple image classification tasks (Yao et al. 2024b,a). Unfortunately, when it comes to complex visual tasks that require the use of sophisticated neural network architectures, SNNs fall short.\nFor instance, in image segmentation, an additional segmentation head is required alongside the backbone used for image classification, resulting in a network structure that is significantly more complex than that for classification. Simply converting complex Artificial Neural Networks (ANNs) into spiking versions or directly applying residual designs from classification tasks often leads to a notable drop in performance. Consequently, the few existing SNN models (Kim, Chough, and Panda 2022; Zhang, Fan, and Zhang 2023; Yao et al. 2024a; Su et al. 2024; Patel et al. 2021) that tackle image segmentation tasks tend to perform poorly.\nThis work explores the application of SNNs with more complex architectures to image segmentation tasks. Specifically, Mask2Former (Cheng et al. 2022) is a classic Transformer-based per-mask classification architecture consisting of three parts: a backbone network; a Feature Pyramid Network (FPN) pixel decoder with a multi-scale deformable transformer encoder block; and a transformer decoder block. Directly converting the Mask2Former architecture into a spiking version results in obvious performance degradation and non-convergence. To address this, we investigated which modules in the spiking Mask2Former contribute to the significant loss of information, where the spiking neurons in these modules nearly cease to fire.\nThe first module with severe spike degradation is the deformable attention transformer encoder block in the FPN decoder. In the vanilla Mask2Former, the query operation in the deformable attention block is inherently sparse; if the queried information consists of sparse spikes, this could result in significant information loss. To address this, we incorporate convolution blocks in encoder blocks and redesign the deformable attention blocks to preserve more effective information and energy efficiency. The second key module that leads to information loss is the final mask embedding layer. This layer is crucial as it outputs the final segmentation results; however, being at the deepest part of the network, the semantic information that reaches this point is already greatly diminished. To end this, we build an auxiliary information branch to enhance the representation of mask embedding.\nAnother challenge is that, beyond architectural design, spiking neurons inherently introduce information loss by converting continuous values into binary spikes. This issue has long plagued the SNN field, leading to the development"}, {"title": "Related Works", "content": "SNN Training\nTraining methods in Spiking Neural Networks (SNNs) mainly fall into two categories: ANN-to-SNN conversion and direct training. While ANN-to-SNN conversion inherits from traditional neural networks, it suffers from longer time steps and limited real-time processing capabilities (Li et al. 2022; Bu et al. 2022; Wu et al. 2021). Direct training with surrogate gradients (Neftci, Mostafa, and Zenke 2019b) offers better flexibility but typically yields lower performance. Inspired by recent Integer-based Leaky Integrate-and-Fire (I-LIF) neurons, which bridge the gap between training and inference through virtual timesteps, we adopt the direct training approach for its architectural flexibility (Hu et al. 2024b; Fang et al. 2021; Wu et al. 2018). Recently, Luo et al. introduced a novel spiking neuron that trains the model with Integer activation as a virtual timesteps and converts the Integer into binary spikes by expanding virtual timesteps during inference. We take this inspiration and further improve it for more complex application scenes.\nSNN Backbone Design\nSNN backbone designs can be broadly categorized into CNN-based and Transformer-based approaches. CNN-based SNNs primarily focus on improving the Spike-ResNet architecture (Zheng et al. 2021) through variations such as SEW-ResNet (Fang et al. 2021) and MS-ResNet (Hu et al. 2024b). These variations utilize diverse residual connections to alleviate performance degradation and support deeper architectures. Recently, Transformer-based methods have gained prominence in SNN backbone design. Approaches such as (Wang et al. 2023; Zhou et al. 2023; Leroux, Finkbeiner, and Neftci 2023; Yao et al. 2024b,a; Hu et al. 2024a; Yao et al. 2024c) incorporate spiking neurons into self-attention mechanisms to enhance model performance. These methods are relatively straightforward and lack complex interactions, making them easier to train within the SNN framework. Additionally, Luo et al. recently introduced a spike-driven SpikeYOLO model, adapted from YOLOv8 for object detection, which achieves competitive performance. However, to mitigate information loss during feature interactions, this method simplifies the model design, failing to fully address the challenges of applying SNNs to complex architectures.\nImage Segmentation\nIn ANNs, image segmentation can be categorized into per-pixel and per-mask classification. Per-pixel classification, often using CNNs, employs Feature Pyramid Networks (FPN) to generate segmentation masks. MaskFormer (Cheng, Schwing, and Kirillov 2021) redefined this as per-mask classification, generating binary masks and assigning them semantic classes. Mask2Former (Cheng et al. 2022) and MaskDINO (Li et al. 2023) further improve the Mask-Former with more precise refinement of learnable query. In SNNs, image segmentation remains challenging (Li et al. 2022; Kirkland et al. 2020; Patel et al. 2021). In 2022, (Kim, Chough, and Panda 2022) introduced spike-driven decoders (spike-FCN and spike-Deeplab) by converting ANN architectures to SNNs, while Spiking-CGNet (Zhang, Fan, and Zhang 2023) in 2023 developed a segmentation-specific backbone. More recently, Meta-SpikeFormer (Yao et al. 2024a) directly trained on FPN achieved competitive results on ADE20k (Zhou et al. 2017), highlighting SNN potential. However, the performance and energy efficiency of these method are poor and limit their application to various scenarios."}, {"title": "Method", "content": "Spike2Former adopts the architecture of Mask2Former, which includes a pyramid backbone, a pixel decoder with a deformable transformer encoder, a transformer decoder, and a mask embedding module. In this section, we present the improvement in the deformable transformer encoder and mask embedding module, followed by a discussion of the newly proposed spiking neuron NI-LIF.\nInformation Deficiency in Query\nQuery features are essential in transformer-based methods (Ding et al. 2023; Wang et al. 2024; Jin et al. 2023). They are"}, {"title": "Spike-driven Deformable Transformer Encoder", "content": "Spike-driven Deformable Transformer Encoder(SDTE) consists of a stack of successive blocks, including an Energy-efficient Separable Convolution(ESC) module to enhance local connections, a Spike-Driven Deformable Attention(SDDA) module that conducts deformable attention within queries, and a Channel-MLP layer to learn non-linear representations.\nEnergy-efficient Separable Convolution Blocks Yao et al. utilize a separable convolution to enhance the inductive bias. However, this design significantly increases energy consumption due to the direct connection of depthwise and pointwise convolutions. Thus, we propose adding spiking neuron before the second pointwise convolution for energy efficiency, denoted as ESC(\u00b7), and formulated as follows:\n$\\begin{aligned} U_{p w 1} &=\\operatorname{Conv}_{p w 1}(\\operatorname{SN}(U)) \\\\ U_{d w} &=\\operatorname{Conv}_{d w}\\left(\\operatorname{SN}\\left(U_{p w 1}\\right)\\right) \\\\ U_{p w 2} &=\\operatorname{Conv}_{p w 2}\\left(\\operatorname{SN}\\left(U_{d w}\\right)\\right) \\end{aligned}$\nSpike-Driven Deformable Attention The MSDeformAttn transformer encoder in Mask2Former refines features by attending to the global context, dynamically computing weights from the inputs, and utilizing deformability to adapt the receptive field size (Cavagnero et al. 2024). Although effective in ANNs, the sparse sampling strategy in deformable attention leads to information loss and unstable gradients (Fig. 2). Thus, we propose converting the attention weights into spikes instead of spiking the feature queries to better preserve the semantic information contained within the queries. Furthermore, using multi-scale features as queries significantly increases energy consumption, especially with high-resolution inputs. We recommend using single-scale deep image features with convolution blocks to enhance local connectivity, thereby improving energy efficiency while maintaining performance.\nSpecifically, for the calculation of attention weight and sampling offsets, we propose adding depthwise convolution (DWConv) to enhance the understanding of scene context. As shown in Fig. 1(B), given an input feature map $x_{g}$, the Spike-Driven Deformable Attention can be formulated as:\n$\\text { DeformableSDSA }\\left(P_{q}, x_{g}\\right)=\\sum_{g=1}^{G} \\sum_{k=1}^{K} W_{g} A_{g k} \\cdot W_{g}^{\\prime} \\cdot x_{g}\\left(P_{o}+P_{k}+\\Delta p_{g k}\\right)$"}, {"content": "where G represents the total number of aggregation groups. For the g-th group, $W_{g}$ and $W_{g}^{\\prime}$ represent the location-irrelevant projection weights. $A_{g k}$ denotes the attention weight corresponding to the k-th sampling point within the g-th group. $\\Delta p_{g k}$ represents the offset for the k-th sampling location $p_{k}$ within the g-th group. Subsequently, $W_{g}, A_{g k}$, and $\\Delta p_{g k}$ are calculated as follows:\n$\\begin{aligned} W_{g} &=\\operatorname{ESC}\\left(x_{g}\\right), \\\\ x &=\\operatorname{BN}\\left(\\operatorname{DWConv}\\left(\\operatorname{SN}\\left(x_{g}\\right)\\right)\\right), \\\\ A_{g k} &=\\operatorname{SN}\\left(\\operatorname{BN}\\left(\\operatorname{Conv}\\left(\\operatorname{SN}\\left(x_{g}\\right)\\right)\\right)\\right), \\\\ \\Delta p_{g k} &=\\operatorname{BN}\\left(\\operatorname{Conv}\\left(\\operatorname{SN}\\left(x_{g}\\right)\\right)\\right) . \\end{aligned}$\nWhere the SN represents the spiking neuron. We add spiking neuron for $A_{g k}$ to convert the attention weight into spike and maintain the effective information in feature query. Finally, we apply an ESC block to the input that has been sampled to the embedding dimension. The output from the SDTE is then fed into the SpikeFPN (Yao et al. 2024a) to generate per-pixel embedding."}, {"title": "Spike-Driven Transformer Decoder", "content": "The Spike-Driven Transformer Decoder (SDTD) contains a Spike-Driven Cross-Attention (SDCA) layer, a Spike-Driven Self-Attention (SDSA) layer, and a Channel-MLP layer. The for-"}, {"title": "mulation of SDTD can be written as:", "content": "$\\begin{aligned} Q^{\\prime} &=Q+\\operatorname{SDCA}\\left(Q, F_{i}\\right), i=1,2,3 \\\\ Q^{\\prime \\prime} &=Q^{\\prime}+\\operatorname{SDSA}\\left(Q^{\\prime}\\right), \\\\ Q^{\\prime \\prime \\prime} &=Q^{\\prime \\prime}+\\text { ChannelMLP }\\left(Q^{\\prime \\prime}\\right) . \\end{aligned}$\nwhere $Q \\in \\mathbb{R}^{N \\times C}$ are the $N$ learnable query with_with learnable positional embedding and $F_{i} \\in \\mathbb{R}^{H \\times W \\times B i}$ indicate the multi-scale feature maps obtained from the pixel decoder, with $i \\in\\{1,2,3\\}$.\nAdditionally, we replace the re-parameterization convolution (Yao et al. 2024a) with the combination of Linear and BatchNorm for energy efficiency. The formulation of SDSA can be written as:\n$\\begin{aligned} Q_{s}, K_{s}, V_{s} &=\\operatorname{SN}\\left(\\operatorname{BN}\\left(\\operatorname{Conv}\\left(\\operatorname{SN}\\left(Q_{l-1}\\right)\\right)\\right)\\right), \\ Q_{l} &=\\operatorname{BN}\\left(\\operatorname{Conv}\\left(\\operatorname{SN}\\left(Q_{s} K_{s}^{T} V_{s} * \\text { Scale }\\right)\\right) . \\end{aligned}$\nwhere the scale of SDSA can be re-parameterized into the spiking neuron's threshold. Note that the SDCA obtains the K and Vs from multi-scale feature maps $F_{i}$."}, {"title": "Spike-Driven Mask Embedding", "content": "The mask embedding module in Mask2Former (Cheng et al. 2022) uses a Multi-Layer Perceptron (MLP) to convert the per-segment embedding Q to N mask embedding $\\widehat{S}_{\\text {mask }} \\in \\mathbb{R}^{N \\times C}$, where C denotes the object class and N denotes the number of query and obtain the binary mask prediction $M \\in[0,1]$ through dot product between per-segment embedding M and per-pixel embedding $\\widehat{S}_{\\text {pixel }}$. However, the rich diversity semantic"}, {"title": "NI-LIF Spiking Neuron", "content": "The spiking neuron layer integrates spatio-temporal information into the membrane potential and then converts it into binary spikes for spike-driven computing in the following layer. Different from the image classification task, dense prediction necessitates a higher demand for numerical stability. Recent work (Luo et al. 2025) shows a performance increase in Object Detection with Integer Leaky Integrate-and-Fire (I-LIF). However, when we attempt to extend the I-LIF to a more complex architecture like Mask2Former, we found"}, {"title": "Experiment", "content": "Dataset. We conduct semantic segmentation on ADE20k (Zhou et al. 2017), CityScapes (Cordts et al. 2016), and Pascal VOC2012 (Everingham et al. 2010) datasets. The details of the training strategy are shown in Tab. 2.\nTraining setting. We present all our main semantic segmentation results in mean Intersection over Union (mIoU) under single scale inference setting. We use Meta-Spikeformer (Params:15M) (Yao et al. 2024a) as our backbone, which is pre-trained on ImageNet-1k for 200 epochs. More training details can be found in the Appendix."}, {"title": "Experiment Results", "content": "In Tab. 1, we comprehensively compare Spike2Former with other ANN and SNN methods in mIoU, parameters, and power. The proposed Spike2Former significantly improves the performance upper bound of SNNs on three public datasets. We obtain 46.3% mIoU, 75.2% mIoU, and 75.4% mIoU in ADE20k, CityScapes, and Pascal VOC2012 which is +14.3%, +9.1%, and +14.3% higher than the previous state-of-the-art SNN method (Yao et al. 2024a; Zhang, Fan, and Zhang 2023), respectively. Spike2Former also demonstrates significant advantages over the existing SNNs in terms of energy consumption: Spike2Former vs. SpikeFPN (Yao et al. 2024a): mIoU 44.6% vs. 33.6%; Power: 68mJ vs. 88.1mJ in ADE20k dataset and Spike2Former vs. SpikeFPN: mIoU 75.4% vs. 61.1%; Power 63.0mJ vs. 179.4mJ in Pascal VOC2012 dataset. Moreover, the performance gap between SNNs and ANNs is significantly narrowed. Spike2Former got +1.2% mIoU compared with MaskFormer(R50) and is competitive with the current classical ANN architecture Mask2Former(R50) in ADE20k dataset, which is 46.3% vs. 44.5% vs. 47.2% mIoU in mIoU while the energy consumption is much lower for 3.58\u00d7 and 4.80\u00d7 energy efficiency."}, {"title": "Ablation study", "content": "We conduct ablation studies on the various componects of Spike2Former to evaluate the contribution of each parts.\nSpike-Driven Deformable Transformer Encoder Tab. 3 highlights the performance of our spike-driven Deformable Transformer Encoder (SDTE). As shown in Fig. 2, directly spiking query features reduces mIoU by 3.2%, likely due to excess retention of attention weights, diminishing effective information of query. Our SDTE improves mIoU by 2.5% over SpikeFPN (Yao et al. 2024a) without Transformer encoder and by 1.1% over a vanilla Transformer. While using multi-scale features as queries, as in Mask2Former, yields 46.7% mIoU, it doubles energy consumption (68.0mJ vs. 136.5mJ), prompting us to prioritize single-scale queries for energy efficiency.\nInformation Deficiency in Query Preserving query information is crucial for effectively applying SNNs to Mask2Former. As demonstrated in Tab. 3, removing the ME-Shortcut connection leads to a significant performance drop (approximately -4.5% mIoU). Fig. 3 visualizes the average binary mask prediction for each query over the validation set of ADE20k, revealing that incorporating shortcut connections (ME-Shortcut) results in more sophisticated and dis-"}, {"title": "NI-LIF Spiking Neuron", "content": "Tab. 3 shows that directly applying I-LIF (Luo et al. 2025) to Spike2Former results in sub-optimal performance (37.9% mIoU compared to 46.3%), primarily due to information deficiency in the cross-attention layer of transformer decoder. Normalizing activations within the cross-attention layer alone yields a substantial +5.3% mIoU improvement compared with I-LIF, highlighting the need for numerical stability and precise representation of Transformer-based model. Furthermore, when applying the NI-LIF to the whole network, the performance increases 8.4%. This improvement demonstrates the adaptability of NI-LIF in complex architecture and the effectiveness of NI-LIF in reducing the quantization error and enhancing the precise representation of features.\nFurther analysis in Tab. 1 (Pascal VOC2012) reveals the impact of varying timesteps (T) and quantization steps (D). Increasing timesteps from T=1 to T=2 (with D=2) raises energy consumption from 50.6 mJ to 98.3 mJ with only 0.3%mIoU. Additionally, increasing quantization steps from D=2 to D=4 improves performance from 61.8% to 75.1% mIoU, with a moderate power increase from 50.6 mJ to 63.0 mJ. This suggests that increasing quantization steps can enhance the performance while maintaining reasonable energy consumption."}, {"title": "Conclusion", "content": "This work reduces the performance gap between Spiking Neural Networks (SNNs) and Artificial Neural Networks (ANNs) in image segmentation. The proposed Spike2Former introduces architectural innovations and spiking neuron optimizations, including two key modifications to address information deficiency. The Normalized Integer LIF (NI-LIF) mitigates information loss and enhances training stability by converting normalized integers into binary spikes. Spike2Former achieves state-of-the-art performance on three benchmark datasets, highlighting the potential of SNNs for complex segmentation tasks. Our analysis of spike degradation and information deficiency emphasizes the need to reduce information loss in SNNs for advanced architectures. This work lays a foundation for extending SNNs to dense prediction tasks with sophisticated designs."}, {"title": "Spike2Former vs. Mask2Former", "content": "Architecture design\nSpike2Former adopts the same meta-architecture as Mask2Former (Cheng et al. 2022), which comprises a backbone for image feature extraction, a Feature Pyramid Network (FPN) pixel decoder with a Multi-scale Deformable self-attention transformer encoder for generating per-pixel embeddings, a stack of transformer decoders to process object queries in relation to the image features, and an embedding module to decode the per-pixel embeddings into binary mask predictions and class embeddings. Spike2Former introduces key modifications to the Multi-Scale Deformable Self-Attention transformer encoder and the mask embedding module, while making minor adjustments to the remaining components. The following section delves into the specific changes implemented within the transformer decoder and pixel decoder.\nSpike-Driven Transformer Decoder\nTransformer Decoder Design in Mask2Former In Mask2Former, learnable object queries interact with multi-scale image features in a round robin fashion through a cross-attention mechanism. This strategy enables the queries to capture information about objects at various scales. After each Transformer decoder layer, Mask2Former employs an mask embedding module to generate binary mask predictions. These predictions are then converted into attention masks with a Sigmoid activation function and fed into the subsequent cross-attention layer, guiding the object queries to focus on regions containing segmented objects. However, this design proves less effective in the SNNs. The sparse nature of spike-based computation limits the ability of the attention mask, derived from the previous layer's binary predictions, to enhance the query's representation in the subsequent cross-attention layer. Furthermore, incorporating the mask attention can hinder the query's ability to effectively perceive and process the raw image features. Therefore, we omit this design, opting not to use mask attention within the cross-attention layers of our Spike2Former."}, {"title": "Spike-Driven Self-Attention", "content": "The spike-driven self-attention has various variants. In this work, We take the inspiration of Meta-Spike2Former (Yao et al. 2024a) and proposed a spike-driven self-attention capable of dualing with sequence feature (Object Query). In Mata-Spikeformer, the calculation of self-attention can be written as:\n$\\begin{aligned} Q_{s} &=\\operatorname{SN}\\left(\\text { RepConv }_{1}(U)\\right), \\\\ K_{s} &=\\operatorname{SN}\\left(\\operatorname{RepConv}_{2}(U)\\right), \\\\ V_{s} &=\\operatorname{SN}\\left(\\operatorname{RepConv}_{3}(U)\\right), \\\\ \\operatorname{SDSA}\\left(Q_{s}, K_{s}, V_{s}\\right) &=\\operatorname{SN}\\left(Q_{s}\\left(K_{s}^{T} V_{s}\\right)\\right), \\\\ U^{\\prime} &=U+\\operatorname{RepConv}_{4}\\left(\\operatorname{SDSA}\\left(Q_{s}, K_{s}, V_{s}\\right)\\right) . \\end{aligned}$\nwhere U represents the input membrane potential, and RepConv() denotes the re-parameterization convolution operation with a kernel size of 3 \u00d7 3.\nWhile this type of convolution has demonstrated significant energy efficiency gains with high-resolution images, we"}, {"title": "Spike-Driven Cross-Attention", "content": "The calculation of spike-driven cross-attention are similar to the self-attention operation. Moreover, the Ks and Vs in cross-attention are obtained from the multi-scale image features generated from the pixel decoder. We calculate the spike-driven cross-attention as follow:\n$\\begin{aligned} Q_{s} &=\\operatorname{SN}\\left(\\operatorname{BN}\\left(\\operatorname{Conv}\\left(\\operatorname{SN}\\left(U_{l-1}\\right)\\right)\\right)\\right), \\\\ K_{s}, V_{s} &=\\operatorname{SN}\\left(\\operatorname{BN}\\left(\\operatorname{Conv}\\left(\\operatorname{SN}\\left(F_{i}\\right)\\right)\\right)\\right), i=1,2,3, \\\\ U^{\\prime} &=\\operatorname{BN}\\left(\\operatorname{Conv}\\left(\\operatorname{SN}\\left(Q_{s} K_{s}^{T} V_{s} * \\text { Scale }\\right)\\right) \\end{aligned}$\nWhere $F_{1}, F_{2}, F_{3}$ indicates the multi-scale feature maps in $\\{\\frac{H}{4}, \\frac{H}{8}, \\frac{H}{16}\\}$, The scale of SDCA can be re-parameterized into the spiking neuron's threshold."}, {"title": "Feature Pyramid Network Pixel Decoder", "content": "In Spike2Former, we introduce a subtle yet efficient modification to the FPN pixel decoder. Specifically, we substitute the 3 \u00d7 3 channel convolution in the original Spike-FPN with a more computationally efficient 3 \u00d7 3 depth-wise convolution. This adjustment aims to reduce the energy consumption associated with generating per-pixel embeddings.\nAs evidenced by the results presented in Table 1, this alteration yields a notable improvement in energy efficiency. We observe a significant reduction in energy consumption"}, {"title": "Panoptic segmentation", "content": "Panoptic segmentation is a more challenging segmentation task that provides comprehensive scene understanding by simultaneously performing semantic and instance segmentation. The core requirement is to assign every pixel a semantic label, such as \"road\" or \"tree,\" while also distinguishing between different instances of objects within the same category, like multiple \"cars.\" This task is particularly challenging because it must effectively combine instance-level and pixel-level segmentation into a single, coherent output, a contrast to semantic segmentation, which does not differentiate between individual instances of the same class. Panoptic segmentation is essential for advanced applications like autonomous driving, where precise scene interpretation is critical. To the best of our knowledge, no panoptic segmentation results have been reported in SNNs. In this work, we present the first panoptic segmentation results in SNNs on the widely-used COCO2017 panoptic dataset. We evaluate the performance with Panopic Quality(PQ) in 'All' object, 'Thing', and 'Stuff', denoting as $P Q_{A l l}, P Q_{Thing}, and P Q_{Stuff}$, separately.\nCOCO Panoptic COCO panoptic is one of the most widely used datasets for panoptic segmentation, containing 133 categories (80 \"thing\" categories with instance-level annotations and 53 \"stuff\" categories). It includes 118,000 images for training and 5,000 images for validation. All images are sourced from the COCO2017 dataset. We employ large-scale jittering augmentation as described in (Du et al. 2021) and crop the images to 1024 \u00d7 1024 during training."}, {"title": "Theoretical Energy Consumption", "content": "The power consumption of ANNs and SNNs can be estimated using the following equations:\n$\\begin{aligned} &E_{A N N}=O^{2} \\times C_{i n} \\times C_{o u t} \\times k_{w} \\times E_{M A C}, \\\\ &E_{S N N}=(T \\times D) \\times R_{L I F} \\times O^{2} \\\\ &\\times C_{i n} \\times C_{o u t} \\times k^{2} \\times E_{A C} . \\end{aligned}$\nHere, O represents the output size, $C_{i n}$ and $C_{o u t}$ denote the number of input and output channels respectively, k is the kernel size, $R_{L I F}$ represents the firing rate of the LIF spiking neuron, T is the timestep, and D is the upper limit of integer-activation during training.\nWe adopt a widely used energy consumption evaluation method for SNNs(Yao et al. 2024b,a; Wang et al. 2023), assuming a 32-bit floating-point implementation on a 45nm technology. In this context, $E_{M A C}$ = 4.6pJ and $E_{A C}$ = 0.9pJ represent the energy consumed per MAC (Multiply-ACcumulate operations) and AC (ACcumulate operations) operation respectively (Yao et al. 2024b).\nAs shown in Eq. 13, the reduced power consumption of SNNs stems from their reliance on sparse addition operations, reflected in the use of EAC."}, {"title": "Training Setting", "content": "We conducted our experiments using the popular code-bases MMSegmentation (Contributors 2020) and MMDetection (Chen et al. 2019), which are widely used for semantic"}, {"title": "Panoptic segmentation", "content": "Tab. 2 presents a comprehensive comparison of Spike2Former against state-of-the-art ANN and SNN methods on the COCO2017 panoptic segmentation benchmark, considering metrics such as mIoU, parameter count, and power consumption. For this evaluation, we re-implemented Spike-FPN (Yao et al. 2024a) within our Spike-mmdetection codebase.\nOur proposed Spike2Former significantly advances the performance limits of SNNs for this task. We achieve 30.9% PQ, 31.7% PQ, and 75.4% PQ for the \"All,\" \"Things,\" and \"Stuff\" categories, respectively. These results represent substantial improvements of +16.4%, +21.9%, and +8.1% over the previous SNN state-of-the-art, Spike-FPN (Yao et al. 2024a). Furthermore, Spike2Former exhibits considerable advantages in energy efficiency compared to existing SNNs. Specifically, Spike2Former attains a PQAll of 30.9% with a power consumption of 222.6mJ, while SpikeFPN (Yao et al. 2024a) achieves 33.6% PQAll at the cost of 448.0mJ. While there remains a performance gap between"}, {"title": "Ablation Study on T and D", "content": "We conducted an ablation study to investigate the effects of varying timesteps (T) and quantization steps (D). As illustrated in Table 3, increasing T from 1 to 2 (while keeping D constant at 2) leads to a considerable rise in energy consumption, from 50.6 mJ to 98.3 mJ. Conversely, increasing D from 2 to 4 (with T fixed at 1) yields a substantial performance boost, raising the mIoU from 61.8% to 75.1%, while incurring only a moderate power increase from 50.6 mJ to 63.0 mJ. This suggests that increasing the quantization steps is an effective strategy for enhancing performance while maintaining reasonable energy efficiency.\nNotably, we observed a significant performance drop when the virtual timestep was set to 1. We hypothesize that this is because a pure 0/1 spike sequence is not well-suited for the Mask2Former architecture, adversely affecting numerical stability and hindering effective interactions between features. Therefore, we recommend using a virtual timestep of 2~4 to normalize the output into integer spikes for complex models."}, {"title": "Visualization", "content": "As shown in Fig. 2,Fig. 3, and Fig. 4, we visualize the semantic segmentation results of Spike2Former on the ADE20k, CityScapes, and Pascal VOC2012 datasets. Our proposed Spike2Former demonstrates remarkable capabilities in identifying small objects, accurately classifying object categories, and producing precise segmentation along object boundaries."}]}