{"title": "Quantifying and Enabling the Interpretability of CLIP-like Models", "authors": ["Avinash Madasu", "Yossi Gandelsman", "Vasudev Lal", "Phillip Howard"], "abstract": "CLIP is one of the most popular foundational models and is heavily used for many vision-language tasks. However, little is known about the inner workings of CLIP. To bridge this gap we propose a study to quantify the interpretability in CLIP like models. We conduct this study on six different CLIP models from OpenAI and OpenCLIP which vary by size, type of pre-training data and patch size. Our approach begins with using the TEXTSPAN algorithm and in-context learning to break down individual attention heads into specific properties. We then evaluate how easily these heads can be interpreted using new metrics which measure property consistency within heads and property disentanglement across heads. Our findings reveal that larger CLIP models are generally more interpretable than their smaller counterparts. To further assist users in understanding the inner workings of CLIP models, we introduce CLIP-InterpreT, a tool designed for interpretability analysis. CLIP-InterpreT offers five types of analyses: property-based nearest neighbor search, per-head topic segmentation, contrastive segmentation, per-head nearest neighbors of an image, and per-head nearest neighbors of text.", "sections": [{"title": "Introduction", "content": "CLIP [24], a large-scale vision-language (VL) model, is extensively used as a foundational model for tasks such as video retrieval, image generation, and segmentation [21, 20, 2, 11, 18]. Given its widespread use, it is imperative to understand the inner workings of CLIP. Towards this end, we introduce a systematic methodology for quantifying the interpretability in CLIP like models. We perform this study on six types of CLIP models: ViT-B-16, ViT-B-32, and ViT-L-14 from OpenAI, as well as ViT-B-16, ViT-B-32, and ViT-L-14 from OpenCLIP [17].\nFirst, we identify interpretable structures within the individual heads of the last four layers of the model, using a set of text descriptions. To accomplish this, we employ the TEXTSPAN algorithm [13], which helps us find the most appropriate text descriptions for each head. After identifying these text descriptions, we assign labels to each head, representing the common property shared by the descriptions. This labeling process is carried out using in-context learning with ChatGPT. We begin by manually labeling five pairs of text descriptions and their corresponding property labels, which serve as examples. These examples are then used to prompt ChatGPT to assign labels for the remaining heads. This approach systematically connects the attention heads to the properties they learn during large-scale pre-training, offering insights into the roles of individual heads.\nWe introduce two metrics, the entanglement score and the association score, to quantify interpretability in CLIP models. These metrics are specifically designed to assess how easily properties can be linked"}, {"title": "Related work", "content": "Early research on interpretability primarily concentrated on convolutional neural networks (CNNs) due to their intricate and opaque decision-making processes [29, 25, 26, 12, 16]. More recently, the interpretability of Vision Transformers (ViT) has garnered significant attention as these models, unlike CNNs, rely on self-attention mechanisms rather than convolutions. Researchers have focused on task-specific analyses in areas such as image classification, captioning, and object detection to understand how ViTs process and interpret visual information [8, 10, 23, 28, 6, 9]. One of the key metrics used to measure interpretability in ViTs is the attention mechanism itself, which provides insights into how the model distributes focus across different parts of an image when making decisions [5, 4]. This has led to the development of techniques that leverage attention maps to explain ViT predictions. Early work on multimodal interpretability, which involves models that handle both visual and textual inputs, probed tasks such as how different modalities influence model performance [3, 22] and how visual semantics are represented within the model [15, 19]. Aflalo et al. [1] explored interpretability methods for vision-language transformers, examining how these models combine visual and textual information to make joint decisions. Similarly, Stan et al. [27] proposed new approaches for interpreting vision-language models, focusing on the interactions between modalities and how these influence model predictions. Our work builds upon and leverages the methods introduced by Gandelsman et al. [13, 14] to interpret attention heads, neurons, and layers in vision-language models, providing deeper insights into their decision-making processes."}, {"title": "Methodology", "content": "In this section, we outline the methodology used in our analysis, beginning with an explanation of the TEXTSPAN algorithm. We then describe how we extend this algorithm to apply it across all attention heads in multiple CLIP models using in-context learning.\nThe TEXTSPAN algorithm [13] is designed to decompose individual attention heads by associating them with corresponding text descriptions. It requires an initial set of text descriptions broadly capturing the concepts in an image. The algorithm starts by generating two matrices: $C$, which contains the outputs for a specific head (denoted as $(l, h)$), and $R$, which includes representations of candidate text descriptions projected onto the span of $C$. In each iteration, TEXTSPAN calculates the dot product between each row of R and the outputs in C to identify the row with the highest variance, known as the \"first principal component.\" Once identified, this component is projected away from all rows, and the process is repeated to find additional components. The projection step ensures that each new component adds variance that is orthogonal to the earlier components, thereby isolating distinct aspects of the text descriptions relevant to each head.\nTEXTSPAN is effective at identifying text descriptions that are most similar to a given head. To label the common property shared by these text descriptions, we employ in-context learning with ChatGPT. We begin by manually labeling properties for five heads, which serve as examples. These examples are then used to prompt ChatGPT to generate labels for the remaining heads. This approach allows us to systematically label the properties associated with each head."}, {"title": "Quantifying interpretability in CLIP models", "content": "Given the numerous CLIP-like models which are available, a key question arises: can we quantify how interpretable these models are? To answer this, we introduce a set of metrics specifically designed to assess how easily properties can be linked to each layer and head within the models. These metrics provide a way to measure the interpretability of the models, helping us understand how clearly different properties are represented."}, {"title": "Entanglement score", "content": "We define the entanglement score as the mean frequency at which heads exhibit the same TEXTSPAN label (property). A higher score suggests that the property is shared among multiple heads or layers, making it more challenging to distinctly associate the property with a particular head. If the score is zero, then all the heads are disentangled from each other.\nNotably, larger CLIP models exhibit significantly lower entanglement scores compared to their base model counterparts, suggesting a more distinct association of properties to specific heads and layers in these larger models. This reduced entanglement is an important result as it highlights the improved interpretability and clarity in larger models.\nAdditionally, the table highlights another key observation: OpenAI's smaller models are less entangled than the models from OpenCLIP, indicating a potential difference in how these models manage property associations at a smaller scale. Conversely, the entanglement scores for OpenAI's larger models are higher than expected, showing more entanglement than their smaller versions, which could suggest a trade-off in complexity and property association within these models. These findings emphasize the importance of model size and architecture in determining the level of entanglement,"}, {"title": "Association score", "content": "Previously, we explored how the TEXTSPAN labels assigned to different attention heads in a model can reflect the model's interpretability. This led to the question of whether all the text descriptions linked to a given head genuinely correspond to the assigned property label. To investigate this, we manually evaluated how frequently the text descriptions align with the property label. We introduced the \"association score\" to quantify this, which is defined as the average frequency of heads with at least three text descriptions matching the property label.\nThe data clearly shows that larger CLIP models have more heads consistently focusing on a single property, making them more interpretable. This observation is consistent with the results from the entanglement scores, which lead to a similar conclusion.\nFrom Tables 1 and 2, it is evident that larger models have heads that learn properties independently of other heads while consistently focusing on a single property. This independence allows for the easy isolation of head-property pairs, enhancing the interpretability of the model's individual heads. Additionally, OpenCLIP's smaller models have lower association scores compared to their OpenAI counterparts, while the opposite is true for larger models. This pattern mirrors the findings seen in the entanglement scores, reinforcing the insights gained from both metrics."}, {"title": "CLIP-InterpreT", "content": "In the previous sections, we outlined a systematic approach to quantifying interpretability in CLIP models. However, the true value of interpretability lies in its accessibility to users. To address this, we introduce a new application called CLIP-InterpreT, a comprehensive tool designed to empower users with insights into the inner workings of CLIP-like models. This application provides an intuitive interface, as shown in Figure 1. In the user interface, users can easily upload an image of their choice and select one of six CLIP models for analysis. These models include ViT-B-16, ViT-B-32, and ViT-L-14 from OpenAI, as well as ViT-B-16, ViT-B-32, and ViT-L-14 from OpenCLIP [17]. The application offers five distinct types of interpretability analyses, which we will discuss next."}, {"title": "Property-based nearest neighbors search", "content": "In this analysis, we demonstrate that the layers and heads of CLIP models can be characterized by specific attributes such as colors, locations, animals, and more. Since multiple heads can learn the same attribute, we combine the representations of these heads, which have been labeled using in-context learning, to create unified image representations. To evaluate the similarity between an input image and a set of candidate images from the ImageNet validation dataset [7], we calculate the cosine similarity between the unified representation of the input image and each candidate image. The top four images with the highest similarity scores are then retrieved, showing which images are most similar to the input image based on the learned attributes."}, {"title": "Per head topic Segmentation", "content": "In contrast to Gandelsman et al. [13], we study topic segmentation for each individual head in various CLIP-like models using CLIP-InterpreT. This analysis focuses on projecting a segmentation map corresponding to an input text onto the reference image. The segmentation map is computed using various heads of the model to highlight the specific properties that each head captures, which allows us to visualize how different heads focus on different elements within the image based on the given text description."}, {"title": "Contrastive Segmentation", "content": "The contrastive segmentation analysis provided in CLIP-InterpreT contrasts the visual interpretations of a single image when described by two different text inputs. By projecting the segmentation maps corresponding to each input onto the original image, we reveal how the model visually interprets and differentiates the input texts."}, {"title": "Per-head Nearest Neighbors of an Image", "content": "CLIP-InterpreT can also visualize the nearest neighbors retrieved for an input image based on similar-ity scores computed using a single attention head. Certain heads are specialized in capturing specific image properties, allowing us to leverage their intermediate representations for better interpretability. By calculating the similarity of direct contributions from individual heads, we can identify images that closely match the input image in terms of specific aspects, such as colors or objects."}, {"title": "Per-head Nearest neighbors of a Text Input", "content": "To determine whether CLIP-like models can link image representations to a given text input, CLIP-InterpreT provides the ability to retrieve the nearest neighbors for a given input text using different attention heads. We utilize the top TEXTSPAN outputs identified for each head."}, {"title": "Conclusion", "content": "In this work, we conducted an analysis to quantify the interpretability of six different CLIP-like models from OpenAI and OpenCLIP, varying in size, pre-training data, and patch size. We used the TEXTSPAN algorithm and in-context learning to decompose individual attention heads into specific properties and then evaluated interpretability using newly developed metrics. Our findings show that larger CLIP models are generally easier to interpret than smaller ones. To help users explore these insights, we introduced CLIP-InterpreT, a tool offering five types of interpretability analyses."}]}