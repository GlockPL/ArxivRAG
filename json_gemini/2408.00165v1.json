{"title": "Non-convolutional Graph Neural Networks", "authors": ["Yuanqing Wang", "Kyunghyun Cho"], "abstract": "Rethink convolution-based graph neural networks (GNN)\u2014they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation. Here, we design a simple graph learning module entirely free of convolution operators, coined random walk with unifying memory (RUM) neural network, where an RNN merges the topological and semantic graph features along the random walks terminating at each node. Relating the rich literature on RNN behavior and graph topology, we theoretically show and experimentally verify that RUM attenuates the aforementioned symptoms and is more expressive than the Weisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level classification and regression tasks, RUM not only achieves competitive performance, but is also robust, memory-efficient, scalable, and faster than the simplest convolutional GNNs.", "sections": [{"title": "1 Introduction: Convolutions in GNNS", "content": "Graph neural networks (GNNs) [1, 2, 3, 4, 5]\u2014neural models operating on representations of nodes (V) and edges (E) in a graph (denoted by G = {V,E},E \u2286 V \u00d7 V, with structure represented by the adjacency matrix Aij = 1[(vi,vj) \u2208 \u0190])\u2014have shown promises in a wide array of social and physical [6, 7, 8, 9, 10] modeling applications. Most GNNs follow a convolutional scheme, where the D-dimensional node representations X \u2208 R|V|\u00d7D are aggregated based on the structure of local neighborhoods:\nX' = \u00c2X.\nHere, A displays a unique duality\u2014the input features doubles as a compute graph. The difference among convolutional GNN architectures, apart from the subsequent treatment of the resulting intermediate representation X', typically amounts to the choices of transformations (A) of the original adjacency matrix (A)\u2014the normalized Laplacian for graph convolutional networks (GCN) [1], a learned, sparse stochastic matrix for graph attention networks (GAT) [11], powers of the graph Laplacian for simplifying graph networks (SGN) [12], and the matrix exponential thereof for graph neural diffusion (GRAND) [13], to name a few. For all such transformations, it is easy to verify that"}, {"title": "2 Related works: ways to walk on a graph", "content": "Walk-based GNNs. RAW-GNN ([22], compared and outperformed in Table 9) also proposes walk-based representations for representing node neighborhoods, which resembles our model without the anonymous experiment (Wu in Equation 5). CRaWl ([23, 24], outperformed in Table 3) also incorporates a similar structural encoding for random walk-generated subgraphs to feed into an iterative 1-dimensional convolutional network. AWE ([25], Table 3) and Wang et al. [26], like ours, use anonymous experiments for graph-level unsupervised learning and temporal graph learning, respectively. More elaborately, [27] and AgentNet ([28], Table 3) use agent-based learning on random walks and paths on graphs.\nRandom walk kernel GNNs. In a closely related line of research, RWK ([29], Table 3) employs the reproducing kernel Hilbert space representations in a neural network for graph modeling and GSN ([30], Table 3) counts the explicitly enumerated subgraphs to represent graphs. The subgraph counting techniques intrinsically require prior knowledge about the input graph of a predefined set of node and edge sets. For these works, superior expressiveness has been routinely argued, though usually limited to a few special cases where the WL test fails whereas they do not, and often within the unlabelled graphs only.\nMore importantly, focusing mostly on expressiveness, no aforementioned walk-based or random walk kernel-based GNNs address the issue of over-smoothing and over-squashing in GNNs. Some of these works are also de facto convolutional, as the random walks are only incorporated as features in the message-passing scheme. Interestingly, most of these works are either not applicable to, or have not been tested on, node-level tasks. In the experimental Section 5, we show that RUM not only outperforms these baselines on most graph-level tasks (Table 3) but also competes with a wide array of state-of-the-art convolutional GNNs on node-level tests (Table 2). Moreover, random walk kernel-based architectures, which explicitly enumerate random walk-generated subgraphs are typically much slower than WL-equivalent convolutional GNNs, whereas RUM is faster than even the simplest variation of convolutional GNN (Figure 4).\nState-of-the-art methods to alleviate oversmoothing. Stochastic regularization. DropEdge ([31], Figure 2) regularizes the smoothing of the node representations by randomly disconnecting edges. Its associated Dirichlet energy indeed decreases slower, though eventually still diminishes as the number of layers increases. Graph rewiring. [32, 33] and GPR-GNN ([34], Appendix Table 9) rewire the graph using personalized page rank algorithm [35] and generalized page rank on graphs, respectively. Similar to JKNet [36], they mitigate over-smoothing by allowing direct message passing between faraway nodes. Constant-energy methods. Zhao and Akoglu [37], Rusch et al. [38] constrain the pair-wise distance or Dirichlet energy among graphs to be constant. Nevertheless, the non-decreasing energy does not necessarily translate to better performance, as they sometimes come with the sacrifice of expressiveness, as argued in Rusch et al. [18]. Residual GNNs. Residual connection [39] can naturally be added to the GNN architecture, such as GCNII ([40], Table 2), to restraint activation to be similar to the input to allow deep networks. They however can make the model less robust to perturbations in the input. In sum, these works have similar, if not compromised expressiveness compared to a barebone GCN."}, {"title": "3 Architecture: combining topologic and semantic trajectories of walks", "content": "Random walks on graphs. An unbiased random walk w on a graph G is a sequence of nodes w = (\u03c5\u03bf, \u03c51,...) with landing probability:\nP(vj|(Vo,..., Vi\u22121)) = 1[(vi, vj) \u2208 Eg]/D(vi),\nwhere D(vi) = \u2211 Aij is the degree of the node vi. Walks originating from or terminating at any given node v can thus be easily generated using this Markov chain. We record the trajectory of embeddings associated with the walk as wx(w) = (Xi) = (X0, X1, . . ., X\u2081). In this paper, we only consider finite-long l-step random walk |w| = l \u2208 Z+. In our implementation, the random walks are sampled ad hoc during each training and inference step directly on GPU using Deep Graph Library [41] (see Appendix Section A). Moreover, the walk considered here is not necessarily a path, as repeated traversal of the same node vi = vj, i \u2260 j is not only permitted, but also crucial to effective topological representation, as discussed below."}, {"title": "Anonymous experiment.", "content": "We use a function describing the topological environment of a walk, termed anonymous experiment [42], wu(w) : R\u00b9 \u2192 R\u00b9 that records the first unique occurrence of a node in a walk (Appendix Algorithm C). To put it plainly, we label a node as the number of unique nodes insofar traversed in a walk if the node has not been traversed, and reuse the label otherwise. Practically, this can be implemented using any tensor-accelerating framework in one line (w is the node sequence of a walk) and trivially parallelized 2: (1*(w[..., :, None]==w[..., None,:])).argmax(-1)\nUnifying memory: combining semantic and topological representations. Given any walk w, we now have two sequences wx (w) and wu (w) describing the semantic and topological (as we shall show in the following sections) features of the walk. We project such sequential representations onto a latent dimension to combine them (illustrated in Table 1):\nh(w) = f(x(\u03c9x(W)), \u03c6u(Wu(W))),\nwhere \u201e : Rl\u00d7D \u2192 RD maps the sequence of semantic embeddings generated by a l-step walk to a fixed D-dimensional latent space, qu : R\u00b9 \u2192 RDu maps the indicies sequence to another latent space Du, and f : RD \u2295RDu \u2192 RD combines them. We call Equation 5 the unifying memory of a random walk. Subsequently, the node representations can also be formed as the average representations of l-step (l being a hyperparameter) walks terminating (for the sake of gradient aggregation) at that node:\n\u03c8(v) = \u03a3 p(w)h(w),\n{w},|w|=1,\u03c9\u03b9=\u03c5\nwhich can be stochastically sampled with unbiased Monte Carlo gradient and used in downstream tasks as such node classification and regression. We note that this is the only time we perform SUM or MEAN operations. Unlike other GNNs incorporating random walk-generated features (which are sometimes still convolutional and iterative), we do not iteratively pool representations within local neighborhoods. The likelihood of the data written as:\nP(y G, X) = \u03a3 p(w)p(y|G, X, w)\n{w},|w|=\u03b9,\u03c9\u03b9=\u03c5\nThe node representation can be summed\n\u03a8(G) = \u03a3 \u03c8(\u03c5)\nVEVCG\nto form global representations for graph classification and regression. We call & in Equation 6 and \u03a8 in Equation 8 the node and graph output representations of RUM.\nLayer choices. Obvious choices to model f include a feed-forward neural network after concatenation, and \u03a6\u03b1, \u03c6u recurrent neural networks (RNNs). This implies that, different from most convolutional GNNs, parameter sharing is natural and the number of parameters is going to stay constant as the model incorporates a larger neighborhood. Compared to dot product-based, transformer-like modules [43], RNNs not only have linear complexity (see detailed discussions below) w.r.t. the sequence length but also naturally encodes the inductive bias that nodes closer to the origin have stronger impact on the representation. The gated recurrent unit (GRU)[21] variant is used everywhere in this paper. Additional regularizations are described in Appendix Section B.1.\nRuntime complexity. To generate random walks for one node has the runtime complexity of O(1), and for a graph O(|V|), where |V| is the number of nodes in a graph G = {V,E}. To calculate the anonymous experiment, as shown in Appendix Algorithm C, has (1) complexity (also see Footnote 2). If we use linear-complexity models, such as RNNs, to model \u03a6\u03b1, \u03c6u, the overall complexity is O(|V|lkD) where l is the length of the random walk, k the samples used to estimate Equation 6, and D the latent size of the model (assumed uniform). Note that different from convolutional GNNs, RUM does not depend on the number of edges |E| (which is usually"}, {"title": "4 Theory: RUM as a joint remedy.", "content": "We have insofar designed a new graph learning framework-convolution-free graph neural networks (GNNs) that cleanly represent the semantic (wx) and topological (wu) features of graph-structured data before unifying them. First, we state that RUM is permutation equivariant,\nRemark 1 (Permutation equivariance). For any permutation matrix P, we have\nPX(G) = X(P(G)),\nwhich sets the foundation for the data-efficient modeling of graphs. Next, we theoretically demonstrate that this formulation jointly remedies the common pathologies of the convolution-based GNNs by showing that: (a) the topological representation wu is more expressive than convolutional-GNNs in distinguishing non-isomorphic graphs; (b) the semantic representation w\u2081 no longer suffers from over-smoothing and over-squashing."}, {"title": "4.1 RUM is more expressive than convolutional GNNs.", "content": "For the sake of theoretical arguments in this section, we assume that in Equation 5:\nAssumption 2. \u0424, \u0424\u0438, \u0192 are universal and injective.\nAssumption 3. Graph G discussed in this section is always connected, unweighted, and undirected.\nAssumption 2 is easy to satisfy for feed-forward assuneural networks [45] and RNNs [46]. Composing injective functions, we remark that h(w) is also injective w.r.t. wx(w) and wu(w); despite of Assumption 3, our analysis can be extended to disjointed graphs by restricting the analysis to the connected regions in a graph. Under such assumptions, we show, in Remark 8 (deferred to the Appendix), that & is injective, meaning that nodes with different random walks will have different distributions of representations (v1) \u2260 \u03c8(v2). We also refer the readers to the Theorem 1 in Micali and Zhu [42] for a discussion on the representation power of anonymous experiments on unlabelled graphs. Combining with the semantic representations and promoting the argument from a node level to a graph level, we arrive at:\nTheorem 4 (RUM can distinguish non-isomorphic graphs). Up to the Reconstruction Conjecture [47], RUM with sufficiently long l-step random walks can distinguish non-isomorphic graphs satisfying Assumption 3.\nThe main idea of the proof of Theorem 4 (in Appendix Section D.2) involves explicitly enumerating all possible non-isomorphic structures for graphs with 3 nodes and showing, by induction, that if the theorem stands for graph of N 1 size it also holds for N-sized graphs. We also show in Appendix Section D.1 that a number of key graph properties such as cycle size (Example 8.1) and radius (Example 8.2) that convolutional GNNs struggle [48, 49] to learn can be analytically expressed using wu. As these are solely functions of wr, they can be approximated arbitrarily well by universal approximators. These examples are special cases of the finding that RUM is stricly more expressive than Weisfeiler-Lehman isomorphism test [14]:\nCorollary 4.1 (RUM is more expressive than WL-test). Two graphs with N nodes G1, G2 labeled as non-isomorphic by Weisfeiler-Lehman isomorphism test is the necessary, but not sufficient condition that the representation resulting from RUM with walk length N + 1 are also different.\n\u03a8(G1) \u2260 \u03a8(G2)"}, {"title": "4.2 RUM alleviates over-smoothing and over-squashing", "content": "Over-smoothing refers to the phenomenon where the node dissimilarity (e.g., measured by Dirichlet energy in Equation 2) decreases exponentially and approaches zero with the repeated rounds of message passing. Cai and Wang [17] relates Dirichlet energy directly with the convolutional operator:\nLemma 3.1 from Cai and Wang [17].\nE((1 \u2013 \u2206)X) \u2264 (1 \u2013 \u03bb)\u00b2E(X)\nwhere X is the smallest non-zero eigenvalue of \u2206, the normalized Laplacian of a graph.\nFree of convolution operators, it seems only natural that RUM does not suffer from this symptom (Figure 2). We now formalize this intuition by first restricting ourselves to a class of non-contractive mappings for f in Definition 5.\nDefinition 5. A map f is non-contractive on region \u03a9 if \u2203 \u2208 [1, +\u221e) such that |f(x) - f(y)| \u2265 a||x - y||, \u2200x, y \u2208 \u03a9.\nA line of fruitful research has been focusing on designing non-contractive RNNs [51, 52], and to be non-contractive is intimately linked with desirable properties such as preserving the long-range information content and non-vanishing gradients. From this definition, it is easy to show that, for each sampled random walk in Equation 5, the Dirichlet energy is greater than its input. One only needs to verify that the integration in Equation 6 does not change this to arrive at:\nLemma 6 (RUM alleviates over-smoothing.). If \u00a2x, f are non-contractive w.r.t. all elements in the sequence, the expected Dirichlet energy of the corresponding RUM node representation in Equation 6 is greater than its initial value\n\u0395(\u0395(\u03c8(X))) \u2265 E(X).\nThis implies that the expectation of Dirichlet energy does not diminish even when l \u2192 +\u221e, as it is bounded by the Dirichlet energy of the initial node representation, which is consistent with the trend shown in Figure 2, although the GRU is used out-of-box without constraining it to be explicitly non-contractive.\nRUM alleviates over-squashing is deferred to Appenxix Section B.2, where we verify that the inter-node Jacobian |\u018fX(1+1)/8X(0) | decays slower as the distance between u, v grows vis-\u00e0-vis the convolutional counterparts. Briefly, although RUM does not address the information bottleneck with exponentially growing reception field (the 1/(\u00c2l+1)uv term in Equation 16), it nevertheless can have a non-vanishing (nor exploding) gradient from the aggregation function (|x|)."}, {"title": "5 Experiments", "content": "On a wide array of real-world node- and graph-level tasks, we benchmark the performance of RUM to show its utility in social and physical modeling. Next, to thoroughly examine the performance of RUM, we challenge it with carefully designed illustrative experiments. Specifically, we ask the following questions in this section, with Q1, Q2, and Q3 already theoretically answered in Section 4:\nQ1: Is RUM more expressive than convolutional GNNs? Q2: Does RUM alleviate over-smoothing? Q3: Does RUM alleviate over-squashing? Q4: Is RUM slower with convolutional GNNs? Q5: Is RUM robust? Q6: How does RUM scale up to huge graphs? Q7: What components of RUM are contributing most to the performance of RUM?\nReal-world benchmark performance. For node classification, we benchmark our model on the popular Planetoid citation datasets [50], as well as the coauthor [53] and co-purchase [54] datasets common in social modeling. Additionally, we hypothesize that RUM, without the smoothing operator, will perform competitively on heterophilic datasets [55]\u2014we test this hypothesis. For graph classification, we benchmark on the popular TU dataset [56]. We also test the graph regression performance on molecular datasets in MoleculeNet [57] and Open Graph Benchmark [58]. In sum, RUM almost always outperforms, is within the standard deviation of, the state-of-the-art architectures, as shown in Tables 2, 3, 4, 5, as well as in Tables 6, 7, 8, 9 moved to the Appendix due to space constraint.\nOn sparsity: the subpar performance on the Computer dataset. The most noticeable exception to the good performance of RUM is that on the Computer co-purchase [54] dataset, where RUM is outperformed even by GCN and GAT. This dataset is very dense with an average node degree (|E|/|V|) of 18.36, the highest among all datasets used in this paper. As the variance of the node embedding (Equation 6) scales with the average degree, we hypothesize that dense graphs with very high average node degrees would have high-variance representations from RUM.\nOn the other hand, RUM outperforms all models surveyed in two large-scale benchmark studies on molecular learning, GAUCHE [59] and MoleculeNet [57]. The atoms in the molecules always have a degree of 2 ~ 4 with intricate subgraph structures like small rings. This suggests the utility of unifying memory in chemical graphs and furthermore chemical and physical modeling."}, {"title": "6 Conclusions", "content": "We design an innovative GNN that uses an RNN to unify the semantic and topological representations along stochastically sampled random walks, termed random walk with unifying memory (RUM) neural networks. Free of the convolutional operators, our methodology does not suffer from symptoms characteristic of Laplacian smoothing, including limited expressiveness, over-smoothing, and over-squashing. Most notably, our method is more expressive than the Weisfeiler-Lehman isomorphism test and can distinguish all non-isomorphic graphs up to the reconstruction conjecture. Thus, it is more expressive than all of convolutional GNNs equivalent to the WL test, as we demonstrate theoretically in Section 4. RUM is significantly faster on GPUs than even the simplest convolutional GNNs (Section 5) and shows superior performance across a plethora of node- and graph-level benchmarks.\nLimitations. Very dense graphs. As evidenced by the underwhelming performance of the Computer [54] dataset and discussed in Section 5, RUM might suffer from high variance with very dense graphs (average degree over 15). Tottering. In our implementation, we have not ruled out the 2-cycles from the random walks, as that would require specialized implementation for walk generation. This, however, would reduce the average information content in a fixed-length random walk (known"}, {"title": "A Experimental details", "content": "Code availability. All architectures, as well as scripts to execute the experiment, are distributed open-source under MIT license at https://anonymous.4open.science/r/rum-834D/. Core dependencies of our package include PyTorch [67] and Deep Graph Library [41].\nHyperparameters. All models are optimized using Adam [73] optimizer and SiLU [74] activation functions. 4 random walk samples are drawn everywhere unless specified. Other hyperparameters\u2014learning rate (10-5 ~ 10-2), hidden dimension (32 ~ 64), L2 regularization strength (10-8 ~ 10-2), walk length (3 ~ 16), temperature for Lconsistency (0 ~ 1), coefficient for Lconsistency (0 ~ 1), coefficient for Lself, and dropout probability\u2014are tuned using the Ray platform [75] with the default Ax [76] search algorithm with 1000 trails or 24 hours tuning budget on a Nvidia A100\u00ae GPU."}, {"title": "B Additional technical details", "content": "B.1 Self-supervised regularization.\nThe stochasticity encoded in our model naturally affords it with some level of regularization. Apart from using the consistency loss (Lconsistency) used in Feng et al. [68] for classifications, we further regularize the model by using the RNNs in 4 to predict the semantic representation of the next node on the walk given wu and jointly maximize this likelihood:\n\u1f62xi+1 = g({Wx1,Wx2,..., Wx\u2081 }, Wu|0);\nLself(0) = log P(x+1|0),\nwhere g(10) is modeled as the sequence output of the RNN x in Equation 5. The total loss is modeled as a combination of:\nL(0) = -log P(y|G,X,0) + Lself + Lconsistency\nB.2 RUM attenuates over-squashing\nSimilarly, we can show that if the composing neural networks defy the vanishing gradient problem (w.r.t. the input) [77], the sensitivity analysis in Equation 3 [20] has a lower bound for RUM.\nLemma 7 (RUM attenuates over-squashing). If $x, f have lower-bounded derivatives, the inter-node Jacobian for nodes u, v separated by a shortest path of length l, RUM with walk length l also has a lower bound:\n|\u03b4X(l+1)/\u03b4X(0)|\u2265 \u221a\u03bb\u2207f|(\u00c2\u00b9)uv,\nwhere Aij = Aij/; Aij is the degree-normalized adjacency matrix.\nLike the upper bound in Equation 3, this lower bound is also controlled by the power of the (nor- malzied) adjacency matrix, albeit the absence of self-loop will result in a slightly looser bottleneck.\nThe term (Al)uv corresponds to the probability of the shortest path among all possible walks as a product of inverse node degrees (see Equation 4). There is no denying that the lower bound is still controlled by the power of the adjacency matrix, which corresponds to the exponentially growing receptive fields. One can also argue that, without prior knowledge, the contribution of the sensitivity analysis by the power of the graph adjacency matrix can never be alleviated, since there are always roughly 1/(\u00c2l+1)uv (assuming uniform node degree) structurally equivalent nodes. Nevertheless, since or is not necessarily an iterative function, we alleviate the over-squashing problem by eliminating the power of the update function gradient term.\nNow, we plug in the layer choices of a GRU [21] unit. Its success, just like that of long short- term memory (LSTM) [78], can be attributed to the near-linear functional form of the long-range gradient. The term \u2207 is controlled by a sequence of sigmoidal update gates, which can be optimized to approach 1 (fully open). If we ignore the gradient contribution of X to these gates, the non-linear activation function has only been applied exactly once on X; therefore, the gradient |\u03b4X(1+1)/\u03b4X(0) | is neither rapidly vanishing nor exploding."}, {"title": "C Additional results", "content": "Remark 8 (Inequality in distribution). For two nodes V1, V2 with distribution of random walks terminating at v1, v2 not equal in distribution p(wu(W1)) \u2260 p(wu(W2)) or p(wx(W1)) \u2260 p(\u03c9x(W2)), the node representations in Equation 6 are also different \u03c8(v1) \u2260 \u03c8(v2).\nOne way to construct h function in Equation 5 is to have h(w) positive only where p(wu(W1)) > p(wu(W2)); the same thing can be argued for wr. In other words, one only needs to prove two walks terminating at two nodes p(wu(W1)) \u2260 p(wu(W2)) or p(wx(W1)) \u2260 p(wx(w2)) are not equal in distribution to verify that RUM can distinguish two nodes. Conversely, we can also show that p(\u03c9\u03b9(W1)) = p(wu(W2)) implies that v1, v2 are isomorphic without labels\u2014 we refer the readers to the Theorem 1 in Micali and Zhu [42] for a discussion on reconstructing unlabelled graphs using anonymous experiments."}, {"title": "D Missing mathematical arguments.", "content": "D.1 Examples of Theorem 4\nExample 8.1 (Cycle detection.). A k-cycle Ck is a subgraph of G consisting of k nodes, each with degree two. The existence of k-cycle can be determined by:\n1(Ck \u2286 G) = 1[P(Wxj+1 = Wx0, Wx\u2081 \u2260 wxj,\u2200i < j) > 0]\nExample 8.2 (Diameter.). The diameter, dg of graph G which equals the length of the longest shortes path in G, can be expressed as\n\u03b4\u03b5 = argmax Wx"}]}