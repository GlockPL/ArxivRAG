{"title": "A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain Sequential Recommendation", "authors": ["Yunzhe Li", "Junting Wang", "Hari Sundaram", "Zhining Liu"], "abstract": "Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions in unseen domains without the need for additional training or fine-tuning, making it particularly valuable in data-sparse environments where traditional models struggle. Recent advancements in large language models (LLMs) have greatly improved ZCDSR by leveraging rich pretrained representations to facilitate cross-domain knowledge transfer. However, a key challenge persists: domain semantic bias, which arises from variations in vocabulary and content focus across domains. This misalignment leads to inconsistencies in item embeddings and hinders generalization.\nTo address this issue, we propose a novel framework designed to enhance LLM-based ZCDSR by improving cross-domain alignment at both the item and sequential levels. At the item level, we introduce a generalization loss that promotes inter-domain compactness-aligning embeddings of similar items across domains-while maintaining intra-domain diversity to preserve unique item characteristics. This prevents embeddings from becoming overly generic while ensuring effective transferability. At the sequential level, we develop a method for transferring user behavioral patterns by clustering user sequences in the source domain and applying attention-based aggregation for target domain inference. This dynamic adaptation of user embeddings allows effective zero-shot recommendations without requiring target-domain interactions.\nComprehensive experiments across multiple datasets and domains demonstrate that our framework significantly improves sequential recommendation performance in the ZCDSR setting. By mitigating domain bias and enhancing the transferability of sequential patterns, our method provides a scalable and robust approach for achieving more effective zero-shot recommendations across domains.", "sections": [{"title": "1 Introduction", "content": "Zero-shot cross-domain sequential recommendation (ZCDSR) extends beyond conventional recommendation tasks [17, 30] by addressing the critical challenge of making accurate predictions in unseen domains where no prior interaction data exists. This capability is indispensable for recommendation systems in dynamic environments-such as new markets, product categories, or emerging user segments-where collecting domain-specific interaction data is often impractical, and it is particularly relevant in modern applications like e-commerce, streaming platforms, and online education, which frequently encounter rapid changes and the introduction of new domains. Traditional models, which rely heavily on domain-specific training data, often struggle to adapt in such scenarios, resulting in poor recommendation quality and a diminished user experience. In contrast, ZCDSR plays a crucial role in overcoming this limitation by enabling recommendation systems to make effective predictions without domain-specific data, ensuring they remain robust and relevant in diverse and evolving contexts.\nZCDSR represents a more challenging setting compare to few-shot adaptation, as it requires the system to generalize effectively without any prior exposure to the target domain, which fundamentally challenges the model's generalizability.\nExisting works in zero-shot recommendation [5, 7, 12, 36] leverage various forms of metadata-such as textual descriptions, images, and item popularity-as mediums to transfer knowledge from source domains to unseen target domains, thereby enhancing model adaptability and generalization. The rise of large language models (LLMs) has further advanced this approach, offering powerful tools for capturing rich item semantics and user intent through pretrained language representations. This enables predictions in sparse or unseen domains by encoding item metadata. We classify LLM-based recommendation methods (LLM4Rec) into two main categories: (1) studies like [1, 16, 44], which use LLMs as direct recommenders that generate predictions directly from textual item descriptions or user queries, and (2) approaches like [18, 20, 28, 29], which employ LLMs as feature encoders to generate reusable semantic embeddings for downstream recommendation models.\nWhile using LLMs as feature encoders has proven effective for enhancing zero-shot recommendation performance by generating semantic embeddings, LLM-based methods suffer from a critical limitation: domain semantic bias. This bias stems from the distinct and often non-overlapping vocabularies, content focuses, and priorities across different domains, hindering the ability of LLM-encoded embeddings to generalize effectively. As a result, achieving cross-domain semantic alignment between the source and target domains remains a significant challenge, restricting the full potential of LLMs in zero-shot scenarios. For example, Table 1 illustrates this issue by comparing item descriptions from two contrasting domains within the Amazon Review dataset: Industrial & Scientific and Video Games. Descriptions in the Industrial & Scientific domain highlight technical specifications and emphasize features such as magnet strength, durability, and functional completeness. These details cater to outdoor enthusiasts and treasure hunters, reflecting the domain's focus on utility and performance. On the other hand,, the Video Games domain prioritizes aesthetics and user experience, with descriptions centered on controller customization, grip comfort, and visual appeal. Terms like faceplate, anti-slip, and soft touch, are prevalent, resonating with gamers seeking personalization and ergonomic design.\nThis divergence in semantic focus highlights the domain-specific nature of item descriptions, which can significantly impact LLM-generated embeddings. When item embeddings trained on one domain are applied to another, the model may produce suboptimal recommendations due to the misalignment of feature representations. For instance, an LLM trained on the technical, performance-oriented descriptions of the Industrial & Scientific domain may fail to capture the subjective, experiential attributes emphasized in the Video Games domain. Such misalignment underscores the need for improved methods to bridge semantic gaps across domains, ensuring more robust and generalizable recommendation systems.\nOur Insight: Domain-specific semantic bias exists in LLM-based zero-shot sequential recommenders. Therefore, it is crucial to strike a balance between generalization across domains and the preservation of domain-specific characteristics. Specifically, aligning item embeddings across domains while retaining the unique attributes of each domain is key to achieving effective cross-domain recommendation. Moreover, user behaviors tend to exhibit similarities across domains [27, 37], particularly in terms of the temporal or relational structures that govern item progression within a sequence, such as dynamic relationships and preference transitions. By capturing and leveraging these sequential patterns, we can enhance the recommendation process and make more accurate predictions, even in the absence of target-domain interaction data.\nPresent Work: We propose a novel model-agnostic LLM-based Recommendation Generalization framework, LLM-RecG. LLM-RecG comprehensively addresses domain bias by capturing transferable sequential patterns while preserving domain-specific nuances, (e.g., distinct vocabularies, interaction behaviors, and content focuses), ensuring accurate recommendations in unseen target domains. Specifically, we introduce a novel training objective that balances inter-domain compactness and intra-domain diversity at the item level. Inter-domain compactness ensures that item embeddings are closely aligned across different domains, facilitating knowledge transfer and reducing domain-specific biases. In contrast, intra-domain diversity maintains the fine-grained distinctions among items within the same domain, ensuring the model does not overfit to dominant source-domain features.\" These two complementary objectives balance cross-domain alignment and domain-specific nuance, resulting in improved generalization and more accurate recommendations across domains. Furthermore, we transfer sequential patterns from the source domain to the target domain. Unlike item-level embedding generalization, sequential patterns capture how items are interacted with in specific sequences, providing critical context for user preferences. By clustering source user sequences into patterns, LLM-RecG learns to aggregate information from relevant sequential patterns through attention mechanisms during target domain inference. This enables dynamic adaptation of user embeddings without requiring target-domain interaction data, ensuring effective zero-shot recommendations. In summary, our main contributions are as follows:\nDomain Semantic Bias-aware Framework: To the best of our knowledge, we are the first to identify and address the issue of domain semantic bias in LLM-based zero-shot cross-domain sequential recommendation (ZCDSR). Previous work typically focuses on leveraging domain-agnostic representations and overlooks the domain semantic bias, which significantly impact the transfer process. We, on the other hand, lay the groundwork for more effective knowledge transfer across domains by analyzing the sources of domain semantic bias and highlighting its effect on the performance of zero-shot recommendation tasks, which in turn leads to more accurate and reliable zero-shot predictions in unseen domains. Our results, both qualitative and quantitative, demonstrate that addressing this bias is crucial for ZCDSR and significantly enhances the model's generalizability."}, {"title": "2 Problem Definition", "content": "In this section, we formally define the zero-shot cross-domain sequential recommendation (ZCDSR) task and introduce the notations used throughout this work.\nLet Ds = {Vs, Us, Xs } represent the source domain, where Vs = {1, 2, ..., v | } denotes the set of items, and Us = {u\u2081, u2, ..., uU} is the set of users in domain Ds. Each item is associated with metadata x, forming Xs, the metadata set mapped one-to-one to the items. The goal of sequential recommendation in the source domain is to learn a scoring function that predicts the next item v, for a user us, given their interaction history Hj = {0,0,0,1,0,-1}\nFormally, the scoring function is defined as: F (v,t | Hj, Xs), where Foutputs a ranking score for candidate items based on the user's historical interactions and the item metadata.\nZero-shot Cross-Domain Sequential Recommendation: Given a new target domain Dt = {Vt, Ut, Xt}, where V\u2081 \u2229 Vs = 0 and Ut Us = 0, the objective is to produce a scoring function F' for Dt without training on D\u2081 directly. This setting assumes no overlap in items or users between the source and target domains, making it a purely zero-shot transfer scenario.\nWe specifically focus on the zero-shot transfer setting rather than a few-shot setting, as it presents a more challenging yet crucial problem to study. Addressing this challenge is essential and fundamental for improving the scalability and adaptability of sequential recommendation systems across diverse domains."}, {"title": "3 Methodology", "content": "In this section, we introduce LLM-RecG, a model-agnostic generalization framework designed for sequential recommenders. LLM-RecG addresses the challenge of zero-shot cross-domain sequential recommendation (ZSCDSR), a more difficult task than few-shot adaptation. Unlike few-shot approaches, ZSCDSR aims to significantly enhance the scalability and adaptability of sequential recommenders, allowing them to perform effectively across domains without requiring extensive retraining.\nAt its core, LLM-RecG is built upon a semantic sequential framework (\u00a7 3.1), which generalizes existing sequential recommenders to ZSCDSR by incorporating an LLM-based semantic projection layer (\u00a7 3.1.1). To further strengthen the generalization process, we propose a dual-level generalization strategy (\u00a7 3.2 and \u00a7 3.3), which mitigates domain-specific discrepancies and improves the generalizability of the model in zero-shot scenarios."}, {"title": "3.1 Semantic Sequential Framework", "content": "The semantic sequential framework forms the backbone of LLM-RecG. It encodes item metadata into rich, high-dimensional embeddings and then mapping these embeddings into a space that captures sequential dependencies between items using any exisiting sequential recommenders, making it adaptable across domains. The use of an LLM-based semantic projection layer (\u00a7 3.1.1) ensures that the model can process and integrate item-specific features in a way that captures the latent relationships between items in a sequence, allowing us to not only retain the domain-specific semantics but also make it transferable to a target domain in a zero-shot setup."}, {"title": "3.1.1 LLM-based semantic projection layer", "content": "We leverage the power of LLMs to capture rich semantic information from item metadata. Specifically, we employ LLMs to extract semantic embeddings that encode contextual information from textual descriptions. For each item ve Vs in the source domain, we aggregate its associated metadata (e.g.,, title, features, and description) into a unified textual description x, following a predefined template as shown in Table 2. We then adopt &, a LLM-based semantic encoder to generate a high-dimensional semantic embedding e\u015fem of the item :\ne\u015fem = &(x), (1)\nwhere e\u015fem \u2208 Rdh and dh denotes the dimensionality of the semantic embedding space. Then, we apply a projection layer that maps the embeddings into a lower-dimensional latent space to reduce dimensionality and align the embeddings with user interaction patterns, enhancing the model's ability to capture sequential dependencies:\ne = Wpesem + bp, (2)\nwhere e \u2208 Rd\u0131, Wp \u2208 Rd\u0131\u00d7dn, and bp \u2208 Rd\u0131 are learnable parameters of the projection layer, with d\u2081 < dh."}, {"title": "3.1.2 Sequential Dependencies Modeling", "content": "Once the semantic embeddings for the items are obtained, we turn to sequential recommenders to capture the inherent sequential dependencies between items in a user's interaction history. We denote the sequence of low-dimensional embeddings corresponding to user us's interaction history H = {1,0,1,\u00b7\u00b7\u00b7,,|H,|} in the source domain as:\nH = {e},0, e,1,\u2026\u2026\u2026, ej,|H; | }, (3)\nwhere H \u2208 Rhxd\u0131 is the sequence of embeddings for user u in the source domain. This sequence is then passed to any sequential recommendation models SeqRec, forming the core of our model-agnostic approach. These models process the sequence and output"}, {"title": "3.2 Item-Level Generalization", "content": "At the item level, we propose that semantic embeddings of items should satisfy the following two key properties:\nDefinition 3.1 (Inter-Domain Compactness). Inter-domain compactness ensures that item embeddings from different domains are closely aligned in the embedding space, reducing domain-specific biases and enabling effective knowledge transfer.\nDefinition 3.2 (Intra-Domain Diversity). Intra-domain diversity ensures that item embeddings within the same domain remain distinct, capturing fine-grained variability and preventing representation collapse."}, {"title": "3.2.1 Inter-Domain Compactness", "content": "As defined, inter-domain compactness aligns item embeddings from different domains by minimizing the entropy of the similarity distribution between embeddings and the centers of other domains. Let {cald \u2208 {Ds, Dt}} represent the domain centers, and {e, ev\u2208 Vs, v \u2208 V\u2081} denote the item embeddings from the source and target domains.\nThe domain center ca for a domain d (d \u2208 {Ds, Dt }) is defined as the mean of all embeddings belonging to that domain:\ncd = \\frac{1}{|V_d|} \\sum_{v_d \\in V_d} v_d, (8)\nwhere Va represents the set of items in domain d, and |Va| is the number of items in that domain.\nTo focus specifically on inter-domain alignment, the inter-domain compactness loss is formulated as:\nLinter = \u03a3\u03a3 Lid log Qid (9)\nvi\u2208Vd\u2208 {Ds, Dt}\nd\u2260di\nwhere di indicates the domain of item vi, and Qid is the probability of embedding ed being associated with domain d. This probability is computed as:\nQid = \\frac{exp (cos(ed, c_d)/\u03c4)}{\\sum_{d'\\e{D_s, D_t})} exp (cos(e_d, c_{d'})/\u03c4)} (10)\nd'\u2260di\nwhere cos() represents cosine similarity, and \u03c4 > 0 is a temperature parameter that controls the sharpness of the similarity distribution. Minimizing Linter encourages embeddings to align with the centers of other domains, enhancing inter-domain compactness and reducing domain-specific biases. However, it alone is insufficient to ensure diversity within the same domain. To address this, we propose an additional objective for intra-domain diversity, which prevents embeddings from collapsing into overly similar representations and preserves fine-grained variability within each domain."}, {"title": "3.2.2 Intra-Domain Diversity", "content": "As defined, intra-domain diversity ensures distinctiveness among embeddings within the same domain, effectively capturing item-level variability and preventing representation collapse. For a domain d\u2208 {Ds, Dt} with item embeddings {edvd \u2208 Va}, the intra-domain diversity loss is formulated as:\nL_{intra} = - \\frac{1}{|V_d|} \\sum_{d \\in \\{D_s, D_t\\}} \\sum_{v_i \\in V_d} \\sum_{v_j \\in V_d} P_{ij}log P_{ij}, (11)\nwhere Pij denotes the similarity-based probability between items and v within domain d. This probability is computed using cosine similarity with temperature scaling:\nPij = \\frac{exp (cos(e^d_i, e^d_j)/\u03c4)}{\\sum_{v^u \\in V_d} exp (cos(e^d_i, e^u)/\u03c4)} (12)\nwhere \u03c4 > 0 regulates the sharpness of the similarity distribution. Lower \u03c4 sharpens the distribution, amplifying differences, while higher t smooths it, promoting more uniform probabilities. Maximizing this entropy-based objective discourages embeddings within the same domain from collapsing into similar points, fostering richer intra-domain representation.\nThe generalization loss combines intra-domain diversity and inter-domain compactness:\nLgen = -\u03b1 Lintra + \u03b2Linter, (13)\nwhere \u03b1 > 0 and \u03b2 > 0 control the balance between the two terms. This formulation promotes variability within domains while aligning embeddings across different domains, mitigating domain-specific biases. To balance the relative influence, \u03b2 is scaled relative to \u03b1 based on the number of domains |{Ds, Dt}| and the total number of items |N|:\n\u03b2 = \u03b1 \\frac{N}{|{D_s, D_t}|3} (14)\nThis scaling ensures that inter-domain compactness contributes appropriately without overpowering intra-domain diversity\u00b9.\nThe generalization loss Lgen complements the primary recommendation objective Lrec during training. The overall objective function is:\nLtotal = Lrec + Lgen. (15)\nTo preserve semantic information and enhance generalization, we introduce an additional projection layer. This layer maps semantic embeddings into another latent space, complementing the initial projection. The final item embeddings are obtained by merging the outputs of both projection layers."}, {"title": "3.3 Sequence-level Generalization", "content": "Item-level generalization focuses on aligning static item embeddings across domains, but it overlooks the sequential dependencies in user behavior, which are crucial for understanding preferences and predicting future interactions. A sequential pattern abstracts the temporal or relational structure governing item progression within a sequence, capturing dynamic relationships and transitions in user preferences. These patterns highlight commonalities in user behavior across domains, making them particularly valuable for knowledge transfer. To bridge the gap left by item-level generalization, we propose leveraging these sequential patterns from the source domain to enhance user sequence representations during target domain inference."}, {"title": "3.3.1 Sequential Pattern Extraction from Source Domain", "content": "We extract sequential patterns that encapsulate common user behavioral trajectories by clustering the sequence embeddings of users from the source domain. Let y \u2208 Rdi denote the sequence embedding of user us in the source domain, obtained by encoding the interaction history h = {1, 2, \u2026\u2026\u2026, h3 | }.\nThe set of k sequential patterns S = {$1, $2, ..., sk} is extracted by applying k-means clustering [26] over the sequence embeddings:\nS = k-means({y} | u; \u2208 Us}), (16)\nwhere Us represents the set of users in the source domain. Each pattern si \u2208 Rdi corresponds to the centroid of a cluster, representing shared sequential behaviors observed in the source domain."}, {"title": "3.3.2 Soft Sequential Pattern Attention for Target Sequences", "content": "For zero-shot inference in the target domain, target user sequences are encoded to produce sequence embeddings y for user u, based on their interaction history h = {1,0,...}. }. Since target domain interaction data is unavailable during training, the model leverages source domain sequential patterns to guide the recommendation process. Rather than selecting the closest pattern directly, a soft attention mechanism is applied to aggregate information from multiple patterns.\nThe similarity between the target sequence embedding y and each sequential pattern s; is computed using cosine similarity:\nsi = cos(y, si) = \\frac{y^T s_i}{||y||||s_i||} (17)\nThese similarity scores are normalized using the softmax function to obtain attention weights over the sequential patterns:\n\u03b1aj,i = \\frac{exp(s_1)}{\\sum_{i=1}^k exp(s_1)} (18)\nThe attended sequential pattern representations for user u is computed as the weighted sum of the patterns:\n= \\sum_{i=1}^k \u03b1_{ji} s_i (19)\n3.  3.3.3 Fusion of User Embedding and Pattern Representation. To integrate source domain patterns while preserving target-specific information, the target user embedding y is concatenated with the attended pattern representation :\nf = [y;], (20)\nwhere [\u00b7;\u00b7] denotes concatenation. The fused representation f\u2208 R2di is then projected back into the original embedding space:\ng = Wff, (21)\nwhere Wf \u2208 Rd\u0131\u00d72d\u0131 is a learnable projection matrix. The resulting fused embedding g\u02bb captures both the user's target-specific preferences and the transferable sequential patterns from the source domain. It is subsequently used for next-item prediction."}, {"title": "4 Experiments", "content": "We conduct extensive experiments on three real-world datasets to evaluate the performance of LLM-RecG, following the settings outlined in \u00a7 3. To guide our analysis, we address the following research questions (RQs): RQ1: How well does LLM-RecG enhance baseline models in both in-domain and zero-shot scenarios? RQ2: What is the effectiveness of each component in LLM-RecG? RQ3: How does the key parameter impact performance? RQ4: How does LLM-RecG affect the alignment and uniformity of item embeddings across domains?"}, {"title": "4.4 Overall Comparison", "content": "To address RQ1-how well LLM-RecG enhances baseline models in both in-domain and zero-shot scenarios-we analyze its performance under these settings. The results highlight the effectiveness of LLM-RecG in improving recommendation accuracy and robustness, as detailed below.\n4.  4.1 ZCDSR performance. For the ZCDSR task, we follow the experimental settings outlined in Section 3.1. From Table 4, we observe the following key insights: a) Across all models and domain pairs, our method achieves substantial improvements over variants that directly use LLM-based semantic embeddings, with average gains exceeding 20%. This demonstrates the effectiveness of the proposed generalization framework in enhancing zero-shot recommendation performance while addressing domain semantic bias. b) Our method also improves robustness to domain shifts. For instance, GRU4Rec-sem shows significant performance degradation depending on the relationship between source and target domains, with performance dropping by over 31% in the VG domain when sourced from IS or MI. In contrast, our approach mitigates this variability, delivering more consistent and reliable results across diverse domain pairs. c) GRU4Rec and its variants consistently underperform compared to SASRec and Bert4Rec. This disparity highlights the interaction between model architecture and the challenges posed by zero-shot recommendations. GRU4Rec's inability to fully align with or utilize these rich embeddings exacerbates its performance gap. In contrast, more expressive architectures like SASRec and Bert4Rec can better integrate LLM-based embeddings, benefiting from their stronger capacity to capture and transfer sequential patterns.\n5.  4.2 In-domain Sequential Recommendation Performance Comparison. In the in-domain scenario, we conduct experiments using three base models and their corresponding variants. The results, presented in Table 5, lead to the following observations: a) Variants incorporating semantic information consistently outperform their base models, highlighting that semantic features effectively enhance in-domain recommendation performance. b) The generalization-based variant significantly outperforms the model that directly integrates LLM-based embeddings. This indicates that the generalization method effectively mitigates domain overfitting caused by the gap between LLMs and upstream sequential recommendation models. c) The improvement in performance is greater for SASRec compared to Bert4Rec. We attribute this to SASRec's unidirectional architecture, which benefits more significantly from the generalization method as it compensates for SASRec's limited capacity to fully integrate contextual semantic features."}, {"title": "4.5 Ablation Study", "content": "To address RQ2-how the different components of LLM-RecG contribute to its performance-we conduct an ablation study to analyze the impact of each generalization module under the ZCDSR setting. Table 6 shows the performance of BERT4Rec on VG and MI with IS as the source domain. The results highlight the role of item-level and sequential-level generalization in improving ZCDSR performance.\nBelow, we summarize the findings for each variant:\n(a) Remove Item level Generalization (IG). When the item-level generalization is removed, the performance on both VG and MI drops noticeably (e.g., R@10 decreases from 35.81 to 32.49 on VG and from 30.10 to 26.49 on MI). This demonstrates the importance of IG in aligning item embeddings across domains and improving transferability.\n(b) Remove Intra-domain Diversity (ID). Ablating intra-domain diversity (ID) causes the most significant performance degradation across all metrics, with R@10 dropping to 28.26 on VG and 20.93 on MI, even worse than \"w/ -sem\" variant. This emphasizes that maintaining fine-grained distinctions within each domain is crucial for preventing over-simplified representations, which can lead to poor generalization and reduced recommendation accuracy.\n(c) Remove Inter-domain Compactness (IC). Removing inter-domain compactness (IC) also results in a notable performance drop, particularly on VG (e.g., R@10 decreases from 35.81 to 32.74). This indicates that aligning item embeddings across domains is essential for enabling effective knowledge transfer. However, the impact of removing IC is less severe than removing ID, suggesting that intra-domain diversity plays a more critical role in retaining domain-specific nuances.\n(d) Remove Sequential level Generalization (SG). Ablating sequential-level generalization (SG) reduces performance across both VG and MI, with R@10 dropping to 33.17 and 28.63, respectively. This demonstrates that leveraging transferable sequential patterns is vital for capturing user behavior dynamics in the target domain. The smaller performance drop compared to IG and ID suggests that SG complements item-level generalization rather than acting as a standalone solution."}, {"title": "4.6 Sensitivity Analysis", "content": "To address RQ3-how the key parameter a impacts performance in ZCDSR tasks-we analyze its effect on the zero-shot performance of BERT4Rec-RecG across the IS and MI datasets. The hyperparameter a balances the recommendation loss and the generalization loss, making it a critical factor for optimizing ZCDSR performance. The results are visualized in Figure 2, and the following observations are made:\nA moderate value of a = 0.001 achieves the best performance across both IS and MI domains, with significant improvements in Recall@10 and NDCG@10. This suggests that a = 0.001 effectively balances the recommendation loss, which ensures accurate predictions, and the generalization loss, which aligns item embeddings across domains. However, as a increases beyond 0.001, performance declines consistently, indicating that overly large a values overemphasize the generalization loss, leading to suboptimal embeddings and reduced recommendation accuracy. The consistent trends across IS and MI further highlight a's robustness and its importance as a key parameter for optimizing generalization in ZCDSR tasks."}, {"title": "4.7 Visualization Analysis", "content": "To address RQ4-how LLM-RecG affects the alignment and uniformity of item embeddings across domains-we analyze the item embeddings using t-SNE visualizations, as shown in \u00a7 4.6. These visualizations provide insights into the impact of our generalization framework on embedding alignment and generalization. The LLM-based semantic embeddings demonstrate a strong capability to distinguish items by leveraging prior knowledge. However, this strong semantic separation can hinder the generalization of the downstream recommender system. By incorporating the proposed generalization methods, the embeddings produced by the -RecG variants exhibit greater uniformity and are harder to distinguish across domains. This improved generalization enhances the model's robustness and adaptability."}, {"title": "5 Related Work", "content": "LLMs' ability to generalize and understand text and long-term context has attracted interest in improving recommender systems. Research in this area is typically divided into generative and discriminative approaches based on LLMs' roles [40].\nGenerative recommender systems frame recommendation as natural language generation problems, enhancing interactivity and explainability. For instance, Rec Interpreter [43], ChatRec [9], and He et al. [12] use prompting strategies to convert user profiles or historical interactions into inputs for LLMs in conversational recommendations. ONCE [21] improves content-based recommendations by fine-tuning open-source LLMs and leveraging prompts with closed-source models. TALLRec [1] fine-tunes Alpaca [35] using self-instruct data to provide binary feedback (\"yes\" or \"no\"). GenRec [16] takes a different approach by directly generating the target item for recommendation. Additionally, CCF-LLM [23] encodes semantic and collaborative user-item interactions into hybrid prompts, fusing them via cross-modal attention for prediction.\nUnlike generative systems, discriminative recommender systems do not require LLMs to respond during the inference stage, making them more practical and efficient. Early approaches, such as UniSRec [15], fine-tuned BERT [4] to associate item descriptions with transferable representations across recommendation scenarios. Harte et al.[10] improved sequential recommendation by replacing conventional model embeddings with LLM embeddings. LLM-ESR[22] further enhanced sequential recommendation by integrating LLM-generated semantic embeddings via retrieval-augmented self-distillation, avoiding additional inference costs. Additionally, [8, 39] leveraged LLMs to augment multi-domain recommendation models with scenario-specific knowledge, though their performance depends heavily on sample quality and domain expertise. In contrast, our method eliminates the reliance on domain knowledge, offering a more general and universal framework for cross-domain sequential recommendation."}, {"title": "5.1 Cross-Domain Sequential recommendation", "content": "Unlike conventional sequential recommendation methods [17, 19, 33] and cross-domain recommendation approaches [24, 32], cross-domain sequential recommendation (CDSR) requires models to leverage sequential dependencies to enhance recommendation performance across multiple domains. Early works, such as \u03c0-net [25] and other approaches [34, 38, 42], employ recurrent neural networks to capture sequential information among overlapping users. More recently, C\u00b2DSR [3] introduces a contrastive Infomax loss with graph neural networks to capture both inter-sequence and intra-sequence item relationships. Unlike these methods, which assume full user overlap across domains, AMID [41] devises a multi-interest debiasing framework capable of handling both overlapping and non-overlapping users. In this paper, we address a more general challenge: how to effectively utilize user interactions and item metadata to improve recommendation performance in scenarios where there is no overlap between users or items across domains."}, {"title": "5.2 LLM-based Recommendation", "content": "LLMs' ability to generalize and understand text and long-term context has attracted interest in improving recommender systems. Research in this area is typically divided into generative and discriminative approaches based on LLMs' roles [40].\nGenerative recommender systems frame recommendation as natural language generation problems, enhancing interactivity and explainability. For instance, Rec Interpreter [43], ChatRec [9], and He et al. [12] use prompting strategies to convert user profiles or historical interactions into inputs for LLMs in conversational recommendations. ONCE [21] improves content-based recommendations by fine-tuning open-source LLMs and leveraging prompts with closed-source models. TALLRec [1] fine-tunes Alpaca [35] using self-instruct data to provide binary feedback (\"yes\" or \"no\"). GenRec [16] takes a different approach by directly generating the target item for recommendation. Additionally, CCF-LLM [23] encodes semantic and collaborative user-item interactions into hybrid prompts, fusing them via cross-modal attention for prediction.\nUnlike generative systems, discriminative recommender systems do not require LLMs to respond during the inference stage, making them more practical and efficient. Early approaches, such as UniSRec [15], fine-tuned BERT [4] to associate item descriptions with transferable representations across recommendation scenarios. Harte et al.[10] improved sequential recommendation by replacing conventional model embeddings with LLM embeddings. LLM-ESR[22] further enhanced sequential recommendation by integrating LLM-generated semantic embeddings via retrieval-augmented self-distillation, avoiding additional inference costs. Additionally, [8, 39] leveraged LLMs to augment multi-domain recommendation models with scenario-specific knowledge, though their performance depends heavily on sample quality and domain expertise. In contrast, our method eliminates the reliance on domain knowledge, offering a more general and universal framework for cross-domain sequential recommendation."}, {"title": "6 Conclusion", "content": "This work addresses the challenge of domain semantic bias in LLM-based zero-shot cross-domain sequential recommendation (ZCDSR), a critical task for achieving accurate predictions in unseen domains. We propose LLM-RecG, a model-agnostic generalization framework that mitigates domain bias by operating at both the item and sequential levels. At the item level, it balances inter-domain compactness with intra-domain diversity to align item embeddings while preserving domain-specific nuances. At the sequential level, it transfers user behavioral patterns through clustering and attention-based aggregation, enabling dynamic adaptation without requiring target-domain interaction data.\nExtensive experiments on multiple datasets demonstrate that LLM-RecG significantly enhances ZCDSR performance, offering a robust and scalable solution for cross-domain recommendation. Future work could explore extending the framework to incorporate richer metadata and address cold-start scenarios, further improving personalized zero-shot recommendations."}, {"title": "Appendix", "content": "To balance the contributions of intra-domain diversity and inter-domain compactness, we establish a deterministic relationship between \u03b2 and \u03b1. The scaling reflects the relative sizes and computational scales of the two terms.\nScale of Intra-Domain Diversity. The inter-domain compactness term Linter involves computing similarities between item embeddings and domain centers. Since each item embedding is compared to all domain centers, the computational complexity is proportional to:\nO(|N||D|), (22)\nwhere N is the total number of items and |D| is the number of domains.\nBalancing the Terms. To ensure that Lintra and Linter contribute proportionally to the overall loss, we scale \u03b2 relative to \u03b1 as:\n\u03b2 = \\frac{Scale \\: of \\: L_{intra}}{Scale \\: of \\: L_{inter}} (23)\nComplexity Analysis: - The intra-domain diversity term Lintra involves pairwise comparisons within each domain, leading to a complexity of:\n\\sum_{d \\in D} \\left( \\frac{|N|}{|D|} \\right)^2 = \\frac{O(|N|^2)}{|D|} (24)\nassuming the items are evenly distributed across domains.\n- The inter-domain compactness term, as mentioned, scales as:\nO(|N||D|). (25)\nRatio of Complexities: By comparing the complexities of the intra-domain and inter-domain terms, we derive:\n\\frac{O(\\frac{|N|^2}{|D|})}{O(|N|\u00b7|D|)} = \\frac{|N|}{|D|^2} (26)\nThus, to balance the contributions of Lintra and Linter, we set:\n\u03b2 = \\frac{|N|}{|D|^3} (27)\nTherefore,\n\u03b2 = \u03b1 \\frac{|N|}{|D|^3} (28)"}]}