{"title": "Latenrgy: Model Agnostic Latency and Energy Consumption Prediction\nfor Binary Classifiers", "authors": ["Jason M. Pittman"], "abstract": "Machine learning systems increasingly\ndrive innovation across scientific fields and industry,\nyet challenges in compute overhead\u2014specifically dur-\ning inference\u2014limit their scalability and sustainabil-\nity. Responsible AI guardrails, essential for ensur-\ning fairness, transparency, and privacy, further ex-\nacerbate these computational demands. This study\naddresses critical gaps in the literature, chiefly the\nlack of generalized predictive techniques for latency\nand energy consumption, limited cross-comparisons\nof classifiers, and unquantified impacts of RAI\nguardrails on inference performance. Using Theory\nConstruction Methodology, this work constructed a\nmodel-agnostic theoretical framework for predicting\nlatency and energy consumption in binary classifica-\ntion models during inference. The framework synthe-\nsizes classifier characteristics, dataset properties, and\nRAI guardrails into a unified analytical instrument.\nTwo predictive equations are derived that capture the\ninterplay between these factors while offering gener-\nalizability across diverse classifiers. The proposed\nframework provides foundational insights for design-\ning efficient, responsible ML systems. It enables re-\nsearchers to benchmark and optimize inference per-\nformance and assists practitioners in deploying scal-\nable solutions. Finally, this work establishes a theoret-\nical foundation for balancing computational efficiency\nwith ethical AI principles, paving the way for future\nempirical validation and broader applications.", "sections": [{"title": "1 Introduction", "content": "Machine learning (ML) has become integral to diverse\nscientific fields and business applications. In genomics,\nML helps to decode complex genetic patterns, while in\nclimatology, it improves the predictive accuracy of ex-\ntreme weather events. Across industries, ML is revolu-\ntionizing healthcare through diagnostic support and ad-\nvancing finance via fraud detection systems.\nDespite its widespread success, the field of ML faces\npersistent challenges. One such challenge is compute\noverhead or the computational resources consumed dur-\ning the training and inference phases of ML models.\nTraining involves the extensive energy and processing\npower required to optimize model parameters across\nlarge datasets. Inference, on the other hand, focuses on\ngenerating predictions from trained models, where com-\npute overhead is characterized by the interplay between\nlatency (the time required to produce a prediction) and\nenergy consumption (the power expended during infer-\nence tasks). High latency or energy consumption can\nlimit the scalability, accessibility, and sustainability of\nML systems, especially in resource-constrained environ-\nments such as mobile and edge devices (Henderson et al.,\n2020).\nAdding to these challenges is the growing emphasis\non Responsible AI (RAI). RAI is a framework of prin-\nciples aimed at ensuring AI technologies are ethical,\nfair, and trustworthy. RAI principles include trans-\nparency, accountability, fairness, privacy, and robust-\ness (Li, Liu, Yang, & Ren, 2024). To operationalize\nthese principles, technical controls and guardrails are\nemployed. While essential for trustworthy Al deploy-\nment, these principles impose additional computational\nburdens during training and inference. Doing so exacer-\nbates existing issues of latency and energy consumption.\nSurprisingly given the importance of RAI, the liter-\nature offers limited insights into how guardrails in\nparticular impact compute overhead during inference\n(Elesedy, Esperan\u00e7a, Oprea, & Ozay, 2024). While this\ngap may seem abstract at a broad level, it becomes highly\nrelevant in specific scenarios, such as binary classifica-\ntion models deployed in resource-sensitive environments.\nUnderstanding these impacts is critical for guiding the\ndesign and scaling of ML systems in both scientific and\nindustrial contexts.\nThis study is motivated by three specific chal-\nlenges within the broader gap. First, there is a\nlack of generalized predictive techniques for esti-\nmating classifier latency and energy consumption\n(Mallik, Wang, Xie, Chen, & Han, 2023). Sec-\nond, limited cross-comparison of classification\nalgorithms has hindered understanding of how\ndifferent models contribute to these overheads\n(Cassales, Gomes, Bifet, Pfahringer, & Senger, 2022).\nFinally, the potential impacts of RAI guardrails, such\nas explainability and interpretability mechanisms, on\ninference latency and energy consumption remain\nunderexplored (Li et al., 2024).\nIn response to these challenges, this work sought to con-\nstruct a model-agnostic equation for predicting latency\nand energy consumption in binary classification models\nduring inference with RAI guardrails. By addressing\nthese issues, this study contributes a theoretical founda-\ntion for optimizing compute overhead while balancing\nthe computational efficiency and ethical robustness of\nML systems.\nThe remainder of this paper is organized as follows: Sec-\ntion 2 reviews related work, providing a foundation of\nbackground research. Section 3 details the theoretical\nmethodology used to derive the predictive equation. Sec-\ntion 4 presents the derived equation and its components.\nFinally, Section 5 concludes with a discussion of the\nstudy's implications and directions for future research."}, {"title": "2 Related work", "content": "A comprehensive understanding of this study's contribu-\ntion requires familiarity with three key topics: bench-\nmarking ML compute overhead, the trade-off between\nlatency and energy consumption, as well as the founda-\ntion for RAI. The following sections summarize seminal\nand highly influential works in each topic. Such exist-\ning literature provides necessary context and grounding\nfor this study's theoretical framework and its focus on\nmodel-agnostic predictions of compute overhead."}, {"title": "2.1 Benchmarking ML Compute Overhead", "content": "Benchmarking compute overhead in machine learning\n(ML) is essential for understanding and optimizing the\nperformance and efficiency of ML systems across diverse\ntasks and deployment scenarios. Compute overhead en-\ncompasses the computational resources consumed during\nboth training and inference phases, with significant im-\nplications for scalability, sustainability, and accessibility\n(Strubell, Ganesh, & McCallum, 2020; Henderson et al.,\n2020). While training requires substantial resources to\noptimize model parameters, inference focuses on gen-\nerating predictions in real-time. With inference, met-\nrics such as latency (prediction time) and energy con-\nsumption (power usage) are critical (Mattson et al., 2020;\nReddi et al., 2020) to total cost of ownership and user\nexperience. Thus, effective benchmarking provides a\nfoundation for evaluating and improving ML systems\nwhere achieving low latency and high energy efficiency\nis paramount (Cassales et al., 2022; Mallik et al., 2023).\nAdditionally, benchmarks such as MLPerf and related\nstudies have emphasized the growing importance of quan-\ntifying compute overhead to address operational effi-\nciency and environmental impact (Tschand et al., 2024).\nA critical distinction exists between compute overhead\nduring training and inference. Training involves iterative\noptimization over large datasets, requiring substantial\ncomputational resources and prolonged processing times\n(Strubell et al., 2020). Inference, by contrast, focuses\non real-time applications, where latency (the time re-\nquired to produce a prediction) and energy consumption\n(the power required to perform inference) are paramount\n(Henderson et al., 2020)). Although the literature has tra-\nditionally emphasized the training phase, inference has\nreceived comparatively less attention.\nTo address some of these challenges, benchmarking\nframeworks such as MLPerf have been developed.\nMLPerf provides comprehensive benchmarks for both\ntraining and inference, enabling standardized perfor-\nmance evaluations across hardware and software plat-\nforms (Mattson et al., 2020). The MLPerf Inference\nBenchmark evaluates system performance on tasks such\nas image classification and object detection, offering in-\nsights into latency and energy efficiency across different\nimplementations (Reddi et al., 2020). Further, MLPerf\nPower introduces methodologies for assessing energy ef-\nficiency, reflecting the growing concern over the environ-\nmental impact of AI workloads (Tschand et al., 2024).\nWhile these benchmarks are instrumental in understand-\ning empirical performance, they focus on specific tasks\nand lack predictive models that generalize across classi-\nfiers or operational contexts.\nDespite advancements in benchmarking, significant gaps\nremain. First, current benchmarks such as MLPerf pro-\nvide empirical performance data but do not offer general-\nized predictive techniques for estimating latency and en-\nergy consumption across classifiers. This limitation hin-\nders the ability to anticipate performance bottlenecks or"}, {"title": "2.2 The Latency and Energy Consumption\nTradeoff", "content": "The relationship between latency and energy consump-\ntion during machine learning inference is complex, of-\nten involving trade-offs influenced by model architec-\nture, hardware, and optimization strategies. Generally,\nreducing latency requires increased computational re-\nsources, which can lead to higher energy consumption.\nConversely, minimizing energy usage may involve tech-\nniques that introduce additional processing time, thereby\nincreasing latency. This inverse relationship is particu-\nlarly evident in resource-constrained environments, such\nas edge devices, where balancing performance and effi-\nciency is critical.\nRecent studies have explored the trade-off between\nlatency and energy consumption during machine\nlearning inference, with varying levels of generaliz-\nability across classification algorithms. For instance,\nresearchers examining multilayer perceptrons (MLPs)\ndemonstrated that hyperparameter optimization could\nsignificantly reduce energy consumption during infer-\nence with minimal impact on classification accuracy\n(Desislavov, Mart\u00ednez-Plumed, & Hern\u00e1ndez-Orallo,\n2021). By tuning model complexity, such as reducing\nhidden layers or using lower-precision arithmetic, the\nstudy highlights strategies that, while tested on MLPs,\nmay generalize to other model architectures. However,\nthe reliance on specific algorithmic properties limits the\nimmediate applicability of these findings to non-neural\nnetwork classifiers.\nIn contrast, Hauschild and Hellbr\u00fcck\n(Hauschild & Hellbr\u00fcck, 2022) analyzed convolu-\ntional neural networks (CNNs) deployed on Internet of\nThings (IoT) edge devices, emphasizing the dependency\nof latency and energy consumption on model complexity\nand wireless data rates. The results show that simplifying\nCNN architectures can yield substantial efficiency gains\nin resource-constrained environments, underscoring the\nimportance of tailoring models to deployment scenarios.\nHowever, this approach is tightly coupled to CNNs and\ndoes not address broader classification paradigms, such\nas decision trees or support vector machines.\nWhile these studies offer valuable insights into optimiz-\ning latency and energy efficiency, the work reflects a\nbroader trend in the literature of focusing on specific\nmodels or hardware configurations (Cassales et al., 2022;\nTschand et al., 2024). This limitation underscores the\nneed for generalized predictive techniques that span di-\nverse classification algorithms, bridging the gap between\ntheoretical models and empirical benchmarks. Address-\ning this challenge is critical for advancing the scalability\nand efficiency of ML systems, particularly as the integra-\ntion of Responsible AI (RAI) guardrails introduces addi-\ntional computational overhead."}, {"title": "2.3 RAI Controls and Guardrails", "content": "Put simply, RAI ensures AI systems are developed and\ndeployed in ways that are ethical (Floridi et al., 2018;\nMittelstadt, Allo, Taddeo, Wachter, & Floridi, 2016).\nEthical, in this context, includes fairness, transparency,\nprivacy, security, and trustworthiness as core principles.\nThe idea is an Al system can be considered responsible\nwhen the set of relevant principles are present. Here,\none should consider present as technical continuous\nmonitoring.\nTo that end, ethical principles have experienced rapid\ntheoretical and practical expansion. In a short time, re-\nsearchers have developed robust technical frameworks to\nmeasure and evaluate these principles. Two prominent\nexamples are the Microsoft Responsible Toolbox and the\nIBM AI 360 Toolkit. Yet, as much as AI practitioners\ncan use these frameworks to evaluate models, researchers\n(Radclyffe, Ribeiro, & Wortham, 2023; Lu et al., 2024)\nsuggest RAI is one of the most critical challenges present\nin AI and ML.\nCulturally, the rapid expansion has been motivated\nby demonstrable harm arising from a lack of RAI."}, {"title": "3 Method", "content": "This work was motivated by a single research question:\nWhat variables, coefficients, and propositional operations\nare necessary for a model-agnostic equation to be ca-\npable of predicting latency and energy consumption in\nbinary classification models during inference with RAI\nguardrails? To answer this question, the study employed\nTheory Construction Methodology (TCM) to derive the\nmodel-agnostic equation.\nTCM is a structured approach to developing theoretical\nframeworks by defining key variables, establishing rela-\ntionships, and formalizing them into mathematical mod-\nels (Dubin, 1978). While TCM has been widely applied\nin theoretical modeling, its application to derive predic-\ntive equations for latency and energy consumption in the\ncontext of RAI guardrails represents a novel adaptation\nof this methodology. This approach is particularly well-\nsuited to the research problem because the abstraction\nand generalization required for a predictive equation ap-\nplicable across diverse classifiers necessitates a theoreti-\ncal framework (Kaplan & Haenlein, 2019).\nThe TCM process began with identifying core variables\ninfluencing latency and energy consumption during in-\nference. These variables were selected based on prior\nempirical findings and theoretical reasoning, ensuring\nrelevance to diverse classification contexts and com-\nputational scenarios. For example, the computational\noverhead introduced by explainability and interpretabil-\nity guardrails, such as those implemented using SHAP\n(Lundberg, 2017) or LIME (Ribeiro, Singh, & Guestrin,\n2016), was identified as a critical variable. This as-\nsumption is supported by computational complexity the-\nory, which posits that even linear increases in input size\n(O(n)) result in proportional growth in computational de-\nmand. In the context of RAI guardrails, the overhead\narises from explainability mechanisms that augment in-\nference operations with additional interpretive computa-\ntions.\nRelationships among these variables\u2014such as the in-\nverse correlation between latency and energy con-\nsumption are then proposed based on prior research\n(Henderson et al., 2020; Mallik et al., 2023). For in-\nstance, studies such as those by Hauschild and Hellbr\u00fcck\n(Hauschild & Hellbr\u00fcck, 2022) demonstrate how compu-\ntational trade-offs between latency and energy efficiency\nare particularly evident in edge computing environments.\nCoefficients are incorporated to represent adjustable fac-\ntors, including the type of classifier and specific deploy-\nment conditions. These variables and coefficients are\nconnected through mathematical operations, such as ad-"}, {"title": "4 Discussion", "content": "The development of a model-agnostic equation for pre-\ndicting latency and energy consumption began with iden-\ntifying foundational variables (Table 1). These variables\nare organized into three sets-classification algorithm,\nRAI guardrail, and dataset characteristics\u2014all of which\nserve as inputs to a prediction function f. The outputs\nof the function, latency (L) and energy consumption (E),\nare represented collectively as O."}, {"title": "4.1 General Equation", "content": "A general equation (1) was constructed to unify the di-\nmensions of latency and energy consumption into a cohe-\nsive analytical framework:\n$0 = f(A, D, G)$ (1)\nThis equation serves two purposes. First, it provides\na unified framework to compare inference performance\nacross binary classifiers. Second, it establishes a foun-\ndation for synthesizing disparate dimensions of model\nperformance into a predictive tool, enabling cross-model\ncomparisons, performance prediction, and the integration\nof RAI guardrails into system design."}, {"title": "4.2 Expanded Variables", "content": "Each variable set in the general equation is expanded into\nmeasurable elements. Algorithm type (A) contains four\ndiscrete elements: support vector machines (SVM), k-\nnearest neighbors (k-NN), random forests (RF), and neu-\nral networks (NN). Categorical encoding is used to rep-\nresent binary classifiers as a \u2208 SVM,k-NN,RF,NN, with\nA encoded as 1,0,0,0 to predict L or E for SVM, for in-\nstance.\nDataset characteristics (D) include the number of sam-\nples (n), feature dimensionality (p), and data type (t).\nData type is represented as a categorical variable with\ntabular data encoded as 0, text as 1, and image data as 2.\nRAI guardrails (G) encompass five principles: explain-\nability, fairness, interpretability, safety, and privacy. Each\nprinciple is modeled as a binary state ([0,1]), which,\nwhen active, can include a continuous intensity score.\nFor example, explainability (expl) could take a value of\n0.7, representing partial feature-level explanations cover-\ning the top 70% of features."}, {"title": "4.3 Prediction Equations", "content": "The general equation was expanded into two prediction\nequations, capturing latency (L) and energy consumption\n(E). These equations model inference performance as a\nfunction of algorithm type, dataset characteristics, and\nthe computational cost of RAI guardrails.\nThe latency equation (2) incorporates logarithmic scal-\ning for dataset size, capturing the diminishing impact of\nlarger datasets on prediction time:\n$L= \\alpha + \\beta_1A+\\beta_plog(n)+\\YDp+\\dpt+\\sum\\Phi_{G,i}g_i+ \\pounds$ (2)\nThe energy consumption equation (3) applies linear scal-\ning for dataset size to account for cumulative resource\ndemands during inference:\n$E = \\alpha' + \\beta_1\u0391 + \\beta n+YDp+\\dpt +\\sum\\Phi_{Gi}g_i + \\varepsilon'$ (3)\nBoth equations use coefficients to model the contribution\nof each variable, as summarized in Table 2."}, {"title": "4.4 Novelty and Practical Implications", "content": "These equations provide a novel approach to predicting\ninference performance across diverse binary classifiers.\nUnlike prior studies, which focus on empirical bench-\nmarking or specific algorithms (Cassales et al., 2022;\nMallik et al., 2023), this framework offers generalizabil-\nity and scalability. Furthermore, it uniquely integrates\nthe computational cost of RAI guardrails, addressing a\ncritical gap in the literature (Li et al., 2024; Ribeiro et al.,\n2016).\nFuture empirical validation will use benchmarks such as\nMLPerf (Mattson et al., 2020) to evaluate the predictive\naccuracy of these models. Practical applications include\noptimizing ML systems for edge devices, estimating re-\nsource demands for RAI-integrated classifiers, and en-\nabling informed trade-offs between latency, energy con-\nsumption, and ethical robustness."}, {"title": "5 Conclusion", "content": "AI broadly, and ML in specific, continues to transform\nscience and industry. Yet, AI and ML scalability and\naccessibility are often constrained by compute overhead.\nThe literature suggests such issues are particularly no-\ntable during inference. Challenges such as the lack of\ngeneralized predictive techniques for latency and energy\nconsumption, limited cross-comparison of classification\nalgorithms, and the unquantified computational impact\nof RAI guardrails have left critical gaps in the literature.\nThis study aimed to address these gaps by developing\na model-agnostic equation capable of predicting latency\nand energy consumption in binary classification models\nduring inference with RAI guardrails.\nThe key contributions of this work include a model-\nagnostic theoretical framework for analyzing inference\nperformance and two predictive equations for latency and\nenergy consumption. These models synthesize algorithm\ncharacteristics, dataset properties, and the computational\noverhead of RAI guardrails into a cohesive analytical\ntool. Unlike previous studies that focus on specific clas-\nsifiers or empirical benchmarks, this work offers gener-\nalizability and scalability, bridging theoretical modeling\nwith practical performance evaluation.\nThe broader significance of this research lies in its impli-\ncations for designing and deploying efficient, responsible\nML systems. For researchers, the predictive equations\nprovide a foundational tool for benchmarking and opti-\nmizing inference performance across diverse classifiers.\nFor practitioners, they enable informed decisions about\ndeploying models in resource-constrained environments,\nsuch as edge or mobile devices, while maintaining ethi-\ncal robustness. This work also aligns with the growing\nneed for sustainable AI, offering a pathway to balance\ncomputational efficiency with ethical considerations.\nIn conclusion, this study provides a theoretical founda-\ntion for understanding and predicting inference perfor-\nmance in ML systems. By addressing critical gaps in the\nliterature, it lays the groundwork for future advancements\nin model-agnostic performance prediction, enabling the\nnext generation of scalable and responsible AI systems."}, {"title": "5.1 Limitations", "content": "While this study provides a foundational framework for\npredicting inference latency and energy consumption in\nbinary classification models, five limitations should be\nacknowledged.\nFirst, the prediction equations rely on assumptions about\nvariable relationships, such as logarithmic scaling for\ndataset size in latency prediction and linear scaling\nfor energy consumption. While these assumptions are\ngrounded in prior research and theoretical reasoning, they\nmay not fully capture real-world complexities in all sce-\nnarios. Additional research, more especially practical ex-\nperimentation may reveal to what extent such a limitation\nis addressable.\nSecond, the focus on binary classification tasks excludes\nmulti-class classification and other ML tasks, such as\nregression or clustering, which may involve different\ncomputational trade-offs. Along similar thinking, this\nwork does not account for potential innovations becom-\ning available in the future.\nThird, the representation of RAI guardrails, while prac-\ntical, simplifies potential computational impact. Com-\nplex guardrails, such as differential privacy or trustwor-\nthiness mechanisms, may require more nuanced model-\ning to fully capture resource demands.\nFourth, the framework abstracts dataset characteristics to\nsize, feature dimensionality, and data type. Other impor-\ntant factors, such as data quality or sparsity, are not in-\ncluded and could affect predictions in specific contexts.\nFinally, this study presents theoretical equations without\nempirical validation. While the models are rigorous, their\naccuracy and generalizability remain untested. Future\nwork will involve validating these equations with exper-\nimental data across diverse classifiers, datasets, and de-\nployment environments to ensure their practical applica-\nbility."}, {"title": "5.2 Future work", "content": "There are several areas for future work based on the the-\noretical framework demonstrated in this research.\nForemost, experimentation is necessary to validate and\nquantify the coefficients in the latency (L) and energy\nconsumption (E) prediction equations. Empirical studies\nusing benchmark datasets and platforms such as MLPerf\nwill help calibrate these coefficients, ensuring their accu-\nracy across diverse classifiers and deployment environ-\nments. Validation efforts should also explore the sensitiv-\nity of the equations to different input variables, such as\ndataset characteristics and RAI guardrails, to refine the\nmodels further.\nFurthermore, the generalizability of the L and E predic-\ntive equations may be investigated by varying the set A\nacross a variety of AI subfields. Of particular interest,\ngiven the mainstream perception of AI, might be the ap-\nplication of the framework to Large Language Models\n(LLMs), where inference latency and energy efficiency\nare critical due to their size and complexity. Additionally,\nfrontier research areas such as neuro-symbolic AI repre-\nsent a compelling opportunity for extending the frame-\nwork to hybrid models that combine symbolic reasoning\nwith deep learning. These extensions could provide valu-\nable insights into the computational trade-offs in emerg-\ning AI paradigms.\nAnother avenue for future work involves refining the\nrepresentation of RAI guardrails. Current binary and\nintensity-scale representations may oversimplify the\ncomputational demands of advanced guardrails, such as\ndifferential privacy, adversarial robustness, or nuanced in-\nterpretability mechanisms. Developing more granular or\ncontext-aware models for guardrail contributions could\nenhance the framework's precision and applicability.\nFinally, while this study focused on binary classifica-\ntion tasks, future research could extend the framework\nto multi-class classification and other ML tasks, such as\nregression or clustering. These extensions would test\nthe framework's scalability and adaptability, addressing\nbroader applications in AI."}]}