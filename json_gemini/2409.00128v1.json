{"title": "Can AI Replace Human Subjects? A Large-Scale Replication of Psychological Experiments with LLMs", "authors": ["Ziyan Cui", "Ning Li", "Huaikang Zhou"], "abstract": "Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) such as GPT-4 have shown promise in replicating human-like responses in various psychological experiments. However, the extent to which LLMs can effectively replace human subjects across diverse experimental contexts remains unclear. Here, we conduct a large-scale study replicating 154 psychological experiments from top social science journals with 618 main effects and 138 interaction effects using GPT-4 as a simulated participant. We find that GPT-4 successfully replicates 76.0% of main effects and 47.0% of interaction effects observed in original studies, closely mirroring human responses in both direction and significance. However, only 19.44% of GPT-4's replicated confidence intervals contain the original effect sizes, with the majority of replicated effect sizes exceeding the 95% confidence interval of the original studies, and showing a 71.6% rate of unexpected significant results where the original studies reported null findings, suggesting potential overestimation or false positives. Our results demonstrate the potential of LLMs as powerful tools in psychological research but also emphasize the need for caution in interpreting AI-driven findings. While LLMs can complement human studies, they cannot yet fully replace the nuanced insights provided by human subjects.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) is rapidly transforming scientific research, a shift often referred to as \u201cAI for science\u201d (1,2). In the social sciences, where understanding human behavior, cognition, and perception is key, large language models (LLMs) are emerging as powerful tools that could revolutionize established research methods (3\u20135), which have long relied on experiments, surveys, and interviews with human participants. However, recent advancements in LLMs have shown that these models can effectively mimic human-like responses and behaviors (6,7).\nEarly studies have shown promise in using LLMs to replicate human responses with high accuracy in psychological assessments (4,6,8,9) and economic decision-making (10\u201313). However, despite these advances, recent research has also highlighted limitations in LLMs\u2019 ability to simulate human psychological behaviors, particularly at the individual level and for specific demographic profiles (14). While these pioneering efforts have laid the groundwork, they often rely on arbitrarily selected experiments, which can introduce bias, limit generalizability, and overlook critical areas where LLMs might excel or struggle.\nCrucial questions remain: To what extent can LLMs supplement or even replace human subjects across diverse psychological experiments? Are there systematic differences between human and AI responses, particularly in areas where such divergences might be more pronounced, such as socially sensitive topics (15)? Addressing these questions is vital for determining the applicability and limitations of LLMs in social science research (3).\nTo fill the critical knowledge gap, we conducted a large-scale study replicating 154 randomly selected psychological experiments from five top social science journals using LLMs, involving 53,840 participants and 82,870 replication records. This sample size allows for robust statistical analysis while remaining manageable. Our primary objective was to evaluate whether"}, {"title": "Methods", "content": "We aim to systematically evaluate the capability of Large Language Models (LLMs) to replicate human responses in psychological experiments, employing a carefully designed procedural framework (Figure 1)\u00b9."}, {"title": "Prompt Design and Adaptation", "content": "In designing prompts for GPT-4, we structured each prompt into four key parts: (1)\ncontext and role setting, (2) scenario description, (3) variable measurement, and (4) response format configuration. For context setting, we used prompts like \u201cImagine you are a person invited to participate in an experiment,\u201d adding specific details as needed to align with the original study design. The scenario descriptions closely mirrored the original experimental materials, ensuring that GPT-4 received the same context as human participants\u00b3. Instructions for variable evaluation and reasoning guided the model in assessing key variables according to the study's objectives. To facilitate data analysis, we configured GPT-4 to output its responses in a structured JSON format, enabling accurate extraction and direct comparison with human participant data. Our goal was to use the exact materials from the original experiments whenever possible, maintaining the integrity of the original studies and ensuring the Al model received clear and consistent instructions.\nBefore proceeding with full-scale replication, we conducted pretests on the prompts to assess their effectiveness. During this pretesting phase, we identified several areas where adaptations were necessary. These adaptations were required due to various factors, such as the complexity of the original experimental designs, the need to translate visual elements into descriptive text, and limitations in the model's ability to perform certain tasks directly. Specifically, 35.06% of the prompts required some form of adaptation to accommodate these challenges. These adaptations were systematically coded, allowing us to conduct additional analyses on how they might impact replication outcomes."}, {"title": "GPT-4 Replication Process and Data Analysis", "content": "Next, we conducted large-scale simulations using the public OpenAI API to generate responses from GPT-4 for our entire sample of experiments. We set the temperature parameter to the default value of 1.0 to balance response diversity with coherence and relevance. To replicate the original studies, we collected a sample 1.5 times the size of the original study for each experiment. This oversampling strategy was designed to account for potential data loss and to ensure sufficient statistical power to detect effects in the replication. When the original studies did not provide precise sample sizes for each condition, we distributed the total sample size evenly across conditions.\nIn our analysis, we adhered strictly to the analytical methods and tools used in the original studies to ensure comparability. Our analyses included a range of statistical techniques, such as descriptive statistics, regression analysis, ANOVA, t-tests, structural equation modeling, and chi-square analysis. When the original study did not specify an analytical method, we employed the most commonly used approaches in the field."}, {"title": "Replication Analysis and Comparison", "content": "We further conducted a comprehensive analysis of the replication results, focusing on the reproducibility of main effects and interaction effects reported in the original articles. Specifically, we developed a detailed coding sheet for each effect, capturing essential information such as journal, sample characteristics, data collection platforms, variables involved, and key metrics for both human and GPT-4 studies. The coding of variables involved included categorizing the topics into different domains, such as socially sensitive topics like race, gender, and ethics-areas where LLMs may respond differently (29). These metrics also included sample"}, {"title": "Results", "content": "Our large-scale replication effort using GPT-4 yielded compelling insights into the potential of Large Language Models (LLMs) for psychological research. We began by ensuring sufficient statistical power, with an average of 98.79% across all replications (see SI for details), far exceeding the conventional 80% threshold. This high power level provides confidence that our study was well-equipped to detect effects if they existed, ruling out the possibility that replication failures were due to lack of sensitivity."}, {"title": "Replication Success Rate", "content": "Our analysis encompassed a total of 618 main effects from the original studies. Among these, 417 were significant main effects with clear directions\u2074. Additionally, we examined 138"}, {"title": "Statistical Significance Patterns", "content": "Our analysis of p-values revealed interesting patterns in the replication results. For the main effect, as shown in Figure 2-a, the p-values from GPT-4 replications are generally smaller compared to those from the original studies (Original: Mean = 0.117, SD = 0.265; Replication: Mean = 0.056, SD = 0.172). This suggests that GPT-4 tends to produce stronger evidence against the null hypothesis."}, {"title": "Antecedents of Replication Rate and Effect Size Distribution", "content": "To examine the factors that significantly influence replication success (1 = replicated, 0 =\nnot replicated), the likelihood of the original effect size falling within the 95% confidence interval (CI) of the replication, the difference between effect sizes (GPT r - human r), and the consistency between the directions (1 = consistent, 0 = not consistent), we conducted regression analyses (Figure 5).\nSignificant findings revealed that studies involving race or ethnicity (b = -1.427, p < 0.01) and ethical or moral variables (b = -0.609, p < 0.05) had lower replication success rates, indicating challenges in replicating effects related to these socially sensitive topics. Additionally, experiments that required scenario adaptations for GPT-4 (b = -0.567, p < 0.05) showed lower replication success, indicating that these studies may be inherently more challenging for LLMs to understand. Even with modifications to make the scenarios more accessible to the model, the complexity or nuance of these experiments likely contributed to the reduced replication success.\nThe analysis showed that studies published in management journals (b = 0.889, p < 0.001) were significantly more likely to have their original effect sizes fall within the 95% CI of the replication. Similarly, studies conducted on MTurk or Prolific platforms (b = 0.826, p < 0.05) also had a higher likelihood of their original effect sizes being captured within the replicated 95% CI. This could be because participants on these platforms, who typically respond to hypothetical scenarios, may have response patterns more aligned with those of GPT-4, which"}, {"title": "Discussion", "content": "This study addresses the critical question of how LLMs can supplement or potentially replace human subjects in psychological experiments, providing a nuanced understanding of their capabilities and limitations.\nOur key finding of a high replication rate (75.4% for main effects) is particularly striking in the context of the ongoing debate about reproducibility in psychological science. This rate is substantially higher than the 36-47% reported by the Open Science Collaboration (OSC) in their"}, {"title": "Implications for Psychological Science and Replication", "content": "The findings of our study suggest a potential shift in how psychological experiments could be conducted. LLMs like GPT-4 offer a promising avenue for pilot studies, providing a cost-efficient method to explore hypotheses and experimental designs before involving more resource-intensive human trials. By using LLMs in the early stages of research, scientists can identify and refine psychological mechanisms that warrant further investigation with human subjects. This approach could accelerate the discovery of new psychological phenomena while conserving valuable time and resources (3,8).\nHowever, it is important to acknowledge the limitations of relying on LLMs for psychological research. While LLMs like GPT-4 can effectively replicate the direction of effects observed in human studies, they may not accurately estimate true effect sizes, potentially leading to false-positive results. GPT-4's tendency to generate significant findings where human subjects did not can be seen as a double-edged sword. On one hand, this sensitivity suggests that the model is highly attuned to subtle patterns in data that might be difficult to detect in human populations. However, this same sensitivity raises the risk of overfitting\u2014where the model detects noise or spurious correlations as meaningful effects.\nThis overfitting could be problematic in psychological research, where human variability and contextual factors play a crucial role in determining whether an effect is real or simply an artifact of the data. The unexpected significance rate observed in our study underscores this risk, revealing the potential for GPT-4 to overestimate the presence of effects, leading to false positives. These false positives are not merely statistical anomalies; they could represent a"}, {"title": "Implications for Understanding LLMs and Human Cognition", "content": "Our study not only sheds light on the replicability of psychological experiments but also offers insights into the behavior of LLMs themselves. By using a range of psychological"}]}