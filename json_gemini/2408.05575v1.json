{"title": "In-Context Exploiter for Extensive-Form Games", "authors": ["Shuxin Li", "Chang Yang", "Youzhi Zhang", "Pengdeng Li", "Xinrun Wang", "Xiao Huang", "Hau Chan", "Bo An"], "abstract": "Nash equilibrium (NE) is a widely adopted solution concept in game theory due\nto its stability property. However, we observe that the NE strategy might not\nalways yield the best results, especially against opponents who do not adhere to NE\nstrategies. Based on this observation, we pose a new game-solving question: Can\nwe learn a model that can exploit any, even NE, opponent to maximize their own\nutility? In this work, we make the first attempt to investigate this problem through\nin-context learning. Specifically, we introduce a novel method, In-Context Exploiter\n(ICE), to train a single model that can act as any player in the game and adaptively\nexploit opponents entirely by in-context learning. Our ICE algorithm involves\ngenerating diverse opponent strategies, collecting interactive history training data\nby a reinforcement learning algorithm, and training a transformer-based agent\nwithin a well-designed curriculum learning framework. Finally, comprehensive\nexperimental results validate the effectiveness of our ICE algorithm, showcasing\nits in-context learning ability to exploit any unknown opponent, thereby positively\nanswering our initial game-solving question.", "sections": [{"title": "1 Introduction", "content": "The domain of game-solving has consistently served as a benchmark for the advancement in Artificial\nIntelligence (AI) (Tammelin et al., 2015; Morav\u010d\u00edk et al., 2017). It tests the boundaries of the strategic\nreasoning and decision-making capabilities of AI systems. It is well known that in game theory,\nNash equilibrium (NE) (Nash, 1950) is the standard solution concept, which describes a situation\nwhere no player can increase their utility by unilaterally deviating. In many security-related cases,\nthe NE strategy plays a critical role since it is the most conservative strategy that can achieve the\nbest performance in the worst-case scenario (Jain et al., 2011, 2013). However, the NE strategy may\nnot achieve the best utility in some scenarios where the opponents do not play the NE strategy. For\nexample, in the classic game of rock-paper-scissors, an opponent adhering strictly to an NE strategy\nwould randomize their selections, ensuring no discernible pattern emerges. If an adversary were to\ndeviate from this randomness and favor one choice, a non-NE strategy that exploits this bias could\nyield a better outcome (a detailed explanation can be found in Sec. 3).\nThis brings us to the core question of game-solving:\nCan we learn a model which can exploit any opponent to maximize his own utility?\nPut simply, is it possible to consistently obtain the best utility by exploiting any type of opponent,\neven when we have no prior knowledge of their strategy?\nThe field of opponent exploitation has seen significant advancements. Foerster et al. (2017) introduced\na method for deducing the parameters of opponents' policies based on historical interaction data.\nHowever, this approach requires extensive training data to accurately adapt to new opponents.\nRecently, Wu et al. (2022) developed a deep learning framework for implicit opponent modeling,\ncalled Learning-to-Exploit (L2E) which includes an adversarial training procedure to autonomously\ngenerate opponents, thereby reducing the training data requirements. However, these methods are\nnot suited to address the game-solving problem we propose, since they necessitate retraining upon\nencountering a new opponent. This limitation points to a lack of generalizability, meaning they\ncannot exploit any opponent only using a model without parameter updating.\nTo solve these issues, we resort to in-context learning which has gained significant attention for its\nability to effectively infer tasks from contextual information. Notably, large language models such\nas GPT-3 (Brown et al., 2020), have shown remarkable abilities in tackling various tasks like text\ncompletion and code generation merely via language-based prompts. It also has been extended to\nthe Reinforcement Learning (RL) area. Laskin et al. (2022) proposed the Algorithm Distillation\n(AD) algorithm, demonstrating in-context reinforcement learning by sequentially modeling offline\ndata with an imitation loss. Later, Lee et al. (2023) developed the Decision-Pretrained Transformer\n(DPT), a transformer model pre-trained through supervised learning to predict optimal actions from\nan in-context dataset of interactions.\nThis in-context learning ability aligns perfectly with our game-solving problem, as we aim for a\nmodel to effectively exploit any unknown opponent based on these online interactions with the\nopponent. For this reason, we make the first attempt to study the proposed game-solving problem\nin extensive-form games through in-context learning. In this paper, we propose a novel framework,\nIn-Context Exploiter (ICE), designed to train a single model that can act as any player in the game to\nadaptively exploit the opponents without updating the parameters. A comparison of the traditional\nequilibrium finding framework with our ICE approach is depicted in Fig.1. Essentially, the ICE\nmethod can self-improve through in-context learning when against any opponent, unlike the NE\nstrategy, which tends to adopt the safest, but not necessarily optimal, behavior.\nThe ICE algorithm comprises three phases: i) generating opponent strategies through distinct ap-\nproaches to ensure diversity; ii) gathering interactive history data using a reinforcement learning\nalgorithm to maximize their utility against these opponent strategies; iii) training a transformer model\non the history data within a curriculum learning framework to enhance the effectiveness and stability\nof the training process. In summary, our contributions are threefold: i) we make the first attempt to"}, {"title": "2 Background and Related Work", "content": "Imperfect-Information Extensive-Form Games. An imperfect-information extensive-form game\n(EFG) can be represented by a tuple (N, H, A, P, T, u) (Shoham & Leyton-Brown, 2008). N\nrepresents the set of players, i.e., N = {1, ..., n} and H represents the set of histories which is\nthe past action sequence. In particular, when the game starts, the history is an empty sequence \u00d8,\nrepresenting the root node of the game tree. Additionally, every prefix of any sequence within H is\nalso included in H. There is a set of special histories, called terminal histories, which are sequences\nthat end in the leaf nodes of the game tree. Z is used to represent the set of terminal histories which\nis a subset of H, i.e., Z \u2282 H. A(h) = {a : (h, a) \u2208 H} represents the set of available actions at any\nnon-terminal history h \u2208 H \\ Z. P represents the player function which maps each non-terminal\nhistory to a player, i.e., P(h) \u2192 N\u222a {c} in which e denotes chance player, representing these\nstochastic events beyond players' control. The information set, represented by I\u2081, forms a partition\nover the set of histories where i takes action, such that player i \u2208 N cannot distinguish these histories\nwithin the same information set Ii. Therefore, each information set Ii \u2208 Zi corresponds to one\ndecision-making point for player i which means that P(h\u2081) = P(h2) and A(h\u2081) = A(h2) for any\nh1, h2 \u2208 Ii. For convenience, we can employ A(Ii) and P(Ii) to denote A(h) and P(h) for any\nhistory h within Ii. ui represents the utility function of player i that maps every terminal history to\nreal numbers, i.e., uz: Z \u2192 R.\nThe behavior strategy for player i, denoted by oi, is a function that maps every information set\nIi to a probability distribution over the available action A(Ii). The set of strategies for player\ni is denoted by \u03a3i, i.e., \u03c3\u03b5 \u2208 \u03a3\u012f. Given a strategy profile \u03c3 = (\u03c31,02,...,\u03c3\u03b7), the expected\nvalue to player i is the sum of the expected payoff of these resulting terminal nodes, i.e, u\u2081(\u03c3) =\n\u2211\u03c4\u03b5\u03b6 \u03c0\u03bf(z)ui(z). \u03c0\u00ba(z) = \u03a0i\u2208NU{c} \u03c0\u00bf(z) is the reaching probability of terminal history z\nand \u03c0\u03af(z) is the contribution of player i to reach the terminal history z. The common solution\nconcept for the imperfect-information extensive-form game is Nash equilibrium (NE) (Nash, 1950),\ndefined as a strategy profile such that no player can increase their expected utility by unilaterally\nswitching to a different strategy. Formally, a strategy profile \u03c3* forms an NE if it satisfies ui(\u03c3*) =\nmax\u03c3\u03af\u03b5\u03c2; \u03ba\u03b9(\u03c3\u03af, \u03c3*\u2081),\u2200i \u2208 N, where \u03c3*; refers to all the strategies in o except for \u03c3\u012f. The NE\nstrategy is the safest and most stable strategy. However, it may not be the optimal strategy in many\ncases as we described in Sec.3. In this paper, we aim to develop a model that can always effectively\nexploit different opponents to increase its utility.\nIn-Context Learning (ICL). ICL is the ability to infer tasks and adapt strategies based on contextual\ninformation. A seminal example in this domain is the GPT series by OpenAI, particularly GPT-3\n(Brown et al., 2020), which has demonstrated remarkable flexibility in handling a variety of tasks\nthrough language prompts alone. Recent advancements have also seen the integration of in-context\nlearning into reinforcement learning domains. Laskin et al. (2022) introduced Algorithm Distillation\n(AD), a novel method that employs sequential modeling of offline data with an imitation loss for\nin-context reinforcement learning. This method has shown promising results in enhancing the\nadaptability of RL agents to a variety of environments. Afterward, Lee et al. (2023) proposed the\nDecision-Pretrained Transformer (DPT), a model pre-trained through supervised learning. This\ntransformer model also exhibits its ability to solve a range of RL problems via in-context learning.\nThese developments underscore the growing importance of in-context learning. By enabling models\nto intuitively adapt to new tasks based on contextual cues, in-context learning represents a significant\nstep towards more flexible and general-purpose AI systems. Our research draws inspiration from\nthese innovations, seeking to further explore and expand the capabilities of in-context learning in\nexploiting different opponents under extensive-form game settings.\nCurriculum Learning (CL). CL has seen considerable exploration and application in recent years.\nThis approach, inspired by the way humans learn, involves gradually increasing the complexity of\nlearning tasks, thereby enhancing learning efficiency. Bengio et al. (2009) were among the first to\nformalize the idea of curriculum learning in the context of machine learning. They demonstrated that\nstarting with easier examples and progressively increasing difficulty could significantly improve the"}, {"title": "3 Problem Statement", "content": "We start by presenting our motivation with a well-known game of rock-paper-scissors, leading us to\ndefine our research problem based on the insights from this example.\nNE is Safe, but not Optimal against Non-NE\nOpponent. Given a rock-paper-scissors (RPS)\ngame where the only NE is each player plays\nthe three actions with the same probability, i.e.,\n1/3, and the expected utility of each player is\n0. However, when the opponent is not rational,\nplaying the NE strategy is not always preferred.\nSuppose that an opponent always plays rock, the\nexpected utility of playing NE strategy against\nhim is still 0, while the expected utility of always playing paper is 1. Therefore, playing the NE\nstrategy would be a safe option but is not optimal in even two-player zero-sum symmetric games.\nDecision-making Problem. The RPS game demonstrates that adherence to the Nash Equilibrium\n(NE) strategy, while safe, may not always yield optimal results, especially against non-rational players.\nThis observation leads us to propose a novel game-solving problem: Can we learn a model which can\nexploit any, even NE, opponent to maximize their own utility? In other words, can we build a model\nthat can intelligently exploit different opponents to maximize their own utility across various strategic\nsituations? In this paper, we start an initial exploration of investigating this game-solving problem\nand propose a framework, In-Context Exploiter (ICE), to train a model that can act as any player\nin the game to adaptively exploit the opponents without parameter updating, i.e., self-improvement\nentirely in context."}, {"title": "4 In-Context Exploiter", "content": "In this section, we introduce our algorithm, In-Context Exploiter (ICE), specifically crafted to train\na model to exploit any unknown opponent and increase its utility through its in-context learning\nability. Fig.2 provides an overview of ICE methodology. As depicted, the ICE approach includes\nthree primary stages: generating diverse opponent strategies, collecting interactive history through\na reinforcement learning algorithm, and training a model within a curriculum learning framework.\nEach of these stages plays a critical role in ensuring the model's adaptability and generalizability."}, {"title": "4.1 Opponent Generation", "content": "The key to developing a robust deep learning-based model that can effectively exploit various\nopponents is the creation of a comprehensive and representative set of opponent strategies for training.\nThe diversity of these opponent strategies plays an important role, not only in enriching the training\ndataset but also in improving the model's capacity to generalize and adapt to exploit any new opponent\nthrough in-context learning. Our opponent generation method employs two approaches to ensure\nboth diversity and representativeness: random generation and learning-based generation.\nRandom Generation. Just as its name implies, random generation involves creating opponent\nstrategies randomly. Specifically, for every opponent's possible information set, we randomly sample\na policy for the information set to generate the opponent's strategy. This randomness ensures that"}, {"title": "4.2 Interactive History Collection", "content": "Given a diverse dataset consisting of opponent strategies, exploiting different opponent strategies\ncan be regarded as a multi-task reinforcement learning problem. Then we can apply the multi-task\npre-train and fine-tuning framework to solve this problem. However, the agent requires fine-tuning\nwhenever meeting a new opponent. Considering the huge number of potential opponents, this\nframework is impractical for real-world tasks. To mitigate these issues, we aim to train a model,\nequipping in-context learning ability, i.e., self-improvement without parameter updating. We borrow\nthe idea from Algorithm Distillation (AD) (Laskin et al., 2022) that distills RL algorithms into neural\nnetworks.\nIn a word, we can utilize any suitable RL algorithm to exploit different opponents and then record\nthese learning histories to distill a neural network model, equipping the in-context learning ability. In\nthis section, we first introduce the collection of these historical data. Notably, each opponent's strategy\ncorresponds to a unique learning task whose goal is to maximize their utility against the opponent.\nDuring the RL learning process, we record the interactive histories, capturing the interactions between\nthe algorithm and the opponent. The recorded data comprises a sequence of information sets, actions\ntaken, and the resulting utilities. The sequential data will be used for model training, as it provides\nrich, contextual insights into decision-making processes. In our experiments, we use proximal policy"}, {"title": "4.3 Curriculum Learning", "content": "Note that after obtaining this learning historical dataset, we can train a model with the AD algorithm\non these learning historical data directly. However, it may not be efficient or effective due to the\nhigh diversity of opponent strategies. The randomly ordered opponent strategies pose challenges\nin training, as the model must adapt to a wide range of behaviors and tactics, potentially leading to\nslower learning and reduced efficiency in the training process. To address this issue, we design a\ncurriculum learning framework to enhance the stability and efficiency of the training process since\nthe curriculum learning approach mimics the human learning process, where training begins with\nsimpler tasks and gradually progresses to more complex challenges."}, {"title": "4.3.1 Curriculum Generation", "content": "In this section, we outline the method for generating a curriculum. Since the curriculum learning\nmirrors the natural learning progression, we should generate the curriculum from simple tasks to\ndifficult tasks. For the specific task of opponent exploitation, we recognize the difficulty of this\ntask depends on the gap between the opponent strategy and the NE strategy since NE is the most\ndifficult to exploit according to the definition of NE. It means that opponent strategies closer to NE\nare typically harder to exploit. Thus, we can generate the curriculum based on this understanding.\nNatural Order of Learning-Generated Opponents. Notably, the opponent's strategies generated by\nthe learning-based generation method align naturally with the difficulty level, as they are sorted based\non their proximity to NE. Thus, we can directly utilize this natural order to generate the curriculum for\nprogressive training, starting from simpler strategies and moving towards more complex strategies.\nIntegration of Randomly-Generated Opponent. Unfortunately, the opponent's strategies generated\nthrough the random generation method do not have the natural order. Of course, we can sort them\naccording to the gap between these opponent strategies and the NE strategy. However, it would be\ntime-consuming to compute these gaps since there are many opponent strategies. Fortunately, we find\nthat most of the opponent's strategies generated through the random generation method are generally\nsimple. Therefore, we can just intersperse these random opponents among the ordered opponents\ngenerated by the learning-based generation method to enhance the stability of the training process.\nThe method for generating a curriculum is detailed in Algorithm 1. It involves preserving the natural\norder of opponent strategies generated by the learning-based generation method. In this sequence, we\nstrategically insert randomly generated opponent's strategies at regular intervals."}, {"title": "4.3.2 Transformer Training", "content": "Before going into the details of the whole curriculum learning framework, we first introduce the\ntraining loss. Similar to the AD algorithm, we consider the training of the model as a sequence\nprediction problem. Therefore, any sequence model such as RNNs (Williams & Zipser, 1989) can be\nused. In this paper, we adopt a causal transformer, renowned for its robustness in sequence modeling\n(Vaswani et al., 2017) and its adaptability to downstream tasks via in-context learning.\nFor convenience, we represent the training data generated by the learning-based generation oppo-\nnent strategies as D\u2081 = {D11, ..., Din}, and the training data from the random generation opponent\nstrategies as Dr = {Dr1,..., Drm}. These training data include the entire learning history for each\ntask, capturing the sequence of interactions - information sets (It), actions (at), and rewards (rt) - en-\ncountered when applying an RL algorithm. Formally, D\u2081 = (I(i), ai), ri), ..., Ii), a), rv)), Di \u2208\nDr U Di.\nThe essence of our approach lies in distilling the behaviors learned by the RL algorithms into the\ntransformer. The transformer, denoted as Me with parameters 0, is trained to map long histories"}, {"title": "4.3.3 Step-by-step Training", "content": "After generating a curriculum and defining the training loss for the transformer, we introduce how\nto train the transformer step-by-step according to the generated curriculum. A critical problem of\ntraining is catastrophic forgetting, especially when training on many tasks. Before into the training\ndetails, we introduce our method for mitigating this issue.\nPreventing Catastrophic Forgetting. To mitigate catastrophic forgetting, our training approach\nincorporates periodic reviews and retraining on previously tackled tasks. This continual learning\napproach ensures that the model retains its proficiency in earlier learned tasks while acquiring new\ncapabilities. The rate of revising previous tasks is carefully calibrated to balance the retention of old\nknowledge with the acquisition of new skills, maintaining a comprehensive understanding across a\nspectrum of opponent strategies.\nTraining Process. The whole curriculum training framework is depicted in Algorithm 2. The\ncurriculum Sorder and corresponding interactive history dataset D are taken as inputs. A transformer\nmodel Me is instantiated with parameters @ and a previous rate o is defined to control the blend of\nnew and prior tasks to prevent catastrophic forgetting.\nAs the training iterations progress from t = 1 to T, the algorithm systematically selects the current\ntrain task St from the curriculum Sorder. To avoid catastrophic forgetting, the training regimen\nincorporates a review mechanism where, upon training on a new task, the algorithm revisits previous\ntasks with a frequency determined by the rate \u03c3 (Lines 9-13). This review strategy is crucial for\nmaintaining and reinforcing the knowledge previously gained, thereby enhancing the model's ability\nfor generalization across various scenarios. When all tasks have been trained, the algorithm shifts to\na phase that consolidates learning by training across the entire task spectrum, i.e., randomly sampling\na task from the trained task set (Line 15). After determining the training task, we perform the training\non the corresponding dataset of the task Ds, according to Eq.(1) (Lines 17-18)."}, {"title": "5 Empirical Evaluation", "content": "To assess the effectiveness of our ICE algorithm, we conduct comprehensive experiments on several\npopular extensive-form games. We begin by detailing our experimental setting. Then we provide a\ndetailed analysis of the results, which are structured around answering several key research questions."}, {"title": "5.1 Experimental Setting", "content": "Experimental Subject. For our experiments, we selected a range of poker games as test subjects,\nincluding two-player and three-player versions of Kuhn Poker, Leduc Poker, and Goofspiel with five\ncards. These games serve as diverse platforms to evaluate our algorithm's performance.\nEvaluation Testbed. To rigorously assess our algorithm, we constructed three distinct types of\ntestbeds by randomly sampling opponents: in-distribution, out-of-distribution, and NE opponent. For\nthe in-distribution testbed, we selected approximately 30 opponent tasks from the task dataset utilized\nduring training. For the out-of-distribution testbed, we randomly sampled 20 opponent strategies to\ncreate a diverse set of test tasks. Finally, for the NE opponent testbed, we specifically configured the\nopponent's strategy to align with the Nash Equilibrium, thereby forming test tasks that directly reflect\nNE strategies. This multifaceted testing approach allows us to thoroughly evaluate the adaptability\nand effectiveness of our algorithm in various strategic scenarios.\nBaselines. We first choose two widely used strategies, the Best Response (BR) strategy and the Nash\nEquilibrium (NE) strategy, as baselines. Notably, BR is a theoretically optimal approach in an online\nsetting since it is tailored against a known opponent's strategy. In the online setting, we include the\nBR strategy as a theoretical upper-bound benchmark. We select the NE strategy as a baseline since it\nis the most conservative strategy against any opponent. Since this is an online setting, we also select\none online learning algorithm, proximal policy optimization (PPO) (Schulman et al., 2017) algorithm\nas baselines. In addition to this, we also include multi-task pre-training with fine-tuning framework\nas one baseline since we can consider exploiting different opponents as multi-task learning. Since\nthe BR and NE strategies are fixed once the opponent is given, we directly simulate these strategies\nagainst the opponent's strategy to evaluate their performance. For the PPO algorithm, multi-task\npre-training with fine-tuning, and our ICE algorithm, we conduct evaluations under a limited number\nof online interactions with the opponent. This limitation is deliberate, as our goal is to assess the\ncapability of these algorithms to quickly and effectively exploit an unknown opponent."}, {"title": "5.2 Experimental Results", "content": "To better demonstrate our results, we present our results by answering the following research questions\n(RQs)."}, {"title": "6 Conclusion", "content": "In this paper, we investigate a pioneering game-solving problem: Can we learn a model that can\nexploit any opponent to maximize their utility? To this end, we propose a framework, the In-Context\nExploiter (ICE) algorithm, to train a single model that can act as any player in the game and\nadaptively exploit opponents through in-context learning. Our approach begins with generating\nopponent strategies to create diverse and representative tasks. We then apply an RL algorithm to\nsolve these tasks, gathering the interactive history as training data. Subsequently, we develop a\ncurriculum learning framework to effectively train a transformer model. Experimental results verify\nthe effectiveness of the ICE algorithm in exploiting any unknown opponent and the model's ability\nto quickly adapt and optimize its strategies in various scenarios. The success of the ICE algorithm\nhighlights the significant potential of in-context learning and this research not only answers our\nproposed game-solving problem affirmatively but also opens avenues for further exploration in the\nrealm of strategic learning."}, {"title": "A Discussion", "content": "A.1 Comparing ICE with Opponent Modeling\nICE can be viewed as a method with implicit opponent modeling (He et al., 2016; Albrecht & Stone,\n2018), where the modeling of the opponent is implicitly encoded into the parameters of the model.\nThere are several advantages of ICE over opponent modeling: i) ICE does not need an explicit model\nfor the opponents, where the explicit model in the opponent modeling may restrict the generalizability\nof the methods, ii) ICE can exploit different opponents without changing the parameters, where the\nopponent modeling may need to fit the parameters of the opponent model during game play and\nthen make the decision in response to the opponent. To summarize, ICE is simpler, more efficient,\nand more generalizable. ICE also has the disadvantage, i.e., the ability to model the opponents is\nlargely determined by the length of the in-context. With longer in-context, ICE can model more\nopponents, while the model will also be larger and the training cost will be increased. We will discuss\nthe methods to reduce the length of the in-context in the next section. On the other hand, we can also\nintroduce an explicit model for opponents into ICE, where the parameters of the opponent model\ncan be fitted through in-context learning. The explicit opponent model can help us to understand the\ninternal mechanism of ICE.\nA.2 Comparing ICE with Online Learning, Multitask Learning, and Meta Learning\nICE, as well as other in-context learning methods (Laskin et al., 2022; Lee et al., 2023), is similar to\nonline learning methods, e.g., no-regret learning (Shalev-Shwartz et al., 2012). However, ICE does\nnot change the parameters of the model during the game play with the opponents, which differs from\nonline learning. We believe that online learning, especially no-regret learning, can be used to analyze\nthe behaviors of ICE and in-context learning methods, which will be explored in future works. We\nalso consider online learning methods as our baselines. We note that PPO is an online learning and\non-policy method and PPO is scalable and widely used. Therefore, we include PPO as the baseline in\nour experiments.\nThe training of ICE is also similar to multitask learning (Mandi et al., 2022) and meta-learning (Finn\net al., 2017), where multi-task learning learns a policy for different tasks, and meta-learning enables\nthe fast adaption of the learned policy on specific tasks. ICE also learns a policy for different tasks,\nwhere the model parameters are not changed, but the behaviors are changed during game play. In the\nexperiment section, we choose the PPO method initialized with a pre-trained policy to benchmark\nmulti-task and meta-learning methods, as shown in Figure 6.\nA.3 Limitations and Future Works\nIn this section, we provide a detailed discussion about the limitations of ICE and the future works.\nDynamic Opponents. The main objective of this work is to demonstrate the generalizability of\nin-context learning in solving extensive-form games, therefore we only consider the different static\nopponents, i.e., the opponents' policies are not changed during playing with our model. In future work,\nwe will consider applying ICE against dynamic opponents, where the opponents can be rule-based\nagents, learning agents, or even in-context learning agents. The dynamic opponents will bring extra\ndifficulties to ICE, including the enormous types of dynamic opponents, the instabilities when all\nplayers are changing their behaviors, and the difficulties for training. Therefore, novel methods are\nneeded to make ICE robustly and safely exploit dynamic opponents without being exploited.\nReducing the In-Context Length. As discussed above, the length of the in-context will significantly\ninfluence the ability of ICE, which is also demonstrated in our experiments. To handle the problems\nwith more complicated dynamics and higher dimensions of the observations, the input dimensions\nof the model will grow drastically. Therefore, novel methods are required to reduce the in-context\nlength of ICE. One possible option is that we can introduce the memory (Sumers et al., 2023) to ICE,\neither internal or external, and we can query the relevant experiences from the memory to form the\nin-context.\nGeneralizability of ICE. In this work, we only consider the case where the model will play\nagainst different opponents of a game. A more challenging case for the generalizability of ICE"}, {"title": "B Implementation Details", "content": "In this section, we provide the experimental details of our ICE algorithm from its three main stages.\nOpponent Generation. In this paper, we employ two methods, as introduced in the main paper, to\ngenerate a diverse range of opponent strategies. To implement the random generation method, we\ntraverse through all the information sets of an opponent and assign a randomly generated strategy\nto each information set. This approach allows us to generate various opponents exhibiting random\nbehaviors. To implement the learning-based generation method, we utilize a well-known algorithm,\nCounterfactual Regret Minimization (CFR) (Zinkevich et al., 2007), as the equilibrium-finding\nalgorithm. By applying CFR to solve the game, we record the average strategy for each player at\neach iteration. This process generates a series of opponent strategies that evolve from random to\nincreasingly robust over time. These two methods collectively ensure that our dataset includes a\nwide spectrum of opponent strategies, ranging from entirely unpredictable to highly strategic. Such\na comprehensive dataset is instrumental in training our model to adapt and respond effectively to\nvarious levels of opponent sophistication and strategy.\nInteractive History Collection. It's important to recognize that when an opponent's strategy is\nknown, the task of exploiting that opponent to maximize utility effectively becomes a reinforcement\nlearning (RL) problem. Consequently, each distinct opponent strategy corresponds to a unique RL\ntask. To collect interactive history data from our diverse opponent strategies for training purposes, we\nadopt the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) to address each\nof these RL tasks. During this process, we systematically record the learning history of the PPO\nalgorithm, specifically capturing the contents of the reply buffer used by PPO.\nCurriclum Learning. The curriculum learning framework plays a pivotal role in effectively training\nthe transformer model, with the core aspect being the design of the curriculum itself. In the main\npaper, we have thoroughly detailed the process of generating the curriculum and the overarching\nstructure of the learning framework. This section will not delve into the specifics of the curriculum\nlearning framework. However, it's important to emphasize that the strategic design of the curriculum is\nintegral to the success of our model's training. The gradual escalation in complexity and the structured\nprogression of tasks ensure that the model is not overwhelmed and can build its understanding and\ncapabilities incrementally. This approach aligns with the principles of in-context learning, enabling\nthe transformer to adapt and respond effectively to a wide range of strategic scenarios."}, {"title": "C Additional Experimental Results.", "content": "In this section, we present further experimental results to substantiate the effectiveness of our\nICE algorithm. While the main paper provided the performance of ICE in three two-player game\nscenarios evaluated across three distinct testbeds, here we extend our analysis to include results from\nexperiments conducted on three different three-player games.\nFirstly, we present the results from the in-distribution testbed in Fig.9. In these three three-player\ngames, it is evident that the model trained using the ICE algorithm successfully functions as any\nplayer in the game, demonstrating in-context learning ability with increasing iterations. The Best\nResponse (BR) strategy, while theoretically the optimal approach since it is tailored against a known\nopponent's strategy, isn't practical in real-world scenarios where an opponent's strategy isn't known in\nadvance. In our results, the BR strategy's performance is included merely as a theoretical benchmark.\nNotably, while the ICE-trained model doesn't achieve the theoretical optimal values of the BR\nstrategy, it consistently surpasses both the NE strategy and the PPO algorithm. This observation is\nsignificant as it indicates that the ICE-trained model can exploit the opponents more effectively than\nthe NE strategy, which is generally considered the most conservative approach. The ability of ICE\nto outperform in these multi-player game scenarios demonstrates its potential as a powerful tool for\nstrategic decision-making in complex, real-world situations.\nNext, Fig.10 shows the results from the out-of-distribution testbed, where we observe trends similar\nto those in the in-distribution testbed. The key distinction here is that the opponents in the out-\nof-distribution testbed are randomly generated, which often results in simpler strategic scenarios.\nIn contrast, the in-distribution testbed encompasses a mix of randomly generated and learning-\ngenerated opponents, leading to potentially more complex and challenging interactions. An interesting\nobservation in the three-player Goofspiel game is that, after 500 interactions, the PPO algorithm\nbegins to match the performance of ICE. This trend could be attributed to the simpler nature of the\nrandomly generated opponents in the out-of-distribution testbed, which might be easier for PPO\nto adapt to and exploit over time. Despite this, ICE demonstrates a faster convergence to high-\nperformance levels compared to the PPO algorithm and consistently outperforms the NE strategy.\nIt indicates that ICE is not only capable of quickly adapting to new opponents but also effectively\nmaximizing performance in diverse opponent settings, including both simple and complex strategic\nenvironments.\nLastly, we discuss the results against NE opponents, as shown in Fig.11. Our findings reveal that the\nICE algorithm achieves better or comparable performance to the NE strategy only in the three-player\nKuhn poker game. However, in other game scenarios, while ICE does not outperform the NE strategy,\nit still maintains a higher level of performance than the PPO algorithm. The less optimal performance\nof ICE in these cases can be attributed to the highly dynamic game environment and stability of the\nNE opponents. In three-player games, the player faces two opponents simultaneously, and if both\nadopt the conservative NE strategy, exploiting them concurrently becomes significantly challenging.\nThis observation highlights an area for future development. Improving the ICE algorithm to more\neffectively handle situations where multiple opponents employ highly conservative strategies, such as\nthe NE, will be a focus of our future research."}], "equations": ["u_i(\\sigma^*) = \\max_{\\sigma'_i \\in \\Sigma_i} u_i(\\sigma'_i, \\sigma^*_{-i}), \\"]}