{"title": "In-Context Exploiter for Extensive-Form Games", "authors": ["Shuxin Li", "Chang Yang", "Youzhi Zhang", "Pengdeng Li", "Xinrun Wang", "Xiao Huang", "Hau Chan", "Bo An"], "abstract": "Nash equilibrium (NE) is a widely adopted solution concept in game theory due to its stability property. However, we observe that the NE strategy might not always yield the best results, especially against opponents who do not adhere to NE strategies. Based on this observation, we pose a new game-solving question: Can we learn a model that can exploit any, even NE, opponent to maximize their own utility? In this work, we make the first attempt to investigate this problem through in-context learning. Specifically, we introduce a novel method, In-Context Exploiter (ICE), to train a single model that can act as any player in the game and adaptively exploit opponents entirely by in-context learning. Our ICE algorithm involves generating diverse opponent strategies, collecting interactive history training data by a reinforcement learning algorithm, and training a transformer-based agent within a well-designed curriculum learning framework. Finally, comprehensive experimental results validate the effectiveness of our ICE algorithm, showcasing its in-context learning ability to exploit any unknown opponent, thereby positively answering our initial game-solving question.", "sections": [{"title": "1 Introduction", "content": "The domain of game-solving has consistently served as a benchmark for the advancement in Artificial Intelligence (AI) (Tammelin et al., 2015; Morav\u010d\u00edk et al., 2017). It tests the boundaries of the strategic reasoning and decision-making capabilities of AI systems. It is well known that in game theory, Nash equilibrium (NE) (Nash, 1950) is the standard solution concept, which describes a situation where no player can increase their utility by unilaterally deviating. In many security-related cases, the NE strategy plays a critical role since it is the most conservative strategy that can achieve the best performance in the worst-case scenario (Jain et al., 2011, 2013). However, the NE strategy may not achieve the best utility in some scenarios where the opponents do not play the NE strategy. For example, in the classic game of rock-paper-scissors, an opponent adhering strictly to an NE strategy would randomize their selections, ensuring no discernible pattern emerges. If an adversary were to deviate from this randomness and favor one choice, a non-NE strategy that exploits this bias could yield a better outcome (a detailed explanation can be found in Sec. 3).\nThis brings us to the core question of game-solving:\nCan we learn a model which can exploit any opponent to maximize his own utility?\nPut simply, is it possible to consistently obtain the best utility by exploiting any type of opponent, even when we have no prior knowledge of their strategy?\nThe field of opponent exploitation has seen significant advancements. Foerster et al. (2017) introduced a method for deducing the parameters of opponents' policies based on historical interaction data. However, this approach requires extensive training data to accurately adapt to new opponents. Recently, Wu et al. (2022) developed a deep learning framework for implicit opponent modeling, called Learning-to-Exploit (L2E) which includes an adversarial training procedure to autonomously generate opponents, thereby reducing the training data requirements. However, these methods are not suited to address the game-solving problem we propose, since they necessitate retraining upon encountering a new opponent. This limitation points to a lack of generalizability, meaning they cannot exploit any opponent only using a model without parameter updating.\nTo solve these issues, we resort to in-context learning which has gained significant attention for its ability to effectively infer tasks from contextual information. Notably, large language models such as GPT-3 (Brown et al., 2020), have shown remarkable abilities in tackling various tasks like text completion and code generation merely via language-based prompts. It also has been extended to the Reinforcement Learning (RL) area. Laskin et al. (2022) proposed the Algorithm Distillation (AD) algorithm, demonstrating in-context reinforcement learning by sequentially modeling offline data with an imitation loss. Later, Lee et al. (2023) developed the Decision-Pretrained Transformer (DPT), a transformer model pre-trained through supervised learning to predict optimal actions from an in-context dataset of interactions.\nThis in-context learning ability aligns perfectly with our game-solving problem, as we aim for a model to effectively exploit any unknown opponent based on these online interactions with the opponent. For this reason, we make the first attempt to study the proposed game-solving problem in extensive-form games through in-context learning. In this paper, we propose a novel framework, In-Context Exploiter (ICE), designed to train a single model that can act as any player in the game to adaptively exploit the opponents without updating the parameters. A comparison of the traditional equilibrium finding framework with our ICE approach is depicted in Fig.1. Essentially, the ICE method can self-improve through in-context learning when against any opponent, unlike the NE strategy, which tends to adopt the safest, but not necessarily optimal, behavior.\nThe ICE algorithm comprises three phases: i) generating opponent strategies through distinct ap- proaches to ensure diversity; ii) gathering interactive history data using a reinforcement learning algorithm to maximize their utility against these opponent strategies; iii) training a transformer model on the history data within a curriculum learning framework to enhance the effectiveness and stability of the training process. In summary, our contributions are threefold: i) we make the first attempt to"}, {"title": "2 Background and Related Work", "content": "Imperfect-Information Extensive-Form Games. An imperfect-information extensive-form game (EFG) can be represented by a tuple $(N, H, A, P, T, u)$ (Shoham & Leyton-Brown, 2008). $N$ represents the set of players, i.e., $N = {1, ..., n}$ and $H$ represents the set of histories which is the past action sequence. In particular, when the game starts, the history is an empty sequence \u00d8, representing the root node of the game tree. Additionally, every prefix of any sequence within $H$ is also included in $H$. There is a set of special histories, called terminal histories, which are sequences that end in the leaf nodes of the game tree. $Z$ is used to represent the set of terminal histories which is a subset of $H$, i.e., $Z \u2282 H$. $A(h) = {a : (h, a) \u2208 H}$ represents the set of available actions at any non-terminal history $h \u2208 H \\ Z$. $P$ represents the player function which maps each non-terminal history to a player, i.e., $P(h) \u2192 N\u222a {c}$ in which $c$ denotes chance player, representing these stochastic events beyond players' control. The information set, represented by $I_i$, forms a partition over the set of histories where $i$ takes action, such that player $i \u2208 N$ cannot distinguish these histories within the same information set $I_i$. Therefore, each information set $I_i \u2208 \\Z_i$ corresponds to one decision-making point for player $i$ which means that $P(h_1) = P(h_2)$ and $A(h_1) = A(h_2)$ for any $h_1, h_2 \u2208 I_i$. For convenience, we can employ $A(I_i)$ and $P(I_i)$ to denote $A(h)$ and $P(h)$ for any history $h$ within $I_i$. $u_i$ represents the utility function of player $i$ that maps every terminal history to real numbers, i.e., $u_i: Z \u2192 \\R$.\nThe behavior strategy for player $i$, denoted by $\u03c3_i$, is a function that maps every information set $I_i$ to a probability distribution over the available action $A(I_i)$. The set of strategies for player $i$ is denoted by $\u03a3_i$, i.e., $\u03c3_i \u2208 \u03a3_i$. Given a strategy profile $\u03c3 = (\u03c3_1,\u03c3_2,...,\u03c3_n)$, the expected value to player $i$ is the sum of the expected payoff of these resulting terminal nodes, i.e, $u_i(\u03c3) = \\sum_{z\u2208Z} \u03c0^\u03c3(z)u_i(z)$. $\u03c0^\u03c3(z) = \\prod_{i\u2208N\u222a{c}} \u03c0_i(z)$ is the reaching probability of terminal history z and $\u03c0_i(z)$ is the contribution of player i to reach the terminal history z. The common solution concept for the imperfect-information extensive-form game is Nash equilibrium (NE) (Nash, 1950), defined as a strategy profile such that no player can increase their expected utility by unilaterally switching to a different strategy. Formally, a strategy profile $\u03c3*$ forms an NE if it satisfies $u_i(\u03c3*) = max_{\u03c3_i\u2208\u03a3_i} u_i(\u03c3'_i, \u03c3*_{-i}), \u2200i \u2208 N$, where $\u03c3*_{-i}$ refers to all the strategies in $\u03c3$ except for $\u03c3_i$. The NE strategy is the safest and most stable strategy. However, it may not be the optimal strategy in many cases as we described in Sec.3. In this paper, we aim to develop a model that can always effectively exploit different opponents to increase its utility.\nIn-Context Learning (ICL). ICL is the ability to infer tasks and adapt strategies based on contextual information. A seminal example in this domain is the GPT series by OpenAI, particularly GPT-3 (Brown et al., 2020), which has demonstrated remarkable flexibility in handling a variety of tasks through language prompts alone. Recent advancements have also seen the integration of in-context learning into reinforcement learning domains. Laskin et al. (2022) introduced Algorithm Distillation (AD), a novel method that employs sequential modeling of offline data with an imitation loss for in-context reinforcement learning. This method has shown promising results in enhancing the adaptability of RL agents to a variety of environments. Afterward, Lee et al. (2023) proposed the Decision-Pretrained Transformer (DPT), a model pre-trained through supervised learning. This transformer model also exhibits its ability to solve a range of RL problems via in-context learning. These developments underscore the growing importance of in-context learning. By enabling models to intuitively adapt to new tasks based on contextual cues, in-context learning represents a significant step towards more flexible and general-purpose AI systems. Our research draws inspiration from these innovations, seeking to further explore and expand the capabilities of in-context learning in exploiting different opponents under extensive-form game settings.\nCurriculum Learning (CL). CL has seen considerable exploration and application in recent years. This approach, inspired by the way humans learn, involves gradually increasing the complexity of learning tasks, thereby enhancing learning efficiency. Bengio et al. (2009) were among the first to formalize the idea of curriculum learning in the context of machine learning. They demonstrated that starting with easier examples and progressively increasing difficulty could significantly improve the"}, {"title": "3 Problem Statement", "content": "We start by presenting our motivation with a well-known game of rock-paper-scissors, leading us to define our research problem based on the insights from this example.\nNE is Safe, but not Optimal against Non-NE Opponent. Given a rock-paper-scissors (RPS) game where the only NE is each player plays the three actions with the same probability, i.e., 1/3, and the expected utility of each player is 0. However, when the opponent is not rational, playing the NE strategy is not always preferred. Suppose that an opponent always plays rock, the expected utility of playing NE strategy against him is still 0, while the expected utility of always playing paper is 1. Therefore, playing the NE strategy would be a safe option but is not optimal in even two-player zero-sum symmetric games.\nDecision-making Problem. The RPS game demonstrates that adherence to the Nash Equilibrium (NE) strategy, while safe, may not always yield optimal results, especially against non-rational players. This observation leads us to propose a novel game-solving problem: Can we learn a model which can exploit any, even NE, opponent to maximize their own utility? In other words, can we build a model that can intelligently exploit different opponents to maximize their own utility across various strategic situations? In this paper, we start an initial exploration of investigating this game-solving problem and propose a framework, In-Context Exploiter (ICE), to train a model that can act as any player in the game to adaptively exploit the opponents without parameter updating, i.e., self-improvement entirely in context."}, {"title": "4 In-Context Exploiter", "content": "In this section, we introduce our algorithm, In-Context Exploiter (ICE), specifically crafted to train a model to exploit any unknown opponent and increase its utility through its in-context learning ability. Fig.2 provides an overview of ICE methodology. As depicted, the ICE approach includes three primary stages: generating diverse opponent strategies, collecting interactive history through a reinforcement learning algorithm, and training a model within a curriculum learning framework. Each of these stages plays a critical role in ensuring the model's adaptability and generalizability.\n4.1 Opponent Generation\nThe key to developing a robust deep learning-based model that can effectively exploit various opponents is the creation of a comprehensive and representative set of opponent strategies for training. The diversity of these opponent strategies plays an important role, not only in enriching the training dataset but also in improving the model's capacity to generalize and adapt to exploit any new opponent through in-context learning. Our opponent generation method employs two approaches to ensure both diversity and representativeness: random generation and learning-based generation.\nRandom Generation. Just as its name implies, random generation involves creating opponent strategies randomly. Specifically, for every opponent's possible information set, we randomly sample a policy for the information set to generate the opponent's strategy. This randomness ensures that"}, {"title": "4.2 Interactive History Collection", "content": "Given a diverse dataset consisting of opponent strategies, exploiting different opponent strategies can be regarded as a multi-task reinforcement learning problem. Then we can apply the multi-task pre-train and fine-tuning framework to solve this problem. However, the agent requires fine-tuning whenever meeting a new opponent. Considering the huge number of potential opponents, this framework is impractical for real-world tasks. To mitigate these issues, we aim to train a model, equipping in-context learning ability, i.e., self-improvement without parameter updating. We borrow the idea from Algorithm Distillation (AD) (Laskin et al., 2022) that distills RL algorithms into neural networks.\nIn a word, we can utilize any suitable RL algorithm to exploit different opponents and then record these learning histories to distill a neural network model, equipping the in-context learning ability. In this section, we first introduce the collection of these historical data. Notably, each opponent's strategy corresponds to a unique learning task whose goal is to maximize their utility against the opponent. During the RL learning process, we record the interactive histories, capturing the interactions between the algorithm and the opponent. The recorded data comprises a sequence of information sets, actions taken, and the resulting utilities. The sequential data will be used for model training, as it provides rich, contextual insights into decision-making processes. In our experiments, we use proximal policy"}, {"title": "4.3 Curriculum Learning", "content": "Note that after obtaining this learning historical dataset, we can train a model with the AD algorithm on these learning historical data directly. However, it may not be efficient or effective due to the high diversity of opponent strategies. The randomly ordered opponent strategies pose challenges in training, as the model must adapt to a wide range of behaviors and tactics, potentially leading to slower learning and reduced efficiency in the training process. To address this issue, we design a curriculum learning framework to enhance the stability and efficiency of the training process since the curriculum learning approach mimics the human learning process, where training begins with simpler tasks and gradually progresses to more complex challenges.\n4.3.1 Curriculum Generation\nIn this section, we outline the method for generating a curriculum. Since the curriculum learning mirrors the natural learning progression, we should generate the curriculum from simple tasks to difficult tasks. For the specific task of opponent exploitation, we recognize the difficulty of this task depends on the gap between the opponent strategy and the NE strategy since NE is the most difficult to exploit according to the definition of NE. It means that opponent strategies closer to NE are typically harder to exploit. Thus, we can generate the curriculum based on this understanding.\nNatural Order of Learning-Generated Opponents. Notably, the opponent's strategies generated by the learning-based generation method align naturally with the difficulty level, as they are sorted based on their proximity to NE. Thus, we can directly utilize this natural order to generate the curriculum for progressive training, starting from simpler strategies and moving towards more complex strategies.\nIntegration of Randomly-Generated Opponent. Unfortunately, the opponent's strategies generated through the random generation method do not have the natural order. Of course, we can sort them according to the gap between these opponent strategies and the NE strategy. However, it would be time-consuming to compute these gaps since there are many opponent strategies. Fortunately, we find that most of the opponent's strategies generated through the random generation method are generally simple. Therefore, we can just intersperse these random opponents among the ordered opponents generated by the learning-based generation method to enhance the stability of the training process.\nThe method for generating a curriculum is detailed in Algorithm 1. It involves preserving the natural order of opponent strategies generated by the learning-based generation method. In this sequence, we strategically insert randomly generated opponent's strategies at regular intervals.\n4.3.2 Transformer Training\nBefore going into the details of the whole curriculum learning framework, we first introduce the training loss. Similar to the AD algorithm, we consider the training of the model as a sequence prediction problem. Therefore, any sequence model such as RNNs (Williams & Zipser, 1989) can be used. In this paper, we adopt a causal transformer, renowned for its robustness in sequence modeling (Vaswani et al., 2017) and its adaptability to downstream tasks via in-context learning.\nFor convenience, we represent the training data generated by the learning-based generation opponent strategies as $D_l = {D_{l1}, ..., D_{ln}}$, and the training data from the random generation opponent strategies as $D_r = {D_{r1},..., D_{rm}}$. These training data include the entire learning history for each task, capturing the sequence of interactions - information sets ($I_t$), actions ($a_t$), and rewards ($r_t$) - encountered when applying an RL algorithm. Formally, $D_i = (I_t^{(i)}, a_t^{(i)}, r_t^{(i)}, ..., I_{T_i}^{(i)}, a_{T_i}^{(i)}, r_{T_i}^{(i)})$, $D_i \u2208 D_r \u222a D_l$.\nThe essence of our approach lies in distilling the behaviors learned by the RL algorithms into the transformer. The transformer, denoted as $M_\u03b8$ with parameters $\u03b8$, is trained to map long histories"}, {"title": "4.3.3 Step-by-step Training", "content": "After generating a curriculum and defining the training loss for the transformer, we introduce how to train the transformer step-by-step according to the generated curriculum. A critical problem of training is catastrophic forgetting, especially when training on many tasks. Before into the training details, we introduce our method for mitigating this issue.\nPreventing Catastrophic Forgetting. To mitigate catastrophic forgetting, our training approach incorporates periodic reviews and retraining on previously tackled tasks. This continual learning approach ensures that the model retains its proficiency in earlier learned tasks while acquiring new capabilities. The rate of revising previous tasks is carefully calibrated to balance the retention of old knowledge with the acquisition of new skills, maintaining a comprehensive understanding across a spectrum of opponent strategies.\nTraining Process. The whole curriculum training framework is depicted in Algorithm 2. The curriculum $S_{order}$ and corresponding interactive history dataset $D$ are taken as inputs. A transformer model $M_\u03b8$ is instantiated with parameters $\u03b8$ and a previous rate $\u03c3$ is defined to control the blend of new and prior tasks to prevent catastrophic forgetting.\nAs the training iterations progress from $t = 1$ to $T$, the algorithm systematically selects the current train task $S_t$ from the curriculum $S_{order}$. To avoid catastrophic forgetting, the training regimen incorporates a review mechanism where, upon training on a new task, the algorithm revisits previous tasks with a frequency determined by the rate $\u03c3$ (Lines 9-13). This review strategy is crucial for maintaining and reinforcing the knowledge previously gained, thereby enhancing the model's ability for generalization across various scenarios. When all tasks have been trained, the algorithm shifts to a phase that consolidates learning by training across the entire task spectrum, i.e., randomly sampling a task from the trained task set (Line 15). After determining the training task, we perform the training on the corresponding dataset of the task $D_{S_t}$, according to Eq.(1) (Lines 17-18)."}, {"title": "5 Empirical Evaluation", "content": "To assess the effectiveness of our ICE algorithm, we conduct comprehensive experiments on several popular extensive-form games. We begin by detailing our experimental setting. Then we provide a detailed analysis of the results, which are structured around answering several key research questions.\n5.1 Experimental Setting\nExperimental Subject. For our experiments, we selected a range of poker games as test subjects, including two-player and three-player versions of Kuhn Poker, Leduc Poker, and Goofspiel with five cards. These games serve as diverse platforms to evaluate our algorithm's performance.\nEvaluation Testbed. To rigorously assess our algorithm, we constructed three distinct types of testbeds by randomly sampling opponents: in-distribution, out-of-distribution, and NE opponent. For the in-distribution testbed, we selected approximately 30 opponent tasks from the task dataset utilized during training. For the out-of-distribution testbed, we randomly sampled 20 opponent strategies to create a diverse set of test tasks. Finally, for the NE opponent testbed, we specifically configured the opponent's strategy to align with the Nash Equilibrium, thereby forming test tasks that directly reflect NE strategies. This multifaceted testing approach allows us to thoroughly evaluate the adaptability and effectiveness of our algorithm in various strategic scenarios.\nBaselines. We first choose two widely used strategies, the Best Response (BR) strategy and the Nash Equilibrium (NE) strategy, as baselines. Notably, BR is a theoretically optimal approach in an online setting since it is tailored against a known opponent's strategy. In the online setting, we include the BR strategy as a theoretical upper-bound benchmark. We select the NE strategy as a baseline since it is the most conservative strategy against any opponent. Since this is an online setting, we also select one online learning algorithm, proximal policy optimization (PPO) (Schulman et al., 2017) algorithm as baselines. In addition to this, we also include multi-task pre-training with fine-tuning framework as one baseline since we can consider exploiting different opponents as multi-task learning. Since the BR and NE strategies are fixed once the opponent is given, we directly simulate these strategies against the opponent's strategy to evaluate their performance. For the PPO algorithm, multi-task pre-training with fine-tuning, and our ICE algorithm, we conduct evaluations under a limited number of online interactions with the opponent. This limitation is deliberate, as our goal is to assess the capability of these algorithms to quickly and effectively exploit an unknown opponent.\n5.2 Experimental Results\nTo better demonstrate our results, we present our results by answering the following research questions (RQs)."}, {"title": "6 Conclusion", "content": "In this paper, we investigate a pioneering game-solving problem: Can we learn a model that can exploit any opponent to maximize their utility? To this end, we propose a framework, the In-Context Exploiter (ICE) algorithm, to train a single model that can act as any player in the game and adaptively exploit opponents through in-context learning. Our approach begins with generating opponent strategies to create diverse and representative tasks. We then apply an RL algorithm to solve these tasks, gathering the interactive history as training data. Subsequently, we develop a curriculum learning framework to effectively train a transformer model. Experimental results verify the effectiveness of the ICE algorithm in exploiting any unknown opponent and the model's ability to quickly adapt and optimize its strategies in various scenarios. The success of the ICE algorithm highlights the significant potential of in-context learning and this research not only answers our proposed game-solving problem affirmatively but also opens avenues for further exploration in the realm of strategic learning."}, {"title": "A Discussion", "content": "A.1 Comparing ICE with Opponent Modeling\nICE can be viewed as a method with implicit opponent modeling (He et al., 2016; Albrecht & Stone, 2018), where the modeling of the opponent is implicitly encoded into the parameters of the model. There are several advantages of ICE over opponent modeling: i) ICE does not need an explicit model for the opponents, where the explicit model in the opponent modeling may restrict the generalizability of the methods, ii) ICE can exploit different opponents without changing the parameters, where the opponent modeling may need to fit the parameters of the opponent model during game play and then make the decision in response to the opponent. To summarize, ICE is simpler, more efficient, and more generalizable. ICE also has the disadvantage, i.e., the ability to model the opponents is largely determined by the length of the in-context. With longer in-context, ICE can model more opponents, while the model will also be larger and the training cost will be increased. We will discuss the methods to reduce the length of the in-context in the next section. On the other hand, we can also introduce an explicit model for opponents into ICE, where the parameters of the opponent model can be fitted through in-context learning. The explicit opponent model can help us to understand the internal mechanism of ICE.\nA.2 Comparing ICE with Online Learning, Multitask Learning, and Meta Learning\nICE, as well as other in-context learning methods (Laskin et al., 2022; Lee et al., 2023), is similar to online learning methods, e.g., no-regret learning (Shalev-Shwartz et al., 2012). However, ICE does not change the parameters of the model during the game play with the opponents, which differs from online learning. We believe that online learning, especially no-regret learning, can be used to analyze the behaviors of ICE and in-context learning methods, which will be explored in future works. We also consider online learning methods as our baselines. We note that PPO is an online learning and on-policy method and PPO is scalable and widely used. Therefore, we include PPO as the baseline in our experiments.\nThe training of ICE is also similar to multitask learning (Mandi et al., 2022) and meta-learning (Finn et al., 2017), where multi-task learning learns a policy for different tasks, and meta-learning enables the fast adaption of the learned policy on specific tasks. ICE also learns a policy for different tasks, where the model parameters are not changed, but the behaviors are changed during game play. In the experiment section, we choose the PPO method initialized with a pre-trained policy to benchmark multi-task and meta-learning methods, as shown in Figure 6.\nA.3 Limitations and Future Works\nIn this section, we provide a detailed discussion about the limitations of ICE and the future works.\nDynamic Opponents. The main objective of this work is to demonstrate the generalizability of in-context learning in solving extensive-form games, therefore we only consider the different static opponents, i.e., the opponents' policies are not changed during playing with our model. In future work, we will consider applying ICE against dynamic opponents, where the opponents can be rule-based agents, learning agents, or even in-context learning agents. The dynamic opponents will bring extra difficulties to ICE, including the enormous types of dynamic opponents, the instabilities when all players are changing their behaviors, and the difficulties for training. Therefore, novel methods are needed to make ICE robustly and safely exploit dynamic opponents without being exploited.\nReducing the In-Context Length. As discussed above, the length of the in-context will significantly influence the ability of ICE, which is also demonstrated in our experiments. To handle the problems with more complicated dynamics and higher dimensions of the observations, the input dimensions of the model will grow drastically. Therefore, novel methods are required to reduce the in-context length of ICE. One possible option is that we can introduce the memory (Sumers et al., 2023) to ICE, either internal or external, and we can query the relevant experiences from the memory to form the in-context.\nGeneralizability of ICE. In this work, we only consider the case where the model will play against different opponents of a game. A more challenging case for the generalizability of ICE"}, {"title": "B Implementation Details", "content": "In this section, we provide the experimental details of our ICE algorithm from its three main stages.\nOpponent Generation. In this paper, we employ two methods, as introduced in the main paper, to generate a diverse range of opponent strategies. To implement the random generation method, we traverse through all the information sets of an opponent and assign a randomly generated strategy to each information set. This approach allows us to generate various opponents exhibiting random behaviors. To implement the learning-based generation method, we utilize a well-known algorithm, Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2007), as the equilibrium-finding algorithm. By applying CFR to solve the game, we record the average strategy for each player at each iteration. This process generates a series of opponent strategies that evolve from random to increasingly robust over time. These two methods collectively ensure that our dataset includes a wide spectrum of opponent strategies, ranging from entirely unpredictable to highly strategic. Such a comprehensive dataset is instrumental in training our model to adapt and respond effectively to various levels of opponent sophistication and strategy.\nInteractive History Collection. It's important to recognize that when an opponent's strategy is known, the task of exploiting that opponent to maximize utility effectively becomes a reinforcement learning (RL) problem. Consequently, each distinct opponent strategy corresponds to a unique RL task. To collect interactive history data from our diverse opponent strategies for training purposes, we adopt the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) to address each of these RL tasks. During this process, we systematically record the learning history of the PPO algorithm, specifically capturing the contents of the reply buffer used by PPO.\nCurriclum Learning. The curriculum learning framework plays a pivotal role in effectively training the transformer model, with the core aspect being the design of the curriculum itself. In the main paper, we have thoroughly detailed the process of generating the curriculum and the overarching structure of the learning framework. This section will not delve into the specifics of the curriculum learning framework. However, it's important to emphasize that the strategic design of the curriculum is integral to the success of our model's training. The gradual escalation in complexity and the structured progression of tasks ensure that the model is not overwhelmed and can build its understanding and capabilities incrementally. This approach aligns with the principles of in-context learning, enabling the transformer to adapt and respond effectively to a wide range of strategic scenarios."}, {"title": "C Additional Experimental Results.", "content": "In this section, we present further experimental results to substantiate the effectiveness of our ICE algorithm. While the main paper provided the performance of ICE in three two-player game scenarios evaluated across three distinct testbeds, here we extend our analysis to include results from experiments conducted on three different three-player games.\nFirstly, we present the results from the in-distribution testbed in Fig.9. In these three three-player games, it is evident that the model trained using the ICE algorithm successfully functions as any player in the game, demonstrating in-context learning ability with increasing iterations. The Best Response (BR) strategy, while theoretically the optimal approach since it is tailored against a known opponent's strategy, isn't practical in real-world scenarios where an opponent's strategy isn't known in advance. In our results, the BR strategy's performance is included merely as a theoretical benchmark. Notably, while the ICE-trained model doesn't achieve the theoretical optimal values of the BR strategy, it consistently surpasses both the NE strategy and the PPO algorithm. This observation is significant as it indicates that the ICE-trained model can exploit the opponents more effectively than the NE strategy, which is generally considered the most conservative approach. The ability of ICE to outperform in these multi-player game scenarios demonstrates its potential as a powerful tool for strategic decision-making in complex, real-world situations.\nNext, Fig.10 shows the results from the out-of-distribution testbed, where we observe trends similar to those in the in-distribution testbed. The key distinction here is that the opponents in the out- of-distribution testbed are randomly generated, which often results in simpler strategic scenarios. In contrast, the in-distribution testbed encompasses a mix of randomly generated and learning- generated opponents, leading to potentially more complex and challenging interactions. An interesting observation in the three-player Goofspiel game is that, after 500 interactions, the PPO algorithm begins to match the performance of ICE. This trend could be attributed to the simpler nature of the randomly generated opponents in the out-of-distribution testbed, which might be easier for PPO to adapt to and exploit over time. Despite this, ICE demonstrates a faster convergence to high- performance levels compared to the PPO algorithm and consistently outperforms the NE strategy. It indicates that ICE is not only capable of quickly adapting to new opponents but also effectively maximizing performance in diverse opponent settings, including both simple and complex strategic environments.\nLastly, we discuss the results against NE opponents, as shown in Fig.11. Our findings reveal that the ICE algorithm achieves better or comparable performance to the NE strategy only in the three-player Kuhn poker game. However, in other game scenarios, while ICE does not outperform the NE strategy, it still maintains a higher level of performance than the PPO algorithm. The less optimal performance of ICE in these cases can be attributed to the highly dynamic game environment and stability of the NE opponents. In three-player games, the player faces two opponents simultaneously, and if both adopt the conservative NE strategy, exploiting them concurrently becomes significantly challenging. This observation highlights an area for future development. Improving the ICE algorithm to more effectively handle situations where multiple opponents employ highly conservative strategies, such as the NE, will be a focus of our future research."}]}