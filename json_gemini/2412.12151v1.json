{"title": "SMARTCAL: An Approach to Self-Aware Tool-Use Evaluation and Calibration", "authors": ["Yuanhao Shen", "Xiaodan Zhu", "Lei Chen"], "abstract": "The tool-use ability of Large Language Models (LLMs) has a profound impact on a wide range of industrial applications. However, LLMs' self-control and calibration capability in appropriately using tools remains understudied. The problem is consequential as it raises potential risks of degraded performance and poses a threat to the trustworthiness of the models. In this paper, we conduct a study on a family of state-of-the-art LLMs on three datasets with two mainstream tool-use frameworks. Our study reveals the tool-abuse behavior of LLMs, a tendency for models to misuse tools with over-confidence. We also find that this is a common issue regardless of model capability. Accordingly, we propose a novel approach, SMARTCAL, to mitigate the observed issues, and our results show an average of 8.6 percent increase in the QA performance and a 21.6 percent decrease in Expected Calibration Error (ECE) compared to baseline models.", "sections": [{"title": "1 Introduction", "content": "The tool-use ability of LLMs has a profound impact on a wide range of applications. Agents that are fine-tuned on various human-computer interaction scenarios such as web browsing (Nakano et al., 2022), code writing (Li et al., 2023a), or even Internet shopping (Yang et al., 2023) have been successfully deployed to streamline workflows and boost efficiency in multiple realms within the industry. Recent research has also achieved impressive results by welding various tools into the step-wise reasoning of Retrieval Augmented Generation (RAG), such as a retriever (Khattab et al., 2023), a database operator (Jiang et al., 2023; Cheng et al., 2023; Hu et al., 2023), or a collection of tools (Schick et al., 2024; Paranjape et al., 2023). While incorporating tools into LLMs is critical for many applications, Mallen et al. (2023) argue that the tool-use step can negatively impact the performance in some circumstances: e.g., when LLMs have reliable parametric memory. This motivates further studies exploring adaptive retrieval strategies (Asai et al., 2024; Maekawa et al., 2024). However, many existing tool-use frameworks rely on either passive in-context learning from existing few-shot examples (Paranjape et al., 2023; Khattab et al., 2023; Hu et al., 2023) or fine-tuning on dedicated datasets (Hao et al., 2023; Schick et al., 2024; Jiang et al., 2023; Cheng et al., 2023). The absence of a model's active thinking in tool-use thus leaves a crucial question under-studied: Are LLMs aware of when to use which tool?\nTo understand the performance of using tools, we conduct a series of experiments under the scenario of open domain QA (Roberts et al., 2020). Our results raise concerns related to the above question: the tracking of LLM tool usage across ChatGPT series (OpenAI, 2023) and llama-3-instruct on Entity Questions data (Sciavolino et al., 2021) shows that on average, a model misuses one or more types of tools in over 20% of its total reasoning steps. Additionally, when the model is asked to report its confidence in selecting a certain tool within each step, more than 90% of its stated confidence falls in the confidence bin where the reported confidence level is higher than the actual answering accuracy, indicating the model's overconfidence with respect to tool choice. The bottom part of the first two columns in Figure 1 demonstrates such tool-abuse phenomenon.\nIn this paper, we propose SMARTCAL, a novel approach to helping mitigate tool-abuse. SMARTCAL consists of three components (i) Self-Evaluation (SE), (ii) Confidence Prior Collection (CPC), and (iii) Augmented Reasoning (AR), which mitigate tool-misuse and provide a more reliable calibration performance. Deployment of SMARTCAL on two different tool-use frameworks,"}, {"title": "2 SMARTCAL: A Tool-Use Recalibration Approach", "content": "Motivated by the self-verification feature that constitutes the reasoning capability in a multi-agent system (Pezeshkpour et al., 2024), we introduce a novel framework SMARTCAL that helps control tool-misuse based on multiple LLM agents. Different from existing approaches that emphasize in-context learning from demonstrations such as Automatic Multi-step Reasoning and Tool-use (ART) (Paranjape et al., 2023) shown in the left column in Figure 1 and Demonstrate Search Predict (DSP) (Khattab et al., 2023), SMARTCAL incorporates extra evaluation steps to examine the legitimacy of tool usage within each step. Additionally, compared to existing tool-use frameworks where each step is controlled by a single agent, SMARTCAL features an enhanced pipeline that promotes the collaboration among the agents, ensuring accurate and reliable tool usage during step-wise reasoning. Specifically, when prompted with an input task, SMARTCAL first derives an optimized strategy about when to use which tool. Then, the collaboration between specialized agents actively interferes with and corrects potential tool-abuse risks in the enhanced pipeline. Table 1 shows a comparison of SMARTCAL with DSP and ART.\nWe provide an overview of our framework in Figure 1, which depicts ART as an example of tool-use frameworks. Meanwhile, SMARTCAL is also compatible with existing tool-use frameworks that incorporate in-context learning with few-shot examples. In our experiments, we report SMARTCAL results on both ART and DSP. We also derive ART (V) and DSP (V) that incorporate verbalized calibration and compare the accuracy with SMARTCAL. Specifically, SMARTCAL has three components: (i) Self-Evaluation (SE) provides tool-use instructions, (ii) Confidence Prior Collection (CPC) collects model-specific confidence prior, and (iii) Augmented Reasoning (AR) combines the previous results into a collaborative pipeline. These components aim to mitigate tool-abuse from the following perspectives: (1) introducing constraints on tool usage from self-evaluation and (2) incorporating tool confidence prior into the reasoning process."}, {"title": "2.1 Self-Evaluation (SE)", "content": "The SE component employs a teacher model $g(x)$ to conduct self-evaluation, where we denote x as the input task plus few-shot tool-use examples. Taking as an example the question \u201cWhere was Robert E. Clary educated?\u201d, SMARTCAL applies $g(x)$ based on two dimensions: (1) $g_{fam}(x)$ for Task Familiarity and (2) $g_{sim}(x)$ for Example Similarity. Familiarity evaluation focuses on assessing whether the parametric memory itself is already sufficient to handle the task. If the task is solvable using model's own knowledge, $g_{fam}(x)$ will include \"[Internal Knowledge]\" as an option and tell the model to be more careful when using tools. Otherwise, 9fam(x) will provide a verdict to encourage tool-use model to use tools. For similarity evaluation, it focuses on extracting specific tools used in the selected examples and picks out the ones that are useful to solve the task. In this example, $g_{sim}(x)$ extracts \u201c[search]\u201d and \u201c[check"}, {"title": "2.2 Confidence Prior Collection (CPC)", "content": "Building on the SE, the CPC component collects model-specific prior calibration information in order to provide more accurate tool confidence scores. We pre-run a heldout subset D with tool-use model $f(x)$, and add self-evaluation instructions I in the reasoning process. Motivated by recent studies that achieve decent calibration performance through verbalized confidence elicitation (Lin et al., 2022; Xiong et al., 2024; Tian et al., 2023), we adapt this technique into step-wise confidence elicitation during the tool-use phase of the agent. Denote a dev set task $t_i \\in D$ with K steps of tool-use, each step containing verbalized confidence $C_j$. We calculate the average $C_{t_i}$ to represent the agent's overall confidence in using tools. The answers from D with calculated confidence scores are binned at a preset stepsize and the accuracy is calculated respectively. The calibration results are then organized as a confidence-accuracy lookup table ${conf\\_level, acc}$. The formula of confidence calculation and confidence prior structure are shown in the CPC block of Figure 1.\nThe performance of the heldout dataset is regarded as the approximation of the underlying confidence-accuracy distribution on the test dataset. The results will serve as the prior reference for the model when editing the output tool confidence using a calibration model."}, {"title": "2.3 Augmented Reasoning (AR)", "content": "Once we obtain the self-evaluation results and confidence prior, the AR component will integrate the previous results in the following procedure. First, self-evaluation instruction I is generated by the teacher model $g(x)$ and is augmented on selected tool-use examples. Then, the tool-use model $f(x)$ is called to output the intermediate reasoning contexts with controlled usage of tools and verbalized tool confidence. Finally, the confidence prior D expressed in a lookup table is used to detect and correct overconfidence or underconfidence on tool usage. We describe the reasoning pipeline of AR using the QA example in Figure 1: tool-use agent outputs reasoning context with more controlled tool usage following instructions in the SE module to include \u201c[search]\u201d and \u201c[Internal Knowledge]\u201d. Calibration model $h(x, d)$ interacts with both tool-use agent result and confidence prior to provide edited confidence evaluations and the final answer to the question."}, {"title": "3 Experiment Setup", "content": "Tasks and Datasets. We perform our experiments under the open-domain QA setup (Roberts et al., 2020) using three benchmark datasets: Mintaka (Sen et al., 2022), PopQA (Mallen et al., 2023), and Entity Questions (Sciavolino et al., 2021). A histogram of the popularity distribution of these datasets can be found in Figure 2.\nFollowing the findings from Mallen et al. (2023) which point out that retrieval is mandatory when the model lacks parametric memory, we sample the tail distribution of the three datasets in Figure 2 to simulate the setting when tool-use agents are dealing with out-of-scope knowledge. Specifically, we set dedicated threshold based on each dataset to construct the low popularity subset.Appendix A.1 offers a detailed description as well as the augmentation of popularity information of the three datasets."}, {"title": "4 Experiment Results and Analysis", "content": "We conduct our study on two tool-use frameworks, DSP (Khattab et al., 2023) and ART (Paranjape et al., 2023). In addition to the original setting, we also introduce verbalized confidence elicitation settings of the two frameworks denoted as ART (V) and DSP (V). In Table 2, we report both settings and compare them in conjunction with SMARTCAL. We can see that when SMARTCAL is augmented on both frameworks, it either surpasses or performs on par in terms of QA performance compared to the baseline setting as well as the verbalized calibration setting. The baseline settings of DSP achieves an average of 41.9% on all datasets, while ART has an average accuracy of 45.4%. In comparison, SMARTCAL achieves 51.5% when adapted to DSP and 53.0% when adapted to ART, with an average advantage of 8.6% in accuracy improvement. We also observe an excessive inferiority in QA accuracy for gpt-3.5-turbo on PopQA dataset, where the model is unwilling to answer most questions. We elaborate this observation in Appendix A.3.1."}, {"title": "4.2 Calibration Performance", "content": "Table 3 presents the ECE score with ART (V) and SMARTCAL. For almost all experiments, ART (V) yields a higher calibration error, with an average ECE of 0.264. SMARTCAL achieves an average ECE of 0.207 on the testing datasets, with an average of 21.6% fewer errors in the confidence alignment. Again for gpt-3.5-turbo, we observe inferiority in ART (V) when tested on PopQA data. We elaborate this observation in Appendix A.3.2.\nIn addition to the ECE performance in Table 3, we also record QA accuracy and ECE performance on less capable GPT models and create a trend plot on Mintaka data in Figure 3. Interestingly, we find qualitatively from the plot that ECE results remain stable with fluctuations between 0.15 to 0.50, despite increasing model capability. In contrast, QA accuracy continues to improve from 47% to near 60% with an evolving model ability."}, {"title": "4.3 Detailed Analysis", "content": "Are LLMs aware of when to use which tool? Our results above raise concerns that tool-misuse poses a threat to the QA performance. Also, despite a certain level of awareness, LLMs lack more targeted tool-use calibration methods. Thus, SMARTCAL aims to provide a preliminary solution from the two perspectives as detailed below."}, {"title": "4.4 Ablation Study", "content": "In this section, we further study the relative importance of each component within SMARTCAL. We choose the ART setup to conduct ablations using the Mintaka data on three models. Specifically, we mask either the SE or the CPC component in SMARTCAL and measure the QA accuracy and ECE respectively. Table 4 and Table 5 showcase the results.\nIn terms of the SE module, we find it useful both in increasing QA performance as well as in lowering calibration error. Among the three models tested when CPC is masked, SE module achieves an average of 2.9% increase in QA accuracy compared with the baseline when both SE and CPC are disabled. It can also be observed that adding self-evaluation also helps the model to be more aware of tool-use confidence reports. The second column in Table 5 with SE module enabled achieves an average of 21.6% lower in calibration error compared to the baseline.\nFor the CPC module, it can be seen from the ablation results that it further helps lower the calibration error, with an average of 39.4% lower in calibration error when comparing with the baseline in Table 5. This further suggests that with the integration of confidence prior, it helps the model become more informed on providing reliable confidence scores."}, {"title": "5 Related Work", "content": "Retrieval Augmented Generation (RAG). Task decomposition techniques (Wei et al., 2022; Yang et al., 2022; Ozturkler et al., 2023; Kazemi et al., 2023; Reppert et al., 2023; Creswell et al., 2023; Puerto et al., 2024; Fang et al., 2024) augmented with retrieved contexts in knowledge-intensive NLP tasks (Karpukhin et al., 2020; Nakano et al., 2022; Li et al., 2023b) have been shown to be very effective in various complex NLP tasks. Recent work (Jiang et al., 2023; Cheng et al., 2023; Hu et al., 2023) has augmented Chain-of-Thought with external database operations to facilitate LLM reasoning on tabular data. Knowledge distillation approaches (Schick et al., 2024; Paranjape et al., 2023; Cai et al., 2024) have also been proposed to teach LLM to create and use tools in order to enhance reasoning performance."}, {"title": "6 Conclusion", "content": "In this paper, we identify tool-abuse in LLM reasoning, which involves a combination of tool-misuse and degraded tool calibration performance. We also observe a consistently high calibration error regardless of increasing model scales. We then propose a novel framework SMARTCAL to mitigate this issue. To our knowledge, this is among the first efforts to study the topic of recalibration for LLM-based tool-use."}, {"title": "7 Limitations", "content": "As for our future work, we would like to extend the proposed method to complex multi-step reasoning tasks. Also, our experiments and results are limited to a subset of the existing datasets to observe tool-misuse behavior. It would be interesting to observe if such behavior remains consistent in more complex datasets elicited by humans that contain multiple reasoning paths."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Dataset Details", "content": ""}, {"title": "A.1.1 Mintaka", "content": "Sen et al. (2022) collect a human elicited dataset that requires complex reasoning with an amalgamation of eight distinct symbolic operations, spanning more than eight different topics, totaling a number of 20,000 labeled questions. We augment the Mintaka dataset with popularity information 2 and make a test set that contains 151 questions with low popularity. Since the number of questions in the training set with low popularity is more limited (50 questions), we randomly sample 200 data to construct the dev set in confidence calibration."}, {"title": "A.1.2 PopQA & Entity Questions", "content": "PopQA (Mallen et al., 2023) and Entity Questions (Sciavolino et al., 2021) are two synthetic datasets that contain knowledge intensive QA tasks. The entities are organized in a triplet containing subject, relationship, and object wrapped in a fixed template to form a question. Given that the two datasets all contain the Wikidata-scraped popularity information, we directly filter out the low popularity section within those datasets, providing a total of 2,349 questions in test set. For the dev set in confidence calibration, we sample 200 questions from PopQA and 500 questions from Entity Questions that are of low popularity in the training set."}, {"title": "A.2 Experiment Details", "content": ""}, {"title": "A.2.1 Models", "content": "InstructGPT. First released in November 2022, InstructGPT is a series of models that is trained by OpenAI to conduct text completion tasks. The original text-davinci series is considered less capable at understanding instructions. OpenAI deprecated their older text-davinci series and updated their instruct models in September 2023 with gpt-3.5-turbo-instruct, making it more capable at following instructions.\nChatGPT. We also include a spectrum of models with different capabilities in the ChatGPT series (OpenAI, 2023), including gpt-3.5-turbo, gpt-4, and gpt-4-turbo.\nLlama-3 Instruct. As an updated version from llama-2 (Touvron et al., 2023), llama-3 is trained with more recent corpora from various sources and achieves a better performance in various benchmarks. Different from the GPT family, Llama models are completely open-source. llama-3-instruct features two models divided by parameter sizes, including llama-3-instruct-8b and llama-3-instruct-70b.\nIn SMARTCAL, gpt-3.5-instruct-0914 is used for similar task selection in the ART framework. For the teacher model in the SE module described in section 2, we select gpt-4-turbo to provide self-evaluation results. For the calibration model in the AR module, we employ gpt-3.5-instruct-0914 for better instruction following to edit the tool-use context. The temperature of all models tested is set to 0.7 in both ART and DSP modules according to the best reported results from Paranjape et al. (2023) and Khattab et al. (2023). The max token length for each reasoning step in ART is set to be 500 and it is 800 in DSP. For maximum steps within the reasoning process, ART has a maximum of 10 steps, while DSP is set to 3 steps."}, {"title": "A.2.2 Evaluation Metrics", "content": "In our experiments, we use a more generic version of Exact Match (EM). Denote the answer from the model as am, and the label as L. The answer is considered correct if:\n$AM \\cap L \\neq \\varnothing \\cup L \\subseteq a_M$                                (1)\nFor calibration evaluation, we use the ECE score. ECE essentially describes the deviation between the model's stated confidence and its true accuracy. It bins the answers according to the model's stated confidence and calculates the average first norm distance between the QA accuracy within the bin and the confidence score. Denoting am as the answer from the model, and PM as the probability assigned by the model that am is correct, p is the actual QA accuracy in this confidence bin. ECE is calculated as follows:\n$E_{P_M} [|P(a_M|p_M=p)-p|]$         (2)"}, {"title": "A.3 Result Analysis", "content": ""}, {"title": "A.3.1 Tool-Use Behavior Analysis", "content": "In this section, we provide more detailed analysis following the reported results in Section 4. As we mentioned earlier, gpt-3.5-turbo achieves unexpectedly low QA accuracy on PopQA dataset on ART framework. We provide several examples that record the history of gpt-3.5-turbo reasoning when tested on PopQA data in Figure 6. We can see from the history that for most of the tested questions, gpt-3.5-turbo refuses to provide a concrete answer that follows the few-shot structure in the ART framework. Instead, it either states that the question needs extra information or it simply can't assist in answering the question. Based on our results, this answer pattern is common regardless of other settings, including the incorporation of verbalized confidence elicitation and SMARTCAL."}, {"title": "A.3.2 Tool-Use Calibration", "content": "Following the calibration performance in Table 3, we observe that under the schema of the verbalized confidence elicitation, the model tends to assign a fixed and consistent confidence score (i.e. 80% confidence whenever it uses the [search] tool in the reasoning step), which in turn makes the aggregated tool-use confidence clustering around a certain confidence interval. This observation is consistent with the results obtained by Lin et al. (2022).\nAdditionally, the unexpected behavior elaborated in Appendix A.3.1 also affects the calculation of calibration performance. When calculating average tool confidence, we default the confidence score to zero when we fail to extract tool usage from the generated reasoning history. An edge case of such a setting is when the overall QA accuracy is also extremely low and those wrong answers happen to be all binned in the lowest possible confidence interval. This will provide misleading ECE result indicating that the model is \"perfectly\" calibrated. The second column of gpt-3.5-turbo in Figure 8 showcase such scenario."}, {"title": "A.3.3 Tool-Use Collection", "content": "We collect the tool usage distribution in both ART and SMARTCAL for different models and demonstrate the results in Figure 7. There is a clear divergence in tool usage between ART and SMARTCAL, where ART tends to include more tools that are unnecessary (such as \u201c[string operations]\u201d or \u201c[code generate]\u201d) to augment its reasoning. The incorrect usage of tools often results in the introduction of redundant information in the context, which consequently degrades QA performance."}, {"title": "A.3.4 Calibration Curve Plot", "content": "We also plot the ECE results for our framework on two approaches in Figure 8. We select calibration results from ART (V) and compare them with ART augmented with SMARTCAL. We segment the model stated confidence into 10 bins and calculate their QA accuracy with respect to each bin. We can see from the plot that under most cases, SMARTCAL has a more sparse and aligned distribution along the reliance curve, i.e. the model stated confidence deviates less from the actual answer accuracy."}, {"title": "A.4 Prompts", "content": "In this section, we list the prompts that constitute the three major components in SMARTCAL described in Section 2. We also provide ART (V) and DSP (V) prompts where we incorporate a verbalized calibration method that elicits model confidence on step-wise tool usage. For SE module, we curate three prompts, including task familiarity SE (Table 8), task similarity SE (Table 9), and tool-use instruction SE (Table 10). In our experiments, we use all three prompts in ART. Given that DSP only incorporates the retriever as the tool to use, we only use the task familiarity prompt in DSP. Note here for confidence prior collection phase in CPC, the prompt is essentially similar to prompts in ART (V) and DSP (V). For AR module, we include the calibration prompt in Table 11."}]}