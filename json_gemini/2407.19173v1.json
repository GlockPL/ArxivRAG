{"title": "FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity\nMeasurement of Persian Social Networks Informal Texts", "authors": ["Seyed Mojtaba Sadjadi", "Zeinab Rajabi", "Leila Rabiei", "Mohammad-Shahram Moin"], "abstract": "Social networks generate an enormous amount of data through user interactions, which needs to be analyzed and\nreviewed. One fundamental task for natural language processing is to determine the similarity between two texts and\nevaluate the extent of their likeness. The previous methods for the Persian language have low accuracy and are unable\nto comprehend the structure and meaning of texts effectively. Additionally, these methods primarily focus on formal\ntexts, but in real-world applications of text processing, there is a need for robust methods that can handle colloquial\nand informal texts. This requires algorithms that consider the structure and significance of words based on context,\nrather than just the frequency of words. The lack of a proper dataset for this task in the Persian language makes it\nimportant to develop such algorithms and construct a dataset for Persian text. This paper introduces a new transformer-\nbased model to measure semantic similarity between Persian informal short texts from social networks. In addition, a\nPersian dataset named FarSSiM has been constructed for this purpose, using real data from social networks and\nmanually annotated and verified by a linguistic expert team. The proposed model involves training a large language\nmodel using the BERT architecture from scratch. This model, called FarSSiBERT, is pre-trained on approximately 104\nmillion Persian informal short texts from social networks, making it one of a kind in the Persian language. Moreover,\na novel specialized informal language tokenizer is provided that not only performs tokenization on formal texts well\nbut also accurately identifies tokens that other Persian tokenizers are unable to recognize. It has been demonstrated that\nour proposed model outperforms ParsBERT, laBSE, and multilingual BERT in the Pearson and Spearman's coefficient\ncriteria specific to the similarity measurement tasks. Additionally, the pre-trained large language model has great\npotential for use in other NLP tasks on colloquial text and as a tokenizer for less-known informal words.", "sections": [{"title": "1. INTRODUCTION", "content": "Social networks have become a prevalent part of\nmodern-day social life, with a vast number of users\nand interactions between them. This creates a vast\npool of textual data that can be harnessed for\nnatural language processing tasks such as text\nsummarization [1][2], spam detection [3], news\nclustering [4], information retrieval [5][6],\nrecommender systems [7], sentiment analysis [8],\n[9], healthcare [10][11] and more. However,\nprocessing these often-short texts presents\nchallenges due to their conversational and\ninformal nature.\nGloVe [12] and Word2Vec [13] are neural\nnetworks-based word embedding methods that\ngenerate static representation vectors by analyzing\nsemantic, syntactic relationships between words.\nWhile these methods do not consider the context\nof the words, transformer-based methods such as\nthe BERT [14] use the context of each word to\ncreate a word vector in their equations. This\nfeature help better understanding the meanings of\nsentences, because like the real language where\neach word has different meanings, each word\nwould have different embedded vectors.\nWith the BERT language model and its derived\nmethods, various natural language processing\ntasks in English and other languages have been\naccomplished with very good results [15]. Data\nfrom various sources including Wikipedia, news,\nand books have been used to train these large\nlanguage models in various medical and scientific\napplications. In the Persian language, ParsBERT\n[16] is the only model that has been created for the\nPersian language with official written text from\nWikipedia and books.\nSocial networks texts differ from formal written\ntexts like Wikipedia and news articles in terms of\ntheir characteristics. They are brief and use\ninformal grammar, irregular vocabulary,\nabbreviations, and hashtags. Applying pre-trained\nlinguistic models designed for large-scale regular\ntext sets with formal grammar and regular\nvocabularies to analyze social networks short texts\ncan be challenging. This is mainly because, the\ninformal nature of social networks texts may not\nbe compatible with these models. To the best of\nour knowledge, there are no pre-trained language\nmodel on a large-scale Persian social network\ncorpus.\nIn order to address this issue, we train the first\nPersian language model for social network short\ntexts through training with a 48 GB collection of\n104 million Persian tweets which is based on the\nBERT-base model configuration. We evaluate our\nmodel and compare it to strong competitors,\nnamely ParsBERT and multilingual BERT, and\nlaBSE in the downstream task of semantic\nsimilarity measurement of Persian informal short\ntexts. Due to the lack of a suitably labeled dataset\nfor this task in the Persian language, we collected\na dataset containing 1123 text-pair samples and\nlabeled them with 4 annotations, which we use in\nmodel testing and comparing methods.\nExperiments show that our model performs better\nthan other models. Our contributions are as\nfollows:\n\u2022\n\u2022\n\u2022\nProposing the first large-scale pre-trained\nlanguage model for Persian informal short\ntexts (FarSSiBERT).\nPre-training the first large language model\nwith more than 2 billion tokens in colloquial\nlanguage from scratch.\nConstructing the first annotated semantic\nsimilarity dataset for informal short texts with"}, {"title": "2. RELATED WORKS", "content": "Since this research discusses the task of measuring\nsemantic similarity of informal texts using a\nlanguage model, we present related works in two\nparts: Language modeling and Semantic Similarity\nTask.\n2.1. Language Modeling\nThe BERT model was the pioneering work to show\nthat large language models can be efficiently fine-\ntuned for other natural language processing tasks.\nThis model includes two phases: pre-training and\nfine-tuning, which in the former phase are trained\nwith Masked Language Modelling (MLM) and\nNext Sentence Prediction (NSP) without\nsupervision. MLM refers to the prediction of\nmasked words in a sentence, while NSP involves a\ntask for predicting whether the second sentence in\na pair of sequences is a true substitute for the first\nsentence. In the fine-tuning phase, the pre-trained\nmodel is tuned for a specific task with labeled data.\nIn the paper, the author tests two types of BERT\nmodels with 12 layers (BERT-base) and 24 layers\n(BERT-large) Transformers [17].\nIn [18], ROBERTa is presented, which has used\n160 GB of textual data to train a large language\nmodel. This model is pre-trained with larger batch\nsize and more training steps on longer sequences\n(512 vs. 128) as compared to BERT. Additionally,\nthe research findings indicate that utilizing the\nNSP procedure is not useful for end-task\nperformances.\nMultilingual BERT [19] is a Transformer-based\nmodel that is trained using Wikipedia data for 104\nworld languages. This model only uses the MLM\nprocedure for pre-training. Although multilingual\nlanguage models have their advantages, it is\nevident that monolingual models, specifically\ntrained for the tasks of that language, outperform"}, {"title": "2.2. Semantic Similarity Task", "content": "There is a limited amount of research available on\nmeasuring semantic similarity in Persian language\ndue to the absence of appropriate datasets.\nMajority of the studies conducted have relied on\ntraditional techniques and have not utilized\nlanguage models. Furthermore, the datasets used\nare all formal texts that have been sourced from\ninstances of plagiarism or subtitles of films and TV\nseries. Below are some of these studies that have\nbeen reviewed.\nIn [22], Moghadam Emami et al presented a\nmethod for semantic similarity measurement on\nshort Persian texts using a deep neural network.\nTheir proposed method consists of three main\nparts: in the first step, they collect data and form\nthe desired dataset. The next step is pre-processing\nand text normalization. In the final stage, semantic\nsimilarity is done with the help of creating a three-\nlayer neural network model for vocabulary\ntraining. For this study, the researchers utilized a\ndataset containing 35,266 pairs of sentences that\nwere taken from movie and show subtitles. In\norder to accurately measure semantic similarity\nbetween text pairs, they made use of the\nparaphrased sentences that were available in the\nmovie subtitle database.\nShahabi et al [23] conducted research where they\nintroduced a fuzzy approach to segment Persian\ntext. Their method involves using a semantic\nsimilarity measurement between sentences,\nachieved through fuzzy similarity and fuzzy\napproximation relation. They first obtain the roots\nof Persian words and verbs, followed by creating a\nfuzzy similarity relationship with synonyms.\nThrough this, sentences with similar meanings are\ncalculated using fuzzy proximity relationship. To\ncalculate the proximity of sentences in a fuzzy\nmanner, they use a semantic external knowledge\nbase. Their method serves as a text summarizer\nthat summarizes valuable information from\nPersian text. However, its accuracy and reliability\nare limited due to its dependence on an external\nknowledge base to find synonyms."}, {"title": "3. PROPOSED MODEL", "content": "This section outlines the methodology used to\ncreate the proposed model. The model is created\nby pre-training a large language model that follows\nthe BERT-base architecture. The pre-trained\nmodel is then utilized to extract word embedding\nvectors in the downstream task of measuring\nsemantic similarity in Persian informal short texts.\nIn Figure 1, the training steps of the proposed\nmodel are illustrated. The process begins with\nextracting the social network data texts followed\nby necessary pre-processing. The cleaned data is\nthen broken into appropriate tokens using the\ntransformer architecture and fed to the model for\ntraining. The transformer model consists of 12\nlayers, and its weights are utilized to extract the\nembeddings of both sentences and words."}, {"title": "3.1. Training data", "content": "Earlier, pre-trained models like ParsBERT were\ntrained using formal data from sources like\nWikipedia and books. Although these models have\ntheir strengths, they have limitations in detecting\ninformal language tokens and the irregular\nstructure of short texts. To overcome this, we\ntrained our model using data from social networks\nthat feature colloquial and informal language. We\nhave gathered a vast collection of Persian-\nlanguage tweets from Twitter, consisting of over\n104M tweets from 250 million between the years\n2019 to 2023. Our training dataset size is about\n38GB, with 104551461 unique tweets and 2.18"}, {"title": "3.2. Pre-processing", "content": "Once the training dataset is gathered, it is crucial\nto perform specific pre-processing steps, such as\ncleaning,\nremoving, replacement, and\nnormalization, to transform the dataset into a\ncompatible format. Table 1 lists these essential\nsteps. In semantic methods processing,\nreplacement is a crucial step as it removes tokens\nthat do not contribute to the meaning of the text.\nFor example, without replacing the @username\nwith the term USERNAME, the word of the\nperson's name mentioned in the middle of tweet,\nmay add other meanings to the tweet, which will\ncause the model to misunderstand. In addition,\nbecause Emojis carry meaning and to preserve\ntheir meaning, a literal equivalent has been\nsubstituted for each emoji in the text."}, {"title": "3.3. Pre-training", "content": "The proposed model is based on a transformer and\ntrained according to BERT-base architecture,\nwhich includes 12 hidden layers, 12 attention\nheads, and 768 hidden sizes. The pre-training of\nthe BERT model consists of two processes\naccording to the type of data one or both processes\ncan be used in model training. In this study, the\nMasked Language Model (MLM) is used for pre-\ntraining for the following reasons:\n\u2022\n\u2022\n\u2022\nInformal data do not follow a specific\nstructure for sentences and it is impossible\nto separate sentences.\nShort texts are generally single sentences\nThere is no suitable Persian-labeled data\nfor the Next Sentence Prediction (NSP)\nprocedure of informal texts.\nLikewise, a masked language model is used to\ntrain the model to predict masked tokens using\ncross-entropy loss. Out of the total number of\ntokens, 15% of them are masked by the [MASK]\ntoken, and the model is trained by trying to predict\nthese masked words. The size of the hidden layer\nis 768, the batch size is 32, the length of the input\nstrings is 512, and the learning rate is 1e-12. We\noptimize the model using Adam [25] and pre-train\nFarSSiBERT for 5 epochs in about 23 days in\nabout 16M steps across one A100 GPU (40GB\nRAM). Figure 2 shows the loss curve for model\ntraining in 5 epochs."}, {"title": "3.4. Semantic Similarity Measurement", "content": "As previously mentioned, we utilize the pre-\ntrained model to measure the semantic similarity\nof short informal Persian texts. The process is\nillustrated in Figure 3, which outlines the use of\nFarSSIBERT. Two short texts are inputted into the\npre-trained model after necessary pre-processing\nand tokenization. Token embeddings are extracted\nusing the average of the last 4 layers of\nFarSSiBERT, and sentence embeddings are\ncreated through a pooling operation. We\nexperimented with three pooling strategies: the\nCLS token output, computing the mean of all\noutput vectors (Mean strategy), and computing the\nmaximum time of the output vectors (Max\nstrategy). The Mean strategy is the default\nconfiguration.\nAfter extracting the embedding vectors of the\nsentences, the degree of similarity between the two\nvectors is calculated by the cosine distance and a\nsimilarity score between [0,5] is given to these\ntwo texts. Due to left-to-right and right-to-left text\nprocessing, the proposed model has well\nunderstood and trained the semantic similarities\nbetween words in the context, which is suitable for\nsemantic tasks."}, {"title": "3.5. Metrics", "content": "Pearson's and Spearman's coefficients are common\nmeasures to measure the semantic similarity of\ntexts [26], [27], which are described below."}, {"title": "3.5.1. Pearson coefficient", "content": "The Pearson coefficient is a measure of how\nclosely the results of a given test match up with\nhuman judgments. It is computed as Eq.1.\nr=\\frac{\\Sigma(x_i \u2013 x)(y_i \u2212 \u04ef)}{\\sqrt{(x_i - x)^2 - (y\u00ec \u2013 \u04ef)^2}}(1)\nWhere x\u2081 = values of the x-variable in a sample,\nx = mean of the values of the x-variable,\ny\u2081 = values of the y-variable in a sample, y = mean\nof the values of the y-variable."}, {"title": "3.5.2. Spearman's Coefficient", "content": "The classification is determined by measuring the\nsimilarity between sentences and comparing it to\nthe similarity determined by human judgments.\nSpearman is calculated as follow:\n\u03c1 = 1 - \\frac{6 \\Sigma d_i}{n(n\u00b2 \u2013 1)}(2)\nWhere di corresponds the difference of the ranks\nof xi and yi, and n corresponds to the pair\nsentence number."}, {"title": "3.5.3. Mean Square Error", "content": "Also, for error analysis, the Mean Square Error\n(MSE) criterion is used, the equation of which is\ngiven below:\nMSE = \\frac{1}{n} \\Sigma (I_{desired} - I_{predicted})^2\nWhere ldesired is real label and Ipredicted is the\nlabel predicted by the model."}, {"title": "4. DATASET CONSTRUCTION", "content": "4.1. Persian dataset for semantic similarity\nMashhadi Rajab and Shamsfard [28][29]created a\ndataset for evaluating plagiarism detection systems\nusing scientific texts. The collection contains\n11089 documents that are prepared from articles in\nthe fields of computer science and electrical\nengineering. There are 11603 cases of plagiarism\nin this collection. Only one percent of this corpus\ncan be used to measure semantic similarity. Also,\nthey are official scientific documents and due to\nthe focus on the field of computer science and\nelectricity, this corpus includes a smaller range in\nterms of data variety.\nMinaei and Niknam [29][30] collected a set of\n3218 documents and 2308 cases of plagiarism to\nfind similar parts of two given documents. They\nused Wikipedia articles to build their collection\nand a set of text operations to generate instances of\nplagiarism such as copying text from a source\ndocument or replacing words with their synonyms.\nSharif Abadi [31] prepared a collection to detect\nplagiarism from Persian scientific texts containing\ncases of plagiarism. 4707 articles used in this\ncollection have been collected from the fields of\nhumanities, sciences, veterinary medicine,\nagriculture and natural sciences, engineering, art,\nand architecture. The collection contains 5000\nplagiarism cases of various lengths. Semantic\nplagiarism cases are only 20% of this collection,\nequivalent to 1172 cases."}, {"title": "4.2. FarSSiM", "content": "To measure semantic similarity, datasets consist of\npairs of paraphrased text which are labeled to\nindicate the level of similarity between them.\nFinding these kinds of samples with equal meaning\nrequires identifying paraphrases. A method to do\nthis is identifying common words between tweets\non a particular topic and time. Once the pairs of\nparaphrased\ntweets are identified, expert\nannotators compare them semantically and assign\na score as a label. The final score for each pair is\ndetermined by the average of the annotators' votes\nand inserted into the dataset [35][36][37].\n4.2.1. Identification of candidate pairs\nThe required data is collected by a web crawler\nfrom Persian Twitter at specific times when there\nis a popular topic. To identify the candidate pairs,\nall the data was pre-processed, and items that did\nnot contribute to the semantic similarity\nmeasurement of the texts (tweets with a length of\nless than 4 words) were removed (values greater\nthan and less than 4 were tested and the best result"}, {"title": "4.2.2. Semantic Similarity Scoring", "content": "Rather than categorizing text pairs as either\nparaphrasing or non-paraphrasing, we assign a\nmore accurate semantic similarity score. Our team\nfollowed the guidelines outlined for the semantic\nsimilarity task, and annotators scored each text pair\ndirectly based on the criteria outlined in Table 2.\nWe hired four Persian-speaking experts as\nannotators and provided them with a set of\nguidelines to score the data. Our instructions\nincluded examples and explanations of similarity\nscores to ensure consistency. The label for each\ndataset entry was determined by averaging the\nscores of the four annotators. All Persian\nannotators are native speakers who possess proper\nknowledge and understanding of social media,\nPersian grammar, and background knowledge of\nnatural language processing. Furthermore, all\nannotated documents were reviewed by an\nexperienced annotator.\nIn constructing the dataset, the paraphrases in the\ntexts of the social network posts were selected\nbased on the following criteria:\n1) Posts from various domains and topics were\ncollected without limiting to trends in one topic. 2)\nAdvertising posts were excluded from the\ncollection. 3) To maintain diversity, posts from\nboth influential users with a high number of likes\nand ordinary users with low influence were\nselected. We also included the standard deviation\nand variance of their scores in the dataset for\nreference. Three examples of the instances\navailable in the FarSSiM are given in Table 3. For\nmore information, we have provided the English\ntranslation of the dataset text alongside the Persian\nexamples."}, {"title": "5. EVALUATION", "content": "We compare the performance of FarSSIBERT\nwith baselines on the downstream NLP task of\ninformal\nshort-text\nsemantic\nsimilarity\nmeasurement, using two datasets.\n5.1. Dataset\nAs discussed in Section 4.2, there was no adequate\nPersian dataset available for our research task, so\nwe had to gather and prepare our own test dataset.\nHowever, we did come across another dataset\ncalled FarSick [40], which consists of 10,000 pairs\nof short texts in Persian. This dataset was\noriginally in English and was later translated into\nFarsi. The text pairs in this dataset are written in\nformal language and lack colloquial and informal\ndata. Each sample is assigned a numerical label\nranging from 0 to 5.\nTo assess the performance of the FarSSiBERT\nmodel, we used not only the dataset we collected\nfor this research (FarSSiM) but also the FarSick\ndataset to evaluate how well the model performs\non formal short texts.\n5.2. Baselines\nmultilingual BERT model [19]: a pre-trained\nmodel that was trained in 104 languages of the\nworld and used the data available in Wikipedia for\ntraining.\nParsBERT [16]: As mentioned in previous works,\nthe ParsBERT model is trained with about 48\nmillion Persian data and fine-tuned in various\ntasks.\nlaBSE [21]: A large language model based on the\nBERT architecture, trained for 104 world\nlanguages with official data from Wikipedia and\nother sources.\nFarSSiBERT-6M: Since the number of training\ndata has a significant effect on the performance of\nthe model, a smaller model of the proposed method\nis trained with fewer data (6M) to be compared\nwith the main model of this research,\nFarSSIBERT-104M."}, {"title": "5.3. Tokenizers Comparison", "content": "In this study, a new tokenizer is introduced that can\nidentify new informal tokens and outperforms\nother tokenizers. We compare the FarSSiBERT\ntokenizer with the ParsBERT tokenizer and\nevaluate their performance on both official and\nunofficial data types, which differs from previous\nstudies. Examples in Table 4 highlight how the\nFarSSiBERT tokenizer model compares to the\nParsBERT model. The ParsBERT model's\ntokenizer is incompetent to recognize informal\nwords as independent tokens, leading to these\nwords being broken down into smaller tokens. This\ncan cause the model to misunderstand sentence\nmeanings. In contrast, the FarSSiBERT model\nidentifies each of these words as meaningful\ntokens, enabling the model to understand them\naccurately."}, {"title": "5.5. Results", "content": "The proposed model is evaluated in the\ndownstream task of semantic similarity of\ninformal Persian short texts in social networks.\nTables 5 and 6 respectively show the values of the\nscores obtained by the models in the FarSick and\nFarSSiM datasets. For each pre-trained model,\nPearson and Spearman correlation values are\ncalculated.\nIt is evident that the FarSSiBERT-104M model\noutperforms all other models. Despite the fact that\nthe instances in the FarSick dataset are of a formal\nlanguage type, it was anticipated that the\nParsBERT model, which was trained with this type\nof formal data, would outperform the other\nmodels. However, the proposed model performed\nabout 3% better than ParsBERT. Furthermore, the\nmodel outperformed the laBSE model, which is\ntrained with formal text. The multilingual BERT\nand FarSSiBERT-6M models ranked lower and\nshowed the lowest performance, indicating that the\nnumber of training data has a significant impact on\nthe model's performance.\nIn another experiment, the results of comparisons\nwith other methods are shown in Table 6. The"}, {"title": "6. DISCUSSION", "content": "The previous section compared the model's\nperformance with baseline models. The results are\ninfluenced by how the model performed with\ndifferent data inputs and labels. To gain a better\nunderstanding of these findings, an analysis of the\nmodel's errors is conducted. The proposed model\nis evaluated alongside baseline models to predict\nappropriate labels for similarity measurement. The\nerror between the predicted label and the original\nlabel is calculated for each data sample, and the\nresults can be found in Table 7. This table displays\nthe mean square error for each label across\ndifferent models in the FarSSiM and FarSick\ndatasets. For instance, the column (0<L <1)\npresents the mean square error for data samples\nwith labels between 0 and 1.\nFarSSiM error analysis: Based on the error values\nfor different labels in the FarSSiM dataset, it is\nevident that, in general, all models perform better\nwith larger labels (indicating more similarity of\ntext pairs) than with smaller labels. The models\nshow the lowest error in labels between 4 and 5.\nFor data with labels between 0 and 1, the proposed\nmodel has the least error, with an MSE value of\n1.23, outperforming other models. On the other\nhand, the multilingual BERT model displays the\nworst performance with an error of 5.09, indicating\nits poor performance in dealing with non-\nparaphrased data for measuring semantic\nsimilarity.\nThe labSE model has slightly less error in data\nwith labels between 1 and 2, but the proposed\nmodel maintains good performance throughout the\nrest of the labels, outperforming other models.\nFigure 4 shows that the majority of labels in the\nFarSSiM dataset are placed in labels between 2 and\n4. This distribution and Table 7 values reflect the\neffectiveness of the proposed model in detecting\nsemantic similarity in borderline conditions\nbetween paraphrasing and non-paraphrasing of\ntwo texts. Accordingly, according to Table 7, the\nproposed model has the least error in these labels\nand can detect semantic similarity between two\ntexts better than other models.\nFarSick error analysis: The error values in the\nFarSick dataset decrease as the labels increase,\nsuggesting improved model performance for\nsimilar texts. There are no labels between 0 and 1\nin this dataset, so the values in this column are null.\nThe proposed model demonstrates lower error\nrates across all labels and better identifies\nsimilarities in informal short texts. Following the\nproposed model, the labSE model is ranked next,\nwith the ParsBERT and multilingual BERT\nmodels following. The table values indicate that\nerror values greater than 1 have been obtained for\ndata labeled between 2 and 4 -representing the\nboundary conditions of paraphrasing and non-\nparaphrasing of two texts- indicating higher errors\ncompared to the FarSSiM dataset. This\ndemonstrates the good quality of the FarSS\u0130M\ndataset, as most models displayed lower errors for\nthese values.\nAfter analyzing errors in various labels and\nevaluating the performance of the proposed model,\nit can be concluded that using appropriate data and\na transformer-based model that can analyze texts\nbidirectionally and identify the semantic\nrelationship between tokens helps measure the\nsemantic similarity of informal short texts from\nsocial networks. The use of a special tokenizer,\nproper pre-processing, data cleaning methods, and\naccurate substitutions significantly contribute to\nthis result. Unlike the baseline models, the\nproposed models only utilized the masked\nlanguage modeling and pre-training procedure for\nmodel training. If higher-quality data can be\nobtained for fine-tuning, they are expected to\nexhibit improved accuracy and performance."}, {"title": "7. CONCLUSION", "content": "We have presented the first large-scale language\nmodel FarSSiBERT, which is pre-trained for short\ninformal Persian texts. We demonstrate the utility\nof FarSSIBERT by showing outperforming its\nbaseline ParsBERT, laBSE, and multilingual\nBERT and helping provide better performance\nthan previous SOTA models for the downstream\ntask of semantic similarity. Also, due to the fact\nthat there is no suitable dataset for the task of\nsemantic similarity measurement of informal\nPersian short texts, we collected and labeled the\nFarSSiM dataset containing 1123 text pairs for\ntesting the model.\nOur pre-trained FarSSiBERT was developed using\nover 104 million Persian tweets and over 2.18\nbillion tokens. It trained in five epochs and was\nevaluated based on Pearson and Spearman criteria\nfor the semantic similarity test. The tokenizer used\nin this model is highly effective at identifying\ncolloquial and informal tokens. To facilitate the\nuse of the FarSSiBERT in other natural language\nprocessing tasks, a Python package has been\ncreated and made publicly available, along with\nthe FarSSiM dataset."}]}