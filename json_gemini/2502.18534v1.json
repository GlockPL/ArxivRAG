{"title": "MAFE: Multi-Agent Fair Environments for Decision-Making Systems", "authors": ["Zachary McBride Lazri", "Anirudh Nakra", "Ivan Brugere", "Danial Dervovic", "Antigoni Polychroniadou", "Furong Huang", "Dana Dachman-Soled", "Min Wu"], "abstract": "Fairness constraints applied to machine learning (ML) models in static contexts have been shown to potentially produce adverse outcomes among demographic groups over time. To address this issue, emerging research focuses on creating fair solutions that persist over time. While many approaches treat this as a single-agent decision-making problem, real-world systems often consist of multiple interacting entities that influence outcomes. Explicitly modeling these entities as agents enables more flexible analysis of their interventions and the effects they have on a system's underlying dynamics. A significant challenge in conducting research on multi-agent systems is the lack of realistic environments that leverage the limited real-world data available for analysis. To address this gap, we introduce the concept of a Multi-Agent Fair Environment (MAFE) and present and analyze three MAFEs that model distinct social systems. Experimental results demonstrate the utility of our MAFEs as testbeds for developing multi-agent fair algorithms.", "sections": [{"title": "1. Introduction", "content": "As artificial intelligence (AI) and machine learning (ML) become increasingly embedded in everyday life, concerns about their potential to exacerbate social biases have grown (Sweeney, 2013; Angwin et al., 2016; Larson et al., 2016; Buolamwini & Gebru, 2018). Algorithms aimed at addressing these issues, known as algorithmic fairness, must minimize bias, integrate seamlessly into decision-making systems, and provide meaningful interventions against systemic disparities. Early approaches focused on mitigating static biases, such as group (Kamiran & Calders, 2012; Hardt et al., 2016), individual (Dwork et al., 2012), and causal (Kusner et al., 2017; Coston et al., 2020) bias.\nHowever, studies have shown that these methods can fail to address, or even worsen, the differential harms that emerge as decisions unfold over time (Liu et al., 2018; D'Amour et al., 2020). For example, in healthcare, a treatment algorithm may allocate resources equally at diagnosis, ensuring the fairness of an isolated decision. However, as time progresses, differing demographic factors such as access to follow-up care or socioeconomic disparities-can lead to worsened outcomes for certain groups, highlighting the need for interventions that can continuously address and mitigate these disparities dynamically over time.\nTo address this issue, recent works have reframed fairness as a sequential decision-making problem, often modeled using Markov Decision Processes (MDPs) (Yin et al., 2024; Xu et al., 2024) or structural causal models (Hu & Zhang, 2022). While these methods often model the decision-making process of an agent at a single decision point within the system, many real-world systems involve multiple decision points that cumulatively impact the entire system. For example, in healthcare, one decision point might be providing insurance to individuals, while another consists of ensuring they have access to timely and quality healthcare services, such as a hospital bed, when such resources are limited. If some individuals receive insurance but live in areas with restricted healthcare access, disparities in outcomes may still emerge. Comprehensive strategies such as insurance assistance or access to healthcare-are thus essential to address systemic disparities and ensure fairness across decision stages.\nThis underscores the need for realistic frameworks to study fairness dynamics in multi-agent systems over time. However, the lack of realistic multi-agent environments for studying such systems presents a major obstacle for developing and evaluating effective algorithms for addressing disparities in various social contexts. To bridge this gap, we propose a framework for Multi-agent Fair Environments (MAFEs) that simulate modular social systems.\nSummary of Contributions: We introduce MAFEs as a novel framework for analyzing fairness in multi-agent systems and present three benchmark MAFEs designed to simulate critical social domains: loan processing, healthcare, and higher education. To demonstrate the versatility of MAFEs and facilitate empirical evaluation, we present a cooperative use case. Within this use case, we define success metrics tailored to algorithms operating in these MAFEs and show-case the adaptability of a multi-agent reinforcement learning (MARL) algorithm in this setting. Additionally, we conduct thorough experimental analysis of our MAFEs and offer insights into their dynamics."}, {"title": "2. Related Works", "content": "2.1. Single-Agent Long-Term Fairness.\nTo overcome the limitations of static fairness formulations, several approaches have re-framed fairness as a dynamic systems problem. Effort-based fairness analyzes the differing efforts required by groups to achieve outcomes (Heidari et al., 2019; Guldogan et al., 2022), while causal models use structural causal models and interventions to introduce fairness (Hu et al., 2020; Hu & Zhang, 2022). Another approach incorporates fairness within dynamic systems through reinforcement learning (RL), with early work using multi-armed bandits (Joseph et al., 2016) and recent efforts employing Markov Decision Processes (MDPs). Puranik et al. (Puranik et al., 2022) introduce the Fair-Greedy policy in an admissions case study, balancing applicants' scores with group proportions. Yin et al. (Yin et al., 2024) frame the long-term fairness RL problem to maximize profits while minimizing unfairness, measured by regret and distortion. To address temporal bias, Xu et al. (Xu et al., 2024) propose a fairness measure based on the ratio-after-aggregation and modify the proximal policy optimization algorithm (PPO) to satisfy this constraint. Though these works reduce temporal disparities, they do not analyze their source. Deng et al. (Deng et al., 2024) use causal analysis to trace sources of inequality over time. While these works extend static fairness to long-term outcomes, Hu et al. (Hu et al., 2023) argue that long-term fairness should focus on the convergence of input feature distributions, proposing a PPO variant with pre-processing and regularization to balance short- and long-term fairness.\n2.2. Multi-Agent Long-Term Fairness.\nIn systems with multiple decision-making entities, modeling fairness explicitly across agents becomes crucial for understanding their interventions and their effects on system dynamics. Several studies have explored fairness in multi-agent contexts. Jiang and Lu (Jiang & Lu, 2019) introduce the Fair-Efficient Network, a hierarchical RL model where homogeneous agents aim to balance fairness and efficiency. Zheng et al. (Zheng et al., 2022) use two-level deep RL to design agents that reduce income inequality via taxation and redistribution, with equity measured by the Gini Index. Reuel and Ma (Reuel & Ma, 2024) provide a survey on fairness in RL, covering both single- and multi-agent systems. They highlight key gaps, such as fairness in RL from human feedback, and emphasize the challenges of ensuring fairness in dynamic real-world environments, which underscores the need for realistic simulation environments.\n2.3. Long-Term Fairness Environments.\nA major challenge in long-term fairness research is designing appropriate environments for measuring, simulating, and assessing fairness algorithms. Among the growing body of research on long-term fairness, some works have introduced environments that consider the complexities of real-world decision-making. For example, D'Amour et al. (D'Amour et al., 2020) introduce lending and attention environments, while Atwood et al. (Atwood et al., 2019) focus on infectious disease environments. However, these environments are single-agent based. Real-world systems, by contrast, often consist of multiple interacting entities that influence outcomes. By not explicitly modeling these entities as agents, such environments limit the ability to flexibly analyze the various forms of intervention and the effects that these different entities may have on the system's underlying dynamics.\nAlthough there are existing multi-agent fair environments, such as those developed by Jiang and Lu (Jiang & Lu, 2019), their approach is limited to focusing on fairness among homogeneous agents. By modeling fairness at the agent level rather than for a broader population, their environments lack the necessary structure to analyze group fairness. Additionally, their environments are simpler compared to real-world social systems, where stakeholders in fields like healthcare and finance have diverse decision-making processes. In contrast, our proposed framework supports heterogeneous agents with varied decision-making strategies and emphasizes group fairness, ensuring equitable outcomes across demographic groups and enabling a more comprehensive analysis of societal impacts. Furthermore, while Zheng et al.'s (Zheng et al., 2022) environment offers a detailed model, its context is restricted to economic outcomes. Our framework spans multiple domains-including loan allocation, healthcare, and education-each requiring tailored approaches and supporting multiple fairness measures across diverse contexts."}, {"title": "3. MAFE: A Fair Dec-POMDP Framework", "content": "In this section, we formulate the notion of a MAFE by extending the standard decentralized partially observable Markov decision process (Dec-POMDP) (Gronauer &\nDiepold, 2022) into an eight element tuple given by:\n$(N, S, \\{A_n\\}, \\{O_n\\}, T, \\gamma, \\{c^{(R)}\\}, \\{c^{(F)}\\})$.\nIn this tuple, $N = \\{1, ..., N\\}$ denotes the set of N agents. S represents the set of unobserved global system states. $A_n$ and $O_n$, represent the action and observation spaces of Agent n, and $A = \\bigcup_{n=1}^{N} A_n$ and $O = \\bigcup_{n=1}^{N} O_n$ represent the joint action and observation spaces of all agents. Each agent's observation consists of a subset of the global state information, meaning that $O_n \\subseteq S$. $T : S \\times U \\times A \\rightarrow S$ is the state transition function which probabilistically updates the environment's state given its current state and the actions taken by all agents at the current time step. $\\gamma$ is the discount factor. Figure 1a provides an illustration of this MAFE framework. It alters the standard Dec-POMDP with two key components:\nReward Component Functions: We replace the standard agent reward functions with reward component functions $\\{c^{(R)}\\}$. Each reward component function, $c^{(R)} : S \\times U \\times A \\rightarrow \\mathbb{R}^{j_n}$, produces a vector of dimension $j_n$, which may vary across agents. These vectors are used to measure agents' rewards in the environment. For instance, the reward component function of a central planning agent could output the following vector:\n$[\\#(\\text{Deaths})_{t}, \\#(\\text{Ended Illness})_{t}]^{T}$.\nFairness Component Functions: We introduce fairness component functions $\\{c^{(F)}\\}$. Each fairness component function, $c^{(F)} : S \\times U \\times A \\rightarrow \\mathbb{R}^{j_n}$, produces a vector of dimension $j_n$, which may vary across agents. These vectors are used to evaluate the fairness of agents' actions in the environment. For example, to evaluate geographic disparities in moralities in Regions A and B, a central planning agent's fairness component function might output the following vector:\n$[\\#(\\text{Deaths})^A_{t}, \\#(\\text{Deaths})^B_{t}, \\#(\\text{Ended Illness})^A_{t}, \\#(\\text{Ended Illness})^B_{t}]^{T}$,\nwhere mortality rates are calculated as the ratio of individuals who have passed away to the total number of previously ill individuals who have recovered or passed away in each region. Using component functions in our design provides the flexibility to calculate fairness and rewards using either step-wise or aggregation-based metrics, as described by (Xu et al., 2024). See Appendix D.5 for more details.\nTo illustrate the MAFE framework, consider a healthcare example (Figure 1b) involving three agents: an insurance company, a hospital, and a central planning agent. These agents make decisions such as setting premiums, allocating medical resources, and funding public health initiatives, impacting outcomes like population health, including the number of individuals who remain healthy, fall ill, or die.\nA key strength of the MAFE framework is its flexibility in modeling multi-agent interactions. Reward and fairness components can be tailored to specific scenarios, allowing for the modeling of diverse agent relationships. For example, agents may share a common utility function in a cooperative setting or have distinct, even conflicting, goals in a competitive one. In a healthcare setting, the insurance company may prioritize cost minimization, the hospital may focus on improving patient outcomes, and the central planner may emphasize equity. MAFEs also allow users to define success metrics for each agent, such as patient recovery rates or equity in resource distribution, providing a more nuanced understanding of fairness in complex systems.\nAnother important feature of MAFEs is their support for heterogeneous agents, each with distinct observation and action spaces, reflecting the varying roles and information access of real-world entities. For example, the insurance agent may observe and offer premiums to the entire population, while the hospital only observes patients seeking treatment. This design captures disparities in information and decision-making capabilities across agents.\nIn Section 5, we demonstrate how to use MAFEs through a cooperative use case, where we define specific reward and fairness metrics for success. This use case illustrates how MAFEs can be tailored to various scenarios, serving as a powerful tool for studying fairness in multi-agent systems."}, {"title": "4. Designed MAFEs for Social Applications", "content": "In this section, we introduce three distinct MAFEs, which simulate the dynamics of a loan processing pipeline, healthcare system, and higher education system. These MAFEs are implemented within a flexible framework for multi-agent environments, similar to popular MARL ecosystems like Gym, Gymnasium, and PettingZoo. Each MAFE follows the standard pattern of producing observations, rewards, and dones, and allows agents to interact through the env.step(action) method, which updates the environment based on the current action. In our implementation, rewards comprises both the reward and fairness component vectors. This design enables easy integration with existing MARL libraries, allowing researchers to extend the environments by defining new agents, reward functions, and fairness constraints. We summarize the three MAFEs below, and a detailed description of them is provided in Appendices E-G.\nLoan MAFE: This MAFE simulates a financial institution's loan pipeline with three agents: (1) an Admissions Agent that approves or rejects loan applications; (2) a Funds Disbursement Agent that controls the timing of loan disbursements; and (3) a Debt Management Agent that manages the percentage of debt adjustments applied to individuals' payments. Throughout an episode, individuals apply for loans, some of whom are approved while others remain in the applicant pool. Approved borrowers await fund disbursement before beginning to make regular payments. A borrower's financial profile is positively updated after the full loan is repaid, while defaults have negative impacts. Borrowers rejoin the applicant pool after repayment or default, reflecting real-world financial cycles.\nHealthcare MAFE This MAFE simulates a healthcare system involving three agents that represent an insurance company, a hospital, and a central planner. The Insurance Agent determines insurance price offerings to individuals, influencing healthcare access. The Hospital Agent provides treatment based on capacity constraints, allocating hospital beds to sick individuals in the population at each time step. The Central Planner Agent allocates budgets periodically for hospital infrastructure, public health initiatives, and insurance subsidies. Individuals choose whether to purchase insurance, affecting their healthcare access and health outcomes. Patients in sick states seek treatment, where hospital capacity and treatment timing impact recovery.\nEducation MAFE This MAFE tracks population transitions across three stages: tertiary population, university students, and workforce members, reflecting progressions from higher education to employment. It includes four agents: a University Admissions Agent that selects applicants for enrollment; a University Budget Allocation Agent that distributes university funds for different resources, affecting student success and resource quality; an Employer Agent that sets the salaries for the workforce; and a Central Planner Agent that allocates resources for tertiary education, university funding, and workforce diversity incentives. Individuals transition from the tertiary population to the university and then to the workforce or directly to the workforce if rejected from the university. Students may leave the university at any time step, with the duration of their education determining the highest degree and qualifications they attain. Employers set salaries based on worker qualifications, linking educational outcomes to career trajectories."}, {"title": "5. Demonstrating MAFE Usage: A Use Case in Cooperative Settings", "content": "In this section, we focus on a cooperative multi-agent setting as a use case for analyzing the MAFEs introduced in Section 4, where agents work towards shared objectives and fairness concerns arise from disparities in outcomes over time. This design choice serves to demonstrate the MAFE framework's application in a concrete scenario, while the framework itself remains adaptable to other scenarios.\n5.1. Formulating the Multi-agent Decision Problem\nIn this section, we formalize the concept of success in our cooperative decision-making scenario. Let $o_{n,t}$ and $a_{n,t}$ represent the observation received and the action taken by the nth agent at time t and $o_{1:\u221e}$ and $a_{1:\u221e}$ represent the collections of all observations seen and actions produced by every agent at time t. Colon notation over these temporal indices denotes a time interval. Let $R_n^{(k)} (o_{1:\u221e}, a_{1:\u221e}) = R_n^{(k)} (c^{(R)} (o_{1:\u221e}, a_{1:\u221e}))$ represent the total reward for the kth of K rewards for Agent n, and similarly, let $F_n^{(m)} (o_{1:\u221e}, a_{1:\u221e}) = F_n^{(m)} (c^{(F)} (o_{1:\u221e}, a_{1:\u221e}))$ represent the total violation of the mth of M fairness measures for Agent n. For brevity, we refer to these values as $R_n^{(k)}$ and $F_n^{(m)}$, respectively. We describe the functional forms that we select for $R_n^{(k)}$ and $F_n^{(m)}$ in use case in Sections 5.2 and 5.3. Finally, let $\\theta_n$ represent the parameters of the model used to produce the action taken by Agent n. Then, a maximization problem for Agent n may be given by Equation 1:\n$\\max_{\\theta_n} \\sum_{k=1}^{K} \\alpha_{n,k} E_{\\theta_n} [R_n^{(k)}]$\ns.t. $E_{\\theta_n} [F_n^{(m)}] < \\epsilon^{(m)}, 1 \\le m \\le M, \tagenumeration*1\nwhere $\\alpha_{n,k}$ is a user-defined weight for the kth reward in Agent n's objective function.\nUsing regularization, this problem can be rewritten as:\n$\\max_{\\theta_n} \\sum_{k=1}^{K} \\alpha_{n,k} E_{\\theta_n} [R_n^{(k)}] + \\sum_{m=1}^{M} \\beta_{m,n} E_{\\theta_n} [F_n^{(m)}],  \tagenumeration*2\nwhere $\\beta_{m,k}$ is a user-defined weight for the mth fairness penalty in Agent n's objective function.\nThe cooperative setting is a special case of this problem in which, for all n agents, $\\alpha_{n,k} = \\alpha_k, \\beta_{m,n} = \\beta_m, c^{(R)} = c_n^{(R)}, and c^{(F)} = c_n^{(F)}$. In this scenario, the objective function becomes identical for all agents. Thus, we can rewrite Problem 2 in the following form:\n$\\max_{\\theta_n} \\sum_{k=1}^{K} \\alpha_{k} E_{\\theta} [R^{(k)}] + \\sum_{m=1}^{M} \\beta_{m} E_{\\theta_n} [F^{(m)}].  \tagenumeration*3$\nThus, success over an episode can be measured directly computing the following value once an episode is terminated:\n$\\sum_{k=1}^{K} \\alpha_{k} R^{(k)} + \\sum_{m=1}^{M} \\beta_{m} F^{(m)}.  \tagenumeration*4$\n5.2. Reward Structure Customization\nWe design two types of rewards for agents: direct rewards and rate-based rewards. Direct rewards are explicit values, such as profits, that an agent aims to optimize. Rate-based rewards are expressed as ratios, such as the proportion of insured individuals to the total population, representing relative measures that agents aim to optimize. With this, we now provide the form of the reward summation in Problem 3.\nLet $K = j + l$, and define the reward components $[r_{1,t},..., r_{j+2l,t}] = c^{(R)} (o_{1:\u221e}, a_{1:\u221e})$, where $r_{1,t},..., r_{j,t}$ are the direct rewards, $r_{j+1,t}, ..., r_{j+l,t}$ are numerators for rate-based rewards, and $r_{j+l+1,t}, ..., r_{j+2l,t}$ are denominators for the rate-based rewards at time t. Then, the final structure of the rewards summation in Equation 3 can be rewritten as the sum of its direct and rate-based constituents:\n$\\sum_{i=1}^{j} [\\sum_{t=0}^{A_i} r_{i,t}] + \\sum_{i=j+1}^{j+l} [\\frac{\\sum_{t=0}^{A_i} r_{i,t}}{\\sum_{t=0}^{A_i} r_{i+l,t}}].  \tagenumeration*5$\n5.3. Fairness Measure Structure Customization\nGiven that the most common disparities in algorithmic fairness are rate-based, such as differences in insured rates across geographic regions in healthcare, we now describe how F(m) in Problem 3 is structured to measure these disparities when the number of groups is two or more.\nTwo-group case. In the two-group case, the disparity between two groups is measured using the directly interpretable absolute difference in rates. Define the fairness components $[f_{1,t},\u2026\u2026, f_{4M,t}] = c^{(F)}(o_{1:\u221e}, a_{1:\u221e})$, where $f_{4m-3,t},..., f_{4m,t}$ represent the numerator and denominator for the rates of Groups 1 and 2 for the mth fairness measure. Then, the fairness violation is given by:\n$F^{(m)} = |\\frac{\\sum_{t=0}^{A_i} f_{4m-3,t}}{\\sum_{t=0}^{A_i} f_{4m-2,t}} - \\frac{\\sum_{t=0}^{A_i} f_{4m-1,t}}{\\sum_{t=0}^{A_i} f_{4m,t}}|$.  \tagenumeration*6\nD-group case. When the number of groups, D, exceeds two, an absolute difference is inadequate for capturing disparities, as it fails to reflect the distribution of rates across multiple groups. To address this, we use standard deviation to quantify fairness disparities in the D-group case. Its simplicity provides an interpretable measure of how evenly rates are distributed among groups, making it particularly suitable for assessing fairness in multi-group settings. We define this measure as follows. Let the fairness components, $[f_{1,t},\u2026\u2026, f_{2DM,t}] = c^{(F)} (o_{1:\u221e}, a_{1:\u221e})$, where $f_{2D(m-1)+1,t},\u2026\u2026, f_{2Dm,t}$, provide the numerator and denominator of each of D groups for which we use for measuring the mth rate. Let $Y_d^{(m)} = \\frac{\\sum_{t=0}^{A_i} f_{2D(m-1)+d,t}}{\\sum_{t=0}^{A_i} f_{2D(m-1)+d+1,t}}$, and$\\mu^{(m)} = \\frac{1}{D} \\sum_{d=1}^{D}Y_d^{(m)}$. Then, the fairness measure is given by:\n$F^{(m)} = \\sqrt{\\frac{\\sum_{d=1}^{D} (Y_d^{(m)} \u2013 \\mu^{(m)})^2}{D}}$.  \tagenumeration*7$\nAs the value of F(m) approaches its upper limit of 0, the disparity in rates across different demographic groups diminishes, improving the parity among them."}, {"title": "6. Results and Analysis", "content": "In Sections 6.1, 6.2, and 6.3, we demonstrate how MAFE actions address system disparities, verify agent learnability, and analyze agent action strategies. We introduce the Fair Multi-Agent Cross Entropy Method (F-MACEM) for optimizing these tasks, with details in Appendix B and additional results in Appendix C.\n6.1. Validating Interventions for Correcting Disparities\nThis section shows that actions shaped in our MAFEs can effectively mitigate disparities. Each MAFE is designed in a way that can incorporate structural biases, which may lead to disparate outcomes across demographic groups. The core attributes influencing outcomes vary by environment: qualification scores in the Loan MAFE, health risk scores in the Healthcare MAFE, and baseline GPA in the Education MAFE. These attributes reflect inherent biases across sensitive groups, calculated by regressing over dataset features used to construct each MAFE's feature vectors. To enhance these biases for the purpose of supporting fairness research, we have resampled the original feature distributions, exacerbating disparities. Figure 2 illustrates these biased distributions at the start of each MAFE episode.\nTo assess whether agent actions can correct disparities, we conducted fixed intervention experiments, summarized in Figure 3. Using a fixed random seed, we compare the impact of specific interventions on environmental indicators with and without the intervention, repeating the process across five seeds. In the Loan MAFE, we examined debt management's effect on qualification scores. In the Healthcare MAFE, we evaluated incidence and mortality rates under varying conditions like hospital bed availability, insurance coverage, and public health investments. In the Education MAFE, we analyzed the impact of investments, scholarships, mentorship programs, and diversity incentives on graduation rates and employer utility.\nThe results shown in Figure 3 illustrate significant improvements when interventions are applied (dashed red lines) compared to baseline scenarios (solid black lines). In each plot, there is significant bias in the red dash line when compared with the black solid lines. The direction of the arrow (upward or downward) above each plot signifies improvement in the indicator of interest, indicating the positive impacts that each intervention has on improving outcomes for members of the population. Thus, applying these interventions strategically for sub-population groups should allow agents to effectively mitigate disparities among different sensitive attribute groups.\n6.2. Compound Effects of Reward Terms\nIn this section, we explore the cumulative impact of incorporating different terms into the F-MACEM's objective function for each MAFE, specifically examining how various combinations of terms influence the observed outcomes for each individual term. We categorize these terms into three distinct groups, as outlined in Sections 5.2 and 5.3: direct rewards, fairness penalties, and rate-based rewards. To analyze their effects, we train the F-MACEM using three configurations of the objective function: (1) including only direct rewards, (2) including both direct rewards and fairness penalties, and (3) including direct rewards, fairness penalties, and rate-based rewards. For consistency, all elements in each objective function are uniformly weighted.\nThe results of this analysis are presented in Figure 4. Each row corresponds to a different environment, while each column tracks the evolution of a specific reward category throughout training. Within each plot, the plotted curves differentiate the explicit reward terms included in the objective function. As expected, the red line-representing the objective function that explicitly incorporates all reward categories-shows steady improvement across all reward types during training. In contrast, configurations excluding certain terms often exhibit less consistent and volatile performance. For example, in the Education environment, the rate-based reward curve for the F-MACEM, trained solely with direct rewards, declines from its initial value during training and only approximately returns to its starting point by the final epoch on average. Similarly, in the Loan environment, excluding rate-based rewards causes the corresponding reward curve to plateau at a significantly lower value than observed in the fully-inclusive configuration. These patterns underscore the utility of integrating diverse reward terms to balance learning objectives effectively within each MAFE.\nThis analysis also highlights environment-specific characteristics. Notably, the Healthcare MAFE shows smaller performance differences between training configurations compared to the Loan and Education MAFEs. While this might seem counterintuitive, it reflects the MAFE's design: individuals transition between healthy, sick, and deceased states, with insurance profit as the primary reward. Insurers benefit most when the population maintains a high insured rate and remains healthy, minimizing claims. As a result, agents learn to balance interventions that optimize profitability and health outcomes. This alignment between agent objectives and system well-being offers a key insight: even when explicit stakeholder priorities diverge, overlapping indirect objectives can foster cooperative strategies that outperform narrow, self-serving approaches.\n6.3. Policy Action Analysis\nIn this section, we analyze the actions that the F-MACEM learns to produce over the training process when direct rewards, rate-based rewards, and fairness penalties receive uniform weighting in the objective function for the Education MAFE. We visualize how the Central Planner Agent distributes funds across different interventions, how the Employer Agent sets salaries, and how the university distributes the resources it receives for different interventions that improve student academic success.\nFigures 5a-5c illustrate these agent actions. The Central Planner Agent primarily invests in tertiary resources and employer diversity incentives (Figure 5b), suggesting that tuition revenue adequately covers university operations. The University Budget Allocation Agent shifts its strategy the training process (Figure 5c). Initially, it allocates a significant portion of its budget to faculty salaries to ensure stability, but since faculty salaries are fixed, the agent refines its strategy by directing more resources to student-specific interventions, like scholarships and mentorship programs for underrepresented groups. This change helps reduce disparities in GPAs between majority and underrepresented students, improving overall educational and career outcomes.\nNotably, Figure 5a shows a significant trend reversal in the Employer Agent's salary-setting behavior midway through the training process. Initially, the Employer Agent decreases average salaries; however, this trend inverts as training progresses, leading to a steady increase in salaries. This shift results from multiple factors. First, the Central Planner Agent's investment in diversity incentives directly boosts the salaries of underrepresented minority groups. Second, as the Central Planner Agent and University Budget Allocation Agent optimize their investments in tertiary resources and university student aid, overall student performance improves. These enhancements in educational outcomes translate to better career success, indirectly driving higher salaries.\nCoordination among agents in each MAFE can create a positive feedback loop for improving system rewards, enabled by the flexible intervention structure our MAFEs offer. This structure facilitates the development of coordinated strategies that are more realistic and useful than the simplified abstractions used in single-action environments."}, {"title": "7. Conclusion and Discussion", "content": "In this work, we introduce the concept of Multi-Agent Fair Environments (MAFEs) as a framework for analyzing fairness in multi-agent systems. We provide a formal definition of algorithmic success within a MAFE, and develop three MAFEs modeling key social systems using a Python-based code implementation akin to popular reinforcement learning libraries like Gym, Gymnasium, and Petting Zoo. Through experimental analysis, we validate that our MAFEs can be used to analyze interventions that correct for system biases.\nFairness-aware algorithms require testing in environments that closely replicate real-world systems. While modeling human-centric systems involves some simplification, our MAFEs enhance the realism of decision-making in FairAI research. Acknowledging that domain experts may have varying perspectives on realism, our modular MAFEs offer flexible customization to meet diverse research needs. The models presented here represent one implementation, but our framework is adaptable and extensions will be analyzed in future work."}, {"title": "A. Additional Related Work", "content": "Agent-based Social Simulations. Agent-based models (ABMs) have been employed to study various societal phenomena, such as the spread of misinformation in social networks, the propagation of epidemics, resource management, and economic systems (Perez & Dragicevic, 2009; Asgharpour et al., 2010; Giabbanelli et al., 2021; Benthall et al., 2021; Gausen et al., 2022). ABMs offer a bottom-up approach to understanding sociological phenomena, where the interactions between individual agents can lead to emergent behaviors (Elsenbroich & Polhill, 2023). Traditionally, such modeling has been conducted using surveys, network analysis, data mining, and game theory (Bonabeau, 2002). Recently, MARL has emerged as a powerful tool for analyzing complex group dynamics (Busoniu et al., 2008). However, the majority of existing MARL environments focus on specialized applications, such as games or autonomous navigation (Terry et al., 2021; Li et al., 2022) with limited relevance to fairness-oriented research. In contrast, our work analyzes fairness\u2014an essential metric for assessing social and institutional interactions in an MARL context."}, {"title": "B. A Multi-agent Algorithm for Solving a MAFE", "content": "Algorithm 1 Fair Multi-Agent Cross Entropy Method (F-MACEM)\n1: repeat\n2: Initialize buffers R and P and parameters \u03bc and \u03c32\n3: for episode = 1... number-of-episodes do\n4: Sample \u03b8 = {\u03b81, ..., \u03b8N} from N(\u03bc, diag(\u03c3\u00b2))\n5: Run episode, storing rewards and fairness components in R and O in P\n6: end for\n7: Update \u03bc and o\u00b2 based on top p% of policies ranked by Equation 4.\n8: until Convergence\n9: Return \u03b8 = \u03bc\nIn this section, we introduce the Fair Multi-agent Cross Entropy Method (F-MACEM), a simple yet effective algorithm for optimizing the objective function in Problem 3. The F-MACEM is an extension of the standard cross-entropy method (CEM), tailored to multi-agent systems with fairness considerations. This method is employed for performance analysis in Section 6.\nThe standard CEM is an evolutionary policy-based algorithm that optimizes a policy by sampling its parameters from a parametric distribution, such as a Gaussian. For each sample, the policy weights, \u03b8, are used to run a full episode, and the resulting rewards are observed. In each training epoch, multiple episodes are run with different policy weight samples. The top-performing policies, referred to as the elite set, are then used to update the distribution from which the policy weights are sampled. This process iterates until the average episodic rewards converge.\nIn the fully cooperative MARL setting, the standard CEM can be directly extended to handle multiple agents by updating the model weights for all N agents, \u03b8 = {\u03b81,..., \u03b8N}, simultaneously in each epoch. This update is based on the top-performing weight samples, which maximize episode rewards. These elite samples are then used to update the distribution from which \u03b8 is drawn. An overview of the algorithm is provided in Algorithm 1."}, {"title": "C. Additional Experiments", "content": "C.1. Policy Action Analysis\nIn this section, we provide the complete action analysis results from Section 6.3. Specifically, we present action analyses for each MAFE when direct, rate-based, and fairness violation terms are weighted uniformly in the objective function.\nFor the Loan MAFE, we analyze the average admissions threshold set by the Admissions Agent, which determines the number of people approved for loans in an episode, and the debt management factor set by the Debt Management Agent, which helps the customer population avoid loan defaults. In the Healthcare MAFE, we examine how the Central Planner Agent allocates its budget across interventions and how the Insurance Agent sets premiums. For completeness, we restate the Education MAFE action analysis, focusing on how the Central Planner Agent distributes funds for interventions, how the Employer Agent sets salaries, and how the University Budget Allocation Agent allocates resources to improve student academic success.\nFor the Loan MAFE, Figure 6a shows the average admission threshold over 40 training epochs. As training progresses, the agent learns to lower the threshold, effectively admitting nearly all applicants. This strategy increases the admission rate among the global population, thereby improving the rate-based reward. However, admitting more applicants without additional safeguards can increase default rates, risking the bank's financial"}]}