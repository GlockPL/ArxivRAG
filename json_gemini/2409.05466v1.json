{"title": "Proto-OOD: Enhancing OOD Object Detection\nwith Prototype Feature Similarity", "authors": ["Junkun Chen", "Jilin Mei", "Liang Chen", "Fangzhou Zhao", "Yu Hu"], "abstract": "The limited training samples for object detectors commonly\nresult in low accuracy out-of-distribution (OOD) object detection. We\nhave observed that feature vectors of the same class tend to cluster tightly\nin feature space, whereas those of different classes are more scattered.\nThis insight motivates us to leverage feature similarity for OOD detec-\ntion. Drawing on the concept of prototypes prevalent in few-shot learn-\ning, we introduce a novel network architecture, Proto-OOD, designed for\nthis purpose. Proto-OOD enhances prototype representativeness through\ncontrastive loss and identifies OOD data by assessing the similarity be-\ntween input features and prototypes. It employs a negative embedding\ngenerator to create negative embedding, which are then used to train\nthe similarity module. Proto-OOD achieves significantly lower FPR95\nin MS-COCO dataset and higher mAP for Pascal VOC dataset, when\nutilizing Pascal VOC as ID dataset and MS-COCO as OOD dataset.\nAdditionally, we identify limitations in existing evaluation metrics and\npropose an enhanced evaluation protocol.", "sections": [{"title": "Introduction", "content": "Artificial intelligence systems often face a gap between training and real-world\napplication. Training typically involves a subset of class data, while in practice,\nthe model encounters a broader range of data, including unknown classes. This\ncan lead to unreliable predictions for OOD data, potentially causing critical\nissues, for example, misidentifying OOD data as passable areas in autonomous\ndriving could result in traffic accidents.\nRecently, OOD object detection is receiving attention [4, 6, 33]. This task\nbuilds upon the accurate object localization of existing detectors by integrat-\ning an OOD classifier. The main challenge of this task is to obtain a classifier\nwith high discriminability. SAFE [33] attempts to improve classifier training by\ngenerating adversarial samples, but falls short in compensating for the vast dis-\ncrepancy between adversarial samples and the test set. VOS [6] utilizes a small\ndistance parameter to capture feature outliers for training classifier, but the ac-\ncuracy of OOD classifier is tied to the optimal distance selection. We believe\nthat the key to addressing these issues is how to extract highly discriminative\nfeatures for each class from the training data."}, {"title": "Related Work", "content": ""}, {"title": "OOD Detection for Image Classification", "content": "Numerous studies have previously addressed OOD detection for image classifica-\ntion tasks [1, 14, 26, 29, 30, 34, 37]. Vahdat et al. [1] assert the existence of optimal\nfeature layers in the feature space, which can effectively differentiate between ID\ndata and OOD data by leveraging the features in these optimal layers. Mean-\nwhile, Huang et al. [14] propose that the feature distribution of OOD data is\nconcentrated in the feature space. Smith et al. [30] train a RBF network and\nassess the uncertainty of feature vectors and centroids to distinguish OOD data.\nAdditionally, some approaches aim to introduce noise to the features during\nmodel training [22, 28]. Moreover, Mahalanobis distance and enhanced Maha-\nlanobis distance serve as common methods for OOD data detection [17, 23].\nA common strategy for OOD detection in image classification involves cre-\nating negative examples to train networks. GANs [10] be used to produce low-\nconfidence samples for training, as seen in works by [16, 25, 31]. Du et al. [5]\napply stable diffusion to sample outliers in the low-resolution region of the ID\ndata as negative examples. Adversarial examples are also used as OOD data to\ntrain classifiers [13, 18].\nSeveral approaches focus on detecting OOD data by examining the distribu-\ntion of neural network outputs. Yu et al. [36] add a secondary classifier to their\nmodel, using the output differences between the two classifiers to spot OOD data.\nWang et al. [32] strive to generate a virtual-logit for OOD data. By comparing\nthe output logit with the generated virtual-logit, they can detect OOD data.\nDong et al. [3] introduce the neural mean discrepancy method, which identifies\nOOD data by comparing the neural means of input samples with those of the\ntraining data.\nAlthough many papers show significant advancements in OOD detection for\nimage classification, the effectiveness of these methods in the more intricate\ndomain of object detection still needs to be tested."}, {"title": "OOD Detection for Object Detection", "content": "There is a scarcity of research on OOD object detection. SIREN [4] introduced\na distance-based approach, mapping various classes of features into a hyper-\nsphere and utilizing trainable distance parameters to adjust the energy score of"}, {"title": "Prototypes for Few-shot Learning", "content": "Prototypes have a wide range of applications in few shot learning. Prototype\nnetwork [24] collects the mean of support set features as the prototypes and\ncomputes the distance from the input feature to the prototypes to determine\nthe class. Liu et al. [8] propose a cosine similarity based prototypical network\nto compute basic prototypes of the novel classes from the few samples. Zhao\net al. [38] employ prototypes for few-shot learning on 3D point cloud data. In\ntheir method, they use a momentum coefficient to update the prototypes during\nthe training stage, making the prototypes more robust. Gao et al. [9] propose\ncontrastive prototype learning (CPL) for few-shot learning. CPL aims to pull\nthe query samples of the same class closer and those of different classes further\naway. Allen et al. [2] propose infinite mixture prototypes to adaptively represent\ndata distributions for few-shot learning.\nIn few-shot learning, prototypes are frequently employed for identifying in-\nput feature classes due to their strong categorical representation. This robust\nattribute can be used for OOD object detection."}, {"title": "Method", "content": "In this section, we present our approach to OOD object detection. The prelim-\ninaries and overviews be introduced in section 3.1 and 3.2. In section 3.3, we\nelucidate the prototype learning and contrastive loss. Following that, in section\n3.4, the similarity module and the negative embedding generator are introduced.\nFinally, in section 3.5, we detail how to train Proto-OOD and how to conduct\nOOD object detection during the testing stage."}, {"title": "Preliminaries", "content": "Let the input image set and label set be X and Y, respectively. $Y_{id}={1,2,..,t}$ de-\nnote ID data class space and $Y_{ood}$ denote the OOD data class space ($Y_{id}\u2229Y_{ood} = \u00d8$\n). For each y \u2208 Y, y = {l, $X_{min}$, $Y_{min}$, $X_{max}$, $Y_{max}$ } where l, ($X_{min}$, $Y_{min}$, $X_{max}$, $Y_{max}$)\ndenote the class label and the coordinate of the object, respectively. During\ntraining stage, the ID dataset $D^{ID}$ is used to train the model's parameters \u03b8. \u0391"}, {"title": "Overviews", "content": "We illustrate the framework in Fig. 2. We add a similarity module, project head\nand negative embedding generator to the network to detect OOD data. Given\nan input image x, the feature extractor, utilizing a transformer architecture,\nprocesses the image to extract feature query Q. The query Q is used to predict\nobject class and box by class head and box head.\nThe project head is a two-layer MLP that maps the query Q to a lower-\ndimensional embedding r. Subsequently, the similarity module utilizes the em-\nbedding r and the prototypes p collected during the training stage to predict\nthe similarity score s. After combining the similarity score s with the cosine\nsimilarity between the embedding r and the prototypes p, it is converted into\nenergy E, which is used to determine whether the object belongs to ID or OOD"}, {"title": "Prototype Learning and Contrastive Loss", "content": "In the prototype network [24], prototypes are utilized to determine the class\nof input features. In Proto-OOD, whether the input features belong to OOD\ndata can be determined by the similarity between the input features and the\nprototypes. Proto-OOD utilizes a project head, a two-layer MLP, to map the\nquery Q\u2208 $R^{n\u00d7h}$ to a lower-dimensional embeddings r \u2208 $R^{n\u00d7d}$(d<h) and collects\nthe prototypes from embeddings r. During the early training stage, embeddings\nr may not represent features well. Thus, Proto-OOD collects prototypes after a\nset number of epochs, using the following method:\n$p_c = (\u03b1p_c + (1 \u2212 \u03b1)r_c)$\nwhere \u03b1 is update factor and $r_c$ refers to embeddings from class c. The original\nvalue of $p_c$ is 0.\nTo enhance the representativeness of prototypes, contrastive loss is added,\naiming to enlarge the distances between embeddings r of different classes in the\nfeature space. We chose RT-DETR as the object detector. RT-DETR employs\nthe Hungarian algorithm to select the best-matched predictions for calculating\nthe loss. For a batch best-matched predictions, contrastive loss $L_{con}$ for embed-\nding r is calculated as follow:\n$L_{con} = \\frac{1}{M}  \\sum_{i=1}^{M}f(z_i)$\n$f(z_i) = \\begin{cases}\n-log\\frac{exp(z_i z_i/\u03c4)}{\\sum_{j=1}^{M} exp(z_i z_j/\u03c4)},& ||l_i|| = 1 \\\\\n-log\\frac{\\sum_{j=1,j\u2260i}^{M} {Y_i ==Y_j} \\cdot exp(z_i z_j/\u03c4)}{\\sum_{j=1,j\u2260i}^{M} exp(z_i z_j/\u03c4)}, & otherwise\n\\end{cases}$\n,where $r_i= z_i=\\frac{r_i}{||r_i||}\u03c4$ is a scaling factor. li represents the class label of the $r_i$\n||li|| denote the number of objects of class li in a batch of data. ||ri|| denote the\nnorm of $r_i$. M is total number of objects in a batch of data."}, {"title": "Similarity Module and Negative Embedding Generator", "content": "After collecting representative prototypes, similarity between input features and\nprototypes can be use to determine whether the object belongs to ID or OOD\ndata. Inspired by the relation network [27], Proto-OOD employs a smilarity\nmodule to compute the similarity between embeddingds r and prototypes. The"}, {"title": "Score Selection", "content": "To train the similarity module, we need to sample positive and negative simi-\nlarity scores $S_p$ and $S_n$. We choose the Hungarian algorithm to select positive\nsimilarity score. The best-matched results of the Hungarian algorithm are sam-\npled as positive scores $S_p$ to train the similarity module. These positive scores $S_p$\nensure that the similarity module outputs a high similarity score s for ID data.\nAdditionally, we need to sample negative similarity scores $S_n$. The negative sim-\nilarity scores $S_n$ sampled from s' which generated from negative embeddings r'\nusing a similarity module.\nThen, We compute the similarity scores s'of the embeddings r'. We sample\nnegative similarity scores $S_n$ from s' that are less than a threshold to train\nsimilarity module. During the loss calculation stage, We choose several objects\nthat have the lowest IOU with the ground true box and use their similarity score\nas negative similarity score for training similarity module.\nWe use the focal loss [19] train the two-layer MLP in similarity module:\n$L_{RN} = focal\\_loss(S,l_{rn})$\n$l_{rn} = \\begin{cases}\n1,& S\u2208 S_p\\\\\n0,& S\u2208 S_n\n\\end{cases}$\nwhere S denote similarity scores , $l_{rn}$ is the label of the S."}, {"title": "Training and Testing", "content": "Training and testing procedures are represented in Algorithm 1. During training\nstage, because the representation of embeddings r are poor in the early stages\nof training, we set a hyperparameter \u03bb to determine when to start collecting\nprototypes. Specifically, when the training epoch is greater than \u5165, Proto-OOD\nstart to collect prototypes. We first train the object detector with parameters \u03b8\nand the project head with parameters & in stagel (line 1 to line 5 in Algorithm\n1). The loss function is as follow:\n$L_{stage1} = L_{cls} + L_{box} + L_{con}$\nWhen epoch is greater than \u5165, we begin to collect prototypes. After \u03c9 epochs, we\nhave collected some robust prototypes, and then, we begin to train the similarity\nmodule. The loss in stage2 (line 6 to line 7 Algorithm 1) as follow:\n$L_{stage2} = L_{cls} + L_{box} + L_{con} + L_{RN}$\nwhere the $L_{cls}$ is focal_loss [19], and $L_{box}$ is L1 loss.\nAfter training the entire network, we can obtain an object detector with OOD\ndetection functionality.\nDuring testing stage, the object detector outputs not only the class score and\nbox position, but also the object feature embeddings r and similarity score s.\nGiven a object embedding $r^*$, the energy E(g; $r^*$, p) generated by embedding\n$r^*$ and prototypes p is used to determine whether the object belongs to the ID\ndata. The energy E(g; $r^*$, p) is calculated as follow:\nE(g; $r^*$, p) = H($r^*$,p) \u00b7 $s^*$ = exp(Cosine_similarity($r^*$, p)) \u00b7 $s^*$\nwhere p denote prototypes. $s^*$ represents the similarity score output by the\nsimilarity module. We set a threshold y for the energy E(g; $r^*$, p) to distinguish\nbetween ID and OOD objects:\ng($r^*$,p) =$\\begin{cases}\n1,& E(g; r^*, p) >= \u03b3\\\\\n0,& E(g; r^*, p) < \u03b3\n\\end{cases}$\nif g($r^*$,p) equals 1, the object is considered as ID data. If g($r^*$,p) equals 0, the\nobject is considered as OOD data."}, {"title": "Experiments", "content": "In Section 4.1, we present the experimental setup. In Section 4.2, we introduce\nthe evaluation metrics and come up with an improved evaluation protocol. Ex-\nperimental results also displayed in this section."}, {"title": "Experimental Setting", "content": "Datasets. Same as SIREN [4], we use the PASCAL-VOC [7] and Berkley\nDeepDrive-100K [35] (BDD100K) datasets as our ID datasets for training and\nevaluation. The subset of the MS-COCO [20] and OpenImages [15] are used as\nOOD dataset to evaluation.\nImplementation Details. The RT-DETR [21], with ResNet50 [12] as its\nbackbone, is utilized as an object detector. We set the scaling factor T=0.2 in\nEquation(3), and the scaling factor T=2 in Equation (5). For Pascal VOC [7]\ndataset, the X = 40 and the w = 5, which means that the prototypes col-\nlected after 40 epoch, and the similarity module trained after 45 epoch. For\nBDD100K [35], the X = 25 and the w = 5. We discover that the model predicts\nmany inaccurate results when testing ID data, including boxes containing only\nbackground or with a small IOU with groud true box. Consequently, for OOD\nmetrics evaluation, we selectively choose K objects with the highest scores from\nthe ID dataset, where K corresponds to the number of labeled objects in an\nimage. When evaluation OOD detection metrics, to prevent a single object from\nparticipating repeatedly in the computation, Non-maximum suppression(NMS)\nis utilized in the post-processing stage."}, {"title": "Evaluation Metrics and Results", "content": "Evaluation Protocol. We utilize the evaluation protocol as protocolA, de-\nfined by SIREN [4] to evaluation Proto-OOD. we report the metric mAP for\nID dataset. For OOD dataset, we report the false positive rate of samples when\nthe true positive rate of ID objects is at 95% (FPR95) and the area under\nthe receiver operating characteristic curve (AUROC). A lower FPR95 indi-\ncates superior model performance, while a higher AUROC signifies better model\nperformance. Additionally, we note that previous methods [4, 6, 33] overlook the\nfilter out inaccurate predictions when calculating metrics. This may lead to inac-\ncuracies in their OOD evaluation metrics. Specifically, the formula for the false\npositive rate (FPR) is as follows:\n$FPR = \\frac{FP}{FP+TN}$\nwhere FP represents the misclassification of negative samples as positive samples\nand TN indicates that the negative sample is correctly predicted to be negative.\nPrevious methods [4, 6, 33] don't filtering out inaccurate predictions that led to\nFP in Equation(14) being too large when calculating FPR.\nHence, we propose a new evaluation protocol as protocolB. To avoid these\ninaccurate predictions leading to an increase in FP, only the K objects with the\nhighest score are selected when calculating the OOD detection metrics for the\nID dataset, where K is equal to the number of annotated objects in the image.\nResults and Analysis. The performance of Proto-OOD under protocolA\ncompared with other methods is shown in the Table 1. It is not surprising that\nProto-OOD achieves the best FRP95 and AUROC on MS-COCO when using"}, {"title": "Conclusion", "content": "We propose Proto-OOD network architecture for OOD object detection. Proto-\nOOD is a structure that uses prototypes for OOD object detection for the first\ntime. Proto-OOD distinguishes whether the input features belong to ID or OOD\ndata use similarity. Proto-OOD provides a new way for OOD object detection.\nProto-OOD gradually collects prototypes of ID data during the training\nstage, and samples negative samples with low similarity to the prototype in\nthe feature space to train a similarity module, so that the similarity module can\nbe used as an effective similarity predictor. Proto-OOD shows good performance\nfor OOD object detection.\nFurthermore, We elucidate the limitations of current detection metrics, and\ncome up with a more reasonable evaluation protocol. Later, we will endeavor to\nexplore benchmarks for OOD object detection."}]}