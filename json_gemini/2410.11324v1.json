{"title": "DIFFUSION-BASED OFFLINE RL FOR IMPROVED\nDECISION-MAKING IN AUGMENTED ARC TASK", "authors": ["Yunho Kim", "Jaehyun Park", "Heejun Kim", "Sejin Kim", "Byung-Jun Lee", "Sundong Kim"], "abstract": "Effective long-term strategies enable AI systems to navigate complex environments\nby making sequential decisions over extended horizons. Similarly, reinforcement\nlearning (RL) agents optimize decisions across sequences to maximize rewards,\neven without immediate feedback. To verify that Latent Diffusion-Constrained\nQ-learning (LDCQ), a prominent diffusion-based offline RL method, demonstrates\nstrong reasoning abilities in multi-step decision-making, we aimed to evaluate its\nperformance on the Abstraction and Reasoning Corpus (ARC). However, applying\noffline RL methodologies to enhance strategic reasoning in AI for solving tasks\nin ARC is challenging due to the lack of sufficient experience data in the ARC\ntraining set. To address this limitation, we introduce an augmented offline RL\ndataset for ARC, called Synthesized Offline Learning Data for Abstraction and\nReasoning (SOLAR), along with the SOLAR-Generator, which generates diverse\ntrajectory data based on predefined rules. SOLAR enables the application of offline\nRL methods by offering sufficient experience data. We synthesized SOLAR for a\nsimple task and used it to train an agent with the LDCQ method. Our experiments\ndemonstrate the effectiveness of the offline RL approach on a simple ARC task,\nshowing the agent's ability to make multi-step sequential decisions and correctly\nidentify answer states. These results highlight the potential of the offline RL\napproach to enhance AI's strategic reasoning capabilities.", "sections": [{"title": "1 INTRODUCTION", "content": "Effective long-term strategies involve deliberate reasoning, which refers to the thoughtful evaluation\nof options to determine the best course of action (Kahneman, 2011). This type of reasoning requires\nconscious effort and allows intelligent beings to systematically plan and execute multi-step strategies\nto achieve complex long-term goals. Similarly, reinforcement learning (RL) agents make decisions\nwith the goal of maximizing rewards over extended sequences of actions, even without immediate\nfeedback. In both cases, reasoning involves considering a sequence of actions to reach an optimal\noutcome. We believe that the way Q-values guide an RL agent toward desired outcomes aligns with\nthe subgoals of deliberate reasoning, particularly in terms of multi-step decision-making to achieve\nlong-term objectives.\nRecent approaches to offline RL combined with generative diffusion models have shown significant\nimprovements in multi-step strategic decision-making abilities for future outcomes (Janner et al.,\n2022; Ajay et al., 2023; Liang et al., 2023; Li et al., 2023). In particular, Latent Diffusion-Constrained\nQ-learning (LDCQ) (Venkatraman et al., 2024) leverages diffusion models to sample various latents\nthat compress multi-step trajectories. These latents are then used to guide the Q-learning process. By\ngenerating diverse data based on in-distribution samples, diffusion models help overcome the limita-\ntions of fixed datasets. This integration of diffusion models into offline RL enhances agents' reasoning\nabilities, allowing them to consider multiple plausible trajectories across extended sequences.\nTo rigorously evaluate whether offline RL methods possess the advanced reasoning abilities required\nto solve complex tasks, we chose the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019), one\nof the key benchmarks for measuring the abstract reasoning ability in AI. As shown in Figure 1,\nthe ARC training set consists of 400 grid-based tasks, each requiring the identification of common\nrules from demonstration examples, which are then applied to solve the test examples. ARC tasks are\nparticularly challenging for AI models because they demand high-level reasoning abilities, integrating"}, {"title": "2 PRELIMINARIES", "content": "2.1 ARC LEARNING ENVIRONMENT (ARCLE)\nARCLE (Lee et al., 2024) is a Gymnasium-based environment developed to facilitate RL approaches\nfor solving ARC tasks. ARCLE frames ARC tasks within a Markov Decision Process (MDP) structure,\nproviding an environment where agents can interact with and manipulate grid-based tasks. This MDP\nstructure enables ARC tasks to be solved through sequential decision-making.\nARCLE handles states and actions following the O2ARC web interface (Shim et al., 2024). As\nshown in Figure 2, when ARCLE executes an action at on the current state st, it returns the next\nstate st+1, along with episode information about the reward and termination status. A state st\nconsists of (input grid, grid, clipboard) at timestep t. The input grid represents the initial state\nof the test example, the grid, denotes the current grid at time t after several actions have been\napplied, and the clipboard, stores the copied grid by the Copy operation. An action at consists of\n(operation, xt, Yt, ht, wt), where operation, represents the type of transformation, xt and yt denote\nthe coordinates of the top-left point of the selection box, and ht and we represent the difference\nbetween the bottom-right and top-left coordinates. All subsequent notations for st and at will adhere\nto this definition for clarity. Reward is only given when the Submit operation is executed at the\nanswer state, and the episode terminates either after receiving the reward or when Submit is executed\nacross multiple trials. All possible operations are mentioned in Appendix B.1."}, {"title": "2.2 DIFFUSION-BASED OFFLINE REINFORCEMENT LEARNING", "content": "Offline RL focuses on learning policies from previously collected data, without interacting with\nthe environment. However, Offline RL faces challenges, including data distribution shifts, limited\ndiversity in the collected data, and the risk of overfitting to biased or insufficiently representative\nsamples. To address these issues, several works in offline RL have focused on improving learning\nefficiency with large datasets and enhancing generalization to unseen scenarios while balancing\ndiversity and ensuring data quality (Fujimoto et al., 2019; Kidambi et al., 2020; Levine et al., 2020).\nRecent offline RL methods offer promising solutions in long-horizon tasks and handling out-of-\nsupport samples through diffusion models. For instance, Diffuser (Janner et al., 2022) generates\ntailored trajectories by learning trajectory distributions, reducing compounding errors. Beyond this,\na range of advanced diffusion-based offline RL approaches, such as Decision Diffuser (DD) (Ajay\net al., 2023), AdaptDiffuser (Liang et al., 2023), HDMI (Li et al., 2023), have demonstrated the\neffectiveness of combining diffusion models with offline RL."}, {"title": "2.3 LATENT DIFFUSION-CONSTRAINED Q-LEARNING (LDCQ)", "content": "Latent Diffusion-Constrained Q-learning (LDCQ) (Venkatraman et al., 2024) leverages latent diffu-\nsion and batch-constrained Q-learning to handle long-horizon, sparse reward tasks more effectively.\nThe LDCQ method uses sampled latents that encode trajectories of length H to train the Q-function,\neffectively reducing extrapolation error. The training process of LDCQ is shown in Figure 3: 1)\ntraining the B-VAE to learn latent representations, 2) training the diffusion model using the latent\nvectors encoded by the B-VAE, and 3) training the Q-network with latents sampled from the diffusion\nmodel. More details about the LDCQ method are described in Appendix A."}, {"title": "3 SYNTHESIZED OFFLINE LEARNING DATA FOR ABSTRACTION AND\nREASONING (SOLAR)", "content": "We developed a new dataset called Synthesized Offline Learning data for Abstraction and Reasoning\n(SOLAR) that can be used to train offline RL methods. Solving ARC tasks can be considered a\nprocess of making multi-step decisions to transform the input grid into the output answer grid. We\nbelieve that the process of making these decisions inherently involves applying core knowledge priors,\nobjectness, goal-directedness, numbers and counting, and basic geometry and topology (Chollet,\n2019), which are necessary for solving ARC tasks. The ARC training set lacks information on how\nto solve the task, and it only provides a set of demonstration examples and a test example for each\ntask, as shown in Figure 1. To address this, We aim to provide the trajectory data to solve the task\nthrough SOLAR, enabling them to learn how actions change the state based on the application of\ncore knowledge priors."}, {"title": "3.1 SOLAR STRUCTURE", "content": "SOLAR contains various transition data (st, at, St+1), where actions at are taken in different\nstates st, and the result st+1 observed. To facilitate effective learning and a combination of core\nknowledge, we use ARCLE (Lee et al., 2024). By designing a simple reward system that only provides\nrewards upon reaching the correct solution, we can guide the agent towards the desired state using\nreinforcement learning methods.\nAs shown in Figure 4, SOLAR consists of two key components: Demonstration Examples and Test\nExample with Trajectory. The demonstration examples and the test examples serve the same roles\nas in ARC. Through the demonstration examples, the common rule for transforming the input grid\nto the output grid is identified and then applied to solve the test example. Trajectory data means the\nepisode data that starts from test input so."}, {"title": "3.2 SOLAR-GENERATOR", "content": "We developed the SOLAR-Generator to synthesize SOLAR. SOLAR-Generator augments ARC\ntrajectories by following ARCLE formalism, addressing the inherent complexity and diversity of\nARC tasks. Figure 4 illustrates the data synthesis procedure, which is carried out in three steps: 1)\nLoading Synthesized Data, 2) Validating Trajectories with ARCLE, and 3) Structuring SOLAR.\nLoading Synthesized Data The first step in SOLAR-Generator is to load the synthesized data\nfor the target tasks. SOLAR provides the Grid Maker with common parameters such as maximum\ngrid size and the number of demonstration examples per test example. Each task has its own specific\nGrid Maker, which synthesizes demonstration examples, test examples, and corresponding action\nsequences (selections and operations) based on the task's constraints and rules. If desired, non-\noptimal trajectories containing random actions can also be synthesized. At this stage, the Grid Maker\nsynthesizes only grid pairs and possible action sequences. The full trajectory data for the test example\nis constructed after passing through ARCLE. More details about how the Grid Maker synthesizes the\ninput-output grids and action sequences are described in Appendix B.\nValidating Trajectories with ARCLE After synthesizing various grids and action sequences with\nthe Grid Maker, SOLAR-Generator checks whether the action sequences are valid in ARCLE. The\nGrid Maker serves as a data loader, enabling it to load and validate the synthesized data. Through\nthis process, ARCLE provides intermediate states, rewards, and termination status for each step, and\nverifies that each action is correctly executed in the current state. This step is particularly important\nfor non-optimal trajectories, where operations and selections may be generated randomly, as invalid\nselections can sometimes be synthesized by the Grid Maker. For gold standard trajectories, intended\nas correct solutions, SOLAR-Generator ensures that the final grid of the trajectory matches the\nexpected output grid of the test example. As a result, this stage is useful for checking and debugging\nthe synthesized trajectories, preventing unintended errors.\nStructuring SOLAR After the trajectory validation is complete, the episodes are saved into\nSOLAR. In this step, user can determine the necessary information to include in SOLAR. At its core,\nSOLAR includes episodes consisting of state, action, reward, and termination information at each\nstep, which are essential for training with offline RL methods. In addition to the previously mentioned\ninformation, SOLAR can also store various data from ARCLE, such as grid sizes at each step, binary\nmask versions of selections, and other relevant information needed for different learning methods.\nIn this research, we designed the data to work with methods like LDCQ, which require trajectories\nof fixed horizon length H. Therefore, the trajectories are segmented into fixed-length chunks with a\nhorizon length of H.\nThrough these three steps, SOLAR-Generator synthesizes diverse solutions by altering action orders\nor using alternative operation combinations. This is achieved by the Grid Maker, which generates\ndata using pre-implemented algorithms, enabling the user to create as many trajectories as needed.\nSOLAR provides a sufficient training set for learning various problem-solving strategies. By offering\ndiverse trajectories while adhering to the task-solving criteria, SOLAR bridges the gap between\nARC's reasoning challenges and the sequential decision-making process of offline RL. For additional\ndetails about SOLAR and SOLAR-Generator, see the project website\u00b9 and Appendix B."}, {"title": "4 DESIGN SOLAR FOR A SIMPLE TASK", "content": "One of the most crucial factors in solving ARC tasks is the ability to recognize whether the current\nstate is the answer state and to submit the correct answer accordingly. In ARC, each task embodies\na single analogy, but this analogy can be approached through various action sequences (Johnson\net al., 2021; Kim et al., 2022). Some solution paths may better exemplify the underlying analogy,\nwhile others might be less optimal or clear (Kim et al., 2022). Moreover, even when solving different\ntest examples within the same task where the same rule is applied, the actual action sequence can\nvary depending on factors like the grid size or the arrangement of elements in the input grid. The\ndiversity in potential action sequences to solve a single ARC task highlights the complexity of abstract\nreasoning and the importance of identifying the core analogy. Therefore, an agent's ability to judge"}, {"title": "5 EXPERIMENTS AND RESULTS", "content": "5.1 EVALUATION PROCESS USING ARCLE\nAfter training the agent using LDCQ on the SOLAR dataset, we conducted an evaluation of its\nperformance. To evaluate our experiment, we synthesized an evaluation SOLAR set with 100 test\nexamples, each paired with three synthesized demonstration examples. The evaluation SOLAR set\nwas synthesized by the SOLAR-Generator using the same tasks but with a random seed different\nfrom the one used for the training set. To measure the effectiveness of decision-making using the\nQ-function, two accuracy metrics are measured: 1) Whether the agent reaches the answer state, and\n2) Whether it predicts the Submit operation at the answer state to receive a reward.\nThe evaluation process is carried out through ARCLE, which manages the problem and its corre-\nsponding solution from SOLAR. ARCLE handles state transitions, performs actions, and verifies\nwhether the submitted solution is correct. As depicted in Figure 6, ARCLE interacts with the LDCQ\ninference network by alternating the exchange of st and at, facilitating the decision-making process\ntoward reaching the correct answer state. The latent z\u0142 represents a segment trajectory spanning from\ntimestep t to t + H \u2013 1, and is trained to accurately decode actions for any state within this segment\ntrajectory.\nIn the original LDCQ methodology, inference is performed by executing several horizons using a\nsingle latent, followed by predicting the next latent. However, in the task used for this research, which\nhas a gold standard trajectory consisting of five steps, it is possible to complete the task with just\none latent sampling from the initial state. While reaching the correct answer in this manner is not\ninherently problematic, one of the primary goals of this research is to analyze whether the agent\nlearns the knowledge prior to how actions work across various states. Thus, instead of focusing solely\non solving the problem in as few steps as possible, only one action is conducted per latent. With this,\nthe results demonstrate that the agent can make far-sighted decisions to reach the answer not just\nfrom the beginning to the end, but also through intermediate steps."}, {"title": "5.2 RESULTS", "content": "To demonstrate the strengths of the diffusion-based offline RL method guided by Q-function, we\ncompare three approaches:\n\u2022 VAE prior (VAE): This method uses a latent sampled from the VAE state prior pw (zt St).\nThe VAE state prior is trained in B-VAE training stage by calculating the KL divergence\nbetween pw(zt|st) and the posterior q\u00a2(zt|Tt), aligning the latent distribution with the\ntrajectory starting from state st.\n\u2022 Diffusion prior (DDPM): This method uses a latent sampled from the diffusion model\nPy(zt|st) through the DDPM method (Ho et al., 2020). The sampled latents closely resemble\nthe training data, with added variance during the denoising process. This method is similar\nto behavior cloning in that it operates without guidance from rewards or value functions.\n\u2022 Max Q latent (LDCQ): This method selects a latent with the highest Q-value from those\nsampled by the diffusion model, argmaxz~py(zt|st) Q(St, z), to make a decision at st.\nThe evaluation of each approach was conducted five times for the evaluation SOLAR set. The results,\nsummarized in Figure 7a, show the success rates for: 1) Whether the agent reaches the correct answer\nstate and 2) Whether the agent executes Submit operation in the answer state. When using the VAE\nprior, the agent reaches the correct answer state in only about 10% of test episodes and submits\nthe answer in just 1%. With latents sampled using DDPM, about 10% of the answers are correctly\nsubmitted, while the agent reaches the answer state approximately 37% of the time. When using\nLDCQ, the agent reaches the answer state in over 90% of cases and successfully submits the correct\nanswer in about 77% of test episodes. These results demonstrate that the Q-function enhances the\nagent's ability to both reach the correct answer and recognize when it has arrived at the answer state."}, {"title": "6 LIMITATIONS & DISCUSSIONS", "content": "In our experiment, the LDCQ method showed significant improvement in reaching the goal. However,\nin approximately 16% of cases, the agent reached the correct state but proceeded with another action\ninstead of submitting the solution, even with the assistance of the Q-function. This issue arises because\nthe Q-function, while enhancing decision-making, sometimes assigns higher values to actions other\nthan submission, causing the agent to bypass the goal state. This suggests that the Q-function is not\nperfectly aligned with the final objective in ARC. Notably, in ARC tasks, even when solving different\ntest examples within the same task where the same rule is applied, the actual action sequence can\nvary depending on factors like grid size or the arrangement of elements in the input grid. The current\nQ-values are calculated based on the absolute state values, which occasionally leads to misjudgments\nwhen submitting the correct solution. Therefore, improving the agent's ability to accurately determine\nwhen to submit the correct answer is necessary for future research.\nWhile the LDCQ approach performs well in a simple ARC task setting, more complex tasks and\nmulti-task environments present additional challenges. Unlike single-task scenarios, where the agent\nfollows a fixed strategy toward a predefined answer, multi-task settings demand flexibility to adapt to\nchanging goals or new possibilities during task execution. We expect that addressing these challenges\ncould involve integrating task classifiers for Q-learning. Additionally, incorporating modules so\nthat the agent can revise its strategy during task execution\u2014adjusting based on evolving states or\nobjectives rather than rigidly following the initial strategy-may enhance its adaptability.\nIn traditional supervised RL approaches, such as those described by Ghugare et al. (2024), stitching\ntypically occurs only when the goal remains consistent across tasks. To address this limitation, we\nemployed temporal data augmentation, which involves starting from an intermediate state near the\ngoal and setting a new target. In SOLAR, this could be extended by using non-optimal paths as goals\nin non-optimal trajectories. However, in ARC, where goals are determined by demonstration pairs,\naugmenting all goals is impractical. More careful strategies are needed to enable stitching for entirely\nnew goals not previously encountered. If methodologies are developed that can combine existing\nactions toward different goals, we expect that SOLAR will facilitate these combinations.\nGoing forward, refining how the Q-function evaluates states and actions will be crucial. To improve\nperformance, especially in multi-task environments, incorporating mechanisms that not only assess\nthe state and action in relation to the goal but also guide the agent toward the most effective path to\nachieve the ultimate objective will be beneficial. Recognizing the task's context and how close states\nare to the correct solution is essential for ensuring that the Q-function helps navigate toward the goal\nefficiently."}, {"title": "7 CONCLUSION", "content": "This research demonstrates the potential of offline reinforcement learning (RL), particularly the\nLatent Diffusion-Constrained Q-learning (LDCQ) method, for efficiently sequencing and organizing\nactions to solve tasks in grid-based environments like the Abstraction and Reasoning Corpus (ARC).\nTo our knowledge, this work is the first to tackle ARC using a diffusion-based offline RL model\nwithin a properly designed environment, guiding agents step-by-step toward correct solutions without\ngenerating the full ARC grid at once. Through training on SOLAR, we successfully applied and\nevaluated offline RL methods, showing that agents can learn to find paths to the correct answer state\nand recognize when they've reached it. This suggests that RL with a well-designed environment\nis promising for abductive reasoning problems, potentially reducing data dependency compared to\ntraditional methods. As tasks become more complex, especially in multi-task settings, refining the\nQ-function to address unique reward structures is crucial, with multi-task environments requiring\ntask-specific adaptations to account for varying states and rewards. Integrating modules like task\nclassifiers or object detectors could enhance the agent's ability to dynamically adjust its strategy,\npromoting more flexible decision-making. This research opens new avenues for program synthesis\nin analogical reasoning tasks with RL environments, potentially integrating with analogy findings\ntechniques (hypothesis search with LLMs) to handle a wider range of ARC tasks."}, {"title": "A TRAINING DETAILS", "content": "A.1 LATENT DIFFUSION CONSTRAINED Q-LEARNING (LDCQ)\nTraining Latent Encoder and Policy Decoder The first stage in training with LDCQ is to train a\nB-VAE that learns latent representations. In this stage, the \u1e9e-VAE learns how actions are executed\nover multiple steps to change the state. With H-horizon latents, it becomes easier to capture longer-\nterm changes in the state. We use SOLAR as the training dataset D, which contains H-length\nsegmented trajectories T. Each Tt consists of state sequences St:t+H = [St, St+1, ..., St+H\u22121]\nand action sequences at:t+H = [at, at+1, ..., at+H\u22121], along with additional information such as\ndemonstration examples. As shown in Figure 3a, during the B-VAE training stage, the encoder q is\ntrained to encode Tt into the latent representation zt, and the low-level policy decoder \u03c0\u03b8 is trained\nto decode actions based on the given state and latent. For example, given the latent zt and a state\nfrom the segment trajectory, st+h where h \u2208 [0, H), the policy decoder decodes the action at+h for\nSt+h. The B-VAE is trained by maximizing the evidence lower bound (ELBO), minimizing the loss\nin Eq. 1. The loss consists of the reconstruction loss from the low-level policy decoder and the KL\ndivergence between the approximate posterior q$(zt|Tt) and the prior pw(zt|st).\nLVAE (\u03b8, \u03c6, \u03c9) = -ET~D Eq(2t|Tt) [ET~D Eq(2t|Tt) ] (1)\nTraining Latent Diffusion Model In the second stage, latent diffusion model is trained to generate\nlatents based on the latent representations encoded by the B-VAE. The training data consists of\n(st, zt) pairs, which are used to train a conditional latent diffusion model py(zt|st) by learning the\ndenoising function \u03bc\u1ff3(z\u00ee, st, j), where j \u2208 [0,T] is diffusion timestep. This allows the model to\ncapture the distribution of trajectory latents conditioned on st. q(z|z\u0142) denotes the forward Gaussian\ndiffusion process that noising the original data. Following previous research (Ramesh et al., 2022;\nVenkatraman et al., 2024), we predict the original latent rather than the noise, balancing the loss\nacross diffusion timesteps using the Min-SNR-y strategy (Hang et al., 2023). The loss function used\nto train the diffusion model is shown in Eq. 2. Here, zi, j \u2208 [0, T] represents noised latent on j-th\ndiffusion time step, when j = 0 then z = zt and z\u0142 f is Gaussian noise.\nL(\u03c8) = Ej~[1,T],TH~D,z\u2081~q\u2084(zt|Tt),z?~q(z\u2021\\z?) [min{SNR(j), 7}||z\u0142 \u2013 \u03bc\u03c6 (z, St, j) ||2] (2)\nTraining Q-Network Finally, the latent vectors sampled by the latent diffusion model are used for\nQ-learning. For sampling latents, we use the DDPM method (Ho et al., 2020). The trained diffusion\nmodel samples latents by denoising random noise using the starting state information st. We use\nthe data consisting of (st, Zt, rt:t+H, St+H) for training Q-Network, where rt:t+H = \u03a3 t+H-1 ri\ndeontes the discounted sum of rewards. Here, DDPM sampling is used to sample Zt+H for st+H.\nFor Q-learning, we use Clipped Double Q-learning (Fujimoto et al., 2018) as shown in Eq. 3 with\nPrioritized Experience Replay buffer (Schaul, 2016) to improve learning stability and mitigate\noverestimation. The trained Q-network Q(st, zt) evaluates the expected return of performing various\nH-length actions, with z\u0142 sampled via DDPM based on st. This allows the network to efficiently\ncalculate the value of actions over H-steps to estimate future returns.\nQ(St, Zt)\u2190( rt:t+H+Y Q(st+H, argmax Q(St+H, Z)) (3)"}, {"title": "A.2 HYPERPARAMETERS", "content": "We used a horizon length of 5 for encoding skill latents, meaning the model plans and evaluates\nactions over a five-step lookahead.\nWe trained the diffusion model with 500 diffusion steps. If the number of diffusion steps is too small,\nit can lead to high variance in the sampling process, potentially causing errors during the decoding of\noperations or selections in ARCLE. To minimize these errors, we set the number of diffusion steps to\n500, ensuring more accurate operation and selection decoding from the sampled latents.\nWe set the discount factor to 0.5 to ensure the model appropriately balances immediate and future\nrewards. Since the total steps required to reach the correct answer in ARCLE are usually fewer than\n20, a high discount factor could cause the agent to struggle in distinguishing between submitting at\nthe correct state and continuing with additional steps, which could lead to episode failure.\nThe hyperparameters that we used for training three stages of LDCQ are shown in Tables 1, 2 and 3."}, {"title": "A.3 HARDWARE", "content": "We used an NVIDIA A100-SXM4-40GB GPU to train the model. Training the B-VAE took about 7\nhours, while training the diffusion model and Q-network each took around 6 to 10 hours."}, {"title": "B DETAILS OF SOLAR-GENERATOR", "content": "B.1 OPERATIONS IN SOLAR\nThe operations from 0 to 34 are identical to those used in ARCLE (Lee et al., 2024). Since Submit\nis an operation that receives a reward, it should only be used when the state is considered correct\nand not excessively. Due to LDCQ's fixed horizon, and to ensure that the agent only uses Submit\nwhen the state is definitively correct, we added a None operation that fills all subsequent states after\nSubmit with the 11th color (10), which does not exist in the original ARC (0\u20139). In other words,\nduring training, the None action emphasizes that the episode ends after Submit.\nB.2 GRID MAKER\nFor generating SOLAR, we create a generator that can synthesize a large amount of data for a given\nrule. Grid Maker is a hard-coded program specific to each task. Grid Maker contains the rules for\nsynthesizing demonstration examples and test examples, and the synthesized solution action path\nconsists of operations and selections. In Grid Maker, data is formatted to be compatible with ARCLE.\nThe Grid Maker constructs analogies with the same problem semantics but with various attributes\nsuch as the shape, color, size, and position of objects. SOLAR-Generator can generate intermediate\ntrajectories by interacting with ARCLE. The algorithm of the SOLAR-Generator is designed to\naugment specific tasks using the Grid Maker, which can primarily be divided into three parts.\nGrid Maker was built as a data loader, which is used in ARCLE. In the original ARCLE environment,\nthere was no need to load operations and selections-only the grid was loaded since the problem\nalone was sufficient. To change this structure, the entire environment would need to be recreated.\nInstead, operations and selections are now loaded from the data loader's description, allowing us\nto retain the original environment. Therefore, the process of creating input-output examples and\ngenerating action sequences works within a single file.\nSpecifying Common Parts Each task in the ARC dataset usually contains 3 demonstration exam-\nples, with common elements observed across these pairs. In the common parts, attributes such as\ncolor, the type of task, and the presence of objects are predetermined using random values before pair\ngeneration.\nSynthesizing Examples In the example synthesis phase, the input of the original task is augmented\nin a way that ensures diversity while preserving the integrity of the problem-solving method. A\nrandom input grid is generated under conditions that satisfy the analogy required by the task. A\nsolution grid is created using a hard-coded algorithm. For tasks involving pattern-based problems, as\nexperimented in the paper, selections are made to fit the grid size, and various operations are executed\neither randomly or in a predetermined order. For object-based problems, the solution grid is generated"}, {"title": "Converting to ARCLE Trajectories", "content": "This stage involves the creation of an ARCLE-based trajectory\nthat meticulously adheres to the problem-solving schema of the synthesized examples. The entire\nprocess is carried out through a hard-coded algorithm. During the example synthesis process, the\nlocations of objects may already be known, or they can be identified using a search algorithm. The\ninformation obtained is then used to make the appropriate selections, and the trajectory is converted\ninto an ARCLE trajectory through an algorithm that leads to the correct solution.\nIf all steps are properly coded, it is possible to generate the operations and selections that lead to the\ncorrect solution for any random input grid. These are then fed into ARCLE to obtain intermediate\nstates, rewards, and other information, and to verify whether the correct result is reached. Once steps\n1) to 3) are correctly implemented, SOLAR-Generator can continuously and automatically generate\nas much data for the given task as the user desires, using the Grid Maker."}, {"title": "B.3 EXAMPLE OF DATA SYNTHESIS IN GRID MAKER AND THE GENERATION OF SOLAR", "content": "SOLAR-Generator can synthesize SOLAR for object-based tasks. Figure 9 shows a variant of Task 2\nfrom Figure 1. Grid Maker generates random input grids with some variances firtst. In this variant,\neach episode randomly selects two colors for the boxes. Each inputs can have different grid sizes,\nand rules are established for objects of each color within the episode. Then it generates the answer\noutput grids for the input grids through algorithm. The solution algorithm in Grid Maker proceeds as\nfollows: 1) Find the top-left corner of the orange square and repeat the coloring process to draw a\ndiagonal line to the grid's edge. 2) Find the bottom-right corner of the red square and repeatedly color\ndiagonally until the end of the grid is reached. With these algorithms, Grid Maker can synthesize as\nmany examples and SOLAR trajectories as the user desires."}, {"title": "B.4 ALGORITHM OF SOLAR-GENERATOR", "content": "With the synthesized data through the Grid Maker module, the SOLAR-Generator checks the sanity\nof the synthesized trajectory, and then saves the data. The whole algorithm for SOLAR-Generator is\ndescribed in Algorithm 1."}, {"title": "B.5 OTHER SOLAR EXAMPLES", "content": "Figure 10 illustrates two examples of episodes from the tasks used in the experiment. Each episode\nincludes three random demonstration examples and a trajectory for a test example. Figure 10a shows\na gold standard trajectory, which represents the ideal sequence of actions to reach the correct answer\nstate. Figure 10b shows a non-optimal trajectory that, while not a gold standard, also reaches the\nanswer state. The clip grid, reward, and termination information are not displayed."}, {"title": "C SOLAR-GENERATOR VS RE-ARC: COMPARING ARC DATA\nAUGMENTATION APPROACHES", "content": "During the development of SOLAR, another augmentation scheme called RE-ARC (Hodel, 2024) was\nindependently developed and released. While both aim to generate diverse examples for ARC tasks,\nthey differ significantly in their design objectives, underlying architectures, and dataset structures.\nUnderlying Architectures SOLAR is built upon the ARCLE framework, designed for training\nreinforcement learning agents. It uses a limited set of actions based on the O2ARC web interface,\nwhich, despite their simplicity, are sufficient primitives to solve all ARC tasks. This design choice\nresults in sequential trajectories directly applicable to reinforcement learning models. In contrast,\nRE-ARC is based on a more comprehensive Domain Specific Language (DSL) developed by Hodel,\nfeaturing 141 primitives. This expanded set of operations provides greater flexibility in expressing\nsolutions, allowing for more complex transformations.\nData Generation Approach SOLAR generates sequential trajectories that mirror the step-by-step\napproach humans use when solving ARC tasks. This aligns well with typical reinforcement learning\nmodels that execute actions sequentially. RE-ARC, leveraging its expansive DSL, generates solutions\nin the form of directed acyclic graphs (DAGs). This approach allows for more complex problem-\nsolving strategies but may present challenges when applied to traditional reinforcement learning\nframeworks.\nDataset Structure and Utility SOLAR provides complete episodes with detailed trajectories,\nincluding all intermediate states. This feature is crucial for training agents with offline reinforcement\nlearning methods, allowing models to learn from the entire problem-solving process. RE-ARC focuses\non augmenting input-output pairs without explicitly providing intermediate steps. While valuable\nfor increasing example diversity and testing generalization capabilities, it may require additional\nprocessing for direct application in a reinforcement learning context.\nFlexibility and Potential for Integration SOLAR's design allows for easy generation of large\nepisode datasets and"}]}