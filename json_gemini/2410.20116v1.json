{"title": "Estuary: A Framework For Building Multimodal Low-Latency Real-Time Socially Interactive Agents", "authors": ["Spencer Lin", "Basem Rizk", "Miru Jun", "Andy Artze", "Caitl\u00edn Sullivan", "Sharon Mozgai", "Scott Fisher"], "abstract": "The rise in capability and ubiquity of generative artificial intelligence (AI) technologies has enabled its application to the field of Socially Interactive Agents (SIAs). Despite rising interest in modern AI-powered components used for real-time SIA research, substantial friction remains due to the absence of a standardized and universal SIA framework. To target this absence, we developed Estuary: a multimodal (text, audio, and soon video) framework which facilitates the development of low-latency, real-time SIAs. Estuary seeks to reduce repeat work between studies and to provide a flexible platform that can be run entirely off-cloud to maximize configurability, controllability, reproducibility of studies, and speed of agent response times. We are able to do this by constructing a robust multimodal framework which incorporates current and future components seamlessly into a modular and interoperable architecture.", "sections": [{"title": "1. Background", "content": "Rapid advancements in AI have catalyzed the development of SIAs, which integrate complex technologies to facilitate nuanced human-computer interactions across various domains, producing affective virtual agents [14], multi-agent simulations of human behavior [19, 26, 12], and more [15, 16, 24]. To build effective SIAs, multiple microservices and components need to be integrated and managed, including Automatic Speech Recognition (ASR) and Text-To-Speech (TTS). Implementing these features require significant efforts. A modern, comprehensive SIA framework is useful to streamline efforts and reduce redundant work [17] and to provide a scalable and interoperable standard that can support new models and microservices."}, {"title": "1.1. SIA & Conversational AI Frameworks.", "content": "Several existing toolkits help streamline the process of building SIAs. The Virtual Human Toolkit (VHToolkit) [13] and Greta [21] are such examples, however, they do not support generative AI models such as Large Language Models (LLMs). Moreover, a recent wave of lightweight, high performing models (Speech-To-Text (STT) [23, 20], LLM [4], and TTS [8]) can be run on-edge to overcome privacy concerns evident in cloud-based services. These gaps and improvements in current tools add additional motivation for the creation of a cohesive framework tailored for SIAS.\nRecent projects such as Pipecat [11] and NVIDIA ACE [2] provide frameworks for building conversational agents by connecting AI microservices. However, Pipecat is not tailored for building SIAs and lacks integration with game engines, which SIA research heavily relies upon. NVIDIA ACE, at the time of writing, is not open-source and restricts users to only running NVIDIA-approved AI microservices. This may be a critical limitation for research involving custom AI microservices and pipelines. Furthermore, NVIDIA ACE requires a cost-prohibitive enterprise plan if developers would like to host their microservices off-cloud or on-prem."}, {"title": "1.2. Limitations of Current Approaches.", "content": "Currently, several factors hinder SIA research: 1) computational limitations of devices (e.g., head-mounted displays (HMDs)) which prohibit running advanced AI models on a standalone device, 2) hardware architecture incompatibilities with AI models which necessitate an inferencing server, and 3) high latency of cloud-based microservices. To address these shortcomings, we developed"}, {"title": "2. System", "content": "Estuary brings five core value-propositions: 1) an interoperable microservice architecture, 2) multi-platform support, 3) off-cloud capabilities, 4) support for multimodal input and analysis, and 5) an open-source nature that opens it to community contributions."}, {"title": "2.1. Microservice Architecture.", "content": "We use a modular design to universally wrap local model or online API service within a Stage, which are asynchronous and parallelizable as denoted in Figure 3. A Stage component wraps the ML inference logic and hosts it on a child process, aggregating inputs and dispatching outputs. A selection of Stages (e.g., Whisper [22], GPT-3.5 [6], gTTS [10]) are connected into a Pipeline according to the flow of choice. The Pipeline internally orchestrates the flow of a standardized data type DataWindow, which consists of one or more DataPacket(s) (e.g., AudioPacket, VisionPacket, TextPacket, etc.). A DataPacket, with its source and creation timestamp, acts as identifiable placeholders for the Stage outcomes. This micro-service architecture allows us to plug and play new models rapidly by implementing them as Stage(s). A Pipeline out of the box runs as a background process on a server that can communicate with any client through SocketIO protocol [3]."}, {"title": "2.2. Multi-platform.", "content": "Estuary is a distributed framework that uses the SocketIO protocol to establish a connection between a client device and a host device running our framework. It can"}, {"title": "2.3. Off-Cloud.", "content": "In addition to leveraging cloud APIs, Estuary can be hosted locally and/or entirely with off-cloud microservices. This reduces latency by eliminating multiple cloud endpoints, improves reproducibility, and adds privacy and security as no data ever enters the cloud. As shown in our video demo, it takes approximately 1.2~2.5 seconds (compared to an average 2.8 second latency from ChatGPT-40 [18]) from the end of a user's dialogue to the first utterance from the SIA's TTS module through a FasterWhis-perBase.EN [1] \u2192 GPT-3.5 API [6] \u2192 XTTS [8] pipeline on a desktop with a RTX 4090 graphics card. This is made possible through several optimizations such as simultaneously streaming the LLM and TTS response. Furthermore, off-cloud microservices ensure reproducibility, which is of utmost importance especially in fields relating to psychology [9]. In Estuary, the exact same versions of LLM, ASR, TTS, and other microservices can be maintained and loaded for future use, whereas cloud-based services do not have guarantees to remain unchanged overtime."}, {"title": "2.4. Multimodal Input And Analysis.", "content": "Multimodal input [18] is critical for agents to understand the physical world and multiple modalities to produce a better cognitive model [5, 7]. Estuary integrates with Unity and packages like ARKit [25] to empower embodied agents with basic semantic understanding of the physical world and pathfinding capabilities in Augmented Reality (AR). Currently, Estuary supports text and audio datastreams. We plan to expand to video as well which will shift reliance away from hardware-specific packages."}, {"title": "2.5. Open-Source.", "content": "Estuary is open-source and promotes growth from community contribution. By nature, Estuary is flexible and hugely extensible to support integration of microservices now and into the future. Researchers and developers have"}, {"title": "3. Demonstration", "content": "Our first scenario consists of a computer hosting Estuary interfaced to an HMD over a local network to demonstrate an advanced embodied conversational agent that can classify objects in its physical surroundings and interact accordingly. The second scenario consists of a computer hosting both Estuary and a desktop frontend to demonstrate the versatility of our framework. Estuary's source code can be found in our GitHub 1 and a demo video here on our website 2."}]}