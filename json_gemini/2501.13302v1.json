{"title": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers", "authors": ["Akshit Achara", "Anshuman Chhabra"], "abstract": "AI Safety Moderation (ASM) classifiers are designed to moderate content on social media platforms and to serve as guardrails that prevent Large Language Models (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential for disparate impact, it is crucial to ensure that these classifiers: (1) do not unfairly classify content belonging to users from minority groups as unsafe compared to those from majority groups and (2) that their behavior remains robust and consistent across similar inputs. In this work, we thus examine the fairness and robustness of four widely-used, closed-source ASM classifiers: OpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL) API, and Clarifai API. We assess fairness using metrics such as demographic parity and conditional statistical parity, comparing their performance against ASM models and a fair-only baseline. Additionally, we analyze robustness by testing the classifiers' sensitivity to small and natural input perturbations. Our findings reveal potential fairness and robustness gaps, highlighting the need to mitigate these issues in future versions of these models.", "sections": [{"title": "1 Introduction", "content": "AI Safety Moderation (ASM) classifiers are designed to mitigate hateful, unsafe, toxic, and problematic content for two primary applications: (1) content moderation online on social media platforms (e.g. Facebook), and (2) as safety guardrails to ensure that Large Language Models (LLMs) are not fine-tuned on harmful data. The access to these ASM models is often provided in a closed-source black-box manner (OpenAI). ASM models play a major and consequential role in the aforementioned applications. For instance, given the exponential growth in content generation across social media platforms, ASM classifiers are essential in automating moderation tasks that would otherwise be impractical to manage only manually. Similarly, as ASM models moderate what user content LLMs can be fine-tuned on by filtering training data, they directly impact the behaviors the models learn. For instance, OpenAI's Moderation API needs to be used prior to fine-tuning their GPT models. With this growing dual use of ASM classifiers for social media content moderation and LLM fine-tuning, it's vital to ensure they are unbiased, robust and safe to use. Due to their closed-source nature, ASM models may unfairly target or overlook marginalized groups, leading to biased outcomes in content moderation and LLMs trained on filtered data. Bias in moderation can damage trust in online social media platforms, potentially suppress essential voices, and perpetuate inequalities in AI systems trained on the moderated data. Similarly, a lack of robustness can allow exploitative behaviors to bypass moderation efforts, compromising both user safety and data integrity for any subsequent AI training Both these case scenarios are visualized in Figures 1 and 2.\nTo our best knowledge, large scale end-user audits have only been conducted on one ASM model (Perspective API), particularly highlighting issues that affect marginalized communities. However, these evaluations required users to highlight the issues manually and did not utilize a fairness analysis framework relying on analytical fairness metrics. To our knowledge, no formal fairness analysis has been conducted on close-sourced ASM models to date.\nThrough this paper, we seek to bridge this gap and study fairness and robustness for four commonly used closed-source ASM classifiers, namely, OpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL) API and Clarifai API, across multiple predictive tasks. In summary, we make the following contributions:\n\u2022 We formally model the group fairness and robustness problems in classification in the context of ASM models to study closed-source ASM models.\n\u2022 Through extensive experiments on various datasets, we find that the OpenAI ASM model is more unfair as compared to the other ASMs and find that these models are not robust to minimal LLM-based perturbations in the input space.\n\u2022 We highlight that the LLM-based perturbation allows unsafe comments to bypass the ASM models and provide further insights through qualitative examples (see details in Appendix G)."}, {"title": "2 Related Works", "content": "Progress has been made in evaluating fairness in social media content moderation and measuring bias in open-source text classification ASM models. In , the authors show that German content is moderated more than other languages by the Perspective API. However, recent research emphasizes the need for fairness evaluation and improved ASM models for closed-source LLM services. In , methods to jailbreak ASM models and fine-tune LLMs to induce bias and make them unsafe are discussed. Research in shows that LLMs can produce unsafe content through prompt-based techniques. In , the authors utilize LLMs as toxicity classifiers and show performance improvement over Perspective API. Overall, while the broader problem of bias in LLMs has been explored; the analysis of fairness and robustness in close-source ASM models remains unaddressed."}, {"title": "3 Problem Statement", "sections": [{"title": "3.1 AI Safety Moderation", "content": "We first begin by describing a simple framework for ASM classifiers. More specifically, we will ensure that it is general, so that different ASM models can be studied and analyzed under this framework with respect to fairness and robustness. Formally, an ASM classifier $C$ takes as input some natural language input $X_i$ and then outputs a value $Y_i$ that takes on 0 if the input text is safe and 1 if the text is considered unsafe by the model."}, {"title": "3.2 Analyzing ASM Fairness", "content": "We wish to evaluate the ASM classifier for fairness across multiple protected groups and sensitive attributes (e.g. ethnicity and gender). The goal is to ensure predictive outcomes made by the model are not unfairly biased across marginalized/minority protected groups. We will consider two popular fairness metrics: Demographic Parity (DP) and Conditional Statistical Parity (CSP) . More details regarding the metrics are provided in Appendix B. Additionally, the legitimate factors required for the CSP computation are obtained using the BERT regard classification model which measures language polarity towards a demographic along with the social perceptions of that demographic. For example, a male could be mentioned in a positive or negative aspect and this classification can help analyze the ASM models in a fine-grained manner (see details in Appendix F). Note that both DP and CSP lie between [0, 1] and values closer to 0 imply higher fairness, indicating less group-dependent classification error in predictions made by the classifier."}, {"title": "3.3 Measuring ASM Robustness", "content": "We now study the robustness properties of ASM models. A simple definition of natural robustness implies that minimal perturbation of the input space should not lead to high variance in predicted output by the classifier. We perturb text inputs minimally and measure the variation in model performance. We employ two strategies for perturbations that retain semantic similarity: (1) Backtranslation and (2) LLM-based. In the former, we randomly back-translate one sentence of the input text sequence from German and in the latter, we utilize GPT-3.5-Turbo to paraphrase the input sentence. Our detailed prompts for the LLM-based method and additional details on backtranslation are provided in Appendix K.\nTo measure robustness analytically, consider such a perturbation (using one of our two methods) applied to a given input text dataset $X$ which outputs a semantically similar input instance $X^*$. Then, we can simply measure the error in classification as: $f_{robust} = |E_X(C(X)) - E_{X^*}(C(X^*))|$."}]}, {"title": "4 Experimental Results", "content": "We conduct experiments using two datasets: Jigsaw Toxicity and a manually collected and annotated Reddit comments dataset. The former is a dataset for toxicity classification of Wikipedia comments released by Google/Jigsaw, and contains labels for gender, race/ethnicity, religion, sexual orientation and disability, along with toxicity. Each of these constitutes a subdataset (as comments are different) and we refer to these 4 tasks as: Jigsaw-Gender, Jigsaw-Ethnicity, Jigsaw-Disability, Jigsaw-Sexual_Orientation. Moreover, recent work has found that LLMs are biased in terms of political ideology . Further, as LLMs serve as teacher models for ASM training (e.g. OpenAI Moderation API was trained using GPT-4), it is important to analyze ASM ideological biases/unfairness as well. Hence, we provide an additional dataset based on comments from the Reddit platform. To do so, we scraped 1147 comments from explicitly political left-leaning and right-leaning subreddits and 3 graduate students manually annotated them for left-leaning or right-leaning political ideology, to conduct this analysis.\nWe provide additional dataset details below:\n(1) Jigsaw-Gender: It is a toxic comment detection dataset shared as a part of the Jigsaw toxicity detection challenge. The comments are labeled with identities that cover aspects like gender, race/ethnicity, religion, sexual orientation and disability. In this work, we only use the comments that have a single identity label i.e. each comment is only labeled with one group and one associated concept. For example, a comment can be labeled with female identity associated with gender aspect.\n(2) Jigsaw-Ethnicity: This is a subset derived from the Jigsaw toxic comment dataset and consists of comments labeled with ethnic groups, namely asian, black, latino, other and white.\n(3) Jigsaw-Disability: It consists of Jigsaw comments labeled with different types of disabilities, namely intellectual_or_learning_disability, physical_disability, psychiatric_or_mental_illness and other.\n(4) Jigsaw-Sexual_Orientation: It is a collection of Jigsaw comments labeled with categories related to sexual orientation, namely bisexual, heterosexual, homosexual_gay_or_lesbian and other.\n(5) Reddit-Ideology: We include ideological leaning (left or right) in our fairness analysis. In this manually annotated dataset, we collect 1147 new comments from the following explicitly political left-leaning and right-leaning sub-Reddits: r/Conservatives, r/conservatives, r/Democrats, and r/Socialism, which are passed through a BERT based political classifier to filter out explicitly political comments. We obtain an inter-annotator agreement of 0.959 by computing the Cohen's Kappa.\nModels. We consider 4 proprietary ASM classifiers commonly used in the community: OpenAI Moderation API, Perspective API , GCNL API, Clarifai API. Moreover, we also consider a simple Always Fair baseline for fairness reference, which always assigns moderation labels (safe/unsafe) uniformly randomly- achieving high fairness but low accuracy. More details on the ASM models and the baseline are provided in Appendix A."}, {"title": "5 Discussion", "content": "More fine-grained fairness analysis. Through our experiments, we observe that there are clear fairness issues in OpenAI, Perspective, and Clarifai ASM models, especially when considering sexual orientation as a sensitive attribute. While the analysis does not flag any significant fairness issues for the GCNL ASM model, an additional experiment specific to the domain could be performed by downweighting the labels provided by this model. This is because the model provides 16 labels which might not be related to safety in all the practical scenarios (see additional details in Appendix F where we show that the ratio of unsafe to safe comments is higher for the GCNL API as compared to the other ASM models for all the regard labels).\nMinimal perturbations lead to significant ASM robustness issues. We show that minimal LLM-based perturbations using GPT-3.5 Turbo can cause all ASM models to change their initial predictions (see Figure 4b) and this error in robustness is the highest for OpenAI Moderation API ASM across all the datasets (see Table 2 in Appendix D for more details). The perturbed samples generated as part of our experiments can also serve as a benchmark for comparing against any updates to closed-source ASM models. For instance, the text-moderation-007 model behind the OpenAI Moderation API might be updated with a newer model which can be compared with our results to gain insights.\nBypassing guardrails and adversarial attacks. We observe in Figure 4b that for the OpenAI, Perspective and Clarifai ASM models, the LLM-based perturbation causes majority of the initially unsafe comments to be classified as safe. This opens up possibilities for adversarial attacks such as AutoDAN and persuasively adversarial prompts (PAP) where mailicious actors could exploit these perturbations to intentionally bypass the ASM models.\nUnderstanding impact of perturbations on harmful inputs. Our LLM based perturbation paraphrases the input text into a similar text while preserving its semantic meaning. To understand the effect of this LLM-based perturbation on harmfulness of originally harmful inputs, we manually evaluate the perturbed inputs. Specifically, we select 50 inputs each from the Jigsaw datasets (gender, ethnicity, disability and sexual orientation) and, select 100 harmful examples from the Reddit-Ideology dataset to label as harmful/harmless post perturbation. We find that for the Jigsaw datasets, 19 out of 200 harmful inputs become harmless and for Reddit-Ideology, 16 out of 100 harmful inputs become harmless, indicating that perturbed inputs retain semantically relevant harm information.\nIntersectional fairness studies. In our work, we mainly focus on cases where only one protected attribute is present, as motivated by prior work on fairness. In Appendix I, we highlight the need for an intersectional analysis of fairness and perform experiments to study the same using the OpenAI ASM model. Future research in this direction can focus on larger scale intersectional studies on ASM fairness.\nChoosing ASM model thresholds. The ASM Models provide an output score upon which a threshold is applied to obtain the binary safe and unsafe labels. In our study, we use a threshold of 0.5 to conduct a fair comparison study. However, in Appendix J, we show the impact of applying a threshold of 0.7 on the ASM model fairness. We observe that the choice of theshold may improve or worsen the fairness of ASM models and thus, future work can provide more insights on threshold selection and its impact of fairness of ASM models."}, {"title": "6 Conclusion", "content": "We perform a fairness and robustness analysis on the AI Safety Moderation Classifiers (OpenAI, Perspective, GCNL and Clarifai) that are used for social media content moderation and as guardrails for fine-tuning closed-source LLMs. We highlight the issues in fairness and robustness based on the predictions made by ASM models on two datasets with several sensitive attributes (gender, ethnicity, disability, sexual orientation and ideology). Notably we observe that there are significant issues with ASM models in terms of robustness. Our work highlights the potential risks associated with the use of current ASM models and the dire need to mitigate these in future work."}, {"title": "Limitations", "content": "We considered the available text-moderation-007 OpenAI Moderation API model for our experiments. This version might be updated with a newer model in the future, changing results. Additionally, one of our perturbation strategies for robustness analysis utilizes the GPT-3.5-Turbo LLM, which can also be updated or deprecated by OpenAI in the future. The amount of perturbation may be of concern is some cases where the harmfulness of the inputs is changed. Finally, our work is limited to the English language, but it is of paramount importance to consider low-resource languages and specialized domains in future work. Our work is also localized to textual input, but future work can consider fairness for multimodal data."}, {"title": "Ethics Statement", "content": "Our work is important for understanding the behaviour of ASM models that are used to moderate a variety of social media content and also serve as guardrails for LLM fine-tuning. Maintaining fairness in these systems is crucial to prevent discrimination against minority groups. Additionally, the robustness analysis helps in flagging issues with the inconsistency in the behaviour of ASM models. It is important to ensure that the behaviour of these systems is consistent, fair, and unbiased our work is a preliminary step towards achieving this."}, {"title": "Appendix", "sections": [{"title": "A ASM Model Descriptions", "content": "In this section, we describe the ASM models analyzed in our study.\nAlways Fair Baseline. We use a randomly uniform classifier as our baseline ASM model for the fairness analysis. Since the uniformly random classifier assigns the predictions 0 (for safe) and 1 (for unsafe) to a comment with equal probabilities i.e the prediction is independent of the bias and harm aspects of the input comment which makes it a good choice as a fairness baseline.\nOpenAI Moderation API. This API serves as an ASM model for the OpenAI GPT models . It captures various aspects of safety using labels like hate, harassment, etc Each of the labels have associated probabilities and binary flags. Overall, a binary output flag is provided where True indicates an unsafe input and False indicates a safe input.\nPerspective API. This API is a BERT-based ASM model that covers toxicity aspects in terms of the following labels: toxicity, severe_toxicity, identity_attack, insult, profanity and threat.\nGCNL API. This PaLM2 based moderation API serves as an ASM model which covers several safety aspects in terms of labels listed here.\nClarifai API. This BERT-based ASM model classifies a comment into the following labels: toxic, severe_toxic, obscene, threat, insult and identity_hate.\nFor Perspective, GCNL and Clarifai APIs, each label is provided with a probability score where we consider a comment unsafe if any of the scores are greater than or equal to 0.5 and safe otherwise."}, {"title": "B Definitions and Terminology", "content": "In this section, we discuss the fairness definitions used in our work. As described in the section 3.1, $X$ is the set of input texts and $Y$ is the set of outputs indicating whether the input is safe or unsafe. Specifically, $Y = \\{Y_i\\}_{i=1}^n \\in \\{0, 1\\}^n$. We denote the protected group memberships for a batch of samples as $G = \\{G_i\\}_{i=1}^n \\in \\{0, 1\\}^n$ where 0 indicates the minority or under-represented group and 1 the majority or over-represented group. Note that we only have black-box access to the model $C$ and can only access generated output predictions $\\hat{Y}$ on the input texts $X$. We now describe two fairness measurement functions discussed in section 3.2."}, {"title": "B.1 Demographic Parity (DP)", "content": "Demographic parity is a fairness metric which is satisfied if model outcomes are independent of the input's membership in sensitive group.\nDemographic Parity (DP) can then be defined as: $f_{DP}(C, X) = |E_X(\\hat{Y} = 1|G = 0) - E_X(\\hat{Y} = 1|G = 1)|$.\nA DP value closer to 0 implies higher fairness as that indicates less group-dependent classification error in predictive parity of the classifier."}, {"title": "B.2 Conditional Statistical Parity (CSP)", "content": "Conditional Statistical Parity is a fairness metric that is satisfied when inputs from both protected and unprotected groups have an equal probability of receiving a positive outcome from the model.\nCSP is similar to DP but also controls for a set of legitimate factors $L$ in the fairness measurement. For example, this could indicate all text samples that are written with negative sentiment. That is, we could measure fairness only on this subset of comments where negative sentiments $(L = 1)$ were exhibited by the text author. CSP can then be defined as: $f_{CSP}(C, X) = |E_X(\\hat{Y} = 1|L = 1, G = 0)| - E_X(\\hat{Y} = 1|L = 1, G = 1)|$.\nThe details of regard classifier used in our experiments to obtain the legitimate factors $L$, are discussed in Appendix F. We specifically considered the negatively labelled comments for the CSP computation. Note that similar to DP, a CSP value closer to 0 implies higher fairness."}, {"title": "C Runtime Analysis", "content": "In this section, we show the time consumption for each of the ASM models used in our work. It can be seen in Table 1 that the highest time for moderation is consumed by the Perspective and GCNL APIs followed by OpenAI and Clarifai. This could be attributed to the limit on batch size along with the processing time of these ASM models. The Clarifai API allows a batch size of 128 which is higher than the alternatives resulting in faster moderation. Additionally, we used multithreading (using 5 threads) for the Perspective and GCNL APIs."}, {"title": "D Further Robustness Analysis", "content": "It can be observed in Table 2 that the error in classification robustness of OpenAI ASM is higher than other ASM models for both the input perturbations whereas the Clarifai ASM model had the lowest error. Moreover, the robustness errors are significantly higher in the LLM-based perturbation as compared to backtranslation perturbation for all the ASM models."}, {"title": "E Fairness Groups", "content": "In this section, we discuss the majority and minority groups considered for our fairness analysis in section 3.2. Table shows the majority groups for each of the datasets in consideration except for the Reddit-Ideology dataset where there are only two groups (left and right). For these datasets, we combined all the comments with labels of other groups (except majority) to form a minority group."}, {"title": "F Regard Classification", "content": "In this section, we provide the details on the regard classification used in the fairness analysis of our work. The regard classifier classifies an input text into one of the following categories: negative, positive, neutral and other. To compute the CSP fairness metric discussed in Section B.2, we used the comments labelled as negative by the regard classifier. For all the comments in our datasets combined, there were 67.3% negative, 9.1% neutral, 16.2% other and 7.4% positive comments. It can be seen in Figure 5 that the negatively labelled comments are more unsafe than other comments for all the ASM models. Additionally, the GCNL ASM model labels a significantly higher proportion of comments as Unsafe in contrast to the other ASM models where more comments are labelled as Safe. This could be attributed to the relatively broader range of sensitive topics/labels considered by the GCNL API."}, {"title": "G Qualitative Examples", "content": "In this section, we provide qualitative examples to investigate the robustness of ASM models. We select examples where all the ASM models changed their classification from unsafe to safe. Table 4 shows examples where minor perturbation has allowed the inputs, that are initially flagged as unsafe by all ASM models, to bypass all the 4 proprietary ASM models. We observe that the LLM-based perturbation may sometimes perturb the input in a way that replaces offensive words with other alternatives (while conveying the same message)."}, {"title": "H Topic Modeling", "content": "In this section, we perform a qualitative analysis on the comments from the selected datasets. Figure 6 shows the qualitative examples for the top 3 topics for each of the datasets considered in our work. The associated keywords are underlined in each of the examples and the examples are representative of the common comments corresponding to the protected groups of the datasets."}, {"title": "I Intersectional Fairness Analysis", "content": "There are cases where it is of interest to understand the bias with respect to more than one protected attribute. Therefore, we perform experiments by considering samples that contain two protected attributes. We compute the DP on these samples for both the protected attributes and compare them with the original DP values computed for each attribute individually. Specifically, we consider samples with gender + ethnicity related attributes where DP(gender) decreased from 0.074 to 0.035 (less unfair) but DP(ethinity) increased from 0.051 to 0.104 (significantly more unfair). When considering the gender and sexual orientation together, the DP(gender) decreases from 0.074 to 0.056 (slightly less unfair) and the DP(sexual orientation) increases from 0.132 to 0.171 (more unfair). For gender and disability, DP(gender) decreased from 0.074 to 0.048 (less unfair) and DP(disability) increased from 0.033 to 0.065 (more unfair). These results are obtained for the OpenAI ASM model on the Jigsaw dataset and highlight the issues in evaluating fairness for multiple protected groups simultaneously."}, {"title": "J ASM Model Thresholds", "content": "The binary labels for the input texts are obtained by applying a threshold on the prediction scores provided by the Perspective, GCNL and Clarifai ASM models with the exception of the OpenAI ASM model where the output labels are directly provided. To conduct a fair analysis, we apply a threshold of 0.5 on the scores provided by the ASM models. However, this threshold may not be optimal for all the ASM models. For instance, for the Perspective ASM model, it is recommended to use a threshold of 0.7 or higher. To this end, we conduct an experiment by selecting a threshold of 0.7 and plot the fairness metrics of Perspective, GCNL and Clarifai ASM models. In Figure 7, it can be seen that the fairness of Perspective ASM model has improved whereas that of the GCNL ASM model has worsened. Therefore, a suitable threshold can be selected depending on the use case and the fairness analysis can even aid in this selection."}, {"title": "K Code and Implementation Details", "content": "In this section, we provide the implementation details relevant to our experiments. We utilize the nlpaug library for performing the backtranslation-based input perturbation and used the GPT-3.5 Turbo to perturb the input using the input prompt: Rewrite the comment: comment. We utilize the regard to obtain the legitimate factors required to compute the CSP fairness metric. For topic modelling experiments, we use the bertopic library. The code implementation and any corresponding datasets are provided in our GitHub repository:"}]}]}