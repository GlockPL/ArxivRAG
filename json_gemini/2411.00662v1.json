{"title": "MoNTA: Accelerating Mixture-of-Experts Training with Network-Traffic-Aware Parallel Optimization", "authors": ["Jingming Guo", "Yan Liu", "Yu Meng", "Zhiwei Tao", "Banglan Liu", "Gang Chen", "Xiang Li"], "abstract": "The Mixture of Experts (MoE) is an advanced model architecture in the industry that combines multiple specialized expert models from various domains into a single supermodel. This approach enables the model to scale without significantly increasing the computational costs of training and inference, while maximizing model performance. However, current distributed training frameworks do not consider the ultimate optimization of communication, especially for large base models. This paper proposes a network-traffic-aware parallel optimization method that selects the optimal parallel strategy based on the communication volume, and the training cluster's inter-node and intra-node network topologies. Compared to the DeepSpeed, MONTA achieves an 8x increase in AllToAll communication performance under 8-card tensor parallelism. Compared to the baseline, training a 2x70B model using 16 A800 cards with an 8K sequence, results in a 13% overall latency performance improvement.", "sections": [{"title": "Introduction", "content": "The Mixture of Experts (MoE) model is an ensemble learning approach that combines multiple specialized sub-models or \"experts\" to enhance the capabilities of large language models without significantly increasing computational cost. Through gating mechanisms, dynamic expert selection is employed to facilitate efficient multitask learning. Due to the substantial communication requirements among the experts in the MoE architecture, overall computational efficiency can be affected. Currently, distributed training frameworks do not consider the intricate relationship between MoE communication efficiency, communication load, and memory usage, especially for large base models. Existing methods do not address All-ToAll communication optimizations for experts under tensor parallelism or leverage inter-node and intra-node communication for parallelism. This paper introduces a network-traffic-aware parallel optimization method that selects the optimal parallel strategy based on the size of the communication volume and the network topology of the training cluster's intra-node and inter-node communications. To the best of our knowledge, this is the first proposal that integrates communication volume, communication efficiency, and network topology in parallel optimization. By exploiting data redundancy in AllToAll communication under tensor parallelism, the AllToAll communication is equivalently transformed into a combination of inter-node AllToAll and intra-node communication. Based on the correspondence between communication volume and communication efficiency, the communication data is divided into different slices to ensure communication efficiency and achieve greater communication overlap. This approach effectively utilizes high-bandwidth intra-node connections to enhance communication efficiency, thereby improving chip compute utilization. The main contributions of this paper are as follows:\n\u2022 We Propose a communication-aware parallel optimization method MONTA: utilize inter-node and intra-node communication resources, implement inter-node All-ToAll and intra-node communication pipelining, establish a performance model for communication volume, communication efficiency, and parallel schemes, achieve MoE AllToAll communication overlap, and improve compute utilization efficiency;\n\u2022 We introduce pipelining of intra-node communication and D2D copying to further reduce AllToAll overhead;\n\u2022 We analyze communication conflict issues during the training process of the MoE model and provide a communication priority scheme;\n\u2022 We propose an expansion method for distributed parallel training cluster of the long context Mixture of Experts model, which generates a distributed parallel expansion strategy for MoE based on cluster resource parameters, model parameters, and context length.\nExperimental results show that compared to DeepSpeed baseline, MoNTA achieves a maximum performance improvement of approximately 8x in AllToAll communication"}, {"title": "Background and Motivation", "content": "This section introduces the background of MoE structure, expert parallelism, tensor parallelism, and AllToAll communication.\nMoE The concept of MoE (Mixture of Experts) first emerged in a paper in 1991. With the emergence and development of sparse-gated MoE, especially when combined with large-scale language models based on Transformers, the continuous expansion of language model capabilities like LLM can be achieved without significantly increasing computational requirements. This year, research related to MoE has shown strong growth, with the successive release of large models. Additionally, there has been a trend of creating large models that combine MoE with other models. A typical MoE model consists of two parts: a gating network and a sparse expert layer. The gating layer determines which expert processes a token and is typically composed of linear layers, softmax layers, gating functions (such as TopK), and so on. The sparse expert layer replaces the FFN (Feed-Forward Network) layer in Transformers and is usually composed of multiple FFNs, each representing an independent expert.\nExpert Parallel Expert Parallel routes tokens to different experts within Transformer blocks for computation. Each token is routed to a set of different experts, significantly reducing the number of parameters that each token must interact with by skipping certain experts. After communication via GPU-to-GPU AllToAll connections, the experts process the tokens, which must then be sent back to their original GPUs. In expert parallelism, different experts can be distributed across different compute nodes, enabling data to be processed in parallel, thus improving computational efficiency."}, {"title": "MONTA", "content": "MoE Memory consumption\nDistributed parallel training strategies are constrained by GPU memory storage, and different parallel methods have varying memory resource usage. We analyze the memory consumption for MoE structured models. For ease of reference, we summarize commonly used symbols in Table 2.\nThe storage of MoE model weights can be divided into two parts: storage for the Non-MoE module $\\psi_1$ and storage for the MoE module $\\psi_2$. Taking the Adam optimizer as an example, weights and gradients use fp16/bf16, while the optimizer's momentum is stored in fp32, along with the fp32 master weight. A combination of data parallelism, tensor parallelism, expert parallelism, and pipeline parallelism is utilized, as shown in Table 3.\nThe storage occupancy on a single card consists of model weight storage and activation storage:\n$\\text{Mem}_{\\text{total}} = \\psi_1 + \\psi_2 + \\text{Mem}_{\\text{act}},$ (1)\nwhere $\\psi_1$ and $\\psi_2$ are storage for the Non-MoE module and MoE module, respectively. $\\text{Mem}_{\\text{act}}$ is activation storage.Megatron-LM  estimated the memory usage of activations in the Transformer architecture. Without parallelism, the storage of activations in MoE structures can be calculated as:\n$\\text{Mem}_{\\text{act}} = \\frac{b s h}{l} (13 + 2 l k + \\frac{5 a s}{h}),$ (2)\nActivation recomputation can reduce storage pressure and is a commonly used memory optimization technique during the training of large models. As shown in Table 4, the memory usage of activation storage varies under different optimization techniques."}, {"title": "Pipelining", "content": "AllToAll and AllGather Pipelining In the existing training process of large MoE models, due to the large size of each expert model and the distribution of different experts across nodes, AllToAll communication usually utilizes inter-node bandwidth.Since inter-node bandwidth is smaller than intra-node bandwidth, communication becomes a significant portion of the overall overhead. Fully utilizing intra-node bandwidth has become one of the effective approaches to optimize AllToAll communication.\nTo fully utilize the separated hardware architecture resources for inter-node and intra-node communication, this paper proposes a method that parallelizes AllToAll and AllGather, implementing pipelining between inter-node AllToAll and intra-node communication. This approach achieves overlapping of AllToAll communication, avoiding communication resource waiting, and enhances model training efficiency. The input data for AllToAll communication can be divided into multiple independent chunks, with each chunk executing All-ToAll and AllGather operations separately. The AllGather of the current chunk overlaps with the AllToAll of the next chunk. Since AllToAll utilizes inter-node communication resources, typically connected by InfiniBand, while All-Gather utilizes intra-node communication resources, usually connected by high-speed NVLink, both are executed on different communication resources in parallel. However,different chunks' AllToAll or AllGather occupy the same resources, leading to their sequential execution. The AllGather of the second chunk needs to wait for the completion of the AllGather of the first chunk, even if the AllToAll of the second chunk has finished.\nCorresponding to Fig. 4 and Fig. 5, Fig. 7 illustrates the computation/communication flow of the MoE module under pipelining, where the communication data is divided into two chunks (Gather operator and subsequent AllGather are omitted in the figure). Step1(AllToAll):The first data chunk performs in inter-node pairwise AllToAll communication, where A1 in Node 1 communicates with A2 in Node 2, and B1 in Node 1 communicates with B2 in Node 2.\nStep2(AllGather pipeline with AllToAll): The first data chunk performs intra-node AllGather communication, where G1 in Node 1 communicates with G2, and G5 in Node 2 communicates with G6. At the same time, the second data chunk performs inter-node pairwise AllToAll communication."}, {"title": "AllGather and D2D copy Pipelining", "content": "We analyze the data flow of parallel execution for AllToAll and AllGather. For the AllToAll and AllGather in the Sequential mode shown in Fig. 6, the data arrangement remains the same as in the original unoptimized AllToAll 1. In contrast, for the Parallel mode in Fig. 72, the input data is split into chunk1 and chunk2. After pipelining, the final results differ, with data from different chunks arranged in an interleaved manner. To ensure complete data equivalence, Device-to-Device copies are necessary. Once chunk1/chunk2 complete their AllGather operations, they are copied to the corresponding positions based on index offsets, resulting in the final output that is entirely equivalent to the original data. The D2D copy operation performed after the AllGather for each chunk can be further optimized to achieve pipelining with AllGather, as shown in Fig. 9. The D2D copy of the current chunk and the AllGather of the next chunk are executed in parallel, further overlapping the overall communication time."}, {"title": "Network-traffic-aware", "content": "The size of the communication data and the distribution of data between nodes can influence the communication load and efficiency. In theory, the more chunks are split in Fig. 6, the higher the parallelism of AllToAll and AllGather, and the longer the communication time that can be overlapped. However, the actual execution of communication operators has a fixed overhead , that is independent of the amount of communication data. Splitting the data into two chunks and executing them sequentially may take longer than completing the operation in a single run.\nThis paper proposes a network-traffic-aware parallel optimization method called MONTA, which selects the optimal parallel strategy based on the size of the communication volume and the network topology for intra-node and inter-node connections in the training cluster. FIG. 10 is a schematic diagram of the MONTA overview. MONTA Inputs consist of AllToAll traffic inputs and Cluster network topology inputs \u2460. Based on the input information, the optimal chunk size for AllToAll pipelining is searched and selected, and a performance model for various optimization strategies is established to determine the final strategy \u2461. MONTA outputs the optimal execution strategy while estimating overall latency, throughput, and other performance metrics such as MFU 3. The accuracy of the performance model is cross-validated through software frameworks, communication operator kernels, and hardware experiments 4.\nAllToAll traffic inputs: The total communication volume $I$ for a single AllToAll operation is given by $b*s*h*BPE$, which is related to the model parameters, training parameters, and context length. Here, $b$ represents the microbatch size, $s$ denotes the sequence length, $h$ stands for the hidden size, and $BPE$ signifies the number of bytes per data element.\n$I = b s h * B P E,$ (3)"}, {"title": "Algorithm 1: Find Optimal Chunk Size O2", "content": "Input: b, s, h, BPE,t, e,B1,r1,B2,r2,B3,r3,Iminimal\nOutput: N, T\n1: j = 0\n2: Tmin = +\u221e\n3: Ibsh * BPE\n4: for N = 1 to +\u221e do\n5: IAA = NIAG = NI\n6: if IAA < Iminimal or IAG < Iminimal then\n7: Break;\n8: end if\n9: lookup r\u2081 according to I/(N*t),lookup r2 according to I/N\n10: AAj = NI *t e\u22121e * B1\u2217r1\n11: AGj = NI * t\u22121t * B2\u2217r2\n12: D2Dj = NI \u2217 B3\u2217r3\n13: if AAj < AGj + D2Dj then\n14: T = AAj + (AGj + D2Dj) * N\n15: else\n16: T = AAj * N + AGj + D2Dj\n17: end if\n18: if T < Tmin then\n19: T = Tmin\n20: end if\n21: end for\n22: Return N,T\nHere, B1 represents the theoretical bandwidth for All-ToAll, r1 corresponds to the AllToAll communication efficiency for I/(N * t), which is obtained by referencing the curve that relates communication volume to inter-node communication efficiency based on the value of I/(N *t). B2 stands for the theoretical bandwidth for AllGather, and r2 denotes the AllGather communication efficiency for I/N, derived from the curve relating communication volume to intra-node communication efficiency based on the value of I/N. The curves relating communication volume to inter-node and intra-node communication efficiency can be pretested and plotted. B3 is the theoretical memory bandwidth, and r3 is the memory bandwidth utilization rate corresponding to I/N. r1,r2,r3 are variables that change with the variation of data volume.\nAlgorithm 1 is designed to select the optimal chunk size. The input parameters b, s, h, BPE are related to the All-ToAll communication volume, and are associated with formula (3).Input parameters t and e are distributed parallelism parameters related to the selection of parallel strategies. The input parameters B1, r1, B2, r2 are communication-related parameters that can be obtained through pre-testing. The input parameter Iminimal represents the minimum communication volume, determining the maximum chunk split N. When the size of a single chunk is less than Iminimal, communication is primarily composed of fixed communication overhead."}, {"title": "Algorithm 2: Find Optimal Chunk Size O3", "content": "Input: b, s, h, BPE,t,e,B1,r1,B2,r2,B3,r3,Iminimal\nOutput: N, T\n1: j = 0\n2: Tmin = +\u221e\n3: Ibsh * BPE\n4: for N = 1 to +\u221e do\n5: IAA = NIAG = NI\n6: if IAA < Iminimal or IAG < Iminimal then\n7: Break;\n8: end if\n9: lookup r\u2081 according to I/(N*t),lookup r2 according to I/N\n10: AAj = NI *t e\u22121e * B1\u2217r1\n11: AGj = NI * t\u22121t * B2\u2217r2\n12: D2Dj = NI \u2217 B3\u2217r3\n13: if AAj < AGj then\n14: T = AAj + (AGj) * N + D2Dj\n15: else\n16: T = AAj * N + AGj + D2Dj\n17: end if\n18: if T < Tmin then\n19: T = Tmin\n20: end if\n21: end for\n22: Return N,T\nmance model is established to achieve network-traffic-aware parallel optimization.\nUsing the O1 strategy, the cost model is represented as in Equation (11), where r\u2081 is the AllToAll communication efficiency corresponding to I/t and r2 is the AllGather communication efficiency corresponding to I.\n$T_{O1} = \\frac{I}{t} \\frac{e-1}{e} * \\frac{1}{B_1 * r_1} + I * \\frac{t-1}{t} * \\frac{1}{B_2 * r_2}$ (11)\nUsing the O2 strategy, the cost model is obtained by the Optimal Chunk Selection module, which searches for the optimal split Nand To2.\nUsing the 03 strategy, by adding pipelining for AllGather and D2D, the cost model also utilizes the Optimal Chunk Selection module to search for the optimal split N and To3 (Algorithm 2). Since both AllGather and D2D copy use load/-store instructions and are influenced by kernel scheduling, the execution efficiencies r2 and 13 differ from those of the O2 strategy."}, {"title": "Algorithm 3: Strategy Select", "content": "Input:T01, T02,TO3\nOutput: S\n1: Get cost To\u20811,TO2,TO3\n2: Tmin=Min(T01, T02,TO3)\n3: Get Tmin Strategy\n4: Return S\nbreak the latency, throughput, and Model FLOPs Utilization (MFU) for the training process using this strategy S.\nImplementations: Through the framework of optimization strategy implementation and operator kernel implementation, we carry out experiments on hardware clusters, cross-validation and calibrate the performance model. The communication kernel addresses communication conflicts. The implementation also includes cluster expansion strategies for long context MoE training."}, {"title": "Communication Conflict", "content": "This section analyzes the communication conflict handling methods in the communication kernel of MONTA implementations. FIG. 11 illustrates the forward and backward computation and communication timing diagram of the MoE model training process. TP/SP utilize intra-node bandwidth for communication, while EP/PP/CP/DP utilize inter-node bandwidth, which includes southbound scale-out and northbound NIC, among others. EP communication occurs within expert parallel groups, which can be optimized as a combination of inter-node AllToAll communication and intra-node AllGather communication. PP communication involves sending and receiving activations at each pipeline corresponding to the cards. CP communication occurs within context parallel groups, where Attention calculations use RingAttention or AllGather operations to pass KV-Cache, reducing the memory requirement on a single card, typically used for long context training. DP communication involves performing AllReduce on gradients between data parallel group nodes after each batch processing to ensure consistency of model parameters across all nodes.\nIn FIG. 11(a), the vertical axis represents computation, TP/SP, EP, PP, CP, DP, while the horizontal axis represents the timing of execution for the Attention module and MoE module, with each row corresponding to its respective computational/communication operation. If PP uses synchronous communication, during forward computation, there will be no communication conflicts among EP/PP/CP/DP. However, if PP uses asynchronous communication, PP communication during backward may conflict with the forward CP communication's AllGather operation.\nIn FIG. 11(b), when DP uses asynchronous communication, there may be three potential conflicts: (1) Conflict between MoE All2All EP communication and synchronization of W1/W3 gradients; (2) Conflict between CP communication and Postlinear gradient synchronization; (3) Conflict between PP communication and QKV gradient synchronization. If the expert parallelism involves northbound NIC com-"}, {"title": "Cluster Expansion", "content": "This section analyzes the cluster expansion handling methods for long context MoE training in MoNTA implementations. In the training of the MoE model, it typically involves multiple training steps with Context lengths ranging from 4K to 128K, and even up to 1M tokens. This paper proposes a distributed parallel training extension method for Long Context mixture of expert models. Based on cluster resource parameters, model parameters, and Context length, a strategy for expanding expert model distributed parallelism is generated by combining Expert Parallelism and Context Parallelism.\nIn FIG. 12, each node in the current model training cluster consists of 8 GPUs, arranged in the order of [TP/SP, EP, CP, PP, DP]. The tensor parallelism is set to 8, where each of the 8 training cards operates in tensor parallelism. Each training card stores 1/8 of the weights of the Non-MoE modules, as well as 1/8 of the weights of the MoE module. The expert parallelism is set to 8, where the weights of each expert network are split into 8 parts distributed within a node, allowing 8 nodes to store the weights of 8 expert networks. The 8 cards corresponding to the TP positions of 8 nodes form EP Groups. Both expert parallel and context parallel communications utilize internode communication resources, typically through northbound NIC or southbound scale \u2013 out. In vertical expansion, the tensor parallel groups and expert parallel groups follow the same strategy as horizontal expansion, with CP parallel groups being the point of difference. In the vertical expansion strategy, the CP_Group and EP-Group are aligned in the same direction. In vertical CP expansion, the minibatch size of EP Group is 1. In horizontal CP expansion, the network topology between EP and CP is orthogonal, requiring a larger switch network to satisfy the communication needs for both EP and CP, with a scale of $\\text{cp_world_size * ep_world_size}$. In vertical CP expansion, CP and EP are aligned in the same dimension, with the network scale being max(ep_world_size,cp_world_size).\nWhen the input sequence length is less than what an EP-Group can handle, i.e. cp_world_size $<$ ep_world_size, and the cluster network switch meets the requirement of $\\text{ep_world_size * cp_world_size}$, horizontal scaling can be employed to enhance single node utilization. When the input sequence is greater than or equal to the length that an EP_Group can handle, i.e., when cp_world_size >= ep_world_size, use vertical expansion. This allows for flexible cluster configuration based on the minimum expansion granularity to meet the GlobalBatch training requirement."}, {"title": "Evaluation", "content": "In this section, we use a 16-GPU A800 cluster with IB 200Gb/s connections between nodes. First, we tested the IB utilization during AllToAll communication with 16 cards under different communication volumes. Then, we establish performance models under different optimization strategies and compare the performance of the AllToAll under these strategies against the DeepSpeed baseline. After that, we evaluate the loss convergence of a 2x70B model under dif-"}, {"title": "AllToAll performance", "content": "Communication Efficiency: We test the communication efficiency of 8-card intra-node AllGather, as shown in FIG. 13(a). The communication efficiency of the 16-card AllToAll with pxn enabled under varying communication volumes is presented in FIG. 13(b). For the 2-node 16-card setup, NVLink + IB network cards are used, with the bottleneck being the IB network card. The equivalent bandwidth of the IB network card is calculated using the formula (S * (N \u2212 8))/(N * t), where S is the original communication volume bsh, N is the number of cards, and t is the communication time. It can be observed that when the data volume is very small, latency dominates, resulting in nearly 0 bandwidth utilization.\nWe establish a performance model under different optimization strategies based on communication efficiency. Based on this, in FIG. 14, we test the performance of the All-ToAll single operator under different communication volumes. It can be observed that as the volume of communication increases, the performance improves more significantly with 01/02/03 optimizations. When the communication volume decreases, the performance of O\u2081 may surpass that of 02/03. In FIG. 14, all sequences are split into 4 chunks for both O2 and 03. When the sequence is 256K, MONTA selects 03. When the sequence is between 32K and 128K, MONTA chooses O2. When the sequence is less than 16K, MONTA selects 01."}, {"title": "Implementation", "content": "Establish multiple streams to run different kernels, as shown in FIG. 15. Stream 1 runs the AllToAll inter-node communication kernel, Stream 2 runs the AllGather intra-node communication kernel, Stream 3 runs the D2D copy kernel, and Stream 4 runs the exert computation kernel.\nAssuming the input sequence is divided into two data chunks, within Stream 1, the first two AllToAll operations represent the communication data chunks processed before the expert computation, while the last two AllToAll operations represent the communication data chunks processed after the expert computation. Each stream executes in parallel, while execution within a stream is serial. The arrows between different streams in FIG. ?? represent event dependency. For example, the AllGather process of the communication data chunks in Stream 2 depends on the AllToAll process before the expert computation in Stream 1. The expert computation process depends on the completion of the D2D copy processes for all data chunks. The last two AllToAll processes in Stream 1 depend on the results of the expert computation."}, {"title": "End-to-End time on Model", "content": "In large model training, the initial context length is typically set to 4K or 8K, as in Llama3 which"}, {"title": "Dissussions", "content": "According to equations (8) and (9), we obtain the following latency for the parallel execution of AllToAll and AllGather:\n$\\frac{A A_j}{A G_j} = \\frac{(e - 1) * B_2 * r_2}{e* (t - 1) * B_1 * r_1}$ (12)\nWhere the communication bandwidth and efficiency for All-ToAll are B\u2081 and r1, respectively, and the communication bandwidth and efficiency for AllGather are B2 and r2. For the 2x70B model with 2 nodes and 16 cards, B2/B1 = 200/25 = 8,e = 2,t = 8. From FIG. 13, we can obtain r\u2081 = 0.741,r2 = 0.803, AAj/AGj \u2248 2/3. According to the performance model, the theoretical upper limit of the speedup ratio is:\n$\\frac{T_{O_3}}{T_{base}} = \\frac{7}{32} = 0.21$ (13)\nFrom FIG. 14, it can be seen that when the input sequence length is 256K, the latency of O3 compared to the Baseline is 0.253, approaching the theoretical limit, and the actual results are consistent with the theoretical analysis. The communication volume is related to microbatch size, sequence length, and hidden dimension; as the communication volume continues to increase, the optimization performance can be further enhanced. When the input sequence length is 8K, we present a comparative analysis of the communication volume before and after optimization in Table 8:\nIn the case of an 8K input sequence, r\u2081 = 0.632, r2 = 0.776, under the O\u2081 strategy:\n$\\frac{T_{O_1}}{T_{base}} = 0.31$ (14)\nFIG. 14 shows that O1/Baseline = 0.30, which is consistent with the theoretical estimate. After splitting into 4 chunks for pipelining, the communication volume is reduced, and the AllToAll communication efficiency is around 40%,r1 = 0.427, r2 = 0.726, AAj/AGj = 0.979.The theoretical upper limit of speedup can be obtaine:\n$\\frac{T_{O_2}}{T_{base}} = 0.176$ (15)\n$\\frac{T_{O_3}}{T_{base}} = 0.164$ (16)\nThe theoretical time for AllToAll and AllGather for a single chunk is similar, allowing for better overlapping, as shown in (14) and (15). However, from FIG. 14, it can be seen that when the input sequence length is 8K, the performance of 02/03 is worse than that of O1. This is because the AllGather communication kernel requires memory operations from NVLink to L3, leading to memory conflicts with the D2D copy kernel. At small data volumes, in addition to communication time, fixed latency dominates, lowering actual performance. Furthermore, inconsistencies in progress between processes result in misaligned dispatch times across different node hosts, affecting the execution time of the first chunk. When the data volume is large, the AllGather communication time is long, but the memory copy efficiency is"}, {"title": "Related Work", "content": "Mixture-of-Experts (MoE):\nMoE has gradually gained popularity as an effective way to enhance the performance of large language models (LLMs), with its structure varying. Gshard  was the first to introduce MoE into the Transformer model, demonstrating the significant potential of the MoE architecture in scaling model capacity. The Mixtral-8x7B model provides an approach to alleviate the issue of load imbalance, utilizing the dropless MoE algorithm proposed by Megablocks . Megablocks addresses the problem of variable-length inputs with multiple experts using Grouped GEMM. DeepSeek MoE offers a more fine-grained expert partitioning, allowing different experts to learn more specialized knowledge. Additionally, it introduces shared experts that activate for all input tokens.\nOptimization of all-to-all:\nThe PKU-DAIR laboratory proposed a high-performance distributed MoE training system, HetuMoE , which supports various gate operator optimizations, such as Top1 and K Top1. It developed a hierarchical communication operator, Hierarchical AllToAll, for single NIC network nodes. Tutel implements adaptive parallelism switching, dynamically adjusting the combination of data parallelism (DP), model parallelism (MP), and expert parallelism (EP) based on the characteristics of the input data. It segments input tokens to form a pipeline, allowing expert computation and All-to-All communication to be pipelined. Flux proposed overlapping tensor parallel computation and communication through kernel fusion methods. FastMoE employs a dynamic routing algorithm that selects experts for computation based on their load and the characteristics of the input data. PipeMoE introduced dispatch, expert computation, and combine pipelining, showing improved performance compared to FasterMoE and Tutel. MPMOE proposed profile-based algorithm optimizations for pipelining and memory reuse. DeepSpeed-TED integrates Zero data parallelism, Megatron-LM's tensor parallelism, and DeepSpeed-MOE's expert parallelism, now incorporated into DeepSpeed. It reduces All-to-All communication volume through Duplicate Token Dropping. However, optimizations for the MoE module under tensor parallelism have not been merged into the official DeepSpeed, and we submitted patch1. Additionally, leveraging cluster communication across different hardware resources, we achieved pipelining for AllToAll and All-Gather, realizing a network-traffic-aware parallel optimization scheme."}, {"title": "Conclusion and Future Works", "content": "This paper proposes a network-traffic-aware parallel optimization method called MoNTA, which utilizes inter-node and intra-node communication resources to achieve pipelining for inter-node AllToAll and intra-node communication. It establishes a performance model for communication volume, communication efficiency, and parallel schemes, effectively overlapping MoE AllToAll communication. Additionally, it analyzes communication conflict issues during the training process of MoE models and presents a communication priority scheme. Finally, it proposes a distributed parallel training extension method for the long context MoE models. Experimental results show that MONTA can achieve a performance improvement of approximately 8x in AllToAll communication under 8-card tensor parallelism compared to the baseline. When using 16 A800 cards to train a 2x70B model with an 8K sequence, the overall latency performance is improved by 13% compared to the baseline.\nThe next step will be analyzing the impact of kernel scheduling on MoE parallel optimization performance, refining the performance model for training and inference, and integrating it into the framework and operator implementations. Similar to Flux , achieving overlapping of AllToAll and expert computation through software kernel fusion is also a direction for future exploration."}]}