{"title": "It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers", "authors": ["Benjamin Clavi\u00e9", "Nathan Cooper", "Benjamin Warner"], "abstract": "While encoder-only models such as BERT and ModernBERT are ubiquitous in real-world NLP applications, their conventional reliance on task-specific classification heads can limit their applicability compared to decoder-based large language models (LLMs). In this work, we introduce ModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its masked language modeling (MLM) head for generative classification. Our approach employs an intentionally simple training loop and inference mechanism that requires no heavy pre-processing, heavily engineered prompting, or architectural modifications. ModernBERT-Large-Instruct exhibits strong zero-shot performance on both classification and knowledge-based tasks, outperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's MMLU performance with 60% less parameters. We also demonstrate that, when fine-tuned, the generative approach using the MLM head matches or even surpasses traditional classification-head methods across diverse NLU tasks. This capability emerges specifically in models trained on contemporary, diverse data mixes, with models trained on lower volume, less-diverse data yielding considerably weaker performance. Although preliminary, these results demonstrate the potential of using the original generative masked language modeling head over traditional task-specific heads for downstream tasks. Our work suggests that further exploration into this area is warranted, highlighting many avenues for future improvements.", "sections": [{"title": "1. Introduction", "content": "Since the release of BERT [1] in 2018, Transformer [2]-based encoder-only pretrained models have been omnipresent in Natural Language Processing, being used to perform a wide variety of tasks and set state-of-the-art results across many of them. In the following years, much work has focused on further pretrained language models with increasing emphasis on generative, decoder-only models trained on vast quantities of data, generally called Large Language Models (LLMs) following the release of ChatGPT and the impressive performance of instruction-tuned AI assistants in general [3, 4, 5]. Notably, LLMs have demonstrated their strong potential as zero-shot [6] and few-shot [7] learners, especially when instruction-tuned [8]. However, while LLMs are very powerful, a large number of real-world tasks continue to be performed by encoder-only models, such as BERT, ROBERTa [9], or DeBERTaV3 [10]. There are multiple reasons for the continued prevalence of encoder models. One important factor is their greatly reduced cost; they can process large volumes of data with computational requirements orders of magnitude lower than those of LLMs [11]. Moreover, although decoder-only models are often better logical reasoners-thanks to their autoregressive nature and techniques such as Chain-of-Thought prompting [12]-encoder-only models have consistently demonstrated strong performance on more traditional tasks, such as text classification, where fine-tuned encoder-only models can match the performance of LLMs that have 100\u00d7 more parameters [13]. When using encoder-only models, classification tasks are traditionally performed with task-specific classification heads, where the model's representations are pooled into a single vector that is then used to assign an appropriate label. However, this approach has traditionally lagged behind in zero-shot classification, as it requires a substantial fine-tuning step to yield good results. As a result, zero-shot classification methods have typically relied on leveraging the classification head by reframing tasks in an appropriate way. The dominant approach has been to repurpose textual entailment: after extensive pretraining on entailment tasks such as MNLI [14]-where the model predicts whether a sentence is entailed or contradicted by another the final model is used to determine whether a label is entailed by a given sentence [15]. Further studies have shown that framing a very large number of tasks as entailment tasks can produce an even stronger zero-shot classifier [16]. Another promising approach treats zero-shot classification as a form of next-sentence prediction [17]. Researchers have explored using the Masked Language Modeling (MLM) head-the generative component of encoder-only models for classification. Early studies demonstrated strong potential in few-shot learning by turning tasks into Cloze-style questions [18], where filling in a single token"}, {"title": "1.1. Contributions", "content": "Many existing approaches to generative classification using encoder-only masked language models rely on substantial overhead. These methods demand either extensive prompt design, require converting the model to operate in an autoregressive manner (thereby reducing the computational efficiency benefits of single-forward pass models), or necessitate adjustments to the attention mechanism. In this paper, we showcase the potential of a simple generative approach to encoder-only models that requires little-to-no prompt engineering and no significant modifications to the model. We do so by introducing ModernBERT-Large-Instruct, an encoder-only model fine-tuned to perform classification tasks through its MLM head without further modification. We show that ModernBERT-Large-Instruct, although trained on the older FLAN instruction dataset, is remarkably strong for its size. Our approach is built around an extremely simple training loop to highlight its potential without heavy engineering requirements. Zero-Shot. We show that ModernBERT-Large-Instruct is remarkably strong in multiple application settings. On classification tasks, it is competitive with previous approaches-outperforming all of them in true zero-shot settings on two out of three tasks. On zero-shot MMLU and MMLU-Pro, which evaluate the model's knowledge and reasoning ability, it is the best overall model of its size class on MMLU, outperforming even similarly sized LLMs such as SmolLM2 [30] and performing closer to the much larger Llama3.2-1B [4]. On MMLU-Pro, it is the second-best model of its size class, losing only to another encoder model that employs an attention mask specifically designed to improve multiple-choice question performance. Fine-Tuning. We also show that once finetuned, ModernBERT-Large-Instruct either outperforms or matches the performance of fully finetuned classification-head methods across a large number of common classification benchmarks spanning multiple domains\u2014including news subject detection, textual entailment, forum post topic identification, and emotion detection-with even greater gains observed on more fine-grained tasks. Base Model Impact. Finally, we demonstrate that these results are unique to ModernBERT [29]. Models based on older architectures, such as ROBERTa-Large [9], or those trained on less varied data mixes, such as GTE-en-MLM-Large [31], show"}, {"title": "2. Methods", "content": "We theorize that the ability to function as a generative multitask learner might be an emergent capability more modern, larger-scale data mixes, which have been shown to greatly improve zero-shot performance in decoder-only models [32, 26]. Another potential factor is the use of a more modern model architecture, more closely resembling the Transformer++ [24, 33] than the original Transformer [2]. Moreover, a model with a context length exceeding 512 tokens the norm for most existing masked language models [1, 9]\u2014allows for longer instructions and would therefore also be beneficial. We also believe that model size is a strong contributor in unlocking instruction-following, with Flan-T5 showing that substantial gains from multi-task instruction tuning are obtained at larger model sizes, with the smallest model variants showing much more modest improvements [34]. We choose to use the recently released ModernBERT [29], which satisfies all of the above requirements, as it is an 8k context-length model using a modernized architecture and trained on a large-scale, modern data mix. We further explore this decision in Section 5 by comparing ModernBERT to other encoders, which do not satisfy all three constraints. Similarly to previous studies, we focus our explorations on ModernBERT-Large, the 395 million parameter ModernBERT variant, as is common practice in the exploration of encoder-only model zero-shot capabilities [22, 17]."}, {"title": "2.1. Base Model", "content": "We propose a simple training objective to further pretrain a masked language model (MLM) to turn it into a capable zero-shot model. This training objective is a variant of the model's pretraining objective, utilizing the model's MLM head. Normal MLM In classical MLM training, used to pre-train backbone models such as BERT [1], ROBERTa [9], De-BERTa [35] or ModernBERT [29], a proportion of input tokens, between 15 and 40%, is randomly masked: that is, replaced by a [MASK] token. The model's training objective then becomes a denoising objective: in a single pass, it must predict what the masked tokens originally were. Masked Instruction Tuning The aim of this work is to functionally instruct-tune the Masked Language Modeling head of an MLM model to use its generative capabilities to perform a wide array of downstream tasks, in a way similar to sequence-to-sequence [36] models such as T5 [37] and its instruct-tuned variant, Flan-T5 [34]. An immediate limitation of this method is that, unlike causal language modeling, masked language modeling generates its outputs in a single forward"}, {"title": "2.2. Training Setting", "content": "pass-replacing all [MASK] tokens simultaneously. Consequently, we must format our data so that the model is expected to predict only a single token for a given task. Answer Token Prediction We propose a simple variant, inspired by large language models' instruction tuning pipeline: answer token prediction. This objective is a very simple tweak to the normal MLM objective. Rather than masking multiple tokens throughout the input text, we mask a single token, which is the verbalizer for a label or answer. This can be considered a restricted form of sequence-to-sequence learning[36], akin to the way generative models such as LLMs perform tasks. Effectively, this means that all tasks are reframed in the format of a Cloze question, where the input is formatted so answering the question requires generating a single verbalizer token. This templating process is further detailed in Section 2.5. Verbalizers In the encoder-prompting literature [38], a verbalizer refers to a single token that a model outputs to represent its entire answer. Such a token is considered semantically meaningful if it accurately describes the label (for example, the token \"Positive\" in the context of emotion classification). However, in many cases, it is not possible to express the correct model output with a single meaningful token. In these cases, semantically empty verbalizers, such as \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201cD\u201d, etc., are used to identify potential answers."}, {"title": "2.3. Training Data", "content": "The Single Token Constraint Since MLM models are not pretrained in a generative fashion but rather to fill all [MASK] tokens, adding multiple [MASK] tokens could bias the model to consistently predict the longest possible answer. Rather than train the model to ignore extra masked tokens, a setting we leave for future work, we elected to reformulate the instruction tuning data and downstream tasks to require a single [MASK] token response. Modern Instruction Sets This severely constrains our data selection process, as many of the newer instruction-tuning sets, such as T\u00fcluV3 [3], are explicitly constructed to train helpful instruct-tuned assistants. The expected model outputs are noticeably longer than a single token. As a result, many of the most popular instruction-tuning datasets are not suitable for this exploratory work, as they would require considerable preprocessing to be usable. FLAN Older instruction-tuning datasets which predate the spread of assistant-style models ushered in by ChatGPT, such as the FLAN-2022 [8, 39] collection, contain a large proportion of examples where the expected model answer is a single token. FLAN is a large-scale instruction-tuning dataset which contains hundreds of datasets further divided into thousands of tasks, with each task's associated dataset preprocessed to match a specific template. This multi-task fine-tuning data has previously been shown to allow sequence-to-sequence models to reach improved downstream performance across all tasks, including previously unseen tasks [34]. Filtering The FLAN collection, in total, contains 396 million examples, of which 120 million are single-token answers. However, a large proportion of those examples come from just"}, {"title": "2.4. The Elephant in The [MASK]: Dummy Examples Improve Performance", "content": "Objective Mix One of the worries with this approach is the potential for catastrophic forgetting due to the relative over-representation of verbalizers with no strong semantic meaning. To attempt to mitigate this effect, we experimented with introducing an Answer Token Prediction vs MLM ratio. Instead of applying answer token masking to all samples, we replace Answer Token Prediction on a subset of samples with standard MLM masking across all tokens. This results in each sample having either a single mask token for the answer, or thirty percent of all tokens randomly masked as during pretraining [29]. Based on ablation runs, we choose a Answer Token Prediction vs MLM ratio of 20%\u00b9, meaning that 80% of our examples used a single masked token objective, as described above, and the remaining examples used the typical random masking MLM pre-training objective. Dummy Examples However, we introduced a labeling bug where all the true [MASK] tokens were incorrectly labeled as [MASK], rather than as their actual token. This meant that 20% of the predicted tokens were of little value, as the model rapidly learned that in cases where multiple [MASK] tokens are present, their label is always the same, and the loss for these examples plummeted and stayed low throughout training. While"}, {"title": "2.5. Templating", "content": "In order to leverage the Masked Language Modeling head for various tasks, we employ light templating. This templating process is different for training and inference. At training time,"}, {"title": "2.5.1. Training", "content": "During instruction training we add little additional templating to FLAN. Instead, we choose to reuse the extensive templating work already present in the FLAN dataset, based on the assumption that a diverse way of presenting similar tasks will lead to stronger generalization in the final model. Accordingly, we apply a single templating step to all training examples: we prepend the [MASK] token (which represents the expected answer) with a previously untrained token that serves as an anchor 2, indicating that the following token is the model's final prediction. This is inspired by theof prefix tokens in the neural information retrieval literature to help the model distinguish between queries and corpus documents [43, 44, 45]."}, {"title": "2.5.2. Inference", "content": "During inference, we apply a simple formatting template, with minor tweaks depending on the task. The inference template follows a simple sequence, where (1) basic instructions are given to the model, followed by (2) the text to be labeled or responded to, (3) the list of potential labels, and finally, (4) an answer section that ends in a [MASK] token for the model to predict."}, {"title": "3. Zero-Shot Performance", "content": "Commonsense Reasoning and Knowledge We use two standard LLM benchmarks to evaluate the knowledge and commonsense reasoning capabilities of ModernBERT-Large-Instruct: MMLU [40] and MMLU-Pro [46] In MMLU, models are asked general knowledge questions across 57 subjects and must select the correct answer out of 4 possible options. To further evaluate the models' robustness on this task, we use MMLU-Pro, a smaller evaluation set designed to be more challenging. In MMLU-Pro, all questions were filtered to remove trivial knowledge tasks and favor reasoning-heavy questions. Additionally, rather than 4 options, each question has 10, lowering the impact of random guesses and assessing the ability to distinguish among multiple similar answers. Zero-Shot Classification Evaluating zero-shot classifiers is surprisingly hard, as a large proportion of common benchmarks have been in use for many years, and data contamination during the post-training phase is considerably harder to avoid. The Real-world Annotated Few-shot Tasks (RAFT) benchmark [47] contains multiple tasks which are part of FLAN and other commonly used datasets. Its full evaluation process relies on a held-out evaluation mechanism via an online platform which is currently offline. As part of our evaluation, we choose three of the RAFT tasks. Two of them are publicly available and commonly used outside the RAFT suite itself, while the final one is the NeurIPS Impact Statement (NIS) task, where the model must determine if an impact statement involves AI safety risks or not. For this task, we use the annotated training set as our evaluation set\u00b3. The publicly available datasets are ADEv2 [48], a medical domain task where the model is given a sentence and must determine whether it is related to a negative reaction to a medication or not; and One Stop English [49]"}, {"title": "3.1. Setting", "content": "overview While ModernBERT-Large-Instruct reaches the best zero-shot performance across the evaluated tasks on average, in both classification and knowledge-based QA, it appears to have clear weaknesses, as discussed above. Noticeably, these results point to a clear pattern indicating that various encoder-based methods have different strengths, as evidenced by the large swings in performance across datasets. Interestingly, UniMC [22], another MLM-head based classification method, although relying on more complex mechanisms, strongly outperforms ModernBERT-Large-Instruct on two datasets, being the only model to do so, while being noticeably behind on three others. These results further reinforce the strong potential of using \"non-traditional\" classification heads for zero-shot tasks with encoder models, and warrant further research into the best ways to combine these methods."}, {"title": "4. Full-Finetune: Can One Head Do It All?", "content": "Having observed the strong zero-shot performance of ModernBERT-Large-Instruct, we now seek to explore whether this behavior holds true in fully fine-tuned settings. The common approach to encoder-model classification is to fine-tune a task-specific head, relying on pooling and a final layer projecting the pooled representation to a hidden dimension matching the number of potential labels. To do so, we further finetune the ModernBERT-Large-Instruct model resulting from our experiments in section 2 on a variety of downstream tasks. We use a simple fine-tuning mechanisms where all examples are trained on the single token prediction objective described in Section 2.27."}, {"title": "4.1. Setting", "content": "For this section, we evaluate the models on an array of widely used classification and Natural Language Understanding (NLU) tasks. We select 6 widely used [17, 22, 13] classification benchmarks, comprising of 3 topic classification detection and 3 emotion classification tasks. The three topic classification tasks are AGNews, where a news tagline must be classified into one of 4 categories, Yahoo! Answers Topic [51], a large-scale dataset where messages from the Yahoo Answer forum must be classified into one of ten topics, and 20newsgroup, where posts from online newsgroups must be classified into one of 20 forum categories. The emotion detection tasks are IMDB [52], SST-2 and SST-5 [53], where movie reviews or extracts from such reviews must be classified into either positive or negative, in the case of IMDB and SST-2, or a finer-grained 5-level classification ranging from Very Negative to Very Positive for SST-5. We also evaluate the model on MNLI [14], an extremely common benchmark of NLU capabilities [54], where the model is expected to predict the textual entailment between two sentences: that is, whether a hypothesis is entailed by, neutral toward, or contradicted by a given premise."}, {"title": "4.2. Result", "content": "The results are presented in Table 3. Overall, it appears that with full-dataset fine-tuning, ModernBERT-Large-Instruct outperforms a fully fine-tuned ModernBERT-Large using a classification head on average. On a task-by-task basis, both approaches trade blows, with both methods coming slightly ahead on certain datasets. Given our extremely simple training loop and limited optimization, we believe that this highlights the robust performance and strong potential of MLM-head based downstream tasks when compared to classification-head based approaches. While the traditional classification method performs better on simpler tasks, such as the SST-2 binary sentiment classification dataset, ModernBERT-Large-Instruct achieves noticeably better results on its more fine-grained version, SST-5, where the emotion expressed in a sentence is broken down into five levels rather than just two. ModernBERT-Large-Instruct's MNLI performance also highlights that this method is not restricted to single-sentence tasks, but is able to correctly label the relationship between two sentences."}, {"title": "5. Are Older Encoders Also Generative Classifiers?", "content": "A final question we seek to answer, raised in Section 2.1, is whether or not this behavior is specific to ModernBERT, and thus is emergent to the combination of a more modern architecture along with a sizeable mixed-sources 2 billion token pretraining, or was already present in previous encoder models."}, {"title": "5.1. Setting", "content": "To answer this question, we reproduce the training setting method presented in Section 2 on two other encoder-only models. The first model is ROBERTa-Large [9], one of the best performing and currently the most widely used MLM model 8. We also evaluate GTE-en-MLM-Large [31], which provides an interesting comparison: while it has been trained on a limited data mix, its architecture is modernized and follows the more modern Transformer++ architecture [24, 33]."}, {"title": "5.2. Results", "content": "We present the outcome of this experiment in Table 4. The results are stark: RoBERTa-Large, despite a similar parameter count to ModernBERT-Large and almost competitive performance in traditional, classification-head based approaches, achieves poor results in all zero-shot contexts when using the MLM head. GTE-en-MLM-Large [31], with its modern architecture and longer context length, fares better than RoBERTa-Large on all tasks except MMLU-Pro, where they are equal, but remains considerably weaker than ModernBERT-Large-Instruct. As such, we hypothesize that strong generalization potential from a Masked Language Model's MLM head is a property relying mainly on a large-scale, varied pretraining data mix, with architecture itself playing a smaller, but still important, overall role."}, {"title": "6. Conclusion", "content": "In this work, we explored the potential of encoder-only masked language models (MLMs) as competitive zero-shot and fine-tuned classifiers through their generative MLM heads. We achieve this by designing a very straightforward training recipe and introducing ModernBERT-Large-Instruct. This model is trained on a filtered subset of the FLAN dataset with a simple training objective. Interestingly, we observed that the use of an unconventional training setup, where 20% of the examples are replaced by dummy samples, vastly outperforms more traditional training approaches. We demonstrated that modern encoder models can achieve remarkably strong classification performance via the use of their masked language modeling head, in a variety of settings. In zero-shot contexts, ModernBERT-Large-Instruct is competitive with previous approaches requiring more complex training or architectural modifications, even outperforming them on some tasks. On zero-shot knowledge benchmarks such as MMLU, it performs closer to the considerably larger Llama-3.2-1B than the similarly sized SmolLM2-360M. When fine-tuned for a single specific tasks, this language-modeling-head classification approach reaches stronger overall performance than traditional classification-head fine-tuning on a wide range of downstream domains, encompassing entailment, topic classification, and emotion detection. These early"}, {"title": "7. Limitations & Future Work", "content": "Newer Instruction Sets Our work focuses on the exploration of FLAN. While FLAN remains a well-constructed dataset with strong previous empirical results, it is an earlier form of instruction tuning. In modern LLM instruction-tuning, FLAN has often been superseded by much broader instruction sets focusing on more diverse tasks and phrasings [3, 55, 56], although many continue to include FLAN as a subset [3, 55]. However, we choose to use FLAN as the more complex data pre-processing methods required to convert modern turn-based instruction datasets into single token data are out-of-scope for this study. As such, FLAN and its preprocessed format making adapting a large subset of it to a single token generation format very straightforward, represents an ideal experimentation dataset. Few-Shot and In-Context Learning We focus on exploring the potential of ModernBERT-FLAN and, more generally, of leveraging the MLM head for classification tasks in both zero-shot and full fine-tuning contexts, showcasing its superiority over previous methods. However, an area of particular interest is few-shot learning, either via in-context learning [57, 21] or sample-efficient fine-tuning [20]. We leave this to future work. QA-Specific Attention Masks In this work, we focus on a simple training loop to highlight the baseline capabilities"}]}