{"title": "A Novel Approach to Scalable and Automatic Topic-Controlled Question Generation in Education", "authors": ["ZIQING LI", "MUTLU CUKUROVA", "SAHAN BULATHWELA"], "abstract": "The development of Automatic Question Generation (QG) models has the potential to significantly improve educational practices by reducing the teacher workload associated with creating educational content. This paper introduces a novel approach to educational question generation that controls the topical focus of questions. The proposed Topic-Controlled Question Generation (T-CQG) method enhances the relevance and effectiveness of the generated content for educational purposes. Our approach uses fine-tuning on a pre-trained T5-small model, employing specially created datasets tailored to educational needs. The research further explores the impacts of pre-training strategies, quantisation, and data augmentation on the model's performance. We specifically address the challenge of generating semantically aligned questions with paragraph-level contexts, thereby improving the topic specificity of the generated questions. In addition, we introduce and explore novel evaluation methods to assess the topical relatedness of the generated questions. Our results, validated through rigorous offline and human-backed evaluations, demonstrate that the proposed models effectively generate high-quality, topic-focused questions. These models have the potential to reduce teacher workload and support personalised tutoring systems by serving as bespoke question generators. With its relatively small number of parameters, the proposals not only advance the capabilities of question generation models for handling specific educational topics but also offer a scalable solution that reduces infrastructure costs. This scalability makes them feasible for widespread use in education without reliance on proprietary large language models like ChatGPT.", "sections": [{"title": "1 Introduction", "content": "One of the most significant and pertinent challenges facing education systems today is the teachers' workload. It is argued to be the main reason behind issues associated with teachers' retention in the profession as well as the lack of interest among graduate students to go into teaching professions. On the other hand, according to the Global Report on Teachers published by the Teacher Task Force and UNESCO, 44 million additional teachers will be needed by 2030 to meet Sustainable Development Goal 4 (SDG4), which aims to achieve universal primary and secondary education for all [52], without any improvements to the status quo. Among these concerns, generative AI in education is seen as an opportunity to 'transform a teacher's day-to-day work' [29] by reducing their workload and improving educational outcomes through the automation of routine tasks.\nCreating lesson materials and generating topic-specific, relevant, and age-appropriate questions for teaching have long been identified as time-intensive tasks for teachers, and an area where increased consistency is also expected to improve educational outcomes for students [30]. Although learning analytics and AI in Education researchers have long explored ways to support teachers' question generation capabilities through data-driven insights and models, attempts on Topic-Controlled Question Generation (T-CQG) have been less successful, primarily due to the lack of quality in the generated content. The use of large language models (LLMs) in teacher-facing interfaces, however, has the potential to address these quality concerns by leveraging recent advancements in NLP for automatic educational question generation (EdQG). EdQG can help teachers reduce the labor-intensive task of generating questions to promote classroom discussions, design formative and summative assessments, create lesson hooks, or address student misconceptions which are all activities that teachers consider among the most time-consuming in their profession [29]. Although most issues related to teachers' workload are complex, ecosystem-level socio-technical challenges [15], T-CQG can serve as a small yet important practical step towards enhancing teachers' productivity, aiming to mitigate workload and address issues related to teacher retention and attraction to the profession.\nIn addition to their potential to support teachers, EdQG (and T-CQG) models can be integrated into learning management systems (LMSs) and intelligent tutoring systems (ITSs), to advance the system's capability to perform precise diagnostics on learner's knowledge gaps. The responses received from learners can inform the learning analytics pipeline more precisely and frequently to have a refined learner state representation, that can empower the system with targeted interventions. However, such interventions require advancements to generic neural network question generation models that do not have the ability to contextualise generation with constraints.\nThe novelties that we introduce through this work are three-fold. We 1) propose a novel method to generate a dataset with contrastive examples in order to effectively train a T-CQG model and 2) validate and propose novel ways of evaluating the topical relatedness of the generations to the controlled topic using semantic relatedness metrics while 3) this is the only work that attempts in using a very small language model (sLM) with \u2248 60M parameters, and succeeds in producing a T-CQG neural model."}, {"title": "2 Problem Definition, Background Research, and Research Questions", "content": "In this section, we introduce the formal problem definition and prior work, leading to the research questions."}, {"title": "2.1 Problem Definition", "content": "Although language models have been employed for question generation, their application in educational settings has only recently begun to be systematically explored with a heavy focus on the potential practical applications of proprietary models (e.g. GPT models' prompt engineering and RAG applications for question generation). While existing research in relevant academic communities with a more technical focus explores generating questions from descriptive texts [22, 55], the task remains highly complex and there is less focus on the educational value of the generated questions in evaluations. Context plays a crucial role in the educational value of EdQG, yet much existing work has focused"}, {"title": "2.2 Related Work", "content": "Question Generation (QG) involves automatically generating questions from a specific text passage or a document. The main goal of QG is to produce questions that are not only syntactically and semantically correct but also contextually relevant and meaningful for the intended use. There has been a growing use of computational models to generate contextually relevant and grammatically correct questions [54]. In educational contexts specifically, QG has been implemented in various systems including intelligent tutoring systems [56], writing support systems [46], and knowledge assessment platforms [36].\nExisting research categorises QG into two types: answer-aware and answer-agnostic [59]. In answer-aware QG, the target answer is predetermined, and questions are generated to correspond with this answer within the given text context. On the other hand, answer-agnostic QG does not provide the target answer to the language model, allowing for more open-ended question generation which are considered to be educationally more valuable. However, answer- agnostic QG is a more challenging task for NLP research. Early research in answer-agnostic QG relied heavily on rule-based techniques that required experienced educators to develop rules that could convert declarative sentences into interrogative forms [1, 33]. These methods, while effective, are labour-intensive and time-consuming, demanding significant manual effort in creating high-quality, handcrafted rules [11], which inherently limits their scalability and diversity in question generation. These limitations led more recent research investigations to focus on data-driven neural network (NN) approaches."}, {"title": "2.3 Research Questions", "content": "This paper aims to address these challenges associated with the topic-controlled EdGQ. We conducted supervised fine-tuning on a pre-trained T5-small model (hereafter referred to as the T5 model), an approach that is preferable and safer for educational entities to manage and control the language model (LM) with minimal infrastructure costs. The fine-tuning process utilised the novel MixSQUAD dataset, an enrichment of the SQUAD dataset [50], which is a commonly used general question generation dataset characterised by its shallow questions. Additionally, we designed experiments to explore the impacts of pre-training strategies, text data augmentation, and model quantisation on the model's performance. We evaluated the model on the novel MixKhanQ dataset, derived based on the KhanQ dataset [31], which features human-like, in-depth questions sourced from Khan Academy, an online education platform. This is designed to assess the model's effectiveness on academic materials, and its ability to generate educationally meaningful questions to explore its practical value for teaching and learning contexts. Based on these steps, the paper proposes a\nnovel set of models that can perform high-precision topic-controlled educational question generation (T-CQG). The research questions addressed through this work are as follows:\n\u2022 RQ1: What are the most representative metrics for automated measures of generated questions on their topic relevance considering human evaluations as the ground truth?\n\u2022 RQ2: Is it feasible to fine-tune a pre-trained language model (PLM) to perform T-CQG?\n\u2022 RQ3: Can further pre-training of the PLM on scientific text improve the quality of T-CQG?\n\u2022 RQ4: How does model quantisation affect the performance of the fine-tuned models while improving scalability?\n\u2022 RQ5: To what extent can data augmentation further improve the quality of T-CQG?"}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Datasets Utilised", "content": "We used the SQUAD 1.1, the Stanford Question Answering Dataset, comprising over 100,000 questions crafted by crowd workers based on a selection of 536 Wikipedia articles [50] as the source for creating new datasets (SQuAD+, MixSQUAD and MixSQUAD2X as described in section 3.2 below) for finetuning the models. When training the TopicQGedu Model (see section 3.3.3 below), we used PeS2O dataset [51], a collection of scientific abstracts, to perform the pre-training as prior work has shown this may increase the model's performance in educational settings [7].\nFor evaluation, we used the KhanQ dataset [31] as it presents a more relevant challenge for educational question generation. It includes 1,034 high-quality questions in the STEM fields generated by learners, which aim to probe deep understanding of subjects taught in Khan Academy's online courses. Despite its smaller size relative to SQUAD, KhanQ aligns more closely with our objective to generate topic-based and relevant educational questions (as per prior work [26]). To adapt the dataset for topic-based evaluation, we use the same approach as MixSQUAD (section 3.2.2) to create a dataset with contrasting topic-based questions. We refer to the transformed version of the KhanQ dataset as MixKhanQ dataset."}, {"title": "3.2 Creating Novel Datasets for T-CQG", "content": "A core contribution of this work is to introduce a novel data enrichment method that leads to the creation of new datasets that are derived from conventional question generation datasets. As described in 3.1, we derive the new datasets from SQUAD and KhanQ. These datasets already contain the context c and the label question qt from a human (contrast to \u011dt in equation 1 which denotes the generated question). We append an additional field to the dataset, Topic t, and create three novel datasets, 1) SQUAD+, 2) MixSQuAD, and 3) MixSQUAD2X for the T-CQG task.\n3.2.1 Linking the target topic to data points, SQuAD+ dataset. To identify semantic annotations for every context and question, we employ wikification [45], which annotates text inputs with relevant concepts from Wikipedia (Tc). We retain the top 5 concepts for each text (context and question) based on their PageRank scores, which reflect the authority of the concept over the annotated text. To make sure that we can link the topical alignment between the question and the context, we only retain examples where at least one common Wikipedia concept is present between the context and the question pair (i.e. |Tc \u2229 Tq| \u2265 1). We select the concept with the highest PageRank score in the question (most authoritative) as the target topic t. This method ensures that the most closely related annotation is selected as the topic\nfor each pair, and confirms that the topic is appropriately aligned with both the context and the question, thus avoiding situations where the topic may be relevant to one but not the other. As a result, both datasets have been enhanced to include paragraph-level contexts c, identified topics t, and corresponding questions qt, as shown in figure 1.\n3.2.2 MixSQUAD dataset. We also create an enhanced dataset to synthesise a contrastive learning setting while fine- tuning the PLM for T-CQG leading to the MixSQUAD dataset. When creating this dataset, we randomly pick pairs of observations from the SQuAD+ dataset described in section 3.2.1. For each pair of examples i and j containing (ci, ti, qt\u2081) and (cj, tj, qt\u2081) respectively, we create two new examples where they share a common context cicj where the two contexts are concatenated. The data representation of the MixSQUAD dataset is presented in figure 1. This approach aims to enhance the model's understanding of topics and the relationship between context, topic, and question by serving novel contrastive examples. An added benefit of the novel MixSQuAD dataset is that the context presented to the model during fine-tuning is guaranteed not to be previously encountered in the large corpora used for training foundational models. This method results in a diverse collection of 10,000 mixed data entries in the MixSQUAD dataset, fostering a robust learning environment for the models.\n3.2.3 MixSQuAD2X dataset. The MixSQUAD2X dataset is very similar to MixSQUAD dataset, but the main difference is the utilisation of data augmentation to expand the dataset. In contrast to MixSQUAD, we introduce two additional examples to the dataset with the context c2c1 by reversing the order when concatenating the two randomly chosen contexts. This leads to a dataset that is twice as big as the MixSQUAD dataset."}, {"title": "3.3 Developing T-CQG Models for Education", "content": "With the relevant datasets created, we built multiple models to be evaluated in a series of experiments to answer the research questions outlined in section 2. All the models used in experiments are created by finetuning the T5-Small [49] model, a small Language Model (sLM) that has also been used for educational question generation in the past [7, 26]. We fine-tuned the foundational model (t5-small from HuggingFace library\u00b2) using the Adam optimizer with a batch size of 64, the learning rate of 1e - 3, and epsilon of 1e - 8. We use a maximum sequence length of 512 for the encoder, and 128 for the decoder. We train all models for a maximum of 50 epochs with an early stopping based on the validation loss 3.\n3.3.1 Baseline Model to Answer RQ2. We conducted fine-tuning for T-CQG using the same finetuning approach used by [42] for controlling complexity in simplifying texts. We used the proposed SQuAD+ dataset (described in section 3.2.1) to finetune the T5 PLM.\n3.3.2 TopicQG to Answer RQ2. The key difference between the baseline model and the proposed TopicQG model lies in the data used for fine-tuning the T5-small model. We introduced the TopicQG model to contrastive examples using the novel dataset created, MixSQUAD (described in section 3.2.2). Such mixed contexts, which may feature sentences with vastly differing concepts, are designed to enhance the T5 model's understanding of the semantic relationships between context c, topic t, and question qt.\n3.3.3 TopicQGedu to Answer RQ3. Further refining the approach, we developed TopicQGedu, which incorporates an additional pre-training step. In this approach, the sLM model undergoes further training with scientific text documents before being fine-tuned. This step is intended to imbue the model with scientific terminology and concepts, crucial for crafting high-quality educational questions [7].\n3.3.4 Quantised TopicQG Models to Answer RQ4. Quantisation allows reducing the memory footprint of neural models significantly to enhance their scalability. To evaluate the degree of loss due to quantising the trained models, we created the quantised versions of the TopicQG model. We used 8-bit quantisation utilising the LLM.int8 algorithm [18] and 4-bit precision employing the QLoRa algorithm [19] to create TopicQG8bit and TopicQG4bit models respectively.\n3.3.5 TopicQG2X to Answer RQ5. This model is trained similarly to the Topic QG model, but it exploits data augmentation by being finetuned on the newly proposed MixSQuAD2X dataset (described in section 3.2.3). The MixSQUAD2X dataset effectively doubles its size by changing the order of concatenation of contexts, introducing new examples to finetune the model with. This strategy has the potential to enhance the model's robustness and generalisation abilities, improving the relevance and educational value of the generated questions to the given topics.\n3.3.6 Example Questions Generated with Models for the Experiments. Table 1 presents a random set of topic-controlled question generations based on the context text provided in five different subject areas (Computing, Economics, Chemistry, Art, and Biology)."}, {"title": "3.4 Human Annotation-based Evaluation of Semantic Relatedness Metrics", "content": "To assess how representative BERTScore and WikiSemRel are when measuring topical relatedness (RQ1). We created a small gold-standard dataset via human annotation. The annotators (n = 4) consisted of two female and two male postgraduate students in the 20-30 age bracket from a masters degree programme at a university in the UK.\nTo set up the annotation task, we randomly selected 30 questions from the MixKhanQ dataset (KhanQ dataset transformed using the method described in section 3.2.2). For each sample, we provided the participants with the reference question qt and two corresponding generated questions, 1) the question \u011dt generated using the relevant/prescribed topic and 2) the question \u011dt' generated with an alternative topic. Annotators were required to independently determine which of the two generated questions \u011dt or \u011dt is more closely aligned with the reference questionqt, the same tasks the SemRel metrics are going to do. The generated question \u011d(.) annotators selected as closely relevant to the reference question is given 1 and the other 0. We calculated the Mean Absolute Error (MAE) between the mean score assigned by human annotators and the respective SemRel Score as per equation 2."}, {"title": "3.5 Experimental Setup for Automated Performance Evaluations", "content": "Figure 2 illustrates the experimental setup designed to address RQs 2-5. A total of six models (including TopicQG's base, 8bit, and 4bit versions) have been developed as described in detail in section 3.3 and represented as coloured boxes in figure 2. Each model is evaluated using the MixKhanQ dataset."}, {"title": "3.6 Evaluation Metrics", "content": "When evaluating the final models, we focused on two main aspects. 1) The generated question qt is of high linguistic quality so it has the potential to be used in educational settings, 2) The generated question qt is semantically related to the prescribed topic t so that it can address the AI-generated questions' common problem of being \"too general to be useful in practice\" in educational settings.\n3.6.1 Evaluating the quality of generations. To assess the quality of the generated questions, the similarity between the reference question and the generated question is measured. We employed a suite of metrics, including BLEU [43],\nMETEOR[4], ROUGE [38], F1 score, and Perplexity [32], which have been used frequently in previous research [7].\nThese metrics provide a comprehensive evaluation of the fluency, relevance, and coherence of the generated questions, serving as scalable indicators of the automated evaluation of the generated questions' quality.\n3.6.2 Semantic relatedness between the questions generated and the topic. For measuring the semantic relatedness, SemRel (qt, qt), we needed metrics that can quantify the relatedness between the reference question qt and the generated question \u011dt. We used the BERTScore [60] and the Wikipedia-based Topic Semantic Relatedness (WikiSemRel) [27]\nmetrics for these evaluations.\nBERTScore. leverages BERT contextual embeddings of tokens to calculate the similarity between two text extracts,\nimproving upon the traditional exact match methods. Our early experiments showed that the BERTSCore tend to inflate\nthe similarity between qt and \u011dt, as there are words like \"what\" and \"why\" that overlap even if the generated question is\nnot about the salient topic t of the reference question. Therefore, we excluded stopwords in the reference and generated\nquestions prior to calculating the BERTScore. BERTSCore is a score in the range (0,1) where 0 indicates no relatedness.\nWikiSemRel. quantifies the semantic relatedness between the Wikipedia-based concepts extracted from the reference question qt and the generated question \u011dt. We employ the WAT API [45] service to calculate semantic relatedness using\nthe 1) w2v-based method, that builds embeddings for Wiki entities based on their co-occurrence in Wikipedia pages and\n2) Jaccard-based measure, that uses the outward links to other Wikipedia pages to calculate similarity [47]. We Wikify the generated question to compute the WikiSemRel score which is within range (0,1) where 0 indicates no relatedness."}, {"title": "4 Results", "content": "In this section, we present the results from the experiments described in section 3. The results of the human evaluations answering RQ1 is presented in table 2. The offline evaluations to validate RQ 2-5 following the methodology illustrated\nin figure 2 are summarised in tables 3 and 4. While table 3 presents metrics relating to the linguistic quality of the generation, Table 4 presents the semantic closeness between the prescribed topic and the generated questions. The perplexity calculation in table 3 is done using the TextDescriptives python library with the en_core_web_lg language model as the reference language distribution [32]."}, {"title": "4.1 Most Representative Automated Topic Relevance Metric to Human Evaluations (RQ1)", "content": "In human evaluations of the 30 randomly selected question pairs for topical alignment, only two pairs did not reach a consensus among the participants with one outlier in each case (with the Fleiss' kappa [28] measure of inter-rater agreement among multiple raters being 0.933). This indicates strong inter-rater reliability in the gold-standard human evaluator data. As per table 2, the WikiSimRel metrics are more aligned with the human judgements in comparison to the BERTScore. Among the three candidates, we can observe the embedding based (BERT and w2v) methods showing inferior representativeness. This could be due to the fact that embeddings can represent many different attributes about the entities and tokens they represent (e.g. whether the text is a question or a statement). This hypothesis is further reinforced by previous observations that including stopwords like \"what\", \"why\" leads to the inflation of BERTScore."}, {"title": "4.2 Topical Relevance and the Effect of Pre-training on Generated Questions (RQ 2 and RQ 3)", "content": "Table 3 provides us an indication of the degree to which the generated question \u011dt resembles the reference question qt. This is a proxy for topical relevance as the reference question is implicitly aligned with the controlled topic. The results indicate that the proposed TopicQG model outperforms the baseline model in all but perplexity metric. Outperforming in terms of BLEU scores at multiple levels (BLEU1 through BLEU4), indicates enhanced linguistic precision in question generation. It also achieves higher F1, ROUGE-L, and METEOR scores, reflecting the model's capability to generate questions that are not only relevant and accurate but also semantically aligned with reference texts. Compared with the baseline, a slight increase in perplexity suggests that the TopicQG model may generate questions that diverge from the reference language, potentially due to its ability to learn more complex educational expressions. The perplexity does not raise significant concerns over the quality of generations as the random examples in table 1 doesn't indicate visible signs of deterioration. It is noteworthy that the randomly selected examples in table 1 are not as good as typical questions generated using a very large language model such as ChatGPT. We hypothesise the size of our model being a main reason for the relatively low quality of generations. However, our own prior work has also shown that such generations can be improved to humanly acceptable levels by simply post-processing them through a pre-trained grammar correction model [25, 26] retaining the accessibility and sustainability benefits of sLMs.\nTable 4, the stronger indicator of topic alignment gives us evidence that the proposed TopicQG models significantly outperform the baseline. In terms of the semantic difference between the educational questions generated with the controlled topic vs. a different topic (using WikiSimRel (Jaccard), the most representative metric from table 2), all newly proposed models except the 4bit quantised TopicQG model outperforms the baseline. This can be expected as extreme quantisation can deteriorate the accuracy of the model.\nIn terms of the TopicQGedu model that is pre-trained on scientific text, the results are mixed and more difficult to interpret. While it surpasses the predictive performance on the Baseline in a few metrics, it performs below the TopicQG model across all metrics in table 3. While pre-training on scientific content is hypothesised to improve the topical relevance of the model, we do not observe improvements in this case. To rigorously assess whether the observed differences in performance metrics are statistically significant, we alsoconducted a paired t-test comparing the performance scores of TopicQGedu and TopicQG across the same set of questions. The results yielded a p-value of 0.083 (>0.05), suggesting that there is no statistically significant difference that TopicQGedu underperforms compared to TopicQG. Given that the T5 model is primarily trained on web-crawled data and Wikipedia articles [49], the absence of scientific texts in the training corpus could potentially weaken the model's performance in scientific concepts and language. Thus pre-training strategies may need to be further explored, especially in specialized domains where deeper domain knowledge might be crucial, even if immediate improvements in conventional metrics are not evident."}, {"title": "4.3 Impact of Model Quantisation (RQ4) and Data Augmentation (RQ5)", "content": "We investigated the effects of 8-bit and 4-bit quantisation on the TopicQG model (the best-performing model on the MixSQUAD dataset), referred to as TopicQG8bit and TopicQG4bit respectively. In comparison to the TopicQG model, the quantised models retain best performance with respect to metrics such as BLEU, F1-Score, MATEOR and ROUGE-L with very minor decreases (\u2264 0.01) according to table 3. As expected, a drop in performance in comparison to the TopicQG model (with no quantisation) is observed. Similarly in table 4), a small drop in metrics is observed although it is not a drastic difference. This can be attributed to the fact that the generations change to a very small degree with quantisation indicated by the small deviations in table 3.\nRegarding memory usage, the full-precision TopicQG model occupies a memory size of \u2248 230 MB. In contrast, the TopicQG8bit model significantly reduces this footprint to \u2248 110 MB (59%), and the TopicQG4bit model further reduces it to \u2248 94 MB (53%). The potential of quantisation demonstrated in this study is twofold: 1) it significantly lowers the hardware requirements for running the models, and 2) it maintains a satisfactory level of performance, making it feasible to deploy educational topic-controllable question generation on platforms where computational resources are limited. This accessibility could dramatically widen the applications of such models, making them more ubiquitous in educational and other real-time interactive applications on mobile devices. The reduction in model size not only implies lower memory requirements but also suggests lower power consumption, leading to cheaper infrastructure costs and a lower carbon footprint. Such properties are crucial for deploying these models in educational contexts of resource-constrained environments such as middle and low-income countries, mobile devices and embedded systems.\nThe comparisons between TopicQG and TopicQG2X models in table 4 show that the data augmentation has an obvious effect on improving the models performance on topical relevance. The greater diversity of examples where the same example is presented to the model in two different ways helps the model better understand to follow the topical theme prescribed in the instruction with the context. It surpasses all other models, including TopicQG, demonstrating superior alignment of the generated questions with the input context and topic. This highlights the effectiveness of data augmentation in enhancing the model's capacity to generate questions with topic relevance and better contextual consideration of texts."}, {"title": "5 Discussion", "content": "This paper tackled the challenge of topic-based educational question generation with a high degree of specificity. Due to the novelty of the task itself, we evolved our method over multiple steps to propose a method that can lead to high-quality T-CQG while validating novel approaches to evaluate the topical relevance of such generations. The results show that the novel method proposed and evaluated here is capable of generating topical educational questions while retaining coherent grammatical structure. Further experiments also showed how data augmentation increases the model's performance in topical relevance leading to improved results. The final experiments exploring quantisation indicate that the model's memory footprint can be halved with minimal loss of generative performance. Supported by human evaluation, the findings provide solid evidence that the questions generated by the proposed model are of high quality and meaningfully related to the educational content and topics, thereby affirming the effectiveness of our topic-controllable educational question generator.\nSimilar to trends in educational research in general [17], the interest in the use of Generative Artificial Intelligence (GenAI) in LA research community has significantly increased in recent years. Regarding content generation, LLMs are used in tackling challenges such as grammar/ code correction [13, 20], question generation [23, 26], explanations and hints provision [37, 44], in STEM subjects such as mathematics [2, 12] and science [7, 23, 41] to non-STEM domains like law [14] and language learning [10]. Nevertheless, the majority of the community resorts to in-context learning [21] within enormous LLMs such as ChatGPT [17]. For instance, there are increasing numbers of attempts of topic-controlled EdQG relying on Model-as-a-Service (MaaS) products that use externally hosted LLMs like ChatGPT (e.g. [23]). While practically valuable to varying degrees of success, these approaches introduce significant privacy, ethics, and governance challenges [30]. The extensive costs associated with the training and deployment of these models on-premise also make them impractical for educational stakeholders from both operational and financial perspectives [26]."}, {"title": "5.1 Implications of the Results for Research and Practice", "content": "Regarding educational practice, the proposed topic-controlled question generating model can be useful for different tasks within the education domain. Primarily, we see such a tool as a teacher assistant tool to propose questions to teachers to select from. Such a tool would keep teachers in the loop as final decision makers but help them with tasks such as generating topic-specific, relevant, and age-appropriate questions for teaching. As discussed in the introduction, these tasks have long been identified as time-intensive tasks for teachers [30]. We envision tools where a teacher can point the system to a video, a presentation or a collection of learning resources where the system will automatically detect numerous salient topics and present them to the teacher as potential educational questions and the draft of a new quiz can be created in a matter of few clicks. This approach has the potential to change the degree of formative assessment due to decreased workload and can further stimulate well-anticipated innovation in education systems [40].\nWe argue that the model proposed and evaluated here has the potential to decrease teachers' workload on such tasks. Second, the model can be integrated into multiple roles within the learning analytics infrastructures. The key to a precise learner state representation is having precise tests that can verify skill mastery of individuals at finer grain. The proposed method can lead to tools that can generate high-precision assessments within a personalised learning management system that can feed better data into learning analytics. While investing significant resources to create a relatively high coverage question banks is still feasible for short course and MOOC platforms that focus on narrow scopes of knowledge, as the world is gradually moving towards informal, lifelong learning such an investment would be infeasible. Models such as the one proposed here can play a critical role for continuous topic-specific, high quality and relevant question generation in educational systems.\nThird, the model can also bring efficiencies to the implementation of question generation in mobile and resource scarce contexts. As we are dealing with very small models, systems built on these models are scalable with minimal costs and has the potential to run on mobile devices without having to connect to the Internet. These considerations are of utmost importance for more equitable use of AI in Education [8].\nRegarding LA and AI in Education research in general, the methodology proposed in this work can be extended to other forms of generation such as feedback, explanations, and content summaries in education. Also, aspects that that go beyond topical relevance (such as linguistic complexity and cognitive load etc.) can be controlled in future explorations to further advance learning analytics systems paving the way to re-imagining the limits of personalised learning material generation with AI. Furthermore, existing research either ignores the evaluation of whether the generated questions truly respond to the controlled conditions, or relies on extensive manual scoring by humans,"}, {"title": "6 Conclusion", "content": "This paper proposes a novel approach to fine-tuning pre-trained sLMs to effectively address the challenge of generating topic-controllable questions based on paragraph-level context within educational settings. In addition, a novel method to synthesise training data for this task is presented with a novel Wikipedia concept-based evaluation method. The results show that the model proposed here has the potential to decrease teacher workload and improve personalised learning platforms also proving the effectiveness of training data. The model can also be scaled financially and operationally at a minimal cost to decrease academic researchers' over-reliance on commercial LLMs like ChatGPT.\nThis study, while advancing topic-controllable question generation in education, acknowledges several limitations. The limited human evaluation sample size hinders the statistical power of our findings about the semantic relatedness metrics although the extremely high inter-annotator agreement improves reliability of the result. More extensive human annotations would strengthen the results further. While we demonstrate the proposed novel method that randomly pairs contexts enabling the model T-CQG performance to improve, different pairing strategies that respect the subject domain, subtopics, difficulty level etc. can also lead to more effective training sets and should be explored in future studies. Finally, while the proposed method can be used to train the pre-trained model to contextualise generations to topical relevance, it focuses on topical relevance only. However, how to incorporate multiple aspects in addition to topical relevance such as linguistic complexity and generation length (e.g. short question) together should be explored in the future."}, {"title": "MAE(Human, SimRel)", "content": "$\\frac{\\sum_{\\hat{q} \\in Q} |Human(\\hat{q}) - SemRel(\\hat{q})|}{\\left |Q \\right |}$ where $i \\in \\{q^1_t, q^2_t, ..., q^{30}_t, q^1_{t'}, ..., q^{30}_{t'} \\}$"}]}