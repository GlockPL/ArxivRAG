{"title": "Mamba-Shedder: Post-Transformer Compression for Efficient Selective Structured State Space Models", "authors": ["J. Pablo Mu\u00f1oz", "Jinjie Yuan", "Nilesh Jain"], "abstract": "Large pre-trained models have achieved outstanding results in sequence modeling. The Transformer block and its attention mechanism have been the main drivers of the success of these models. Recently, alternative architectures, such as Selective Structured State Space Models (SSMs), have been proposed to address the inefficiencies of Transformers. This paper explores the compression of SSM-based models, particularly Mamba and its hybrids. We study the sensitivity of these models to the removal of selected components at different granularities to reduce the model size and computational overhead, thus improving their efficiency while maintaining accuracy. The proposed solutions, collectively referred to as Mamba-Shedder, achieve a speedup of up to 1.4x during inference, demonstrating that model efficiency can be improved by eliminating several redundancies with minimal impact on the overall model performance.", "sections": [{"title": "1 Introduction", "content": "We have seen an outstanding increase in the number of Transformer-based models (Vaswani et al., 2017) developed to tackle tasks from Natural Language Processing (NLP) and other domains (Parmar et al., 2018; Dosovitskiy et al., 2021; Arnab et al., 2021; Gong et al., 2021) due to their effectiveness at modeling sequences. However, these models also present critical efficiency challenges. For example, the cost of training these models scales quadratically in the sequence length. In the generation stage, Transformers, in their original form, require large caches to store the previously seen tokens. Several variants of Transformers have been proposed to address these efficiency challenges, but researchers have also explored alternative post-Transformer architectures to address these limitations. Structured state space models (SSMs), e.g., S4 (Gu et al., 2022), followed by Selective state space models, e.g., Mamba (Gu and Dao, 2023; Dao and Gu, 2024) have been proposed as efficient alternatives that achieve training time with linear scaling in sequence length, and during generation, maintain constant state size.\nModel compression methods, e.g., pruning and quantization, have been broadly explored and applied to Transformer-based models. However, more must be done to explore compression in their structured state space counterparts. This paper explores the pruning of these alternative architectures, presenting results that provide insights into potential opportunities to increase their efficiency without sacrificing accuracy. The rest of the paper discusses the following contributions:\n\u2022 A pruning solution, Mamba-Shedder, which targets structures in selective structured state space models, improving their computational and memory efficiency.\n\u2022 Comprehensive experiments to determine the tolerance of SSM-based models to the removal of their structures.\n\u2022 Insights on how the differences in the SSM building blocks and their interaction with Transformer blocks in hybrid models affect the trade-off between efficiency and accuracy.\nThe following content is organized as follows: Section 2 provides the reader with details of the alternative architectures utilized in our study and popular strategies for element removal in large models. Section 3 describes methods to study network pruning in Mamba and hybrid architectures. Section 4 presents the results of our experiments and ablation studies, and we offer concluding remarks in Section 5. A Related Work section is included in the Appendix."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 State Space Models", "content": "State space models (SSMs) have a long history of modeling sequences and dynamic systems. Recently, structured SSMs, e.g., S4 (Gu et al., 2022), have been proposed as an alternative to Transformers because of their efficient capabilities for mapping input to output signals. When dealing with discrete sequences as in Natural Language Processing (NLP), the parameters A, B and C of these models are discretized to transform an input sequence, xt, and hidden state, ht, to obtain the output sequence, Yt. It can be formalized as:\n$h_t = Ah_{t-1} + Bx_t$, $Y_t = Ch_t$.    (1)\nMamba: Selective State Space Models S4 and other structured SSMs are linear time-invariant (LTI), i.e., their parameters are fixed, limiting their effectiveness for sequence modeling. For instance, structured state space models fail in many content-and context-based reasoning tasks. These limitations have motivated the development of time-varying alternatives, e.g., Mamba (Gu and Dao, 2023), which incorporate selection mechanisms and are suitable for solving tasks previously SSM generations failed. Specifically, Mamba's SSM module, S6, allows its parameters to depend on the input, thereby modifying the formulation from time-invariant to time-varying. A second improvement proposed in Mamba compared to previous SSMs is a hardware-aware algorithm that speeds up execution while reducing memory IOs.\nFurthermore, Mamba-2 (Dao and Gu, 2024) improves the original Mamba architecture by proposing state space duality (SSD), which improves its efficiency on hardware accelerators compared to S6. This improvement is achieved by changing the state matrix, A, which directly controls the latent state, h. A is modified from being structured as a diagonal matrix to a formulation that utilizes a scalar-times-identity structure.\nAdditionally, Mamba-2 introduces the concept of heads in SSMs inspired by how multi-head attention (MHA) works and implementing a grouped-value attention (GVA) head structure. Overall, the Mamba-2 architecture, with its SSD core component, allows for improved parallelism of the block's projections.\nMamba block Mamba models comprise several blocks stacked after each other."}, {"title": "2.2 Hybrid Models", "content": "Lately, new models have been proposed that achieve the best of both worlds (Transformers and Selective SSMs) by proposing architectures with both classes of blocks. Zamba (Glorioso et al., 2024) is one example of such a hybrid model. It combines the strengths of Mamba's backbone and the efficiency of selective SSMs with a shared Transformer block that incorporates Transformers' powerful in-context learning capabilities. The shared attention mechanism, in which two attention blocks are reused and interleaved in an ABAB pattern throughout the network, is a characteristic innovation of Zamba. This model also applies LORA adapters (Hu et al., 2022) to the shared MLP blocks, achieving specialization when interacting with the affected layers, memory efficiency, and faster inference with reduced computational overhead.\nAnother example of a hybrid model is Hymba (Dong et al., 2024). This model takes a different approach than Zamba, proposing an entirely new hybrid-head module, in which the SSM and Attention mechanisms contribute in parallel to the sequence modeling. Additionally, Hymba benefits from group query attention, cross-layer KV cache sharing, and learnable meta-tokens, resulting in higher throughput, reduced memory requirements, and competitive performance compared to models of similar size."}, {"title": "2.3 Model Pruning", "content": "A popular model compression technique, pruning (LeCun et al., 1989), has been effectively used to reduce the size of deep learning models and improve their efficiency. Network pruning operates at two levels: (1) Unstructured pruning identifies the importance of individual weights that can be masked to minimize their impact on overall model behavior. At a different level, (2) structured pruning focuses on removing more significant structural components of the model, such as whole Trans-"}, {"title": "3 Methodology", "content": "Due to the large sizes of current state-of-the-art sequence models, Mamba-Shedder requires an efficient strategy to identify structures that can be removed without significantly affecting the model's accuracy. We approach this problem using a training-free approach, in which the least essential elements are considered for removal. Similar strategies have been explored in Transformer-based large language models (Ashkboos et al., 2024; Men et al., 2024; Zhong et al., 2024). However, to our knowledge, no study explores the removal of structures in Selective Structured State Space models. Mamba-Shedder conducts structure removal of Mamba models and their hybrid variants at different granularities. As illustrated, in the case of models with only Mamba blocks, we explore the iterative removal of entire Mamba blocks (\u00a72.1), or their SSM subcomponents, either S6 or SSD modules depending on the version of Mamba .\nThe proponents of the Mamba architecture do not provide a rationale for the number of Mamba blocks required to build robust models, opening an opportunity for Mamba-Shedder to investigate whether some components might be redundant and hence removed from the model with a minor impact in accuracy.\nIn addition to these components, in the case of hybrid models that also contain Transformer blocks, we also explore the removal of entire Transformer blocks or their subblocks: multilayer perceptrons (MLP) modules and multi-head attention (MHA) modules. In hybrid models, Mamba-Shedder also explores the removal of structures at a finer granularity by targeting groups of channels in the MLP's linear layers, i.e., based on a channel group size, g, Mamba-Shedder explores the removal of ng channels, where n is the number of groups that could be removed based on their impact of the overall model performance."}, {"title": "4 Experiments", "content": "We evaluate Mamba-Shedder and study the removal of structures from SSM-based models utilizing several open-source models and datasets. We analyze their absolute and relative drop in accuracy and quantify the inference speedup obtained by the pruned models. Next, we discuss the resources utilized for our experiments and details of our setup and results."}, {"title": "4.1 Models", "content": "Our experiments employed the following pretrained Mamba and hybrid models: Mamba-2.8b (Gu and Dao, 2023), consists of 64 S6 blocks. Mamba2-2.7b (Dao and Gu, 2024), consists of 64 SSD blocks . Both Mamba models were trained on 300B tokens on the Pile dataset (Gao et al., 2020). For our choice of a hybrid model, we explored Zamba2-2.7B (Glorioso et al., 2024). It has 54 layers, including 45 single Mamba-2 Blocks and 9 hybrid layers composed of both Mamba-2 Blocks and Transformer Blocks. Zamba-2 was trained on 3T tokens from open web datasets, including Zyda (Tokpanov et al., 2024), and subsequently annealed with 100B additional tokens. The aforementioned models are all of the same size and can be compared directly. For Mamba models of different sizes, we also explored Falcon-Mamba-7B (Zuo et al., 2024), which is based on the Mamba-1 architecture and is the best-performing Mamba model at this scale in the literature, as well as Hymba-1.5B-Base (Dong et al., 2024), which features a hybrid architecture incorporating both Mamba and Attention heads."}, {"title": "4.2 Datasets", "content": "Following the language modeling evaluation of Mamba (Gu and Dao, 2023; Dao and Gu, 2024), we utilize lm-eval-harness (Gao et al., 2023) to assess the zero-shot performance, which includes measuring perplexity on Lambada (Paperno et al., 2016), and accuracy on the following downstream tasks: HellaSwag (Zellers et al., 2019), Physical Interaction Question Answering (PIQA) (Bisk et al., 2020), AI2 Reasoning Challenges (Arc-e, Arc-c) (Clark et al., 2018), Large-scale Winograd Schema Challenge (WinoGrande) (Sakaguchi et al., 2021), and the Open Book Question Answering (Mihaylov et al., 2018) dataset.\nRegarding the calibration dataset, we follow BlockPruner (Zhong et al., 2024) in using the Alpaca dataset as the calibration dataset and employ perplexity as the metric for calculating importance scores. All the hyperparameters used in our experi-"}, {"title": "4.3 Results", "content": ""}, {"title": "4.3.1 Pruning Target: Mamba Block", "content": "This section explores the impact of pruning Mamba blocks on model performance. The model that utilizes the first version of Mamba blocks (S6) appears to tolerate a higher number of removed blocks without significantly affecting its performance. Specifically, the Mamba-2.8B model demonstrates robustness, with its perplexity (PPL) increasing from 4.23 to 7.51 and average accuracy dropping from 59.9 to 53.8 when the pruning ratio reaches 20.86%. In contrast, the Mamba2-2.7B and Zamba2-2.7B models exhibit more significant performance degradation, although they performed better before pruning (Dense). The poorer pruning performance of Zamba2-2.7B may be attributed to the pruning of Mamba blocks disrupting a certain balance within the hybrid layers. Overall, the effects of Mamba block pruning vary across different models, depending on the model architecture and the characteristics of the pre-training stage. In this round, Mamba-1 comes out on top."}, {"title": "4.3.2 Pruning Target: SSM Module", "content": "In this section, we delve into assessing the impact of pruning only the SSM modules within Mamba blocks on the performance of various models. When using the same target in Mamba-2.8B, we observe that further pruning SSMs results in a noticeable increase in perplexity, soaring to 22.55 and decreasing average accuracy to 44.1. This result indicates a significant sensitivity to SSM pruning for Mamba-1, where performance degradation is pronounced even at moderate pruning levels. Conversely, Mamba2-2.7B and Zamba2-2.7B exhibit remarkable resilience to SSM pruning. Even with 24 SSMs pruned, the model maintains a relatively stable performance. This robustness suggests that Mamba-2 blocks can tolerate higher SSM module pruning, potentially due to Mamba-2's optimizations or different training strategies with Mamba-1. The Zamba2-2.7B model, with the hybrid architecture, outperforms both Mamba-1 and Mamba-2. Pruning 12 out of its 54 SSMs results in a negligible PPL increase from 4.01 to 4.02, while the average accuracy slightly decreases from 67.2% to 67.0%. The hybrid nature of Zamba2-2.7B may contribute to its ability to maintain performance despite SSM pruning. Overall, these findings underscore the importance of model architecture and training strategies in determining the impact of SSM pruning. They offer valuable insights for optimizing model efficiency without compromising performance. In this round, the model with Mamba-2 blocks comes out on top."}, {"title": "4.3.3 Pruning Target: Finer-grained removal of Mamba and Transformer blocks, and their subcomponents", "content": "Table 3 presents the results of pruning various components of the Zamba2-2.7B model, including combinations of Mamba-2 blocks, entire Transformer blocks, and their subcomponents, i.e., MHA blocks, MLP blocks, MLP channels, and SSM modules. We design four search spaces to study the effectiveness of different granularities and their combinations. \"&\" indicates that the pruning targets are considered together in the same pruning step, while \"+\" signifies the distinction between pruning stages, with pruning occurring sequentially:\nMamba Block & Transformer Block Pruning This experiment involves pruning the entire Mamba-2 blocks and Transformer blocks.\nMamba Block & MLP & MHA Pruning This experiment decomposes the transformer block into sub-blocks, pruning Mamba-2 blocks as well as MHA and MLP.\nMamba Block & MLP & MHA + MLP Channel Pruning This experiment prunes the Mamba-2 blocks, MHA, and MLP at the first stage and further prunes the MLP channels at the next stage.\nMamba Block & MLP & MHA + MLP Channel Pruning + SSM Add additional SSM pruning following the previous solution.\nThe results indicate that pruning Mamba blocks and Transformer blocks alone leads to significant performance degradation. However, more granular pruning strategies show a more favorable trade-off between pruning ratio and performance. Specifically, pruning Mamba blocks, MLP, MHA and MLP channels subsequently performs the best. Inspired by the SSM pruning of Mamba-2 in Section 4.3.2, we further add SSM pruning to the third strategy, and the results show that removing around 18 SSMs can maintain accuracy performance while reducing computational overhead. An interesting finding is that pruning SSMs can even lower PPL; for instance, at a 10% pruning ratio, PPL decreases from 5.45 to 5.18, suggesting that some SSM modules are redundant after the second pruning stage. Overall, these findings indicate that multi-granularity pruning methods, particularly those including MLP channels and SSM modules, can effectively reduce the complexity of hybrid Mamba models while maintaining a higher level of performance."}, {"title": "4.3.4 Pruning Mamba Models of Other Sizes", "content": "Table 4 shows the results of Mamba-Shedder with training-free Hymba Block pruning for Hymba-1.5B-Base. The dense configuration achieves an average accuracy of 63.8, which decreases as more blocks are pruned, dropping to 60.5 when 8 blocks are pruned, indicating a general decline in performance across benchmarks. Further analysis of inference acceleration and recovery tuning experiments for Hymba-1.5B-Base will be discussed in the subsequent sections.\nFalcon-Mamba While the previous sections focused on exploring the pruning of Mamba models with sizes around 2.7B or 2.8B, we also investigated the impact of Mamba-Shedder on a larger-scale Mamba model, specifically Falcon-Mamba-7B (Table 5). Pruning SSM modules in the Falcon-Mamba-7B model shows better tolerance in terms of perplexity, suggesting that SSM pruning is more effective in maintaining lower perplexity. Regarding average accuracy, pruning entire Mamba blocks is more beneficial.\nAdditionally, it is important to note that pruning entire Mamba blocks yields more significant computational benefits than SSM pruning, suggesting that while SSM pruning is advantageous for maintaining perplexity, pruning Mamba blocks offers a better trade-off between computational efficiency and accuracy. The choice of pruning strategy should be guided by the specific performance metric of interest and the desired balance between computational efficiency and model accuracy.\nNone of the above results have undergone finetuning to improve the performance of the pruned models. As in many other works, the drop in the accuracy performance of pruned models can be recovered by fine-tuning, which will be incorporated in Section 4.5."}, {"title": "4.4 Inference Acceleration", "content": "Through the above analysis, we have gained a good understanding and insight into the impact of Mamba-Shedder's structured pruning on model accuracy and perplexity performance. In addition, through structured pruning, Mamba-Shedder achieves an additional speedup to these already highly efficient models. Next, we discuss the impact of inference acceleration. All the following tests were conducted on a single Tesla V100 32GB GPU.\nMamba-1 When removing entire Mamba blocks, as shown in Table 6, Mamba-Shedder speeds up the decoding stage up to 1.29x when removing 14 blocks, and 1.13x when removing only 7 blocks, which highlights the potential of Mamba-Shedder to optimize computational efficiency in Mamba models. The user's decision on how aggressively to prune will impact the average accuracy or the perplexity as observed in Table 1.\nMamba-2 As detailed in Table 7, removing 24 SSM modules (44% of the total number of modules) results in up to a 1.20x speedup in the prefill stage and a 1.18x speedup in the decoding stage during of inference. A more conservative pruning ratio achieves 1.11x speedup when removing 16 SSM modules. Based on previous observations, the impact on performance metrics is minimal (0.4% for accuracy and 0.16 for PPL). These results underscore the effectiveness of SSM pruning in enhancing computational efficiency while barely affecting model performance, making it a viable strategy for optimizing Mamba models.\nZamba-2 As detailed in Table 8, we observe significant acceleration on inference after multiple granularities pruning of Zamba-2. Specifically, pruning Mamba blocks, MLP, and MHA blocks along with MLP channels results in a 1.34x speedup in the decoding stage. When SSM pruning is included, the speedup increases to 1.39x, indicating that a comprehensive pruning strategy that includes multiple components can significantly enhance inference speed while maximizing the preservation of model performance.\nHymba As shown in Table 9, the hymba block pruning of Hymba-1.5B-Base demonstrates notable improvements in inference speed. By removing 7 out of 64 Hymba blocks, Mamba-Shedder achieves a 1.15x speedup in the prefill stage and a 1.24x speedup in the decoding stage, suggesting that significant computational efficiency gains can be realized even with a relatively modest pruning ratio. The results highlight the potential of Mamba-Shedder to optimize the performance of Hymba models, making them more efficient for real-time applications without substantial sacrifices in model accuracy."}, {"title": "4.5 Recovery Tuning of the Pruned Model", "content": "Following most of the work (Ma et al., 2023; Zhong et al., 2024), we performed post-training on the Mamba-Shedder compressed model using the cleaned version of Alpaca. The results demonstrate substantial performance gains after just two epochs of recovery tuning (see Appendix for more hyperparameters). For instance, the Mamba-Shedder model obtained by removing Mamba Blocks & MLPs & MHAs + MLP Channels + SSM in Zamba-2 , initially exhibits a perplexity of 5.18 and an average accuracy of 65.9 when 18 out of 54 SSMs are pruned. However, after recovery tuning, it achieves a significantly reduced PPL of 4.58 and an improved average accuracy of 67.0, which is almost on par with the Dense model. Similarly, as shown in Table 12, the recovery tuning of the Hymba-1.5B-Base model also yields significant improvements. Initially, the pruned model with 7 out of 32 Hymba blocks removed shows an average accuracy of 61.7. After recovery tuning, the average accuracy increases to 63.7, which is nearly equivalent to the dense model's accuracy of 63.8."}, {"title": "4.6 Insights on the Compression Sensitivity of the Variants of Mamba", "content": "A research question during our investigation considered, will the improvements in Mamba-2 make it more sensitive to removing its inner structures?\nThe proponents of Mamba modified the original architecture to restrict the expressivity in Mamba-2 and increase the training efficiency. As illustrated our experiments suggest that these changes make Mamba-2 models less robust to removing entire blocks than the previous version of the Mamba block. As soon as we remove blocks with the least importance, Mamba-1 exhibits a more robust behavior. However, Mamba-2 demonstrates a significantly higher tolerance to removing SSMs, maintaining a stable perplexity even as more SSMs are pruned, suggesting that while Mamba-2's architectural improvements have made it more sensitive to the removal of Mamba blocks, they have also enhanced its robustness to SSM pruning."}, {"title": "5 Conclusion", "content": "Selective structure state space models have become an efficient alternative to Transformer-based models. In this paper, we propose Mamba-Shedder and investigate structured pruning strategies to remove elements from Mamba and hybrid models and reduce model size, accelerating inference. The results demonstrate that selective structured state space architectures have several redundancies that can be removed without significantly affecting the model's performance."}, {"title": "Limitations", "content": "Despite their outstanding results, large sequence models are still under investigation to better understand their capabilities and limitations. Mamba-Shedder is, to the best of our knowledge, the first work to investigate the removal of structures in Mamba-based models, including hybrids with Transformer blocks. Our goal is to motivate the research community to better understand this class of models to identify opportunities for future improvements in the model architecture and applicable compression techniques. The results indicate that these models contain redundant elements that might be removed to improve their efficiency. However, future work must explore and attempt to better understand the trade-offs between efficiency and accuracy when removing these models' components. Even more research questions can be entertained when considering Transformer blocks and hybrid models, as in the case of Zamba. For instance, there is much to understand about the right mix of the SSM- and Transformer-based elements."}, {"title": "Ethics Statement", "content": "Due to the well-known flaws in modern sequence models, e.g., hallucinations, many guard rails must be in place when considering deploying them in production. Our research focuses on improving the efficiency of these models in existing downstream tasks and datasets. However, further experimentation and analysis are needed when considering deploying these compressed models in environments where their output might affect people's well-being."}]}