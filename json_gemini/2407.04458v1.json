{"title": "Robust Multimodal Learning via Representation Decoupling", "authors": ["Shicai Wei", "Yang Luo", "Yuji Wang", "Chunbo Luo"], "abstract": "Multimodal learning robust to missing modality has attracted increasing attention due to its practicality. Existing methods tend to address it by learning a common subspace representation for different modality combinations. However, we reveal that they are sub-optimal due to their implicit constraint on intra-class representation. Specifically, the sample with different modalities within the same class will be forced to learn representations in the same direction. This hinders the model from capturing modality-specific information, resulting in insufficient learning. To this end, we propose a novel Decoupled Multimodal Representation Network (DMRNet) to assist robust multimodal learning. Specifically, DMRNet models the input from different modality combinations as a probabilistic distribution instead of a fixed point in the latent space, and samples embeddings from the distribution for the prediction module to calculate the task loss. As a result, the direction constraint from the loss minimization is blocked by the sampled representation. This relaxes the constraint on the inference representation and enables the model to capture the specific information for different modality combinations. Furthermore, we introduce a hard combination regularizer to prevent DMRNet from unbalanced training by guiding it to pay more attention to hard modality combinations. Finally, extensive experiments on multimodal classification and segmentation tasks demonstrate that the proposed DMRNet outperforms the state-of-the-art significantly.", "sections": [{"title": "1 Introduction", "content": "Multimodal learning has yielded significant advancements across a wide array of vision tasks, including classification [15, 25, 37], object detection [18, 36, 44], and segmentation [4, 16, 31]. Nevertheless, most of these state-of-the-art approaches assume that models are trained and evaluated with the same modality data. In fact, limited by device [24, 30], and working condition [2, 22], it is often very costly or even infeasible to collect complete modality data during the inference stage. Consequently, there exists a compelling need to improve the inference robustness of multimodal models for incomplete input.\nTypically, existing solutions can be divided into two categories: data imputation-based methods and common subspace-based methods. Data imputation-based methods"}, {"title": "2 Related work", "content": "In this work, we focus on the model robustness for incomplete modality data during inference. Specifically, while a variety of sensory modalities can be collected for training, not all of them are always available during testing due to the device [24, 30] and working condition [2, 22]. To this end, many incomplete multimodal learning methods have been proposed and can be roughly categorized into two types: data imputation-based methods and common subspace-based methods.\nData imputation-based methods: These methods first impute the samples or representations of missing modalities and then apply the traditional multimodal algorithm directly. Early works relied on generative adversarial networks to reconstruct the modality sample [3, 19, 24]. Due to the complexity of sample reconstruction, recent methods reduce the problem from input space to latent space to impute the modality representation via generative adversarial networks [17, 23] or knowledge distillation [6, 9, 10, 14, 22]. Although these methods achieve promising results, almost all of them consider the scenarios with two modalities only and are difficult to be scaled to scenarios with multiple modalities."}, {"title": "2.1 Incomplete multimodal learning", "content": "In this work, we focus on the model robustness for incomplete modality data during inference. Specifically, while a variety of sensory modalities can be collected for training, not all of them are always available during testing due to the device [24, 30] and working condition [2, 22]. To this end, many incomplete multimodal learning methods have been proposed and can be roughly categorized into two types: data imputation-based methods and common subspace-based methods.\nData imputation-based methods: These methods first impute the samples or representations of missing modalities and then apply the traditional multimodal algorithm directly. Early works relied on generative adversarial networks to reconstruct the modality sample [3, 19, 24]. Due to the complexity of sample reconstruction, recent methods reduce the problem from input space to latent space to impute the modality representation via generative adversarial networks [17, 23] or knowledge distillation [6, 9, 10, 14, 22]. Although these methods achieve promising results, almost all of them consider the scenarios with two modalities only and are difficult to be scaled to scenarios with multiple modalities."}, {"title": "2.2 Probabilistic Embeddings", "content": "Probabilistic representations of data have a long history in machine learning. They were introduced for word embeddings to model the uncertainty about the target concepts with which the input may be associated [21, 28, 38]. Recently, probabilistic embeddings have been introduced for vision tasks. Sun et al. propose view-invariant probabilistic embedding to handle the project uncertainty between 2D and 3D poses [35]. Shi et al. introduce the probabilistic embedding for face recognition system to deal with the quality uncertainty of face image [33]. And Chang et al. extends it by making the mean of distribution learnable to get better intra-class compactness and inter-class separability [8]. More recent works leverage probabilistic embedding for image-text retrieval to handle the semantic uncertainty of image input [5, 27]."}, {"title": "3 Methods", "content": "In this work, we first analyze the representation constraint of the conventional common subspace-based method. Then we introduce the DMRNet to assist incomplete multimodal learning. As shown in Fig. 2, it consists of two components: decoupled multimodal representation and hard combination regularizer. Specifically, decoupled multimodal representation helps relax the direction constraints on inference representation. The hard combination regularizer further improves the model representation ability for hard modality combination inputs.\nNotations. The notations used in this paper are described as follows. \\(X = \\{x_i, y_i\\}_{i=1}^N\\) is a multimodal dataset that has N samples. Each \\(x_i\\) consists of V inputs from different modalities as \\(x_i = (x_i^{(1)}, ..., x_i^{(V)})\\) and \\(y_i \\in [1, 2, ..., M]\\), where M is the number of categories. \\(\\Delta_i = \\{\\delta_i^{(1)}, ...\\delta_i^{(V)}\\}\\) is the Bernoulli sequence corresponding to the V modality encoders, where \\(\\delta_i^{(v)} \\in \\{0,1\\}\\) is the Bernoulli indicator for the vth encoder. \\(f_v\\) denotes the functions for vth modality encoder. \\(f_f\\), \\(f_{\\mu}\\), and \\(f_{\\sigma}\\) denote the functions for calculating the fusing representation z, mean value \\(\\mu\\), and standard deviation"}, {"title": "3.1 Analysis of Representation Constraint", "content": "We introduce the analysis of the representational constraint problem for incomplete multimodal learning and find that different input combinations from the same class will be forced to learn the embeddings with the same direction. This limits the model's representation ability for the specific information of different modality combinations, leading to insufficient representation learning.\nSpecifically, the multimodal embedding \\(z_i\\) of \\(x_i\\) in the conventional common subspace-based model can expressed as follows,\n\\[\\begin{cases}\nz_i = f_f (\\Theta_f, r_i^{(1)} * \\delta_i^{(1)}, ..., r_i^{(V)} * \\delta_i^{(V)})\\\\r_i^{(v)} = f^{(v)} (\\Theta_e, x_i^v), v\\in [1, V]\n\\end{cases}\\]\nwhere \\(r_i^{(v)}\\) is the embedding of the vth modality. \\(\\Theta_f\\) is the parameters for the fusion module. \\(\\Theta_e\\) is the parameters for the vth modality encoder. \\(\\delta_i^v \\in [0, 1]\\) is the Bernoulli indicator for the vth modality of \\(x_i\\). It is randomly set to either 0 or 1 to simulate random modality missing. This makes the model robust for incomplete inference data.\nRepresentatively, we take the classification task as an example for analysis here. Let \\(W \\in \\mathbb{R}^{C\\times M}\\) denote the parameters of the final linear classifier. The typical cross entropy loss \\(L_{CE}(.)\\) for the model is defined as follows,\n\\[L_{CE} = -\\frac{1}{N} \\sum_{i=1}^N log \\frac{e^{W_{y_i}g(z_i)}}{\\sum_{k=1}^M e^{(W_kg(z_i))}}\\]\nwhere C is the channel dimension \\(W_{y_i}\\) means the \\(y_i\\)-th column of the W. \\(g(z_i)\\) denotes the feature vector after the global average pooling and flatten operators.\nFor a well-trained model, it needs to minimize the loss of each class. Here, \\(W_y\\) is fixed for \\(y_i\\)-th class. To minimize the loss of class \\(y_i\\), \\(W_{y_i}g(z_i)\\) should be maximized."}, {"title": "3.2 Decoupled Multimodal Representation", "content": "As discussed, the representation ability of conventional common subspace-based methods is limited by the intra-class representation constraint for different input combinations. To solve the problem, we introduce the decoupled multimodal representation technique to alleviate the direction constraint on the inference representation by decoupling the training and inference representations. Specifically, it consists of two parts: representation probabilization and distribution regularization.\nRepresentation Probabilization. Different from existing methods that leverage the deterministic embedding in Eq.(1) for task predictors directly, we consider building probabilistic embeddings, i.e. \\(z_i \\sim p(z_i|x_i)\\), for a more flexible representation space. For simplicity, we define the probabilistic embedding \\(z_i\\) obeys a multivariate Gaussian distribution,\n\\[p(z_i|x_i) = N(z_i; \\mu_i, \\sigma_i^2)\\]\nwhere both the parameters (\\(\\mu_i\\) and \\(\\sigma_i\\)) of the Gaussian distribution are input-dependent predicted. Different from existing methods [8, 33] that estimate \\(\\mu_i\\) and \\(\\sigma_i\\) for the feature vector after pooling, we estimate the \\(\\mu_i\\) and \\(\\sigma_i\\) for the feature map directly. This not only contributes to better performance but also enables the model to handle the dense prediction task, such as segmentation. For a specific, \\(\\mu_i\\) and \\(\\sigma_i\\) are defined as follows,\n\\[\\begin{cases}\n\\mu_i = f_{\\mu}(\\Theta_{\\mu}, z_i)\\\\log(\\sigma_i) = f_{\\sigma}(\\Theta_{\\sigma}, z_i)\n\\end{cases}\\]\n\\(\\Theta_{\\mu}\\) and \\(\\Theta_{\\sigma}\\) are the parameters for \\(f_{\\mu}(.)\\) and \\(f_{\\sigma}(.)\\), respectively. Here we predict \\(log \\sigma_i\\) instead of \\(\\sigma_i\\) directly for a better stability [5, 27]. In detail, we implement \\(f_{\\mu}(.)\\) and \\(f_{\\sigma}(.)\\) with a simple 1x1 convolution with the batch norm, respectively, which introduces negligible parameters.\nNow, the representation of each sample becomes a stochastic embedding sampled from \\(N(z_i; \\mu_i, \\sigma_i^2)\\). Nevertheless, the sampling operation is not differentiable. Thus, we consider the reparameterization trick [20] to enable backpropagation,\n\\[s_i = \\mu_i + \\epsilon \\sigma_i, \\epsilon \\sim N(0, I)\\]\nIn general, we sample noise from \\(N(0, I)\\) and obtain the embedding \\(s_i\\) following Eq.(7) instead of directly sampling from \\(N(z_i; \\mu_i, \\sigma_i^2)\\). Here, \\(s_i\\) is the embedding for the training of the prediction module and \\(\\mu_i\\) is the final embedding for inference.\nIn this way, DMRNet decouples the training and inference embedding. And the cross-entropy loss for the model can be rewritten as follows,\n\\[L_{CE} = -\\frac{1}{N} \\sum_{i=1}^N log \\frac{e^{W_{y_i}g(s_i)}}{\\sum_{k=1}^M e^{(W_kg(s_i))}}\\]"}, {"title": "3.3 Hard Combination Regularizer", "content": "Since DNNs tend to first memorize simple samples before overfitting hard samples [1, 11, 41]. The common subspace model tends to fit the discriminative modality combinations first. Consequently, their performance will significantly decline when facing hard modality combinations. To handle this unbalanced training issue, we introduce the hard combination regularizer. It mines the hard modality combination relying on the estimated \\(\\sigma_i\\) and then introduces independent gradient paths for them to regularize their optimization. Compared with the traditional regularization methods [7, 40, 43] that use additional modules, this regularizer shares the parameters with the prediction module of DMRNet, and thus introduces no extra parameters.\nHard Combinations Mining. Since the model tends to fit the input from discriminative modality combinations first, the hard combinations will have a bigger variance than the discriminative combinations that are well optimized by \\(L_{DR}\\). And we can mine the hard combinations through simple variance ranking.\nSpecifically, given V modalities, we can obtain \\(2^V\\) modality combinations by setting the components in \\(\\{\\delta_i^{(1)}, ...\\delta_i^{(V)}\\}\\) as 0 or 1 randomly. Here, we calculate the variance set \\(D = \\{d_1, ..., d_{2^V-1}\\}\\) of all combinations among the training dataset,\n\\[d_j = \\frac{1}{N_j * c * h * w} \\sum_{i \\in \\Omega_j} \\sum_{c=1}^C \\sum_{h=1}^H \\sum_{w=1}^W (\\sigma_i^2)\\]"}, {"title": "3.4 Total Loss", "content": "The total training loss L for the DMRNet is defined as follows,\n\\[L = L_{TTL} + \\alpha L_{DR} + \\beta L_{HCR}.\\]\nwhere \\(L_{TTL}\\) is the traditional target task loss for input data. \\(\\alpha\\) and \\(\\beta\\) are the hyperparameters. Specifically, \\(\\alpha\\) controls the level of relaxation, and \\(\\beta\\) controls the regularization degree for hard modality combinations. A detailed analysis of them can be seen in Section 4.4."}, {"title": "3.5 Relationship to Prior Work", "content": "Some concurrent works [5, 33] seem similar to our DMRNet, which also utilize the technique of probabilistic representation. However, there are still differences in motivations and implementations with ours. They leverage probabilistic embedding to deal with the mapping uncertainty caused by input variants. In contrast, we utilize probabilistic embedding to introduce the uncertainty to the multimodal representation proactively, decoupling the training and inference representation to relax the direction constraint caused by the intra-class direction constraint.\nBesides, they estimate the mean and variance for the 1D feature vectors, however, we estimate them for the 2D feature maps directly. This not only contributes to better performance (see ablation experiments in Table 4) but also enables DMRNet to handle the dense prediction task (see ablation experiments in Table 5)."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Performance and Comparison on Face Anti-spoofing Task", "content": "Datasets: We performed experiments on CASIA-SURF [42] dataset. It contains the RGB, Depth, and IR modalities. To evaluate the performance of DMRNet, we followed"}, {"title": "4.2 Performance and Comparison on Audio-Visual Recognition Task", "content": "Datasets: We perform experiments on CREMA-D and Kinetics-Sounds datasets. Both of them contain audio and visual modalities. This helps to evaluate the generalization of the proposed DMRNet to non-visual modalities. Specifically, CREMA-D contains 7,442 video clips for 6 common emotions. Following its benchmark, the whole dataset is randomly divided into a 6698-sample training set and a 744-sample testing set. Besides, Kinetics-Sounds is a large-scale dataset that contains 19k 10-second video clips for 34 actions (15k training, 1.9k validation, 1.9k test).\nComparison: Here, we set OGM-MD as the baseline, which is the variant of OGM [29] by adding the Bernoulli indicator \\(\\delta\\) after the modality encoder. We mainly compare DMRNet with the recent SOTA method, ShaSpec [39] and MMANet [40]. The metric is classification accuracy.\nImplementation We unify the backbone and fusion module as those of OGM [29] for a fair comparison. Specifically, for the visual encoder, we take 1 frame as input; for the audio encoder, we slightly change the input channel of ResNet18 from 3 to 1, and the rest parts remain unchanged. We process the audio dataset into a spectrogram of size 257\\times299 for CREMA-D and 257\\times1,004 for Kinetics-Sounds. We use SGD with 0.9 momentum and 1e-4 weight decay as the optimizer. The learning rate is 1e-3 initially and multiplies 0.1 every 70 epochs.\nResult: As shown in Table 2, DMRNet also achieves the best average performance on both datasets. Specifically, it outperforms the ShaSpec and MMANet by 3.41% and 3.69%, respectively, in CREMA-D and by 4.39% and 4.07%, respectively in Kinetics-Sounds. This verifies its effectiveness on non-visual modality tasks. Moreover, compared to the CREMA-D, the performance gain of DMRNet on Kinetics-Sounds is higher, showing its superiority in processing large-scale datasets."}, {"title": "4.3 Ablation Study", "content": "In this section, we first detail the definition of feature diversity. Then we study the effectiveness of the decoupled multimodal representation and hard combination regularizer. Besides, we compare different implementations of distribution estimation and study the generalization of DMRNet to dense prediction tasks.\nMetric for feature diversity. Since the feature diversity is the channel diversity of the feature maps [12], we introduce the inter-channel distance metrics \\(D_{channel}\\) to evaluate the intra-modality and inter-modality feature diversity. Specifically, given the modality embedding \\(F_v = f(x^{(v)})\\), \\(v \\in [1, V]\\), we reshape its shape as \\(\\mathbb{R}^{c\\times h*w}\\), where c, h, w are the channel number, height, and width of the embedding. The metric is defined as follows,\n\\[\\begin{cases}\nD_{channel}[k, :] = 1 - (F_v[k,:]/ ||F_v[k,:] ||_2)\\\\F_O=F_m * (F_n)^T, m, n \\in [1,V]\n\\end{cases}\\]\nwhere the notation [k, :] denotes the kth row in a matrix. If m = n, we can get the intra-modality feature diversity that calculates the cosine distance between the channel coming from nth modality. if m \\neq n, we can get the inter-modality feature diversity that calculates the cosine distance between the channel coming from mth and nth modalities. Generally, higher \\(D_{channel}\\) means higher channel difference, meaning richer feature diversity."}, {"title": "4.4 Parameters Analysis", "content": "The effect of \\(\\alpha\\). We first investigate the influence of different \\(\\alpha\\) for DMRNet. Here we use the decoupled multimodal representation only and \\(\\beta\\) is set as 0. The result is shown in Table 6. Firstly, the model achieves the best performance when \\(\\alpha\\) = 1e-3. Secondly,"}, {"title": "Generalization to dense prediction task.", "content": "We conduct multimodal segmentation experiments on the NYUv2 [34] dataset to study the generalization of the proposed DMRNet to dense prediction tasks. The dataset comprises 1,449 indoor RGB-D images, of which 795 are used for training and 654 for testing. We use the common 40-class label setting and report our results on the validation set, measured by mean IOU (mIOU).\nwe set ESANnet-MD as the baseline, which is the variant of ESANet [32] by adding the Bernoulli indicator \\(\\delta\\) after the modality encoder. for a fair comparison, we unify the backbone and fusion module of DMRNet as those used in ESANet. While RGB-Depth semantic segmentation requires the representation after each block to predict the output, DMRNet only decouples the fusing representation at the penultimate layer to assist model optimization. Otherwise, the optimization process will not converge due to higher uncertainty introduced by multiple random representation samplings.\nAs shown in Table 5, DMRNet outperforms the ESANet-MD on each modality combination consistently and improves the mIOU by 2.46% on average. This shows the generalization of the proposed DMRNet to enhance inference robustness for the dense prediction tasks."}, {"title": "5 Conclusion", "content": "In this paper, we reveal the intra-class representation constraint in conventional common subspace-based methods and propose a novel framework DMRNet to assist robust multimodal learning. It consists of two key components: decoupled multimodal representation and hard combination regularizer. Decoupled multimodal representation decouples the training and inference embedding for each sample to relax the constraint on inference embedding. Hard combination regularizer mines and regularizes the hard modality combinations to deal with the unbalanced training of different combinations. Finally, extensive experiments on multimodal classification and segmentation tasks demonstrate the effectiveness of the proposed method for incomplete multimodal learning. The code will be public after acceptance."}]}