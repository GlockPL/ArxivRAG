{"title": "Robust Multimodal Learning via Representation Decoupling", "authors": ["Shicai Wei", "Yang Luo", "Yuji Wang", "Chunbo Luo"], "abstract": "Multimodal learning robust to missing modality has attracted increasing attention due to its practicality. Existing methods tend to address it by learning a common subspace representation for different modality combinations. However, we reveal that they are sub-optimal due to their implicit constraint on intra-class representation. Specifically, the sample with different modalities within the same class will be forced to learn representations in the same direction. This hinders the model from capturing modality-specific information, resulting in insufficient learning. To this end, we propose a novel Decoupled Multimodal Representation Network (DMRNet) to assist robust multimodal learning. Specifically, DMRNet models the input from different modality combinations as a probabilistic distribution instead of a fixed point in the latent space, and samples embeddings from the distribution for the prediction module to calculate the task loss. As a result, the direction constraint from the loss minimization is blocked by the sampled representation. This relaxes the constraint on the inference representation and enables the model to capture the specific information for different modality combinations. Furthermore, we introduce a hard combination regularizer to prevent DMRNet from unbalanced training by guiding it to pay more attention to hard modality combinations. Finally, extensive experiments on multimodal classification and segmentation tasks demonstrate that the proposed DMRNet outperforms the state-of-the-art significantly.", "sections": [{"title": "1 Introduction", "content": "Multimodal learning has yielded significant advancements across a wide array of vision tasks, including classification [15, 25, 37], object detection [18, 36, 44], and segmentation [4, 16, 31]. Nevertheless, most of these state-of-the-art approaches assume that models are trained and evaluated with the same modality data. In fact, limited by device [24, 30], and working condition [2, 22], it is often very costly or even infeasible to collect complete modality data during the inference stage. Consequently, there exists a compelling need to improve the inference robustness of multimodal models for incomplete input.\nTypically, existing solutions can be divided into two categories: data imputation-based methods and common subspace-based methods. Data imputation-based methods"}, {"title": "2 Related work", "content": "In this work, we focus on the model robustness for incomplete modality data during inference. Specifically, while a variety of sensory modalities can be collected for training, not all of them are always available during testing due to the device [24, 30] and working condition [2, 22]. To this end, many incomplete multimodal learning methods have been proposed and can be roughly categorized into two types: data imputation-based methods and common subspace-based methods.\nData imputation-based methods: These methods first impute the samples or representations of missing modalities and then apply the traditional multimodal algorithm directly. Early works relied on generative adversarial networks to reconstruct the modality sample [3, 19, 24]. Due to the complexity of sample reconstruction, recent methods reduce the problem from input space to latent space to impute the modality representation via generative adversarial networks [17, 23] or knowledge distillation [6, 9, 10, 14, 22]. Although these methods achieve promising results, almost all of them consider the scenarios with two modalities only and are difficult to be scaled to scenarios with multiple modalities."}, {"title": "2.1 Incomplete multimodal learning", "content": ""}, {"title": "2.2 Probabilistic Embeddings", "content": "Probabilistic representations of data have a long history in machine learning. They were introduced for word embeddings to model the uncertainty about the target concepts with which the input may be associated [21, 28, 38]. Recently, probabilistic embeddings have been introduced for vision tasks. Sun et al. propose view-invariant probabilistic embedding to handle the project uncertainty between 2D and 3D poses [35]. Shi et al. introduce the probabilistic embedding for face recognition system to deal with the quality uncertainty of face image [33]. And Chang et al. extends it by making the mean of distribution learnable to get better intra-class compactness and inter-class separability [8]. More recent works leverage probabilistic embedding for image-text retrieval to handle the semantic uncertainty of image input [5, 27]."}, {"title": "3 Methods", "content": "In this work, we first analyze the representation constraint of the conventional common subspace-based method. Then we introduce the DMRNet to assist incomplete multimodal learning. As shown in Fig. 2, it consists of two components: decoupled multimodal representation and hard combination regularizer. Specifically, decoupled multimodal representation helps relax the direction constraints on inference representation. The hard combination regularizer further improves the model representation ability for hard modality combination inputs.\nNotations. The notations used in this paper are described as follows. X = {x\u1d62, y\u1d62}\u1d62=\u2081\u1d3a is a multimodal dataset that has N samples. Each x\u1d62 consists of V inputs from different modalities as x\u1d62 = (x\u1d62\u207d\u00b9\u207e, ..., x\u1d62\u207d\u1d5b\u207e) and y\u1d62\u2208 [1, 2, ..., M], where M is the number of categories. \u0394\u1d62 = {\u03b4\u1d62\u207d\u00b9\u207e, ...\u03b4\u1d62\u207d\u1d5b\u207e} is the Bernoulli sequence corresponding to the V modality encoders, where \u03b4\u1d62\u207d\u1d5b\u207e \u2208 {0,1} is the Bernoulli indicator for the vth encoder. f\u207d\u1d5b\u207e denotes the functions for vth modality encoder. ff, f\u00b5, and f\u03c3 denote the functions for calculating the fusing representation z, mean value \u00b5, and standard deviation \u03c3. ft, and f\u1d63 denote the mapping functions for task predictor and hard combination regularizer, respectively."}, {"title": "3.1 Analysis of Representation Constraint", "content": "We introduce the analysis of the representational constraint problem for incomplete multimodal learning and find that different input combinations from the same class will be forced to learn the embeddings with the same direction. This limits the model's representation ability for the specific information of different modality combinations, leading to insufficient representation learning.\nSpecifically, the multimodal embedding z\u1d62 of x\u1d62 in the conventional common subspace-based model can expressed as follows,\n$$z_i = f_f(\\theta_f, r_i^{(1)} * \\delta_i^{(1)}, ..., r_i^{(V)} * \\delta_i^{(V)})$$ $$r_i^{(v)} = f^{(v)}(\\theta_e, x_i^v), v\\in [1, V]$$\nwhere r\u1d62\u207d\u1d5b\u207e is the embedding of the Vth modality. \u03b8f is the parameters for the fusion module. \u03b8e is the parameters for the vth modality encoder. \u03b4\u1d62\u1d5b \u2208 [0, 1] is the Bernoulli indicator for the Uth modality of x\u1d62. It is randomly set to either 0 or 1 to simulate random modality missing. This makes the model robust for incomplete inference data.\nRepresentatively, we take the classification task as an example for analysis here. Let W \u2208 \u211d\u1d9c\u00d7\u1d39 denote the parameters of the final linear classifier. The typical cross entropy loss L_CE(.) for the model is defined as follows,\n$$L_{CE} = -\\frac{1}{N} \\sum_{i=1}^N log \\frac{e^{W_{y_i}g(z_i)}}{\\sum_{k=1}^M e^{(W_kg(z_i))}}$$\nwhere C is the channel dimension Wy\u1d62 means the yx-th column of the W. g(z\u1d62) denotes the feature vector after the global average pooling and flatten operators.\nFor a well-trained model, it needs to minimize the loss of each class. Here, Wy\u1d62 is fixed for y\u1d62-th class. To minimize the loss of class y\u1d62, Wy\u1d62g(z\u1d62) should be maximized."}, {"title": "3.2 Decoupled Multimodal Representation", "content": "As discussed, the representation ability of conventional common subspace-based methods is limited by the intra-class representation constraint for different input combinations. To solve the problem, we introduce the decoupled multimodal representation technique to alleviate the direction constraint on the inference representation by decoupling the training and inference representations. Specifically, it consists of two parts: representation probabilization and distribution regularization.\nRepresentation Probabilization. Different from existing methods that leverage the deterministic embedding in Eq.(1) for task predictors directly, we consider building probabilistic embeddings, i.e. z\u1d62 \u223c p(z\u1d62|x\u1d62), for a more flexible representation space. For simplicity, we define the probabilistic embedding z\u1d62 obeys a multivariate Gaussian distribution,\n$$p(z_i|x_i) = \\mathcal{N}(z_i; \\mu_i, \\sigma_i^2)$$\nwhere both the parameters (\u00b5\u1d62 and \u03c3\u1d62) of the Gaussian distribution are input-dependent predicted. Different from existing methods [8, 33] that estimate \u00b5\u1d62 and \u03c3\u1d62 for the feature vector after pooling, we estimate the \u00b5\u1d62 and \u03c3\u1d62 for the feature map directly. This not only contributes to better performance but also enables the model to handle the dense prediction task, such as segmentation. For a specific, \u00b5\u1d62 and \u03c3\u1d62 are defined as follows,\n$$\\mu_i = f_{\\mu}(\\theta_{\\mu}, z_i)$$\n$$log(\\sigma_i) = f_{\\sigma}(\\theta_{\\sigma}, z_i)$$\n\u03b8\u00b5 and \u03b8\u03c3 are the parameters for f\u00b5(.) and f\u03c3(.), respectively. Here we predict log \u03c3\u1d62 instead of \u03c3\u1d62 directly for a better stability [5, 27]. In detail, we implement f\u00b5(.) and f\u03c3(.) with a simple 1x1 convolution with the batch norm, respectively, which introduces negligible parameters.\nNow, the representation of each sample becomes a stochastic embedding sampled from \ud835\udca9(z\u1d62; \u00b5\u1d62, \u03c3\u1d62\u00b2). Nevertheless, the sampling operation is not differentiable. Thus, we consider the reparameterization trick [20] to enable backpropagation,\ns\u1d62 = \u00b5\u1d62 + \u03b5\u03c3\u1d62, \u03b5\u223c \ud835\udca9(0, I)\nIn general, we sample noise from \ud835\udca9(0, I) and obtain the embedding si following Eq.(7) instead of directly sampling from \ud835\udca9(z\u1d62; \u00b5\u1d62, \u03c3\u1d62\u00b2). Here, si is the embedding for the training of the prediction module and \u00b5\u1d62 is the final embedding for inference.\nIn this way, DMRNet decouples the training and inference embedding. And the cross-entropy loss for the model can be rewritten as follows,\n$$L_{CE} = \\frac{1}{N} \\sum_{i=1}^N log \\frac{e^{W_{y_i}g(s_i)}}{\\sum_{k=1}^M e^{(W_kg(s_i))}}$$"}, {"title": "Distributional Regularization", "content": "This only requires the sampled embedding g(si) to share the same direction with Wy\u1d62. The inference embedding \u00b5\u1d62 of different input combinations belonging to the same class could be non-parallel. This relaxes the direction constraint on the inference representation and enables the model to capture the specific information for different modality combinations. In particular, the value of the \u03c3\u1d62 controls the degree of relaxation. When \u03c3\u1d62 = 0, si will equal \u00b5\u1d62, which degenerates into the vanilla subspace-based methods without relaxation. In contrast, bigger \u03c3\u1d62 means a bigger sampling range and the direction constraint from si passed to \u00b5\u1d62 will become weaker.\nDistributional Regularization As discussed, bigger \u03c3\u1d62 will lead to weaker constraints. Therefore, the model tends to predict a bigger \u03c3\u1d62 to improve the model's ability to capture the specific information of different modality combinations, improving the representation ability. However, bigger \u03c3\u1d62 will also introduce high uncertainty, which would hinder the optimization of W. Therefore, we need to introduce a regularization term for \u03c3\u1d62 to limit its range. Inspired by previous probabilistic embedding methods [8, 33], we introduce a regularization term during the optimization to explicitly constrain the distance between \ud835\udca9 (z; \u00b5\u1d62, \u03c3\u1d62\u00b2) and normal Gaussian distribution, \ud835\udca9(0, I),\n$$L_{DR} = \\frac{1}{N} \\sum_{i=1}^N KL [\\mathcal{N} (z_i; \\mu_i, \\sigma_i^2) ||\\mathcal{N}(\\epsilon | 0, I)]$$\n$$= \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2}(1 + log \\sigma_i^2 - \\mu_i^2 - \\sigma_i^2)$$\nwhere KL(.||.) means the KL divergence."}, {"title": "3.3 Hard Combination Regularizer", "content": "Since DNNs tend to first memorize simple samples before overfitting hard samples [1, 11, 41]. The common subspace model tends to fit the discriminative modality combinations first. Consequently, their performance will significantly decline when facing hard modality combinations. To handle this unbalanced training issue, we introduce the hard combination regularizer. It mines the hard modality combination relying on the estimated \u03c3\u1d62 and then introduces independent gradient paths for them to regularize their optimization. Compared with the traditional regularization methods [7, 40, 43] that use additional modules, this regularizer shares the parameters with the prediction module of DMRNet, and thus introduces no extra parameters.\nHard Combinations Mining. Since the model tends to fit the input from discriminative modality combinations first, the hard combinations will have a bigger variance than the discriminative combinations that are well optimized by LDR. And we can mine the hard combinations through simple variance ranking.\nSpecifically, given V modalities, we can obtain 2^V modality combinations by setting the components in {\u03b4\u2081\u207d\u00b9\u207e, ...\u03b4\u2081\u207d\u1d5b\u207e } as 0 or 1 randomly. Here, we calculate the variance set D = {d\u2081, ..., d\u2082\u1d5b\u207b\u00b9} of all combinations among the training dataset,\n$$d_j = \\frac{1}{N_j * c* h* w}\\sum_{p=1}^{N_i} \\sum_{c} \\sum_{h} \\sum_{w} (\\sigma_i^2)^2$$\nwhere j = \u2211(2^(k-1) * \u03b4^k), 1 <= k <= V, N\u2c7c denotes the number of the jth combination, c, h, w denote the channel, height and width of the estimated variance.\nFor a fair comparison with existing methods, we also select V combinations as the hard ones for regularization. Thus the hard combinations set H consisting of the combinations \u0394\u1d62 that have the top-V variance in D.\nHard Combinations Regularization As shown in Fig. 2, to encourage the model to pay more attention to the hard modality combination input, we introduce an auxiliary predictor to guide it to decide the hard modality combination inputs independently. Specifically, given a sampling embedding si coming from the modality combination \u0394\u1d62, it calculates the hard combination regularizer loss L_HCR for xi as follows,\n$$L_{HCR} = \\begin{cases} L_{CE}(f_r(g(s_i)), y_i)  & \\Delta_i \\in H \\\n0 & \\Delta_i \\notin H \\end{cases}$$\nwhere fr is the mapping function that converts the embedding into output required by the task loss function. It shares the parameter with the ft in the task predictor."}, {"title": "3.4 Total Loss", "content": "The total training loss L for the DMRNet is defined as follows,\n$$L = L_{TTL} + \\alpha L_{DR} + \\beta L_{HCR}$$\nwhere LTTL is the traditional target task loss for input data. \u03b1 and \u03b2 are the hyperparameters. Specifically, \u03b1 controls the level of relaxation, and \u03b2 controls the regularization degree for hard modality combinations. A detailed analysis of them can be seen in Section 4.4."}, {"title": "3.5 Relationship to Prior Work", "content": "Some concurrent works [5, 33] seem similar to our DMRNet, which also utilize the technique of probabilistic representation. However, there are still differences in motivations and implementations with ours. They leverage probabilistic embedding to deal with the mapping uncertainty caused by input variants. In contrast, we utilize probabilistic embedding to introduce the uncertainty to the multimodal representation proactively, decoupling the training and inference representation to relax the direction constraint caused by the intra-class direction constraint.\nBesides, they estimate the mean and variance for the 1D feature vectors, however, we estimate them for the 2D feature maps directly. This not only contributes to better performance (see ablation experiments in Table 4) but also enables DMRNet to handle the dense prediction task (see ablation experiments in Table 5)."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Performance and Comparison on Face Anti-spoofing Task", "content": "Datasets: We performed experiments on CASIA-SURF [42] dataset. It contains the RGB, Depth, and IR modalities. To evaluate the performance of DMRNet, we followed"}, {"title": "4.2 Performance and Comparison on Audio-Visual Recognition Task", "content": "Datasets: We perform experiments on CREMA-D and Kinetics-Sounds datasets. Both of them contain audio and visual modalities. This helps to evaluate the generalization of the proposed DMRNet to non-visual modalities. Specifically, CREMA-D contains 7,442 video clips for 6 common emotions. Following its benchmark, the whole dataset is randomly divided into a 6698-sample training set and a 744-sample testing set. Besides, Kinetics-Sounds is a large-scale dataset that contains 19k 10-second video clips for 34 actions (15k training, 1.9k validation, 1.9k test).\nComparison: Here, we set OGM-MD as the baseline, which is the variant of OGM [29] by adding the Bernoulli indicator \u03b4 after the modality encoder. We mainly compare DMRNet with the recent SOTA method, ShaSpec [39] and MMANet [40]. The metric is classification accuracy.\nImplementation We unify the backbone and fusion module as those of OGM [29] for a fair comparison. Specifically, for the visual encoder, we take 1 frame as input; for the audio encoder, we slightly change the input channel of ResNet18 from 3 to 1, and the rest parts remain unchanged. We process the audio dataset into a spectrogram of size 257\u00d7299 for CREMA-D and 257\u00d71,004 for Kinetics-Sounds. We use SGD with 0.9 momentum and 1e-4 weight decay as the optimizer. The learning rate is 1e-3 initially and multiplies 0.1 every 70 epochs.\nResult: As shown in Table 2, DMRNet also achieves the best average performance on both datasets. Specifically, it outperforms the ShaSpec and MMANet by 3.41% and 3.69%, respectively, in CREMA-D and by 4.39% and 4.07%, respectively in Kinetics-Sounds. This verifies its effectiveness on non-visual modality tasks. Moreover, compared to the CREMA-D, the performance gain of DMRNet on Kinetics-Sounds is higher, showing its superiority in processing large-scale datasets."}, {"title": "4.3 Ablation Study", "content": "In this section, we first detail the definition of feature diversity. Then we study the effectiveness of the decoupled multimodal representation and hard combination regularizer. Besides, we compare different implementations of distribution estimation and study the generalization of DMRNet to dense prediction tasks.\nMetric for feature diversity. Since the feature diversity is the channel diversity of the feature maps [12], we introduce the inter-channel distance metrics D_channel to evaluate the intra-modality and inter-modality feature diversity. Specifically, given the modality embedding F_o = f(x^(v)), v \\in [1, V], we reshape its shape as R^(c\\times h*w), where c, h, w are the channel number, height, and width of the embedding. The metric is defined as follows,\n$$D_{channel}[k, :] = 1 - (F_o[k,:]/ ||F_o[k,:] ||_2)$$\n$$F_o=F_m * (F_n)^T, m, n \\in [1,V]$$\nwhere the notation [k, :] denotes the kth row in a matrix. If m = n, we can get the intra-modality feature diversity that calculates the cosine distance between the channel coming from nth modality. if m \u2260 n, we can get the inter-modality feature diversity that calculates the cosine distance between the channel coming from mth and nth modalities. Generally, higher D_channel means higher channel difference, meaning richer feature diversity.\nThe effect of decoupled multimodal representation. We conduct comparison experiments with the vanilla SF-MD and its variant with decoupled multimodal representation (DMR). The results are shown in Table 3. We can see that DMR improves the performance of vanilla SF-MD by 2.89% on average. More importantly, DMR brings performance gain on all modality combinations consistently. This demonstrates the effectiveness of decoupling the training and inference representation to alleviate the representation constraints, which enables the model to capture modality-specific information for each modality combination."}, {"title": "Generalization to dense prediction task", "content": "We conduct multimodal segmentation experiments on the NYUv2 [34] dataset to study the generalization of the proposed DMRNet to dense prediction tasks. The dataset comprises 1,449 indoor RGB-D images, of which 795 are used for training and 654 for testing. We use the common 40-class label setting and report our results on the validation set, measured by mean IOU (mIOU).\nwe set ESANnet-MD as the baseline, which is the variant of ESANet [32] by adding the Bernoulli indicator \u03b4 after the modality encoder. for a fair comparison, we unify the backbone and fusion module of DMRNet as those used in ESANet. While RGB-Depth semantic segmentation requires the representation after each block to predict the output, DMRNet only decouples the fusing representation at the penultimate layer to assist model optimization. Otherwise, the optimization process will not converge due to higher uncertainty introduced by multiple random representation samplings.\nAs shown in Table 5, DMRNet outperforms the ESANet-MD on each modality combination consistently and improves the mIOU by 2.46% on average. This shows the generalization of the proposed DMRNet to enhance inference robustness for the dense prediction tasks."}, {"title": "4.4 Parameters Analysis", "content": "The effect of \u03b1. We first investigate the influence of different \u03b1 for DMRNet. Here we use the decoupled multimodal representation only and \u03b2 is set as 0. The result is shown in Table 6. Firstly, the model achieves the best performance when \u03b1 = 1e-3. Secondly, when \u03b1 is set as 0, the model learns a bigger variance (\u03c3\u00b2), i.e. 4.78, and the average ACER (8.18) is even worse than the baseline SF-MD model (7.52) that does not use decoupled multimodal representation. This demonstrates the necessity of distributional regularization for decoupled multimodal representation. Thirdly, when \u03b1 is increasing, \u03c3\u00b2 will decrease, verifying the effect of \u03b1 * L_DR to control the span of estimated distribution. Finally, too large a, such as 1e-1, may cause the convergence problem since the estimated distribution becomes a completely random Gaussian distribution. Thus, for other tasks and datasets, we mainly search for the best parameter \u03b1 among 1e-4, 1e-3, and 1e-2. And for all the CRAME-D, Kinetics-Sounds, and NYUv2 datasets, \u03b1 is set as 1e-3.\nThe effect of \u03b2. We then investigate the influence of different \u03b2 for DMRNet. Here \u03b1 is set as 1e-3. The result is shown in Table 7. Firstly, our method achieves the best performance when the \u03b2 is set as 0.7. Secondly, too small or large \u03b2 may lead to poor performance. On the one hand, too small \u03b2 (0.1) may cause insufficient regularization for hard combinations. On the other hand, too large \u03b2 (0.9) may limit the optimization of the primary loss L_TTL, which will also lead to sub-optimal performance. Thus, for other tasks and datasets, we mainly search for the best parameter \u03b2 among 0.3, 0.5, and 0.7. And \u03b2 for the CREMA-D, Kinetics-Sounds, and NYUV2 datasets is set as 0.3, 0.3, and 0.5, respectively."}, {"title": "5 Conclusion", "content": "In this paper, we reveal the intra-class representation constraint in conventional common subspace-based methods and propose a novel framework DMRNet to assist robust multimodal learning. It consists of two key components: decoupled multimodal representation and hard combination regularizer. Decoupled multimodal representation decouples the training and inference embedding for each sample to relax the constraint on inference embedding. Hard combination regularizer mines and regularizes the hard modality combinations to deal with the unbalanced training of different combinations. Finally, extensive experiments on multimodal classification and segmentation tasks demonstrate the effectiveness of the proposed method for incomplete multimodal learning. The code will be public after acceptance."}]}