{"title": "Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence Modeling for Resource-Constrained Environments", "authors": ["Mohamed A. Taha", "Ahmed.Boin@gmail.com"], "abstract": "Long-range sequence modeling is a crucial aspect of natural language processing and time series analysis. However, traditional models like Recurrent Neural Networks (RNNs) and Transformers suffer from computational and memory inefficiencies, especially when dealing with long sequences. This paper introduces Logarithmic Memory Networks (LMNs), a novel architecture that leverages a hierarchical logarithmic tree structure to efficiently store and retrieve past information. LMNs dynamically summarize historical context, significantly reducing the memory footprint and computational complexity of attention mechanisms from O(n\u00b2) to O(log(n)). The model employs a single-vector, targeted attention mechanism to access stored information, and the memory block construction worker (summarizer) layer operates in two modes: a parallel execution mode during training for efficient processing of hierarchical tree structures and a sequential execution mode during inference, which acts as a memory management system. It also implicitly encodes positional information, eliminating the need for explicit positional encodings. These features make LMNs a robust and scalable solution for processing long-range sequences in resource-constrained environments, offering practical improvements in efficiency and scalability.", "sections": [{"title": "1 Introduction", "content": "The ability to process long sequences is essential for numerous machine learning applications, including natural language processing (NLP), speech recognition, time-series forecasting, and genomics. Efficient handling of long-range dependencies is particularly challenging for traditional sequence models, which often struggle with computational efficiency and memory usage-key considerations for deployment on resource-constrained environments such as mobile or edge devices."}, {"title": "2 Related Work", "content": "Several techniques have been developed to overcome the limitations of the existing methods for long sequence modeling. The next section will review some of them here."}, {"title": "2.1 Recurrent Neural Networks and Their Limitations", "content": "Recurrent Neural Networks (RNNs), including Long Short-Term Memory (LSTM) [5] and Gated Recurrent Units (GRU) [7], were historically the go-to architectures for processing sequential data. These models attempt to capture temporal dependencies by maintaining a hidden state across time steps. However, RNNs are limited by their inability to effectively capture long-range dependencies due to vanishing and exploding gradients. While LSTMs and GRUs mitigate some of these issues, they still struggle with very long sequences. The complexity of training RNNs, as they require multiple passes through the entire sequence to update the hidden state, leading to slower training times and difficulty in parallelization. This sequential nature further exacerbates the challenge of scaling to large datasets or longer sequences."}, {"title": "2.2 Memory-Augmented Networks", "content": "To address the inefficiency of RNNs with long sequences, researchers began exploring memory-augmented networks [21, 22]. The Neural Turing Machine (NTM) [15] and Memory Networks [16] are two notable models in this category. NTMs extend the concept of Turing Machines by using an external memory matrix to store and retrieve information, enabling them to model long-term dependencies more effectively than traditional RNNs. Building upon the NTM, the Differentiable Neural Computer (DNC) [17] introduces additional enhancements, such as dynamic memory allocation and improved addressing mechanisms, further improving memory utilization and scalability. The key complexity of these models arises from the need to access the external memory, which adds a significant computational overhead, especially when scaling to large datasets. Memory retrieval operations, often involving attention mechanisms."}, {"title": "2.3 Transformers and Their Impact", "content": "Transformers [9, 23, 24, 25, 26] revolutionized sequence modeling by introducing a multi-headed self-attention mechanism that allows for the parallel processing of input sequences. Unlike RNNs, Transformers do not rely on sequential processing, making them more efficient for training and inference. However, their reliance on quadratic complexity with respect to sequence length (O(n\u00b2)) imposes a significant computational and memory burden, making them inefficient for very long sequences. The self-attention mechanism compares each token to every other token, leading to memory and computation costs that increase rapidly with the sequence length. This makes Transformers particularly challenging when processing long sequences in real-time applications or on devices with limited resources.\nTo address the limitations of standard Transformers, several Transformer variants have been developed to handle long sequences more efficiently. The key challenge is that the self-attention mechanism in traditional Transformers scales quadratically with the sequence length, which can lead to prohibitively high memory and computational costs for long sequences. Below are some important Transformer variants designed to address this issue:\n\u2022 Transformer-XL: This model introduces recurrence over segments of text, enabling it to maintain memory of previous segments. By retaining information from past segments, Transformer-XL can process longer texts without needing to retrain on past data, making it more efficient for handling long sequences [12].\n\u2022 Longformer: Longformer leverages sparse attention mechanisms, where instead of attending to all tokens in the sequence, it focuses on a smaller subset (e.g., via a sliding window). This reduces both memory and computational costs, allowing the model to process much longer sequences efficiently [13].\n\u2022 Linformer: Linformer takes a similar approach to Longformer by using low-rank approximations for the attention mechanism. This approximation reduces the complexity of attention, enabling the model to handle longer sequences with reduced memory and time requirements [14].\nDespite these innovations, the variants still encounter challenges. While they improve the efficiency of long-sequence processing, they do not entirely eliminate the computational cost and memory limitations. These models still struggle to process extremely long sequences in real-time applications or large datasets, especially in scenarios where computational resources are limited."}, {"title": "2.4 State Space Models (SSMs)", "content": "State Space Models (SSMs) [27, 28] have emerged as a continuous-time approach to sequence modeling. These models treat sequences as states evolving over time and have been shown to improve the stability and computational efficiency of long-sequence processing. Notable works in this area include HiPPO [18], S4 [19], and Mamba [20], which introduce novel formulations to enhance SSM capabilities. The computational complexity of SSMs is typically linear with respect to the sequence length, O(n), which improves upon the quadratic complexity of RNNs and Transformers. However, despite these advancements, SSMs remain challenging to train due to inherent instabilities, such as exploding or vanishing values when processing long sequences. To address this, weights in SSMs often need to be relatively near to 1 for stability; however, even with such constraints, there is no guarantee of stable behavior during training. Additionally, these models require complex optimization techniques and can be computationally expensive in certain scenarios, particularly when parallelized during training."}, {"title": "3 Methodology (LMNs)", "content": "In this section, a detailed explanation of Logarithmic Memory Networks (LMNs), the proposed model, is provided. The core idea of LMNs is the use of a logarithmic tree structure to represent memory. In the following sections, the architecture, the memory construction process, the summarization operation, and the attention mechanism will be discussed."}, {"title": "3.1 Architecture Overview", "content": "The Logarithmic Memory Network consists of several key modules designed to efficiently model long-range dependencies through a hierarchical memory structure. The architecture integrates an embedding layer, memory construction, attention mechanism, and output generation, as shown in Figure 2a. Each of these modules plays a crucial role in processing and transforming information within the network.\n1. Input Embedding: The input data is passed through an embedding layer, which maps the raw input into a fixed-dimensional vector space, providing a structured representation for further processing.\n2. Memory Construction: The embedded representation is used to construct a memory structure, which is organized in a hierarchical tree format. This allows the model to capture multi-level dependencies across the input.\n3. Single Vector Attention: A single-vector attention mechanism is employed to access the relevant information stored in the hierarchical memory. This mechanism enables the model to focus on important features across long-range sequences.\n4. Output Generation: The results from the attention mechanism are combined with the original input data, generating the final output. This output is a transformed version of the input, considering both short and long-range dependencies."}, {"title": "3.2 Memory Construction", "content": "The memory construction process is crucial for LMN's efficiency. The process constructs the hierarchical tree by summarizing past information, which reduces the memory footprint and provides logarithmic access time to the memory. LMNs provide two ways to perform memory construction: sequential and parallel. The two methods both construct a similar tree-like memory, with the main difference being the process of construction."}, {"title": "3.3 Summarizer Layer", "content": "The summarizer layer plays a crucial role in condensing information during the memory construction phase. It leverages a linear projection to combine two memory nodes into a single node, effectively summarizing the information from both inputs. Specifically, the layer takes the concatenation of two nodes, represented as a vector of size 2E where E is Embedding, and outputs a new node of size E in the subsequent memory location. This output node contains the combined information from the two input nodes, but the memory location is not updated after each operation. Instead, the memory update depends on the position within the sequence.\nAn important feature of the summarizer layer is its ability to encode relative position information. The input to the summarizer is processed through a linear layer, which inherently captures the features of the nodes along with their relative positions in the sequence. As the memory construction progresses through the sequence, higher-level memory locations begin to encode a more generalized understanding of previous summaries. These summaries retain information about relative positions, which helps in understanding the current position of each summary in relation to its previous context. This enables the model to maintain a coherent sense of the sequence, where each new summary not only integrates prior information but also understands its own relative position within the larger context.\nThis mechanism facilitates the creation of a hierarchical structure where, in higher-level memory locations, the relative positions of earlier summaries are encoded. This encoding is crucial for maintaining the integrity of the sequence, allowing the model to form a unified concept of the entire sequence by the time it reaches the final memory layer."}, {"title": "3.3.1 Parallel Memory Construction", "content": "In parallel mode, memory is constructed in a single pass by hierarchically summarizing the past inputs using the 'parallel' function, as shown in Figure 3.\n\u2022 Initialization: Input sequence X with a shape of [B, L, E] serves as the lowest level (level 0) of the tree.\n\u2022 Iterative Summarization: In each level i, the input sequence of the previous level Xi-1 is grouped in pairs. These pairs are passed to the summarizer layer to produce a summarized representation for the current level.\n\u2022 Expanding and Aligning: To keep the same size of the input sequence, the generated summarized nodes are repeated to align with the input sequence. The repeating ensures that the number of nodes remain the same for every memory level.\n\u2022 Result: The function returns the memory with the shape [B, L, log(L), E]."}, {"title": "3.3.2 Sequential Memory Construction", "content": "In sequential construction mode, memory is constructed iteratively, by processing the input sequence sequentially, and summarizing past information when needed. The process is described below and visualized in Figure 4.\n\u2022 Initialization: The initial memory is an empty list. A pointer is used to track the current position within the sequence. It is initialized to 0."}, {"title": "3.4 Single Vector Attention", "content": "The attention mechanism is a crucial component of LMNs, enabling the model to selectively access past information stored in the logarithmic memory. The single-vector attention mechanism works in the following way, also shown in Figure 5:\n\u2022 QKV projection: The memory tensor with shape [B, L, log(L), E] is passed through the linear layer 'qkv' to generate the query (Q), key (K) and value (V). The result is then masked based on the non-zero values of the original input to keep the same zero values if there were any padding if parallelized.\n\u2022 Score Calculation: Attention scores are calculated using the single-vector attention mechanism. The score for each memory level is calculated by performing a matrix multiplication of Q and the transpose of K while only taking the first vector from K which is the current token. The resulting scores have a shape of [B, L, log(L)]. This drastically reduces the complexity of the attention calculation with log(sequence).\n\u2022 Normalization and Masking: Scores are normalized by dividing the square root of the Embedding E to ensure stable training. The scores are masked by setting non-valid locations to negative infinity to avoid zero padded sequence during softmax.\n\u2022 Weighted Sum: the scores are converted to attention weights using a softmax, which is used to perform a weighted sum of V to produce the output of attention with shape [B, L, E].\n\u2022 Reduce Parameters: The feed forward layer can use the same embedding for both of its layers, effectively reducing the number of parameters. While this approach does not yield the exact same results, it still outperforms GPT-2, achieving better performance with only half the number of parameters."}, {"title": "3.5 Multi-Bank Memory", "content": "To enhance memory performance, multiple memory banks can be utilized to store additional information. In sequential mode or hierarchical tree construction, the summarizer layer is employed to create the memory. Multiple memory banks or trees are generated in parallel using different summarizer layers, with each layer creating its own memory. Subsequently, the single-vector attention mechanism combines all the individual memories into one unified, larger memory, as shown in Fig. 6."}, {"title": "3.6 Path-Through Positional Encoding", "content": "Unlike transformers, no explicit positional encoding is added in LMNs. Instead, positional information is implicitly encoded through the hierarchical tree structure. As shown in the figure 7, the summarization layer encodes relative positions as the paths tokens take during the summarization process. In parallel mode, while in sequential mode, it is derived from the summarization process within the memory structure. This hierarchical encoding effectively embeds positional context corresponds to the path through, eliminating the need for external positional encodings."}, {"title": "3.7 Expander Summarizer Architecture", "content": "The Expander Summarizer Architecture addresses the summarization bottleneck in long-sequence processing by emulating how humans summarize information. When reading, presenting, or learning, humans initially focus on detailed content but gradually shift to retaining high-level summaries of earlier material. This approach allows them to manage cognitive load, prioritizing new information while maintaining a condensed outline of prior content. For instance, a reader progresses from understanding chapter details to recalling only key ideas, and a teacher emphasizes core concepts over time while condensing earlier material.\nThe architecture mirrors this process by summarizing older content into hierarchical memory levels while keeping recent information detailed. To mitigate the loss of specific details in heavily summarized distant content, it introduces an expander layer that dynamically increases memory capacity at deeper levels, enabling better retention of critical information. This strategy effectively balances abstraction and detail, much like how the human brain summarizes and recalls information over time.\nAlthough this method closely mimics human memory behavior, it does not fully replicate the ability to retrieve specific details unaffected by summarization. While expanded memory reduces information loss, humans uniquely retain exact details alongside high-level summaries, a capability the model approximates but cannot entirely match. Future efforts aim to develop a mechanism for storing critical information intact, without being affected by the summarization process, to achieve a more complete imitation of human cognitive functions.\nThis is achieved using a 1D Transposed Convolution, parameterized by an expansion factor that determines the number of extra slots added. By default, the expansion factor is set to 1 but can be adjusted as needed to accommodate longer sequences or higher information retention. This mechanism provides flexibility and enhances memory capacity without significantly increasing the network's width. The process is illustrated in Figure 8."}, {"title": "4 Implementation Details", "content": "The implementation of the proposed \u2018LogarithmicMemory' is done using PyTorch and is provided in the repository for transparency and reproducibility. The core implementation is modular, making it easy to integrate into various sequence modeling tasks."}, {"title": "4.1 Training Process", "content": "During training, the memory is constructed in parallel (described in Section 3.3.1). This approach leverages the computational power of GPUs to handle the heavy workload associated with training on very long sequences. Parallel computation ensures efficiency by reducing the time complexity of memory construction during the forward pass.\nTo enable this, the LogarithmicMemory implementation includes a parallelize flag that can be set to True. When this flag is active, the parallel memory construction method is automatically invoked during the model's forward pass, allowing seamless integration of GPU-accelerated operations into the training pipeline."}, {"title": "4.2 Inference Process", "content": "Inference is optimized for resource-constrained environments by using the sequential memory construction method (described in Section 3.3.2). This method dynamically updates the memory at each step of the inference process, functioning like a recurrent model. The memory storage acts as a feedback hidden state, which makes it highly efficient for scenarios requiring low computational and memory overhead.\nDuring the forward pass of the model in inference mode, the sequential method is invoked, providing a lightweight solution to long-sequence modeling. This enables the system to perform efficiently even on devices with limited resources."}, {"title": "4.3 Testing and Benchmarking", "content": "The repository includes a suite of built-in tests integrated directly into the core implementation. These tests ensure the functionality, correctness, and performance of the \u2018LogarithmicMemory' module. Additionally, a set of Jupyter notebooks is provided for benchmarking, focusing on inference time and memory footprint for long sequences. Another notebook compares the training and validation loss of \u2018LogarithmicMemory' with attention-based mechanisms (e.g., GPT-2), demonstrating that it achieves competitive or even better results with a significantly smaller number of parameters."}, {"title": "5 Results", "content": "This section presents an analysis of Logarithmic Memory Networks (LMNs) performance compared to GPT-2 and other models. Evaluations were conducted in two setups:\n\u2022 Apple M1 Chip (8-core CPU, 7-core GPU, 8GB unified RAM): Computations used the Metal Performance Shaders (MPS) backend in PyTorch.\n\u2022 Google Colab (Free Tier with T4 GPU): Some experiments were conducted using Nvidia Tesla T4 GPUs.\nEvaluations focused on parameters, training/validation loss, inference time, computational complexity, and memory usage."}, {"title": "5.1 Model Comparison: Parameters and Losses", "content": "Table 1 compares LMNs with 2 banks, GPT-2, TinyLogMem (Feed-Forward without embedding expansion) with 2 banks and Expander Summarizer Architectures 1 bank & 1 expander. The same number of memory banks can be used or set to 1 across all models, with efforts made to ensure that all models have nearly similar number of parameters while maintaining comparable performance. The models were trained on the Tiny Shakespeare dataset, with a batch size of 16, block size of 512, and a maximum of 5000 iterations. The learning rate was 1 \u00d7 10-3, with 200 evaluation iterations.\nLMNs have slightly more parameters than GPT-2 but outperform it in both training and validation loss. TinyLogMem, with fewer parameters, still performs competitively. Increasing the embedding size (128) improves Tiny version performance while keeping parameter counts lower than GPT-2.\nFurther testing with other open-source architectures could be conducted, though this diverges from the primary objective of reducing computational complexity and memory footprint. The results are presented to ensure the model operates as expected and does not exhibit poor performance. Although outperforming existing models was not the focus, the model demonstrates promising results on a smaller scale, with the expectation that such performance may be maintained in larger-scale implementations."}, {"title": "5.2 Computational Overhead", "content": "LMNs reduce computational complexity from O(n\u00b2) to O(log(n)) with their hierarchical tree structure, The complexity should ideally be O(log(n)\u00b2). However, since single-vector attention only needs to focus on the original token, no additional attention is required. This reduces the computational complexity to O(log(n)), in the worst case it could be O(k/2 \u22c5 bank \u22c5 log2(n)).\nFor example, for n = 1024, the compression factor is:\n\\frac{1024^2}{\\log_2(1024)} = 104,857.6 \\text{ or } 1,048,576% (1)\nFor n = 8192, the compression factor becomes:\n\\frac{8192^2}{\\log_2(8192)} = 5,162,220.3 (2)\nThis reduction becomes more pronounced for longer sequences, making LMNs ideal for long-range dependencies. If QKV values are cached, complexity reduces further to O(n) and O(log(n))."}, {"title": "5.3 Inference Time and Memory usage", "content": "In attention, it performs similarly to LMNs for short sequences. However, its performance declines significantly as sequence length increases. In sequential mode, we observed performance degradation beyond a sequence length of 2048 with embedding of 32, at which point the processing time became prohibitive. As a result, we shifted to parallel mode at sequence length 2048 and benchmarked up to a maximum of 32,768 due to memory limitations. Traditional attention failed to handle sequences longer than 32,768 due to memory constraints.\nLMNs demonstrate stable performance across all sequence lengths in both sequential and parallel modes. In sequential mode, LMNs efficiently manage longer sequences, while in parallel mode, they scale without memory issues, unlike traditional attention. Despite memory and time limitations, LMNs outperform traditional attention, particularly for longer sequences, as shown in Figure 10, where LMNs consume significantly less memory, especially for extended sequences. While traditional attention may be more efficient for shorter sequences, LMNs reduce both memory footprint and computational overhead, providing a more resource-efficient solution for longer sequences, making them ideal for resource-constrained environments such as mobile or edge devices."}, {"title": "5.4 Key Insights", "content": "LMNs achieve competitive results in terms of parameter efficiency and loss performance compared to GPT-2, with their hierarchical memory structure reducing computational overhead and memory usage, particularly for long sequences. LMNs offer higher efficiency without a significant performance sacrifice. They also provide a significant improvement in computational complexity, making them ideal for mobile and edge computing applications, particularly in long-range sequence processing tasks."}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Optimization Challenges and Strategies", "content": "While the LMNs architecture demonstrates considerable potential, its full benefits have not yet been realized due to the need for further optimization. The hierarchical memory structure requires fine-tuning to fully exploit its capabilities. Several modifications have been tested, some producing better results, demonstrating the architecture's versatility and potential for enhancement across different configurations. These adjustments, such as increasing the depth of the summarization layer, replacing depthwise separable convolutions with standard 1D convolutions, and experimenting with normalization techniques, show significant improvements in model performance. However, they come with trade-offs in computational cost, increased parameter count, and are less suitable for resource-constrained environments. Additionally, approaches like increasing embedding dimensions, expanding memory banks, and scaling multi-head attention can boost capacity, but at the cost of additional computational load.\nInterestingly, reducing the feedforward dimension from four times the embedding size to the original size has yielded satisfactory results, lowering both parameter count and computational overhead while maintaining performance. This adjustment proves beneficial in cases where resource efficiency is critical."}, {"title": "6.2 Improved Memory Management and Model Flexibility", "content": "For potential optimization, extending the summarization layer to summarize more than two tokens at once could reduce the number of memory locations required, leading to a more compact memory representation. Additionally, making the tree structure in the summarization layer more flexible by experimenting with different kernel sizes and varying strides could improve the robustness and adaptability of the architecture, enabling the model to capture complex, hierarchical relationships between tokens. However, increasing the stride of the kernel beyond three means each node in the tree could have more than three branches, requiring the encoding of relative positions of tokens within these branches. In contrast to traditional position encodings, the summarization layer dynamically encodes relative positions based on past summaries, which reduces the need for absolute positional encodings and allows the network to focus on relative dependencies."}, {"title": "6.3 Challenges in Long Sequence Handling and Future Directions", "content": "Despite these optimizations, large language models still face challenges when dealing with long sequences due to the quadratic growth of the attention mechanism. The inherent quadratic complexity of the attention mechanism becomes a bottleneck for tasks requiring long-range dependencies. The mixture of experts (MoE) [29, 30, 31] approach can reduce computational overhead by selectively activating subsets of model parameters, but it does not fully address the challenges in processing long sequences. Further optimization is necessary to overcome these challenges.\nThese optimizations often come with trade-offs, such as higher computational costs and increased memory requirements. However, LMNs show promise beyond the long-sequence problem and could advance language models in applications. With further optimization, LMNs could enable the development of intelligent personal assistants with greater efficiency and contextual awareness, opening doors to smarter, more resource-efficient AI systems for everyday applications in computationally constrained environments."}, {"title": "7 Conclusion", "content": "This paper introduces Logarithmic Memory Networks (LMNs), a novel architecture designed to address the challenges of long-range sequence modeling in natural language processing and time series analysis. LMNs utilize a hierarchical logarithmic tree structure that efficiently stores and retrieves past information, significantly reducing the memory footprint and computational complexity compared to traditional models like RNNs and Transformers. The model operates in two modes: a parallel execution mode during training to enable faster processing, and a sequential execution mode during inference to optimize memory usage. By incorporating a single-vector attention mechanism and implicitly encoding positional information, LMNs eliminate the need for explicit positional encodings, further reducing computational cost. These features make LMNs a scalable and efficient solution for processing long-range sequences, particularly in resource-constrained environments such as mobile and edge devices. The architecture demonstrates strong potential in advancing sequence modeling tasks and offers a path toward deploying intelligent, real-time AI systems that can operate efficiently, thus enabling more efficient and context-aware AI applications in real-world scenarios."}]}