{"title": "AutoRDF2GML: Facilitating RDF Integration in Graph Machine Learning", "authors": ["Michael F\u00e4rber", "David Lamprecht", "Yuni Susanti"], "abstract": "In this paper, we introduce AUTORDF2GML, a framework designed to convert RDF data into data representations tailored for graph machine learning tasks. AUTORDF2GML enables, for the first time, the creation of both content-based features-i.e., features based on RDF datatype properties and topology-based features i.e., features based on RDF object properties. Characterized by automated feature extraction, AUTORDF2GML makes it possible even for users less familiar with RDF and SPARQL to generate data representations ready for graph machine learning tasks, such as link prediction, node classification, and graph classification. Furthermore, we present four new benchmark datasets for graph machine learning, created from large RDF knowledge graphs using our framework. These datasets serve as valuable resources for evaluating graph machine learning approaches, such as graph neural networks. Overall, our framework effectively bridges the gap between the Graph Machine Learning and Semantic Web communities, paving the way for RDF-based machine learning applications.", "sections": [{"title": "1 Introduction", "content": "Knowledge representation based on RDF is designed to be interpretable by both humans and machines. Integrating RDF with graph machine learning, such as in Graph Neural Network (GNN) approaches, however, presents significant challenges, as RDF differs remarkably from the data representations used in machine learning. The primary challenge lies in modeling entity relationships and attributes as feature vectors, diverging from RDF with its explicit knowledge representation. Additionally, the inherent heterogeneity (variety of entity and relation types) and sparsity of RDF data (few relations per entity) potentially affect the consistency and robustness of the learning process [50,51].\nExisting frameworks for preparing RDF data for graph machine learning (GML) tasks typically lack the capability to transform RDF data into a propositionalized format, such as a feature matrix format. Instead, they convert RDF data into a standard feature matrix without considering the graph structure [3]. Thus, they currently ignore both the different entity types and the object properties of RDF instances, which are crucial parts of RDF data.\nFurthermore, current benchmarks in graph machine learning, such as those provided by PyTorch Geometric, differ in the provisioning of node features, i.e., the modeling of nodes. Typically, we can categorize the available node features for datasets for graph machine learning into the following types: (1) content-based natural language descriptions (NLD), (2) other content-based literals (e.g., numeric, categorical, or boolean values), and (3) topology-based features that encapsulate the graph structure [16,36,25]. While existing benchmarks cover both homogeneous and heterogeneous graphs, they focus on different aspects. For homogeneous graphs, they typically prioritize the content-based features, i.e., the node features derived from natural language descriptions (NLD) of node attributes such as their labels and descriptions, while benchmarks for heterogeneous graphs typically prioritize the diversity in the graph structures or topology. As a consequence, there is a significant gap in these benchmarks regarding the consideration of different kinds of semantics and a systematic analysis of their impact on graph machine learning models. This issue becomes evident when evaluating GNN-based models, as they frequently compute topology-based features for benchmarks that do not provide node features for all node types on-the-fly. Thus, analyzing whether a superior performance of a GNN-based model stems from its advanced architecture, or merely from the topology-based node features (which is then feature engineering), presents a significant challenge [35].\nIn this paper, we present AUTORDF2GML, a framework to effortlessly transform any given RDF data into ready-to-use heterogeneous graph datasets for graph machine learning. The generated datasets contain numeric vector features represented in feature matrices as the node features, derived from content-based (i.e., RDF datatype properties) and topology-based (i.e., RDF object properties) information of the RDF data. A notable advantage of the framework is its ability to automatically select and transform content-based features from the RDF data. Our framework allows users who are less familiar in RDF and SPARQL, such as those in the GNN field, to easily leverage RDF data for their research and applications. AUTORDF2GML can be installed via pip install autordf2gml and is easily set-up with a single-file configuration design: users are only required to define the RDF classes and properties, eliminating the need for specifications of complex SPARQL queries. Therefore, it effectively serves as a bridge between the Graph Machine Learning and Semantic Web communities, facilitating an access to a vast amount of Linked Open Data for GML purposes."}, {"title": "2 Related Work", "content": "In this section, we first address the processing of RDF data for use in graph machine learning applications, such as graph neural networks, a process known as propositionalization. Subsequently, we outline heterogeneous graph benchmarks."}, {"title": "2.1 Propositionalization of RDF Data", "content": "Propositionalization of RDF data refers to the task of transforming raw RDF data into the format required by a given learning algorithm, such as a graph neural network [32]. Most data mining algorithms require a feature vector representation of the data as input, thus each instance is represented as a feature vector (f1, f2,...,fn), where the features can be binary, numerical, or nominal values [40,42]. Several approaches to generate such features from RDF data have been proposed."}, {"title": "2.2 Heterogeneous Graph Benchmarks", "content": "Several benchmarks for tasks on heterogeneous graphs (i.e., graphs with several node types) have been proposed (see Table 2). Our comparison includes all heterogeneous graph benchmark datasets provided by PyTorch Geometric, including heterogeneous graph benchmarks from Open Graph Benchmark (OGB) and Heterogeneous Graph Benchmark (HGB) [16,36,25]. For the purpose of our analysis, we categorize node features into three distinct groups: (1) The first category encompasses natural language description (NLD) features. These are content-based features that are derived from textual descriptions given in natural language (e.g., label or description). (2) The second category is referred to as content-based other Literals, denoted as Literals\\NLD. This group includes content attributes that are not natural language descriptions, such as numeric, categorical, or boolean values. Together, both NLD features and Literals\\NLD constitute the broader set of content-based features (literals). (3) The third category diverges from content-based attributes and is focused on the graph structure, i.e., topology-based features.\nFrom Table 2, it becomes apparent that existing graph benchmarks offer either content-based or topology-based node features across all node types, but not both. This means that most benchmark datasets focus on the heterogeneity of graph structures instead of the diversity of the node features. The benchmarks also seldom provide node features for all node types, and when they do, the generation of these features largely depends on the inherent natural language description (NLD) of the elements.\nFurthermore, existing benchmarks do not include separately evaluated topology-based features, which is a remarkable oversight. This is particularly relevant when new graph neural network architectures are developed, as they often calculate topology-based features on-the-fly during evaluations. Thus, it remains unclear whether the performance is a result of advancements in the graph neural network model itself, or due to the optimized feature engineering through the topology-based node features [35]. The issues with the current benchmarks thus motivated us to construct new benchmarks datasets with our proposed AUTORDF2GML framework. We created SOA-SW, LPWC, AIFB, and Linked-MDB based on publicly-available RDF knowledge graphs (see Sec. 4)."}, {"title": "3 AutoRDF2GML", "content": "In this section, we present our new framework AUTORDF2GML that seamlessly transforms RDF data into data representations for graph machine learning tasks. The generated data representations contain numeric vector features represented in feature matrices as node features. The features can be derived from content-based or topology-based information in the underlying RDF data. A notable strength of the framework is its ability to automatically select and transform the content-based features. This enable the users, even those rather unfamiliar with RDF and SPARQL, to utilize RDF data in a straightforward manner. The user experience is further enhanced by a user-centric setup: users are only required to define the RDF classes and properties for node and edge transformation, eliminating the need for complex specifications of SPARQL queries.\nFigure 1 provides an overview of AUTORDF2GML. First, the user supplies an RDF dump file and configuration file, specifying the RDF classes and properties for feature construction. AUTORDF2GML uses the rdflib Python package, thus it supports all common RDF dump formats (e.g., Turtle, N-Triples, JSON-LD). Next, nodes are extracted from the RDF data, and their features are automatically generated based on either content-based or topology-based semantic information. Edges between the nodes are then automatically formed, completing the graph structure integration. The output of AUTORDF2GML is a ready-to-use heterogeneous graph machine learning dataset compatible with graph machine learning frameworks such as PyTorch Geometric [16] and DGL [48].\nIn the following, we outline the two main steps of AUTORDF2GML: (1) Automatic Generation of Nodes and Node Features in Sec. 3.1, and (2) Automatic Integration of Edges and Edge Features in Sec. 3.2."}, {"title": "3.1 Automatic Generation of Nodes and Node Features", "content": "In RDF data, entities belong to specific classes and are uniquely identified by URIs [37]. Given the relevant classes specified in the configuration file, all corresponding entities are extracted to represent the nodes in the resulting graph dataset. This step is necessary for isolating the relevant classes for a specific use-case (e.g., recommendation). Subsequently, AUTORDF2GML provides two approaches to compute the node features: (1) content-based node features, and (2) topology-based node features, outlined in the following."}, {"title": "3.1.1 Content-based Node Features", "content": "After identifying the relevant entities and their corresponding URIs, AUTORDF2GML can generate features using RDF datatype properties. RDF datatype properties link entities to specific types of data, known as literals. These literals hold valuable information about the entity and can serve as important input features for machine learning models. Within its architecture, AUTORDF2GML includes an automatic module tailored to the construction of numeric node features based on available RDF datatype properties. The automatic transformation of RDF datatype properties and their associated literal values into usable vectorized features includes the automatic feature selection and transformation, as outlined in the following:\na) Automatic Feature Selection: Automating the feature selection is necessary because RDF data typically contains a huge number of datatype properties. A manual analysis and evaluation of all datatype properties is time consuming, and especially challenging for data scientists from other disciplines. In addition, feature selection based on RDF datatype properties is a complex task that requires addressing the following challenges:\n1. Property Sparsity: The filling degree of some datatype properties can be extremely sparse.\n2. Identicality and Uniqueness of Values: Datatype properties can include predominantly identical values or, conversely, be characterized by completely unique entries for each entity.\n3. Redundancy: Different properties can sometimes reflect similar information patterns, resulting in high correlation between properties.\nWe do not consider the features with any of the above listed characteristics in feature selection because they distort the underlying data dynamics, lack the necessary variance, or pose a risk of overfitting due to redundant information [37]. Therefore, it is necessary to pre-process the available datatype properties for feature generation and only select datatype properties that do not break into any of the mentioned characteristics. The discarding of properties with unique values is only applied to nominal features that are not an NLD [37]. If the values of certain selected properties strongly correlate with each other (based on the Pearson correlation score), one of them is discarded [21].\nb) Automatic Feature Transformation: After the relevant features, i.e., the relevant datatype properties, are selected, they need to be transformed into a numeric vector representation to build the node features. AUTORDF2GML distinguishes between 6 literal types and their associate transformation rules (see Table 3). Strings that are natural language descriptions (NLD) are encoded using a text encoder (e.g., a language model like BERT [10] or SciBERT [4]), following a common practice to generate node features [25,26,57]. Categorical values are either one-hot encoded or label encoded depending on the number of unique values. Numeric values and years are normalized. Boolean values are label encoded with 0 and 1. For dates, the unix timestamp is calculated and then normalized. In general, we adhere to the established machine learning feature transformation techniques as outlined in previous works [37,42]. Finally, the features are normalized into a standard range, and not-a-number (NaN) values are filled with their mean, following [21]."}, {"title": "3.1.2 Topology-based Node Features", "content": "Another approach for generating features for RDF graphs is to leverage the topological information of the underlying RDF data. This is because the structure and relationships of the RDF object properties (e.g., relations to other entities) provide a rich semantic information about the data. Topology-based node features are particularly flexible because they retain the complete semantic information from the topological structure, even when using a subgraph with only a small subset of the RDF classes. One widely used approach to obtaining the topology-based representation are knowledge graph embedding techniques. Knowledge graph embedding models such as TransE [6], DistMult [53], ComplEx [46], and RotatE [44] have gained great popularity in recent years due to their effectiveness in representing RDF entities as encoded feature vectors [3,14,43,28]. Following that, AUTORDF2GML automatically computes the topology-based node feature vectors using these widely recognized knowledge graph embedding techniques."}, {"title": "3.2 Automatic Integration of Edges and Edge Features", "content": "To represent a complete graph structure in our data representation for Graph Machine Learning, we need to construct the edges. The edges of the transformed graph are based on RDF object properties. RDF object properties are directed relations that link two entities [50]. Since RDF knowledge graphs may contain several object properties sharing the same range and domain and have similar semantic meanings (e.g. two properties linked with owl:equivalentProperty [34]), these properties might need to be mapped to the same edge type. Thus, AUTORDF2GML enables defining a list of RDF object properties that can bemapped to the same edge type. In the following, we describe four types of RDF object property patterns and how they are used within AUTORDF2GML.\n(a) Binary Relations: In RDF, an object property is a binary relation that links two entities. AUTORDF2GML creates an edge list for each edge type based on the specified RDF object properties in the configuration file. Although the possible subjects (i.e., start nodes) and objects (i.e., end nodes) of a relation can be defined via rdfs:range and rdfs:domain in the ontology [51], our framework does not depend on this information, as we do not perform any OWL reasoning.\n(b) N-ary Relations: N-ary relations [19,49] are used when a simple binary relationship between two entities is insufficient, for instance when we need to model the certainty, strength, or relevance of the relationship, as well.\nThe ontological pattern for modeling additional attributes that describe a relationship involves introducing a new auxiliary class. This class is linked between the subject and the object of the relationship containing additional attributes with information about the relationship [19,49]. Fig. 2 shows an example of n-ary relation between Paper and Author, where the auxiliary class AuthorRelation contains additional information about the author's contribution (e.g., conceptualization, supervison, or software), the author's position (e.g., first author, middle author or last author), and if the author is the corresponding author.\nThe datatype properties of the auxiliary classes that contain information about the relationships are used as edge features in the transformed heterogeneous graph dataset. If the datatype properties do not contain numerical values, they are either one-hot encoded or label encoded.\n(c) Multi-hop Relations: In RDF, two entities can be interconnected through a chain of object properties (see, e.g., Fig. 3). To represent these chained connections directly, edges can be formed between entities that are connected across multiple object properties. This approach simplifies the representation and connects entities that might be several hops apart in the original RDF data [49]. In addition, this further enables new use cases, such as link prediction across multiple properties in the underlying RDF data. Fig. 3 shows an example multi-hop relation from the real-world RDF knowledge graph Linked Papers With Code [13] where Dataset and Method are connected through the properties hasPaper and hasMethod. Given such property chain, a new edge directly connecting Dataset and Method is created, allowing the recommendation of methods for a specific dataset.\n(d) Custom Relations: RDF data can also contain indirect relations between classes that cannot be extracted by linear graph traveling. To extract such complex and non-linear relations and map them as explicit edges in the transformed graph, SPARQL queries can be used that explicitly define the relation."}, {"title": "4 Semantic Graph Machine Learning Benchmarks", "content": "In the following, we present how to apply our framework to large RDF knowledge graphs, considering the semantic features of the knowledge graphs. We provide the resulting Graph Machine Learning datasets publicly available for the community as benchmarks (see links on page 1)."}, {"title": "4.1 SemOpenAlex-Semantic Web (SOA-SW)", "content": "SemOpenAlex [14] is a vast RDF dataset containing over 26 billion RDF triples that describe 249 million publications from various academic disciplines. It features a rich schema and is interconnected with other Linked Open Data sources.\nSOA-SW Knowledge Graph Curation. In line with previous research on dataset curation for graph machine learning, we derive a subgraph of SemOpenAlex [14] based on specific filter rules to create a basis for a graph benchmark dataset for GNN-based recommendations [8,9]. To ensure its validity, SOA-SW contains only SemOpenAlex entities that meet the following conditions: (1) Every author included has at least one semantic web paper published and between 3 and 200 papers published in total. (2) From these authors, only papers with an abstract, publication year > 2005 and citation count > 10 are included. We exclude authors whose papers do not meet these requirements.\nSOA-SW based on the SemOpenAlex version from 2023-04-24, consists of 21,978,026 RDF triples. SOA-SW includes the comprehensive semantic information about these entities as defined in the rich SemOpenAlex ontology, covering 13 entity types, including the entity types works, authors, institutions, sources, publishers and concepts, as well as 87 semantic relation types [14].\nBenchmark Creation. For creating the benchmark, we use the SOA-SW data dump as input for AUTORDF2GML. We also add a custom relation to model the co-author relations directly in the transformed graph (see GitHub). They are not directly included in the underlying RDF data, but can be retrieved using a SPARQL query. Modeling this relationship directly in the data allows to consider a new use case like collaboration recommendation."}, {"title": "4.2 Linked Papers With Code (LPWC)", "content": "Linked Papers With Code (LPWC) [13] is an RDF knowledge graph that provides extensive information on approximately 400,000 publications in the Machine Learning field. It includes details on the tasks addressed, datasets used, methods implemented, and evaluations conducted, along with their results. We use the AUTORDF2GML framework to transform LPWC into a Graph Machine Learning dataset. We construct both content-based and topology-based node features. For the content-based features the following data type properties are selected: (a) Paper (3 out of 7 data type properties selected), (b) Method (4 out of 7 data type properties selected), (c) Task (2 out of 2 data type properties selected), (d) Dataset (7 out of 10 data type properties selected).\nAUTORDF2GML detects NLD features for all node types. The detected NLD features are concatenated, and a 128-dimensional embedding is calculated for the combined data with SciBERT [4]. For the topology-based features, again we use TransE [6], since it gives the best results in the embedding for LPWC [13]. Using the entire LPWC RDF dump, AUTORDF2GML computes 128-dimensional embeddings for all nodes, with all available training data.\nHeterogeneous Graph Dataset LPWC. Figure 5 shows an overview of the schema of the created heterogeneous graph dataset based on LPWC, including the nodes and the edges. In total it is composed of 4 different nodes types and 6 different edge types. Table 6 gives an overview of the number of nodes and the presence of different categories of node characteristics for them. Remarkably, all node types of LPWC have node features from all three categories (Literals\\NLD, NLD and topology). This allows for detailed analyses of the impact of semantic node features on the performance of GNN-based machine learning tasks, such as recommendation tasks. An overview of the number of edges of the different edge types is shown in Table 7."}, {"title": "4.3 Further Benchmark Datasets", "content": "We applied AUTORDF2GML to two other RDF knowledge graphs to show its applicability across various settings and domains. The resulting benchmarks (see page 1) were created from the RDF knowledge graphs AIFB [5], a commonly used dataset for reasoning tasks, and LinkedMDB [22], the RDF version of IMDb, covering movies and related entities such as actors and directors."}, {"title": "5 Applications and Use Cases", "content": "In this section, we outline the impact and use case examples of our framework, demonstrating its utility in both academic research and industry applications.\nEnhanced Accessibility and User-Friendliness for Semantic Data. AUTORDF2GML enables individuals, including both established researchers and newcomers unfamiliar with RDF(S) and SPARQL, to leverage semantic web data without the need for SPARQL queries. This includes a significant number of researchers in the core Machine Learning community, including those focused on areas such as Graph Neural Networks, as well as those involved in explainable AI (XAI) and human-computer interaction (HCI). In the industry, data scientists represent a major user group for our framework, reflecting the growing demand for AI expertise.\nIncreasing Use of RDF Knowledge Graphs and Linked Open Data. The availability of RDF knowledge graphs, particularly in the Linked Open Data cloud, is increasing across various sectors such as e-commerce, academia, and entertainment. AUTORDF2GML supports RDF knowledge graphs without being constrained by any schema restrictions from OWL files or RDFS. We have established benchmarks in these domains as well, allowing for a systematic evaluation of systems such as recommender systems for products [27], scientific papers [2], datasets [15], and movies [29]. These examples not only demonstrate the availability of knowledge graphs but also an industry demand for such resources.\nScalability and Big Data Benchmarking. So far, most benchmarks have been considerably small, often consisting of only a few thousand nodes and edges (see Table 2). In contrast, AUTORDF2GML has been applied to large RDF knowledge graphs, such as LPWC with 8 million RDF triples and LinkedMDB with 6.1 million RDF triples. These benchmarks, along with others easily generated using AUTORDF2GML, are crucial for advancing the field and providing standardized datasets for real-world Machine Learning applications. There is a particular need for large benchmarks that include both content (node features) and structural information (topological features) to enhance AI-based systems.\nEnhancing Recommendation Systems. Graph Machine Learning datasets are increasingly used in various applications, including deep learning-based search and recommender systems. Unlike systems limited to topological data and, thus, collaborative filtering approaches, the graph datasets we have developed enable more precise and higher-performing systems. Initial evaluations of recommender systems using heterogeneous graph neural networks and datasets generated with AUTORDF2GML have demonstrated an improvement in F1 score (see our GitHub repository). In addition, knowledge graph-based recommender systems, as summarized in [52], offer several benefits. For instance, the rich semantic relationships among items in knowledge graphs helps improving item representation [47], and further enhances the interpretability of the recommendation results [55].\nFoundation for Neurosymbolic AI. AUTORDF2GML is well positioned to contribute to the field of neurosymbolic AI and language models. While large language models (LLMs) have been widely developed and utilized, they come with limitations such as knowledge cutoffs and significant hardware requirements. An emerging alternative involves leveraging language models, such as BERT [10] and T5 [38] integrated with knowledge graphs [1,45,54]-an approach sometimes referred as knowledge-guided language models. For instance, [45] introduces approach using smaller LMs combined with KGs that achieve results comparable to or even surpass those of LLM-based methods. Such approach provides capabilities in explaining the model outputs, as well, such as in recommendation systems [55], by linking to KGs as explicit knowledge representations."}, {"title": "6 Conclusion", "content": "In this paper, we introduced AUTORDF2GML, a novel framework designed to efficiently convert RDF data into benchmarks tailored for graph-based machine learning applications, potentially bridging the gap between the Graph Machine Learning and Semantic Web communities. AUTORDF2GML framework is characterized by its modular design, automated feature extraction, and one-file configuration design, making it accessible even to users who may not be familiar with semantic technologies such as SPARQL. Furthermore, we demonstrated the utility of AUTORDF2GML by applying it to large RDF knowledge graphs, successfully transforming them into heterogeneous graph datasets, each enriched with unique semantic features.\nIn the future, we plan to enhance our framework to operate across multiple RDF knowledge graphs within the Linked Open Data cloud in parallel and to incorporate reasoning through OWL concepts. This enhancement will include mechanisms for handling ontological relationships across different knowledge graphs, such as equivalentClass links."}]}