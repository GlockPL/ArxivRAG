{"title": "AutoRDF2GML: Facilitating RDF Integration in Graph Machine Learning", "authors": ["Michael F\u00e4rber", "David Lamprecht", "Yuni Susanti"], "abstract": "In this paper, we introduce AUTORDF2GML, a framework designed to convert RDF data into data representations tailored for graph machine learning tasks. AUTORDF2GML enables, for the first time, the creation of both content-based features-i.e., features based on RDF datatype properties and topology-based features i.e., features based on RDF object properties. Characterized by automated feature extraction, AUTORDF2GML makes it possible even for users less famil- iar with RDF and SPARQL to generate data representations ready for graph machine learning tasks, such as link prediction, node classification, and graph classification. Furthermore, we present four new benchmark datasets for graph machine learning, created from large RDF knowledge graphs using our framework. These datasets serve as valuable resources for evaluating graph machine learning approaches, such as graph neu- ral networks. Overall, our framework effectively bridges the gap between the Graph Machine Learning and Semantic Web communities, paving the way for RDF-based machine learning applications.", "sections": [{"title": "1 Introduction", "content": "Knowledge representation based on RDF is designed to be interpretable by both humans and machines. Integrating RDF with graph machine learning, such as in Graph Neural Network (GNN) approaches, however, presents significant chal- lenges, as RDF differs remarkably from the data representations used in ma- chine learning. The primary challenge lies in modeling entity relationships and attributes as feature vectors, diverging from RDF with its explicit knowledge representation. Additionally, the inherent heterogeneity (variety of entity and relation types) and sparsity of RDF data (few relations per entity) potentially affect the consistency and robustness of the learning process [50,51].\nExisting frameworks for preparing RDF data for graph machine learning (GML) tasks typically lack the capability to transform RDF data into a propo- sitionalized format, such as a feature matrix format. Instead, they convert RDF data into a standard feature matrix without considering the graph structure [3]. Thus, they currently ignore both the different entity types and the object prop- erties of RDF instances, which are crucial parts of RDF data.\nFurthermore, current benchmarks in graph machine learning, such as those provided by PyTorch Geometric, differ in the provisioning of node features, i.e., the modeling of nodes. Typically, we can categorize the available node features for datasets for graph machine learning into the following types: (1) content- based natural language descriptions (NLD), (2) other content-based literals (e.g., numeric, categorical, or boolean values), and (3) topology-based features that en- capsulate the graph structure [16,36,25]. While existing benchmarks cover both homogeneous and heterogeneous graphs, they focus on different aspects. For homogeneous graphs, they typically prioritize the content-based features, i.e., the node features derived from natural language descriptions (NLD) of node attributes such as their labels and descriptions, while benchmarks for heteroge- neous graphs typically prioritize the diversity in the graph structures or topology. As a consequence, there is a significant gap in these benchmarks regarding the consideration of different kinds of semantics and a systematic analysis of their impact on graph machine learning models. This issue becomes evident when eval- uating GNN-based models, as they frequently compute topology-based features for benchmarks that do not provide node features for all node types on-the-fly. Thus, analyzing whether a superior performance of a GNN-based model stems from its advanced architecture, or merely from the topology-based node features (which is then feature engineering), presents a significant challenge [35].\nIn this paper, we present AUTORDF2GML, a framework to effortlessly transform any given RDF data into ready-to-use heterogeneous graph datasets for graph machine learning. The generated datasets contain numeric vector fea- tures represented in feature matrices as the node features, derived from content- based (i.e., RDF datatype properties) and topology-based (i.e., RDF object prop- erties) information of the RDF data. A notable advantage of the framework is its ability to automatically select and transform content-based features from the RDF data. Our framework allows users who are less familiar in RDF and SPARQL, such as those in the GNN field, to easily leverage RDF data for their research and applications. AUTORDF2GML can be installed via pip install autordf2gml and is easily set-up with a single-file configuration design: users are only required to define the RDF classes and properties, eliminating the need for specifications of complex SPARQL queries. Therefore, it effectively serves as a bridge between the Graph Machine Learning and Semantic Web communities, facilitating an access to a vast amount of Linked Open Data for GML purposes."}, {"title": "2 Related Work", "content": "In this section, we first address the processing of RDF data for use in graph machine learning applications, such as graph neural networks, a process known as propositionalization. Subsequently, we outline heterogeneous graph benchmarks."}, {"title": "2.1 Propositionalization of RDF Data", "content": "Propositionalization of RDF data refers to the task of transforming raw RDF data into the format required by a given learning algorithm, such as a graph neural network [32]. Most data mining algorithms require a feature vector rep- resentation of the data as input, thus each instance is represented as a feature vector $ (f_1, f_2,...,f_n) $, where the features can be binary, numerical, or nominal values [40,42]. Several approaches to generate such features from RDF data have been proposed. A comparison of the prominent techniques for the proposition- alization of RDF data is summarized in Table 1, and outlined below.\nCheng et al. [7] present an approach for extracting features from RDF data based on user-specified feature types and SPARQL. The evaluation results sug- gest that utilizing semantic features (e.g., the taxonomy) improves the perfor- mance of the models in comparison to utilizing solely standard features, such as the attributes. However, unlike AUTORDF2GML, no content-based infor- mation (i.e., RDF datatype properties such as descriptions) is used for feature construction and the user is required to define the SPARQL queries manually.\nLiDDM [30] is a framework for data mining on the Semantic Web, and the data is typically retrieved via SPARQL to extract the features. LiDDM supports the integration of data from various Linked Open Data resources alongside a range of pre-processing techniques, including data filtering and data segmenta- tion. However, these operations must be performed manually by the user.\nRapidMiner's semweb plugin [31] follows a similar approach and transforms RDF data into feature vectors, enabling its use within RapidMiner. However, un- like AUTORDF2GML, the user still needs to define SPARQL queries to obtain the desired data.\nFeGeLOD [37] and its successor, the RapidMiner Linked Open Data Exten- sion [39], are techniques for automatically enriching data with features derived from multiple Linked Open Data sources without the need to specify SPARQL queries. However, in contrast to AUTORDF2GML, their main purpose is to enrich an existing dataset with relevant features instead of transforming RDF data into a graph dataset for graph machine learning.\nLiteral2Feature [3] is a framework to automatically transform RDF data into a standard feature matrix by traversing the RDF graph. It starts with a set of entities of interest and automatically retrieves literals to a pre-configured walk length to build the feature matrix. For generating the feature vectors, only the literals are used. Literal2Feature is mainly used to obtain Spark DataFrames as input for conventional machine learning models [33,12]. In contrast to Au- TORDF2GML, no graph structure is used for feature generation.\nIn summary, there is currently no approach available for transforming RDF data into a propositionalized format, i.e., feature matrix, that considers both RDF data type and object properties. Our proposed framework AUTORDF2GML allows the representation of nodes and edges with their corresponding features as vectors. In addition, as with the other automatic approaches, the feature selec- tion and transformation is performed automatically without requiring the user to define any SPARQL queries. The user only needs to define the key aspect of the desired GML datasets in a configuration file (e.g., the node and edge types), and the rest of the processes are handled automatically.\nKnowledge Graph Embeddings. In addition to classical propositionalization meth- ods, knowledge graph embeddings offer an approach to convert entities into dense vector representations. For instance, RDF2Vec [41] transforms RDF graphs into graph random walks and Weisfeiler-Lehman graph kernels, and further applies CBOW and Skip-gram models to learn latent entity representations based on the knowledge graph topology. Other graph embedding techniques includes TransE [6], a translation distance model, DistMult [53] and ComplEx [46], semantic"}, {"title": "2.2 Heterogeneous Graph Benchmarks", "content": "Several benchmarks for tasks on heterogeneous graphs (i.e., graphs with sev- eral node types) have been proposed (see Table 2). Our comparison includes all heterogeneous graph benchmark datasets provided by PyTorch Geometric, in- cluding heterogeneous graph benchmarks from Open Graph Benchmark (OGB) and Heterogeneous Graph Benchmark (HGB) [16,36,25]. For the purpose of our analysis, we categorize node features into three distinct groups: (1) The first category encompasses natural language description (NLD) features. These are content-based features that are derived from textual descriptions given in nat- ural language (e.g., label or description). (2) The second category is referred to as content-based other Literals, denoted as Literals\\NLD. This group includes content attributes that are not natural language descriptions, such as numeric, categorical, or boolean values. Together, both NLD features and Literals\\NLD constitute the broader set of content-based features (literals). (3) The third cat- egory diverges from content-based attributes and is focused on the graph struc- ture, i.e., topology-based features.\nFrom Table 2, it becomes apparent that existing graph benchmarks offer either content-based or topology-based node features across all node types, but not both. This means that most benchmark datasets focus on the heterogeneity of graph structures instead of the diversity of the node features. The benchmarks also seldom provide node features for all node types, and when they do, the generation of these features largely depends on the inherent natural language description (NLD) of the elements.\nFurthermore, existing benchmarks do not include separately evaluated topology- based features, which is a remarkable oversight. This is particularly relevant when new graph neural network architectures are developed, as they often cal- culate topology-based features on-the-fly during evaluations. Thus, it remains unclear whether the performance is a result of advancements in the graph neu- ral network model itself, or due to the optimized feature engineering through the topology-based node features [35]. The issues with the current benchmarks thus motivated us to construct new benchmarks datasets with our proposed AUTORDF2GML framework. We created SOA-SW, LPWC, AIFB, and Linked- MDB based on publicly-available RDF knowledge graphs (see Sec. 4)."}, {"title": "3 AutoRDF2GML", "content": "In this section, we present our new framework AUTORDF2GML that seamlessly transforms RDF data into data representations for graph machine learning tasks. The generated data representations contain numeric vector features represented in feature matrices as node features. The features can be derived from content- based or topology-based information in the underlying RDF data. A notable strength of the framework is its ability to automatically select and transform the content-based features. This enable the users, even those rather unfamiliar with RDF and SPARQL, to utilize RDF data in a straightforward manner. The user experience is further enhanced by a user-centric setup: users are only required to define the RDF classes and properties for node and edge transformation, eliminating the need for complex specifications of SPARQL queries.\nFigure 1 provides an overview of AUTORDF2GML. First, the user supplies an RDF dump file and configuration file, specifying the RDF classes and proper- ties for feature construction. AUTORDF2GML uses the rdflib Python package, thus it supports all common RDF dump formats (e.g., Turtle, N-Triples, JSON- LD). Next, nodes are extracted from the RDF data, and their features are au- tomatically generated based on either content-based or topology-based semantic information. Edges between the nodes are then automatically formed, completing the graph structure integration. The output of AUTORDF2GML is a ready-to- use heterogeneous graph machine learning dataset compatible with graph ma- chine learning frameworks such as PyTorch Geometric [16] and DGL [48].\nIn the following, we outline the two main steps of AUTORDF2GML: (1) Au- tomatic Generation of Nodes and Node Features in Sec. 3.1, and (2) Automatic Integration of Edges and Edge Features in Sec. 3.2."}, {"title": "3.1 Automatic Generation of Nodes and Node Features", "content": "In RDF data, entities belong to specific classes and are uniquely identified by URIs [37]. Given the relevant classes specified in the configuration file, all cor- responding entities are extracted to represent the nodes in the resulting graph dataset. This step is necessary for isolating the relevant classes for a specific use-case (e.g., recommendation). Subsequently, AUTORDF2GML provides two approaches to compute the node features: (1) content-based node features, and (2) topology-based node features, outlined in the following."}, {"title": "3.1.1 Content-based Node Features", "content": "After identifying the relevant en- tities and their corresponding URIs, AUTORDF2GML can generate features using RDF datatype properties. RDF datatype properties link entities to spe- cific types of data, known as literals. These literals hold valuable information about the entity and can serve as important input features for machine learning models. Within its architecture, AUTORDF2GML includes an automatic mod- ule tailored to the construction of numeric node features based on available RDF datatype properties. The automatic transformation of RDF datatype properties and their associated literal values into usable vectorized features includes the automatic feature selection and transformation, as outlined in the following:\na) Automatic Feature Selection: Automating the feature selection is necessary because RDF data typically contains a huge number of datatype prop- erties. A manual analysis and evaluation of all datatype properties is time con- suming, and especially challenging for data scientists from other disciplines. In addition, feature selection based on RDF datatype properties is a complex task that requires addressing the following challenges:\n1. Property Sparsity: The filling degree of some datatype properties can be extremely sparse.\n2. Identicality and Uniqueness of Values: Datatype properties can include pre- dominantly identical values or, conversely, be characterized by completely unique entries for each entity.\n3. Redundancy: Different properties can sometimes reflect similar information patterns, resulting in high correlation between properties.\nWe do not consider the features with any of the above listed characteristics in feature selection because they distort the underlying data dynamics, lack the necessary variance, or pose a risk of overfitting due to redundant information [37]. Therefore, it is necessary to pre-process the available datatype properties for feature generation and only select datatype properties that do not break into any of the mentioned characteristics. The discarding of properties with unique values is only applied to nominal features that are not an NLD [37]. If the values of certain selected properties strongly correlate with each other (based on the Pearson correlation score), one of them is discarded [21].\nb) Automatic Feature Transformation: After the relevant features, i.e., the relevant datatype properties, are selected, they need to be transformed into a numeric vector representation to build the node features. AUTORDF2GML distinguishes between 6 literal types and their associate transformation rules (see Table 3). Strings that are natural language descriptions (NLD) are encoded using a text encoder (e.g., a language model like BERT [10] or SciBERT [4]), following a common practice to generate node features [25,26,57]. Categorical values are either one-hot encoded or label encoded depending on the number of unique values. Numeric values and years are normalized. Boolean values are"}, {"title": "3.1.2 Topology-based Node Features", "content": "Another approach for generating fea- tures for RDF graphs is to leverage the topological information of the underlying RDF data. This is because the structure and relationships of the RDF object properties (e.g., relations to other entities) provide a rich semantic information about the data. Topology-based node features are particularly flexible because they retain the complete semantic information from the topological structure, even when using a subgraph with only a small subset of the RDF classes. One widely used approach to obtaining the topology-based representation are knowl- edge graph embedding techniques. Knowledge graph embedding models such as TransE [6], DistMult [53], ComplEx [46], and RotatE [44] have gained great popularity in recent years due to their effectiveness in representing RDF entities as encoded feature vectors [3,14,43,28]. Following that, AUTORDF2GML auto- matically computes the topology-based node feature vectors using these widely recognized knowledge graph embedding techniques."}, {"title": "3.2 Automatic Integration of Edges and Edge Features", "content": "To represent a complete graph structure in our data representation for Graph Machine Learning, we need to construct the edges. The edges of the transformed graph are based on RDF object properties. RDF object properties are directed relations that link two entities [50]. Since RDF knowledge graphs may contain several object properties sharing the same range and domain and have similar semantic meanings (e.g. two properties linked with owl:equivalentProperty [34]), these properties might need to be mapped to the same edge type. Thus, AUTORDF2GML enables defining a list of RDF object properties that can be"}, {"title": "4 Semantic Graph Machine Learning Benchmarks", "content": "In the following, we present how to apply our framework to large RDF knowl- edge graphs, considering the semantic features of the knowledge graphs. We provide the resulting Graph Machine Learning datasets publicly available for the community as benchmarks (see links on page 1)."}, {"title": "4.1 SemOpenAlex-Semantic Web (SOA-SW)", "content": "SemOpenAlex [14] is a vast RDF dataset containing over 26 billion RDF triples that describe 249 million publications from various academic disciplines. It fea- tures a rich schema and is interconnected with other Linked Open Data sources.\nSOA-SW Knowledge Graph Curation. In line with previous research on dataset curation for graph machine learning, we derive a subgraph of SemOpen- Alex [14] based on specific filter rules to create a basis for a graph benchmark dataset for GNN-based recommendations [8,9]. To ensure its validity, SOA-SW contains only SemOpenAlex entities that meet the following conditions: (1) Ev- ery author included has at least one semantic web paper published and between 3 and 200 papers published in total. (2) From these authors, only papers with an abstract, publication year > 2005 and citation count > 10 are included. We exclude authors whose papers do not meet these requirements.\nSOA-SW based on the SemOpenAlex version from 2023-04-24, consists of 21,978,026 RDF triples. Table 4 shows the number of included entities in SOA- SW. SOA-SW includes the comprehensive semantic information about these entities as defined in the rich SemOpenAlex ontology, covering 13 entity types, including the entity types works, authors, institutions, sources, publishers and concepts, as well as 87 semantic relation types [14].\nBenchmark Creation. For creating the benchmark, we use the SOA-SW data dump as input for AUTORDF2GML. We also add a custom relation to model the co-author relations directly in the transformed graph (see GitHub). They are not directly included in the underlying RDF data, but can be retrieved using a SPARQL query. Modeling this relationship directly in the data allows to consider a new use case like collaboration recommendation.\nWe construct both content-based and topology-based node features for the benchmark. For the content-based node features, the relevant data type proper- ties are automatically selected and subsequently transformed. From all available data type properties, the following are selected: (a) Work (8 out of 18 data type properties), (b) Author (4 out of 10 data type properties), (c) Concept (4 out of 11 data type properties), (d) Source (10 out of 19 data type properties), (e) Institution (5 out of 14 data type properties), and Publisher (5 out of 12 data type properties). Furthermore, for work-author edges, the data type values of the property soa:position and for work-concept edges, the data type values of the property soa:score are used as edge features.\nAutoRDF2GML detects NLD features for the work nodes, specifically the properties dcterms:title and dcterms:abstract. The work titles and ab- stracts are concatenated, and subsequently, a 128-dimensional embedding is gen- erated for this combined data using SciBERT embeddings [4].\nFor the topology-based feature generation, we apply TransE [6], since it yields the best results in embedding evaluation (summarized in our repository). Using the entire SOA-SW RDF dump, AutoRDF2GML computes 128-dimensional em- beddings for all nodes, utilizing all available data for the training process.\nHeterogeneous Graph Dataset SOA-SW. Fig. 4 shows an overview of the schema of the created heterogeneous graph dataset based on SOA-SW KG, including in total 6 different nodes types and 7 different edge types. An overview of the number of nodes and the availability of different categories of node features for them is summarized in Table 4. Table 5 presents the number of edges by edge type and indicates whether they include features. It can be seen that the transformed heterogeneous graph dataset has rich node features capturing different kinds of semantics that is available in the raw RDF data. The semantic node features can then be used for GNN-based machine learning tasks, such as link prediction."}, {"title": "4.2 Linked Papers With Code (LPWC)", "content": "Linked Papers With Code (LPWC) [13] is an RDF knowledge graph that pro- vides extensive information on approximately 400,000 publications in the Ma- chine Learning field. It includes details on the tasks addressed, datasets used, methods implemented, and evaluations conducted, along with their results. We use the AUTORDF2GML framework to transform LPWC into a Graph Ma- chine Learning dataset. We construct both content-based and topology-based node features. For the content-based features the following data type properties are selected: (a) Paper (3 out of 7 data type properties selected), (b) Method (4 out of 7 data type properties selected), (c) Task (2 out of 2 data type properties selected), (d) Dataset (7 out of 10 data type properties selected).\nAUTORDF2GML detects NLD features for all node types. The detected NLD features are concatenated, and a 128-dimensional embedding is calcu- lated for the combined data with SciBERT [4]. For the topology-based features, again we use TransE [6], since it gives the best results in the embedding for LPWC [13]. Using the entire LPWC RDF dump, AUTORDF2GML computes 128-dimensional embeddings for all nodes, with all available training data.\nHeterogeneous Graph Dataset LPWC. Figure 5 shows an overview of the schema of the created heterogeneous graph dataset based on LPWC, including the nodes and the edges. In total it is composed of 4 different nodes types and 6 different edge types. Table 6 gives an overview of the number of nodes and the presence of different categories of node characteristics for them. Remarkably, all node types of LPWC have node features from all three categories (Literals\\NLD, NLD and topology). This allows for detailed analyses of the impact of semantic node features on the performance of GNN-based machine learning tasks, such as recommendation tasks. An overview of the number of edges of the different edge types is shown in Table 7."}, {"title": "4.3 Further Benchmark Datasets", "content": "We applied AUTORDF2GML to two other RDF knowledge graphs to show its applicability across various settings and domains. The resulting benchmarks (see page 1) were created from the RDF knowledge graphs AIFB [5], a commonly used dataset for reasoning tasks, and LinkedMDB [22], the RDF version of IMDb, covering movies and related entities such as actors and directors."}, {"title": "5 Applications and Use Cases", "content": "In this section, we outline the impact and use case examples of our framework, demonstrating its utility in both academic research and industry applications.\nEnhanced Accessibility and User-Friendliness for Semantic Data. AUTORDF2GML enables individuals, including both established researchers and newcomers unfamiliar with RDF(S) and SPARQL, to leverage semantic web data without the need for SPARQL queries. This includes a significant number of researchers in the core Machine Learning community, including those focused on areas such as Graph Neural Networks, as well as those involved in explainable AI (XAI) and human-computer interaction (HCI). In the industry, data scientists represent a major user group for our framework, reflecting the growing demand for AI expertise.\nIncreasing Use of RDF Knowledge Graphs and Linked Open Data. The availability of RDF knowledge graphs, particularly in the Linked Open Data cloud, is increasing across various sectors such as e-commerce, academia, and en- tertainment. AUTORDF2GML supports RDF knowledge graphs without being constrained by any schema restrictions from OWL files or RDFS. We have estab- lished benchmarks in these domains as well, allowing for a systematic evaluation of systems such as recommender systems for products [27], scientific papers [2], datasets [15], and movies [29]. These examples not only demonstrate the avail- ability of knowledge graphs but also an industry demand for such resources.\nScalability and Big Data Benchmarking. So far, most benchmarks have been considerably small, often consisting of only a few thousand nodes and edges (see Table 2). In contrast, AUTORDF2GML has been applied to large RDF knowledge graphs, such as LPWC with 8 million RDF triples and LinkedMDB with 6.1 million RDF triples. These benchmarks, along with others easily gener- ated using AUTORDF2GML, are crucial for advancing the field and providing standardized datasets for real-world Machine Learning applications. There is a particular need for large benchmarks that include both content (node features) and structural information (topological features) to enhance AI-based systems.\nEnhancing Recommendation Systems. Graph Machine Learning data- sets are increasingly used in various applications, including deep learning-based search and recommender systems. Unlike systems limited to topological data and, thus, collaborative filtering approaches, the graph datasets we have de- veloped enable more precise and higher-performing systems. Initial evaluations of recommender systems using heterogeneous graph neural networks and data- sets generated with AUTORDF2GML have demonstrated an improvement in F1 score (see our GitHub repository). In addition, knowledge graph-based rec- ommender systems, as summarized in [52], offer several benefits. For instance, the rich semantic relationships among items in knowledge graphs helps improv- ing item representation [47], and further enhances the interpretability of the recommendation results [55].\nFoundation for Neurosymbolic AI. AUTORDF2GML is well positioned to contribute to the field of neurosymbolic AI and language models. While large language models (LLMs) have been widely developed and utilized, they come with limitations such as knowledge cutoffs and significant hardware require- ments. An emerging alternative involves leveraging language models, such as BERT [10] and T5 [38] integrated with knowledge graphs [1,45,54]-an approach sometimes referred as knowledge-guided language models. For instance, [45] in- troduces approach using smaller LMs combined with KGs that achieve results comparable to or even surpass those of LLM-based methods. Such approach provides capabilities in explaining the model outputs, as well, such as in recom- mendation systems [55], by linking to KGs as explicit knowledge representations."}, {"title": "6 Conclusion", "content": "In this paper, we introduced AUTORDF2GML, a novel framework designed to efficiently convert RDF data into benchmarks tailored for graph-based machine learning applications, potentially bridging the gap between the Graph Machine Learning and Semantic Web communities. AUTORDF2GML framework is char- acterized by its modular design, automated feature extraction, and one-file con- figuration design, making it accessible even to users who may not be familiar with semantic technologies such as SPARQL. Furthermore, we demonstrated the utility of AUTORDF2GML by applying it to large RDF knowledge graphs, successfully transforming them into heterogeneous graph datasets, each enriched with unique semantic features.\nIn the future, we plan to enhance our framework to operate across multiple RDF knowledge graphs within the Linked Open Data cloud in parallel and to incorporate reasoning through OWL concepts. This enhancement will include mechanisms for handling ontological relationships across different knowledge graphs, such as equivalentClass links."}]}