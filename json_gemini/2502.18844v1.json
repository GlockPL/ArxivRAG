{"title": "BARKXAI: A LIGHTWEIGHT POST-HOC EXPLAIN- ABLE METHOD FOR Tree SPECIES CLASSIFICATION WITH QUANTIFIABLE CONCEPTS", "authors": ["Yunmei Huang", "Songlin Hou", "Zachary Nelson Horve", "Songlin Fei"], "abstract": "The precise identification of tree species is fundamental to forestry, conservation, and environmental monitoring. Though many studies have demonstrated that high accuracy can be achieved using bark-based species classification, these models often function as \"black boxes\", limiting interpretability, trust, and adoption in critical forestry applications. Attribution-based Explainable AI (XAI) methods have been used to address this issue in related works. However, XAI applications are often dependent on local features (such as a head shape or paw in animal applications) and cannot describe global visual features (such as ruggedness or smoothness) that are present in texture-dominant images such as tree bark. Concept-based XAI methods, on the other hand, offer explanations based on global visual features with concepts, but they tend to require large overhead in building external concept image datasets and the concepts can be vague and subjective without good means of precise quantification. To address these challenges, we propose a lightweight post-hoc method to interpret visual models for tree species classification using operators and quantifiable concepts. Our approach eliminates computational overhead, enables the quantification of complex concepts, and evaluates both concept importance and the model's reasoning process. To the best of our knowledge, our work is the first study to explain bark vision models in terms of global visual features with concepts. Using a human-annotated dataset as ground truth, our experiments demonstrate that our method significantly outperforms TCAV and Llama3.2 in concept importance ranking based on Kendall's Tau, highlighting its superior alignment with human perceptions. Source code will be released upon acceptance.", "sections": [{"title": "INTRODUCTION", "content": "Forests play a crucial role in climate solutions by sequestering carbon, regulating ecosystems, and supporting biodiversity. Tree species identification, as an essential means of forest management, helps understanding forest composition, and ecosystems. With the rapid advancement in AI, tree species identification automation with deep neural networks is increasingly gaining attention. Many studies (Carpentier et al., 2018; Bertrand et al., 2018; Robert et al., 2020; Wu et al., 2021; Yamabe & Saitoh, 2022) have demonstrated that high accuracy in tree species identification can be achieved using bark images, which are available year-around compared with leaf, flower, and fruits. However, most of the existing studies fall short of explaining the reasoning process behind their models. The inherent complexity of these models often renders them black boxes, making it difficult to understand. This lack of transparency can hinder trust and transferability in the model's output, especially in critical applications like forestry, where accurate and reliable species identification is important (Onishi & Ise, 2021; Cheng et al., 2022; H\u00f6hl et al., 2024).\nConsidering the intra- and inter-species complexity of tree bark images, understanding deep neural networks for tree species classification is important to unveil the black box and provide insights for future applications. To address these challenges, Explainable AI (XAI) has emerged to enhance transparency and trust in deep neural networks by shedding light on their reasoning processes (Mostafa et al., 2023). Existing \u03a7\u0391I methods can be broadly categorized into attribution-based XAI (which focus on attributing importance to input features that drive model decisions) and concept-based XAI (which interprets models using high-level human-understandable concepts).\nAttribution-based XAI methods such as Crown-CAM (Marvasti-Zadeh et al., 2023), and Grad-CAM (Onishi & Ise, 2021; Ahlswede et al., 2022; Kim et al., 2022; Huang et al., 2024) have been used to visualize which image regions contribute to classification decisions in aerial and ground imagery and improve the explainability of tree species identification. Similarly, Shapeley Additive explanations (SHAP) (Lundberg, 2017), have been utilized in species richness modeling(Brugere et al., 2023), urban vegetation mapping (Abdollahi & Pradhan, 2021) and mulberry leaf disease classification (Nahiduzzaman et al., 2023). Local Interpretable Model-Agnostic explanations(LIME) has been applied to provide localized visual interpretations in medicinal plant species identification (Nikam et al., 2022) and microscopic wood classification (Zhan et al., 2023).\nWhile attribution-based methods are more popular in explaining vision models, they usually cannot provide satisfactory results when interpreting vision classifiers on bark images due to a lack of prominent local features. Attribution-based methods work best on natural images containing distinct local features such as animals (head, paws, tails) and vehicles (wheels, doors) by providing explanations that highlight these local features, which are easily verifiable by users. However, for texture-dominant images that lack prominent local features, such as tree bark, these methods struggle to provide explanations that align with human reasoning. Contrary to the explanation from attribution-based methods, domain experts typically rely on global visual characteristics such as stripe patterns, roughness, and surface irregularities\u2014to distinguish tree species, which are not revealed by attribution-based methods. The gap in explanation is further illustrated in Figure 1.\nIn contrast, concept-based XAI (Kim et al., 2018; Chen et al., 2019) methods offer a more intuitive approach to interpreting models using high-level, human-understandable concepts to explain model decisions, which aligns better with our intuition when distinguishing tree species based on bark images (Kazhdan et al., 2020; Sagar et al., 2023). However, extant concept-based models typically require pre-defined concepts and external effort in collecting images for each concept, which quickly becomes labor-extensive when the number of concepts is large(Kim et al., 2018; Hou et al., 2024). One exception is ProtoPNet(Chen et al., 2019), which eliminates the requirements of collecting external concept images by using image patches from training set as prototypes to provide concept-based explanations. However, the way image patches are used as concepts still limits the ability to describe global visual features, like attribution-based methods. Besides, concepts that exhibit a clear hierarchical relationship (e.g., light red vs. deep red) or require precise quantification (e.g., 30-degree vs. 60-degree inclination) pose significant challenges for existing concept-based methods since accurately defining and representing such concepts using external concept images is vague and subjective.\nTo provide better explanation on bark visual models that matches the intuition of domain experts while mitigating the limitations discussed above, we propose BarkXAI, which is a lightweight post-hoc concept-based method optimized for bark image classifiers. To the best of our knowledge, few, if any, of the studies have adopted concept-based XAI methods when interpreting vision models on bark images. Our work is the first to provide a concept-based explanation on global visual features on vision models on bark image or texture-dominant images identification. Our main contributions are listed as follows."}, {"title": "2 BARK\u03a7\u0391\u0399: PROPOSED METHOD", "content": "Given the limitations in existing XAI models for explaining vision models, we aim to address how to design an XAI method that: 1) Can explain any trained black-box texture vision models, 2) Uses quantifiable concepts meaningful for texture analysis, and 3) Minimizes additional computational overhead. To solve this, we propose BarkXAI, a novel XAI approach optimized for image classifiers trained on texture images (e.g., bark images). Inspired by LIME and TCAV (Kim et al., 2018), our method uses perturbed images to assess the impact of concept-of-interest on classification. Unlike LIME, which relies on superpixel segmentation, we focus on global visual features (e.g., smoothness or tone) that are visually significant for texture analysis. These features are extracted and perturbed, with impacts analyzed using surrogate models like linear regression or decision trees. Similar to TCAV, our method explains model decisions through concepts but uses parameterized operators instead of externally curated datasets, enabling efficient concept evaluation without extensive dataset preparation. Our proposed approach is illustrated in Figure 2."}, {"title": "2.1 GLOBAL VISUAL FEATURES", "content": "Global visual features in images refer to attributes or descriptors that depict the overall structure or appearance of an entire image rather than concentrating on specific objects or localized regions. These features encompass high-level concepts and they can be categorized into the following types.\n\u2022 Color: Features of colors can be reflected in several ways, such as color histogram and color correlograms. They reflect the distribution or correlation of pixel colors across the image. For bark images, color can be an important indicator for species classification.\n\u2022 Texture: Patterns or surface properties, such as smoothness, coarseness, or regularity, are utilized to characterize textures. In conjunction with variations in illumination conditions, smoothness, and coarseness can provide insights into the evenness or roughness of the surface. In the case of bark images, the presence of ridges and cracks serves as important indicators of tree species. These features also contribute to estimating the age and health condition of the trees.\n\u2022 Shape: The geometric structure of the texture surface can be described by properties such as lines, edges, and circles, among others. The dominant orientation or directionality, along with the presence of these features, provides valuable insights into the texture type.\n\u2022 Groove and surfaces: Grooves in tree bark typically exhibit significant visual activity that can be meaningful to species classification. More complex surfaces are characterized by a greater number of edges, variations, and detail while simpler surfaces tend to display fewer visual elements and greater uniformity."}, {"title": "2.2 \u0421\u043e\u043cMUTATIVE OPERATORS", "content": "To extract global visual features from texture images, we introduce a collection of operators F, which serve as feature extractors. Each operator $f \\in F$ is generally designed as an unary function, which takes one input x and outputs x', which is a perturbed version of x. For some operators $f \\in F$, there might be a parameterized version that can take a vector of parameters such as $f_\\theta$.\nEach of the operators ($f \\in F$) is designed to involve exactly one concept key-value pair. The concept refers to the type of global visual feature being evaluated, while the key-value pairs under"}, {"title": "2.2.1 COLOR (TUNE) OPERATOR", "content": "The color operator is designed to manipulate the hue component of an image, altering its color characteristics. This operator takes an image input, which is then transformed into the HSV (Hue, Saturation, Value) color space. Applying the HSV conversion allows us to separate the chromatic content (hue) from the image, allowing for pixel-intensity-agnostic manipulation.\nWe adjust the first channel (which is the hue channel) by adding a predefined parameter representing the changes in hue. The adjustment is performed modulo 180 (range of hue value) to ensure that the updated hue values remain within valid bounds. The adjusted image is converted back to the RGB color space to keep consistency with the original input image. We use the standard deviation ($\\sigma$) of hue values from all bark images as a standard unit to quantify the variation of hue values. In experiments, we increase the tune value by 5 (close to 1 \u00d7 $\\sigma$) and 10 (close to 2 \u00d7 $\\sigma$), to simulate the color changes in texture images."}, {"title": "2.2.2 SMOOTH OPERATOR", "content": "The smooth operator refines the texture image surface. While various smoothing algorithms, such as Gaussian and median blur, often blur edges and obscure geometric structures, a bilateral filter is used to preserve edge integrity. This filter achieves edge-preserving smoothing by considering both spatial proximity and pixel intensity differences, with adjustable sensitivity to control smoothness levels. Two sensitivity values (+150, +300) are tested, affecting the degree of smoothing and potential texture detail loss. The results are visually inspected to ensure integrity."}, {"title": "2.2.3 GROOVE/SURFACE REMOVAL OPERATOR", "content": "A groove is a small valley structure found on the surface of stems, branches, and other plant organs of woody plants; for bark images, they are roughly divided into grooves and surface areas for ease of evaluation. Compared to the surface areas, which are generally smoother, grooved areas are spongier or more porous, with a slightly rougher or raised texture relative to the surrounding regions. The differences in shape often result in color variation due to uneven illumination, and the edges with the highest color contrast are identified as the contours of the grooved area.\nWhile accurate segmentation of the grooved area should be ideal for perturbing images accurately, it defeats our purpose of building a simple and intuitive operator for image explanation with extra human effort. Additionally, the accurate segmentation of grooves, even if manageable, does not improve the explainability in our proposed solution. Instead, we propose and implement a simple vision-based pipeline to segment the grooved areas based on the color contrastness of the texture images."}, {"title": "2.3 PROCESS TO EXPLAIN IMAGE", "content": "The pipeline consists of four steps. (1) First, it begins by converting the input image to a grayscale representation to simplify the image data as a way to reduce computational complexity while preserving essential structural information. (2) The resultant image is thresholded to separate the foreground from the background, resulting in a binary image. To determine an optimal threshold value automatically, we employ Otsu's thresholding method which adaptively determines the threshold value. 3) We apply morphological operations to enhance feature detection by removing noises and filling small gaps. With these two operations, only the significant features can be retained. (4) Lastly, we scan the image and identify all the contours, which are the boundaries of connected components in the image, returning a list of contour points.\nThe contours which correspond to the significant features in the image are used as binary masks to segment the area of the grooves. Conversely, we can segment the surface area using a flipped mask"}, {"title": "2.3.1 SHAPE OPERATORS", "content": "Flipping and rotating images effectively alter the directionality of dominant texture features while preserving dimensional consistency. However, \u00b190-degree rotations may cause pixel loss due to clipping in non-square images, and \u00b130-degree rotations can introduce black pixels. Flipping shifts dominant features to the opposite side, and when combined with rotation, helps assess the impact of feature positioning on vision models."}, {"title": "2.3.2 \u0421\u043eMMUTATIVITY UNDER COMPOSITION OPERATIONS", "content": "Commutativity under Composition Operations (CUCO) is a property defined for a set of operators F. Let $f: X \\rightarrow X$ and $g : X \\rightarrow X$ be two operators/functions defined on a set X. The operation of composition of these functions, denoted $(f \\circ g)(x) = f(g(x))$, is said to be commutative if and only if:\n$f \\circ g = g \\circ f$  (1)\nThat is, for all $x \\in X$,\n$f(g(x)) = g(f(x))$ (2)\nCUCO property is desired for the operators in our settings because it allows us to study the independent influence of each operator on the vision model without accounting for the order of their application. This simplifies the modeling process by eliminating the need to consider inter-correlations between operators based on sequence order. Specifically, for any combination of the mentioned operators, we expect the final result to be unique and independent of the order in which the operators are applied. While it may not be feasible for our designed operators to fully satisfy this property due to unavoidable information loss in certain operations, such as rotation, operators that partially satisfy CUCO remain valuable. Furthermore, testing this property theoretically can be impractical given the dissimilarity in the internal procedures of each operator, so we employ a numerical approach to assess the degree to which our operators achieve the CUCO property.\nIn our experiment, we calculate the average MAE value between images (with pixel range 0 \u2013 255) generated with same set of operators but applied in different orders. The average MAE across our dataset is less than 30, which indicates the operators we defined partially satisfy this property."}, {"title": "2.4 EXPLAINING WITH SURROGATE MODELS", "content": "We randomly sample a sequence of operators $f_1, f_2, ..., f_n$ from the set of all defined operators $f \\in F$ and apply them to perturb the input image x. The perturbed image, denoted as $x' = f_1 \\circ f_2 \\circ ... \\circ f_n(x)$, is obtained after sequentially applying all the operators. The image x' is then used as input to the vision model, and the confidence value (probability) $p(y = i_{GT}|x')$, corresponding to the class ID $i_{GT}$ in the output probability distribution (typically generated by a softmax activation function), is recorded. Operators included in the sampled sequence are labeled as 1, while those excluded are labeled as 0, resulting in a binary vector $\\phi \\in \\{0,1\\}^{|F|}$, where |F| denotes the cardinality of the operator set. The sampling process is repeated for m > |F| iterations to ensure that every operator is selected at least once. To avoid redundancy, the confidence value is computed only for unique sequences, regardless of the order of the operators. The confidence values are concatenated to form a vector $c \\in \\mathbb{R}^m$, while the binary vectors for all sampled sequences are concatenated to construct a 2D binary matrix $\\Phi \\in \\{0,1\\}^{m \\times |F|}$. The selection matrix $\\Phi$ and the confidence vector c are utilized to train a surrogate model. Both linear regression and Classification and Regression Tree (CART) models are employed to analyze the impact of operator combinations on the vision model's confidence values. Linear regression identifies the independent effect of each operator, while CART explores interactions between operators, providing a comprehensive analysis of the"}, {"title": "3 EXPERIMENT RESULTS: UNDERSTAND BARK IMAGES WITH \u03a7\u0391\u0399", "content": "We built a tree species dataset comprising bark images from 21 species (Table 3), with 8,184 images in total. Images were collected by dendrologists in Indiana, with 3 to 5 photos of each tree from different distances and perspectives, ranging from 1 to 4 meters from the tree trunks. Due to the diversity of species present in both natural forests and plantations, the number of images per species varies but each class contains at least 80 images in the dataset. We further integrated an automated pipeline using YOLOv11 (Jocher et al., 2023) and the Segment Anything Model (Kirillov et al., 2023), which includes tree detection and bark segmentation. With the clean bark images (examples in Figure 3), we trained MobileNetv2, a light-weighted deep neural network, to classify tree species only using bark images, with an accuracy of 90+%.\nTo interpret how models predict tree species, we used all operators (Table 1) and evaluated how each operator impacts the model's decision process. In Figure 7, we illustrate the feature importance of various operators applied on the model in all images. For the vision model, MobileNetV2, \"remove grooves\" and \"remove surface\u201d emerge as the most influential operations affecting species prediction. Meanwhile, different rotation angles produced similar effects on both models."}, {"title": "3.2 PERFORMANCE COMPARISON", "content": "To quantitatively compare our proposed method with others in providing explanations that align with human intuition, we created a test dataset with the reserved bark images and labeled concepts (21 classes with 20 images in each class). For each image, five concepts (rugged, plated, furrow, vertical stripped, and smooth) were ranked by human visual interpretation as ground truth based on how their importance in identifying tree species. Each concept-based method then predicted the ranking of each concept for every image. The predicted orderings were compared with the ground truth using Kendall's Tau.\nNotably, while our method allows for quantifiable concepts by adjusting parameters, we need to compromise by using limited concepts that are easier to define in most concept-based methods such as TCAV. Similar to other variants, TCAV relies solely on collected concepts images and quantifiable concepts are thus hard to define. Some of the concepts, such as color, are also not well-defined concept images, making quantifiable concepts difficult to define. Some concepts, such as color,"}, {"title": "4 CONCLUSION", "content": "In this paper, we propose a lightweight post-hoc XAI method that provides human-interpretable explanations for tree bark vision models, enhancing transparency and facilitating expert validation. Compared to other similar works in vision model explanation, our approach utilizes concepts rather than the commonly used attribution-based methods to explain the global visual features in tree barks. While other concept-based XAI methods typically require building external concept image datasets, we employ operators to create quantifiable concepts without additional overhead. Compared to TCAV and Llama3.2, our method offers explanations that align more closely with human perception. Although our approach demonstrates superior explainability for bark images, we acknowledge that its performance can heavily depend on carefully designed operators, which may require expert knowledge for adaptation across different domains. Furthermore, similar to other concept-based XAI methods, while our approach effectively captures global visual features that are difficult to achieve with attribution-based methods, the explanations remain susceptible to subjectivity."}, {"title": "A APPENDIX", "content": "For easier comparison with other XAI methods, we map our calculated concepts derived from operators into inferred concepts (Smooth, Plated, Rugged, Furrow, Vertical Stripped) which are more closely related to bark features by combining several calculated operations. We denote feature importance of a calculated concept c with key k and value v as FI(k, v) and significance of inferred concept as Sig(c). We used values related to each operator and sort the inferred concepts from the most significant to the least significant based on significance values. Specifically, Sig(smooth) is the maximum value between FI(smooth, +150) and FI(smooth, +300). Sig(vertical stripped) is calculated as the average value of FI(rotate, -30) and FI(rotate,+30). Sig(rugged) is the direct value of FI(groove, remove). Sig(plated) is the average value of FI(rotate, -30), FI(rotate, +30), FI(flip, horizontal) and FI(flip, vertical). Sig(furrow) is the average value of Sig(rugged) and Sig(vertical stripped). The inferred concepts are then ordered based on the significance values. Please note, the way how the concept significance values is calculated is based on how each concept can be further decomposed and represented with the calculated concepts from operators, alternative interpretation can exist since perception of concept can always be subjective, due to the nature of concept-based XAI methods."}]}