{"title": "CCUP: A Controllable Synthetic Data Generation Pipeline for Pretraining Cloth-Changing Person Re-Identification Models", "authors": ["Yujian Zhao", "Chengru Wu", "Yinong Xu", "Xuanzheng Du", "Ruiyu Li", "Guanglin Niu"], "abstract": "Cloth-changing person re-identification (CC-ReID), also known as Long-Term Person Re-Identification (LT-ReID) is a critical and challenging research topic in computer vision that has recently garnered significant attention. However, due to the high cost of constructing CC-ReID data, the existing data-driven models are hard to train efficiently on limited data, causing overfitting issue. To address this challenge, we propose a low-cost and efficient pipeline for generating controllable and high-quality synthetic data simulating the surveillance of real scenarios specific to the CC-ReID task. Particularly, we construct a new self-annotated CC-ReID dataset named Cloth-Changing Unreal Person (CCUP), containing 6,000 IDs, 1,179,976 images, 100 cameras, and 26.5 outfits per individual. Based on this large-scale dataset, we introduce an effective and scalable pretrain-finetune framework for enhancing the generalization capabilities of the traditional CC-ReID models. The extensive experiments demonstrate that two typical models namely TransReID and FIRe\u00b2, when integrated into our framework, outperform other state-of-the-art models after pretraining on CCUP and finetuning on the benchmarks such as PRCC, VC-Clothes and NKUP. The CCUP is available at: https://github.com/yjzhao1019/CCUP.", "sections": [{"title": "I. INTRODUCTION", "content": "Person re-identification (ReID) aims to identify gallery images containing persons with the same identity as the query image in a cross-camera scenario. Furthermore, cloth-changing person re-identification (CC-ReID) is a more challenging task to identify the same person but with different clothes.\nIn recent years, deep learning-based models have been widely used to learn the discriminative features of person images for ReID and its extended task CC-ReID [1]\u2013[3]. However, there are two main challenges for the CC-ReID task. Challenge 1: the high cost of sampling and labeling real CC-ReID images limits the size of existing datasets, causing low performance of training models due to the lack of sufficient ground truth for supervision. Building a ReID dataset requires a complex environmental setup of places, devices and pedestrians, as well as manual labeling without violating privacy (DukeMTMC-ReID has been retracted due to privacy concerns). In particular, the complexity and costs of generating a CC-ReID dataset further increase significantly since it is difficult to capture images of the same person wearing various outfits on a large spatial and temporal scale. In contrast, synthetic datasets are emerging to reduce costs and address privacy concerns. As shown in Tab. I, we provide the statistic of identities (#IDs), images (#Images), cameras (#Cam) and average outfits per identity (#avgClo) of some typical CC-ReID and synthetic datasets. We could observe that the whole size and especially #avgClo of all the previous CC-ReID benchmark datasets such as PRCC, LTCC and VC- Clothes are obviously limited, and Celeb-reID and LaST are even created from celebrity street photography and movies rather than real surveillance scenes. Besides, the existing commonly-used synthetic ReID datasets are not designed for cloth-changing scenarios and therefore lack rich cloth- changing ground truth. To address these issues, we propose a controllable and low-cost pipeline for generating large- scale synthetic data more suitable for CC-ReID tasks.\nChallenge 2: clothes-irrelevant features are hard to be extracted via the existing models straightly trained on a limited CC-ReID dataset. Specific to the CC-ReID task, the most pivotal purpose is to extract discriminative clothes-irrelevant features. Therefore, a clothes-based adversarial loss (CAL) strategy is proposed to extract clothes-irrelevant fea- tures from original RGB images by penalizing the predictive power of the ReID model [15]. Yang et al. propose the first causality-based auto-intervention model (AIM) to analyze the impact of clothing on model inference and eliminate clothing bias during training [16]. Besides, various auxiliary information such as gait, skeleton, and 3D shape could be exploited for supplementing more clothes-irrelevant features [17]\u2013[19]. However, all the previous CC-ReID models suffer from extremely scarce training data, limiting their performance specifically some advanced visual transformer-based models. To address this challenge, we employ a scalable pretrain- finetune framework leveraging our large-scale synthetic dataset to enhance the model performance of CC-ReID. Overall, the contributions of our work are three-fold:\n\u2022\tWe construct a high-quality synthetic CC-ReID dataset named CCUP with our low-cost and controllable data generation pipeline, which is the first large-scale (over 1,000,000 images) dataset for the CC-ReID task.\n\u2022\tWe exploit a scalable pretrain-finetune framework, which could improve the performance of CC-ReID via fine- tuning the same model pretrained on our large-scale synthetic dataset CCUP.\nThe extensive experimental results on multiple bench- mark datasets including LTCC, VC-Clothes and NKUP illustrate that our framework outperforms other state-of- the-art baseline models significantly and consistently."}, {"title": "II. METHODOLOGY", "content": "Considering the numerous advantages of synthetic data, such as good controllability, a high degree of automation, significantly lower costs compared to capturing real-world data, and the excellent ability to simulate various real-world environments, we prioritize synthetic data generation for CC- ReID tasks. Accordingly, we propose a low-cost, high-quality, large-scale, and controllable data generation pipeline along with a new dataset CCUP for pretraining CC-ReID models. Specifically, the CCUP generation process consists of three main procedures: (1) generating skeletal meshes of realistic human characters, (2) simulating surveillance in diverse sce- narios, and (3) producing self-labeled detection results for cloth-changing pedestrian, as illustrated in Fig 1.\nA. Generate person skeletal mesh\nThe skeletal mesh is the basis for building synthetic data, which contains mesh data and skeletal data. Mesh data stores vertex positions, normal vectors and texture coordinates, etc. Skeletal data represents the skeletal node hierarchy. Specifi- cally, we construct the person skeletal mesh using MakeHu- man\u00b9 software, an open-source 3D human modeling software that helps users create high-quality, vividly realistic human body models. We modify Makehuman's AssetDownloader plugin and employ it to collect assets from the MakeHuman community that can be used to build 3D person models such as skin, hair, clothes, etc. Besides, we modify MakeHuman's MassProduce plugin to create 6,000 person skeletal meshes with different physiological features. In this way, unique combinations of physiological parameters determine unique pedestrian IDs. In terms of the same person's mesh body, we perform random replacement of clothes so that the same person could be appended to multiple clothes, which benefits more diverse outfits per individual specific to a CC-ReID dataset. Inspired by Unrealperson, we create more than 10,000 clothes textures for subsequent clothing changes [13].\nB. Simulate surveillance in scenarios\nUnreal Engine2 is a game engine that offers a wide range of rendering functions and is popular in many applications such as game and movie development. To generate the CC-ReID data, we employ Unreal Engine (version 5.3.2) to simulate real surveillance scenarios. We configure animations for the skeletal meshes generated in section II-A so that they can walk around the simulation scenarios. Then, we choose three scenarios for simulation in the epic games marketplace: an European alleyway, an office building, and a park with 50, 25, and 25 cameras, respectively. Particularly, we design travel routes of persons in three scenarios and place cameras along the routes with diverse viewpoints. Benefiting from a well- designed detection strategy, the video of the person could be automatically captured if this person passes under the camera. Then, the video frames with automatically labeled person IDs, camera IDs and cloth IDs are generated. To guarantee high- quality labeled data, each image contains only one pedestrian by adapting the starting time and the speed of this person. Last but not least, we replace the texture of pedestrians when they pass by different cameras to create a cloth-changing scenario.\nC. Detect and label the bounding boxes\nBased on the surveillance video frames, we employ the advanced RTMdet model to detect pedestrians and generate their corresponding bounding boxes [20]. As explained in section II-B, each frame contains only one individual, ensuring that the number of bounding boxes per frame is either 0 or 1. Therefore, the frame's label corresponds directly to the label of the bounding box, which significantly simplifies the dataset labeling process and conforms to the configuration of the previous benchmark datasets.\nIn our pipeline, we not only change outfits at the mesh body level but also apply diverse texture replacements for the clothes, greatly enhancing the variety of outfits. Besides, our approach is highly extensible, requiring only minor code mod- ifications to generate entirely new datasets tailored to different tasks. The cost of dataset generation is remarkably low due to the high level of automation in the procedure of generating video frames and self-labeling, allowing produce datasets with hundreds of thousands of images in just a few days. Ultimately, we generate a large-scale CC-ReID dataset CCUP, which includes 6,000 individuals, 100 cameras, 1,179,976 images, and an average of 26.5 outfits per individual."}, {"title": "III. EXPERIMENT", "content": "After obtaining a large amount of CC-ReID data, we intro- duce the pretrain-finetune framework for CC-ReID tasks. We pretrain two models on our CCUP and finetune in the down- stream benchmarks to extract more robust clothes-irrelevant features. Furthermore, we demonstrate the superiority of our proposed dataset by comparing the performance of the model pretrained on our CCUP and other synthetic datasets as well as several state-of-the-art baselines.\nA. Dataset and evaluation metrics\nFor a comprehensive comparison, we select some typical synthetic datasets for pretraining, including UnrealPerson, PersonX, and ClonedPerson. Besides, PRCC, VC-Clothes, and NKUP are chosen as the downstream benchmark datasets for finetuning and obtaining the evaluation results. It is noteworthy that only clothes changing ground truth samples are used in PRCC and VC-Clothes datasets while both clothes-changing and clothes-consistent ground truth samples are used in NKUP. The detailed comparison of these datasets are shown in Tab. I. Subsequently, two frequently-used metrics Rank-1 and mAP are employed to evaluate each model.\nB. Implementation details\nWe select two representative baselines: a general ReID model TransReID with the backbone ViT and a CC-ReID model FIRe2 with the backbone ResNet50 as our pretrained models [3], [30], [31]. Besides, the other 11 typical CC-ReID models are utilized for comparison. Particularly, we fix some parameters the same as other baselines for fair comparison. For TransReID, input images are resized to 256 \u00d7 128, the patch size is set to 16, and the stride size is 12. The SGD optimizer is employed with the weight decay of 1 \u00d7 10\u22124 and the learning rate is initialized as 4 \u00d7 10-3. For FIRe2, Input images are resized to 384 \u00d7 192 and the batch size is 32. The Adam optimizer is employed with the weight decay of 5\u00d710-4 and the learning rate is initialized as 3.5 \u00d7 10-4. We pretrain the two pretrained models for 10 epochs on UnrealPerson, Personx, ClonedPerson, and CCUP, respectively, and finetune these models on the benchmark datasets for 80 epochs.\nC. Comparison with state-of-the-art methods\nWe compare our model with some state-of-the-art methods in Tab. II, where CAL, AIM, and CCFA uses person cloth-labels in training process. We observe that both the pretrained models TransReID and FIRe2 empowered by our CCUP illus- trate the superior performance than the other models. Specifi- cally, TransReID+CCUP achieves 59.0% mAP on PRCC and 28.8% rank-1 on NKUP, and FIRe2 achieves 64.7% rank- 1 on PRCC, and 85.1% rank-1 and 85.0% mAP on VC- Clothes. The best performance of mAP on NKUP is obtained by TransReID pretrained by UnrealPerson, showing the ef- fectiveness and scalability of our pretrain-fine framework. In particular, FIRe2 without pretraining outperforms TransReID, while the performance of TransReID pretrained on our CCPU dataset can be significantly improved. The experimental results as 28.8% (+7.6%) rank-1 on NKUP. Furthermore, FIRe2 pretrained by CCUP obtains the best preformance on PRCC and NKUP, achieving 64.7% (+5.6%) rank-1, 57.7 (+7.2%) MAP on PRCC, 26.4% (+4.0%) rank-1 and 17.8 (+1.8%) mAP on NKUP. We believe that it is the powerful person clothes-changing number of CCUP that leads to this result.\nE. Visualization\nTo visually compare the regions that a model fucuses on before and after pre-training, we visualize the inference results of FIRe2 on PRCC as shown in Fig. 2 [32]. We observe that the model is improved via pretrained on CCUP in the following two aspects: (1) The model puts more focus on the human body, rather than the background, as illustrated by the three sets of images in the first row of Fig. 2. (2) The model pays more attention to the face, neck, shoulders, wrists, and shoes (Shoes change less frequently than other clothes ) of a person, which are more conducive to extracting clothes- irrelevant features."}, {"title": "IV. CONCLUSIONS", "content": "In this paper, we propose a controllable synthetic dataset generation pipeline and construct a CC-ReID dataset named CCUP containing 6000 IDs, 100 cameras, 1,179,976 images, and 26.5 outfits per individual. To address the challenge that existing models are hard to extract robust and accurate clothes- irrelevant features in CC-ReID task, we introduce the pretrain- finetune framework commonly used in NLP. After pretraining both FIRe\u00b2 and TransReID baselines, our model outperforms other state-of-the-art methods on three benchmarks: PRCC, VC-Clothes, and NKUP. Furthermore, we conduct compari- son experiments using different synthetic datasets as pretrain datasets to demonstrate the superiority of CCUP."}]}