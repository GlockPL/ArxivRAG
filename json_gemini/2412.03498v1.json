{"title": "A Bidirectional Siamese Recurrent Neural Network for Accurate Gait Recognition Using Body Landmarks", "authors": ["Proma Hossain Progga", "Md. Jobayer Rahman", "Swapnil Biswas", "Md. Shakil Ahmed", "Arif Reza Anwary", "Swakkhar Shatabda"], "abstract": "Gait recognition is a significant biometric technique for person identification, particularly in scenarios where other physiological biometrics are impractical or ineffective. In this paper, we address the challenges associated with gait recognition and present a novel approach to improve its accuracy and reliability. The proposed method leverages advanced techniques, including sequential gait landmarks obtained through the Mediapipe pose estimation model, Procrustes analysis for alignment, and a Siamese biGRU-dualStack Neural Network architecture for capturing temporal dependencies. Extensive experiments were conducted on large-scale cross-view datasets to demonstrate the effectiveness of the approach, achieving high recognition accuracy compared to other models. The model demonstrated accuracies of 95.7%, 94.44%, 87.71%, and 86.6% on CASIA-B, SZU RGB-D, OU-MVLP, and Gait3D datasets respectively. The results highlight the potential applications of the proposed method in various practical domains, indicating its significant contribution to the field of gait recognition.", "sections": [{"title": "1 Introduction", "content": "Biometrics refers to the automatic identification or authentication of individuals by analyzing their physiological and behavioral characteristics. Physiological biometrics, such as the face, fingerprints, iris, and retina, are stable means of authenticating and identifying people. However, these traits require cooperation from the subject and a controlled environment, making them unsuitable for surveillance systems. Even though these techniques work well in a lot of situations, they can be hard to use in others. They can have problems like obstructed views, and distant or poorly defined data, and frequently necessitate the subject's cooperation.\nGait recognition identifies individuals based on their walking posture, and is a non-invasive technique that is hard to copy, making it ideal for access control, covert video surveillance, criminal investigation, and forensic analysis. Human walking follows a repeating pattern where the right leg steps, followed by the left leg, and then the right leg again, forming a gait cycle [1]. This gait cycle encompasses 32 gait features, such as stride, torso movement, hand position, joint angles, foot spacing, and foot length. Gait recognition has the advantage of operating at a distance and with low-resolution images, making it applicable in diverse situations [2]. However, gait recognition faces challenges related to different intraclass variations in appearance and environment, such as clothing, carrying variation, illumination, walking surface, and view angle, which can significantly reduce performance [3].\nGait analysis has been extensively studied, particularly in biometrics and human identification. Researchers have utilized various techniques to extract gait features, including spatiotemporal features, frequency domain features, and wavelet-based features [4,5]. These features capture different aspects of gait, such as body segment movements, frequency components, and time-frequency characteristics of gait signals. Physical sensors are commonly used in addition to these techniques to identify gaits [6]. These sensors are placed on the feet, legs, and torso to measure parameters such as stride length, step time, and cadence. The measurements obtained from these sensors can then be used to extract gait features and classify them using techniques such as SVMs, neural networks, and decision trees [7, 8]. However, wearable sensor-based approaches have achieved state-of-the-art performances. It is uncomfortable to wear it all day, modeling, and eventually, some people may forget to do so. However, the detective range is constrained by the comparatively expensive installation costs of these environmental-based sensors.\nThe field of computer vision has witnessed a surge in the adoption of modern deep learning-based algorithms, which have exhibited exceptional performance in various tasks, including person reidentification, pose estimation, and gait recognition [9,10]. These advances have paved the way for significant improvements in gait recognition, a vital aspect of biometric identification. Particularly, advancements"}, {"title": "2 Related work", "content": "Human identification is an important research topic with numerous applications in security, surveillance, and healthcare. Gait analysis [17, 18] is an attractive method of identification as it can be performed at a distance, is non-invasive, and does not require any special equipment or training. In recent years, there has been growing interest in the use of gait analysis for human identification [19], and a considerable body of research has been published on this topic. It started with the advancement of cameras and web technology. However, humans can be identified by biometric traits with high accuracy and reliability due to their unique and distinctive nature. Wearable biometrics is an active research area with interesting applications for real-life scenarios. Current state-of-the-art research has demonstrated that various characteristics can be utilized to accurately identify the users of the employed devices in a continuous manner [20-22].\nThe accurate identification of individuals from a long distance can be challenging, as the biometric features required for identification may be too small or obscured to be reliably captured. This can be a major obstacle in applications such as security and surveillance, where it is important to be able to identify individuals at a distance. Gait analysis is used to identify individuals from a distance based on their walking patterns [23]. Fu et al. [24] employs a pose-based approach for gait recognition, showcasing comparable results to silhouette-based methods. The proposed GPGait framework introduces HOT, HOD, and PAGCN, demonstrating superior cross-domain performance and potential for effective pose-based gait recognition.\nThere are generally two types of gait analysis methods used in the current research community: vision-based, and wearable sensor-based. In the context of signals recorded using video sensors, the Gait Energy Image (GEI) representation has been widely utilized. An improved version of the GEI method is also employed to enhance its effectiveness in [25]. Anwary et al. [26] propose a gait evaluation method using Procrustes and Euclidean distance matrix analysis that collects real-time accelerometer and gyroscope data from inertial measurement unit (IMU) sensors and he investigated the optimal location for wearable sensors. In another study [27], the Kinect device is used to capture three-dimensional coordinates of human bones, and the distances between bone nodes are used as features. In addition, Anwary et al. [28] investigated the optimal location for wearable sensors, automated the extraction of gait parameters, and evaluated gait abnormalities [29]. A support vector machine (SVM) classifier is utilized, employing one-versus-one and one-versus-all algorithms to solve the multi-classification task. Another often-used approach is the application of deep Convolutional Neural Networks (CNN). A deep CNN architecture was developed by [3] consisting of eight layers: four convolution layers and four pooling layers. This architecture is less sensitive to several typical variations and occlusions reducing the quality of gait recognition. Deep CNN has been successfully used to classify images from various sources, as demonstrated in a study [30]. Current skeleton-based gait recognition struggles with distinguishing walking styles across views. To address this, Huang et al. [31] the proposed Condition-"}, {"title": "3 Methodology", "content": "In this research, we propose a method for gait recognition using sequential gait landmarks. The primary objective of this approach is to accurately identify and distinguish individuals based on their gait patterns.\nThen, the pose estimation model (MediaPipe) is used to capture sequential gait frames (N) based on foot landmarks, ensuring that frames complete a full walking cycle and the corresponding landmarks of individuals are collected. To address the variability caused by individuals approaching from different angles, we applied Procrustes analysis to align the gait frames. Finally, we employ a Siamese BiGRU-dualStack Neural Network to identify individuals based on gait, as shown in Fig. 2. The Siamese BiGRU-dualStack Network takes pairs of gait sequences as input and learns to distinguish between different individuals. Through experiments and evaluations, we demonstrate the effectiveness of our proposed approach to accurately identifying individuals based on their gait patterns.\nRecurrent Neural Networks (RNNs) are a subset of neural networks that is specifically intended to process sequential data by storing information from previous time steps in hidden states. However, traditional RNNs suffer from the vanishing gradient problem, which occurs during training when gradients diminish exponentially as they propagate through time. The RNN's capacity to identify long-term dependencies in sequential data is hampered by this problem. To address the vanishing gradient problem and capture long-term dependencies more effectively, the Gated Recurrent Unit (GRU) [72] and other recurrent neural network variations, such as the Long Short-Term Memory (LSTM) architecture, were introduced.\nGRUs are specifically designed to address the vanishing gradient problem in traditional RNNs, which occurs when gradients diminish exponentially over time and make it difficult for the network to learn long-term dependencies. GRUs achieve this by using gating mechanisms that allow the network to selectively update and reset information. While GRUs excel at capturing long-term dependencies, they also perform well at modeling short-term dependencies. GRUs typically have fewer parameters than standard RNNs. Traditional RNNs have separate input, output, and hidden state parameters, whereas GRUs combine these into a single update gate and reset gate. This reduction in parameters makes GRUs more memory-efficient and computationally faster. Moreover, having fewer parameters reduces the risk of overfitting, especially when working with limited training data. GRU has two main gates: an update gate and a reset gate, which control the flow of information through the network. The previous hidden state (h) is divided into two parts: the amount that should be sent to the current time step (t) by the update gate (z) and the amount that should be forgotten by the reset gate (r). However, bidirectional GRU (BiGRU) [73] is an extension of the GRU model that incorporates information from both past and future time steps. It addresses the limitations of traditional GRU and allows the model to capture dependencies in both directions. In a BiGRU, the input sequence is processed in two directions: forward and backward. The forward GRU processes the sequence from the beginning to the end, while the backward GRU processes it from the end to the beginning. The output of the BiGRU is obtained by concatenating the hidden states from both the forward and backward GRU layers. This combined representation captures the context of each time step in the sequence.\nThe formulas for computing the hidden states in a BiGRU are as follows:\nForward GRU:\n$z_{f(t)} = \\sigma(W_{zf} * [h_{f(t-1)}, x_t])$ (1)\n$r_{f(t)} = \\sigma(W_{rf} * [h_{f(t-1)}, x_t])$ (2)\n$\\tilde{h}_{f(t)} = tanh(W_f * [r_{f(t)} * h_{f(t-1)}, x_t])$ (3)\n$h_{f(t)} = (1 - z_{f(t)}) * h_{f(t-1)} + z_{f(t)} * \\tilde{h}_{f(t)}$ (4)\nBackward GRU:\n$z_{b(t)} = \\sigma(W_{zb} * [h_{b(t+1)}, x_t])$ (5)\n$r_{b(t)} = \\sigma(W_{rt} * [h_{b(t+1)}, x_t])$ (6)\n$\\tilde{h}_{b(t)} = tanh(W_b * [r_{b(t)} * h_{b(t+1)}, x_t])$ (7)\n$h_{b(t)} = (1 - z_{b(t)}) * h_{b(t+1)} + z_{b(t)} * \\tilde{h}_{b(t)}$ (8)\nCombined Output:\n$h_t = [h_{f(t)}, h_{b(t)}]$ (9)\nIn the above formulas, $z_{f(t)}$ and $z_{b(t)}$ are the update gate activations for the forward and backward GRUs, respectively. On the other hand, $r_{f(t)}$ and $r_{b(t)}$ are the reset gate activations, $h_{f(t)}$ and $h_{b(t)}$ are the candidate hidden states and $h_{f(t)}$ and $h_{t(t)}$ are the forward and backward hidden states at time step t.\nThe proposed model incorporates a Siamese network, a specialized architecture comprising two identical branches that share weights and structures. This enables direct comparison between input sequences, enhancing the model's capability to discern nuanced gait patterns. In this case, each branch of the Siamese network incorporates two bidirectional bidirectional Gated Recurrent Units (GRUs) with 128 units and a Rectified Linear Unit (ReLU) activation function. This configuration is designed to capture and encode the temporal dynamics of the gait sequences effectively. In each branch, the bidirectional GRUs enable the network to analyze the gait information in both forward and backward directions, allowing a comprehensive understanding of the gait patterns within the sequences. This bidirectional approach enhances the network's ability to recognize and distinguish between different individuals based on their gait. However, after processing the input sequences in the Bidirectional GRUs, the outputs of both branches are concatenated. Fig 3 shows an overview of the network architecture. Following the concatenation, a 1x1 dense layer is applied to the combined output. This dense layer serves as a transformation step, linearly mapping the concatenated features to a new space. The use of a 1x1 dense layer allows for a flexible adjustment of the feature dimensions. Subsequently, a sigmoid activation function is applied to the transformed features. The sigmoid activation function is chosen to introduce non-linearity and ensure that the network produces output values within the range of [0, 1].\nOverall, the approach aims to advance the field of gait recognition by providing an effective method-"}, {"title": "4 Experimental Analysis", "content": "We conducted extensive evaluations of our proposed model using indoor datasets such as CASIA-B, SZU, and OU-MVLP. Furthermore, to enrich our analysis, we incorporated the Gait3D dataset. Each dataset offers unique characteristics and challenges, contributing to a comprehensive assessment of our model's performance.\nCASIA-B [74]: CASIA-B gait dataset stands out as one of the most extensive publicly available repositories of gait information. The images in the CASIA-B gait dataset are stored in PNG format, and each image has a resolution of 128 X 64 pixels. The labels contain information about the subject ID. It includes 124 subjects, captured from 11 viewpoints (93 men and 31 women). The view range is 0\u00b0 to 180\u00b0, with a distance of 18\u00b0 between the two closest perspectives. There are 6 normal walking sequences (\"nm\"), 2 bag walking sequences (\"bg\"), and 2 coat walking sequences (\"cl\"). Figure 4 shows the samples from different views of a subject's normal walking. In line with the experimental setup of previous studies [75-77], we adopt a similar approach for subject partitioning. Specifically, we allocate the first 74 subjects for training purposes, while the remaining subjects are reserved for testing.\nSZU [78]: SZU is a large RGB-D gait dataset. It contains 99 subjects, with 8 sequences for each subject in two different views. The first one is the side view 90\u00b0, and the second is about 30\u00b0 away from the side view 60\u00b0. For each view, there were 4 video sequences captured. Two sequences are right-walking ones, and two are left-walking. So there are 8 different sequences for each subject. When subjects walk, synthesized color images (RGB images) and depth images are captured. Gait data from 99 subjects was stored in 792 (99 \u00d7 4 \u00d7 2views) sequences. Figure 4 illustrates various walking motions, captured from different angles. The color and depth image resolutions are all 640 X 480 and are stored in PNG format. Following the experimental setup described in [79], we assigned the first 49 subjects for training, while the remaining subjects were reserved for testing.\nOU-MVLP [80]: Multi-View Large Population Database with Pose Sequence (OUMVLP-Pose),"}, {"title": "4.2 Data Pre-processing", "content": "Data preprocessing involves three essential steps: sequential frame extraction, landmark collection, and normalization. These steps ensure the input data is appropriately prepared for subsequent analysis and model training.\nMediapipe is a powerful framework developed by Google that provides a comprehensive solution for various multimedia processing tasks, including pose estimation. It provides a set of pre-built tools and models that make visual data analysis accurate and efficient. In the context of this work, Mediapipe plays a significant role in data preprocessing by performing sequential frame extraction and landmark collection. The Mediapipe Pose estimation model specifically focuses on the accurate detection and localization of human poses, capturing key points and landmarks that define the body's posture and configuration.\nThe first step of data preprocessing for two datasets such as CASIA-B, SZU involves sequential frame (N) extraction based on the foot landmark. In this case, the left foot landmark is utilized. By leveraging the Mediapipe Pose algorithm [61], the sequential frames (N = 6) are extracted in a manner that ensures the left foot landmark values progress from the most negative to the most positive. This sequential arrangement effectively represents the complete cycle of a walking motion. However, in our study, we consider six sequential frames following the experimental setup described in [39], as it has been found to provide a satisfactory level of accuracy. By focusing on the left foot landmark and extracting frames in this manner, the resulting sequential frames encapsulate the relevant temporal information necessary for gait recognition and analysis.\nThen, the Mediapipe Pose estimation model is utilized to collect landmarks from each frame. For each individual, their gait data comprises a sequence of six frames. In each frame, the Mediapipe Pose estimation model collects 33 landmarks, with each landmark represented by three coordinates (x, y, z). As a result, the total number of values collected for a single individual amounts to 33 * 3 * 6, which equals 594 values. The x, y, and z coordinates of each landmark represent the spatial positioning of specific key points in the gait sequence. These key points correspond to various body parts, joints, or limbs, providing detailed information about the posture and configuration of the individual during each frame. By collecting these landmarks from all frames, a comprehensive representation of the gait sequence is obtained. The sequential arrangement of the landmarks preserves the temporal dynamics of the gait, while the x, y, and z coordinates capture the three-dimensional spatial information. The OUMVLP-Pose dataset organizes each sample as a sequence of frames. Within each frame, a set of pose points are recorded, resulting in a comprehensive representation of an individual's pose. The cumulative values captured within each frame across the entire sequence for a single person form the basis for analyzing the data and training the model. In the Gait3D dataset, sequences are sourced from 4,000 subjects, with 3,000 subjects used for training and 1,000 for testing."}, {"title": "4.3 Procrustes Analysis", "content": "Normalizing the lengths and times of gait features is a common method for quantifying and comparing human walking patterns. These features encompass eight distinct aspects, including stride length, stride time, stride rate, step length, step time, step speed, stance time, and swing time. These measurements are taken from Cartesian coordinates representing the movements of the right and left legs. The x and y axes represent the characteristics of the respective right and left legs, while dimensionless values unify the data. This framework facilitates the visualization of how both legs move through the depiction of feature curves. Procrustes analysis [82] is employed to examine shape variations within a dataset. It is a mathematical and statistical approach that disregards time and size when assessing curve shape and shape changes. In this context, the Ordinary Procrustes Analysis (OPA) finds the optimal translation vector, rotation matrix, and scaling factor to align two configurations closely. Generalized Procrustes Analysis (GPA) is utilized to find the best-fit model within a group of entities [83]. This method avoids the necessity of comparing all potential matrix pairs separately. Instead, it simplifies the process by uniformly adjusting rotation, translation, and scale to achieve the best possible fit. GPA is particularly advantageous for investigating Normalized Mean Gait Shapes (NMGS) and studying the walking patterns of individuals. It emerges when multiple data matrices exhibit a least squares relationship.\nConsider a set of m matrices denoted as $X_i (i = 1,2,3, ..., m)$ representing configurations with landmarks indicating gait traits. These landmarks are described by k shapes, with variations in size or shape. Changes in translation, rotation, and size of a configuration are denoted by $c_i$ (scale factor), $O_i$ (rotation matrix), and t (translation vector) respectively. The relationship is described as:\n$X_i = c_i X_i O_i + j^t$ (10)\nHere, X represents the new point locations of interest in the configuration. The objective of GPA is to transform, rotate, and scale configurations iteratively to minimize the sum of squared distances between corresponding points, thus achieving the best possible alignment among configurations. Iterative steps within the GPA process aim to minimize discrepancies. The shapes undergo resizing, rotation, and translation adjustments until the sum of squared distances reaches a predefined threshold. This process results in a reduction of similar features across all shapes. With a focus on gait traits, Procrustes superimposition determines a representative shape, termed Normalized Mean Gait Shape"}, {"title": "4.4 Model Training", "content": "During the model training phase, pairs were constructed to facilitate the learning process. Positive pairs were created using gait sequences from the same individual, while negative pairs were formed by pairing gait sequences from different individuals. For the CASIA-B dataset, which consists of 124 individuals, 74 individuals were used for training purposes. Since there are 74 individuals in the training set, the total number of positive pairs is also 74. However, for negative pairs, we need to consider the combinations of individuals. The number of negative pairs can be calculated as 74C2. This yields a larger number of possible negative pairs. To ensure a diverse and representative set of negative pairs, we randomly selected a subset of negative pairs from the total number of possible combinations. Figure 6 demonstrates the relationship between the number of pairs and the performance of the model. As the number of pairs increases, the model's performance improves. Based on this observation, we chose to utilize 400 pairs for training. in both the CASIA-B and SZU datasets. In the CASIA-B dataset, 74 pairs are considered positive, indicating similar samples, while the remaining (326) pairs are labeled as negative, representing dissimilar samples. On the other hand, in the SZU dataset, 49 pairs are labeled as positive, indicating similar samples, while the remaining (351) pairs are considered negative, representing dissimilar samples. Similarly, for the OU-MVLP Gait dataset, the training phase involved the systematic creation of pairs. A 1:1 ratio of positive and negative pairs was maintained to ensure a diverse and representative training set. However, for the Gait3D dataset, characterized by its wild and diverse nature, we maintained a 1:2 ratio of positive and negative pairs to enhance learning performance. This approach uses 3,000 positive samples and the remaining subjects as negative samples to ensure that the models are well-equipped to handle a variety of scenarios and differentiate between individuals effectively."}, {"title": "4.5 Evaluation Metrics", "content": "To assess the performance of our proposed approach, we employed several evaluation metrics, including Contrastive Loss and Accuracy. These metrics provide insights into the effectiveness and accuracy of our gait recognition system.\nContrastive Loss [84] is a commonly used loss function in siamese network-based models for gait recognition. It measures the similarity or dissimilarity between pairs of gait sequences. The contrastive loss encourages similar gait sequences to have a smaller distance or dissimilarity score, while dissimilar sequences are encouraged to have a larger distance. By minimizing the contrastive loss, we aim to enhance the discrimination and separability of gait patterns. The contrastive loss is computed using the distance or dissimilarity metric between the feature representations of paired gait sequences. The contrastive loss (L) is calculated using the Euclidean distance metric and is defined as follows:\n$L = (1 - Y) * D^2 + Y * max(0, m - D)^2$ (11)\nwhere: Y is the binary label indicating whether the pair of gait sequences is similar (0 for similar, 1 for dissimilar). D is the Euclidean distance between the feature representations of the paired gait sequences. m is a hyperparameter that controls the separation margin between similar and dissimilar pairs. This loss function helps to optimize the model parameters and improve the overall accuracy of gait recognition.\nRank 1 accuracy is a specific evaluation metric commonly used in gait recognition research to assess the performance of a system in correctly identifying an individual from a gallery of candidates based on their gait patterns. It measures the accuracy of the top-ranked prediction, considering only the most probable match. Rank 1 accuracy can be calculated as follows:\nRank 1 Accuracy\n$\\frac{Number \\space of \\space correctly \\space identified \\space individuals \\space at \\space rank \\space 1}{Total \\space number \\space of \\space individuals} * 100$ (12)\nThis metric focuses on the top-ranked prediction, indicating the system's ability to correctly match"}, {"title": "4.6 Individual Gait Differences", "content": "In the context of this study, we have examined the latent space representation of gait patterns using encoded vectors generated by a computational model. The encoded vectors effectively capture the complex intricacies of gait dynamics, providing a robust method for assessing the similarities and variations in individuals' gait patterns.\nIn order to measure the magnitude of these disparities, we have utilized the Euclidean distance metric. The calculation of the Euclidean distance between two vectors offers a direct and uncomplicated method for quantifying their dissimilarity inside a multi-dimensional space. The method considers the magnitude of variations along each dimension and calculates the Euclidean distance between the ends of the two vectors. A higher Euclidean distance observed in the encoded vectors indicating gait patterns indicates a more pronounced disparity in the gait dynamics among the individuals.\nTable I presents a comprehensive quantitative analysis of the Euclidean distances across various individuals' gait patterns. This facilitates an in-depth analysis of the data, which is organized in an 8 x 8 symmetric matrix. The results of our study provide novel insights regarding the unique characteristics of gait patterns. When examining the walking patterns of a single individual, it is constantly observed that the distances are almost zero. The observed result confirms the model's capacity to effectively capture the fundamental regularity that characterizes an individual's walking pattern, as expected. In contrast, significant variations in Euclidean distances are observed when comparing the walking patterns of various individuals. Greater values are suggestive of significant variations from the fundamental gait patterns, so suggesting the existence of distinct gait dynamics among individuals. Figure 7 visually represents the variation in encoded gait vectors among random 15 individuals. The heatmap utilizes color gradients to highlight the magnitude of variations, with stronger colors representing more significant disparities."}, {"title": "4.7 Comparison with the SOTA Methods", "content": "In this section, we present a comprehensive analysis of the empirical results obtained from our experiments, focusing on a meticulous evaluation of our method's performance. Our assessment covers the utilization of four distinct datasets, enabling a thorough comparative analysis against existing models. For the CASIA-B dataset, our evaluation encompasses model-based gait recognition methods and also outperforms several state-of-the-art approaches. In the appearance-based category, we consider contemporary models such as VTM [85], ViDP [86], LRDF [77], C3A [87], MGANs [76], CNN [75], GaitSet [14], Gaitnet [39], GaitPart [15] and Gaitref [88], relying on visual appearance for recognition. The assessment includes Rank-1 Accuracy for normal walking (NM) at various camera viewpoints (54\u00b0, 90\u00b0, and 126\u00b0), along with the mean accuracy across these viewpoints.\nSimultaneously, the evaluation covers model-based approaches, including PoseGait [36], GaitGraph [37], GaitGraph2 [89], and our proposed BiGRU-dualStack model. Model-based methods aim to leverage inherent structures and dependencies within gait data for recognition. Table II summarizes the Rank-1 Accuracy results for the considered models. Notably, in the appearance-based category, GaitSet [14], GaitPart [15] and Gaitref [88] exhibit high accuracy, with the proposed BiGRU-dualStack model demonstrating competitive performance. In the model-based approach, our proposed Siamese biGRU-dualStack outperforms the other compared methods in terms of accuracy across different camera viewpoints. In comparison with other state-of-the-art methods such as GaitMixer, notable differences emerge in terms of dataset scope, model architecture, and evaluation metrics. GaitMixer achieved a mean Rank-1 accuracy of 95.8% (NM) on the CASIA-B dataset across angles (54\u00b0, 90\u00b0, and 126\u00b0), considering 60 frames from the middle of the sequence data. In contrast, our BiGRU-dualStack method considered only 6 frames. Specifically, Gait Mixer achieved a high Rank-1 accuracy on CASIA-B across 54\u00b0 and 90\u00b0, slightly outperforming our BiGRU-dualStack. However, our method demonstrated its robustness by excelling on a broader range of datasets, including SZU, OU-MVLP, and Gait3D.\nAdditionally, Table III shows the comparison of our Siamese BiGRU-dualStack approach on the SZU RGB-D Gait dataset with the GEI+PCA [90], GES [78], SPAE [79] and GaitNet [39] methods, which are well-known approaches in gait recognition. The models were trained using the gait data of the first 49 subjects and the rest were used for testing.\nAccording to the results in Table III, our Siamese biGRU-dualStack achieved higher accuracy compared to the GEI+PCA, GES, SPAE, and GaitNet methods on the SZU RGB-D Gait dataset. This suggests that our proposed approach is effective in handling the RGB-D gait data and extracting discriminative features for accurate recognition.\nTo evaluate the proposed method's generalization, an experiment was conducted on the OU-MVLP dataset [80], known as the largest public gait dataset. The dataset encompasses a substantial number of objects, and the comparison results are displayed in Table IV. Following strict adherence to"}, {"title": "4.8 Ablation Study", "content": "The study aimed to evaluate the impact of different recurrent neural network (RNN) architectures on the CASIA-B(90\u00b0) and SZU datasets. Specifically, we investigated the performance of RNN, LSTM, GRU, Bidirectional RNN [96], Bidirectional LSTM [97], Bidirectional GRU, 2-stacked Bidirectional RNN, 2-stacked Bidirectional LSTM and biGRU-dualStack. Table VI presents the performance results of the ablation study for the CASIA-B and SZU datasets.\nThe basic RNN architecture achieved lower accuracy compared to other architectures, indicating that it struggles to capture and utilize long-term dependencies in gait sequences. The vanishing gradient problem is a common issue with basic RNNs, which hampers their ability to effectively model long-term dependencies. On the other hand, LSTM (Long Short-Term Memory) networks outperformed the basic RNN architecture. LSTMs are designed to overcome the vanishing gradient problem by incorporating a memory cell and gating mechanisms, which enable them to retain and propagate relevant information across long sequences. The higher accuracy (SZU: 92.53%, CASIA-B(90\u00b0): 90.33%) and lower loss (SZU: 11.13%, CASIA-B(90\u00b0): 13.31%) suggest that LSTMs are effective at capturing the complex temporal dynamics in gait sequences. However, GRU (Gated Recurrent Unit) networks achieved competitive results, but slightly lower than LSTMs. GRUs have a simplified gating mechanism compared to LSTMs, resulting in fewer parameters. While they may not capture long-term dependencies as effectively as LSTMs, they can still model temporal dependencies reasonably well. Bidirectional variants of RNN, LSTM, and GRU architectures incorporate information from both forward and backward directions of the input sequence. This allows them to leverage past and future context simultaneously, leading to a more comprehensive representation of gait patterns. Consequently, the bidirectional variants generally outperformed their unidirectional counterparts, achieving higher accuracy and lower loss. Furthermore, adding multiple stacked layers further enhances the model's capacity to learn complex representations. The dual-stack architectures, whether RNN, LSTM, or GRU, demonstrated improved performance compared to their single-layer counterparts. The additional layers enable the model to capture more intricate temporal dependencies and achieve better discriminative power for gait recognition.\nConsidering the performance metrics and the goal of accurate gait recognition, the biGRU-dualStack architecture was chosen as it achieved the highest accuracy with the lowest loss on the SZU and CASIA-B datasets. The bidirectional nature of GRU layers and the utilization of dual stacking contribute to its ability to effectively model gait patterns from both directions and capture complex temporal dependencies."}, {"title": "5 Discussion", "content": "In this research paper", "Selection": "We examine the influence of landmark selection on the accuracy and robustness of human gait identification. The human gait is a unique biometric characteristic that can be used for person recognition. We employed a state-of-the-art landmark detection system", "subsets": 11, "Bags": "We also explore the impact of clothing, specifically loose cloth, and objects like bags, on the accuracy and reliability of human gait identification using the Mediapipe landmark detection system.\nDuring our experiments, we observed that landmark detection and subsequent gait identification performed exceptionally well on individuals wearing regular clothing. Human pose estimation algorithms, such as Mediapipe, heavily rely on detecting specific body landmarks to accurately infer body postures and movements. While the system has demonstrated impressive performance on human subjects wearing regular clothing, it is essential to assess its effectiveness when dealing with individuals wearing loose or baggy clothing. However, we encountered challenges when dealing with subjects wearing loose clothing.\nThe landmark detection using MediaPipe for these individuals was slightly reduced compared to subjects in regular attire. The loose fabric of the clothing tended to obscure some key body landmarks, leading to reduced accuracy in certain poses. Consequently, the pose estimation algorithm exhibited challenges in accurately tracking body joints and postures in such instances. Similarly, carrying objects such as bags also impacted the accuracy and reliability of human gait identification using MediaPipe as it led to missing some landmarks. However, when certain body parts were partially obstructed by the bags, the algorithm efficiently inferred the missing landmarks based on their spatial relationships with other detected joints. Table VIII summarizes the accuracy of individuals in normal walking (NM), walking while carrying a bag (BG) and walking with a coat (CL) at different angles. The mean"}]}