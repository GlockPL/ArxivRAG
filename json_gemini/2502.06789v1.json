{"title": "Information-theoretic Bayesian Optimization: Survey and Tutorial", "authors": ["Eduardo C. Garrido-Merch\u00e1n"], "abstract": "Several scenarios require the optimization of non-convex black-box functions, that are noisy expensive to evaluate functions with unknown analytical expression, whose gradients are hence not accessible. For example, the hyper-parameter tuning problem of machine learning models. Bayesian optimization is a class of methods with state-of-the-art performance delivering a solution to this problem in real scenarios. It uses an iterative process that employs a probabilistic surrogate model, typically a Gaussian process, of the objective function to be optimized computing a posterior predictive distribution of the black-box function. Based on the information given by this posterior predictive distribution, Bayesian optimization includes the computation of an acquisition function that represents, for every input space point, the utility of evaluating that point in the next iteraiton if the objective of the process is to retrieve a global extremum. This paper is a survey of the information theoretical acquisition functions, whose performance typically outperforms the rest of acquisition functions. The main concepts of the field of information theory are also described in detail to make the reader aware of why information theory acquisition functions deliver great results in Bayesian optimization and how can we approximate them when they are intractable. We also cover how information theory acquisition functions can be adapted to complex optimization scenarios such as the multi-objective, constrained, non-myopic, multi-fidelity, parallel and asynchronous settings and provide further lines of research.", "sections": [{"title": "1 Introduction", "content": "The fields of machine learning and artificial intelligence in general have gone through significant advancements in recent years, driven by the development of sophisticated algorithms, the rise of computing and the availability of extensive datasets. However, the successes of these algorithms often depend on the meticulous hyperparameter tuning process, that can be both time-consuming"}, {"title": "2 Fundamentals of Bayesian optimization", "content": "As we have stated in the previous section. Bayesian optimization is a probabilistic model-based approach to optimize expensive black-box functions. Its theoretical foundation, initially proposed by Mockus [35], provides a structured framework for sequentially selecting configurations of a black-box problem, such as hyper-parameter values in the hyper-parameter tuning problem, to minimize (or equivalently maximize) an unknown objective black-box function, f(x). More formally, we seek to obtain an approximation to x* evaluating as less configurations x as possible, as every different evaluation of f(x) is costly:\nx* = argminxex(f(x)),\nwhere X is the configuration input space. In order to solve this global non-convex optimization problem, Bayesian optimization uses two key components: a probabilistic surrogate model M(D) that is fitted according to the dataset of observed configurations D = {X, y} and an acquisition function a(M(x)), which is a criterion that uses the information given by the probabilistic model M(x) to estimate how useful is to evaluate a point of the input space x \u2208 X in order to gain information about the optimum of the problem x* in the following iteration using a trade-off between exploration and exploitation. Both components must be chosen by the practitioner, being the most popular choices for the probabilistic surrogate a Gaussian process model [51], a random forest or a Bayesian neural network. In the case of acquisition functions, the most popular choices are the expected improvement, the probability of improvement or information theoretical acquisition functions, which are the topic of this manuscript.\nAs the most popular model choice in Bayesian optimization, Gaussian processes offer a flexible representation to the unknown objective function f(x), being non-parametric models that are equivalent, under certain assumptions, to neural networks and configurable through kernel functions k(x, x'), defining a functional space F where we assume that the objective function belongs to f(x) \u2208 F. In other words, they are non-parametric probabilistic models used to infer a distribution over functions F, or more formally, a collection of random variables, any finite subset of which follows a joint Gaussian distribution. More formally, a Gaussian process is defined by its mean function m(x) and covariance function k(x, x'), written as:\nf(x) ~ GP(m(x), k(x,x')),\nwhere m(x) = E[f(x)] is the expected value of the function and k(x, x') = Cov(f(x), f(x')) describes the covariance between the function values at two points, controlling smoothness and variability.\nIf the assumption about the objective function f(x) belonging to the distribution of functions Fis satisfied, f \u2208 F, then, using a Gaussian process will"}, {"title": "3 Information Theory Applied Concepts in Bayesian optimization", "content": "Having seen the fundamentals of Bayesian optimization we continue this work with a short but rigurous exposition of basic information theory concepts [11] that must be understood to comprehend the information theory Bayesian optimization approaches that will be explained in detail in the following section.\nInformation theory was pioneered by Claude E. Shannon in his seminal paper \"A Mathematical Theory of Communication\" [44]. In this work, Shannon introduced foundational concepts that revolutionized communication and data transmission. Based on that exposition, we extract the concepts that are mostly used for Bayesian optimization.\nWe begin with the Shannon information content of an event x of a random variable X, with probability p(X = x), being defined as:\nSIC(x) = log2(1/p(x)),\nwhere SIC(x) is measured in bits if the logarithm is base 2. This quantity represents the surprise of the outcome x as a result of an experiment of the random variable X. We illustrate in Figure 2 how events with higher probability have a lower surprise. If surprises of the events were lower in average, then, the\nE(SIC(X)) = H(X), as we visualize in Figure 3, where we see how the entropy can be used to measure our knowledge about the values that will result as an outcome of a random variable. More formally, the entropy is given by the\nH(X) = \u2211xEX p(x) log(1/p(x)) = \u2212\u2211xEX p(x) log p(x),\nwhere the sum is substituted with an integral in the case of continuous random variables, expression that is known as the differential entropy of a continuous random variable X, and that uses its probability density function:\nH(X) = \u2212\u222b p(x) log p(x)dx,\nAs the entropy is computed wrt a random variable, it could be from a multi-variate random variable or from a joint distribution, for example let X and Y be random variables then the entropy of the joint random variable is H(X,Y) = \u222b p(x, y) log p(x, y)dxdy.\nThe previous result is particularly useful for Bayesian optimization, as it is usually interesting to know whether an observation gives you information"}, {"title": "4 Information theoretical based approaches to Bayesian Optimization", "content": "Having covered the fundamentals of Bayesian optimization and information theory we now describe in detail the information-theoretic based approaches to Bayesian optimization in chronological order, to justify their motivation.\nThe first work on informational theoretic Bayesian optimization that we will include in the chronology of this topic was the informational approach to\nI(p(x*\\D), p(x*\\DU (x, y))) = H(p(x*\\D)) \u2013 H(p(x*\\D\u222a (x, y))),\nbeing the maximum of the previous acquisition function the input space point with highest expected information gain, being the recommendation of IAGO for being evaluated. In order to compute the conditional entropy H(p(x*\\DU (x,y))), IAGO executes an empirical mean of a discrete conditional entropy through simulations using a quantization operator Q, hence discretizing the image space fo(X) = Q(f(X)). It basically discretizes the space by a finite set of M real numbers, avoiding the need to estimate the differential conditional entropy [49]. However, this is a costly procedure for multi-dimensional problems, as the computational cost of keeping the same accuracy when dimensionality rises leads to a exponential increase of complexity, being desirable to directly minimize the entropy in the original input space X.\np(x*) = p(x* = argmin f(x)) = \u222bF p(f) \u03a0x=x* \u03b8(f(x*) - f(x))df,\nsuch that the @ factors are heaviside functions, hence being one only if the point is the extremum and 0 otherwise. As the previous expression is the expectation on the functional space F defined by the Gaussian process, we find that the previous expression is equivalent to the IAGO expression but considering that certain functions have more probability than others in the functional space, which is more rigurous theoretically than just formalizing the modelization of\np(x*) = \u222bfern N(f\\\u03bc, \u03a3) \u03a0x\u2260x* \u03b8(f(x*) \u2013 f(x))df,\nsuch that N are the discrete locations obtained in the discretization and the prior over functions p(f) is a multivariate normal given by the Gaussian process evaluated on those points. However, the previous integral is intractable, but the authors solve it through an approximation qmin given by the variational inference expectation propagation algorithm. If you are not familiar to these complex approximation methods to integrals we refer to a variational inference tutorial [9] and a expectation propagation exposition [34]. Although the approximation is effective, its computational complexity O(N4) incurs in problems in high-dimensionality scenarios, as the resolution of the space will be low in these settings. Finally, the previous modelization is used to obtain the information gain of every possible point in the input space using a first-order approximation of the expected change in the entropy of p(x*), as one advantage of the expectation propagation approximation qmin to p(x*) is that it provides analytic derivatives, that can be used to optimize the acquisition function. For more technicalities on Entropy Search, we refer the reader to its paper [22] and to the Gaussian probabilities and the Expectation Propagation algorithm paper that explains the steps followed in the approximation of the Gaussian integral and the heaviside factors [12].\na(x) = H(p(x*\\D)) \u2013 Ep(y\\D,x) (H(p(x*\\DU {x,y}))),\nwhere we can use the symmetric property of the mutual information to obtain the following equivalent acquisition function, which is the one approximated by predictive entropy search:\na(x) = H(p(y\\D, x)) \u2013 Ep(x*\\D)(H(p(y\\D, x, x*))),\nwhere p(y D, x) is the posterior predictive distribution of y given by the conditioned Gaussian process and the location of the global maximizer. Now, approximations are easier than in the previous case as the first term H(p(y\\D, x)) is just the entropy of a normal distribution plus the noise of the evaluation. The second term, the entropy of the conditional probability distribution H(p(y\\D, x, x*)), however, still demands to use the expectation propagation algorithm to be approximated, where the expression being approximated contains several factors that are used to condition the distribution to the fact that x* is the optimum of the problem. If the reader is interested in those details and how the factors are incorporated into the conditional probability distribution, they are available in the supplementary material of the predictive entropy search paper [25]. As several acquisition functions have been introduced and depending on the problem some outperform others, an entropy search portfolio [43] was proposed as a meta policy u(X|D) or meta acquisition function, that consists on a portfolio of acquisition function that is motivated by information theoretic considerations. Concretely, it basically consists on determining which of the recommendations x \u2208 X made by a set of acquisition functions a(X) minimizes the most the expected reduction of entropy of the minimizer p(x D):\nu(X|D) = argminx1,...,xnEp(y\\D,x) (H(p(x*\\D\u222a {x,y}))),\nwhere n is the countable size of the set of acquisition functions a(X) of the portfolio, so intuitively we are selecting in each iteration the acquisition function that minimizes the expected entropy about the minimizer. Interestingly, this meta-policy could be applied to any number of information-theoretic acquisitions and other acquisitions."}, {"title": "4.1 Bayesian optimization applied to complex scenarios", "content": "Until now we have described how we can use information-theoretic concepts in vanilla Bayesian optimization, where we want to obtain a configuration whose value approximates the optimum of a objective function f : X \u2192 R. However, all the approaches presented previously can be adapted to more complex scenarios where we can also apply Bayesian optimization [18]. In this section, we briefly illustrate some of these approaches and the problems that they solve."}, {"title": "4.1.1 Constrained Bayesian optimization", "content": "Constrained Bayesian optimization only allows as solutions to the problem those points where a set of m black-box constraints c(x) are validated. More formally, addresses problems of the form:\nmax f(x) subject to ci(x) \u2264 0 \u2200i \u2208 {1, . . .,m},\nwhere f(x) is the black-box objective to be maximized, and c\u2081(x) are black-box constraint functions usually modeled as independent from each other and the objective. We provide a visualization of an example in Figure 6. In this scenario,"}, {"title": "4.1.2 Multi-objective scenario", "content": "Multi-objective black-box optimization focuses on solving problems where not only one but a set of conflicting black-box objectives f(x) must be satisfied. The goal is hence to approximate the Pareto frontier y, which represents the set of trade-off solutions where no objective can be improved without degrading another. The key concept here is that we can not model the entropy of the optimum but about different random variables associated with the solution of the multi-objective problem, such as the Pareto set X* defined as:\nX* = {x \u2208 X \\ \u2260 x' \u2208 X, f(x') < f(x)} .\nStrategies like PESMO [23] and MESMO [4] are direct generalizations of the MES and PES approaches seen in the previous section. Concretely, that guide the search by selecting evaluations that maximize the expected information gain about the Pareto set X* in the case of PES and about the Pareto frontier Y in the case of MES. We illustrate the Pareto frontier in Figure 7. For example, in the case of PESMO, using the symmetric property of mutual information, we obtain that now the conditional distribution is conditioned on the Pareto set X* and the expectation is over the distribution of the Pareto set X*:\na(x) = H(y | D, x) \u2013 Ex* [H(y | D, x, X*)]"}, {"title": "4.1.3 Constrained multi-objective scenario", "content": "We can combine the previous two scenarios into a single one, where we want to estimate the Pareto set X* of a set of conflictive black-box objectives f(x) such\nmin f1(x),..., fk (x)\nXEX\ns.t. C1 (x) \u2265 0, . . ., cc (x) \u2265 0,\nwhere k is the number of black-box objectives and c is the number of black-box constraints. Hence, the would like to estimate the feasible Pareto set F*. Each black-box can be modelled using a Gaussian process that is going to be conditioned with the observations of the problem D. Previous approaches can be generalized towards this problem such as a generalization of PES called predictive entropy search with multiple objectives and constraints, PESMOC [20] that search for the location of the feasible Pareto set by adding together the non-Gaussian factors belonging to the validation of the constraints and the validation of a point belonging to the Pareto set that are approximated as Gaussians using the expectation propagation algorithm, which makes the approach engineeringly very difficult to implement. Analogously, we find in the literature a generalization of MES, called max-value entropy search for multiple objectives and constraints, MESMOC [5], that is easier to implement than PESMOC as the approximation is done analytically. In particular, in MESMOC+ [13] the improved version of MESMOC, approximates the following expression with"}, {"title": "4.1.4 Parallel Bayesian optimization", "content": "In previous subsections we are always interested in selecting the point x that maximizes an information-theoretic metric towards some random variable that is related to the optimizer of the black-box function f(x). This relies on the assumption that we only have one sequential evaluator of the objective function. However, this does not need to be the case, as for example in hyper-parameter tuning we may have a computing cluster. Hence, it is also interesting to suggest a batch of L points X that maximizes the information gain about the optimum x*. We illustrate this idea in Figure 8. Several approaches have extended\na(X) = H[p(Y | D, X)] \u2013 Ep(x*\\D) [H[p(Y | D, X, X*)]],\nwhere X is a set of B input space points such that B is the number of points of the batch, X* is the feasible Pareto set, D is the dataset of previous evaluations and Y is the evaluation of X under the black boxes and batches of points. As it can be seen, as the problem is more complex it involve more complexity of the acquisition functions, full of factors to approximate, which makes these approaches powerful but very difficult to implement."}, {"title": "4.2 Other complex scenarios", "content": "In this subsection, we discuss more advanced Bayesian optimization methods that have not been dealt before by focusing on specialized scenarios such as partial evaluations, input noise, multi-fidelity expansions, and multi-objective or high-dimensional tasks that have been tackled using an information-theoretic approach, as in the previous subsections.\nFreeze-thaw Bayesian optimization [47] tackles the challenge of incomplete or interrupted evaluations by dynamically finishing the experiments if they are not going to be worth as a result of loss machine learning curves. It can be done by assessing the information of the curve, whether it is expected to provide better performance or not. Noisy input entropy search [15] extends classical entropy-based criteria by including noisy input variables, refining the entropy search acquisition function with an explicit modeling of input uncertainty to maintain robust predictions. Multi-fidelity MES parallel [16] is an extension of MES to address multi-fidelity scenarios, where some configurations are evaluated using less resources and if they are successful then they are evaluated using more to save resources, that is the idea of multiple fidelities. Alternatively, multi-fidelity multi-objective OES [6] further generalizes this framework by incorporating multiple objectives, also extending the entropy search approach. All the approaches seen until now work successfully for less than 8 dimensions, having trouble with a higher number of dimensions, consequently, high-dimensional entropy search [30] tackles the curse of dimensionality by employing scalable approximations that retain an information-theoretic perspective on exploration and exploitation in high dimensional input spaces. Multi-agent Bayesian optimization [31] introduces decentralized decision-making among coordinated agents, enabling simultaneous exploration with localized updates and a global consensus mechanism that generates collective learning. Multi-objective scenarios seen until now assume independence between the conflictive objectives, but usually they are negatively correlated, henc,e multi-task entropy search [36] exploits correlations across related tasks to share knowledge and speed up convergence, using task-specific covariance structures to guide sampling policies. High-dimensional many-objective entropy search [8] enhances the high-dimensional approach to tackle not only two or three but more objectives in a high-dimensional input space."}, {"title": "5 Conclusions and Further Lines of Research", "content": "This document has shown a comprehensive tutorial and survey on how modern information-theoretic Bayesian optimization approaches work, covering their methodological evolution in a historical fashion after illustrating the basic information-theoretic concepts. We have also illustrated the versatility of these methodologies across a diverse range of complex Bayesian optimization scenarios, like the constrained multi-objective scenario, emphasizing the adaptability of information-theoretical Bayesian optimization to complex problem settings. As these approaches are difficult to implement in practice, to support further exploration, we have provided the references for each approach, where all the technicalities are explained properly.\nThere are several future research directions that are worth exploring for advancing this field. Key areas include developing improved approximations for the entropy search acquisition functions, such as using approaches as Power Expectation Propagation [33], or exploring generalized notions of entropy, like the Sharma-Mittal entropy, to develop more general acquisition functions [2]. In order to determine the hyper-parameters that those acquisition functions are going to have, meta Bayesian optimization approaches, including bandits and Bayesian model selection, represent another line for innovation in designing information-theoretic acquisition functions. Additionally, as they are going to appear a lot of acquisition functions based on those concepts, it is going to be useful to study theoretical guarantees of convergence of these acquisitions, to determine for example upper bounds on the cumulative regret of these approaches. Another interesting research direction includes adapting and selecting probabilistic surrogate models for specific problems, including ensembles of them. Finally, enhancing scalability through information compression techniques of the evaluations to compete with metaheuristics in problems where cheaper evaluations can be done is also a setting worth to explore."}]}