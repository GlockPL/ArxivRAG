{"title": "Overview of Factify5WQA: Fact Verification through\n5W Question-Answering", "authors": ["Suryavardan Suresh", "Anku Rani", "Parth Patwa", "Aishwarya Reganti", "Vinija Jain", "Aman Chadha", "Amitava Das", "Amit Sheth", "Asif Ekbal"], "abstract": "Researchers have found that fake news spreads much times faster than real news [1]. This is a major\nproblem, especially in today's world where social media is the key source of news for many among the\nyounger population. Fact verification, thus, becomes an important task and many media sites contribute\nto the cause. Manual fact verification is a tedious task, given the volume of fake news online. The\nFactify5WQA shared task aims to increase research towards automated fake news detection by providing\na dataset with an aspect-based question answering based fact verification method. Each claim and its\nsupporting document is associated with 5W questions that help compare the two information sources.\nThe objective performance measure in the task is done by comparing answers using BLEU score to\nmeasure the accuracy of the answers, followed by an accuracy measure of the classification. The task\nhad submissions using custom training setup and pre-trained language-models among others. The best\nperforming team posted an accuracy of 69.56%, which is a near 35% improvement over the baseline.", "sections": [{"title": "1. Introduction", "content": "Manual fact-checking is a laborious process where journalists must scour multiple online and\noffline sources, assess their reliability, and synthesize the information to reach a final verdict,\noften taking hours or days depending on the claim's complexity. With the rise of social media\nand rapid news dissemination, automated fact-checking has emerged as an important AI problem\nto combat the dangers of fraudulent claims masquerading as reality. As per surveys from Statista\n[2], no country had over of 80% of its people trusting media, with the number being below 50%\nin USA.\nThe preceding paragraph highlights the importance of such tasks and the requirement for\na capable automated fact verification pipeline. Aiming to encourage development of such"}, {"title": "2. Related Work", "content": "Several datasets and shared tasks on fact verification have been introduced to benchmark\nadvancements in automated fact-checking, encouraging the development of robust algorithms.\nOver the years, researchers have produced a wide range of datasets and articles addressing the\nmany challenges involved in automated fact checking.\nAn avenue of research deals with the analysis of the claim without an associated evidence,\nsome examples include analyzing linguistic characteristics, stylometry etc. [7, 8, 9]. There\nalso exists active research towards multilingual claim detection [10, 11] and fact checking\nwith respect to a specific domain [12, 13, 14]. Multi-modal datasets have also been explored\nwith datasets for image, audio and video based fact checking [3, 15, 16, 17]. Datasets with\ntextual claim and supporting evidence to validate or refute the claim are predominantly used,\nincluding datasets that provide a synthetic claim for the evidence [18, 19]. Shared tasks have\nalso proven to be great avenues to introduce fact verification datasets and establish fact checking\nmethodologies [17, 18, 20, 21].\nFAVIQ [22] has claims authored by crowdworkers and the authors present a fact checking\napproach that uses information seeking questions to classify a given claim-evidence pair as\nfake or not. In Factify5WQA, we add to the fact checking task by incorporating 5W questions\nthat help highlight relevant context, with respect to the claim. We integrate data from several\nbenchmark fact-checking datasets and complement them with 5W questions and answers.\nDetails of our dataset are provided in next section and in [23]."}, {"title": "3. Task Details", "content": "The Factify5WQA dataset [23] was constructed with prior fact checking work as its backbone.\nThe dataset was curated by manually inspecting and selecting a subset of claims from six\nexisting fact-checking datasets - FEVER [18], VITC [24], Factify 1.0, Factify 2.0, FaVIQ [22], and"}, {"title": "3.1. Evaluation", "content": "As described in the previous sub-section, the dataset contains a set of questions for each given\nsample along with answers based on the claim and evidence respectively. Further each sample\nis assigned a class with respect to the relation between the claim and evidence i.e. Support,\nNeutral or Refute. The approach we define to evaluate performance on this dataset is with\nthe use of BLEU score. The average BLEU score for the answers from the claim and evidence\nare compared to a threshold. If it is crosses the threshold, which we set to 0.3, and the label\nprediction matches the test data, the prediction is considered correct. The final score for the\ntask is simply the percentage of such predictions i.e. # of correct answers \u00f7 # total samples."}, {"title": "4. Participating systems", "content": "For the baseline model, we setup the pipeline shown in Figure 1. We passed the claim and\nevidence to the Flan model [26] along with the 5W questions. For each question and claim/evi-\ndence pair, the prompt to the generative model is to generate an answer to the question based"}, {"title": "5. Results", "content": "Table 3 shows the results all final submissions to the task along with the baseline. Team\nTrifecta [28] is the best performing team with an improvement of about 35% over the baseline.\nThey also outperform the team that places second in the shared task by over 20%. The second\nand third team i.e. SRL_Fact_QA [29] and Jiankang Han, are seperated only by 0.05%.\nWhile all teams outperformed the baseline, it can be seen in Table 4 that all participants had\npoor results for the Support category. On the other hand, all teams made the correct predictions\non nearly 50% of the Neutral or Refute samples, if not more. We note that, as per the BLEU\nscores, Team Trifecta got about 15% of the generated answers incorrect while the other teams\ngot 33% incorrect. Finally, we can see that team trifecta has the best performance on all the\nclasses."}, {"title": "6. Conclusion and Future Work", "content": "In this paper, we describe the the shared task Factify5WQA and provided a summary of partici-\npating systems. We saw that teams used LLMs or BERT. The best performing team achieved a\nscore of 69.56%, which shows that the problem remains unsolved.\nFuture work could include expanding the 5wQA framework to multi-modality (text + images)\nand to other languages."}]}