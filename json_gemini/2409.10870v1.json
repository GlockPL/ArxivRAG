{"title": "ADAPTIVE LARGE LANGUAGE MODELS BY LAYERWISE ATTENTION SHORTCUTS", "authors": ["Prateek Verma", "Mert Pilanci"], "abstract": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational attention shortcuts. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.\nIndex Terms\u2014 Attention, LLMs, Adaptive Models.", "sections": [{"title": "1. INTRODUCTION", "content": "Transformer architectures have revolutionized modern AI advancements, powering almost all diverse fields with a unified approach [1]. Modern Transformer language models consist of a simple stack of Transformer decoder blocks iteratively in modalities such as text [2], raw audio waveforms[3], acoustic and music tokens [4, 5, 6], videos [7] to name a few. The flexibility of the architecture to handle any input has generated an enormous interest in the field, and the advent of multi-modal architectures proposed by Google with its Gemini family [8] or multi-modal models like [9] that can allow these models to hear, see and read. However, the information is processed sequentially in all these architectures, one block after another, to allow features to learn representations at multiple levels and depths. In this paper, we explore if we can make these architectures' depth and context adaptive depending on the contents of the input token. We allow the final layer's attention mechanism to bypass intermediate layers' computation and directly contribute to the next token prediction. This would allow more straightforward tokens present in the input, which are easier to predict, to directly learn features in shallow layers to predict the outout. It can thus better utilize and reserve deeper complex self-attention blocks for tougher token predictions. Mainly two ideas inspired this paper \u2013 First, several papers utilize intermediate layers that capture information at various scales [10, 11, 12, 13], for solving downstream tasks in natural language such as sentiment analysis, word representations, etc. These intermediate representations are often helpful for capturing useful features of text at multiple scales. Further, such behavior is observed for modalities like speech, where [14] showed it that different layer representations were useful for transformer-based architecture for several downstream tasks related to speech recognition and spoken language understanding. Even for a seq-to-seq architecture, Whisper-AT [15] showed the importance of intermediate layers by using a frozen Whisper-encoder and a temporal transformer to learn feature mapping to classify acoustic sounds. With all of these, we want to state that there is enough evidence to show that intermediate Transformer layers capture meaningful information, capturing different aspects of input signals across modalities. In this paper, we answer\n1. Can we leverage intermediate layer embeddings for improving pre-training of a Large Langauge Model?\nThe second evidence we address in our architecture is that for certain kinds of inputs, the output of several Transformer layers can be approximated by a single MLP layer, as shown in a recent paper [16]. This is fascinating find as they showed that for specific input tokens, we can bypass mapping of several layers in depth and context to predict the same output as a simple function approximation. Our paper allows this behaviour to be learned during pre-training. This allows our model to avoid wasting complex attention layers when learning trivial feature maps. This is the basis of our paper \u2013\n2. Can we make LLM context and depth adaptive with input content to bypass several layers and tokens in context by learning simple feature maps for the final layer?\nThis differs from Pathway's architecture [17] proposed by Google (Palm) as they have separate streams of MLP and at-"}, {"title": "2. DATASET", "content": "We utilize four publicly available commonly used datasets for showcasing the powerfulness of our method, with context length of 512. These are chosen from three modalities namely speech, symbolic music and natural language. For speech, we take LibriSpeech corpus [18], which consists of 960 hours of spoken speech for training. We convert it into discrete tokens using ENCODEC [19]. We use the entire training corpus i.e. train-clean-100/360, and train-other-500 as our training data. This gave us a total of 270 million tokens. We utilize the coarsest codebook as proposed in Vall-E [20] to report our results. The model is trained with the vocabulary size of 1024 tokens. For natural language we used two dataset, using two different tokenizer. The first is character level language modelling on text-8 which uses 26 characters and an extra space token as its vocabulary with all other text or symbols removed and replaced with space. This choice removes any biases that may occur due to tokenizers and the dataset itself is widely reported. It has 100M characters from wikipedia. We also use Wiki-103 dataset with GPT-2 tokenzier that has 50,257 tokens. For both of them, we randomly crop 1 million crops with 512 context length yielding a total of 512 million tokens. This is done to have variable context for the same token to make the data more robust. Finally, we use MAESTRO [21], which consists of 1000 piano pieces from classical music in MIDI format. We convert the MIDI track to discrete tokens using Google's tokenizer yielding 388 sized vocabulary. We compare the performance of our architecture with and without the modifications proposed. The paper is written in an academic setting on shrunk down version of GPT architecture ubiquitously used for language modelling."}, {"title": "3. METHODOLOGY", "content": "In all methods, we use Transformer decoder layers. The architecture for all modalities is the same except for attention mechanism of the final layer. We have already mentioned vocabulary sizes for different modalities in the dataset section. For all the experiments, as a baseline, we use a stack of 10 Transformer decoder layers with a context length of 512, a model dimension of 128, and eight attention heads. The size of the feed-forward dimension was chosen to be four times that of the model dimension. The models were trained for 12-13 epochs, thus seeing more than 10 billion tokens. All architectures were trained with Adam optimizer with starting learning rate 2e-4 and then divide by half mid-way. We take the same architecture with the following modifications for our proposed architecture. The output of each Transformer layer in the second, fourth, and eighth layers is passed to a feature learning module, a 2-layer MLP with 1024 neurons, followed by a dense layer of the size of the model dimension. This allows us to learn feature maps out of embeddings at intermediate layers from depths for all the tokens present in our context. A standard decoder only Transformer formulation is,\n$x^{l+1} = x^{l} + MLP(LayerNorm(x^{l} + Attention(LayerNorm(x^{l}))))$\nwhere $x^{l+1}$, is the output of the $l^{th}$ Transformer decoder layer. Our paper introduces an MLP layer where we learn features directly from $x^{l}$, and we call it $x^{l}_{feat}$ which is,\n$x^{l}_{feat} = MLP(x^{l})$. Now, we keep the Transformer decoder blocks similar to the baseline architecture except for the final layer decoder block where we allow features with Attention shortcuts: We allow the final prediction at every token to"}, {"title": "4. RESULTS AND DISCUSSION", "content": "We wanted to showcase the power of our architecture across four different datasets and modalities. As described in the previous section, we use text-8, Wiki-103, LibriSpeech, and MAESTRO. We compare the performance of our architecture with that of the baseline only in terms of negative log-likelihood scores. This is in line with other papers in natural language processing [23, 24] that report the performance of architecture only in terms of advancement of how well it does in pretraining in likelihood scores. Neural architectures scale well with the size of the architecture [25]. In addition, keeping all the aspects of the training recipe the same, i.e., training corpus, optimizers, and post-training like DPO or RLHF, the reason for the improved performance of LLMs is their ability to predict the next token correctly. Large Language Model-based generative architectures utilize sampling from the distribution of the next token to solve generative tasks, e.g., music generation, text generation, etc. Large architecture performs better because fewer errors in predicting the next token are present due to the probability distribution closer to the desired distribution. Further, the importance of likelihood-based metrics can also judged by the fact that in latest advanced models, like Gemini, these scores were conspicuously absent and were left blank from the results and plots. Hence, with this background, we will begin discussing our results.\n4.1. Performance across modalities\nFor all four datasets mentioned in Section 2, we perform our experimentation as described in Table 1. We can see that our methodology is generic across four datasets. We report how well we do in terms of next token prediction and report the negative log-likelihood loss. We can see that we achieve the best speed-up for the MAESTRO corpus; one reason is that"}, {"title": "4.2. Ablation Studies", "content": "For Wiki-103, we carry out ablation studies on our proposed architecture as compared to a baseline model to see the effects of i) allowing the final layer Transformer decoder to attend to all of the intermediate representation and ii) only allowed to attend to intermediate representation in the middle layers iii) Seeing the effects of without/without adding MLP features. For baseline architecture, we utilize the same architecture described for Wiki-103: a 10-layer decoder-only architecture with eight heads and 128 as model dimensions. We can see that learning MLP feature maps for all the intermediate layers (i.e., layers 2, 4, and so on) is helpful compared to just attending to the final and middle layers. Further, if we remove the MLP features and attend to the original embeddings, we still beat we see a substantial gain in learning MLP features, though we still beat our baseline architecture by a thin margin."}, {"title": "4.3. Scaling with Size", "content": "Since the experiments carried out in this paper were at a small academic scale, we see the effect of model dimension for one of the datasets. For this, we conduct two experiments with"}, {"title": "4.4. Attention Maps", "content": "We would expect the architecture to learn complex dependencies as per the contents of the input tokens. This is because, for tokens where it is simpler to predict the next token, the model need not waste layers to learn a linear or an MLP mapping. It must be able to attend to it directly, bypassing the computations afterward. To show this, we take the attention maps of the last layer that attends to all intermediate representations. We take an input with sequence length 512 from the Wiki-103 dataset and compute the attention maps. As shown in Figure 3, for every token in the context length, different attention heads give weightage to the learned MLP features from different layers instead of keeping the weights constant. Further, these change for every token for different attention heads, giving the model much more flexibility than a simple serialized operation. We see in Figure 3 that in almost all of the heads, for initial tokens, most of the weight is given to the final layers. This is intuitive, as in the initial layers, there is minimal amount of context present. Therefore, our architecture tries hard and utilizes deeper layers for prediction."}, {"title": "5. CONCLUSION AND FUTURE WORK", "content": "We have showcased a method that improves large language model pre-training on small academic-scale architectures by allowing the neural architecture to attend to the intermediate feature representations. We give evidence that the attention map attends to all of the intermediate representations depending on the contents of the input signal. This allows the architecture to develop attention shortcuts as it deems fit to directly attend to utilize shallow representations if necessary, thereby freeing the subsequent layers to carry out heavier computations. Finally, exploring several possible variants of our model will be interesting, for example, combining it with pathways and other kinds of efficient neural architecture."}]}