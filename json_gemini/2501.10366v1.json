{"title": "Participatory Assessment of Large Language Model Applications in an Academic Medical Center", "authors": ["Giorgia Carra", "Bogdan Kulynych", "Fran\u00e7ois Bastardot", "Daniel Kaufmann", "No\u00e9mie Boillat-Blanco", "Jean Louis Raisaro"], "abstract": "Although Large Language Models (LLMs) have shown promising performance in healthcare-related applications, their deployment in the medical domain poses unique challenges of ethical, regulatory, and technical nature. In this study, we employ a systematic participatory approach to investigate the needs and expec- tations regarding clinical applications of LLMs at Lausanne University Hospital, an academic medical center in Switzerland. Having identified potential LLM use-cases in collaboration with thirty stakeholders, including clinical staff across 11 departments as well nursing and patient representatives, we assess the current feasibility of these use-cases taking into account the regulatory frameworks, data protection regulation, bias, hallucinations, and deployment constraints. This study provides a framework for a participatory approach to identifying institutional needs with respect to introducing advanced technologies into healthcare practice, and a realistic analysis of the technology readiness level of LLMs for medical applica- tions, highlighting the issues that would need to be overcome LLMs in healthcare to be ethical, and regulatory compliant.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) represent a significant step forward in computer systems' ability to process and work with text-based information. The healthcare field, which routinely handles a large amount of written documentation, sees LLMs as a useful tool to help overcome pressing issues (Thirunavukarasu et al., 2023; Clusmann et al., 2023) such as lack of qualified clinical professionals, alarm fatigue, an ever-growing administrative burden, and increasing pressure due to an aging society. Indeed, LLMs have shown promising performance in several healthcare tasks including medical writing, editing, summarization (Michael G. Madden, 2023; Umeton et al., 2024; Wu et al., 2024).\nIn this study, we use a systematic participatory approach to evaluate the emerging needs and expec- tations regarding LLM applications in a European academic medical center-Lausanne University Hospital in Switzerland-with a representative pool of thirty healthcare professionals across different roles and hospital departments, and discuss technical, ethical, and regulatory feasibility of these use-cases. Our analysis shows what technical advances, institutional changes, and regulatory updates would be needed to implement these systems effectively."}, {"title": "2 Methods", "content": "We organized a working group (WG) of institutional stakeholders to discuss the needs, opportunities and strategic priorities regarding LLMs. The working group included 30 participants from across the hospital: clinical staff from 11 departments, nursing and medical directors, patient advocates, legal experts, IT staff, clinical informaticians, and representativies of the Biomedical Data Science Center of the hospital. We held regular monthly discussions among the members over a six-month period.\nPreliminary discussions The initial meeting focused on discussing the vision, brainstorming institutional needs, and identifying potential applications where LLMs could add value. In the second meeting, we established key evaluation criteria to score the use-cases. Evaluation criteria were mostly based on transversal goals such as potential impact on patient engagement, quality of care, reduced administrative workload.\nPhase I: Survey Following the identification of a broad list of use-cases and their evaluation criteria, we designed and distributed a structured survey among the WG members. The survey aimed to assess the use-cases which seemed most promising to the WG members, as well as obtain the initial assessments of the application in terms of the identified criteria. Based on the results of the survey, we identified seven most voted applications, which we studied further in the next phase. See Appendix A for details.\nPhase II: Participatory workshop We held a participatory workshop in which the members of the WG organizing team presented in detail each of the seven use-cases identified as most popular in the survey, and facilitated a discussion among the members on various aspects of the use-cases. After the discussions, we conducted an anonymous survey in which the members ranked each of the applications according to the criteria established previously."}, {"title": "3 Results", "content": "Next, we present the outcomes of the WG activities: the identified framework for assessing LLM applications, and the ranking of LLM applications. Then, we comment on the technical and regulatory feasibility of these applications."}, {"title": "3.1 Working Group Outcomes", "content": "Vision and key evaluation criteria Based on the outcomes of the WG, we identified five major areas of improvement at the hospital: quality of care, patient engagement and satisfaction, admin- istrative burden, research and digital innovation, and continuous education. Through discussions, brainstorming, and literature review, we then identified specific use-cases for LLMs applications that target these areas. See the complete list in Appendix A.\nMoreover, during the discussions, the WG identified the following key evaluation criteria for appli- cations: institutional impact, impact on patient management and quality of care, impact on patient satisfaction and engagement, whether the impact is measurable, whether the intended use is as software as medical device, and safety, defined as whether the risk of harm is low."}, {"title": "3.2 Technical and Regulatory Feasibility", "content": "Regulation of medical uses Under the United States Food and Drugs Administration (FDA) regulations, European Law on Medical Devices Regulations (MDR), and the recently approved European Union (EU) AI Act, general purpose LLMs would not automatically be classified as medical devices (Mesk\u00f3 and Topol, 2023). It is the intended use that dictates the regulatory framework. As such, LLMs, or general-purpose LLMs, that are developed, fine-tuned or modified in order to serve a more specific medical purpose might be treated as medical devices. Classification as medical device triggers a series of requirements that may be challenging to meet for LLMs. For example, regulatory requirements apply to the entire development lifecycle of the device, not only to the phase where the model is adapted to medical purpose, posing a challenge for models that derive from general-purpose LLM. For these models, the development process most likely did not adhere to stringent medical device regulations. Even if it did, the resulting documentation may not be publicly accessible.\nMoreover, current risk assessment approaches may be inadequate for LLMs. For example, in the case of an assistive chatbot for clinical decision-making\u2014identified as a top priority use-case in our survey-several questions arise: How can we conduct a comprehensive risk assessment considering that the questions that future users may ask are potentially infinite? what role does the context in which the tool is used play in relation to its safety? Should we consider user-training as the main risk mitigation strategy for medical LLMs? Finally, in case of a LLMs-driven adverse event, would for-cause auditing be possible considering LLMs limited explainability? To date, these remain open questions, and proposed risk mitigation strategies focus on extensive user training and consequent user liability. In practice, starting from the assumption that users of medical LLMs tools would be capable of identifying subtle issues in LLMs output. This framework could be particularly problematic for applications such as the chatbot for medical education, where users may have limited abilities to question LLMs-generated content. Regarding clinical support applications, clinical investigations will play a crucial role in LLMs risk assessment, with some investigations already ongoing (Andreoletti et al., 2024).\nThe regulatory framework for continual fine-tuning, or retraining, is another crucial point. In January 2021, the FDA took a step toward addressing this issue by publishing its action plan to facilitate AI-/ML innovation (FDA). The proposition was to regulate AI-powered software as medical device (SaMD) throughout their lifespan, introducing the so-called \u201cpredetermined change control plan.\" The guiding principles for the predetermined change control plans for ML-enabled medical devices were later published in October 2023. With this action the FDA aimed at aligning the speed of regulatory certification with the rapid release of new highly-performant ML models, including LLMs. The EU followed a similar approach in the newly released AI Act (EU), which states that new certification is not deemed for changes in algorithms or algorithms performance that were pre-determined by the manufacturer and pre-assessed at the time of relevant conformity assessment. It remains unclear how this process will work for LLMs as further re-training or fine-tuning typically involve new, larger datasets and a significantly different architecture. Continuous re-training may be required for many of the discussed use-cases, given the continuous advancements of medical care, new guidelines and scientific discoveries.\nHallucinations and omissions In their outputs, LLMs tend to both provide contextually plausible but factually incorrect information, a phenomenon known as hallucinations, as well as potentially omit crucial pieces of information (Ji et al., 2023). Depending on the use-case, these could pose\""}, {"title": "4 Discussion", "content": "Our findings reveal a practical reality: although healthcare professionals see multiple promising applications for large language models in their practice, regulatory and infrastructural constraints significantly limit immediate implementation options. The preference for administrative applications, particularly discharge note generation and summarization, suggests a pragmatic path forward that balances regulatory compliance with feasibility.\nTwo key limitations of our study warrant discussion. First, the conclusions of our participatory methodology, which are limited to the specific context of Lausanne University Hospital, may not generalize across healthcare institutions. That said, we do hypothesize that our findings could largely generalize to similar medium-sized academic medical centers in Western Europe. Moreover, our study provides a framework for institutions to examine their unique needs and constraints through structured stakeholder engagement. Second, despite achieving representation across departments and roles, our participant pool likely shows selection bias, as the self-selected nature of participation in the working group means we may have captured perspectives from professionals who are inherently more enthusiastic about language models in healthcare.\nDespite these limitations, our findings highlight how successful implementation of generative models in healthcare settings requires careful navigation of practical constraints, particularly around medical device regulation and data protection."}]}