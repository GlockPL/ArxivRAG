{"title": "Probe-Free Low-Rank Activation Intervention", "authors": ["Chonghe Jiang", "Bao Nguyen", "Anthony Man-Cho So", "Viet Anh Nguyen"], "abstract": "Language models (LMs) can produce texts that appear accurate and coherent but contain untruthful or toxic content. Inference-time interventions that edit the hidden activations have shown promising results in steering the LMs towards desirable generations. Existing activation intervention methods often comprise an activation probe to detect undesirable generation, triggering the activation modification to steer subsequent generation. This paper proposes a probe-free intervention method FLORAIN for all attention heads in a specific activation layer. It eliminates the need to train classifiers for probing purposes. The intervention function is parametrized by a sample-wise nonlinear low-rank mapping, which is trained by minimizing the distance between the modified activations and their projection onto the manifold of desirable content. Under specific constructions of the manifold and projection distance, we show that the intervention strategy can be computed efficiently by solving a smooth optimization problem. The empirical results, benchmarked on multiple base models, demonstrate that FLORAIN consistently outperforms several baseline methods in enhancing model truthfulness and quality across generation and multiple-choice tasks. Our implementation can be found at https://github.com/nguyenngocbaocmt02/EFI.", "sections": [{"title": "Introduction", "content": "Transformer-based language models (LMs) have revolutionized generative modeling for natural language processing (NLP) (Radford et al., 2019; Brown et al., 2020; Anthropic, 2024; Jiang et al., 2023). The LM training pipeline in various NLP areas typically involves three steps: (i) provide the model with prompts that include demonstrations of the task, (ii) learn from the examples, and (iii) make predictions without additional training. In the generating process, there is evidence (Ji et al., 2023; Rawte et al., 2023; Xu et al., 2024) showing that LM output can contain undesirable contents, such as inaccurate answers, toxic answers, or answers without linguistic meaning.\nOne of the predominant solutions to the above issues is activation intervention (Subramani et al., 2022; Hernandez et al., 2023; Li et al., 2024b; Nguyen et al., 2025). Compared with previous studies on model editing (Zhang et al., 2023) and supervised fine-tuning (Li et al., 2024a), activation intervention alters the model activations during inference time. Therefore, it does not need to alter model weights using a subset of text samples, requiring fewer computational resources. However, new challenges must be addressed in the activation intervention task: (Q1) How to improve the quality of activation intervention through appropriate modeling with theoretical underpinnings? (Q2) How to enhance the computational efficiency of activation intervention by designing a low-cost intervention strategy?\nFor question (Q1), Burns et al. (2022) reveals latent knowledge inside the internal activations of a language model and finds a direction in activation space that satisfies logical consistency properties. Following this observation, several works focus on finding a good intervention systematically by two-stage methods (Yin et al., 2024; Li et al., 2024b; Pham and Nguyen, 2024). In the first stage, they train a classifier (the probe) on the activations of a network to identify a subset of attention heads that are most important for learning the specific task. In the second step, they propose intervention policies based on the probe and geometric transformation, e.g., linear operations (Li et al., 2024b), addictive offset bias vector (Yin et al., 2024), linear transformation using chance constrained programming (Nguyen et al., 2025), and householder transformation (Pham and Nguyen, 2024)."}, {"title": "Related Work", "content": "The preliminary success of these activation intervention methods motivates us to improve the desirable generation of LMs. Specifically, we want to design an inference activation intervention method that does not require the 'detect and rectify' procedure, addressing (Q1) and (Q2) simultaneously. Our core idea is to depict the region of the desirable answers and consider the related low-rank transformation mapping for intervention. Based on these, the parameters in the intervention mapping are the solutions to a nonlinear low-rank optimization problem, which minimizes the distance between the intervention vector and its projection on the region. Compared with the previous region modelings of the LM activation space (Mamou et al., 2020; Janiak et al., 2024), we give the concrete formulation of the region with an analytical projection operator under the suitable distance measure.\nFrom the computational aspect, our FLORAIN intervention method edits the activation vectors on one layer of the Transformer network. This contrasts to ITI (Li et al., 2024b) that intervene different heads spread out in multiple layers of the network. Intervening in one layer, as FLORAIN does, has two significant advantages: First, it provides conditions for parallelization to reduce inference intervention time. Second, since the intervention vectors are concentrated in one layer, the strategy reduces the representation shifts by avoiding misleading intervention in the subsequent layers.\nControllable generation. Model editing (Wang et al., 2023; Zhang et al., 2024) alters the model parameters to control the output, making it a powerful method for controllable generation. Another important category in controllable generation is fine-tuning, which includes Supervised Fine-Tuning (SFT, (Peng et al., 2023; Gunel et al., 2020)) and Reinforcement Learning from Human Feedback (RLHF, (Ouyang et al., 2022; Griffith et al., 2013)). These methods typically require altering model weights, incurring substantial resources and costs for computation.\nActivation intervention at inference time is an emerging technique for controllable generation (Li et al., 2024b; Singh et al., 2024; Yin et al., 2024). Unlike model editing and fine-tuning techniques, the inference time intervention does not require altering the model parameters, leading to cheaper computational costs. Li et al. (2024b) proposes a headwise intervention method for eliciting truthful generated answers of a language model. Singh et al. (2024) considers the optimal transport plan between two empirical distributions to carry out the intervention. LoFit (Yin et al., 2024) identifies a specific subset of attention heads crucial for learning a particular task. It then fine-tunes the intervention vectors in those chosen heads. Another recent work (Pham and Nguyen, 2024) considers doing intervention activation using modified householder transformation based on the linear probe framework.\nRegion Modeling in LM. Various works aim to reveal how the semantic features influence the 'region' of embedding vectors in the transformer-based LM. The work (Mamou et al., 2020) utilizes mean-field theoretic manifold analysis to connect the geometry of feature representations with the linear separability of classes. Janiak et al. (2024) identifies stable regions in the residual stream of Transformers, where the model output remains insensitive to small activation changes but exhibits high sensitivity at the region boundaries. How-"}, {"title": "Nonlinear Low-Rank Intervention Mapping", "content": "This section defines the nonlinear low-rank activation intervention maps. In our probe-free intervention framework, this mapping serves as the computationally efficient intervention function during inference time.\nFor clarity, we consider a specific layer l of the transformer network, and the index of the layer l will be omitted in the following statement. This layer contains H attention heads; each head is of dimension d, so the output of this layer is a $D = d \\times H$ dimensional activation vector. For an input $x_i$, the activation in the layer l is denoted by $a_i = [a_{i1}; a_{i2};\\cdot\u2026\u2026 ; a_{iH}] \\in \\mathbb{R}^D$. Our inference-time intervention will modify the activation vector head-wise for $a_{ih}$ for all $h \\in [H]$.\nThe construction of the low-rank mapping involves two key considerations: (i) the intervention strategy should be generalized to different inputs, and (ii) the intervention strategy should not change a desirable answer too much. Motivated by these requirements, we construct our efficient nonlinear low-rank intervention as follows:\n$f: a \\rightarrow (I+L(a)R^T)a + s$, (1a)\nwhere $L(a) \\in \\mathbb{R}^{D\\times k}$ is a low-rank matrix that depends on the input a, $R \\in \\mathbb{R}^{D\\times k}$ is a constant low-rank matrix and $s \\in \\mathbb{R}^{D\\times 1}$ is a vector. The parametrization of $L(a)$ is described as follows:\n$L_i(a) = \\phi(W_ia + b_i) \\forall i\\in [k]$, (1b)\nwhere $L_i(a)$ and $W_i$ refer to the i-th column of $L(a)$ and W, respectively. The notation $\\circ$ denotes"}, {"title": "Probe-Free Intervention Framework", "content": "the Hadamard product of two vectors. The matrix $W \\in \\mathbb{R}^{D\\times k}$ is the weight matrix, $b \\in \\mathbb{R}^{D\\times k}$ is the bias term, and $ \\phi $ is the activation function applied component-wise to the input. One candidate for this activation function is $\\phi(\\cdot) = \\tanh(\\cdot)$. This choice enhances intervention performance, as the mapping $x \\rightarrow \\tanh(x)$ can output both negative and positive real numbers. Additionally, the output of the mapping is bounded between (-1,1), resolving the scaling issues of the bilinear terms $L(a)R^T$ in formulation (1a) automatically. If the components of $L(a)$ are unbounded, one can multiply W by any positive constant $ \\kappa $ and divide R by $ \\kappa $ to obtain the same intervention. To address this issue, prior works on asymmetric low-rank optimization (Tu et al., 2016; Bhojanapalli et al., 2016) typically incorporate a regularization term to close the norm gap between the scales of two low-rank matrices. In contrast to these methods, the optimization problem in equation (2) does not require rescaling the two low-rank matrices L(a) and R due to the boundness property of the hyperbolic tangent function. These properties also help stabilize the training process.\nFor a better understanding of the mapping constructed, we consider the degenerate case: if W is a zero matrix and $ \\phi $ is a linear function, then $L(a) = b$ and $f(a) = (I + bR^T)a + s$. This represents the classical bilinear low-rank mapping, which does not depend on the input vector a.\nWe propose a probe-free intervention framework based on the low-rank mapping described above. The framework performs the following tasks: (i) models the desirable answer region, and (ii) computes the intervention parameters by solving the associated smooth optimization problem. We begin by presenting the data matrix and the framework setup.\nFrom the training data, we collect the activations of the desirable output and form a matrix $G_q \\in \\mathbb{R}^{d\\times |G(q)|}$ for each attention head, where $|G(q)|$ denotes the cardinality of the set $G(q)$. Each column of $G_q$ represents the activation of one desirable answer. We construct the undesirable matrix $B_q \\in \\mathbb{R}^{d\\times |B(q)|}$ using the same approach. Let $M_q$ denote a generic manifold representing the region"}, {"title": "Algorithms for Training Nonlinear Low-Rank Mapping", "content": "of desirable answer data points. The nonlinear low-rank mapping f is trained by solving:\n$\\min \\sum_q \\sum_{i \\in B(q) \\cup G(q)} c_q(f(a_i), \\text{Proj}_{M_q}(f(a_i)))$, (2)\nwhere $c_q$ denotes the distance measure for question q, $f(a_i)$ denotes the terminal point of the intervention on activation $a_i$, and $\\text{Proj}_{M_q}$ denotes the projection onto the manifold $M_q$. The performance of the intervention depends on the choice of $c_q$ and the manifold $M_q$. The manifold $M_q$ should have a semantic interpretation in the modeling process and must be capable of separating the desirable answers from the undesirable answers. Additionally, solving problem (2) should be computationally efficient.\nWe now describe one specific instance of our framework. The manifold $M_q$ for question q is chosen as an ellipsoid of the form\n$M_q = \\{x : (x - \\mu_q)^T \\Sigma_q^{-1} (x - \\mu_q) \\leq \\rho_q\\}$, (3a)\nwhere $\\mu_q$ is the center of the ellipsoid, $ \\Sigma_q $ is a positive definite matrix that prescribes the shape, or orientation, of the ellipsoid, and $ \\rho_q $ is the radius of the ellipsoid.\nMoreover, we choose $c_q$ as the squared Mahalanobis distance (De Maesschalck et al., 2000)\n$c_q(a, a') = (a - a')^T \\Sigma_q^{-1} (a - a')$, (3b)\nwhich is the distance of the candidate point from the center of mass divided by the width of the ellipsoid in the direction of the candidate point. This distance is rooted in the construction of the manifold (3a). We use the following projection:\n$\\text{Proj}_{M_q}(y) = \\arg \\min_{x \\in M_q} c_q(x, y)$. (3c)\nTheorem 4.1 gives the optimization problem under the Mahalanobis distance with analytical projection expression.\nTheorem 4.1 (Ellipsoidal manifold and Mahalanobis distance). Suppose that the manifold, the distance measure, and the projection operator are chosen as in (3). Problem (2) becomes\n$\\min_f \\sum_q \\sum_{i \\in B(q) \\cup G(q)} [(\\sqrt{c_q(f(a_i), \\mu_q)} - \\sqrt{\\rho_q})_+]^2$, (4)\nwhere $ (y)_+ = \\max\\{0, y\\}$.\nThe objective function for the optimization problem is straightforward to compute, as it transforms the distance between $(I + L(a)R)a + s$ and its projection into the distance between $(I + L(a)R)a + s$ and a fixed vector $\\hat{\\mu}_q$. Readers can refer to the appendix B for the proof of Theorem 4.1.\nWe now discuss how to derive the parameters to specify the manifold. The main challenge here is the high dimensionality of the activation vectors (D = 4096 for Llama3-8B), while for each question q, we observe fewer than ten samples in total . Next, we present a practical method for estimating $\\mu_q$, $ \\Sigma_q $, and the radius $ \\rho_q $. From the training data, we compute the question-specific mean vector for question q and the mean vector computed by all questions\n$\\hat{\\mu}_q^+ = \\frac{1}{|G_q|} \\sum_{i=1}^{|G_q|} (G_q)_i$, (5)\n$\\hat{\\mu}^+ = \\frac{1}{N^+} \\sum_{q \\in \\mathcal{Q}} \\sum_{i=1}^{|G_q|} (G_q)_i$, (5)\nwhere $N^+$ denotes the sample size of the desirable subset of data. Analogously, we compute $\\hat{\\mu}_q^-$ and $ \\hat{\\mu}^- $ for the undesirable subset.\nDue to the noise incurred when computing $\\hat{\\mu}_q^+$ with small sample sizes, additional information must be incorporated to accurately construct the mean vector $\\mu_q$. We propose the following extrapolation scheme:\n$\\mu_q = \\hat{\\mu}_q^+ + \\lambda [\\alpha \\frac{\\hat{\\mu}_q^+ - \\hat{\\mu}_q^-}{|\\hat{\\mu}_q^+ - \\hat{\\mu}_q^-||} + (1 - \\alpha) \\frac{\\hat{\\mu}^+ - \\hat{\\mu}^-}{|\\hat{\\mu}^+ - \\hat{\\mu}^-||} ]$, (6)\nwhere $ \\lambda > 0 $ controls the magnitude of extrapolation. The extrapolation direction consists of a question-specific direction dictated by $\\hat{\\mu}_q^+ - \\hat{\\mu}_q^-$, and an overall direction dictated by $\\hat{\\mu}^+ - \\hat{\\mu}^-$. Both terms aim to guide $\\mu_q$ away from the undesirable values $ \\hat{\\mu}_q^- $ and $ \\hat{\\mu}^- $. The parameter $ \\alpha \\in [0, 1] $ controls the relative strength between the question-specific (local) and overall (global) directions. Li et al. (2024b) proposed a translation of the activation along the truthful direction $ \\hat{\\mu}^+ - \\hat{\\mu}^- $, which coincides with the overall direction in our formula.\nFor the covariance matrix, we assume that $ \\Sigma^{-1} $ is constant across all q. This assumption is a fundamental component of linear discriminant analysis in machine learning (Tharwat et al., 2017). We begin by computing the empirical covariance matrix:\n$ S= \\frac{1}{| \\mathcal{Q} |} \\sum_{q=1}^{| \\mathcal{Q} |} [\\frac{1}{N^+-1} \\sum_i \\sum_{i=1}^{|G(q)|} [(G_q)_i - \\mu_q] [(G_q)_i - \\mu_q]^T]. $ (7)\nDue to small sample sizes, the empirical covariance matrix S may be non-invertible. To address this issue, we adopt the linear shrinkage method (Sch\u00e4fer and Strimmer, 2005; Ledoit and Wolf, 2004), which shrinks the matrix towards a diagonal target, and set:\n$ \\hat{\\Sigma}_q^{-1} = (\\beta S + (1 - \\beta) \\text{diag}(S))^{-1} \\forall q, $ \nwhere $ \\beta \\in (0, 1) $ is the shrinkage parameter.\nTo estimate radius $ \\rho $, the heuristic approach is to find the minimum $ \\rho $ such that all samples remain within the desirable answer region, centered at $ \\hat{\\mu}_q $. For question q, we define:\n$ \\rho_q = \\max_{i \\in G_q} [(\\hat{\\mu}_q - (G_q)_i)^T \\hat{\\Sigma}_q^{-1} [(\\hat{\\mu}_q - (G_q)_i)]. $"}, {"title": "Algorithms for Training Nonlinear Low-Rank Mapping", "content": "The degenerated bilinear low-rank optimization problem has many applications in the statistical estimation field (e.g., matrix sensing, matrix recovery, PCA) (Chi et al., 2019). The literature shows that simple methods such as gradient descent can converge to the ground truth under mild assumptions with good initialization. In contrast, our objective function $ (W, R, b, s) \\rightarrow F(W, R, b, s) $ cannot be represented by the factorization formulation and contains an extra nonlinear activation function, making it challenging to derive similar theoretical guarantees. However, the first-order algorithm is still a good choice with lightweight computational complexity.\nWe emphasize that the activation function $ \\phi (\\cdot) = \\tanh(\\cdot) $ introduces smoothness to the objective function, thereby stabilizing the optimization processes during gradient descent. Furthermore, we propose to use preconditioned gradient descent, a powerful approach that has gained popularity in (bilinear) low-rank optimization. This method effectively balances the scale at each iteration, accelerating convergence while maintaining the same computational complexity.\n$W_{t+1} = W_t - \\eta \\nabla_W F(W_t, R_t, b_t, s_t) (R_t^T R_t + \\epsilon I)^{-1}$,\n$R_{t+1} = R_t - \\eta \\nabla_R F(W_t, R_t, b_t, s_t) (W_t^T W_t + \\epsilon I)^{-1}$,\n$b_{t+1} = b_t - \\eta \\nabla_b F(W_t, R_t, b_t, s_t) (b_t b_t^T + \\epsilon I)^{-1}$,\n$s_{t+1} = s_t - \\eta \\nabla_s F(W_t, R_t, b_t, s_t) (s_t s_t^T + \\epsilon I)^{-1}$. (8)\nwhere $ \\epsilon $ is a small number to ensure the scaling term is invertible. In comparison to vanilla gradient descent, the search directions of the variables in equation (8) are scaled. Intuitively, this scaling acts as a preconditioner, enhancing the search direction and enabling the use of larger step sizes. The preconditioner is adaptive and varies across iterations. Computationally, the scaled gradient descent introduces minimal overhead, as the inversion of the small-sized matrix is computationally inexpensive. Therefore, the per-iteration cost remains in the same order as standard gradient descent. In the context of bilinear low-rank matrix estimation, specifically in the problem $ \\min_{b, R} f (b R^T) $ (Tong et al., 2021), it has been shown that scaled gradient descent achieves a condition number-free convergence rate that outpaces standard gradient descent under mild assumptions."}, {"title": "Empirical Results", "content": "This section presents the empirical results of our proposed algorithm FLORAIN. Section 6.1 clarifies the setting of our experiments on the TruthfulQA dataset, including details about datasets, tasks, metrics, baselines, and computational resources. Section 6.2 showcases the superiority of our proposed methods to other baselines. We introduce the above contents in the Toxic Comments Classification Challenge dataset in Appendix C."}, {"title": "Experimental Setup", "content": "Tasks and Metrics: We evaluate our framework on the multiple choice and generation tasks, which are commonly used to verify the truthfulness of language models:\n\u2022 In the generation task, the model produces a complete answer for each question using greedy autoregressive decoding. While the accuracy and informativeness of the answers are ideally assessed by humans, this evaluation method is both costly and time-consuming. As a result, most works in this area rely on a well-trained large language model as an alternative evaluation tool. In our approach, we employ two fine-tuned GPT-3.5-instruct models: one for classifying whether the answer is correct or incorrect, and another for determining whether the response is informative. According to Li et al. (2024b), the percentage of answers labeled as correct by the first model is referred to as the truthful score (True %). Meanwhile, the percentage of answers labeled as informative by the second model corresponds to the informative score (Info %). Following the methodology of Li et al. (2024b), we report both the truthful score (True %) and the product of the truthful and informative scores, True*Info (%).\n\u2022 In the multiple-choice task, the model computes the log probability of completion for a given question and its corresponding set of choices. Following the approach of Lin et al. (2021), we report two metrics: MC1 and MC2. MC1 identifies the correct answer as the one with the highest probability among the available choices, and the overall accuracy across all questions is denoted as MC1. MC2 represents the normalized total probability assigned to the set of true answers, given a question and its associated set of true/false reference answers.\n\u2022 We also include two other metrics named Kullback-Leiber divergence (KL) of the model's next-token prediction distribution post-versus-pre-intervention and Cross-Entropy Loss (CE). These two metrics assess the extent to which the generation distribution shifts after the intervention. Lower values are preferred, indicating that the intervention minimally alters the behavior of the original model, reducing the likelihood of producing abnormal characters or unnatural sentences. The calculation of these metrics is detailed in Li et al. (2024b).\nDatasets: We use the TruthfulQA dataset to benchmark the effectiveness of FLORAIN. This dataset consists of 817 questions across 38 categories, designed to elicit false answers from language models, posing a challenge to generating accurate responses. Following the splits provided by Li et al. (2024b) and Yin et al. (2024), we divide the dataset into training (326 questions), validation (82 questions), and test (407 questions) sets. For evaluation, we perform 2-fold cross-validation, as done in Li et al. (2024b) and Yin et al. (2024). Additionally, we process the dataset into 5,918 question-answer pairs, each labeled with a binary indicator of desirability. The training set is used to develop our intervention policy, while the validation set is reserved for parameter tuning.\nModels: We apply our method to multiple pre-trained models to enhance their truthfulness, including Llama-7B (Touvron et al., 2023b), Llama2-chat-13B (Touvron et al., 2023a), and Llama3-8B (Dubey et al., 2024). Our approach can be integrated into existing fine-tuning pipelines, helping to generate more accurate answers. To demonstrate its versatility, we also evaluate our method on models that have already been fine-tuned for the same task, treating these as both baseline models and competitors to our approach. Detailed descriptions of these models are provided in the baselines section.\nHyperparameters: We manipulate the behavior of FLORAIN by tuning two hyperparameters $ \\lambda $ and $ \\alpha $ in determining the $ \\mu_q $ by equation (6). The details about selecting them for each pre-trained model are discussed in Appendix A.\nBaselines: We compare FLORAIN against competitive baselines with the same goal of eliciting truthful answers from language models:\n\u2022 Inference-time Intervention (ITI, Li et al. 2024b) is a state-of-the-art method that allows intervention without finetuning. We follow the hyperparameter settings provided in the original paper (Li et al., 2024b) and their GitHub repository."}, {"title": "Numerical Results", "content": "Notably, FSP is a prompt engineering technique orthogonal to intervention methods like FLORAIN and ITI. Therefore, we also report results for combinations of FSP with either ITI or FLORAIN. The FSP + FLORAIN combination consistently achieved the highest performance in metrics such as True * Info and True across all three models, with scores reaching 45% for Llama-7B, 42% for Llama3-8B, and 61% for Llama2-chat-13B.\nWhen applied independently, FLORAIN outperforms ITI in four key metrics across both the generation and multiple-choice tasks, significantly enhancing the truthfulness and quality of the pre-intervention models. Notably, FLORAIN improves the True * Info(%) score from 21.15 to 31.46 for Llama-7B, 32.88 to 36.78 for Llama3-8B, and 51.87 to 60.68 for Llama2-chat-13B. The KL values remain minimal across all models, indicating that the intervention effect is not overly aggressive."}, {"title": "Conclusion and Future Directions", "content": "In this paper, we introduced FLORAIN, a novel probe-free low-rank intervention framework for activation intervention. The framework aims to minimize the distance between the post-intervention vector and its projection onto the desirable answer region, an ellipsoid that is carefully estimated using first-order and second-order statistical information. Unlike existing intervention methods, our probe-free approach does not require classifiers to identify the responsible heads or layers for intervention. Instead, it formulates an optimization problem that incorporates region modeling and analytical projection. Additionally, the intervention is performed in a single layer of the output transformer network, allowing for efficient parallel computation and mitigating distribution shift phenomena. A potential direction for future research is to extend the region modeling approach, such as by considering the subspace spanned by the desirable answer matrix as the desirable answer region."}, {"title": "Limitations", "content": "Our method relies on the scale of the training dataset. In cases where the number of training samples is small, constructing the ellipsoid region becomes more challenging, as the estimated mean and covariance matrix are more susceptible to higher bias. The smooth optimization problem derived in the main text exhibits nonconvexity. Due to our limited understanding of its optimization landscape, the algorithm may converge to local minimizers. As a result, our first-order algorithm does not guarantee convergence to the global optimum. To solve this problem, we employ standard gradient descent without implementing preconditioning."}, {"title": "Hyper-parameter Tuning", "content": "In our framework, two key hyperparameters are $ \\lambda $ and $ \\alpha $. We determine their values by evaluating performance metrics and text generation quality in the validation set. Through a grid search over $ \\{2,3,4,5\\} $ for $ \\lambda $ and $ \\{0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\} $ for $ \\alpha $, we find that setting $ \\alpha = 0.2 $ and $ \\lambda = 5 $ yields strong performance across various cases. As a result, unless otherwise specified, we adopt this combination for all our experiments.\nAdditionally, the choice of the intervened layer is crucial. Given the limited number of layers, we can efficiently conduct a search procedure to optimize validation test performance. Specifically, we intervene at layer 11 for Llama-7B, Alpaca-7B, Vicuna-7B, layer 12 for Llama3-8B, and layer 14 for Llama2-chat-13B."}, {"title": "Proof of Theorem 4.1", "content": "The intervention maps the activation of undesirable answer $ a_i, i \\in B(q) $ onto the ellipsoid region of the desirable answers, which can be defined below.\n$ E_q = \\{x : (x - \\mu_q)^T \\Sigma_q^{-1} (x - \\mu_q) \\leq \\rho_q\\} $ (9)\nIn the proof part, we simplify the notation of $ \\Sigma_q $ (resp. $ E_q, \\mu_q $) as $ \\Sigma $ (resp. $ E, \\mu $) for clarity. Let the Mahalanobis projection be\n$ \\text{Proj}_E(y) = \\arg \\min_{x \\in E} (y - x)^T \\Sigma^{-1} (y - x), $\nwhere\n$ E = \\{x : (x - \\mu) ^T \\Sigma^{-1} (x - \\mu) \\leq \\rho\\}. $\nProof of Theorem 4.1. If $ y \\in E $, the result can be immediately derived by the definition of the projection. If $ y \\notin E $, we can transform the coordinates of x, y to simplify the projection problem. We define\n$ \\hat{y} = \\Sigma^{-1}y, \\hat{x} = \\Sigma^{-1}x. $\nThe projection problem has been transformed into\n$ \\arg \\min_{\\hat{x} \\in \\hat{E}} ||\\hat{y} - \\hat{x}||^2, $\nwhere $ \\hat{E} = \\{\\hat{x} : ||\\hat{x} - \\Sigma^{-1}\\mu||^2 \\leq \\rho\\}. $\nThe transformed problem allows for a closed-form solution for the point outside of the region\n$\\hat{x}^* = \\Sigma^{-1} \\mu + \\sqrt{\\rho} \\frac{\\hat{y} - \\Sigma^{-1} \\mu}{|\\hat{y} - \\Sigma^{-1} \\mu||} $\n$ = \\Sigma^{-1} \\mu + \\sqrt{\\rho} \\frac{\\Sigma^{-\\frac{1}{2}} y - \\Sigma^{-1} \\mu}{|\\Sigma^{-\\frac{1}{2}} y - \\Sigma^{-1} \\mu||} $\nTherefore\n$ \\hat{x}^* = \\mu + \\sqrt{\\rho} \\frac{\\Sigma^{\\frac{1}{2}} y - \\mu}{|\\Sigma^{\\frac{1}{2}} y - \\mu||} $\n$\\ \\ \\ \\ \\ \\ \\ \\ = \\mu + \\sqrt{\\rho} \\frac{\\Sigma^{\\frac{1}{2}} (y - \\mu)}{|\\Sigma^{\\frac{1}{2}} (y - \\mu)||} $\n$\\ \\ \\ \\ \\ \\ \\ \\ = \\mu + \\Sigma^{\\frac{1}{2}} \\sqrt{\\rho} \\frac{(y - \\mu)}{\\sqrt{(y - \\mu)^T \\Sigma^{-1} (y - \\mu)}} $\n$ \\ \\ \\ \\ = \\mu + (\\Sigma^{\\frac{1}{2}} \\Sigma^{\\frac{1}{2}}) \\sqrt{\\rho} \\frac{(y - \\mu)}{\\sqrt{(y - \\mu)^T \\Sigma^{-1} (y - \\mu)}} $\nFor an arbitrary q and i $ \\in $ B(q) $ \\cup $ G(q), \n$ f(a_i) - \\text{Proj}(f(a_i)) $\n$ = f(a_i) - \\hat{\\mu}_q $\n$\\ \\ \\ \\ =  f(a_i) - \\mu_q - \\frac{\\sqrt{\\rho_q}}{\\sqrt{(f(a_i) - \\mu_q)^T \\Sigma^{-1} (f(a_i) - \\mu_q)}} \\Sigma (f(a_i) - \\mu_q) $\n$= [1- \\frac{\\sqrt{\\rho_q}}{\\sqrt{(f(a_i) - \\mu_q)^T \\Sigma^{-1} (f(a_i) - \\mu_q)}}](f(a_i) - \\mu_q). $\nBy computation, if $ f(a_i) \\in M_q $, then\n$ c_q (f(a_i), \\text{Proj}_{c_q, M_q} (f(a_i)) = 0. $\nOtherwise, the distance is\n$[\\sqrt{(f(a_i) - \\mu_q)^T \\Sigma^{-1} (f(a_i) - \\mu_q)} - \\sqrt{\\rho_q}]^2. $\nWe complete the proof."}, {"title": "Experimental results on the toxicity mitigation task", "content": "We evaluate the models based on three key metrics: toxicity, fluency, and diversity. For each prompt in the dataset, the models generate 25 responses, each limited to 20 tokens. These outputs are analyzed using the Perspective API 4, which estimates the likelihood that a human would perceive the text as toxic."}]}