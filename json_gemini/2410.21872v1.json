{"title": "Advancing Efficient Brain Tumor Multi-Class\nClassification -- New Insights from the Vision\nMamba Model in Transfer Learning", "authors": ["Yinyi Lai", "Anbo Cao", "Yuan Gao", "Jiaqi Shang", "Zongyu Li", "Jia Guo"], "abstract": "Abstract-Early and accurate diagnosis of brain tumors is\ncrucial for improving patient survival rates. However, the\ndetection and classification of brain tumors are challenging\ndue to their diverse types and complex morphological\ncharacteristics. This study investigates the application of\npre-trained models for brain tumor classification, with a\nparticular focus on deploying the Mamba model. We fine-\ntuned several mainstream transfer learning models and\napplied them to the multi-class classification of brain tumors.\nBy comparing these models to those trained from scratch,\nwe demonstrated the significant advantages of transfer\nlearning, especially in the medical imaging field, where\nannotated data is often limited. Notably, we introduced the\nVision Mamba (Vim), a novel network architecture, and\napplied it for the first time in brain tumor classification,\nachieving exceptional classification accuracy. Experimental\nresults indicate that the Vim model achieved 100%\nclassification accuracy on an independent test set,\nemphasizing its potential for tumor classification tasks.\nThese findings underscore the effectiveness of transfer\nlearning in brain tumor classification and reveal that,\ncompared to existing state-of-the-art models, the Vim model\nis lightweight, efficient, and highly accurate, offering a new\nperspective for clinical applications. Furthermore, the\nframework proposed in this study for brain tumor\nclassification, based on transfer learning and the Vision\nMamba model, is broadly applicable to other medical\nimaging classification problems.", "sections": [{"title": "I. INTRODUCTION", "content": "RAIN tumors present significant challenges in\nmedical diagnosis due to their diverse types and\ncomplex morphological characteristics. There are\ncurrently over 120 known types of brain and central nervous\nsystem (CNS) tumors, ranging from benign to malignant,\nwhich makes detection and classification highly challenging.\nThis complexity significantly complicates diagnosis, and the\nlow survival rates associated with brain tumors further\nunderscore their severity [1]. Malignant brain tumors can\nrapidly compromise a patient's health due to their invasive\nnature, making early and accurate diagnosis critical for\nimproving survival outcomes [2].\nMagnetic Resonance Imaging (MRI) and Computed\nTomography (CT) are the primary imaging tools used for\ndiagnosing brain tumors. MRI is preferred for its ability to\ngenerate high-resolution images of brain tissue while being\nnon-invasive and avoiding ionizing radiation [3]. MRI can\nproduce detailed images using different sequences, such as\nT1-weighted, T2-weighted, and FLAIR, which are\ninstrumental in depicting soft tissue structures and detecting\nand characterizing brain tumors [4][5]. Although CT scans\nhave their uses, particularly in emergency settings, their\nreliance on ionizing radiation limits their application when\nfrequent imaging is required.\nRecent advances in deep learning have revolutionized\nmedical image analysis, significantly enhancing the capacity\nto tackle complex tasks like brain tumor classification [6].\nConvolutional Neural Networks (CNNs), a key part of deep\nlearning, have substantially improved image classification\naccuracy by automatically learning hierarchical features\ndirectly from raw image data [7]. This approach minimizes\ndependence on manual feature extraction, allowing models to\nexcel in complex tasks. To capture long-range dependencies,\nTransformer architectures have been introduced, which have\ndemonstrated excellent performance in visual tasks by\ncapturing global relationships within image data [8]. However,\ntraditional Transformers have quadratic attention complexity,\nwhich poses substantial computational and memory\nchallenges. This makes them inefficient, particularly for\nlarge-scale or 3D medical data, reducing their practicality in\nclinical settings [9].\nRecent research has generated significant interest in State\nSpace Models (SSMs), which are highly efficient for\nprocessing long sequences due to their convolutional and\nnear-linear computational characteristics [10]. One notable\nadvancement is the Mamba model [11], which incorporates\ntime-varying parameters into the SSM framework and utilizes\na hardware-aware algorithm, achieving efficient training and\ninference without relying on attention mechanisms. This\nresults in subquadratic computational complexity and linear\nmemory complexity [12]. Mamba's scalability makes it a\npromising alternative to Transformers, particularly in\nlanguage modeling. Additionally, Mamba-related models\nhave been successfully introduced in visual processing,\ndemonstrating superior performance in image classification,\nlesion segmentation, and modality transformation tasks\ncompared to traditional CNNs and some Transformer-based\nnetworks.\nDespite these advancements, current research reveals\ncertain limitations. Deep learning models generally have a\nsignificant demand for data, and in medical imaging, high-\nresolution annotated datasets are often difficult to obtain.\nMoreover, existing specialized models exhibit poor\ngeneralization across different datasets and may underperform\non rare or atypical cases. These models also have high\ncomputational costs associated with training [13].\nThis study aims to address these challenges by exploring\nthe performance of pre-trained models in brain tumor\nclassification tasks, specifically focusing on the Mamba\nmodel, which has not yet been applied to this field. The main\nmotivation is to evaluate the effectiveness of the Mamba\nmodel in distinguishing between different types of brain\ntumors and to investigate its potential to improve\nclassification accuracy. By leveraging pre-trained models and\nfine-tuning, this study aims to enhance diagnostic outcomes\nand provide new insights into medical image analysis. We\nprovide a comprehensive analysis of the Mamba model's\nperformance and compare it to existing models, demonstrating\nthe efficiency and lightweight characteristics of the Mamba\nmodel in processing complex medical image data. This\nprovides a new perspective and potential solutions for the\nchallenging task of brain tumor classification.\nThe contributions of this paper are as follows:\n\u2022\tFine-Tuning of Mainstream Models: We fine-tuned\nseveral mainstream transfer learning models for brain\ntumor classification, including Vision Transformer,\nSwin Transformer, EfficientNet-B0, Inception-V3,\nand ResNet50.\n\u2022\tTransfer Learning Benefits: Transfer learning,\nparticularly with pre-trained models, is advantageous\nwhen working with limited labeled medical data. We\ncompared models trained from scratch with those\nusing transfer learning to demonstrate the clear\nbenefits of pre-trained models.\n\u2022\tIntroduction of Vision Mamba: We introduced the\nVision Mamba (Vim), a novel network architecture\nbased on Mamba and applied it for the first time to a\nmulti-class brain tumor classification task. Vim\nachieved outstanding accuracy, further emphasizing\nthe superior capabilities of the Mamba architecture.\nThese results highlight the effectiveness of transfer learning\nmethods in brain tumor classification, demonstrating their\npotential impact on improving the diagnostic accuracy of\nmedical image analysis. At the same time, they indicate the\ngreat potential of Vim in tumor classification tasks. The brain\ntumor classification framework proposed in this study is also\napplicable to all other medical image classification problems,\nwith strong transferability."}, {"title": "II. RELATED WORK", "content": "In this section, we discuss the current state of brain tumor\nclassification using both traditional machine learning methods\nand deep learning approaches.\nTraditional machine learning methods have been widely\napplied to brain tumor classification for many years. Support\nVector Machine (SVM) is a classic supervised learning\nalgorithm commonly used for classification and regression\ntasks [14]. In brain tumor classification, SVM constructs an\noptimal hyperplane in a high-dimensional space to separate\ntumors of different categories. The advantage of SVM lies in its\nstrong generalization ability and suitability for small datasets.\nLourzikene et al. employed SVM for feature extraction and\nclassification of MRI image sequences, achieving high\naccuracy [15]. However, SVM is highly sensitive to parameter\nsettings, such as kernel type and penalty parameters,\nnecessitating complex parameter tuning.\nAdditionally, XGBoost is a gradient-boosted decision tree\nalgorithm recognized for its computational efficiency and\nexcellent classification performance [16]. In brain tumor\nclassification, XGBoost iteratively constructs decision trees,\nusing errors from previous iterations to improve the model's\naccuracy. Liu et al. applied XGBoost to brain tumor\nclassification, demonstrating its advantage in handling high-\ndimensional data [17]. However, XGBoost also requires\ncomplex parameter tuning to achieve optimal results.\nIn recent years, the rise of deep learning has led to the\ndevelopment of CNNs capable of extracting deep features from\nMR images, resulting in successful classification outcomes.\nThe Residual Network (ResNet) architecture proposed by He et\nal. has been widely adopted in image classification tasks,\nlargely due to its use of skip connections, which help retain\noriginal input information [18]. In brain tumor classification,\nResNet's deep feature extraction capabilities allow it to capture\nfine-grained details of different tumor types, thereby improving\naccuracy. Oladimeji and Ibitoye achieved notable results using\nResNet for brain tumor classification [19]. Similarly, Sharif et\nal. utilized DenseNet201 for brain tumor classification.\nDenseNet's unique structure, characterized by its dense\nconnectivity, allows the output of each layer to be directly\nconnected to all subsequent layers [20]. This dense connectivity\nenhances feature reuse and mitigates the vanishing gradient\nproblem, enabling effective classification even with limited\ntraining data."}, {"title": "III. MATERIALS AND METHODS", "content": "This study employs a publicly accessible dataset of brain\ntumor images, commonly used for evaluating algorithms\nrelated to multi-class tumor classification 1. The dataset\ncomprises a total of 4,449 authentic MRI slices acquired in the\naxial plane, including T1-weighted imaging (T1WI), contrast-\nenhanced T1-weighted imaging (CE-T1WI), and T2-weighted\nimaging (T2WI). These images were categorized into six\ndistinct classes of brain tumor pathologies, as illustrated in Fig.\n1.\n1)\tGlioma\nTumors originating from glial cells, which play a\nsupportive role in maintaining normal neural function\nwithin the brain and spinal cord. Gliomas are classified into\nseveral subtypes based on the cell type of origin and\nmalignancy level: astrocytoma, ganglioglioma,\nglioblastoma, oligodendroglioma, and ependymoma.\nHigh-grade gliomas, such as glioblastoma multiforme, are\nmore invasive with a poor prognosis, while low-grade\ngliomas grow more slowly and generally respond better to\ntreatment.\n2)\tMeningioma\nTumors arising from the meninges, which are the\nprotective membranes surrounding the brain and spinal\ncord. Meningiomas are usually benign but can be further\nclassified into subtypes such as grade I (benign), atypical,\nanaplastic, and transitional based on cellular characteristics\nand malignancy potential.\n3)\tNeurocitoma\nTumors arising from neuroglial cells that provide support\nto neurons in the brain and spinal cord. Neurocytomas are\nprimarily categorized as either central neurocytoma or\nextraventricular neurocytoma, depending on their\nanatomical location.\n4)\tNormal Brain Tissue\nThis category includes normal brain tissue, without\npresence of tumor of any kind.\n5)\tOutros\nThis category includes conditions other than tumor such as\nabscesses, cysts, and miscellaneous encephalopathies.\nAlthough not classified as typical brain tumors, these\nlesions can present together with brain tumors and thus\nrequire appropriate diagnostic and therapeutic intervention.\n6)\tSchwannoma\nTumors originating from the cells of the neural sheath,\nwhich form the protective covering around nerve fibers.\nSchwannomas include types such as acoustic, vestibular,\nand trigeminal schwannomas.\nGiven the diversity of brain tumor types, accurate\nclassification is crucial for the development of targeted\nintervention strategies and treatment protocols, underscoring\nthe clinical significance of precise tumor typing.\nDespite the dataset includes multiple imaging modalities,\nCE-T1WI is often regarded as optimal for diagnosing brain\ntumors. However, CE-T1WI is not always available in clinical\npractice. Therefore, this study focuses on the most used imaging\nmodality in clinical settings, T1WI, as input. This subset\ncomprises 1,554 images. For transfer learning, pre-trained\nmodels based on the ImageNet dataset require an input image\nchannel count of three. Since MRI images are grayscale, we\nreplicated the grayscale values across three channels. The\nspecific division of the dataset for training, validation, and\ntesting is shown in Table I. The training set contains 80% of the\nimages, while the validation and testing sets each contain 10%.\nTransfer learning is a technique where knowledge gained\nfrom a model trained on one task is transferred to a different but\nrelated task, thereby improving the learning performance in the\ntarget domain. In this study, we first evaluated the performance\nof five classic pre-trained models on the brain tumor\nclassification task and subsequently assessed the novel Vim\nmodel. All six models were initialized with pre-trained weights\nfrom the ImageNet dataset, and transfer learning was applied\nfor tumor classification:\n1)\tViT\nThe self-attention mechanism in ViT is one of its key\ninnovations, providing a novel way to process image data\ncompared to traditional CNNs by capturing global\ndependencies among different parts of the input.\nSpecifically, ViT calculates relationships between each\nimage patch and all other patches to obtain contextual\ninformation, allowing for an enhanced understanding of the\nimage content through global information [33]. In the self-\nattention mechanism, each patch undergoes a linear\ntransformation to produce query, key, and value vectors.\nThe model calculates the attention scores between these\nvectors, indicating the significance of each patch in\nunderstanding the global image context.\nBy taking a weighted average of value vectors, ViT\ngenerates a context-aware representation that allows each\npatch to incorporate information from others. Unlike CNNs,\nwhich primarily capture local features. ViT can process the\nentire image at once, which is especially important when\ndealing with large datasets to identify complex patterns and\ndetails.\n2)\tSwin Transformer\nSwin Transformer is an advanced visual Transformer\nmodel that optimizes high-resolution image processing\nthrough hierarchical self-attention and a shifting window\nmechanism [34]. It divides images into non-overlapping\nlocal windows, applying self-attention within each window\nto efficiently capture local features. The window-shifting\nstrategy then facilitates communication between different\nregions, thereby improving the model's global\nunderstanding of the image.\n3)\tEfficientNet-B0\nEfficientNet-B0 introduces an efficient network scaling\nmethod compared to traditional approaches. The model\nuses the compound scaling principle to uniformly scale all\ndimensions, including depth, width, and resolution, using\npredefined scaling coefficients. EfficientNet-B0 employs\nMobileNetV2-inspired inverted bottleneck convolution\n(MBConv) blocks and depth-wise separable convolutions,\nminimizing parameters and computational load while\nmaintaining accuracy [35]. The model also uses squeeze-\nand-excitation optimization to enhance the network's\nrepresentational capacity by recalibrating channel-level\nfeature responses.\n4)\tInception-V3\nInceptionV3, part of the Inception model series developed\nby Google, is known for its efficiency and accuracy in\nimage classification tasks. The core idea of InceptionV3 is\nto optimize feature extraction by applying convolution\noperations with different kernel sizes (e.g., 1x1, 3x3, 5x5)\nin parallel [36]. This approach allows for multi-scale\nfeature extraction without significantly increasing\ncomputational complexity. 1x1 convolutions are used for\ndimensionality reduction before applying larger kernels to\nfurther reduce computational costs.\n5)\tResNet-50\nResNet-50 is a deep CNN architecture from the ResNet\nfamily, proposed by Microsoft Research [37]. Its main\ninnovation is residual learning through skip connections,\nwhich alleviates the vanishing gradient problem in deep\nnetworks. ResNet-50 has 50 layers, including residual\nblocks that allow the model to learn residual functions,\nthereby facilitating deeper feature extraction. These skip\nconnections ensure that even very deep networks can\nmaintain performance without degradation.\n6)\tVision Mamba\nThe Vim model is a novel deep learning architecture\nspecifically designed for efficient visual representation\nlearning. This model is based on SSM, particularly adapted\nfrom the Mamba model initially developed for natural\nlanguage processing tasks involving long sequence\nmodeling [12]. Vim extends this approach to the visual\ndomain, effectively addressing unique challenges such as\npositional sensitivity and the need for global context in\nimage data.\nVim overcomes these challenges by introducing a\nbidirectional state space model, which allows it to process both\nforward and backward information in the sequence, thereby\ncapturing a comprehensive global context of data dependencies.\nAs a pure SSM-based visual backbone network, Vim treats\nimages as sequences of flattened patches and compresses visual\nrepresentations through a bidirectional selective state space\nmodel without relying on the self-attention mechanism, which\nis commonly used in ViT. This approach overcomes the\nquadratic computational complexity associated with\nTransformers. By introducing positional embeddings, Vim can\nbetter understand complex spatial relationships in visual data."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "To tailor the original models for the specific domain of\nmulti-class brain tumor classification, we modified their\npenultimate layers. The original fully connected (FC) layer in\neach transfer learning architecture was replaced with a new FC\nlayer with an output dimension of six, corresponding to the six\nbrain tumor classes under investigation. The Adam optimizer\nwas used for optimization, with cross-entropy loss as the loss\nfunction an appropriate choice for multi-class classification\ntasks. The learning rate was dynamically adjusted during\ntraining, incorporating scheduled reductions at predetermined\nphases to mitigate overfitting risks. Additionally, early stopping\nwas used as a regularization technique to halt training when\nperformance on the validation set plateaued for a specific\nnumber of epochs.\nFirst, we trained the network model from scratch; then, we\nloaded pre-trained weights and fine-tuned the layers closer to\nthe output. Finally, we froze the encoder layers of the pre-\ntrained model and trained only the fully connected layers for\nclassification. Our findings indicated that the model achieved\nsuperior training efficacy while maintaining computational\nefficiency when the transfer learning involved unfreezing the\nlast 20 layers only. For the Vim model, however, all layers were\nunfrozen after loading pre-trained weights to maximize its\nadaptability. The settings were consistent for the other models.\nWe employed a comprehensive set of metrics to evaluate\nmodel performance, including accuracy, precision, recall, fl-\nscore, specificity, sensitivity, and the area under the receiver\noperating characteristic curve (AUC). Classification results\nwere visualized using confusion matrices, which facilitated the\ncalculation of accuracy, precision, recall (or sensitivity),\nspecificity, and fl-score. Additionally, AUC was derived from\nclass probability estimates provided by each model.\nWe also conducted an analysis to explore the relationship\nbetween model complexity, represented by the number of\nparameters and the number of floating-point operations\n(FLOPs), and the accuracy metric. This analysis aimed to\nprovide insights into the trade-offs between model complexity\nand performance. Our evaluation was designed to be thorough,\ncomparing the merits and drawbacks of each model. This\ncomprehensive assessment enabled us to identify the most\nsuitable model for the brain tumor multi-classification task,\nconsidering predictive accuracy, computational efficiency, and\ngeneralizability."}, {"title": "V. DISCUSSION", "content": "Medical imaging encompasses a wide range of heterogeneity,\nmaking accurate image detection essential for the interpretation\nof medical data. In clinical practice, MRI and CT imaging are\ncommonly used for brain tumor identification, with MRI\nfrequently preferred for tumor detection and classification. This\nstudy utilized deep learning techniques based on MRI imaging\nto classify different types of brain tumors, demonstrating that\nvarious standard CNNs can achieve high accuracy in this task.\nSpecifically, we employed mainstream transfer learning models,\nincluding Swin Transformer, Vision Transformer, EfficientNet,\nInceptionV3, and ResNet50, to perform multi-class\nclassification of six types of brain tumors: Glioma, Meningioma,\nNeurocitoma, Normal Brain Tissue, Other Brain Lesions,\nSchwannoma.\nIn addition, we compared the performance of these\nmainstream transfer learning models to the Vision Mamba\nmodel in multi-class classification of brain tumors. To explore\nthe benefits of transfer learning based on pre-trained weights,\nwe trained each model three times: once from scratch, once\nusing pre-trained weights with fine-tuning of the later network\nlayers, and once using pre-trained weights where only the fully\nconnected layers for output were trained. The models were\nevaluated based on metrics such as accuracy, precision, recall,\nfl-score, specificity, sensitivity, and area under the receiver\noperating characteristic curve (AUC). Our experiments showed\nthat transfer learning significantly improved the accuracy and\nefficiency of classification tasks, with the Vision Mamba model\nachieving a classification accuracy of 100% on the independent\ntest set. Additionally, we analyzed the relationship between the\nnumber of model parameters, FLOPs, and classification\naccuracy, finding that Vision Mamba is both lightweight and\nefficient.\nThese results demonstrate the effectiveness of transfer\nlearning for brain tumor classification and underscore the\nsignificant potential of the Vision Mamba model in multi-class\ntumor classification tasks. Transfer learning with pre-training\nprovides better generalization compared to models specifically\ntrained for a single task, and it converges more quickly and\nefficiently during training. Conversely, models trained from\nscratch are more challenging to optimize and require strict\ncontrol over hyperparameters. The framework proposed in this\nstudy, based on transfer learning and the Vision Mamba model\nfor brain tumor classification, is broadly applicable to other\nmedical imaging classification problems.\nHowever, this study also has limitations. First, the dataset\nused in this study is relatively limited in size. Furthermore,\naccording to the World Health Organization (WHO), there are\nnumerous types of brain tumors, whereas our current dataset\nonly includes a small number of categories. As a result, we\ncannot guarantee that the model's performance will remain\nconsistent when expanded to include additional tumor types. An\nimportant area for future research is the creation of a dataset\nthat includes images from different MRI scanners and covers\nmore brain tumor categories, thereby increasing the diversity\nand volume of data for classification tasks. Additionally,\nincorporating other clinical imaging modalities, such as CT,\ncould further improve classification precision."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "This study introduces the Mamba-based Vim model to brain\ntumor classification, using transfer learning to optimize\nperformance within medical imaging. By reducing data\ndependency and computational complexity, Vim offers a novel\nframework that could benefit a range of clinical imaging tasks\nbeyond brain tumor classification. Its state-space model\narchitecture bypasses the need for traditional attention\nmechanisms while maintaining high accuracy, making it a\npromising, lightweight solution for real-world clinical\napplications. Furthermore, the framework proposed in this\nstudy for brain tumor classification, based on transfer learning\nand the Vision Mamba model, is broadly applicable to other\nmedical imaging classification problems.\nIn future work, data augmentation techniques could be\nemployed to expand the dataset size, thereby improving model\nrobustness. We also intend to further study training strategies\nfor transfer learning to better apply these pre-trained models to\ndownstream tasks, such as image classification. Exploring ways\nto optimize the Vision Mamba model's architecture to make it\neven more lightweight and efficient will also be a key focus,\nenhancing its utility in clinical applications. Future research\ncould also focus on different stages of various diseases,\nincluding early prevention, clinical treatment, and prognosis\nprediction, utilizing deep learning technology to improve\nclinical management and outcomes."}]}