{"title": "MULTIMODAL FUSION BALANCING THROUGH GAME-THEORETIC REGULARIZATION", "authors": ["Konstantinos Kontras", "Thomas Strypsteen", "Christos Chatzichristos", "Paul Pu Liang", "Matthew Blaschko", "Maarten De Vos"], "abstract": "Multimodal learning can complete the picture of information extraction by uncovering key dependencies between data sources. However, current systems fail to fully leverage multiple modalities for optimal performance. This has been attributed to modality competition, where modalities strive for training resources, leaving some underoptimized. We show that current balancing methods struggle to train multimodal models that surpass even simple baselines, such as ensembles. This raises the question: how can we ensure that all modalities in multimodal training are sufficiently trained, and that learning from new modalities consistently improves performance? This paper proposes the Multimodal Competition Regularizer (MCR), a new loss component inspired by mutual information (MI) decomposition designed to prevent the adverse effects of competition in multimodal training. Our key contributions are: 1) Introducing game-theoretic principles in multimodal learning, where each modality acts as a player competing to maximize its influence on the final outcome, enabling automatic balancing of the MI terms. 2) Refining lower and upper bounds for each MI term to enhance the extraction of task-relevant unique and shared information across modalities. 3) Suggesting latent space permutations for conditional MI estimation, significantly improving computational efficiency. MCR outperforms all previously suggested training strategies and is the first to consistently improve multimodal learning beyond the ensemble baseline, clearly demonstrating that combining modalities leads to significant performance gains on both synthetic and large real-world datasets.", "sections": [{"title": "INTRODUCTION", "content": "Exploiting multimodal data has made significant progress, with advances in generalizable representations and larger datasets enabling solutions to previously unattainable tasks [27; 29; 37; 43; 42; 44; 50; 53; 62]. However, studies indicate that multimodal data is often utilized suboptimally, underperforming compared to ensemble unimodal models or even the best single modality [55; 60]. The expectation that adding a modality should improve performance, assuming independent errors and above-chance predictive power [17], is frequently contradicted in practice.\nHuang et al. [20] attribute this issue to modality competition, where one modality quickly minimizes training error, misdirecting and suppressing the learning of others. Factors like noise levels, relationship complexity with the target, feature dimensionality, and data quality can cause one modality to fit faster than another. This imply that adding task-relevant information doesn't guarantee better performance, primarily due to complications during training. To address these issues, it's crucial to monitor each modality's contribution during training and apply corrective measures.\nSeveral balancing strategies have been proposed to tackle this issue [5; 6; 9; 10; 21; 26; 28; 40; 41; 54; 55; 57; 60]. A central aspect of these methods is estimating each modality's contribution to the output. Most assume distributional independence between modalities on predicting the target, measuring contribution via unimodal performance [60; 26; 40; 6; 54]. Some methods bypass"}, {"title": "ANALYSIS OF MULTIMODAL COMPETITION", "content": "Consider a dataset of N i.i.d. datapoints sampled from a distribution D, where each datapoint consists of M modalities X = (\u04251,.., \u0425\u043c) and a target Y. Our objective is to learn a pa- rameterized function f : X;0 \u2192 Y. We define the unimodal encoder for each modality as fm: Xm;0m\u2192 Zm, which encodes input Xm into a latent representation Zm. The fusion network fc: [Z1,..., ZM];0c \u2192 Y takes these latent representations and predicts the target Y, as does independently on each modality the unimodal task head fcm: Zm;0cm \u2192 Ym. The families of unimodal and multimodal models are defined as follows:\nUnimodal models Fum:fum (Xm;0um) = fcm(fm (Xm;0m);0cm), for m = [1, .., \u039c]. (1)\nMultimodal models F : f(X; 0) = fc (f1 (X1; 01), . . ., f\u043c (\u0425\u043c;0\u043c);\u0472\u0441). (2)\nFor simplicity, we continue our analysis with M = 2, focusing on models with two modalities.\nThe limitation of supervised multimodal training\nIn supervised learning with multimodal inputs X1 and X2, the objective is to learn representations Z1 = f1(X1;01) and Z2 = f2(X2;02) that, when combined through a fusion model fc(Z1, Z2), accurately predict a target Y. This can be expressed as minimizing the expected task loss or by maximizing the mutual information between the fused representation and the target:\n(Z1, Z2) = arg max I(fc(Z1, Z2); Y). (3)\nZ1:=f1 (X1;01),\nZ2:=f2(X2;02)\nDuring training, the model often undertrains the weaker modality, prioritizing the more accessible or higher-quality one. Such over-reliance on one modality limits the model's ability to effectively utilize all available information. Consequently, the mutual information becomes dominated by the stronger modality, resulting in I(fc(Z1, Z2); Y) \u2248 I(Z\u2081; Y). In this scenario, the conditional mu- tual information (CMI) I(Z2; Y | Z\u2081) \u2248 0, indicating that once the model learns from Z1, the information from Z2 contributes little to predicting Y. Appendix A.1 presents a brief experiment demonstrating this phenomenon.\nThis phenomenon is not exclusive to multimodal learning. In single-modality feature learning, mod- els often fit faster to dominant features, neglecting others that could enhance generalization. Regular- ization techniques like L1/L2 penalties and dropout [38; 46] were introduced to encourage balanced feature use and promote the learning of diverse patterns. While these techniques have become the standard in training models, their adaptation to multimodal learning has proven more challenging. For instance, Xiao et al. [58] attempted to apply dropout individually to each modality, but subse- quent research [40] demonstrated that this approach is still limited in addressing the core issue. The central problem lies in effectively regulating how modalities interact and compete, which remains an open question in the field.\nMultimodal competition\nMultimodal competition occurs when the network primarily optimizes for one modality, leading to a decline in generalization. That modality reduces training error by overfitting to the data, limiting"}, {"title": "MULTIMODAL COMPETITION REGULARIZER", "content": "In this section, we introduce LMCR, a set of targeted loss components designed to directly address and mitigate the challenges of multimodal competition. These losses are derived by framing the multimodal learning task as optimizing the mutual information between the target Y and the joint distribution of unimodal input features X = (X1, X2). The method is focusing on two input modal- ities, though an extension to M modalities is presented in Appendix A.5. The motivation behind these loss components stems from this formulation. To clearly define each loss, we first decompose the mutual information into distinct terms:\nI(X1; X2; Y) = I(X1; Y | X2) + I(X2; Y | X1) + I(X1; X2) \u2212 I(X1; X2 | Y) (5)\nThis decomposition is illustrated by the Venn diagram in Figure 1. The first two terms, the CMIs I(X1; Y | X2) and I(X2; Y | X1), represent the unique information each modality holds about the target that is not shared with the other modality. Maximizing these terms encourages the model to extract modality-specific task-relevant features. The third term, I(X1; X2), quantifies shared infor- mation between the modalities. Maximizing it encourages the model to align the representations and effectively utilize the common information between [22; 39; 44].\nThe third term, I(X1; X2), quantifies how much information the two modalities share. Maximiz- ing this term encourages the model to align the representations and effectively utilize the common information between them [22; 39; 44]. The third term, I(X\u2081; X2), quantifies the shared informa- tion between the modalities. Maximizing this term encourages to explore common information and aligns the representations, similar to how contrastive learning exploits multi-view redundancy to en- hance representation quality [22; 39; 44]. The final term, I(X1; X2 | Y), represents task-irrelevant shared information. It penalizes the shared information between modalities but not useful for pre- dicting the target. For each of these terms, we introduce a corresponding loss component. The total regularization loss is defined as follows:\nRegularization Loss: LMCR = LCon + LCEB + LMIPD, with (6)\nCompetition Terms: LMIPD = (LMIPD1 \u2212 LMIPD2 ) + (LMIPD2 \u2212 LMIPD1 ). (7)\n\u03b81\n\u03b81\n\u03b82\n\u03b82\nThe proposed regularizer consists of three key losses: LCon, a contrastive loss that captures shared information between modalities, LCEB, which filters out task-irrelevant information, focusing on relevant features for the downstream task, and LMIPD, which measures each modality's affection to the multimodal prediction by assessing output variations under perturbations. The latter has contradicting terms since increasing the importance of one modality decreases the importance of the other, and hence we model it as a game where each modality encoder (with parameters 01, 02) competes to increase the importance of its own modality while decreasing the other. In the remainder of this section, we will examine each component individually and further elaborate on framing multimodal competition using game theory.\nAPPROXIMATING MI TERMS\nTo approximate each CMI and capture the unique contribution of each modality, the Mutual Infor- mation Perturbed Difference (MIPD) serves as a surrogate function, measuring how input pertur- bations affect the model's output. By comparing predictions with and without these perturbations, MIPD estimates how much information each modality provides. If a modality is crucial, altering its input should significantly change the output, revealing its importance.\nThe perturbed version of modality X\u2081 is denoted as X1. We use permutations as our perturbation method, which will be detailed further in Section 3.3.\nEstimating the CMI directly I (X1; Y | X2) = H(Y | X2)\u2013 H(Y | X1, X2) is typically intractable. Instead we use the MIPD, which is defined as:\nMIPD(X1; Y | X2) = I(X1; Y | X2) \u2013 I(X1; Y | X2) \u2264 I(X1;Y | X2). (8)\nUsing the entropy interpretation of the mutual information terms, each CMI can be expressed as the difference of the log probabilities with and without the perturbations:\nMIPD(X1; Y | X2) = H(Y | X2, X\u2081) \u2013 H(Y | X2, X1)\n= E [\u2212Ex1~p(x1) [logp(y | x2, x1)] + log p(y | x2, x1)]. (9)\nX1, X2 ~ P(X1,X2,Y)\ny~p(y)"}, {"title": "THE GAME OF MULTIMODAL FUSION", "content": "Moreover, we employ the Jensen-Shannon divergence (JSD) [32], which is symmetrically bounded, to avoid training instabilities leading to the following objective:\nLMIPDx\u2081 = \u2212 MIPD(X1; Y | X2) = \u2212E [JSD(P(y | x2, x1), P(y | X2, X1))]. (10)\nX1, X2 ~ P(X1,X2,Y)\ny ~p(y)\nX1~p(x1)\nNext, the I(X1; X2) MI term measures how much information the two modalities share with each other. It the common patterns between the two, ensuring that the model recognizes and aligns these shared aspects. We exploit the available label information employing the supervised contrastive loss [24] LCon. The term is defined as follows:\n\u03c8(x1,x++) (11)\nLCon (X1, X2) = E log k\nX1, Y~ p(x1,y)\n \u03a3z+~p(x2 | y) \u03c8(x1, x2)\nx2~p(x2-y)\nwhere \u03c8 is the critic function, which, in our case, is the exponential dot product. Minimizing the LCon, maximizes a lower bound on both the MI between the two modalities and the CMI terms:\nI(X2; Y|X1) + I(X1; Y|X2) + 2 \u00b7 I(X2; X1) \u2265 log N \u2212 Lopt (12)\nCon\nAs N increases the bounds becomes tighter, while the bound is not affected by the number of positive samples (same class datapoints). More details are provided in Appendix B.\nFinally, I(X1; X2 | Y) captures irrelevant shared information between modalities, and minimizing an upper bound on this ensures the model retains only task-relevant content. For this purpose, we exploit CEB [7], targeting superfluous information in multimodal representations via a reconstruc- tion loss. A small reconstruction head, h : Y;\u03b8\u03b7 \u2192 Z = (Z1, Z2), predicts back into the latent space, effectively filtering out irrelevant content:\nLCEB (X1; X2) = Ex1,x2,y~p(x1,x2,y) || [f1(x1), f2(x2)] - h(p(y | x1, x2))||2 (13)\nPenalizing irrelevant information has been shown to enhance calibration and robustness [8], but it must be carefully evaluated, as it can introduce constraints that may hinder overall performance.\nTHE GAME OF MULTIMODAL FUSION\nIn this section, we present the game-theoretic approach used to balance the terms of the proposed multimodal training regularizer. The key insight is that the total contribution from all modalities, expressed through modality importance, remains constant. This implies that when one modality's"}, {"title": "PERTURBATIONS", "content": "contribution increases significantly, it may diminish the contributions of other modalities. This allows to frame multimodal competition as a constant-sum game where each modality acts as a player. The LMIPD corresponding to each modality represents the payoff function for that modality, where each modality aims to maximize its own contribution to the overall output. The players take concurrent actions, as each modality is updated during every optimization step. We constrain the players' actions by enforcing a fixed strategy throughout the entire training process. We explore three strategic actions for the modalities:\nCollaborative Actions: All modalities work together to increase each other's contributions. The LMIPD terms are applied across all parameters, resulting in a joint minimization, min LMIPD\n\u03b8\nIndependent Actions: Each modality focuses on maximizing its own contribution by optimizing its respective LMIPD term, without regard to the contributions of other modalities, leading to min LMIPDx\n\u03b8\u03b5\nGreedy Actions: Each modality seeks to maximize its own contribution by: 1) minimizing its own LMIPD term, and 2) maximizing the LMIPD terms of other modalities. In the constant-sum game, reducing the influence of other modalities enhances its own contribution, resulting in a min-max strategy, min max LMIPDx, 1.\n\u03b8\u03b5 \u03b8\u03ad\nIn Figure 3, we provide an example of the choices the video modality can make, illustrating how it can either assist, ignore, or diminish the importance of the audio modality. By default, we adopt the greedy actions strategy, unless otherwise specified.\nPERTURBATIONS\nTo measure the importance of modality X\u2081, we observe the difference in its predictive output when presented with {X1, X2} compared to the normal input {X1, X2}.\nPrevious works have explored various perturbation techniques. Gat et al. [10] used additive noise to estimate functional entropy by maximizing output variance. Maximizing this variance can increase sensitivity to noise, undermining the goals of Lipschitz smoothness [1] and adversarial robustness [47]. Ji et al. [21] and Liang et al. [31] proposed task-specific augmentations, that may not be available in most tasks and modalities, while their method relies heavily on the success of these aug- mentations. Li et al. [28] employed zero-masking to estimate Shapley values and trained directly to maximize them. However, Shapley values can produce arbitrary or unreliable estimates, particularly in high-dimensional settings [35]. Additionally, each of these methods perturbs the input, requiring extra forward passes and proportionally increasing computational and memory demands.\nTo address the limitations of the suggested perturbations and face their computational need, we propose as perturbation to use within-batch permutations \u03c3\u03b5 ~ Uniform(P), where P represents the set of all possible permutations [11]. Permutations differ from previous methods by applying the transformation directly to the latent space, eliminating the need for multiple forward passes through the unimodal encoders, thereby reducing both computational and memory overhead. To analyze the effects of permutations, we examine two cases separately: when the label of the original data matches the label of the permuted pair, and when it does not:\nMatching Labels: The normal and perturbed latent vectors represent the same class, allowing us to test if changes in task-irrelevant information impact the output, resembling the optimal augmentation described in Liang et al. [31]. Maximizing their output difference can help the model spread the datapoint representations within the class boundaries.\nNon-Matching Labels: In this case, the normal and perturbed latent vectors belong to different classes. Maximizing their output difference can promote better class discrimination, refining the decision boundaries and reinforcing model's robustness and calibration on out-of-distribution cases.\nA detailed analysis of the computational benefits of applying perturbations to the latent space is provided in Appendix A.6, along with a comparison of perturbations in Appendix C.1.\nThe notation i refers to the rest of the modalities except i."}, {"title": "EXPERIMENTS", "content": "We perform extensive experiments on synthetic and real-world multimodal datasets to evaluate the effectiveness of our proposed method, MCR, against various baselines and previous methods:\nUnimodal Training: Separate training for each modality.\nEnsemble: Combines unimodal predictions without any further training.\nJoint Training: Trains all modalities under a single-loss objective, without any balancing.\nMulti-Loss [52]: Incorporates additional unimodal task losses alongside the joint objective.\nMMCosine [59]: Equalizes modality contributions by standardizing features and weights.\nMSLR [60]: Adapts unimodal encoders' learning rates based on validation performance.\nPMR [6]: Similar to Multi-Loss, uses prototype classifiers and adjusts unimodal loss weights.\nOGM [40]: Extracts unimodal performance from the last layer, compares improvements, and mod- ulates gradients by adjusting each encoder's learning rate.\nAGM [28]: Uses zero-masking Shapley values as predictor for the unimodal task losses and mod- ulate them by adjusting their coefficients.\nMLB [26]: Combines Multi-Loss for enhanced unimodal performance assessment and adjusts each encoder's learning rate.\nUni-Pre Frozen: Uses pre-trained unimodal encoders with frozen weights during joint training.\nUni-Pre Finetuned: Employs pre-trained unimodal encoders, fine-tuned during joint training.\nSYNTHETIC DATASET\nWe generate an artificial scenario where the mutual information between the label and the modal- ities varies, demonstrating the phenomenon of modality competition. Although many factors can contribute to this effect, a comprehensive exploration is beyond the scope of this work. Instead, we focus on the imbalance in modality informativeness to highlight the motivation behind our approach.\nData: We generate task-irrelevant information for each modality by sampling N1, N2 ~ N(0, I) and the 5-class label Y from a uniform distribution Y ~ Uniform(5). Using fixed transformations, similar to Liang et al.[31], each modality is converted into a high-dimensional vector. We relate both modalities to the label through a linear relationship: X1 = N1 + Y and X2 = N2 + Y. Data points are distributed such that either both modalities contain label information (Shared Information) or only one of the modalities (Unique Information). We vary the percentage of data points with shared and unique information to analyze how models perform under different conditions."}, {"title": "DISCUSSION", "content": "In certain instances by approaching the best-performing baselines but it generally shows a decline in that category. This analysis is encouraging for two reasons: it highlights a significant percentage of unimodal information that is not adequately leveraged by the multimodal model, and it indicates potential for further improving the discovery of emergent information, both of which are crucial for achieving performance gains that justify the effort required to collect, process, and compute the additional multimodal data.\nThis paper examines the challenge of modality competition in multimodal learning, where cer- tain modalities dominate the training process, resulting in suboptimal performance. We introduce the Multimodal Competition Regularizer (MCR), a novel approach inspired by information theory, which frames multimodal learning as a game where each modality competes to maximize its con- tribution to the final output. MCR efficiently computes lower and upper bounds to optimize both unique and shared task-relevant information for each modality. Our extensive experiments show that MCR consistently outperforms existing methods and simple baselines on both synthetic and real-world datasets, providing a more balanced and effective multimodal learning framework. MCR paves the way for fulfilling the long-standing promise of multimodal fusion methods to achieve performance that surpasses the combined results of unimodal training.\nLooking ahead, further advancements could involve incorporating additional terms to explicitly encourage synergetic interactions between modalities, alongside an exploration of game-theoretic modeling to enable more flexible and adaptive strategies. Future work may focus on refining these strategies, enabling individualized and adaptive decisions for each modality, and tightening mutual information bounds to unlock even greater performance improvements."}]}