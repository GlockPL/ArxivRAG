{"title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With Structured Memories", "authors": ["Dulhan Jayalath", "James B. Wendt", "Nicholas Monath", "Sandeep Tata", "Beliz Gunel"], "abstract": "Long-range tasks require reasoning over long inputs. Existing solutions either need large compute budgets, training data, access to model weights, or use complex, task-specific approaches. We present PRISM, which alleviates these concerns by processing information as a stream of chunks, maintaining a structured in-context memory specified by a typed hierarchy schema. This approach demonstrates superior performance to baselines on diverse tasks while using at least 4x smaller contexts than long-context models. Moreover, PRISM is token-efficient. By producing short outputs and efficiently leveraging key-value (KV) caches, it achieves up to 54% cost reduction when compared to alternative short-context approaches. The method also scales down to tiny information chunks (e.g., 500 tokens) without increasing the number of tokens encoded or sacrificing quality. Furthermore, we show that it is possible to generate schemas to generalize our approach to new tasks with minimal effort.", "sections": [{"title": "Introduction", "content": "Problems requiring long information contexts are prevalent in natural language processing. The prototypical example is long document summarization, where a lengthy piece of text must be summarized into a short form. To solve this task with a large language model (LLM), the model is typically prompted with the text and outputs a summary of the content. However, this requires a model with a context long enough to fit the document. Many practitioners, as well as those in the research community, rely on models with short context lengths because they are limited by the inference cost of long-context models, open source or on-premises requirements, local compute constraints, or other barriers. In response, we design an approach that is task-agnostic, requires no training data, uses a small compute budget, and does not need access to model weights. Existing approaches do not satisfy these constraints together.\nOur proposal uses a typical incremental processing strategy, treating information as a sequential stream of chunks, processed in the order of their appearance alongside an in-context memory of prior chunks. For example, in incremental summarization, the document may be split into consecutive segments, each fitting within context and shown to the LLM in sequence. Here, the memory is a running summary of the content seen so far.\nWhile incremental methods are not new (Chang et al., 2024; Hwang et al., 2024b), existing approaches are task-specific and are not economic in terms of tokens processed. To address this in our strategy, rather than seeing a natural language memory, the LLM sees a structured memory of relevant prior information and outputs a proposed revision to the memory based on the current chunk (Figure 1). The memory is specified by a user-defined typed hierarchy schema, supporting any kind of long-range task. We call our approach Processing Incrementally with Structured Memory (PRISM).\nPRISM uses the structured memory to track salient"}, {"title": "Related Work", "content": "There are a range of existing approaches for long-range reasoning with limited contexts that utilize memories. For book summarization, Chang et al.\n(2024) propose a hierarchical processing approach that leverages contextual information from previous summaries. Among other domain-specific methods, Hwang et al. (2024a) and Hwang et al. (2024b) use natural language and JSON-encoded knowledge representations respectively for updating memories online as new information arrives, while Fei et al. (2024) specifically target retrieval-based question-answering. Focusing on conversational agents, Packer et al. (2023) perform stateful reasoning in LLMs using function calls to read and write data to storage.\nAnother line of research embeds memories into the architecture or representation. He et al. (2024) provide a method that augments LLMs with memory embeddings to transform them into recurrent models. Similarly, Munkhdalai et al. (2024) utilize a latent-space memory as part of the network. Wang et al. (2023) go even further, designing a new model architecture with a retrieval side-network to cache and update a memory. Finally, Ivgi et al. (2023) propose using a language model encoder to encode overlapping chunks and fuse information with a pre-trained decoder."}, {"title": "Method", "content": "We seek to solve long-range tasks token-efficiently without long-context models using an incremental processing strategy because of the many benefits it provides (Table 1). In this incremental view, instead of seeing the entire context at once, the LLM sees contiguous segments (which we refer to as chunks) in sequence. Since, in each step, the LLM can only see the current chunk and not previous chunks, it must receive prior information through the previous LLM output. This output must encode any information relevant to the task that was previously seen. In the current call, this output is revised based on the information in the current chunk. The use of the previous output as a memory in this way is characteristic of solving incremental tasks using LLMs. In this section, we provide a way to structure this memory using a typed hierarchy schema and show how to efficiently process these tokens across multiple LLM calls."}, {"title": "Incremental Processing Formulation", "content": "In a typical incremental processing strategy, data arrives in increments, forming an ordered sequence of chunks $(d_1, d_2, . . ., d_n)$. An LLM is prompted, incrementally from $i \\in \\{1, ..., n\\}$, with task in-"}, {"title": "Using Structured Memories", "content": "Natural language (or unstructured) memories do not necessarily encode the most salient information for a specific task because the output format is unconstrained. This can impair task performance. We improve the typical incremental processing formulation by introducing a structured memory and structured output to increase information relevance and reduce the cognitive burden on the LLM. In PRISM, we prompt the language model at step i with a modified tuple $(T, S, m_i, d_i)$ where we replace the natural language memory $0_{i\u22121}$ with a user-defined structured memory $m_i$ specified by a typed hierarchy schema.\nDefinition 3.2. A typed hierarchy is a structure of primitive types and simple data structures (e.g., integers, floating points, strings, and lists) in a hierarchy laid out by nested key-value maps.\nDefinition 3.3. A structured memory m has a schema S specified with a typed hierarchy.\nFor example, a simple schema for narrative summarization could be str: list<str> i.e., a key-value mapping of character names to strings describing events involving that character. After seeing a new story chunk, we can revise information about a character by adding to the entries for that character. We choose to use typed hierarchies because they are easily addressable (using a path specified by keys and indices) and updatable. We specify a new schema for each task as this structure determines the information that will be encoded in the memory. To revise the memory, instead of generating a structured memory to overwrite the prior memory with, the output of the model is a proposed memory revision $r_i$, which provides a path to programmatically revise $m_i$ with a new value."}, {"title": "", "content": "Definition 3.4. A structured memory revision r is a tuple (p, o, v) where p specifies an addressable path in the memory, o is a binary operation that is either add or update and v is the value to revise the memory with.\nIf o is add, p specifies a new path to which v is added; if update, p specifies an existing path in the memory whose current value should be replaced with v. After validating the proposed revision by programmatically ensuring it conforms to the expected structure, the memory $m_i$ is revised with $r_i$ to the next memory state $m_{i+1}$. Figure 2 provides an overview of our approach and Figure 3 gives a concrete example. In practice, $r_i$ may consist of more than one proposed revision. After all chunks are processed, the final state of the memory (alongside the query and a specification of the memory structure) are provided to the LLM with an instruction asking the model to give a final answer. Algorithm 1 shows all steps."}, {"title": "Token-Efficient Memory Encoding", "content": "A significant issue with using memories is that they extend the size of the prompt (compared to having no memory) and therefore increase the number of tokens that need to be encoded. This can become a significant bottleneck when there are many chunks of information in an incremental task or if the size of the memory dominates the rest of the prompt. One way to improve encoding efficiency is to utilize prefix KV caching (Zheng et al., 2023). With this method, if there is a prefix of the prompt that matches a prior encoded prompt, the model can reuse the KV activations previously computed for this prefix. Thus, maximizing the length of this prefix is essential for cache efficiency. For simplicity, our experiments implement prefix KV caching such that the KV activations are reused for only the longest prefix matching the last encoded prompt. To leverage the cache utilization improvements"}, {"title": "Generating Memory Schemas", "content": "The memory schema can be automatically generated by prompting an LLM. To do so, we hand-craft three schemas from a variety of domains, using these as few-shot examples, and prompt the LLM (Appendix C) to generate a schema for a different domain given a simple description of the query domain and an example query. For example, if the task is code retrieval, the prompt should describe the query domain (the task of retrieving a function given a code repository) and provide an example query which describes the procedure of a function as well as its inputs and outputs. The output of the LLM is then a schema that defines the structure of a memory that encodes information relevant to this task from the chunks seen by the LLM. This could be something like a map from the names of functions seen to a brief description of what the"}, {"title": "Experiments", "content": "Datasets. We evaluate our approach with three state-of-the-art long-range datasets. The first, BooookScore (Chang et al., 2024), is both a long-context book summarization dataset and benchmark. The dataset contains very large books (each over 100k tokens) which the authors were careful to ensure did not exist in the pre-training datasets of public large language models at the time of publication. Chang et al. (2024) additionally introduce a reference-free summarization metric that they also call BooookScore which we use to measure the coherency of the final summaries generated. This is an LLM-assisted measure that computes the percentage of sentences in the summary that do not contain one of a number of identified error types. The second dataset is a long-range code understanding and retrieval benchmark called RepoQA (Liu et al., 2024). Here, inputs are large code repositories made up of multiple code files totalling above 100k tokens. The task is to retrieve a function, which is described in natural language without being named, from the repository. A memory is useful to reason about this task because the\nfunction descriptions can describe function behavior through relationships with functions in other chunks. We measure accuracy, marking an output as correct if it names the described function exactly, and incorrect otherwise. Our final task is a version of the Spider dataset (Yu et al., 2018) adapted by Lee et al. (2024) to be a long-range question answering task. This task, which we refer to as LOFT-Spider, requires answering a set of questions directly (rather than via SQL commands) from a large SQL database. Response accuracy on this task is measured using exact matching. These datasets evaluate opposite boundaries in LLM reasoning. BooookScore is a highly unstructured natural language reasoning task, while RepoQA and LOFT-Spider are well-structured retrieval and reasoning tasks.\nModels. To establish a quality ceiling, we compare our baselines to a state-of-the-art long context model (Gemini 1.5 Pro (Reid et al., 2024)) with a context of 1M tokens. This is large enough to fit the longest samples from each of the datasets we study within context. For all other baselines, we use the same model with 32k context. This allows us to compare similarly capable models with different context sizes. We use top k sampling (k = 40) with temperature 0.8. For LLM-assisted schema generation, we use the same model.\nBaselines. We use incremental merging and hierarchical merging as our short-context baselines for BooookScore. These were proposed by Chang et al. (2024) alongside the dataset. Incremental merging follows the characteristic incremental task formulation of revising a running summary in natural language as new chunks are seen; hierarchical merging summarizes all chunks, then summarizes consecutive pairs of summaries hierarchically in layers until a single summary remains at the last layer. As the RepoQA paper does not use any short-context baselines, we propose our own baseline based on Chang et al. (2024)'s incremental merging approach. We modify the prompt to be suitable for the RepoQA task rather than summarization. We also construct a similar baseline for LOFT-Spider. A hierarchical baseline is not naturally amenable to these latter tasks as it is unclear how to merge summaries of independent functions nor why it would be beneficial.\nAblations. Our experiments evaluate several variations of our method. First, we compare in-place memories to amendments (Figure 4). Second, we evaluate when the proposed revision (Definition"}, {"title": "Structured Memories Improve Task Performance", "content": "In Table 2, our method beats both existing baselines (incremental and hierarchical merging) on BooookScore to a statistically significant degree (at worst p = 0.02) and almost matches the performance of the long context model. All variants of our approach perform similarly well on BooookScore, except for using amendments with adds and updates. This case slightly outperforms the other approaches. It is encouraging to see that the amendments method performs equally well if not better than an in-place memory. This is a promising signal that cache-optimized memories can be just as effective at producing strong final answers.\nWe see a similar picture with RepoQA and LOFT-Spider. Our method outperforms the baselines and approaches the long-context ceiling. These tasks are particularly difficult because they necessitate reasoning and aggregating over multiple code files and tables. It is not trivial to define a schema that optimally supports the reasoning involved in these tasks. We also believe that critical information is clustered in these tasks. Failing to add relevant information to the memory during the processing of an important chunk is likely to be substantially more costly than in summarization."}, {"title": "Amended Memories Are Scalably Token-Efficient", "content": "Incremental processing requires many more LLM calls than long context models and storing a memory in-context has a cost. We investigate whether our programmatic memory revisions and key-value caching optimizations bring about compute efficiency advantages. In this section, we measure cache hit rate as the proportion of tokens whose key-value activations could be reused (i.e., the number of tokens in the longest matching prefix to the last input prompt divided by the total number of tokens encoded). We also compute a cost index to compare the relative compute cost of each method. The weighting of the index was determined by examining the per-token pricing of popular public APIs, where decoding is approximately triple the cost of encoding (e.g., OpenAI GPT-40\u00b9 and Claude 3.5 Sonnet\u00b2).\nIn Table 3, we see that variants of our method achieve the best results for all metrics across both datasets. Notably, using amendments with updates leads to the highest cache hit rates, and in the case of BooookScore, also the lowest estimated cost. Similarly, for RepoQA, using amendments (but this time without updates) leads to the lowest cost.\nAs compute constraints can significantly reduce viable context lengths, we also analyze how the characteristics of our method change as we reduce the context size. In Figure 5, we use different chunk sizes on the RepoQA dataset using our cache-efficient amended memory approach without updates. For smaller chunk sizes, accuracy slightly decreases. The net encoded tokens stays relatively constant since cache hit rate improves. This is because a larger proportion of the context is taken by the memory, for which the majority of the tokens are pre-computed because of our cache-efficient approach. Meanwhile, tokens decoded increases as more incremental steps are required. However, tokens encoded dominates tokens decoded. Thanks to this property, remarkably, the weighted cost remains similar regardless of the chunk size. The near-constant cost of our approach allows it to scale down easily."}, {"title": "LLM Generations Compete With Hand-Crafted Schemas", "content": "While our method is domain-agnostic, it requires expertise to develop the schema. This schema should be defined such that the memory encodes all of the information required to complete the task without being verbose. In narrative summarization, this may be the most important events in the book; for code retrieval this may be a brief summary of each function or a direct measure of how closely it matches the description. Thus, it requires knowledge of the task. We determine whether this can be avoided by generating schemas with an LLM.\nIn Table 4, we use LLM-generated schemas (Appendix A.2) constructed by providing a brief description of the task and an example query (alongside some examples for other tasks) to the LLM. The output is a schema that we use to specify the memory. We compare this to the best result using our hand-crafted schemas from Table 2. The results reveal that our approach is competitive with hand-crafted expert schemas. Our method can be applied to tasks with little human input or domain expertise."}, {"title": "Conclusion", "content": "We proposed a method to solve long-range problems with short-context models using an incrementally revised in-context structured memory specified with a typed hierarchy schema. With this ap-"}, {"title": "Limitations", "content": "While we have shown that structured memories can be task-agnostic, improve quality, and improve token-efficiency, there remain some limitations to our work. First, we explore only two hand-crafted schemas for the book summarization and code retrieval tasks. There is likely to be a large space of effective and useful schemas for various types of tasks. Understanding how schemas should be designed for different tasks would help use structured memories more effectively. Second, the analysis of chunk size and token efficiency is an interesting preliminary study that demonstrates the cost-efficiency of our approach even in increasingly context-constrained environments. However, we were only able to examine a single dataset with just five different chunk sizes. Evaluating on more datasets and more chunk sizes would not only allow us to be more confident in our approach, but also would help investigate the presence of a scaling law for token-efficiency. Perhaps the most significant limitation is that, although we outperform other short context approaches, the ultimate goal is to achieve on-par performance with long-context models. Although we achieved this for the summarization task, there remains a gap to bridge for the more well-structured reasoning tasks."}]}