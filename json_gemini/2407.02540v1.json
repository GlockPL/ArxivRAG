{"title": "ANALYTICAL SOLUTION OF A THREE-LAYER NET-WORK WITH A MATRIX EXPONENTIAL ACTIVATION FUNCTION", "authors": ["Kuo Gai", "Shihua Zhang"], "abstract": "In practice, deeper networks tend to be more powerful than shallow ones, but this has not been understood theoretically. In this paper, we find a analytical solution of a three-layer network with a matrix exponential activation function, i.e.,\n\n\\(f(X) = W_3 \\exp(W_2 \\exp(W_1X)), X \\in C^{dxd}\\)\n\nhave analytical solutions for the equations\n\\[\n\\begin{cases}\n  Y_1 = f(X_1) \\\\\n  Y_2 = f(X_2)\n\\end{cases}\n\\]\n\nfor \\(X_1, X_2, Y_1, Y_2\\) with only invertible assumptions. Our proof shows the power of depth and the use of a non-linear activation function, since one layer network can only solve one equation,i.e.,\\(Y = WX\\).", "sections": [{"title": "INTRODUCTION", "content": "Deep neural networks have become successful in many fields, including computer vision, natural language processing, bioinformatics, etc. However, the mathematical principle of deep learning is still not fully understood, especially why deeper networks with non-linear activation functions tend to be more powerful than shallower ones.\n\nIt is well known that sufficient large depth-2 neural networks with reasonable activation functions can approximate any continuous function on a bounded domain (Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989; Barron, 1994; Pinkus, 1999), but this requires the width of networks to be exponential. Recent authors have shown that some functions can be approximated by deeper net-works with fewer neurons than by shallower ones, such as radial functions (Eldan & Shamir, 2016), Boolean circuit (Rossman et al., 2015) or functions induced by neural network (Telgarsky, 2016). However, these functions are far from the function approximated by neural networks in practice.\n\nThere are also some studies on approximating data points of a fixed number instead of continuous functions, which is more general since data points can be sampled from arbitrary distributions. However, such works focus more on width rather than depth. For instance, the notable framework neural tangent kernel(NTK)(Jacot et al., 2018) proved that neural networks can fit the data with error 0 if the width is infinite. However, such wide neural networks would also have an extremely large number of parameters, and extract random features of data. Moreover, current state of art results are typically achieved by deep neural networks (He et al., 2016; Krizhevsky et al., 2012). Generally, when the width of the network is bounded since the function class of neural networks becomes more complex after the composition of layers, the optimization process of neural networks may not find the global optimal solution. There are some empirical explorations which reveal non-trivial properties of the landscape (Goodfellow et al., 2014; Li et al., 2018). However, these properties"}, {"title": "PRELIMINARY", "content": "The matrix exponential is a matrix function on the square matrices analogous to the ordinary exponential function. Let X be an d \u00d7 d complex matrix. The exponential of X, denoted by exp(X) is the d x d matrix given by the power series\n\\[\nexp(X) = \\sum_{k=0}^{\\infty} \\frac{1}{k!}X^k\n\\]\nwhere \\(X^0\\) is defined to be the identity matrix I with the same dimensions as X. The matrix exponential is well studied in the theory of Lie group and has many good properties.\n\nProposition 1. Let X,Y \u2208 Cd\u00d7d. If XY = YX, then exp(X) exp(Y) = exp(X + Y)\n\nProposition 2. The matrix exponential gives a surjective map\n\\[\nexp: M_d(C) \\rightarrow GL(d, C)\n\\]\nwhere \\(M_d(C)\\) is the space of all d \u00d7 d complex matrices and GL(d, C) is the general linear group of degree d, i.e. the group of all d \u00d7 d invertible matrices.\n\nIn general, exp(X) exp(Y) can be expressed by the Baker Campbell Hausdorff (BCH) formula, and when X and Y commute, the computation of BCH formula can be simplified as in Proposition 1. Proposition 2 means every invertible matrix X can be written as the exponential of some other matrix Z (for this, it is essential to consider the field C and not R).\n\nZ can be calculated through the logarithm of matrix. First we need to find the Jordan decomposition of X and calculate the logarithm of the Jordan blocks. For instance, we can write a Jordan block as\n\\[\nB = \\begin{bmatrix}\n\\lambda & 1 & 0 & 0 & 0 & 0 \\\\\n0 & \\lambda & 1 & 0 & 0 & 0 \\\\\n0 & 0 & \\lambda & 1 & 0 & 0 \\\\\n0 & 0 & 0 & \\lambda & 1 & 0 \\\\\n0 & 0 & 0 & 0 & \\lambda & 1 \\\\\n0 & 0 & 0 & 0 & 0 & \\lambda\n\\end{bmatrix} = \\lambda\\begin{bmatrix}\n1 & {\\lambda^{-1}} & 0 & 0 & 0 & 0 \\\\\n0 & 1 & {\\lambda^{-1}} & 0 & 0 & 0 \\\\\n0 & 0 & 1 & {\\lambda^{-1}} & 0 & 0 \\\\\n0 & 0 & 0 & 1 & {\\lambda^{-1}} & 0 \\\\\n0 & 0 & 0 & 0 & 1 & {\\lambda^{-1}} \\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix} = \\lambda(I + K)\n\\]"}, {"title": "MAIN RESULT", "content": "The basic task of machine learning is to find a function which maps the data to its label, i.e., for given {(xi, Yi)}_1 where xi \u2208 Rdx, yi \u2208 Rdy, solve the equations f(xi) = yi, i = 1,\u2026,n.\n\nSpecifically, for neural networks, f is composed of linear transformations and nonlinear activation functions, i.e., for m-layer network,\n\\[\nf(\u00b7) = W_m \\circ (W_{m-1}\u00b7\u00b7\u00b7\\circ (W_1\u00b7))\n\\]\nwhere o is the nonlinear activation function and W\u2081 \u2208 Rdz\u00d7d1, Wk \u2208 Rdk\u22121\u00d7dk, k = 2,\u2026, m-1, Wm\u2208 Rdm-1\u00d7dy. o is elementwise function such as ReLU, sigmoid and tanh function. Generally, proving the existence of solution of nonlinear system is hard, especially when the element-wise function o does not integral well with the linear transformation matrix W. For instance, let o(x) = x\u00b2, then \u03c3(A) = A \u25e6 A for A \u2208 ]Rd\u00d7d', where o is the Hadamard product. As we know, generally, A \u25e6 A can not be expressed as a polynomial of A, i.e., A \u25cb A \u2260 poly(A). This causes difficulties in finding the analytical solution of neural networks, since we can not transform the output of each layer to a operable form. To address this issue, we use matrix exponential function as nonlinear activation function instead, which gives chance to find the solution to the system when number of layers is more than one.\n\nTo make matrix exponential well-defined, we assume X, Y, W are square. To make the solution exists, we assume the items of X, Y, W in C. Consider X, Y \u2208 Cd\u00d7d and X is invertible, then W = YX-1 can solve the equation Y = WX. There doesn't exist solution of Y\u2081 = WX1, Y2 = WX2 for X1, X2, Y1, Y2 \u2208 Cd\u00d7d except degenerate cases, since the number of parameter d\u00b2 is less than the number of equations 2d2. If we let the weight matrix be 'wider', i.e.,\n\\[\nW = \\begin{bmatrix} W_1 & 0 \\\\ 0 & W_2 \\end{bmatrix}\n\\]\nthen with the assumption that X\u2081 and X2 are invertible, W\u2081 = Y\u2081X\u012b\u00af\u00b9 and W2 = Y2X21 can solve the equations\n\\[\n\\begin{bmatrix} Y_1 \\\\ Y_2 \\end{bmatrix} = \\begin{bmatrix} W_1 & 0 \\\\ 0 & W_2 \\end{bmatrix} \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}\n\\]\nThe above equation has solution because we can separate it to two sub-problems and solve W\u2081 and W2 sequentially. However, this will not happen when we compose W1 and W2 (two-layer network with identity activation function), which means, solving the equation\n\\[\nY_1 = W_2W_1X_1; Y_2 = W_2W_1X_2\n\\]\nWhen W\u2081 is fixed, then W2 with d\u00b2 parameters is involved in 2d2 equations, i.e., Y\u2081 = W2(W1X1) and Y2 = W2(W1X2) and has no solution in general. Situation changes again by adding non-linear activation function, i.e., solving the equations\n\\[\n\\begin{aligned}\n  Y_1 &= W_2\\sigma(W_1X_1) \\\\\n  Y_2 &= W_2\\sigma(W_1X_2)\n\\end{aligned}\n\\]\nFrom the second equation, we obtain W2 = Y2\u03c3(W1X2)-1. Taking it into the first equation, we have\n\\[\nY_1 = Y_2\\sigma(W_1X_2)^{-1}\\sigma(W_1X_1)\n\\]"}, {"title": "EXPERIMENTAL RESULTS", "content": "Since we already found the analytical solution of a three-layer network with matrix exponential activation function, numerical experiments is not necessary. In this section, we focus on experiments on element-wise activation functions such as Relu and sigmoid using similar method. As discussed in Section 2, similar equation for two-layer network with element-wise activation \u03c3, i.e.,\n\\[\n\\begin{cases}\n  Y_1 = W_2\\sigma(W_1X_1) \\\\\n  Y_2 = W_2\\sigma(W_1X_2)\n\\end{cases}\n\\]\nwhich equals to solving W\u2081 and W2 sequentially through\n\\[\n\\begin{cases}\n  Y_1 = Y_2\\sigma(W_1X_2)^{-1}\\sigma(W_1X_1) \\\\\n  W_2 = Y_2\\sigma(W_1X_2)^{-1}\n\\end{cases}\n\\]\nIn our experiments, we optimize ||Y\u2081 \u2013 Y2\u03c3(W1X2)\u207b\u00b9\u03c3(W1X1)|| with gradient descent. Each item of X1,X2, Y1 and Y2 is sampled from Gaussian distribution N(0,1). For comparison, we compute the same value when o is the identity function, i.e., ||Y1 \u2013 Y2(W1X2)\u207b\u00b9W1X1|| = ||Y1 - Y2X2\u207b\u00b9X1||. Then we can construct a score to measure the benefit of using sigmoid function or ReLU function in the training process\n\\[\ns = \\frac{||Y_1 \u2013 Y_2\\sigma(W_1X_2)^{-1}\\sigma(W_1X_1)||}{||Y_1 - Y_2X_2^{-1}X_1||}\n\\]\nIn the experiment (Fig.1), we find that both ReLU and Sigmoid function can find the optimal W\u2081 with s close to 0. This indicates that a two-layer network with ReLU or Sigmoid activation function has obvious benefits compared with the identity function and has the potential to solve twice the number of equations. Also the s score decrease with the increasing of dimension, which means, the optimization problem becomes easier in high dimention space. However, it is hard to prove the existence of a solution of equality (34) and the existence of a path from initial weights to global optimal weights with gradient descent."}, {"title": "CONCLUSION", "content": "In this paper, we design a problem for a three-layer network with matrix exponential as an activation function and find the analytical solution. By doing this, we show the power of depth by comparing our three-layer networks to single-layer ones. Our result has merit compared with existing studies, both the studies finding special functions to show the power of depth and studies analyzing the width of networks through optimization methods. We also shed light on two-layer networks with element-wise activation functions through experiments, indicating that neural networks have the potential to solve the number of equations equaling the number of parameters. As activation function, matrix exponential may provide less non-linearity as element-wise activation function do, but it may be possible to analyze based on the results in Lie theory. In the future, we will try to extend our method to multi-layer cases."}]}