{"title": "ANALYTICAL SOLUTION OF A THREE-LAYER NETWORK WITH A MATRIX EXPONENTIAL ACTIVATION FUNCTION", "authors": ["Kuo Gai", "Shihua Zhang"], "abstract": "In practice, deeper networks tend to be more powerful than shallow ones, but this has not been understood theoretically. In this paper, we find a analytical solution of a three-layer network with a matrix exponential activation function, i.e.,\n\n$f(X) = W_3 exp(W_2 exp(W_1X)), X \\in C^{dxd}$\n\nhave analytical solutions for the equations\n\n$\\begin{cases}\nY_1 = f(X_1)\\\\\nY_2 = f(X_2)\n\\end{cases}$\n\nfor $X_1, X_2, Y_1, Y_2$ with only invertible assumptions. Our proof shows the power of depth and the use of a non-linear activation function, since one layer network can only solve one equation,i.e.,$Y = WX$.", "sections": [{"title": "INTRODUCTION", "content": "Deep neural networks have become successful in many fields, including computer vision, natural language processing, bioinformatics, etc. However, the mathematical principle of deep learning is still not fully understood, especially why deeper networks with non-linear activation functions tend to be more powerful than shallower ones.\n\nIt is well known that sufficient large depth-2 neural networks with reasonable activation functions can approximate any continuous function on a bounded domain (Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989; Barron, 1994; Pinkus, 1999), but this requires the width of networks to be exponential. Recent authors have shown that some functions can be approximated by deeper net-works with fewer neurons than by shallower ones, such as radial functions (Eldan & Shamir, 2016), Boolean circuit (Rossman et al., 2015) or functions induced by neural network (Telgarsky, 2016). However, these functions are far from the function approximated by neural networks in practice.\n\nThere are also some studies on approximating data points of a fixed number instead of continuous functions, which is more general since data points can be sampled from arbitrary distributions. However, such works focus more on width rather than depth. For instance, the notable framework neural tangent kernel(NTK)(Jacot et al., 2018) proved that neural networks can fit the data with error 0 if the width is infinite. However, such wide neural networks would also have an extremely large number of parameters, and extract random features of data. Moreover, current state of art results are typically achieved by deep neural networks (He et al., 2016; Krizhevsky et al., 2012). Generally, when the width of the network is bounded since the function class of neural networks becomes more complex after the composition of layers, the optimization process of neural networks may not find the global optimal solution. There are some empirical explorations which reveal non-trivial properties of the landscape (Goodfellow et al., 2014; Li et al., 2018). However, these properties"}, {"title": "PRELIMINARY", "content": "The matrix exponential is a matrix function on the square matrices analogous to the ordinary expo-nential function. Let X be an $d \\times d$ complex matrix. The exponential of X, denoted by exp(X) is the $d \\times d$ matrix given by the power series\n\n$exp(X) = \\sum_{k=0}^{\\infty} \\frac{1}{k!}X^k$  (1)\n\nwhere $X^0$ is defined to be the identity matrix I with the same dimensions as X. The matrix exponential is well studied in the theory of Lie group and has many good properties.\n\nProposition 1. Let $X, Y \\in C^{d \\times d}$. If $XY = YX$, then $exp(X) exp(Y) = exp(X + Y)$\n\nProposition 2. The matrix exponential gives a surjective map\n\n$exp: M_d(C) \\rightarrow GL(d, C)$ (2)\n\nwhere $M_d(C)$ is the space of all $d \\times d$ complex matrices and $GL(d, C)$ is the general linear group of degree d, i.e. the group of all $d \\times d$ invertible matrices.\n\nIn general, $exp(X) exp(Y)$ can be expressed by the Baker Campbell Hausdorff (BCH) formula, and when X and Y commute, the computation of BCH formula can be simplified as in Proposition 1. Proposition 2 means every invertible matrix X can be written as the exponential of some other matrix Z (for this, it is essential to consider the field C and not R).\n\nZ can be calculated through the logarithm of matrix. First we need to find the Jordan decomposition of X and calculate the logarithm of the Jordan blocks. For instance, we can write a Jordan block as\n\n$B =\\begin{bmatrix}\n\\lambda & 1 & 0 & 0 & \\cdots & 0\\\\n0 & \\lambda & 1 & 0 & \\cdots & 0\\\\n0 & 0 & \\lambda & 1 & \\cdots & 0\\\\n\\vdots & \\vdots & \\vdots & \\vdots &  & \\vdots\\\\n0 & 0 & 0 & 0 & \\cdots & \\lambda\n\\end{bmatrix} = \\lambda \\begin{bmatrix}\n1 & \\lambda^{-1} & 0 & 0 & \\cdots & 0\\\\n0 & 1 & \\lambda^{-1} & 0 & \\cdots & 0\\\\n0 & 0 & 1 & \\lambda^{-1} & \\cdots & 0\\\\n\\vdots & \\vdots & \\vdots & \\vdots &  & \\vdots\\\\n0 & 0 & 0 & 0 & \\cdots & 1\n\\end{bmatrix} = \\lambda (I + K)$ (3)"}, {"title": "MAIN RESULT", "content": "The basic task of machine learning is to find a function which maps the data to its label, i.e., for given ${(x_i, Y_i)}_{i=1}^n$ where $x_i \\in R^{d_x}$, $y_i \\in R^{d_y}$, solve the equations $f(x_i) = y_i, i = 1,\\dots,n$. Specifically, for neural networks, f is composed of linear transformations and nonlinear activation functions, i.e., for m-layer network,\n\n$f(\\cdot) = W_m \\circ (W_{m-1} \\cdots \\sigma (W_1 \\cdot))$ (6)\n\nwhere $\\sigma$ is the nonlinear activation function and $W_1 \\in R^{d_2 \\times d_1}, W_k \\in R^{d_{k-1} \\times d_k}, k = 2,\\dots, m-1$, $W_m \\in R^{d_{m-1} \\times d_y}$. $\\sigma$ is elementwise function such as ReLU, sigmoid and tanh function. Generally, proving the existence of solution of nonlinear system is hard, especially when the element-wise function $\\sigma$ does not integral well with the linear transformation matrix W. For instance, let $\\sigma(x) = x^2$, then $\\sigma(A) = A \\circ A$ for $A \\in ]R^{d \\times d'}$, where $\\circ$ is the Hadamard product. As we know, generally, $A \\circ A$ can not be expressed as a polynomial of A, i.e., $A \\circ A \\neq poly(A)$. This causes difficulties in finding the analytical solution of neural networks, since we can not transform the output of each layer to a operable form. To address this issue, we use matrix exponential function as nonlinear activation function instead, which gives chance to find the solution to the system when number of layers is more than one.\n\nTo make matrix exponential well-defined, we assume X, Y, W are square. To make the solution exists, we assume the items of X, Y, W in C. Consider $X, Y \\in C^{d \\times d}$ and X is invertible, then $W = YX^{-1}$ can solve the equation $Y = WX$. There doesn't exist solution of $Y_1 = WX_1, Y_2 = WX_2$ for $X_1, X_2, Y_1, Y_2 \\in C^{d \\times d}$ except degenerate cases, since the number of parameter $d^2$ is less than the number of equations $2d^2$. If we let the weight matrix be 'wider', i.e.,\n\n$W = \\begin{bmatrix}\nW_1 & 0 \\\\n0 & W_2\n\\end{bmatrix}$  (7)\n\nthen with the assumption that $X_1$ and $X_2$ are invertible, $W_1 = Y_1X_1^{-1}$ and $W_2 = Y_2X_2^{-1}$ can solve the equations\n\n$\\begin{bmatrix}\nW_1 & 0 \\\\n0 & W_2\n\\end{bmatrix} \\begin{bmatrix}\nX_1 & 0 \\\\n0 & X_2\n\\end{bmatrix} = \\begin{bmatrix}\nY_1 & 0 \\\\n0 & Y_2\n\\end{bmatrix}$ (8)\n\nThe above equation has solution because we can separate it to two sub-problems and solve $W_1$ and $W_2$ sequentially. However, this will not happen when we compose $W_1$ and $W_2$ (two-layer network with identity activation function), which means, solving the equation\n\n$Y_1 = W_2W_1X_1; Y_2 = W_2W_1X_2$  (9)\n\nWhen $W_1$ is fixed, then $W_2$ with $d^2$ parameters is involved in $2d^2$ equations, i.e., $Y_1 = W_2(W_1X_1)$ and $Y_2 = W_2(W_1X_2)$ and has no solution in general. Situation changes again by adding non-linear activation function, i.e., solving the equations\n\n$\\begin{aligned}\nY_1 &= W_2 \\sigma(W_1 X_1)\\\\\nY_2 &= W_2 \\sigma(W_1 X_2)\n\\end{aligned}$ (10)\n\nFrom the second equation, we obtain $W_2 = Y_2 \\sigma(W_1 X_2)^{-1}$. Taking it into the first equation, we have\n\n$Y_1 = Y_2 \\sigma(W_1 X_2)^{-1} \\sigma(W_1 X_1)$ (11)"}, {"title": "CONCLUSION", "content": "In this paper, we design a problem for a three-layer network with matrix exponential as an activation function and find the analytical solution. By doing this, we show the power of depth by comparing our three-layer networks to single-layer ones. Our result has merit compared with existing studies, both the studies finding special functions to show the power of depth and studies analyzing the width of networks through optimization methods. We also shed light on two-layer networks with element-wise activation functions through experiments, indicating that neural networks have the potential to solve the number of equations equaling the number of parameters. As activation function, matrix exponential may provide less non-linearity as element-wise activation function do, but it may be possible to analyze based on the results in Lie theory. In the future, we will try to extend our method to multi-layer cases."}]}