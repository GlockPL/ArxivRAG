{"title": "Big Cooperative Learning", "authors": ["Yulai Cong"], "abstract": "Cooperation plays a pivotal role in the evolution of human intelligence; moreover, it also underlies the recent revolutionary advancement of artificial intelligence (AI) that is driven by foundation models. Specifically, we reveal that the training of foundation models can be interpreted as a form of big cooperative learning (abbr. big learning), where massive learning individuals/tasks cooperate to approach the unique essence of data from diverse perspectives of data prediction, leveraging a universal model. The presented big learning therefore unifies most training objectives of foundation models within a consistent framework, where their underlying assumptions are exposed simultaneously. We design tailored simulations to demonstrate the principle of big learning, based on which we provide learning-perspective justifications for the successes of foundation models, with interesting side-products. Furthermore, we reveal that big learning is a new dimension for upgrading conventional machine learning paradigms, valuable for endowing reinvigorations to associated applications; as an illustrative example, we propose the BigLearn-GAN, which is a novel adversarially-trained foundation model with versatile data sampling capabilities. Code is available at https://github.com/YulaiCong/BigCooperativeLearning.", "sections": [{"title": "Introduction", "content": "Cooperation is essential to the survival of human society and to the formulation and evolution of human intelligence. \u201cWithout the cooperation of its members society cannot survive, and the society of man has survived because the cooperativeness of its members made survival possible It was not an advantageous individual here and there who did so, but the group\" (Ashley Montagu, 1965) [18]. Regarding the formulation of human intelligence, cooperative learning is an effective and efficient educational tool extensively researched by anthropologists and educational psychologists [36, 16, 17, 15], where different individuals (often students with different capabilities) cooperate with each other to achieve a common goal.\nThe world is experiencing a groundbreaking AI revolution with cooperation among scientists and the rise of big/foundation models [7, 48], such as the GPT series [28, 27, 29, 9], Sora [8], Geminis [34, 40], DALL-Es [33, 32, 6], and BERTs [11, 21, 24], which brings advanced AI capabilities to diverse application domains with impressive robustness [38], adaptability [13], and generalization [33], lighting up the way towards Artificial General Intelligence (AGI).\nIn addition to advancing human intelligence, we reveal that cooperation also plays a crucial role in the extraordinary successes of foundation models. Specifically, the training of most foundation models (such as mask-and-predict [11] and next-token-prediction [9]) can be interpreted as big cooperative learning (abbreviated as big learning), where massive learning individuals (e.g., learning tasks with various masks/preceding-contexts for mask-and-predict/next-token-prediction) have different and diverse characteristics (i.e., focus on different conditional data predictions) but share a common goal (i.e., to approach the unique essence of data with a universal set of model parameters). We will show that the presented big cooperative learning contains most training objectives of foundation"}, {"title": "Preliminary", "content": "Big/Foundation models. Taking shape in the field of natural language processing (NLP), foundation models have dramatically changed AI-related research and applications, sparking a groundbreaking Al revolution that is sweeping the world [7, 48]. Representative foundation models include the BERT series [11, 21, 38, 24, 19], which reform NLP applications, the GPT series [31, 9, 29, 27, 28], which give rise to a worldwide explosion of large language models [40, 34, 3, 2, 1], and multi-modal foundation models [35, 4, 6, 8], which initiate a new revolution of AI-Generated Content (AIGC).\nFoundation models are mainly trained/pretrained under the maximum-likelihood-learning principle with either mask-and-predict (i.e., masked auto-encoding or masked language modeling) [11, 13] or next-token-prediction (i.e., auto-regressive/causal language modeling) [31, 9].\nSpecifically, the mask-and-predict training seeks to optimize the universal set of parameters \u03b8 via\nmax Eq(x)q(5) log pMAE (XscXs) or max Eq(xs) Eq(xsc|xs)log pMAE (Xsc|xs),\n(1)\nwhere x \u2208 RL\u00d7D, sampled from the underlying data distribution q(x), denotes a sequence of L tokens of dimension D (e.g., x \u2208 Z6\u00d71 may represent the 6-word sentence of \u201cbig learning is a general concept", "big _ is a general _": "and xsc is \u201c_ learning concept"}, {"title": "Big Cooperative Learning", "content": "Assumptions. Throughout the paper, we assume trustworthy data samples from q(x), sufficient model capacity of p\u03b8(x), and the existence of an optimal \u03b8\u2217 satisfying p\u03b8\u2217 (x) = q(x). Note p\u03b8\u2217 (x) = q(x) means that the data essence is implicitly summarized in \u03b8\u2217 and that\n\u2022 both sides (e.g., joint distributions of x = [x1,\u2026,xL]T \u2208 RL\u00d71) share all the marginal and conditional distributions (i.e., p\u03b8\u2217 (xs) = q(xs) and p\u03b8\u2217 (xT|xs) = q(xT|xs) hold for any non-overlapping subsets (S, T) of the index set L = {1,\u2026\u2026, L}); and\n\u2022 both sides are identical in any transformed domain, i.e., with X = g(x) to transform p\u03b8\u2217 (x)/q(x) into p\u03b8\u2217 (X)/q(X) in the transformed X domain, we have p\u03b8\u2217 (xT|xs) = q(xT|xs), \u2200(S, T).\nBelow we first reveal that a single data sample demonstrates versatile data-sampling capabilities and that existing foundation models leverage a portion of these demonstrations to form their training/pretraining. Next, based on the revelations, we propose our big cooperative learning (abbr. big learning), which exhaustively exploits data information via versatile data-sampling demonstrations in a cooperative manner. We then design tailored 2-D simulations to explicitly demonstrate the principle of big cooperative learning, with additional interesting side-products. Finally, we bring to light that big learning is a new dimension for upgrading conventional machine learning paradigms and, as an illustrative example, we upgrade the standard GAN into the big-learned BigLearn-GAN.\n3.1 Versatile but Underutilized Data-Sampling Demonstrations Within a Single Data Sample\nIn conventional machine learning paradigms, a complete data sample x \u223c q(x) is often only utilized in the joint space (i.e., all tokens {xt}\nL\nt=1 of the joint x = [x1,\u2026, xL]T are always used"}]}