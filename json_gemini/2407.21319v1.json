{"title": "Big Cooperative Learning", "authors": ["Yulai Cong"], "abstract": "Cooperation plays a pivotal role in the evolution of human intelligence; moreover, it also underlies the recent revolutionary advancement of artificial intelligence (AI) that is driven by foundation models. Specifically, we reveal that the training of foundation models can be interpreted as a form of big cooperative learning (abbr. big learning), where massive learning individuals/tasks cooperate to approach the unique essence of data from diverse perspectives of data prediction, leveraging a universal model. The presented big learning therefore unifies most training objectives of foundation models within a consistent framework, where their underlying assumptions are exposed simultaneously. We design tailored simulations to demonstrate the principle of big learning, based on which we provide learning-perspective justifications for the successes of foundation models, with interesting side-products. Furthermore, we reveal that big learning is a new dimension for upgrading conventional machine learning paradigms, valuable for endowing reinvigorations to associated applications; as an illustrative example, we propose the BigLearn-GAN, which is a novel adversarially-trained foundation model with versatile data sampling capabilities. Code is available at https://github.com/YulaiCong/BigCooperativeLearning.", "sections": [{"title": "1 Introduction", "content": "Cooperation is essential to the survival of human society and to the formulation and evolution of human intelligence. \u201cWithout the cooperation of its members society cannot survive, and the society of man has survived because the cooperativeness of its members made survival possible It was not an advantageous individual here and there who did so, but the group\" (Ashley Montagu, 1965) [18]. Regarding the formulation of human intelligence, cooperative learning is an effective and efficient educational tool extensively researched by anthropologists and educational psychologists [36, 16, 17, 15], where different individuals (often students with different capabilities) cooperate with each other to achieve a common goal.\nThe world is experiencing a groundbreaking AI revolution with cooperation among scientists and the rise of big/foundation models [7, 48], such as the GPT series [28, 27, 29, 9], Sora [8], Geminis [34, 40], DALL-Es [33, 32, 6], and BERTs [11, 21, 24], which brings advanced AI capabilities to diverse application domains with impressive robustness [38], adaptability [13], and generalization [33], lighting up the way towards Artificial General Intelligence (AGI).\nIn addition to advancing human intelligence, we reveal that cooperation also plays a crucial role in the extraordinary successes of foundation models. Specifically, the training of most foundation models (such as mask-and-predict [11] and next-token-prediction [9]) can be interpreted as big cooperative learning (abbreviated as big learning), where massive learning individuals (e.g., learning tasks with various masks/preceding-contexts for mask-and-predict/next-token-prediction) have different and diverse characteristics (i.e., focus on different conditional data predictions) but share a common goal (i.e., to approach the unique essence of data with a universal set of model parameters). We will show that the presented big cooperative learning contains most training objectives of foundation"}, {"title": "2 Preliminary", "content": "Big/Foundation models. Taking shape in the field of natural language processing (NLP), foundation models have dramatically changed AI-related research and applications, sparking a groundbreaking Al revolution that is sweeping the world [7, 48]. Representative foundation models include the BERT series [11, 21, 38, 24, 19], which reform NLP applications, the GPT series [31, 9, 29, 27, 28], which give rise to a worldwide explosion of large language models [40, 34, 3, 2, 1], and multi-modal foundation models [35, 4, 6, 8], which initiate a new revolution of AI-Generated Content (AIGC).\nFoundation models are mainly trained/pretrained under the maximum-likelihood-learning principle with either mask-and-predict (i.e., masked auto-encoding or masked language modeling) [11, 13] or next-token-prediction (i.e., auto-regressive/causal language modeling) [31, 9].\nSpecifically, the mask-and-predict training seeks to optimize the universal set of parameters \u03b8 via\n$\\max_{\\theta} \\mathbb{E}_{q(x), q(S)} \\log p_{\\theta}^{MAE}(x_{S^c}|x_S)$ or $\\max_{\\theta} \\mathbb{E}_{q(x_S)} \\mathbb{E}_{q(x_{S^c}|x_S)} \\log p_{\\theta}^{MAE}(x_{S^c}|x_S)$,\n(1)\nwhere $x \\in \\mathbb{R}^{L \\times D}$, sampled from the underlying data distribution $q(x)$, denotes a sequence of $L$ tokens of dimension $D$ (e.g., $x \\in \\mathbb{Z}^{6 \\times 1}$ may represent the 6-word sentence of \u201cbig learning is a general concept\u201d), $S$ (sampled from a predefined $q(S)$) is a random subset of the token index set $L = \\{1, \\dots, L\\}$ (e.g., $S = \\{1, 3, 4, 5\\}$, its complement $S^c = \\{2, 6\\}$, $x_S$ is \u201cbig _ is a general _\u201d, and $x_{S^c}$ is \u201c_learning concept\u201d), and the model $p_{\\theta}^{MAE}(x_{S^c}|x_S)$ adopts the conditional independence assumption that ignores the dependency among the tokens of $x_{S^c}$, i.e.,\n$p_{\\theta}^{MAE}(x_{S^c}|x_S) = \\prod_{t \\in S^c} p_{\\theta}^{MAE}(x_t|x_S)$,\n(2)\nwhere $p_{\\theta}^{MAE}(x_t|x_S)$ is often modeled as a Categorical distribution, i.e., Categorical($x_t|p_{\\theta}(s)$) with probabilities $p_{\\theta}(s)$, for discrete (text) token $x_t \\in \\mathbb{Z}^{|\\mathcal{V}|}$ [11], and a Gaussian distribution, i.e., $\\mathcal{N}(x_t|\\mu_{\\theta}(x_S), \\sigma^2)$ with mean $\\mu_{\\theta}(x_S)$ and variance $\\sigma^2$, for continuous (image) token $x_t \\in \\mathbb{R}^D$ [13].\nDifferent from mask-and-predict, the next-token-prediction training optimizes the universal parameter set \u03b8 in an auto-regressive manner via\n$\\max_{\\theta} \\mathbb{E}_{q(x)} \\sum_{t=1}^L \\log p_{\\theta}^{AR}(x_t|x_{<t})$ or $\\max_{\\theta} \\mathbb{E}_{q(x)} \\mathbb{E}_{q(t)} \\log p_{\\theta}^{AR}(x_t|x_{<t})$,\n(3)"}, {"title": "3 Big Cooperative Learning", "content": "Assumptions. Throughout the paper, we assume trustworthy data samples from q(x), sufficient model capacity of $p_{\\theta}(x)$, and the existence of an optimal $\\theta^*$ satisfying $p_{\\theta^*}(x) = q(x)$. Note $p_{\\theta^*}(x) = q(x)$ means that the data essence is implicitly summarized in $\\theta^*$ and that\n* both sides (e.g., joint distributions of $x = [x_1,\\dots,x_L]^T \\in \\mathbb{R}^{L \\times 1}$) share all the marginal and conditional distributions (i.e., $p_{\\theta^*}(x_S) = q(x_S)$ and $p_{\\theta^*}(x_T|x_S) = q(x_T|x_S)$ hold for any non-overlapping subsets (S, T) of the index set $L = \\{1,\\dots, L\\}$); and\n* both sides are identical in any transformed domain, i.e., with $X = g(x)$ to transform $p_{\\theta^*}(x)/q(x)$ into $p_{\\theta^*}(X)/q(X)$ in the transformed X domain, we have $p_{\\theta^*}(X_T|X_S) = q(X_T|X_S), \\forall (S, T)$.\nBelow we first reveal that a single data sample demonstrates versatile data-sampling capabilities and that existing foundation models leverage a portion of these demonstrations to form their training/pretraining. Next, based on the revelations, we propose our big cooperative learning (abbr. big learning), which exhaustively exploits data information via versatile data-sampling demonstrations in a cooperative manner. We then design tailored 2-D simulations to explicitly demonstrate the principle of big cooperative learning, with additional interesting side-products. Finally, we bring to light that big learning is a new dimension for upgrading conventional machine learning paradigms and, as an illustrative example, we upgrade the standard GAN into the big-learned BigLearn-GAN.", "sections": [{"title": "3.1 Versatile but Underutilized Data-Sampling Demonstrations Within a Single Data Sample", "content": "In conventional machine learning paradigms, a complete data sample $x \\sim q(x)$ is often only utilized in the joint space (i.e., all tokens $\\{x_t\\}_{t=1}^L$ of the joint $x = [x_1,\\dots, x_L]^T$ are always used"}, {"title": "3.2 Big Cooperative Learning for Exhaustive Data Exploitation", "content": "Our main motivations include (i) versatile data-sampling demonstrations are readily given in data samples, (ii) these demonstrations can be used to form massive learning tasks (i.e., learning individuals in human cooperative learning), (iii) they are different manifestations of the"}, {"title": "3.3 Tailored 2-D Simulations to Demonstrate the Big-Learning Principle", "content": "It's not easy to comprehend the principle of a general concept like big cooperative learning. To explicitly demonstrate the cooperation among the diverse matching tasks in Eq. 6, we leverage Gaussian Mixture Models (GMMs) to design simple but controllable 2-D simulations, where (i) the model capacity is guaranteed to be sufficient and (ii) the loss of each matching task can be computed traversely and shown as an image (thus no need to consider the influence of optimization).\nSpecifically, both q(x) and $p_{\\theta}(x)$ are set as a GMM with K = 2 components (abbr. 2-GMM), i.e.,\n$q(x) = p_{\\theta^*}(x) = \\sum_{i=1}^2 \\mathcal{N}(x | \\mu_i, \\Sigma), and  p_{\\theta}(x) = \\sum_{i=1}^2 \\mathcal{N}(x | \\mu_i^\\theta, \\Sigma^\\theta)$,\n(7)\nwhere $\\mu_1 = [-1,0]^T, \\mu_2 = [1,0]^T, \\mu_1^\\theta = [\\mu_{1},0]^T, \\mu_2^\\theta = [\\mu_{2},0]^T, \\theta = [\\mu_{1}, \\mu_{2}]^T$ contains 2-D trainable parameters, and $\\Sigma_1 = \\Sigma_2 = \\Sigma_1^\\theta = \\Sigma_2^\\theta = \\sigma^2 I$ with hyperparameter $\\sigma^2$. Accordingly, Eq. 6 with $X = x$ consists of 1 joint matching, 2 marginal matchings, and 2 types of conditional matchings. We set the arrow \u201c\u2192\u201d as the reverse KL divergence, which is closely related to adversarial learning. For example, the loss for joint matching is $KL[p_{\\theta}(x)||q(x)]$, which is a function w.r.t. the 2-D \u03b8.\nConsidering that practical applications often have dependent tokens (e.g., correlated image patches), we reveal that the dependence among tokens enables cooperation among joint, marginal, and conditional matchings. Specifically, we modify the above 2-D simulation by using a rotation transformation (i.e., $y = Ax$) to introduce dependence among tokens of the transformed y; we then perform joint, marginal, and conditional matchings in the transformed y-domain. Fig. 3 demonstrates the loss surfaces when A represents 15\u00b0, 45\u00b0, and 60\u00b0 rotations, respectively. It's evident from the losses associated with the 15\u00b0 rotation that, in situations with dependent tokens, cooperation arise among different matchings because local optima unstably vary with the matching task but the global optima are stably the same; similar phenomena also hold across diverse transformed domains (e.g., see the loss surfaces of the $y_1$-marginal matchings under different rotations). Therefore, multitask learning with diverse matchings is expected to overlook the unstable local optima but focus on the stable global ones, manifested as a power of exploration.\nThe explicit demonstrations in Fig. 3 justify the principle of our big cooperative learning (with Eq. (6)) in a lightweight way, where different learning individuals (i.e., matching tasks with different"}, {"title": "3.4 Big Learning as a New Dimension for Upgrading Machine Learning Paradigms", "content": "After noticing that (i) groundbreaking foundation models are pretrained by exploiting a potion of data-sampling demonstrations inherent in their training data, (ii) these demonstrations can be exhaustively exploited to form versatile matchings (see Eq. (6)), among which cooperation could arise to help overlook local optima and focus on global ones, (iii) the data-sampling demonstrations are also available in conventional machine learning paradiagms, where often only joint matching is employed (see Eqs. (4) and (5)), we bring to light that big cooperative learning is a new dimension for upgrading conventional machine learning paradigms.\nBefore presenting an illustrative example, we first note that a similar idea already underlies several recent/concurrent papers. For example, [10] simultaneously employs joint, marginal, and trans-formed marginal matchings to effectively address the local-optimum challenge of the conventional expectation-maximization algorithm. [4] trains a diffusion model via simultaneous joint, marginal, and conditional matchings and delivers diverse SOTA cross-modality data-sampling capabilities.\nBelow we leverage our big cooperative learning to upgrade the standard GAN [12] into its big-learning variant, termed BigLearn-GAN, which can alternatively be interpreted as a novel adversarial pretraining method for foundation models. Specifically, given data x and the transformed continuous $X \\in \\mathbb{R}^{L \\times D}$, the BigLearn-GAN performs big cooperative learning in Definition 1 (see Eq. (6)) in an adversarial-learning fashion with the objective of\n$\\min_{\\theta} \\max_{\\Phi} \\mathbb{E}_{q(S,T)q(x_S)} [\\mathbb{E}_{q(X_T|X_S)}\\log\\sigma[f_{\\Phi}(X; S, T)] + \\mathbb{E}_{p_{\\theta}(X_T|X_S)}\\log\\sigma[-f_{\\Phi}(X; S,T)]]$,\n(8)\nwhere the universal $p_{\\theta}(X_T|X_S)$ models the generative processes of outputs $X_T$ conditioned on inputs $X_S$ for all (S, T) pairs (an example implementation based on the transformer architecture is given in"}]}, {"title": "4 Experiments", "content": "We design challenging simulations based on the tailored ones in Section 3.3 and conduct several high-dimensional experiments to further demonstrate the principle of big cooperative learning, which, in turn, also justifies the successes of foundation models from the perspective of learning.\nSpecifically, we first design a 25-GMM reverse-KL-minimization simulation, where severe mode collapse emerges from numerous local optima for conventional joint reverse-KL matching; however, our empirical findings demonstrate that big learning is capable of delivering remarkable exploration power. Next, we implement our BigLearn-GAN on the uni-modal MNIST and CelebA datasets and demonstrate the versatile realistic data-sampling capabilities of the big-learned universal model. Finally, we expose the potential of big learning in multi-modal applications, e.g., to unify classification and generation or to serve as a superior fine-tuning strategy.", "sections": [{"title": "4.1 Remarkable Exploration Power of Big Cooperative Learning", "content": "Based on Section 3.3, we conduct a more challenging reverse-KL-minimization simulation, where both q(x) and $p_{\\theta}(x)$ are set as a 25-GMM and $x \\in \\mathbb{R}^2$ (see Fig. 4 and Appendix C for details). The loss for conventional joint matching, i.e., $KL[p_{\\theta}(x)||q(x)]$, has numerous local optima (refer to Fig. 2), which are closely related to the notorious mode collapse problem of adversarial learning.\nTo demonstrate the remarkable exploration power of big learning, we select a challenging initialization, where all the 25 components of $p_{\\theta}(x)$ are initialized in the lower-left corner of Fig. 4a. As expected, conventional joint matching quickly gets stuck in a strong local optimum, as shown in Fig. 4b. In comparison, our big learning manages to deliver a remarkable exploration power (see Fig. 4c), which overlooks local optima and gradually seeks out most components in the mode-seeking reverse-KL-minimization territory. It's worth noting that more powerful exploration may be harvest from versatile matchings across diverse transformed domains, as discussed in Appendix C."}, {"title": "4.2 Versatile Realistic Data-Sampling Capabilities of the Universal BigLearn-GAN Generator", "content": "Next, we implement the BigLearn-GAN in Section 3.4 on the MNIST and CelebA datasets and demonstrate the big-learned versatile realistic data-sampling capabilities of the universal generator, providing empirical evidence that big cooperative learning is a new dimension for upgrading conven-tional machine learning paradigms. We employ X = x and the transformer architecture to design both the generator and discriminator in Eq. (8); further details are provided in Appendix D."}, {"title": "4.3 Revealing the Potential of Big Cooperative Learning in Multi-Modal Applications", "content": "Following Remark 5 of Definition 1, we additionally test the presented big learning in general multi-modal settings. Specifically, we consider 2 simplified situations with two modalities (i.e., $x = (x', y')$ contains paired tokens x' and a label y'), the first of which unifies classification and generation on MNIST, while the other one concerns using big learning as a fine-tuning strategy on the GLUE benchmark [44]. We use X = x; more details are given in Appendix E. After big cooperative learning, the universal $p_{\\theta}(x_T|x_S)$ is expected to simultaneously deliver versatile capabilities by specifying different (S, T)s, such as generation $p_{\\theta}(x')$, conditioned generation $p_{\\theta}(x'|y')$, classification $p_{\\theta}(y'|x')$, various completion $p_{\\theta}(x'|x_S)$, conditioned completion $p_{\\theta}(x'|x_S, y')$, etc.\nDue to space limitations, we move the results to Appendix E and present a brief summary here. Similar to the versatile data-sampling capabilities demonstrated in Section 4.2, the former MNIST experiment proves that big cooperative learning is also capable of providing versatile cross-modal capabilities with a universal model. The latter GLUE experiments reveal that big learning has the potential to serve as a superior fine-tuning strategy than the naive one."}]}, {"title": "5 Conclusions", "content": "Cooperation plays a key role in the success of foundation models. Based on this, big cooperative learning is proposed, which unifies the training of foundation models and delivers remarkable power of exploration. Tailored 2-D simulations and diverse experiments are conducted to demonstrate its principle, which in turn explicitly justifies the success of foundation models from the perspective of learning. Big learning is a new dimension for upgrading conventional machine learning paradigms."}, {"title": "A Big Cooperative Learning With Multi-Modal Data", "content": "For simplicity, we first consider multi-modal applications where all the modals of data $x = [x', y', z', \\dots]$ have the same data type (like images). In such situations, one may naively con-duct big cooperative learning in the x-space or its transformed X-space, as discussed in the main manuscript.\nHowever, for general multi-modal applications, each modal often has a different data type; for example, x' is a continuous image while y' represents discrete texts. In these situations, one may consider (i) employing modal-specific transformations to align all the modals in a latent space followed by big cooperative learning in that latent space mimicking [33, 35] or (ii) to implicitly"}, {"title": "B Details and Interesting Side-Products of Tailored 2-D Simulations", "content": "With a parameterized model $p_{\\theta}(x)$ to approximate the underlying (data) distribution $q(x)$, two popular learning paradigms are represented by the mode-covering forward KL minimization $\\min_{\\theta} KL[q(x)||p_{\\theta}(x)]$ (e.g., maximum log-likelihood learning) and the mode-seeking reverse KL minimization $\\min_{\\theta} KL[p_{\\theta}(x)||q(x)]$ (closely related to adversarial learning).\nAs discussed in the main manuscript, we demonstrate the principle of big cooperative learning via the mode-seeking reverse KL minimization in tailored situations with 2-GMMs. The simulation (see Section 3.3 of the main manuscript) is actually an optimization issue with 1-dimensional observation (i.e., only the first dimension of x is associated with the parameters $\\mu_1$ and $\\mu_2$ that constitute $\\theta$) put in a higher 2-dimensional observation space ($x \\in \\mathbb{R}^2$).\nAll the analyses are possible because of the simplicity of GMMs, i.e., (i) GMMs have analytical marginal, conditional, transformed marginal, and transformed conditional distributions, and (ii) the result of a GMM convolved with a Gaussian distribution (i.e., noising) is also analytically expressed. On the other hand, GMMs with sufficient components are also powerful enough to approximate any continuous distribution arbitrarily well [23, 30], justifying the representativeness of the tailored simulations in Sections 3.3 and 4.1 of the main manuscript."}, {"title": "C More Details of the 25-GMM Reverse-KL-Minimization Simulation", "content": "Similar to the tailored 2-D simulation of Section 3.3 of the main manuscript, we set both q(x) and $p_{\\theta}(x)$ as a GMM with 25 components (i.e., a 25-GMM) and concern minimizing the differences between them via reverse-KL minimization. Specifically,\n$q(x) = p_{\\theta^*}(x) = \\sum_{i=1}^{25} \\frac{1}{25} \\mathcal{N}(x | \\mu^*_i, \\Sigma^*)$ and $p_{\\theta}(x) = \\sum_{i=1}^{25} \\frac{1}{25} \\mathcal{N}(x | \\mu_i, \\Sigma_i)$,\n(10)\nwhere $\\mu^*$ is set as shown in Fig. 11, $\\Sigma^* = \\sigma^2 I$ with $\\sigma^2 = 0.05$, $\\Sigma_i = L_i L_i^T$ with trainable lower-triangular $L_i$, and $\\theta = [\\{\\mu_i\\}_{i=1}^{25}, \\{L_i\\}_{i=1}^{25}]^T$. $\\{\\mu_i\\}$s are randomly initialized with $\\mathcal{N}(-5, 0.01)$.\nFor training, we employ a SGD optimizer with a learning rate of 0.1; we use 100 samples for diverse reverse-KL minimization (e.g., 100 xs from $p_{\\theta}(x)$ for joint reverse-KL matching); we also adopt random orthogonal transformations (i.e., $y = Bx$ with B denoting an orthogonal transformation) for transformed marginal and conditional matchings.\nWe only employ diverse joint and transformed marginal matchings to conduct big cooperative learning, because we empirically find it sufficient. We separate our big cooperative learning into two phases. In the first burn-in phase, we employ transformed marginal matchings to maximize exploration, and after that, we use both joint and transformed marginal matchings in the second phase, with probabilities of [0.1, 0.9], respectively."}, {"title": "D BigLearn-GAN: Big Cooperative Learning Generative Adversarial Nets", "sections": [{"title": "D.1 Network Architectures of BigLearn-GAN", "content": "Below we discuss the employed network architectures for the BigLearn-GAN generator and discrimi-nator in Eq. (8) of the main manuscript.\n demonstrates the employed BigLearn-GAN generator and discriminator, both of which are constructed with Transformers to exploit their modeling capabilities and flexibilities.\n* BigLearn-GAN Generator. Following the MAE [13], we design the universal generator $p_{\\theta}(x_T|x_S)$ with an autoencoder-like architecture, which employs an encoding G-Encoder and a decoding G-Decoder, as shown in Fig. 12a. The G-Encoder encodes the source patches $x_S$ (if any) to their latent codes; then, these codes are combined with the mask tokens $[M]$, patch-wise noise embeddings, and new positional encodings to serve as the input of the G-Decoder; finally, the G-Decoder transforms its input to generate the target patches $x_T$.\n$[M]$ tokens are inserted later in a middle layer, because doing this often improves perfor-mance and lowers the computational burden [42, 13]. A noise z is mapped with an 8-layer MLP to produce the patch-wise noise embeddings $\\{n_1, \\dots, n_L\\}$. Note we also introduce another token $[M_i]$ to indicate no noise embeddings are necessary at the corresponding source locations in S.\n* BigLearn-GAN Discriminator. As shown in Fig. 12b, we also modify the Transformer architecture to construct the universal discriminator $\\sigma(f_{\\Phi}(x; S, T))$ that applies to all (S, T) cases. We employ an additional CLS token mimicking the BERT, whose output indicates whether the input patches are realistic or not (more specifically, whether they form a \u201creal\u201d data from $q(x|x_S)$ or a fake one from $p_{\\theta}(x_T|x_S)$, by referring to Eq. (8) of the main manuscript). The input of the discriminator consists of patch embeddings, positional embeddings, and two new special tokens $([M_S]$ and $[M_T])$ that indicate source or target patches mimicking the sentence tokens in the BERT."}, {"title": "D.2 Additional \u201cCommunications\u201d in Big Cooperative Learning", "content": "We notice that Eq. 8 of the main manuscript is equivalent to\n$\\min_{\\theta} \\max_{\\Phi} \\mathbb{E}_{(S,T)}[\\mathbb{E}_{q(X_{SUT})}\\log\\sigma[f_{\\Phi}(x; S, T)] + \\mathbb{E}_{p_{\\theta}(X_T|X_S)q(X_S)}\\log\\sigma[-f_{\\Phi}(x; S, T)]]$,\n(11)\nwhere the optimal $f_{\\Phi^*}(x; S, T) = \\log \\frac{q(X_{SUT})}{p_{\\theta}(X_T|X_S)q(X_S)}$. Accordingly, Eq. 8 and Eq. 11 ideally performs $\\min_{\\theta} \\max_{\\Phi} \\mathbb{E}_{(S,T)} JS[q(X_{SUT})||p_{\\theta}(X_T|X_S)q(X_S)]$, i.e., encouraging the joint matchings between $q(X_{SUT})$ and $p_{\\theta} (X_T|X_S)q(X_S)$ for different (S, T).\nNoticing that the universal $p_{\\theta} (X_T|X_S)$ possesses versatile data sampling capabilities, one may further expand the learning-task scope of big cooperative learning with \u201ccommunications\u201d (i.e., matchings) between two model distributions $p_{\\theta} (X_{T1}|X_{S1})q(X_{S1})$ and $p_{\\theta} (X_{T2}|X_{S2})q(X_{S2})$ with $S^1 \\cup T^1 = S^2 \\cup T^2$, because they share the same ultimate goal of matching $q(x|x_s)$.\nSpecifically, the additional matching tasks for big cooperative learning is\n$\\min_{\\theta} \\max_{\\Phi} \\mathbb{E}_{q(S^1, T^1)q(S^2, T^2)} \\Big[\\mathbb{E}_{p_{\\theta}(X_{T^1}|X_{S^1})q(X_{S^1})}\\log [f_{\\Phi}(x; S^2, T^2) - f_{\\Phi}(x; S^1, T^1)]+ \\mathbb{E}_{p_{\\theta}(X_{T^2}|X_{S^2})q(X_{S^2})}\\log [f_{\\Phi}(x; S^1, T^1) - f_{\\Phi}(x; S^2, T^2)]\\Big]$,\n(12)\nwhere the \"communication\u201d discriminator can be implicitly constructed with the same neural network $f_{\\Phi}(x; S, T)$ from Eq. (8), as proved below."}, {"title": "D.3 Training Settings of BigLearn-GAN", "content": "We employ the same network architectures in the previous section for both experiments on the MNIST and CelebA datasets, with the detailed hyperparameters summarized in Table 2. The AdamW optimizer [25] with \u03b2 = (0.1, 0.999) is employed, with constant learning rates used for updating both the generator and the discriminator. We follow Algorithm 1 for big cooperative learning. The experiments are conducted with a NVIDIA GeForce RTX 3090 GPU with a 24GB memory. An MNIST experiment takes half a day, while a CelebA one takes about 2 3 days. Code will be released upon publication."}, {"title": "D.4 Additional Experimental Results of BigLearn-GAN", "content": "More experimental results, complementing the limited demonstrations of the main manuscript, are given below. Please refer to the captions for details."}]}, {"title": "E Big Cooperative Learning in Multi-Modal Applications", "content": "The following experiments are conducted with a NVIDIA GeForce RTX 3090 GPU with a 24GB memory. The total running time is about several days.", "sections": [{"title": "E.1 Leveraging Big Learning to Unify Classification and Generation", "content": "We follow [5, 33] to vector-quantize an image into discrete tokens $x' \\in \\mathbb{Z}^{(L-1) \\times 1}$, which is then combined with the label $y' \\in \\{1, \\dots, C'\\}^{1 \\times 1}$ to constitute a multi-modal data $x \\in \\mathbb{Z}^{L \\times 1}$. We employ no transformations (i.e., X = x) and construct the universal Transformer-based $p_{\\theta} (x_T|x_S)$ with the two-stream attention [47] to model the diverse dependencies of x on $x_S$ for various (S, T)s. Finally, we implement big cooperative learning of Definition 1 in the maximum-likelihood-learning territory"}, {"title": "E.2 Using Big Cooperative Learning as a New Fine-Tuning Strategy", "content": "Instead of naively leveraging big cooperative learning to pretrain a foundation model from scratch (which is extremely expensive), we consider using it as a special fine-tuning strategy, to demonstrate its effectiveness in a lightweight manner.\nSpecifically, we design experiments based on the Hugging Face transformers library [46], the GLUE benchmark [44], and the XLNET [47] that outperforms the BERT on many NLP tasks. We employ the same pretrained xlnet-base-cased model and continually train it on the downstream RTE/MRPC/SST-2 classification tasks via (i) the naive fine-tuning (i.e., identical to the original XLNET [47], termed FT) and (ii) big cooperative learning (termed big-learn), respectively. As a multi-modal data $x \\in \\mathbb{Z}^{L \\times 1}$ here also consists of tokens $x' \\in \\mathbb{Z}^{(L-1) \\times 1}$ and a label $y' \\in \\{1, \\dots, C'\\}^{1 \\times 1}$, other settings are set similar to Section E.1.\nThe objectives for both FT and big-learn are detailed below.\n* FT. For fine-tuning on classification applications, often a cross-entropy loss is employed, which is identical to\n$L_{FT}(\\theta) = \\mathbb{E}_{q_{downstream}(x, y)} [-\\log p_{\\theta}(y|x)]$,\n(16)\nwhere $q_{downstream}(x, y)$ is the distribution of the data from the downstream classification task.\n* Big-learn. For direct comparisons, we formalize the big-learn objective as\n$L_{big-learn}(\\theta) = L_{FT}(\\theta) + \\beta_{BigLearn}\\mathcal{L}(\\theta)$,\n(17)\nwhere $\\beta_{BigLearn}$ is a hyperparameter and $\\mathcal{L}(\\theta)$ is the loss in Eq. (15). For other hyperparame-ters, we simply reuse those from Section E.1, without careful tuning."}]}, {"title": "F Limitations", "content": "The main assumptions of big cooperative learning are listed at the beginning of Section 3 of the main manuscript. However, those assumptions might be violated in practice, potentially resulting in unexpected behavior of the presented big cooperative learning.\n* We assume trustworthy data samples from q(x). Although this assumption is widely employed, it's often violated in practice, e.g., due to limited accessibility to i.i.d. samples, unintentional interventions in data collection and preprocessing, or even hostile attack. Careful data engineering should be able to significantly alleviate the violation.\n* We assume sufficient model capacity of $p_{\\theta} (X_T|X_S), \\forall (S,T)$. This is another common issue of deep-learning methods. Accordingly, we cannot justify this assumption in the experiments of Sections 4.2 and 4.3. The simulations of Sections 3.3 and 4.1 should ease the concern.\nIn addition to the above common limitations, the presented big cooperative learning, as a general concept widely applicable in diverse domains, are not comprehensively verified in"}]}