{"title": "How Well Can a Long Sequence Model Model Long Sequences? Comparing Architectural Inductive Biases on Long-Context Abilities", "authors": ["Jerry Huang"], "abstract": "Long sequences occur in abundance within real-world scenarios, hence properly modelling them opens numerous down-stream use-cases. Deep neural networks, however, have often struggled with these for a variety of reasons. Recent advances, both in system engineering as well as model design, have enabled the scaling up of model that are purported to support extended context length. In particular, the state-space and linear recurrent neural network families of models hypothetically can entend to infinite sequence lenth. However, is this too good to be true? We conduct an evaluation to show that while such claims may be sound theoretically, there remain large practical gaps that are empirically observed. In particular, recurrent models still suffer in the same settings as long-context LLMs with attention. We further show that different inductive biases have inconsistent extrapolation capabilities, highlighting the need to further study such paradigms and investigate why long-context models seemingly fail to behave as one might expect.", "sections": [{"title": "1 Introduction", "content": "Advances in AI system engineering (Dao et al., 2022; Dao, 2024; Rasley et al., 2020) and design of models (Katharopoulos et al., 2020; Jiang et al., 2023; AI21, 2024) have opened up language models to the broader public for a diverse set of purposes and use-cases. However, Transformer-based architechtures (Vaswani et al., 2017) remain bounded in terms of their context windows, as they require fixed-length positional embedding representations (Press et al., 2022; Su et al., 2023; Peng et al., 2024) which cannot be modified a posteriori. With this glaring limitation, linear sequence models (Gu et al., 2022; Gu and Dao, 2024; Orvieto et al., 2023; Qin et al., 2023; Peng et al., 2023; De et al., 2024; Dao and Gu, 2024) have emerged an alternative that present a seeming ability to extend to infinite-length contexts in theory while retaining all the original benefits of the Transformer related to training-based parallization.\nHowever, despite the temptation to assert linear sequence models as superior, properly testing for information retention from long-context tasks remains callenging. While some works have attempted to evaluate this ability through long contexts (Shaham et al., 2022; Pang et al., 2022; Dong et al., 2024; Bai et al., 2023; Li et al., 2023; Han et al., 2024), whether or not they truly require the use of long-contexts is uncertain and ascertaining long-context abilities from these tasks is difficult. This has prompted the use of more synthetic tasks (Hsieh et al., 2024), such as needle-in-a-haystack (NIAH) (Kamradt, 2023) and passkey retreival (Mohtashami and Jaggi, 2023), to better control and evaluate the context sizes of models. Nevertheless, an outstanding question remains whether or not long-context models can effectively model long contexts. While some works (Gu and Dao, 2024; Fu et al., 2023; Poli et al., 2023; Peng et al., 2024; Team, 2024) purport to be able to extrapolate towards sequences of long length (100k tokens+), further investigation has suggested differently. For example, Hsieh et al. (2024) claim modern LLMs significantly over-state true context windows on a number of synthetic tasks. Meanwhile Han et al. (2024) observe models to perform reasonably well on synthetic tasks, but struggle on real-world tasks, as do Li et al. (2023). Hence despite a consistent trend in models behaving underwhelmingly, it remains to be understood why this occurs. Yet one interesting question is whether or not linear sequence models are in fact more suited for these compared to Transformer-based ones, as has been claimed repeatedly.\nTo this end, we further analyze the behaviour of sequence models to observe how differently they behave compared to Transformer-based ones. We perform a more extensive study into each type of model, as well as a mixture of both, to better investigate how they perform in principle and how they change in behaviour when extending to longer and longer sequences. On both synthetic and realistic data, we conduct a thorough study and observe:\n\u2022 All models, whether they use pure sequence layers, attention or a mix, struggle with extrapolating beyond their training context length.\n\u2022 The abiliy to extrapolate can vary signficantly based on the format of the sequence even if the task remains constant.\nThese results highlight that long sequence models suffer from significant limitations despite their theoretical soundness, highlighting a need to better understand this striking dissonance between expectation and observation and how to amend it for better long-context understanding and reasoning."}, {"title": "2 Related Work", "content": "Efficient Long-Context Models. Due to the computational bottleneck of attention (Bahdanau et al., 2015) relative to sequence length, significant modifications have been made to overcome this limitation of the Transformer (Child et al., 2019; Katharopoulos et al., 2020; Su et al., 2023) yet they remain theoretically bounded in terms of its context length. Alternatively, sequence models (Rumelhart et al., 1986; Jordan, 1986; Hochreiter and Schmidhuber, 1997; Cho et al., 2014) originally faced significant issues that limited their application but recent modifications (Gu et al., 2020, 2021) have led to the prominence of linear sequence models which are significantly more compute-effective than Transformer-based architechtures.\nOn the Limits of Long Sequence Models. Due to their more intuitive and interpretable architechture, long/linear sequence models remain easier to analyze when placed in comparision to Transformers. As such, their limitations also become easier to discover and analyze. Vardasbi et al. (2023) first show that SSMs struggle at sequence-to-sequence tasks due to to the use of a fixed-size hidden representation which compresses the entire prior context, making it difficult to extract information from the past, fact further substantiated by Jelassi et al. (2024). Park et al. (2024) additionally demonstrate that these models have difficulty with more complex in-context learning tasks, while Merrill et al. (2024) show them to possess similar limiations in terms of representational power as Transformers (Merrill and Sabharwal, 2023). Waleffe et al. (2024) finally make a comparision between Mamba, Transformers as well as a hybrid and observe hybrid models to perform better on long-context tasks, while Mamba2 often trails behind Transformers. These observations thus beg a question: can long sequence models really model long sequences? Given the hints that long sequence models may not always be as they seem, a more formal investigation is necessary. We distinguish ourselves by conducting a more controlled but intricate study which aims to uncover why some of the prior results might occur, which we discuss in the work that follows."}, {"title": "3 Background", "content": "Attention and Long Sequences. Self-attention as used in Transformers is powerful but costly. When provided an embedded text representation as a sequence of tokens $x \\in \\mathbb{R}^{L\\times d}$, each Transformer layer in the network applies a function\n$T_{\\ell}(x) = \\text{FF}_{\\ell}(A_{\\ell}(x) + x) + A_{\\ell}(x)$ (1)\nwhere $A_{\\ell}$ is the self-attention mechanism of the $\\ell$-th layer and $\\text{FF}_{\\ell}$ is the following feed-forward network\u00b9. Self-attention computes, for every position, a weighted average of the feature representations of all other positions with a weight proportional to a similarity score between the representations.\n$Q_{\\ell} = xW_\\ell^Q$ $K_{\\ell} = xW_\\ell^K$ $V_{\\ell} = xW_\\ell^V$\n$A_{\\ell}(x) = V = \\text{softmax}(\\frac{Q_{\\ell}K_{\\ell}^T}{\\sqrt{d}}) V_\\ell$ (2)\nAs the softmax operation operates in $O(L^2)$ time when applied naively, this limits the ability to process long sequences.\nTransformers to Sequence Models. SSMs model a dynamical system, traditionally mapping a 1-D continuous input signal $x(t) \\in \\mathbb{R}$ to an $n$-dimensional hidden state $h(t) \\in \\mathbb{R}^n$ that is projected back to a 1-D output $y(t) \\in \\mathbb{R}$ using:\n$\\begin{cases}h'(t) = Ah(t) + Bx(t) \\\\y(t) = Ch(t) + Dx(t)\\end{cases}$ (3)\nwhere $A, B, C$ and $D$ are all trainable parameters. Gu et al. (2021) use this paradigm to define a recurrent model to work on discrete signals, in which case the input can be regarded as discretized data"}, {"title": "4 Experiments and Results", "content": "Datasets. We conduct an initial evaluation using RULER (Hsieh et al., 2024), a set of synthetic benchmarks that test long-context information retention, before conductin a more fine-grained evaluation on a general needle-in-the-haystack task. We use this benchmark as for more granular control over the exact information that must be retained. Results are measured in terms of accuracy based on exact matching of predicted tokens.\nBaselines. Our main objective is to compare how long-sequence models fare on long context tasks. To this end, we compare models with the same number of parameters that are evenly trained on the same data. Hence we first use Mamba2 (Dao and Gu, 2024) as well as a Transformer variant (Transformer++) as well as a hybrid Mamba2Attn, each with 2.7 billion parameters. We further add Sheared-LLaMA (Xia et al., 2024) and Recurrent-Gemma (Botev et al., 2024) baselines (with and without intruction-tuning) as same-sized baselines trained under different conditions. We finally add a 3 billion RWKV (Peng et al., 2023) variant as another sequence model baseline.\nResults. We present initial results on the base set of RULER tasks (as defined by its original authors) in However, we present two additional ablation studies. In the first, we use a single needle hidden within a large haystack, however we modify its relative position within the context. The goal of this ablation, presented in and is to observe how the use of a unified hidden state rather than attention can affect the ability to retain information throughout a long sequence. The second further test how this information retention may change when the content that is being memorized changes (ex. numbers versus UUIDs within a haystack of repeated sentences or essays)."}, {"title": "5 Discussion", "content": "All models have limits. Our first observation is that regardless of the model, performance drops steeply upon testing with sequences that are longer than what the model was initially trained on. This is made clear in where the decline in performance is greatest once the evaluated sequences are longer than the training context (with the mild exception of RWKV which demonstrates approximately linear degredation as the sequences progressively double in length). However, an important observation is that linear sequence models do appear to extrapolate slightly better than pure-attention models, whose performance drop to near 0 performance upon the increase, as these models do show non-trivial accuracy even when evaluated on the longer sequences. This distinction is less clear when comparing between pure linear sequence models and hybrid models which alternate between sequence-model layers and attention layers, as there is no explicit pattern as to when one class will perform better on one length or another.\nBeing lost in the middle is a common event. Being lost in the middle, whereby models have difficulty recalling relevant information positionally located in the middle of long contexts (Liu et al., 2024), has been observed as a common limitaiton among attention-based models. In this appears to be a common feature among all models we test, as all classes of models see increasing drops in performance as the information is more closely located at the center of the sequence. This suggests that despite their long-context modelling ability, recurrent models cannot effectively reason over their entire context window when prompted. However, when extending past the training context length there is less of a consistent pattern. In particular, while Mamba models still appear lost-in-the-middle, other recurrent models such as RecurrentGemma and RWKV have no clear depth-performance trends, further bringing into question their general long-context modelling abilities.\nExtrapolation can inconsistent. Furthermore, extrapolation can be inconsistent based on characteristics of the model as well as the data. In we can first note that depending on the data format of the haystack, key and value to be retrieved, the performance of each model can vary significantly even if we use the same task template, context length and needle position. Furthermore, extrapolation can vary signficantly based on the model as these characteristics change. For example, pure sequence layers (Mamba2) appears to only extrapolate when the haystack is a repeated sequence and the retrived value is a number related to a key word. Upon changing the haystack to be essays, extrapolation craters and the model fails. An equally trained hybrid model (M2A) can meanwhile always extrapolate to some degree, but performance on sequences up to the training context length appears to compare much worse. Pure attention (TPP) meanwhile performs favorably only when evaluating on the extact training context length under specific data formats, but otherwise underwhelms."}, {"title": "6 Conclusion", "content": "In this work, we conduct a comprehensive comparision between the long-sequence models and attention-based language models, showing that long-context abilities of such sequence models may hold from a theoretical perspective, they empirically still struggle in comparison to models that make no guarantees. This highlights the need to improve long sequence reasoning abilties not only for Transformer-based LLMs, but also SSMs and new classes of RNNs, which hopefully can serve as motivation to further analyze this topic."}, {"title": "7 Limitations", "content": "We limit ourself to a model size in which it is easy to compare models of various paradigms. As such, some perhaps more powerful models are not explored as the analysis between such models can become difficult due to multiple additional changing variables that can perhaps lead to incorrect or undersupported claims."}, {"title": "8 Ethical Concerns", "content": "This paper discusses how different types of language models behave on long-context data. It follows that mistakes in our methodology (both experimental and analytical) could lead to unsupported confidence or skepticism about LLMs. Though neither are unethical, unsupported confidence can be very dangerous. However, given that the overall claim is that LLMs should not be assumed to support context length that extend beyond what they have trained, regardless of their training data, we do not think this paper in itself could be misinterpreted for particularly dangerous outcomes.\nAs for model choices, we use publicly available models where the license agreements do not restrict what we can say about the model. This should give the reader confidence that our views are unbiased. This is unlike ChatGPT or GPT4, which include an unrestricted indemnity-clause in their license agreement, which could make us financially liable for damages."}]}