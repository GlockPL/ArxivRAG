{"title": "VFIMamba: Video Frame Interpolation with State Space Models", "authors": ["Guozhen Zhang", "Chunxu Liu", "Yutao Cui", "Xiaotong Zhao", "Kai Ma", "Limin Wang"], "abstract": "Inter-frame modeling is pivotal in generating intermediate frames for video frame interpolation (VFI). Current approaches predominantly rely on convolution or attention-based models, which often either lack sufficient receptive fields or entail significant computational overheads. Recently, Selective State Space Models (S6) have emerged, tailored specifically for long sequence modeling, offering both linear complexity and data-dependent modeling capabilities. In this paper, we propose VFIMamba, a novel frame interpolation method for efficient and dynamic inter-frame modeling by harnessing the S6 model. Our approach introduces the Mixed-SSM Block (MSB), which initially rearranges tokens from adjacent frames in an interleaved fashion and subsequently applies multi-directional S6 modeling. This design facilitates the efficient transmission of information across frames while upholding linear complexity. Furthermore, we introduce a novel curriculum learning strategy that progressively cultivates proficiency in modeling inter-frame dynamics across varying motion magnitudes, fully unleashing the potential of the S6 model. Experimental findings showcase that our method attains state-of-the-art performance across diverse benchmarks, particularly excelling in high-resolution scenarios. In particular, on the X-TEST dataset, VFIMamba demonstrates a noteworthy improvement of 0.80 dB for 4K frames and 0.96 dB for 2K frames.", "sections": [{"title": "1 Introduction", "content": "Video Frame Interpolation (VFI), a fundamental task in video data processing, is gaining substantial attention for its ability to generate intermediate frames between consecutive frames (Liu et al., 2017). Its utility spans many practical applications, including creating slow-motion videos through temporal upsampling (Jiang et al., 2018), enhancing video refresh rates (Reda et al., 2022), and generating novel views (Flynn et al., 2016; Szeliski, 1999). VFI workflows typically encompass two primary stages (Zhang et al., 2023): firstly, capturing the inter-frame dynamics of input consecutive frames; and secondly, leveraging this information to estimate inter-frame motion and generate intermediate frame appearance. In practice, VFI often deals with high-resolution inputs (e.g., 4K) (Sim et al., 2021), which results in significant object displacement and imposes high demands on the large receptive field of the modules that model information between frames. Additionally, since VFI is commonly applied to long-duration videos such as movies, model speed is also of paramount importance. Thus, striking a delicate balance between a sufficient receptive field and fast processing speed in modeling inter-frame information is the key aspect in crafting effective VFI models.\nCurrent methods for modeling inter-frame information predominantly rely on convolutional neural networks (CNNs) (Liu et al., 2017; Kong et al., 2022; Huang et al., 2022) and attention-based models (Lu et al., 2022; Zhang et al., 2023; Park et al., 2023; Liu et al., 2024a). However, as illustrated in the first three rows of Table 1, these methods either (1) lack flexibility and cannot adaptively model based on the input, (2) do not have sufficient receptive fields to capture inter-frame correlations at high resolutions, or (3) suffer from prohibitive computational complexity.\nOn the other hand, Natural Language Processing (NLP) has recently witnessed the emergence of structured state space models (SSMs) (Gu et al., 2021). Theoretically, SSMs combine the benefits of Recurrent Neural Networks (RNNs) and CNNs, leveraging the global receptive field characteristic of RNNs and the computational efficiency inherent in CNNs. One particularly notable SSM is the Selective State Space Model (S6), also known as Mamba (Gu & Dao, 2023), which has garnered significant attention within the vision community. Mamba's novel feature of making SSM parameters time-variant (i.e., data-dependent) enables it to effectively select relevant context within sequences, a crucial factor for enhancing model performance. However, to the best of our knowledge, S6 has not yet been applied to low-level video tasks.\nTo address the challenges faced by current VFI models and to explore the potential of the S6 model (Gu & Dao, 2023) in low-level video tasks, we propose VFIMamba, a novel frame interpolation method that adapts the S6 model for efficient and dynamic inter-frame modeling. As shown in Table 1, VFIMamba provides the advantages of a global receptive field with linear complexity while maintaining data-dependent adaptability.\nSpecifically, we introduce the Mixed-SSM Block (MSB) to replace existing modules for inter-frame information transfer. The original S6 model can only process a single sequence, so it is necessary to merge tokens from two frames into one sequence for effective inter-frame modeling. After thorough analysis and exploration, we figured out that interleaving tokens from both frames into a \u201csuper image\" is more suitable for VFI. We then conduct multi-directional SSMs on this image to model inter-frame information. This interleaved approach facilitates interactions between adjacent tokens from different frames during sequence modeling and ensures that the intermediate tokens of any pair of tokens in the sequence are from their spatiotemporal neighborhood. By stacking MSB modules, our model effectively handles complex inter-frame information exchange. Finally, we use the extracted inter-frame features to estimate motion and generate the appearance of intermediate frames.\nWhile the S6 model boasts the advantages listed in Table 1, it is crucial to employ appropriate training strategies to fully exploit its potential. Inspired by Bengio et al. (2009), we propose a novel curriculum learning strategy that progressively teaches the model to handle inter-frame modeling across varying motion amplitudes. Specifically, while maintaining training on Vimeo90K (Xue et al., 2019), we incrementally introduce large motion data from X-TRAIN (Sim et al., 2021), increasing"}, {"title": "2 Related work", "content": "2.1 Video frame interpolation\nThe performance of VFI methods has seen significant advancements with the emergence of deep learning models. (1) CNNs-based approaches (Bao et al., 2019; Liu et al., 2017; Huang et al., 2022; Niklaus & Liu, 2018; Choi et al., 2020; Zhu et al., 2024b; Jia et al., 2022; Niklaus et al., 2017; Kalluri et al., 2023): Initially, DVF (Liu et al., 2017) utilized a U-Net-like (Ronneberger et al., 2015) network to model two input frames and predicted the voxel flow for warping the two frames into the intermediate frame. Following this, CtxSyn (Niklaus & Liu, 2018) introduced ContextNet and RefineNet, where ContextNet extracts context information from each frame, and RefineNet further refines the coarse intermediate frame produced by warping. RIFE (Huang et al., 2022) proposed a novel, efficient framework that employs self-distillation to significantly reduce computational load and parameters while maintaining high performance. Due to its simplicity, many convolutional modeling works (Kong et al., 2022; Jia et al., 2022) have improved upon RIFE. (2) Attention-based approaches (Lu et al., 2022; Zhang et al., 2023; Park et al., 2023; Liu et al., 2024a): VFIFormer (Lu et al., 2022) was the first to use attention to model inter-frame information, replacing the encoder part of U-Net with Transformer blocks. After that, EMA-VFI (Zhang et al., 2023) uses Swin-based (Liu et al., 2021) local attention to simultaneously capture local appearance and motion information. AMT (Li et al., 2023) used a multi-scale cost-volume construction similar to RAFT (Teed & Deng, 2020) to further enhance motion modeling capabilities. BiFormer (Park et al., 2023) introduced quasi-global bilateral attention to further increase the receptive field for large motions. SGM-VFI (Liu et al., 2024a) introduced sparse global matching to model motion between frames. However, current models struggle to balance sufficient receptive fields with computational overhead. In contrast, our method introduces the first interpolation model based on State Space Models (SSMs) (Gu & Dao, 2023) and further pushes the performance boundaries of VFI tasks.\n2.2 State space models\nIn the field of NLP, SSMs (Gu et al., 2021; Smith et al., 2022; Mehta et al., 2022; Fu et al., 2022) have recently emerged as one of the most promising contenders to challenge the dominance of Transformers. The Structured State Space Sequence Model (S4) (Gu et al., 2021) was initially introduced for linear complexity modeling of long sequences. Subsequent works have improved its computational efficiency and model capacity. S5 (Smith et al., 2022) proposed a parallel scan and MIMO SSM, and GSS (Mehta et al., 2022) enhanced the model's capability by introducing"}, {"title": "3 Method", "content": "3.1 Preliminaries\nSSMs are mainly inspired by the continuous linear time-invariant (LTI) systems, which apply an implicit latent state $h(t) \\in \\mathbb{R}^N$ to map a 1-dimensional sequence or function $x(t) \\in \\mathbb{R} \\rightarrow y(t) \\in \\mathbb{R}$. Specifically, SSMs can be formulated as an ordinary differential equation (ODE):\n$$h'(t) = Ah(t) + Bx(t),$$\n$$(1)$$\n$$y(t) = Ch(t),$$\n$$(2)$$\nwhere contains evolution matrix $A \\in \\mathbb{R}^{N \\times N}$, projection parameters $B \\in \\mathbb{R}^{N \\times 1}$ and $C \\in \\mathbb{R}^{1 \\times N}$. However, it is hard to solve the above differential equation in deep learning settings and needs to be approximated through discretization. Recent SSMs (Gu et al., 2021) propose to introduce a timescale parameter $A$ to transform the $A, B$ to their discrete counterparts $\\overline{A}, B$, i.e.,\n$$h_t = \\overline{A}h_{t-1} + Bx_t,$$\n$$(3)$$\n$$y_t = Ch_t,$$\n$$(4)$$\n$$\\overline{A} = \\exp{(\u25b3 A)},$$\n$$(5)$$\n$$B = (\u25b3 A)^{-1} (\\exp{(\u25b3 A)} - I) \\cdot \u25b3 B.$$\n$$(6)$$\nThe above SSMs are performed for each channel separately and their parameters are data-independent, meaning that $A, B$ and $C$ are the same for any input in the same channel, limiting their flexibility in sequence modeling. Mamba (Gu & Dao, 2023) proposes the selective SSMs (S6), which dynamically generate the parameters for each input data $x_t \\in \\mathbb{R}^L$ using the entire $x_t$:\n$$B_i = S_B x_i,   C_i = S_C x_i,   \u25b3_i = Softplus(S_\u25b3 x_i),$$\n$$(7)$$\nwhere $S_B \\in \\mathbb{R}^{N \\times L}, S_C \\in \\mathbb{R}^{N \\times L}, S_\u25b3 \\in \\mathbb{R}^{L \\times L}$ are linear projection layers. The $B_i$ and $C_i$ are shared for all channels of $x_i$, $A_i$ contains $A$ of $L$ channels, and $A$ are the same as previous SSMs. By the discretization in equations 5 and 6, $A$ and $B$ become different based on input data.\n3.2 Overall pipeline\nGiven two consecutive frames $I_0, I_1 \\in \\mathbb{R}^{H \\times W \\times 3}$ along with a timestep $t$, the objective of the frame interpolation task is to generate the intermediate frame $I_t \\in \\mathbb{R}^{H \\times W \\times 3}$. As illustrated in Figure 2, the overall pipeline of VFIMamba consists of three main components: frame feature extraction, inter-frame modeling, and frame generation. Firstly, we employ a set of lightweight convolutional layers to independently extract shallow features from each frame, progressively reducing the resolution to facilitate more efficient inter-frame modeling. This process can be formulated as:\n$$F_i = L(I_i),$$\n$$(8)$$\nwhere $L$ represents the set of convolutional layers, and $F_i$ denotes the extracted low-level feature for $I_i$. Next, we perform multi-resolution inter-frame modeling using the proposed Mixed-SSM Block (MSB). Each scale comprises N MSBs, and downsampling between scales is achieved through overlapping patch embedding (Wang et al., 2022). We define the resulting inter-frame features as $F_{SSM}$. Finally, we utilize these high-quality inter-frame features for frame generation, which involves motion estimation between two frames and appearance refinement:\n$$I_t = G(F_{SSM}, F_{SSM}),$$\n$$(9)$$\nwhere $G$ denotes the frame generation network. Since this work primarily focuses on exploring the use of SSMs for inter-frame modeling, we largely follow the design from Zhang et al. (2023) and Huang et al. (2022) for the frame generation components, with detailed specifications provided in the appendix.\n3.3 State space models for inter-frame modeling\nEffective inter-frame modeling is crucial for frame interpolation tasks (Zhang et al., 2023). Methods such as RIFE (Huang et al., 2022) and EMA-VFI (Zhang et al., 2023) employ simple convolution layers or local attention for inter-frame modeling, achieving high inference speeds but limiting receptive field. Conversely, SGM-VFI (Liu et al., 2024a) uses global inter-frame attention for motion estimation, which improves performance but sacrifices efficiency. In this work, we propose to use state space models (SSMs), specifically S6 (Gu & Dao, 2023), to achieve both efficiency and effectiveness in inter-frame modeling.\n3.3.1 Mixed-SSM block\nAs illustrated in Figure 2, we introduce the Mixed-SSM Block (MSB) for inter-frame modeling. The overall design of the MSB is analogous to Transformers (Vaswani et al., 2017), but with two pivotal distinctions: (1) We substitute the attention mechanism with an enhanced S6 Block (Gu & Dao, 2023), which facilitates global inter-frame modeling with linear complexity. (2) Drawing inspiration from Guo et al. (2024) and Behrouz et al. (2024), which identified the lack of locality and inter-channel interaction in SSMs, we replace the multilayer perceptron (MLP) with a Channel-Attention Block (CAB) (Hu et al., 2018).\nThe original S6 Block is limited to processing one-dimensional sequences, necessitating a strategy for scanning the feature maps of two input frames for inter-frame modeling. As depicted in Figure 3, there are two primary methods to rearrange the two frames: sequential rearrangement, where the frames are concatenated into a single super image, and interleaved rearrangement, where the tokens of the two frames are interleaved to form a super image. Regardless of the rearrangement method, following Liu et al. (2024b), the super image can be scanned in four directions: horizontally, vertically, and in their respective reverse directions. The S6 Block is then employed to model each direction independently, and the resulting sequences are rearranged and merged back.\n3.3.2 Discussion on how to rearrange\nHere, we discuss which rearrangement method is better for inter-frame modeling in the context of frame interpolation. First, let us introduce a conclusion from (Ali et al., 2024): the S6 layers can be"}, {"title": "3.4 Curriculum learning for VFIMamba", "content": "Despite the advantageous characteristics of the S6 model, such as data dependence and global receptive field, it is crucial to fully exploit its potential through appropriate training strategies. Currently, two main training strategies are employed for frame interpolation: (1) Vimeo90K Only: Most methods training models exclusively on the Vimeo90K (Xue et al., 2019). Although Vimeo90K offers a rich variety of video content, as analyzed by Liu et al. (2024a) and Kiefhaber et al. (2024), its motions have limited magnitude. This restriction hampers the model's performance on inputs with large motions or high resolution. (2) Sequential Learning: To mitigate the limitations of training solely on Vimeo90K, some methods (Liu et al., 2024a; Park et al., 2023) further train the model on X-TRAIN (Sim et al., 2021), a dataset characterized by large motions and high-resolution content, after initial training on Vimeo90K. While this approach successfully enhances the model's"}, {"title": "4 Experiments", "content": "We provide two models: a lightweight model, VFIMamba-S, and a high-performance model, VFI-Mamba. Both models have N = 3; the only difference is that VFIMamba has twice the number of channels as VFIMamba-S. As described in Section 3.4, we employ a curriculum learning strategy in which T = 50 and train for 300 epochs in total. More model configurations and training details are provided in the appendix.\n4.1 Comparison with the State-of-the-Art Methods\nQuantitative comparison. To validate the versatility of our proposed VFIMamba, we evaluated its performance (PSNR/SSIM) (Wang et al., 2004) across a variety of well-known benchmarks with"}, {"title": "4.2 Ablation Study", "content": "In this section, we conduct ablation studies using the VFIMamba-S model for efficiency.\nEffect of the S6 for VFI. As a core contribution of this work, the S6 model balances computational efficiency and high performance for inter-frame modeling. To validate its effectiveness, as shown in Table 4, we experimented by removing the SSM model from the MSB (w/o SSM), replacing the MSB"}, {"title": "5 Conclusion", "content": "In this paper, we have introduced VFIMamba, the first approach to adapt the SSM model to the video frame interpolation task. We devise the Mixed-SSM Block (MSB) for efficient inter-frame modeling using S6. We also explore various rearrangement methods to convert two frames into a sequence, discovering that interleaved rearrangement is more suitable for VFI tasks. Additionally, we propose a curriculum learning strategy to further leverage the potential of the S6 model. Experimental results demonstrate that VFIMamba achieves the state-of-the-art performance across various datasets, in particular highlighting the potential of the SSM model for VFI tasks with high resolution."}, {"title": "A Appendix", "content": "A.1 Broader impact\nIn this work, we introduce VFIMamba, the first video frame interpolation model based on SSMs. Video frame interpolation has wide-ranging applications in real-world video data processing, such as increasing the frame rate of AI-generated videos and generating slow-motion videos. Enhancing performance in various scenarios is crucial. However, as a research-oriented work, we trained our model on a very limited set of datasets (Vimeo90K (Xue et al., 2019) and X-TRAIN (Sim et al., 2021)), which might result in some degree of overfitting. Consequently, there could be significant artifacts when applied in real-world usage. This issue can be mitigated by training on a more diverse and extensive set of datasets.\nA.2 Limitations and future work\nAs the first work to explore the application of SSM models in frame interpolation tasks, we have achieved high performance, but there are still some limitations. First, although our method is much faster than attention-based methods, it still falls short of real-time requirements. Future work on designing a more efficient SSMs would be highly valuable. Second, in this work, we primarily focused on the role of SSM in inter-frame modeling and did not explore its use in the frame generation module. In the future, directly using SSM for generating intermediate frames could also be a promising direction for exploration.\nA.3 Visualizations on effective receptive field\nTo further evaluate the effective receptive field (ERF) of the S6 model in comparison with other efficient models (CNN, Local Attention) for inter-frame modeling, we used the method described by Luo et al. (2016). Given a specific region in $I_0$, we visualized the corresponding receptive fields in $I_1$ for different methods.\nAs shown in Figure 6, when the motion between $I_0$ and $I_1$ is significant, neither convolution nor local attention can focus on the corresponding region in $I_1$ before or after training. In contrast, the S6 model exhibits a larger global receptive field even before training, with notable concentration in both horizontal and vertical directions. We attribute this to the sequence rearrangement, where tokens closer together tend to have higher weights, a phenomenon also observed in VMamba (Liu et al., 2024b).\nAfter training, the S6 model's focus becomes more concentrated on the horizontal region of $I_1$, aligning better with the specified region in $I_0$. This indicates that the S6 model can better capture dynamics even with significant motion between frames.\nA.4 Generalization of curriculum learning\nTo validate the generalization capability of curriculum learning, we also trained the RIFE (Huang et al., 2022) and EMA-VFI (Zhang et al., 2023) from scratch using curriculum learning. As shown in Table 6, after training, all models maintained their performance on the low-resolution dataset Vimeo90K while significantly improving performance on the X-TEST and SNU-FILM, fully verified the generalization of curriculum learning. Among these, our VFIMamba achieved the most significant improvement and the highest performance ceiling, further demonstrating the potential of the S6 model.\nA.5 More qualitative comparison\nAs shown in Figure 7, we provide more visualization comparisons. VFIMamba demonstrates better visual quality compared to other methods.\nA.6 Model details\nA.6.1 Frame feature extraction\nAs shown in Figure 8, our frame feature extraction consists of multiple convolutional layers and PRELU (He et al., 2015). The first convolution maps the image from 3 channels to C, with C = 16 for VFIMamba-S and C = 32 for VFIMamba. Each time patch embedding is applied, the image resolution is halved, and the number of channels is doubled. Finally, we obtain the shallow features $F_i$ for each frame.\nA.6.2 Frame generation\nAs depicted in Figure 9, our frame generation includes an iterative intermediate flow estimation, local flow refinement, and appearance refinement using RefinNet. First, the intermediate flow estimation module uses the features $F_{SSM}$ obtained from inter-frame modeling with MSB for rough flow estimation. Specifically, we follow the design of EMA-VFI (Zhang et al., 2023), first utilizing features $F_{SSM}$ from the \u00d7 scale and the original image $I_i$ for predicting the flow $f$ and occlusion mask M by several convolutional layers. Then, we iteratively estimate the flow residual $\u0394f$ and mask residual $\u0394M$ using the $F_{SSM}$ from the \u00d7 scale. After that, inspired by Jia et al. (2022), which recognizes that the flow obtained through global inter-frame modeling may be coarse for high-resolution or large-motion scenes, we also introduce the IFBlock (Huang et al., 2022) to further enhance flow accuracy in local details. We then use the predicted motion to backward warp (Huang et al., 2022) the input frames to get the coarse intermediate frame $\u012a_t$. Finally, we adopt a U-Net-like (Ronneberger et al., 2015) structure to predict the appearance residual using shallow features $F_i$ and inter-frame features $F_{SSM}$, resulting in the final frame $I_t$.\nA.7 Training details\nTraining loss We used the same training loss as Zhang et al. (2023), which is a weighted combina- tion of Laplacian loss (Niklaus & Liu, 2020) and warp loss (Liu et al., 2019), with weights of 1 and 0.5, respectively.\nTraining setting We used curriculum learning to train our model. For the data from Vimeo90K (Xue et al., 2019), we randomly cropped the frames from 256 \u00d7 448 to 256 \u00d7 256. For the data from X-TRAIN (Sim et al., 2021), since each sample contains 64 consecutive frames, we first randomly select two frames, starting with an interval of 1, which doubles every 50 epochs. Then, we randomly resized the frames from 512 \u00d7 512 to S \u00d7 S, where S is initially 256 and increased by a factor of"}, {"title": "A.8 Evaluation protocols", "content": "In our paper, we primarily evaluated our methods on six benchmarks in terms of PSNR/SSIM(Wang et al., 2004): Vimeo90K (Xue et al., 2019), UCF101 Soomro et al. (2012), SNU-FILM (Choi et al., 2020), Xiph (Montgomery, 1994), X-TEST (Sim et al., 2021), and X-TEST-L (Liu et al., 2024a).\nWe followed the test procedures of Huang et al. (2022) for Vimeo90K and UCF101, Kong et al. (2022) for SNU-FILM, Niklaus & Liu (2020) for Xiph, Reda et al. (2022) for X-TEST with iterative 8\u00d7 frame interpolation, and Liu et al. (2024a) for X-TEST-L with largest interval interpolation.\nA.9 License of datasets and pre-trained models\nAll the dataset we used in the paper are commonly used datasets for academic purpose. All the licenses of the used benchmark, codes, and pretrained models are listed in Table 7."}]}