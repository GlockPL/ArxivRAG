{"title": "Scaled and Inter-token Relation Enhanced Transformer for Sample-restricted Residential NILM", "authors": ["Minhajur Rahman", "Yasir Arafat"], "abstract": "Recent advancements in transformer models have yielded impressive results in Non-Intrusive Load Monitoring (NILM). However, effectively training a transformer on small-scale datasets remains a challenge. This paper addresses this issue by enhancing the attention mechanism of the original transformer to improve performance. We propose two novel mechanisms: the inter-token relation enhancement mechanism and the dynamic temperature tuning mechanism. The first mechanism reduces the prioritization of intra-token relationships in the token similarity matrix during training, thereby increasing inter-token focus. The second mechanism introduces a learnable temperature tuning for the token similarity matrix, mitigating the over-smoothing problem associated with fixed temperature values. Both mechanisms are supported by rigorous mathematical foundations. We evaluate our approach using the REDD residential NILM dataset, a relatively small-scale dataset and demonstrate that our methodology significantly enhances the performance of the original transformer model across multiple appliance types.", "sections": [{"title": "I. INTRODUCTION", "content": "Energy efficiency is a growing challenge due to increasing energy demands, making effective power management critical [1]. Smart grids (SGs) and smart meters (SMs) provide real-time data on energy usage, enabling utilities and consumers to implement advanced demand-side management (DSM) strategies [2]. NILM is crucial in home energy management systems (HEMS), disaggregating household energy use into individual appliance data. This granular insight allows consumers to optimize their energy consumption, reduce wastage, and shift loads to reduce peak demands [3]. Additionally, NILM aids in load forecasting and identifying appliance malfunctions, contributing to an efficient and automated HEMS [4], [5].\nNILM is framed as a single-channel blind source separation (BSS) problem, traditionally tackled using statistical models like hidden markov models (HMMs) and conditional random fields (CRFs) [6], [7]. These models identify appliance states but struggle with scalability and generalizing to unseen devices. Recent advancements in deep learning have improved NILM through sequence-to-sequence (seq2seq) models, enabling better accuracy and efficiency with architectures like convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short-term memorys (LSTMs), and transformers [8], [9]. These methods address computational limitations of traditional approaches and improve performance in detecting appliances not included in training datasets, pushing forward progress in energy management systems [9].\nIn the past several years, transformer [10] has received significant attention due to its excellent performance over CNNs in many applications such as natural language processing (NLP), computer vision (CV), and signal processing. The core idea behind the transformer is self-attention which empowers its effective feature learning [10]. Despite its success, transformers do not use local inductive biases such as convolutions, therefore, they need to be trained with large datasets to improve the lack of locality information in the learned representation. However, training with large datasets requires higher computational resources and more training time. The pioneering work of Yue et al. first introduced transformers for NILM [9]. However, it is not always possible to have large datasets for training transformers due to privacy and regulatory concerns [11]. For this reason, there is a need to develop approaches for obtaining good performance of transformers with small-scale datasets.\nThere have been a few notable attempts to make transformer training with smaller datasets effective. For example, hierarchical structure-based transformer architecture design with multi-scale receptive field and modified self-attention mechanism. Although these designs effectively enforce locality-preserving inductive bias into the learned representation which leads to improved performance, they still need medium-scale datasets for training. Moreover, those architecture designs are developed for visual and language data such as images and text which are structurally different from NILM data, therefore, they require architectural adjustments which may be a challenging task considering the innovation required."}, {"title": "A. Related Works", "content": "Classical machine learning approaches to NILM, initially relied on statistical models like Hidden Markov Models (HMMs) to disaggregate household energy consumption into individual appliances [6]. Zia et al. used HMMs to identify appliances based on unique power patterns, while Kim et al. enhanced this by incorporating appliance usage timing for better accuracy [6], [12]. Further improvements by Kolter and Jaakkola addressed inference challenges in HMMs [13]. Conditional Random Fields (CRFs) were later introduced to overcome label bias in HMMs by considering all observations together, improving accuracy [7]. However, these models are computationally expensive and slow, making them less practical for real-world applications.\nDeep learning methods have become popular in NILM due to their ability to capture complex patterns more effectively than statistical models. Kelly and Knottenbelt were early adopters of neural networks, applying CNNs, LSTMs, and autoencoders to NILM, achieving better results than classical methods [8]. CNNs have shown particular success in detecting key features in power consumption data [14], but they struggle to capture long-range dependencies. Attention mechanisms, particularly transformer models, have proven more effective for NILM tasks by capturing both local and global patterns in energy usage. Notable transformer-based approaches include Yue's BERT4NILM [9], Sykiotis's ELECTRIcity-NILM [15], and Wang's Midformer [16], which leverage the self-attention mechanism to improve performance on time-series power data. These models offer a promising direction for NILM due to their ability to handle long-range temporal dependencies efficiently."}, {"title": "B. Our Contribution", "content": "In this paper, we experimentally show that the original formulation of the attention mechanism which is the core of transformer architecture produces poor attention results when small-scale datasets are used. Specifically, we show that when many tokens are used, the distribution of attention scores becomes over-smoothed. As a result, the self-attention mechanism can not attend to import tokens in a local manner. To improve this condition, we propose two simple yet effective mechanisms. Our proposed mechanisms effectively improve the distribution of the attention scores computed by the self-attention mechanism with the removal of self-tokens and dynamic temperature tuning. They enforce locality in attention by improving the focus between different tokens.\nWe conduct comprehensive experiments of our method with the reference energy disaggregation dataset (REDD) [17]. The experimental results show that our proposed method can obtain better results than the original attention based transformer. Specifically, our method performs better than the original transformer method and other state-of-the-art methods across four types of residential appliances."}, {"title": "II. PROPOSED METHOD", "content": "The original formulation of attention in transformer [10] is formulated as follows. Suppose, we have a set of tokens $X \\in \\mathbb{R}^{n\\times d}$, where d is the dimension of token embedding and n is the number of tokens (for simplicity of discussion, we are omitting the description of how these tokens are computed in this section; however, it is discussed in the following section). The attention is computed by applying three projections on X which produces a query matrix $Q \\in \\mathbb{R}^{n\\times d_k}$, key matrix $K \\in \\mathbb{R}^{n\\times d_k}$ and value matrix $V \\in \\mathbb{R}^{n\\times d_v}$. Formally, these projections are obtained as follows:\n$Q = XW_Q, K=XW_K, V=XW_V$,\nwhere, where $W_Q \\in \\mathbb{R}^{d\\times d_k}$, $W_K \\in \\mathbb{R}^{d\\times d_k}$ and $W_v \\in \\mathbb{R}^{d\\times d_v}$ are learnable weight matrices without any constraints. The attention is computed via a similarity matrix $S \\in \\mathbb{R}^{n\\times n}$ between Q and K, defined as:\n$S_{ij} = Q_iK_j$.\n$S_{ij}$ is similarity between the i-th query and the j-th key, capturing the semantic relationship between these tokens. S further goes through a normalisation with $softmax(\\cdot)$ operator and scaled by a factor of $\\frac{1}{\\sqrt{d_k}}$. Formally, the attention A for the i-th token is computed as follows.\n$A_i = \\sum_{j=1}^n softmax(\\frac{S_{ij}}{\\sqrt{d_k}})V_j$\nwhere $A_i \\in \\mathbb{R}^{d_v}$ is the attention matrix for the i-th token, and the $softmax(\\cdot)$ operator is applied row-wise to the scaled similarity matrix S. Note that A reflects self-attention scores.\nIn Eq. (3), the scaling factor $\\frac{1}{\\sqrt{d_k}}$ plays a key role during transformer training for stabilizing the gradient since the dot product between query and key in Eq. (2) tends to increase excessively. A smaller-valued $d_k$ effectively prevents that. However, this scaling can also lead to undesirable effects in practice. Based on the literature, we find that the entries of the attention matrix A tend to become similar to each other as training progresses, regardless of the underlying token relationships. This phenomenon can undermine the attention mechanism's ability to discriminate between important and unimportant token relationships.\nWe identify two potential causes for this issue which are given below in detail:\nSince both $Q_i$ and $K_i$ are obtained via linear projections to the same input $X_i$, the resulting token embedding naturally exhibit high similarity. Consequently, the similarity matrix $S_{ij} = Q_iK_j$ tends to have larger diagonal entries compared to off-diagonal entries. This can be mathematically expressed as:\n$S_{ii} = Q_iK_i = (X_iW_Q)(X_iW_K), S_{ij} = Q_iK_j$,\nwhere $S_{ii}$ is typically larger than $S_{ij}$ for $i \\neq j$ due to the shared linear transformation over $X_i$. As a result, the $softmax(\\cdot)$ operator disproportionately favors the diagonal entries, i.e., $softmax(S_{ii}) \\gg softmax(S_{ij})$ for $i \\neq j$. This results in attention being focused primarily on the individual token itself, rather than on meaningful interactions between tokens, thus resulting in degraded model performance.\nThe scaling factor $\\frac{1}{\\sqrt{d_k}}$ in self-attention acts similarly to a temperature parameter in softmax distributions."}, {"title": "B. Proposed Solution", "content": "Our method addresses the above two issues with the following solutions. Below we provide a formal and comprehensive explanation of each, underpinned by theoretical foundations.\nThe self-attention mechanism inherently prioritizes intra-token relationships due to the significant influence of the diagonal entries in the similarity matrix S. This emphasis on self-similarity hinders the model's ability to attend to meaningful inter-token relationships that are crucial for capturing long-range dependencies and contextual information essential for NILM. The proposed inter-token relation enhancement mechanism addresses this limitation by eliminating the diagonal entries of the similarity matrix.\nLet $S \\in \\mathbb{R}^{n\\times n}$ be the token similarity matrix, where each element $S_{ij}$ represents the similarity between token i and token j. The diagonal entries $S_{ii}$ represent the intra-token relationships, while the off-diagonal entries $S_{ij}$ for $i \\neq j$ represent the inter-token relationships. In standard self-attention, the diagonal entries $S_{ii}$, which capture self-similarity, typically dominate the off-diagonal inter-token relations. This dominance leads to an overemphasis on the intra-token attention score.\nLet $A \\in \\mathbb{R}^{n\\times n}$ be the attention matrix computed from the softmax-normalized similarity matrix S. If $S_{ii}$ has large positive values, the $softmax(\\cdot)$ operator $softmax(S/\\tau)$ will allocate a disproportionately large fraction of attention to the diagonal entries, reducing the model's capacity to attend to other tokens. Specifically,\n$lim_{S_{ii}\\rightarrow+\\infty} A_{ii} \\rightarrow 1, \\forall i$.\nTo counter this effect, we propose to nullify the diagonal entries, thereby forcing the model to focus on inter-token relationships. We enhance inter-token relationships by replacing the diagonal entries $S_{ii}$ with $-\\infty$, effectively removing the intra-token self-similarity from consideration during attention computation: $S_{ii} \\leftarrow -\\infty, \\forall i = 1,2,...,n$. This ensures that the attention scores allocated to intra-token relationships approach zero. The resulting attention matrix, A, is then computed as\n$A_{ij} = softmax(\\frac{S_{ij}}{\\sqrt{d_k}})$ for $i\\neq j$, $A_{ii} \\approx 0$.\nBy removing the diagonal entries from the similarity matrix S, we ensure that the $softmax$ operator assigns minimal or zero attention to intra-token relationships. As a result, the total attention is distributed entirely across inter-token relationships, i.e.,\n$\\sum_{j\\neq i} A_{ij} = 1$ and $A_{ii} \\approx 0$.\nThis mechanism improves the model's ability to capture and emphasize the relationships between distinct tokens, leading to enhanced modeling of global dependencies across tokens.\nIn the original self-attention formulation, the similarity scores in S are scaled by a fixed temperature $\\sqrt{d_k}$, where $d_k$ is the dimensionality of the key vectors. While this scaling helps prevent excessively large values from dominating the softmax operator, it lacks flexibility. The optimal temperature may vary across different datasets, appliance types, and training phases, yet the use of a fixed temperature restricts the model's capacity to adapt to these variations. Therefore, we propose a learnable temperature parameter, $\\tau$, which is dynamically adjusted during training through a meta-network.\n The temperature-scaled softmax function with temperature $\\tau$ applied to the similarity matrix S is defined as: $A_{ij} = \\frac{exp(\\frac{S_{ij}}{\\tau})}{\\sum_{k=1}^{n}exp(\\frac{S_{ik}}{\\tau})}$"}, {"title": "Remark 2. Dynamic Temperature Adaptation", "content": "By learning $\\tau$ via the meta-network, the model can dynamically adjust the sharpness of its attention distributions during training. In the early stages of training, when global context is more important, the meta-network can assign a higher value of $\\tau$, leading to smoother, more distributed attention. In later stages, as the model focuses on more specific token interactions, $\\tau$ can decrease, sharpening the attention focus.\nThis adaptability improves the model's performance across various datasets and tasks, avoiding over-smoothing or overly peaked distributions."}, {"title": "III. EXPERIMENTAL SETTINGS AND RESULTS", "content": "We use the REDD dataset [17], which includes power consumption data from six houses (i.e., residential) in USA. The dataset contains time-series data for both whole-home and appliance-specific channels, recorded at high (15 kHz) and low (1 Hz) frequencies. For evaluation, we focus on low-frequency recordings and follow standard data processing protocols from previous studies [8], [9], [15]. To evaluate model generalization, data from houses 2 to 6 is used for training, and house 1 for testing. We select common appliances across all households, excluding devices with limited presence or faulty data. The four appliances used are the fridge, washer, microwave, and dishwasher."}, {"title": "B. Network settings (architecture) and implementation", "content": "We train separate models for each appliance using a transformer network consisting of an embedding block, a transformer encoder of multiple layers, and a reconstruction block. The embedding block has a convolution layer (kernel size=5, padding=2), and the reconstruction block uses a deconvolution layer (kernel size=4, stride=2, padding=1). We use similar losses and their combinations as [9]. The transformer network is implemented in PyTorch. Models were trained for 100 epochs with the AdamW optimizer and BERT-style losses [9] on a P100 GPU from a cloud HPC. Following [18], we use 2 transformer layers/heads, a hidden dimension of 16, dropout ratio of 0.5, and masking ratio of 0.3 for optimal performance."}, {"title": "C. Evaluation metrics", "content": "We used four common metrics for evaluation: Accuracy (Acc.), F1 score, Mean Relative Error (MRE), and Mean Absolute Error (MAE) [8], [9], [15]. 1) Acc. Accuracy is the ratio of true positives (TP) and true negatives (TN) over the total number of predictions, Acc. = $\\frac{TP + TN}{TP + TN+FP + FN}$, where FP and FN are false positives and false negatives. 2) F1 score: F1 evaluates the model's performance with imbalanced classes, F1 = $\\frac{TP}{TP+(FP + FN)}$. 3) MRE: MRE measures the accuracy of appliance energy estimates, MRE = $\\frac{1}{N}\\sum_{i=1}^{N}|\\frac{Y_i-Y_i}{max(Y_i)}|$. 4) MAE: MAE calculates the average prediction error, MAE = $\\frac{1}{N}\\sum_{i=1}^{N}|Yi - \\hat{Y_i}|$"}, {"title": "D. Experimental Results", "content": "Table I shows the experimental results of our proposed method. We compare our results with 16 recent SOTA (state-of-the-art) methods under four evaluation metrics. Specifically, we compare three groups of methods. The first group uses standard deep networks with relatively simpler training mechanisms and the methods in this group are GRU + [9], LSTM + [9], CNN [9], BERT4NILM [9], Seq2Point [20], Seq2Seq [19], RNN [19] and Compact Transformer [18]. The second group uses advanced training mechanisms without significant changes in network architecture and the methods in this group are TransformNILM [22], ELECTRIcity [15] and CTA-BERT [21]. The third group uses advanced training mechanisms with significant changes in network architecture and the methods in this group are ELTransformer [20], Switch T/F [19], WindowGRU [19], SGNet [20] and LA-InFocus [23].\nIt can be seen that our proposed method surpassed the performance of many methods of all three groups across all metrics and appliance types. Several second and third-group methods show strong results indicating the strength of using advanced training and network architectures. In comparison to these methods, our method is trained with a simple training mechanism and still uses a very similar architecture to the original transformer model [9]. Instead of being competitive with them, we think that incorporating our proposed attention improvement mechanisms into their pipeline could further improve their performance since those mechanisms are model and training-agnostic from the point of principle."}, {"title": "E. Ablation study", "content": "We conducted an ablation study of our proposed method to assess the effectiveness of inter-token enhancement and dynamic temperature tuning mechanisms. Table II presents the findings across four appliances from the REDD dataset."}, {"title": "Study on inter-token relation enhancement mechanism", "content": "The results demonstrate that the inter-token relation enhancement mechanism significantly improves performance, outperforming the baseline method and many SOTA approaches."}, {"title": "Study on dynamic temperature tuning mechanism", "content": "We experimented with several pre-selected $\\tau$ values, i.e., $\\sqrt{d_k}/8$, $\\sqrt{d_k}/4$, $\\sqrt{d_k}/2$, $2\\sqrt{d_k}$, $4\\sqrt{d_k}$, and $8\\sqrt{d_k}$, to evaluate the effectiveness of the learning-based dynamic temperature scaling mechanism. The findings indicate a clear impact of temperature scaling across different appliances, with $\\sqrt{d_k}/4$ and $\\sqrt{d_k}/2$ yielding the best performance, as expected. We also analyzed the learned $\\tau$ values, which showed a significant advantage when utilizing our proposed meta-network.\nTo further validate the effectiveness of our meta-network-based learning mechanism, we also conducted experiment using $\\tau$ as a learnable parameter by initializing it randomly and trained in an end-to-end manner, without the meta-network. The results indicate that the meta-network is essential for achieving improved performance, as the alternative approach led to a negative during the learning process, resulting in poorer outcomes."}, {"title": "F. Computational Complexity", "content": "We compare the compute time of our proposed method with the standard attention mechanism [9], [18] using a P100 GPU on an HPC cluster. Our method adds a small amount of computation time compared with the standard attention model. However, it can significantly improve the attention module at the expense of this negligible computing cost. Powerful GPUs may improve this condition."}, {"title": "IV. CONCLUSION AND FUTURE WORKS", "content": "In this paper, we proposed two mechanisms for improving the original transformer model in terms of its capacity. Our experimental results effectively validate that our proposed mechanisms improve the NILM performance on smaller datasets with transformer architecture. We provide theoretical analysis of our proposed mechanisms. In the future, we plan to experiment with larger datasets and optimize compute time."}]}