{"title": "Enhancing Diffusion Models for Inverse Problems with Covariance-Aware\nPosterior Sampling", "authors": ["Shayan Mohajer Hamidi", "En-Hui Yang"], "abstract": "Inverse problems exist in many disciplines of science and\nengineering. In computer vision, for example, tasks such\nas inpainting, deblurring, and super-resolution can be effec-\ntively modeled as inverse problems. Recently, denoising dif-\nfusion probabilistic models (DDPMs) are shown to provide\na promising solution to noisy linear inverse problems with-\nout the need for additional task-specific training. Specifically,\nwith the prior provided by DDPMs, one can sample from\nthe posterior by approximating the likelihood. In the litera-\nture, approximations of the likelihood are often based on the\nmean of conditional densities of the reverse process, which\ncan be obtained using Tweedie's formula. To obtain a bet-\nter approximation to the likelihood, in this paper we first de-\nrive a closed-form formula for the covariance of the reverse\nprocess. Then, we propose a method based on finite differ-\nence method to approximate this covariance such that it can\nbe readily obtained from the existing pre-trained DDPMs,\nthereby not increasing the complexity compared to existing\napproaches. Finally, based on the mean and approximated co-\nvariance of the reverse process, we present a new approxima-\ntion to the likelihood. We refer to this method as covariance-\naware diffusion posterior sampling (CA-DPS). Experimental\nresults show that CA-DPS significantly improves reconstruc-\ntion performance without requiring hyperparameter tuning.\nThe code for the paper is put in the supplementary materials.", "sections": [{"title": "1\nIntroduction", "content": "Denoising diffusion probabilistic models (DDPMs) (Ho,\nJain, and Abbeel 2020) have made remarkable advance-\nments in data synthesis over the past few years, revolution-\nizing fields such as image synthesis (Nichol et al. 2022; Sa-\nharia et al. 2022; Zhang et al. 2023), video generation (Ho\net al. 2022) and audio synthesis (Kong et al.).\nGiven the powerful ability of DDPMs to estimate tar-\nget distributions, one promising application is to use them\nto solve linear inverse problems such as denoising, inpaint-\ning, deblurring, and super-resolution. These tasks aim to re-\ncover a signal xo (e.g., a face image) from a measurement\ny, where y is related to xo through the forward measure-\nment operator A and detector noise \u03b7 (Song, Meng, and\nErmon 2021; Chung et al. 2023; Song et al. 2023; Dou and\nSong 2024; Peng et al. 2024). A naive approach to using\nDDPMs for solving inverse problems is to train a conditional\nDDPM to estimate the posterior p(x0|y) through supervised\nlearning. However, this approach can be computationally de-\nmanding, as it requires training separate models for different\nmeasurement operators.\nTo tackle the issue mentioned above, a newer method to\napproximate the posterior seeks to leverage pre-trained un-\nconditional DDPMs that estimate the prior p(x0), thereby\navoiding the need for additional training. In this approach,\nthe prior p(x0) obtained from DDPMs is combined with the\nlikelihood p(y|x0) to sample from the posterior distribution\nfor inverse problems. However, because the likelihood term\np(y|xo) is analytically intractable in the context of DDPMS\ndue to their time-dependent nature, it must be approximated\nin some way (Chung et al. 2023).\nTo approximate the likelihood p(y|xo), there are mainly\ntwo approaches in the literature as we discuss in the sequel.\nThe first approach relies on projections onto the measure-\nment subspace (Song et al. 2021; Chung, Sim, and Ye 2022;\nChoi et al. 2021). However, these projection-based meth-\nods perform poorly in the presence of noise in the mea-\nsurements, as the noise tends to be amplified during the\ngenerative process due to the ill-posed nature of inverse\nproblems (Chung et al. 2023). The second approach lever-\nages the relationship $p(y|x_t) = \\int p(y|x_0)p(x_0|x_t)dx_0$ in\nDDPMs; as such, assuming that p(y|xo) is known, one can\napproximate p(y|xt) by estimating p(xo|xt). Although the\ndistribution of p(xoxt) is still intractable, the conditional\nmean x = E(xoxt) can be analytically obtained using\nTweedie's formula (Efron 2011). The conditional mean xo\nis then used by Chung et al. (2023) to approximate p(xoxt)\nby delta distribution \u03b4(x \u2013 20), and used by Song et al.\n(2023) to approximate p(xoxt) with a Gaussian distribu-\ntion N(\u00e3o, r\u00b2I) with a heuristically selected variance r7.\nNonetheless, approximating p(xoxt) using only its first\nmoment (mean) is prone to sub-optimal performance due\nto biases in reconstruction (Jalal et al. 2021b; Meng et al.\n2021). As a remedy, this paper aims to improve the approx-\nimation of p(x0xt) by incorporating its second moment.\nParticularly, we derive a closed-form expression for the con-\nditional covariance Cov(xoxt) in DDPMs, and show that\nit depends on the Hessian H+ = \u22072, log pt (yxt). Yet, the\nHessian Ht is not directly available for DDPMs, as these\nmodels only provide the score function x\u2081 log pt (yxt). To"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Diffusion Models for Inverse Problems", "content": "The use of diffusion models for solving inverse problems by\nsampling from the posterior has recently gained significant\ntraction in various fields, including image denoising (Kawar\net al. 2022), compressed sensing (Bora et al. 2017; Kad-\nkhodaie and Simoncelli 2021), magnetic resonance imaging\n(MRI) (Jalal et al. 2021a), projecting score-based stochastic\ndifferential equations (SDEs) (Song et al. 2022), and vari-\national approaches (Mardani et al. 2023; Feng and Bouman\n2023). In particular, the most relevant line of work, which we\nwill review in detail in Section 3.2, involves using Tweedie's\nformula (Efron 2011) to approximate the smoothed likeli-\nhood, as deployed in methods like diffusion posterior sam-\npling (DPS) (Chung et al. 2023) and pseudo-guided diffu-\nsion models (IGDM) (Song et al. 2023). Similar strate-\ngies are also employed using singular-value decomposition\n(SVD) based approaches (Kawar, Vaksman, and Elad 2021)."}, {"title": "2.2 Higher Order Approximation of reverse\nprocess", "content": "The approach presented in this paper can be seen as a variant\nof high-order denoising score matching (Meng et al. 2021;\nLu et al. 2022), which aims to train a diffusion model capa-\nble of learning the higher-order moments of the reverse pro-\ncess. However, these methods are typically limited to small-\nscale datasets due to their computational complexity.\nSimilarly to our work, Boys et al. (2023) aims to esti-\nmate the covariance of the reverse process. However, their\nmethod require that the second-order scores or the Jacobian\nof the first-order score be available by the diffusion model.\nIn addition, Peng et al. (2024) proposed a method to opti-\nmize the posterior likelihood. They proposed two methods\nfor (i) when reverse covariance prediction is available from\nthe given unconditional diffusion model, and (ii) when re-\nverse covariance prediction is not available. Their first ap-\nproach is different from our proposed method as our method\ndo not require reverse covariance to be available. Addition-\nally, their second approach is based on Monte Carlo estima-\ntion which incurs extra complexity to the sampling process.\nWe also acknowledge the work of Stevens et al. (2023), who\nexplored a maximum-a-posteriori approach to estimate the\nmoments of the posterior."}, {"title": "3 Background and Preliminaries", "content": null}, {"title": "3.1 Diffusion Models", "content": "Diffusion models characterize a generative process as the\nreverse of a noise addition process. In particular, (Song\net al. 2021) introduced the It\u00f4 stochastic differential equa-\ntion (SDE) to describe the noise addition process (i.e., the\nforward SDE) for the data x(t) over the time interval t \u2208\n[0,T], where x(t) \u2208 Rd for all t.\nIn this paper, we adopt the variance-preserving form of\nthe SDE (VP-SDE) (Song et al. 2021), which is equivalent\nto the DDPM framework (Ho, Jain, and Abbeel 2020) whose\nequation is given as follows\n$dx = \\frac{\\beta(t)}{2}x dt + \\sqrt{\\beta(t)} dw,$\nwhere \u1e9e(t): IR \u2192 R+ represents the noise schedule of the\nprocess, which is typically chosen as a monotonically in-\ncreasing linear function of t (Ho, Jain, and Abbeel 2020).\nThe term w denotes the standard d-dimensional Wiener\nprocess. The data distribution is defined at t = 0, i.e.,\nx(0) ~ Pdata, while a simple and tractable distribution,\nsuch as an isotropic Gaussian, is achieved at t = T, i.e.,\nx(T) ~ N(0, 1).\nThe goal is to recover the data-generating distribution\nfrom the tractable distribution. This can be accomplished by\nformulating the corresponding reverse SDE for Equation (1),\nas derived in (Anderson 1982):\n$dx = [\\frac{\\beta(t)}{2}x-\\beta(t)\\nabla_{x_t}log p_t (x_t)  ]dt + \\sqrt{\\beta(t)}dw,$\nwhere dt represents time running backward, and dw corre-\nsponds to the standard Wiener process running in reverse."}, {"title": "3.2 Diffusion Models for Solving Inverse\nProblems", "content": "We consider the linear inverse problems for reconstructing\nan unknown signal x \u2208 Rd from noisy measurements y \u2208\nRm:\n$y = Ax_0 + n,$\nwhere A \u2208 Rm\u00d7d is a known measurement operator and\n\u03b7 ~ \u039d(0,021) is an i.i.d. additive Gaussian noise with a\nknown standard deviation of \u03c3. This gives a likelihood func-\ntion p(y|xo) = N(y|Ax\u03bf, \u03c32\u0399).\nUsually, we are interested in the case when m < d, which\nfollows many real-world scenarios. When m < d, the prob-\nlem is ill-posed and some kind of prior is necessary to ob-\ntain a meaningful solution. In the Bayesian framework, one\nutilizes p(xo) as the prior, and samples from the posterior\np(xoy), where the relationship is formally established with\nthe Bayes' rule: p(xo|y) = p(y|xo)p(xo)/p(y). Leverag-\ning the diffusion model as the prior, it is straightforward to\nmodify Equation (2) to arrive at the reverse diffusion sam-\npler for sampling from the posterior distribution:\n$dx = [-\\frac{\\beta(t)}{2}x - \\beta(t) (\\nabla_{x_t} log p_t (x_t) + \\nabla_{x_t} log p_t (y|x_t))] dt + \\sqrt{\\beta(t)}dw,$\nwhere we have used the fact that\n$\\nabla_{x_t}log p_t(x_t| y) = \\nabla_{x_t} log p_t(x_t) + \\nabla_{x_t} log p_t(y|x_t).$\nIn Equation (6), there are two terms that need to be com-\nputed: the score function V\u00e6\u2081 log pt (xt) and the likelihood\nVx+log pt (yxt). To compute the former, involving pt(xt),\nwe can directly use the pre-trained score function se*. How- ever, the latter term is challenging to obtain in closed-form\ndue to its dependence on time t (note that there is only an\nexplicit relationship between y and xo). As such, the likeli-\nhood pt (yxt) shall be estimated. One approach to achieve"}, {"title": "4 Covariance-Aware Diffusion Posterior\nSampling", "content": "In this section, we aim to improve the approximation of\nthe reverse process p(x0|xt) compared to DPS and IGDM.\nSpecifically, instead of heuristically approximating the con-\nditional covariance of xo given \u00e6t as done by IGDM, we\nderive a closed-form formula for it. To this end, we first in-\ntroduce the following theorem.\nTheorem 1. Under the same conditions as in Proposition 1,\nthe posterior covariance Cov(ny) satisfies\n$(\u2207_yT(y))Cov(\\eta|y)\u2207_yT(y) = \\nabla^2 log p(y) \u2013 \\nabla^2 log p_0 (y) \u2013 \\nabla^2T(y) \\odot E(\\eta|y),$\nwhere E(ny) is obtained using Tweedie's formula in Propo-\nsition 1. Additionally, the operator denotes a contraction\noperation between the three dimensional tensor \u22072T(y)\nand the vector E(ny). Specifically, assuming that y \u2208\nRr and n \u2208 Rk (which yields \u22072T(y) \u2208 Rr\u00d7r\u00d7k and\nE[n[y] \u2208 Rk), then \u22072T(y) \u2299 E(n|y) \u2208 Rr\u00d7r is defined\nas\n$[\\nabla^2T(y) \\odot E(\\eta|y)]_{ij} = \\sum_k [\\nabla^2T(y)]_{ijk}E(\\eta_k|Y).$\nNext, we use Theorem 1 for DDPMs to find a closed-form\nexpression for the conditional covariance Cov(xo|Xt)."}, {"title": "5 Experiments", "content": "In this section, to demonstrate the superior performance of\nCA-DPS compared to state-of-the-art alternatives, we eval-\nuate its effectiveness across a range of inverse problems-\nincluding inpainting, deblurring, and super-resolution\u2014\nusing two popular datasets. Specifically, we present quan-\ntitative and qualitative results in Section 5.1 and Section 5.2,\nrespectively. To further illustrate that the superiority of CA-\nDPS stems from its improved approximation of the true\nposterior, we compare its ability to estimate the true poste-\nrior against benchmark methods in Section 5.3, using a toy\ndataset with a known posterior."}, {"title": "5.1 Quantitative Results.", "content": "Experimental setup. Following (Chung et al. 2023;\nDou and Song 2024), we perform experiments on FFHQ\n256x256 (Karras, Laine, and Aila 2019) and ImageNet\n256x256 datasets (Deng et al. 2009), on 1k validation im-\nages each. All images are normalized to the range [0, 1].\nFor a fair comparison, we use the experimental settings\nin (Chung et al. 2023) for all the methods. All measure-\nments are corrupted by Gaussian noise with mean zero and\n\u03c3 = 0.05. For the backward process during the inference,\nwe set the number of time steps as N = 1000 and use\nthe pre-trained score model from (Chung et al. 2023) for\nthe FFHQ dataset, and the score model from (Dhariwal and\nNichol 2021) for the ImageNet dataset.\nThe measurement models used are mostly based on\n(Chung et al. 2023): (i) for box-type inpainting, we mask out\na 128 \u00d7 128 box region, and for random-type inpainting, we\nmask out 92% of the total pixels (across all RGB channels);\n(ii) for super-resolution (SR), we perform bicubic downsam-\npling; (iii) for Gaussian blur, we use a kernel size of 61 \u00d7 61\nwith a standard deviation of 3.0, and for motion blur, we\nuse randomly generated kernels from the code\u00b2, with a size\nof 61 \u00d7 61 and an intensity value of 0.5 (these kernels are\nthen convolved with the ground truth image to produce the\nmeasurements).\nBenchmark methods. We compare the performance\nof CA-DPS with the following benchmark methods: DPS\n(Chung et al. 2023), IGDM (Song et al. 2023), denoising\ndiffusion restoration models (DDRM) (Kawar et al. 2022),\nmanifold constrained gradients (MCG) (Chung et al. 2022),\nPlug-and-play alternating direction method of multipliers\n(PnP-ADMM) (Chan, Wang, and Elgendy 2016), Score-\nSDE (Song et al. 2021) and total-variation (TV) sparsity reg-\nularized optimization method (ADMM-TV). For a fair com-\nparison, we used the same score function for all the differ-\nent methods that are based on diffusion (i.e. CA-DPS, DPS,\nDDRM, MCG, score-SDE).\nEvaluation metrics. To evaluate different methods, we\nfollow (Chung et al. 2023) to use three metrics: (i) learned\nperceptual image patch similarity (LPIPS) (Zhang et al.\n2018), (ii) Frechet inception distance (FID) (Heusel et al.\n2017), and (iii) structure similarity index measure (SSIM).\nThese metrics enable a comprehensive assessment of image\nquality. All our experiments are carried out on a single A100\nGPU.\nExperimental results. The results for both datasets are\nlisted in Table 1. The results demonstrate that CA-DPS out-\nperforms baselines significantly in almost all the tasks. It\nis remarkable that in the challenging inpainting tasks (box\nand random), CA-DPS achieves the best performance. When\nassessing performance across the three metrics, CA-DPS\nemerges as the front-runner in three of them, with superior\nresults compared to the other benchmark methods."}, {"title": "5.2\nQualitative Results", "content": "In this section, we aim to visualize the reconstructed im-\nages from CA-DPS and compare them with those recon-"}, {"title": "5.3 Toy Dataset", "content": "In this subsection, we aim to illustrate that the superior re-\nsults obtained by CA-DPS arise from its enhanced approxi-\nmation of the true posterior. To support this, we shall com-\npare its ability to estimate the true posterior against that of\nbenchmark methods.\nTo this end, we generate a toy dataset whose distribu-\ntion po(xo) is a mixture of 25 Gaussian distributions\u00b3. The\nmeans and variances for each mixture component are de-\ntailed in the Supplementary materials, where we also explain\nhow, for a given set of observations y, measurement matrix\nA, and noise standard deviation \u03c3, the target posterior can\nbe computed exactly.\nTo assess the effectiveness of posterior sampling meth-\nods, we generate multiple measurement models (y, A) \u2208\nRmx Rmxd for combinations of dimensions and obser-\nvation noise levels (d,m,\u03c3) \u2208 {8,80,800} \u00d7 {1,2,4} x\n{10-2,10-1,100}, while each Gaussian mixture compo-\nnent is equally weighted. By choosing different dimension\nsizes, we aim to understand how posterior sampling meth-\nods perform across varying dimensions, while controlling\nthe noise level allows us to evaluate how these methods per-\nform at different signal-to-noise ratios.\nNext, we generate 1000 samples for each of the above\nscenarios (3 \u00d7 3 \u00d7 3 = 27 scenarios). Then, we use CA-\nDPS, IGDM and DPS to estimate the posterior probability\nthrough 1000 denoising steps. Afterward, to evaluate how\nwell each algorithm estimates the posterior distribution com-\npared to the target posterior, we utilize the sliced Wasserstein\n(SW) distance (Kolouri et al. 2019). We calculate the SW\ndistance using 104 slices for 1000 samples.\nTable 2 shows the 95% confidence intervals, derived from\n20 randomly selected measurement models (A) for each pa-\nrameter setting (d, m, \u03c3). In addition, Figure 4 illustrates the\nfirst two dimensions of the estimated posterior distributions\nfor the configuration (80, 1) from Table 2, using one ran-\ndomly generated measurement model (A,\u03c3 = 0.1). This\nvisualization gives insights into how well the algorithms es-\ntimate the posterior distribution, showing that CA-DPS pro-\nvides a more accurate estimate of the target posterior com-\npared to IGDM and DPS, as it captures all modes, whereas\nIGDM and DPS do not."}, {"title": "6 Conclusion", "content": "In this paper, we proposed CA-DPS, a method designed to\nenhance the performance of DDPMs in solving inverse prob-\nlems. To achieve this, we derived a closed-form expression\nfor the covariance of reverse process in DDPMs. We then\nproposed a method based on finite differences to approxi-\nmate this covariance, making it easily obtainable from ex-\nisting pre-trained DDPMs. Utilizing the mean and the ap-\nproximated covariance of the reverse process, we present a\nnew approximation for the likelihood. Finally, we conducted\nthree sets of experiments to demonstrate the superiority of\nCA-DPS: (i) quantitative evaluations using various metrics\nto assess the quality of reconstructed images, (ii) qualitative\nassessments by visualizing some of the reconstructed im-\nages, and (iii) testing the proximity of estimated posterior to\nthe true posterior using a toy dataset with a known posterior."}, {"title": "7 Anonymous Repo of the Project", "content": "Please use the following website for the code and the repo of the paper:\nhttps://anonymous.4open.science/r/AAAI-2025-Covariance-Aware-Diffusion-Models-80E8/README.md"}, {"title": "8 Proof of Theorem 1", "content": "Proof. The marginal distribution p(y) could be expressed as\n$p(y) = \\int p(y/\\eta)p(\\eta)d\\eta$\n$ = \\int Po(y) exp (\\eta^TT(y) \u2013 \\varphi(\\eta))p(\\eta)d\\eta.$\nThen, the derivative of the marginal distribution p(y) with respect to y becomes\n$\\nabla_{y_i}P(Y) = \\nabla_{y_i} Po (Y) \\int exp (\\eta^TT(y) \u2013 \\varphi(\\eta))p(\\eta)d\\eta + \\int(\\nabla_{y_i}T(y))^T \\eta po (y) exp (\\eta^TT(y) \u2013 \\varphi(\\eta))p(\\eta)d\\eta$\n$ = \\frac{\\nabla_{y_i} Po (Y)}{Po(y)} \\int p(y/\\eta)p(\\eta)d\\eta + (\\nabla_{y_i}T(y))^T\\int \\eta p(y/\\eta)p(\\eta)d\\eta$\n$ = \\frac{\\nabla_{y_i} Po (Y)}{Po(y)}p(y) + (\\nabla_{y_i}T(y))^T \\int \\eta p(y,\\eta)d\\eta$\nTherefore,\n$\\frac{\\nabla_{y_i}P(Y)}{p(y)} = \\frac{\\nabla_{y_i}Po(Y)}{Po(y)} + (\\nabla_{y_i}T(y))^T \\int \\eta p(\\eta|y)d\\eta$\nwhich is equivalent to\n$(\u2207_yT(y))E[\u03b7|y] = \u2207_y log p(y) \u2013 \u2207_y log p_0(y)$\nNow, we take another derivative w.r.t. y[j] from both sides of Equation (33):\n$\\nabla_{yj}\\nabla_{y_i}P(y) = \\nabla_{yj} \\frac{\\nabla_{y_i}Po (y)}{Po(y)} \\int exp (\\eta^TT(y) \u2013 \\varphi(\\eta))p(\\eta)d\\eta$\n$+ \\nabla_{y_i}Po(y)\\int (\\nabla_{y_j}T(y))^T \\eta exp (\\eta^TT(y) \u2013 \\varphi(\\eta))p(\\eta)d\\eta$\n$\\nabla_{y_i}Po(y)\\int(\\nabla_{y_j}T(y))^T \\eta exp (\\eta^TT(y) \u2013 \\varphi(\\eta))p(\\eta)d\\eta$\n$+ \\nabla_{yj} \\frac{\\nabla_{y_i}Po (y)}{Po(y)} \\int\\eta exp (\\eta^TT(y) \u2013 \\varphi(\\eta))p(\\eta)d\\eta$\n$+ \\nabla_{y_i}Po(y)\\int (\\nabla_{y_j}T(y))^T \\eta \\eta exp (\\eta^TT(y) \u2013 \\varphi(\\eta))p(\\eta)d\\eta$\n$ = \\frac{\\nabla_{yj}\\nabla_{y_i} Po (Y)}{Po(y)} \\int p(y,\\eta)d\\eta + \\int(\\nabla_{y_j}T(y))^T \\eta p(y,\\eta)d\\eta$\n$+ \\nabla_{y_i}Po(y)\\frac{\\nabla_{yj}Po(y)}{Po(y)} \\int\\eta exp (\\eta^TT(y) \u2013 \\varphi(\\eta))p(\\eta)d\\eta$\n$\\int(\\nabla_{y_i}T(y))^T \\eta \\eta \\nabla_{y_j}T(y) exp (\\eta^TT(y) \u2013 \\varphi(\\eta))p(\\eta)d\\eta$"}, {"title": "9 Toy dataset", "content": "The generation of this dataset is inspired from Boys et al. (2023).\nAs explained earlier in the paper, we model po(x0) as a mixture of 25 Gaussian distributions. Each of these Gaussian\ncomponents has a mean vector Ui,j in Rd, defined as Ui,j = (8i, 8j, ..., 8i, 8j) for each pair (i, j) where i and j take values\nfrom the set {-2, -1, 0, 1, 2}. All components have the same variance of 1. The unnormalized weight associated with each\ncomponent is wi,j = 1.0. Additionally, we have set the variance of the noise, \u03c3\u03be, to 10\u22124.\nRecall that the distribution pt(xt) can be expressed as an integral: pt(xt) = \u222b Pt|0(Xt|xo)po(x)dxo. Since po(x0) is a\nmixture of Gaussian distributions, pt(xt) is also a mixture of Gaussians. The means of these Gaussians are given by \u221aatUi,j,\nand each Gaussian has unit variance. By using automatic differentiation libraries, we can efficiently compute the gradient\nVxt log Pt (xt).\nWe have set the parameters Bmax = 500.0 and \u1e9emin = 0.1, and we use 1000 timesteps to discretize the time domain. For\na given pair of dimensions and a chosen observation noise standard deviation (d,m, \u03c3), the measurement model (y, A) is\ngenerated as follows:\nMatrix A: First, we sample a random matrix A from a Gaussian distribution N (0m\u00d7d, Imxd). We then compute its singular\nvalue decomposition (SVD), \u00c3 = USVT. For each pair (i, j) in {\u22122, -1,0,1,2}2, we draw a singular value si,j from a\nuniform distribution on the interval [0, 1]. Finally, we construct the matrix A = Udiag({Si,j}(i,j)\u2208{\u22122,-1,0,1,2}2)VT."}, {"title": "10 More Qualitative Results", "content": "In this section, we depict more reconstructed images using CA-DPS and compare it with those reconstructed by DPS. To this\nend, we pick 9 images from FFHQ dataset, and conduct super-resolution task (16\u00d7) with a Gaussian noise whose standard\ndeviation is a = 0.05. The results are depicted in Figure 4.\nFurthermore, to visualize the reconstruction process over 1000 timesteps, we select a single image and display the recon-\nstructed images throughout the denoising process, as illustrated in Figure 3."}]}