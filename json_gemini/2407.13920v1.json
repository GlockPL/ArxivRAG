{"title": "DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global Attention", "authors": ["Xiaoya Tang", "Bodong Zhang", "Beatrice S. Knudsen", "Tolga Tasdizen"], "abstract": "We here propose a novel hierarchical transformer model that adeptly integrates the feature extraction capabilities of Convolutional Neural Networks (CNNs) with the advanced representational potential of Vision Transformers (ViTs). Addressing the lack of inductive biases and dependence on extensive training datasets in ViTs, our model employs a CNN backbone to generate hierarchical visual representations. These representations are then adapted for transformer input through an innovative patch tokenization. We also introduce a 'scale attention' mechanism that captures cross-scale dependencies, complementing patch attention to enhance spatial understanding and preserve global perception. Our approach significantly outperforms baseline models on small and medium-sized medical datasets, demonstrating its efficiency and generalizability. The components are designed as plug-and-play for different CNN architectures and can be adapted for multiple applications.", "sections": [{"title": "1 Introduction", "content": "The Vision Transformer (ViT) [4] has significantly advanced the adaptation of transformers from language to vision, demonstrating superior performance over CNNs when pre-trained on large datasets. ViT employs a patch tokenization process that converts images into a sequence of uniform token embeddings. These tokens undergo Multi-Head Self-Attention (MSA), transforming them into queries, keys, and values that capture extensive non-local relationships. Despite their potential, ViTs can underperform similarly-sized ResNets [10] when inadequately trained due to their lack of inductive biases such as translation equivariance and locality[19,14], which are naturally encoded by CNNs. Recent efforts have focused on mitigating ViTs' limitations by integrating convolutions or adding self-supervised tasks. Prevalent approaches combine CNN feature extractors with transformer encoders [1,4,24,18,28,6,15,5], such as the 'hybrid' ViT [4]. Other methods like knowledge distillation [20] transfer biases from CNNs to ViT. Nonetheless, ViTs' smaller receptive fields compared to CNNs limit their ability to capture detailed spatial relationships [1], which can be partially alleviated by techniques like enriched spatial shifting patches [14].\nHistopathology image analysis, critical in medical diagnostics, involves ex-amining whole slide images (WSIs) to detect and interpret complex tissue struc-tures and cellular details. This analysis faces challenges due to the varied scales of visual entities within WSIs, such as the differing sizes of cell nuclei and vascu-lar structures, both of which can contribute to a model's task of distinguishing low- and high-risk kidney cancers. Moreover, vital global features of cancer and its microenvironment, observable only at lower scales, are crucial for various downstream tasks. The neglect of these multiple scales can significantly impair the performance of deep learning models in medical image recognition tasks. CNNs tackle this issue by utilizing a hierarchical structure created by lower and higher stages, which allows them to detect visual patterns from simple low-level edges to complex semantic features. Conversely, ViTs employ fixed-scale patches, thereby overlooking crucial multi-scale information within the same image [19], which can hinder their performance across diverse tasks. By harnessing a hierarchical structure similar to that of CNNs, ViTs can be prevented from overlooking the critical multi-scale features, while also imparting necessary inductive biases. Existing works on directly integrating multi-scale information into ViTs vary primarily in the placement of convolutional operations: during patch tokenization[28,26,8], within [12,8,16] or between self-attention layers, including query/key/value projections[24,28], forward layers [15], or positional encoding [26], etc. Despite the benefits of hierarchical configurations [11], a definitive model for visual tasks has yet to emerge. The challenge remains in effectively producing and utilizing features across various scales. In response, we propose a novel hierarchical Vision Transformer model, outlined as follows:\n1. Our proposed multi-scale tokenization involves a single-layer projection, patch indexing, and concatenation, assembling features from different stages of the CNN into multi-scale tokens.\n2. We introduce a novel MSA mechanism called scale attention, combined with patch attention. This approach enables the model to recognize connections across scales, expanding ViT's receptive field and bridging the gap between CNN and Transformer architectures.\n3. Our proposed scale token, part of the scale attention, is initialized with a fused embedding derived from hierarchical representations. It enriches the transformer's multi-granularity representation and aggregates scale information, serving as the input for the global patch attention."}, {"title": "2 Related Work", "content": "Various approaches have explored integrating the hierarchical architecture of CNNs into Vision Transformers (ViTs) across different visual tasks, including video recognition[6], image classification [2,3,5,7,11,12,15,16,18,20,22,23,24,26,27][28,29,31], object detection[3,7,11,12,16,22,25,26,27,28,31], and segmentation [3,7][12,16,21,22,26,27,31]. Notable methods emulate the pyramid structure of CNN with stage-wise pooling and convolutional embeddings [11] or integrate pooling within the attention mechanism[6].\nMultiple scales have been exploited beyond mere convolution integration. The Swin Transformer [18] utilizes a shifting window strategy, while Dong et al. [3] split multi-heads to perform self-attention in horizontal and vertical stripes. Chen et al. [2] developed a dual-branch architecture that processes varying patch sizes, and Zhang et al. [31] implemented a multi-granularity strategy. PVT [22] reduces feature size progressively using spatial-reduction attention. A recent study [7] employed a spatial decay matrix to enhance self-attention with spatial priors. UNETR [9] constructs a U-shape transformer encoder and decoder for 3D segmentation, while people also replaced UNet skip connections with attention mechanisms [21] for 2D segmentation. Besides, inductive biases can also be integrated through auxiliary tasks such as unsupervised localization to enhance local processing capabilities [17]."}, {"title": "3 Methodology", "content": "Our model utilizes a CNN as the embedding layer, depicted in Figure 1. Patch tokenization contains two steps represented by the dashed lines in Figure 1: First is extracting hierarchical representations from different stages of the CNN backbone. Second is the projection and patch indexing. After acquiring the multi-scale features, we use our DuoFormer to learn the local dependencies across scales and global dependencies across patches, which are needed for downstream tasks."}, {"title": "3.1 Multi-scale Patch Tokenization", "content": "Given the input to the backbone, $x \\in \\mathbb{R}^{H\\times W\\times 3}$ with $H = W$, we derive hier-archical outputs from four stages, denoted as $x_i \\in \\mathbb{R}^{P_i\\times P_i\\times C_i}$ for $i \\in {0,1,2,3}$. Here, $P_i = \\frac{H}{4\\cdot2^{i}}$ specifies the spatial resolution, and $C_i$ represents the channel dimension. We apply a linear projection to transform these features into em-beddings of dimension $D$. Next, we split the embeddings $x'$ from each stage into $N$ non-overlapping patches, set as $N = 49$. Each scale yields a sequence of flattened tokens with spatial size $P^2$, where $P = \\frac{H}{4\\cdot2^{i}\\sqrt{N}}$. We index and con-catenate multi-scale embeddings from each patch across all scales to form the multi-scale tokens $X_{\\sum}$, illustrated in Figure 2. The equation for this process is:\n$x'_i = \\text{Projection}(x_i), x'_i \\in \\mathbb{R}^{P_i\\times P_i \\times D}$\n$x''_i \\in \\mathbb{R}^{P_i^2\\times N \\times D}, P = \\frac{HW}{16\\cdot 4^i \\cdot N}, i \\in {0, 1, 2, 3},$\n(1)\n$X_{\\sum} = \\text{concat}(x''_i) \\in \\mathbb{R}^{S\\times N \\times D}, S = \\sum P_i^2$."}, {"title": "3.2 Duo Attention Module", "content": "Our tokenization directly embeds multiscale spatial information into the scale dimension, inherently enriching the model's inductive biases. Subsequently, our encoder employs scale and patch attentions to respectively focus on detailed image features and broader contexts, as illustrated in Figure 3(a). Our scale at-tention adapts the Multi-Head Self-Attention (MSA) framework by incorporat-ing an additional scale dimension. This adaption integrates multi-scale analysis directly into the attention mechanism and alters tensor operations to accommo-date multi-dimensional tokens. Details are explained in the equation below and depicted in Figure 3(b).\n$X_T = X_{\\sum} + MSA(LN(X_{\\sum})), Y = X_{\\sum} + FFN(LN(X_T))$ (2)"}, {"title": "3.3 Scale Token", "content": "To enhance the hierarchical representations, we use a downsampling strategy involving simple convolutional layers followed by max pooling. This process nor-malizes the spatial dimensions of all embeddings to N, maintaining consistent channel dimensions. N denotes number of patches, set as 49 in our experiments. These embeddings are then concatenated along the channel dimension and pro-jected into the embedding dimension D using lightweight convolutions. The re-sulting scale token, concatenated with multi-scale tokens, serves as the input for the scale attention, guiding it effectively, as detailed below.\n$x_0 = MaxPool(Conv(x_0)), x_1 = MaxPool(Conv(x_1))$\n$x_2 = MaxPool(x_2), x_3 = x_3, where x_i \\in \\mathbb{R}^{N\\times C_i},$\n$X_{\\sum} = concat(x_0, x_1, x_2, x_3) \\in \\mathbb{R}^{N\\times C}, C = \\sum C_i,$\n(3)\n$Scale Token = ReLU(BN(Conv(\\check{X_{\\sum}}))) \\in \\mathbb{R}^{N\\times D}$"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Our evaluation utilized two datasets, Utah ccRCC and TCGA ccRCC [30], with varying ResNet backbones for a thorough analysis. The Utah ccRCC dataset comprises 49 WSIs from 49 patients, split into training (32 WSIs), validation (10 WSIs), and testing (7 WSIs). Tiles were extracted from marked polygons at 400x400 pixel resolution at 10X magnification with a 200-pixel stride and center-cropped to 224x224 pixels for model compatibility. The training set included 28,497 Normal/Benign, 2,044 Low Risk, 2,522 High Risk, and 4,115 Necrosis tiles, with validation and test sets proportionately distributed. The TCGA ccRCC dataset features 150 labeled WSIs divided into 30 for training, 60 for validation, and 60 for testing, using similar cropping methods but adjusted strides to gather more training patches. It contains 84,578 Normal/Benign, 180,471 Cancer, and 7,932 Necrosis tiles in the training set, with similar distributions in validation and test sets. For model details, please refer to Appendix."}, {"title": "4.2 Result Analysis", "content": "In this study, we utilized ResNet18 and ResNet50 backbones [10] pre-trained on extensive datasets, assessing our model under two paradigms: with ImageNet supervised pre-trained and pathology(TCGA and TULIP) self-supervised pre-trained [13] backbones. Results, shown in Table 1, indicate our model surpasses ResNet baselines by over 2% across all settings and outperforms various Hybrid-ViTs in all scenarios. The results underscore our model's capacity to harness multi-scale features and integrate crucial inductive biases without necessitating additional tasks or additional pre-training of the transformer encoder.\nIn the supervised pre-training scenario, particularly with TCGA using a ResNet 50 backbone, deeper encoders sometimes hindered performance, high-lighting the need for careful design when integrating CNN architectures, es-pecially considering domain shifts. Our DuoFormer improved performance by 3.83%, demonstrating its effectiveness in leveraging multi-scale representations and likely guiding the feature extractor to adapt better to domain shifts when trained together. In the self-supervised pre-trained experiments, our model sig-nificantly outperformed the baseline by 9.88% and clearly surpassed the Hybrid-ViTs, showing the superiority of our model in leveraging multi-scale features. These findings suggest that with the proposed designs, the model can effectively capture essential local features while preserving global attention capabilities, thereby addressing the typical inductive bias limitations found in transformers."}, {"title": "4.3 Ablation Studies", "content": "Ablation on Scale Attention For ablation studies, we utilized our best mod-els in both settings, employing ResNet18 pretrained on ImageNet for UTAH and ResNet50 pretrained on histopathology images for TCGA. We evaluated the individual contributions of scale and patch attention mechanisms using con-figurations of 6 layers for UTAH and 8 layers for TCGA. The different depths were chosen to adapt to the larger size of the TCGA dataset and the smaller size of the UTAH dataset. Results in Table 2 indicate that scale attention alone outperforms setups using only patch attention, suggesting the robustness of our scale token and attention mechanism in harnessing multi-scale features. The re-sults also confirm that the best performance on both datasets is achieved when both attention modules are employed together, emphasizing the necessity of in-tegrating both local and global information for effective visual processing.\nAblation on Scale Token To evaluate the role of the scale token, we con-ducted experiments comparing configurations with and without a scale token, and against a learnable scale token, as shown in Table 3. Our scale token effec-tively enhanced local information capture, outperforming the learnable version. Without a scale token, using the first token from scale attention yielded better results than averaging all tokens, likely due to the first token's representation of the final CNN stage's output, which provides crucial, concise information. This suggests that averaging introduces noise.\nAblation on Multi-Scale Representations We explored the impact of dif-ferent combinations of stages on the UTAH dataset. So represents the shallowest stage (56 \u00d7 56), and S3 is the deepest (7\u00d77). According to the results, incorpo-rating all stages slightly harmed performance, likely due to overfitting given the small UTAH dataset. Including S3 generally improved performance, highlight-ing the final stage's importance for classification accuracy. Including So often decreased performance, possibly due to its larger spatial embeddings and higher overfitting risk. Conversely, on larger datasets, as shown in Table 1, including all stages proved beneficial. The highest three configurations here used S1, S2, and S3, demonstrating the benefits of multi-scale integration while managing computational complexity."}, {"title": "5 Conclusion", "content": "In this study, we introduced a novel hierarchical transformer with dual atten-tion mechanisms that enhance visual data interpretation across scales, improv-ing medical image classification. Ablation studies confirm performance optimiza-tion, demonstrating the model's robustness and adaptability across various CNN backbones and tasks, paving the way for broader applications in medical imaging and vision-related fields."}, {"title": "6 Appendix", "content": "Model Training Details All models, including the ResNet baselines, were trained using the Adam optimizer with B\u2081 = 0.9 and \u03b22 = 0.999, without applying weight decay. For the DuoFormer model, batch sizes were set to 32 for the Utah dataset and 6 for the TCGA dataset. We employed a OneCycle learning rate scheduler that starts from a minimal learning rate, progressively increasing to a set rate of 1 \u00d7 10\u22124. Each model underwent training for 50 epochs on Utah and 100 epochs on TCGA, utilizing early stopping with patience of 20 and 50 epochs, respectively. We saved the best-performing model from the validation data for inference. Model performance was evaluated using balanced accuracy for both datasets. All computations were performed on an NVIDIA RTX A6000 with 48 GB of memory.\nAblation on Numbers of Heads and Layers We assessed our model's sensi-tivity to two hyperparameters: the number of heads and the number of layers in two attention modules. Initially, we fixed the number of heads at 12 and varied the number of layers from 4 to 12 to identify optimal configurations for each dataset. Subsequently, we tested heads from 4 to 12, excluding 10 due to in-compatibility with the feature dimension D = 768, using the optimal number of layers. We observed that performance generally increases and then decreases with attention depth. Specifically, performance peaks at 6 layers for the Utah dataset and at 8 layers for the TCGA dataset, likely due to the varying sizes of the datasets. Additionally, we noted a similar pattern of initial increase followed by a decrease in performance for the number of heads across both datasets, peaking at 8 heads. Notably, our models with all tested numbers of heads and layers performed better than baseline ResNets, except in one case where performance was slightly worse, demonstrating the effectiveness of our proposed model."}]}