{"title": "Score matching through the roof: linear, nonlinear, and latent variables causal discovery", "authors": ["Francesco Montagna", "Philipp M. Faller", "Patrick Bl\u00f6baum", "Elke Kirschbaum", "Francesco Locatello"], "abstract": "Causal discovery from observational data holds great promise, but existing methods rely on strong assumptions about the underlying causal structure, often requiring full observability of all relevant variables. We tackle these challenges by leveraging the score function \\(\\nabla\\log p(X)\\) of observed variables for causal discovery and propose the following contributions. First, we generalize the existing results of identifiability with the score to additive noise models with minimal requirements on the causal mechanisms. Second, we establish conditions for inferring causal relations from the score even in the presence of hidden variables; this result is two-faced: we demonstrate the score's potential as an alternative to conditional independence tests to infer the equivalence class of causal graphs with hidden variables, and we provide the necessary conditions for identifying direct causes in latent variable models. Building on these insights, we propose a flexible algorithm for causal discovery across linear, nonlinear, and latent variable models, which we empirically validate.", "sections": [{"title": "1 Introduction", "content": "The inference of causal effects from observations holds the potential for great impact arguably in any domain of science, where it is crucial to be able to answer interventional and counterfactual queries from observational data [1, 2, 3]. Existing causal discovery methods can be categorized based on the information they can extract from the data [4], and the assumptions they rely on. Traditional causal discovery methods (e.g. PC, GES [5, 6]) are general in their applicability but limited to the inference of an equivalence class. Additional assumptions on the structural equations generating effects from the cause are, in fact, imposed to ensure the identifiability of a causal order [7, 8, 9, 10]. As a consequence, existing methods for causal discovery require specialized and often untestable assumptions, preventing their application to real-world scenarios.\nFurther, the majority of existing approaches are hindered by the assumption that all relevant causes of the measured data are observed, which is necessary to interpret associations in the data as causal relationships. Despite the convenience of this hypothesis, it is often not met in practice, and the solu- tions relaxing this requirement face substantial limitations. The FCI algorithm [11] can only return an equivalence class from the data. Appealing to additional restrictions ensures the identifiability of some direct causal effects in the presence of latent variables: RCD [12] relies on the linear non-Gaussian additive noise model, whereas CAM-UV [13] requires nonlinear additive mechanisms. Nevertheless, the strict conditions on the structural equations hold back their applicability to more general settings."}, {"title": "2 Model definition and related works", "content": "In this section, we introduce the formalism of structural causal models (SCMs), separately for the the cases with and without hidden variables."}, {"title": "2.1 Causal model with observed variables", "content": "Let X be a set of random variables in \\(\\mathbb{R}^k\\) defined according to the set of structural equations\n\\[X_i := f_i(X_{\\mathrm{PA}_i}, N_i), \\forall i = 1,..., k.\\]\n\\(N_i \\in \\mathbb{R}\\) are mutually independent random variables with strictly positive density, known as noise or error terms. The function \\(f_i\\) is the causal mechanism mapping the set of direct causes \\(X_{\\mathrm{PA}_i}\\) of \\(X_i\\) and the noise term \\(N_i\\), to \\(X_i\\)\u2019s value. A structural causal model (SCM) is defined as the tuple \\((X, N, F, P_N)\\), where \\(F = (f_i)_{i=1}^k\\) is the set of causal mechanisms, and \\(P_N\\) is the joint distribution relative to the density \\(p_n\\) over the noise terms \\(N \\in \\mathbb{R}^k\\). We define the causal graph \\(\\mathcal{G}\\) as a directed acyclic graph (DAG) with nodes \\(X = \\{X_1, ..., X_k\\}\\), and the set of edges defined as \\(\\{X_j \\to X_i : X_j \\in X_{\\mathrm{PA}_i}\\}\\), such that \\(\\mathrm{PA}_i\\) are the indices of the parent nodes of \\(X_i\\) in the graph \\(\\mathcal{G}\\). (In the remainder of the paper, we adopt the following notation: given a set of random variables \\(Y = \\{Y_1, ..., Y_n\\}\\) and a set of indices \\(Z \\subseteq N\\), then \\(Y_Z = \\{Y_i | i \\in Z, Y_i \\in Y\\}\\).)\nUnder this model, the probability density of X satisfies the Markov factorization (e.g. Peters et al. [1] Proposition 6.31):\n\\[p(x) = \\prod_{i=1}^k p(x_i|x_{\\mathrm{PA}_i}),\\]\nwhere we adopt the convention of lowercase letters referring to realized random variables, and use \\(p\\) to denote the density of different random objects, when the distinction is clear from the argument. This factorization is equivalent to the global Markov condition (e.g. Peters et al. [1] Proposition 6.22) that demands that for all \\(\\{X_i, X_j\\} \\in X\\), \\(X_Z \\subseteq X \\setminus \\{X_i, X_j\\}\\), then\n\\[X_i \\perp X_j | X_Z \\implies X_i \\perp X_j | X_Z,\\]\nwhere \\(\\perp \\| \\cdot\\)\\) denotes probabilistic conditional independence of \\(X_i, X_j\\) given \\(X_Z\\), and \\((\\cdot)\\) is the notation for d-separation, a criterion of conditional independence defined on the graph \\(\\mathcal{G}\\)"}, {"title": "2.2 Causal model with unobserved variables", "content": "Under the model (1), we consider the case where the set of variables X is partitioned into the disjoint subsets of observed random variables \\(V = \\{V_1, . . ., V_d\\}\\) and unobserved (or latent) random variables \\(U = \\{U_1,..., U_p\\}\\). We assume that the following set of structural equations is satisfied:\n\\[V_i := f_i(V_{\\mathrm{PA}_i}^V, U_i^V, N_i), \\forall i = 1,...,d,\\]\nwhere \\(U_i^V\\) stands for the set of unobserved parents of \\(V_i\\), and \\(V_{\\mathrm{PA}_i}^V = \\{V_k | k \\in \\mathrm{PA}_i, V_k \\in V\\}\\) are the observed direct causes of \\(V_i\\). Some of the causal relations and the conditional independencies implied by the set of equations (4) can be summarized in a graph obtained as a marginalization of the DAG \\(\\mathcal{G}\\) onto the observable nodes \\(V\\)."}, {"title": "3 Theory for a score-based test of separation", "content": "In this section, we show that for \\(V \\subset X\\) generated according to Equation (4) the Hessian matrix of log \\(p(V)\\) identifies the equivalence class of the marginal MAG \\(M^V\\). It has already been proven that cross-partial derivatives of the log-likelihood are informative about a set of conditional independence relationships between random variables: Spantini et al. [21] (Lemma 4.1) shows that, given \\(V_Z \\subseteq X\\) such that \\(\\{V_i, V_j\\} \\subseteq V_Z\\), then\n\\[\\frac{\\partial^2}{\\partial v_i \\partial v_j} \\log p(V_Z) = 0 \\Longleftrightarrow V_i \\perp V_j | V_Z \\setminus \\{V_i, V_j\\}.\\]\nEquation (3) resulting from faithfulness and the directed global Markov property immediately implies that this expression can be used as a test of conditional independence to identify the Markov equivalence class of the graph \\(M^V\\), as commonly done in constraint-based causal discovery (for reference, see e.g. Section 3 in Glymour et al. [4]). This result generalizes Lemma 1 of Montagna et al. [16], where it is used to define constraints to infer edges in the causal structure without latent variables."}, {"title": "4 A theory of identifiability from the score", "content": "In this section, we show that, under additional assumptions on the data-generating process, we can identify the direct causal relations that are not influenced by unobserved variables, as well as the presence of unobserved active paths (Definition 5) between nodes in the marginalized graph \\(M^V\\).\nAs a preliminary step before diving into causal discovery with latent variables, we show how the properties of the score function identify edges in directed acyclic graphs, that is in the absence of latent variables (when \\(U = \\emptyset\\) and \\(\\mathcal{G} = M^V\\)). The goal of the next section is two-sided: first, it introduces the fundamental ideas connecting the score function to causal discovery that also apply to hidden variable models, second, it extends the existing theory of causal discovery with score matching to additive noise models with both linear and nonlinear mechanisms."}, {"title": "4.1 Warm up: identifiability without latent confounders", "content": "In this section, we summarise and extend the theoretical findings presented in Montagna et al. [17], where the authors show how to derive constraints on the score function that identify the causal order of the DAG \\(\\mathcal{G}\\) where all the variables in the set X are observed. Define the structural relations of (1) as:\n\\[X_i := h_i(X_{\\mathrm{PA}_i}) + N_i, \\forall i = 1,..., k,\\]\nwith three times continuously differentiable mechanisms \\(h_i\\), noise terms centered at zero, and strictly positive density \\(p_N\\). Given the Markov factorization of Equation (2), the components of the score function \\(\\nabla \\log p(x)\\) are:\n\\[\\partial_{x_i} \\log p(x) = \\partial_{x_i} \\log p(x_i|x_{\\mathrm{PA}_i}) + \\sum_{j \\in CH_i} \\partial_{x_i} \\log p(x_j|x_{\\mathrm{PA}_j})\\]\n\\[= \\partial_{n_i} \\log p(n_i) - \\sum_{j \\in CH_i} \\partial_{x_i} h_j(x_{\\mathrm{PA}_j}) \\partial_{n_j} \\log p(n_j),\\]\nwhere \\(CH_i\\) denotes the set of children of node \\(X_i\\). We observe that if a node \\(X_i\\) is a sink, i.e. a node satisfying \\(CH_i = \\emptyset\\), then the summation over the children vanishes, implying that:\n\\[\\partial_{x_i} \\log p(x) = \\partial_{n_i} \\log p(n_i).\\]\nThe key point is that the score component of a sink node is a function of its structural equation noise term, such that one could learn a consistent estimator of \\(\\partial_{x_i} \\log p_x\\) from a set of observations of the noise term \\(N_i\\). Given that, in general, one has access to X samples rather than observations of the noise random variables, authors in Montagna et al. [17] show that \\(N_i\\) of a sink node can be consistently estimated from i.i.d. realizations of X. For each node \\(X_1, ..., X_k\\), we define the quantity:\n\\[R_i := X_i - \\mathbb{E}[X_i|X_{\\setminus x_i}],\\]\nwhere \\(X_{\\setminus x_i}\\) are the random variables in the set \\(X \\setminus \\{X_i\\}\\). \\(\\mathbb{E}[X_i|X_{\\setminus x_i}]\\) is the optimal least squares predictor of \\(X_i\\) from all the remaining nodes in the graph, and \\(R_i\\) is the regression residual. For a sink node \\(X_i\\), the residual satisfies:\n\\[R_i = N_i,\\]"}, {"title": "4.2 Identifiability in the presence of latent confounders", "content": "We now introduce the last of our main theoretical results, that is: given a pair of nodes \\(V_i, V_j\\) that are adjacent in the graph \\(M^V\\) with \\(U \\neq \\emptyset\\), we can use the score function to identify the presence of a direct causal effect between \\(V_i\\) and \\(V_j\\), or that of an active path that is influenced by unobserved variables. Given that the causal model of Equation (4) ensures identifiability only up to the equivalence class, we need additional restrictive assumptions. In particular, we enforce an additive noise model with respect to both the observed and unobserved noise variables. This corresponds to an additive noise model on the observed variables with the noise terms recentered by the latent causal effects."}, {"title": "4.3 A score-based algorithm for causal discovery", "content": "Building on our theory, we propose AdaScore, a generalization of NoGAM to linear and nonlinear additive noise models with latent variables. The main strength of our approach is its adaptivity with respect to structural assumptions: based on the user's belief about the plausibility of several modeling assumptions on the data, AdaScore can output an equivalence class (using the condition of Proposition 1 instead of conditional independence testing in an FCI-like algorithm), a directed acyclic graph (as in NoGAM), or a mixed graph, accounting for the presence of unobserved variables.\nWe now describe the version of our algorithm whose output is a mixed graph, where we rely on score matching estimation of the score and its Jacobian (Appendix C.2). At an intuitive level, we find unoriented edges using Proposition 1, i.e. checking for dependencies in the form of non-zero entries"}, {"title": "5 Experiments", "content": "We use the causally\u00b3 Python library [26] to generate synthetic data with known ground truths, created as Erd\u00f6s-R\u00e9nyi sparse and dense graphs, respectively with probability of edge between pair of nodes equals 0.3 and 0.5. We sample the data according to linear and nonlinear mechanisms with additive noise, where the nonlinear functions are parametrized by a neural network with random weights, a common approach in the literature [18, 26, 27, 28, 29]. Noise terms are sampled from a uniform distribution in the [-2, 2] range. Hidden causal effects are obtained by randomly picking two nodes and dropping the corresponding column from the data matrix. See Appendix D.1 for further details on the data generation. As metric, we consider the structural Hamming distance (SHD) [30, 31], a simple count of the number of incorrect edges, where missing and wrongly directed edges count as one error. We fix the level of the hypothesis tests of AdaScore to 0.05, which is a common choice in the absence of prior knowledge. We compare AdaScore to NoGAM, CAM-UV, RCD, and DirectLiNGAM, whose assumptions are detailed in Table 1. In the main manuscript, we comment on the results on datasets of 1000 observations from dense graphs, with and without latent variables. Additional experiments including those on sparse networks are presented in Appendix E. Our synthetic data are standardized by their empirical variance to remove shortcuts in the data [18, 32]."}, {"title": "6 Conclusion", "content": "The existing literature on causal discovery shows a connection between score matching and structure learning in the context of nonlinear ANMs: in this paper, (i) we formalize and extend these results to linear SCMs, and (ii) we show that the score retains information on the causal structure even in the"}, {"title": "A Useful results", "content": "In this section, we provide a collection of results and definitions relevant to the theory of this paper."}, {"title": "A.1 Definitions over graphs", "content": "Let \\(X = X_1, ..., X_d\\) a set of random variables. A graph \\(\\mathcal{G} = (X, E)\\) consists of finitely many nodes or vertices X and edges E. We now provide additional definitions, separately for directed acyclic and mixed graphs."}, {"title": "A.2 Equivalence between m-separation and d-separation", "content": "In this section, we provide a proof for equation (5), stating the equivalence between m-separation and d-separation in a formal sense."}, {"title": "A.3 Additive noise model identifiability", "content": "We study the identifiability of the additive noise model, reporting results from Peters et al. [9]. We start with a formal definition of identifiability in the context of causal discovery."}, {"title": "A.4 Other auxiliary results", "content": "We state several results that hold for a pair of random variables that are not connected by an active path that includes unobserved variables (active paths are introduced in Definition 5). For the remainder of the section, let V, U be a pair of disjoint sets of random variables, \\(X = V \\cup U\\) generated according to the structural causal model defined by the set of equations (1), \\(\\mathcal{G}\\) the associated causal graph, and \\(M^V\\) the marginalization onto V.\nThe first statement provides under which condition the unobserved parents of two variables in the marginal MAG are mutually independent random vectors."}, {"title": "B Proofs of theoretical results", "content": ""}, {"title": "B.1 Proof of Proposition 1", "content": ""}, {"title": "B.2 Proof of Proposition 2", "content": ""}, {"title": "B.3 Proof of Proposition 3", "content": ""}, {"title": "C Algorithm", "content": ""}, {"title": "C.1 Detailed description of our algorithm", "content": "In Proposition 1 we have seen that score matching can detect m-separations and therefore the skeleton of the PAG describing the data. If one is willing to make the assumptions required for Proposition 3 it could be desirable to use this to orient edges, since the interpretation of PAG edges might be cumbersome for people not familiar with ancestral models. Therefore, one could simply find the skeleton of the PAG using the fast adjacency search [5] and then orient the edges by applying Proposition 3 on every subset of the neighbourhood of every node. This would yield a very costly algorithm. But if we make the assumptions required to orient edges with Proposition 3 we can do a bit better. In Algorithm 2 we present an algorithm that still has the same worst case runtime but runs polynomially in the best case. The main intuition is that we iteratively remove irrelevant nodes in the spirit of the original SCORE algorithm [15]. To this end, we first check if the is any unconfounded sink if we consider the set of all remaining variables. If there is one, we can orient its parents and ignore it afterwards. If there is no such set, we need to fall back to the procedure proposed above, i.e. we need to check the condition of Proposition 3 on all subsets of the neighbourhood of a node, until we find no node with a direct outgoing edge. In Proposition 4 we show that this way we do not fail orient edge or fail to remove any adjacency. In the following discussion, we will use the notation"}, {"title": "C.2 Finite sample version of AdaScore", "content": "All theoretical results in the paper have assumed that we know the density of our data. Obviously, in practise we have to deal with a finite sample instead. Especially, in Proposition 1 and Proposition 3 we derived criteria that compare random variables with zero. Clearly, this condition is never met in practise. Therefore, we need find ways to reasonably set thresholds for these random quantities.\nFirst note, that we use the Stein gradient estimator [35] to estimate the score function. This means especially that for a node \\(V_i\\) we get a vector"}, {"title": "C.3 Complexity", "content": ""}, {"title": "D Experimental details", "content": "In this section, we present the details of our experiments in terms of synthetic data generation and algorithms hyperparameters."}, {"title": "D.1 Synthetic data generation", "content": "In this work, we rely on synthetic data to benchmark AdaScore's finite samples performance. For each dataset, we first sample the ground truth graph and then generate the observations according to the causal graph."}, {"title": "D.2 AdaScore hyperparameters", "content": "For AdaScore, we set the a level for the required hypothesis testing at 0.05. For the CAM-pruning step, the level is instead set at 0.001, the default value of the dodidscover Python implementation of the method, and commonly found in all papers using CAM-pruning for edge selection [15, 16, 17, 36]. For the remaining parameters. The regression hyperparameters for the estimation of the residuals are found via cross-validation during inference: tuning is done minimizing the generalization error on the estimated residuals, without using the performance on the causal graph ground truth. Finally, for the score matching estimation, the regularization coefficients are set to 0.001."}, {"title": "D.3 Computer resources", "content": "All experiments have been run on an AWS EC2 instance of type p3.2xlarge. These machines contain Intel Xeon E5-2686-v4 processors with 2.3 GHz and 8 virtual cores as well as 61 GB RAM. All experiments can be run within a day."}, {"title": "E Additional Experiments", "content": "In this section, we provide additional experimental results. All synthetic data has been generated as described in Appendix D.1."}, {"title": "E.1 Non-additive mechanisms", "content": "In Figure 1 we have demonstrated the performance of our proposed method on data generated by linear SCMs and non-linear SCMs with additive noise. But Proposition 1 also holds for any faithful distribution generated by an acyclic model. Thus, we employed as mechanism a neural network-based approach similar to the non-linear mechanism described in Appendix D. Instead of adding the noise term, we feed it as additional input into the neural network. Results in this setting are reported in Figure 2. As neither AdaScore nor any of the baseline algorithms has theoretical guarantees for the orientation of edges in this scenario, we report the F\u2081-score (popular in classification problems) w.r.t. to the existence of an edge, regardless of orientation. Our experiments show that AdaScore can, in general, correctly recover the graph's skeleton in all the scenarios, with an F\u2081 score median between 1 and ~ 0.75, respectively for small and large numbers of nodes."}, {"title": "E.2 Sparse graphs", "content": "In this section, we present the experiments on sparse Erd\u00f6s-Renyi graphs where each pair of nodes is connected by an edge with probability 0.3. The results are illustrated in Figure 3. For sparse graphs, recovery results are similar to the dense case, with AdaScore generally providing comparable performance to the other methods."}, {"title": "E.3 Increasing number of samples", "content": "In the following series of plots we demonstrate the scaling behaviour of our method w.r.t. to the number of samples. Figure 5 shows results with edge probability 0.5 and Figure 4 with 0.3. All graphs contain seven observable nodes. As before we observe that AdaScore performs comparably to other methods. E.g. in Figures 4a and 5b we can see that the median error AdaScore improves with additional samples and in all plots we see that no other algorithm seems to gain an advantage over AdaScore with increasing sample size."}, {"title": "E.4 Limitations", "content": "In this section, we remark the limitations of our empirical study. It is well known that causal discovery lacks meaningful, multivariate benchmark datasets with known ground truth. For this reason, it is common to rely on synthetically generated datasets. We believe that results on synthetic graphs should be taken with care, as there is no strong reason to believe that they should mirror the benchmarked algorithms' behaviors in real-world settings, where often there is no prior knowledge about the structural causal model underlying available observations."}]}