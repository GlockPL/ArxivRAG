{"title": "Score matching through the roof: linear, nonlinear, and latent variables causal discovery", "authors": ["Francesco Montagna", "Philipp M. Faller", "Patrick Bl\u00f6baum", "Elke Kirschbaum", "Francesco Locatello"], "abstract": "Causal discovery from observational data holds great promise, but existing methods rely on strong assumptions about the underlying causal structure, often requiring full observability of all relevant variables. We tackle these challenges by leveraging the score function \u2207logp(X) of observed variables for causal discovery and propose the following contributions. First, we generalize the existing results of identifiability with the score to additive noise models with minimal requirements on the causal mechanisms. Second, we establish conditions for inferring causal relations from the score even in the presence of hidden variables; this result is two-faced: we demonstrate the score's potential as an alternative to conditional independence tests to infer the equivalence class of causal graphs with hidden variables, and we provide the necessary conditions for identifying direct causes in latent variable models. Building on these insights, we propose a flexible algorithm for causal discovery across linear, nonlinear, and latent variable models, which we empirically validate.", "sections": [{"title": "1 Introduction", "content": "The inference of causal effects from observations holds the potential for great impact arguably in any domain of science, where it is crucial to be able to answer interventional and counterfactual queries from observational data [1, 2, 3]. Existing causal discovery methods can be categorized based on the information they can extract from the data [4], and the assumptions they rely on. Traditional causal discovery methods (e.g. PC, GES [5, 6]) are general in their applicability but limited to the inference of an equivalence class. Additional assumptions on the structural equations generating effects from the cause are, in fact, imposed to ensure the identifiability of a causal order [7, 8, 9, 10]. As a consequence, existing methods for causal discovery require specialized and often untestable assumptions, preventing their application to real-world scenarios.\nFurther, the majority of existing approaches are hindered by the assumption that all relevant causes of the measured data are observed, which is necessary to interpret associations in the data as causal relationships. Despite the convenience of this hypothesis, it is often not met in practice, and the solu- tions relaxing this requirement face substantial limitations. The FCI algorithm [11] can only return an equivalence class from the data. Appealing to additional restrictions ensures the identifiability of some direct causal effects in the presence of latent variables: RCD [12] relies on the linear non-Gaussian additive noise model, whereas CAM-UV [13] requires nonlinear additive mechanisms. Nevertheless, the strict conditions on the structural equations hold back their applicability to more general settings."}, {"title": null, "content": "Our paper tackles these challenges and can be put in the context of a recent line of academic research that derives a connection between the score function \u2207 log p(X) and the causal graph underlying the data-generating process [14, 15, 16, 17, 18, 19]. The use of the score for causal discovery is practically appealing, as it yields advantages in terms of scalability to high dimensional graphs [16] and guarantees of finite sample complexity bounds [20]. Instead of imposing assumptions that ensure strong, though often impractical, theoretical guarantees, we organically demonstrate different levels of identifiability based on the strength of the modeling hypotheses, always relying on the score function to encode all the causal information in the data. Starting from results of Spantini et al. [21] and Lin [22], we show how constraints on the Jacobian of the score \u22072 log p(X) can be used as an alternative to conditional independence testing to identify the Markov equivalence class of causal models with hidden variables. Further, we prove that the score function identifies the causal direction of additive noise models, with minimal assumptions on the causal mechanisms. This extends the previous findings of Montagna et al. [17], limited by the assumption of nonlinearity of the causal effects, and Ghoshal and Honorio [14], limited to linear mechanisms. On these results, we build the main contributions of our work, enabling the identification of direct causal effects in hidden variables models.\nOur main contributions are as follows: (i) We present the necessary conditions for the identifiability of direct causal effects and the presence of hidden variables with the score in the case of latent variables models. (ii) We propose AdaScore (Adaptive Score-based causal discovery), a flexible algorithm for causal discovery based on score matching estimation of \u2207 log p(X) [23]. Based on the user's belief about the plausibility of several modeling assumptions on the data, AdaScore can output a Markov equivalence class, a directed acyclic graph, or a mixed graph, accounting for the presence of unobserved variables. To the best of our knowledge, the broad class of causal models handled by our method is unmatched by other approaches in the literature."}, {"title": "2 Model definition and related works", "content": "In this section, we introduce the formalism of structural causal models (SCMs), separately for the the cases with and without hidden variables."}, {"title": "2.1 Causal model with observed variables", "content": "Let X be a set of random variables in IR defined according to the set of structural equations\nXi := fi(XPA, Ni), \u2200i = 1,..., k. (1)\nNi \u2208 R are mutually independent random variables with strictly positive density, known as noise or error terms. The function fi is the causal mechanism mapping the set of direct causes XPAS of Xi and the noise term Ni, to Xi's value. A structural causal model (SCM) is defined as the tuple (X, N, F, PN), where F = (fi)ki=1 is the set of causal mechanisms, and PN is the joint distribution relative to the density pn over the noise terms N \u2208 Rk. We define the causal graph G as a directed acyclic graph (DAG) with nodes X = {X1, ..., Xk}, and the set of edges defined as {Xj \u2192 Xi : Xj \u2208 XPAS }, such that PA are the indices of the parent nodes of X\u00bf in the graph G. (In the remainder of the paper, we adopt the following notation: given a set of random variables Y = {Y1, ..., Yn } and a set of indices Z \u2286 N, then Yz = {Yi|i \u2208 Z, Y; \u2208 Y}.)\nUnder this model, the probability density of X satisfies the Markov factorization (e.g. Peters et al. [1] Proposition 6.31):\nk\np(x) = \u220fp(Xi|XPAi), (2)\ni=1\nwhere we adopt the convention of lowercase letters referring to realized random variables, and use p to denote the density of different random objects, when the distinction is clear from the argument. This factorization is equivalent to the global Markov condition (e.g. Peters et al. [1] Proposition 6.22) that demands that for all {Xi, Xj} \u2208 X, Xz \u2286 X \\ {Xi, Xj}, then\nXiXjXz\u21d2 Xi \\ Xj|Xz,\nwhere (\u00b7|\u00b7|\u00b7) denotes probabilistic conditional independence of Xi, Xj given Xz, and (\u00b7) is the notation for d-separation, a criterion of conditional independence defined on the graph G"}, {"title": null, "content": "(Definition 5 of the appendix). As it is commonly done, we assume that the reverse direction\nXiXjXz XXXjXz hold, and we say that the density p is faithful to the graph G\n[2, 24] (hence the faithfulness assumption). Together with the global Markov condition, faithfulness\nimplies an equivalence between the probabilistic and graphical notions of conditional independence:\nX\u00a1 \u00a6 XjXzXiXjXz. (3)\nIn general, several DAGs may entail the same set of d-separations: graphs sharing such common structure form a Markov equivalence class (see Definition 6 in the appendix).\nThe above model assumes that there aren't any unobserved causes of variables in X, other than the noise terms in N. As we are interested in distributions with potential hidden variables, we will now generalize our model to represent data-generating processes that may involve latent causes.\nDefinitions on graphs. As graphs play a central role in our work, Appendix A.1 provides a detailed overview of the fundamental notation and definitions that we rely on in the remainder of the paper. For the next section, we advise the reader to be comfortable with the notions of ancestors (Definition 2) and inducing paths (Definition 3) in DAGs.\nClosely related works. Several methods for the causal discovery of fully observable models using the score have been recently proposed. Ghoshal and Honorio [14] demonstrates the identifiability of the linear non-Gaussian model from the score, and it is complemented by Rolland et al. [15], which shows the connection between score matching estimation of \u2207 log p(X) and the inference of causal graphs underlying nonlinear additive noise models with Gaussian noise terms, also allowing for sample complexity bounds [20]. Montagna et al. [17] provides identifiability results in the nonlinear setting, without posing any restriction on the distribution of the noise terms. Montagna et al. [16] is the first to show that the Jacobian of the score provides information equivalent to conditional independence testing in the context of causal discovery, limited to the case of additive noise models. All of these studies make specialized assumptions to find theoretical guarantees of identifiability, whereas our paper provides a unifying view of causal discovery with the score function, which generalizes and expands the existing results."}, {"title": "2.2 Causal model with unobserved variables", "content": "Under the model (1), we consider the case where the set of variables X is partitioned into the disjoint subsets of observed random variables V = {V\u2081, . . ., Va} and unobserved (or latent) random variables U = {U\u2081,..., Up}. We assume that the following set of structural equations is satisfied:\nVi := fi(VPA, U\u00b2, Ni), Vi = 1,...,d, (4)\nwhere U\u017c stands for the set of unobserved parents of V\u2081, and VPAS = {Vk|k \u2208 PA, Vk \u2208 V} are the observed direct causes of Vi. Some of the causal relations and the conditional independencies implied by the set of equations (4) can be summarized in a graph obtained as a marginalization of the DAG G onto the observable nodes V.\nDefinition 1 (Marginal graph, Zhang [25]). Let X = V\u00d9U and G be a DAG over X. The following construction gives the marginal graph M\u1ef9, with nodes V and edges found as follows:\n\u2022 pair of nodes Vi, V; are adjacent in the graph Mv if and only if there is an inducing path between them relative to U in G;\n\u2022 for each pair of adjacent nodes V\u012b, V; in M\u2081, orient the edge as V\u2081 \u2192 Vj if V\u2081 is an ancestor of V; in G, else orient it as V\u00bf \u2194 Vj.\nWe define the map G \u2192 M\u2081 as the marginalization of the DAG G onto V, the observable nodes.\nThe graph resulting from the above construction is a maximal ancestral graph (MAG, Definition 4), hence we will often refer to it as the marginal MAG of G. Intuitively, a directed edge denotes the presence of an ancestorship relation, whereas bidirected edges represent dependencies that can not be removed by conditioning on any of the variables in the graph."}, {"title": null, "content": "In the case of DAGs, d-separation encodes the probabilistic conditional independence relations between the variables of X in the graph G, as explicit by Equation (3). Such notion of graphical sepa- ration has a natural generalization to maximal ancestral graphs, known as m-separation (Definition 5 of the appendix). Zhang [25] shows that m-separation and d-separation are in fact equivalent (see Lemma 1 of the appendix), such that given Vz CV and {Vi, V;} CV, the following holds:\nm\nm\nVV;|Vz \\ {V, V; }VqV;|Vz \\ {Vi, V;}, (5)\nwhere( \u00b7|\u00b7) denotes m-separation relative to the graph M. Just like with DAGs, MAGS that imply the same set of conditional independencies define an equivalence class. Usually, the common structure of these graphs is represented by partial ancestral graphs (PAGs, Definition 7 of the appendix). We use PM to denote the PAG relative to M.\nProblem definition. In this work, our goal is to provide theoretical guarantees for the identifiability of the Markov equivalence class of the marginal graph M\u1ef9 and its direct causal effects with the score, where variables Vi are defined according to Equation (4).\nWithout further assumptions on the data-generating process, we can identify the graph M\u1ef9 only up to its partial ancestral graph, as discussed in the next section.\nClosely related works. Causal discovery with latent variables have been first studied in the context of constraint-based approaches with the FCI algorithm [11], which shows the identifiability of the equivalence class of a marginalized graph via conditional independence testing. The RCD and CAM-UV [12, 13] approaches instead demonstrate the inferrability of directed causal edges via regression and residuals independence testing. Both methods rely on strong assumptions on the causal mechanisms: their theoretical guarantees apply to models where the effects are generated by a linear (RCD) or nonlinear (CAM-UV) additive contribution of each cause. Our work demonstrates that using the score function for causal discovery unifies and generalizes these results, presenting an alternative to conditional independence testing for constraint-based methods, and being agnostic about the class of causal mechanisms of the observed variables, under the weaker requirement of additivity of the noise terms."}, {"title": "3 Theory for a score-based test of separation", "content": "In this section, we show that for V C X generated according to Equation (4) the Hessian matrix of log p(V) identifies the equivalence class of the marginal MAG M. It has already been proven that cross-partial derivatives of the log-likelihood are informative about a set of conditional independence relationships between random variables: Spantini et al. [21] (Lemma 4.1) shows that, given Vz \u2286 X such that {Vi, V;} \u2286 Vz, then\n\u22022\nlog p(Vz) = 0VV;|Vz \\ {Vi, V;}. (6)\nEquation (3) resulting from faithfulness and the directed global Markov property immediately implies that this expression can be used as a test of conditional independence to identify the Markov equivalence class of the graph Mv, as commonly done in constraint-based causal discovery (for reference, see e.g. Section 3 in Glymour et al. [4]). This result generalizes Lemma 1 of Montagna et al. [16], where it is used to define constraints to infer edges in the causal structure without latent variables.\nProposition 1 (Adapted\u00b2 from [21]). Let V be a set of random variables with strictly positive density\ngenerated according to model (4). For each set Vz \u2286 V of nodes in My such that {Vi, V;} \u2286 Vz,"}, {"title": null, "content": "the following holds for each supported value vz:\n\u22022\nlog p(vz)\n= 0V qV;|Vz \\ {Vi, V; }.\nThe result of Proposition 1 presents an alternative to conditional independence testing in constraint- based approaches to causal discovery, showing that the equivalence class of the graph My can be identified using the cross partial derivatives of the log-likelihood as a test of conditional independence between variables, much in the spirit of the Fast Causal Inference algorithm [11]. Identifying the Markov equivalence class is the most we can hope to achieve without further hypotheses. As we will see in the next section, the score function can also help leverage additional restrictive assumptions on the causal mechanisms of Equation (4) to identify direct causal effects."}, {"title": "4 A theory of identifiability from the score", "content": "In this section, we show that, under additional assumptions on the data-generating process, we can identify the direct causal relations that are not influenced by unobserved variables, as well as the presence of unobserved active paths (Definition 5) between nodes in the marginalized graph M.\nAs a preliminary step before diving into causal discovery with latent variables, we show how the properties of the score function identify edges in directed acyclic graphs, that is in the absence of latent variables (when U = \u00d8 and G = Mv). The goal of the next section is two-sided: first, it introduces the fundamental ideas connecting the score function to causal discovery that also apply to hidden variable models, second, it extends the existing theory of causal discovery with score matching to additive noise models with both linear and nonlinear mechanisms."}, {"title": "4.1 Warm up: identifiability without latent confounders", "content": "In this section, we summarise and extend the theoretical findings presented in Montagna et al. [17], where the authors show how to derive constraints on the score function that identify the causal order of the DAG G where all the variables in the set X are observed. Define the structural relations of (1) as:\nXi := hi(XPA) + Ni, i = 1,..., k, (7)\nwith three times continuously differentiable mechanisms hi, noise terms centered at zero, and strictly positive density px. Given the Markov factorization of Equation (2), the components of the score function log p(x) are:\n\u03a3\n\u2202X, log p(x) = \u2202Xi log p(Xi|XPAi) + \u2202Xi log p(Xj|XPA))\n\u03a3\n\u2202Xi log p(ni) \u2212 \u2202Xihj (XPA, )\u2202N, log p(nj), (8)\nwhere CH denotes the set of children of node X\u2081. We observe that if a node X, is a sink, i.e. a\nnode satisfying CH = 0, then the summation over the children vanishes, implying that:\n\u2202X, log p(x) = \u2202N, log p(ns). (9)\nThe key point is that the score component of a sink node is a function of its structural equation noise term, such that one could learn a consistent estimator of dx, log px from a set of observations of the noise term Ns. Given that, in general, one has access to X samples rather than observations of the noise random variables, authors in Montagna et al. [17] show that Ng of a sink node can be consistently estimated from i.i.d. realizations of X. For each node X1, ..., Xk, we define the quantity:\nRi := Xi \u2212 E[Xi|X\\Xi ], (10)\nwhere X\\Xi are the random variables in the set X \\ {Xi}. E[Xi|X\\Xi ] is the optimal least squares predictor of X\u2081 from all the remaining nodes in the graph, and Ri is the regression residual. For a sink node X, the residual satisfies:\nRs = Ns, (11)"}, {"title": null, "content": "which can be seen by rewriting E[X|X\\Xs ] = hs(XPA) + E[NS|XDES, XNDS ] =\nhs(XPA) + E[N], where XDES and XND. denotes the descendants and non-descendants of Xs,\nrespectively. Equations (9) and (11) together imply that the score d\u2116\u2116 log p(Ns) is a function of Rs,\nsuch that it is possible to find a consistent approximator of the score of a sink from observations of Rs.\nProposition 2 (Generalization of Lemma 1 in Montagna et al. [17]). Let X be a set of random\nvariables, generated by a restricted additive noise model (Definition 9) with structural equations (7),\nand let X; \u2208 X. Consider rj in the support of Rj. Then:\n2\nlog p(X))2] =r\n[(E [dx, log(X) | Rj = r;] \u2013 dx, (12)\nX; is a sink EE\nOur result generalizes Lemma 1 in Montagna et al. [17], as they assume X generated by an\nidentifiable additive noise model with nonlinear mechanisms. Instead, we remove the nonlinearity\nassumption and make the weaker hypothesis of a restricted additive noise model, which is provably\nidentifiable [9], in the formal sense defined in the appendix (Definition 8). This result doesn't come\nas a surprise, given the previous findings of Ghoshal and Honorio [14] showing that the score infers\nlinear non-Gaussian additive noise models: Proposition 2 provides a unifying and general theory\nfor the identifiability of models with potentially mixed linear and nonlinear mechanisms.\nBased on these insights, Montagna et al. [17] propose the NoGAM algorithm to exploit the con-\ndition in (12) for identifying the causal order of the graph: being E [dx, log p(X) | Ri] the opti-\nmal least squares estimator of the score of node Xi from Ri, a sink node is characterized as the\nargmini E [E [dx, log p(X) | Ri] \u2013 dx, log p(X)]2, where in practice the residuals Ri, the score\ncomponents and the least squares estimators are replaced by their empirical counterparts. After a\nsink node is identified, it is removed from the graph and assigned a position in the order, and the\nprocedure is iteratively repeated up to the source nodes. Being the score estimated by score matching\ntechniques [23], we usually make reference to score matching-based causal discovery.\nIn the next section, we show how we can generalize these results to identify direct causal effects\nbetween a pair of variables in the marginal MAG M when U \u2260 \u00d8"}, {"title": "4.2 Identifiability in the presence of latent confounders", "content": "We now introduce the last of our main theoretical results, that is: given a pair of nodes Vi, Vj that are adjacent in the graph M\u2081 with U \u2260 0, we can use the score function to identify the presence of a direct causal effect between Vi and Vj, or that of an active path that is influenced by unobserved variables. Given that the causal model of Equation (4) ensures identifiability only up to the equivalence class, we need additional restrictive assumptions. In particular, we enforce an additive noise model with respect to both the observed and unobserved noise variables. This corresponds to an additive noise model on the observed variables with the noise terms recentered by the latent causal effects.\nAssumption 1 (SCM assumptions). The set of structural equations of the observable variables specified in (4) is now defined as:\nVi := fi(VPA) + gi(U\u00b2) + Ni, \u2200i = 1, ..., d, (13)\nassuming the mechanisms fi to be of class C3(RVPA) and mutually independent noise terms with\nstrictly positive density function. The Ni's are assumed to be non-Gaussian when fi is linear in some\nof its arguments.\nCrucially, our hypothesis is weaker than those required by two state-of-the-art approaches, CAM-UV\n[13] and RCD [12]: CAM-UV assumes a Causal Additive Model (CAM) with structural equations\nwith nonlinear mechanisms in the form Vi := \u2211k\u2208PA& fik(Vk) + \u2211u gik(U) + N\u017c, and RCD\nrequires an additive noise model with linear effects of both the latent and observed causes. Thus,\nour model encompasses and extends the nonlinear and linear settings of CAM-UV and RCD, such\nthat the theory developed in the remainder of the section is valid for a broader class of causal models.\nOur first step is rewriting the structural relations in (13) as:\nVi := fi(VPA) + Ni,\n\u00d1i := gi(U\u00b2) + Ni, \u2200i = 1, . . ., d, (14)"}, {"title": null, "content": "which provides an additive noise model in the form of (7). Next, we define the following regression residuals for any node Vk in the graph M:\nRk(Vz) := Vk \u2013 E[Vk | Vz\\{k}], (15)\nwhere Vz\\{k} denotes the set of random variables Vz \\ {Vk}.\nGiven these definitions, we are ready to show how directed edges, and the presence of unobserved variables can be identified from the score of linear and nonlinear additive noise models."}, {"title": "4.2.1 Identifiability of directed edges", "content": "Consider V\u2081, V; adjacent nodes in the PAG PM: we want to investigate when a direct causal effect Vi \u2208 VPA can be identified from the score. We make the following observations: for\nVz = VPA \u222a {V;} and VPA | Ui, by Equation (15) it follows\nR;(Vz) = \u00d1; \u2013 E[\u00d1;], (16)\nwhere we use V VPASU to write E[\u00d1;|Vz\\{j}] = E[\u00d1;]. Moreover, we note that Vj is a sink node relative to M, the marginalization of G onto Vz. In analogy to the case without latent variables, we can show that dv, log p(Vz) is a function of \u00d1;, the error term in the additive noise model of Equation (14), such that the score of Vj can be consistently predicted from observations of the residual Rj (Vz).\nProposition 3. Let X be generated by a restricted additive noise model with structural equations (7), and causal graph G. Consider Vi, Vj adjacent in M\u2081, marginalization of G. Further, assume that the score component dv; log p(Vz) is not constant for uncountable values of Vz.\n(i) Let Vz = VPA \u222a {Vi, Vj}, and rj \u2208 R in the support of Rj(Vz). Then:\n\u2200PAUA VE VPAS\n2\nE[dv; log p(Vz) \u2013 E[dv; log p(Vz)|Rj (Vz) = rj]]\u00b2 = 0.\n(ii) Let Vz \u2286 V, such that {Vi, V;} \u2286 Vz. Then:\nVPAS U V V & VPA\n2\nE[dv; log p(Vz) \u2013 E[dv; log p(Vz)|R;(Vz) = r;]]\u00b2 \u2260 0.\nIntuitively, the proposition has two essential implications. Part (i) provides the condition for the\nidentifiability of the potential direct causal effect between a pair Vi, V\u2081, that is, when the association\nbetween Vj and its observed parents is not influenced by active paths that involve latent variables.\nThis condition is necessary: given an active path such that VPA\nUi, the score could not identify\na direct causal effect Vi \u2192 Vj, which is the content of the second part of the proposition.\nWe have established theoretical guarantees of identifiability for linear and nonlinear additive noise models, even in the presence of hidden variables: we find that the score function is a means for the identifiability of all direct parental relations that are not influenced by unobserved variables; all the remaining arrowheads of the edges in the graph My are identified no better than in the equivalence class. Based on these insights, we propose AdaScore, a score matching-based algorithm for the inference of Markov equivalence classes, direct causal effects, and the presence of latent variables."}, {"title": "4.3 A score-based algorithm for causal discovery", "content": "Building on our theory, we propose AdaScore, a generalization of NoGAM to linear and nonlinear additive noise models with latent variables. The main strength of our approach is its adaptivity with respect to structural assumptions: based on the user's belief about the plausibility of several modeling assumptions on the data, AdaScore can output an equivalence class (using the condition of Proposition 1 instead of conditional independence testing in an FCI-like algorithm), a directed acyclic graph (as in NoGAM), or a mixed graph, accounting for the presence of unobserved variables.\nWe now describe the version of our algorithm whose output is a mixed graph, where we rely on score matching estimation of the score and its Jacobian (Appendix C.2). At an intuitive level, we find unoriented edges using Proposition 1, i.e. checking for dependencies in the form of non-zero entries"}, {"title": "5 Experiments", "content": "We use the causally\u00b3 Python library [26] to generate synthetic data with known ground truths, created as Erd\u00f6s-R\u00e9nyi sparse and dense graphs, respectively with probability of edge between pair of nodes equals 0.3 and 0.5. We sample the data according to linear and nonlinear mechanisms with additive noise, where the nonlinear functions are parametrized by a neural network with random weights, a common approach in the literature [18, 26, 27, 28, 29]. Noise terms are sampled from a uniform distribution in the [-2, 2] range. Hidden causal effects are obtained by randomly picking two nodes and dropping the corresponding column from the data matrix. See Appendix D.1 for further details on the data generation. As metric, we consider the structural Hamming distance (SHD) [30, 31], a simple count of the number of incorrect edges, where missing and wrongly directed edges count as one error. We fix the level of the hypothesis tests of AdaScore to 0.05, which is a common choice in the absence of prior knowledge. We compare AdaScore to NoGAM, CAM-UV, RCD, and DirectLiNGAM, whose assumptions are detailed in Table 1. In the main manuscript, we comment on the results on datasets of 1000 observations from dense graphs, with and without latent variables. Additional experiments including those on sparse networks are presented in Appendix E. Our synthetic data are standardized by their empirical variance to remove shortcuts in the data [18, 32].\nDiscussion. Our experimental results on models without latent variables of Figure 1a show that when causal relations are linear, AdaScore can recover the causal graph with accuracy that is comparable with all the other benchmarks, with the exception of DirectLiNGAM. On nonlinear data AdaScore presents better performance than CAM-UV, RCD, and DirectLiNGAM while being comparable to NoGAM in accuracy. This is in line with our expectations: in the absence of finite sample"}, {"title": "6 Conclusion", "content": "The existing literature on causal discovery shows a connection between score matching and structure learning in the context of nonlinear ANMs: in this paper, (i) we formalize and extend these results to linear SCMs, and (ii) we show that the score retains information on the causal structure even in the"}, {"title": "A Useful results", "content": "In this section, we provide a collection of results and definitions relevant to the theory of this paper."}, {"title": "A.1 Definitions over graphs", "content": "Let X = X1, ..., Xa a set of random variables. A graph G = (X, E) consists of finitely many nodes or vertices X and edges E. We now provide additional definitions, separately for directed acyclic and mixed graphs.\nDirected acyclic graph. In a directed graph, nodes can be connected by a directed edge (\u2192), and between each pair of nodes there is at most one directed edge. We say that X\u2081 is a parent of X; if X\u2081 \u2192 X\u2081 \u2208 E, in which case we also say that X; is a child of X\u00bf. Two nodes are adjacent if they are connected by an edge. Three nodes are called a v-structure if one node is a child of the other two, e.g. as Xi \u2192 Xk \u2190 Xj is a collider. A path in G is a sequence of at least two distinct vertices Xi1,..., Xim such that there is an edge between Xik and Xik+1. If Xik \u2192 Xik+1 for every node in the path, we speak of a directed path, and call Xin an ancestor of Xik+1, Xik+1 a descendant of Xi. Given the set DE of descendants of a node X\u2081, we define the set of non-descendants of X\u2081 as ND = X \\ (DE\u222a{X}). A node without parents is called a source node. A node without children is called a sink node. A directed acyclic graph is a directed graph with no cycles.\nMixed graph. In a mixed graph nodes can be connected by a directed edge (\u2192) or a bidirected edge (+), and between each pair of nodes there is at most one directed edge. Two vertices are said to be adjacent in a graph if there is an edge (of any kind) between them. The definitions of parent, child, ancestor, descendant, path provided for directed acyclic graph also apply in the case of mixed graphs. Additionally, X\u2081 is a spouse of Xj (and vice-versa) if X\u00bf \u2194 Xj \u2208 E. An almost directed cycle occurs when Xi \u2194 X; \u2208 E and X\u2081 is an ancestor of X; in G.\nFor ease of reference from the main text, we separately provide the definition of inducing paths and ancestors in directed acyclic graphs.\nDefinition 2 (Ancestor). Consider a DAG G with set of nodes X, and Xi, X; elements of X. We say that X is an ancestor of X; if there is a directed path from X\u2081 to X; in the graph, as in Xi... Xj.\nDefinition 3 (Inducing path). Consider a DAG G with set of nodes X, and Y, Z disjoint subsets such that X = Y\u00dcZ. We say that there is an inducing path relative to Z between the nodes Y\u00bf, Y; if every node on the path that is not in ZU {Y\u2081, Y; } is a collider on the path (i.e. for each Yk \u2208 Y on the path the sequence Y \u2026 \u2026 \u2026 \u2192 Yk \u2190 ... Y; appears) and every collider on the path is an ancestor of Y\u00bf or Yj.\nOne natural way to encode inducing paths and ancestral relationships between variables is represented by maximal ancestral graphs.\nDefinition 4 (MAG). A maximal ancestral graph (MAG) is a mixed graph such that:\n1. there are no directed cycles and no almost directed cycles;\n2. there are no inducing paths between two non-adjacent nodes.\nNext, we define conditional independence in the context of graphs.\nDefinition 5 (m-separation). Let M be a mixed graph with nodes X. A path \u03c0 in M between Xi, Xj elements of X is active w.r.t. Z \u2286 X \\ {Xi, Xj} if:\n1. every non-collider on \u03c0 is not in Z\n2. every collider on \u03c0 is an ancestors of a node in Z.\nX\u2081 and X are said to be m-separated by Z if there is no active path between X\u2081 and X; relative to Z. Two disjoint sets of variables W and Y are m-separated by Z if every variable in W is m-separated from every variable in Y by Z.\nIf m-separation is applied to DAGs, it is called d-separation."}, {"title": null, "content": "The set of directed acyclic graphs that satisfy the same set of conditional independencies form an equivalence class, known as the Markov equivalence class.\nDefinition 6 (Markov equivalence class of a DAG). Let G be a DAG with nodes X. We denote with [G] the Markov equivalence class of G. A DAG G with nodes X is in [G] if the following conditions are satisfied for each pair Xi, X, of distinct nodes in X:\n\u2022 there is an edge between Xi, Xj in G if and only if there is an edge between X\u00ed, Xj in G;\n\u2022 let Z \u2286 X \\ {X\u2081, X;}. Then Xi || X;|Z \u2194 XIX;|Z;\n\u2022 let \u03c0 be a path between X\u2081 and Xj. Xk is a collider in the path \u03c0 in G if and only if it is a collider in the path \u03c0 in G.\nIn summary, graphs in the same equivalence class share the edges up to direction, the set of d- separations, and the set of colliders.\nJust as for DAGs, there may be several MAGs that imply the same conditional independence statements. Denote the Markov-equivalence class of a MAG M with [M]: this is represented by a partial mixed graph, the class of graphs that can contain four kinds of edges: \u2192, \u2194, \u0970\u2014\u25cb and o\u2192, and hence three kinds of end marks for edges: arrowhead (>), tail (-) and circle (0).\\nDefinition 7 (PAG, Definition 3 of Zhang [25]). Let [M] be the Markov equivalence class of an arbitrary MAG M . The partial ancestral graph (PAG) for [M], \u0420\u043c, is a partial mixed graph such that:\n\u2022 PM has the same adjacencies as M (and any member of [M]) does;\n\u2022 A mark of arrowhead is in PM if and only if it is shared by all MAGs in [M]; and\n\u2022 A mark of tail is in PM if and only if it is shared by all MAGs in [M].\nIntuitively, a PAG represents an equivalence class of MAGs by displaying all common edge marks shared by all members of the class and displaying circles for those marks that are not in common."}, {"title": "A.2 Equivalence between m-separation and d-separation", "content": "In this section, we provide a proof for equation (5), stating the equivalence between m-separation and d-separation in a formal sense.\nLemma 1 (Adapted from Zhang [25]). Let G be a DAG with nodes X = V UU, with V and U disjoint sets, and Me the marginalization of G onto V. For any {Vi, Vj} \u2208 V and Vz \u2286 V\\{V, V; }, the following equivalence holds:\nViV; Vz ViVi Vz.\nProof. The implication Vi V; Vz \u2192 Vi V; Vz is a direct consequence of Lemma 18 from Spirtes and Richardson [34], where we set S = \u00d8, since we do not consider selection bias. The implication Vi V; Vz follows from Lemma 17 by Spirtes and Richardson [34], again with S = 0. Note, that in their terminology \u201cd-separation in MAGs\u201d is what we call m-separation."}, {"title": "A.3 Additive noise model identifiability", "content": "We study the identifiability of the additive noise model, reporting results from Peters et al. [9]. We start with a formal definition of identifiability in the context of causal discovery.\nDefinition 8 (Identifiable causal model). Let (X, N, F, PN) be an SCM with underlying graph G and px joint density function of the variables of X. We say that the model is identifiable from observa- tional data if the distribution px can not be generated by a structural causal model with graph G \u2260 G.\nFirst, we consider the case of models of two random variables\nX2 := f(X1) + N, X1 \u22a5 N. (17)"}, {"title": null, "content": "Condition 1 (Condition 19 of Peters et al. [9]). Consider an additive noise model with structural equations (17). The triple (f, px\u2081, PN) does not solve the following differential equation for all pairs X1,X2 with f'(x2)\u03bd\" (x2 \u2212 f(x1)) \u2260 0:\n\u03be''\n''\n2v'' f'' f'\nv'''v' f'' f' v' (f'')2\n2\u03bd\u03bd,\n\u03bd\u03bd.\u03bd, \u03bd . ,\u22122\n= (-+\nHere, \u00a7 := log px\u2081, v := log pn, the logarithms of the strictly positive densities. The arguments f(x1), x1, and x\u2081 of v, \u00a7 and f respectively, have been removed to improve readability.\nNext, we show that a structural causal model satisfying Condition 1 is identifiable, as in Definition 8\nTheorem 1 (Theorem 20 of Peters et al. [9]). Let PX1,X2 the joint distribution of a pair of random variables generated according to the model of equation (17) that satisfies Condition 1, with graph G. Then, G is identifiable from the joint distribution.\nFinally, we show an important fact, holding for identifiable bivariate models, which is that the score \u2202X1 log p(x1, x2) is nonlinear in x1.\nLemma 2 (Sufficient variability of the score). Let PX1,X2 the joint distribution of a pair of random variables generated according to a structural causal model that satisfies Condition 1, with graph G. Then:\n\u2202\n\u2202X1\n(\u03be' (x1) \u2212 f'(x1)\u03bd' (x2 \u2212 f(x1))) \u2260 0,\nfor all pairs (x1,x2).\nProof. By contradiction, assume that there exists (x1, x2) such that\n\u2202\n(\u03be'(x1) \u2212 f'(x1)\u03bd' (x2 \u2212\nf(x1))) = 0. Then:\n\u2202X1\n\u22022\n\u22022\n\u042d\u0425,\u041f(\u04251, X2)\n=0,\nwhere \u03c0(x1, x2) = log p(x1, x2). By explicitly computing all the partial derivatives of the above equation, we obtain that equation 18 is satisfied, which violates Condition 1.\nThese results guaranteeing the identifiability of the bivariate additive noise model can be generalized to the multivariable case, with a set of random variables X = {X1, ..., Xk} that satisfy:\nXi := fi(XPA) + Ni, i = 1,..., k, (19)\nwhere G is the resulting causal graph directed and acyclic. The intuition is that, rather than studying the multivariate model as a whole, we need to ensure that Condition 1 is satisfied for each pair of nodes, adding restrictions on their marginal conditional distribution.\nDefinition 9 (Definition 27 of Peters et al. [9]). Consider an additive noise model with structural equations (19). We call this SCM a restricted additive noise model if for all Xj \u2208 X, Xi \u2208 XPA, and all sets Xs \u2286 X, S CN, with XPA \\ {X} \u2286 Xs \u2286 X&D, \\ {Xi, Xj}, there is a value xs with p(xs) > 0, such that the triplet\n(fj(XPA\\{i}, \u00b7),PX;|Xs=xs,PN;)\nsatisfies Condition 1. Here, fj(XPA\\{i}, \u00b7) denotes the mechanism function xi \u2192 fj(XPA).\nAdditionally, we require the noise variables to have positive densities and the functions f; to be continuous and three times continuously differentiable.\nThen, for a restricted additive noise model, we can identify the graph from the distribution.\nTheorem 2 (Theorem 28 of Peters et al. [9]). Let X be generated by a restricted additive noise model with graph G, and assume that the causal mechanisms fj are not constant in any of the input arguments, i.e. for Xi \u2208 Xpas, there exist xi \u2260 x'; such that fj(XPA\\{i}, Xi) \u2260 fj (XPA\\{i}, X). Then, G is identifiable."}, {"title": "A.4 Other auxiliary results", "content": "We state several results that hold for a pair of random variables that are not connected by an active path that includes unobserved variables (active paths are introduced in Definition 5). For the remainder of the section", "Vz\\{j}": "Vz \\ Vj. Then", "values": "nlog p(vz) = log p(vj|UPAS) + log Q(vz)", "as": "np(vz) = \u2211p(vz|u)p(u)\nu (20)\n= \u2211p(u)p(vzj |u", "p(vz\\{j}|uZ\\{j}).\nThen": "np(vz) = \u2211 p (u2;) p (uz\\{j})p(vz;|uzi, VPA2, )P(vz\\{j} |uZ\\{j})\nu\n= p(2) p(vz,, u2, , PA) \u2211 p (uz\\{j})p(vz\\{j}/uZ\\{j})"}]}