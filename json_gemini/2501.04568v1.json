{"title": "Supervision-free Vision-Language Alignment", "authors": ["Giorgio Giannone", "Ruoteng Li", "Qianli Feng", "Evgeny Perevodchikov", "Rui Chen", "Aleix M Martinez"], "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but their performance is often constrained by the need for extensive, high-quality image-text training data. Curation of these image-text pairs is both time-consuming and computationally expensive. To address this challenge, we introduce SVP (Supervision-free Visual Projection), a novel framework that enhances vision-language alignment without relying on curated data or preference annotation. SVP leverages self-captioning and a pre-trained grounding model as a feedback mechanism to elicit latent information in VLMs. We evaluate our approach across six key areas: captioning, referring, visual question answering, multitasking, hallucination control, and object recall. Results demonstrate significant improvements, including a 14% average improvement in captioning tasks, up to 12% increase in object recall, and substantial reduction in hallucination rates. Notably, a small VLM using SVP achieves hallucination reductions comparable to a model five times larger, while a VLM with initially poor referring capabilities more than doubles its performance, approaching parity with a model twice its size.", "sections": [{"title": "1. Introduction", "content": "Vision-Language Models (VLMs [12, 120]) are essential to deploying expert level artificial intelligence, as human intelligence is predominantly multimodal.\nGenerative VLMs [17, 52, 53, 111] built upon Large Language Models (LLMs) have shown great promises in zero-shot abilities on various downstream vision-linguistic tasks (Fig. 2.(iv)), unlocking new multimodal capacities and providing powerful generalization to specialized machine learning models. By learning a mapping between linguistic tokens and visual features, such VLMs enjoy the strong generation capabilities of LLMs [14, 98] and the understanding of the physical world of computer vision models [25, 77].\nHowever, VLMs derived from pretrained backbones are known to be impacted by the hallucinations and biases from LLMs [79, 82]. It is frequently observed that these VLMs fail to produce text consistent with the visual content (left side Fig. 1), i.e., the generated text describes entities not present in the input image or misses relevant entities altogether, generating content not grounded in the visual input [7, 21]. Addressing these shortcomings is crucial for future deployment of VLMs in high-stakes, real-world applications across the frontiers of scientific discovery [35] and engineering [74, 91].\nResearchers have explored various approaches to solve the above problem in VLMs (bottom Fig. 1). Most of these works focus on fine-tuning VLMs with supervised (carefully curated) data to improve grounding [11, 73, 113, 115, 122] and vision-language alignment [60, 93]. Unfortunately, this data approach tends to be costly and sample-inefficient, requiring large amounts of image-text annotations even for small models to resolve the above stated problem [115]. Preference-based post-training methods [20, 69, 78] as another popular approach align VLM outputs with visual inputs [93, 125] but require curated preference pairs [26, 93]. And, test-time approaches [26, 51, 102, 112] improve grounding without architectural changes, yet their computational demands and model-specific heuristics limit broad applicability.\nTo address the significant challenges posed by the extensive data and annotation requirements of modern VLMs, we propose to leverage external feedback to enhance the alignment between visual and linguistic modalities in a task-agnostic manner (right side Fig. 1).\nDrawing inspiration from human learning, we propose to emulate the way humans efficiently align sensory experiences with language by grounding new information in tangible visual examples leveraging feedback [34, 95, 96]. We hypothesize that spatial and positional reasoning is the key for connecting the low-level visual elements and high-level linguistic representations [68, 72, 99], and that an external visual grounding model [63], agnostic to the VLM's shortcomings, can be used as feedback to extract latent information in the models.\nSpecifically, in this work, we introduce SVP (Supervision-free Visual Projection, Fig. 2), an algorithm founded on two core principles: self-improvement and grounding feedback. The self-improvement approach [5, 32, 117] utilizes the model's own outputs to enhance its performance. And, the grounding feedback provides the VLM with a mechanism to improve its output and select informative samples. Our goal is not to directly build a specialist grounding model, but to leverage grounding as feedback to elicit latent information in the model, with the aim of better aligning language and visual representations without the need of costly image-text annotations [73, 93], preference data [69, 78], or multi-step inference workflows [102, 112].\nSVP is a three-step process:\n(i) Inner-Loop Sampling: A base VLM generates detailed and comprehensive image descriptions. These descriptions are then processed by a small pre-trained grounding model [63]. The resulting spatially enriched grounding output serves as feedback, conditioning the same VLM to generate text tokens that better align with the visual"}, {"title": "3. Method", "content": "3.1. Preliminaries\nNotation We use $p(x|c)$ and $p(z|c)$ to denote auto-regressive distributions for simplicity. Here, c is the conditioning information (image and prompt inputs), z is a model sample or visual projection using grounding feedback, and x is the model's task-specific output. These distributions are shorthand for: $p(x|c) = p(x_T|c) \\Pi_{t=1}^{T} p(x_{t-1}|x_t, c)$. Similarly for $p(z|c)$. In $p(x|z, c)$, z represents sampling trajectories $z_{1:T}$. The conditioning information c is typically a given input to the model.\nGiven a context c = ($c_v$, $c_t$), where $c_v$ is the visual input and $c_t$ is a textual prompt, we write a Visual Projection (VP) as $p(z|c)$, where z is a comprehensive language-based representation for the visual input. When conditioning on grounding information g, we write a grounded VP as $q(z|c,g)$. The conditional entropy is defined as $H[z|c] = - \\Sigma_z p(z|c) log p(z|c)$.\nVision-Language Models Generative VLMs are versatile multimodal models that process text and images together. LLaVA-like models (Fig. 3) comprise a visual encoder $v_{\\theta}(c_v)$, a text encoder $t_{\\theta}(c_t)$, an adapter $a_{\\theta}$ aligning visual output to text, and a large language model. The model $p_{\\theta}$ uses the conditioning c to generate token trajectories z, which are then decoded for various tasks. These models undergo multi-phase training: multimodal pre-training, visual-"}, {"title": "3.2. Supervision-free Visual Projection", "content": "The core idea behind SVP is to generate a task-agnostic language-based representation z, referred as Visual Projection (VP), for the visual input c, utilizing sampling and grounding (Fig. 2.(i)). VPs function as latent variables or generalized captions (Fig. 1). SVP aims to refine these VPs through self-improving iterative methods, drawing inspiration from techniques designed to elicit reasoning in language models [36, 105, 117, 118], and posterior inference in latent variable models [43] (See Appendix E and Appendix G for more details). The goal of SVP is to strengthen the alignment between vision and language modalities, thereby improving the performance of the base VLM across a wide range of tasks.\nGrounding as Posterior Inference VLMs are built using discrete tokens [100]. We assume the VPs are tokenized following a multinomial distribution, and re-write:\n$p_{\\theta}(x|c) = \\sum_{Z} p_{\\theta}(x|z, c) p_{\\theta} (z|c)$,\nwhere c is the conditioning, z is a sample from the model $p_{\\theta}$ with trainable parameters $\\theta$, and x is the model output.\nOur goal is to provide feedback to the model to better ground the language output in the visual input. A simple option is to define a posterior q(z|c, g), where g is the external feedback, for example provided by a small pre-trained grounding model g(z, $c_v$) (Fig. 4). Introducing posterior inference for Eq. (1), we can define a variational lower-bound [43, 48] of the form:\n$E_{q(z|c,g)} [log p_{\\theta} (x, z|c) \u2013 log q(z|c, g)]$.\nFor many practical applications, we can approximate $p_{\\theta}(x|z, c)$ with a deterministic decoder, d(z, c).\nThe bound defined in Eq. (2), while challenging to train for a VLM/LLM [36, 66], is straightforward to evaluate [49]. This insight reveals a promising approach to enhance VLM sampling: utilizing the bound to construct a scoring mechanism that effectively leverage grounding signal (Fig. 2.(i)).\nSampling For a given input c, the process involves executing the posterior q(z|c, g) with grounding feedback g and generating logits for each element v in the vocabulary V and each generated token t in the sequence T. Concurrently, the base model is run to perform the same computations over c, obtaining the prior $p_{\\theta}(z|c)$. By comparing the posterior and prior distributions, we can quantify the grounding effect, providing insights into how visual context influences the model's predictions.\nScoring Following such intuition, we build a scoring of the form (Fig. 2.(ii)):\n$S[q, p] = \\sum_{t} \\sum_{\\upsilon} S_{\\upsilon,t}[q,p]$,\n$S_{\\upsilon,t}[q,p] = w_{\\upsilon,t} [log q_{\\upsilon,t} \u2013 log p_{\\theta \\upsilon,t}]$,\nwhere $Y_t \\in [0, 1]$ accounts for sequence length and $w_{\\upsilon,t}$ is a re-weighting factor (Fig. 5 and Appendix E). Image-text pairs (c, z) with high score based on Eq. (3) are collected and used to fine-tune the base model (Fig. 2). The underlying assumption is that, within a specific model, substantial discrepancies between q and p are primarily attributed to the grounding conditioning g. These discrepancies are expected to provide the most valuable information for the base VLM (Fig. 5).\nLow values of S[q, p] indicate token trajectories that are known by the base model p, offering little learning potential. Conversely, high values of S[q, p] signify trajectories that deviate from the base model's expectations across the vocabulary. These \"surprising\" trajectories are likely to provide valuable feedback and novel information [15, 37, 71], enhancing the vision-language alignment in the base VLM.\nIt is crucial to note that generic, unconditional surprise - samples rich in information from an entropy perspective [87] - does not inherently enhance a VLM's performance. Our working hypothesis posits that if the grounding conditioning is truly informative, surprising instances will contribute to the model's learning process statistically. This assumption underpins our strategy of utilizing grounding and scoring mechanisms to improve the model."}, {"title": "4. Experiments", "content": "Seed Images and Models We utilize a pre-trained grounding model [63] to provide the external feedback signals. For our core experiments, we randomly sampled a subset of C = 1000 natural images from the COCO2014 training set [59]. We conduct a comprehensive comparison against various baselines, including models fine-tuned with self-captioning without grounding and preference-based adaptation methods. Our evaluation encompasses a wide range of model scales (.5, 7, 8, 13, 19, 40 billion parameters), architectures (LLaVA-1.5 [60], LLaVA-1.6 [61], LLaVA-OV [52], VILA [58], InternVL [18]), visual encoders (CLIP [77], SigLIP [119], ViT [25]), language encoders (Vicuna [19], Mistral [42], Qwen2 [109], Yi-2 [114]), and scoring mechanisms S(q, p) and \u0394(q, p).\nImplementation Details We propose two variants of our approach: SVP (C) and SVP (CVQ). The former leverages only grounded self-generated captions, while the latter incor-"}, {"title": "4.1. Vision-Language Benchmarks", "content": "Datasets We evaluate SVP on 6 tasks and 10 core benchmarks for VLMs: we consider COCO2017 [59], NoCaps [2], and Flickr30k [75] for generative captioning; RefCOCO [46], RefCOCO+, RefCOCOg for referring expression generation, ScienceQA [81], VQAv2 [30] and GQA [40] for visual question answering. We use MMBench [64] and MMMU [116] for multitasking and POPE [56] for hallucination and object recall. Given the large amount of models and tasks considered, we use the lite evaluation sets as proposed in the 1mms-eval [123] when available for the captioning and VQA tasks. We use the full evaluation set for MMMU and POPE. We leverage the full eval set for RefCOCO, RefCOCO+, and RefCOCOg experiments.\nGeneral Results Overall, on the 10 datasets and 6 tasks and 11 sub-tasks considered (Fig. 6 and Tab. 1), we achieve large improvement for captioning, referring, hallucination control and object recall; and comparable or better performance for multitasking benchmarks and VQA, obtaining almost 20% improvement for the best performing one (captioning) and around 1% decrease in performance for the worst performing tasks (visual question answering). For referring tasks, we can see that for a VLM with poor refer-"}, {"title": "5. Conclusions", "content": "Limitations SVP requires a VLM capable of in-context learning and multiple samples per input. Its effectiveness is limited by the underlying grounding model's quality and may not improve tasks without spatial understanding or those requiring specialized knowledge. Future work could explore augmentation and guided-search strategies to enhance localized visual input processing.\nIn this work, we introduced SVP, a novel approach that elicits latent information in VLMs through self-captioning and grounding as a feedback mechanism, without requiring additional annotations. SVP is effective in enhancing captioning quality, referring expression generation, hallucination control, and object recall, while maintaining competitive performance in VQA and multitasking. These results underscore the potential of SVP to unlock latent capabilities in"}, {"title": "B. SVP Algorithm", "content": ""}, {"title": "D. Additional Experiments", "content": ""}, {"title": "D.1. Captioning", "content": ""}, {"title": "D.2. Hallucination Rate", "content": ""}, {"title": "D.3. Referring Tasks", "content": ""}, {"title": "D.4. Iteration Ablation", "content": ""}, {"title": "D.5. Model Size Ablation", "content": ""}, {"title": "D.6. Object Grounding Ablation", "content": ""}, {"title": "D.7. Score Ablation", "content": ""}, {"title": "D.8. DPO Ablation", "content": ""}, {"title": "E. Connection with Latent Variable Models", "content": "A VLM can be represented as a conditional distribution:\n$p_{\\theta} (x|c) = \\int p_{\\theta}(x|z, c) p_{\\theta} (z|c) dz = E_{p_{\\theta}(z|c)} [p_{\\theta} (x|z, c)]$.\nThe input conditioning information is represented by c = ($c_v$, $c_t$), while x denotes the model output, which could be an answer to a question or a caption. In scenarios where our output is a classification or regression target, we can approximate the decoding distribution $p_{\\theta} (x|z, c)$ using a deterministic function: $p_{\\theta} (x|z, c) = d(x \u2013 d_{\\theta} (z, c))$. In this context, z represents a latent (or visible) variable that encapsulates a detailed and comprehensive description of the visual information. It serves as a visual projection that fully describes the objects and relationships present in the visual input through output text. Given that VLMs are typically constructed using discrete tokens, we can assume the latent information follows a categorical or multinomial distribution. This allows us to rewrite our formulation as follows:\n$p_{\\theta}(x|c) = \\sum p_{\\theta}(x|z, c) p_{\\theta} (z|c)$.\nOur primary objective is to develop a better prior $p_{\\theta}(z|c)$ that more effectively captures visual information. This approach aims to eliminate the need for collecting an expensive and error-prone dataset of grounded and optimized captions. By doing so, we can enhance the model's ability to understand and represent visual content without relying on manual annotation processes.\nMAP Estimation A simple approach is to leverage additional conditional information leveraging grounding. This strategy involves incorporating context to enhance the model's understanding of visual content. Specifically, we might consider the following forms of conditional information:"}, {"title": "F. Prompting", "content": ""}, {"title": "G. Connection with iterative Improvement in LLMs", "content": "G.1. Self-Taught Reasoner\nOur method is inspired by recent advances in self-improvement in language modelling [24, 32, 117], where the focus has been in improving chain-of-though reasoning leveraging feedback. The Self-Taught Reasoner (STaR [117]) uses ground truth labels and rationalization as feedback [117] and fine-tunes the model on reasoning steps that generate the correct answer [36]. In Fig. 23 we provide a comparison between SVP and rationalization in STaR, where the model tries to find the correct reasoning path given external feedback in the form or a ground truth response.\nG.2. Connections with RLHF and DPO\nIn this subsection we briefly show how recent methods for aligning and improving language models [78, 128] can be re-written using our notation."}, {"title": "H. Visualizations", "content": ""}, {"title": "I. Qualitative Examples", "content": "Here we provide additional visualizations and examples to illustrate our method. The images used for captioning and visual queries were selected from the COCO2014 training set [59], while referring examples were sourced from the RefCOCO dataset [46], specifically the val_lite set [123]. The examples are presented with minimal post-processing, limited to the removal of special characters and correction of obvious grammatical errors. Both models were prompted using the protocols outlined in Appendix F. All hyper-parameters remained consistent across both models. The examples compare output of LLaVA-1.6 models sampling from the base model (w/o SVP) and sampling from the adapted model (w/ SVP). These comparisons provide qualitative insights into the performance differences between the base and adapted models.\nI.1. Captioning Tasks"}]}