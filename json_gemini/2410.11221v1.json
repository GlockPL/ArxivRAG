{"title": "Multi-objective Reinforcement Learning: A Tool for Pluralistic Alignment", "authors": ["Peter Vamplew", "Cameron Foale", "Conor F. Hayes", "Richard Dazeley", "Hadassah Harland"], "abstract": "Reinforcement learning (RL) is a valuable tool for the creation of AI systems. However it may be problematic to adequately align RL based on scalar rewards if there are multiple conflicting values or stakeholders to be considered. Over the last decade multi-objective reinforcement learning (MORL) using vector rewards has emerged as an alternative to standard, scalar RL. This paper provides an overview of the role which MORL can play in creating pluralistically-aligned AI.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has emerged as one of the most powerful tools for creating AI systems capable of autonomous decision-making for sequential tasks [Sutton and Barto, 2018]. Its core mechanism of learning to maximise the expected future return derived from a scalar reward signal can allow it to reach or even exceed human levels of performance. However, this also creates a strong dependency on an accurate definition of the reward signal. If the reward is misspecified or underspecified, the behaviour learned by an RL agent may deviate significantly from what is desired [Taylor, 2016]. Recent studies have shown that current approaches to reward specification may frequently lead to specification errors [Booth et al., 2023, Knox et al., 2023].\nVamplew et al. [2018] argued that framing alignment as a multi-objective problem may assist in overcoming the issues of creating aligned agents using RL. Treating each aspect of the alignment task as a separate objective within a vector reward signal may aid in producing aligned behaviour which is difficult or impossible to achieve using a scalar definition of reward [Vamplew et al., 2022]. Recent years have seen an increasing amount of research activity applying multi-objective reinforcement learning (MORL) techniques to various aspects of alignment."}, {"title": "2 A brief review of MORL", "content": "MORL methods assume that the environment can be represented as a Multi-objective Markov Decision Process (MOMDP), which is a MDP with a vector reward function R:S\u00d7A\u00d7S \u2192 Rd with d objectives. The agent's aim is to discover a policy \u03c0which maximises the return derived from R. However, as both R and the return are vectors, it is not possible to define a full-ordering over policies. In order to do this, MORL algorithms often assume the existence of a utility function u: Rd \u2192 R, which maps the vector value of a policy to a scalar. If u is known in advance and fixed, then the agent can learn a single-policy which is optimal for u [Hayes et al., 2022]. Where u is unknown, subject to change, or difficult to explicitly define, the agent may instead find a set of policies which are optimal under different parameterisations of u. This is known as multi-policy MORL. The final decision of which policy to execute can then be selected at run-time. If u is difficult to explicitly define, this policy selection might be carried out directly by the system's stakeholders (Hayes et al. [2022] describe this as a decision support scenario).\nSome MORL systems assume that u is a linear-weighted sum of the objective values. This is simple to implement, as the MOMDP can be mapped to an equivalent single-objective MDP [Roijers et al., 2013]. However it may fail to correctly capture the intended behaviour, so MORL methods often instead use monotonically-increasing non-linear utility functions. This introduces algorithmic complications, but more accurately represents the stakeholders' true utility. It is important to note that in this context u can not be applied to the reward received on each time-step - instead it is applied to the (possibly discounted) vector returns v accumulated by the agent\u00b9."}, {"title": "3 Applications of MORL to pluralistic alignment", "content": "Sorensen et al. [2024b] define three categories of benchmarks for pluralistic alignment, which can be interpreted as specifying desirable characteristics of pluralistic agents. These characteristics are 1) the agent be multi-objective in nature (which will support value-pluralism [Sorensen et al., 2024a]), 2) the agent is steerable to support customisation of trade-offs between objectives, and 3) the agent is able to consider a diverse set of user preferences. In this section we explore how MORL methods can support each of these desirable characteristics, either individually or simultaneously."}, {"title": "3.1 MORL for value pluralistic alignment", "content": "This aspect of pluralistic alignment supports consideration of a diversity of values (such as personal freedom, societal harmony, economic benefits, and environmental impact). Clearly, MORL naturally supports this aspect of pluralism, as each value can be represented by a separate objective within the reward function R.\nMORL has been applied in a number of contexts to enable a system to balance multiple conflicting values. Examples include performance versus safety tradeoffs [Vamplew et al., 2021, Smith et al., 2023], compliance with moral standards or norms [Rodriguez-Soto et al., 2022, Peschl et al., 2021], or wellbeing, affordability, equity, and environmental sustainability [Chaput et al., 2023].\nIn the context of finetuning LLMs, Wang, K. et al. (2024) present an approach that involves conditioning model weights on preference weights over the objectives, and demonstrate performance on three objectives reflecting different desirable properties of text summarisation. Wu et al. [2024]"}, {"title": "3.2 MORL for Steerable pluralistic alignment", "content": "A key advantage of multi-policy MORL is that it is inherently customisable with respect to stakeholder preferences. Sorensen et al. [2024b] state that for many applications, customisation of the trade-off between objectives at run-time is a desirable characteristic (for example, to match the preferences of the current user of an AI system). Similarly, Chatila et al. [2017] argued for the criticality of being able to adapt to changing values or preferences at a societal level. If the stakeholder using a system changes or if the preferences of an existing stakeholder change, a new policy which is optimal with respect to their preferences can be immediately identified, without the need for re-training which would be required for a single-objective RL agent [Hayes et al., 2022].\nHarland et al. [2023] provides an example of these benefits. Their agent learns a Pareto set of policies, and selects a policy to execute. After each action the agent observes the reaction of a human user. If it detects that the human is displeased, the agent apologises, updates its model of the human's preferences, and selects a new policy which is compatible with that model.\nRame et al. [2024] argue that for very large models, it may be impractical to explicitly learn a Pareto set of policies. Instead they start from a pre-trained model, and separately fine-tune a separate copy per objective. They then use linear weight interpolation across these specialised models at run-time to produce a model which is aligned with a particular desired trade-off between the different objectives."}, {"title": "3.3 MORL for Jury-Pluralistic Alignment", "content": "Jury-pluralism refers to the capacity to take into account the variations in preferences amongst a diverse set of users (or more broadly, people or groups impacted by the decisions of the AI we have used the term stakeholders to encompass all of these possibilities). In order to support this in MORL, two things must be true: (1) The set of objectives and corresponding rewards must include all aspects of the problem considered relevant by any stakeholder, and (2) the choice of policy to be executed must reflect the interests of all stakeholders.\nThis might be achieved by defining in advance a utility-function u representing the interests of all parties. In practice this is likely to be very difficult, and may well simply represent the average preferences of the population, failing to adequately consider minority views (a criticism levelled at existing reinforcement learning from human feedback (RLHF) approaches by Sorensen et al. [2024b]). Alternatively, multi-policy MORL methods might learn a set of Pareto-optimal policies, with the choice of policy to execute made via a consultation or voting process \u2013 this will be time-consuming, and subject to the limitations of voting systems as well studied in social-choice literature [Dai and Fleisig, 2024].\nSorensen et al's definition of jury-pluralism assumes that each stakeholder is mapped to a scalar reward or utility, and that the agent makes decisions so as to maximise a welfare function applied over this set of stakeholder utilities. This definition can be mapped directly onto an MORL framework by assigning an objective within the MOMDP per stakeholder, and then applying a utility function u that finds a suitable tradeoff over all stakeholders.\nThis framework requires a diverse set SH of identified stakeholders {sh1, \u2026, shn}. Each element of the reward R corresponds to a scalar representation of the values of a specific stakeholder (i.e., the number of objectives d = n). The choice of utility function u should appropriately account for the desires of each stakeholder. A relevant line of research here is the body of work on fair MORL, where various fair utility functions have been considered. Both Yu et al. [2023] and Michailidis et al. [2023] use the Generalised Gini social welfare function (GGF), which sorts the utilities of stakeholders into ascending order and applies a linear weighting with weights of decreasing value (i.e. placing more emphasis on the lower-valued utilities)."}, {"title": "3.4 MORL for Fully Pluralistic Alignment", "content": "The approaches described above address each dimension of pluralistic alignment separately. Jury-pluralistic MORL methods (Section 3.3) balance the preferences of a diverse set of stakeholders, while value-pluralistic MORL methods (Section 3.1) find trade-offs across a diverse set of values. Meanwhile, the steerability of multi-policy methods (Section 3.2) applies regardless of the nature of the objectives. Here we consider how MORL methods might support a steerable agent which is both value-pluralistic and jury-pluralistic.\nAs in Section 3.3, a fully-pluralistic agent requires an identified set of n stakeholders SH. However rather than representing each stakeholder's desires directly via a scalar reward value, in this framework the rewards represent varied objectives (values) which may be prioritised differently by each stakeholder. Each stakeholder's preferences over those objectives are then represented by a personalised utility function ui. These individual utility functions are then aggregated by a system-level utility function. For example, the GGF from Equation 2 can be extended to this more general framework as follows:\nMulti-policy MORL methods can learn a coverage set of policies representing possible trade-offs between the objectives thereby supporting fairness across the stakeholders, while also allowing for future changes in the preferences of each stakeholder. The appropriate policy for execution can then be determined at run-time, providing a high level of steerability.\nWhile we are not aware of any existing work on fully-pluralistic MORL agents, the PRISM Alignment Project [Kirk et al., 2024] presents an important enabling step towards fully-pluralistic LLMs, by curating a dataset of human feedback suitable for the construction of the multiple reward models required for fully-pluralistic RLHF. This feedback has been gathered from a more diverse set of participants than other feedback datasets, including attempts to provide wider geographic and cultural coverage. Each feedback item is labelled with demographic and other information about the feedback provider. In addition, the feedback is fine-grained, with a rating provided relative to multiple attributes (overall values, fluency, factuality, safety, diversity, creativity, and helpfulness). The dataset also contains measures of the importance which each provider places on each attribute, which would be suitable for creating personalised utility functions ui."}, {"title": "4 Challenges", "content": "A significant issue impeding the development of pluralistic agents using MORL is the lack of suitable human feedback datasets for developing reward models. While the PRISM dataset is a major step forward, its authors acknowledge that it still lacks sufficient diversity (for example, the content is entirely in English, and feedback was gathered from online workers) [Kirk et al., 2024]. Creation"}]}