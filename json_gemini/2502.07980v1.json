{"title": "CIRCUIT: A BENCHMARK FOR CIRCUIT INTERPRETATION AND REASONING CAPABILITIES OF LLMS", "authors": ["Lejla Skelic", "Yan Xu", "Matthew Cox", "Wenjie Lu", "Tao Yu", "Ruonan Han"], "abstract": "The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs' reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-40, achieves 48.04% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique feature that enables unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-40 can only pass 27.45% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design.", "sections": [{"title": "1 Introduction", "content": "The application of Large Language Models (LLMs) in analog integrated circuit design could pioneer a new era of AI applications in domains traditionally dominated by human expertise. Analog semiconductor chips are the core building blocks in sensing and communication systems. Contrary to digital chip development, where computer-aided design automation has been widely adopted for a few decades, analog design, often perceived more as a craftsmanship than a well-established engineering procedure, relies heavily on the designer's experience and intuition to navigate in the trade space of efficiency, noise, linearity, and speed to meet certain specifications. This domain's depth, requiring a blend of acumen and creativity, underscores the high barriers to entry and the extensive training required to master its intricacies, which exacerbated the critical labor shortfall of the semiconductor industry in this decade [Ravi, 2023].\nThe advent of AI-assisted design automation in analog circuit design holds considerable promise to tackle the aforementioned challenge. It offers the potential to significantly streamline design cycles, enabling engineers to focus more on strategic, high-level design considerations and the exploration of novel ideas and applications. Traditional analog design automation [Wang et al., 2018, Settaluri et al., 2020, Liu et al., 2022, Xue et al., 2023, Zhang et al., 2019] has relied on numerical-based optimization and machine learning techniques to train surrogate models for designing circuits with fixed topologies and semiconductor processes, resulting in reduced generalization capabilities and often suffering from limited interpretability. A shift towards a reasoning and knowledge-based approach, facilitated by LLMs that transcend traditional optimization techniques, could leverage circuit domain expertise to innovate and refine the design of diverse analog circuits.\nA natural starting point towards this ambitious goal is to evaluate existing LLMs' proficiency in executing various analog circuit design tasks. To that end, we introduce the CIRCUIT (Circuit Interpretation and Reasoning Capabilities) benchmark, which focuses on simple topology understanding \u2013 a precursor to performing any complex design task.\nThe dataset is designed to be scalable, enabling a seamless incorporation and automatic evaluation of more advanced analog"}, {"title": "2 Related Work", "content": "Task-specific evaluation plays a crucial role in advancing research in LLM applications by providing precise insights into model capabilities and limitations within defined contexts. The scalability of general-purpose models has demonstrated enhanced task performance in various domains, including language [Brown et al., 2020], mathematics [Aojun Zhou, 2023, Mao et al., 2024], and code generation [Chen et al., 2021]*.\nIn the realm of digital circuit design, noteworthy progress has been made in harnessing LLMs for tasks such as generating Verilog Code, as explored by Mingjie Liu [2023]. Moreover, Cadence's JedAI \u2020 platform exemplifies the first application of LLM technology in chip design, illustrating the feasibility of integrating LLMs into digital design workflows.\nIn the realm of analog design, LLMs have already been integrated into frameworks that automate aspects of the design process [Chang et al., 2024, Lai et al., 2024]. While these works focus on leveraging LLMs directly for circuit design, an essential precursor is to evaluate the knowledge and reasoning capabilities of LLMs on fundamental analog circuit knowledge. Without a deep understanding of their foundational capabilities, the effectiveness and versatility of LLMs in real-world circuit design may be limited. To address this gap, we introduce the CIRCUIT dataset, which serves as a critical first step in the analog design pipeline.\nWhen reviewing existing datasets for other domains, we notice that evaluation proves difficult on complex tasks. Coding tasks utilize unit testing with automatic evaluation, while other fields necessitate human evaluation. LLMs have also been used as evaluating agents. [Mao et al., 2024, Lin et al., 2021] While LLMs can evaluate large volumes of data, do not suffer from fatigue, and are cheaper to utilize, our initial experiments showed that they struggle with understanding and interpreting complex reasoning about analog circuits. Inspired by unit testing, we introduce a simple dataset design and evaluation metric combination that shows promise for the assessment of LLMs across various fields and tasks. This framework is inherently scalable, suitable for cost-effective automatic evaluation, adaptable to more complex analog design tasks, and transferable to other reasoning domains."}, {"title": "3 CIRCUIT Dataset", "content": "The CIRCUIT dataset comprises circuit problems, many of which include associated diagrams. The dataset was made using templates \u2013 problems adapted from sources listed in Appendix A modified to fit different numerical setups and ensure each only asks for a single numerical answer. Figure 2 is an example of a dataset question. The diagram and the template are adapted so that the numerical setup can accommodate different values and ensure different answers to the template question. Therefore, we were able to create multiple numerical setups for each template used for the creation of the dataset. Each template question together with its numerical setups served as a single unit test in the dataset. This design enables a more nuanced evaluation of the models' understanding of different circuit topologies and provides quantifiable insights into how data homogeneity influences model performance.\nInitial experiments indicated that LLMs found it challenging to interpret circuit diagrams, particularly the direction and orientation of circuit components. To aid in understanding circuit topologies, we incorporated netlists into the prompts. Netlist syntax was slightly modified to better suit our needs, detailed in Appendix B. This modification and the inclusion of a syntax explanation in the prompts were aimed at enhancing LLMs' performance on our dataset.\nFigure 2 illustrates an example of a data point consisting of a template question along with its associated diagram, netlist, and a numerical setup. In this scenario, the LLM is tasked with applying Ohm's law ($V = IR$) to calculate the current. The specific setup prompts for a calculation of $I = \\frac{V}{R} = \\frac{5V}{100\\Omega} = 0.05A$, testing the LLM's understanding of this simple circuit topology. Our dataset extends this approach by using various values for V and R for numerical setups, thus methodically exploring the output curve I in a unit-test-like fashion. That is, to test the understanding of this topology, we create multiple data points with different numerical setups, each maintaining the same structure, template question, diagram, and netlist but altering V and R values in the numerical setup to produce data points with different correct answers. Providing correct answers to each numerical setup strongly suggests an understanding of the topology, without requiring a detailed examination of the solution methodology, much like how unit tests in programming verify that a function is correctly implemented."}, {"title": "3.2 Dataset statistics", "content": "The CIRCUIT dataset consists of 510 questions derived from 102 templates, with 5 numerical setups each. 93 templates include diagrams, 79 of which include netlists. Templates are divided into four categories-basic, analog, power, and radio-frequency (RF)\u2014and are graded by levels based on the corresponding MIT course and the typical class year. For example, MIT 6.002 (Circuits and Electronics) problems are level 1 since the class is typically taken by freshmen.\nThe category-level distribution of the dataset is given in Figure 3."}, {"title": "4 Evaluation", "content": "As previously described, each template $t_i$ is associated with $n = 5$ distinct numerical setups in the dataset. These setups yield straightforward numerical outcomes and aim to cover the comprehensive output range pertinent to the respective circuit.\nWe evaluate using both global and template-level accuracies. Global accuracy is defined as:"}, {"title": "4.1 Metrics", "content": "for the entire dataset and its subsets.\nTemplate accuracy, which leverages the unit-test-like structure of our dataset, is gauged by the pass@k/n metric. This metric evaluates the model's understanding of a single circuit topology through n numerical setups (n = 5 for our dataset), which make up a unit test. A template is considered accurate (i.e. a unit test is passed) if at least k of its n setups are correctly solved. Therefore, the template accuracy is defined as:\nand reported for various values of k across all 102 templates (m = 102) and their subsets."}, {"title": "4.2 Methods", "content": "Our straightforward numerical setups allow for the automatic evaluation of LLM performance. We prompt LLMs to give their final numerical answers in a specified format (details in Appendix C) and facilitate parsing via regex from the responses. Additionally, we conduct human evaluations on a subset of responses for error and qualitative analyses."}, {"title": "5 Experiments", "content": "We evaluated gpt-4-turbo [OpenAI, 2024], gpt-40 [OpenAI, 2024] and gemini-1.5-pro [Team et al., 2024] on our dataset, setting the maximum tokens to 1,536 for each. Detailed prompt design is available in Appendix C. Following well-established prompting techniques [Brown et al., 2020, Schulhoff et al., 2024], four different prompts were tested for each model: zero-shot and one-shot, with and without netlists. Models were instructed to give their final numerical answers with a precision of six decimal places."}, {"title": "5.2 Experiments", "content": "In each experiment, models were provided with diagrams for questions that included them. In the first 3 experiments, models received all questions from the CIRCUIT dataset with a 0-shot prompt. In the next 3, models were given 395 questions that had associated netlists and the same 0-shot prompt, along with netlists and customized instructions for interpreting only the elements present in each netlist. In the third set of 3 experiments, models were given all questions with a 1-shot prompt. In the final 3 experiments, models received all questions, a 1-shot prompt with a netlist example, netlists, and the necessary netlist instructions. Details of the prompt design are in Appendix C. Responses from all experiments were quantitatively analyzed, with a subset reviewed for errors and qualitative insights by human evaluators."}, {"title": "5.3 Evaluation", "content": "We used an automatic evaluation method to assess model responses and reported both global and template accuracies. Responses were deemed correct if the absolute difference from the ground truth was less than 0.001. Additionally, we conducted a human evaluation of best-accuracy responses to verify automatic evaluation results, analyze errors, and understand the qualitative aspects of the responses. Errors were categorized into mathematical, response formatting, and reasoning. The models sometimes displayed clear misunderstandings of the circuit topology, which we classified as topology errors, a specific type of reasoning error. A common topology error was misunderstanding element orientation or direction, the rate of which we also reported. More details on error types and subtypes can be found in Appendix D. Human evaluation deemed responses as correct if they were devoid of errors."}, {"title": "6 Results", "content": ""}, {"title": "6.1 Quantitative analysis", "content": ""}, {"title": "6.1.1 Automatic Evaluation", "content": "We assessed model performance across the entire CIRCUIT dataset using automatic evaluation, with results detailed in Table 1. A key observation is that the best-performing prompt varies by model and by the specific accuracy metric. For instance, GPT 4-turbo achieves the highest global accuracy with the 1-shot prompt, while its highest 5/5 template accuracy occurs with the 1-shot prompt with a netlist example. In contrast, Gemini 1.5-pro performs best with the plain 0-shot prompt across all metrics, indicating a potential struggle to integrate additional information from netlists or example-based problem-solving strategies provided in the 1-shot prompts. The most consistent and highest-performing model across both global and template accuracies appears to be GPT 40, which leverages netlists effectively but does not seem to gain further advantage from the 1-shot prompt.\nOne important pattern we observe is that template accuracy decreases as the value of k in pass@k/n increases. This reflects the increasing difficulty in achieving correctness across all five numerical setups in a given template. Notice that pass@3/5 template accuracy closely aligns with global accuracy indicating that relying solely on global accuracy can obscure deeper insights into a model's performance on the given dataset."}, {"title": "6.1.2 Human evaluation", "content": "Automatic evaluation predominantly assesses model outputs by comparing them to numeric ground truths and typically does not penalize incorrect reasoning. Concerns about this method also include mathematical errors and incorrect response formatting. GPT 40 was selected for a detailed human evaluation because it demonstrated superior performance in the automatic assessment.\nResults outlined in Table 3 affirm that the trends observed in human evaluations are consistent with those from automatic evaluation. To further understand the correlation between automatic and human evaluations, we analyzed the occurrence of false positives\u2014instances where responses were deemed correct by automatic metrics but identified as incorrect upon human review. Approximately 5% of the automated evaluations resulted in false positives, impacting even the most rigorous template accuracies. Despite these occasional discrepancies, automatic evaluation proves to be a dependable tool for understanding model performance.\nHuman evaluation involved a thorough error analysis, detailed in Table 4, with error categorization methodologies explained in Appendix D. The primary error types identified were mathematical, formatting, and reasoning-the latter encompassing all errors not directly related to mathematical or formatting issues. Within reasoning errors, misunderstandings related to topology emerged as a significant subcategory, and issues with direction or orientation of elements were recognized as a specific concern within topology errors. Our analysis indicates that mathematical and formatting errors constitute a minor portion of the total errors, and the predominant challenges for models stem from reasoning errors. This highlights the complexity of our dataset which requires a deep understanding of underlying concepts and their applications.\nAdditionally, global per-category and per-level accuracies on human-evaluated responses are summarized in Table 5 and Table 6 respectively. These results highlight the challenges in understanding more complex topologies, as evidenced by significantly lower performance on questions with netlists and at higher levels. The consistently higher accuracy in the 'Basic' category and Level 1 subset of questions across configurations suggests that GPT-40 is better equipped to handle introductory-level circuits than more advanced ones."}, {"title": "6.2 Qualitative analysis", "content": "GPT 40's responses revealed that the model generally employed appropriate tools and formulas and understood which elements were present in the given circuit. However, it struggled with complex circuit topologies; even with netlists, higher-level reasoning remained challenging. Sometimes, even when given a netlist, GPT's response would not indicate its use. We also noticed that netlists often helped GPT understand a part of or the entire given topology. Errors often stemmed from misconceptions about interactions and connections between components and subcircuits. GPT also struggled with directions and element orientations, such as current flow direction from a current source. Sometimes, GPT made minor reasoning errors which didn't affect the correctness of the final solution. While GPT occasionally made mathematical errors, these were primarily confined to approximation errors, often division and logarithmic and exponential calculations, and sometimes careless mistakes in equation manipulation, reinforcing that the primary challenge lies in reasoning rather than basic mathematics. Nevertheless, the fixed error on the final numerical answer was sometimes too stringent for GPT's approximations. GPT occasionally displayed conceptual misunderstandings, failed to follow given instructions, or applied general knowledge without adapting to specific contexts. Hallucinations about non-existent configurations were also noted. For instance, when given an op-amp in negative feedback, GPT hallucinated its non-inverting input was grounded.\nThis qualitative analysis underlines the nuanced challenges GPT faces with our dataset and gives us a glimpse into the data GPT was trained on. More specific examples can be found in Appendix E."}, {"title": "7 Discussion & Future work", "content": "Through our experiments, we gained valuable insight into the capabilities of existing LLMs in understanding and reasoning about various analog circuit topologies. Our quantitative and qualitative analyses indicate that these models possess reasoning abilities and relevant expert knowledge to tackle the problems in our dataset. Their understanding of circuit topologies can be improved when netlists and 1-shot examples are provided, but substantial work remains to be done to improve their performance further on our dataset. Addressing these basic shortcomings in topology understanding is crucial before advancing to more complex analog design tasks-both of which represent exciting directions for future work.\nOur dataset design together with the pass@k/n metric enables an automatic evaluation framework for quick, cost-effective, reliable, and comprehensive evaluation of LLMs' capabilities. pass@k/n offers a more nuanced understanding of model performance than a mere global accuracy score. On our dataset, it reveals that the models are proficient in only a narrow subset of topologies, and a closer look found this subset focused on very simple topologies. It further shows that models are inconsistent across different numerical setups. Enhancing this pass@k/n's potential to yield deeper insights could be explored in future work by enriching templates with more detailed annotations and including intermediate-step evaluation. Uniquely, the metric can be adjusted for different levels of strictness (k), allowing researchers to evaluate model performance under varying levels of precision. The unit-test-like dataset design and pass@k/n metric can be beneficial in domains beyond analog circuits where a deep understanding of nuanced subject matter is critical, and where datasets can be structured with multiple subcomponents per main category to assess comprehensive knowledge. Future work could investigate applying our dataset design and metric to new domains, different unit test designs for distinct evaluation goals, and strategies for evaluating intermediate steps in LLM reasoning to enable a more detailed assessment.\nA key aspect of the CIRCUIT dataset design is its transparency regarding data homogeneity achieved through our unit test setup. When we compare global accuracy to template accuracy, we see the potential pitfall of relying solely on global metrics in model evaluation. Global accuracy provides an aggregate view of model performance but can mask nuanced failures that become apparent when assessing models on a template level. The CIRCUIT dataset's explicit design allows us to observe this distinction more clearly, as it isolates a model's ability to handle both the homogeneity (consistent core structures) and variability (changing numerical setups) inherent in real-world problems. This approach contrasts with traditional datasets, where either the homogeneity may not be explicit or the variability across problem instances may not be systematically controlled. By designing datasets like CIRCUIT, where the relationship between template structure and numerical variability is clear, we can gain deeper insights into model robustness and generalization capabilities. Template pass@k/n accuracies on our dataset show low generalization capabilities across variability in numerical setups. This is concerning for analog circuit design because it suggests that models struggle to adapt to different component values and configurations, which are critical for reliable performance in real-world circuit applications. Therefore, we encourage making homogeneity a more explicit aspect of dataset design and look forward to the insights future work may uncover.\nError analysis showed that most incorrect responses stemmed from reasoning errors, while mathematical inaccuracies were rare. Qualitative analysis further revealed the nature of the reasoning errors, pointing towards significant opportunities for improving the interpretative and reasoning capabilities of these models in future work. There is a potential role for integrating a Python interpreter to mitigate mathematical errors, as noted by Gao et al. [2023], and/or analog circuit simulators to improve model reasoning.\nAlthough the slight improvement in model accuracy with netlists suggests some sensitivity to additional contextual information, the overall impact remains modest. Interestingly, 1-shot prompting improved accuracy mainly on questions without associated netlists. The benefit of the 1-shot example isn't fully realized for questions involving netlists,"}, {"title": "8 Limitations", "content": "This study, while insightful, faces several key limitations. The dataset's size and imbalance across categories, levels, and netlist presence could affect the generalizability of our findings, highlighting the need for a more representative dataset through expansion, particularly the number of numerical setups and better balancing. The dataset could be further enhanced by incorporating more challenging problems that reflect contemporary circuit topologies. Additionally, the limited model selection and narrow focus in human evaluation limits our understanding of broader model capabilities."}, {"title": "9 Conclusion", "content": "We introduced CIRCUIT, the pioneering dataset designed specifically for assessing LLMs in the domain of analog circuit interpretation and reasoning. This work not only demonstrated the utility of unit-test-like dataset design but also highlighted the nuanced capabilities and limitations of leading LLMs through a comprehensive set of evaluations. The pass@k/n metric and the strategic use of netlists significantly advanced our understanding of how models handle complex circuit topologies. Looking ahead, we encourage addressing the challenges posed by our dataset, expanding its scope, exploring our dataset design and metrics in other challenging domains, utilizing our automatic evaluation method, and further refining and developing our methodologies."}, {"title": "Author Contributions", "content": "L. Skelic was responsible for the curation of the dataset and the overall design and execution of the study, including the development of the dataset structure, the creation of custom metrics, automatic and human analysis frameworks, and prompt and experiment design. L. Skelic and Y. Xu conducted the human analysis of the model responses. Y. Xu and M. Cox reviewed the dataset to ensure its quality and consistency. W. Lu, T. Yu, and R. Han provided oversight and guidance throughout the study. R. Han was the principal investigator, and W. Lu and T. Yu offered key support in a supervisory capacity from the industry side."}, {"title": "Ethical Considerations", "content": "We addresses the critical points related to ethical considerations, ensuring that our research is conducted responsibly and transparently."}, {"title": "Data Collection and Privacy", "content": "Our dataset did not involve personal data, ensuring no privacy concerns; however, the dataset will not be shared publicly until informed consent from the authors of sources listed in Appendix A is obtained."}, {"title": "Use of LLMs for Writing Assistance", "content": "Chat GPT was used to refine the clarity and conciseness of our paper."}, {"title": "3.1 Dataset curation", "content": "V=IR"}, {"title": "4 Evaluation", "content": "Aglobal= # correctly answered questions / # total questions"}, {"title": "4.1 Metrics", "content": "Atemplate,k/n= \u03a3i=1 Ati,k / m  where Ati,k= 1 if at least k out of n setups are answered correctly / 0 otherwise"}, {"title": "To better understand the given circuit...", "content": "The netlist: / V N1 0 / R N1 0 ; I"}, {"title": "Final Numerical Answer", "content": "1.400000"}]}