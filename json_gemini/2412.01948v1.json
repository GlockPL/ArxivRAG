{"title": "The Evolution and Future Perspectives of Artificial Intelligence Generated Content", "authors": ["Chengzhang Zhu", "Luobin Cui", "Ying Tang", "Jiacun Wang"], "abstract": "Artificial intelligence generated content (AIGC), a rapidly advancing technology, is transforming content creation across domains, such as text, images, audio, and video. Its growing potential has attracted more and more researchers and investors to explore and expand its possibilities. This review traces AIGC's evolution through four developmental milestones- ranging from early rule-based systems to modern transfer learning models-within a unified framework that highlights how each milestone contributes uniquely to content generation. In particular, the paper employs a common example across all milestones to illustrate the capabilities and limitations of methods within each phase, providing a consistent evaluation of AIGC methodologies and their development. Furthermore, this paper addresses critical challenges associated with AIGC and proposes actionable strategies to mitigate them. This study aims to guide researchers and practitioners in selecting and optimizing AIGC models to enhance the quality and efficiency of content creation across diverse domains.", "sections": [{"title": "I. INTRODUCTION", "content": "THE advancement of Artificial Intelligence (AI) has trans- formed the way content is produced, leading to a promis- ing technology called Artificial Intelligence Generated Content (AIGC). By harnessing AI algorithms, this technology lever- ages human creativity to complement the traditional content generation process, ultimately improving the quality of human- centered production work with applications across diverse digital mediums, including text [1][2], images [3][4], audio [5], and videos [6]. AIGC stands as a crucial development in information and data technology, redefining the efficiency and quality of creative outputs.\nAs shown in Figure 1, many models and algorithms have been developed along the progressive trajectory of AIGC, with each technological advancement built upon the previous ones. Fresh endeavors persist in unfolding, leading to great tools [7][1] and systems [8][9] that support sustainable and adaptable content production across various sectors. Part of the reason for so much interest in this development is the\ncomplexity of the problem and its many associated challenges. While data have been a fundamental drive to advance digital transformation across industries, its collection is often insuffi- cient [10][11] or sometimes impossible [12]. Data annotation is rather time-consuming and costly, leading to the available public datasets often being on a small scale. Examples include breast pathological images [13], the MNIST dataset [14] for handwritten digits, and the CoNLL-2003 dataset [15] for named entity recognition. When significant manpower is added, it becomes harder to maintain consistency in tone, style, and quality. These inherent problems highlight the need for rapid, scalable, and consistent content production methods.\nContent generation approaches vary in terms of methods, and more importantly, in terms of limitations. Some have ex- perimented with simple text generation to simulate human-like language interactions via rule-based methods [16][17], while others capture statistical dependency between text elements in a sequence to predict the next element [18]. These earlier attempts rely heavily on either experts' rules established prior or a set of probabilistic measures, scoping their capabilities within predefined scenarios. With the involvement of deep learning, the performance of generative models has been substantially improved, unlocking opportunities for a vast range of applications [4]. Meanwhile, the critical need for trustworthiness in these models is also raised [19].\nThe evolution of AIGC progresses through multiple stages, beginning with the generation of foundational data, moving to data categorization and classification, and ultimately striv- ing for human-like understanding. Although the processes involved in AIGC vary with the change in the intended out- comes, certain core steps form a robust framework across all applications. This paper conducts a comprehensive literature review to showcase the historical progression of AIGC within such a developmental framework. All AIGC methods are clas- sified into four distinct categories - early rule-based systems, statistical models, deep learning models, and transfer & pre- trained models - each adapts the foundational framework to its unique objectives and strengths. We intend to help researchers and practitioners make informed decisions when selecting AIGC models.\nThe contributions of this paper are twofold. First, this paper offers an in-depth review of the historical development of AIGC, mapping its journey from early rule-based systems to advanced neural network methodologies. More importantly, this review addresses both technological advancements and challenges such as scalability and ethical dilemmas.\nSecond, unlike most of existing review papers that focus"}, {"title": "II. CRITERIA FOR INCLUSION IN THE LITERATURE REVIEW", "content": "To ensure comprehensive coverage of recent and seminal re- search in the field of AIGC, the literature search process begins with several electronic databases, including IEEE Xplore, Sco- pus, ArXiv, and Google Scholar with the keywords \u201cAIGC,\u201d \"artificial intelligence generated content,\u201d \u201cmachine learning content generation,\" and \"automated content creation\". While a time constraint is not imposed on published research, we make sure to incorporate the latest advancements in AIGC technology and applications. Non-English papers, short com- munications, and non-peer-reviewed articles are excluded to ensure the scientific rigor of the review. Each selected paper has to make a demonstrable contribution to the development, application, or evaluation of AIGC systems. The initial search yields more than 300 potential articles, and after screening for relevance to core themes of AIGC, the pool of relevant papers is narrowed to 200. The outcome of this meticulous search process is summarized in Figure 2, which displays the distribution of articles across journals over the years, illustrating the focus areas and publication trends in AIGC research."}, {"title": "III. GENERAL AIGC TECHNIQUE FRAMEWORK", "content": "AIGC is an innovative way of autonomously generating diverse forms of content by algorithms, machine learning models, or other artificial intelligence techniques. The main characteristic of AIGC is its reliance on data input and pre- generated models to produce content, requiring less or no human intervention. Compared to manual creation, AIGC has distinguished advantages in terms of automation, scale, and"}, {"title": "IV. EARLY RULE-BASED SYSTEMS", "content": "The roots of AIGC can be traced back to the 1950s [73][74] when early rule-based systems lays the foundation for its development. These systems employ a set of expert rules defined a priori to guide content generation. Figure 4 presents the specific framework for rule-based systems within the context of AIGC discussed in the previous section."}, {"title": "A. Fundamentals", "content": "Early rule-based systems primarily focus on texts, using NLP techniques to enable computers to understand human in- puts, in which the preprocessing step is attributed to 'parsing'. The common methods include tokenization [69], stemming, stop word removal, rule-based parsing, and entity recognition [17]. Tokenization breaks down the input data into small pieces, like words. Doing so converts a continuous stream of text into a manageable sequence of tokens, making it easier for algorithms to interpret. Stemming reduces words to their basic form, like 'running' to 'run'. This ensures consistency across similar words, reducing complexity with fewer unique words. In the step of stop work removal, common yet nonessential words like 'the' and 'is' are removed. This step helps the system focus on meaningful words only and reduces noise in the data. Rule-based parsing applies grammar rules to understand sentence structure, aiding in the interpretation of word relationships and the comprehension of text syntax and semantics. In addition, Entity recognition helps spot and cate- gorize keywords, enabling the quick identification of important information such as the text's main subjects. These steps streamline the complexity of natural language, making it more structured and manageable for analysis and encoding.\nThe user input, after pre-processing, is then screened through a set of rules pre-defined by domain experts, often referred to as expert rules [75] to guide grammar, style, and, more importantly, the logic of text generation. This screening process is termed an inference that controls which set of expert rules are applied and how they are applied efficiently and accurately to user inputs. One such mechanism is the decision-making structure [76][77], utilizing a flowchart-like or \"if-then\" reasoning to generate text responses. It searches linguistic, formatting, or narrative features or patterns in the input data that match some of \"if\" conditions to determine the proper response. For example, if the system detects the word \"father\" in user input, it applies rules associated with the \"family\" category to construct a response. Of course, there is always a default rule for situations where the system fails to recognize any matching patterns in user inputs, leading to a possible predefined response like \"I can't understand\".\nThe role of post-processing in the rule-based phase is pretty straightforward. It usually consists of some content enrichment processes, such as sentence re-organization, and spelling and grammar corrections, to improve the clarity and fluency of generated results. The decision to enhance the consistency or diversity and depth of the language used is made through content formatting and lexical enrichment."}, {"title": "B. Example", "content": "Early rule-based systems find applications in text gen- eration. For example, Eliza [16] is designed as a chatbot for psychotherapist conversations, SHRDLU [17] advances semantic analysis, and Tale-Spin [78] can create poetry and stories. Here, we detail how Eliza responds to the user input \"generate a research question with the key words: artificial intelligence, healthcare, and ethical implications\" to showcase the operations of early rule-based systems.\nAs shown in Figure 5, Eliza first parses the user's input to identify a number of keywords K. In our case, three keywords are found, which are \u201cArtificial Intelligence\u201d, \u201cHealthcare\u201d, and \u201cEthical Implications\". They are then matched with a dictionary of preset rules for keyword matching. Each rule consists of a 'keyword', a corresponding set of decomposition patterns D, and a set of reassembly policies R used to generate system responses. Note that the dictionary organizes the rules based on categorized keywords. For instance, \u201cArtificial Intel- ligence\", \"Healthcare\u201d, and \u201cEthical Implications\" fall under the categories of technology, background, and application, respectively. During decomposition, each keyword is tagged with a \"@\", and the rest of the user input is replaced by \"*\". Based on their order in the sentence, an index number"}, {"title": "V. STATISTICAL METHODS", "content": "Statistical methods analyze large datasets to identify pat- terns, trends, and probabilities, relying on data-derived \"rules\" rather than expert-defined ones in early rule-based systems. By using probabilistic models, statistical methods estimate the likelihood of various outcomes, which are well-suited to handle dynamic and ambiguous real-world data."}, {"title": "A. Fundamentals", "content": "As shown in Figure 7, preprocessing for NLP tasks in sta- tistical methods resembles early rule-based systems, including steps like tokenization, lemmatization (stemming), and stop- word removal. However, what sets it apart is the calculation of term frequency and inverse document frequency to obtain word importance in the user's input. This step is also known as feature extraction. Additionally, normalization is used to scale features to a uniform range to ensure equal contributions from all inputs to the model's adaptation.\nThe key aspect of statistical methods is their probabilistic models, which capture uncertainty and ambiguity through con- tinuous learning. Initialized with human-set parameters, these models are trained on selected datasets and fine-tuned their parameters using algorithms such as regression, classification, or clustering. Unlike rule-based models that follow strict 'if- then' logic, statistical methods rely on trained models to predict outcomes by adapting to patterns and trends in data."}, {"title": "B. Example", "content": "From the above discussion in Section V.A, it is evident that many statistical methods build upon N-grams, the simplest and most foundational technique. For instance, HMMs are an extension of N-grams with hidden states, adding a layer of abstraction to sequence modeling. Similarly, methods like GMMS, RBMs, and LDA each address specific limitations of N-grams and HMMs, offering more sophisticated approaches to handling ambiguity, complexity, and hierarchical structures within data. To illustrate the operational differences between the statistical methods and early rule-based models, we apply the same example that showcases the operations of early rule-based systems to N-grams - representative of statistical methods. The details are given below.\nIn contrast to early rule-based systems that rely on prede- fined rules, N-grams generate \"relationship rules\" by learning from data. During training, common NLP steps are applied to the data, breaking each sentence into individual words. Special tokens, such as the start of sentences (SOS), end of sentences (EOS), and unknown words (UNK), are then added to the sentence. This step, transforming the original text into a sequence of discrete tokens, enables the model to analyze word relationships and predict word sequence.\nThe formation of n-grams depends on the choice of n \u2013 the number of words in the sequence of interest. The model then calculates the frequency of each n-gram within the training data. For example, a unigram model calculates the probability of each word, while a 2-gram model calculates the probability of any two-word sequence appearing within the dataset. For this presentation, we use a bigram model as an example to illustrate how n-grams work. The 2-gram model is trained using a mini-corpus and then used to generate a response to the user. Figure 8 shows the mini corpus after tokenization, where bigrams can be extracted as pairs of consecutive words. Using the sentence \u201c<SOS> How can AI improve healthcare? <EOS>\u201d, the bigrams are [(<SOS>, How), (How, can), (can, AI), (AI, improve), (improve healthcare), (healthcare, <EOS>)]. Consequently, the likelihood of one word following another is computed as the total occurrences of the bigram divided by the frequency of the first word. For example, $p(What \\mid <SOS>) =3/5$. Using the trained bigram model, a response can then be generated. The decision on which word to start the response depends on how the model is set up and the context provided. The most straightforward way is the start token approach given that the start-of-sentence special token <SOS> is included in each sentence of the training dataset. Typically, the system formulates a response from one of the most probable words following <SOS>. For our case, \u201cWhat\" frequently follows <SOS>, the system then chooses \u201cWhat\u201d as the first word. The same logic applies to generate a word after a word until <EOS> appears, resulting in the response \u201cWhat are the ethical implications of AI in healthcare\u201d."}, {"title": "VI. DEEP LEARNING METHODS", "content": "Neural networks and deep learning models mark a major leap forward in AIGC [94], overcoming the limitations of statistical methods by better managing the growing volume, dimensionality, and complexity of modern data, especially unstructured data such as images, text, and speech. Mim- icking the structure of the human brain, neural networks consist of layers of interconnected nodes (neurons). As data flows through the layers, the networks conduct a step-by- step inference process, automatically recognizing patterns and making predictions from input to output. Extended from this foundational structure, deep learning uses convolutional neural"}, {"title": "A. Convolutional Neural Networks (CNNs)", "content": "CNNs are algorithms inspired by human vision and special- ized in image processing. Similar to rule-based and statistical prototypes, they begin with raw signal ingestion, followed by preprocessing (e.g., initial determination of edges and orienta- tion), figuration (e.g., determination of the object's shape, such as round), and finally object identification (e.g., recognizing it as a basketball). A CNN is composed of convolutional layers, pooling layers, and fully connected layers, which effectively addresses the challenges of large datasets and feature extrac- tion in AIGC. Pooling layers, through downsampling, signifi- cantly reduce data dimensions and computational load, making feature detection robust to changes in scale and orientation. This design also helps mitigate the overfitting issues common in traditional statistical methods. By combining all learned features, the fully connected layers predict the final output, enhancing the quality and accuracy of generated content.\nCNNs are built on the principles of local receptive fields, shared weights, and spatial or temporal invariance, which have helped propel AIGC into an era of rapid growth. Unlike earlier rule-based and statistical models, which often rely on handcrafted features and struggle with data in input, CNNs excel in automatically learning and extracting complex pat- terns. The use of local receptive fields allows CNNs to focus on specific regions of an input, capturing detailed features like edges and textures. Shared weights enable these networks to efficiently apply learned filters across the entire input, improving generalization and scalability. Spatial or temporal invariance, achieved through operations like pooling, ensures that CNNs can recognize patterns regardless of their position, addressing the limitations of previous models in handling diverse and complex inputs. This innovative structure makes CNNs particularly effective for content generation, as they provide robust, flexible, and scalable solutions for feature extraction and pattern recognition."}, {"title": "B. Recurrent Neural Networks (RNNs)", "content": "Just as its name indicates, the core idea of RNN is to utilize the cyclic connections of the network to capture temporal dependencies in sequence data. Compared to traditional neural networks, RNN introduces loops into the network, allowing it to take into account previous information when processing sequence data. This design makes RNNs well-suited for pro- cessing sequential data such as text, speech, and time series in AIGC tasks. The emergence of RNNs has enabled subsequent natural language processing tasks to learn the intrinsic inter- dependencies and patterns of sequential data well and make accurate predictions or decisions.\nHowever, traditional RNNs suffer from the problems of gradient vanishing and gradient explosion, which make it difficult for the network to learn long-term dependencies. To address these problems, researchers have proposed some improved RNN structures such as Long Short-Term Memory Networks (LSTM) and Gated Recurrent Units (GRU) [96]. These improved structures enable the network to better capture long-term dependencies by introducing a gating mechanism, which leads to more favorable results in many sequence processing tasks.\nOverall, RNNs provide a solution for the processing of sequence data through their unique recurrent structure and state-keeping mechanism, making them an indispensable tool in many sequence processing tasks."}, {"title": "C. Transformer", "content": "Despite the progress made in LSTM and GRU, issues with training speed and long-term dependency on RNN-related methods persist until the transformer architecture introduced by Google in 2017[97]. By placing the attention mechanism at its core rather than as a supportive component, the model revo- lutionizes the parallel sequence processing of data. Typically, the mechanism weighs the importance of different elements in a sequence, allowing the model to efficiently capture long-range dependencies and focus on the most relevant parts.\nAs shown in Figure 10, the transformer consists of two main parts: multiple layers j (i.e., j = N) of an encoder and a decoder that have very similar structures but serve different purposes. The former is responsible for encoding the input sequence into a context-rich representation; while the latter is used to generate new information in a sequence based on the given input and its encoded representation.\nBefore the input sequence is fed into the encoder, it is first tokenized, similar to that in early rule-based and statistical methods. What sets the transformer apart is the embedding process, mapping each token into a dense vector representation with a dimension of $dmodel$. Note that such dense vectors are"}, {"title": "D. Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs)", "content": "Built on these backbone techniques, the introduction of VAE and GAN represents further progress in addressing the limita- tions of earlier models, particularly in generating diverse and high-quality content. VAEs are effective at creating variations by learning data distributions. At the same time, GANs stand out for creating realistic images and videos through unique adversarial training, collectively pushing the boundaries of machine-generated creativity.\n1) VAEs: The predecessor of VAEs is Autoencoder (AE) [103], an artificial neural network for unsupervised learn- ing tasks. AEs are commonly used to handle tasks such as dimensionality reduction, feature learning, and data noise reduction and reconstruction. They compress input data into a low-dimensional latent space through an encoder and then reconstruct the data from the latent space using a decoder. The biggest difference between VAEs and AEs is the proba- bilistic layer VAEs introduce in the latent space, allowing them to sample and generate new data. Known for its generative"}, {"title": "VII. TRANSFER LEARNING AND PRE-TRAINED MODELS", "content": "As AI has evolved to date, a persistent challenge is that model training often relies heavily on large, meticulously la- beled datasets and time-consuming feature engineering. These requirements make traditional deep learning methods costly and complex. To address this issue, transfer learning (TL) is designed to reuse knowledge from existing models to solve new problems. By transferring the \"experience\" of a pre- trained model on a large dataset to a new, relevant task, transfer learning greatly reduces the time, data, and resources required to train a model from scratch.\nThe basic idea of TL may seem straightforward, but under- standing what and how to transfer is essential for meaningful progress. To that end, researchers have categorized TLs into four key types based on \u201cwhat is delivered\u201d. In instance-based TL, specific data points from a source domain are reused when they closely match those in a corresponding target domain. Feature-based TL transforms the features learned in the source domain so they can be applied in the target domain, even if the tasks differ, while relationship-based TL capitalizes on data relationships within the source domain to enhance performance in the target domain, particularly when shared patterns exist. Finally, model-based TL involves fine-tuning a pre-trained model from the source domain and adjusting its parameters to suit the new task's specific needs.\nIt is worth noting that in addition to being categorized based on the type of knowledge transferred, TL can also be categorized from other perspectives. For instance, based on the nature of tasks, TL can be classified into isomorphic transfer[113], where source and target tasks are similar and heterogeneous transfer, where they are different. Similarly, TL can be divided into supervised [114], semi-supervised [115], and unsupervised [116] based on the level of supervision in the source and target domains. For clarity, this paper categorizes all TLs based on the type of transferred knowledge and provides details on their differences and commonalities in Sub- section VII.A.\nAfter deciding what to transfer, the next focus is on \"how to transfer\" knowledge effectively across tasks. As shown in Figure 16, many of the steps in TL echo those in deep learning, especially in feature pre- and post-processing. However, a key distinction in TL is the step of choosing a suitable source model. This model needs to provide useful feature represen- tations and knowledge to support the learning of new tasks. Once the source model is chosen, its transferred knowledge is leveraged and fine-tuned to adapt to the specific requirements of the target tasks, resulting in a target model tailored to the new application. This adaptability makes transfer learning"}, {"title": "A. Fundamentals", "content": "In this subsection, we provide a little more detailed ex- planation of the four types of TL according to the type of knowledge transferred. They are instance-based TL, feature- based TL, relationship-based TL, and model-based TL.\n1) Instance-Based TL: Instance-based TL focuses on se- lectively reusing relevant samples from the source domain through different weights to enhance learning in the target domain, particularly when the two domains have overlapping data instances [117]. For example, when classifying rare orchids using ample labeled data for common flowers, more similar flowers like lilies are given higher weights, while less relevant ones, like roses, are given lower weights or ignored. This approach improves generalization to the target task by identifying and weighting the most useful source instances, especially when the target domain has limited data.\n2) Feature-Based TL: Feature-based TL extracts transfer- able representations by mapping raw data from both the source and target domains into a shared feature space [118]. For example, a pre-trained ResNet model can be used to extract features such as edges and textures from images in the source domain, which can serve as inputs for building a new face detection model in the target domain without modifying the ResNet. This process minimizes domain differences, making it ideal for scenarios where source and target domains share underlying structures, even if tasks differ [119].\n3) Relationship-Based TL: Instead of transferring individ- ual features or instances, relationship-based TL focuses on relationships and dependencies between data points. This approach is especially effective for tasks where relationships are essential and the relational structures in the source and target domains are similar. For instance, a knowledge graph of side effects associated with existing drugs can be leveraged to infer potential effects for new drugs [120].\n4) Model-Based TL: Model-based TL focuses on reusing the entire model architecture and its learned parameters from source tasks for targeted tasks [121]. Typically, the reusable model is pre-trained on a large, related dataset. To adapt it to a new application, the early layers are frozen to retain general features while the later layers are fine-tuned for task- specific adaption. The number of layers retrained depends on the complexity of the target task. For example, a small dataset of manufacturing defect images can be used to fine-tune a pre-trained ResNet, allowing it to recognize defect-specific patterns [122]. By fine-tuning or retraining partial layers of the model, the approach significantly improves the model's efficiency and target tasks [123]."}, {"title": "B. Example", "content": "The emergence of the Large Language Model (LLM) pro- vides a solid foundation for advancing TL techniques. Their versatility allows them to be applied across the aforemen- tioned TL categories, enabling broader and more effective applications across diverse domains. Among the currently popular LLMs, such as OpenAI's GPT[124]][1], Google's Gemini (Bard)[125], Meta's Llama series[2], and Anthropic's Claude [126], Meta's Llama series stands out as a well-known example. The latest model, Llama 3, offers high capacity and open-source accessibility, making it easy for fine-tuning and retraining, which meets various application needs. Therefore, we choose Llama 3 to exemplify the TL process in responding to the same prompt: \"Generate a research question with the key words: artificial intelligence, healthcare, and ethical implications\".\nTo achieve \"Transfer\", Llama 3 has two important transfer steps: pre-train and fine-turning. Llama 3 is pre-trained on a massive dataset. The process allows it to grasp grammar, semantics, and even some world knowledge. This aligns with Model-Based TL, where the transformer architecture itself facilitates knowledge transfer to various downstream tasks. During pre-training, Llama 3 also extracts transferable feature representations, such as token embeddings and positional encodings (RoPE[127]). These features capture universal lin- guistic patterns, laying the groundwork for subsequent fine- tuning.\nWhile pre-training provides a foundation, the fine-tuning step, on the other hand, achieves a specialization in the prob- lematic research area. Llama 3 usually hones in on datasets related to AI and ethics. The fine-tuning process refines its vast knowledge \"library\" by deepening its understanding of these areas, and also extends to the unfamiliar healthcare field. After selecting a proper framework like LoRA[128] or QLoRA[129], the source model is adapted to the new target field with our fed healthcare-related dataset. This process also grasps the nuances, such as the specific applications of AI in healthcare (e.g., diagnosis and treatment), and the ethical challenges they pose (e.g., bias and privacy issues). In addition, fine-tuning strengthens connections within the knowledge base, also known as relationship-based transfer, helping Llama 3 identify and prioritize relevant information. Compared to previous stages' examples, Llama 3 learns to rec- ognize the intersection of AI, healthcare, and ethics, allowing it to generate research questions that focus precisely on these critical and interdisciplinary topics.\nWhen prompted, Llama 3 generates a response using a combination of pre-trained and fine-tuned knowledge. First, it identifies key concepts in the request, such as \u201cartificial intelligence,\u201d \u201chealthcare,\u201d and \u201cethical implications,\u201d and considers them as core elements. It then formulates a question based on its understanding of the structure of the research"}, {"title": "VIII. AIGC LIMITATIONS AND CHALLENGES", "content": "AIGC technology has been under development for quite some time, and it is now accessible and usable by a wide range of users, demonstrating undeniable potential. However, there are several limitations and challenges that AIGC presents, which we must overcome to advance its development and applications. We categorize them into two primary directions: data-related issues and model structure limitations, discussed in Subsections A and B, respectively."}, {"title": "A. Data-related limitations and challenges", "content": "As elaborated earlier, data is the key for AIGC regardless of which types of methods are used for content creation. The greater the volume and dimensions of data, the more knowledge methods can gather, and the better content to be created. Dependency on data is also the source of many limitations and challenges.\nFirst, the quality of AIGC is constrained by how much data the methods have \"seen\" and understand. The availability and accessibility of data remain the barriers that current AIGC struggles to overcome while research advancements in areas, such as transfer learning outlined in Section VII, few-short, and zero-short learning [131], strive to reduce this reliance. Inevitably, biases, incorrect or even inappropriate content[132] might be present in the available training data, causing AIGC systems to perpetuate misinformation or unfair results. Therefore, data augmentation, such as data filtering[133], resampling[134], adversarial training[135], synthetic data generation[4], and data validation[136], is essential for diverse and balanced training datasets. Additionally, establishing ethical guidelines and legal frameworks [137] and conducting regular bias audits, although out of the scope of this paper, are also crucial to mitigating risks and ensuring fairness in AIGC.\nTraining data only presents past information, which can cause generated content to appear right but is in fact wrong, leading to hallucinations. That is because AIGC models can't access up-to-date knowledge [138] during training. Recent de- velopment in Retrieval Augmented Generation (RAG) presents a promising solution [139]. By combining the strengths of retrieval-based and generation-based approaches, models' gen- erative abilities are enhanced \u2013 extremely critical for applica- tions, such as in fields like law and medicine, where real-time factual accuracy is essential."}, {"title": "B. Model-structure-related limitations and challenges", "content": "As AIGC evolves from early rule-based systems to deep learning, the size and sophistication of models have been increasing dramatically to improve content generation. The growth in model structures demands more processing power, memory, and storage to handle vast amounts of data necessary for both training and real-time operations. Such large capital investment in time and power limits the widespread adoption of AIGC. Although there are no immediate solutions, some po- tential long-term directions include advancements in hardware accelerators (e.g., GPUs and TPUs [140]), memory storage technologies, and renewable energy to lower computational resource consumption and costs. Recent efforts in model optimization[141] and decentralized training across multiple devices with local data [142] present promising avenues for reducing computational burdens.\nDeep learning is often viewed as a \"black box\" [143]. This lack of transparency makes it difficult for users to understand and trust its processes and outcomes fully. As a result, deep-learning-based AIGC faces similar trustworthiness issues, and the reduced confidence in generated content limits its broader acceptance. However, ongoing research on explainable AI [144][145] and trustworthy AI[146] is offering promising solutions to address these challenges.\nSimilarly, machine learning models are vulnerable to the introduction of adversarial or malicious data, which degrades their performance over time. This vulnerability extends to AIGC systems, as they rely heavily on these machine learning models. Adversarial attacks could subtly manipulate the input data to mislead model generation or directly distort the out- puts. In either case, the result is incorrect, harmful, or biased content. While there has been limited work on integrating defense techniques into AIGC, the trustworthiness and security of generated content are becoming increasingly important as AIGC is applied in sensitive areas such as news generation, ed- ucation, healthcare, and automated legal drafting. Techniques like adversarial training (e.g., Fast Gradient Sign Method [147] and Projected Gradient Descent [148]) and robust optimization [149], which have been extensively used in machine learning, are worth exploring in future developments of AIGC to ensure more secure and reliable content generation.\nFinally, ensuring coherence and relevance in AIGC while preserving its creativity is a significant challenge and a long- term goal for its development. While there are no definitive solutions yet, we believe that human-in-the-loop AIGC is the way forward. As we progress toward a human-centric indus- try 5.0, improved human-machine collaboration will lead to higher-quality and more innovative generated content, making it an ideal approach to achieve AIGVC's essential objectives."}, {"title": "C. Beyond technique issues", "content": "In addition to these core technical limitations, there are ethical and policy-related concerns. AIGC systems can be exploited for malicious activities, such as scams and phishing[150], while issues like copyright disputes[151], data privacy [152], and academic integrity continue to emerge. The ability of AIGC to amplify biases in training data, compromise"}, {"title": "IX. CONCLUSION", "content": "This comprehensive survey reviews the development of AIGC, charting its evolution from basic rule-based systems to the sophisticated deep and transfer learning models that define the current state of AIGC. By dissecting the various develop- ments in statistical modeling, deep learning innovations, and the emerging field of transfer learning, we also shed light on key breakthroughs in expanding generative AI capabilities and applications.\nIn addressing the constraints and challenges of the AIGC development process, we point out the multifaceted challenges and ethical dilemmas posed by the rapid evolution of AIGC technology. From longstanding struggles with data bias and model interpretability to new threats to copyright integrity and content authenticity, we emphasize the importance of techno- logical breakthroughs and framework upgrades and suggest that stakeholders must work together to develop guidelines to ensure responsible development and deployment of AIGC technologies.\nThis paper aims to provide readers with a nuanced un- derstanding of AIGC's history, current status, and future prospects. By highlighting the technological focus at different milestones, we offer insights into the progress and challenges of AIGC. Our hope is to inspire collective efforts toward shaping a future for AIGC that is not only technically sound but also trustworthy, equitable, and human-centered. It is with this expectation that we invite further research, dialog, and innovation in the evolving field of AI-generated content."}]}