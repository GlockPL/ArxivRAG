{"title": "D2Vformer: A Flexible Time Series Prediction Model\nBased on Time Position Embedding", "authors": ["Xiaobao Song", "Hao Wang", "Liwei Deng", "Yuxin He", "Wenming Cao", "Chi-Sing Leung"], "abstract": "Time position embeddings capture the positional information of time steps, often\nserving as auxiliary inputs to enhance the predictive capabilities of time series\nmodels. However, existing models exhibit limitations in capturing intricate time\npositional information and effectively utilizing these embeddings. To address these\nlimitations, this paper proposes a novel model called D2Vformer. Unlike typical\nprediction methods that rely on RNNs or Transformers, this approach can directly\nhandle scenarios where the predicted sequence is not adjacent to the input sequence\nor where its length dynamically changes. In comparison to conventional meth-\nods, D2Vformer undoubtedly saves a significant amount of training resources. In\nD2Vformer, the Date2Vec module uses the timestamp information and feature se-\nquences to generate time position embeddings. Afterward, D2Vformer introduces\na new fusion block that utilizes an attention mechanism to explore the similarity in\ntime positions between the embeddings of the input sequence and the predicted se-\nquence, thereby generating predictions based on this similarity. Through extensive\nexperiments on six datasets, we demonstrate that Date2Vec outperforms other time\nposition embedding methods, and D2Vformer surpasses state-of-the-art methods\nin both fixed-length and variable-length prediction tasks.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting plays a pivotal role in various real-world applications, including resource\nallocation[1, 2], traffic state prediction[3, 4, 5], and weather alert[6, 7]. Initially, statistical methods\nsuch as exponential smoothing[8] and auto-regressive moving averages (ARMA)[9] were commonly\nused for time series forecasting. With the advancement of deep learning, an increasing number of deep\nlearning models have been proposed and applied in this field. Convolutional Neural Network(CNN)\nbased methods(e.g., TCN[10]) use convolution and pooling operations to extract time-series fea-\ntures and aggregate time step information. Recurrent Neural Network(RNN) based methods(e.g.,\nDeepAR[11], LSTNet[12]) utilize recurrent networks to extract time-series features and predict\nsequences. Transformer-based methods(e.g., Informer[13], TDformer[14]) leverage attention mech-\nanisms to capture the similarity relationships between time steps and make predictions using an\nencoder-decoder architecture.\nDLinear[15] posits that while these models demonstrate good predictive performance in specific time\nseries forecasting scenarios, they often lack a comprehensive exploration of time position information.\nThrough empirical experiments, we have demonstrated the existence of this issue in these models. In\nthe experiments, we disrupt the time position information by shuffling the order of the input sequence,"}, {"title": "2 Background", "content": "In time series prediction tasks, models typically extract temporal features from input historical\nsequences and then utilize these features to generate predicted sequences. Given an input historical\ntime series $X \\in R^{L \\times D}$ (L is the length of the historical series and D denotes the variable dimension)\nand the predicted time series $\\hat{Y} \\in R^{O \\times D}$ (O denotes the prediction length), the regular time series\nprediction task can be expressed as:\n$\\hat{Y} = model(f(X)),$                                                                                                (1)\nwhere $f(\\cdot)$ represents the feature extraction module, $model(\\cdot)$ represents the prediction model. To\nimprove performance, some prediction models incorporate time position embedding methods. These\nmethods typically utilize timestamp information as input to generate embeddings capturing the\npositional relationships between time steps. Taking time series prediction tasks with dates as the\nbasic unit, time position embedding involves converting each time step into a date vector, as detailed\nin Appendix A.1. Thus, the time position information of the input sequence is represented as an\nembedding matrix $D_x \\in R^{L \\times M}$, and similarly for the predicted sequence, $D_y \\in R^{O \\times M}$, where M\nis the dimension of the date vector. These time position embeddings are usually concatenated with\ntemporal features as inputs to the prediction model. Consequently, time series models with time\nposition embedding methods can be described as:\n$\\hat{Y} = model(Concat(f(X), E(D_x))),$                                                                             (2)\nwhere $E(\\cdot)$ represents the time position embedding method, and $Concat(\\cdot, \\cdot)$ is the concatenation\noperation. However, these models still face two primary challenges. First, existing time position\nembedding methods struggle to fully capture the complex patterns in the time series. Second, simply\nconcatenating the time position embeddings with the input feature can not effectively utilize this\ninformation. To address these issues, we introduce a new time position embedding method called\nDate2Vec (D2V) and design a novel Fusion Block to enhance the model's utilization of time position\nembeddings. Thus, we propose the D2Vformer model, as detailed below:\n$\\hat{Y} = Fusion(f(X), D2V(f(X), D_x), D2V(f(X), D_y)).$                                       (3)\nThe regular time series prediction tasks, depicted on the left side of Figure 2, involve an input\nsequence of fixed length 'Seq' and a predicted sequence of fixed length 'Pred', both being adjacent\nin timestamps. However, in real-world scenarios, the model's predicted sequence length during\ninference may dynamically change, and the input and predicted sequences may not be adjacent, as"}, {"title": "3 Methodology", "content": "In general, a time series comprises feature sequences aligned with timestamps. Current methods often\nfocus solely on analyzing the feature sequences, overlooking the valuable information contained in\neach timestamp. To address this limitation, we introduce a novel approach called Date2Vec(D2V)\nfor time position embedding. This method encodes the position information for each time step and\nincorporates relationships between different positions implied by the feature sequence. Additionally,\nwe propose a new fusion architecture for integrating position information, enhancing the utilization\nof time position embedding and feature sequence, thus improving prediction accuracy. We refer to\nthis proposed model as D2Vformer, which allows for flexible prediction."}, {"title": "3.1 Overall architecture of D2Vformer", "content": "Figure 3 illustrates the overall architecture of our D2Vformer model, which comprises a Temporal\nFeature Extraction (TFE) module, a Date2Vec (D2V) module, and a Fusion Block. Here, we provide\na brief overview of the overall architecture. Detailed explanations of the TFE module, the D2V\nmodule, and the Fusion Block can be found in the following subsections.\nUnlike most existing time series prediction methods, the D2Vformer model's input includes not only\nthe date matrix $D_x$ of the input time series but also the date matrix $D_y$ corresponding to the predicted\nsequence and the feature sequence X. Since the date matrix can be known beforehand, this approach\ndoes not lead to data leakage. D2Vformer utilizes the TFE module to capture the features from the\nfeature sequence X and input them, along with the date matrices $D_x$ and $D_y$, into the D2V module\nto generate the time position embeddings. This process can be summarized as follows:\n$T = TFE(X),$                                                                                                        (4)\n$\\hat{D_x}, \\hat{D_y} = D2V(T, D_x, D_y),$                                                                                  (5)\nwhere $T \\in R^{L \\times H}$ denotes the features of the input sequence with dimension H. $\\hat{D_x} \\in R^{L \\times H \\times (k+1)}$\ndenotes the time position embedding of the input sequence, and $\\hat{D_y} \\in R^{O \\times H \\times (k+1)}$ denotes the\ncorresponding time position embedding of the predicted sequence. $(k + 1)$ denotes the dimension of\nthe time position embedding, comprising of one linear component and k frequency components. The"}, {"title": "3.2 Temporal Feature Extraction Module", "content": "The Temporal Feature Extraction Module (TFE) is designed to extract the features from the input\nsequences, and the output features will be utilized in D2V. To enhance the extraction of features from\nthe input sequence, TFE incorporates RevIN[30] as a preprocessing step to eliminate non-stationary\ninformation. By employing RevIN, the TFE module effectively stabilizes the input sequence, thus\nreducing the complexity of extracting features. Inspired by DLinear[15], TFE also utilizes a linear\nlayer to extract temporal features T, ensuring the simplicity and efficiency of this module. Specifically,\nTFE can be represented as:\n$T = RevIN(X)W + B,$                                                                                               (7)\nwhere $W \\in R^{D \\times H}$ is the weight matrix, $B \\in R^{L \\times H}$ is the bias matrix. As TFE utilizes RevIN\nto eliminate non-stationary information from the input time series, it is necessary to restore the\nnon-stationary information[30] in its output after the Fusion Block completes the prediction."}, {"title": "3.3 Date2Vec", "content": "We introduce a novel method for time position embedding called Date2Vec (D2V). The architecture\nof D2V is illustrated in Figure 4. This method not only enables the learning of linear patterns in\ntime positions but also facilitates the learning of complex periodic patterns in feature sequences.\nTo achieve this, D2V incorporates the date matrices $D_x \\in\n R^{L \\times M}$ and $D_y \\in R^{O \\times M}$, along with the temporal feature\n$T \\in R^{L \\times H}$ as inputs, where $D_x$ and $D_y$ contain timestamp\ninformation for the input and predicted sequences respectively,\nand T is extracted from the feature sequences via the TFE\nmodule. Specifically, it can be formulated as:\n$\\upsilon_T = \\omega_T T + b_T,$                                                                                                     (8)\n$\\Omega_S = W_S T + B_S,$\n$E = Concat(\\upsilon_T \\otimes D_x + b_1, sin(\\Omega_S \\otimes D_x) + B_2),$\n$F = Concat(\\upsilon_T \\otimes D_y + b_3, sin(\\Omega_S \\otimes D_y) + B_4),$\nwhere $W_T \\in R^{1 \\times L}$, $b_T \\in R^{H}$, $W_S \\in R^{k \\times L}$, $B_S \\in R^{k \\times H}$,\n$b_1, b_3 \\in R^{H}$, $B_2, B_4 \\in R^{k \\times H}$ are trainable parameters,\n$\\upsilon_T \\in R^{H}$ is the linear component and $\\Omega_S \\in R^{k \\times H}$ is the\nfrequency component that includes k frequencies. $\\otimes$ denotes\nthe Kronecker product, and the concatenation operation is\nrepresented as $Concat(\\cdot, \\cdot)$. $E \\in R^{(k+1) \\times H \\times L \\times M}$ represents\nthe time position embedding of the input sequence, while\n$F \\in R^{(k+1) \\times H \\times O \\times M}$ denotes the time position embedding\nof the prediction. To mitigate the space complexity of D2V, we\naggregate the M elements within the time position embedding:\n$\\hat{D_x} = \\sum E_{i,j,k,l}, \\hat{D_y} = \\sum F_{i,j,k,l},$                                                                (11)\nwhere $\\hat{D_x} \\in R^{(k+1) \\times H \\times L}$ is the final time position embed-\nding of the input time series and $\\hat{D_y} \\in R^{(k+1) \\times H \\times O}$ is the\nfinal time position embedding of the prediction."}, {"title": "3.4 Fusion Block", "content": "Existing models often treat time position embeddings, whether generated by T2V[31, 32, 33, 34]\nor vanilla Transformer[35, 23, 22], as mere additional information, integrating them with feature\nsequences through simple addition or concatenation operations as input to the prediction model.\nHowever, these methods fail to fully utilize the time position embeddings provided by the D2V\nmodule. To address this limitation, we propose the Fusion Block, which employs an attention\nmechanism to learn the similarity between the time position embeddings of the input and predicted\nsequences, leveraging this similarity to make predictions. The formulation of Fusion Block is as\nfollows:\n$A_{i,j,l} = \\sum_k (\\hat{D_x})_{i,k,l}(\\hat{D_y})_{j,k,i},$                                                                             (12)\n$\\Upsilon_{ij} = \\sum_{l} A_{i,j,l} (T^T)_{i,l},$                                                                                        (13)\n$\\hat{Y} = FeedForward(\\Upsilon),$                                                                                                         (14)\nwhere $\\hat{D_x} \\in R^{H \\times L \\times (k+1)}$ and $\\hat{D_y} \\in R^{H \\times (k+1) \\times O}$ are the results of permuting the dimensions of\n$\\hat{D_x}$ and $\\hat{D_y}$, respectively, such that $(\\hat{D_x})_{i,l,k} = (\\hat{D_x})_{k,i,l}$ and $(\\hat{D_y})_{j,k,i} = (\\hat{D_y})_{k,i,l}$. The tensor\n$A \\in R^{H \\times O \\times L}$ represents the attention scores, where each element denotes the similarity between the\ninput and predicted time steps. Then, we multiply the attention scores A by the transposed temporal\nfeature $T^T$ to obtain $\\Upsilon \\in R^{H \\times O}$. Finally, we apply $FeedForward(\\cdot)$ to $\\Upsilon$ to get the predicted\nsequence $\\hat{Y} \\in R^{D \\times O}$, where $FeedForward(\\cdot)$ consists of two fully connected layers to ensure the\nfeature dimension of the predicted sequence meets the requirements.\nExisting time series prediction models often require retraining and redeployment to achieve flexible\nprediction. In contrast, D2Vformer employs the Kronecker product within its D2V module to generate\ntime position embeddings for both input and output sequences. Then, in the Fusion Block, it computes\ntheir similarity using batch matrix multiplication and subsequently multiplies the result with the\ntransposed temporal features through batch matrix multiplication. Finally, the predicted output is\nobtained via FeedForward(\u00b7). It is worth noting that in D2Vformer, the dimensions of all trainable\nparameters are unaffected by the prediction length O. Moreover, none of these operations necessitate\npredefining the prediction length. Therefore, a key contribution of D2Vformer is its ability to achieve\nflexible prediction."}, {"title": "4 Experiments", "content": "To assess the performance of D2Vformer, we conduct regular time series prediction experiments\nand flexible prediction experiments on six real-world datasets. We compare the experimental results\nobtained from D2Vformer with several state-of-the-art methods. Additionally, we evaluate the\neffectiveness of D2V and Fusion Block through dedicated experiments, comparing it against several\nexisting relevant approaches."}, {"title": "4.1 Settings", "content": "Datesets: The experiments cover datasets from diverse domains such as energy, exchange rates,\nand healthcare. Specifically, for time series forecasting, we utilize ETT[13], Exchange[12] and\nILI[22] datasets, as commonly used in previous studies[22, 23, 17]. The dataset statistics are\nprovided in Appendix A.2. All datasets are divided into training, validation, and test sets in a\nratio of 6:2:2. Train/val/test sets are zero-mean normalized with the mean and std of training set,\nfollowing [22, 15, 36]."}, {"title": "4.2 Regular Time Series Forecasting", "content": "Table 1 shows the forecasting accuracy of D2Vformer compared to baseline models on six datasets,\nwith an input length of 96 and varying prediction lengths. Notably, due to the small size of the\nILI dataset, as per previous time series forecasting studies[22, 23, 17], we set an input length of\n36 for this dataset. The table highlights the best results in bold, and the second-best results are in\nunderlined. Overall, D2Vformer demonstrates superior performance, achieving either the best or\nsecond-best results in MAE and MSE across the majority of cases. For instance, in the ETTm2 dataset\nexperiments, D2Vformer outperforms the second-best model, DLinear, by 1.24% in MSE and 3.07%\nin MAE, for a prediction length of 96. Particularly noteworthy are the substantial improvements\nD2Vformer achieved in the ILI dataset experiments, with an average enhancement of 11.42% in\nMAE and 10.01% in MSE compared to the second-best model across three experiments with different\nprediction lengths in this dataset."}, {"title": "4.3 Flexible Prediction", "content": "Thanks to the D2V module and the Fusion Block, our D2Vformer can predict a time series with\narbitrary positions and lengths by simply inputting the corresponding date vectors. Therefore,\nin flexible prediction scenarios where the positions or lengths of predictions vary, D2Vformer\nsignificantly reduces training costs, as it only requires a single training session.\nNext, we evaluate the performance of D2Vformer in flexible prediction. As illustrated in the right\nfigure of Figure 2, in this experiment, the input sequence and the predicted sequence are not adjacent.\nThere is a specific length of interval between them. Hence, we have three key parameters for this\nexperiment: (1) 'Seq' representing the length of the input sequence, (2) 'Pred*' representing the\nlength of the predicted sequence, and (3) 'Gap' indicating the length of the interval, which separates\nthe input and predicted sequences. During training, D2Vformer learns from a regular time series\nprediction task where the input and prediction sequences are adjacent, and the prediction length is\nset to 'Gap.' During inference, there's an interval of length 'Gap' between the input and prediction\nsequences. In this scenario, we require D2Vformer to directly generate predictions of varying lengths.\nSince other baseline models cannot directly perform interval-based prediction, we extend the length of"}, {"title": "4.4 The Effectiveness of D2V & Fusion Block", "content": "We further investigate the effectiveness of the D2V and Fusion Block. To validate the effectiveness\nof D2V, we compared it with two alternative algorithms, T2V [31] and the position embedding\nmethod used in vanilla Transformer [35], as shown in Table 3. It is observed that the performance\nof D2V is the best in the majority of cases. To assess the impact of different output modules, we\nreplace the Fusion Block with other commonly used output modules. The experimental results are\npresented in Table 4. From the table, it is evident that the Fusion Block outperforms other modules\nin most prediction scenarios. Furthermore, due to its inherent structure, the Fusion Block can meet\nthe requirements of flexible prediction tasks, providing a significant advantage in output flexibility\ncompared to conventional output modules."}, {"title": "4.5 Efficiency Analysis", "content": "In Section 4.3, we mentioned that one ma-\njor advantage of D2Vformer is its signifi-\ncant reduction in training time, particularly\nin variable-length prediction tasks where\nD2Vformer does not require multiple train-\ning deployments. To validate this advan-\ntage of D2Vformer, in this section, we com-\nprehensively analyze the computational ef-\nficiency of D2Vformer. All the experimen-\ntal settings align with the corresponding\nexperiments outlined in Section 4.2 and\nSection 4.3.\nAs shown in Table 5, we have evaluated\nthe computational efficiency of D2Vformer\nand baseline models across various pre-\ndiction scenarios using three key metrics,\nincluding (1) Parameters, indicating the\nnumber of learnable parameters; (2) Infer-\nence, representing the average inference\ntime for a batch of data; (3) Training, denot-\ning the average time required for training\none epoch. It is worth noting that all exper-\nimental results presented are the average\nresults of 50 trials.\nThe number of parameters in D2Vformer\nis independent of the length of the output\ntime series, giving it a significant advan-\ntage over baseline models in terms of the\nnumber of learnable parameters. For the\nprediction tasks with a prediction length\nof 96, as shown in Table 5(a), D2Vformer\nhas 146,038 parameters, second only to the"}, {"title": "5 Conclusion", "content": "Time position is crucial for distinguishing time steps and representing the sequence order. However,\npreliminary experiments indicate that current methods do not fully leverage this information in time"}, {"title": "A Appendix", "content": "In this section, we provide additional information and results from supplementary experiments that\nsupport the main body of the paper."}, {"title": "A.1 Date Embedding", "content": "Our method to generate date vectors follows the methodology proposed by Dateformer[36], which\nincorporates the following features for a given date at a specific time step: abs_day, year, day (month\nday), year day, weekofyear, lunar_year, lunar_month, lunar_day, lunar_year_day, dayofyear, dayof-\nmonth, monthofyear, dayofweek, dayoflunaryear, dayoflunarmonth, monthoflunaryear, jieqiofyear,\njieqi_day, dayofjieqi. These features are encoded using the following equations:\n$abs_day = \\frac{days \\: that \\: have \\: passed \\: from \\: December \\: 31, \\: 2000}{365.25 * 5}$\n$year = \\frac{this \\: year - 1998.5}{25}$\n$day = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: month}{31}$\n$year_day = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: year}{366}$\n$weekof year = \\frac{weeks \\: that \\: have \\: passed \\: in \\: this \\: year}{54}$\n$lunar_year = \\frac{this \\: lunar \\: year - 1998.5}{25}$\n$lunar_month = \\frac{this \\: lunar \\: month}{12}$\n$lunar_day = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: lunar \\: month}{30}$\n$lunar_year_day = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: lunar \\: year}{384}$\n$dayof year = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: year - 1}{total \\: days \\: in \\: this \\: year - 1} - 0.5$\n$dayo fmonth = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: month - 1}{total \\: days \\: in \\: this \\: month - 1} - 0.5$\n$monthof year = \\frac{months \\: that \\: have \\: passed \\: in \\: this \\: year - 1}{11} - 0.5$\n$dayofweek = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: week - 1}{6} - 0.5$\n$dayo flunaryear = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: lunar \\: year - 1}{total \\: days \\: in \\: this \\: lunar \\: year - 1} - 0.5$\n$dayoflunarmonth = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: lunar \\: month - 1}{total \\: days \\: in \\: this \\: lunar \\: month} - 0.5$\n$monthoflunaryear = \\frac{lunar \\: months \\: that \\: have \\: passed \\: in \\: this \\: lunar \\: year - 1}{11} - 0.5$\n$jieqio f year = \\frac{solar \\: terms \\: that \\: have \\: passed \\: in \\: this \\: year - 1}{23} - 0.5$\n$jieqi_day = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: solar \\: term}{15}$\n$dayof jieqi = \\frac{days \\: that \\: have \\: passed \\: in \\: this \\: solar \\: term - 1}{total \\: days \\: in \\: this \\: solar \\: term - 1} - 0.5$"}, {"title": "A.2 Date Statistics", "content": "We use six popular multivariate datasets from different domains in our experiments. Table 6 shows\nan overview of the datasets. In the following, we briefly describe the individual datasets.\n\u2022 ETT (Electricity Transformer Temperature)[13] encompasses data collected from power\ntransformers in a specific region of China, spanning from July 2016 to July 2018. It includes\nseven distinct features, such as the load and oil temperature of the power transformers.\nETTh1 and ETTh2 consist of 17,420 samples with a sampling interval of 1 hour, encompass-\ning 7 feature dimensions. ETTm1 and ETTm2 comprise 69,680 samples with a sampling\ninterval of 5 minutes, encompassing 7 feature dimensions.\n\u2022 Exchange[12] covers daily exchange rate data for eight currencies for the period January\n1990 to October 2010, and it contains eight feature dimensions. The dataset has a sampling\ninterval of 1 day and contains a total of 7588 samples.\n\u2022 ILI (Influenza-Like Illness)[22] encompasses weekly records of patients diagnosed with\ninfluenza illness, as reported by the U.S. Centers for Disease Control and Prevention, span-\nning from 2002 to 2021. This dataset comprises seven key dimensions of characterization,\nincluding features such as the proportion of patients with ILI and the total number of patients.\nIt follows a sampling interval of 1 week and comprises a total of 966 samples."}, {"title": "A.3 Additional Experimental Details", "content": "All experiments are conducted using the PyTorch framework [37] on a single NVIDIA RTX2080TI\n11GB GPU. MSE is used as the loss function, Adam [38] is used as the optimizer, and the\nmini-batch size is set to 16 for all models. The training is capped at 100 epochs, with early\nstopping applied after 5 epochs of no improvement. To minimize the impact of the initial\nlearning rate (lr) on model performance, we experiment with six different initial learning rates:\n$10^{-3},5 \\times 10^{-4}, 10^{-4},5 \\times 10^{-5}, 10^{-5},5\\times10^{-6}$}. Each model is then evaluated using its optimal\nlearning rate for comparison. Additionally, the learning rate (lr) decay strategy for all models is\ndefined as:\n$Ir = lr \\times 0.75^{\\lfloor Poch/2\\rfloor}.$\nFor each Transformer-based comparison model, we use their common parameter settings: the\nencoders stack 2 layers, the decoder stacks 1 layer, the hidden dimensions are set to {512, 1024},\nand the Dropout rate is 0.1. For D2Vformer, the output dimension H of the Feature Extraction\n(TFE) module is set to 512, and the number of frequency components k in the D2V module is set to\n63. Each experiment is repeated five times with different random seeds, and the average of the five\nprediction results is reported as the model's final performance."}, {"title": "A.4 Baselines", "content": "We conduct comparative experiments with four state-of-the-art time series prediction models. Below\nis a brief introduction to each of these approaches.\n\u2022 PatchTST[17] is a Transformer-based model that employs patch slicing method. For the\nILI dataset, given its limited number of samples, we set the patch length to 6 with a stride of\n2. For the other datasets, we set the patch length to 16 with a stride of 2."}, {"title": "A.5 Additional Experiments of Flexible Prediction", "content": "In Table 2 of the original paper, we present the performance of various methods on short-term\ninterval-based time series prediction tasks. In the following, we first provide the complete results\nof this experiment, as shown in Table 7. Compared to Table 2, the main difference is the addition\nof standard deviation statistics for five repeated experiments of each model. From the experimental\nresults, we can see that the standard deviations are relatively small, indicating that the models are\nminimally affected by random parameters during training and that the outputs are stable. Therefore,"}, {"title": "A.6 Additional Experiments for Effectiveness Analysis", "content": "In this section, we conduct supplementary experiments to evaluate the effectiveness of D2Vformer\nand other methods in various flexible prediction scenarios. The results are presented in Table 10, with\nthe left table displaying the outcomes of prediction with varying prediction lengths, and the right table\nexhibiting the results of prediction with varying interval lengths. In a variety of flexible prediction\nscenarios, our D2Vformer demonstrates lower model complexity, as well as reduced training and\ninference times compared to most of the comparative methods. What's even more notable is that, in\ndynamically changing scenarios, while all other methods necessitate retraining and redeployment of\nmodels to accommodate the updated scenario requirements, D2Vformer does not."}]}