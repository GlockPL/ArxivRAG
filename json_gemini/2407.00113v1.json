{"title": "Personalized Federated Continual Learning via Multi-granularity Prompt", "authors": ["Hao Yu", "Xin Yang", "Xin Gao", "Yan Kang", "Hao Wang", "Junbo Zhang", "Tianrui Li"], "abstract": "Personalized Federated Continual Learning (PFCL) is a new practical scenario that poses greater challenges in sharing and personalizing knowledge. PFCL not only relies on knowledge fusion for server aggregation at the global spatial-temporal perspective but also needs model improvement for each client according to the local requirements. Existing methods, whether in Personalized Federated Learning (PFL) or Federated Continual Learning (FCL), have overlooked the multi-granularity representation of knowledge, which can be utilized to overcome Spatial-Temporal Catastrophic Forgetting (STCF) and adopt generalized knowledge to itself by coarse-to-fine human cognitive mechanisms. Moreover, it allows more effectively to personalized shared knowledge, thus serving its own purpose. To this end, we propose a novel concept called multi-granularity prompt, i.e., coarse-grained global prompt acquired through the common model learning process, and fine-grained local prompt used to personalize the generalized representation. The former focuses on efficiently transferring shared global knowledge without spatial forgetting, and the latter emphasizes specific learning of personalized local knowledge to overcome temporal forgetting. In addition, we design a selective prompt fusion mechanism for aggregating knowledge of global prompts distilled from different clients. By the exclusive fusion of coarse-grained knowledge, we achieve the transmission and refinement of common knowledge among clients, further enhancing the performance of personalization. Extensive experiments demonstrate the effectiveness of the proposed method in addressing STCF as well as improving personalized performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Federated Continual Learning (FCL) is a new practical paradigm aiming at fusing knowledge from different times and spaces without catastrophic forgetting in dynamic Federated Learning (FL) settings [47]. Moreover, Personalized Federated Learning (PFL) tries to fuse implicit common knowledge extracted from various clients and personalize the generalized knowledge for better performance on the client side [37]. However, to better accommodate diverse local requirements in highly heterogeneous FCL scenarios, personalization solutions are necessary for leveraging knowledge fused from different spatial and temporal perspectives. Therefore, Personalized Federated Continual Learning (PFCL) is proposed as a combination of PFL and FCL with broader application scenarios.\nPFCL is more challenging than static PFL because of the higher requirements for handling heterogeneous knowledge. On the one hand, it implies accumulating knowledge against spatial-temporal catastrophic forgetting (STCF), which is the main issue of FCL. On the other hand, it also needs to achieve effective extraction and fusion of client-specific and invariant knowledge, ensuring local personalization post the integration of shared knowledge. This is the primary goal of PFL. Both issues can be solved by the multi-granularity representation of knowledge. Therefore, we can effectively address these issues by constructing a multi-granularity knowledge space, as illustrated in Fig. 1. Existing methods have not taken this way into account.\nThe key to personalization lies in the accurate isolation of knowledge, namely, turning local knowledge into client-invariant and client-specific knowledge. By introducing a multi-granularity knowledge space, it is easy to decompose knowledge into coarse-grained, representing common aspects, and fine-grained, representing specific aspects, during the training stage to fulfill the requirements of personalization. Moreover, on the server side, we consolidate client-invariant knowledge while maintaining the distinctiveness of client-specific knowledge with the exclusive fusion of coarse-grained representations.\nFor STCF, the knowledge learned by the deep network is overly fine-grained, making it highly susceptible to degradation in performance, such as the weighted average of parameters, leading to serious forgetting. In contrast, this multi-granularity knowledge representation exhibits stronger robustness against forgetting. On the one hand, using coarse-grained representation for temporal-spatial invariant knowledge makes it easier to transfer and fuse knowledge across time and space. On the other hand, utilizing fine-grained representation captures time-specific and space-specific knowledge, thereby achieving better personalization.\nInspired by the human cognitive process [47], knowledge transfer is based on shared cognition, such as a common language. In this work, we construct a multi-granularity knowledge space by utilizing prompts of different granularity, namely coarse-grained global prompts and fine-grained local prompts, with a pre-trained Vision Transformer (ViT) [8]. Specifically, we employ the pre-trained ViT as the shared public cognition. We train coarse-grained prompts operating at the input without altering internal parameters to represent temporal-spatial invariant knowledge. Subsequently, leveraging the frozen coarse-grained prompts, we train class-wise fine-grained prompts that directly interact with the multi-head self-attention layer as temporal-spatial specific knowledge. This fine-tuning process enhances the model's ability to adapt to local tasks. This coarse-to-fine cognitive approach also aligns with the human cognitive process, where attention is initially directed toward outlines before focusing on details. Finally, we design a selective prompt fusion mechanism on the server side. This novel prompt fusion approach further mitigates spatial forgetting caused by aggregation. The contributions of this paper are summarized as follows:\n\u2022 We formally define a new personalized federated learning scenario called PFCL, which imposes higher demands on knowledge processing while preventing spatial-temporal forgetting. For the first time, we construct a multi-granularity knowledge space in this scenario, effectively addressing these challenges.\n\u2022 We propose a novel method called Federated Multi-Granularity Prompt (FedMGP), which introduces two distinct prompt levels to represent coarse-grained and fine-grained knowledge, respectively. It effectively overcomes STCF while meeting the requirements for personalization.\n\u2022 Extensive experiments demonstrate that our method achieves state-of-the-art performance in two different scenarios of federated continual learning. Moreover, our approach exhibits superior performance in personalizing and retaining temporal and spatial knowledge."}, {"title": "2 RELATED WORK", "content": "2.1 Multi-Granularity Computing\nMulti-granularity computing addresses the challenge of tackling the coexistence of data with different granularities [45, 46]. Extracting multi-granularity knowledge benefits our understanding of materials and their intrinsic properties.\nVL-PET [13] designs a multi-granularity controlled mechanism to impose control on modular modifications of the pre-trained language model at coarse and fine granularities. [5] constructs a question-answering dataset with yearly, monthly, and daily-grained data and proposes MultiQA to address temporally multi-granularity question-answering. [46] adopts the sequential three-way decision method to extract knowledge of different granularities in open-topic classification tasks. [42] decouples the objects of group re-identification tasks into individual, subgroup, and entire group granularities to handle the dynamic changes in group layout and member variations. [32] introduces granular computing in FL and achieves automatic neural architecture search to adapt the different information granularity across clients. [28] achieves a fine-grained knowledge fusion with layer-wised aggregation. PartialFed [35] transfers cross-domain knowledge of adaptive granularity among clients by automatically switching the learning strategy. [3] utilizes bi-directional guidance with a prior attention mechanism to transfer coarse-grained and fine-grained knowledge among multi-scale local models in an extremely heterogeneous federated system. [26] proposes a hierarchical FL framework to reduce the communication overhead, conducting model aggregation at two granularities.\nCurrently, the core concept of multi-granularity cognition has been gradually embraced by the general public and is progressively being applied to scenarios involving spatial-temporal changes. For the traffic accident predictions, given the dynamic nature of road networks and expanding urban areas, it is challenging when the spatial-temporal granularity of forecasting improves due to the rarity of accident records and the complexity of long-term future dependencies. To address these challenges, [50] propose a unified framework named RiskSeq, which is designed to foresee sparse urban accidents with finer granularities and multiple steps from a spatial-temporal perspective. This approach aims to enhance the accuracy and detail of accident predictions, thereby improving the efficiency of police force allocation and traffic management strategies. For the traffic flow predictions, not only should it consider the temporal dependencies that exist between different nodes in the network, but also the spatial correlations between nodes. [10] propose a Global Spatial-Temporal Network (GSTNet), which is composed of multiple spatial-temporal blocks, in order to capture the global dynamic spatial-temporal correlations.\nHowever, there is currently very limited research involving multi-granularity knowledge transfer in federated learning, and there is almost no research on using multi-granularity knowledge to address the spatial-temporal catastrophic forgetting in FCL. In this paper, we retain fine-grained knowledge in the local prompts and coarse-grained knowledge in the global prompts to achieve spatial-temporal knowledge fusion across tasks and clients."}, {"title": "2.2 Prompt-Based Continual Learning", "content": "Continual Learning (CL) aims to overcome catastrophic forgetting of the previous knowledge after training on new data in non-stationary task streams [6]. Various CL techniques [21, 29, 30] have been proposed to alleviate catastrophic forgetting and achieve knowledge transfer across tasks, including regularization, rehearsal, parameter isolation, and knowledge distillation.\nRecent works introduce prompt learning to CL to achieve more efficient exemplar-free CL [34, 40, 41]. Prompt learning is a novel transfer learning technique applied to adapt general knowledge of pre-trained large language or vision models to downstream tasks by optimizing prompts [14, 15, 20, 49]. CoOp [49] integrates learnable prompts in the vision-language model to facilitate end-to-end learning where the design of task-specific prompts is fully automated. L2P [41] applies learnable task-specific prompts to mitigate forgetting and even outperforms exemplar-based methods in accuracy and efficiency. DualPrompt [40] decouples the learnable prompts into general and expert prompts, encoding task-invariant and task-specific knowledge, respectively. CODA [34] replaces key-value pairs in the prompt selection strategy with an attention-based end-to-end scheme. Pro-KT [25] attaches complementary prompts to a pre-trained large model to efficiently transfer task-aware and task-specific knowledge. LGCL [17] mitigates forgetting in extremely heterogeneous task streams, where the class set of each task is disjoint, by improving the key lookup of the prompt pool and mapping the output feature to class-level language representation.\nIn this paper, we design a local prompt and a global prompt mechanism to extract and encode coarse-grained and fine-grained knowledge, achieving spatial-temporal knowledge transfer."}, {"title": "2.3 Personalized Federated Learning", "content": "Personalized Federated Learning (PFL) focuses on training customized models to accommodate various preferences and requirements of clients in heterogeneous FL. Existing works on PFL can be categorized into data-based and model-based approaches [37]. Per-FedAvg [9] designs a Model-Agnostic Meta-Learning (MAML) framework to find a generalized global model. It trains personalized local models derived from the shared global model. pFedMe [36] integrates L2-norm regularization in the loss function to adaptively control the balance between personalization and generalization in federated MAML. Ditto [22] adds a regularization term in the local objectives as the loss function of the local adaptation process but aggregates the models before the local adaptation to strike a balance of personalization and generalization. FedSteg [44] enables domain adaptation from the shared global model to personalized local models by adding a correlation alignment layer before the softmax layer. FedPer [1, 33] decouples the model into base layers and personalized layers and aggregates the shallow base layers to capture generic knowledge while retaining the deep personalized layers locally to maintain personalized knowledge. FedMSplit [4] adopts multi-task learning to fit related but personalized models for clients. FedCE [2] clusters the clients into several groups based on the similarity of local data distributions and trains multiple global models for each group. FedCP[48] proposes an auxiliary Conditional Policy Network to achieve more fine-grained personalization with sample-wise feature separation. [38] conducts clustering by analyzing the principal angles of local data in the subspaces and delays the training stage until the clustering is accomplished. These works do not explicitly explore the multi-granular knowledge in the processes of generalization and personalization.\nSome recent works incorporated prompting learning methods into PFL. pFedPG [43] utilizes personalized prompt generation globally and personalized prompt adaptation locally to achieve PFL under heterogeneous data. pFedPrompt [11] extracts user consensus from linguistic space and adapts to local characteristics in visual space in a non-parametric manner.\nHowever, extracting and fusing spatial-temporal multi-granular knowledge via prompting to overcome catastrophic forgetting and data heterogeneity has not yet been implemented in PFCL."}, {"title": "3 PROBLEM DEFINITION", "content": "3.1 Personalized Federated Continual Learning\nThe primary goal of PFCL is to accumulate and fuse knowledge from different times and spaces. Clients employ suitable personalized strategies to make the received generalized knowledge better adapted to the characteristics of local data and effectively meet the requirements of local tasks. However, due to FCL itself, PFCL is also susceptible to severe spatial-temporal catastrophic forgetting. Therefore, PFCL has three main objectives. The first is to form more generalized knowledge during server knowledge fusion, avoiding spatial catastrophic forgetting caused by heterogeneous data. The second is for clients to adopt appropriate strategies to overcome temporal forgetting resulting from continual learning. The third is for clients to employ suitable personalization strategies, ensuring that the received generalized global model better adapts to the local task requirements and characteristics of local data.\nNow, we extend the traditional FL to PFCL.\n\u2022 Given a clients (denoted as \\(A = \\{A_1, A_2, ..., A_a\\}\\)), and a central server (denoted as S), each client \\(\\{A_i, 1 \\leq i \\leq a\\}\\) has its unique task sequence \\(T_i\\), where each task encompasses different classes. The task sequence of client \\(A_i\\) is denoted as \\(T_i = \\{T^1, T^2, ..., T^{n_i}\\}\\), where \\(n_i\\) represents the total number of tasks on client \\(A_i\\). The k-th task of \\(T_i\\) contains \\(C_k\\) classes, and \\(C_k = \\{C^{k_1}, C^{k_2}, ..., C^{k_{r_i}}\\}\\).\n\u2022 During the training of task r, the global model on the server already possesses the knowledge of \\(T^1\\) to \\(T^{r-1}\\) from client \\(\\{A_i, 1 \\leq i \\leq a\\}\\). The server S then distributes it back to clients. After personalizing the received global model \\(\\theta_S^{r-1}\\), the client \\(A_i\\) continually trains it on \\(T_i^r\\) as the initial model to get the new local model. The local model \\(\\theta_i^r\\) should perform well in classifying classes from the set \\(\\{C_i^k\\} \\cup C_i^1 \\cup...\\cup C_i^{k-1}\\).\n\u2022 Finally, the server collects the local models from clients who participate in FCL and obtains a new global model \\(\\theta_S^r\\), which has more generalized knowledge of learned tasks from all clients. Clients need to adopt appropriate strategies to personalize the global model \\(\\theta_S^r\\), enabling it to perform better locally.\nAccording to the similarity of task sequences among clients, FCL can be initially divided into two scenarios: synchronous FCL and asynchronous FCL [47]. We will discuss it in detail in Sec. 5.1.2."}, {"title": "3.2 Spatial-Temporal Catastrophic Forgetting", "content": "Catastrophic Forgetting is a fundamental challenge in CL, which refers to a phenomenon that a model would forget the knowledge learned on old tasks when training on new tasks [6]. The reason for catastrophic forgetting is that the well-learned network parameters on the old tasks are overwritten during training on the new tasks [47].\nIn the FCL setting, catastrophic forgetting exists as well. In a real-world scenario, data reaches clients consecutively through task streams [24], causing temporal catastrophic forgetting. At the aggregation stage, the central server collects local models and aggregates them into one global model. Then, the server distributes the global model back to clients. Local models are trained with different training data. Aggregating them leads to the overwriting of certain task-specific crucial parameters, consequently causing a decline in the performance of the global model on local-specific tasks. Adopting the global model consolidated such conflict knowledge exacerbates the temporal catastrophic forgetting of each client's previous tasks.\nThe fundamental reason for STCF is that the knowledge represented by the model's parameters is too fine-grained, leading to a lack of robustness against minor variations. Therefore, it is necessary to represent knowledge in a multi-granularity way. Splitting it into coarse-grained spatial-temporal-invariant knowledge and fine-grained spatial-temporal-specific knowledge and handling them separately can effectively overcome STCF.\nWe design Temporal Knowledge Retention to measure the effectiveness of temporal knowledge transfer and Spatial Knowledge Retention to measure the effectiveness of spatial knowledge transfer in PFCL.\nDefinition 1. (Temporal Knowledge Retention) Given a federated learning system with a clients, the temporal knowledge retention is defined as:\n\\[K R_{t}=\\frac{1}{a} \\sum_{i=1}^{a} \\frac{\\operatorname{Acc}\\left(\\theta_{i}^{r} ; T_{i}^{0}\\right)}{\\operatorname{Acc}\\left(\\theta_{i}^{0} ; T_{i}^{0}\\right)}\\]\nwhere \\(\\operatorname{Acc}\\left(\\theta_{i}^{r} ; T_{i}^{0}\\right)\\) denotes the test accuracy of client \\(A_i\\)'s local model at r-th round on the 0-th task and \\(\\operatorname{Acc}\\left(\\theta_{i}^{0} ; T_{i}^{0}\\right)\\) denotes the test accuracy of client \\(A_i\\)'s local model at the initial round on the 0-th task.\nDefinition 2. (Spatial Knowledge Retention) Given a federated learning system with a clients, the spatial knowledge retention is defined as:\n\\[K R_{s}=\\frac{1}{a} \\sum_{i=1}^{a} \\frac{\\operatorname{Acc}\\left(\\theta_{S}^{r} ; T_{i}^{r}\\right)}{\\operatorname{Acc}\\left(\\theta_{i}^{r} ; T_{i}^{r}\\right)}\\]\nwhere \\(\\operatorname{Acc}\\left(\\theta_{S}^{r} ; T_{i}^{r}\\right)\\) denotes the accuracy of the global model \\(\\theta_S^r\\) on the current local task \\(T_i^r\\) at client \\(A_i\\) and \\(\\operatorname{Acc}\\left(\\theta_{i}^{r} ; T_{i}^{r}\\right)\\) denotes the accuracy of the local model \\(\\theta_i^r\\) on its current local task \\(T_i^r\\)."}, {"title": "4 MULTI-GRANULARITY PROMPT", "content": "In this section, we elaborate on our proposed Federated Multi-Granularity Prompt (FedMGP), which introduces a multi-granularity knowledge space into PFCL for the first time to better address personalized requirements and spatial-temporal forgetting.\nSpecifically, on the client, we design prompts at two granularity levels for knowledge representation, namely Coarse-grained Global Prompt (see Sec. 4.1) and Fine-grained Local Prompt (see Sec. 4.2). Global prompts represent coarse-grained common knowledge, while local prompts, built upon global prompts, represent class-wise fine-grained knowledge. Only fusing the coarse-grained common knowledge facilitates the formation of generalized knowledge and avoids spatial forgetting caused by aggregating fine-grained knowledge. Local prompts based on global prompts aim to personalize the generalized knowledge from the server while preventing temporal forgetting due to class increments.\nOn the server side, we devise a new approach for fusing global prompts called Selective Prompt Fusion (see Sec. 4.3) without spatial forgetting. Aggregating only coarse-grained knowledge not only enhances aggregation speed but also provides further improvements in privacy protection.\nThe overall framework of the proposed method is shown in Fig. 2, and the algorithm is summarized in algorithm 1."}, {"title": "4.1 Coarse-grained Global Prompt", "content": "Due to the heterogeneity of data, significant differences exist among local models, leading to substantial variations in extracted knowledge. This also poses significant challenges for the fusion and transfer of knowledge, as the knowledge learned by each client is overly fine-grained. Inspired by the cognitive processes of humans, knowledge transfer among humans is effective because there is a fundamental shared cognition, enabling the meaningful exchange of knowledge. Therefore, we assign each client with the same pre-trained ViT model as a foundational cognitive system. With ViT's parameters frozen, clients learn global prompts that operate at the input level. Consequently, these global prompts represent coarse-grained knowledge acquired through the common model learning process. Furthermore, as the knowledge is extracted from the same model, it is more convenient to aggregate knowledge on the server side without spatial forgetting.\nThe training of coarse-grained global prompts is based on the frozen ViT model. Moreover, global prompts operate at the input level, not influencing the model's parameters. The purpose is to extract knowledge into a common space through the same model.\n4.1.1 Global Prompt Pool. Taking inspiration from L2P [41], we devise a prompt pool for storing and selecting the global prompts. The prompt pool is defined as\n\\[P_{g}=\\left\\{P_{1}, P_{2}, \\ldots, P_{M}\\right\\},\\newline\\newline\\]\nwhere m is the pool size and \\(P_i\\) is a single global prompt. Then, let x and \\(E = f_{\\theta}(x)\\) be the input and its corresponding embedding feature, respectively. Denoting \\(\\{s_i\\}\\) be the indices of N global prompts, then we can modify the embedding feature as follows:\n\\[E^{\\prime}=\\left[P_{s_{1}}, \\ldots, P_{s_{N}} ; E\\right], 1 \\leq N \\leq M,\\]\nwhere [;] represents concatenation along the token length dimension. The next question is how to choose global prompts.\n4.1.2 Global Query Function. Due to the use of the same model, similar inputs tend to select similar prompts and vice versa. This mitigates the challenge of aggregating heterogeneous knowledge on the server. Based on this, we have designed a key-value pair-based query strategy that dynamically selects suitable prompts by calculating the similarity between the input key and existing prompts' keys.\nWe associate each prompt in the pool with a learnable key, denoted as \\(\\{(K_1, P_1), (K_2, P_2), ..., (K_m, P_m)\\}\\). To ensure that similar inputs have similar keys, we use the output features of the pre-trained ViT V as the key for the input, i.e., \\(K_{in} = V(E)\\). Then, the query process can be summarized by the following expression:\n\\[K_{S}=\\underset{K_{g}}{\\operatorname{argmin}} \\sum_{i=1}^{N} d i s\\left(K_{i n}, K_{g}^{i}\\right),\\newline\\newline\\]\nwhere \\(K_S\\) denotes the subset of top-N keys selected specifically for the input, and \\(K_g\\) represents the set of keys for all global prompts. In this work, we utilize cosine similarity as the distance function to measure the similarity between keys.\n4.1.3 Optimization for Global Prompt. Each client has a global classification head used for training global prompts, denoted as H. At the beginning of training, it is necessary to load the pre-trained model with H to enable it to perform the classification task, and we denote the model with \\(H_j\\) as \\(V_j\\). Overall, the training loss function is as follows:\n\\[\\min _{H, P_{g}, K_{g}} \\mathbb{L}\\left(V_{j}(E^{\\prime}), y\\right)+\\lambda_{1} \\sum_{K} d i s\\left(K_{i n}, K_{i}\\right),\\newline\\newline\\]\nwhere \\(\\lambda_1\\) is a hyperparameter. The initial term comprises the softmax cross-entropy loss, while the subsequent term serves as a surrogate loss aimed at bringing selected keys closer to their corresponding query features."}, {"title": "4.2 Fine-grained Local Prompts", "content": "Once the training of global prompts is completed, they will be frozen and remain unchanged, including both the prompts themselves and their corresponding keys, until the next task training. Based on the frozen global prompts, we further developed fine-grained class-wise local prompts. These prompts directly impact the model's multi-head self-attention (MSA) [39] layers, facilitating the extraction of local, fine-grained knowledge. Additionally, this fine-grained prompting helps overcome temporal forgetting induced by class increments. The hierarchy of prompts, from coarse to fine, simplifies generalized knowledge extraction, fusion, and personalization.\n4.2.1 From Coarse to Fine. Similarly, a prompt pool is constructed for local prompts. However, since it represents class-specific knowledge, the size of the pool depends on the number of data classes. The local prompt pool is defined as\n\\[P_{l}=\\left\\{\\left(K_{1}^{1}, P_{1}^{1}\\right),\\left(K_{1}^{2}, P_{1}^{2}\\right), \\ldots,\\left(K_{1}^{C}, P_{1}^{C}\\right)\\right\\},\\newline\\newline\\]\nwhere C represents the number of classes. It is precisely the class-wise fine-grained knowledge that imparts significant effectiveness to our approach of personalization and addressing temporal forgetting induced by class increments. It is proved in Sec. 5.3.\n4.2.2 Local Query Function. Fine-grained prompts are selected based on the global prompt, so we must first obtain frozen global prompts by allowing the original input x to undergo the global query function. Subsequently, we concatenate to form an input E' with selected global prompts. Then, similar to obtaining the key for global prompts, we acquire the key for local prompts \\(K_{in} = V(E')\\), with the only difference being that the input is now E'. The subsequent steps of calculating similarity and selection are analogous to the corresponding operations for global prompts.\nNote that we do not employ this querying function during the training phase. Instead, we use mask code to select the local prompt corresponding to the data class for training.\n4.2.3 Optimization for Local Prompt. Local prompts directly operate on the model's MSA layer, where we represent the input query, key, and values as \\(h_q, h_k, h_v\\), respectively. MSA layers can be denoted as:\n\\[\\mathrm{MSA}\\left(h_{q}, h_{k}, h_{v}\\right)=\\operatorname{Concat}\\left(h_{1}, \\ldots, h_{z}\\right) W^{O},\\newline\\newline\\]\nwhere \\(h_i = Attention(h_qW^Q, h_kW^K, h_vW^V)\\). W is the project matrix and z is the number of MSA layers. We use the Prefix Tuning (Pre-T) to tune local prompts. Pre-T splits the local prompt \\(P_l\\) into \\(P_K\\) and \\(P_V\\), and adds them to \\(h_k\\) and \\(h_v\\):\n\\[\\mathrm{MSA}^{\\prime}=\\mathrm{MSA}\\left(h_{q},\\left[P_{K} ; h_{k}\\right],\\left[P_{V} ; h_{v}\\right]\\right).\\newline\\newline\\]\nOnce global prompts have completed training, they freeze along with their corresponding keys. The input x first goes through the global query function to find the corresponding global prompts. Subsequently, the embedding of x is concatenated with the prompts to form E'. Then E' is processed by the local query function to find the corresponding fine-grained prompts \\(\\{K_l, P_l\\}\\). Thus, ViT modifies its MSA layer based on \\(P_l\\) and loads the local classification head \\(H_i\\), forming \\(V_i\\). The local prompt training loss function is\n\\[\\min _{H_{l}, P_{l}, K_{l}} \\mathbb{L}\\left(V_{l}(E^{\\prime}), y\\right)+\\lambda_{2} \\sum_{K} d i s\\left(K_{i n}, K_{i}\\right),\\newline\\newline\\]"}, {"title": "4.3 Selective Prompt Fusion", "content": "To fuse global prompts precisely, we devise a novel selective prompt fusion mechanism that aggregates prompts from different prompt pools through knowledge distillation, enhancing their generalization. To our knowledge, it is a novel approach to distill prompts from different clients."}, {"title": "5 EXPERIMENTS", "content": "5.1 Experimental Setup\n5.1.1 Datasets. We conduct extensive experiments on CIFAR-100 [19] with 5 incremental tasks to evaluate the effectiveness of our FedMGP in addressing the challenges of PFCL. CIFAR-100 is a widely used benchmark dataset and consists of 60,000 RGB color images, each of size 32x32 pixels, classified into 100 different classes. We consider two practical scenarios of FCL, namely synchronous FCL and asynchronous FCL.\nIn the synchronous FCL settings [47], clients have the same task sequences but a varied proportion of samples from each class. It is a common setting employed in existing FCL works [7]. The degree of data heterogeneity in this scenario is controlled with the Dirichlet parameter, which is set to be 1 in our experiments. Specifically, we first partition the dataset into 5 tasks, each containing 20 classes, with no overlapping class between tasks. Then, within each task, the samples of each class are randomly divided into the same number of subsets as the total number of clients, ensuring that the data among clients is also non-overlapping.\nIn the asynchronous FCL settings [47], some of the classes are accessible to all clients while others are private to certain clients, which is derived from pathological Non-IID in static FL [31]. In this setting, we consider that there are 15 private classes for each client. Each task contains 8 classes. To be specific, each client first selects 15 classes unique to itself, and only that client has access to the full data of these classes. Therefore, there are 25 classes lefted as public classes shared by all clients. As a result, each client has data for 40 classes. The client then randomly divides these 40 classes into 5 tasks, each containing 8 classes.\n5.1.2 Baselines and Backbones. We compare FedMGP with FedAvg [31], FedEWC [18], FedProx [23] and GLFC [7] on ResNet-18 [12]. Since our method is based on ViT-B/16, we also conduct experiments on ViT-B/16 to compare FedMGP with FedViT. FedViT is a naive combination of FedAvg and ViT [8], which performs federated training by locally updating and globally aggregating the parameters of the classifier heads iteratively. FedL2P and FedDualP are the adapted versions of two effective prompt-based methods in traditional CL, L2P [41] and DualPrompt [40], making them more suitable for use in a federated environment. More detailed descriptions are in Appendix 3."}, {"title": "5.2 Expermental Results", "content": "We use the accuracy of the aggregated global model on local test sets as the metric in Table 1. To examine the impact of different backbone networks on the experimental results, we employed baseline methods based on two backbones, namely ResNet-18 and the pre-trained ViT.\nSurprisingly, all methods generally perform better in the asynchronous setting than in the synchronous setting. This is attributed to the fact that in the synchronous setting, each task involves 20 classes. GLFC, FedAvg, and FedProx failed in both asynchronous and synchronous FCL. As expected, methods based on ViT outperformed those based on ResNet-18 in both scenarios. But FedAvg performs even better than FedViT and FedDual in synchronous FCL. This indicates that in scenarios with similar data distributions, FedAvg has the ability to challenge large pre-trained models.\nIn all methods using ViT as the backbone, FedL2P with prompts performed better than using ViT alone. Unfortunately, FedDualP performed even worse than the simple FedViT. We believe this is due to the heterogeneity in the learned parameters across clients. Moreover, the performance of these methods did not show significant improvement after aggregation. In fact, FedViT experienced a decrease of 3.9% in average accuracy after aggregation in synchronous FCL and a decrease of 7.27% in asynchronous FCL.\nOur method achieved the best performance in both synchronous and asynchronous settings, with accuracies of 90.56% and 83.46%, showing the state-of-the-art performance of fusing heterogeneous knowledge. Although our method performs well on this metric even without some components, such as Ours-w/oGP achieving 89.36% and 81.06%, and Ours-w/oLp achieving 87.29% and 77.93%, the ability to retain spatial-temporal knowledge is significantly affected. In the following section (Sec. 5.3), we will evaluate each method using new metrics, i.e., temporal knowledge retention and spatial knowledge retention, to evaluate the resistance of spatial-temporal catastrophic forgetting."}, {"title": "5.3 Ablation Studies", "content": "To further validate the effectiveness of the multi-granularity knowledge space, we conducted three different ablation experiments under the same experimental setup. These experiments respectively removed the global prompts, local prompts, and the selective prompt fusion mechanism on the server. Results are shown in Fig. 3.\nIn both asynchronous and synchronous settings, ViT-based methods have demonstrated exceptional performance in retaining spatial knowledge. This result also confirms our hypothesis: having similar cognition is the foundation for knowledge sharing. Based on that, the increment of spatial knowledge retention of FedAvg in the synchronous setting is not difficult to understand, as similar data contributes to the similarity of convolutional layers. While these methods have effectively preserved spatial knowledge, none of them demonstrates resistance to temporal catastrophic forgetting. In Fig. 3(b) and Fig. 3(d), it is challenging to distinguish the difference between FedL2P, FedDualP and other methods with ResNet18 as the backbone network, as their temporal knowledge retention rates are all around 20%.\nOur approach not only competes with other ViT methods in terms of spatial knowledge retention but also achieves almost no forgetting in temporal knowledge retention, thanks to the construction of the multi-granularity knowledge space. To evaluate the contribution of the coarse-grained global prompt and the fine-grained local prompt, three different ablation experiments are conducted, which respectively removed global prompts (Ours-w/oGP), local prompts (Ours-w/oLP), and selective prompt fusion (Ours-w/oSPF). In Fig. 3(a), there is a slight decrease in spatial knowledge retention when we remove the global prompt. The other two components have little impact on spatial forgetting. However, things become more complex when it comes to temporal knowledge retention. Without local prompts, it drops significantly to around 15%. And when we remove global prompts, although the retention also decreases, it is not as drastic.\nIt concludes that fine-grained local prompts play a crucial role in preventing temporal catastrophic forgetting, and they still need to be combined with coarse-grained knowledge to better prevent spatial-temporal catastrophic forgetting and achieve personalization. Hence, multi-granularity knowledge representation is a promising direction in PFCL."}, {"title": "5.4 Sensitivity Analysis", "content": "FedMGP involves several hyperparameters, including prompt length, prompt pool size and so on. To further investigate the robustness of FedMGP, we conduct sensitivity analyses of prompt length and pool size on CIFAR-100 with 5 incremental tasks and present the results in Fig. 4."}, {"title": "6 DISCUSSION", "content": "This section will provide a preliminary analysis and discussion of the computational cost", "train": "coarse-grained global prompts and fine-grained local prompts. The size of the global prompt pool of one client is determined by the number"}]}