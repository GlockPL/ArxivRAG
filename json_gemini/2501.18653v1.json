{"title": "Cogito, ergo sum: A Neurobiologically-Inspired Cognition-Memory-Growth System for Code Generation", "authors": ["Yanlong Li", "Jindong Li", "Qi Wang", "Menglin Yang", "He Kong", "Shengsheng Wang"], "abstract": "Large language models-based Multi-Agent Systems (MAS) have demonstrated promising performance for enhancing the efficiency and accuracy of code generation tasks. However, most existing methods follow a conventional sequence of planning, coding, and debugging, which contradicts the growth-driven nature of human learning process. Additionally, the frequent information interaction between multiple agents inevitably involves high computational costs. In this paper, we propose Cogito, a neurobiologically-inspired multi-agent framework to enhance the problem-solving capabilities in code generation tasks with lower cost. Specifically, Cogito adopts a reverse sequence: it first undergoes debugging, then coding, and finally planning. This approach mimics human learning and development, where knowledge is acquired progressively. Accordingly, a hippocampus-like memory module with different functions is designed to work with the pipeline to provide quick retrieval in similar tasks. Through this growth-based learning model, Cogito accumulates knowledge and cognitive skills at each stage, ultimately forming a Super-Role\u2014an all-capable agent to perform the code generation task. Extensive experiments against representative baselines demonstrate the superior performance and efficiency of Cogito. The code is publicly available at https://github.com/ doc0318/Cogito.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated human- like intelligence in tasks such as code generation [Chowd- hery et al., 2022], testing [Fakhoury et al., 2024], and de- bugging [Xia and Zhang, 2023]. Recent studies show the ef- fectiveness of using multiple agents for collaborative tasks, achieving superior performance over single-agent [Islam et"}, {"title": "2 Related work", "content": null}, {"title": "2.1 LLM Agents", "content": "LLM-based agents normally consist of four core components: planning, memory, perception, and action. Planning and memory form the cognitive core, while perception and action enable interaction with the environment to achieve goals [Xi et al., 2023]. The planning component decomposes complex tasks into manageable subtasks and schedules their execution to achieve predefined objectives, while also incorporating the flexibility to adapt plans dynamically in response to exter- nal feedback. The memory component, on the other hand, stores historical actions and observations, enabling agents to draw on past experiences to refine decision-making processes and enhance task execution efficiency. This dual approach fa- cilitates continuous learning and optimization, ensuring im- proved performance over time. Effective memory manage- ment is critical for system performance [Wang et al., 2023; Zhang et al., 2024b]. Due to the suitability of this setup for code generation problems, a large number of works have emerged in this field [Wang et al., 2024a; Liu et al., 2024]."}, {"title": "2.2 Multi-Agent Collaboration for Software Development", "content": "To effectively solve complex problems, tasks are divided into specialized roles, each handling a specific aspect of the pro- cess. This role-based division, combined with agent collabo- ration, boosts efficiency and enhances outcomes. The typical workflow includes task refinement, execution, result valida- tion, and optimization [Lei et al., 2024b; Lei et al., 2024a]. These stages ensure that each component is managed with focus, leading to smoother task execution and more reliable results. For instance, MetaGPT [Hong et al., 2023] mim- ics standardized real-world collaboration procedures, incor- porating five distinct roles. Similarly, MapCoder [Islam et al., 2024] adapts the human programming cycle to define four key roles for task completion. Another approach, Self- Organized [Ishibashi and Nishimura, 2024], organizes tasks through a hierarchical parent-child node structure, promoting iterative progress and efficient collaboration."}, {"title": "2.3 Prompt Engineering", "content": "Prompt engineering plays a crucial role in optimizing code generation tasks by effectively guiding model outputs, ensur- ing both, consistency and efficiency in the process. Inspired by the CoT (Chain of Thought) [Wei et al., 2022] method, there are usually three main stages in code generation tasks to gradually solve the problem while maintaining clarity and structured reasoning: Planning [Talebirad and Nadiri, 2023; Zhang et al., 2024a; Lin et al., 2024], Coding [Rasheed et al., 2024a; Zan et al., 2024; Tao et al., 2024], and Debug- ging [Li et al., 2023; Qin et al., 2024; Rasheed et al., 2024b], AgentCoder [Huang et al., 2023] directs the agent to produce"}, {"title": "3 Cogito", "content": null}, {"title": "3.1 Agent Roles", "content": "Building on the \u201cChain of Thought\u201d (CoT) [Wei et al., 2022] process, we assign three distinct roles within the team: Plan- ner, Coder, and Debugger. The Planner's role is to outline a clear, step-by-step strategy for solving the problem, consid- ering key aspects such as edge cases and performance issues. This guidance helps the Coder translate the plan into func- tional code, ensuring that all critical scenarios are addressed during implementation. After the Coder finishes coding, the solution is tested against a set of sample inputs and expected outputs [Islam et al., 2024]. If the code passes the tests, it is considered finalized. However, if it fails, the Debugger steps in, analyzing the traceback feedback to identify and correct errors. This collaborative process ensures that the final code is both robust and efficient."}, {"title": "3.2 Super-Role", "content": "In this experimental setup, we introduce a shared member known as the Super-Role, who is assigned to each of the three groups sequentially. This member rotates through the roles of Debugger, Coder, and Planner within each group, contributing to a dynamic and collaborative environment. Im- portantly, the public role retains the memory of all its prior ex- periences, which plays a crucial role in informing and guid- ing the execution of its current responsibilities. This mem-"}, {"title": "3.3 The Hippocampus-like Memory Module", "content": "We drew inspiration from the structural divisions of the hu- man hippocampus [Burgess et al., 2002; Berron et al., 2017; Kesner, 2013] to design our storage module, with each re- gion serving distinct functions. The Dentate Gyrus (DG) is primarily responsible for the formation of new memories and processes a large volume of information. The hippocampus contains different regions of the Cornu Ammonis (CA), each responsible for distinct functions.\nCA1 Region. In this model, we input tasks and correspond- ing responses into specific regions of the hippocampus, each playing a distinct role in how memories are formed, retained, and recalled over time. The CA1 region, pivotal for the stor- age and retrieval of long-term memories, serves as the repos- itory for initial responses generated during problem-solving. Once formulated, these responses are stored for long-term ac- cess and ready for future retrieval when related tasks arise,"}, {"title": "3.4 Agent Collaboration Settings", "content": "To mitigate the potential negative impact of answers gener- ated by different roles during the learning phase, we begin by assigning initial weights to each role, specifically 0.4, 0.4, and 0.3, respectively. Subsequently, the answers generated by each role are evaluated, and an importance score is assigned based on the quality of the results. The final score is derived by multiplying the importance score by the initial weight. This approach ensures that poor answers receive lower scores, while high-quality answers are rewarded with higher scores. As a result, this mechanism not only reduces the influence of incorrect answers but also incentivizes the Super-Role to prioritize and utilize better solutions. Moreover, to further en- hance variability and avoid over-committing resources to re- fine incorrect answers, we reintroduce two randomly selected roles, excluding the one currently used by the public role, in each group. This strategy helps prevent excessive resource allocation to erroneous answers and encourages the genera- tion of more reliable solutions. In each group, the debugger role modifies or improves the code based on the obtained ex- ecution results. This process will only be performed once. During the expert phase, the number of debug attempts for experts is uniformly set to 5. We summarize our agent traver- sal in Algorithm 1."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 Experimental Settings", "content": "Datasets. We adopt 8 widely-used benchmark datasets for testing, with 5 datasets containing only simple pro-"}, {"title": "4.2 Overall Performance", "content": "In this section, we conduct a comprehensive evaluation of our proposed process, and all the results are systematically pre-"}, {"title": "4.3 Ablation Study", "content": "Impact of Different Agents. To verify the effectiveness of the proposed approach, we systematically remove the active participation of various key roles involved in the process. The experimental results (Table 3) indicate that omitting the crit- ical planning phase led to a maximum performance drop of 14.02%. The absence of hands-on Implementation practice reduces performance by 12.19%, while the lack of expert De- bugging knowledge causes a 10.36% decline.\nImpact of Work Sequence. To rigorously assess the distinc- tions between our work and previous approaches, particularly in terms of the sequence of experience accumulation, we con-"}, {"title": "4.4 Hyper-parameter Analysis", "content": "Impact of t. It involves a single hyperparameter: the num- ber of self-debugging attempts, denoted as t. As shown in Table 4, increasing the value of t improves the performance. However, this enhancement comes with a trade-off, as it re- quires more computational time and an increased number of tokens to complete the process. This observation highlights the inherent balance between performance and resource con- sumption in the proposed method.\nImpact of the Number of Iterations for Accumulating Ex- perience. Initially, we set the number of roles to 3, indicating that we require three groups per experience-learning cycle. Increasing the number to six seems intuitive for better ex- perience accumulation. However, as a detailed comparison provided in Table 5, increasing the number actually leads to"}, {"title": "4.5 Case Study", "content": "New Random Roles vs. Same Roles\nIn group discussion sessions, each group will reintroduce two new random members to assume two distinct roles. The ques- tion arises: why not allow the same two members to continue participating throughout the role transitions? The rationale behind this decision lies in the potential pitfalls of starting with an incorrect approach. If the direction of code writing is flawed in the initial phase, any subsequent improvements or redesigns will be built upon this foundational error. No matter how many times debugging is performed, the final result will inevitably remain compromised. Therefore, it is essential to ensure that the foundation is correct before moving forward with further development. An example is shown in Figure 6, demonstrating the necessity of introducing new random roles."}, {"title": "5 Conclusion", "content": "In this work, we introduce Cogito, a neurobiologically- inspired multi-agent framework for code generation that re- defines the traditional workflow of planning, coding, and de- bugging by adopting a reverse approach. By mimicking the human growth process, Cogito progressively develops its ca- pabilities, transitioning through specialized roles\u2014Debugger, Coder, and Planner\u2014and ultimately evolving into a Super-"}, {"title": "A Agent Prompt Details", "content": "Here are the prompts for different roles. For certain datasets, there are some changes in their data format."}, {"title": "B Supplementary Experimental Details", "content": null}, {"title": "B.1 Datasets", "content": "For convenience, we used the HumanEval dataset from Map- coder [Islam et al., 2024], which contains a sample column that separately extracts the execution examples provided in the prompt, making it easier to execute and return results. Similarly, in MBPP, they also select some data from the test set as inputs, but maintain the independence of the test set and the exclusivity between MBPP and MBPP-ET. For CodeCon- test, we only use the test section consisting of 165 problems. APPS and xCodeEval utilize a subset of problems extracted from the raw data by MapCoder."}, {"title": "B.2 Evaluation Metric", "content": "The unbiased version of Pass @k is a metric commonly used to evaluate the effectiveness of recommendation systems, es- pecially when the system's recommendations are prone to bi- ases, such as those arising from uneven distributions of rele- vant items or recommendation behaviors. This metric aims to provide a more equitable and robust evaluation by correcting for these biases, which can distort performance assessments. The formula for Pass@k is given by:\n Pass@k = E_{Problems} 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} (2)\nHere, n represents the total number of items available for recommendation, c stands for the number of relevant items (those that the user is interested in), and k denotes the size of the top-k recommendations generated by the system. The metric aims to assess the probability that at least one relevant item appears within the top-k recommended items.\nThe expression $\\binom{n-c}{k} / \\binom{n}{k}$ calculates the probability that none of the relevant items appear in the top-k recommendations. It does so by computing the number of ways to choose k items from the n \u2212 c non-relevant items, normalized by the total number of ways to choose k items from all n items. Subtract- ing this probability from 1 yields the probability that at least one relevant item is present in the top-k recommendations.\nBy considering combinations, the Pass@k metric effec- tively accounts for the distribution of relevant items and cor- rects for any inherent biases in the recommendation pro- cess. This is particularly important because traditional met- rics might overestimate the performance of systems that tend to recommend a small subset of highly popular items, leaving out less obvious but still relevant items. The unbiased for- mula ensures that the evaluation is fairer, reflecting a more realistic measure of a system's ability to recommend relevant items across a diverse set of users or test cases.\nThe expectation Eproblems averages the performance over a set of users or test cases, ensuring that the metric evaluates the system's performance across different scenarios rather than focusing on a single instance. This provides a more compre- hensive measure of the system's overall effectiveness, as it considers the variability of user preferences and the diversity of possible recommendation outcomes.\nIn practical terms, Pass@k is useful for recommendation systems where the goal is to not only recommend relevant"}, {"title": "B.3 Baselines", "content": "We evaluate our approach by comparing it with several base- line methods. First, we use the Direct Method, where the prompt is submitted to the LLM without decomposition to assess its intrinsic reasoning. We then evaluate two struc- tured reasoning methods: Chain-of-Thought (CoT), which solves the problem step-by-step, and Self-Planning, which separates planning and implementation phases. Our ap- proach, which incorporates GitHub searches for relevant code, is compared with Analogical Reasoning, a retrieval-based method. Finally, we include Mapcoder, a state-of- the-art method, as a benchmark. All tests are conducted us- ing GPT-3.5-turbo (GPT-3.5-turbo-0125) and GPT-4 (GPT-4- 0613) from OpenAI."}, {"title": "B.4 An example answer for 5 methods of HumanEval #92", "content": "We present solutions from several methods for the 92nd prob- lem in HumanEval (Figure 7). Except for Cogito, all fail the first test: \"assert candidate('TEST') == 'tgst', 'This prints if this assert fails 1 (good for debugging!)'\". Each method's er- rors are highlighted, with explanations provided. For Cogito, we show its responses at different stages."}, {"title": "B.5 The comparison between GPT-3.5-TURBO and GPT-4 responses.", "content": "We select two examples to demonstrate the performance dif- ferences between GPT-3.5 and GPT-4. The first example, from the HumanEval dataset (Figure ??), illustrates a case where GPT-3.5 fails to produce a correct solution, while GPT- 4 successfully generates the correct code. Similarly, the sec- ond example from the APPS dataset (Figure ??) highlights a scenario where GPT-3.5 struggles, but GPT-4 is able to han- dle the problem effectively. Upon examining both cases, it becomes clear that GPT-4 excels in solving more complex and challenging problems, showing improved reasoning and code generation capabilities compared to GPT-3.5."}, {"title": "B.6 The complete response process of Cogito in APPS #1628", "content": "We provide the responses from Cogito and the roles involved throughout the entire process tackling Task 1628 of APPS (Figure 10). The responses of adjacent roles in each group are also provided, which help to give a better context of cause and effect. By showcasing the interactions between roles, we can better understand how different stages of the process con- tribute to the final outcome.Additionally, we present the er- rors, changes, and correct parts in each response, breaking down the reasons behind each modification. This allows us to explain why the code either passes or fails, highlighting spe- cific areas where improvements are made or where missteps occur."}]}