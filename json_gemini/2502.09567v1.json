{"title": "MorphNLI: A Stepwise Approach to Natural Language Inference Using Text Morphing", "authors": ["Vlad-Andrei Negru", "Robert Vacareanu", "Camelia Lemnaru", "Mihai Surdeanu", "Rodica Potolea"], "abstract": "We introduce MorphNLI, a modular step-by-step approach to natural language inference (NLI). When classifying the premise-hypothesis pairs into {entailment, contradiction, neutral}, we use a language model to generate the necessary edits to incrementally transform (i.e., morph) the premise into the hypothesis. Then, using an off-the-shelf NLI model we track how the entailment progresses with these atomic changes, aggregating these intermediate labels into a final output. We demonstrate the advantages of our proposed method particularly in realistic cross-domain settings, where our method always outperforms strong baselines, with improvements up to 12.6% (relative). Further, our proposed approach is explainable as the atomic edits can be used to understand the overall NLI label.", "sections": [{"title": "Introduction", "content": "Natural Language Inference (NLI), i.e., the task that determines whether a text hypothesis is true, false, or undetermined given a text premise (Condoravdi et al., 2003; Dagan et al., 2005; Bowman et al., 2015), is an important building block of many applications such as question answering, summarization, and dialogue systems, where understanding the logical connection between different pieces of information is essential (Yin et al., 2019; Sainz et al., 2021, 2022). Despite the fact that NLI has received significant attention lately (Raffel et al., 2019; Jiang et al., 2019; Sun et al., 2020; Wang et al., 2021), several analyses have indicated that neural NLI methods fail to capture important semantic features of logic such as monotonicity, and more granular aspects like negation, universal vs. existential quantifiers, and concept modifiers (Rozanova et al., 2022; Akoju et al., 2023). Other significant limitations of current models are caused by task artifacts that oversimplify the NLI problem (Williams et al., 2018; Jiang and de Marneffe, 2022). Large Language Models (LLMs) are prone to contamination (Golchin and Surdeanu, 2024; Sainz et al., 2024), which causes overfitting on these task artifacts (see Section 4.4). LLMs also tend to \"not say what they think\" (Turpin et al., 2024), which reduces the quality and faithfulness of their explanations.\nTo address the above drawbacks, we propose a cautious NLI strategy that decomposes the NLI decision into several simpler and more explainable steps. Specifically, our approach: (a) incrementally transforms the premise into the hypothesis using text morphing (Huang et al., 2018); (b) applies an off-the-shelf NLI model on each morphing iteration; and (c) aggregates the individual NLI labels into an overall label for the given premise-hypothesis pair. We call our method MorphNLI. The advantages of our direction are two fold. First, it performs better out of domain because its individual, smaller decisions reduce the chance of overfitting. Second, it naturally produces an explainable reasoning chain that traces the morphing transformations.\nOur approach is inspired by Natural Logic (NL) (MacCartney and Manning, 2009a, 2014) but is more flexible. First, rather than relying on a formal alignment algorithm between premise and hypothesis, which continues to be a pain point in the development of NL systems (Krishna et al., 2022), we use a more nimble morphing algorithm (Huang et al., 2018) that is trained on synthetic data. Second, instead of using the seven NL logic operators and a relatively complex finite-state automaton to aggregate them, we rely just on the three standard NLI labels (entailment, contradiction, neutral) and on a straightforward, robust aggregation decision that performs well in practice: pick the first non-entailment label in the sequence of NLI decisions."}, {"title": "Related work", "content": "Our work draws inspiration from Natural Logic (Lakoff, 1970), which is a form of reasoning aiming to draw logic inferences by operating directly over linguistic structures. Over the years, this has been implemented for natural language processing in various forms (MacCartney and Manning, 2007; Krishna et al., 2021; Rozanova et al., 2022; Feng et al., 2022; Korakakis and Vlachos, 2023). MacCartney and Manning (2007) introduced one of the first computational models for natural logic, which has been subsequently extended and improved in follow up work (MacCartney and Manning, 2008, 2009b). Natural logic can be useful beyond natural language inference, for tasks such as commonsense reasoning (Angeli and Manning, 2014), fact verification (Krishna et al., 2022; Strong et al., 2024), or polarity tracking (Hu and Moss, 2018). One drawback of natural logic is that it is too strict. For example, natural logic cannot readily accommodate paraphrases or temporal reasoning. Our proposed approach relaxes the strict requirements of natural logic formalism, relying instead on text morphing (Huang et al., 2018) and off-the-shelf NLI models.\nOur work is also related to explainable NLI (Camburu et al., 2018; Thorne et al., 2019; Camburu et al., 2020, inter alia). Importantly, in our proposed approach, the explanations are guaranteed to be faithful (Kumar and Talukdar, 2020), as they are constructed based on the atomic edits produced by the morphing model.\nTangentially, our proposed approach resembles work on modeling edit processes (Guu et al., 2018; Awasthi et al., 2019; Reid and Neubig, 2022; Reid et al., 2023). Very relevant is the work on text morphing (Huang et al., 2018), which we repurpose to generate atomic edits to transform the premise into the hypothesis.\nWe also leverage off-the-shelf NLI models to produce the final label. We refer the interested reader to the survey of Storks et al. (2019). Specifically, we use transformer-based NLI models (Vaswani et al., 2017; Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2019), typically trained on a mixture of NLI datasets (Marelli et al., 2014; Bowman et al., 2015; Williams et al., 2018)."}, {"title": "Approach", "content": "Our proposed method, MorphNLI, uses a modular step-by-step approach for natural language inference (NLI). At a high level, MorphNLI operates in three steps: (a) the premise is incrementally converted into the hypothesis through a sequence of small atomic edits that we call morphisms (see subsection 3.1); (b) an NLI engine is applied to generate NLI labels for each pair of texts in the sequence of transformations; and (c) these labels are aggregated into an overall NLI label for the original premise/hypothesis pair. This is beneficial for two reasons. First, the differences between a premise and a hypothesis are gradually broken into multiple sentences, which makes the task easier for an NLI engine and less prone to overfitting. Second, the trace resulting from the atomic edits can be used as a rationale for the final label, making the method more explainable.\nFigure 2 shows the overall architecture of our pipeline. The first module presents the training of the morphism model, where we use In-Context Learning (ICL) with an LLM as a teacher model to generate a synthetic dataset labeled with morphisms. After a filtering step, we use this dataset for fine-tuning a student model for morphism generation. At inference time, we use the student model for generating the morphisms and an NLI prediction model for generating labels. The labels are then aggregated into one final prediction. We detail all these components below."}, {"title": "The text morphing task", "content": "Before describing these components, we define the morphism generation task, similar in nature with the work of Huang et al. (2018). Formally, this task is the process of changing one initial sentence (i.e., premise) into a destination sentence (i.e., hypothesis) through a series of morphing operations. These operations are similar to the steps used in computing the Levenshtein distance:\n1. Replace - (replace, <old_text>, <new_text>)\n2. Remove - (remove, <text>)\n3. Insert (insert, <text>)\nThere are three important differences between our morphing and Levenshtein distance. First, our morphing operations operate at word/phrase granularity rather than characters. Second, our transformations are encouraged to preserve the syntactic structure of the source sentence (see subsection 3.2). Third, morphisms are generated using an LLM rather than an edit distance algorithm.\nMorphing a premise into the corresponding hypothesis results in a finite sequence M of sentences (morphisms), where each sentence $M_i$ is the result of applying a morph operation on the previous sentence $M_{i-1}$. The first sentence in this sequence is the premise and the last is the hypothesis."}, {"title": "Training the morphism model", "content": "One of our key contributions is training a morphism generation model with minimal supervision. The only supervision we require is: (a) a dataset of premise/hypothesis pairs with the associated NLI labels; and (b) a small pool of sentence pairs annotated with morphisms. To generate synthetic training data for morphisms we use an LLM with ICL (the teacher model). This LLM is coupled with a deterministic filter that increases the quality of the generated data. Using this data, we fine-tune a smaller LLM (the student model) to generate morphisms during inference."}, {"title": "Morphing teacher model and ICL selection", "content": "Given the complexity of the task and the nonexistence of a dataset labeled with morphisms, we steer the design of our method towards ICL. Our ICL pool contains 40 pairs of premises and hypotheses, humanly annotated with intermediate sentences and corresponding morph operations. When generating the morphisms for a pair of sentences, we select the 12 closest examples from the pool of 40 to be used in the prompt. These examples are selected based on the cosine similarity with the input premise and hypothesis, computed on the embeddings generated by a Sentence-BERT (Reimers and Gurevych, 2019).\nThe generation of the morphisms is driven by a Chain-of-Thought (Wei et al., 2024) prompt, where we ask the teacher model to output the morph operations before generating each intermediate sentence. The input prompt also contains formal rules for the morphing task, encouraging the LLM to preserve the syntactic structure of the source sentence, and forcing a strict order for the morph operations: first apply replace operations, then remove operations, and lastly insert operations. We empirically found that enforcing the operations in this order improves the quality of the overall results. The complete prompt and examples of generated training morphisms are included in the appendix."}, {"title": "Morphism filter", "content": "The synthetically annotated morphisms undergo a series of filtering steps for ensuring their quality. First, for obvious reasons, we filter out the examples where no intermediate sentences were generated (we called these examples lazy morphisms).\nSecond, we consider only the examples with intermediate sentences that are longer than either the premise or the hypothesis. We call the phenomenon where some intermediate sentences are too short short morphisms. This phenomenon may bring faulty reasoning processes, as some intermediate sentences may be formed by removing word groups from the initial sentence that may be necessary for future downstream NLI steps. In order to limit these cases, we removed all short morphisms from the generated data.\nLast but not least, we keep only examples where the overall predicted NLI label is identical to the gold label for the given premise/hypothesis pair. Our hypothesis is that morphisms that yield the correct overall label are more likely to be correct. An initial investigation of the generated data validated our hypothesis. To generate individual NLI labels, i.e., between $M_{i-1}$ and $M_i$, we used a BART-large NLI classifier fine-tuned on SNLI, MNLI and FEVER; we aggregated these labels using the aggregation function described below."}, {"title": "Morphing student model", "content": "Using the remaining synthetic data, we fine-tune a smaller LLM as the morphism student model. We used GPT-4o-mini."}, {"title": "Modular reasoning using morphisms", "content": "During inference, MorphNLI operates in 4 steps:\n(1) Voice normalization (VN): We observed that the sequential nature of morphing operations proves to be too rigid when there is a change of voice between the premise and hypothesis, as Figure 4 shows. To address this, we normalize the premise and the hypothesis to active voice using a smaller language model.\n(2) Morphing: We use the above morphism student model to generate the transformations between the premise and hypothesis.\n(3) Generating individual NLI decisions: We use an existing NLI classifier to generate the individual NLI labels between every ($M_{i-1}$, $M_i$) pair of sentences capturing a morphing transformation (see Figure 1 for an example).\n(4) Aggregating NLI decisions: An aggregation function is then used to combine the sequence of NLI labels into an overall label for the given premise/hypothesis pair. To this end, we use a simple heuristic: if all individual NLI labels are entailment, then the overall label is entailment; otherwise the overall label is set to be the first (left-most) individual label that is not entailment. For example, in Figure 1 the first non-entailment label is neutral, which becomes the overall prediction for the example in the figure. In initial experiments, we experimented with aggregating labels using the Natural Logic fine state automaton (MacCartney and Manning, 2014; Krishna et al., 2022), but have observed that this more formal automaton does not translate well to our more flexible setting. In contrast, our heuristic performed better and is efficient, as it does not require substantial additional processing overhead."}, {"title": "Experimental results", "content": "We evaluate the NLI performance of MorphNLI using two datasets: Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018) and Sentences Involving Compositional Knowledge (SICK) (Marelli et al., 2014). MNLI covers 10 genres of written and spoken English and contains fairly complex natural language. SICK contains artificially-generated premise/hypothesis pairs, which were created using a formal set of logic rules that follow syntactic and lexical transformations. As such, SICK exhibits different challenges from MNLI, assessing the ability of NLI models to comprehend complex logic and compositional structures. Considering these differences, these two datasets are a good selection for both in-domain (ID) and out-of-domain (OOD) evaluations. That is, in addition of training and testing in each dataset, we evaluate MorphNLI when the underlying NLI engine is trained on the other dataset.\nWe did not use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) for the NLI evaluations because, as some of its original authors noticed, it \"is not sufficiently demanding to serve as an effective benchmark\u201d (Williams et al., 2018). SNLI ignores important phenomena such as temporal reasoning, compositionality of logic, and they make simplifying coreference assumptions, i.e., that the participants and concepts mentioned in the premise and hypothesis are the same (Williams et al., 2018; Jiang and de Marneffe, 2022).\nHowever, to minimize any potential overfitting, we fine-tune the morphing engine using premise/hypothesis pairs from SNLI (see next subsection). Thus, our morphing component can be seen as always being evaluated out-of-domain."}, {"title": "Experimental settings", "content": "The LLMs used for the teacher and student morphing models are both from the GPT-4 family. Details on the models' identifiers are present in Appendix C, together with experiments using another LLM from the Llama family. The synthetic data set that contains morphisms is generated from the SNLI validation dataset (~10,000 premise/hypothesis pairs) using GPT-4-turbo. After the filtering step (see Section 3.2), we are left with 3,027 pairs labeled with morphisms for fine-tuning. These are split into two sets: one for training (2,127 examples) and one for validation (900 examples). The remaining filtered-out examples are later used to compare the fine-tuning approach with simple ICL for morphing. Our preliminary experiments indicated that fine-tuning outperforms ICL, both in terms of overall performance and model efficiency. For this reason, all experiments described later in this section use a morphing model fine-tuned on the above training data. Moreover, the fine-tuned model proves to be more expressive, with a much lower rate of lazy morphisms (1,575 compared to 4,375 in the case of ICL), having a slight increase in the number of short morphisms (674 compared to 557 in the case of ICL).\nFor the individual NLI decisions we experimented with state-of-the-art NLI prediction models from two different families: encoder-decoder using BART and encoder-only using ROBERTa (large versions)."}, {"title": "Results", "content": "Table 1 shows the accuracy of MorphNLI on the SICK test dataset. We report the results with and without voice normalization, with two different NLI engines (RoBERTa and BART), which are trained both ID and OOD. We compare the performance of our approach to the same NLI model applied directly to the original premise/hypothesis pair, i.e., without morphing (referred to as \"vanilla\"). We draw several observations from this table. First, all models perform better ID than OOD, which indicates a certain degree of overfitting. Second, MorphNLI shows a slight drop in ID performance, which we attribute to the fact that the NLI models were not trained on incremental transformations (see the next subsection for a more detailed analysis). Most importantly, MorphNLI outperforms the \u201cvanilla\u201d NLI model in all four OOD configurations, with an improvement as large as 1.74% for BART with voice normalization. This is an encouraging result, as it validates our hypothesis: that modular NLI improves domain transfer.\nTable 2 shows the same behaviour for the MNLI test dataset. Here the OOD enhancements are more considerable. For example, we observe an increase of 5.29% for RoBERTa and 5.92% for BART, both in settings with no voice normalization. While the voice normalization proved beneficial for the SICK dataset, for all the scenarios tested, for MNLI we see a decline in accuracy when applying it (see the next subsection for a more detailed discussion)."}, {"title": "Discussion", "content": "To further understand MorphNLI's behavior we answer below three important research questions."}, {"title": "What is the quality of MorphNLI's explanations?", "content": "To get a better understanding of how the explanations generated via our modular approach compare to those generated by LLMs (GPT-4o and Llama 3.1 8B), we performed a manual evaluation on a random sample from both MNLI and SICK datasets. We selected 20 instances from each development set, distributed as follows: 5 instances where the NLI model's predictions are correct both with and without morphing, 5 where both predictions are incorrect, 5 where morphing improves the NLI prediction, and 5 where it worsens the prediction. The NLI model used here was RoBERTa, fine-tuned in-domain on each dataset. Four human evaluators awarded a score between 0 and 2, as follows: 2 indicates the explanation is correct; 1 indicates the explanation is partially correct, i.e., it contains correct elements, but it misses required information or includes extraneous elements; and 0 indicates the explanation is completely incorrect.\nWe computed Cohen's Kappa inter-annotator agreement across all six pairs of evaluators from the four annotators. For the MNLI dataset, we calculated an average Kappa agreement of 34%, which indicates fair agreement. Considering the complexity of the task, i.e., evaluators had to evaluate both the correctness of each morphism and the NLI label produced at each step, we consider this a respectable result. Even more encouragingly, the maximum agreement between two annotators was 57%, which falls on the high end of moderate agreement, touching on substantial. This suggests that the agreement can be improved with more training. Similarly, on the SICK dataset, the average agreement is 67% (substantial agreement), with a maximum of 91% (almost perfect). These higher scores highlight that despite the complexity of the task, annotators were trained to perform high-quality annotations.\nWe evaluated: (i) the overall explanation quality with our modular approach; (ii) the quality of the GPT-4o reasoning process and (iii) the quality of the morphisms generated via our approach (we asked the evaluators to discard the NLI label, and reason based on the morphisms alone). Table 4 presents the percentage scores average from the four annotators.\nOur approach delivers uniform explanation quality across the two dataset samples (MNLI and SICK) and the overall quality is considerably larger than that of Llama 3.1 8B, despite the latter model's much larger size. The quality of the GPT-4o explanations is significantly better for MNLI than for SICK, where the explanations via morphisms are superior. An interesting phenomenon reported by the annotators was related to the potential overfitting of the GPT-4o reasoning. In 6 out of the 20 examples sampled from SICK, the model incorrectly assumed that premise and hypothesis refer to the same situation, i.e., the participants and the concepts mentioned are the same between premise and hypothesis. This task artifact exists in the SNLI dataset (Bowman et al., 2015), where both premise and hypothesis are annotated given a single image (Jiang and de Marneffe, 2022), but it has been removed in more recent datasets such as SICK and MNLI. However, since SNLI has been publicly available for approximately a decade it is likely that it \"leaked\" into the GPT-4o training data, which learned this task artifact. As a consequence, the LLM mistakingly labeled these pairs as contradiction and produced completely incorrect explanations. For MNLI, this phenomenon was not as strong, which is likely due to another contamination: GPT models are known to have been contaminated with the MNLI development partition (Sainz et al., 2024). The large difference between MNLI and SICK GPT-40 explanation scores supports this hypothesis.\nA second observation from this analysis is that the off-the-shelf NLI models likely exhibit some degree of overfitting as well. Specifically, they have been trained on the original premise/hypothesis pairs and underperform on our (simpler) incremental inference steps. For instance, the NLI model fails to correctly interpret semantic information in short textual snippets (e.g., understanding that \"grazing a field\" implies the field has", "grass,": "r that", "land": "mplies", "desert": "."}, {"title": "What are MorphNLI's common errors?", "content": "To identify where most errors occur within the MorphNLI pipeline, we randomly sampled 20 errors from each of the two development sets (SICK/MNLI) and manually analyzed these examples. We discovered that: 45% (SICK) and 50% (MNLI) errors were caused by the NLI model (in-domain RoBERTa). As indicated above, this is likely a form of overfitting due to the NLI model's original training data, which did not contain text pairs similar to our incremental transformations. This suggests that valuable future work would be to fine-tune an NLI model that is morphing aware. The second most common error type were faulty morphisms: 45% (SICK) and 20% (MNLI). This observation indicates that our morphing would probably benefit from more fine-tuning. Lastly, 5% (SICK) and 30% (MNLI) were a result of poor voice normalization. MNLI, which contains longer and more complex statements, potentially with several predicates, suffers from this problem more. This suggests that identifying first which verb is the sentence's main predicate might improve voice normalization. All in all, this analysis indicates that MorphNLI's errors are caused by issues of local components, which can be potentially addressed, and are not a limitation of the overall direction."}, {"title": "Are MorphNLI's decisions more interpretable?", "content": "To gain a better insight on how morphing improves the prediction process of the (independent) NLI model, we conducted several analyses using LIME (Ribeiro et al., 2016) on examples from both SICK and MNLI, where we compare a \u201cvanilla\u201d NLI model (in-domain RoBERTa) with MorphNLI using the same NLI engine. As anticipated, providing the NLI model with incremental changes helps it focus on more semantically relevant words."}, {"title": "Lexical sensitivity between the premises and hypotheses", "content": "During our experiments we noticed an inverse correlation between the quality of the morphing process and the syntactic/lexical differences between the premises and hypotheses. For example, where the two share little to no lexical similarities (i.e., often found in MNLI), the morphing operations tend to consider larger textual groups, affecting the performance. On the other hand, the more similar the premise and hypothesis are, the more the morphing operations follow clear logical steps. To a large extent, this carries over to examples with larger syntactic/lexical differences. However, too many lexical differences between the premise and hypothesis hurt multiple NLI techniques, and MorphNLI is no exception. Nevertheless, our approach is less sensitive to lexical differences out-of-domain, indicating a lower degree of overfitting. This phenomenon is further detailed in the Appendix D."}, {"title": "Importance of the filtering stage", "content": "As described in section 3.2, we included filters to remove low-quality data, i.e., examples with no inner sentences (lazy morphisms) and examples with inner sentences that are shorter than both the premise and hypothesis (short morphisms). By filtering these examples before fine-tuning, we significantly reduce the number of short morphisms at inference time, while maintaining a low number of lazy morphisms. In our experiments, for a sample of roughly 5,500 examples during inference, the initial morphing mechanism predicted 26% lazy and 32% short. After introducing our filtering mechanism, the percentage of lazy morphisms had a slight increase to 28%, while the percentage of short morphisms dropped considerably to 12%."}, {"title": "Conclusions", "content": "In this paper, we proposed MorphNLI \u2013 a modular step-by-step approach for natural language inference. Our method uses a language model to generate atomic edits that progressively transform (i.e., morph) the premise into the hypothesis. We then track how these atomic edits impact the entailment between successive sentences, aggregating these intermediate labels into a final answer (see Figure 1). We hypothesized that typical NLI models can better handle examples where the two sentences are lexically close (i.e., they differ only by an atomic edit) Our results confirm that our proposed approach is more robust, outperforming traditional NLI models in all cross-domain settings investigated. Furthermore, our proposed method is explainable. The sequence of intermediate edits together with their corresponding individual NLI labels can be used to explain the overall prediction."}, {"title": "Limitations", "content": "Our work focuses on the task of Natural Language Inference. Although the text morphing process proves to be beneficial in the context of logical reasoning, its applicability in other reasoning tasks is still to be tested. Moreover, we cannot offer an assurance on the level of generalizability of our method. For our experiments, we designed the morphism generation as a general task, as the fine-tuning data is constructed from a different domain than the testing data (SNLI vs. SICK/MNLI). However, we do not know if this generalization is constrained on data specific to the NLI task.\nIn the development of our solution, for the morphism generation task, we have experimented mostly with LLMs from the GPT family. We are unsure if our pipeline may have different behavior for other proprietary LLMs or for much larger LLMs, as we are using a fine-tuned version of GPT-4o-mini. Also, being a closed source model, we do not know if the LLM was previously trained towards this objective of text morphing. Further, it is hard to accurately predict the level of contamination of the model with the test datasets, and what influence it has on the morphism generation.\nThe morphing process is evaluated only on English. We have no assurance that the same techniques could apply on other languages for multilingual models or if it follows only the particularities of the English language."}, {"title": "Ethics", "content": "We have extensively utilized off-the-self LLMs and NLI models, which may contain hidden biases. However, our approach makes inference more explicit, so these potential biases are more likely to be exposed during the morphing process.\nWe mostly used closed models, however our costs are reduced. We believe that this study does not exclude any communities from the point of view of the associated cost."}, {"title": "Lexical sensitivity", "content": "We wanted to see how the performance of our approach varies considering the lexical difference between the premise and hypothesis. We measured the accuracy as the similarity between the hypothesis and the premise varies, and as the word difference varies. For the similarity, we used a sentence transformer (all-MiniLM-L6-v2) and measured the cosine similarity between premise and hypothesis. For the word difference, we computed the difference in words between the lemmatized premise and hypothesis. The results are presented in Figure 11.\nWe see that for both scenarios and datasets, MorphNLI is less sensitive to lexical differences, especially out-of-domain. We observe that in the case of word difference, the in domain vanilla approach is not affected by a large difference. We consider this a clear sign of overfitting, especially as the rest of the methods have a drop in performance. This experiment further shows that our approach is less prone to overfitting and outperforms the vanilla models in out-of-domain scenarios."}]}