{"title": "Beyond Release: Access Considerations for Generative AI Systems", "authors": ["Irene Solaiman", "Rishi Bommasani", "Dan Hendrycks", "Ariel Herbert-Voss", "Yacine Jernite", "Aviya Skowron", "Andrew Trask"], "abstract": "Generative AI release decisions determine whether system components are made\navailable, but release does not address many other elements that change how users\nand stakeholders are able to engage with a system. Beyond release, access to\nsystem components informs potential risks and benefits. Access refers to prac-\ntical needs, infrastructurally, technically, and societally, in order to use available\ncomponents in some way. We deconstruct access along three axes: resourcing,\ntechnical usability, and utility. Within each category, a set of variables per system\ncomponent clarify tradeoffs. For example, resourcing requires access to comput-\ning infrastructure to serve model weights. We also compare the accessibility of\nfour high performance language models, two open-weight and two closed-weight,\nshowing similar considerations for all based instead on access variables. Access\nvariables set the foundation for being able to scale or increase access to users; we\nexamine the scale of access and how scale affects ability to manage and intervene\non risks. This framework better encompasses the landscape and risk-benefit trade-\noffs of system releases to inform system release decisions, research, and policy.", "sections": [{"title": "1 Introduction", "content": "Release decisions for generative AI systems raise ongoing discourse, debates, and regulatory ques-\ntions. Sometimes framed as an \"open versus closed\" debate, release considerations [105] are dis-\ncussed in media [56], a U.S. government executive order [52], and research on release decisions\n[32]. A central issue is determining responsible release that balances tensions of benefits and risks\nalong the gradient of fully open and fully closed systems [103]. Research and policy have focused\non how a model is released, availability of model weights [18], and more recently, what system\ncomponents are released [28, 66, 9], such as model weights, training data, and training code.\nDiscourse and analysis has been too narrowly scoped; instead, focus should include aspects of ac-\ncess. Release analysis cannot only consider whether a system and its components are made available,\nbut also must consider how accessible each component is. Release and access to AI systems are of-\nten used interchangeably, but access includes the resources and qualities needed for stakeholders\nto engage with system components. Overall access, beyond release, more concretely determines\noutcomes.\nGenerative Al release decisions for whether a system component is made available and to whom it\nis available do not provide the full information for long-term risk-benefit considerations. Once com-\nponents are released, making those components accessible determines whether they can be used and"}, {"title": "2 Previous and Related Work", "content": "Existing work to distinguish between how a system is released and how it interacts with the public\nincludes defining release as making model components available and deployment as the vector of\nimpact [53]. Research to prevent AI misuse explains the misuse-use tradeoff, showing a misuse\nchain where harm occurs after release [23]. Laying out the model pipeline gives another approach\nto considering certain post-release variables [44]. Examining openness must also include the re-\nsourcing and materials around a system [118]. Governance proposals specific to open foundation\nmodels highlight downstream use considerations and the impact of open systems on the overall AI\necosystem [30]. Recent papers analyze open systems in sectoral contexts, spanning from national\nsecurity and defense priorities [40] to fact-checking organizations [120]. Motivations to make sys-\ntems more open cite scientific reproducibility, product reliability and data security, and economic\ndiversification. Motivations toward closedness per component cite inability to monitor models with\ndownloadable weights and less control to intervene or revoke access to a given component.\nThere has been a lack of consensus on key terms such as \u201copen source\u201d, misuse of the term, and\npopularization of the term \"openness\" [61]. Works toward concrete terminology lay out openness\ndimensions [28], create an openness framework [117], and propose a formal \u201copen source AI\" defi-\nnition [9]. Further work explores open foundation model marginal risk [59], safety specific to open\nfoundation models [77], and risk mitigation for open foundation models [106]."}, {"title": "3 From Available to Accessed", "content": "Release methods commonly address what is released or made available. The gradient of release\nlays out availability: whether or not a system and its components are released along a spectrum\n[103]. Whether a system's components are released or made open is distinct from how accessible\nthe system is; a system may be fully openly released, but not accessible to the many groups affected.\nWhen system components are released, they are not useful until they are made accessible. Fig. 1\ndistinguishes availability and accessibility and shows the three subsections of access. Release can be\nviewed as the initial conditions for whether a component is available before accounting for making\nthe components accessible to an external stakeholder. Accessible subsections grapple with tradeoffs\nper component, with high level risks and benefits noted in Figure 1."}, {"title": "4 Breaking Down Access", "content": "We most closely examine further considerations for system accessibility. Who is granted access and\nhow they are able to use a given component shapes who is able to benefit and risk for malicious\nuse. In order to examine who can access which component, more granularity is needed. The three\nsubsections of access are resourcing, usability, and utility, as shown in Figure 2 with the respective\nvariables per component."}, {"title": "4.1 Resourcing", "content": "Resourcing refers to if and how a broad and diverse population can host and serve a system and its\ncomponents, particularly around infrastructure and costs. Computational infrastructure is a notable\nbarrier; outside of training, AI systems require varying levels of compute for hosting, inference, and\nfunctions such as fine-tuning. Compute resources skew heavily toward industry organizations [20]\nas costs remain high. The high cost of resourcing has been scrutinized as reducing accessibility of\nopen-weight systems [118].\nLocal hosting refers to hosting a model on independent, local hardware, self-hosting can refer to\nhosting a model locally or renting external cloud instances, and external or managed hosting is\nwhen another organization is responsible for infrastructure and hosting costs. External or managed\nhosting includes dedicated deployments with dedicated infrastructure and serverless deployments\nwith shared infrastructure. Costs for locally and self-hosting can be ambiguous and highly dependent\non how hardware and cloud service providers are chosen. Most popular models are available for free\nlimited use via interface when externally hosted by the developer or a hosting platform [12]."}, {"title": "4.1.1 Model", "content": "Ability to acquire infrastructure includes hardware and local infrastructure as well as cloud com-\npute credits needed to locally, self-host, or serve a model. The type of hardware needed will differ\nby model, with larger models running better on only graphics processing units (GPUs) than the more\naccessible central processing units (CPUs), and some smaller models running on GPU-CPU mixed\ninfrastructure or even CPUs. The global GPU shortage and compute demand skews who is able to\nacquire hardware. Renting cloud instances is often a more accessible option. Developers and de-\nployers often offer free credits (e.g. Google Cloud new customer credits, OpenAI Dev Day credits,\nHF ZeroGPU).\nBenefits: Acquiring infrastructure allows users to access benefits from local and self-hosting models.\nFree credits can help low-resource researchers and developers.\nRisks: Infrastructural needs and related costs may change for newer models. Sectoral gaps in who\nis able to obtain compute resources can disadvantage groups such as academia. Free credits can be\nmisused by malicious actors.\nStorage and hosting costs include costs to locally or self-host a model. For local hosting, this\nincludes cost of hardware, storage capacity for storing and loading models, electricity costs, and\noperational costs such as setup and maintenance. Memory requirements depend on the model and\nhardware. High upfront setup costs for locally hosting often leads to preferencing self-hosting or\nusing API alternatives. Self-hosting requires user accounts with cloud providers and hosting plat-\nforms.\nBenefits: Local or self hosting a model gives more controllability and privacy to the user, especially\nfor sensitive data. Long-term usage costs can be lower, depending on the model and usage. Lo-\ncal hosting ensures stability of access and ensures access to the same version of the model, with\nno external dependencies. It also enables unrestricted fine-tuning and other adaptations for better\ncustomization.\nRisks: Building and managing local infrastructure has a high upfront cost and necessitates hardware\ninvestment, in addition to the technical knowledge of setting up and optimizing resources. Energy\ncosts may also be high, and add to potential costs for general operation, from human hours in\nsetup and maintenance to cooling and security. Locally hosted models are less monitorable or not\nmonitorable, especially compared to models hosted by deployer organizations; malicious actors can\nlocally host a model and adapt or output harmful content while unmonitored."}, {"title": "4.1.2 Code", "content": "Access to libraries includes the ability for users to run code that includes libraries that may be only\navailable to a certain organization.\nBenefits: Full access to all parts of the code can empower researchers to reproduce and build on\nexisting work. Lack of access can prevent malicious actor reproduction.\nRisks: Without access to all libraries included, the code is largely unrunnable, preventing scientific\nreproduction."}, {"title": "4.1.3 Training Data", "content": "Storage and hosting ability refers to capacity to host datasets, especially large datasets and those\nwith modalities with high storage requirements.\nBenefits: Being able to store datasets such as training data enables additional research and the ability\nto improve data quality. The Pile dataset at 1346GB in practice is relatively accessible to host [47]\nand has contributed to many other datasets and models including LLaMA [113].\nRisks: For sensitive datasets, secure storage and sharing is needed to prevent misuse or malicious\nuse."}, {"title": "4.1.4 Comparing models", "content": "Comparing hosting and inference costs for open-weight models depends highly on utilization, task,\nlatency, and hardware used, and varies heavily. Given numbers are broad approximations, and\nchange with hardware, usage, and other factors. Precision affects cost, with many popular models\ntrained with 32 or 16 bits of precision. Memory needed for inference tends to be around 20% higher\nthan model memory [25]. While hardware memory varies, we use NVIDIA H100 GPUs, which have\n80 GB of VRAM, and NVIDIA H200 GPUs, which have 141 GB of HBM3e memory. These GPUs\ncost approximately $25,000 and $32,000 each, respectively. Costs for hardware, renting GPUs,\nenergy, and production models fluctuate heavily over time. See Appendix Resourcing Calculations\nfor more information on determining resourcing for Llama 3.1 405B Instruct..\nLlama 3.1 405B Instruct to run locally, uses at minimum 8 NVIDIA H100 GPUs and 405 GB\nVRAM to load the model in a lower 8-bit precision (FP8) and at least 810 GB VRAM and 11 H100\nGPUs to load it in its original 16-bit precision (FP16). To reach the 128K token maximum context\nlength, cache memory requirements are 123.05 GB, which would bring hardware requirements to\n12 H100 GPUs. Full fine-tuning requires 3.25 TB memory [7]. This results in an upfront cost of at\nleast $200,000 for GPUs in FP8 and $300,000 in FP16, plus additional costs for servers.\nDeepSeek v3 to run locally is recommended to use 8 NVIDIA H200 GPUs to deploy in FP8. The\nmodel is trained in FP8, but can be run in BF16 [42]. Due to its mixed precision training framework,\nthe model is relatively small for its performance, at about 1.3 TB. To deploy the BF16 variant doubles\nmemory requirements to 2.5 TB [88]. At minimum, base infrastructure costs $256,000, plus costs\nfor servers. Distilled versions of DeepSeek's reasoning model, DeepSeek-R1, which is trained on\nDeepSeek v3, boast relatively high performance and can run on one GPU with small models, such\nas the 8B parameter model, able to run on a personal computer [83].\nGPT-4 is available via the OpenAI API, where the developer carries storage, hosting, and mainte-\nnance costs. As a closed-weight model, it is not possible to host locally or self-host. Information on\nhosting costs is unavailable publicly.\nClaude 3.5 Sonnet is available via the Anthropic API, with the developer carrying related hosting\ncosts. It is also closed-weight without publicly available hosting information."}, {"title": "4.2 Technical Usability", "content": "Usability determines whether and how a broad and diverse population can technically access and use\nan AI system and its components. For an average user seeking to query a model, well-designed and\neasy user interfaces have enormously lowered the barrier for people outside the AI field to engage\nwith these systems. However, reaching more people also carries a risk of enabling malicious uses\nfrom a larger set of users."}, {"title": "4.2.1 Model", "content": "End user interfaces, including interfaces to connect inputs and outputs and chat with a model,\nlower the barrier to interact with a model and allow broader populations to access a model. These\ncan be built and maintained by hosting organizations, and projects [80] with open repositories can\nease interface building for locally hosted models. Interfaces are also built for tasks such as training,\nfine-tuning [27], and evaluating models, also lowering those barriers.\nBenefit: Well-designed interfaces surpass the need for technical expertise, and enable users without\na technical background to complete technical tasks [96]. These interfaces can make models more"}, {"title": "4.2.2 Code", "content": "Technical skill includes the ability to run code accompanying system releases. It is related to legi-\nbility and quality of documentation, which refers to how easily code that accompanies models can\nbe understood and used. Legacy code can become too convoluted for easy use by external actors.\nBenefits: High quality code release better incentivizes reuse.\nRisks: Poor quality code can make additional research more difficult, with barriers to onboarding\nresearchers and parsing past decisions."}, {"title": "4.2.3 Training Data", "content": "Technical skill refers to the ability to conduct research and analysis on datasets, in addition to using\ndatasets accompanying system releases for development.\nBenefits: Data research can give insights into model performance and safety.\nRisks: Datasets can contain harmful and illegal material.\nAccess types and eligibility to access datasets is similar to that for models, such as restrictions\nby user age and region. It can be further affected by legal constraints, from intellectual property\n(IP) and personally identifiable information (PII) access. Types of access include direct access, API\naccess to certain functions such as the ROOTS tool [86], API access to statistics about data, legal\n(e.g. GDPR) requests, and court discovery.\nBenefits: Limiting dataset access can prevent unauthorized users, such as minors, from accessing\ninappropriate material. Legal compliance for sensitive data protects data subjects.\nRisks: Restricting access to datasets limits who can conduct research."}, {"title": "4.2.4 License", "content": "Customized licenses can range from permissive use for researchers but restricted for commercial\napplications. Permissive use as allowed by licenses for models, code, and data [117], with the types\nof licenses and applicability differing by system component. For models, use may not be tightly\ncorrelated with weights specifically and instead with the system usage overall. For data, rights\nattached to data are an important factor.\nBenefits: Allowing access to researchers can enhance scientific integrity. Customized licenses can\nvary with size of a model; permissive licenses can be staged to allow open usage for smaller param-\neter count models and tailored, more restrictive licenses for larger models, as seen with Alibaba's\nQwen2 model family [123]. Permissive licenses can grant users security.\nRisks: Licenses, especially when customized, can be difficult to understand for an average user with\nlimited legal experience. Increasingly customized licenses are often more complex legally and can\nbe difficult to enforce."}, {"title": "4.2.5 Comparing models", "content": "Many popular models will have some level of free interface to generate outputs, often with limita-\ntions (rate, filters), and hosted by the developer or third party sources. Latency will vary, depending\non infrastructure and setup for open-weight self-hosted models, and depending on demand for API-\nserved models."}, {"title": "4.3 Utility", "content": "Where usability refers to whether a broad and diverse population can technically access a system,\nutility refers to whether populations can gain utility from accessible capabilities of the system. This\ndoes not cover overall or task-specific model performance, which is noted in Non-Access Consider-\nations."}, {"title": "4.3.1 Model", "content": "Multilingualilty considers high quality outputs in multiple languages.\nBenefits: High quality language performance, especially for non-English and low-resource language\nspeakers, opens new markets to AI and allows native speakers of many languages [17, 115] to more\ncomfortably use, build, and deploy.\nRisks: More language availability and quality gives access to attackers in that language and in\nregions where that language is more popularly spoken. Some languages may be more spoken in\nareas with lower resourcing for monitoring or interventions. This disparate resourcing has proved\ndire for social media monitoring failures [107].\nMultimodality covers the modalities for model inputs and outputs, such as text and code, image,\naudio, and video.\nBenefits: Certain modalities, e.g. text, fit better for given use cases and deployment contexts, e.g.\nsummarization.\nRisks: Different modalities require different approaches to monitoring and safeguarding. Some\nmodalities are more prone to certain attacks than others, such as images for NCII generation."}, {"title": "4.3.2 Code", "content": "Reproducibility includes sharing code for training models and other tasks. Comparisons between\nopen systems and open software continue to influence tradeoff analysis [63].\nBenefit: Code release has proven helpful scientifically for reproducibility and collaboration [87],\nleading to higher citations for publications with accompanying code [125].\nRisk: Malicious actors can use code to reproduce components."}, {"title": "4.3.3 Training data", "content": "Sensitive data includes IP and copyrighted material as well as private and PII.\nBenefits: For some use cases in controlled settings with appropriate compliance, training on sensitive\ndata can produce helpful and tailored outcomes.\nRisks: Including sensitive data in training data risks leakage, stealing, and harm to data subjects."}, {"title": "4.3.4 Documentation", "content": "Comprehensiveness is the robustness and legibility of documentation for system components and\nprocesses. Documentation can give insights into system components such as from where public\ntraining data was sourced. In addition to documentation differing in findability and consumability,\nmany processes and aspects of model training and release are not documented or shared publicly;\nfor example release decisions processes are rarely if ever shared [31].\nBenefits: More comprehensive documentation that includes information such as evaluation results\nand uncertainty increases transparency [68] and trust. Useful details in technical papers increase\nreproducibility for scientific integrity.\nRisks: Incomplete dataset documentation or documentation with sparse sections hurts documen-\ntation quality [124]. The same details in technical papers can be misused by malicious actors to\nreproduce components."}, {"title": "4.3.5 Comparing models", "content": "Utility across the four selected models are largely comparable, with strong multilinguality availabil-\nity in DeepSeek v3. Notable differences between the below open- and closed-weight models are in\ninput modality, where GPT-4 and Claude 3.5 Sonnet are vision language models and allow image\ninputs."}, {"title": "5 Scaling Access: Access to Whom", "content": "Who can and should be able to access components often centers on motivation. Some model licenses\nallow research access and not commercial deployment, while some models are broadly open to\nany usage. Additionally, the ability to store, host, and deploy artifacts may differ by factors such\nas geographic location. For sensitive artifacts such as some datasets, the ability to accommodate\nregional laws [57] is crucial. As systems gain reach, the scale of who can access a model has\na positive relationship with both who can benefit, and potential malicious actors. The scale also\ninfluences deployer organizations' ability to manage risk and intervene in violations. Making system\ncomponents accessible sets the foundation for scaling who is given access. Increased access will add\ncomputing power demands and must consider means of distribution. This can affect variables such\nas latency. Depending on the diversity of users, usability and utility variables may update as well,\nsuch as increasing language compatibility when deploying in new regions. Figure 3 shows the flow\nfrom release, to becoming accessible, to scale considerations."}, {"title": "5.1 Individualized Release and Depth of Access", "content": "One of the central questions of access is which actors qualify for access, especially in limited re-\nleases. Calls for independent scrutiny of closed systems, such as structured access [99], safe harbor\nevaluations [69], and third party audits [2], must ask what components an external actor has access\nto, and what is the permissive ability to access each component. Mostly targeted toward researchers\nand auditors, depth of access enables scrutiny outside of the developer's expertise or incentives. For\nproduct and commercial-oriented actors, depth enables better tailored models for a given application.\nPer system component, the amount of access, or the depth, given to an external actor, researcher, or\nauditor depends on the use case and research area [33]. The depth of access overlaps with what is\nmade available, and is sometimes framed as \"completeness\". Some research often requires model\nweights, such as interpretability [29].\nSafe information sharing mechanisms can encourage collaboration and transparency [114]. Some\ndata may not be fit for public release, often due to legal or security constraints, such as trade secrets,\nIP, PII, and national security information. Existing frameworks, such as the Five Safes, help manage\nsafe research access to data [97].\nFor systems developed with researcher access, more work is needed to understand the most urgent\ntypes of research that should be conducted on that system and the correlating level of access needed.\nCurated external access to a closed-weight system faces challenges of appropriate due diligence for\ndetermining the right users to whom access should be granted. Curating access by select priority\nexpertise areas may overlook potential unforeseen contributions."}, {"title": "5.2 Release Distribution Methods", "content": "Who can and will access a system and its components is tied to how a system is distributed. Distri-\nbution considers the method of distribution and the means by which components are made available.\nThis can include developer and deployer platforms, hosting platforms, curated access to individuals,\nsocial media links, and more rudimentary methods such as USB sticks or QR codes [90]. Staging\nreleases with a set time frame for making model weight available [105] can aid both understanding\nof threats and assuring trust in the release process. Developer and deploy platforms can advertise,\ninfluencing who is incentivized to access the platform. Platforms that host model weights can in-\nclude the model developer's sites, hosting platforms like Hugging Face, and partner platforms. For\nexample, Llama models are available on Meta official websites, Hugging Face repositories, Kaggle,\nand partners for different model sizes [6]. Hosting platforms can enable broad access to systems,\nwhich can be controlled and managed via gating and user access. Varied platform visibility for pro-\nprietary, private, or sensitive systems can allow organizations to selectively share components, such\nas internal only organizational access. Content guidelines and terms of use are applied at the model\nlevel.\nDistribution of content affects how impactful a model and its outputs are, for beneficial and ma-\nlicious purposes. For example, AI generated disinformation is only as impactful as its distribution,\nwith studies showing concerns about AI disinformation are overstated [100]. Concerns can of course\nchange with time and major events, such as elections, with distribution remaining a key factor [104].\nThe perception of heavy distribution can have harmful effects: a survey of 100,000 people across\n47 countries showed 59% of people are worried about false news content [78]. Integrating AI into\ndistribution platforms, such as news media, has been met with public skepticism: the majority of\nrespondents in the same survey in the U.S. and UK noted they would be uncomfortable with news\nproduced mostly with AI [78].\nScale of deploying systems requires heavy investment in maintaining the means of deployment,\noften an API, user interface, or other application. From rapid adoption by daily users [73], to\nincreasing usage by researchers [67], to deployment by startups and businesses [10], reaching more\npeople via deployment also means meeting infrastructure, legal, and management demands. These\ndemands tend to differ by user group."}, {"title": "5.3 Managing Scale", "content": "Manageability includes the ability to establish what constitutes misuse and identify misconduct; to\nmonitor and intervene on misconduct; to reduce the reach of a system; and the cost to manage."}, {"title": "6 Access-Adjacent Considerations", "content": "In addition to the risk-benefit tradeoffs for variables under each access category, overarching factors\npost-release strongly influence threat and utility landscapes. How and with what intentions a system\nis deployed influences reach and subsequently system risks-benefit tradeoffs. Accessibility is linked\nto deployability; whether a commercial entity is able to technically and legally deploy a system\ndepends on variables such as licensing and other governance mechanisms."}, {"title": "6.1 Changes Over Time", "content": "The rapid pace of change in the AI landscape shifts the weight of some variables. Notably, changes in\ncapabilities [22], cost [39], and data availability [70] are some of the most influential recent factors.\nEmergent abilities [116] and how to measure and validate them [94] raise research questions on AI\nsystem future capabilities and risks.\nAdvancement and competitive pressure from releases such as DeepSeek-R1, showcasing low-cost,\nopen-weight, high performance, are already shifting potential for all three access categories. Within\nweeks, releases such as Mistral Small 3 [13] contribute to trends towards smaller models. The Allen\nInstitute for AI (AI2) model T\u00fclu 3 405B model [62] quickly boasted high benchmarks but also\nshared lessons on emphasis on reinforcement learning.\nBeyond overall system performance increasing over time, some aspects of model components have\nimproved. Tokenization leaps for non-English languages has increased language availability, al-\nthough pricing disparities remain [19]. Cohere's Command R+'s tokenizer compressed non-English\nlanguage text better than comparable models at its time of release [93], and models such as DeepSeek\nv3 [42] show significant improvements in non-English and low resource languages, especially com-\npared to previous models. Changes in hardware and hardware costs include advances in memory\ncapacity, better computational price-performance ratio, and also potential walls [50].\nInference cost for existing and newer models can change [38] with commercial needs and energy\nlimitations. Popularization of reasoning models can put more computing pressure on inference,\nwhere thinking tokens are more demanded. Decline in data availability affects the utility of data\ncollection over time, which can advantage organizations with better access to data resources."}, {"title": "6.2 Modality Influence", "content": "As referenced in Utility, modalities for model inputs, model outputs, and datasets impact threat\nmodels. Some modalities have higher malicious use potential and comparably less research on\nsafeguards. For example, voice cloning can be helpful in medical settings for patients who have lost\ntheir voices to disease [75], but raises serious misuse concerns such as scams and fraud, prompting\ngovernment response [4]. Modalities that can more closely infringe on personal likeness, such"}, {"title": "6.3 Smaller Models", "content": "In addition to independent projects that enable users to run models locally, such as llamafile [51],\nlarger model families often offer small model options. High costs for larger models incentivize re-\nsearch to maintain high performance in smaller models, as exemplified by models such as DeepSeek-\nR1, Qwen2 72B Instruct, Llama 3 70B, and Mistral Large 2 128B.\nOn-device deployment means models can be hosted without cloud servers and on everyday hard-\nware such as laptops and phones. Apple's OpenELM [74] models ranging from 270M to 3B parame-\nters, can run on Macbooks and some iPhones. Microsoft's Phi models [16] released a 3.8B parameter\nmodel, phi-3-mini, that can run on a phone and boasts comparable benchmark results to GPT-3.5.\nOn-device hosting sidesteps the security risks associated with using the internet. Additional models\nin this category are Google's Gemma and Mistral's Les Ministraux model families."}, {"title": "6.4 Application and Actualization", "content": "A model's usefulness or harmfulness is directly informed by the application in which it is deployed.\nDue to many generative systems' generality in task performance, general risk assessments are un-\nable to capture all possible risks and magnitudes of risks. Assessing model safety is best done in the\ncontext of the operational domain [60]. Building application-specific infrastructure and optimizing\nfor usability can be weaponized, as seen with deepfake NCII applications [79]. Application-specific\ninfrastructure can also be misused, where AI tools are integrated into platforms such as animation\nand modeling interfaces [119]. For certain risks, such as chemical and biological attacks, mate-\nrial and physical needs and equipment are needed to actualize harm. Creating harmful toxins and\npathogens relies on the feasibility, both operationally and biologically, of developing a high risk\nattack [76]. For high risk threats, effective threat modeling should consider how operationalizable,\ntargeted, and scalable, attacks can be."}, {"title": "6.5 Non-Access Considerations", "content": "Some release considerations are adjacent to but not directly related to model accessibility. This in-\ncludes overall and task-specific performance, scientific and market impacts, and usefulness outside\nof access variables such as agentic behavior [98]. As discussed in Changes Over Time, capabil-\nity increases are often centered in risk discourse, with advances with scaling [58] informing some\ndeployment decisions [3]. High performance for high risk tasks, whether from a large generalized\nmodel or task-specific model, can include output quality. Output quality can range based on task.\nThe realism and human-like quality of outputs affects usefulness. AI outputs close to or indistin-\nguishable from human-generated outputs that are not labeled or shared as Al outputs contribute\nto clouding our information ecosystem. Task-specific performance, especially for high risk tasks,\nand training on dangerous information such as weapons manufacturing, add to potential threats.\nMitigating these threats includes sorting data and redacting dangerous information or blacklisting\npotentially dangerous data sources.\nScientific and economic factors are popularly referenced in release decisions, with themes of broader\ncommunity feedback and the need for openness in scientific integrity and innovation [102]. Re-\nleasing training techiniques, as with DeepSeek-R1 [41], can influence broader investment in for\nexample, reinforcement learning.\nConcentration of power [111] in the market and research field is a commonly cited concern for\nwho is able to access and build systems. This includes sectoral compute gaps and the ability for"}, {"title": "7 Limitations and Future Work", "content": "While access considerations broaden insights into release, tangential artifacts and variables as well\nas non-access variables require more scrutiny.\nSystem-adjacent components that are not inherently part of development may not be included in\nmodel releases. Evaluations and test datasets, which can be developed by model developers or other\nresearch groups, can be withheld out of concern for contaminating training data [45]. Model releases\nthat do include evaluations include AI2's OLMo [8], where the code and data used to produce OLMO\n2's results are on the model release page.\nModalities with comparably less research background and less access demand may have risks\nand benefits that evolve differently than other modalities. Incentives to reduce prices for popular\ncommercial modalities such as text may differ from video. The limited data and publicly available\ndocumentation on some modality releases, such as audio and video, make effective release strategies\ndifficult to meaningfully compare.\nThe pace of advancements and ecosystem influence shifts costs and incentives in timeframes that\nare difficult to reliably model. Pricing changes as new model service providers enter the market\nleads to competitive pricing. Related to Changes Over Time, and similar to demands by modalities\ndiffering, ecosystem variables such as demand for access and economic influences in willingness\nto adjust to costs [64]. Low-cost, open-weight models are more easily able to long-term embed in\ndigital infrastructure, which can encourage broader integration due to version stability. As influ-\nenced by DeepSeek-R1's trend of high performance in low-cost, small models, examining potential\nthresholds for preferencing open-weight model integration is a critical open question that affects\nglobal and geopolitical influence.\nDifferentiating research versus commercial access can be blurry; research access can inform and\nevolve into product and commercial ideas. More insight is needed on potentially distinctive uses\nand needs by sector."}, {"title": "8 Conclusion", "content": "Aspects of access beyond system release provide more clarity to release benefits, risks, and trade-\noffs. Examining system accessibility and possible scale of access can more accurately inform release\nand policy decisions and research. We show through granular analysis and model comparisons that\nsimilar considerations exist across the release spectrum and for both open- and closed-weight mod-\nels. We also show barriers without usable infrastructure, and high risk via accessible interfaces and\nfine-tuning. As accessibility positively impacts scale, reaching more users broadens usage but also\nreaches more malicious actors and affects ability to intervene. The resourcing, technical usability,\nand utility subsections of access exemplify how to more meaningfully weigh usefulness and"}]}