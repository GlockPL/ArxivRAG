{"title": "MILE: Model-based Intervention Learning", "authors": ["Yigit Korkmaz", "Erdem B\u0131y\u0131k"], "abstract": "Abstract-Imitation learning techniques have been shown\nto be highly effective in real-world control scenarios, such\nas robotics. However, these approaches not only suffer from\ncompounding error issues but also require human experts to\nprovide complete trajectories. Although there exist interactive\nmethods where an expert oversees the robot and intervenes if\nneeded, these extensions usually only utilize the data collected\nduring intervention periods and ignore the feedback signal\nhidden in non-intervention timesteps. In this work, we create a\nmodel to formulate how the interventions occur in such cases,\nand show that it is possible to learn a policy with just a\nhandful of expert interventions. Our key insight is that it is\npossible to get crucial information about the quality of the\ncurrent state and the optimality of the chosen action from\nexpert feedback, regardless of the presence or the absence\nof intervention. We evaluate our method on various discrete\nand continuous simulation environments, a real-world robotic\nmanipulation task, as well as a human subject study. Videos\nand the code can be found at https://liralab.usc.edu/mile.", "sections": [{"title": "I. INTRODUCTION", "content": "Imagine training a household robot to help users place the\ndishes in the dishwasher. One way to do this is to use\nreinforcement learning (RL) that has been proven successful\nin several areas ranging from gaming to dialogue systems\nand autonomous driving [1]\u2013[4]. However, its need for lots\nof online interactions with the environment as well as a well-defined reward function make it unsuitable in a real-world\nsituation like this. An alternative is to use imitation learning\n(IL) where an expert provides demonstrations of how to\nplace the dishes. IL requires fewer interactions in the world\nthan RL and does not require a reward function. A common\ndrawback of this approach is the compounding distributional\nshift, which results from the accumulation of errors when\ndeploying a learned policy [5]: the small inaccuracies in the\nrobot's learned policy will move it to an out-of-distribution\nstate where the policy may fail more significantly, which may\nresult in the robot breaking the dishes.\nInteractive learning methods try to overcome this com-\npounding errors problem by iteratively querying the expert\nwith system states, and fine-tuning the policy based on the\nexpert actions [6]\u2013[11]. Most of these methods do not let the\nhuman intervene at will but transfer the control to the human\naccording to some criterion [8], [9]. In others, the human can\ntake over the control at any timestep [7], [12]\u2013[14].\nGoing back to the running example, assume the robot\nhad a mediocre policy in the beginning, thanks to some\ninitial training done in the factory setup. In this interactive\nscheme, we would control the robot only when we think it\nis doing or about to do something wrong. This is clearly"}, {"title": "II. RELATED WORK", "content": "Interactive Imitation Learning. In imitation learning, ex-\npert data is used to train a policy in a supervised way [15]\u2013\n[20]. Interactive imitation learning methods try to overcome\nits compounding errors problem [5], [6] by querying the\nexpert on the learned policy's rollouts [6]\u2013[9], [15], [21].\nWhile the expert relabels either the whole trajectory of the\npolicy with actions, or some parts of it that are automatically\nselected by estimating various task performance quantities\n[6], [9], there are also works where the expert has the\nfreedom to intervene at will [7], [12], [13], [22]. Some of\nthese methods consider the implicit feedback coming from\nthe states where the expert chooses to not intervene. They\nattempt to incorporate the information leaking from non-\nintervention intervals either by enforcing those state-action\npairs to be constrained in the action-value cost functions [14]\nor utilize weighted behavioral cloning (BC) to incorporate\nthe signal from non-interventions, with different heuristics\nused for assigning weights to on-policy robot samples,\nand expert human interventions [12], [13], [22]. However,\nnone of these algorithms uses a model to understand and\nlearn from why the expert chooses to not intervene. In our\nmethod, we propose a model that attempts to capture how\nthe interventions occur, and how satisfied the user is with"}, {"title": "III. PROBLEM DEFINITION", "content": "We formulate the problem as a discrete time Markov deci-\nsion process (MDP) with the standard (S, A, \u03c1, f, R, T, \u03b3)\nnotation. The robot does not know the reward function R or\nthe transition dynamics f of the environment. It starts with\nan initial policy \u03c0\u03b8 : S \u2192 A parameterized by \u03b8.\nIn addition, a potentially noisy expert human can intervene\n(take over the control) with any action ah \u2208 A at any\ntimestep. However, in contrast to some of the existing works\n[35], the human has to make the decision about intervening\nor not without observing the robot's action ar \u2208 A in that\nparticular state. This is a more viable setting in robotics\nwhere it is not realistic to expect the robot to check each\nand every action with the human.\nWe let the robot and the human interact in this setup,\nand we record (s, ar, ah, s') from every timestep, with the\npossibility that ah is undefined if the human did not intervene\nin that timestep. Our objective is to find a policy \u03c0\u03b8* that\nmaximizes the expected cumulative discounted reward:\n$\\theta^* = \\arg \\max_\\theta E_{\\tau \\sim \\pi_\\theta} \\sum_{t=0}^{T-1} R(s_t, a_t)$ \nwhere s0 ~ p(.), and (st, at) pairs are sampled according to\nthe policy \u03c0\u03b8 and transition function f.\nWe assume the human interacts with the robot for N\niterations, each of which includes k episodes. After each\niteration, we train the robot policy using the data collected so\nfar. In the next section, we describe how we do this training."}, {"title": "IV. MODEL-BASED INTERVENTION LEARNING (MILE)", "content": "Our method to solve this problem is based on a compu-\ntational model of when and how the human may decide\nto intervene the robot's operation. We will introduce this\nintervention model in the first subsection. Subsequently, we\nwill leverage this model to create a framework where we\nupdate the robot's policy \u03c0\u03b8 based on its interactions with\nthe human and the environment.\nIntervention Model. Consider our running example:\nwould you intervene the robot's operation when it is quickly\nmoving toward a glass jar that stands at the edge of the\ncountertop? How about if the jar was made of plastic? This\nexample highlights the important factors that affect when\nhumans intervene the robot. Firstly, consider the case of glass\njar. The human is likely to intervene because they predict the"}, {"title": "V. SIMULATION EXPERIMENTS", "content": "Experiment Setup. We tested our method on four different\nsimulation tasks, one with a discrete and three with contin-\nuous action spaces. The discrete action space environment\nis Lunarlander from Gymnasium [42]. The remaining three\ntasks-Drawer-Open, Peg-Insertion, and Button-Press-are\nfrom the Metaworld suite [43] where the primary goal is\nto control a 6 DoF robot arm with a gripper to accomplish\nvarious tasks that require precise joint control and are prone\nto distributional shifts and local minima issues during RL\noptimization. For the Metaworld experiments, we use true\nworld states as observations. To retain temporal information,\nwe concatenate the states of the previous three timesteps\nwith the current timestep. The action space is composed of\n4 DoF: the Cartesian positions of the end effector, which\nalways points down, and +1 dimension for the gripper. Since\nour model does not rely on any assumptions about the\nenvironment's reward structure, and our primary comparisons\nare with other interactive learning methods, we follow the\nprior work [44] to evaluate the methods using success\nrates. Lunarlander is the only exception where we used the\nenvironment rewards due to the simplicity of the task.\nWe use simulated humans to mimic human interventions in\nour simulation experiments. The goal of these experiments is\nto scale up the tasks and to show that it is possible to fine-\ntune an initial policy in a sample-efficient way using our\nframework. The data collection phase involves two distinct\npolicies: a suboptimal agent for generating rollouts and an\nexpert agent for interventions guided by our intervention\nmodel. In place of the simulated human's mental model\nof the robot (\u03c0\u03c2), we train a BC policy on the rollouts of\nthe suboptimal policy. During data collection, we rollout the\nsuboptimal policy and intervene with the simulated human.\nIn other words, the policy \u03c0\u03b7 of the human in the intervention\nmodel is replaced by \u03c0* during the data collection using\nsimulated humans. We see that these simulated interventions\nincrease the success rate by comparing the dashed and the\ndotted lines in Fig. 3. During training, we jointly train the\nsuboptimal agent and the mental model as we described in\nSection IV. At test time, we only use the trained policy and\ndiscard the mental model.\nBaselines. We compare our methods against (i) BC in-\ntervention where the model estimates human's interventions\nwith only a neural network, (ii) HG-DAgger [7], a state-of-\nthe-art algorithm that iteratively finetunes the policy based\non the interventions, (iii) RLIF [28], a recent work which\nmodels the interactive imitation learning as an RL problem\nand uses interventions as the reward signals to train the policy\nin an online fashion, (iv) IWR [13] and (v) Sirius [12], two\nmethods utilizing weighted BC with different heuristics to\nlearn from both intervention and non-intervention timesteps.\nWe start with the same initial suboptimal policy for all\nmethods, and assume there is no access to any expert dataset,\napart from the interventions provided.\nTo be consistent with the realistic settings, we keep the\nintervention ratio in the rollouts to be less than 30% by tuning\nthe hyperparameter c. For our initial results, we train our\nmethod with N = 1 iteration and k = 15 episodes. For\nfairness, we train the other baselines until they have the same\nnumber of interventions in their datasets as ours.\nResults. We present the results in Fig. 3. Our method\nachieves the best results across all environments (only tied"}, {"title": "VI. REAL ROBOT EXPERIMENT", "content": "In order to show the effectiveness of our method in real\nworld settings, we designed an experiment using a WidowX"}, {"title": "VII. HUMAN SUBJECT STUDY ON REAL ROBOT", "content": "Finally, we conducted a user study to analyze how accurately\nour model estimates the humans' interventions, as the success\nof our method relies on the success of the intervention model\ncapturing when and how humans intervene the robot. The\nstudy is approved by the IRB office of the University of\nSouthern California. We used the same task setting and\nexperiment setup as in the previous section.\nProcedure. We recruited 10 participants (3 female, 7 male,\nages 20-28). The study started with a training period where\nsubjects had a chance to get familiar with the robot controls.\nWe had 2 experiment settings. At the beginning of each\nexperiment, the subject was first shown 3 trajectories from\nthe initial policy to make them aware of the robot's capabil-\nities. In the first experiment, the user was asked to collect 5\nintervention trajectories (N = 1, k = 5), where they can take\nover the initial policy at any time. These trajectories were\nthen used to fine-tune the initial policy using our method,\nand the final policy was shown to the user. We asked the\nusers to rate the initial and the final policies.\nThe second experiment differed from the first by only\nusing N = 5 and k = 2. The users could intervene and take\ncontrol at any time during data collection: we did not provide\nany instructions on how often to intervene. We showed the\nfinetuned policy at the end of each iteration and asked the\nuser to rate its performance. At the end of the study, we\ndid a post-study survey to collect participants' feedback\non the method's effectiveness, improvement and the their\nsatisfaction on the final result.\nResults. We compared our intervention model with the\nmajority estimator that always predicts no intervention (the\nintervention rate of humans is around 30-40% throughout the\nprocedure) and a neural network based estimator trained with\nthe data from previous iterations to classify interventions\nin the current iteration. As shown in Fig. 7, our model\noutperforms both methods in predicting when an intervention\nwill occur. Additionally, most users rated our model's effec-ness and adaptation speed as adequate, demonstrating it\nsuccessfully mirrors real human interventions."}, {"title": "VIII. CONCLUSION", "content": "Summary. In this work, we presented MILE, a novel way of\nintegrating interventions in policy learning. We developed a\nfully-differentiable intervention model to estimate when and\nhow the interventions occur, and a policy training framework\nthat uses the intervention model.\nLimitations and Future Work. Our intervention model\nconsiders only the current state to estimate interventions.\nThis does not fully capture the temporal analysis humans\nmake before intervening, e.g., waiting for robot to fail in a\nrecoverable way before taking control. Future work should\nexplore ways of integrating this temporal aspect of inter-vention estimation, possibly with maintaining a belief over\nrobot actions and using Bayesian updates. Another limitation\nis our assumption that humans have an accurate mental\nmodel of the robot policy. Investigating how incorrect mental\nmodels affect the performance is an important direction\nfor future research. Recently, some works showed that the\ninterventions can be used for data generation and allow better\nsim-to-real generalization [45], [46]. Testing our method's\ncapabilities in these domains and exploring whether using a\nstructure behind interventions enables further improvements\nin generalization are interesting future directions. Finally,\nlearning from natural language-based corrections [47], [48]\nis a recent exciting research area that may benefit from our\nintervention model."}, {"title": "A. Pseudocode of the Algorithm", "content": "Algorithm 1 MILE: Model-based Intervention Learning in Iterative Setting\nNotations\nN: maximum deployment rounds, k: number of rollout\nepisodes in each deployment round,\nl: number of batches, b: batch size,\nm: number of epochs in each learning round,\na: learning rate,\n\u03c0\u03b9: initial policy, \u0175: initial mental model\nfor i 1 to N do\nD2+1 \u2190 DEPLOYMENT(\u03c0, D\u00b2)\n\u03c0?+1, +1 \u2190 LEARNING(\u3160, \u3160, D\u00b2)\nfunction DEPLOYMENT(\u03c0\u03bf, D)\nCollect rollouts w/ interventions T1,..., Tk\nD' \u2190 DU {1, ..., Tk}\nreturn D'\nfunction LEARNING(\u03c0\u03bf, \u03c0\u03b5, D)\nfor m epochs and l batches each do\nGet the next mini-batch (s\u00b2, a, an, v\u00b2)=1 ~ D\nCompute (s\u00b2; 0, \u00a7)=p(v\u00b2=1|s\u00b2) based on Eq. (7)\nCompute J(0, \u00a7) based on Eq. (12)\nRun in parallel:\n0 0 - J(0,5)\n\u03be\u2190 \u03be - \u03b1\u2207\u03b5 J(0,\u03be)\nreturn \u03c0\u03bf, \u03c0\u03b5"}, {"title": "B. Simulation Experiment Details", "content": "Depending on the action space, we trained suboptimal initial policies and various levels of expert policies for simulated\nhumans using SAC or DQN. We train a different policy for each task. For each task, the same expert is used to intervene\nacross all methods. To generate the mental models of simulated humans, we collected 100 rollouts of the initial policy and\ntrained a BC agent on them. In all result plots, we display the mean and standard error over 3 seeds for each method.\nRegarding the observation space, we use true world states, i.e. low-level states of the robot and the task-related object. This\nincludes robot joint positions and the positions and orientations of relevant objects. Our action space consists of the change\nin Cartesian coordinates of the robot end effector and the gripper state. Each episode has a maximum of 1000 timesteps,\nwith early termination upon success.\nTo compare our method with RLIF, we used RLPD as its backbone algorithm in the domains with continuous action\nspaces as it was done in the original paper. For domains with discrete action spaces, we used DQN as the backbone of\nRLIF. We initialized the policy networks for all methods as the clones of the initial policy \u03c0\u03b8. For the offline demonstration\nablation, we also initialized RLIF's replay buffer with those demonstrations."}, {"title": "C. Real Robot Experiment Details", "content": "In this experiment, we use image observations (captured from a USB webcam) along with the robot's end effector position.\nWe keep the same action space as in the simulation experiments for real-world settings. The robot begins with the octagonal\nblock in its gripper. The initial BC policy is trained using 120 human-collected trajectories, gathered with a Meta Quest\n2 headset. At the beginning of real-world experiments, we show 3 trajectories to the users to make them aware of the\nrobot's capabilities. We also warm-start the initial mental model using these same demonstrations. This process requires no\nsupervision from the human and minimal effort since the human only needs to observe the robot."}, {"title": "D. Hyperparameters", "content": "We used a Multi-Layer Perceptron (MLP) with hidden dimensions of 256 in both the MetaWorld and real-robot experiments\nfor all methods. In order to get image embeddings in real-robot experiment, we used a pretrained R3M model with ResNet50\narchitecture. To retain temporal information, we concatenate the states of the previous three timesteps with the current\ntimestep. For the LunarLander experiment, we used an MLP with hidden dimensions of 64 for all methods. The network\noutputs an action for the current step.\nThe log-probabilities of the actions vary in scale between the policies used in different tasks. This necessitates adjusting\nthe standard deviation of the normal distribution whose cumulative distribution function (CDF) is used in Equation 2, along\nwith task-specific cost terms (parameter c), to calculate smoother rather than skewed intervention probabilities."}]}