{"title": "RAGVERUS: Repository-Level Program Verification\nwith LLMs using Retrieval Augmented Generation", "authors": ["Si Cheng Zhong", "Jiading Zhu", "Yifang Tian", "Xujie Si"], "abstract": "Scaling automated formal verification to real-world projects\nrequires resolving cross-module dependencies and global contexts, which\nare challenges overlooked by existing function-centric methods. We in-\ntroduce RAGVERUS, a framework that synergizes retrieval-augmented\ngeneration with context-aware prompting to automate proof synthesis\nfor multi-module repositories, achieving a 27% relative improvement on\nour novel RepoVBench benchmark the first repository-level dataset for\nVerus with 383 proof completion tasks. RAGVERUS triples proof pass\nrates on existing benchmarks under constrained language model budgets,\ndemonstrating a scalable and sample-efficient verification.", "sections": [{"title": "Introduction", "content": "Good engineers favor well-written tests to confirm that code works correctly,\nat least for the tested inputs. In software verification, testing is replaced by\na set of complete specification; a verifier conducts a compile-time check to\nensure the implementation matches the specification, providing vital assurances\nespecially for high-confidence systems in critical areas such as operating systems\nand financial applications [17,32]. However, proof construction is a challenging\nprocess that requires domain expertise [29,11,15]. This process is time-consuming\nand non-trivial especially for large software projects.\nAdvances in generative AI for code completion have spurred interest in\nverification-aware program synthesis, aiming to simultaneously enhance code\ntrustworthiness and automate proof generation [8,30,26]. Large language models\n(LLMs) streamline formal verification by rapidly sampling and iterating proof\ncandidates, lowering barriers to adoption [4,1,3]. However, current function-\nlevel program verification approaches inherit vanilla retrieval models [32,3] from\ntheorem proving that solely rely on finetuned embedding encoders using prior\nexamples [31], lacking the rich, precise domain knowledge of code syntax or\nproject structure [24]. Moreover, existing datasets [21] are derived for small-scale,\nisolated contexts, lacking evaluation in complex repository settings to reflect real\npractices in production.\nWe recognize the main challenges of repository-level verification tasks to be\n(1) the vast semantic context and (2) the identification of correct dependent\npremises, as noted in prior works studying large-scale verification in the theorem"}, {"title": "Background", "content": "Verus [12] is an SMT-based verification tool designed to formally verify Rust\nprograms while leveraging Rust's powerful type system, in particular its linearity\nand borrow checking mechanisms. Users write its domain-specific language (DSL)\ninline with Rust, while dividing Rust code into three code modes: specification\nannotation, proof annotation, and executable implementation. Verification in\nVerus is performed by encoding Rust functions and their associated annotations\nas SMT formulas [5], which are then processed by an SMT solver such as Z3 [6].\nRecently, AUTOVERUS [30], an LLM-based approach shows strong performance\nin automating proof completion in Verus solving nearly all tasks of VerusBench.\nRAG [14] is a novel technique to enhance generative models by incorporating\nexternal knowledge retrieval, enabling more accurate and contextually relevant\nresponses in knowledge-intensive tasks. Existing RAG methods [21,32] used in the\nsoftware verification domain are relatively simple, such as directly comparing the\nmethod signatures and post-conditions. We draw inspiration from repository-level\ncode generation [24] and theorem proving [31] domains, examining a framework\nto capture the structured nature of programming tasks."}, {"title": "RAGVERUS Framework", "content": "We propose RAGVERUS, a retrieval-augmented framework for LLM-based\nprogram verification, with a special focus on complex Verus projects. The frame-\nwork modularizes verification task preparation, context retrieval methods, and\nproof generation models, leveraging repository information and verifier feedback\nto align domain knowledge."}, {"title": "The General Pipeline", "content": "As illustrated in Fig. 1, the RAGVERUS pipeline consists of three stages: 1) mining\ncode properties, 2) retrieving task-specific information, and 3) finally generating\nproofs and/or verifiable code.\nMining Code Properties. We preprocess repository-wide code artifacts to\nextract verification-critical metadata (more details in Section 4.1), building on\ninsights from context modeling in repository-level code generation [24]. Through\nstatic analysis over the codebase hierarchy, we identifying function/type signa-\ntures, method calls, module dependencies, and control/dataflow relationships\nessential for proof construction.\nTo enable efficient retrieval, RAGVERUS uses persistent vector stores where\ndocuments are encoded into embeddings using OpenAI's text-embedding-3-large\n[23], indexed via FAISS [7]. Depending on the retrieval method chosen, either the\nRust code themselves or their metadata is indexed. This hybrid representation\npreserves both syntactic and semantic patterns in the form of high-dimensional\nembeddings, enabling adaptive context retrieval in later stages.\nContext Retrieval. We believe two general kinds of retrieval tasks are essential\nfor repository verification. Few-shot example retrieval matches code snippets\nor metadata to provide contextual proof patterns and exemplar code-proof pairs,\nwhile Dependency retrieval identifies function signatures and domain-specific\nsyntax necessary for constructing proofs. Retrieved contexts are integrated into\nprompts to guide proof generation, with examples expanded in Section 3.2."}, {"title": "Instantiations of Retrieval Module", "content": "Building upon the modular framework established in Section 3.1, this subsection\npresents customizable retrieval modules tailored to address distinct program\nverification challenges, each designed to align with specific task requirements\nwithin the verification pipeline.\nFew-Shot Example Retrieval Few-shot example retrieval in RAGVERUS\ninvolves selecting a small number of representative examples to guide the LLM's\nautomated proof generation. The objective is to retrieve relevant code snippets\nor contexts as contextual prompts, ensuring that generated annotations adhere\nto correct styles and practices. To achieve this, the retrieval process leverages\nFAISS [7] to identify the most semantically similar examples, implemented\nthrough LlamaIndex [20]. During retrieval, depending on the method chosen,\neither the input code or its metadata is used as the query vector. The FAISS\nvector store then retrieves the top-k most similar examples, with an ID filter\nensuring that the input document itself is excluded from the retrieved examples.\nTwo separate vector indices are created for each dataset: one for code body\nembeddings and one for code informalization embeddings. The embeddings are\nbuilt only with unverified files, but during retrieval, the corresponding verified\nfiles are also retrieved, forming input-output pairs as few-shot examples. We\nretain a maximum of three examples after ranking, prioritizing diversity and\nrelevance. These examples provide contextual references that enhance the LLM's\nability to generate relevant and accurate proofs [2], ultimately improving the\noverall quality and precision of the verification process.\nCode-Based Example Selection. Searching by code embeddings effectively\nprovides relevant reference examples by capturing structural and functional\nsimilarities between code implementations. This is particularly beneficial for\nthe task of program verification, as it ensures that retrieved examples align\nwith correct coding patterns and verification logic, improving the accuracy\nand consistency of proof generation.\nInformalization-Based Example Selection. This process generates\nnatural language contexts of the target Rust function as supplementary"}, {"title": "Evaluation", "content": "We run the evaluation pipeline against two datasets respectively: VerusBench [30]\nand RepoVBench that we created. Although VerusBench is not a repository-level\nverification dataset, we conducted this experiment to confirm the performance\nincrease after introducing the RAG module.\nModel of Choice. We chose GPT-4o (gpt-4o-2024-08-06) [22], the newest\nversion of large language model offered by OpenAI at the time of the experiment\nas our model of choice. The temperature is set to 1.0 during sampling.\nDataset Preparation and Example Pool Creation. For experiments with\nVerusBench [30], the dataset used for retrieval consists of unverified and verified\nRust files from three benchmarks: Diffy, MBPP, and Misc. In addition, example\nRust files originally used in the refinement phase of the AUTOVERUS [30] pipeline\nwere also included. For experiments on RepoVBench, only code documents within\nthe repository are indexed and retrieved.\nRAG modules. For few-shot example retrieval, we experimented with both\nthe code index and the informalization index. For dependency retrieval, due\nto limited amount of data in our current dataset, we only evaluate the simple\nembedding-based dependency retrieval approach. We leave the more advanced\napproaches for dependency retrieval such as dependency graphs and embedding\nprojection for future work."}, {"title": "Evaluation on Function-Level Verification", "content": "While the full AUTOVERUS pipeline is already highly effective on this relatively\nsmall and constrained dataset (proving 137 out of 150 tasks in VerusBench) [30],\nour goal here is to demonstrate the effectiveness of RAG, which represents an\northogonal contribution. Therefore, we choose our baseline to be the direct-\ngeneration pipeline presented in the original AUTOVERUS paper. While the\noriginal process samples up to 125 answers, we limit sampling to 5 times for each\nof our setups to examine the benefits from retrieval modules.\nThe performance comparison between different retrieval strategies is shown\nin Table 1. RAGVERUS-Code denotes the results achieved by performing retrieval\non the code index; similarly, RAGVERUS-Text retrieves on the informalization\nindex.\nRAGVERUS-Code consistently outperformed other methods, achieving the\nhighest counts of correct, safe, and successful completions overall (87, 134, 84)\nand excelling particularly in MBPP. Retrieval using informalized summaries also\nsubstantially improved over the Baseline, with a notable edge over code-based\nretrieval in count of success for Diffy tasks (23 vs. 19). The experiment results\nconfirmed that context retrieval methods contribute to the overall success of\nRAGVERUS. Comparing to the baseline results reported in the AUTOVERUS\npaper, the same baseline could only solve 67 tasks even with a maximum LLM\ninvocation allowance of 125."}, {"title": "Evaluation on RepoVBench", "content": "Although we still examine proof completion on a per-function basis, repository-\nlevel verification becomes more challenging due to the need for coordinated\nreasoning across interdependent modules.\nWe run ablated trials of RAGVERUS on the new RepoVBench for proof-\ncompletion; specific model parameters are listed in Appendix B. We compare\nto two baselines, the direct-generation as well as the refinement pipelines from\nAUTOVERUS; we then augment each pipeline with our aforementioned retrieval\nmodule with code index. Resulting pass rates are shown in Table 2, reflecting a\nremarkable improvement on every setting augmented by context retrieval."}, {"title": "Related Work", "content": "Automated program verification with LLM. While there exist various\ntechniques for automated program verification with machine learning based meth-\nods like Code2Inv [25], CIDER [19] and Code2RelInv [28], there has been recent\nadvancements focusing on the integration of LLMs to enhance proof generation\ncapabilities [29]. LLMs provide the potential to automate these processes by\ngenerating human-like proofs [29] and code [16]. LLM-aided proof/code gener-\nation has been studied within verification-aware programming languages like\nFrama-C [9], Dafny [13] and Verus [12]. Recent work AUTOVERUS [30] leverages\nLLMs for automated proof synthesis in Rust using finetuned knowledge bases and\nrefinement processes. Clover [26] addresses the case of SV where no specification\nis formally given, and attempts to autoformalize a specification in Dafny by\naligning with the available implementations and documentations. LeanDojo [31]\nintroduces a large premise pool in Lean and uses fine-tuned retrieval models to\nperform RAG for proofs. Although these approaches differ in methodology and\napplication areas, they all focus primarily on single-function verification.\nRepository-level program verification. Repository-level program verifica-\ntion is a relatively emerging field with limited previous research addressing the\ninherent complexity of large software systems. Selene [32] represents a pioneering\neffort in this domain, but the language focus, verification tooling, and depen-\ndency management approaches are different from RAGVERUS. While there has\nbeen recent development in repository-level LLM-based code generation [18],\nRAGVERUS extends this domain into automated verification, ensuring not only\nthe generation of code but also its formal correctness."}, {"title": "Conclusion and Future Work", "content": "RAGVERUS addresses repository-level verification via retrieval-augmented genera-\ntion and context-aware prompting, enabling LLMs to synthesize proofs informed\nby cross-module dependencies and project-wide examples. Supporting this effort,\nRepoVBench provides the first repository-level benchmark for Verus, derived from\nreal-world systems to reflect compositional reasoning challenges, and establishes\na configurable playground for evaluating retrieval strategies. Future directions\ninclude implementing more fine-grained retrieval methods and expanding the\nbenchmark to diverse repositories, possibly using self-evolved methods on verifi-\ncation code [4]. By bridging AI-driven synthesis with practical project demands,\nwe hope this work lays a foundation for realistic, community-driven assessments\non repository-level program verification tools."}, {"title": "RepoVBench Data Example", "content": "We process code data to generate tasks and to prepare informative contexts,\nfollowing the methods outlined in Section 4.1."}, {"title": "RepoVBench Experiment Setup", "content": "We maintain a similar LLM budget across experiments when evaluating on the\nRepoVBench in Section 5.2, so as to ensure a fair comparison between different\napproaches.\nFor all methods that require sampling, we set the temperature to t=1.0 for\ndiverse generation results. Except for DirectGen greedy, which is our determin-\nistic baseline, we set t=0 and only sample once.\nDirect Generation. For DirectGen-sample and DirectRAG, we set the number\nof generations to a constant of 3 samples.\nRefinement Generation. For Refinement and Refinement+RAG, we generate\n2 initial samples and allow a maximum of 2 more repair steps, budgeting under 4\nLLM calls.\nWe note again that the retrieval module used in Section 5.2 searches over\nVeriSMo contents only, and only uses code embedding information when conduct-\ning similarity retrieval."}, {"title": "RepoVBench Qualitative Experiment Results", "content": "Common failure modes in RepoVBench.\nVeriSMo contains nested dependencies in proofs and often requires special\nformats of annotation different from common examples given by the official Verus\ntutorial.\nEspecially for models without our RAG assistance, we observe on the Simple\ndataset that they would produce answers that are logically correct to human, but\nmiss subtle syntax in VeriSMo, such as not using the specially defined integer\ntype for the particular module.\nKey Difficulties Faced in RepoVBench-Complex.\nIn Table 2, as reflected by the success rates between Refinement (59-7=52)\nand Refinement+RAG (75-23-52) on RepoVBench-Complex, the basic context\nreferences provided by the current retrieval method are too simple to provide\nmuch assistance in completing the hard verification tasks.\nThe current experiments fail to handle several situations:\nWe observe that many contextually similar tasks in the hard category require\ndifferent premise sets and proof style, requiring more specialized direction of\nretrieval\nThe ground-truth premise pool is actually larger than the maximum number\nwe return, demanding larger context capacity from the retrieval module\nSome tasks require super long proofs (> 80 proof annotation lines). We\nsuspect any successful run would require hundreds of refinement cycles and\ncompiler feedbacks, in addition to a fully complete premise pool, which is\nout of our current sampling budget"}, {"title": "Generation Style", "content": "Since the number of verification success, as a binary metric,\ndoes not capture gradual improvement in code generation quality, We evaluate\nthe code similarity with respect to the ground truth proof to reflect how much of\nthe proofs are on the right directions. We analyze the average BLEU scores for\nselected pipeline settings from Section 5.2, calculated over the entire RepoVBench\ndataset.\nWe observe that both retrieval augmented pipelines produce more coherent\nanswers, suggesting that they are utilizing the VeriSMo-specific contexts as\nexpected."}]}