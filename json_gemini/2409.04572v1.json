{"title": "Neurosymbolic Methods for Dynamic Knowledge Graphs", "authors": ["Mehwish ALAM", "Genet Asefa GESESE", "Pierre-Henri PARIS"], "abstract": "Knowledge graphs (KGs) have recently been used for many tools and applications, making them rich resources in structured format. However, in the real world, KGs grow due to the additions of new knowledge in the form of entities and relations, making these KGs dynamic. This chapter formally defines several types of dynamic KGs and summarizes how these KGs can be represented. Additionally, many neurosymbolic methods have been proposed for learning representations over static KGs for several tasks such as KG completion and entity alignment. This chapter further focuses on neurosymbolic methods for dynamic KGs with or without temporal information. More specifically, it provides an insight into neurosymbolic methods for dynamic (temporal or non-temporal) KG completion and entity alignment tasks. It further discusses the challenges of current approaches and provides some future directions.", "sections": [{"title": "1. Introduction", "content": "Knowledge Graphs (KGs) [1] have gained attention in the past few years for represent-ing information in a structured way, i.e., entities and relations. It has been used for var-ious applications and domains from search engines and recommendation systems [2] tobio-informatics [3] and social sciences [4]. They capture relationships between entitiesin a way that is both human-readable and machine-processable, enabling advanced rea-soning and inference capabilities. The significance of KGs lies in their ability to inte-grate vast amounts of heterogeneous data, providing a unified framework that supportscomprehensive querying and analysis.\nHowever, the real world is dynamic, with changes happening continuously-newentities emerge, existing relationships evolve, and facts that were once true may becomeoutdated. In such scenarios, traditional static KGs fall short as they cannot accommo-date these changes in real-time. This limitation has led to the development of DynamicKnowledge Graphs (DKGs), which can evolve by incorporating temporal information.Dynamic KGs update the data and track changes, enabling temporal queries and histori-cal analysis."}, {"title": "2. Related Work", "content": "One of the very recent studies provides an overview of the DKGs [5]. However, thischapter mostly focuses on providing a formal definition of different categories of DKGs along with neurosymbolic methods for DKGs, such as learning representations over such symbolic representations using methods based on neural networks along with the downstream tasks such as KG completion and entity alignment.\nSeveral studies have been conducted on learning representations over KGs. For instance, various techniques for refining KGs are summarized in [6], including methodsfor KG completion. A categorization of static KG completion algorithms is given in [7].In contrast, an exhaustive survey on multimodal KG embedding algorithms that utilizeliteral (text, numeric, image) information within the static KGs are presented in [8] alongwith an experimental comparison of these algorithms. Furthermore, a recent article [9]provides an overview of various KG completion tasks, such as transductive and induc-tive link prediction. It details methods incorporating background information from large"}, {"title": "3. Preliminaries", "content": "This section formally defines static, temporal, and dynamic KGs with examples.\nDefinition 1 (Static Knowledge Graph) Let G = (E,R,L,F) be a directed labeledgraph, where E, R, and L are the sets of entities, relations, and literals, respectively.\nFC E\u00d7R\u00d7(EUL) represents a set of facts such that f = (h,r,t) or f = (h,r,l) representone triple where f \u2208 F, h,t \u2208 E, r \u2208 R, and l \u2208 L.\nExample 1 Consider a simple knowledge graph G = (E,R,L,F) where:\nThe definition of a static KG can further be extended to a temporal KG, where each triple can have a time interval representing its temporal validity:\nDefinition 2 (Temporal Knowledge Graph) Let G = (E,R,L,T,Q) be a directed labeled graph, where E, R, L, and T represent the set of entities, relations, literals, andtimestamps, respectively. Q \u2286 E \u00d7 R \u00d7 (EUL) \u00d7 (T\u222a {0}) represents a set of facts suchthat q = (h,r,t, [Ts,te]) or q = (h,r,l, [ts, te]) represent one quadruple where q \u2208 Q, h,t \u2208 E, r\u2208 R, I \u2208 L, and ts, te \u2208 TU {0} and ts and te represent the start and end time, i.e., \u03c4\u03c2 < \u03c4e. If ts = te then it represents a point in time \u03c4\u2208\u03a4.\nExample 2 Consider a simple knowledge graph G = (E,R,L,F) where:"}, {"title": "4. Representing Dynamic and Temporal Knowledge Graphs", "content": "This section presents the different techniques to represent temporal information inknowledge graphs and that can be used for temporal KGs or dynamic KGs.\n4.1. Techniques to Represent Temporal Information in Knowledge Graphs\nThe first nucleus to represent time is the use of various XML Schema Definition [11](XSD) datatypes, such as xsd:dateTime, xsd:duration, xsd:date, and others, thatallows for precise representation of temporal data, including specific moments, durations,calendar dates, and individual time components, in KGs. These datatypes have beendesigned to facilitate the accurate and standardized encoding of temporal information,enabling robust temporal querying and reasoning.\n4.1.1. Temporal Properties\nTemporal properties directly incorporate time into relationships within a KG. Theseproperties provide inherent temporal aspects to the entities and relationships they de-scribe. For example (please note that all examples are provided in the Turtle syntax2):\n4.1.2. Reification\nReification allows a triple to be treated as a subject of another triple, thereby attachingmetadata such as temporal information. There are several ways to reify a triple.\n4.1.3. Time Ontology in OWL\nThe Time Ontology in OWL\u00b3 provides a comprehensive framework for representingtemporal concepts. This ontology includes classes and properties for describing instants,intervals, and durations. Thus, the ontology is designed to support complex temporalreasoning and querying following Allen's interval algebra [20]. For example:\n4.1.4. Named Graphs and Quadruples\nNamed graphs [21] group triples into a single graph that can have metadata associatedwith it. This allows for temporal information to be added at the graph level. For example:\n4.1.5. RDF-star\nRDF-star is an extension of the RDF data model that offers a more compact and intuitiveway to represent complex statements, including those involving temporal information.\n4.1.6. Versioning\nMaintaining historical data with timestamps is crucial for tracking changes over time.Versioning techniques ensure that the evolution of data is documented, enabling tem-poral queries and historical analysis. One can use simple approaches, such as adding atimestamp to each triple, or more complex methods, such as reification or named graphs.\nTable 1 provides a summary of the temporal and dynamic capabilities of the var-ious RDF techniques presented in this section For instance, temporal properties give astraightforward way to annotate data with time, but they may not offer the flexibilityneeded for complex scenarios where relationships between different temporal attributesmust be explicitly defined. Reification and Named Graphs/Quadruples allow for moredetailed metadata, such as temporal context, versioning, or provenance, which can becrucial for historical analyses or data integrity. However, these methods can introducedata redundancy and increase the complexity of queries. Additionally, only one valuecan be used as a fourth value or as the name of the named graph.\nThe Time Ontology in OWL is particularly useful for those needing to model com-plex temporal relations, such as intervals and durations, and supports reasoning overthese data types. This makes it ideal for applications requiring detailed temporal reason-ing, such as historical data analysis or event tracking.\nRDF-star offers a more compact and intuitive representation, which reduces data re-dundancy and simplifies the data structure. This can be particularly advantageous whendealing with large datasets or when the overhead of traditional reification methods be-comes a concern. However, RDF processors must support RDF-star syntax and seman-tics, which might not be universally available.\nVersioning techniques are essential for tracking changes over time, ensuring that aKG can evolve while maintaining a record of past states. This is particularly relevant indomains where data history is as important as the data itself, such as legal or historicalresearch, or dynamic KG completion.\nWhen choosing a method for representing temporal information in KGs, researchersand practitioners should consider the specific requirements of their application, includ-ing the complexity of the temporal relationships, the need for metadata, the size of thedataset, and the capabilities of the RDF processors they are using.\n4.2. Prominent Knowledge Graphs\nWe now describe some of the most prominent general-purpose KGs and how they repre-sent temporal information.\n4.2.1. DBpedia\nDBpedia [13] incorporates temporal aspects into its dataset through properties that de-note specific time-related data. For example, temporal properties such as dbo:birthDateand dbo:deathDate indicate birth and death dates for people, while other prop-erties capture periods of activity or events, such as dbo:productionStartDate anddbo:productionEndDate for manufacturing or production events. This inclusion of tem-poral data allows for queries about the duration of events or the temporal relationships"}, {"title": "5. Dynamic Knowledge Graph Completion", "content": "In the existing literature [26], TKGC methods are generally classified into two cate-gories: interpolation-based and extrapolation-based. Interpolation-based methods aim to predict missing knowledge by leveraging existing quadruplets. In contrast, extrapolation-based methods are designed for continuous TKGs, enabling prediction of future events by learning embeddings from previous or historical snapshots of entities and relations.This work treats interpolation methods as TKGC for TKGs, while extrapolation-basedmethods are applied to dynamic KGs, as defined in Section 3.\n5.1. Temporal Knowledge Graph Completion (TKGC)\nTemporal Knowledge Graphs (TKGs) often contain millions or billions of quadruplets,but they are typically incomplete for several reasons. First, extracting information fromunstructured text sources can be error-prone, resulting in incomplete data. Second, cap-turing or integrating all available information, particularly from diverse or complexsources, is challenging. Third, the sources themselves may lack comprehensiveness orbe biased, leading to selective inclusion of information and omission of other relevantfacts. Lastly, the dynamic nature of information, with knowledge continuously evolving,contributes to these gaps.\nThe KGC task \u2013 commonly known as link prediction (LP) \u2013 aims to predict miss-ing links by utilizing existing information. Various techniques have been developed forthis task. However, a significant limitation of these methods is their difficulty incap-turing the temporal dynamics of facts. They are typically designed for static knowl-edge graphs (KGs) that assume facts do not change over time. Therefore, these meth-ods are ineffective when applied to TKGs, as they overlook the crucial temporal infor-mation inherent in TKGs. Temporal KGC (TKGC) methods have emerged to enhanceLP accuracy by addressing the limitations of traditional KGC methods. TKGC meth-ods improve upon these by incorporating timestamps of facts into the learning processin addition to the facts themselves.\nTKGC aims to predict possible links between two entities at a specific time. Thiscan be accomplished in different ways: i) tail prediction - given the head and relation at acertain time, predicting the tail entity (<h, r,? >), ii) head prediction - given the tail andrelation at a particular time, predicting the head entity (<?, r,t >), iii) relation prediction -given the head and tail at a certain time, predicting the relation (< h, ?,t >). Inspiredby the survey in [27], TKGC methods could be categorized into timestamps-dependent-based TKGC, timestamps-specific functions-based TKGC, and deep learning-basedTKGC methods.\nTraining procedure: Given a TKG G and a set of facts Q in G, a scoring functiong(q) is defined to assign a factual score for a fact or a true quadruplet q \u2208 Q. A negativenegative sampling strategy [74] is employed to create a set of negative samples, i.e., factuallyincorrect quadruplets Q', to enhance the expressiveness of learned representations for theentities and relations in G.\nLoss function: A loss function L aims at maximizing g(q) for all q \u2208 Q and minimiz-ing g(q') for their negative samples q' \u2208 Q. The following are the commonly used lossfunctions by TKGC approaches.\nMargin-based ranking loss (MRL) [75] enforces that the confidence in the cor-rupted quadruplet is lower than in the true quadruplet by a certain margin. MRLis computed as\n$L_{MRL} = \\sum_{q \\in Q} [\u03bb + g(q) \u2013 \\sum_{q' \\in Q'} g(q')]_+,$ (1)\nwhere [x]+ = max(x,0) and \u03bb > 0 is a margin hyperparameter.\nCross-entropy loss (CEL) [76] also aims at obtaining a large gap between truequadruplets and the negative samples but without enforcing a fixed margin for allfacts.\n$L_{CEL} = \\sum_{q \\in Q} \\frac{exp(g(q))}{\\sum_{q' \\in Q'} exp(g(q'))}$ (2)\nBinary cross-entropy loss (BCEL) [77] emphasizes the score of individual truequadruplets and negative samples as follows:\n$L_{BCEL} = \\sum_{q \\in Q \\cup Q'}yg(q) + (1-y)g(q'),$ (3)\nwhere y = 1 if q \u2208 Q and y = 0 otherwise.\n5.2. Non-Temporal Dynamic KG Completion\nTraditional methods for generating KG embeddings do not consider the evolving natureof a KG where new entities and relations are constantly being added to the KG. Thisrequires training the embeddings over the new KG from scratch even if the changes areminimal which can lead to increased computational costs. Various attempts have beenmade to deal with this challenge. One of the first attempts to address this issue is onlinelearning, which learns incrementally as new information arrives. However, one of thedrawbacks of these methods of KG embedding is that they do not consider the problem ofcatastrophic forgetting. To mitigate this issue, continual or lifelong learning [81] methodswere proposed, alleviating the problem of catastrophic forgetting in various tasks. Incatastrophic forgetting, adaptation to a new distribution generally results in a largelyreduced ability to capture the old ones.\nContinual KG Embedding methods (CKGE) has recently received growing attentionwhich perform fine-tuning with only new knowledge, leading to reduced training costs.These methods effectively alleviate the problem of catastrophic forgetting while learningthe representations of the newly added knowledge. Continual Learning-based methodsfurther target two kinds of solutions. First, the full-parameter fine-tuning paradigm whichmemorizes old knowledge by replaying a core old dataset or introducing additional reg-ularization constraints. Although this paradigm effectively mitigates catastrophic forget-ting, it significantly increases training costs, especially when handling large-scale KGs.Second, it adopts the incremental-parameter fine-tuning paradigm, with only a few pa-rameters to learn emerging knowledge. This strategy may still lead to an increase inparameters and training time.\nRecently, low-rank adapters such as LoRA have enabled efficient parameter fine-tuning and are used to reduce the training time in Large Language Models (LLMs). Oneof the very recent studies, FastKGE, introduces incremental low-rank adaptation mech-anisms, IncLoRAs, to reduce training costs for continual learning for KG embeddings.The rest of the section discusses and compares these methods in detail.\n5.2.1. Online Learning Based Methods\npuTransE [82]. The current translational models (KG embedding models usingtranslation-based scoring functions) for static KG embeddings have three significant"}, {"title": "6. Dynamic Entity Alignment", "content": "This section provides an overview of the temporal entity alignment methods betweentwo KGs. In addition to TKG alignment, a few methods have been proposed consideringnew data while performing entity alignment.\n6.1. Temporal Entity Alignment Methods\nMost of the methods for temporal entity alignment use GCNs or GNNs to learn the rep-resentations of the entities alongside the temporal attention mechanism. In the following,we briefly overview the methods proposed so far.\n6.2. Temporal and Evolving Entity Alignment Methods\nTemporal Relational Entity Alignment (TREA) [97] learns alignment-oriented TKGembeddings and represents new emerging entities. The first step is to map entities, re-lations, and timestamps into an embedding space, and the initial feature of each entityis represented by fusing the embeddings of its connected relations and timestamps aswell as its neighboring entities. A Graph Neural Network (GNN) is employed to cap-ture intra-graph information, and a temporal relational attention mechanism is utilized tointegrate relation and time features of links between nodes. Finally, a margin-based fullmulti-class log-loss is used for efficient training, and a sequential time regularizer is usedto model unobserved timestamps.\nIncremental Temporal Entity Alignment (ITEA) [98] targets the problem of temporalityas well as evolving KGs. It uses a combination of knowledge distillation with the GraphAttention Network (GAT) and the GCN acting as the teacher and student models, re-spectively. The proposed model transfers knowledge from a complex model, the teacher,whose output (in terms of probabilities) is used to train a simpler model, the student.The teacher Model within knowledge distillation provides instructional guidance to thestudent model during its training phase."}, {"title": "7. Discussion & Future Directions", "content": "This chapter formally defines various types of DKGs and explores how this knowledgecan be represented within KGs. It then delves into different representation learning tech-niques for both non-temporal and temporal DKGs, focusing on tasks related to KG com-pletion. Additionally, it covers methods for aligning temporal and dynamic KGs. Thestudies discussed in this chapter primarily rely on the triple structure or treat the KG as agraph by incorporating the contextual information of entities and relationships whilelargely overlooking schematic or ontological information [9]. Furthermore, the adapt-ability of these algorithms to domain-specific and real-world applications is limited, es-pecially as KGs can expand to contain billions or even trillions of triples. Many of these"}]}