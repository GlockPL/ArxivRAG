{"title": "Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS", "authors": ["Wei Sun", "Xiaosong Zhang", "Fang Wan", "Yanzhao Zhou", "Yuan Li", "Qixiang Ye", "Jianbin Jiao"], "abstract": "Novel View Synthesis (NVS) without Structure-from-Motion (SfM) pre-processed camera poses-referred to as SfM-free methods-is crucial for promoting rapid response capabilities and enhancing robustness against variable operating conditions.\nRecent SfM-free methods have integrated pose optimization, designing end-to-end frameworks for joint camera pose estimation and NVS. However, most existing works rely on per-pixel image loss functions, such as L2 loss. In SfM-free methods, inaccurate initial poses lead to misalignment issue, which, under the constraints of per-pixel image loss functions, results in excessive gradients, causing unstable optimization and poor convergence for NVS.\nIn this study, we propose a correspondence-guided SfM-free 3D Gaussian splatting for NVS. We use correspondences between the target and the rendered result to achieve better pixel alignment, facilitating the optimization of relative poses between frames. We then apply the learned poses to optimize the entire scene. Each 2D screen-space pixel is associated with its corresponding 3D Gaussians through approximated surface rendering to facilitate gradient back-propagation. Experimental results underline the superior performance and time efficiency of the proposed approach compared to the state-of-the-art baselines.", "sections": [{"title": "Introduction", "content": "Novel-view synthesis serves as a fundamental objective within the realm of computer vision. The recent surge in NVS popularity is largely attributable to the success of Neural Radiance Fields (NeRFs) (Mildenhall et al. 2021) and 3D Gaussian Splatting (3DGS) (Kerbl et al. 2023). However, these methods require densely captured views with accurately labeled camera poses, which is often not feasible in practical scenarios. Often, camera poses are obtained from SfM methods like COLMAP (Schonberger and Frahm 2016) as a pre-processing step to NeRF and 3DGS, which is not only time-consuming but also prone to fail due to its sensitivity to feature extraction errors and difficulties in handling textureless or repetitive regions.\nRecent studies (Bian et al. 2023; Lin et al. 2021; Wang et al. 2021; Fu et al. 2024; Jiang et al. 2024) have focused on reducing the reliance on SfM by integrating pose estimation directly within the NVS framework. However, we would like to note that existing approaches typically rely on per-pixel image loss functions (such as L2 loss) from a pair of RGB images and compute per-pixel color derivatives with respect to desired scene parameters. The rendered result and the target do not sufficiently overlap or align because the camera pose is inaccurate at the initial stage of optimization. This problem is exacerbated when there is significant camera movement between consecutive views, at which point achieving perfect per-pixel alignment between the rendered result and the target becomes even more challenging. This misalignment issue, under the constraints of per-pixel image loss, often results in excessive gradients, leading to instability in the optimization process and difficulty in convergence.\nTo address this problem, we introduce a Correspondence-Guided SfM-free 3D Gaussian Splatting for NVS (CG-3DGS), a novel approach that integrates 2D correspondence detection (Sun et al. 2021; Tang et al. 2022), and computes derivatives on associated points instead of on a fixed grid of pixels. Specifically, we detect the 2d correspondence to find a pixel matching between rendered and target images and design a novel loss function based on the pixel matching. We then develop an approximated surface rendering pipeline for 3D Gaussians, which propagates disturbances from the 2D screen space to the parameters of the 3D Gaussians for differentiable scene optimization. Our derivatives are dense and could account for long-range object motions through the correspondence-based loss function, naturally leading to better robustness in optimization.\nInspired by but fundamentally distinct from CF-3DGS (Fu et al. 2024), we construct a two-step optimization pipeline: (i) We initialize an auxiliary 3D Gaussian set given frame t with depth back-projection, and we sample the next nearby frame t+1. Our goal is to learn an affine transformation that can transform the 3D Gaussians in frame t to render the pixels in frame t+1. Correspondence-based loss function provides the gradients for optimizing the affine transformation, which is essentially optimizing the relative camera pose between frames t \u2013 1 and t. This process continues iteratively until we obtain all the relative poses between frames 0, 1, ..., t. (ii). We initialize another 3D Gaussians set, where we perform scene optimization with all the frames and their corresponding learned camera poses."}, {"title": "Related Work", "content": "Various 3D scene representations are utilized to produce realistic images from new viewpoints, including planes (Horry, Anjyo, and Arai 1997; Hoiem, Efros, and Hebert 2005), meshes (Hu et al. 2020; Riegler and Koltun 2020, 2021), point clouds (Xu et al. 2022; Zhang et al. 2022), and multi-plane images (Tucker and Snavely 2020; Zhou et al. 2018; Li et al. 2021). NeRFs (Mildenhall et al. 2021) have recently become prominent for their superior photorealistic rendering capabilities, with numerous enhancements such as better anti-aliasing (Barron et al. 2021, 2022, 2023; Zhang et al. 2020) and improved reflectance (Verbin et al. 2022; Attal et al. 2023).\nMore recently, the use of point-cloud-based representations has surged due to its rendering efficiency (Xu et al. 2022; Zhang et al. 2022; Kerbl et al. 2023; Luiten et al. 2023; Kopanas et al. 2022; Yifan et al. 2019). For example, Zhang (Zhang et al. 2022) introduce a method to learn the per-point position and view-dependent appearance through a differentiable splat-based renderer initialized from object masks. Furthermore, 3DGS (Kerbl et al. 2023) facilitates real-time rendering of novel views using its explicit representation combined with a novel differential point-based splatting technique. Nevertheless, these methods typically depend on pre-computed camera parameters derived from SfM techniques (Hartley and Zisserman 2003; Schonberger and Frahm 2016; Mur-Artal, Montiel, and Tardos 2015; Taketomi, Uchiyama, and Ikeda 2017).\nInitial efforts in SfM-free novel view synthesis include iNeRF (Yen-Chen et al. 2021), which employs key-point matching to estimate camera poses. NeRFmm (Wang et al. 2021) introduces a method for joint optimization of camera pose and NeRF itself. Techniques such as those proposed in BARF (Lin et al. 2021) focus on learning neural 3D representations and aligning camera frames using hierarchical positional encodings. The approach in Nope-NeRF (Bian et al. 2023) incorporates monocular depth priors to simultaneously capture relative poses and synthesize new views. (Meuleman et al. 2023) uses a combination of pre-trained depth and optical-flow priors to refine blockwise NeRFs, which helps in the sequential recovery of camera poses.\nIn more generalizable settings, methods like SRT (Sajjadi et al. 2022), VideoAE (Lai et al. 2021), RUST (Sajjadi et al. 2023), MonoNeRF (Tian, Du, and Duan 2023), DBARF (Chen and Lee 2023), and FlowCam (Smith et al. 2023) aim to learn a scene representation from unposed videos using the implicit framework of NeRF. Despite these efforts, they often fail to achieve satisfactory view synthesis without specific scene optimization and share NeRF's original limitations, such as the inability to render explicit primitives in real time.\nThe inherent complexity of NeRF's implicit modeling often complicates the simultaneous optimization of scene and camera poses. Recent advancements like 3DGS, with its explicit point-based scene representation, facilitate real-time rendering and efficient optimization. New developments, such as those in CF-3DGS (Fu et al. 2024), continue to push the limits of simultaneous scene and pose optimization. CF-3DGS (Fu et al. 2024) employs a progressive training strategy to reduce the cumulative noise associated with the pose optimization process. However, these methods consistently rely on per-pixel image loss, which always results in excessive gradients and unstable optimization when the optimization target and rendering result deviate significantly from perfect per-pixel alignment. This is a common issue in SfM-free scenarios due to the inaccurate initial pose estimation, which leads us to explore the integration of correspondence in the simultaneous scene and pose optimization."}, {"title": "Method", "content": "In this paper, we leverage 3D Gaussians to reconstruct photo-realistic scenes from sequential frames of a video stream. Given a sequence of unposed images {I1, ..., IK} with camera intrinsics, our goal is to better reconstruct the complete scene via a joint optimization of the camera poses and the 3D representation (i.e. 3D Gaussians). We detail our method in the following sections, starting from a brief review of the representation and rendering process of 3D Gaussians (Sec 3.1). Then, we propose a correspondence-guided pose optimization, a simple yet effective method to estimate the relative camera pose from each pair of nearby frames (Sec 3.2). Finally, we briefly introduce how to reconstruct scenes using the estimated poses (Sec 3.3)."}, {"title": "Revisiting 3D Gaussian Splatting", "content": "3D Gaussian Splatting (3DGS) is a point-based novel view synthesis technique that uses 3D Gaussians to model the scene. The Gaussian attributes are optimized based on a set of input training views denoted by ground truth images $I_{gt} = \\{I_i \\in R^{H \\times W}\\}_{i=1}^{K_1}$ and associated camera poses $P_{gt} = \\{W_i \\in R^{3 \\times 4}\\}_{i=1}^{K_1}$. The Gaussian initialization is derived from a sparse point cloud created via SfM across the training views. To increase the number of Gaussians in areas where small-scale geometry is insufficiently reconstructed, a Gaussian densification process is periodically applied during training.\nEach Gaussian $G_i$ in the scene is described by several parameters: its position $x_i \\in R^3$, scale $s_i \\in R^3$, rotation $r_i \\in R^4$, base color $c_i \\in R^3$, view-dependent spherical harmonics $h_i \\in R^{15\\times3}$, and opacity $\\alpha_i \\in R$. Collectively, these parameters are grouped as:\n$G = \\{G_i = \\{x_i, s_i, r_i, c_i, h_i, A_i\\}\\}_{i=1}^{N},$ (1)\nwhere $N$ denotes the total count of Gaussians.\nDuring synthesis, scaling and rotation parameters are translated into matrices $S_i$ and $R_i$. The Gaussian $G_i$ is spatially characterized in the 3D scene by its center point, or mean position, $x_i$ and a decomposable covariance matrix $\\Sigma_i$:\n$G_i(x_i) = e^{x_i},\\ \\Sigma_i = R_iS_iS_i^T R_i^T$. (2)\nTo facilitate the differentiation of 3D Gaussian rendering, the Gaussian projection process is applied from a specific camera pose $W$, approximating the splatting of a 3D Gaussian onto the 2D image plane:\n$\\Sigma^{2D} = JW \\Sigma W^T J^T$ (3)\nwhere $J$ represents the Jacobian of the affine approximation of the projective transformation.\nFor each pixel, the final rendered color and depth can be formulated as the alpha-blending of $N$ ordered Gaussians that overlap the pixel:\n$\\hat{C} = \\sum_{i=1}^N c_i \\alpha_i \\prod_{j=1}^{i-1} (1-\\alpha_j),$ (4)\n$\\hat{D} = \\sum_{i=1}^N d_i \\alpha_i \\prod_{j=1}^{i-1} (1-\\alpha_j),$ (4)\nwith $c_i$, $\\alpha_i$, and $d_i$ representing the color, opacity, and depth of the Gaussians, respectively.\nThe optimization of the 3DGS model relies on minimizing a composite loss function using stochastic gradient descent:\n$\\mathcal{L}(G|W, I) = ||\\hat{I} - I||_1 + L_{SSIM}(\\hat{I}, I),$ (5)\nwhere $\\hat{I}$ is the rendering result and $I$ is the ground truth image. The overall loss combines L1 loss for residual minimization and SSIM loss for structural similarity."}, {"title": "Correspondence-guided Pose Optimization", "content": "As shown in Fig. 1 (a), for the initial frame $I_1$, which is at timestep 1,\nwe apply a standard monocular depth network to produce a depth map, represented as $D_1$. We then construct the point cloud $P$ by back-projecting the depth map $D_1$ using the default identity camera pose (orthogonal projection) and camera intrinsics, and use $P$ to initialize 3D Gaussians instead of relying on SfM-derived points. Following this initialization, we optimize a set of 3D Gaussians $G_1$, adjusting all attributes to reduce the correspondence-based loss between the rendered image and the ground truth $1_1$,\n$G_1^* = \\arg \\min_{G_1,\\gamma_1,s_1,\\alpha_1} L_{cor} (\\mathcal{R}(G_1), I_1),$ (6)\nwhere $\\mathcal{R}$ denotes the rendering operation of 3DGS. The correspondence-based loss $L_{cor}$ is detailed in Sec. 3.2.3.\nThe problem of camera pose estimation is addressed by predicting the transformation of 3D Gaussians as discussed in CF-3DGS (Fu et al. 2024). Starting with the Gaussian center's position $\\mu$, we project it into the 2D camera plane with camera pose $W$ as $\\mu_{2D} = K(W\\mu)$. Hence, estimating the camera pose effectively involves determining the transformations of these 3D Gaussians.\nFor the relative camera pose estimation, we apply a learnable SE-3 affine transformation $T_t$ to the pretrained 3D Gaussians $G_t^*$, transforming it into frame $t+1$, represented as $G_{t+1} = T_tG_t$. This transformation $T_t$ is refined by minimizing the photometric loss between the rendered images and the subsequent frame $I_{t+1}$:\n$T_t^* = \\arg \\min_{T_t} L_{cor}(\\mathcal{R}(T_t \\copyright G_t), I_{t+1}),$ (7)\nDuring this optimization phase, we preserve the attributes of the pretrained 3D Gaussians $G_t^*$ unchanged to distinctly separate the effects of camera motion from changes such as deformation, densification, pruning, or self-rotation of the Gaussians. The transformation matrix $T$, comprising quaternion rotations $q \\in 50(3)$ and translation vectors $t \\in R^3$, facilitates the estimation of relative camera poses between consecutive frames. After this, we have estimated the relative camera pose between frames $I_t$ and $I_{t+1}$. As the next frame $I_{t+2}$ becomes available, this process is repeated: we optimize the 3D Gaussians to obtain $G_{t+1}^*$, similar to what is described at the end of Sec. 3.2.1; we then optimize the relative pose between $I_{t+1}$ and $I_{t+2}$, and could subsequently infer the relative pose between $I_t$ and $I_{t+2}$.\nWe utilize off-the-shelf detectors (Tang et al. 2022; Sun et al. 2021) to establish the 2D correspondences between ground truth image $I$ and rendered result $\\hat{I}(W)$ for the pose optimization. The 2D screen-space coordinates in $I$ are represented as $K = \\{k^{(1)}, k^{(2)}, ..., k^{(M)}\\}$, where $M$ represents the total number of points. Correspondingly, the 2D screen-space coordinates in $\\hat{I}(W)$ are $K' = \\{k'^{(1)}, k'^{(2)}, ..., k'^{(M)}\\}$. The optimization objective is to align $k$ with $k'$, visualized in Fig. 1 (b).\nTo enable gradient back-propagation from the matching of $k$ and $k'$ to the 3D Gaussians shaping the surface, we employ a differentiable approximate surface renderer, described in Sec. 3.2.4, to render the screen-space coordinates at $k'^{(i)}, i = 1, 2, ..., M$ as $q(k'^{(i)})$. The resulting loss function is expressed as:\n$L_{cor-rgb} = \\sum_{i=1}^{M} ||q(k'^{(i)}) - k^{(i)}||_1.$ (8)\nNotably, $q(k'^{(i)})$ numerically matches $k'^{(i)}$, yet it creates a pathway for gradients to flow back to the underlying 3D representation without altering the original 3DGS.\nMoreover, incorporating short-range relations through pixel-wise supervision can assist in stabilizing the optimization process. This loss is formulated as:\n$L_{pix-rgb} = ||I - \\hat{I}(W)||_1.$ (9)\nFurthermore, the depth matching process involves equating the monocular depth at $k$, denoted as $d(k)$, with the rendered depth at $k'$, denoted as $d(k')$. The corresponding loss term is defined as:\n$L_{cor-depth} = \\sum_{i=1}^{M} ||\\hat{d}(k'^{(i)}) - d(k^{(i)})||_1.$ (10)\nThe correspondence-based loss consolidates these components:\n$L_{cor} = \\lambda_1 L_{cor-rgb} + \\lambda_2 L_{pix-rgb} + \\lambda_3 L_{cor-depth},$ (11)\nwhere $\\lambda_1 = 10$, $\\lambda_2 = 1$, $\\lambda_3 = 1$.\nOur aim in correspondence-based optimization is to transmit gradient information from a 2D screen-space location to its associated 3D surface location. Essentially, we seek to link disturbances at a 2D screen-space location with those at its 3D surface counterpart.\nGiven the volumetric nature of the 3D Gaussian representation, explicit surfaces are not present. However, reconstructing an explicit surface is extremely time-consuming (Park et al. 2019), and modifying the rendering logic of the 3D Gaussian to obtain surfaces would also significantly increase the training duration (Jiang et al. 2024). Instead, according to previous studies (Keetha et al. 2024;\nChung, Oh, and Lee 2024; Fu et al. 2024; Yan et al. 2024),"}, {"title": "Approximated Surface Rendering", "content": "the depth of an expected 3D surface point $D(k)$ relative to a 2D screen-space point $k$ is computed as follows:\n$D(k) = \\sum_i d_i \\alpha_i(k) \\prod_{j=1}^{i-1} (1 - \\alpha_j(k)),$ (12)\nwhere $d_i$ indicates the z-axis position of the Gaussian centers within the camera coordinate system, and $\\alpha_i$ and $\\alpha_j$ represent the alpha-blending coefficients for the $i$th and $j$th Gaussian, respectively.\nAs illustrated in Fig. 2, the corresponding expected 3D surface point $\\Psi(k)$ at $k$ could then be defined by:\n$\\Psi(k) = \\sum_i \\mu_i \\alpha_i(k) \\prod_{j=1}^{i-1} (1 - \\alpha_j(k)),$ (13)\nwhere $\\mu_i \\in R^3$ represents the center position of the $i$th Gaussian. This formula provides an approximation of the 3D surface point without relying on time-consuming surface reconstruction method like signal distance function (SDF) (Park et al. 2019).\nIn our pose optimization process, correspondences between the rendered views and the reference views are stored in a cache. By reusing these correspondences in subsequent iterations, we achieve a significant reduction in computation time without significantly degrading performance, as adjacent frames in continuous video tend to exhibit similar features and poses. Concretely, rather than recalculating correspondence points for each image pair in every iteration, we strategically update these correspondences every $H$ iterations\u2014where $H$ is empirically set to 50."}, {"title": "Scene Optimization", "content": "Following the camera pose optimization, we proceed to optimize a new set of 3D Gaussians that ultimately represent the scene. Similarly to the pose optimization phase, we start by generating a set of initialized 3D Gaussians using the depth estimation from the first frame $I_1$. Here, we keep the camera pose fixed and focus solely on minimizing the photometric loss as in the vanilla 3DGS (Kerbl et al. 2023). During the optimization, we randomly sample frames from the training set of the scene and utilize the associated optimized poses for training, as shown in Fig. 1 (c)."}, {"title": "Experimental Setup", "content": "We conduct extensive experiments on several real-world datasets, including Tanks and Temples (Knapitsch et al. 2017) and CO3D-V2 (Reizenstein et al. 2021).\nFollowing the methodology in Nope-NeRF (Bian et al. 2023), we assess the quality of novel view synthesis and the accuracy of pose estimation across eight diverse scenes that encompass both indoor and outdoor environments. We select seven images from each 8-frame sequence for training and evaluate the novel view synthesis on the remaining frame. Camera poses are estimated and assessed on all training images following alignment according to Umeyama's method (Umeyama 1991). CO3D-V2: This dataset comprises thousands of object-centric videos, maintaining view of the full object while the camera moves in a complete circle around it. Deriving camera poses from CO3D videos is more challenging compared to Tanks and Temples due to the large and complex camera movements. We randomly select four scenes from different object categories and follow the same protocol as CF-3DGS (Fu et al. 2024) to divide the training and testing sets.\nWe assess the performance of novel view synthesis and camera pose estimation tasks. For the former, we evaluate using standard metrics such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM) (Wang et al. 2004), and Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al. 2018). For the latter, we utilize established visual odometry metrics such as Absolute Trajectory Error (ATE) and Relative Pose Error (RPE).\nOur implementation leverages the PyTorch framework (Paszke et al. 2017) and adheres to the optimization parameters specified in 3DGS (Kerbl et al. 2023), unless noted otherwise. Importantly, we continuously adjust the opacity throughout the training process to effectively limit the unchecked growth of Gaussian components caused by inaccuracies in pose estimation. For the Tanks and Temples and CO3D V2 datasets, the off-the-shelf monocular depth networks used are DPT (Ranftl, Bochkovskiy, and Koltun 2021) and Zoe (Bhat et al. 2023), respectively. The initial learning rate is set at 10-5 and is progressively reduced to 10-6 until the model converges. All experiments are performed on a single RTX 3090 GPU."}, {"title": "Comparing with SfM-Free Methods", "content": "In this subsection, we compare our method with several baselines including CF-3DGS (Fu et al. 2024), Nope-NeRF (Bian et al. 2023), BARF (Lin et al. 2021) and NeRFmm (Wang et al. 2021) on both novel view synthesis and camera pose estimation.\nIn contrast to conventional approaches where camera poses for testing views are provided, we need to additionally ascertain the camera poses of test views. We utilize the same protocol as outlined in CF-3DGS to optimize the camera poses for these testing views. This identical procedure is applied across all baseline models to maintain a consistent basis for comparison.\nWe present the comparative analysis on the Tanks and Temples dataset in Table 1. Our approach consistently surpasses competing methods across all evaluated metrics. Remarkably, our direct training strategy achieves superior PSNR values even compared to CF-3DGS, which leverages carefully designed progressive 3D Gaussians training strategy, with a notable increase of 3.5 points in the Family scene.\nQualitative results are shown in Fig. 3. The images generated using our method are distinctly sharper and could retain small objects within the scene, such as the walking person in the first scene shown in Fig. 3, which correlates with the significantly improved scores for SSIM and LPIPS, as detailed in Table 1."}, {"title": "Camera Pose Estimation", "content": "The learned camera poses are post-processed by Procrustes analysis as in (Lin et al. 2021; Bian et al. 2023) and compared with the ground-truth poses of training views. The quantitative results of camera pose estimation are summarized in Table 2. Our approach achieves comparable performance with the current state-of-the-art results. We hypothesize that the relatively poorer performance in terms of RPE, may be attributed to relying solely on photometric loss for relative pose estimation. In contrast, Nope-NeRF incorporates additional constraints on relative poses beyond photometric loss, including the chamfer distance between two point clouds. As indicated in (Bian et al. 2023), omitting the point cloud loss leads to a significant decrease in pose accuracy."}, {"title": "Performance in Complex Camera Motions", "content": "While the camera motions involved in the Tanks and Temples dataset are relatively minor, we extend the validation of our method's robustness to the CO3D videos, which feature more intricate and demanding camera movements.\nAs shown in Table 3 and Table 4, our approach not only excels in novel view synthesis but also clearly surpasses CF-3DGS in pose estimation, reinforcing the findings from the Tanks and Temples experiments and underscoring the precision and robustness of our proposed method in scenarios characterized by complex camera motions."}, {"title": "Ablation Study", "content": "We assess the impact of correspondence-guided optimization by substituting it for traditional pixel-wise supervision. Performance metrics for novel view synthesis and camera pose estimation with and without correspondence-guided optimization are detailed in Table 5. Our observations confirm that correspondence plays a crucial role in enhancing both novel view synthesis and pose estimation accuracy. In the absence of correspondence, inaccurate initial poses lead to significant deviations between the screen space coordinates of objects in the rendered images and those in the GT images, resulting in poor gradients quality and unstable optimization of the 3D Gaussians model."}, {"title": "Comparison with 3DGS with SfM Poses", "content": "Our analysis extends to comparing the quality of novel view synthesis of our method with that of the conventional 3DGS model (Kerbl et al. 2023), which utilizes poses derived from SfM technique on the Tanks and Temples dataset. As shown in Table 6, our integrated optimization framework delivers performance on par with the 3DGS model that incorporates SfM-derived poses. In scenes where SfM pose estimation is challenging, there is a significant improvement in performance, as observed in the Horse scene."}, {"title": "Conclusion", "content": "We introduce a novel correspondence-guided SfM-free 3D Gaussian splatting for NVS method that enhances novel-view synthesis by avoiding SfM pre-processing. Our approach effectively optimizes relative poses between frames through correspondence estimation and achieves a differentiable pipeline using our proposed approximated surface rendering technique. Experimental results confirm the superiority of our method in terms of quality and efficiency."}]}