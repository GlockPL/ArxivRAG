{"title": "PEAR: POSITION-EMBEDDING-AGNOSTIC ATTENTION RE-WEIGHTING ENHANCES RETRIEVAL-AUGMENTED GENERATION WITH ZERO INFERENCE OVERHEAD", "authors": ["Tao Tan", "Yining Qian", "Ang Lv", "Hongzhan Lin", "Songhao Wu", "Yongbo Wang", "Feng Wang", "Jingtong Wu", "Xin Lu", "Rui Yan"], "abstract": "Large language models (LLMs) enhanced with retrieval-augmented generation (RAG) have introduced a new paradigm for web search. However, the limited context awareness of LLMs degrades their performance on RAG tasks. Existing methods to enhance context awareness are often inefficient, incurring time or memory overhead during inference, and many are tailored to specific position embeddings. In this paper, we propose Position-Embedding-Agnostic attention Re-weighting (PEAR), which enhances the context awareness of LLMs with zero inference overhead. Specifically, on a proxy task focused on context copying, we first detect heads which suppress the models' context awareness, thereby diminishing RAG performance. To weaken the impact of these heads, we re-weight their outputs with learnable coefficients. The LLM (with frozen parameters) is optimized by adjusting these coefficients to minimize loss on the proxy task. As a result, the coefficients are optimized to values less than one, thereby reducing their tendency to suppress RAG performance. During inference, the optimized coefficients are fixed to re-weight these heads, regardless of the specific task at hand. Our proposed PEAR offers two major advantages over previous approaches: (1) It introduces zero additional inference overhead in terms of memory usage or inference time, while outperforming competitive baselines in accuracy and efficiency across various RAG tasks. (2) It is independent of position embedding algorithms, ensuring broader applicability.", "sections": [{"title": "1 INTRODUCTION", "content": "Retrieval-augmented generation (RAG, (Lewis et al., 2021)) is widely utilized to enhance large language models (LLMs) on tasks like question answering. Typically, an RAG framework retrieves documents related to users' question from external knowledge bases or web pages, and then arranges them in the LLMs' context as the references to form answers. This LLM-based question-answering paradigm has given rise to a promising web search paradigm (Microsoft, 2023; OpenAI, 2024).\nRecent research demonstrated LLMs' limitations on context awareness, especially when processing long context. These limitations in LLMs' context awareness challenge the effectiveness and robustness of RAG frameworks. For instance, Liu et al. (2023) found that when performing in-context retrieval tasks, LLMs exhibit insensitivity to information located in the middle of the context, a phenomenon referred to as \u201clost in the middle.\" Chen et al. (2024) identified a mathematical property in rotary position embedding (RoPE, (Su et al., 2023)) that results in LLMs assigning less attention to specific contextual positions, leading to varying context awareness throughout the entire context."}, {"title": "2 RELATED WORKS", "content": "In this section, we discuss two research areas closely related to this paper: enhancements to LLMs' context awareness and studies on mechanistic interpretability."}, {"title": "2.1 CONTEXT AWARENESS ENHANCEMENT", "content": "Many studies highlighted limitations in LLMs' context awareness. For example, Lu et al. (2022) found that the order of in-context learning (ICL) demonstrations significantly affects ICL accuracy. Liu et al. (2023) demonstrated that LLMs exhibit stronger awareness of content at the beginning and end of context but weaker awareness in the middle, a phenomenon termed \u201clost in the middle.\" Chen et al. (2024) proposed that LLMs' context awareness fluctuates across token positions due to mathematical properties in position embeddings. These challenges impact applications like RAG that rely on robust context awareness.\nSeveral approaches have been proposed to tackle these issues. However, existing methods often come at the cost of increased inference time or memory overhead. Attention Buckets (AB, (Chen et al., 2024)) enhances context awareness by integrating positional information from a set of various ROPE angles, but it incurs significant inference overhead due to multiple parallel forward passes, leading to increased memory usage. Ms-PoE (Zhang et al., 2024) calculates distinct re-scaling factors for each attention head, requiring multiple non-parallel forward passes that introduce noticeable time delays. MoICE (Lin et al., 2024) builds on Attention Buckets by employing a Mixture-of-Experts (MoE, (Shazeer et al., 2017)) approach, treating each RoPE embedding as a unique in-context expert, thereby limiting extra attention computations to within each layer rather than across the entire forward pass.\nOur proposed method, PEAR, enhances context awareness by weakening the RAG-suppression heads during the forward pass. It introduces no additional modules or extra forward passes, resulting in zero additional overhead in memory usage and inference time. Additionally, PEAR operates independently of position embedding algorithms, which is applicable to more LLMs compared to existing approaches."}, {"title": "2.2 MECHANISTIC INTERPRETABILITY", "content": "Investigating the role of a specific head during a forward pass is one of key focuses in mechanistic interpretability research. Wang et al. (2023) reported that in GPT-2 small (Radford et al., 2019), the 7th head in the 10th layer, termed the \u201cnegative head,\u201d significantly hinders answer copying from context. McDougall et al. (2023) comprehensively studied this head, suggesting it functions as a self-repair mechanism to prevent overconfident outputs. Lv et al. (2024a) found that negative heads exist across various LLMs, employing different mechanisms to mitigate overconfidence, such as generating counteracting vectors or introducing high-frequency tokens' information. This paper does not examine what specific mechanisms the heads employ to suppress RAG performance but instead aims to discover and suppress heads negatively impactful across general RAG tasks.\nYu et al. (2023) detected two types of heads in Transformer-based language models during counter-factual task execution (where counter-factual knowledge is provided in the context): memory heads,"}, {"title": "3 PRELIMINARIES: DISCOVERY OF INFLUENTIAL ATTENTION HEADS", "content": "For a particular task, research has shown that only a sparse sub-network is activated during the forward pass in Transformer language models (Wang et al., 2023; Merullo et al., 2024; Gong et al., 2024). Such a sub-network is referred to as a circuit (Olah et al., 2020). Discovering circuits provides interpretability into the working mechanisms of language models and offers insights for model enhancement.\nThe primary method for circuit discovery is based on causal mediation analysis. The core idea is to view the forward computation graph as a causal graph, where the output of one module serves as the input for the next. In such a case, if the output of a module is changed, the computation of subsequent modules in the causal graph is also affected, as their inputs change.\nIn this paper, we primarily focus on analyzing the working mechanism of attention heads in language models. We briefly introduce a paradigm from a series of works (Wang et al., 2023; Zhang & Nanda, 2024; Wang et al., 2024) that discovers which attention heads are crucial for processing an input sequence $X$ of length $n$. Suppose the language model consists of $L$ layers, with $H$ attention heads per layer. Let $A^{(l,h)}$ denote the h-th attention head in the l-th layer, and let its outputs be denoted by $a^{(l,h)} \\in \\mathbb{R}^{n\\times d}$. We use $a^{(l,h)}_i \\in \\mathbb{R}^{d}$, where $1 \\le i \\le n$, to represent the output at position i. The discovery paradigm typically includes three steps, as illustrated in Figure 1:\n1. In the normal run, with an input sequence $X$ (e.g., $X$ =\u201cThe capital of France is\u201d), $a^{(l,h)}$ for every attention head are recorded.\n2. In the perturbed run, the forward computation runs using the same input sequence $X$, but with some mediation. This mediation either changes the discrete input tokens within $X$ by substituting specific keywords (e.g., replacing \u201cFrance\u201d with \u201cEngland\u201d), or corrupts the hidden states by adding noise. The modified $\\tilde{a}^{(l,h)}$ for each attention head are then recorded.\n3. We conduct an intervention on a particular head $A^{(l,h)}$ at a specific position $i$ (e.g., the country token position in above examples) in the normal run by substituting its outputs with $\\bar{a}^{(l,h)}$. The subsequent activations in the computational graph are then recomputed (these reccomputed activations are denoted as $\\hat{a}$ in the figure). If the final output of the language model as the intervention expects (e.g., the predicted token changes from \"Paris\" to \"London\"), the head $A^{(l,h)}$ is considered to have a positive influence on the processing of sequence $X$.\nThis overview outlines a simplified discovery paradigm; detailed measurements of intervention impact are tailored to specific experimental needs."}, {"title": "4 \u041c\u0415\u0422\u041dODOLOGY", "content": "In this section, we provide a detailed introduction to our proposed method, PEAR, which is executed in two stages: (1) discovering RAG-suppression heads and (2) re-weighting coefficient learning. The first stage discovers attention heads that have a negative impact on general RAG tasks based on circuit discovery for a proxy task. In the second stage, we optimize learnable coefficients to re-weight the outputs of the discovered heads, aiming to mitigate their RAG-suppression effect. These coefficients remain fixed during inference, irrespective of the specific input. Figure 2 demonstrates the overview of PEAR."}, {"title": "4.1 DISCOVERY OF RAG-SUPPRESSION HEADS", "content": "We set up a proxy task and use this task as input for circuit discovery algorithms to discover influential attention heads that hamper LLMs' performance on general RAG tasks.\nTask Input For each input sample, we create a sequence of length $n$, denoted as ${x_1,...,x_n}$, where each $x_i$ is a randomly sampled token from the vocabulary. This sequence is repeated to form an input sample $X = {x_1,...,x_{2n}}$, with $X_i = X_{i+n}$ for $i \\in [1,n]$. Research has shown that, in semantically meaningless contexts, models tend to check if the last few tokens in the sequence appeared previously and copy the suffix of their last appearance as the output (Olsson et al., 2022; Lv et al., 2024b). We consider an arbitrary LLM to successfully perform the proxy task when, at position $n + i + 1$, the token with the highest output logits is $x_i$. Table 1 shows an example input.\nThis proxy task exhibits two key characteristics that facilitate the effective discovery of RAG-related heads:\n1. Completing the proxy task requires LLM capabilities essential for a robust RAG framework, such as in-context retrieval and generation based on context, making it suitable for discovering RAG-related attention heads.\n2. The random token composition in $X$ ensures semantically meaningless input, minimizing knowledge bias and thereby enabling the discovered attention heads to have general RAG-related functions, independent of specific downstream tasks.\nHead discovery We previously outlined the head discovery algorithm in Section 3. Here, we provide additional practical details for the first stage of PEAR.\n1. During the normal run, the input sequences $X$ are constructed as above described, with a length of $2n$.\n2. In the perturbed run, we do not modify the input or hidden states; instead, we average the outputs of each attention head along the sequence dimension and record the resulting mean vectors.\n3. We focus on detecting changes in logits at position $2n$, where the model is expected to copy the token from position $n - 1$. Consequently, we intervene at $a_{i}^{(l,h)}$ by replacing it with the saved mean vectors.\n4. Our intervention measurements are based on the logits difference, defined as:\n$$\\Delta \\pi_\\tau^{(l,h)} = \\frac{\\pi_{2n}^{(\\tau^{(l,h)})}[X_{n-1}]}{\\pi_{2n}[X_{n-1}]} - 1,$$\nwhere $\\pi_{2n}$ represents the final logits at position $2n$ during the normal run, and $[X_{n-1}]$ denotes selecting the value of the token $x_{n-1}$ from the logits. $\\pi_{2n}^{(\\tau^{(l,h)})}$ indicates the logits after intervention on $A^{(l,h)}$. We contend that a higher value of this metric suggests a stronger suppression effect from $A^{(l,h)}$.\n5. For an arbitrary LLM, we repeat the proxy task multiple times with varying values of n to mitigate bias in context length. The final metric score for each head is the average of the results from these repeated experiments. The detailed setup is provided in Section 5.1."}, {"title": "4.2 RE-WEIGHTING COEFFICIENT LEARNING", "content": "Optimization In standard multi-head attention mechanisms, the outputs of all attention heads are aggregated with equal weighting. We propose that re-weighting these relative aggregation weights to values less than 1 can mitigate the RAG-suppression effect from our discovered heads. To implement this, we modify the forward computation by multiplying the output of each head, $A^{(l,h)}$, in the set $S$ by a learnable scalar, $r^{(l,h)}$, referred to as the re-weighting coefficient. The modified output for each head is:\n$$a^{(l,h)} = r^{(l,h)} * a^{(l,h)}, \\text{ for each } A^{(l,h)} \\in S.$$\nTo optimize these re-weighting coefficients for RAG-suppression heads, we freeze the original parameters of the LLM and train only the re-weighting coefficients to minimize the loss on a proxy task. Importantly, the loss is calculated only over the latter half of the sequence, optimizing the coefficients to enhance in-context retrieval capacities rather than predicting the next token. Formally, our adopted loss can be written as:\n$$\\mathcal{L} = - \\sum_{i=n}^{2n-1} \\log p(x_{i+1} | x_{1:i}).$$\nFigure 2 illustrates a re-weighting process during optimization. Notably, the re-weighting process shown in this figure adds extra multiplication operations in a forward pass. In practice, when coefficient learning ends, we re-scale $W^{(l,h)}$ (the output projection matrix in head $A^{(l,h)}$) by $r^{(l,h)}$, which is equivalent to Eq. 2 and does not add any extra computation during inference.\nInference on Downstream Tasks We highlight several points regarding the inference process of our proposed PEAR on downstream RAG tasks:\n1. In downstream RAG tasks, the re-weighting coefficients are task-independent and remain fixed.\n2. RAG-suppression heads are optimized once for each LLM via the proxy task. For a new RAG task, head discovery and coefficient learning do not need to be repeated.\nIn theory, our approach, PEAR, introduces zero additional overhead during inference on downstream RAG tasks, as it does not incorporate extra computational modules; instead, it only adjusts the aggregation weights of specific heads. In practice, however, the re-weighting process involves an additional multiplication, which results in zero additional overhead in inference time and memory usage. Additionally, the learning of re-weighting coefficients is independent of the LLM architecture, thus making our method compatible with various position embedding algorithms."}, {"title": "5 EXPERIMENTS", "content": "5.1 SETUP\nIn this section, we introduce the LLMs we used for experiments, the baseline methods for enhancing context awareness, the setups for the proxy tasks, hyperparameters for learning re-weighting coefficients.\nModels and baselines We conducted experiments with three LLMs, each employing a different position embedding algorithm: Llama2-7B-chat-4k (Touvron et al., 2023) using RoPE (Su et al., 2023), OPT-6.7B-2k (Zhang et al., 2022) using learnable position embeddings, and Baichuan-13B-4k (Baichuan, 2023) using Alibi position embeddings (Press et al., 2022).\nWe also compared several competitive baseline methods for enhancing LLMs' context awareness, including Attention Buckets (AB, (Chen et al., 2024)), Ms-PoE (Zhang et al., 2024), and MOICE (Lin et al., 2024). Details on these methods can be found in Section 2.1.\nDetailed setups of proxy task For head discovery, we constructed 200 task samples. In the case of the Llama and OPT models, we repeat the discovery process four times with varying values of n: 10, 15, 25, 50. For the Baichuan model, the n values are 10, 20, 50, 80. We found that each model has a group of heads with significantly large $\\Delta \\pi$ values, leading us to select the K values based on the observed group sizes: 30, 22, and 21, respectively.\nFor the re-weighting coefficient learning stage, we constructed 500 task samples, setting n to 50 for all models.\nHyperparameters for re-weighting coefficient learning We employed the AdamW optimizer with a learning rate of 0.005 and parameters $(\\beta_1, \\beta_2) = (0.9, 0.999)$. $\\tau$ are initialized at 1.0. Training was performed for a single epoch using BF16 precision on an A100-PCIE-40GB GPU."}, {"title": "5.2 COMPARISON WITH BASELINES ON RAG TASKS", "content": "We compare PEAR against various baselines on RAG tasks we constructed using three datasets: 2WikiMultihopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), and Qasper (Dasigi et al., 2021). The first two datasets require the model to answer questions based on multiple documents, while the third focuses on questions related to NLP research papers, formulated and answered by NLP researchers. We truncate the context to 4,000 tokens for the first two datasets; the third dataset has an average context length of 3,619 tokens.\nOur experiments are conducted with Llama2-7B-chat-4k, as the baselines are tailored specifically for ROPE. We evaluate the models' performance using exact match scores. Notably, our method achieves the highest average improvement across all three tasks. Although PEAR does not achieve the top performance on the MuSiQue task, it outperforms the original model by a large margin.\nAdditionally, we present inference time and memory costs for these datasets. PEAR does not increase GPU memory usage and inference time costs. This makes it significantly more efficient than other enhancement methods.\nThese experiments underscore the effectiveness and efficiency of PEAR in enhancing LLMs for RAG tasks."}, {"title": "5.3 APPLICABILITY TO LLMS USING VARIOUS POSITION EMBEDDINGS", "content": "In this section, we demonstrate the applicability of PEAR to LLMs utilizing different position embeddings. We conduct a multi-document question-answering (MDQA) experiment based on data from (Liu et al., 2023). Following Liu et al. (2023); Chen et al. (2024), we position the gold document (i.e., the document contains the ground truth answer) at various contextual positions to evaluate the robustness of a context-awareness enhancement method. In our experiments, we set the maximum document count to 10 and assess the question-answering accuracy when the gold document is placed as the 1st, 3rd, 5th, 7th, and 10th document, respectively."}, {"title": "5.4 PEAR DOES NOT DIMINISH KNOWLEDGE CAPABILITIES IN LLMS", "content": "Previous research (Geva et al., 2023; Lv et al., 2024a) has shown that certain attention heads store or play a crucial role in eliciting parametric knowledge. This raises the question of whether PEAR"}, {"title": "5.5 ANALYSIS: THE EFFECT OF K", "content": "While we have demonstrated the effectiveness of PEAR from various angles, a key point for discussion is the role of K, representing the number of heads to re-weight. Using Llama2-7B-chat as a case study, we vary K and observe its impact on PEAR's performance. The findings indicate that PEAR performs optimally when K matches the inherent threshold of the model, i.e., the number of heads with a significantly higher $\\Delta \\pi$ than others. Re-weighting fewer heads fails to fully alleviate the suppression from RAG-suppression heads, while exceeding this optimal number can harm the performance of non-RAG-suppression heads, ultimately diminishing overall effectiveness."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce PEAR, a position-embedding-agnostic method designed to enhance the performance of LLMs on RAG tasks with zero inference overhead. Our method not only outperforms competitive baselines in both effectiveness and efficiency but also demonstrates broad applicability across various LLMs. We also presented that PEAR improves context awareness in LLMs without compromising their inherent knowledge capabilities. These benefits make PEAR a promising approach for a wide range of applications that require robust context abilities, such as in-context learning and strict instruction following, which we leave for future research."}, {"title": "A HEAD DISCOVERY RESULTS", "content": ""}]}