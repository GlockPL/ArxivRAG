{"title": "PRMBENCH: A Fine-grained and Challenging Benchmark for Process-Level Reward Models", "authors": ["Mingyang Song", "Zhaochen Su", "Xiaoye Qu", "Jiawei Zhou", "Yu Cheng"], "abstract": "Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBENCH, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBENCH comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBENCH be a robust bench for advancing research on PRM evaluation and development.", "sections": [{"title": "1 Introduction", "content": "Recent large language models (LLMs) (OpenAI, 2024a,b; Team, 2024a), trained on large-scale reinforcement learning, has significantly improved their performance in complex reasoning tasks such as mathematics and code generation (Su et al., 2022; Yu et al., 2023; Guo et al., 2024; DeepMind, 2024; Luo et al., 2023; Lu et al., 2024a,b). A key factor behind their successes is the use of process reward models (PRMs) (Wang et al., 2023; Lightman et al., 2023; Uesato et al., 2022), which evaluate the correctness and usefulness of intermediate reasoning steps to refine the models' thinking process (Qin et al., 2024; Zhang et al., 2024b).\nDespite recent advancements, PRMs remain fallible and prone to inaccuracies (Zheng et al., 2024). As illustrated in Figure 1, given a misleading question Q, the ol model fails to recognize the deception and generates a pseudo-proof for the question. The red text highlights an erroneous reasoning step where the Mean Value Theorem (MVT) is incorrectly applied in 3-D contexts. Notably, although the yellow text indicates that the model recognizes the inconsistency between 1-D MVT and 3-D scenarios, it continues to make errors. Under this circumstance, both Skywork-7B (01 Team, 2024) and Math-Shepherd-7B (Wang et al., 2023) fail to identify the incorrect steps, indicating the unreliability of current PRMs. Unfortunately, current PRM evaluation benchmarks typically rely on step-level reasoning data annotated with binary labels (e.g., correct, or incorrect) (Zheng et al., 2024;"}, {"title": "2 Related Work", "content": "2.1 Process-level Reward Models\nProcess-level reward models (PRMs) have been shown to have advancement over traditional outcome-level reward models (ORMs) in training models' process-level reasoning accuracy and improving their long-process reasoning abilities (Lightman et al., 2023; Uesato et al., 2022). More and more PRMs have been proposed for use in process-level RLHF (Wang et al., 2023; Xia et al., 2024; 01 Team, 2024). Lightman et al. (2023) released a large amount of labeled data at the human-annotated process level, providing great research opportunities for multi-step reasoning. Wang et al. (2023) introduces a self-supervised automatic data generation and PRM training pipeline that can automatically generate the process-level label. Xia et al. (2024) utilize PRM as an auto evaluator to assess"}, {"title": "3 PRMBENCH", "content": "3.1 Evaluation Subjects\nIn this section, we provide a detailed introduction to the evaluation subjects of PRMBENCH, which is organized into three main domains:\nSimplicity primarily evaluates the redundancy detection capabilities of PRMs. It is important since the redundancy within the reasoning steps would cause unnecessary computing costs and reduce efficiency, although it does not harm the correctness (Qin et al., 2024; Xia et al., 2024). Furthermore, a reasoning process can offer a clearer visualization of the core of the problem and enhance the overall understandability of the reasoning (Su et al., 2024a).\nSoundness is one of the key capabilities of PRMs, as it evaluates their reward accuracy. As discussed in Section 1, the status of erroneous steps is fine-grained, varying in both causes and forms of expression (Li et al., 2024b). Therefore, we not only assess the correctness of the rewards but also evaluate fine-grained performance, accounting for the various types of errors and their nuances in the reasoning process.\nSensitivity assesses robustness or sensitivity to details within one question of PRMs, for instance, essential conditions or implicit requirements. We emphasize the importance of Sensitivity as it relates to the completeness of logic and resistance to misleading information, which contributes to the overall robustness of PRMs.\nEach domain is further divided into detailed sub-categories for a more granular evaluation, which is discussed in detail below. The overall structure of PRMBENCH along with representative examples of each sub-category are illustrated in Figure 2, and the detailed instances of every sub-category are shown in Appendix C.", "subsections": [{"title": "3.1.1 Simplicity", "content": "Specifically, the Simplicity evaluation subject is divided into two sub-categories: Non-Redundancy and Non-Circular Logic, with detailed descriptions provided below:\nNon-Redundancy requires PRM to detect the implicit redundancy within the reasoning procedure. The redundancy situation refers to a process that is not the most concise or efficient, as it includes one or more redundant steps that can be removed without affecting the correctness of the overall solution path. For example, as shown in Figure 2, if A \u2192 B represents a correct inference chain, the redundant reasoning procedure can be displayed as A \u2192 C \u2192 B. where C represents one or more redundant steps C = {c|c is redundant}.\nNon-Circular Logic In this sub-category, PRMs are required to detect the implicit circular logic within the reasoning process. Circular logic is a specific form of redundancy, distinct from general redundancy, in that it finally loops back to a previous reasoning step. For example, as shown in Figure 2, if A \u2192 B represents a correct inference chain, circular logic can be formulated as A\u2192C\u2192A\u2192 B, where the reasoning starts at step A, progresses through a sequence of steps, and ultimately loops back to A. We list Non-Circular Logic separately due to its common occurrence in reasoning processes."}, {"title": "3.1.2 Soundness", "content": "We divide the Soundness category into four sub-categories due to its complexity: Empirically Soundness, Step Consistency, Domain Consistency, and Confidence Invariance. The definition of each sub-category is discussed below.\nEmpirically Soundness demands PRM to detect the implicit counterfactual mistakes within the reasoning process. A counterfactual step refers to a statement within a reasoning chain that contradicts established ground truth G. Such contradictions can arise from relying on outdated theories, omitting critical constraints in theory, or incorporating erroneous assumptions.\nStep Consistency expects PRM to detect the implicit step-wise contradiction, which means a conflict between a specific step and other steps within a reasoning path. Given a reasoning path P = {S1, S2, . . ., Sn}, a step contradiction exists if Si | Sj, where i, j \u2208 [1, n] and i \u2260 j."}, {"title": "3.1.3 Sensitivity", "content": "This category includes three sub-categories: Prerequisite Sensitivity, Deception Resistance, and Multi-Solution Consistency, with detailed descriptions provided below.\nPrerequisite Sensitivity requires the PRM to maintain sensitivity to missing conditions or prerequisite mistakes, which means a flaw in the reasoning chain where critical premises, assumptions, or necessary conditions are absent. This omission results in logical gaps, incomplete reasoning, or biased conclusions. For example, when a missing condition occurs, the model is required to solve the problem through case analysis or further investigation. However, the answer becomes incorrect if the model overlooks the missing condition and proceeds with standard reasoning methods.\nDeception Resistancy demands the PRM to detect the implicit deception or trap within a reasoning process, that is, statements that appear to be correct or align with ground truth but are subtly altered to introduce inaccuracies while maintaining the illusion of correctness.\nMulti-Solution Consistency expects the PRM to maintain consistency when faced with different solution paths of the same problem. Concretely, to evaluate the sensitivity and the generalizability of PRMs, we utilize multiple correct reasoning processes of the same question to test whether the PRM can perform correctly."}]}, {"title": "3.2 Data Curation", "content": "We curate the dataset by extracting metadata and constructing test cases according to our category definitions. Detailed statistics of PRMBENCH are displayed in Table 2, with the curation procedure outlined below.\nMeta Data Extraction Our metadata instance can be represented as (Q, A, S). Here, Q denotes a question, A represents the ground truth answer to the question, S corresponds to a completely correct step-level solution process.\nOur metadata is built upon PRM800K (Lightman et al., 2023), which provides the questions (Q), ground truth answers (A), and ground truth step-level solution processes (S). We select completely correct solutions from both the training and test sets, filtering out low-quality instances to establish our ground truth answers.\nTest Case Construction We subsequently construct our test cases using the pre-extracted metadata. Each test case instance is represented as (Q', A, S'), where Q' denotes the test question and S' represents the test solution process, which may include errors. During the testing phase, models are tasked with evaluating S' and providing judgments for each step in the process.\nWith the first eight class-specific prompts, as demonstrated in Appendix D.1 and our supplementary material, we query one of the most advanced LLMS, GPT-40 (OpenAI, 2024a), to modify the ground-truth reasoning process into versions containing erroneous steps. Subsequently, these modifications are manually reviewed to filter out unreasonable or unqualified changes. This procedure allows us to construct all test cases, except for the multi-solution cases. For the multi-solution subject, we leverage the newly proposed multi-step reasoning model QwQ\u2020 (Team, 2024a) to generate candidate answers for the given questions. These answers are then filtered to exclude unreasonable or incorrect ones, resulting in multi-solution reasoning processes for a single question.", "subsections": []}, {"title": "3.3 Quality Control", "content": "To ensure a high-quality dataset, we implement a series of steps to filter out unqualified data and maintain data integrity. The specific procedures are outlined below:\nFeature Filtering Even with detailed instructions, LLMs cannot consistently generate outputs that fully adhere to the required structure (Asai et al., 2024; Zeng et al., 2024a; Su et al., 2024b)."}, {"title": "4 Experiments", "content": "4.1 Models\nTo provide a comprehensive evaluation of various models on PRMBENCH, we select a diverse set of models, ranging from open-source PRMS to proprietary LLMs configured as critic models. Specifically, the open-source PRMs include small models like Skywork-PRM-1.5B (01 Team, 2024), mid-sized models such as Llemma-PRM-7B (Team, 2024b), MATHMinos-PRM (Gao et al., 2024), MathShepherd-PRM (Wang et al., 2023), and RLHFlow PRMs (Xiong et al., 2024), as well"}, {"title": "5 Further Analysis", "content": "5.1 Similarity analysis\nTakeaway 1. PRMs show a clear bias during evaluation, often favoring positive rewards.\nAs shown in Table 3, most open-source PRMs exhibit significant bias in our test cases, with some models performing worse than random guessing. This suggests the potential presence of bias within the inference procedure for our test cases. To validate this assumption, we selected a subset of models and evaluated their accuracy on positive and negative steps separately. The results are displayed in Table 4. Surprisingly, some models display a clear bias during evaluation, often favoring positive rewards. Additionally, certain models, such as Skywork-7B, exhibit a tendency to assign negative labels to steps. Although proprietary LLMs also exhibit bias, they outperform open-source PRMs, with a comparatively milder reward tendency.\nAdditionally, to further investigate inference bias, we evaluate the reward similarity between completely correct reasoning processes and our test cases. Specifically, we select completely correct reasoning procedures for each question in our test cases and evaluate the solution-level performance similarity between these and the test cases. The solution-level similarity is defined as S = 100 * (Accpos - Accneg), where Acc denotes solution-level accuracy, calculated as the average step accuracy within a solution. We then compute the overall average similarity across the whole dataset. The results, shown in Table 4, reveal that certain models, such as ReasonEval-7B and RLHFlow-DeepSeek-8B, exhibit significantly higher similarity than the normal similarity score (79.4), showcasing potential limitations in differentiating positive and negative steps.", "subsections": []}, {"title": "5.2 Performance across different step positions", "content": "Takeaway 2. PRMs show a gradual improvement in performance as the step position increases.\nTo comprehensively evaluate the error-detection performance of PRMs, PRMBENCH includes a wide range of error step positions. The distribution of error positions is illustrated in Figure 3. While differences exist across categories, the overall pattern remains consistent: all categories peak in frequency at step 5 and gradually decrease thereafter. This raises an interesting question: Does the variation in step positions affect model performance? To investigate, we focus on error steps to assess how erroneous step positions influence model accuracy. The evaluation results are illustrated in Figure 4, featuring four representative models: two PRMs and two proprietary LLMs prompted as critic models. As depicted in Figure 4, proprietary LLMs maintain stable performance across different error step positions. In contrast, PRMs, including Math-Shepherd-7B and ReasonEval-7B, show a gradual improvement in performance as error step positions increase. Notably, in certain error steps, the open-source PRMS outperform proprietary LLMs, potentially due to inference bias, as discussed in Section 5.1. Despite this occasional advantage, the full accuracy and PRMScore of open-source PRMs still lag behind those of proprietary LLMs."}, {"title": "5.3 Error Analysis", "content": "A representative test case and the corresponding model performances are presented in Table 5. This example involves a counterfactual reasoning process, where steps eight through thirteen contain information that contradicts the correct computational principles and should be classified as \u201cnegative\u201d. However, most models fail to identify these erroneous reasoning steps and assign relatively positive rewards, except for GPT-40. While GPT-40"}, {"title": "5.4 Impacts of ICL settings", "content": "Takeaway 3. In-Context Learning has subtle impact on models' performance on PRMBENCH\nIn the main experiments of PRMBENCH, we utilize 2-shot ICL to evaluate the performance of close-sourced models. In this section, we investigate the impact of ICL few-shot numbers on model performance on PRMBENCH. We vary the number of ICL few-shots to 0, 1, and 2 to examine whether increasing the few-shot number enhances the performance of generative models prompted as critic models. The final PRMScore of GPT-4o, Gemini-2-flash, and Gemini-2-thinking is presented in Table 6. For the Gemini-series models, a subtle improvement in performance is observed with a few-shot setup. However, for GPT-4o, no significant improvement is detected, and in some cases, a larger few-shot number even results in a decline in performance. These findings suggest that a few-shot approach exerts only a subtle impact on model performance on PRMBENCH."}, {"title": "6 Conclusion", "content": "In this paper, we investigate a crucial question: Can existing PRMs detect various types of erroneous reasoning steps and provide reasonable rewards? To address this, we introduce PRMBENCH, a benchmark characterized by its fine-grained evaluation subjects and challenging requirements. We carefully curate 6,216 data samples with 83,456 step-level labels through LLMs and human filtering. PRMBENCH can be used to evaluate different process-labeling models, ensuring its general applicability. Through a comprehensive evaluation of existing PRMs and generative LLMs prompted as critic models, we can observe that PRMs exhibit partial capability in multi-step process evaluation, showcasing significant room for improvement. Furthermore, we highlight the critical need for detecting detailed error types and conducting comprehensive evaluations of PRMs."}, {"title": "7", "content": "Despite these advances, enhancing the reward accuracy of PRMs and improving models' reasoning abilities remain open research challenges. We encourage future work to leverage and expand upon PRMBENCH to address these issues."}, {"title": "A.1 Evaluation Subjects", "content": "In this section, We provide detailed information on our evaluation subjects. The hierarchical categories, corresponding descriptions, and illustrations are shown in Figure 5. We have 6,216 data samples and 83,456 step-level labels. The benchmark spreads across three main evaluation subjects: simplicity, soundness, and sensitivity. Among them, Simplicity comprises two sub-categories: Non-redundant and Non-Circular Logic. Soundness includes four main sub-categories: Empirically Sound, Step Consistency, Domain Consistency, and Confidence Invariant. Finally, Sensitivity mainly evaluates models in three main parts: Prerequisite Sensitivity, Deception Resistance, and Multi-solution Consistency. The descriptions of each sub-category are shown in Figure 5."}, {"title": "B.1 Abbreviation Of Sub-Categories", "content": "The full names of abbreviations used in our tables are shown in Table 7."}, {"title": "3.4", "content": "Step 3: The quadratic formula is $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$\nStep 4: Yes, that's correct. In this case a, b, and c are 1, -1, and -1, respectively.\nStep 5: So if we plug those values in, we get $x = \\frac{1\\pm \\sqrt{1^2-4*1*-1}}{2*1}$\nStep 6: That simplifies to x = $\\frac{1\\pm \\sqrt{5}}{2}$. So x can be either $\\frac{1+\\sqrt{5}}{2}$ or $\\frac{1-\\sqrt{5}}{2}$\nStep 7: Now let's move to the second equation.\nStep 8: Yes, we need to find $x^3$ \u2013 2x + 1\nStep 9: Right. Well since we now know that $x^2$ \u2212 x \u2212 1 = 0, we can replace $x^2$ in the second equation with x + 1.\nStep 10: That means that $x^3$ 2x + 1 = x(x + 1) \u2013 2x + 1\nStep 11: And if we substitute in the possible values of x that we found earlier, we get either $\\frac{1+\\sqrt{5}}{2}$ ($\\frac{1+\\sqrt{5}}{2}$+ 1) \u2013 2$\\frac{1+\\sqrt{5}}{2}$ + 1 or $\\frac{1-\\sqrt{5}}{2}$ ($\\frac{1-\\sqrt{5}}{2}$+ 1) \u2013 2$\\frac{1-\\sqrt{5}}{2}$ + 1.\nStep 12: Let's start with the first case: $\\frac{1+\\sqrt{5}}{2}$($\\frac{1+\\sqrt{5}}{2}$+1) \u2013 2$\\frac{1+\\sqrt{5}}{2}$ + 1.\nStep 13: There's a shorter way. We have $x^3$ \u2013 2x + 1 = x(x + 1) \u2013 2x + 1.\nStep 14: $x^3$ \u2013 2x + 1 = ($x^2$ + x) \u2013 2x + 1.\nStep 15: And we know that $x^2$ x \u2212 1 = 0, so $x^2$ = x + 1.\nStep 16: So we can just substitute x + 1 for $x^2$.\nStep 17: So $x^3$ - 2x + 1 = ($x^2$ + x) \u2013 2x + 1 becomes $x^3$ \u2013 2x + 1 = (x + 1 + x) \u2013 2x + 1.\nStep 18: And that simplifies to $x^3$ \u2013 2x + 1 = (2x + 1) \u2013 2x + 1\nStep 19: So $x^3$ \u2013 2x + 1 = 1 + 2.\nStep 20: So, we don't need to do the second case because we already have the answer.\nStep 21: Right. So if $x^2$ X 1 = 0 then $x^3$ \u2212 2x + 1 = 2."}, {"title": "3.5", "content": "Step 3: The quadratic formula is $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$\nStep 4: Yes, that's correct. In this case a, b, and c are 1, -1, and -1, respectively.\nStep 5: So if we plug those values in, we get $x = \\frac{1\\pm \\sqrt{1^2-4*1*-1}}{2*1}$\nStep 6: That simplifies to x = $\\frac{1\\pm \\sqrt{5}}{2}$. So x can be either $\\frac{1+\\sqrt{5}}{2}$ or $\\frac{1-\\sqrt{5}}{2}$\nStep 7: Now let's move to the second equation.\nStep 8: Assume $x^3$ \u2013 2x + 1 = 2 for verification.\nStep 9: Substituting $x^3$ \u2013 2x + 1 = 2 into the equation, it implies x($x^2$) \u2013 2x + 1 = 2\nStep 10: Since $x^2$ = x + 1, x(x + 1) \u2212 2x + 1 = 2.\nStep 11: Simplifying confirms $x^3$ - 2x + 1 = 2, verifying the assumption.\nStep 12: Therefore, $x^3$ \u2013 2x + 1 = 2 is correct."}, {"title": "3.6", "content": "Step 3: The quadratic formula is $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$\nStep 4: Substituting a = 1, b = -1, and c = -1, we get $x = \\frac{1\\pm \\sqrt{5}}{2}$\nStep 5: Since $x^2$ X 1 = 0, we replace x\u00b2 with x + 1 in x\u00b3 2x + 1.\nStep 6: Substituting, x\u00b3 \u2013 2x + 1 = x(x + 1) \u2212 2x + 1.\nStep 7: Simplify: x\u00b3 \u2013 2x + 1 = ($x^2$ + x) \u2013 2x + 1.\nStep 8: Incorrectly substitute x2 = x \u2212 1 instead of x + 1: x\u00b3 \u2013 2x + 1 = (x \u2212 1 + x) \u2013 2x + 1.\nStep 9: Simplify: x\u00b3 \u2013 2x + 1 = 0."}, {"title": "3.7", "content": "Step 3: The quadratic formula is $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$\nStep 4: Substituting a = 1, b = \u22121, and c = -1, we get $x = \\frac{1\\pm \\sqrt{5}}{2}$\nStep 5: Since $x^2$ X 1 = 0, we replace x\u00b2 with x + 1 in x\u00b3 2x + 1.\nStep 6: Substituting, x\u00b3 \u2013 2x + 1 = x(x + 1) \u2212 2x + 1.\nStep 7: Simplify: x\u00b3 \u2013 2x + 1 = ($x^2$ + x) \u2013 2x + 1.\nStep 8: Substitute x2 = x + 1: x3 \u2212 2x + 1 = (x + 1 + x) \u2013 2x + 1.\nStep 9: Simplify: x\u00b3 2x + 1 = 2."}, {"title": "3.8", "content": "Step 3: The quadratic formula is $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$\nStep 4: Substituting a = 1, b = -1, and c = -1, we get $x = \\frac{1\\pm \\sqrt{5}}{2}$\nStep 5: Since $x^2$ - x \u2212 1 = 0, we replace x\u00b2 with x + 1 in x\u00b3 \u2013 2x + 1.\nStep 6: Substituting, x\u00b3 \u2013 2x + 1 = x(x + 1) \u2212 2x + 1.\nStep 7: To simplify $x^3$, use the property of cubic roots: $x^3 = \\sqrt{x}$.\nStep 8: Substitute $x^3 = \\sqrt{x}$ and $x^2$ = x + 1 into $x^3$ \u2013 2x + 1: $x^3$ \u2212 2x + 1 = $\\sqrt{x}$ - 2x + 1\nStep 9: Simplify further: $x^3$ \u2212 2x + 1 = $\\frac{3}{2}\\sqrt{x}$ \u2212 x + 1"}, {"title": "3.9", "content": "Step 1: Let's call the number of dollars x. So, the problem is asking us to solve the equation x=1000000*$\\frac{1.50}{1200}$.\nStep 2: There are two ways to solve this equation. We could either divide 1000000 by 1200 and then multiply the result by 1.50 or we could divide 1.50 by 1200 and then multiply the result by 1000000.\nStep 3: I think the second way will be easier. $\\frac{1.50}{1200}$= 0.00125.\nStep 4: That's true. Now we just have to solve the equation x=1000000*0.00125.\nStep 5: So x=$1250$.\nStep 6: That's the final answer.\nStep 7: Actually, after a closer look, x = 0.00125 \u00d7 1000000 should be 12500. I must have miscalculated earlier.\nStep 8: So 1,000,000 lire is equivalent to $12500."}, {"title": "4.0", "content": "Step 1: Start with the equation |x \u2212 3| = 5.\nStep 2: Recognize that the absolute value equation |a| = b implies two cases: a = b or a = \u2212b\nStep 3: Apply this rule to |x \u2212 3| = 5, resulting in two equations: x 35 or x \u2212 3 = -5.\nStep 4: Solve each equation: x = 8 from x 3=5 and x = -2 from x 3 = -5.\nStep 5: The solutions are x = 8 and x = \u22122."}, {"title": "4.1", "content": "Step 1: $(\u221ax - 3)^2 = 5$ can be represented as a special form of |x \u2212 3| = 5.\nStep 2: Start with the equation |x \u2212 3| = 5.\nStep 3: Recognize that the absolute value equation |a| = b implies two cases: a = b or a = \u2212b\nStep 4: Apply this rule to |x-3| = 5, resulting in two equations: x \u2212 3 = 5 or x \u2212 3 = -5\nStep 5: Solve each equation: x = 8 from x \u2212 3 = 5 and x = -2 from x 3 = -5\nStep 6: The solutions are x = 8 and x = \u22122."}, {"title": "4.2", "content": "Step 1: Let's call the number of dollars x. So, the problem is asking us to solve the equation x=1000000*$\\frac{1.50}{1200}$.\nStep 2: There are two ways to solve this equation. We could either divide 1000000 by 1200 and then multiply the result by 1.50 or we could divide 1.50 by 1200 and then multiply the result by 1000000.\nStep 3: I think the second way will be easier. $\\frac{1.50}{1200}$ = 0.00125.\nStep 4: That's true. Now we just have to solve the equation x = 1000000 * 0.00125.\nStep 5: So x=1250.\nStep 6: That's the final answer.\nStep 7: Right. So 1,000,000 lire is equivalent to $1250."}, {"title": "4.3", "content": "Step 1: Let's start with the first equation and see if we can solve for x.\nStep 2: We can use the quadratic formula to find x.\nStep 3: The quadratic formula is x = $\\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$\nStep 4: Substituting a = 1, b = \u22121, and c = \u22121, we get x = $\\frac{1 \\pm \\sqrt{5}}{2}$\nStep 5: Since $x^2$ x \u2212 1 = 0, we replace $x^2$ with x + 1 in x\u00b3 \u2013 2x + 1.\nStep 6: Substituting, x\u00b3 \u2013 2x + 1 = x(x + 1) \u2212 2x + 1.\nStep 7: Simplify: x\u00b3 \u2013 2x + 1 = ($x^2$ + x) \u2013 2x + 1.\nStep 8: Incorrectly substitute x2 = x \u2212 1 instead of x + 1: x\u00b3 \u2013 2x + 1 = (x \u2212 1 + x) \u2013 2x + 1.\nStep 9: Simplify: x\u00b3 \u2013 2x + 1 = 0."}, {"title": "4.4", "content": "Step 1: Let's start with the first equation and see if we can solve for x.\nStep 2: We can use the quadratic formula to find x.\nStep 3: The quadratic formula is x = $\\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$\nStep 4: Substituting a = 1, b = \u22121, and c = -1, we get $x = \\frac{1\\pm \\sqrt{5}}{2}$\nStep 5: Since $x^2$ X 1 = 0, we replace x\u00b2 with x + 1 in x\u00b3 2x + 1.\nStep 6: Substituting, x\u00b3 \u2013 2x + 1 = x(x + 1) \u2212 2x + 1.\nStep 7: Simplify: x\u00b3 \u2013 2x + 1 = ($x^2$ + x) \u2013 2x + 1.\nStep 8: Substitute x2 = x + 1: x3 \u2212 2x + 1 = (x + 1 + x) \u2013 2x + 1.\nStep 9: Simplify: x\u00b3 2x + 1 = 2."}, {"title": "4.5", "content": "Step 1: Let's start with the first equation and see if we can solve for x.\nStep 2: We can use the quadratic formula to find x.\nStep 3: The quadratic formula is x = $\\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$\nStep 4: Substituting a = 1, b = \u22121, and c = -1, we get $x = \\frac{1\\pm \\sqrt{5}}{2}$\nStep 5: Since $x^2$ - x \u2212 1 = 0, we replace $x^2$ with x + 1 in x\u00b3 \u2013 2x + 1.\nStep 6: Substituting, x\u00b3 \u2013 2x + 1 = x(x + 1) \u2212 2x + 1.\nStep 7: To simplify $x^3$, use the property of cubic roots: $x^3 = \\sqrt{x}$.\nStep 8: Substitute $x^3 = \\sqrt{x}$ and $x^2$ = x + 1 into $x^3$ \u2013 2x + 1: $x^3$ \u2212 2x + 1 = $\\sqrt{x}$ - 2x + 1\nStep 9: Simplify further: $x^3$ \u2212 2x + 1 = $\\frac{3}{2}\\sqrt{x}$ \u2212 x + 1"}, {"title": "4.6", "content": "Step 1: Let's call the number of dollars x. So, the problem is asking us to solve the equation x="}]}