{"title": "Chain of Grounded Objectives: Bridging Process and Goal-oriented Prompting for Code Generation", "authors": ["Sangyeop Yeo", "Seung-won Hwang", "Yu-Seung Ma"], "abstract": "The use of Large Language Models (LLMs) for code generation has gained significant attention in recent years. Existing methods often aim to improve the quality of generated code by incorporating additional contextual information or guidance into input prompts. Many of these approaches adopt sequential reasoning strategies, mimicking human-like step-by-step thinking. However, such strategies may constrain flexibility, as they do not always align with the structured characteristics of programming languages. This paper introduces the Chain of Grounded Objectives (CGO), a method that embeds functional objectives into input prompts to enhance code generation. By leveraging appropriately structured objectives as input and avoiding explicit sequential procedures, CGO adapts effectively to the structured nature of programming tasks. Empirical evaluations demonstrate that CGO effectively enhances code generation, addressing limitations of existing approaches.", "sections": [{"title": "1 Introduction", "content": "The advancement of Large Language Models (LLMs) has significantly transformed software development, particularly in natural language-driven code generation (Zhu et al., 2024; Hui et al., 2024). LLMs demonstrate exceptional ability to generate code that aligns closely with problem statements described in natural language, thereby improving development efficiency. However, ensuring the quality of generated code, like accuracy and efficiency, remains a significant challenge.\nRecent approaches to improving code generation quality have focused on enhancing the input to LLMs by incorporating additional context or guidance, and these methods can be classified based on the type of supplementary information provided:\nFirst, example-based guidance approaches involve supplying sample codes or problem-solution examples to guide LLMs; with few-shot prompting (Brown, 2020) being a typical example.\nSecond, process-oriented reasoning, focuses on guiding the model through the procedural steps required to solve a problem, essentially explaining \"how\" to achieve the solution. Prompts like Chain of Thought (Wei et al., 2022) and self-planning (Jiang et al., 2024) exemplify this approach by decomposing the problem-solving process into a sequence of logical steps. However, these prompts faces key limitations in code generation. It often emphasizes surface-level syntactic alignment over a deep understanding of problem logic or algorithms.\nThird, goal-oriented reasoning centers on identifying and defining the objectives (\"what\") that needs to be achieved without necessarily detailing the steps (Tian et al., 2023; Wang et al., 2023). However, its practical application is hindered by the lack of clear guidelines on how to structure goals effectively. Abstract or poorly defined goals may fail to provide sufficient guidance to the model. Conversely, overly concrete goals, such as test cases, typically assume code execution for validation, resulting in significant computational overhead. These challenges make existing goal-oriented methods impractical for real-world applications.\nOur distinction lies in the Chain of Grounded Objectives (CGO), a new approach that addresses the limitations of existing code generation methods. CGO bridges the strengths of goal-oriented and process-oriented reasoning by embedding functional objectives directly into input prompts, described in natural language or other structured forms. Unlike traditional methods focused on human-readability and emphasizing process, CGO prioritizes formats suited for computational interpretation, ensuring compatibility with programming languages. Specifically, CGO utilizes the functional objectives that closely resembles code comments format that LLMs have extensively learned during training. By representing objectives in this grounded manner, CGO ensures alignment between the problem's goals and the generated code solution.\nWhile CGO avoids explicitly enforcing step-by-step processes, its objectives inherently reflect logical structures, allowing it to benefit from process-oriented reasoning. By focusing on \"what\" needs to be achieved while indirectly supporting \"how,\" CGO balances adaptability and structured guidance, aligning well with programming tasks.\nThe key contributions of this study are as follows:\n\u2022 CGO bridges the strengths of goal-oriented and process-oriented reasoning, combining the clarity of goal definitions with the implicit logical structure of procedural reasoning\n\u2022 CGO improves code generation performance by ensuring alignment between the generated code and functional objectives, leveraging the LLM's familiarity with code comments and documentation patterns.\n\u2022 Experimental results show that CGO outperforms baseline methods across code generation benchmarks, achieving higher accuracy in metrics such as pass@1 and pass-ratio@10."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Code Generation Using LLMs", "content": "The development of pretrained language models has significantly advanced code generation research. Models like CodeT5 (Wang et al., 2021) and UnixCoder (Guo et al., 2022), which learn from textual and code data, demonstrate strong performance across tasks. Models such as Codex, CodeGen, AlphaCode, and CodeGeeX, with larger models consistently improving performance, underscoring the value of scaling model parameters for better outcomes (Chen et al., 2021; Nijkamp et al., 2022; Li et al., 2022; Zheng et al., 2024). Open-source models like LLAMA and Mistral have furthered the field (Touvron et al., 2023a; Jiang et al., 2023), offering accessible and high-performing solutions to researchers. Notably, CodeLLaMA is highly specialized for code generation tasks (Touvron et al., 2023b). Proprietary models, including the GPT series, Gemini, and Claude, continue to demonstrate leading performance in code generation (Achiam et al., 2023; Team et al., 2023)."}, {"title": "2.2 Prompting Strategies for Code Generation", "content": "The simplest way to generate code with LLMs is to input the coding problem directly and let the model produce a solution. However, as effective prompting has been shown to improve LLM performance, various studies have explored prompting techniques for code generation.\nExample-based guidance approaches, such as few-shot prompting (Brown, 2020), provide example problem-solution pairs but have shown limited impact on code quality, according to our preliminary experiments. Process-oriented reasoning methods decompose the problem-solving process into a sequence of manageable steps. CodeCoT adapts CoT for code generation by refining intermediate steps to align with coding tasks (Huang et al., 2023), Structured Chain of Thought (SCOT) uses pseudocode as intermediate steps (Li et al., 2023). Self-Planning (Jiang et al., 2024) prompts the model to create a detailed plan before generating code, effectively breaking complex problems into smaller, manageable subtasks. Goal-oriented reasoning includes methods like Test-Case-Driven Chain of Thought (TCoT) (Tian et al., 2023). TCOT focuses on defining goals in the form of test cases to validate generated code through execution and iterative refinement. However, its reliance on execution-based feedback loop incurs computational costs.\nBeyond these code-focused prompting techniques, research has also explored integrating programming methods with natural language reasoning. Approaches such as ReAct (Gao et al., 2023), PAL (Gao et al., 2023), and Think-and-Execute (Chae et al., 2024) combine reasoning capabilities with code simulation or execution. These methods utilize LLMs to generate intermediate code or executable steps, bridging the gap between reasoning and computation and highlighting the broader potential of code-generating prompts."}, {"title": "3 Proposed Technique: CGO Prompting", "content": "In this study, we propose Chain of Grounded Objectives (CGO) prompting, a technique designed to enhance the performance of LLMs in automated code generation tasks. The core idea behind CGO is to provide functional objectives as additional input to guide the LLM. These objectives are presented in the form of abstract representations that are likely familiar to the LLM based on its training data, such as structured natural language descriptions commonly found alongside code."}, {"title": "3.1 Motivation", "content": "LLMs are typically trained on datasets that include source code alongside comments, documentation, and other structured explanations. These elements often describe the intended functionality in a way that abstracts implementation details. By incorporating such familiar patterns, we expect that CGO prompting:\n\u2022 Provides clarity to the LLM about the task requirements.\n\u2022 Improves the model's ability to map functional objectives to code implementations.\n\u2022 Leverage its training familiarity with structured descriptions to improve the overall quality of code generation."}, {"title": "3.2 CGO Prompting Mechanism", "content": "Figure 1 illustrates the workflow of the CGO-based code generation process consists of two stages: (1) the 'Objective Generation Stage', where the LLM generates functional objectives from the problem description, and (2) the 'Code Generation Stage', where these objectives are used as additional context to guide the LLM in generating the code.\nThe CGO-based code generation process can be formally described as follows (detailed examples are provided in Tables 4):\n(1) Objective Generation Stage\nIn the first stage, the LLM generates a set of functional objectives $O = {o_1, o_2, ..., o_n}$ based on the given problem description P.\n$O = GenObj(P)$ (1)\nHere, GenObj is a function that maps the problem description P to a set of objectives O, where each objective $o_i$ represents a sub-goal that the final code is expected to achieve. The LLM is guided by a specifically designed prompt, such as \"Write objectives for the problem.\" The output O is intended to capture key goals and sub-goals derived from the coding problem P.\nBy framing functional objectives to reflect the developer's intent, this approach bridges natural language and code, using the LLM's reasoning to align with the developer's thought process.\n(2) Code Generation Stage\nIn the second stage, the LLM synthesizes the final code C using both the problem description P and the objectives O as input. This process is represented by the function GenCode:\n$C = GenCode(O | P)$ (2)\nThe notation O | P indicates that while the objectives O are derived from the problem P, they are treated as additional context that supplements the original problem description. The function GenCode leverages this enriched context to generate code that not only solves the problem but also adheres to the specific goals outlined in O. This supports the LLM in developing a clear understanding of the problem's requirements, resulting in code that is both functionally correct and contextually appropriate.\nThe entire CGO-based code generation process can be represented by combining the two stages:\n$C = GenCode(GenObj(P) | P)$ (3)"}, {"title": "4 Evaluation", "content": ""}, {"title": "4.1 Experimental Setup", "content": "We used LLaMA-3-based models and GPT-3.5-Turbo for generating objectives and code, with further details provided in Appendix A. To evaluate the performance of the generated code, we used the pass@k and pass-ratio@n evaluation metrics. More information is available in Appendix B."}, {"title": "4.1.1 Benchmarks", "content": "Experiments were conducted on HumanEval and MBPP benchmarks, along with their extended versions, HumanEval+ and MBPP+, to assess robustness with additional edge cases.\n\u2022 HumanEval: A dataset of 164 problems by OpenAI, including descriptions, reference solutions, and an average of 7.7 test cases per problem.\n\u2022 MBPP-sanitized: A subset of MBPP with verified problems, each containing a description, a solution, and three test cases. (Austin et al., 2021)\n\u2022 HumanEval+ and MBPP+: Extended datasets with 80x and 35x more test cases, focusing on edge case robustness. (Liu et al., 2024)"}, {"title": "4.1.2 Baselines", "content": "We compared the proposed method with the following baseline approaches (detailed examples are provided in Tables 5 6, and 7):\n\u2022 Direct prompting: Generates code solely from the problem description.\n\u2022 Few-shot: Includes example problems and solutions to guide the model.\n\u2022 CodeCoT: Uses Chain of Thought (CoT) reasoning to generate intermediate steps and final code in two stages.\n\u2022 Self-Planning: Extracts sub-tasks from the problem and generates a plan, which guides code generation in two stages."}, {"title": "4.2 RQ 1: How Does CGO Perform Compared to Baselines?", "content": "This section evaluates the performance of the CGO prompting technique compared to other prompting methods for code generation. The experiments were conducted using the LLaMA-3 70B Instruct model, with Few-shot, CodeCoT, and Self-Planning employed as baseline approaches. The findings are summarized in Table 1."}, {"title": "4.3 RQ 2. Does CGO depend on LLM Size?", "content": "This section examines the consistency of CGO in generating high-quality code across different model sizes, from small to hyper-scale LLMs. Experiments were conducted on LLaMA-3 8B Instruct and GPT-3.5-Turbo, comparing performance with baseline prompting techniques. The results are presented in Table 2."}, {"title": "LLAMA-3 8B Instruct (sLLM)", "content": "Experimental results reveal that CGO achieved pass@1 results of 62.4 and 68.1 on the HumanEval and MBPP-sanitized benchmarks, respectively, using the LLaMA-3 8B Instruct model. Additionally, CGO recorded pass-ratio@10 of 65.8 and 68.8, surpassing all baseline methods and demonstrating its robustness and efficacy across diverse benchmarks.\nIn comparison, CodeCoT showed improvements over Direct Prompting on both benchmarks in sLLM-based experiments. However, its performance varied significantly with model scale. While effective in prior experiments with the LLaMA-3 70B Instruct model, CodeCoT underperformed against Direct Prompting on HumanEval in this setting, highlighting its sensitivity to model size and task complexity.\nSelf-Planning demonstrated consistent performance improvements over Direct Prompting in SLLM-based experiments, maintaining stability across varying model scales and benchmark difficulties. However, it exhibited comparatively lower effectiveness than CGO, indicating limitations in adequately capturing and aligning with code objectives.\nThe effectiveness of CGO was further validated on the HumanEval+ and MBPP+ benchmarks, where it consistently outperformed CodeCoT, delivering significant improvements in pass@1 and pass-ratio@10. Similarly, CGO demonstrated higher results compared to Self-Planning on these benchmarks, emphasizing its strength in tackling diverse and complex code generation tasks."}, {"title": "GPT-3.5-Turbo (Hyper-scale LLM)", "content": "With GPT-3.5-Turbo, CGO outperformed all baseline prompts across four benchmark datasets. CGO achieved pass@1 of 74.6 and 86.0 on HumanEval and MBPP-sanitized, respectively, and pass-ratio@10 of 77.4 and 86.5, demonstrating its efficacy in addressing code generation tasks.\nOn HumanEval and MBPP-sanitized benchmarks, CGO performed comparably to other baselines, with minimal differences. However, on HumanEval+ and MBPP+, CGO displayed a distinct advantage, maintaining consistently high score even with datasets featuring a larger number of test cases and more complex problems. These results show CGO's ability to handle diverse scenarios and challenging conditions in code generation effectively.\nInterestingly, datasets with fewer test cases, such as HumanEval and MBPP-sanitized, appeared to limit the impact of model scale on code quality. In contrast, larger and more intricate datasets like HumanEval+ and MBPP+ allowed CGO to generate solutions that better captured the underlying problems, showcasing its robust capabilities.\nSelf-Planning consistently underperformed compared to CGO across all benchmarks, mirroring the trend observed in sLLM-based experiments, including those conducted with GPT-3.5-Turbo.\nThese findings demonstrate CGO's adaptability and efficiency in producing high-quality code across different model scales and benchmark complexities, establishing it as a robust and reliable prompting technique for diverse code generation tasks."}, {"title": "5 Conclusion", "content": "This paper proposed the Chain of Grounded Objectives (CGO), a prompting method that embeds functional objectives directly into input prompts to enhance LLM-driven code generation. CGO employs a two-stage process: generating functional objectives and incorporating them as additional context during code generation. By aligning input prompts with the structured and logical paradigms of programming languages, CGO addresses the limitations of process-oriented reasoning approaches and offers greater flexibility. Empirical evaluations demonstrate that CGO outperforms baseline methods in accuracy metrics such as pass@1 and pass-ratio@10, highlighting its effectiveness.\nFuture studies could focus on adapting the level of functional objectives to align with various stages of the software development process, including requirements gathering, design, and testing."}]}