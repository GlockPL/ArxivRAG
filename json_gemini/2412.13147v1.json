{"title": "Are Your LLMs Capable of Stable Reasoning?", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Ziyi Wang", "Kuikun Liu", "Songyang Gao", "Wenwei Zhang", "Songyang Zhang", "Kai Chen"], "abstract": "The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, a significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial. This work makes two key contributions. First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the model's peak performance potential and its stability. Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. Our findings reveal substantial room for improvement in LLMs' \"realistic\u201d reasoning capabilities, highlighting the need for more robust evaluation methods.", "sections": [{"title": "1 Introduction", "content": "Since the emergence of large language models (LLMs), the complex reasoning, particularly in mathematical problem-solving, has been regarded as the crown jewel of LLM capabilities. Numerous models have demonstrated remarkable performance on mathematical tasks, from general-purpose models like GPT-40 (OpenAI, 2024a), and the LLaMA series (AI, 2024) to specialized models such as DeepSeek-Math (Shao et al., 2024) and Qwen-Math (Yang et al., 2024b), which excel in complex and high-difficulty mathematical reasoning. More recently, long-chain-of-thought (Long-CoT) models like OpenAI-01 (OpenAI, 2024b), QwQ (Team, 2024a), and DeepSeek-R1-Lite-Preview (DeepSeek-R1-Lite-Preview, 2024) have further advanced the state-of-the-art in mathematical problem-solving.\nIn real-world applications, LLMs typically employ sampling with predefined decoding parameters (including temperature, top-k, top-p, and repetition penalty) to maintain response diversity. Users often regenerate responses or initiate new sessions until obtaining satisfactory answers to specific questions. However, conventional evaluation metrics for LLMs such as Greedy Accuracy, Pass@K (Chen et al., 2021), Best-of-N (BoN), and Majority Voting demonstrate significant limitations in measuring real-world performance, particularly regarding long-term consistency. While these metrics effectively capture either instantaneous accuracy or peak performance across multiple samples, they inadequately address output stability. Such instability poses significant challenges for applications requiring reliable and predictable outcomes, highlighting the need for evaluation metrics that effectively balance response diversity with consistent performance.\nTo address these challenges, we introduce G-Pass@k, a novel evaluation metric that simultaneously assesses both problem-solving capability and performance consistency. The core"}, {"title": "2 Related Work", "content": "Mathematical Reasoning Benchmarks for LLMs. The assessment of large language models (LLMs) in mathematical reasoning has led to the development of specialized benchmarks focusing on different aspects of an LLM's mathematical proficiency. GSM8K (Cobbe et al., 2021) presents a dataset of 8,500 elementary-level math word problems, segregated into training and testing sets, that demand multi-step reasoning and detailed solution paths. MATH (Hendrycks et al., 2021b) encompasses 12,500 problems derived from high school math competitions, challenging LLMs with advanced topics like calculus and algebra, and providing step-by-step solutions to facilitate coherent training. MMLU (Hendrycks et al., 2021a) evaluates pre-trained language models across 57 subjects, including STEM fields, featuring a mathematics section designed to gauge knowledge and problem-solving skills in mathematics. FineMath (Liu et al., 2024b) assesses fine-grained mathematical reasoning through a dataset comprising core elementary concepts, categorized into 17 types of math vocabulary questions, manually annotated, and classified by difficulty. GAOKAO-Bench (Zhang et al., 2023), incorporating 2,811 Chinese questions, examines LLMs in a zero-sample setting, resembling China's college entrance examination format, encompassing a range of question types from objective to subjective. MathBench (Liu et al., 2024a) is a hierarchical benchmark that assesses both theoretical and applied mathematical abilities, consisting of 3,709 questions spanning basic arithmetic to university level, structured across five educational tiers. Omni-Math (Gao et al., 2024) focuses on Olympic-level mathematical reasoning, featuring 4,428 competition-level problems categorized into over 33 subfields and 10 difficulty levels, spanning from entry-level to professional international competitions.\nStability of LLM Reasoning. Large language models (LLMs) exhibit remarkable performance in reasoning tasks, encompassing question answering, programming, and mathematical problem-solving. Despite their prowess, the output stability of LLMs poses a significant challenge, whereby the model's outputs can vary for the same input due to random sampling or hallucinations, impacting the model's reliability and predictability (Gupta et al., 2024; Xu et al., 2024; Atil et al., 2024). Atil et al. (2024) introduced two new metrics: TARr@N for the total agreement rate at N runs over raw output and TARa@N for total agreement over parsed-out answers. However, TARr@N and TARa@N focus solely on measuring output consistency, our work introduces a novel evaluation metric G-Pass@k for evaluating the mathematical reasoning proficiency of LLMs. This metric aims to assess the model's true reasoning ability by not only considering output consistency but also emphasizing correctness."}, {"title": "3 Generalized Metric for LLM Reasoning", "content": "In this section, we begin with a review of the classical evaluation metric Pass@k and then introduce our proposed metric G-Pass@k. Finally, we outline the key differences between these two metrics.\n3.1 Preliminary: Pass@k\nPass@k is initially proposed to evaluate the functional correctness of code generated by models (Kulal et al., 2019; Chen et al., 2021). With the expanding application of large language models (LLMs) in various reasoning tasks (Rajani et al., 2019; Imani et al., 2023;"}, {"title": "3.2 Generalized Metric: G-Pass@k", "content": "Pass@k can indicate a model's performance potential, however, it does not account for the stability of the model's reasoning performance. To assess both the potential and stability of models, we propose a generalized metric called G-Pass@k. In simple terms, G-Pass@k measures the consistency of a model in generating correct solutions across multiple generations by assessing the probability of obtaining correct solutions in all k generations.\nDefinition of G-Pass@k & G-Pass@kt. Let p* denote the probability that a model provides the correct solution for a question. Given that each generation is independent and identically distributed (i.i.d.), the probability of obtaining m correct solutions follows a binomial distribution:\n$m \\sim B(n, p^*).$ (2)\nSince p* is usually inaccessible, we leverage the hypergeometric distribution to approximate the binomial distribution:\n$\\lim_{n\\rightarrow\\infty} H(m;k,c,n) \\rightarrow B(m;n, p^*).$ (3)\nTherefore, G-Pass@k can be defined as:\nG-Pass@k = $E_{Questions} \\frac{\\binom{c}{k}}{\\binom{n}{k}}$ (4)\nwhere n represents the total number of generations per question, and c denotes the number of generations resulting in correct solutions. Considering the stringent nature of Equation (4), we draw inspiration from the mean Average Precision (mAP) metric (Everingham et al., 2010) used in object detection to introduce a threshold \\tau\\in (0.0, 1.0], leading to the definition of G-Pass@k\\tau:\nG-Pass@k\\tau = $E_{Questions} \\frac{\\sum_{j= \\lceil\\tau \\cdot k\\rceil}^{c}\\binom{c}{j}}{\\binom{n}{k}}$ (5)\nwhere $\\lceil\\tau\\cdot k\\rceil$ denotes the smallest integer greater than or equal to $\\tau\\cdot k$. Conceptually, for \\tau < 1.0, there is flexibility to allow up to k \u2212 $\\lceil\\tau\\cdot k\\rceil$ incorrect solutions within the k generations. Recall that we utilize hypergeometric distributions to approximate binomial distributions, which provides a good estimation when n is sufficiently large. We provide more discussions about the estimation in Appendix A.2.\nPass@k is the special instance of G-Pass@kt. It can be observed that Pass@k is essentially a special case of G-Pass@kt, as demonstrated by the following theorem."}, {"title": "Theorem 3.1.", "content": "Pass@k is is the special case of G-Pass@k as t approaches 0, formally described as:\n$\\lim_{\\tau\\rightarrow 0} \\frac{\\sum_{j=[\\tau \\cdot k]}^{c}\\binom{c}{j}}{\\binom{n}{k}} = \\frac{\\binom{c}{k}}{\\binom{n}{k}} = 1-\\frac{\\binom{n-c}{k}}{\\binom{n}{k}}$(6)\nThe proof is provided in Appendix A.3.\nDefinition of mG-Pass@k. When the threshold \u03c4 is low, G-Pass@kt tends to measure the model's performance potential. Conversely, at higher \u03c4 values, G-Pass@kt evaluates the model's stability or its level of mastery over the question. Thus, G-Pass@kt facilitates the continuous observation of both performance potential and stability. We further define mG-Pass@k as:\n$\\text{mG-Pass@k}_{\\tau} = 2\\int_{0.5}^{1.0} G-\\text{Pass@k}_{\\tau} d\\tau = \\frac{2}{k}\\sum_{i=[0.5\\cdot k]+1}^{k} G-\\text{Pass@k}_{i}.$(7)\nIntuitively, mG-Pass@k provides an interpolated estimate of the area under the curve of mG-Pass@k[0.5:1.0], serving as a comprehensive metric that integrates all G-Pass@kt values where \u03c4\u2208 [0.5, 1.0]. For optimal and stable models, the mG-Pass@k value should approach 1.\n3.3 Pass@k v.s. G-Pass@k\nTo facilitate a more intuitive comparison between Pass@k and G-Pass@k, Figure 2 illustrates the metric values for various c values with n = 80. The figure demonstrates that while"}, {"title": "4 Performance and Analysis", "content": "To effectively analyze the G-Pass@k performance of large language models, we construct a new and challenging benchmark named LiveMathBench. LiveMathBench will undergo ongoing updates with new questions to continuously evaluate the mathematical reasoning performance of models.\n4.1.1 Benchmark Construction\nLiveMathBench is specifically designed to include four challenging out-of-domain question sets from various mathematical competitions, aiming to avoid data contamination issues in existing LLMs and public math benchmarks (Zhou et al., 2023; Li et al., 2024; Ni et al., 2024).\nLiveMathBench (version of 202412) incorporates the latest problems from the China National Mathematical Olympiad (CNMO), China's College Entrance Examination (CCEE), American Mathematics Competition (AMC), and William Lowell Putnam Mathematical Competition (WLPMC). These datasets encompass diverse levels of difficulty and linguistic variations and have low overlap with publicly available datasets, ensuring a comprehensive evaluation of the generalization capabilities of LLMs across various mathematical scenarios. More details about LiveMathBench can be found in Appendix A.1.\n4.1.2 Benchmark Statistics"}, {"title": "4.2 Setup", "content": "4.2.1 LLMS\nWe evaluate various LLMs recognized for their strong mathematical reasoning capabilities, including InternLM2-Math-Plus-20B (Ying et al., 2024), DeepSeek-Math-7b-RL (Shao et al., 2024), DeepSeek-V2.5-1210 (DeepSeek-AI, 2024), Llama-3.1-8B-Instruct (Dubey et al., 2024), Llama-3.1-70B-Instruct (Dubey et al., 2024), Llama-3.3-70B-Instruct (AI, 2024), NuminaMath-72B-CoT (Beeching et al., 2024), Mistral-Large-Instruct-2411 (Team, 2024b), Qwen2.5-7B-Instruct (Qwen Team, 2024), Qwen2.5-Math-7B-Instruct (Yang et al., 2024b), Qwen2.5-32B-Instruct (Qwen Team, 2024), Qwen2.5-72B-Instruct (Qwen Team, 2024), Qwen2.5-Math-72B-Instruct (Yang et al., 2024b), Claude-3.5-Sonnet (Anthropic Inc., 2024), Gemini-1.5-Pro (Research, 2024), and GPT-40-2024-11-20 (OpenAI, 2024a). Additionally, we include several o1-like LLMs, such as QwQ-32B-Preview (Team, 2024a), Skywork-o1-Open-Llama-3.1-8B (01 Team, 2024), and OpenAI 01-mini (OpenAI, 2024b).\n4.2.2 Implementation Details\nIn all experiments, we set the number of generations, n, to 16 \u00d7 3 = 48 and report the greedy accuracy, Pass@k (G-Pass@k\u21920), and G-Pass@k values, where k \u2208 {4,8,16} and \u03c4\u2208 {0.25,0.5,0.75, 1.0}. For the sampling parameters of open-source models, we configure the temperature to 0.7, top-p to 1.01, top-k to 50 and repetition-penalty to 1.0. For open-source models, the maximum number of tokens is set to 8, 192 for non-o1 LLMs and 32,768 for o1-like LLMs. And for close-source models, due to constraints of inference costs, we configured the maximum completion tokens to 4,096 for non-o1 LLMs and 16,384 for OpenAI 01-mini. We use the OpenCompass (Contributors, 2023) platform to evaluate all LLMs.\nDue to the diverse formats of the final answers produced by models in complex mathematical questions, we leverage Qwen-2.5-72B-Instruct (Yang et al., 2024a) to judge whether the content generated by the tested model aligns with the standard answer. In our judge pipeline, we provide the original question, reference answer, and model-generated answer, prompting Qwen-2.5-72B-Instruct to determine whether the candidate solution is consistent with the reference answer. The details of the judging process can be found in Appendix A.4.\n4.2.3 Additional Public Benchmarks\nWe also include widely used public benchmarks MATH500 (Hendrycks et al., 2021b; Lightman et al., 2024) and AIME2024 (AIME2024; Yang et al., 2024b), both of which are designed to rigorously evaluate LLMs on their mathematical reasoning capabilities."}, {"title": "4.3 LiveMathBench & Public Benchmark Performance", "content": "Competition-Level Questions Still Remain Challenging for Current LLMs. Our analysis reveals that competition-level questions such as those in LiveMathBench and AIME2024-45 continue to pose substantial challenges for all evaluated models, even those at the cutting edge of current research. For instance, despite being the top-performing general-purpose model, Gemini-1.5-Pro-Latest achieves only a greedy decoding accuracy of 49.2% on LiveMathBench. Similarly, the best-performing mathematical reasoning model, Qwen2.5-Math-72B-Instruct, attains an accuracy of 50.4%, which, while slightly higher, still falls short of perfection. Most of the LLMs we evaluated scored between 10% and 45% on greedy decoding. Notably, several high-performing models are close-source, such as GPT-40 with 39.9% and Claude-3.5-Sonnet with 37.0%. Moreover, on another challenging benchmark, AIME2024-45, the best-performing non-o1-like LLMs only achieve about 20% greedy accuracy and the performance of most LLMs is at a low level. These results underscore the ongoing difficulty in achieving high accuracy on the latest complex mathematical problems. While O1-like models, equipped with long-chain-of-thought (long-CoT) and reflective mechanisms, perform significantly better on intricate tasks, they too face considerable challenges in LiveMathBench. For example, the optimal-performing OpenAI o1-mini achieves a score of 66.51%, and the most powerful open-source O1-like model, QwQ-32B-Preview, scores 64.3% on LiveMathBench. Despite these improvements, the gap between current model performance and human-level proficiency remains notable."}, {"title": "4.4 Further Analysis", "content": "4.4.1 Performance w.r.t. k\nFigure 3 presents the results of selected models for G-Pass@4, G-Pass@8, and G-Pass@16. For G-Pass@4, Deepseek-Math-7b-RL shows a significant decline in performance as \u03c4 increases, dropping from around 40% to 20%. Qwen-2.5-Math-72B-Instruct and QwQ-32B-Preview also decline but maintain higher performance levels, starting around 65% and 80%, respectively, and ending around 50% and 70%. For G-Pass@8, the trend is similar, with Deepseek-Math-7b-RL showing a steep decline from 40% to 20%, while Qwen-2.5-Math-72B-Instruct and QwQ-32B-Preview start at 60% and 80%, respectively, and end around 45% and 70%. For G-Pass@16, Deepseek-Math-7b-RL declines from 30% to 10%, while Qwen-2.5-Math-72B-Instruct and QwQ-32B-Preview start at 60% and 80%, respectively, and end around 35% and 60%. In general, G-Pass@k can achieve consistent evaluation results under different k values, which indicates robustness as a metric. In addition, for advanced reasoning models with strong performance, a larger value of k has better differentiation."}, {"title": "4.4.2 Performance w.r.t. n", "content": "As previously noted, the number of attempts n is crucial for the accuracy of the estimates. We selected two models, DeepSeek-Math-7b-RL and NuminaMath-72B-CoT, to conduct experiments with n = {16} \u00d7 {1,2,3,5,8,15} = {16, 32, 48, 128, 240}, and reported G-Pass@16\u03c4.The results are illustrated in Figure 4. When n is small, the estimation deviation is large, as shown by the significant fluctuations in the G-Pass@16\u03c4 values for both models. Conversely, for larger n, G-Pass@16\u03c4 tends to stabilize, indicating a more consistent and reliable performance. Specifically, the DeepSeek-Math-7b-RL model shows a steady performance around 20% for n \u2265 48, while the NuminaMath-72B-CoT model stabilizes around 30% for n \u2265 48. Therefore, empirically, we recommend making at least n = 3k generations when calculating G-Pass@k to ensure estimation accuracy."}, {"title": "4.4.3 Impact of Questions Difficulty", "content": "We also examine the performance of models with respect to questions of varying difficulty levels. We analyze CCEE and WLPMC datasets from LiveMathBench. CCEE is a college entrance examination that primarily involves fundamental high school mathematics knowledge, whereas WLPMC is a prestigious collegiate mathematics competition that presents significantly greater challenges."}, {"title": "4.4.4 Does Data Contamination or Overfitting Affect Stability?", "content": "Data contamination arises when the test data is mixed into training data, also referred to as data leakage (Dickson, 2024; Dong et al., 2024). To investigate the influence of varying extents of data contamination or overfitting on our proposed G-Pass@k metric, we performed a series of experiments using the Qwen2.5-7B model on the MATH500-L5 dataset.\nThe training process began with a base set of 200,000 randomly sampled instructions from the Numina-Math-CoT corpus (LI et al., 2024), which served as the uncontaminated training set. Subsequently, we introduced incremental rounds of data contamination, consisting of 0, 6, 8, 10, and 16 rounds, where a round of 0 indicates the absence of contamination, i.e., training exclusively on the original NuminaMath data. The model's efficacy was assessed across these five conditions, as illustrated in Figure 5."}, {"title": "5 Conclusion", "content": "In this work, we introduced G-Pass@k, a novel evaluation metric that assesses both problem-solving capability and performance consistency of LLMs across varying thresholds of correctness. To demonstrate the practical implications of G-Pass@k, we presented LiveMath-Bench, a dynamic multilingual mathematics benchmark that continuously incorporates contemporary challenging problems, ensuring relevance to the latest developments in both model capabilities and mathematical discourse. After detailed evaluations on LiveMathBench with G-Pass@k, we find 1) Despite demonstrating considerable potential in terms of Pass@K and Greedy Accuracy, most models exhibit instability during sampling. 2) Scaling up the model size or overfitting to the dataset can enhance Greedy Accuracy, but may not necessarily lead to significant improvements in stability. We hope that G-Pass@k and LiveMathBench can serve as pivotal tools for the research community, facilitating deeper insights into the development and evaluation of language models."}, {"title": "A.1 LiveMathBench Details", "content": "A.1.1 Data Sources\nLiveMathBench (version of 202412) is composed of 4 parts including CNMO, \u0421\u0421\u0415\u0415, \u0410\u041c\u0421, and WLPMC.\nCNMO. The CNMO section features curated questions from the latest Chinese National Mathematics Olympiad. To enhance the difficulty level, single-choice questions are transformed into problem-solving tasks by concealing answer options, necessitating models to reason independently and provide solutions.\nCCEE. In the CCEE segment, we have selected questions from recent mock exams of China's College Entrance Examination, excluding multi-modal proof problems. We have excluded multiple-choice questions and converted single-choice items into problem-solving questions, removing provided answer choices to assess the models' ability to generate solutions autonomously.\nAMC. The AMC section includes questions from the latest American Mathematics Competition, where each original question typically offers five possible answers labeled A through E, with only one correct option. Consistent with our approach in other sections, we convert these single-choice questions into problem-solving cues, encouraging models to deduce solutions without the aid of provided options.\nWLPMC. We also include questions from the latest William Lowell Putnam Mathematical Competition (WLPMC). Regarded as one of the most prestigious university-level mathematics competitions globally, the WLPMC challenges participants with problems that span a broad spectrum of mathematical disciplines. These include geometry, algebra, trigonometry, calculus, linear algebra, combinatorics, probability theory, number theory, complex numbers, and differential equations.\nA.1.2 Data Samples\nHere we provide some samples in LiveMathBench."}, {"title": "Example in CNMO", "content": "[Question]\n\u8bbe\u590d\u6570z,w\u6ee1\u8db3z + w = 2,\u6c42S = |z\u00b2 \u2013 2w| + |w\u00b2 \u2013 2z|\u7684\u6700\u5c0f\u53ef\u80fd\u503c\u3002\n[Answer]\n8\u221a5-16\n[Question Type]\n\u95ee\u7b54"}, {"title": "A.2 Estimation of G-Pass@k", "content": "To demonstrate the unbiasedness of Equation (5), we conduct the simulation experiment illustrated in Figure 6. Specifically, we assume the probability of a model providing the correct solution in a single run is p* = 0.4. For each n, we perform several random Bernoulli samplings to obtain different values of c to calculate G-Pass@kt, and then compute the mean and variance to generate the figure. From Figure 6, it can be observed that Equation (5) is an unbiased estimator, facilitating fair comparison across different values of n."}, {"title": "Example in C\u0421\u0415\u0415", "content": "[Question]\n\u51fd\u6570f(x) = x3e3x-3lnx-1(x > 0)\u7684\u6700\u5c0f\u503c\u662f\n[Answer]\n3\n[Question Type]\n\u586b\u7a7a"}, {"title": "Example in AMC", "content": "[Question]\nThe graph of y = ex+1 + e-x \u2013 2 has an axis of symmetry. What is the reflection of the point (-1, 2) over this axis?\n[Answer]\n(0,2)\n[Question Type]\nProblem-Solving"}, {"title": "Example in WLPMC", "content": "[Question]\nA sequence Y1, Y2, ..., Yk of real numbers is called zigzag if k = 1, or if y2 \u2013 \u04231, \u0423\u0437 \u2014\ny2,\u2026\u2026\u2026, Yk - Yk\u22121 are nonzero and alternate in sign. Let X1, X2, ..., Xn be chosen\nindependently from the uniform distribution on [0, 1]. Let a(X1, X2, ..., Xn) be the\nlargest value of k for which there exists an increasing sequence of integers 11, 12,\ndots, ik such that Xi\u2081, Xi2,..., Xik is zigzag. Find the expected value of\na(X1, X2,..., Xn) for n \u2265 2.\n[Answer]\n2n+2\n3\n[Question Type]\nProblem-Solving"}, {"title": "A.3 Proof of Theorem 3.1", "content": "Proof. Since j starts iterating at the upward rounding of $\\lceil\\tau\\cdot k\\rceil$ and \u03c4\u2208 (0, 1], so we have:\n$\\lim_{\\tau\\rightarrow 0} \\frac{\\sum_{j=[\\tau \\cdot k]}^{c}\\binom{c}{j}}{\\binom{n}{k}} = \\frac{\\sum_{j=1}^{c}\\binom{c}{j}}{\\binom{n}{k}}=\\frac{\\binom{n}{k}-\\binom{n-c}{k}}{\\binom{n}{k}}$ (8)\nAccording to the Vandermonde's Identity (Vandermonde, 1772), the numerator term on the right side of Equation (8) can be written as\n$\\sum_{j=0}^{c} \\binom{c}{j} \\binom{n-c}{k-j} = \\binom{n}{k} \\Rightarrow  \\binom{c}{k} = \\frac{\\binom{c}{j} \\binom{n-c}{k-j}}{\\binom{n}{k}}$ (9)\nSo we conclude that:"}, {"title": "A.4 Judge Details", "content": "A.4.1 Configurations of Judge Model\nInspired by previous works (Zheng et al., 2023; Son et al., 2024), we leverage Qwen2.5-72B-Instruct (Yang et al., 2024a) to judge if the answers generated by the models are consistent with the golden answers, consider the high inference cost of the closed source models such as OpenAI models. We set the temperature to 0.0, and maximum output tokens to 8, 192.\nA.4.2 Prompt for Judge\nWe leverage the following prompts to judge the consistency between candidate answers and reference answers."}, {"title": "Chinese Version of Judge Prompt", "content": "\u8bf7\u4f60\u4f5c\u4e3a\u4e00\u4e2a\u6570\u5b66\u9605\u5377\u4e13\u5bb6,\u5224\u65ad\u4e0b\u9762\u7684\u7b54\u6848\u662f\u5426\u4e0e\u6807\u51c6\u7b54\u6848\u4e00\u81f4,\u5373\u8003\u751f\u662f\u5426\u56de\u7b54\u6b63\u786e\u3002\u4e0b\u9762\u662f\u4e00\u4e9b\u8bc4\u5224\u6807\u51c6:\n1. \u6709\u4e9b\u7b54\u6848\u53ef\u80fd\u5305\u542b\u591a\u9879\u5185\u5bb9,\u53ef\u80fd\u6709\u5355\u9009\u9898,\u591a\u9009\u9898,\u586b\u7a7a\u9898\u548c\u95ee\u7b54\u9898,\u53ea\u8981\u7b54\u6848\u4e0e\u6807\u51c6\u7b54\u6848\u4e00\u81f4\u5373\u53ef,\u5bf9\u4e8e\u591a\u9009\u9898\u548c\u591a\u4e2a\u7a7a\u7684\u586b\u7a7a\u9898,\u9700\u8981\u8003\u751f\u5bf9\u5e94\u7684\u9009\u9879\u6216\u7a7a\u90fd\u56de\u7b54\u6b63\u786e\u624d\u7b97\u6b63\u786e\u3002\n2. \u6709\u4e9b\u7b54\u6848\u53ef\u80fd\u901a\u8fc7\u4e0d\u540c\u7684\u65b9\u5f0f\u8868\u8fbe,\u6bd4\u5982\u6709\u4e9b\u7b54\u6848\u53ef\u80fd\u662f\u4e00\u4e2a\u6570\u5b66\u8868\u8fbe\u5f0f,\u6709\u4e9b\u7b54\u6848\u53ef\u80fd\u662f\u4e00\u4e2a\u6587\u5b57\u63cf\u8ff0,\u53ea\u8981\u8868\u8fbe\u7684\u610f\u601d\u4e00\u81f4\u5373\u53ef\u3002\u4e14\u6709\u4e9b\u516c\u5f0f\u901a\u8fc7\u4e0d\u540c\u7684\u65b9\u5f0f\u8868\u8fbe,\u4f46\u7b49\u4ef7,\u4e5f\u662f\u6b63\u786e\u7684\u3002\n3. \u4f60\u4e0d\u9700\u8981\u91cd\u65b0\u8ba1\u7b97\u95ee\u9898\u7b54\u6848,\u56e0\u4e3a\u6807\u51c6\u7b54\u6848\u5df2\u7ecf\u7ed9\u51fa,\u53ea\u9700\u8981\u6839\u636e\u95ee\u9898\u5f62\u5f0f\u6765\u5224\u65ad\u8003\u751f\u7684\u7b54\u6848\u662f\u5426\u4e0e\u6807\u51c6\u7b54\u6848\u4e00\u81f4,\u662f\u5426\u6b63\u786e\u5373\u53ef\u3002\n\u8bf7\u4f60\u6839\u636e\u4e0a\u8ff0\u6807\u51c6,\u5224\u65ad\u4e0b\u9762\u7684\u7b54\u6848\u662f\u5426\u4e0e\u6807\u51c6\u7b54\u6848\u4e00\u81f4,\u5982\u679c\u4e00\u81f4,\u8bf7\u5728\u6700\u540e\u8f93\u51fa\\\\boxed{{yes}},\u5426\u5219\u8f93\u51fa\\\\boxed{{no}}, \u5982\u679c\u96be\u4ee5\u5224\u65ad,\u8bf7\u4e5f\u8f93\u51fa\\\\boxed{{no}}.\n\u539f\u95ee\u9898:{question}\n\u6807\u51c6\u7b54\u6848:{reference_answer}\n\u8003\u751f\u7b54\u6848:{candidate_answer}\n\u5206\u6790:"}, {"title": "English Version of Judge Prompt", "content": "Please act as an expert in grading mathematics exam papers, and judge whether the following answers match the standard answers, i.e., whether the examinee answered correctly. Here are some evaluation criteria:\n1. Some answers may contain multiple parts, such as single-choice questions, multiple-choice questions, fill-in-the-blank questions, and problem-solving questions. As long as the answer matches the standard answer, it is considered correct. For multiple-choice questions and fill-in-the-blank questions with multiple blanks, the examinee must answer all corresponding options or blanks correctly to be considered correct.\n2. Some answers may be expressed in different ways; for example, some answers may be mathematical expressions, while others may be textual descriptions. As long as the meaning conveyed is consistent, it is considered correct. Additionally, some formulas may be expressed differently but are equivalent, which is also considered correct.\n3. You do not need to recalculate the problem answers, as the standard answers are already provided. You only need to judge whether the examinee's answer matches the standard answer based on the form of the question and whether it is correct.\nPlease judge whether the following answer matches the standard answer according to the above criteria. If they match, output \\\\boxed{{yes}}, otherwise output \\\\boxed{{no}}. If it is difficult to judge, also output \\\\boxed{{no}}.\nOriginal Question: {question}\nStandard Answer: {reference_answer}\nExaminee's Answer: {candidate_answer}\nAnalysis:"}, {"title": "A.4.3 Evaluation of Judge Model", "content": "To evaluate the effectiveness of our judge model, we compared the agreement rate between Qwen2.5-72B-as-Judge and GPT40-as-Judge (OpenAI, 2024a). Specifically, we randomly selected 300 samples from the generations of five different models and used the judgments from GPT40 as the ground truth. We then calculated the agreement rate between the judgments made by our model and those by GPT4o."}]}