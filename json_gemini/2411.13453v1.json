{"title": "LIMBA: AN OPEN-SOURCE FRAMEWORK FOR THE PRESERVATION AND VALORIZATION OF LOW-RESOURCE LANGUAGES USING GENERATIVE MODELS", "authors": ["Salvatore Mario Carta", "Stefano Chessa", "Giulia Contu", "Andrea Corriga", "Andrea Deidda", "Gianni Fenu", "Luca Frigau", "Alessandro Giuliani", "Luca Grassi", "Marco Manolo Manca", "Mirko Marras", "Francesco Mola", "Bastianino Mossa", "Piergiorgio Mura", "Marco Ortu", "Leonardo Piano", "Simone Pisano", "Alessia Pisu", "Alessandro Sebastian Podda", "Livio Pompianu", "Simone Seu", "and Sandro Gabriele Tiddia"], "abstract": "Minority languages are vital to preserving cultural heritage, yet they face growing risks of extinction due to limited digital resources and the dominance of artificial intelligence models trained on high-resource languages. This white paper proposes a framework to generate linguistic tools for low-resource languages, focusing on data creation to support the development of language models that can aid in preservation efforts. Sardinian, an endangered language, serves as the case study to demonstrate the framework's effectiveness. By addressing the data scarcity that hinders intelligent applications for such languages, we contribute to promoting linguistic diversity and support ongoing efforts in language standardization and revitalization through modern technologies.", "sections": [{"title": "1 Introduction", "content": "Languages, including those spoken by minority groups, represent a priceless cultural heritage that reflects the history, traditions, and identity of a people. The preservation and enhancement of minority languages are of paramount importance for maintaining linguistic diversity and safeguarding the distinctive knowledge associated with the communities that speak them. Each language represents a unique way of interpreting the world, and the loss of a language results in the loss of this perspective. Promoting minority language rights and intercultural dialogue contributes to social cohesion and the richness of the global human heritage.\nThe increasing importance of generative models, including those based on artificial intelligence, is evident in numerous domains. These models are utilized in various applications, including the generation of text, images, and music, as well as in more practical contexts such as machine translation, code completion, and virtual assistance. However, these models are primarily developed and optimized for high-resource languages, such as English, Chinese, French, and numerous other European languages, which have extensive training data. This creates a disparity between minority and less documented languages, which frequently remain less supported and benefit less from AI technologies. Overcoming this challenge could enhance equitable access to technology and promote greater language inclusivity globally.\nThe aforementioned models necessitate the availability of substantial quantities of data to ensure effective training, a requirement that presents a considerable challenge for endangered languages. These languages are afflicted by a severe scarcity and fragmentation of available data, with few written or digital resources and a steadily decreasing community of speakers. The paucity of data makes it challenging to develop high-quality language models for these languages, thereby exacerbating the risk of cultural and linguistic extinction. It is imperative to address this data deficit in order to preserve and enhance endangered languages, which can be achieved by promoting initiatives to collect and digitize texts, recordings, and other language resources.\nIndeed, in recent years, research has been aimed at narrowing the gap between more widely spoken languages and those that are endangered by seeking methods of revitalization and enhancement [41]. In the context of information technology, where these languages are frequently designated as low-resource languages due to the paucity of available data, considerable emphasis has been placed on the creation of high-impact artificial intelligence-based linguistic resources that could facilitate the preservation and dissemination of lesser-known languages [37]. The generation of the described tools is, therefore, constrained by the availability of data rather than by the complexity of development. Consequently, several entities, including Google and Mozilla, have initiated programs to encourage data collection. Google's Project Euphonia and Mozilla Common Voice, for instance, encourage the crowdsourcing of voice data, whereby native speakers can contribute by recording their voice.\nThis paper proposes a methodology, still under development, for generating a set of linguistic tools capable of constructing new data in little-used languages so that they can be used to train a language model. In order to evaluate the effectiveness of the work presented, the model generation pipeline is tested on Sardinian, a language that, due to its complexity and risk of disappearance [33], is a perfect case study for the set task. In fact, this language is often the cause of cultural and political debates regarding its standardization, an issue that has aroused the curiosity of several researchers [35]. The application of a methodology such as the one described in this paper could even facilitate the task of standardizing the language and defining its sociolinguistic situation more clearly.\nThe main objectives of this work are therefore:\n\u2022 defining an end-to-end framework for low-resource languages to generate a language model by defining a set of tools suitable for analyzing and generating data in a minority language;\n\u2022 testing the effectiveness of the framework using the Sardinian language as a case study.\nThe remainder of the paper is organized as follows: Section 2 will give an overview of existing work in this context, Section 3 will describe the proposed work in detail, Section 4 will outline some considerations regarding the impact of the devised framework, and finally Section 5 will conclude the paper."}, {"title": "2 Background", "content": "This section, aimed at providing an adequate context, describes the state of the art of low-resource languages and their development in the field of language processing. An overview of the existing literature in linguistics on these languages will be given, and then it will be described how they have been treated from a computer science perspective. In addition, it will be highlighted how the Sardinian language has been related in these contexts."}, {"title": "2.1 Foundations of Low-Resource Languages", "content": "In the following, we provide a proper background, from giving a proper definition and the main characteristics of low-resource languages to describing the actions taken for their preservation and improvement."}, {"title": "2.1.1 Preliminaries", "content": "A low-resource language refers to a language for which there is a lack of digital data, such as text or audio recordings, useful for the development of advanced linguistic technologies, e.g., speech recognition or machine translation. Therefore, the development of tools and applications that can effectively incorporate it is limited. In summary, such languages can be interpreted as those with the least information and low density [10]. It is clear that such languages present numerous challenges, including the difficulty of developing accurate Artificial Intelligence (AI) models, the lack of annotated corpora, and the limited technological support for digitization. In addition, the scarcity of language documentation, and often the lack of financial or institutional resources to preserve it, makes it even more difficult to make the language more accessible and usable in modern technologies. Magueresse et al. [29] provide an overview of the recent improvements and the future challenges."}, {"title": "2.1.2 Languages Taxonomy", "content": "Linguistic taxonomies symbolically represent the hierarchical relationships between terms or entities in a language. Although they are widely used in a variety of contexts, manually updating and maintaining them is a complex process and difficult to scale. As this procedure is even more complicated for minority or low-resource languages, a process of taxonomy enrichment is often undertaken. In the work of Takeoka et al. [51], pre-trained language models are used to compensate for the lack of information in low-resource environments.\nFrom this point of view, the Sardinian language has an extremely complicated taxonomy, characterized by several variants that are still being defined and by great dialectal diversity. Sardinian is a Romance language that emerged during the Romanization of Sardinia in the 3rd century BC. Several experts have classified Sardinian as an Italo-Romance language due to its affinity with Italian as an administrative and cultural reference [45]. Nevertheless, due to its unique linguistic characteristics and structural distance from other Romance languages [36, 57, 17], Sardinian, nowadays, is considered a distinct linguistic branch, as it also shares traits from both Eastern and Western Rom\u00e0nia, impeding a precise categorization within the Romance family [55]. These characteristics make this language an ideal study case for work in description."}, {"title": "2.1.3 Preservation and Valorization", "content": "Preserving and promoting lesser-used languages is critical to safeguarding the world's cultural and linguistic diversity. These languages often convey the traditions, histories, and identities of minority communities, which are at risk of disappearing without appropriate intervention. Valuing these languages also promotes social inclusion and ensures that even the smallest communities have access to modern technological tools. Investing in their documentation and digitization is essential to prevent the loss of unique knowledge, thereby contributing to a richer and more diverse global heritage. Many scholars in the field have worked to find new ways to keep such languages alive and prevent them from disappearing. Bird [5], for example, suggest new ways of working with Indigenous communities; similarly, Muzoora et al. [40] highlight the challenges of language policy in the context of African education, documenting how the educational aspect occupies a prominent position in the process of strengthening and preserving a language.\nEven for the Sardinian language, it seems unquestionable to state an ongoing process of revalorization of the language in the perception of the island's inhabitants [54, 38] and the diffusion of Sardinian into new domains, often public and formal [31, 18, 49, 27, 32], apparently - although this point still needs to be further investigated by specialized studies - with some linguistic features of common diffusion and partially distant from those observable in diaphasically low Sardinian. What is taking place can perhaps be classified, at least if we take into consideration certain areas of Sardinia, in particular the major urban centers, as a process of diacrolectia, i.e., a situation in which the local language is increasing its use in high contexts, but remains in great difficulty in the spheres of everyday conversation, now the preserve of the national language, a fact that still places Sardinian in a serious condition of danger of extinction."}, {"title": "2.2 Low-Resource Language Processing", "content": "The following sections describe the background of Natural Language Processing (NLP) in the context of data-poor languages, with the aim of presenting state-the-art in terms of data collection, tools to support language analysis, processing of audio material in minority languages, translators, and finally, language models."}, {"title": "2.2.1 Resource Collection", "content": "As mentioned above, one of the main challenges for those doing research in low-resource languages is the scarcity of digitized language resources, such as corpora, audio recordings, or annotated texts. Many of these languages are spoken by small communities or in remote areas, making it difficult to collect data systematically. In addition, a lack of standardization and language tools, such as dictionaries or natural language processing models, further complicates the process. It is therefore crucial to develop innovative data collection strategies that use adaptive technologies and actively involve local communities to ensure that languages are adequately represented in the digital landscape.\nThe proposal of Madaan et al. [28] describes a method for collecting high-quality data for low-resource languages to be used to train a machine translation system from a more widely used language (English) to a minority language (Hindi). The method used is based on obtaining captions for images that illustrate concepts relevant to most of the world's languages. In other cases, as in the proposal of de Jesus and Nunes [11], a custom data collection pipeline is used to automate and streamline the process of building text corpora from the Web for low-resource languages. Of course, text corpora can also be extracted from dialogues between people; in this context, the work of Yusupujiang and Ginzburg [60] proposes a method for generating a dialogue corpus for Uyghur, but generalizable to other low-resource languages.\nAs with other minority languages, several data collection projects have focused on Sardinian; [46] describe a corpus of contemporary Sardinian collected from interviews with Sardinians living outside Sardinia. However, the annotation and dataset creation process is done manually, leaving room for errors and requiring a lot of resources and time. Therefore, there is an urgent need to develop algorithms that perform this process automatically based on reliable and powerful AI systems."}, {"title": "2.2.2 Linguistic Modeling", "content": "The Linguistic Modeling task considered in this paper refers primarily to the grammatical analysis tools used to study linguistic details for languages that are still under study. Such tools automatically annotate portions of text, facilitating the work of a linguist; their use can also aid teaching and assessment tasks for uncommon language knowledge. This paper focuses on three main tools that the Sardinian language, among other languages, urgently needs: a Part-of-Speech tagger, a lemmatizer, and a language variant identifier."}, {"title": "Part-of-Speech Tagger", "content": "A Part-of-Speech (PoS) tagger is an NLP tool that assigns a grammatical category to each word in a sentence, such as a noun, verb, or adjective. It helps to identify the function of each word in the context of the sentence, making syntactic analyses easier. Such a tool can be critical in developing low-resource languages, as it automates linguistic analysis even in contexts lacking structured linguistic resources. With the PoS tagger, the creation of annotated corpora and translation tools can be accelerated, contributing to the preservation and development of these underrepresented languages.\nSeveral Machine Learning algorithms have been used to develop these kinds of tools. Mercer et al. [34] and Christodoulopoulos et al. [9] exploit clustering models to annotate each word and assign a label. Another technique often used involves Hidden Markov Models (HMMs) approaching the tagging problem as a sequence-to-sequence problem; for example, some models are trained on a \u201cnative\" language whose annotated data are available, then applying a grounding step to obtain the annotations in the language of interest [6, 8]. Finally, some works interpret PoS tagging as a classification problem; among them, Duong et al. [14] proposed a softmax classifier trained on projected English-language annotations and subsequently adjusted to the tags of the language of interest. However, the performance of such a classifier was enhanced in further works by using bidirectional Long Short-Term Memory models [15, 16].\nIn the context of the Sardinian language, to the best of our knowledge, there is no automatic annotation tool like those described above. An automated PoS tagger prototype is currently under development [39]. In particular, the model is generated by fine-tuning a small language model (BERT) adapted for the token classification task on a manually annotated Sardinian language dataset."}, {"title": "Lemmatizer.", "content": "A lemmatizer is an NLP tool that reduces words to their basic form or lemma, eliminating inflection and grammatical variants. This process helps to unify terms in the text, thereby improving the accuracy of linguistic analysis. Such a tool can be crucial for developing low-resource languages, as it simplifies linguistic standardization even in languages with complex variants. The lemmatizer can improve the quality of tools such as dictionaries or translation models, thus accelerating the creation of language resources.\nSome research has focused on training specific models to perform the task of lemmatization. Yarowsky et al. [59] proposed, for instance, a training process for a multilingual lemmatizer. This procedure comprises a bootstrap phase of the system, initiated from the projections of one language onto a second language of interest. The strength lies in its capacity to process noisy and incomplete initial projections. The generation of lemmatizers also employs editing-based strategies that utilize Long-Short-Term-Memory (LSTM) models, as they are optimally suited to handle text sequences [30].\nThe absence of an automatic lemmatizer in this language confirms the adherence of the case study in Sardinian. However, an ongoing project [46] aims to manually annotate and lemmatize dialogue parts to create a training dataset for a Sardinian-language lemmatizer."}, {"title": "Language Variant Identifier.", "content": "A language variant identifier is a tool designed to detect and distinguish language variants, such as dialects or regional forms, within a text. This enables the precise analysis of multivariant texts, adapting language resources to the specific context. Such a tool is invaluable for the languages described in this paper, as it enables the mapping and preservation of local, poorly documented variants. Recognizing and classifying these variants can facilitate the development of richer and more representative language resources.\nSuch tools are not particularly widespread as not all languages possess profoundly different variants from one another. However, some works have attempted to provide a direction for identifying minority languages [22]. Other works have proposed algorithms based on Markov models [13] or Monte Carlo methods [47] for language identification in multilingual texts. The use of high-performance technologies for this task would facilitate the massive acquisition of data from sources such as the Web or large databases, a task not suitable for manual work."}, {"title": "2.2.3 Speech Processing", "content": "Speech Processing is a field of NLP that deals with analyzing, interpreting, and generating spoken language. It includes two main technologies: speech-to-text (s-t-t), which transcribes speech into written form, and text-to-speech (t-t-s), which transforms a written text into a synthetic voice. These tools make it possible to create speech interfaces, improve accessibility, and automate speech understanding. This area of language processing represents a strategic resource for minority languages, as the creation of s-t-t and t-t-s models, even for languages with limited digital resources, can contribute to their preservation and dissemination. It also improves access to educational and communicative tools for communities that use these languages, increasing their presence in the digital world.\nOne of the most popular approaches to construct a speech-to-text for a resource-poor language is integrating a translator into the audio-to-text transformation process. This method was adopted in the work of Bansal et al. [3], in which a neural encode-decoder model learns to translate foreign speech directly in a scenario where data and computational resources are scarce. Similarly, it has been shown that automatic speech-to-text translation improves significantly for data-poor languages by pre-training a model on high-resource language data [50]. The t-t-s procedure, which represents a reversal of the aforementioned methodology, is inherently more complex. Indeed, the objective is to generate audio from written text. End-to-end models for low-resource languages are discussed in the literature. The work of Tu et al. [52] demonstrates that data scarcity can be overcome by constructing t-t-s models using transfer learning techniques, whereby knowledge is transferred from a high-resource language. Research in this area also provides guidance and advice for building speech synthesis models for data-poor languages [20].\nThe Sardinian language is devoid of any tools for audio processing. It is, therefore, imperative that performant resources be generated with the utmost urgency to prevent the technological gap between Sardinian and other languages from widening further."}, {"title": "2.2.4 Machine Translation", "content": "Machine translation is a technology that employs algorithms to automatically translate text from a source language to a target language, thereby facilitating communication and access to information between different languages. This process is based on language models that analyze the structure and meaning of text from a source language and produce a translation into a target language, striving to maintain consistency and accuracy. The translation of text from low-resource languages, which are often less well documented and have few digital resources, facilitates the creation of cultural and linguistic bridges, thereby enabling their integration into the global digital landscape. Furthermore, machine translation not only contributes to the preservation and development of these languages by making content available in multiple languages and promoting greater linguistic inclusiveness but is also an indispensable tool for generating new data simply by using texts from languages other than the target language.\nAs in the case of speech processing, transfer learning is often used as a training method for translation models. In the proposal of Zoph et al. [63], this strategy is applied to a neural machine translation model to improve BLEU scores (a common evaluation metric in translation tasks) in several low-resource languages. However, other approaches propose the introduction of a prior model to training; Kumar et al. [24], for example, adopted the Zero-Shot Translation technique to train an encoder model, whereas, in the work of Baziotis et al. [4], a Language Model adds a regularization term allowing the predictions of a neural machine translation model to be validated. This strategy can be interpreted as a distillation technique; in essence, the language model teaches the neural model the target language.\nTyers et al. [53] describe the process of generating an automatic translation system from Sardinian to Italian. The authors exploit a Rule-Based Machine Translation technology to train the translator and use metrics such as Word Error Rate (WER) to evaluate its performance. In a follow-up study, the same authors provided an overview of the improvements and extensions to other languages of the created translator [21]. Their work also illustrates how a machine translation platform for low-resource languages can be crucial to language technology access, preventing minority languages from being ignored."}, {"title": "2.2.5 Generative Modeling", "content": "Generative models are Artificial Intelligence systems that can create new data from learned patterns, such as text, images, or sounds. These models learn from existing information and generate original content that follows the same rules or styles as the source material, making them useful for many creative and analytical applications. In this paper, we will focus primarily on generative language models, which can be a revolutionary resource in the context of low-resource languages. These can produce texts and resources in low-resource languages, expand the available corpus, and support the creation of educational, cultural, and technological content. They also enable the development of advanced linguistic tools for these languages, contributing to their preservation and dissemination in digital and non-digital environments.\nThe availability of a large number of data strongly constrains the construction of models of this type. As pointed out earlier, this fact severely limits the training of generative models for low-resource languages. However, some studies have proposed methods of constructing datasets suitable for pre-training Large Language Models (LLMs) in minority languages, e.g., a dataset containing high-quality material for the Hindi language [42]. Similarly, for the S\u00e1mi language, in the work of Paul et al. [44], Web resources are provided in order to create a clean dataset for training language models, even comparing some LLMs on the interpretation ability of this language. In some cases, one approach to achieving generative artificial intelligence tools in low-resource languages is to \"teach\" large multilingual language models new languages using In-Context Learning [7] or Fine-tuning [25] techniques.\nHowever, building an accurate language model for low-resource languages is still an open field of research and is under study. To the best of our knowledge, no end-to-end pipelines describe step-by-step how to build new data and train language models. The Sardinian language, like the remaining minority languages, also suffers from the lack of a dedicated language model. Although some LLMs, such as GPT4 and Llama3, are able to generate some parts of the Sardinian text, such text is often grammatically incorrect, inaccurate, and, in some cases, even made up. It is expected that a model utterly dedicated to the Sardinian language, although smaller in size than the models mentioned before, can achieve significantly higher performance."}, {"title": "3 Methodology", "content": "This section describes the devised framework, providing a general overview by presenting the high-level architecture and, subsequently, the details of each component."}, {"title": "3.1 Modules and Communication", "content": "Figure 1 depicts the architecture of the developed framework, which is characterized by the concatenation of multiple modules, each producing a result in its own right. Hence, in addition to providing a method for generating a language model for low-resource languages, AI-based tools suitable for developing and enhancing the language of interest are described and developed."}, {"title": "3.2 Cross-Module Components", "content": "This part outlines how the cross-module components mentioned above will be created."}, {"title": "3.2.1 Language and Variant Identifier", "content": "In addition to facilitating data filtering, as previously discussed, the language identification tool enables the categorizing of low-resource languages within their respective variants, allowing domain experts to access more detailed language analyses and identify aspects of the language that are not immediately apparent. From a computer science perspective, developing a language identifier involves finding a tool capable of assigning a label to a block of text (it should be noted that, in this paper, we are limited to language identification from text), i.e., the problem is addressed in a classification scenario. The proposal we present to address this problem is to construct a classifier based on pre-trained language models.\nGiven a dataset $T = \\{(t_j, l_j)\\}| j = 1, ..., n; l \\in \\{l_1,...,l_m\\}\\}$, where $t_j$ represents a chunk of text, $l_j$ represents the language, chosen from a finite number of languages, in which the text is written, the goal is to train a classifier that takes as input a generic text t and returns its correct language label. One potential methodology for achieving this objective is fine-tuning a pre-trained language model. This entails utilizing the dataset T, or a subset thereof, to instruct the selected model in the target language. Considering the availability of computing resources, a selection of larger or smaller models may be made. In the case of the Sardinian language, experiments are currently underway to assess the efficacy of the BERT [12] language model in classifying the three macro-variants that comprise the Sardinian language. The selection of this model was guided by its relative \"lightness\" in comparison to other LLMs, which allows for a more streamlined and comprehensive fine-tuning phase. However, it is expected to extend the number of labels for this model, i.e., to teach the model to distinguish Sardinian from other similar languages, such as Italian and Spanish, so these types of languages can be used for translation tasks."}, {"title": "3.2.2 Language Quality Checker", "content": "The quality checker module is indispensable for building high-performance, high-quality models. When used during the training phase of language models, it has been shown that high-quality data can increase their accuracy [58].\nAccordingly, we introduce a proper strategy to determine whether data meets quality standards. Such a task can be interpreted mathematically as a binary classification problem, i.e., given a set $T = \\{t_j|j = 1, ..., n\\}$, where $t_j$ represents a text chunk, we want to estimate a function $f$ such that:"}, {"title": null, "content": "$f(t) = \\begin{cases} \\text{high-quality} & \\text{if } t \\text{ is an high-quality text chunk} \\\\ \\text{low-quality} & \\text{if } t \\text{ is a low-quality text chunk} \\end{cases} \\text{ for } t \\in T.$"}, {"title": null, "content": "Similar to the Language and Variant Identifier described in Section 3.2.1, we propose to fine-tune a BERT model, suitably adapted to binary classification, to classify text. To do this requires a domain expert to manually annotate each text chunk and thereby create a dataset of the type $T = \\{(t_j, q_j)|j = 1, . . ., n; q_j \\in \\{\\text{high-quality, low-quality}\\}\\}$, where $t_j$ represents a text chunk and $q_j$ the associated qualitative degree. The latter would then represent the training set of the proposed model. With the correct data amount and training setting, this model can optimally discriminate input data by filtering them by quality."}, {"title": "3.3 Linguistic Modeling Component", "content": "This block represents the initial phase of generating a language model for a low-resource language. In detail, a portion of the collected data will be analyzed grammatically, providing insights into the language structure and offering practical assistance to scholars engaged in this field of study. Let us remark that, although in this paper we will focus on the PoS tagging problem, other grammatical analysis tools can be constructed following the guidelines outlined below."}, {"title": "3.3.1 Problem Formalization", "content": "Mathematically, the aim is to define a function capable of assigning a label to each word in a sentence that describes the grammatical category of the word. Given a sentence s, representable as a vector of the type $[s_1,..., s_m]$ in which each entry $s_k$ is a word or punctuation of the sentence s, and a list of PoS tags U, the goal is to estimate a function f such that:"}, {"title": null, "content": "$f(s) = \\begin{pmatrix} (s_1, u_1) \\\\ ... \\\\ (s_m, u_m) \\end{pmatrix} \\text{ where } u_1,..., u_m \\in U.$"}, {"title": null, "content": "This function associates each word with the correct tag, selected from the list U."}, {"title": "3.3.2 Resource Collection", "content": "In order to develop the model, data collection must be conducted by a domain expert, such as a linguist or glottologist. This individual is responsible for identifying which sentences are most effective at teaching an AI model the grammatical features of a language. Additionally, the data set must include consistent tags for each class. This approach helps to ensure that the training dataset is not imbalanced towards specific tags, which could negatively impact model performance and lead to overfitting issues."}, {"title": "3.3.3 Resource Pre-Processing", "content": "Subsequently, the collected resources must be transformed in order to be fed to the artificial intelligence algorithms. In particular, it is necessary for a domain expert to manually create a training dataset that contains an appropriate number of sentences. Each sentence must be broken into words or punctuation marks, and each must be associated with the correct tag. In this way, the dataset can be used to teach a model to correctly tag parts of text.\nOur contribution, concerning the Sardinian language, is the tagging of approximately 700 sentences by a domain expert, which were subsequently imported and stored into a proper digital structure through the datasets\u00b9 Python package. In doing so, we selected the Universal POS Tag\u00b2 as the tag set, and each word in the sentence was tokenized through the BERT model tokenizer to be properly prepared for the next step. Finally, the collected data set was divided into two parts: one set to be used for training the model and one to evaluate its performance."}, {"title": "3.3.4 Model Development", "content": "The subsequent phase is model training. We propose to utilize a multi-class classification model that is, however, capable of interpreting the written text and extracting information from the relationships of the words that comprise a sentence. Once more, the BERT model, renowned for its bidirectional capabilities, represents an optimal choice for fulfilling the tagging task. Furthermore, the number of weights that comprise the model is relatively modest in comparison to other models, which enables effective fine-tuning even in the absence of substantial computational resources.\nIt is worth pointing out that in our experiments related to the Sardinian language, we have fine-tuned various BERT models, including the BERT model in its large version, using the dataset described in the previous section. The configuration employed is outlined in Table 1."}, {"title": "3.3.5 Model Evaluation", "content": "In order to evaluate a model in the PoS tagging task, it is essential to employ appropriate metrics. Given that the problem has been defined as a multi-class classification task, relying on the metrics defined for such problems, namely Recall, Precision, and F1-score [19], is appropriate. For the experiments described above, the aforementioned metrics were calculated on a set of approximately 200 sentences, yielding excellent values of approximately 89% for all metrics. Nevertheless, comprehensive methodologies to enhance the values of these metrics are still under investigation."}, {"title": "3.4 Machine Translation Component", "content": "The next step is to create a model capable of producing a large number of text data to create a database sufficient for generating a language model. For low-resource languages, a clever way to procure such data is to take advantage of a translator that transforms texts in a high-resource language into the target language. In this section, we aim to explain how to build an effective machine translator; such methodology also covers the scenario where the goal is to translate text from a low-resource language to a high-resource language."}, {"title": "3.4.1 Problem Formalization", "content": "From a formal point of view, a translator from an L\u2081 language to an L2 language can be interpreted as follows. Given a set of texts T\u2081 in the L\u2081 language and a set of texts T2 in the L2 language, with L1 \u2260 L2, a translator is a function f such that:"}, {"title": null, "content": "$\\begin{array}{c} f: T_1 \\rightarrow T_2 \\\\ s^1 \\rightarrow s^2 \\end{array}$"}, {"title": null, "content": "where s\u00b9 is a sentence in the L\u2081 language and s\u00b2 its corresponding translation in L2.\nIt is clear that the one provided is an extreme simplification of what a translator represents. However, approaching the problem in this way, it is intuitable that the estimation of the function f can be constructed using pre-trained language models. The details of such interpretation will be provided in later sections."}, {"title": "3.4.2 Resource Collection", "content": "Building a suitable dataset for training a translation model is crucial for this task. The data to be collected must be texts in the low-resource language coupled, however, with corresponding translations in the high-resource language. One procedure for confirming the goodness of the collected translations is to have a domain expert view the dataset to ensure that no errors are present and that the material is of optimal quality for training a model.\nCurrently, our project is conducting an extensive research phase to create a Sardinian-Italian dataset in which pieces of Sardinian text are paired with corresponding Italian translations. The choice of Italian as a high-resource language fell on the similarity in syntax and vocabulary between this language and Sardinian, and also, for obvious geographic-political reasons, such a translator would have a major impact on the enhancement of the language. This discussion is easily generalizable to any low-resource language. Hence, constructing a dataset that pairs texts in the source language with texts in the most similar high-resource language is recommended."}, {"title": "3.4.3 Resource Pre-Processing", "content": "The data should be prepared for later use in the model training phase, as in the previous section. Since the collected texts will later be fed to a language model, it is necessary to use a tokenizer to transform the data into tokens in the most appropriate way for the model that will receive the input texts. There are several options for the tokenizer; on the one hand, a pre-trained tokenizer based on a language similar to the low-resource language can be adopted. On the other hand, the tokenizer can be directly trained for the target language [2].\nAs in previous cases, the dataset preprocessed with the described approaches will be split into two parts: one for the translation model training step and one for the evaluation step. Currently, analyses are underway to determine whether, in the case of the Sardinian language, a tokenizer of the Italian language is sufficient to represent the entire vocabulary of the minority language or whether it is more convenient to train a dedicated tokenizer from scratch. The fact that there are no language models for Sardinian implies that there is no tokenizer for this language either, so it seems more appropriate to train it."}, {"title": "3.4.4 Model Development", "content": "The following step is to define the machine translation model, which can be built through a fine-tuning phase of pre-trained LLMs [61, 62"}]}