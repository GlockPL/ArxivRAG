{"title": "Representational Alignment with Chemical Induced Fit for Molecular Relational Learning", "authors": ["Peiliang Zhang", "Jingling Yuan", "Qing Xie", "Yongjun Zhu", "Lin Li"], "abstract": "Molecular Relational Learning (MRL) is widely applied in natural sciences to predict relationships between molecular pairs by extracting structural features. The representational similarity between substructure pairs determines the functional compatibility of molecular binding sites. Nevertheless, aligning substructure representations by attention mechanisms lacks guidance from chemical knowledge, resulting in unstable model performance in chemical space (e.g., functional group, scaffold) shifted data. With theoretical justification, we propose the Representational Alignment with Chemical Induced Fit (ReAlignFit) to enhance the stability of MRL. ReAlignFit dynamically aligns substructure representation in MRL by introducing chemical-Induced Fit-based inductive bias. In the induction process, we design the Bias Correction Function based on substructure edge reconstruction to align representations between substructure pairs by simulating chemical conformational changes (dynamic combination of substructures). ReAlignFit further integrates the Subgraph Information Bottleneck during fit process to refine and optimize substructure pairs exhibiting high chemical functional compatibility, leveraging them to generate molecular embeddings. Experimental results on nine datasets demonstrate that ReAlignFit's outperforms state-of-the-art models in two tasks and significantly enhances model's stability in both rule-shifted and scaffold-shifted data distributions.", "sections": [{"title": "I. INTRODUCTION", "content": "Molecular Relational Learning (MRL) predicts the interaction between molecular pairs by mining features and properties [1], [2]. MRL has garnered significant attention in natural science research due to its applications in new material design and drug discovery [3]\u2013[5]. The molecular structure representation-based methods have advantages in MRL and demonstrate satisfactory performance in downstream tasks [6]\u2013[8]. Therefore, accurately representing molecular features becomes critical in MRL.\nThe functional compatibility of binding sites is an essential determinant of molecular relationships [9]\u2013[11]. Recent research primarily quantifies functional compatibility by calculating the similarity between substructure representations, focusing on molecular representational alignment by attention-based inductive bias [11]\u2013[13]. The molecular interaction is regarded as an essential source of inductive bias, which aligns the properties of paired molecules with the representations of the molecular core features by modeling the chemical reactions between the molecules. The core challenge lies in effectively computing inductive bias within molecular relationships. Existing methods have explored molecular-level [14], substructure-level [15], and hybrid strategy alignment [16].\nNevertheless, applying inductive bias-based representational alignment to MRL involves two key considerations: (1) Guidance from Domain Knowledge: Molecular feature representations are frequently influenced by domain knowledge in chemistry or biology [11], [17], [18]. Changes in adjacent atoms or scaffolds within the chemical space influence the chemical properties of molecular substructures [19], [20]. Most of the existing representational alignment methods [2], [21], [22] predominantly compute inductive bias using attention mechanisms, resulting in the results reflecting statistical correlations and overlooking other active atoms that may affect the properties of these substructures. (2) Dynamic Adaptability of Inductive Bias: The substructures (functional groups within molecules) involved in chemical reactions are closely related to their paired substructures [23]\u2013[25], meaning that substructures involved in different chemical reactions within the same molecule may differ. Structural shifts in functional groups and scaffolds may induce dynamic changes in the importance patterns of substructures in chemical reactions. Static inductive bias based on attention mechanisms [26], [27] tends to concentrate weights on certain substructures with specific features, failing to account for dynamic changes such as the adaptive adjustment of substructures in MRL.\nTo tackle these challenges, we first theoretically demonstrate that aligning information between core substructure pairs facilitates stable MRL. With theoretical justification, we propose the Representational Alignment with Chemical Induced Fit (ReAlignFit) to improve MRL stability. ReAlignFit generates substructure embedding through the GNN encoder. Inspired by Induced Fit theory [9], we design the Dynamic Represen-"}, {"title": "II. PRELIMINARIES", "content": "In this section, we illustrate how the attention mechanism-based inductive bias affects the stability of MRL with specific examples (Section II-A). We formally describe stable MRL (Section II-B) and conduct a theoretical analysis to reveal feasible solutions to improve MRL stability (Section II-C).\nA. Motivating Example\nThe inherent biases and errors in the inductive bias due to attention mechanisms [28] are critical factors affecting the stability of MRL. In this subsection, we explain in further detail how the error in inductive bias affects the representational alignment between molecular pairs and the stability of MRL with the example in Fig. 1.\nWe will explain the impact of inductive bias errors on the representational alignment between substructure pairs by considering the dynamic adaptation of the induction bias and the guidance of chemical domain knowledge. (1) The impact of dynamic adaptability of inductive bias on representational alignment: The core substructure determines the reaction may change depending on the paired molecule. In Fig. 1(a), when molecule A reacts with B, the reactive substructure in A is -COOH, while with C, it is -NH2. The scaffold of molecule B is \u2013OH, while that of molecule C is C=O. This variation in scaffolds results in different reactive substructures in molecule A. However, the -COOH is slightly more reactive than -NH2 in chemical reactions. The data co-occurrence model of the attentional mechanism causes the inductive bias to overly focus on -COOH and ignore the -NH2, which are the true drivers of chemical reactions. This phenomenon indicates that attention-based inductive bias lacks dynamic adaptability to paired molecules in MRL, making it difficult to identify and align the representations of core substructures involved in chemical reactions. (2) The guidance of domain knowledge on representational alignment: The properties of molecular substructures are susceptible to influence by surrounding atoms. As shown in Fig. 1(b), the strong electron-adsorption group -Cl in molecule E enhances the reactivity of-CO. In the esterification reaction, the \u2013COCl in molecule E will replace -CO and react with the hydroxyl group in alcohols. Inductive bias lacking guidance from domain knowledge usually assigns higher weights to more frequently occurring -CO, hindering the alignment of representations for core substructures that drive chemical reactions. Therefore, a key challenge in enhancing the stability is how to dynamically align the representations of core substructures under the guidance of chemical domain knowledge.\nB. Stable Molecular Relational Learning\n1) Molecular Representation and MRL: We first introduce the problem definitions of molecular representation and MRL in detail. For any molecule G, it can be represented as G = (V,E,X, A). Here, V = {V1, V2,\u2026\u2026, VN} denotes the set of nodes. E \u2208 N \u00d7 N represents the connections between atoms within the molecule, which is closely related to the adjacency matrix A. If (vi, vj) \u2208 E, then Aij = 1; otherwise, Aij = 0. X \u2208 RB\u00d7N is the feature matrix, consisting of the atom feature representations. For a given molecular pair (Gx, Vxy, Gy), the objective of MRL is to construct a model FMRL = (FEnd, FPred) that generates molecular embedding representations Hr and Hy through the encoder FEnd, and predicts the relationship Yxy between moleculars with the classifier Fpred.\n\n\u0176xy=FMRL(Gx, Vxy,Gy)=FPred(FEnd(Gx,Gy),Vxy) (1)"}, {"title": "2) Stable MRL", "content": "We further give a detailed definition of Stable MRL based on MRL. Given the dataset D = {Dtra, Dval, Dtes}, Dtra, Dval, Dtes are the training set, validation set, and test set, respectively. In the case of data distribution bias, the data distribution in Dtra, Dval, Dtes are different from each other. Formally, S(Dtra) \u2260 S(Dval), S(Dtra) \u2260 S(Dtes), S(Dval) \u2260 S(Dtes). The specific setup and partitioning of Dtra, Dval, Dtes will be described in detail in Section IV-A. Stable MRL aims to train a model FMRL = (Frad, Fale Flared, Freed) using Dtra and Dval that generalizes well to Dtes. The model FMRL should maintain relatively stable predictive performance in different distribution shifts.\n\nFMRL\n{FPred(Gx, Vxy, Gy) \u2013 Fered(Gu, Yuv, Gv)}\ns.t.Gx, Gy \u2208 Dval, Gu, Gv \u2208 Dtes\n (2)\nTherefore, ReAlignFit aims to learn stable molecular representations consisting of core substructures, thereby improving the model's predictive performance."}, {"title": "C. Theoretical Analysis of Stable MRL", "content": "Considering that modeling molecular relationships solely from the data perspective easily leads to instability in the model, we introduce the Induced Fit theory in chemistry to identify key factors affecting the stability of MRL at the theoretical analysis stage.\n1) Theoretical Analysis: The Induced Fit theory describes the dynamic mechanism of specific molecular binding. It emphasizes that matching binding sites (representation similarity between paired substructures) is critical for enhancing binding stability [9], [29]. Inspired by the Induced Fit theory, we attempt to analyze the contribution of matching between substructure pairs to MRL performance stabilization on the theoretical level.\nTheorem II.1. Given the molecular pair (Gx,Gy) and the prediction target Y, where the substructure Gs of G consists of core substructure G\u00ba and confounding substructure Gn. For VGx, Gy \u2208 G, according to the law of conditional probability, P(Gx,Gy; V) \u2265 P(G\u00ba;Y|G", "that": "nP(Gx, Gy; V)-P(G, G\u2084; V)+P(G; G\u2122)+P(GG)\u2264\u2208 (3)\nwhere P(Gx, Gy; V) is the true probability between molecular pair and the prediction target. P(G,G; V) is interaction probability between core substructure captured by model learning and prediction target called learning probability. P(G\u00a3;G) and P(G; G) are the confounding probabilities between core and confounding substructures.\nThe & measures the similarity between true probability, learning probability, and confounding probability. The discrepancy between true and learned probabilities is derived from the task's prediction loss, whereas the degree of calibration between substructure representations quantifies the confusion probability. Theorem II.1 demonstrates that when \u03b7 is sufficiently small, meaning the prediction loss and confusion probability are small enough, the relationships between molecular pairs can be stably represented. Inspired by Theorem II.1, we optimize model learning by combining prediction loss and confusion probability. This approach guides us in identifying core substructures by substructure representational alignment while reducing the impact of non-core substructures on molecular representations, thereby improving the stability of MRL.\n2) The Proof of Theorem II.1: Since P(G; VG) is nonnegative and Gr, Gu have little effect on Y, according to the law of conditional probability, P(Gx, Gy; V) is expressed as:\nP(Gx,Gy; V) = P(G\u00a3; V) + P(G; V|G\u00a3, GH, G\u2084)\n+P(G; V\\G\u00a3) + P(G\u2084; V|G\u00a3,G\u2122,) (4)\n> P(G\u00a3; V) + P(G; V|G)\nP(G; V) represents the probability between G and Y, which contains at least the conditional probability PG; VG). We can obtain P(Gx,Gy;V) > P(GE; Y|G", "probability": "nP(GE; Y|G", "5)": "P(G\u00a3; V) + P(G\u2084; V) = P(G\u2084, G; V)\nBased on Eq (5), and considering that P(G; Gr) and P(G\u2084;G", "as": "nP(Gx,Gy; V)-P(G\u00a3,G\u00a2;V)+P(G\u00a3;G)+P(G\u2084;G)\u22650 (6)\nTherefore, by increasing P(G, G; V) and decreasing P(G\u00a3;G) + P(G; G) (i.e., increasing the correlation between the core substructures captured by the model and prediction targets, and decreasing the impact of the confounding substructures on the core substructure representation), we can ensure that there exists a minimal positive number & that satisfies the following relationship:\nP(Gx, Gy; V)-P(G, G\u2084; V)+P(G\u00a3; G)+P(G; G)|\u2264\u20ac (7)"}, {"title": "III. METHODOLOGY", "content": "Inspired by Theorem II.1, we propose ReAlignFit to enhance the stability of MRL by simulating the dynamic mechanism of molecular binding, as shown in Fig. 2. Based on Substructure Representation-based Interaction Network (SRIN) in Fig. 2(a), we design the Dynamic Representational Alignment Module (DRAM), as shown in Fig. 2(b), which employs BCF and S-GIB to align the core substructure representations (Section III-A). Additionally, we detail the transformation and solution process of Subgraph Information Bottleneck (S-GIB) (Section III-B) and analyze the computational complexity of ReAlignFit (Section III-C).\nA. Representational Alignment with Induced Fit\nInduced Fit suggests that chemical reactions between molecules are realized by combining the pre-adaptation of substructures and dynamic adjustments during binding [9]. Guided by it, we design SRIN to simulate the pre-adaptation process. In DRAM, BCF simulates the molecular induction process to align substructure representations, while S-GIB identifies and refines substructures with high chemical functional compatibility."}, {"title": "1) Substructure Representation-based Interaction Network", "content": "Considering the irregularity of molecular spatial topologies and the effectiveness of GNN in dealing with topological features [8], [30], ReAlignFit utilizes the GNN encoder and an adjacency aggregation strategy to generate embeddings for irregular substructures.\nInitially, molecule G represented by SMILES [31] are converted into GNN-compatible representations by RDKit [32]. Subsequently, we use neighbourhood feature weighting consisting of adjacency matrix E, node feature representation Z(vn), and neighbourhood structural coefficient C as the input for irregular substructure generation. The node feature representation Zl+1(vn) in (l + 1)th layer and neighborhood structural coefficient C are defined as follows:\nZl+1 (vn) = \u03a3\u03c5\u03bcEN(vn) (WuZ\u00b2 (vu) + WnZ\u00b9 (vn))\nC =\n\u03c3(Z(vn)) \u00b7 log \u03c3(Z(vn))\n\u03a3\u03c5\u03bc\u0395\u039d (\u03bd\u03b7) \u03c3(Z(vu)) \u00b7 logo(Z(vu))\n(8)\nwhere N(vn) denotes the set of neighbor nodes of node vn. W is the weight matrix and o is the activation function. Z(*) is the embedding representation obtained from the GNN encoder, which can be selected from GIN [33], MPNN [34], GAT [35], or GCN [36]. For molecule G, its irregular substructure Z(GS) is derived by aggregating the central node un and its K-hop neighbours wh, weighted by C:\nK\nZ(G) = \u03a3\u03ba=1 \u03a3\u03c5\u03bc (Z(vn)||CZ(vu))\n(9)\nIn the subsequent step, we calculate the interaction probability between substructures by Eq (10) to simulate the pre-adaptation process of substructures. We further optimize the molecular substructure representations by noise elimination with Eq (11) results. The substructure interaction probability for pre-adaptation is calculated as follows:\n\nR\u2081=\u2211(Z(G), Z(G))+((G), Z(G)) (10)\nGEG\nGk G\nwhere \u03c3() is a probability function. Go and G denote the substructures of the paired molecules (Gx,Gy). G\u2081\u2084 represents the substructure set of molecule Gy, containing J elements. ReAlignFit eliminates unimportant substructures (i.e., those with lower Ri) via the Ri and the noise rejection function A to control the effect of confounding information on substructure representations while generating the optimized substructure representation Z(G's):\nZ(G's) = h(G\u00b3, di, \u03b7) = diZ(Gsi) + (1 \u2212 \u03bb\u03af)\u03b7\n\u5165\u00bf ~Bernoulli(o(R\u2081)), \u03b7 ~ (\u03bcz(G*), oz(98)) (11)\nZ(Gs)\nwhere \u03bcz(G) and 02(98) are mean and variance of Z(G), respectively. The refined Z(G1s) and Z(G1) are utilized to dynamic representational alignment."}, {"title": "2) Dynamic Representational Alignment Module", "content": "Representational alignment methods introducing inductive bias via attention mechanisms rely on static combinations between substructures, ignoring dynamic adjustments in reactions. This limits their ability to capture core substructures driving different chemical reactions. The Induced Fit theory highlights conformational changes of paired substructures, i.e., the dynamic variations of substructure topologies and binding sites during molecular interactions. Therefore, we design the BCF to dynamically adjust substructure binding sites and representational alignment while integrating the S-GIB to identify highly functional compatible substructures (core substructures).\nConsidering that GNN-based molecular representation methods may ignore the connecting mechanisms of the chemical bonds, we employ a substructure edge reconstruction strategy in DRAM to represent substructure information as accurately as possible. Specifically, we remove the edges between all substructures Gs in G at the initial stage. DRAM computes the Bernoulli distribution eik between substructures Gsi and Gsk by edge sampling, and reconstruct the edges with Cik = 1. Cik ~ Bernoulli(\u03b8ik), where dik is computed using a Gaussian Kernel function. The new substructure obtained in reconstruction is denoted as Gnews.\nAfter edge reconstruction, we design the Bias Correction Function y to quantify the degree of alignment between substructures before and after reconstruction. The computation of y is provided in Eq (22).\n\n2||o(Z(Gnews), Z(G)) ||2\n\u03a3\nJJ(||o (Z(G), Z(G)||+||\u2642(Z (G\u00a3*), Z(G) ||2)\nj<J\n(12)\nwhere Gnews is reconstructed from the substructures Gi and Gok of molecule Gr. Grepresents the substructure of molecule Gy paired with Gx.\nIn the Induced Fit theory, conformational changes aim to achieve functional compatibility at binding sites, which can be approximately quantified through similarity in vector space [29]. Therefore, we employ cosine similarity to compute the components of y, quantifying the chemical functional compatibility between substructures.\nDRAM dynamically determines whether to perform edge reconstruction and align substructure representation. based on \u03b3. The substructure representation after dynamic adjustment is defined as:\n\nG =\n{\nif y\u2265 1, Gnews\nif y < 1, { if oij \u2265 Okj, Gsi\nif oij < Okj, GSk\nx\n (13)\nwhere \u03c3ij = ||\u2642(Z(G*), Z(G))||2. The substructure G after dynamic adjustment of Gy can be computed similarly.\nConsidering the advantages of the Graph Information Bottleneck (GIB) in graph optimization and the specific requirements of MRL tasks [5], [15], [37], we further extend the optimization objective of GIB to the subgraph level for core substructure selection and optimization. The refined aim is defined as follows:\nDefinition III.1. (S-GIB) Given a set of graphs and their interaction relationships (Gx, V,Gy), (G, G\u2084) and (G, G)\nare the core subgraph pairs and confounding subgraph pairs of (Gx, Gy) and Y, respectively. The subgraph G\u00b3 = {G\u00a9,Gn}. According to the minimal sufficient principle of mutual information, the optimization objective is defined as:\nG\u00ba\u00ba=argmin(&I(G,G)+BI(G\u2084,G)-I(V; G,G)) (14)\n-I(V; G, G\u2084) allows the model to fully learn the information of core subgraphs relevant to the prediction target. aI(G&, G) + BI(G\u00fc, G\u00fc) minimizes the influence of confounding subgraphs on core subgraphs by eliminating confounding information. The detailed solution process for S-GIB will be presented in Section III-B.\nWe regard core substructures as core subgraphs in S-GIB and non-core substructures as confounding subgraphs. During the optimization of S-GIB, core substructures representing stable molecular representations are identified, while the influence of other substructures on molecular representations is minimized. Through the synergy between Eqs (12), (13) and (14), ReAlignFit dynamically selects and aligns the embedded representations of core substructures determining chemical reactions with full consideration of paired molecules.\nThe core substructure optimized by S-GIB is denoted as Geo. The final embedding representation H is generated by aggregating the core substructures of the molecules.\nH = Readout(Z(G\u00ba\u00ba1)||\u2022\u2022\u2022 ||Z(GCOM)), \u041c \u00ab N (15)\nwhere M is the number of core substructures."}, {"title": "B. Model Optimization and S-GIB Solution", "content": "We design the loss function L with Theorem II.1 and demonstrate the transformation and solution process of S-GIB.\n1) Model Loss Function: We design the model optimization objective L consistent with the derived conclusion in Theorem II.1, which consists of the sum of prediction loss Lpred and calibration loss LKL.\nL = Lpred + ALKL + BLKL (16)\nwhere a and \u03b2 are hyperparameters that consistent with the settings in Eq (14). To reduce the complexity of mutual information calculation, we transform Eq (14) as follows.\n2) The Upper Bound of &I(G,G) + BI(G\u2084,G) in Eq (14): We utilize information-theoretic principles to derive the upper bounds of I(G, Gr) and I(GG, G), respectively.\nProposition III.1. (Upper bound of I(G, Gn)) Since Go, Gn are subgraphs of Gx, according to the information transfer-ability of Markov chain, we have\nI(G,G)\u2264min(I(G\u00a3, G\u00a3),I(G\u2122, Gx))\n=\nmin/fpG/Gx,y)log(\n[P(GG)\np(GG) log(\np(G|Gx, y)\np(Gm)\nPG|Gx,y))dGdG\n)dGdGx) (17)\n:= min(KL(p(G\u00a3|Gx,y)||p(G\u00a3)),\nKL(p(G|Gx, Y)||p(G))))\n= min(CKL, LKL)\nwhere p() is a posterior distribution function.\nTherefore, minimizing C\u2081 = min(LKL, LKL) provides an upper bound for the minimization of I(G, G). Similarly, the upper bound for minimizing I(GG, G) can be obtained by minimizing LKL = min(LKL) CKL\u2084).\n\u03b2\nThe proof of Eq (17). The I(G,G) is expressed in integral form as:\nI(G, Gx)=\n[ P(G, G\u2082) log(PCC)\np(G)p(Gx)\n-)dGdGx (18)\nThe probability function y in Eq (12) is utilized to adjust the conditional probability distribution p(G, Gx), and G, Gr are conditionally independent of G. We can obtain p(G|Gx) = p(G|Gx, y). Further, I(G, Gx) is expressed as:\nI(G, Gx)=\nffp(G/Gz,y) log(\nx\np(G\u00a3|Gx,y)\np(G)\n:= KL(p(G\u00a3|Gx,y)||p(G\u2084))\n= CKL\n)dGdGx (19)"}, {"title": "The result of Eq (19) indicates that", "content": "I(G,G) can be computed by the KL divergence between G and G. Similarly, I(G,Gx) = KL(p(G|Gx,y)||p(G)) = LKL\u201e\u00b7\nFinally, we have the following equation as:\nI(G, G) \u2264 min(LKL, LKL) (20)\nThe solution to Eq (17). To reduce the difficulty of mutual information computation in Eq (17), we decompose probability distributions p(Gc|G, \u03b3) and p(GC) into variational approximation and multivariate Bernoulli distribution, respectively.\nGsi\nWe redefine p(G) using the variational approximation w(G\u00ba), represented by lik. w(GC) = (1 - eGi/Gk).\nInspired by [38], we parameterize p(G|Gx, y) as a multivariate Bernoulli distribution:\np(G|Gx,y) = \u03a0\u03a1. \u03a0 (1-p) (21)\nGEG\nG&G\nwhere p is the probability distribution given G and y, which can be computed via p(G|G,Gx).\n\nP=p(G|G,Gx)=\u03c3(Z(G\u00a3),(Z(Gn1)||\u00b7\u00b7\u00b7||Z(Gni))) (22)\nwhere (*) is a sigmoid function. Therefore, the LKL in Eq (17) is calculated as follows:\n\nlog pe +(1-p) log 1-P\nLKL = KL[p(G\u00a3|G,Gx)||w(G\u00b0)]\n=\u03a3\ne 1-e\n(23)\nSimilarly, we can obtain the LKL, LKLy and LKLY\n3) The Lower Bound of I(V;G,G) in Eq (14): We compute the lower bound of I(V; G, G) using the given graph pair (Gx, Gy) and the label Y.\nProposition III.2. (Lower bound of I(V; G\u00a3,G\u2084)) Given a graph pair (Gx, Gy), its label information Y, and the learned core subgraphs (G, G\u2084), we have:\n\nIca= I(V; G\u00a3, G\u2084)\n= H (V) + fp(V) [[ Fea(Go, G|V)dGdGdy\n>\n:=fp(V)(ff Fea(G\nca\n, G|V)dGdG\u2084)dy\n1 \u03a3\u03a3 (Gan,Gym)|Y)\nNM\nn=1 m=1\n= -Lpred\n(24)\nFea(G, G|V) = q(G, G|V) log(P(9)\nq(G, G\u2084)\nwhere q(G, G|V) is the variational approximation distribution used to approximate the posterior distribution p().\nThe Eq (24) indicates that minimizing the prediction loss Lpred achieves the minimization of \u2212I(V; G, G\u2084).\nThe proof of Eq (24). For the term Ica = I(V;G, G\u2084), by definition:\n\nIca=[[[p(V, G,G)log(\nP(V, G, G\u2084),\np(V)\n)dydGdg (25)\nWe introduce the variational approximation distribution q(G,G|V) to approximate the conditional probability distribution p(G, G|V). Then, the Ica is expressed as:\n\nIca=fp()ffq(G,GV) log( P(V, G, G\u2084)\n)dYdGdG\nq(GG)\n=fporff q(V, GG|V)log(p(V)p(G&Gq|V)\n(G)\nlog( dydGdG\nq(GG)\n=fp(V) (logp(V) + [[ Fea(Go, G|V)AGAG;)dV\n=fp(y) log p(y)dy\n+fp(V)][Fea(Go, G|V)dGdG)dy\n= H(V)+fp(V)/Fca (G,G|V)dG\u00a3dGqdY\n(26)\nH(V) is the entropy of Y. Finally, we have the following equation as:\n\nI(V;G,G) \u2265\nfp(V) [[ Fea (G\nFea(Go, GV)dGdGdy\n(27)\nThe solution to Eq (24). The prediction term q((GG) V) in Eq (24) emphasizes leveraging the core substructure pair (GG) to predict the relationship between molecules Gr and Gy.\nAccording to Eq (15), the embedding representations of molecules Gr and Gy, composed of their core substructures, are expressed as Hr and Hy, respectively. Finally, the prediction loss Lpred of ReAlignFit is denoted as:\n-1\nLpred = MNE(G,Gy)~y [log(\u03c3(Hx, Hy))|V] (28)\nIn MRL, Lpred can be chosen as the cross-entropy loss for Drug-Drug Interaction (DDI) prediction or the mean absolute error loss for Molecular Interaction (MI) prediction.\nC. Model Analysis\nWe train ReAlignFit by the iterative optimization between SRIN and DRAM. The pseudocode is provided in Algorithm 1. The computational complexity of ReAlignFit is mainly derived from iterative optimization. The complexity of SRIN is O(T\u2081\u00b7L\u00b7|N|2), and that of DRAM is O(T2\u00b7 J\u00b2). T represents the number of iterations, L is GNN layers. Consequently, the overall computational complexity of ReAlignFit is approximately O(T\u2081\u00b7L\u00b7 |N|\u00b2 + T2 \u00b7 J\u00b2). Owing to J \u226a N, the added computational overhead is still manageable."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we validate the prediction performance of ReAlignFit in two types of tasks on nine datasets. By analyzing and summarizing the relevant experiments, we aim to answer the following research questions:\n\u2022 RQ1: How does the performance of ReAlignFit in MRL, and whether is it susceptible to backbone?\n\u2022 RQ2: Can ReAlignFit improve the stability of MRL in distribution-shifted data?\n\u2022 RQ3: What is the key to ReAlignFit's performance improvement?\n\u2022 RQ4: How does confounding information affect the performance of ReAlignFit?\n\u2022 RQ5: Can the results of ReAlignFit be visually supported?\nA. Experimental Setup\n1) Datasets: Following the related research [2], [8], [15], [39], we conduct extensive Molecular Interaction (MI) prediction and Drug-Drug Interaction (DDI) prediction experiments on nine datasets, as detailed in Table I. Chromophore [40], MNSol [41], FreeSolv [42], CompSol [43], Abraham [44], and CombiSolv [45] are chromophore datasets used to describe the free energy of solutes in solvents. These six datasets are widely used for MI prediction [2], [8], [15]. ZhangDDI [46], HetionetDDI [47], and DrugBankDDI (pay version) [48] are commonly utilized for DDI prediction [30]. Following the setup of related work in MRL [2], [15], we divide the datasets into training, validation, and test sets with the ratio of 6:2:2. For DDI datasets that contain only positive examples, we generated negative samples using rule matching and scaffold clustering methods, respectively. To better simulate the real-world data distribution, we set up in-distribution data (Original) with the same distribution as the original data and two different distribution-shifted data (P1 and P2) for DDI prediction, which were used for ReAlignFit learning.\n\u2022 In-Distribution Data (Original): The original data generated negative samples based on rule matching and is randomly divided into training, validation, and test sets.\n\u2022 Rule-based Partitioning (P1): We generated negative samples for original data based on rule matching and partitioned them by ID to ensure that at least one drug does't repeat in training, validation, and test sets. P1 simulates application scenarios of discovering unknown interactions in existing molecules, such as drug repurposing.\n\u2022 Scaffold-based Partitioning (P2): We generated negative samples for the original data based on scaffold clustering. We used METIS [49] to iteratively partition the drug interaction graph, ensuring that molecules in training, validation, and test sets are entirely distinct. P2 simulates scenarios of discovering interactions between previously unknown molecules, such as drug discovery.\n2) Evaluation Metrics: In this study, we consider MI prediction and DDI prediction as regression and classification tasks, respectively. Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) are commonly used metrics in regression prediction. Considering the high positive correlation between RMSE and MAE, we employ RMSE for performance evaluation in MI prediction. We assess model performance for DDI prediction based on classification evaluation metrics, including Area Under the Receiver Operating Characteristic Curve (AUROC), Accuracy (ACC), F1-score (F1), Precision (Pre), and Area Under the Precision-Recall Curve (AUPR).\n3) Training Details: In all experiments, we employ a three-layer MPNN as the encoder to extract molecular feature representations and train ReAlignFit based on two NVIDIA GeForce RTX 4090 24G GPUs. We optimize the parameters using the Adam optimizer and train the model for 50 epochs. The batch size is 128, and the dropout rate is 0.1. For other hyperparameters, we choose from a specific range: learning rate lr \u2208 {0.01, 0.005, 0.001, 0.0005, 0.0001}, both a and \u03b2 are selected from {0.5, 0.3, 0.1, 0.01, 0.001}, and the iterations is searched from {1, 3, 5, 10, 15}. The code of ReAlignFit will be publicly available after the paper is accepted.\nB. Overall Performance (RQ1)\nWe compare ReAlignFit with four backbone models four backbone models (GCN [36], GAT [35], MPNN [34], and GIN [33]), three molecular representation-based models (CI-GIN [50], MIRACLE [26], and MMGNN [8]), and five"}, {"title": "C. Stability Analysis (RQ2)", "content": "In this section", "follows": "nRPDM =\nEva - Evari\nEvari\n\u00d7 100%\n(29)\nPi\nM\nwhere Eva represents the evaluation metrics. Eva\nand Evari denote the prediction performance in\nthe original and drift distributions", "Analysis": "Experimental results show that data distribution variations significantly impact the predictive performance of models. ReAlignFit maintains the best stability in different data distributions compared to methods that do not consider representational alignment. Compared to CGIB, ReAlignFit improved ACC prediction stability by at least 4.5% and 6% in two data distribution scenarios. ReAlignFit exhibits the most minor performance degradation and achieves the highest stability in varying data distributions. ReAlignFit's performance degradation ranges from 6% to 18% in the P1 scenario, with certain evaluation metrics showing only around 25% decline in P2. In contrast, CGIB and CIGIN experience performance fluctuations exceeding 35% in most metrics on P2. These findings show that stability is the critical factor in the success of ReAlign"}]}