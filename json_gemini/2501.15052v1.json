{"title": "Graph-Based Cross-Domain Knowledge Distillation for Cross-Dataset Text-to-Image Person Retrieval", "authors": ["Bingjun Luo", "Jinpeng Wang", "Zewen Wang", "Junjie Zhu", "Xibin Zhao"], "abstract": "Video surveillance systems are crucial components for ensuring public safety and management in smart city. As a fundamental task in video surveillance, text-to-image person retrieval aims to retrieve the target person from an image gallery that best matches the given text description. Most existing text-to-image person retrieval methods are trained in a supervised manner that requires sufficient labeled data in the target domain. However, it is common in practice that only unlabeled data is available in the target domain due to the difficulty and cost of data annotation, which limits the generalization of existing methods in practical application scenarios. To address this issue, we propose a novel unsupervised domain adaptation method, termed Graph-Based Cross-Domain Knowledge Distillation (GCKD), to learn the cross-modal feature representation for text-to-image person retrieval in a cross-dataset scenario. The proposed GCKD method consists of two main components. Firstly, a graph-based multi-modal propagation module is designed to bridge the cross-domain correlation among the visual and textual samples. Secondly, a contrastive momentum knowledge distillation module is proposed to learn the cross-modal feature representation using the online knowledge distillation strategy. By jointly optimizing the two modules, the proposed method is able to achieve efficient performance for cross-dataset text-to-image person retrieval. Extensive experiments on three publicly available text-to-image person retrieval datasets demonstrate the effectiveness of the proposed GCKD method, which consistently outperforms the state-of-the-art baselines.", "sections": [{"title": "Introduction", "content": "Person retrieval is a fundamental problem in the field of video surveillance, which has attracted extensive attention from both scientific research and practical applications (Jing et al. 2020; Li et al. 2023). Text-to-image person retrieval is a subtask of person retrieval, which aims to retrieve the target person from the image gallery that matches the given text description (Li et al. 2017; Yan et al. 2023). This task is associated with both image-text retrieval (Li et al. 2021) and image-based person retrieval (He et al. 2021). Different from image-based person retrieval, text-to-image person retrieval is able to retrieve the target person only based on the given query text, which is more user-friendly and easily accessible than using query image (Jiang and Ye 2023; Bai et al. 2023). Thus, text-to-image person retrieval has attracted increasing attention in recent years (Zhu et al. 2021; Shu et al. 2022; Zhu et al. 2024). With the advancement of deep learning technology (Fan et al. 2024; Gu et al. 2020), the performance of text-to-image person retrieval has significantly improved. Recently, large language models (Radford et al. 2021; Li et al. 2021; Shao et al. 2024) have further enhanced this performance.\nHowever, most existing text-to-image person retrieval methods are trained in the supervised manner, which requires a large amount of high-quality annotated image-text alignment datasets in the target application scenario. These methods usually rely on large-scale alignment data collection and manual annotation, which are difficult to obtain and expensive. This limits the generalization of these methods in practical applications (Jiang and Ye 2023; Yan et al. 2023).\nA feasible solution is to adopt the transfer learning paradigm, which transfers the knowledge learned from the existing large-scale supervised datasets to the target unlabeled dataset, i.e., cross-dataset text-to-image person retrieval as shown in Fig. 1. In the cross-dataset text-to-image person retrieval task, there are mainly two challenges: 1) Domain shift. For the cross-domain transferring task, an important challenge is the domain shift, i.e., the distribution difference between the source and target datasets. 2) Modality gap. Another important challenge for this task is the modality heterogeneity between visual and textual modalities (Jiang and Ye 2023). In the visual modality, data is usually high-dimensional and continuous, where differences mainly lie in the background, pose, lighting, etc. In contrast, in the textual modality, data is usually low-dimensional and discrete, where differences mainly lie in semantic ambiguity and order. In the cross-dataset scenario, due to the lack of text-image matching annotations on the target dataset, the modality heterogeneity will be further exacerbated. In the cross-dataset text-to-image person retrieval task, the superposition of these two challenges makes the performance of existing text-to-image person retrieval methods decrease in cross-domain scenarios, making this task more difficult.\nTo address the above challenges, we propose a novel unsupervised domain adaptation method for cross-dataset text-to-image person retrieval, named Graph-based Cross-domain Knowledge Distillation (GCKD). The proposed method is based on the vision language pre-training models, which have shown great power in the field of text-to-image person retrieval recently (Yan et al. 2023; Bai et al. 2023; Jiang and Ye 2023). The proposed GCKD method consists of two novel components. The first component is a graph-based multi-domain propagation (GMP) module, which aims to address the domain shift problem by propagating feature information between the source and target domains on the dynamic cross-domain graph. The second component is a contrastive momentum knowledge distillation (CMKD) module, which aims to address the modality gap by constructing high-confidence pseudo text-image similarity labels through momentum knowledge distillation, which can guide the model to learn modal-invariant feature representation. By integrating these two modules, our method can effectively address the domain shift and modality gap challenges, and achieve more accurate and robust performance in the cross-dataset text-to-image person retrieval task.\nIn summary, the contributions of this paper are three-fold:\n\u2022 We propose a novel unsupervised domain adaptation method, i.e. GCKD, to improve the cross-domain performance of text-to-image person retrieval. To the best of our knowledge, this is the first work to adopt the vision language pre-training models for cross-dataset text-to-image person retrieval.\n\u2022 We introduce a cross-domain graph propagation mechanism to address the domain shift challenge, and a contrastive momentum knowledge distillation strategy to address the modality gap problem. These components are integrated into a unified framework to learn the cross-modal representation.\n\u2022 We conduct extensive experiments on three commonly used datasets, and the results demonstrate that our proposed method outperforms the state-of-the-art methods in the cross-dataset text-to-image person retrieval task."}, {"title": "Related Work", "content": "With the advancement of smart city (Xi et al. 2024a,b), the integration of text-to-image person retrieval is gaining increasing attention. The primary challenge in text-to-image person retrieval is cross-modal alignment, which can be categorized into two main strategies: cross-modal interaction-based and cross-modal interaction-free methods. Cross-modal interaction-based methods (Li et al. 2017; Zheng et al. 2020; Zhu et al. 2021) utilize attention mechanisms to identify local correspondences (e.g., patch-word, patch-phrase) between images and texts, predicting matching scores for image-text pairs. Notably, DSSL (Zhu et al. 2021) separates visual data into person and surroundings information for effective surroundings-person distinction. In contrast, cross-modal interaction-free methods (Chen et al. 2021; Bai et al. 2023; Shu et al. 2022) achieve high performance without complex interactions. The success of Transformer architectures in Vision and Language Tasks has led to the development of various Transformer-based models (Li, Cao, and Zhang 2022; Shao et al. 2022), such as LGUR (Shao et al. 2022), which learns granularity-unified representations for text and image modalities in an end-to-end manner.\nWhile existing methods typically require labeled data for downstream tasks, our work operates in an unsupervised setting, eliminating the need for extensive alignment data collection and manual annotation, thus demonstrating greater efficiency in cross-domain scenarios.\nUnsupervised Domain Adaptation\nUnsupervised domain adaptation (UDA) seeks to transfer knowledge from a labeled source domain to an unlabeled target domain, but its application in text-to-image person retrieval is limited. MAN (Jing et al. 2020) introduces a moment alignment network for cross-modal text-image alignment, but its reliance on CNN and LSTM limits adaptability to Transformer architectures. POUF (Tanwisuth et al. 2023) aligns prototypes and target data in latent space using transport-based distribution alignment and mutual information maximization. ReCLIP (Hu et al. 2024) presents a source-free domain adaptation method for vision-language models through cross-modality self-training with learned pseudo labels. However, both POUF and ReCLIP face modality heterogeneity issues specific to text-to-image person retrieval tasks.\nIn contrast to these approaches, our model leverages vision-language pre-training for unsupervised domain adaptation in text-to-image person retrieval, effectively addressing challenges related to domain shift and modality gaps."}, {"title": "Problem Definition", "content": "Denote $\\mathcal{D} = {\\mathcal{V}_s, \\mathcal{T}_s}$ as a well-annotated text-to-image person retrieval dataset from the source domain. The source dataset $\\mathcal{D}_s$ consists of a labeled image set $\\mathcal{V}_s = \\{(v_s^{(i)}, y_s^{(i)})\\}_{i=1}^{M_s}$ and a labeled text set $\\mathcal{T}_s = \\{(t_s^{(j)}, y_s^{(j)})\\}_{j=1}^{N_s}$ respectively, where $v_s^{(i)}$ and $y_s^{(i)}$ are the image and corresponding identity label of the i-th visual sample, and $t_s^{(j)}$ and $y_s^{(j)}$ are the text and corresponding identity label of the j-th textual sample. Denote $\\mathcal{D}_t = {\\mathcal{V}_t, \\mathcal{T}_t}$ as an unlabeled dataset from the target domain. The target dataset $\\mathcal{D}_t$ only contains unpaired images $\\mathcal{V}_t = \\{v_t^{(i)}\\}_{i=1}^{M_t}$ and texts $\\mathcal{T}_t = \\{t_t^{(j)}\\}_{j=1}^{N_t}$, where $v_t^{(i)}$ and $t_t^{(j)}$ are the i-th image and j-th text sample in the target dataset, respectively. Given the labeled source dataset $\\mathcal{D}_s$ and the unlabeled target dataset $\\mathcal{D}_t$, the goal of cross-dataset text-to-image person retrieval is to learn a model that can effectively retrieve the target person image $v_t$ for each given text query $t_t$ in the target domain."}, {"title": "Our Model", "content": "In this section, we present the proposed Graph-based Cross-domain Knowledge Distillation (GCKD) method for cross-dataset text-to-image person retrieval.\nModel Overview\nIn this paper, we propose a novel graph-based cross-dataset text-to-image person retrieval method, named Graph-based Cross-domain Knowledge Distillation (GCKD). As shown in Fig. 2, the proposed GCKD method consists of two main components as the following. Firstly, a graph-based multi-domain propagation module is proposed to address the domain shift problem in the cross-modal retrieval task. By unifying visual and textual features from the source and target domains into a holistic cross-domain embedding graph, the module is designed to bridge the correlation and align the cross-modal features between the source and target domain. Secondly, a contrastive momentum knowledge distillation module is proposed to address the modality gap problem in the cross-domain scenario. Different from the single-domain visual language model, the module introduces momentum strategy into the domain adaptation task and proposes cross-domain fine-grained matching tasks to learn the shared representation among different modalities and domains. By jointly optimizing the two modules, the proposed model is able to achieve efficient performance for cross-dataset text-to-image person retrieval.\nGraph-based Multi-domain Propagation\nThe domain shift problem, arising from variations in image conditions (e.g., resolutions, angles) and text styles (e.g., descriptive approaches, paragraph lengths) across datasets, significantly impacts the performance of text-to-image person retrieval models. To mitigate this, we propose a graph-based multi-domain propagation module that connects features from both source and target domains. By utilizing a unified graph, this module bridges correlations and reduces discrepancies across domains.\nEmbedding Memory The embedding memory stage is designed to store the embeddings from the source and target domains. We propose two types of memory banks to store the most recent C embedding of D-dimmension from the visual and textual modalities during training. The image memory bank is further composed of the source image memory $\\mathbf{Q}_{SI} \\in \\mathbb{R}^{C \\times D}$ and $\\mathbf{Q}_{TI} \\in \\mathbb{R}^{C \\times D}$, which stores the image embeddings of source and target domains respectively. The setting is similar for the textual modal, which stores the most recent C text embeddings of source and target domains respectively, i.e., $\\mathbf{Q}_{St} \\in \\mathbb{R}^{C \\times D}$ and $\\mathbf{Q}_{Tt} \\in \\mathbb{R}^{C \\times D}$. At each training iteration, the image and text memories are updated iteratively from the recent embeddings of both source and target domains. By leveraging the memory banks and the corresponding updating mechanism, the module can capture the multi-modal embedding information across batches and even epochs, which effectively extends the scope of multi-domain propagation.\nGraph Construction For each training iteration, we are given a batch of image/text embeddings $\\mathbf{F}_m = \\{f_m^{(k)}\\}_{k=1}^{B} = \\{f_I^{(k)}, f_T^{(k)}\\} \\in \\mathbb{R}^{B \\times K}$ from target domains, where $m = I$ denotes image modality embeddings and $m = T$ denotes text modality embeddings. Based on the input embeddings and the existing source and target memories, the dynamic cross-domain graph $\\mathcal{G}$ is constructed using the K-nearest neighbor (KNN) algorithm.\nSpecifically, the graph $\\mathcal{G}$ can be determined by the vertex set $\\mathcal{V}$, the vertex embedding matrix $\\mathbf{X}$, and the adjacency matrix $\\mathbf{A}$. The vertex of the graph comes from three parts:\n$\\mathcal{V} = \\mathcal{V}_{input} \\cup \\mathcal{V}_{src\\_mem} \\cup \\mathcal{V}_{tgt\\_mem}$ (1)\nwhere $\\mathcal{V}_{input} = \\{v_i\\}_{i=1}^{B}$ is the vertex set of the given image/text batch $\\mathbf{F}_m$, $\\mathcal{V}_{src\\_mem} = \\{v_i\\}_{i=B+1}^{B+C}$ and $\\mathcal{V}_{tgt\\_mem} = \\{v_i\\}_{i=B+C+1}^{B+2C}$ are the vertex set of the source and target memories $\\mathbf{Q}_{Sm}$, $\\mathbf{Q}_{Tm}$, respectively. The vertex embedding matrix $\\mathbf{X} \\in \\mathbb{R}^{(B+2C) \\times K}$ is constructed by concatenating the input embeddings and the embeddings from the source and target memories:\n$\\mathbf{X} = (\\mathbf{F}_m^T, \\mathbf{Q}_{Sm}^T, \\mathbf{Q}_{Tm}^T)$ (2)\nThe adjacency matrix $\\mathbf{A} \\in \\mathbb{R}^{(B+2C) \\times (B+2C)}$ is dynamically computed by the KNN algorithm based on the cosine similarity in vertex embedding matrix $\\mathbf{X}$:\n$\\mathbf{A}_{ij} = \\begin{cases} 1, & \\text{if } v_i \\text{ is one of the K nearest neighbors of } v_j \\\\ 0, & \\text{otherwise} \\end{cases}$ (3)\nBy constructing a dynamic cross-domain graph $\\mathcal{G}$ from the input embeddings and source and target memories, the module maps multi-domain embeddings into a unified graph structure, bridging multi-domain correlations.\nGNN Propagation After constructing the dynamic cross-domain graph $\\mathcal{G}$, we introduce a Graph Neural Network (GNN) to propagate embeddings across domains, effectively learning the graph structure and capturing sample correlations. Specifically, we adopt a two-layer GNN model for this propagation. The GNN propagation is performed by iteratively updating the vertex embedding matrix $\\mathbf{X}$ based on the adjacency matrix $\\mathbf{A}$ and the vertex embedding matrix $\\mathbf{X}$. For the l-th layer of the GNN, the update rule can be formulated as:\n$\\mathbf{X}^{(l+1)} = \\text{GNNConv}(\\mathbf{X}^{(l)}, \\mathbf{A}; \\Theta^{(l)})$ (4)\nwhere GNNConv(\u00b7) is the GNN convolution operation, and $\\Theta^{(l)}$ is the learnable parameters of the l-th layer. By iteratively updating the vertex embedding matrix $\\mathbf{X}$ for L = 2 layers, the last layer is used to generate the final vertex embedding matrix $\\mathbf{X}^{(L)}$, which is further used to compute the final domain-aware embeddings $\\mathbf{F}_m = \\mathbf{X}_L$ for the image/text samples in the target domain. By propagating the embeddings across the dynamic cross-domain graph $\\mathcal{G}$, the module is able to bridge the correlation among the samples from both source and target domains and reduce the domain discrepancy in the cross-dataset scenario.\nContrastive Momentum Knowledge Distillation\nThe modality difference between text and image data creates a significant challenge known as the modality gap. Existing contrastive learning methods, primarily designed for single-domain visual-language tasks, often degrade in cross-domain scenarios (Li et al. 2021). To address this, we propose a novel contrastive momentum knowledge distillation module that combines cross-modal contrastive learning with cross-domain knowledge distillation.\nCross-domain Momentum Distillation As shown in Fig. 2, the backbone model consists of an image encoder and a text encoder, referred to as the student model $\\mathcal{E}n_{Student}(\\cdot, \\Theta_{Student})$ in knowledge distillation, which extracts visual and textual features from input samples. We also create a teacher model with the same structure, $\\mathcal{E}n_{Teacher}(\\cdot, \\Theta_{Teacher})$. Both models are initialized with pre-trained weights from the source dataset, which provide rich knowledge from the source domain.\nDuring the training process, the parameters of the teacher model are not optimized by the gradient descent method, but are updated by the exponential moving average (EMA) mechanism as follows:\n$\\Theta_{Teacher} \\leftarrow m\\Theta_{Teacher} + (1 - m)\\Theta_{Student}$ (5)\nwhere $m \\in [0,1]$ is the momentum coefficient. By updating the teacher model with EMA, its parameters lag behind those of the student model, retaining more source domain knowledge. This allows the teacher model to generate pseudo labels that encapsulate source domain knowledge, guiding the student model to learn better representations in the target domain.\nCross-modal Contrastive Learning After generating pseudo labels from the teacher model, the next challenge is effective cross-domain contrastive learning for knowledge transfer and improved representations in the target domain. We propose a cross-modal image-text contrast loss that leverages source domain knowledge from the teacher model to enhance target domain representations.\nSpecifically, given a batch of paired source domain samples $(v_s, t_s)$, and unpaired target domain visual and textual samples $v_t$ and $t_t$, the student model is used to generate the pseudo target domain image features $f_{TI}$ and pseudo target domain text features $f_{TT}$. At the same time, the student model is used to extract the source domain image features $f_{SI}$ and source domain text features $f_{ST}$, and the domain-aware target domain image features $f_{TI}$ and target domain text features $f_{TT}$. The cross-domain image-text contrast loss is defined as:\n$\\begin{aligned} \\mathcal{L}_{cd-itc} = & -\\sum_{f_{TI}, f_{TT}} \\log \\frac{\\exp(d(f_{TT}, f_{TI}) / \\tau)}{\\sum_{q \\in \\mathbf{Q}_{TT}} \\exp(d(f_{TI}, q) / \\tau)} - \\\\ & \\sum_{f_{TT}, f_{TI}} \\log \\frac{\\exp(d(f_{TI}, f_{TT}) / \\tau)}{\\sum_{q \\in \\mathbf{Q}_{TI}} \\exp(d(f_{TT}, q) / \\tau)} \\end{aligned}$ (6)\nwhere\n$\\begin{aligned} s_{i2t} = \\frac{\\exp(d(f_{TI}, f_{TT}) / \\tau)}{\\sum_{q \\in \\mathbf{Q}_{TT}} \\exp(d(f_{TI}, q) / \\tau)} \\end{aligned}$ (7)\n$\\begin{aligned} s_{t2i} = \\frac{\\exp(d(f_{TT}, f_{TI}) / \\tau)}{\\sum_{q \\in \\mathbf{Q}_{TI}} \\exp(d(f_{TT}, q) / \\tau)} \\end{aligned}$ (8)\nare the pseudo similarity targets generated by the teacher model, d(\u00b7, \u00b7) is the cosine similarity function, \u03c4 is the temperature parameter, $\\mathbf{Q}_{TT}$, $\\mathbf{Q}_{TI}$ are the target text and image memory respectively. By introducing the memory banks, the contrastive loss function is extended to almost all samples in the target domain, enabling the model to explore more positive and negative samples and facilitating feature extraction.\nCross-domain Fine-grained Matching As noted in (Li et al. 2021), the image-text matching task is a binary classification problem, with positive pairs from the same identity. To enhance fine-grained matching in the target domain, we propose a cross-domain image-text matching task using target-domain positive pairs and cross-domain hard negative pairs. Positive pairs are generated from high-confidence pseudo labels from the teacher model, while negative pairs consist of the most challenging cross-domain pairs for the student model to distinguish.\nFor each target domain image feature $f_{TI}$, the positive text feature $f_{TT}$ is selected from the target text memory $\\mathbf{Q}_{TT}$ only if $d(f_{TI}, f_{TT}) > \\delta$ where \u03b4 is a predefined threshold. The negative text feature $f_{ST}$ is selected from the source domain feature batch if $f_{TI}$ has the highest cosine similarity with $f_{ST}$, i.e., $f_{TI}$ and $f_{ST}$ comes from different identities (even different domains) but have the highest similarity score. Given the image and text sample pair, the multimodal encoder in the backbone network is utilized to produce the binary matching probability $\\hat{p}(\\cdot, \\cdot)$ between the given pairs. After a softmax function, the cross-domain image-text matching loss is defined as:\n$\\mathcal{L}_{cd-itm} = -\\sum_{(f_{TI}, f_{TT})} \\log \\text{Softmax}(\\hat{p}(f_{TI}, f_{TT})) - \\sum_{(f_{TI}, f_{ST})} \\log (1 - \\text{Softmax}(\\hat{p}(f_{TI}, f_{ST})))$ (9)\nThe overall optimization objective is as follows:\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_{cd-itc} + \\lambda_2 \\mathcal{L}_{cd-itm} + \\lambda_3 \\mathcal{L}_{mlm}$ (10)\nwhere \u03bb1, \u03bb2, \u03bb3 are the coefficient hyperparameters for each loss term, $\\mathcal{L}_{cd-itc}$ is the cross-domain image-text contrast loss defined in Eq. (6), $\\mathcal{L}_{cd-itm}$ is the cross-domain fine-grained matching loss defined in Eq. (9), $\\mathcal{L}_{mlm}$ is the masked language model loss proposed by (Li et al. 2021)."}, {"title": "Experiment Setup", "content": "In this section, we introduce the experiment setup of this paper, including datasets, baselines, task settings, evaluation metrics, and implementation details.\nDatasets\nIn this paper, we conduct experiments on three publicly available text-to-image person retrieval datasets: ICFG-PEDES, RSTPReid, and CUHK-PEDES. ICFG-PEDES (Ding et al. 2021) is the largest public dataset for text-to-image person retrieval, which consists of 54,522 images from 4,102 identities in total. RSTPReid (Zhu et al. 2021) is a newly released text-to-image person retrieval dataset. The dataset comprises 20,505 images from 4,101 identities. CUHK-PEDES (Li et al. 2017) is also a commonly used dataset in the text-to-image person retrieval field, which is composed of 5 parts from different scenarios: SSM, VIPER, CUHK01, CUHK03, and Market-1501.\nBaselines\nTo comprehensively evaluate the proposed method, we follow recent works (Hao et al. 2023; Zhu et al. 2024) to select state-of-the-art baselines and assess them in two training settings: Source Only (SO) and Source and Target (ST).\nFor the Source Only (SO) setting, we select several state-of-the-art methods from the single-domain text-to-image person retrieval task, trained solely on the labeled source dataset and tested directly in the target domain. For the main experiments, the baselines include RaSa (Bai et al. 2023), APTM (Yang et al. 2023), IRRA (Jiang and Ye 2023), CFine (Yan et al. 2023), IVT (Shu et al. 2022). For the intra-dataset experiments on CUHK-PEDES, the baselines include EAIBC (Zhu et al. 2024), RaSa (Bai et al. 2023), SSAN (Ding et al. 2021), MIA (Niu et al. 2020), SCAN (Lee et al. 2018), CMPM-CMPC (Zhang and Lu 2018).\nFor the Source and Target (ST) setting, due to the lack of unsupervised domain adaptation methods for text-to-image person retrieval with available code, we select several state-of-the-art UDA baselines from related text-image multimodal tasks. These include POUF (Tanwisuth et al. 2023) and ReCLIP (Hu et al. 2024) for main experiments, and MAN (Jing et al. 2020), ECN (Zhong et al. 2019), and ADDA (Tzeng et al. 2017) for intra-dataset experiments on CUHK-PEDES.\nEvaluation Metrics and Settings\nTo quantitatively evaluate models in the cross-dataset text-to-image person retrieval task, we adopt two common metrics: Recall at Rank-K (Rank-K) and Mean Average Precision (mAP), following existing works (Bai et al. 2023; Yan et al. 2023). Rank-K measures the proportion of target person images in the top K (K = 1, 5, 10) results, while mAP reflects the mean average precision of all results. Higher values for both metrics indicate better performance.\nIn the main cross-dataset experiments, we evaluate various baselines in the cross-dataset scenario for the above 3 datasets including ICFG-PEDES, RSTPReid, and CUHK-PEDES. For each dataset as the source set, we train the baselines on the source set and test them on the other two datasets respectively. In the intra-dataset cross-domain experiments, we follow the cross-domain settings proposed by (Jing et al. 2020) within the CUHK-PEDES dataset. Specifically, we select SSM (S) as the source domain and consider 4 transfer tasks on CUHK03, Market-1501, VIPER, and CUHK01.\nImplementation Details\nThe proposed model is implemented based on PyTorch 1.10 framework on Python 3.8 and Ubuntu 20.04. For each dataset, the image-text data is split according to the existing protocol (Bai et al. 2023; Yang et al. 2023). ALBEF (Li et al. 2021) is adopted as the backbone of the vision language pre-training model and initialized with the pre-training weights on the source dataset. During training, the batch size is set to 4, and the optimizer is AdamW with an initial learning rate of le-5 and cosine scheduler strategy. The hyperparameters of the proposed method are set as follows: the number of graph layers L = 2, the number of neighbors K = 10, the temperature \u03c4 = 0.07, the momentum coefficient \u03b1 = 0.999, the loss coefficients \u03bb1 = \u03bb2 = 0.5, \u03bb3 = 1. All the experiments are conducted on NVIDIA GeForce RTX 4090."}, {"title": "Result and Analysis", "content": "In this section, we present the experimental results of our method and state-of-the-art baselines on the cross-dataset text-to-image person retrieval task. We also conduct ablation studies to analyze the effectiveness of each component.\nComparison with State-of-the-arts\nTo have a comprehensive evaluation of the proposed method, we compare it with state-of-the-art baselines on the cross-dataset text-to-image person retrieval task. As mentioned in Baselines, the compared baselines include the text-to-image person retrieval methods in the Source Only (SO) setting and the unsupervised domain adaptation methods in the Source and Target (ST) setting. The performance of the supervised training setting is also reported for reference. The results on ICFG-PEDES as the source dataset are shown in Table 1. The results on RSTPReid as the source dataset are shown in Table 2. The results on the CUHK-PEDES in intra-dataset cross-domain settings are shown in Table 3. From the results, we can make the following observations:\n(1) The proposed method consistently outperforms the compared baselines on different transfer tasks. Compared with the second-best baseline, the proposed method achieves 4.37% improvement of Rank-1 Recall and 3.29% improvement of mAP on average. The results demonstrate that the proposed method can effectively address the domain shift and modality heterogeneity challenges, and successfully transfer the knowledge learned from the source dataset to the target dataset for better text-to-image person retrieval performance.\n(2) Existing single-domain text-to-image person retrieval methods generally suffer from significant performance degradation in cross-dataset scenarios. For example, the SOTA methods, APTM and RaSa, suffer an average performance drop of 25.30% and 24.35% in Rank-1 Recall respectively. This is mainly due to the challenge of data distribution differences between the source and target domains. The domain shift between different datasets makes it difficult for the single-domain model to transfer the knowledge and generalize well to the target domain.\n(3) Existing unsupervised domain adaptation methods generally underperform. In the Source & Target setting, these baselines consistently lag behind single-domain retrieval baselines. The large image gallery scale exacerbates modality heterogeneity challenges in text-to-image person retrieval, limiting UDA method performance.\nAblation Study\nTo analyze the effectiveness of each component in the proposed method, we conduct ablation studies on ICFG-PEDES and CUHK-PEDES as the source dataset respectively. Specifically, we add each component to the baseline step by step, train the model according to the same settings as the main experiments, and evaluate the model on the target dataset. Since the GMP module does not have any loss function and must rely on the training of the CMKD module, we evaluate the following three possible combinations:\n\u2022 Baseline: The backbone only.\n\u2022 CMKD: The model with the CMKD module.\n\u2022 CMKD + GMP (Proposed): The model with both the CMKD and GMP modules, i.e. the proposed GCKD.\nThe ablation study results are shown in Table 4. As observed from the results, the baseline method achieves the lowest performance, which is due to the lack of domain adaptation. Adding the CMKD module to the baseline model improves the performance by a large margin, which demonstrates the effectiveness of the CMKD module in addressing the domain shift challenge. The proposed GMP module further improves the performance, which demonstrates the effectiveness of the proposed method in addressing the modality heterogeneity challenges. The results demonstrate the effectiveness of the proposed components in the cross-dataset text-to-image person retrieval task."}, {"title": "Conclusion", "content": "This paper presents a novel unsupervised domain adaptation method named Graph-Based Cross-Domain Knowledge Distillation (GCKD) for cross-dataset text-to-image person retrieval. In this method, a graph-based multi-modal propagation module and a contrastive knowledge distillation module are proposed to bridge the cross-domain correlation among the visual and textual samples and learn the cross-modal feature representation using the momentum knowledge distillation strategy. In the future, we plan to explore various advanced techniques to further improve cross-dataset retrieval performance, such as metric learning and adversarial learning."}]}