{"title": "Layer-of-Thoughts Prompting (LoT): Leveraging LLM-Based Retrieval with Constraint Hierarchies", "authors": ["Wachara Fungwacharakorn", "Nguyen Ha Thanh", "May Myo Zin", "Ken Satoh"], "abstract": "This paper presents a novel approach termed Layer-of-\nThoughts Prompting (LoT), which utilizes constraint hierar-\nchies to filter and refine candidate responses to a given query.\nBy integrating these constraints, our method enables a struc-\ntured retrieval process that enhances explainability and au-\ntomation. Existing methods have explored various prompting\ntechniques but often present overly generalized frameworks\nwithout delving into the nuances of prompts in multi-turn in-\nteractions. Our work addresses this gap by focusing on the hi-\nerarchical relationships among prompts. We demonstrate that\nthe efficacy of thought hierarchy plays a critical role in devel-\noping efficient and interpretable retrieval algorithms. Lever-\naging Large Language Models (LLMs), LoT significantly im-\nproves the accuracy and comprehensibility of information re-\ntrieval tasks.", "sections": [{"title": "Introduction", "content": "In recent years, there has been an explosion of interest in var-\nious prompting techniques for leveraging Large Language\nModels (LLMs). However, many existing works provide\nframeworks that are overly general, lacking the specificity\nneeded to tackle detailed prompt-related challenges. One\ncritical aspect that has been largely overlooked is the differ-\nentiation between prompts in multi-turn interactions.\nOur paper seeks to fill this gap by examining the impor-\ntance of hierarchical relationships among prompts. Specif-\nically, we introduce the concept of thought hierarchies to\nfilter and refine candidate responses, creating more struc-\ntured and explainable retrieval processes. We believe that\nthe strength of thought hierarchy is a pivotal factor in de-\nsigning algorithms that are both efficient and interpretable.\nBy incorporating constraint hierarchies within LoT, our ap-\nproach not only enhances retrieval accuracy but also ad-\ndresses the scalability and comprehension issues associated\nwith complex queries.\nThe LoT framework extends the Graph-of-Thoughts by\nrepresenting reasoning as a graph where nodes, called\nthoughts, denote reasoning steps. These thoughts are par-\ntitioned into layers, and categorized into layer thoughts,\nwhich handle conceptual step given by the user, and option\nthoughts, which assist in finding solutions.\nThe process starts with initializing layer thoughts, and\nproceeds from the first layer down to the last. Each layer\nthought receives input from the prior layer and branches into\noption thoughts, generating partial solutions. Aggregated\noutputs from option thoughts are passed to the next layer\nor used to expand thoughts dynamically until the last layer\nis reached.\nFor retrieval tasks, the LoT framework utilizes hierarchi-\ncal levels to filter and rank documents from a given corpus\nbased on a query. Relevance scores can be aggregated us-\ning several metrics, ensuring efficient and effective docu-\nment ranking. Outputs at each layer progressively filter doc-\numents, providing clear explanations based on aggregated\nscores to justify document relevance."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Traditional Information Retrieval", "content": "Before neural networks became widely popular, classical\nNLP approaches were the go-to methods for solving infor-\nmation retrieval tasks (Cooper 1971; Luhn 1957; Salton and\nBuckley 1988). These techniques primarily employed var-\nious lexical matching strategies. The researchers proposed\nboth logical and statistical models to measure the similarity\nbetween queries and potential matches. Despite their ad-\nvantagessuch as fast computation and versatilitythese\napproaches mainly relied on text morphology. Since mor-\nphological similarity does not necessarily equate to seman-\ntic similarity, achieving high accuracy in semantic similarity\nposed a challenge. As a result, the performance of these\nmethods is limited, particularly in cases where document-\nquery pairs exhibit non-overlapping text but semantic rele-\nvance, or overlapping text lacking semantic relation."}, {"title": "2.2 Neural Information Retrieval", "content": "Early pre-trained neural network models, including pre-\ntrained word embeddings such as Word2Vec (Mikolov et\nal. 2013), GloVe (Pennington, Socher, and Manning 2014),\nand FastText (Mikolov et al. 2018), have proven highly ef-\nfective in capturing semantic relationships between words.\nIn the legal domain, a significant advancement is Law2Vec\n(Chalkidis and Kampas 2019), a specialized word embed-\nding trained on legal texts, which has demonstrated substan-\ntial effectiveness."}, {"title": "2.3 Reasoning Topologies for LLM", "content": "The field of LLM prompting has seen a surge in inter-\nest regarding reasoning topologies, also known as X-of-\nThoughts (Besta et al. 2024b; Liu et al. 2023). A seminal\nwork in this area is Chain-of-Thoughts (Wei et al. 2022),\nwhich instructs LLMs to not only generate an output but\nalso reveal their step-by-step reasoning process (each step\nis called a thought). This concept was extended to Tree-\nof-Thoughts (Yao et al. 2024), where LLMs can branch\ninto multiple potential thoughts after each step and evaluate\ntheir progress towards the solution. Subsequently, Graph-of-\nThoughts (Besta et al. 2024a) generalized this further, allow-\ning each thought to connect with others. Connections can\ninvolve aggregating outputs from other thoughts or refining\nthe thought itself. Graph-of-Thoughts presents a model to\nformalize topological prompting using four components:\n1. Reasoning process: The reasoning process is represented\nas a directed graph. Each node in the graph represents one\nreasoning step producing a piece of information towards\na solution. A direct edge from one thought to another\nrepresent that an output (the piece of information) from\nthe start node is considered as an input of the end node.\n2. Thought transformation: The thought transformation\nis a transition function (probably involving LLMs) that\ngives next plausible reasoning processes from the cur-\nrent one. The function includes adding new thoughts and\nedges based on the existing thought (e.g., branching or\naggregating) or adding new edges without adding new\nthoughts (e.g., refining).\n3. Evaluation function: This function evaluates thoughts in\nthe reasoning progress as a score towards the solution, and\nLLMs may involve in this function. Sometimes, an evalu-\nation function needs to consider the whole reasoning pro-\ncess because a score of one thought may be relative to\nothers.\n4. Ranking function: This function ranks thoughts to con-\nsider a sole output for the main task. This function is re-\nlated to the evaluation function as it mostly rank thoughts\naccording to the scores."}, {"title": "2.4 Constraint Hierarchies", "content": "LoT Prompting in this paper is inspired by constraint hierar-\nchies (Borning, Freeman-Benson, and Wilson 1992), which\nare used for modelling constraints with strengths. In con-\nstraint hierarchies, each constraint is evaluated by an error\nfunction $e(c, \\theta)$ that returns a non-negative real number in-\ndicating how nearly constraint c is satisfied for a valuation\n$\\theta$; and the function returns a zero value when the c is ex-\nactly satisfied. The strengths of the constraints are indi-\ncated by a non-negative integer. Conventionally, constraints\nwith strength level 0 are hard constraints (or required con-\nstraints) and constraints with strength level $i \\geq 1$ are soft\nconstraints (or preferential constraints) where a higher value\nof the strength indicates that the constraint is weaker. Fol-\nlowing this, we can represent a constraint hierarchy H as\na partition $H_0, H_1,..., H_e$ where each $H_i$, called a level,\ncontains all of the constraints in H with strength i.\nSolutions to constraint hierarchies must satisfy all hard\nconstraints and satisfy soft constraints as much as possible.\nThat is, there is no valuation outside the solutions that bet-\nter satisfies the soft constraints and satisfies all of the hard\nconstraints. To consider which valuation better satisfies the\nsoft constraints, constraint hierarchies introduce compara-\ntors, which are later extended into level comparators and hi-\nerarchical comparators (Hosobe and Matsuoka 2003). Given\nvaluations $\\theta, \\theta', \\theta''$, a level $H_i$ of a constraint hierarchy H\nwith strength i, the level comparator $\\iota$ must satisfy the fol-\nlowing conditions\n1. If $e(c, \\theta) = e(c, \\theta'')$ for every $c \\in H_i$, then $\\theta \\iota \\theta'$ if and\nonly if $\\theta'' \\iota \\theta'$\n2. If $e(c, \\theta') = e(c, \\theta'')$ for every $c \\in H_i$, then $\\theta \\leq \\iota \\theta'$ if and\nonly if $\\theta \\iota \\theta''$\n3. If $e(c, \\theta) \\leq e(c, \\theta')$ for every $c \\in H_i$, then $\\theta \\leq \\iota \\theta'$\n4. If $\\theta \\iota \\theta'$ and $\\theta' \\leq \\iota \\theta''$, then $\\theta \\leq \\iota \\theta''$\nThe first and second conditions state that if errors after\napplying valuations are the same for every constraint in the\nlevel, then the level comparator works the same. The third\ncondition states that if errors after applying one valuation is\nless than or equal to the errors after applying another for ev-\nery constraint in the level, then the former is better than or\nequal to the latter. The forth condition indicates the transi-\ntive property of the level comparator (we omit one condition\nfrom (Hosobe and Matsuoka 2003) as we consider only one\nconstraint hierarchy each time). For a level comparator i,\nwe write\n* $\\theta \\sim_i \\theta'$ if $\\theta \\leq_i \\theta'$ and $\\theta' \\leq_i \\theta$;\n* $\\theta \\ll_i \\theta'$ if $\\theta \\leq_i \\theta'$ and $\\theta' \\nless_i \\theta$; and\n* $\\theta \\iota \\theta'$ if $\\theta \\nless_i \\theta'$ and $\\theta' \\nless_i \\theta$.\nIf $\\theta \\iota \\theta'$ is not possible (meaning that, we have either\n$\\theta \\sim_i \\theta'$, $\\theta \\iota \\theta'$, or $\\theta' \\iota \\theta$), $\\iota$ is\nsaid to be total. If\n$\\theta \\iota \\theta'$ implies that $e(c, \\theta) < e(c, \\theta')$ for every $c \\in H_i$,\n$\\iota$ is said to be local. A local $\\iota$ is generally not total be-\ncause there might be a level $H_i$ with two constraints $c_1, c_2$\nsuch that $e(c_1,\\theta) > e(c_1,\\theta')$ and $e(c_2,\\theta) < e(c_2, \\theta')$ and\nhence $\\theta \\nless \\iota \\theta'$. If there is a numerical aggregation func-\ntion $g(\\theta, H_i)$ such that $g(\\theta, H_i) \\leq g(\\theta', H_i)$ if and only if\n$\\theta \\iota \\theta'$, then $\\iota$ is said to be global. A global $\\iota$ is always\ntotal because a numerical comparator is total."}, {"title": "3 LoT Prompting", "content": "Figure 1 illustrates Layer-of-Thoughts (LoT) prompting.\nThe approach extends Graph-of-Thoughts (Besta et al.\n2024a), which represents the reasoning process as a graph.\nThe graph contains nodes, called thoughts, representing rea-\nsoning steps. Each thought receives outputs from the previ-\nous thoughts, and utilizes them to instruct LLMs in generat-\ning a partial solution for the next thoughts. In LoT Prompt-\ning, each thought is assigned a number to show which layer\nthey are in. There are two types of thoughts in the graph.\n* Layer thought: Each layer contains a layer thought to\nhandle conceptual step given by the user. A layer thought\nin layer i has a previous layer thought in layer i 1 and\na next layer thought in layer i + 1, except the first and the\nlast layer thoughts. A layer thought has a role to receive\noutputs from the previous layer thought, determine gener-\nating option thoughts, aggregate outputs from the gener-\nated option thoughts, and forward the aggregated output\nto the next layer thought.\n* Option thought: Each layer contains multiple option\nthoughts or even none of them (e.g., the last layer in\nFigure 1). An option thought in layer i receives the in-\nputs from the layer thought in the same layer, and gener-\nates a partial solution as the output for aggregating in the\nlayer thought. A layer thought may generate equal option\nthoughts (e.g., Layer 1 in Figure 1) or prioritize option\nthoughts into several levels, defined by a positive integer\n(e.g., Layer 2 in Figure 1). We consider a layer with equal\noption thoughts as single-level layer and a layer with pri-\noritized option thoughts as multiple-level layer\nThoughts in LoT Prompting are initiated as follows.\n1. A user designs conceptual steps for the task.\n2. Starting from the first step, each step initiates a new layer\nthought. The layer thought receives inputs from the previ-\nous layer thought (or from the main task for the first layer\nthought), and instructs LLMs to suggest appropriate cri-\nteria for that step. The criteria can be generated equally\nor prioritized in order of importance.\n3. For each criterion, an option thought is generated. Each\noption thought instructs LLMs or performs calculation,\nand then provides a partial solution for the task according\nto the criterion.\n4. The layer thought then aggregates the outputs from option\nthoughts in the layer, and determines whether the aggre-\ngated output is appropriate. If it is appropriate, then the\noutput is fed to the new layer thought (invoke Step 2 for\ninitiating a new layer thought from the next step). If it\nis not appropriate (e.g., no inputs from the previous layer\npass the criteria), we can instruct LLMs to refine the cri-\nteria and initiate new option thoughts again (backtrack to\nStep 3)."}, {"title": "3.1 Retrieval Tasks", "content": "In this paper, we explore leveraging LoT Prompting for re-\ntrieval tasks. Given a query q and a document corpus D,\nretrieval tasks aim to find the most relevant documents in\nD with respect to the query q. Retrieval tasks are usually\nevaluated using relevance scores. In LoT Prompting, we as-\nsume that each option thought can be evaluated using a non-\nnegative relevance score $p(d, t)$ of a document $d \\in D$ for an\noption thought t (greater score indicates more relevance). A\nrelevance function is binary if the score can only be either 0\nor 1. That is, the function returns 1 if the document d passes\nthe criterion corresponding to the thought t and returns 0 if\nit fails.\nWe suggest several metrics to aggregate outputs from op-\ntion thoughts in a single-level layer as follows."}, {"title": "4 Application-Driven Experimental Setup", "content": "In this section, we present two experimental applications\ncorresponding to two specific use cases: Japanese Civil Law\nretrieval and normative sentence retrieval. Through these\napplications, we verify the efficacy of the Layer-of-thought\napproach and identify its limitations."}, {"title": "4.1 Japanese Civil Law Retrieval", "content": "Legal AI is a niche field that leverages computer and arti-\nficial intelligence techniques to make legal processes more\nefficient and effective. The rapid progression of AI-based\ntools is set to emancipate legal professionals from cumber-\nsome tasks such as document searches and contract reviews.\nThe COLIEE competition (Goebel et al. 2024) is held an-\nnually to promote research in legal information processing.\nThis competition includes various challenges like document\nretrieval and legal entailment.\nParticipants are provided with a legal question Q from the\nJapanese Bar Exam and are tasked with retrieving relevant\narticles $A_1, A_2,..., A_n$ from the Japanese Civil Code. The\nability to retrieve pertinent legal articles is crucial for several\nreasons. It enables legal professionals to quickly access rel-\nevant legal precedents and statutes, thereby enhancing their\ndecision-making process. However, the task is particularly\nchallenging due to the complex language and vast amount of\ninformation contained within legal texts.\nThe Japanese Civil Code is rich with intricate, specialized\nterminology, making the retrieval task demanding. More-\nover, the volume of information makes it difficult to pinpoint\nthe exact articles that are relevant to a given legal question.\nThese challenges highlight the importance and difficulty of\nthe task, making it a suitable candidate to test the feasibility\nof the Layer-of-Thoughts approach.\nThe evaluation metrics for this task include the macro av-\nerage of F2, precision, and recall. This comprehensive set of\nmetrics ensures a robust assessment of each team's perfor-\nmance. The calculations for these measures are as follows:\n$\\text{Precision} = \\text{avg} \\left(\\frac{\\text{# correct articles}}{\\text{# retrieved articles}}\\right)$\n$\\text{Recall} = \\text{avg} \\left(\\frac{\\text{# correct articles}}{\\text{# relevant articles}}\\right)$\n$\\text{F2} = \\text{avg} \\left(\\frac{5 \\times \\text{Precision} \\times \\text{Recall}}{4 \\times \\text{Precision} + \\text{Recall}}\\right)$\nThe proposed design contains multiple layers following\nthe idea of LoT (as demonstrated in Figure 2):\n1. Keyword Filtering Layer (KFL): The first layer in-\nvolves keyword filtering. The keywords are suggested\nby a Large Language Model (LLM) after it analyzes the\nquery. This layer uses at-least-1 metric."}, {"title": "2. Semantic Filtering Layer (SFL)", "content": "Following the key-\nword filtering, the semantic filtering layer applies condi-\ntions suggested by the LLM to the selected candidates.\nThis is a multiple-level layer, organized from general to\nspecific conditions, helping refine the selection progres-\nsively. This layer uses max-count metric.\n3. Final Confirmation Layer (FCL): In the final layer, the\noriginal query is used once again. The LLM is asked to\nconfirm whether the candidate articles can indeed help an-\nswer the query. This step ensures that only the most rele-\nvant articles are selected. This layer uses all metric.\nBesides, we also challenge the proposed design with two\nsimpler settings. In the first setting, the SFL is omitted,\nmeaning that candidates are directly evaluated against the\noriginal query immediately after keyword filtering. In the\nsecond, even more minimal setting, we remove the SFL and\nKFL entirely.\nUpon examining the results in Table 1, several insights\ncan be drawn. The Proposed Design following LoT demon-\nstrated the highest F2 score (0.835), outperforming the best\nsystems in COLIEE 2024. This high F2 score indicates a\nwell-balanced performance in terms of precision (0.838) and\nrecall (0.839), showcasing the effectiveness of the Layer-\nof-Thoughts (LoT) approach. The structured filtering lay-\ners, including keyword and semantic filtering, ensure a bal-\nanced retrieval of relevant articles, minimizing false posi-\ntives while maximizing true positives.\nIn contrast, the two direct validation systems exhibited\nhigher recall but significantly lower precision. The Direct\nValidation with Query (w. KFL) system achieved a re-"}, {"title": "4.2 Normative Sentence Retrieval", "content": "In the context of automated driving, it is essential to have\na comprehensive understanding of both explicit and im-\nplicit traffic rules. While explicit rules are often codified\nin statutes and regulations, implicit rules are frequently de-\nrived from judicial decisions and case law. These implicit\nrules, which are crucial for ensuring safe and lawful driv-\ning behavior, need to be systematically identified and made\nexplicit.\nExtracting normative sentences from court decisions is vi-\ntal for developing a comprehensive corpus of traffic rules\nfor automated driving systems. Advanced natural language\nprocessing (NLP) techniques, machine learning models, and\nexpert validation can be employed to identify and articu-\nlate implicit rules, thereby enhancing the traffic regulation\nframework. This ensures that automated driving systems op-\nerate under a robust and legally sound set of traffic norms,\npromoting safety and compliance.\nCurrently, there are no annotated datasets available to\ntrain a model for retrieving normative sentences specifically\nrelated to Section 6 of the German Road Traffic Regulations\n(i.e., \u00a7 6 StVO). Normative sentences are those identified\nas additions, clarifications, or interpretations of the traffic\nrule stipulated by \u00a7 6 StVO. Developing rules or patterns to"}, {"title": "5 Discussion", "content": "The experimental results highlight the merits and potential\ndrawbacks of the Layer-of-Thoughts (LoT) Prompting in\nvarious information retrieval tasks. For Japanese Civil Law\nretrieval, the LoT approach consistently outperformed tra-\nditional methods and even advanced retrieval models used\nin the COLIEE competition. This demonstrates that the in-\ntegation of hierarchical thoughts and structured reasoning\ncan significantly enhance the quality of retrieved documents\nby balancing both precision and recall. The high F2 scores\nindicate that LoT Prompting effectively captures the nuances\nof the task, leading to a more precise and well-rounded re-\ntrieval process.\nIn the context of normative sentence retrieval, the LoT\nmethod also showed promising results. Despite the inher-\nently challenging nature of this task, which involves high\nvariability and implicit rules, LoT managed to achieve a\nrecall score close to 1.0. This high recall is particularly\nvaluable in legal contexts, where missing relevant sentences\ncould lead to critical oversights. However, the precision\nscores indicate that while most relevant sentences are cap-\ntured, further refinement is needed to reduce false positives\nand enhance the overall relevance of the extracted sentences.\nThe scalability and efficiency of the LoT Prompting are\nlargely supported by its hierarchical structure. The use of\nstrength metrics to prioritize thoughts ensures that the com-\nputational overhead is managed effectively. By leveraging\nconstraint hierarchies, LoT Prompting efficiently narrows\ndown the search space, making it well-suited for large-scale\ninformation retrieval tasks. This structured approach not\nonly reduces unnecessary computations but also improves\nthe speed and accuracy of retrieving relevant documents.\nOne of the standout features of the LoT Prompting is\nits explainability. By breaking down the retrieval process\ninto hierarchical levels of thoughts, LoT Prompting provides\nclear and interpretable pathways from the query to the fi-\nnal retrieved documents. Each step in the hierarchy can be\ntraced and understood, offering transparency in how deci-\nsions are made. This interpretability is invaluable, partic-\nlarly in legal and regulatory contexts, where the rationale\nbehind document retrieval must be clear and defensible."}, {"title": "6 Conclusion", "content": "In this paper, we introduced the Layer-of-Thoughts (LoT)\nPrompting, which incorporates constraint hierarchies to\nenhance the retrieval of information through structured\nprompting. Our experiments on Japanese Civil Law retrieval\nand normative sentence extraction tasks demonstrated that\nthe LoT Prompting significantly improves both precision\nand recall compared to baseline and state-of-the-art meth-\nods. The ability of LoT to balance these metrics while pro-\nviding explainable and efficient retrieval processes makes it\na compelling tool for complex information retrieval tasks.\nFuture work will focus on further refining LoT Prompting\nto enhance precision, particularly in tasks involving highly\nvariable and implicit information."}]}