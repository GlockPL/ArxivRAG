{"title": "CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based Direct Preference Optimization", "authors": ["Shuming Shi", "Ruobing Zuo", "Gaolei He", "Jianlin Wang", "Chenyang Xu", "Zhengfeng Yang"], "abstract": "Automated theorem proving (ATP) is one of the most challenging mathematical reasoning tasks for Large Language Models (LLMs). Most existing LLM-based ATP methods rely on supervised fine-tuning, which results in a limited alignment between the theorem proving process and human preferences. Direct Preference Optimization (DPO), which aligns LLMs with human preferences, has shown positive effects for certain tasks. However, the lack of high-quality preference data for theorem proving presents a significant challenge. In this paper, we innovatively apply DPO to formal automated theorem proving and introduces a Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP) method. Specifically, we propose a method for constructing preference data which utilizes LLMs and existing theorem proving data to enhance the diversity of the preference data while reducing the reliance on human preference annotations. We then integrate this preference data construction method with curriculum learning to iteratively fine-tune the theorem proving model through DPO. Experimental results on the MiniF2F and ProofNet datasets demonstrate the effectiveness of the proposed method.", "sections": [{"title": "Introduction", "content": "Despite the success of LLMs in various fields [14, 26, 44, 51], they still exhibit significant limitations in mathematical reasoning tasks, particularly in theorem proving. Enhancing the performance of LLMs in these tasks remains a challenging problem, especially for automated theorem proving in strictly formalized interactive theorem provers (ITPs) such as Lean [10] and Isabelle [34]. Many studies integrate LLMs with ITPs for automated theorem proving. Existing approaches generally fall into two categories: using LLMs to provide stepwise proof strategies combined with search algorithms to complete proofs [15,25,36], and leveraging LLMs to independently or informally generate complete proofs [21,43]. While these methods have achieved some success, most rely on supervised fine-tuning, resulting in limited alignment with human preferences, and the effectiveness of theorem proving remains constrained.\nIn recent years, LLM fine-tuning methods aligned with human preferences have gained significant attention. Recent studies demonstrate that preference-based optimization for LLM fine-tuning can yield substantial benefits [40,51]. Reinforcement Learning with Human Feedback (RLHF) [9] is a well-established and effective method for aligning models with human preferences, typically consisting of two steps: (1) training a reward model, and (2) optimizing it using reinforcement learning methods such as PPO [39]. However, this approach has two primary limitations: 1) it is relatively complex; and 2) it entails significant computational overhead.\nTo address the aforementioned issue, Direct Preference Optimization (DPO) [38] offers a simpler and more efficient alternative. DPO eliminates the need for training a reward model, directly optimizing from preferences, which makes it both straightforward and effective. DPO has shown promising results in certain mathematical reasoning tasks [28,33], but has yet to be applied to theorem proving tasks. However, a key challenge for preference-based optimization methods in LLM fine-tuning is the requirement for high-quality human preference labels, which are difficult to obtain.\nTo overcome the challenges mentioned above, this paper proposes a method for automated theorem proving that combines DPO and curriculum learning, named Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP). To the best of our knowledge, this is the first work to integrate DPO with automated theorem proving, filling a gap in DPO-based automated theorem proving. To address the challenge of insufficient theorem proving preference data, this paper proposes a method for constructing preference data based on fine-grained scoring by LLMs, which reduces the reliance on human annotations by utilizing LLMs for preference data construction, and enhances the diversity of positive samples in the generated preference data through the incorporation of fine-grained preference scoring. Using the constructed preference data in conjunction with curriculum learning, we propose a process for iteratively training a prover based on DPO, which enhances the LLM's ability to solve difficult problems through progressively challenging DPO iterations. Specifically, we categorizing automated theorem proving problems into curriculum data based on predefined difficulty levels, and iteratively construct and fine-tune the DPO preference data following the principles of curriculum learning. Experimental results indicate that the pass rate reaches 38.5% on the MiniF2F dataset and 12.7% on the ProofNet dataset, surpassing the baseline methods and highlighting the superior performance of the proposed method.\nOur contributions can be summarized as follows:\n\u2022 We propose a method for constructing DPO preference data based on fine-grained preference scoring by Large Language Models (LLMs), which facilitates the generation of diverse preference data while reducing reliance on human annotations.\n\u2022 Based on the proposed preference data construction method and curriculum learning, we introduce Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP) method, which enhances the capability of LLMs in autumated theorem proving.\n\u2022 Experimental results demonstrate that the proposed method outperforms all baseline approaches, achieving a maximum improvement of 7.4% on the MiniF2F-valid benchmark, thereby effectively enhancing the theorem proving capabilities of LLMs."}, {"title": "Related Works", "content": "Automated Theorem Proving with LLMs. The rapid advancement of Large Language Models (LLMS) has spurred significant progress in automated theorem proving. Various approaches integrate interactive theorem provers (ITP) such as Lean [11], Isabelle [34], and Metamath [31] with language models. A prominent method, exemplified by GPT-f [36], involves the language model generating the next proof step based on the current proof state, followed by tree search to find a complete proof for the theorem. PACT [15] jointly trains a language model with a strategy prediction objective for interactive theorem proving, where the auxiliary task for extracting self-supervised signals is used. HTPS [25] improves the automated theorem proving process by enhancing the MCTS-based proof search strategy. Another approach employs LLMs to derive the complete proof of a theorem, either directly or with the assistance of informal proof languages [21,27,43]. However, most approaches rely on supervised fine-tuning (SFT), which may not align well with human preferences, potentially limiting their proving capabilities.\nAligning LLMs with Human Preference. Reinforcement Learning from Human Feedback (RLHF) [9] is a classical and effective method for aligning LLMs with human preferences. This approach involves initially training a reward model, followed by optimization using reinforcement learning algorithms such as PPO [39], leading to substantial success in models like ChatGPT [2, 7,37], LLaMA [12,42], and Claude [5]. However, using PPO for RLHF is a complex and computationally expensive process. Motivated by this, Direct Preference Optimization (DPO) [38] has emerged as an effective alternative. DPO directly optimizes using preference data, without the need to train a reward model. Several works [3, 13, 18, 24, 45, 48] have proposed variations based on DPO. The simplicity and efficiency of DPO have enabled its application in various downstream tasks, such as mathematical reasoning [22, 29, 46], multimodal tasks [30], and summarization [38]. However, DPO still relies on high-quality preference labels to generate preference data, which requires costly manual annotation. In this paper, we propose a method for constructing DPO preference data and the generated preference data is utilized in DPO iterative training to enhance the theorem proving capabilities of LLMs.\nCurriculum Learning. Curriculum learning [6] is a method that simulates the human learning process by progressing from simpler to more complex tasks. Chang et al. [8] applied curriculum learning to data-to-text generation, achieving improvements in generation quality. By strategically organizing the learning trajectory, curriculum learning enhances model performance or training efficiency and has shown promise in mathematical reasoning tasks such as theorem proving. Polu et al. [35] introduced a curriculum of progressively harder statements by synthesizing inequalities with increasing difficulty, while LeanAgent [23] categorized theorems into three levels of complexity and utilized curriculum learning to train on mathematical repositories. In this paper, we propose a curriculum learning iterative training method based on the difficulty of theorem proof data and demonstrate its effectiveness."}, {"title": "Preliminaries and Notation", "content": "3.1 Preliminaries\nTheorem Proving in Lean. Lean [10,32] is a reliable interactive theorem proving assistant. Unlike natural language-based theorem proving, Lean requires a more rigorous proof process. Specifically, the theorem proving in Lean is interactive, where each step is carefully checked. In Lean, the theorem statement serves as the initial goal, and each formalized proof strategy, called a \"tactic\", is applied to reach new subgoals. The proof is considered complete when the \"no goals\" state is reached.\nDirect Preference Optimization (DPO). DPO [38] is an offline reinforcement learning method that aligns the outputs of Large Language Model (LLM) with human preferences. Unlike Reinforcement Learning from Human Feedback (RLHF) [9], which first trains a reward model and then applies reinforcement learning, DPO directly optimizes based on preference data. Each sample in the DPO preference dataset is a triplet (x, yw, y\u0131), where yw is the preferred response and y\u0131 is the dispreferred response, both corresponding to the input prompt x. The training objective is to minimize the following loss:\n$$L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,y_w,y_l)~D} [log\\sigma(\\beta log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)})]$$\nwhere D is the preference dataset, \u03c3 is the sigmoid function, \u03c0\u03b8 is the model to be optimized, initialized as Tref, and \u03b2 is a hyperparameter used to control the degree of divergence between \u03c0\u03bf and ref.\n3.2 Problem Definition\nThis paper investigates the problem of automated theorem proving in the Lean language. The theorem proving process can be abstracted as a tree search process, where the root node represents the theorem statement to be proven. Each node in the tree corresponds to a proof state s (i.e., a proof subgoal and corresponding premises), and each edge represents a proof tactic t. The initial state so represents the target theorem statement to be proved. Finding a valid proof for so entails identifying a complete path starting from so that successfully resolves the proof goals along the path."}, {"title": "Methodology", "content": "In this section, we introduce the proposed Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP). First, we present our key insights and an overview of the framework of the proposed CuDIP method, which consists of three stages: (1) Preparation, (2) Preference Data Generation, and (3) Curriculum-based DPO Iteration. Subsequently, we provide a detailed introduction to each of these stages.\n4.1 Key Insights and Framework Overview\nOur proposed approach is based on the following observations and assumptions.\n1. The preference-based LLM fine-tuning method DPO can be applied to theorem-proving tasks in LLMs to enhance reasoning capabilities and align better with human preferences.\n2. Similar to the human learning process, where knowledge is acquired progressively from simple to complex, training LLMs in a stepwise manner-from easy to difficult-may enhance their learning process and improve their ability to tackle more complex problems.\n3. The off-the-shelf expert data from the theorem proving process can serve as positive examples for DPO training. Additionally, existing language models can generate counterexamples and more diverse DPO training data based on the available theorem-proving data.\n4. When constructing DPO preference data, applying more fine-grained preference scoring to the data prior to forming preference pairs enhances the diversity of positive examples in the preference data.\nBased on the aforementioned key insights, we propose a curriculum learning-based DPO iterative theorem proving method (CuDIP) for the theorem proving problem defined in Section 3.2. To address the lack of preference data in theorem proving, we propose a fine-grained scoring-based preference data generation method, which utilizes existing theorem proving data and LLMs to generate more diverse preference data, thereby reducing the reliance on human annotations. We then apply curriculum learning-based DPO iteration to the prover, leveraging the proposed preference data generation method and curriculum learning.\nFigure 1 gives an overview of our proposed CuDIP method. Specifically, the overall algorithm can be divided into the following three stages:\nStage 1: Preparation. The existing formal theorem proving dataset D is partitioned based on the difficulty of the tasks to construct a curriculum learning dataset C. The base model of the prover is then fine-tuned in a supervised manner using the entire curriculum learning dataset C, resulting in the base prover Po.\nStage 2: Preference Data Genertion. (1)Based on the current prover model and a subset of the new round of curriculum learning data Cn, the preference score data SCo for training the score generator is derived through Fine-Grained Preference Scoring (FGPS) process. (2) Supervised fine-tuning (SFT) is applied to the current generator model using SCo, resulting in a new score generator model Gn. (3) Gn is then used to score the candidate tactics across all states in Cn, and the preference dataset Dp is obtained through filtering and pairing.\nStage 3: Curriculum-based DPO Iteration. The current prover model is fine-tuned using the generated preference data Dp through the DPO method. Stage 2 and 3 are then repeated iteratively until no further performance improvement is observed.\nNext, we will provide a detailed technical description of the three stages of the proposed method.\n4.2 Stage 1: Preparation\nBefore introducing the process of constructing curriculum data, we first define two concepts.\nDefinition 1. (Distance). For a given proof tree T of a theorem, the distance of a proof state s is defined as the number of proof tactics that need to be executed from s to the completion of the proof (i.e., reach the \"no goals\" state in Lean).\nDefinition 2. (Difficulty). The difficulty of resolving a proof goal s is defined as the distance in the proof tree from s to the final proof goal.\nCurriculum Data Construction. The curriculum dataset C is derived by partitioning the original theorem-proving dataset D based on increasing difficulty values. Specifically, for each theorem and its corresponding proof process in D, we extract the proof states at each step of the proof process and the corresponding subsequent proof tactic t. We then calculate the difficulty DIF of each proof state si. According to definition Definition 2, DIF of si is defined as the distance di. This process results in intermediate data composed of triplets, Dtriplet, which can be formally expressed as:\n$$D_{triplet} = \\{(s_i, t_i, DIF_i) | i = 1, 2, ..., n\\}$$\nwhere si represents the current theorem proving state, ti denotes the next proving tactic of si, and DIF indicates the difficulty of si.\nThen, divide the dataset Dtriplet into subsets in ascending order of DIF\u00bf. This results in a curriculum dataset C organized from easy to hard, which can be formally expressed as:\n$$C = \\{C_n | n \\in \\{DIF_i | (s_i, t_i, DIF_i) \\in D_{triplet}\\}\\}$$\n$$C_n = \\{(s_i, t_i, DIF_i) | DIF_i = n\\}$$\nwhere C is the set of all subsets Cn, each corresponding to a unique difficulty level n, ensuring that triplets with the same difficulty are grouped together. In the subsequent training process, further processing is performed based on dataset C.\nBase Prover Training. To achieve better proving performance, we first train the base model used as the prover model before the subsequent iterations. The entire curriculum learning dataset C is used to perform supervised fine-tuning (SFT) on the base model, resulting in the base prover Po.\nAfter obtaining the curriculum learning data and the base prover, we next construct a preference dataset for DPO fine-tuning.\n4.3 Stage 2: Preference Data Generation\nWe propose a method for constructing preference data by leveraging LLM and existing formal theorem-proving datasets. First, we define a scoring rule and assign fine-grained preference scores to each theorem proof state and its corresponding candidate tactics in curriculum dataset acheived in Section 4.2. Next, the data is filtered and paired based on the scores to form preference pairs, resulting in the final DPO preference dataset. To enhance the efficiency of the data generation process, we introduce a score generator during the preference scoring stage, replacing manually defined scoring rules. The proposed method offers the following benefits: (1) reducing reliance on human annotations by utilizing LLMs for preference data construction, and (2) enhancing diversity of positive samples through fine-grained preference scoring. The method proceeds as follows.\nTactic Samples Generation. For each proof states in the formalized course dataset Cn, k candidate tactics are generated using the current prover model Pn\u22121, forming a candidate tactic set T = {t1, t2, ..., tk}.\nFine-Grained Preference Scoring (FGPS). For the candidate tactic set T originating from proof state s, all tactics in T are evaluated using the following scoring process.\n1. Execute each candidate tactic from s to reach a new proof state and perform nattempt subsequent proof search attempts using the current prover Pn\u22121 and Monte Carlo Tree Search (MCTS).\n2. The score for a given candidate tactic t of proof state s is calculated using the following equation:\n$$Score(t|s) = \\frac{N_{success}}{N_{attempt}}$$\nwhere nsuccess represents the number of successful proofs found after executing the proof tactic t, and Nattempt is the number of subsequent proof search attempts for each candidate tactic.\nAs shown in Figure 2, an example of the proposed fine-grained scoring process is provided. However, The above scoring process necessitates multiple MCTS search attempts for each proof state, incurring significant time costs. Therefore, we randomly select a subset of the course data Cn for preference scoring to obtain the original score data SC\u00ba, and introduce a score generator to score the remaining data in Cn.\nREMARK. For the remaining unscored data, language model-based scoring replaces the proof search scoring process, improving the efficiency of preference data construction.\nScore Generator Training. Using SC\u00ba, the existing language model Gn\u22121 undergoes SFT to produce the score generator Gn. The score generator is then used to evaluate the remaining data, resulting in the final scoring dataset SC, which can be expressed as:\n$$SC = \\{(s_i, T_i) | i = 1, 2, . . ., x\\}$$\n$$T_i = \\{(t_j^{(i)}, Score(t_j^{(i)} | s_i)) | j = 1, 2, ..., k\\}$$\nWhere si represents the i-th proof state in Cn, Ti is the set of candidate proof tactics of si and their corresponding preference scores, t(i) denotes the j-th candidate tactic for si, x is the total number of proof states in Cn, and k is the number of candidate tactics for a given state.\nFiltering and Paring. Perform data filtering and pairing on the dataset SC, which contains all states along with their corresponding candidate tactics and tactic preference scores. Specifically, for a given proof state s, candidate tactics from its tactic set T are selected for pairwise comparison if the difference in their preference scores exceeds a certain threshold Th. A preference pair {s,tw,ti} is considered valid if the following conditions are met:\n$$Score(t_w|s) > Score(t_i|s), Score_{t_w t_i} > T_h,$$\nwhere scoretwst\u2081 = |Score(tw|s) \u2013 Score(ti|s)|.\nAfter the filtering and pairing process, all the preference data (s, tw, t\u0131) satisfying the conditions constitute the preference data set Dp, which can be expressed as:\n$$DP = \\{\\langle s^{(i)}, t_w^{(i)}, t_i^{(i)} \\rangle\\}_{i=1}^{N},$$\nwhere N is the number of all qualified preference pairs in Cn, and si is the state corresponding to the preference candidate tactic pair.\n4.4 Stage 3: Curriculum-based DPO Iteration\nDPO Fine-tuning. After obtaining the preference dataset Dp, we fine-tune the prover model using DPO. For each triplet (s, tw,t\u0131) in Dp, it corresponds to the triplet (x, yw, y\u0131) in the DPO preference data discussed in Section 3.1, where s corresponds to an input, tu denotes the preferred response, and ti denotes the dispreferred response. We then minimize the optimization objective of Section 3.1, where tref corresponds to the prover model before DPO fine-tuning.\nCurriculum-based Iteration (CI). After obtaining the new prover model, the process enters a new round of curriculum-based DPO iteration, as outlined in Algorithm 1. Specifically, for the n-th iteration, the dataset Cn, with difficulty value n, is used as the curriculum data. Tactic sampling is performed using the prover model Pn-1 from the previous iteration, and fine-grained scoring is applied to a subset of the data in Cn to obtain the scoring dataset SCO. This dataset is used to fine-tune the score generator Gn-1 from the previous round through supervised learning, yielding the new score generator Gn. Next, Gn is used to score the remaining data in Cn, resulting in the scoring dataset SCn. Data filtering and pairing are then performed on SCn to obtain the preference pair dataset, followed by a new round of DPO. In the first iteration, Pref is the base prover Po obtained after SFT. In the n-th iteration, Pref is Pn-1."}, {"title": "Experiments", "content": "5.1 Experimental Setup\nDatasets and Metric. We use the Lean 4 formalized theorem-proving dataset Mathlib4\u00b9 as the training set D. To evaluate the performance of the proposed method, we used datasets MiniF2F [49] and ProofNet [4] as evaluation benchmarks.MiniF2F consists of 488 problems derived from high school mathematics competitions, with 244 problems in both the test and validation sets. These problems primarily include those sourced from the MATH [17] dataset, high school mathematics competitions such as AMC, AIME, and IMO, as well as a set of carefully crafted problems designed to match the difficulty level of those in the competitions. ProofNet includes 371 examples, each comprising a formal theorem statement, a natural language theorem statement, and a natural language proof in Lean 3. We manually translated the corresponding Lean 3 proofs in ProofNet into Lean 4, resulting in 360 examples that are suitable for interaction with Lean 4. We use pass@k as the\n5.2 Main Results\nTo validate the effectiveness of the proposed method, we apply the CuDIP method to three different model groups and compare the results with existing baseline methods as well as the corresponding models after fine-tuning with SFT. The proof pass rates on the MiniF2F and ProofNet are presented in Table 1, where our method corresponds to the results of the fourth iteration. As shown in Table 1, the use of the CuDIP method leads to significant improvements over the baseline results. For instance, when using Mathstral as the prover and Llama-3.1 as the score generator with CuDIP, the pass@1 on MiniF2F reaches 38.5%, representing a 4.1% improvement over the baseline method after SFT. Additionally, when using Llama-3.1 as the prover and Gemma2 as the score generator, the highest performance improvements were observed on both benchmarks, with a 7.4% increase on MiniF2F and a 2.2% increase on ProofNet. In the experiments with the remaining group of models, the proof pass rate also showed notable improvements. These results demonstrate the effectiveness of the proposed CuDIP method.\n5.3 Ablation Studies\nTo separately investigate the effects of different components of the proposed CuDIP, we conducted ablation experiments, testing: 1) the effectiveness of the fine-grained preference scoring based on the score generator, and 2) the effectiveness of DPO iteration based on curriculum learning.\nFine-Grained Preference Scoring (FGPS). In order to verify the effectiveness of fine-grained preference scoring during the construction of preference data, we used Mathstral as the prover model and tested two conditions: one where Llama3.1 was used as the score generator in the preference data construction process, and another where no score generator or scoring rules were applied for fine-grained scoring. The test results on the MiniF2F and ProofNet benchmarks are presented in the Table 2 The use of the score generator for fine-grained scoring significantly improved the model's proof success rate, thereby demonstrating the effectiveness of the proposed fine-grained preference scoring process.\nCurriculum-based Iteration (CI). To validate the effect of Curriculum-based Iteration (CI), we combined the curriculum data from the first four iterations and conducted one round of training. The results were then compared with those obtained after four iterations of training. As shown in Table 3, the test results on MiniF2F after training with CI were 4.1%, showing an improvement of 2.1% compared to the results without CI, which further verified the effectiveness of curriculum-based iteration."}, {"title": "Conclusion", "content": "In this paper, we present a curriculum learning-based DPO iterative theorem proving method (CuDIP). Specifically, we first introduce an effective preference data construction approach based on fine-grained preference scoring by large language models (LLMs), which reduces the reliance on human annotations and enhances the diversity of the preference data. Subsequently, we integrate the proposed preference data construction method with curriculum learning to conduct iterative DPO training for the theorem proving model. Experimental results demonstrate that the method proposed herein significantly improves the performance of LLMs in theorem proving tasks and enhances their reasoning capabilities in the context of theorem proving."}, {"title": "More Experimental Results", "content": "In this paper, we used three groups of models to verify our proposed method. For each group of models, we performed 4 rounds of iterations. After each round of iteration, we tested the model. Table 4 shows the test results of the model on the MiniF2F and ProofNet datasets. The results show that the accuracy of the model gradually increases during the iteration process."}, {"title": "Training Details", "content": "B.1 Training Tools\nIn this paper, we used supervised fine-tuning (SFT) and direct preference optimization (DPO) to train the model. In terms of training tools, we used LlamaFactory[50], a unified framework that integrates a suite of cutting-edge efficient training methods. It can help us focus more on the construction of the dataset without worrying about the implementation of the model training code.\nB.2 Base Prover Training\nWe used LlamaFactory to train the three basic models with SFT. The three models used the same prompt template, see Appendix C for details. During training, we used the LoRA[19] method, which can help us save training resources. We set the learning rate to 2.0 \u00d7 10-5, the learning rate scheduler to cosine, the warmup ratio to 0.03, and trained for 3 epochs. We set the floating point precision to bfloat16 and batch size to 4.\nB.3 Score Generator Training\nWhen generating training data for the score generator, we performed a Monte Carlo Tree Search (MCTS) on each proof state in the subset of the curriculum learning data Cn. For each search, we set the number of simulations to 1000 and limited the search depth to 10. We did not use a fixed value for the search time limit, because as the number of iterations increases, the model's ability gradually increases and the model's search time increases. We observed that when the model performs automatic theorem proving, it usually succeeds within 60 seconds, and the probability of the model proving the theorem after that is very low. This phenomenon is caused by the model's insufficient long-chain reasoning ability, which is also one of the challenges faced by large language models (LLMs) at this stage. In order to balance the quality and quantity of training data when computing resources are limited, we limited the search time limit to 60 seconds in the first round of iterations. For each subsequent round, we increased the search time limit by 30 seconds each time, hoping that the model can perform a deeper search.\nB.4 Filtering and Paring\nAfter we score each tactic in the candidate tactic set for a proof state, we need to consider how to construct a preferred data set for DPO training. For those tactics with non-zero scores, it means that after the current proof state applies the tactic, the model has the probability of completing the subsequent proof. We use a pair of tactics with a score difference of more than 0.5 as positive and counterexample in a dpo training data set. The advantage of this method is that for any proof tactic, when constructing the DPO training data set, it will not be used as both a positive example and a counterexample, which can be beneficial to the training of the model. When conducting dpo training, we set \u1e9e in the DPO loss function to 0.1, set the learning rate to 5.0 \u00d7 10-6, and trained for 1 epoch."}, {"title": "Prompt Templates", "content": "C.1 Prove Model Prompt Template\nWe apply this template to an actual proof state and replace the placeholders in the template about the theorem proof state with specific values. Considering that the proof state of a theorem may contain multiple proof goals, we set the upper limit of the number of model input tokens to 4096 and the upper limit of the number of output tokens to 2048, which can meet the needs of the current task. It is worth mentioning that in order to better help us extract the desired tactics from the model output, we limit the input and output of the model to json format.\nThe following is a actual examples of prompts and the responses of the model:\nC.2 Score Generator Prompt Template"}, {"title": "Case Studies", "content": "D.1 mathd_algebra_109\nTheorem mathd_algebra-109 in the valid validation set of Minif2f is defined as follows:\nTheorem mathd_algebra-109 can be proved using two proof tactics, \"subst h\u2081\" and \"linarith\", but this does not mean that there is only one way to prove the theorem. When the model predicts the tactics for the first proof state, it will not only generate the tactic shown above, but also other proof tactics. If the proof tactic of the first step, \"subst h\u2081\", is replaced by \"rw [h1] at ho\", and the proof tactic of the second step is not changed, the theorem can also be proved.\nD.2 mathd_numbertheory_109\nThe theorem is defined as follows"}]}