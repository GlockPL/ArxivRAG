{"title": "Advancing Large Language Models for Spatiotemporal and Semantic Association Mining of Similar Environmental Events", "authors": ["Yuanyuan Tian", "Wenwen Li", "Lei Hu", "Xiao Chen", "Michael Brook", "Michael Brubaker", "Fan Zhang", "Anna K. Liljedahl"], "abstract": "Retrieval and recommendation are two essential tasks in modern search tools. This paper introduces a novel retrieval-reranking framework leveraging Large Language Models (LLMs) to enhance the spatiotemporal and semantic associated mining and recommendation of relevant unusual climate and environmental events described in news articles and web posts. This framework uses advanced natural language processing techniques to address the limitations of traditional manual curation methods in terms of high labor cost and lack of scalability. Specifically, we explore an optimized solution to employ cutting-edge embedding models for semantically analyzing spatiotemporal events (news) and propose a Geo-Time Re-ranking (GT-R) strategy that integrates multi-faceted criteria including spatial proximity, temporal association, semantic similarity, and category-instructed similarity to rank and identify similar spatiotemporal events. We apply the proposed framework to a dataset of four thousand Local Environmental Observer (LEO) Network events, achieving top performance in recommending similar events among multiple cutting-edge dense retrieval models. The search and recommendation pipeline can be applied to a wide range of similar data search tasks dealing with geospatial and temporal data. We hope that by linking relevant events, we can better aid the general public to gain an enhanced understanding of climate change and its impact on different communities.", "sections": [{"title": "1. Introduction", "content": "The impacts of climate change unfold through a vast number of environmental events that carry profound implications for ecosystems and human societies worldwide. These events cover a broad spectrum, from extreme weather phenomena to gradual ecological succession (Ebi et al., 2021), affecting various regions across different timescales (Hase et al., 2021). Investigating similar environmental events is essential for sharing observations, raising awareness, precepting risks, estimating frequency of unusual events, learning lessons from past occurrences, as well as developing response strategy to climate (Albright & Crow, 2019; Huber & Gulledge, 2011; Raymond et al., 2020).\nDespite traditional structured data from sensors, unstructured data such as natural language text can provide valuable insights that structured data alone cannot capture, thus offering a more comprehensive understanding of environmental events (Demanega et al., 2021; Fan et al., 2020; Feng et al., 2022). Mining events concerning climate change typically uses unstructured data from authorized sources such as news articles (Olteanu et al., 2015), or in the form of Volunteered Geographic Information (VGI) (Elwood et al., 2012; Goodchild, 2007) including social media and community reports (Kankanamge et al., 2019; Mihunov et al., 2020; M\u00fcller-Hansen et al., 2023; S. Zhang et al., 2021) which contains rich semantics that refers to local knowledge, individual experiences, and local communities impacted by climate change.\nManually curating environmental events and identifying similar events is practical with help from domain experts (Ramachandran et al., 2016). However, it suffers from low efficiency, subjective biases, scalability issues, and lack of scientific rigor and comprehensiveness (Olteanu et al., 2019). In the era of big data, the volume of information on climate change is vast and continuously expanding, and reliance on manual methods is impractical and inadequate. To address these challenges, there's a growing recognition of the merit of utilizing Natural Language Processing (NLP) to automate the process of harnessing and analyzing environmental events (Effrosynidis et al., 2022). As a pivotal subset of artificial intelligence (AI), NLP can efficiently process large textual datasets and plays an important role in advancing Geospatial Artificial Intelligence (GeoAl) (W. Li, 2020) research such as spatial data search and retrieval (Janowicz et al., 2020; W. Li et al., 2011).\nEarlier works on geospatial semantic search rely on NLP techniques to compute thematic content similarity and expand geospatial terms with gazetteers. For instance, semantic-enabled indexing and query understanding enable more effective spatial data discovery (W. Li et al., 2014, 2019). Another example is metadata topic harmonization and semantic search driven by linked data for GIS portals (Hu et al., 2015), increasing the accessibility of shared geospatial resources including data and services. NLP can also help with keyword refinement, metadata augmentation, and summarization for geographical data understanding and retrieval (Ferrari et al., 2024; Hu et al., 2019; Tian et al., 2023). These methods laid the groundwork for understanding spatial relationships and thematic relevance within large datasets, forming a crucial foundation that recent LLM applications continue to build upon."}, {"title": "Contributions", "content": "Contributions of this work are summarized below:\n(1) We proposed a two-stage LLM-based search and recommendation framework designed for analyzing spatiotemporal events. This framework uniquely incorporates spatial and temporal facets in both retrieval and re-ranking stages.\n(2) We introduced an innovative use of LLMs to enhance the category feature of events through zero-shot named entity recognition, enriching the semantics with minimal human intervention.\n(3) We proposed and developed the GT-R model, a novel re-ranking solution that synthesizes semantic understanding with geographical and temporal relevance.\n(4) We provided empirical evidence demonstrating the superior performance of our proposed framework compared to existing dense retrieval and cutting-edge re-ranking models, highlighting its potential to significantly improve spatiotemporal event recommendation.\nThe remainder of this paper is organized as follows to provide a comprehensive exploration of our framework and findings. Section 2 reviews related work, covering the utilization of LLMs for recommendation systems, domain-specific adaptations related to climate change, and the integration of spatiotemporal dimensions in news recommendation. Section 3 details our proposed method, which harnesses spatiotemporal insights along with LLMs to enhance the accuracy and relevance of event recommendations. Section 4 outlines the experiments to validate our method and compare it against alternative approaches. Section 5 presents the experiment results, showcasing the effectiveness of our approach in environmental event recommendations. Section 6 discusses the broader implications of our findings, particularly the potential of LLMs to transform recommendation systems through a spatiotemporal lens. Finally, Section 7 concludes the paper by summarizing our contributions and suggesting avenues for future research."}, {"title": "2. Related work", "content": null}, {"title": "2.1 Large language models and recommendation", "content": "Recommendation, as a fundamental task in information retrieval, demands a thorough understanding of content relevance to tackle challenges such as the cold-start problem (Arora et al., 2023), where user interactions with content are limited or nonexistent. This issue highlights the importance of a well-founded recommendation system architecture. A common approach in recommendation system design employs a multi-stage ranking pipeline that typically starts with retrieval followed by re-ranking (Yue et al., 2023). In the evolving landscape of NLP and Al research, LLMs have gained significant attention for their strong capabilities across various tasks (Zhu et al., 2023). Recently, researchers have started to investigate how LLMs can advance these stages of recommendation in various ways (Lin et al., 2023; Wang et al., 2023; L. Wu et al., 2023).\nThe retrieval phase aims to shortlist candidates from extensive datasets, typically through sparse retrieval techniques, or dense retrieval utilizing LLM embeddings for semantic search"}, {"title": "2.2 Domain adaptation and climate change research", "content": "Domain adaptation of LLMs is crucial for tailoring these general models to a specific domain without the need for training models from scratch. Prompting emerges as a pragmatic and accessible method for achieving domain adaptation (Rao et al., 2023), employing suitable prompts and instructions to gear LLMs toward generating outputs aligned with the desired context or task without altering LLMs' underlying parameters. For example, prompt engineering based on domain expertise can improve the performance of LLMs because it leverages the model's existing knowledge, providing context, instructions, and examples at inference time to produce relevant responses (Vaghefi et al., 2023). The approach varies from zero-shot learning, requiring no specific examples, to few-shot learning which provides a few examples (Brown et al., 2020; Wei et al., 2021). Additional strategies to enhance prompt engineering include persona adoption (Thulke et al., 2024), which assigns the model with a predefined character or perspective, and the Chain of Thought (CoT) method, which guides the model through a step- by-step reasoning process (Wei et al., 2022). Alternatively, fine-tuning adjusts the model's parameters via training on a domain-specific dataset to help with tasks such as climate-related text classification and geography-related toponym recognition (Webersinke et al., 2021; Zhou et"}, {"title": "2.3 News recommendation and spatiotemporal dimensions", "content": "Event recommendation, particularly within the realm of news recommendation, emerges as a multifaceted problem. Content serves as the primary foundation, and content features can be constructed in terms of semantic, entity, keyword, topic, emotion, and other factors (Jiang et al., 2021; C. Wu et al., 2023). Recent advancements in LLMs have enhanced the capabilities of news recommendation systems, offering new methods for feature construction and contextual understanding (Q. Liu et al., 2023, 2024). These advancements allow for the efficient integration of various contextual aspects, demonstrating the substantial benefits of incorporating not only the content itself but also the context in which news events occur. Such contextually enriched approaches have led to more relevant news recommendations even in the cold start setting (Q. Liu et al., 2024), leveraging LLMs' advanced NLP capabilities to parse and understand complex datasets.\nThe incorporation of spatiotemporal perspectives into news recommendation systems introduces an additional dimension of relevance by employing location and time as critical factors. While attempts to integrate spatiotemporal factors have utilized geotags and timestamps, most applications focus on depicting user context and behavior (Javed et al., 2021; Mitova et al., 2023; Raza & Ding, 2022). Location and date of news have often been overly simply transferred as property features such as distance for radius search, start and end date for time range filtering or recency (Bao & Mokbel, 2013; Shah et al., 2023; Xu et al., 2015). The potential for deeper domain-specific insights into spatial and temporal similarities remains overlooked. For instance, environmental events are inherently linked to their geographical location and time of occurrence. In that sense, they can exhibit latitudinal similarities (Malizia et al., 2020; Nishizawa et al., 2022). Their occurrences might be seasonal rather than strictly date- specific. Examples are yearly wildfires in California and temperature-correlated peak thawing of permafrost in Alaska. Although LLMs have demonstrated efficiency as semantic re-ranker (Sun et al., 2023), using LLMs as a sole solution is limited in handling the complexities of space and"}, {"title": "3. Method", "content": "In this section, we first introduce the proposed framework for the search problem regarding retrieving relevant events with spatiotemporal information. Then, we dive into details of constructing each feature including semantic similarity calculation, category similarity calculation, relevance calculation in terms of space and time. Finally, we explain how to combine those feature lists to form the final ranking list of candidate events."}, {"title": "3.1 Overall framework", "content": "In this section, we introduce a novel framework designed to tackle the challenge of mining relevance of spatiotemporal events/news by pinpointing events that are similar not only in content but also in their spatiotemporal context. As depicted in Figure 1, our framework is structured into two interconnected stages. Initially, the framework requires structuring the event descriptions. This tailors the content to more closely reflect user intent by strategically integrating prefixes and enhancing the descriptions with spatiotemporal metadata, thereby enriching the context. The first stage is dedicated to retrieval. Leveraging an LLM embedding model, the system retrieves a collection of events that are preliminarily relevant to the query based on their semantics. Subsequently, in the second stage, we introduce a refinement process as re-ranking. This stage uses the proposed GT-R model, which unravels the intricacies of semantic pertinence and spatiotemporal coherence. The GT-R model re-evaluates the preliminary set, assessing each candidate event's relevance through a fusion of scores derived from semantic and spatiotemporal features. The outcome of this process is the generation of a final, re-ranked set of events. This set represents the most contextually and temporally relevant events and is presented as the ultimate output of our recommendation framework. This carefully orchestrated sequence ensures that the retrieval system identifies events with relevant information with the combined spatiotemporal search criteria."}, {"title": "3.2 Feature construction", "content": "We detail the methodologies for calculating semantic similarity, category-instructed similarity, spatiotemporal relevance, and the subsequent fusion of these features to construct the final ranking of related events. Algorithm 1 describes the formulation of the GT-R model and its annotations."}, {"title": "3.2.1 Semantic similarity", "content": "The fundamental goal is to achieve an enhanced understanding of the semantics underlying event descriptions. Basic methods such as BM25 are good at matching keywords but often fail to capture the context necessary for discerning relevance among complex events. This calls for a more sophisticated model that can address overt and subtle contextual signals within the data, which can potentially improve retrieval performance. Following advances in NLP, particularly the success of transformer-based models (W. Li & Hsu, 2022) in capturing deep semantic meanings, we leverage the advanced LLM embedding model - OpenAl's Ada (Neelakantan et al., 2022) that has shown proficiency in understanding context in long ranges. By employing semantic embeddings, our method captures the important hidden relationships within text, enabling a richer and more accurate representation of content semantics. This approach is further refined through input segment picking and structuring to better align the model's understanding of spatiotemporal events. Specifically, different combinations of content segments are compared using basic concatenation, and further compared the best combination with adding prefix accordingly.\nAfter metadata structuring, a query event q with a candidate event z from the set of all event records Z are fed into the LLM embedding model separately, and mapped as two high- dimensional vectors for semantic similarity calculation as noted by $f_{sim\\_bi}$. According to semantic"}, {"title": "3.2.2 Category similarity", "content": "Categories are invaluable semantic information tagged by domain experts towards thematic similarities that can be leveraged to assist in re-ranking, especially with well-defined categorization schema. While the common faceted search method uses category tags as simple filters, it is unable to capture the semantic depth of the categories solely on word matching. On the other hand, important entities within news content carry key messages and facilitate a more direct understanding of the content. A recent work suggests that by explicitly revealing the category of these entities, the model can understand and process the content more accurately (D. Liu et al., 2020). To address these challenges, our methodology proposes a hybrid model that not only leverages semantics in category tagging but also enriches it by entity extraction as illustrated in Figure 2. The intuition is that incorporating categorized entities into the news representation will provide a more condensed semantic profile of each article, which in turn will enhance the relevance and accuracy of recommendations in a cold-start scenario. Through the application of prompt engineering, we harness the processing power of LLM in zero-shot settings, supplemented by persona setting, task description, and CoT processes, to effectively incorporate category tags into instructions. In this way, domain knowledge representing category tags is injected to an LLM. Recognizing the limitations of generative models such as output might be malformatted even after multiple retries, we incorporate a \u201cHuman in the Loop\" approach, which involves manual oversight to refine and verify the outputs of this category- entity fusing process by LLM."}, {"title": "3.2.3 Spatiotemporal relevance", "content": "(1) Distance relevance\nWe consider the first spatial feature based on distance. This comes from Tobler's first law of geography, \"Everything is related to everything else, but near things are more related than distant things\" (Tobler, 1970), and the notions of weight and scale from Geographically Weighted Regression (Brunsdon et al., 1998). This also applies to biological systems as illustrated by a general and strong occurrence of geographic distance decay in ecological community similarity (Morlon et al., 2008). Our method uses distance as the spatial proximity to measure the closeness of events in a meaningful way than simple radius search. Distance between an event pair is calculated using the Haversine formula which determines the great- circle distance between two points on a sphere given their longitude and latitude coordinates. This is a more accurate measure of distance because some unusual environmental events are so rare that relevant events might be far even at continental scale.\nLet ($\u03c6_q$, $\u03bb_q$) be the latitude and longitude of where the query event q occurs, and ($\u03a6_z$, $\u03bb_z$) be the latitude and longitude for the occuring location of the candidate event z. The Haversine distance D between q and z can be calculated using Equation 1. R is the Earth's radius, taken as a constant representing a mean radius of 6,371 km. As shown in Algorithm 1, a distance-based ranking list is built to prioritize events that fall within a certain distance $\u03c4_d$, coupled with a boosting factor $\u03b2_d$, thereby enhancing the relevance of search results. This approach ensures an enhanced understanding of spatial relevance is applied to event retrieval, thus fulfilling the objective of providing users with more contextually appropriate search results based on geographical proximity.\n(2) Latitude relevance\nLatitude serves as the second geo-related feature, leveraged to capture similarities in climate, weather patterns, species distribution, and behaviors that are often correlated with latitudinal gradients. Studies have demonstrated that species' responses to climate change, including shifts in phenology and potential for extinction, are significantly modulated by their latitudinal position (Deutsch et al., 2008; Francis & Vavrus, 2012; VanDerWal et al., 2013). Furthermore, the generation and maintenance of latitudinal diversity gradients themselves are intricately linked to spatiotemporal climatic variations (Saupe et al., 2019). By incorporating latitude as a key feature, our model aligns with these established ecological and climatological research findings, as well as a recent GeoAl reproducibility research (W. Li et al., 2024) that show similar patterns and conditions are associated with similar latitudinal zones. We introduce a latitude booster to our ranking algorithm, which is conditioned on a distance threshold $\u03c4_d$. This booster is designed to increase the relevance of events that are not only geographically proximate but also share similar latitudinal characteristics. The boost factor for latitude $\u03b2_\u03c6$, is applied to events within a predefined latitudinal range $\u03c4_\u03c6$, effectively drawing attention to events that occur within\necologically and climatically similar zones. For instance, an event about an early arrival of Arctic tern in Iceland would be deemed more relevant to an event regarding and early observation of pintail ducks in Alaska than an event with a similar category as \u201cbirds\u201d but occurring at a different latitude, such as an event of usual migration routes of whooper swan reached Manitoba. By incorporating latitude into our feature set, we provide a perspective that goes beyond mere geographic coordinates, offering insights into potential environmental and ecological connections between events.\n(3) Temporal relevance\nThe goal of creating a time-related feature is to refine retrieved events by integrating a temporal dimension that resonates with the occurrence of environmental phenomena. Conventional retrieval systems, which may hinge on specific calendar dates, often overlook the cyclical nature of environmental events. To accommodate this temporal expectation, our methodology adopts the day-of-year as a more meaningful temporal metric, allowing us to rank events in a way that more accurately reflects their environmental significance and timeliness. Events are re-ranked based on this temporal proximity of events noted as the function $f_{time}$. The temporal ranking reflects the likelihood of events sharing similar environmental conditions or occurrences based on their seasonal timing. It ensures that events occurring within the same seasonal period are recognized as temporally similar. For example, in the region of Cowichan River in British Columbia, Canada, a drought event that happened in May 2019 is more temporally related to another drought event of this river in summer 2017, rather than a drought event recorded in fall, 2018. This feature prioritizes the alignment of events with seasonal cycles, which is particularly applicable for environmental datasets where timing can be critical."}, {"title": "3.3 Feature combination and rank fusion", "content": "The goal is to optimize the final event rankings, ensuring that the most relevant, contextually and spatiotemporally relevant events are surfaced to the user. Before going into details of the GT-R algorithm, we introduce notations for two statistical concepts: order statistics and rank statistics. Order statistics serve as the backbone for organizing the features derived from spatiotemporal events, allowing us to systematically assess their relevance. By defining a sample {$X_i$}$_{i=1,2,...,n}$ of size n, we utilize order statistics to arrange this event list in either descending or ascending order based on the criteria of each feature. The descending order, signified by $X_{(i)}$, denotes the i-th largest element, ensuring a hierarchy where $X_{(1)}$ > $X_{(2)}$ > \u2026 > $X_{(n)}$. Conversely, the ascending order prioritizes the event from the least to the most significant one, identified as $X_{[i]}$, which represents an inverse order as $X_{[1]}$ < $X_{[2]}$ < \u2026 < $X_{[n]}$. Ranking statistics further complements our framework. By applying ranking statistics to a sample {$Z_i$}$_{i=1,2,...n}$, we can assign a distinct rank based on the event's significance, either in descending order $R^d_i$, from the most to the least significant, or in ascending order $R^a_i$, from the least to the most significant. Through the function $r^d$ for descending ranks and $r^a$ for ascending ranks, we achieve a permutation of integers that mirrors the complex interplay of events.\nAfter constructing features for re-ranking, disparate features are synthesized into the final ranking list through feature combination and rank fusion. The convex combination approach offers a mathematical framework that allows for the weighted integration of various features (Norouzi et al., 2013), providing a flexible means to emphasize or de-emphasize certain aspects based on their relevance or reliability. Reciprocal Rank Fusion (RRF) is an approach that effectively aggregates rankings from diverse sources (Cormack et al., 2009), thereby reducing the bias and variance associated with individual ranking strategies. Our fusion strategy employs both as the rank fusion methodology that serves GT-R, and its procedure is demonstrated in Algorithm 1. It also adjusts individual features (with or without sorting), selectively applies booster factors, and calibrates the feature weights. This refined fusion strategy is designed to synergistically combine semantic-related features including semantic similarity and category relevance similarity into one variable with adjusted weights, then fuse with spatiotemporal proximity features into a unified ranking. By balancing these components within our rank fusion method, we aim to achieve a harmonized ranking output that captures the multifaceted nature of event relevance in the context of relevant spatiotemporal event retrieval."}, {"title": "4. Experiment", "content": "This section gives details of experiments regarding dataset, metrics, and comparing models."}, {"title": "4.1 Dataset", "content": "The experimental dataset is sourced from the Local Environmental Observer (LEO) Network, a collaborative project that shares local news and reports on unusual environmental events, with a focus on the Arctic regions as well as global occurrences (Mosites et al., 2018). Addressing climate change at the human scale, the LEO Network highlights local impacts on communities. The community-driven perspectives (Huntington, 2011) highlighted by the LEO project are essential in effectively tackling climate change challenges (Danielson et al., 2022). The repository of LEO Network events serves to increase public awareness regarding climate dynamics in the Arctic and across the planet.\nAn example of a LEO event is illustrated in Figure 3 (a). An event is structured and encompasses several content segments: title, summary, date, location, category, and a \"see also\" section for relevant event recommendations. Event reporting within the LEO Network is predominantly community-driven, with events initially submitted by local observers. These contributions are subsequently refined by experts possessing relevant domain knowledge to ensure the accuracy and quality of the information, which includes the manual curation of related events. This process is labor intensive, so the goal of this use case is harnessing the power of Al to assist automated retrieval of relevant events.\nAn examination of the lexical elements commonly mentioned in LEO events is visualized in Figure 3(b) through a word cloud. Prominent among these are locational and temporal references (e.g., \u201cAlaska\u201d, \u201cFinland\u201d, \u201cArctic\u201d, \u201cwinter\u201d), meteorological terms (e.g., \u201cstorm\u201d, \u201cflooding\u201d, \u201cwildfire\u201d), and species names (e.g., \u201csalmon\u201d, \u201cseal\u201d, \u201cbear\u201d). Event categories are"}, {"title": "4.2 Evaluation metrics", "content": "We focused on four standard evaluation metrics in information retrieval tasks, i.e., Recall, Hit Rate, Normalized Discounted Cumulative Gain (nDCG), and Mean Reciprocal Rank (MRR) as calculated using Equation (2-6) to assess model performance:\nIn stage 1 (retrieval), which aims to select the top 100 candidates, we employ two key metrics: Recall and Hit Rate. Recall@k measures the fraction of relevant events retrieved within the highest-ranking k positions of the result set. It is formally defined as the ratio of the number of relevant events retrieved in the top k positions ($R_k$) to the total number of relevant events available within the dataset ($R_{total}$) (Equation 2). Hit Rate@k reflects the frequency with which the retrieved set includes at least one relevant event within the top k positions for a given query event. It is calculated as the number of query events with at least one relevant event in the top k positions ($H_k$) divided by the total number of query events ($Q_{total}$) (Equation 3). Both metrics are crucial for assessing the effectiveness of the information retrieval model, with a higher Recall@k indicating a greater proportion of relevant events retrieved, and a higher Hit Rate@k suggesting that the model reliably retrieves at least one relevant event within the top k results across the query events.\nDuring stage 2 (re-ranking), the objective is to prioritize the top 10 most pertinent candidates from the 100 initially retrieved. For this purpose, we employ three metrics: Hit Rate, nDCG, and MRR. Hit Rate@k is used again but to measure the effectiveness of surfacing at least one relevant event to the top k responses when k is very small. The metric is useful when considering the user experience in scenarios where only the highest-ranked results are reviewed. nDCG at k (nDCG@k) is a rank-aware score for top-k retrieved documents. It measures relevance through a rank-sensitive weight factor log(i + 1), by assessing the alignment between a query's computed document ranking and the ideal ranking (Equation 4-5). For the top k responses, the higher nDCG@k \u2208 [0,1] indicates that the computed ranking is closer to the ideal ranking. MRR at k (MRR@k) reflects the ranking ability for the correct answer across k queries. For each query i, the reciprocal rank of its responses is determined by taking the multiplicative inverse of the rank position of the first correct answer (ranki), while MRR@k calculates the mean of the reciprocal ranks for k queries (Equation 6). A higher MRR@k value, denotes a more effective ranking system, as it reflects a tendency to rank the desired relevant event closer to the top position."}, {"title": "4.3 Implementation", "content": "To establish the initial set of top 100 candidates for the GT-R model, we utilized the LlamaIndex framework designed for LLM-based applications. This enabled fast integration of OpenAl's Ada- 002 embedding model to perform stage 1 retrieval. Ada was chosen due to its superior recall@100, using bi-encoding to calculate cosine similarity between event embeddings. Then went to stage 2 re-ranking. A two-step process was implemented to assess category similarity. First, NER was executed using zero-shot capabilities of the GPT-4 Turbo model. Then, cosine similarity was computed via cross-encoding to gauge the categorical alignment of events. We invoked OpenAl's API to output results directly as well-structured JSON objects. This approach streamlined the processing pipeline, significantly reducing the manual effort required for parsing and handling data, thereby easing the integration of human-in-the-loop procedures. Three models were tested for preliminary quality check including GPT-3.5-Turbo-0613, GPT-3.5- Turbo-1106, and GPT-4-1106-preview. With the same detailed CoT instructions, GPT-4 is the model that is mostly inclined to follow instructions and return desired extracted entities.\nNumerical calculations for distance and latitude difference were performed using the longitude and latitude metadata associated with each event. A distance threshold was established at 500 km, with a booster factor of 2, following the LEO Network's heuristic approach. Through empirical testing with thresholds of 3, 5, and 10 degrees, an absolute latitudinal threshold of 5 degrees was set, with a corresponding latitude booster factor of 2. The determination of fusion weights for semantic similarity and category similarity was conducted through grid search. The search was configured to maintain the sum of weights at unity, with increments of 0.1 within the range from 0 to 1. The optimal weight distribution was identified with a semantic similarity weight of 0.1 and a category similarity weight of 0.9, thereby achieving the best balance for the fusion."}, {"title": "5. Result", "content": null}, {"title": "5.1 Model performance", "content": "We use several zero-shot IR methods, leveraging both traditional techniques and novel LLMs, to serve as comparison with our proposed strategy for this content-based cold-start problem. Our evaluation focuses on key questions at the nexus of IR and GIS as below:\n(1) Retrieval stage:\n\u2022 RQ1: Input transformation: What is the optimal approach to curate input for event retrieval that captures content semantics with spatial and temporal elements?\n\u2022 RQ2: LLM embedding model efficacy: Given the refined input, which models excel in the retrieval of events?\n(2) Re-ranking stage:\n\u2022 RQ3: GT-R efficacy: How effective is the GT-R model compared to baseline and State- of-the-art (SOTA) models in the context of spatiotemporal event re-ranking?\n\u2022 RQ4: Spatial-temporal features: What effect do spatial and temporal features exert on the performance of GT-R model?"}, {"title": "5.1.1 Optimizing input for event retrieval (RQ1)", "content": "To answer RQ1, we compare different input settings to examine the role of geospatial and temporal information during either sparse retrieval or dense retrieval. For sparse retrieval, we utilize BM25 (Robertson & Zaragoza, 2009), a well-established approach based on term matching and normalized document length. BM25 models serve as baselines to observe the performance improvement by exact match of added information. For dense retrieval, we use a popular LLM embedding model, \u201ctext-embedding-ada-002\" from OpenAl (Neelakantan et al., 2022), which has the capability to understand context and is trained on massive datasets that potentially includes knowledge of location and date. For each input settings, we want to compare the performance of this pair of models for sparse retrieval and dense retrieval. In this way, we can understand whether a better performance of a good retrieval strategy comes from the versatility of LLM embedding models or the consideration of space and time.\nResults show that the integration of spatial and temporal contents into the retrieval input significantly improves performance, as demonstrated in Table 1. When we extend the input beyond the generic semantic contents, i.e., title and summary, to include location and date, we observe a marked improvement in model performance for both sparse retrieval and dense retrieval. For example, Recall@100 of the BM25 model increases from 70.5 to 76.2 upon the addition of these dimensions. Moreover, LLM embedding models such as Ada can gain a pronounced improvement in retrieval effectiveness when inputs are structured with prefixes that denote machine-understandable contexts. For instance, optimal input formatting involves appending prefixes \u201cTitle\u201d and \u201cSummary\u201d to semantic segments accordingly, and the prefix \"Location\u201d before \u201cKodiak, Alaska, United States\u201d, along with the prefix \u201cDate\u201d before \u201c10/2/23\u201d, indicating proper spatial and temporal context. This improvement suggests a higher compatibility of structured input with the algorithms driving dense retrieval models. Using an embedding model can epitomize our pursuit of a more contextually aware and semantically rich retrieval process. The results indicate that LLM embedding models, when fed with a well- defined spatiotemporal context, are better for the retrieval task compared with models using less structured inputs."}, {"title": "5.1.2 Retrieval model comparison (RQ2)", "content": "We employ a composite input strategy that combines title, summary, location, and date, each prefixed to enhance contextual relevance, as the basis for comparing the retrieval performance of various models. In the retrieval stage, our comparison encompasses a suite of popular"}, {"title": "5.1.3 Evaluating the GT-R model's re-ranking efficacy (RQ3)", "content": "To understand how much of an improvement our model offers over traditional approaches that are already in use, \u201cBM25 + distance & category booster\u201d serves as a baseline which is currently employed by LEON as a heuristic re-ranking method. We also compare a series of models to refine our understanding of the most effective techniques for re-ranking and how various re-ranking approaches perform in the context of spatiotemporal event recommendation. For the cross-encoding approach, the comparison includes two open-source models. One model is \u201cMS- MARCO-MiniLM\" as a cross-encoding model fine-tuned on MS MARCO data set (Reimers & Gurevych, 2020), and another model is \u201cBGE rerank\" (Xiao et al., 2023) for precise result ordering. We also integrate commercial models such as \u201crerank-english-v2.0\" from Cohere (Cohere, 2024b). For the approach directly employing LLMs as re-rankers, we test GPT-based models with different prompting strategies. For pointwise re-ranking via prompting, we test the binary relevance scoring capability of GPT3.5 Turbo based on the log probability of \u201cYes/No\u201d to assess a query-candidate pair (Liang et al., 2022). For listwise re-ranking as prompting more than one candidate at a time, we run the RankGPT model (Sun et al., 2023)"}, {"title": "5.1.4 Assessing the impact of semantics, space, and time to the recommendation results (RQ4)", "content": "To evaluate whether GT-R model benefits from consideration of semantics, space, and time, we conduct an ablation study by removing features of distance, latitude, time, category, and semantic respectively and report the results in Table 4. The model performance will drop when any of these re-ranking criteria is removed. The factor that causes the most significant performance decrease when removed is semantic similarity because of its critical role in the model's ability to rank relevant events effectively. Category similarity also plays a major role but to a lesser extent than semantic similarity. Spatial and temporal features contribute to the model's performance, with geo relevance being slightly more impactful than temporal relevance, and latitudinal relevance having the least impact among the factors studied. This result indicates"}, {"title": "5.2 Result examples", "content": "The performance results demonstrate the efficacy of the proposed framework in recommending contextually similar spatiotemporal events. Figure 4 visualization displays this capability, with Figure 4"}]}