{"title": "Dynamic Chain-of-Thought: Towards Adaptive Deep Reasoning", "authors": ["Libo Wang"], "abstract": "To reduce the cost and consumption of computing resources caused by computational redundancy and delayed reward assignment in long CoT, this research proposes the dynamic chain-of-thought (D-CoT) with adaptive reasoning time and steps. The researcher used simulation experiment to simulate the integration of D-CoT through Python 3.13 IDLE combined with a Python simulator based on GPTs. At the same time, the researcher used DeepSeek R1 as a control group to test and compare the performance of the D-CoT simulator in processing MIT OpenCourseWare's linear algebra exam questions. Experimental results show that D-CoT is better than DeepSeek R1 based on long CoT in three indicators: reasoning time, CoT length (reasoning steps) and token count, which achieves a significant reduction in computing resource consumption. In addition, this research has potential value in deep reasoning optimization that is used as a reference for future dynamic deep reasoning frameworks.", "sections": [{"title": "I.INTRODUCTION", "content": "As an emergent capability of large language models (LLMs) with huge parameter sizes in the reasoning process, chain-of-thought (CoT) allocates additional computing resources in a way that facilitates the gradual decomposition of tasks, which is particularly prominent in context learning (Wei et al. al., 2022). It follows that chain-of-thought provides techniques for gradually unfolding intermediate reasoning steps to enhance LLMs' ability to handle complex problems by disassembling them into coherent sub-steps (Feng et al., 2024). However, affected by context, prompt word design, and model learning bias, CoT has unfaithful interpretations, which leads to deviations between the reasoning process and the actual decision-making mechanism (Turpin et al., 2024). In addition, while CoT enhances the reasoning capabilities of LLMs, it also brings concerns about rising costs by prolonging the reasoning steps and improving quality (Jin et al., 2024). In response to the above-mentioned shortcomings of traditional CoT, more and more cutting-edge LLMs apply long CoT technology to demonstrate excellent reasoning capabilities when processing complex tasks (Chen et al., 2024; Wang et al., 2024).\nLong chain-of-thought (long CoT) aims to promote the model to have self-reflection and adaptive refinement in multi-step complex scene reasoning through hierarchical reasoning and stepwise verification to ensure accuracy and consistency (Wang et al., 2024). For few-shot CoT, the accuracy of LLMs output is linearly related to the number of reasoning steps, which means that the longer the number of steps, the more accurate the response, while reducing the CoT length will significantly reduce the response accuracy (Jin et al., 2024). Jin et al. found that even if errors occur in the intermediate steps of long CoT during the reasoning process, maintaining the necessary reasoning length will produce a high-accuracy response.\nIn the application of OpenAI's ol model, long CoT is combined with reinforcement fine-tuning (RFT) to further optimize LLMs' understanding and memory capabilities of multi-level reasoning processes (Huang et al., 2024; Zhang et al., 2024). In view of the shortcomings of LLMs in intermediate reasoning and adaptive learning capabilities, DeepSeek-R1 introduces large-scale pure reinforcement learning training to reduce reliance on supervised fine-tuning (SFT) (Guo et al., 2025). During training, in order to avoid instability in the initial cold start phase due to no longer relying on large amounts of labeled data, it constructs long CoT data and uses specific collection and processing methods to guide deeper reflection. and verification (Chen et al., 2024; Liu et al., 2024).\nHowever, long CoT requires a large number of intermediate steps in the reasoning practice process, its inherent computational redundancy and delayed feedback problems significantly increase the reasoning cost and consumption of computing resources, which is directly reflected in the exponential growth of reasoning time and steps. In order to improve accuracy, a large number of lengthy reasoning steps do not directly contribute to the final answer but only serve as an auxiliary process, resulting in the accumulation of computational overhead (Dai et al., 2024). These phenomena are often reflected in users' actual applications, especially LLMs with deep reasoning capabilities such as o3 min-high or DeepSeek R1. For example, when users use DeepSeek R1 to perform difficult and complex tasks, the number of reasoning steps increases significantly and the system response delay"}, {"title": null, "content": "increases significantly. Unstable reasoning fluctuations cause low efficiency, and even the response \"The server is busy. Please try again later\" appears frequently due to server resource saturation.\nThe deep gap lies in the fact that long CoT essentially runs on a static reasoning framework, making it difficult to flexibly adjust the number of reasoning steps according to the difficulty of different problems. The lack of dynamic adaptation mechanism usually treats the reasoning process as a linear expansion, which is reflected in the inability to adjust the length of the thinking chain according to the complexity of different tasks and environmental feedback. For example, when DeepSeek R1 faces deep reasoning on difficult tasks, even if some reasoning steps have minimal impact on the final decision, it still cannot actively compress or ignore redundant steps. This design flaw makes it difficult for the system to adaptively allocate computing resources, and the accumulation of inefficient reasoning causes nonlinear growth in reasoning time. In addition, the insufficient coupling between long CoT generation and RL reward mechanism is one of gaps about huge computational overhead. This technology is designed to enhance transparency and explainability, but is not tightly integrated with RL goals, which results in the lack of an effective credit allocation mechanism between reward signals during reasoning. Even though traditional RL relies on immediate or deferred rewards to adjust strategies, the value of a step in a long CoT is usually only determined when rewards are finally obtained."}, {"title": "II.PROPOSED MODULE & ALGORITHMS", "content": "In view of gaps, this research proposes dynamic chain-of-thought (D-CoT) to implement a state compression mechanism with adaptive reasoning steps to reduce computational redundancy.\n2.1. Dynamic Chain-of-Thought Framework\nDynamic chain-of-thought (D-CoT) is an LLMs reasoning framework with adaptive reasoning capabilities that reduces the consumption of cost computing resources by real-time adjustment of chain length (reasoning steps) and reasoning time. Compared with the fixed and linear expansion of reasoning steps of traditional long CoT, D-CoT dynamically adjust the length of CoT in real time, select key steps after rating different tasks. Its specific internal structure is shown in"}, {"title": "2.1.1. Hierarchical Adaptive Reward Optimization", "content": "The operating mechanism of the HARO (Hierarchical Adaptive Reward Optimization) algorithm is based on hierarchical reward allocation and adaptive reasoning adjustment. The algorithm uses the partial reward estimator to instantly evaluate the contribution of decisions at different levels of the deep reasoning process, and uses adaptive thresholding to dynamically correct the weight of the reasoning step (CoT length) to ensure the priority delivery of high-value information. In addition, this algorithm combines an importance-driven pruning strategy to instantly filter inefficient reasoning paths to reduce redundant computing overhead and improve overall reasoning efficiency.\n\u2022 Token Importance Evaluation\nEach reasoning step $c_i$ is assigned an importance score.\n$I(c_i) = \\alpha * A(c_i) + (1 - \\alpha) * GatingScore(c_i)$\nwhere $A(c_i)$ is the dominance estimate derived from RL and GatingScore reflects the token-level contribution.\n\u2022 Dynamic Adaptive Pruning Thresholding\nIt introduces a self-adjusting threshold $t_t$ based on historical success rates.\n$\\tau_t = \\gamma \\cdot \\tau_{t-1}+(1-\\gamma) \\cdot \\frac{1}{N} \\sum_{j=1}^{N} 1[I(c_j)]> \\tau_{t-1}]$\nwhere $t_t$ represents the updated threshold at time step t; $\\gamma$ is an attenuation factor used to control the retention of historical information; $1[I(c_j)>t_{t-1}]$ tracks whether past tokens have exceeded a previous threshold.\n\u2022 Progressive Reasoning Buffer (Adaptive Selection)\nDynamic adjustments in CoT segments are stored in buffers.\n$C_t = C_{t-1} + argmax_{c_i} (I(C_i) - \\tau_t)$\nwhere steps below $t_t$ are discarded unless they contribute significantly to global coherence; $C_t$ represents the CoT state at time t.\nReward Optimization and Auto-Regressive Feedback"}, {"title": "2.2. Detailed Framework Composition", "content": "The D-CoT framework consists of six key parts. First, it converts user input into dense vectors through tokenization and embedding. Then, the MoE-enabled transformer stack is applied to the multi-head self-attention mechanism to enhance semantic expression (Dai et al., 2024). Dynamic CoT Controller performs reasoning step screening through adaptive pruning and attention scores. Subsequently, auto-regressive decoding uses partial reward estimation for mark selection and incremental generation. Finally, the hierarchical CoT assembly integrates the deep reasoning process and uses reward-aligned refinement to optimize the final output."}, {"title": "2.2.1 Tokenization & Embedding", "content": "As the first part of D-CoT, it is responsible for converting natural language input into a dense vector representation that can be processed by the model (Tunstall et al., 2022). This process first decomposes the text into subword units through tokenization, maps it to a high-dimensional space through an embedding layer to capture semantic information and contextual dependencies, and combines it with positional encoding to provide sequence order information (Vaswani, 2017). The following are the supported algorithms in the workflow:\n\u2022 Tokenization Process\n$T = Tokenizer (Q)$\n\u2022 Embedding with Positional Encoding\n$X = E(T) + P,$\nSeamless Transition to MoE Stack\n$X \\longrightarrow MoE-Enabled Transformer$\nwhere converting user query Q into a token sequence T, this algorithm converts Q into discrete labeled units; Mapping tokens into dense embeddings while integrating positional encoding; X represents the final embedding representation that contains the vectorized representation and position encoding; E(T) represents the embedding function that converts mark tokens into corresponding embedding representations; P represents position encoding."}, {"title": "2.2.2 MoE-Enabled Transformer Stack", "content": "The MoE-enabled transformer stack uses a mixture of experts (MoE) mechanism to enhance selective computing capabilities through multi-head self-attention to ensure efficient acquisition of key information (Liu et al., 2024). The feed-forward layers combined with enhanced gating perform feature transformation based on dynamically selected experts to maximize reasoning efficiency (Guo et al., 2025). The residual-norm module provides gradient stability and reduces signal attenuation to ensure smooth flow of information in deep structures. The researcher demonstrated its workflow algorithm as follow.\nExpert Selection\n$at,e = Router (ut, e), Et,active = TopK{at,e | e = 1,..., N}$\n\u2022 Expert Outputs\n$h\u2081 = \\sum_{e \\in E_{active}} a_{te} f_e(u_t), u_t \\in \\mathbb{R}$\n\u2022 Importance Score\n$1\u2081 = \\gamma (\\sum_{e \\in E_{active}}a_{t,e})+(1-\\gamma) \\beta \u03b5$\nAfter expert assignment of input vectors through MoE, the refined semantic information is passed to the dynamic CoT controller for adaptive pruning and dynamic summarization to reduce reasoning redundancy (Fig. 3)."}, {"title": null, "content": "Adaptive Thresholding for Pruning and Summarization\n$\\Tau_{dyn} (r_t) = \\tau_0 + \\eta(r_t - r)$\n\u2022 Hierarchical Decoding & Assembly\n$Y_{t+1} = Assemble (B_T, {macro, micro})$\nwhere $t_{dyn}(r_t)$ is dynamic threshold; rt is partial reward; $\u03c4_0$ is a base threshold, r is a running average reward, and \u03b7 is a scaling factor; yt+1 represents the next generated token after decoding; Br represents the final buffer of CoT segments after iterative refinement; The macro-level summaries compress global information; micro-level expansions retain fine-grained details."}, {"title": "2.2.3 Dynamic CoT Controller", "content": "As the core of D-CoT's adjustment of deep reasoning, the dynamic CoT controller is responsible for optimizing the"}, {"title": null, "content": "length and content of CoT to reduce redundant calculations and the accumulation of unnecessary steps. The key technology is to dynamically adjust the reasoning steps to adapt the reasoning process to different types of task requirements, thereby improving the adaptability of LLMs in complex decision-making scenarios. The operation is based on adaptive pruning and summarization, it calculates the importance score of the token through gating logs and attention scores, and filters and compresses low-impact reasoning steps to ensure that the remaining information is the most relevant to the reasoning. Afterwards, the reasoning fragments are stored in the progressive reasoning buffer to ensure that key information is efficiently used in subsequent steps. The relevant supported algorithms are as follows:\n\u2022 Adaptive Pruning & Summarization\n$I\u2081$= Importance (t) (combining gating + attention)\nif $I\u2081<tdyn(ri)$, prune token t, else, optionally summarize\n\u2022 Progressive Buffer Update\n$B_{t+1} = B \u222a {Summarized t | I\u2081 \u2265 Tdyn(rt)}$\n\u2022 Partial Reward\n$r_t = RLFeedback (t), tdyn(rt) = \u03c4\u03bf + \u03b7 \u2022 (ri - r)$\n\u2022 Adaptive Token Expansion and Pruning\n$Z_{t+1} = Adjust (Generate (SelectTokens (Bt, Ot, rt), \u0392\u03c4, \u03b8\u03c4), rt+1)$\n\u2022 Reward-Guided CoT Assembly"}, {"title": null, "content": "$B\u2081 = AssembleCoT(Bt, {zt}t=-T, It+1 = RewardUpdate (rt, DecodeOut (Zt+1)))$\nwhere It is importance score of token t (gating + attention); Tdyn represents dynamic threshold based on reward ri; \u03c4\u03bf is base pruning threshold; \u03b7 is scaling factor for threshold adjustment; rt is RL feedback at step t / partial reward signal; rt+1 represents the updated reward at step t+1; r represents running average reward; B\u2081 represents token buffer at step t / the final structured CoT assembly after multiple iteration steps; B1+1 represents updated token buffer; t represents the current reasoning step in the iterative decoding process; Summarized t is compressed token representation; RLFeedback(t) represents RL-based reward function for token t; z\u0142 represents tokens at time step t / the generated reasoning tokens at step t; \u03b8\u2081 represents hidden parameters of the decoder at t; SelectTokens() represents token selection process; Generate() is based on generation of new tokens; Adjust() expands or prunes tokens (adaptive pruning or expansion decision) based on rewards and gating logs; T is The total number of CoT reasoning steps; RewardUpdate() updates the reward based on the previous reward and newly decoded token sequence; DecodeOut(z+1) is from the iterative reasoning process at step t+1; AssembleCoT() structures the final CoT by integrating generated reasoning tokens and their associated reward values.\nThe processed CoT fragment is passed to auto-regressive decoding with feedback loop (Fig. 4). It evaluates the effectiveness of each fragment based on the partial reward estimator and optimizes the reasoning strategy through dynamic adjustment to ensure that it reduces computational overhead while maintaining high accuracy output."}, {"title": "2.2.4 Auto-Regressive Decoding with Feedback Loop (Iterative Small Blocks)", "content": "Auto-regressive decoding with feedback loop calculates the relative contribution of tokens in the current reasoning process through the partial reward estimator. It filters qualified tags into iterative generation and gradually builds a complete reasoning chain. The generation process uses small block decoding to ensure that reasoning can optimize the structure under short-term feedback constraints to improve adaptability and efficiency. Then, adjusting/pruning dynamically selects whether to compress, retain or expand tags based on the partial reward signal and internal dynamic threshold to reduce redundant calculations and ensure efficient reasoning. Some of the filtered tokens are fed back to the progressive reasoning buffer to ensure consistent reasoning logic, and weight estimation is dynamically updated. The mathematical expression of attention merging is:\n\u2022 Hierarchical Decoding and Expansion Rule\n$C_{t+1} = CUT+1, Tt+1 = Adjust(Decode (Tt, Ot, ri),  Importance(Tr+1), rt+1))$\nReward-Guided Hierarchical CoT Refinement\n$F(t+1) = Refine (AssembleSplit (T1+1), RewardMap (Mt+1), rt+1))$\nwhere Ct is CoT structure at time step t; Ct+1 represents update CoT after processing new tokens; T\u2081 is token block at time step t; Tr+1 is adjusted token block after pruning/expansion; \u0398\u03b9 is Local model parameters, including gating logs and RL signals; rt represents partial reward signal guiding RL; rt+1 represents partial reward signal guiding hierarchical selection; Importance (T+1) represents function computing token importance; Adjust() is the operator for pruning/expanding token sequences; Decode() represents function generating token sequences from parameters; Ft+1 is final hierarchical CoT representation at step t+1; Mt+1 is macro-level summary tokens, m+1 is micro-level detail tokens; AssembleSplit() represents operator splitting token blocks into macro/micro segments; RewardMap() represents function ranking macro segments based on reward signals; Refine() represents operator merging macro, micro, and reward-driven structures into final hierarchical CoT.\nThe filtered and adjusted marker sequences enter the hierarchical CoT assembly, which integrates multi-level information to enhance the accuracy and adaptability of dynamic reasoning (Fig. 5)."}, {"title": "2.2.5 Hierarchical CoT Assembly", "content": "Hierarchical CoT assembly extracts high-level semantic information through macro summary builder and establishes a global reasoning structure to ensure contextual consistency. The fine-grained information is stored through micro detail buffer, which retains important reasoning details to avoid semantic loss. The stored information is further analyzed by the contextual mapper, which enables the reasoning process to dynamically adapt to contextual changes by integrating different levels of information. The reward-aligned refinement adjusts the reasoning weight through the RL mechanism and dynamically adjusts the contribution of key markers based on the learning reward signal. The algorithm is as follows:\n\u2022 Macro / Micro Separation\n$C_{macro}, C_{micro} = Assemble (C, PartialRewards)$\n\u2022 Reward-Aligned Refinement\n$C_{final} = Refine (C_{macro}, C_{micro}, RewardMap)$\nwhere C still represents CoT; $C_{macro}$ represents is a macro-level reasoning component; $C_{micro}$ is micro-level reasoning component; Assemble (\u00b7) is assembly function, it is responsible for dividing the reasoning steps and decomposing the complete CoT structure into $C_{macro}$ and $C_{micro}$; $C_{final}$ represents final refined CoT; Refine (\u00b7) represents refinement function."}, {"title": "2.2.6 Final Output Generation", "content": "After completing the hierarchical CoT assembly, the final output generation produces the final answer in response. At the same time, integrated RL reward re-evaluates reasoning weights and adjusts pruning and gating policies through importance estimators, and feeds the updated decision signals back to the dynamic CoT controller to optimize future reasoning processes. The supported algorithms are as follows:\n\u2022 Final Answer\n$A = OutputAnswer(C_{final})$\nCompute Episode Reward\n$R_{episode} = RewardFunction (A, EnvironmentState)$\n\u2022 Update Dynamic CoT Parameters (Loop)\n$\\Theta_{CoT} = \\Theta_{CoT} + \u03b7 \u2207 @COT R_{episode}$\nwhere A is final answer; Cfinal represents the output result from hierarchical CoT assembly; Repisode represents episode reward; Ocot is is a parameter of D-CoT; \u2207 @COT Repisode is gradient of"}, {"title": null, "content": "episode reward; \u03b7 is the learning rate that controls the update amplitude of the D-CoT parameters."}, {"title": "III.EXPERIMENTS", "content": "D-CoT is essentially a modular enhancement technology for deep reasoning. Its core mechanism includes adaptive step adjustment and reasoning pruning, but does not involve changes to the pre-training architecture of LLMs. This means that it needs to be embedded into existing LLMs to achieve performance as it relies on its language understanding capabilities rather than independent execution (Kaur et al., 2024). Therefore, this research chose simulation experiment to test D-CoT's adaptive step adjustment, computational resource allocation, reasoning pruning and reward alignment in the deep reasoning process. It allows the researcher to verify the impact of different regulation strategies on reasoning performance in a controlled environment, avoiding being limited by the immutability of the internal architecture of LLMs (Edmonds & Hales, 2005; Kleijnen, 2018).\nIn addition, except for a few open sources such as DeepSeek R1 and MemGPT, mainstream ones such as OpenAI 01 and 03 min-high are still closed-source LLMs, and the API interface cannot provide internal control capabilities for the reasoning steps (Lu et al., 2024). The researcher was unable to directly modify the internal reasoning architecture of LLMs by integrating D-CoT for dynamic step adjustment and reward alignment testing (Arrieta et al., 2025)."}, {"title": "3.1. Experimental Setup", "content": "Considering that it is difficult to directly integrate into existing closed-source LLMs and a simulation experiment was used to test its reasoning performance, the researcher simulated the process of integrating D-CoT into current LLMs through custom GPTs with runnable code. By developing code in Python 3.13 IDLE and uploading it to the Python simulator based on GPTs, the researcher simulate the dynamic control of D-CoT's deep reasoning steps, calculation mechanism adjustment and reward alignment strategies, and simulate its time application scenarios in LLMs. The Python simulator has the highly complex ability to execute code in customized GPTs, which earned it a 4.2-star rating, ensuring flexibility and controllability in testing (Fig. 6). The researcher uploaded the D-CoT code to a Python-based simulator as an experimental group, and selected DeepSeek R1 with deep reasoning capabilities as a control group to compare the performance differences of reasoning."}, {"title": "3.2. Dataset", "content": "Based on the fact that solving linear algebra requires high structure and strong reasoning requirements, it has become an effective test set to evaluate the reasoning ability of CoT. The researcher chose the test questions of 18.06 Spring 2022"}, {"title": null, "content": "Problem Sets and Exams of 18.06 Linear Algebra of MIT OpenCourseWare as experimental data. Linear algebra usually includes multi-step calculations and logical derivation. Both Long CoT and D-CoT need to show the details in the reasoning process. In addition, the test questions cover questions of different difficulty levels and can test the adaptability and robustness of dynamic D-CoT under various reasoning challenges. And because this test question is provided in text format, it is suitable for uploading to GPTs that emulate Python and ensures the independent operation of D-CoT. This data has been placed in Appendix 1, and it is noteworthy that the MIT OpenCourseWare license authorizes the test questions to be used publicly and experimentally for non-commercial purposes."}, {"title": "3.3. Implementation", "content": "The researcher uploaded the D-CoT code developed by Python 3.13 IDLE to the Python simulator based on GPTs to simulate its integration process in LLMs. At the same time, the researcher also set up corresponding instructions in DeepSeek R1 to execute and process linear algebra test questions. Afterwards, the researcher uploaded the 18.06 Spring 2022 Problem Sets and Exams one by one to the simulator and DeepSeek R1 integrated with D-CoT and recorded the experimental results. In order to avoid deviations in image semantics recognition between the experimental group and the control group that would affect the experimental results, all test questions were converted to machine-readable format and renumbered into 18 questions without changing the original meaning.\nIn view of the problem of this research, the researcher sets three core evaluation indicators to evaluate the reasoning cost and computing resource consumption of D-CoT. These indicators are reasoning time, CoT length (reasoning steps) and token count to quantify the computational overhead of the D-CoT simulator and DeepSeek R1 respectively. In addition, since this research is committed to reducing computational redundancy and optimizing reasoning steps rather than verifying the accuracy of calculation results, it does not include accuracy score as an evaluation indicator. Complete experimental records and codes have been uploaded to GitHub repository to ensure reproducibility."}, {"title": "IV. RESULT & DISCUSSION", "content": "After the above execution process, the experimental group and the control group respectively display the data results except for reasoning time, CoT length (reasoning steps) and token count (Fig. 7).\nAccording to the data results, in terms of reasoning time, the maximum reasoning time of DeepSeek R1 reaches 295 seconds, while D-CoT is significantly reduced to 179.65 seconds, showing its computing optimization capabilities in"}, {"title": null, "content": "long reasoning processes. Judging from the distribution trend, the reasoning time of DeepSeek R1 is mostly concentrated in 50 to 150 seconds, but some questions exceed 200 seconds, showing the instability of its reasoning chain length; in comparison, D-CoT mostly maintains in the range of 40 to 120 seconds, and the fluctuation is small, indicating that it can effectively control the reasoning time. In terms of CoT length (reasoning steps), the number of steps of DeepSeek R1 is up to 8 steps, while D-CoT is controlled within 6 steps, and most questions only require 3 to 6 steps, which significantly shows that it can reduce redundant reasoning steps through a dynamic adjustment mechanism. As for the token count, the token usage of DeepSeek R1 is up to 320 tokens, while that of D-CoT is significantly reduced to 180, and its overall distribution is mainly concentrated in the 70 to 180 range. Compared with DeepSeek R1, which exceeded 300 in some complex questions, D-CoT has demonstrated effective suppression of token growth and reduction of computing resource consumption. The results provide the evidence that D-CoT shows significant advantages in reasoning time, CoT length (reasoning steps) and token count usage. It can reduce computational redundancy, optimize the deep reasoning process, and maintain stability in difficult reasoning tasks."}, {"title": "V. LIMITATION & FUTURE RESEARCH", "content": "In light of the fact that external researcher do not have the authority to access and modify the internal architecture of most current mainstream closed-source LLMs, D-CoT is difficult to directly integrate and uses simulation experiments to compare with DeepSeek R1 through a customized GPTs platform. However, the computing power, neural network size and parameters of GPTs are significantly different from DeepSeek R1, which causes the generalizability of the data results to be affected by additional factors. In addition, D-CoT relies on auto-regressive decoding and feedback mechanisms to adjust dynamic reasoning steps. This mechanism causes certain interference in fully reproducing the effects of pruning and reward alignment of LLMs under different contextual conditions in a simulation environment. The above limitations show that obtaining LLMs of open source architecture for internal integration testing in the future will help to more accurately evaluate the actual application performance of D-CoT."}, {"title": "VI. CONCLUSION", "content": "In view of the fact that long CoT has a large number of intermediate steps in reasoning, which increases reasoning costs and consumption of computing resources due to inherent computational redundancy and delayed feedback, this research proposes dynamic chain-of-thought (D-CoT) to implement a dynamic deep reasoning mechanism of adaptive pruning, reward alignment, and step-wise control. It simulated the integration of D-CoT through simulation experiment in Python 3.13 IDLE combined with the Python simulator based on GPTs, and used DeepSeek R1 based on long CoT as a control group to test its performance in solving the 18.06 linear algebra test questions of MIT OpenCourseWare. Experimental results show that D-CoT can effectively reduce computing resource consumption in reasoning time, CoT length and token count, thereby optimizing and reducing the computing cost and resource consumption of deep reasoning. Although D-CoT is limited"}, {"title": null, "content": "by the current closed-source environment and cannot be directly integrated into LLMs for testing, it provides a new feasibility perspective for deep reasoning optimization of LLMs."}, {"title": "APPENDIX 1", "content": "The data for this research comes from the 18.06 Spring 2022 Problem Sets and Exams questions of MIT OpenCourseWare's 18.06 Linear Algebra, which were uploaded to the D-CoT simulator and DeepSeek R1 to answer respectively. In order to avoid the image semantic recognition bias of matrix operations from interfering with the experimental results, the researcher converted all test questions into computer language input and renumbered them into 18 questions without changing the original meaning. This test may be used non-commercially for the experimental purposes of this study under the terms of the Creative Commons Attribution-Non Commercial sharealike (CCBY-NC-SA) license.\nProblem 1 (4+4+6 points):\nThe matrix A is given by\nA = LUL-1U-1\nfor\n$L =\\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ -1 & 1 & 0 & 0 \\\\ 1 & 3 & 1 & 0 \\\\ -1 & 1 & -2 & 1 \\end{pmatrix}$ , $U = \\begin{pmatrix} 1 & 2 & 0 & 1 \\\\ 0 & 1 & -1 & 1 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}$\n(a) Write an expression for A-1 in terms of L, U, L-1, and/or U-1 (but you don't need to actually multiply or invert the terms!)\n(b) What is the determinant of A?\n(c) Solve PAx = b for x, where P is the 4 \u00d7 4 permutation that swaps the\n1stand 4th elements of a vector, and b = $\\begin{pmatrix} -5 \\\\ 4 \\\\ 11 \\\\ -3 \\end{pmatrix}$ (You can\nget partial credit by just outlining a reasonable sequence of steps here that doesn't involve a lot of unnecessary calculation.)\nProblem 2 (4+6 points):\n(a) If a and x are vectors in Rn, then aaTx can be computed using either left-to-right as (aaT )xor right-to-left as a(aTx), where the parentheses indicate the order of operations. Roughly count the number of arithmetic operations (additions and multiplications) in these two approaches: say whether each approach scales proportional to n, n2, n3, etcetera.\n(b) A is an n\u00d7 n real matrix and x is an n-component real vector. Indicate which of the following must be equal to one another:\ntrace(Axx\u00b9), trace(xAxT), trace(xAx), xAx,\ntrace(xTxA), xxTA, trace(xxTA), determinant(xxTA).\nFor the expressions that are equal, indicate how you would evaluate this quantity in a cost (in arithmetic operations) proportional to n\u00b2."}, {"title": null, "content": "Problem 3 (4+4+4+5 points):\nYou have a 4 \u00d7 3 matrix A = (q1 2q2 3q1+4q2), where we have expressed the three columns of A in terms of the orthonormal vectors\n$q1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ -1 \\end{pmatrix}$ $q2 = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$\n(a) What is the rank of A?\n(b) Give a basis for N(A).\n(c) You are asked to calculate the projection matrix Ponto C(A). Your friend Harvey Ard suggests applying the formula P = A(AT A)-1AT he memorized in linear algebra. Explain why this won't work here, and give an even simpler (correct) formula for P in terms of the quantities above. (You need not evaluate P numerically, just write a formula in terms of products of quantities defined above.)\n(d) Find the closest vector to x = $\\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$ in N(AT).\nProblem 4 (3+4+4+6 points):\nThe nullspace N (A) of the real matrix A is spanned by the vector v =\n$\\begin{pmatrix} 1 \\\\ 2 \\\\ 2/3 \\\\ 4 \\end{pmatrix}$\n(a) Give as much true information as possible about the size (the number of rows and columns) of A.\n(b) Give an eigenvector and eigenvalue of the matrix B = (3I - ATA)(3I + AT A)-1 .\n(c) Aside from the eigenvalue identified in the previous part, all other eigen- values A of B must be (circle/copy all that apply): purely real, purely imaginary, zero, negative real part, positive real part, jaj < 1, jaj > 1, jj \u2264 1, jaj \u2265 1.\n(d) Give a good approximate formula for of Bn\n$\\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 8. \\end{pmatrix}$\nfor large n. (Give an explicit numerical vector, possibly including simple functions of n like 2\" or n\u00b3... no other abstract symbolic formulas.)\nProblem 5 (10 points):\nDescribe (give an explicit numerical result with as few unknowns as possible) all possible linear combinations of the vectors\""}, {"title": null, "content": "$a1 = \\begin{pmatrix"}, 1, 1, 0, 0, "end{pmatrix}$ , $a2 = \\begin{pmatrix} 2 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix}$ , $a3 = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\\\ 3 \\end{pmatrix}$ ,$a4 = \\begin{pmatrix} 4 \\\\ -1 \\\\ 4 \\\\ -1 \\end{pmatrix}$\nthat give the vector x = $\\begin{pmatrix} 1 \\\\ 4 \\\\ -1 \\\\ 5 \\end{pmatrix}$\nProblem 6 (8+8 points):\nProfessor May Trix is trying to construct an 18.06 homework question in which dx/dt = Ax has the solution\nx(t) = v\u2081 cos(2t) + v2e\u00af\u00b9 + V3 sin(2t)\nfor some nonzero real constant vectors V1, V2, V3, and some initial condition x(0). Help May construct A, V1, V2, V3, and x(0):\n(a) Write down a numerical formula for a possible real matrix A such that A is as small in size as possible and where A contains no zero entries. Your formula can be left as a product of some matrices and/or matrix inverses you don't need to multiply them out or invert any matrices, but you should give possible numeric values for all of the matrices in your formula. (You don't need to explicitly check that your A has no zero en- tries as long as zero entries seem unlikely. e.g. the inverse of a matrix with no special structure probably"]}