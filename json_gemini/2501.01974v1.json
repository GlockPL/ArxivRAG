{"title": "Hawkes based Representation Learning for Reasoning over Scale-free Community-structured Temporal Knowledge Graphs", "authors": ["Yuwei Du", "Xinyue Liu", "Wenxin Liang", "Linlin Zong", "Xianchao Zhang"], "abstract": "Temporal knowledge graph (TKG) reasoning has become a hot topic due to its great value in many practical tasks. The key to TKG reasoning is modeling the structural information and evolutional patterns of the TKGs. While great efforts have been devoted to TKG reasoning, the structural and evolutional characteristics of real-world networks have not been considered. In the aspect of structure, real-world networks usually exhibit clear community structure and scale-free (long-tailed distribution) properties. In the aspect of evolution, the impact of an event decays with the time elapsing. In this paper, we propose a novel TKG reasoning model called Hawkes process-based Evolutional Representation Learning Network (HERLN), which learns structural information and evolutional patterns of a TKG simultaneously, considering the characteristics of real-world networks: community structure, scale-free and temporal decaying. First, we find communities in the input TKG to make the encoding get more similar intra-community embeddings. Second, we design a Hawkes process-based relational graph convolutional network to cope with the event impact-decaying phenomenon. Third, we design a conditional decoding method to alleviate biases towards frequent entities caused by long-tailed distribution. Experimental results show that HERLN achieves significant improvements over the state-of-the-art models.", "sections": [{"title": "1 Introduction", "content": "Temporal Knowledge Graph (TKG) is a dynamic multi-relational graph used to record evolutionary events and knowledge in the real world. TKGs indicate facts as quadruples (subject, relation, object, time) and are actually sequences of temporal subgraphs divided by the time (timestamp) dimension. Reasoning over TKGs aims to infer the missing quadruple facts, which has two settings: interpolation and extrapolation. Given a TKG with timestamps from to to tk, interpolation aims at inferring missing facts that occur at time t, where to <t<tk. Oppositely, extrapolation attempts to predict facts that occur at time t with t > tk. In this paper, we focus on the extrapolation setting, which has gained much attention in recent years due to its great practical value in event prediction, question answering, and other areas (Wang et al., 2019b; Lan and Jiang, 2020). There are mainly two extrapolation tasks: entity prediction and relation prediction. We aim to propose a unified model that can accomplish both the entity and relation prediction tasks.\nThe key to TKG reasoning is modeling the structural information and evolutional patterns of the TKGs. The prior extrapolation TKG reasoning models such as CyGNet (Zhu et al., 2021) and CENET (Yi et al., 2023), learn the evolutional patterns by generating the historical event vocabulary to predict repetitive events. Later models such as RE-GCN (Li et al., 2021), HisMatch (Li et al., 2022) and HGLS (Zhang et al., 2023b), employ a relational graph convolutional network (RGCN) (Schlichtkrull et al., 2018) to capture the structural information from historical snapshots and use a recurrent neural network (RNN) to model the evolutional patterns. Some recent works such as TITer (Sun et al., 2021) and DREAM (Zheng et al., 2023) introduce reinforcement learning on the TKG reasoning task.\nNevertheless, while TKGs are reflections of the real world, the structural and evolutional characteristics of real-world networks have not been considered in previous models. In the aspect of structure, real-world networks (e.g., social-networks) usually exhibit clear community structure and scale-free (long-tailed distribution) properties (Barab\u00e1si and Albert, 1999). In the aspect of evolution, the impact of an event decays with the time elapsing (Hawkes, 1971). Taking these characteristics into consideration not only can improve the reasoning performance, but also better facilitate the down-stream tasks.\nIn this paper, we propose a novel TKG reasoning model called Hawkes process-based Evolutional Representation Learning Network (HERLN), which learns structural information and evolutional patterns of a TKG simultaneously, considering the characteristics from real-world networks: community structure, scale-free and temporal decaying. Specifically, our model consists of three modules: an embedding initializing module, an evolution encoding module and a conditional decoding module.\nIn the embedding initializing module, to exploit the community structure properties of TKGs, we first find communities in the input TKG, and apply a graph convolution network to get embeddings of events within each community. The embeddings are then used as inputs in the evolution encoding module, which make the evolution encoding module output more similar intra-community embeddings. In the evolution encoding module, to cope with the event impact-decaying phenomenon, we design a Hawkes process-based relational graph convolutional network (HRGCN). The graph convolutional network contracts the structural information of the TKG, while the Hawkes process assigns different weights to the timestamps such that the impacts of events decay over time. In the conditional decoding module, to alleviate biases towards frequent entities caused by long-tailed distribution, we construct a conditional decoder which consists of a hyper network and a query-specific decoder. The hyper network adjusts the parameters according to the query events and the decoder generates conditional intensity scores for the candidate entities based on the adjusted parameters.\nThe main contributions of this work are summarized as follows:\n\u2022 We recognize that the TKGs possess the structural and evolutional characteristics inherited from real-world networks: community structure, scale-free and temporal decaying, but they have not been considered or well exploited in previous studies.\n\u2022 We propose a Hawkes process-based evolutional representation learning network (HERLN), which consists of three modules: (1) An embedding initialize module, which extracts semantic information of community structure; (2) An evolution encoding module, which addresses the temporal decaying of event impact; (3) A conditional decode module, which alleviates the biases towards frequent entities caused by long-tailed distribution.\n\u2022 Our proposed model HERLN can predict entities and relations at the same time. Experimental results on four benchmark TKG datasets show that HERLN achieves significant improvements over the state-of-the-art models."}, {"title": "2 Related work", "content": "Embedding-based methods encode the whole or part of the TKGs to obtain the embeddings of entities and relations, and use the embeddings to evaluate the possibility of missing facts.\nRE-NET (Jin et al., 2020) proposes an autoregressive architecture which uses a graph neural network (GNN) to capture local entity embeddings and a RNN to model interactions between entities over time. RE-GCN (Li et al., 2021) constructs a static graph to get the static attributes and presents a framework that can execute both entity and relation reasoning.\nEvoKG (Park et al., 2022) uses an autoregressive architecture and captures the ever-changing structural and temporal dynamics via recurrent event modeling. HiSMatch (Li et al., 2022) generates background graphs, entity-related graphs and relation-related graphs to jointly model the evolutional patterns. HGLS (Zhang et al., 2023b) models local snapshots or global graphs by using different GNNs and decodes them to get the predicting scores. CENET (Yi et al., 2023) combines historical and non-historical information and identifies highly related entities via contrastive learning. TARGAT (Xie et al., 2023) captures the interactions of multi-facts at different timestamps. DLGR (Xiao et al., 2024) learns the local and global perspective representations in a contrastive manner. DSTKG (Li et al., 2024) introduces two latent variables to capture the dynamic and static characteristics of entities in TKGs.\nL2TKG (Zhang et al., 2023a) finds the missing relationships on the known KGs first and then reasons on the completed graph and the original graph jointly. RETIA (Liu et al., 2023) constructs a hyper-graph to connect different relations in a high-dimensional space. These models use RNNs to represent the temporal information. Thus they are based on an assumption that the temporal sequences are equidistant, which is inconsistent with many real-life event sequences (Sun et al., 2022)."}, {"title": "2.2 Path-based methods", "content": "Path-based methods find several related paths of query facts and select the most relevant one as the answer. TITer (Sun et al., 2021) adopts reinforcement learning to sample actions from query-related trajectories based on a time-shaped reward function. xERTE (Han et al., 2021a) samples and prunes the query-related subgraph according to query-dependent attention scores. TANGO (Han et al., 2021b) explores the neural ordinary differential equation to build a continuous-time model. TLogic (Liu et al., 2022) automatically mines recurrent temporal logic rules by extracting temporal random walks. DREAM (Zheng et al., 2023) use generative adversarial networks to design an adaptive reward function. However, the path-based methods focus on the local structure graph of the query, ignore the potential connection of events, and do not perform well on long-term reasoning.\nIn addition, it is worth noting that methods such as xERTE (Han et al., 2021a) and HISMatch(Li et al., 2022) consider the impact of temporal information on prediction results. They encode timestamps and concat timestamps with entity embeddings. However, different from our work, they actually learns that entities with different time intervals have different impacts on results, rather than considering the gradual decay of event impacts."}, {"title": "2.3 Hawkes process-based methods", "content": "The Hawkes process (Hawkes, 1971) is a stochastic process that models sequential discrete events occurring in continuous time. There are several works that combine the Hawkes process and neural networks for TKG reasoning. Know-Evolve (Trivedi et al., 2017) introduces a temporal point process to model facts evolved in the continuous time domain. GHNN (Han et al., 2020) proposes a graph Hawkes process to capture the potential temporal dependence across different timestamps. However, Know-Evolve and GHNN do not use the graph structural information. GHT (Sun et al., 2022) uses a temporal Transformer to capture long-term and short-term information jointly. However, none of the previous work has considered problem of temporal decaying of events' impacts. Our proposed module uses the Hawkes process to assign different weights to the timestamps during message passing, thus the information of the event impact-decaying is encoded and utilized."}, {"title": "3 Problem Formulation", "content": "A temporal knowledge graph (TKG) $G={E, R, T, F}$ is a directed multi-relational graph, where $E, R, T$ and $F$ denote the sets of entities, relations, timestamps and facts, respectively. A node in G represents an entity $i \u2208 E$, and an edge $e_{ij}$ represents the interaction between node i and node j with relation $r \u2208 R$ at timestamp $t \u2208 T$. A fact in G is a quadruple $q = (s, r, o, t)$ that represents a real-world event consisting of the relation r between a subject entity s and an object entity o at timestamp t.\nGiven a TKG $G_{[t_1:t_k]} = {E, R, T, F | T = [t_1, t_k]}$, the extrapolation reasoning task is to predict object $o_q$ in a query like $(s_q, r_q, ?, t_q)$ where $t_q > t_k$, or predict relation $r_q$ in a query like $(s_q, ?, o_q, t_q)$ where $t_q > t_k$."}, {"title": "4 Method", "content": "The proposed Hawkes process-based evolutional representation learning network (HERLN) model is shown in Fig. 1, it consists of three modules, an embedding initializing module, an evolution encoding module and a conditional decoding module. The embedding initializing module detects communities in the input TKG and embeds the interaction frequencies between entities within each community into the initialized entity embeddings. With the initialized entity embeddings as input, the evolution encoding module updates the entity embeddings by learning the structural information from historical graph. The conditional decoding module uses representations of query quadruple to adjust the parameters and generate scores for candidates."}, {"title": "4.1 Embedding Initializing Module", "content": "To initialize the embeddings of entities, we first identify the communities, and then extract semantic information in the communities to get the initialized entity embedding matrix $H_c$."}, {"title": "4.1.1 Community Detection", "content": "The interactions between entities in real world exhibit distinct community structures. Exploiting of the community structural properties contributes to the improvement of reasoning performance. For example, an entity that cooperates with a country like America is more likely to be a government or an organization rather than the citizens of a country. This information could be used to reduce the scores of entities that are not consistent with the facts.\nWe use a community detection algorithm on the entire TKG, which divides the entities set into different communities according to interaction frequencies, and obtains a graph that only contains inner-community links. The algorithm assigns each entity i to its community ci, and there are a total of K communities in the TKG.\nIn TKGs, there are no inherent community labels. Therefore, we require an unsupervised and reliable method to detect possible communities in the TKGs. And since a TKG is a multi-relational graph, we extend the Louvain algorithm (Blondel et al., 2008) to handle with the multi-relational links. Specifically, we calculate modularity $Q_r$ for different relation r by Eq. 1, which is an indicator that measures the quality of community detection."}, {"title": null, "content": "$Q_r = \\sum_c [\\frac{1}{2m} \\sum_{i,j \\in c} A_{ij}]$ = $\\frac{1}{2m} [\\sum_{in} - (\\frac{\\sum_{tot}}{2m})^2] = [e_c - a^2]$(1)\nwhere $\\sum_{in}$ is the sum of weights of inner-community edges with relation r; $\\sum_{tot}$ is the sum of weights of all the edges with relation r and m is the total weight of edges on the whole graph.\nDuring the optimization process, when a community is merged into another community, the algorithm will calculate the modularity of the new entire graph, compare it with the modularity before merging to get $\u0394Q$ as described in Eq. 2."}, {"title": null, "content": "$\u0394Q = \\sum \u0394Q$\n$= \\sum_r [\\frac{1}{2m} ((\\sum_{i,in} + k_{i,in}) - (\\frac{\\sum_{tot} + k_i}{2m})^2] = [e_c - a^2]$\n$= \\sum_r [\\frac{1}{2m} ((\\sum_{in}  - (\\frac{\\sum_{tot}}{2m})^2 \u2013 (\\frac{k_i}{2m})^2)]$(2)\nwhere $k_{i,in}$ is the sum of the edges' weights between node i and the new community in; $k_i$ is the sum of the edges' weights between node i and all the nodes in the graph. The algorithm ends when $\u0394Q$ no longer changes."}, {"title": "4.1.2 Embedding Initialization", "content": "In order to import the information contained in the communities into the embeddings, we use a GCN to generate embeddings on the community subgraphs, which is formalized as:"}, {"title": null, "content": "$h_i = \u03c3 (\\frac{1}{N_i} \\sum_{j\u2208N_i} W h^{init} \\delta (c_i, c_j) + W_0 h^{init})(3)$\nwhere $h^{init}$ and $h^{init}$ are randomly initialized embeddings of nodes i, j; W is the parameter of message passing between nodes; $W_0$ is the parameter of self updating of a node; $\u03b4(c_i, c_j)$ is an indicator which is set to 1 if i and j belong to a same community and 0 otherwise; \u03c3() is an activate function; $N_i$ is the set of neighbors of node i."}, {"title": "4.2 Evolution Encoding", "content": "After getting the initialized embedding matrix, the next move is to encode the candidate-related historical structure to learn the evolutional patterns of events. In real-world events, the entity's status changes over time. Another common real-world phenomenon is that the impact of a event decays over time, The entity embeddings should be able to cope with the variations. This module achieves these points by updating the entity embeddings with historical information."}, {"title": "4.2.1 The Hawkes Process on TKGS", "content": "The Hawkes process is a stochastic process that models a sequential discrete events that occur chronologically, which is typically modeled by a conditional intensity function. The intensity function $\u03bb_{(s,r,o)}(t)$ represents the probability that events happen at t, it is defined as follows:"}, {"title": null, "content": "$\u03bb_{(s,r,o)}(t) = \\sum_{(s',r',o,t')\u2208 F_o} \u03b3_{(s',r',o,t')} \u03ba(t-t') + \u03bc_{s,r,o}(t)$(4)\nwhere $\u03bc_{s,r,o}(t)$ is the base intensity at time t; $F_o$ is the set of historical events related to node 0; $\u03b3_{(s',r',o,t')}$ represents the amount of excitement induced by the corresponding events on results and $\u03ba()$ is a kernel function to model the effect of historical events on the current event."}, {"title": null, "content": "To integrate the Hawkes Process into the TKG, we treat the Hawkes Process as an encoder-decoder network. For encoding, we use a function En() to get representations $h_s, h_r$ and $h_o$ derived from their historical neighbors' information, as shown in Eq. 5. For decoding, we transfer the representations into a certain intensity value with an appropriate decoding function $D_e()$, as shown in Eq. 6.\n$h_s, h_r, h_o = En(G) (5)$\n$\\lambda_{d(s,r,o)} (t) = D_e(h_s, h_r, h_o) (6)$\nWe present detailed implementations of En() and $D_e()$ in the following sections."}, {"title": "4.2.2 Encoding with Hawkes process-based RGCN", "content": "To get the historical representation of entities described in Eq.5, we design a Hawkes process-based relation graph convolutional network (HRGCN) as En() to pass message and update entity embeddings on the TKG.\nAs shown in Fig. 2, the improvement of HRGCN over traditional RGCN is that HRGCN can effectively learn the structural information from neighbors and assign weights of the messages which represents the temporal decaying of these messages, i.e."}, {"title": null, "content": "$h_0 = \u03c3[\\frac{1}{N_o} (W_o h_o + \\sum_{s,r,t' \u2208 F_t} W_r (h_s + h_r) \u03ba (t - t'))](7)$\nwhere $h_s, h_o$ and $h_r$ are input embeddings of nodes s, o and edge r got from $H_c$ and R, respectively; $F_t$ represents historical neighbors of node o, which contains all quadruples = (s, r, o, t') where t' < t; $W_r$ is the parameter of message passing between s and o; $W_o$ is the parameter of self updating of node o; $\u03ba(t \u2013 t') =  \\frac{\u03ba(t \u2013 t')}{\\sum_{s,r,o,t} \u2208F_t \u03ba (t \u2013 t')}$ is the temporal decaying function, where \u03ba(t \u2013 t') = exp(-d(t - t')).\nThis module outputs the updated embedding matrix Ht that contains historical information."}, {"title": "4.2.3 Gating Integration", "content": "The learned embeddings by HRGCN get the historical information of entities, while some useful original information directly from the input TKG may be overwritten. We use a control gate to balance the contribution of the two kinds of information. Specifically, we employ a fully connected layer to generate a graph embedding $h_g$ according to the embeddings H from HRGCN. Then we use another fully connected layer to calculate the weight of $H_t$ according to graph embedding $h_g$. The final embedding matrix is a weighted sum of $H_t$ and the initialized embedding matrix $H_c$.\n$h_g = \u03c3(W_{graph}H_t + b_{graph}) (8)$\n$\u03b3 = \u03c3(W_{gate}h_g + b_{gate}) (9)$\n$H^t = \u03b3 * H_t + (1 \u2212 \u03b3)H_c (10)$\nwhere $h_g$ is the generated graph embedding, $W_{graph}, W_{gate}, b_{graph}$ and $b_{gate}$ are learnable parameters.\nThe output of the encoding module is the balanced evolutional embedding matrix $H^t$ that encodes the evolutional information on graph $G_{[t_1,t]}$."}, {"title": "4.3 Conditional Decoding", "content": "The last step of our model is to do reasoning according to the embedding matrix and get the conditional intensity scores of the candidate entities, then select the entity with the highest score as the result, as described in Eq.6."}, {"title": "4.3.1 Previous RGCN Decoding", "content": "Existing works (Li et al., 2021, 2022) use ConvTransE as the decoding function De() for traditional RGCN to calculate the certain intensity values $\u03bb_o^t$ of entity o by Eq. 11.\n$\u03bb_o^t = ConvTranse(concat(h_s^t , h_r^t ); h_o^t )(11)$\nNote that the parameters of ConvTransE are fixed over different queries, which leads the model to tend to reason using the few evolutional patterns of the most common events, causing bias against other evolutional patterns of the non-common events. We adjust ConvTransE to avoid the bias caused by long-tailed distribution in the following."}, {"title": "4.3.2 Avoiding Long-tailed Distribution Bias", "content": "We use feature linear modulation (FiLM) to construct a hyper-network, which adjusts parameters of decoder according to different queries, so that it can choose the appropriate query-specific evolutional pattern for reasoning.\nSpecifically, given a query quadruple $(s_q, r_q, o_q, t_q)$, the hyper-network generates a shifting factor $(\u03b1_{s_q,r_q,t_q})$ and a scaling factor $\u03b2_{(s_q,r_q,t_q)}$ according to the vector of the query quadruple to scale and shift the decoding parameters.\n$\u03b1_{(s_q,r_q,t_q)} = \u03c3((h_{s_q} || h_{r_q} )W_\u03b1 + b_\u03b1 )(12)$\n$\u03b2_{(s_q,r_q,t_q)} = \u03c3((h_{s_q} || h_{r_q} )W_\u03b2 + b_\u03b2)(13)$\n$\u03b8^q = (\u03b1_{(s_q,r_q,t_q)} + 1) \u2297 \u03b8 + \u03b2_{(s_q,r_q,t_q)} (14)$\nwhere $h_{s_q}$ and $h_{r_q}$ are embeddings of subject $s_q$ at time $t_q$ and relation $r_q$ respectively; $W_\u03b1, W_\u03b2, b_\u03b1$ and $b_\u03b2$ are learnable parameters; \u03b8 is the original parameters from decoder and $\u03b8^q$ is query-specific parameters; \u2297 is Harmard product."}, {"title": "4.3.3 Adjusted Decoding", "content": "The adjusted decoder extracts the multi-dimensional features of the query quadruple through one-dimensional convolution and gets the conditional intensity scores of candidate entities via the inner product with the embedding matrix.\n$\u03bb_o = ConvTransE([h_{s_q}, h_{r_q}]; h_o^t ; \u03b8^q)(15)$"}, {"title": null, "content": "$\u03bb_o$ is the conditional intensity of candidate entity o; [] means the concat function; $\u03b8^q$ is the adjusted parameters got from the feature transform unit. The decoder chooses the entity which has the highest score as the missing part of the query quadruple."}, {"title": "4.4 Learning Objective", "content": "The task to predict the missing entity of a given quadruple could be seen as a multi-classification task and each entity in the candidate set belongs to one class. The optimization objective of entity prediction task is to maximum the scores of the ground truth entities, which can be convert to a cross-entropy loss $L_e$:\n$L_e = - \\sum_{t_q \u2208 T}  \\sum_{F_{t_q}} \\sum_{k=1}^K y_k log p(o_k|s_q, r_q, t_q)(16)$\nwhere T is the timestamp set; $F_{t_q}$ is the quadruple set with timestamp $t_q$; K is the number of entities; $y_k = 1$ if entity $o_k$ equals to ground truth $o_q$, otherwise 0; $p(o_k|s_q, r_q, t_q)$ is the probability of $o_k$, normalized by the scores of all the candidate entities in the candidate sets."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Experimental Setup", "content": null}, {"title": "5.1.1 Datasets", "content": "We use four benchmark datasets which are generally used in TKG reasoning task to evaluate the effectiveness of HERLN, ICEWS14 (Li et al., 2021), ICEWS18 (Li et al., 2021), WIKI (Jin et al., 2020) and YAGO (Jin et al., 2020). ICEWS is a database got from more than 100 data sources over more than 250 countries and regions. ICEWS14 and ICEWS18 datasets contain events occurred in 2014 and 2018 respectively. WIKI and YAGO are subsets of the Wikipedia history and YAGO3 respectively. We list the statistics of these datasets in Appendix A."}, {"title": "5.1.2 Baselines", "content": "We compare HERLN with 10 TKG reasoning models, which can be categorized into three classes. (1) Embedding-based models, RE-NET (Jin et al., 2020), REGCN (Li et al., 2021), EvoKG (Park et al., 2022), CENET (Yi et al., 2023), HGAT (Shao et al., 2023) and TiPNN (Dong et al., 2024); (2) Path-based models, TG-Tucker (Han et al., 2021b) and TLogic (Liu et al., 2022); (3) Hawkes process-based models, GHNN (Han et al., 2020) and GHT (Sun et al., 2022)."}, {"title": "5.1.3 Evaluation Metrics", "content": "We report MRR, which is the mean of the reciprocal values of the actual missing entities' ranks averaged by all the queries, and Hits@1/3/10, i.e., the proportion of correct test cases that are ranked within top 1/3/10."}, {"title": "5.1.4 Implementation Details", "content": "We implement our model in Pytorch (Paszke et al., 2019) and DGL Library (Wang et al., 2019a). The experiments are conducted on a Nvidia GeForce Titan GPU. To be consistent with the baselines, we set the embedding dimension of entities $d_e$ and relations $d_r$ to 200. The number of HRGCN layers is set to 2 and the dropout rate for each layer is set to 0.2. We set all weights of edges to 1 in the embedding initializing module. We use the same hyperparameter settings of ConvTransE given by Li et al. (2021), the decode unit has 50 convolutional kernels with a size of 2\u00d73 for each kernel. Adam (Kingma and Ba, 2015) is adopted for parameter learning with the learning rate of 0.001 on all the datasets. We report the average experimental results on three random seeds. Our code is available at https://github.com/WisdomMLlab/HERLN."}, {"title": "5.2 Experimental Results", "content": null}, {"title": "5.2.1 Entity Prediction", "content": "Table 1 shows the entity prediction results on the benchmark datasets. The best results are marked in bold and the second best ones are underlined. It can be seen that our proposed HERLN performs the best nearly on all the settings, an only exception is that it achieves the second best Hits @3 score on ICEWS14. GHNN and GHT combines Hawkes point process with neural networks. But they do not consider the temporal decaying of event impact, which limits their performance. RE-GCN uses RNN on static graphs and loses temporal information during long-term evolution. CENET measures the probability of different entities by constructing the historical vocabulary. It does not exploit the graph structural information. TiPNN focuses on query-aware temporal path to capture the feature and doesn't take the potential relations between entities into account.\nOur model can capture the historical structure and the event evolutional patterns at the same time, considering the community structure, long-tailed distribution, and temporal decaying characteristics, thus it makes significant improvements over the baselines on all the datasets."}, {"title": "5.2.2 Relation Prediction", "content": "The results of the relation prediction task in terms of the MRR metric are shown in Table 2. In the relation prediction task experiment, we do not include TG and RE-NET as baseline because they conduct only entity prediction. It can be seen from Table 2 that our proposed HERLN outperforms all the baselines and receives a boost of up to 10% in the MRR metric. The reasons of the performances of both our model and the baselines are similar to those for the entity prediction task."}, {"title": "5.2.3 Ablation Study", "content": "We conduct ablation experiments on the ICEWS14 dataset. (1) OurModel without (w.o) ConvTransE: we replace the ConvTransE in the decoder unit with a simple MLP layer. (2) OurModel without (w.o) FiLM: Instead of FiLM, we directly use ConvTransE. (3) OurModel without (w.o) HRGCN: we replace HRGCN with a RGCN to aggregate snapshots. (4) OurModel without (w.o) Community: we omit the embedding initialize module.\nAs shown in Table 3, replacing any component of our model degrades the performance, which demonstrates that each component of the model has a positive gain on the result. The variant (1) has an almost 6% drop in MRR, suggesting that an appropriate decoder can learn event evolution patterns effectively. The variant (2) shows 2% decreasing in Hits@1, indicating that the hypernetwork helps the model distinguish different event evolution patterns. The variant (3) has 7.89%, 6.89%, 9.31%, 11.90% drops in MRR, Hits@1, Hits@3, respectively, emphasizing the importance of incorporating the temporal information of events through HRGCN. The variant (4) has 3% decreasing in MRR, confirming the helpfulness of extracting community structures."}, {"title": "6 Conclusions", "content": "In this paper, we propose a Hawkes-based evolutional representation learning network (HERLN) to tackle the TKG reasoning tasks. We exploit the structural and evolutional characteristics of TKGS inherited from real-world networks to learn structural information and evolutional patterns. We initialize the embeddings with a community detection algorithm and a graph convolution network to make use of community structure. We design a Hawkes process-based relational graph convolutional network to tackle the temporal decaying phenomenon. We construct a conditional decoder to alleviate the biases caused by scale-free property (long-tailed distribution). Experimental results show the superiority of our proposed model."}, {"title": "7 Limitations", "content": "Note that there are many entities appear only in the test set (named unseen entities), which will continue to appear as the size of TKG increases. Our model could not get sufficient information to assign proper embbedings for these entities. For further work, we plan to find a method to deal with the unseen entities."}]}