{"title": "TabSD: Large Free-Form Table Question Answering with SQL-Based Table Decomposition", "authors": ["Yuxiang Wang", "Junhao Gan", "Jianzhong Qi"], "abstract": "Question answering on free-form tables (TableQA) is challenging due to the absence of predefined schemas and the presence of noise in large tables. While Large Language Models (LLMs) have shown promise in TableQA, they struggle with large free-form tables and noise sensitivity. To address these challenges, we propose TABSD, a SQL-based decomposition model that enhances LLMs' ability to process large free-form tables. TABSD generates SQL queries to guide the table decomposition, remove noise, and processes sub-tables for better answer generation. Additionally, SQL Verifier refines SQL outputs to enhance decomposition accuracy. We introduce two TableQA datasets with large free-form tables, SLQA and SEQA, which consist solely of large free-form tables and will be publicly available. Experimental results on four benchmark datasets demonstrate that TABSD outperforms the best-existing baseline models by 23.07%, 2.84%, 23.24% and 9.32% in accuracy, respectively, highlighting its effectiveness in handling large and noisy free-form tables.", "sections": [{"title": "Introduction", "content": "Tables play a crucial role in managing and analyzing large-scale data. Numerous studies have explored various tabular understanding tasks, such as Table Question Answering (TableQA) (Pasupat and Liang, 2015; Yin et al., 2020), Table Fact Verification (Chen et al., 2020a; Herzig et al., 2020), Table-to-text Generation (Bao et al., 2018; Zhang et al., 2024a). Among these tasks, TableQA remains one of the most challenging, requiring models to comprehend both natural language problems and the corresponding free-form tabular data. Unlike structured tables, free-form tables, such as web tables (Cafarella et al., 2008), lack predefined schemas, making structural-query-based approaches (Pasupat and Liang, 2015; Zhong et al., 2017) inapplicable.\nWith the advent of LLMs demonstrating strong performance, extensive research has been directed towards Free-Form TableQA (Liu et al., 2024; Wang et al., 2024a,b). While some work has demonstrated that the performance of LLM-based TableQA is highly sensitive to noise in tables, existing methods struggle to effectively handle noisy data (Ye et al., 2023; Wang et al., 2024b). Besides, current TableQA datasets is insufficiently reflect real-world application scenarios, such as finance, healthcare, and scientific research (Zavitsanos et al., 2024; Singhal et al., 2025; Pramanick et al., 2024), where large free-form tables are prevalent. Most existing studies focus on datasets with relatively small tables (Cheng et al., 2023; Liu et al., 2024; Wang et al., 2024a), as large free-form tables are scarce in publicly available datasets (Pasupat and Liang, 2015; Nan et al., 2022; Chen et al., 2020a), which weakens model robustness. Furthermore, the inherent noise in large free-form tables remains an open challenge.\nTo address these challenges, we introduce two LLM-augmented datasets featuring large free-form tables: (1) Spider-LLM Large Table QA Dataset (denoted as SLQA), which directly adopts large tables from Spider dataset (Yu et al., 2018) and generates QA pairs using an LLM; and (2) Spider-LLM Expanded Table QA Dataset (denoted as SEQA), which randomly selects small tables from Spider dataset, expands them into large tables and generates corresponding QA pairs using LLMs. Meanwhile, we extract large tables from the WikiTableQuestions dataset (Pasupat and Liang, 2015) and the HybridQA dataset (Chen et al., 2020b) to supplement our benchmark. We then investigate how to enhance LLM-based TableQA models for large free-form tables. Our findings reveal that while LLMs struggle with large free-form tables, their accuracy significantly improves when noisy data is removed \u2013 see Figure 1 for an example. Moreover, although NL-to-SQL methods alone are"}, {"title": "Related Work", "content": "Existing TableQA studies fall into three groups: (1) NL-to-SQL-based, (2) pre-trained language model (PLM)-based, and (3) large language model (LLM)-based.\nNL-to-SQL-based Methods NL-to-SQL-based methods translate natural language questions into logical forms, i.e., SQL queries, which are then executed to retrieve answers. These methods either train a language model or use in-context learning for the translation task. Seq2SQL (Zhong et al., 2017) is a representative for the former category. It trains a seq2seq model using reinforcement learning, leveraging query execution results as feedback to refine SQL generation.\nRecent works fall into the latter category. DIN-SQL (Pourreza and Rafiei, 2023) decomposes the NL-to-SQL task into sub-problems and employs in-context learning to generate the final SQL queries. LEVER (Ni et al., 2023) generates multiple candidate answers via SQL execution and trains a verifier to select the most accurate one. SynTQA (Zhang et al., 2024b) also uses a verifier. It chooses answers from an NL-to-SQL TableQA module and an end-to-end TableQA module. These methods struggle with free-form tables without predefined schemas, where SQL execution is ineffective.\nPLM-based Methods PLMs-based methods also have two sub-categories.\nStructural adaptations for tabular data. These methods modify the Transformer (Vaswani et al., 2017) to better capture tabular structures. TaBERT (Yin et al., 2020) incorporates vertical self-attention to unify textual and tabular representations based on BERT (Devlin et al., 2019). TUTA (Wang et al., 2021) employs a hierarchical tree-based attention with column and table segment masking to model table representations. TAPAS (Herzig et al., 2020) extends BERT by adding row, column, and rank embeddings, along with table cell masking.\nFine-tuned end-to-end models. TAPEX (Liu et al., 2022) pre-trains BART (Lewis et al., 2020) using a large synthetic dataset derived from the WikiTableQuestions dataset. OmniTab (Jiang et al., 2022) also utilizes BART as its backbone while pre-training it on both real and synthetic data, including SQL queries from the Spider (Yu et al., 2018) dataset and synthetic natural language sentences generated from SQL queries via its SQL2NL module. With smaller model sizes, PLMs exhibit limited effectiveness in grasping tabular semantics compared to the latest LLMs, resulting in sub-optimal TableQA accuracy in TableQA.\nLLM-based Methods LLM-based methods use either prompting or program-aided reasoning.\nPrompting-based methods. These methods prompt LLMs to generate answers by providing it an input question and the corresponding table. For example, Mix-SC (Liu et al., 2024) explores the stochastic nature of LLMs by generating multiple candidate answers for each question. It employs two types of prompting techniques: one directs the LLM to act as a Python agent for script execution, while the other utilizes Chain-of-Thought (CoT) prompting (Wei et al., 2022) to guide step-by-step reasoning. The final answer is selected using a self-consistency mechanism, which chooses the most frequent answer.\nProgram-aided reasoning methods. These methods integrate program generation and execution with LLM prompting. Binder (Cheng et al., 2023) first prompts the LLM to generate an initial program based on the input question, then it identifies and refines the challenging segments through iterative LLM engagement. The final refined program"}, {"title": "Problem Formulation", "content": "Given a table T and a question Q about the data in T, the objective of the TableQA task is to develop a model capable of generating an accurate answer for Q. The question $Q = (q_1, q_2,..., q_{\\Q})$ is expressed in natural language, where each $q_i \\in Q$ (i \u2208 [1, |Q|]) represents a token (e.g., a word). The table T is also represented as a token sequence in natural language, where individual cells are separated by special characters such as \u2018|', and rows are delineated by newline characters. The model's performance is evaluated by comparing its generated answers against ground-truth answers in a token-based manner. To tackle TableQA over large tables effectively, our approach addresses two key sub-problems: NL-to-SQL and Table Decomposition.\nNL-to-SQL This problem involves generating an SQL query S from question Q and the table header information H = (h1,h2,...,h|H|) corresponding to table T, where each $h_i \\in \u0397$ (\u0456\u2208 [1,|H|]) represents a column in T. The objective is to produce an S that accurately captures the intent of Q based on H. Rather than executing S, our focus is on parsing S to extract key components for table decomposition, including the column names C, the conditions P (e.g., LIKE clauses), and the values V involved in the filtering operations of S.\nTable Decomposition This problem aims to extract a sub-table $T_{SUB}$ from table T based on the extracted components: column names C, conditions P, and values V. Given table T, where C = {$C_1, C_2,..., C_{|C|}$} represents the set of selected columns, P = {$P_1,P_2,...,P_{|P|}$} represents the filtering conditions, and V = {$V_1, V_2, ..., v_{|v|}$} is the values used in the conditions, the goal is to construct $T_{SUB}$ such that it retains only the specified columns in C and includes only rows satisfying P. This process is implemented using Python DataFrame operations, which handle both column selection and row filtering."}, {"title": "Methodology", "content": "TABSD handles TableQA in three stages: (i) SQL generation, (ii) table decomposition, and (iii) answer generation, as outlined in Algorithm 1.\nThe SQL generation stage (Section 4.1) uses a SQL Generator and a SQL Verifier. The SQL Generator takes question Q and the header information of table T to generate a raw SQL query SRAW. Then, the SQL Verifier verifies and refines SRAW to produce the final SQL query S.\nThe table decomposition stage (Section 4.2) uses a Table Decomposer to parse S and extract a triple (C, P, V), which represents column names, conditions, and values, respectively. The triple is consumed by Python DataFrame operations to construct a sub-table TSUB with only question-relevant rows and columns.\nThe answer generation stage (Section 4.3) uses a Answer Generator that takes Q and TSUB to generate the final answer A with an LLM.\nWe use a SQL Generator to produce a raw SQL query Sraw, which is passed to a SQL Verifier for validation. If Sraw fails validation, the SQL Verifier refines it to generate a final SQL query S.\nThe SQL Generator uses few-shot prompting with a backbone LLM. Given question Q and the"}, {"title": "SQL Generation", "content": "We use a SQL Generator to produce a raw SQL query Sraw, which is passed to a SQL Verifier for validation. If Sraw fails validation, the SQL Verifier refines it to generate a final SQL query S.\nThe SQL Generator uses few-shot prompting with a backbone LLM. Given question Q and the header information of table T, we construct the prompt using examples from a manually annotated NL-to-SQL dataset SQUALL (Shi et al., 2020).\nExample Selection To retrieve relevant NL-to-SQL examples, we compute the cosine similarity between the embedding of Q and those of all questions in SQUALL, denoted as D = {d1, d2, ..., d\u03bd}, using Sentence Transformers (Song et al., 2020):\n$e_Q = f(Q), E_D = {f(d_i)}_{i=1}^{N}$,\nwhere f(\u00b7) is an embedding model, $e_Q \\in R^{1\\times m}$, $E_D \\in R^{N \\times m}$, and m denotes dimensionality.\nWe select the top K examples based on cosine similarity:\n$Z = \\underset{i}{argsort}(-\\frac{e_Q E_D^T}{||e_Q|| \\space ||E_D||}),$\n$TopK(D) = {d_i; |\\space i_j \\in Z}, j \\in [N-K+1: N],$ where i \u2208 [1, |D|], and K = 5 by default.\nThe SQL Generator uses the selected NL-to-SQL examples, the header information of T, and the question Q to build a prompt (see Appendix A). This prompt is then used to query the LLM, generating a raw SQL query SRAW.\nSQL Verification Since our table decomposition highly relies on the quality of the SQL query SRAW, we introduce the SQL Verifier to verify whether SRAW leads to a correct sub-table. The verification procedure is as follow:\n(1) Initial sub-table extraction. We use the Table Decomposer (detailed in the next section) to parse SRAW and obtain a raw sub-table TSUB/.\n(2) Validation prompting. The SQL Verifier is prompted with TSUB/, the header information of T and the question Q. It determines whether TSUB/ contains all the necessary information to answer Q (see the prompt template in Appendix A).\n(3) Decision making. If the SQL Verifier outputs [True], Sraw is accepted as the final SQL query S. Otherwise, the SQL Verifier refines SRAW by prompting the LLM to generate an improved SQL query (see the prompt template in Appendix A).\n(4) Iteration control. This verification and refinement process can be conducted for up to n rounds. By default, n = 1 for cost efficiency.\nAt the end of this SQL verification stage, we obtain the final SQL query S."}, {"title": "Table Decomposition", "content": "The Table Decomposer takes T and S (obtained in the SQL generation stage) as input and adopts a rule-based method to extract sub-table TSUB.\nSQL Parsing To extract key information from S, we employ a regex-based rule-driven approach. We use regular expressions to parse S and extract the column names C, conditions P (e.g., filtering clauses), and values V (e.g., values in the filter-ing clauses). The detailed regular expressions are included in Appendix B.\nTable Decomposition With the extracted triple (C, P, V), we perform table decomposition. We first convert table T into a Python DataFrame. Then, we apply DataFrame operations to extract rows and columns based on (C, P, V), obtaining the final sub-table TSUB. The correspondence between Python DataFrame operations and SQL queries is detailed in Appendix B."}, {"title": "Answer Generation", "content": "At this stage, we exploit the strong semantic-parsing and reasoning capabilities of an LLM-based model to perform end-to-end TableQA. We construct a prompt (see prompt template in Ap-"}, {"title": "Dataset Construction", "content": "Due to the limited availability of TableQA datasets with large tables, to study model performance in such settings, we construct two datasets.\nSLQA is construed by extracting all the large tables (with more than 4,096 tokens) from the Spider dataset (Yu et al., 2018). The original question-answering (QA) pairs in Spider are for NL-to-SQL tasks rather than TableQA. To bridge this gap, we propose an LLM-based QA pair generation method that prompts the LLM to generate questions that cannot be answered easily by NL-to-SQL solutions.\nQA pair generation. We observe that directly prompting an LLM with a large table for QA generation, or arbitrarily selecting a table cell as the ground-truth answer and using it to generate the corresponding question, often leads to low-quality QA pairs, with an acceptance rate well below 50%. To overcome this issue, we propose a self-adaptive QA generation method, where the LLM autonomously selects an answer \u2013 either a specific cell or an answer field (i.e., a section of the table) \u2013 and then generates the corresponding QA pair based on this selection. This method significantly improves QA pair quality, achieving an acceptance rate of over 70% (see details in Appendix C).\nWe propose three answer (field) selection strategies: (i) cell-based, which prompts the LLM to randomly select a single cell in the table as the ground-truth answer and generate a corresponding question; (ii) column/row-based, which prompts the LLM to randomly select a column/row in the table as the answer field and a generate QA pair accordingly; (iii) sub-table-based, which prompts the LLM to randomly select a sub-table (containing multiple rows and columns) as the answer field and generate a QA pair based on this sub-table. Figure 3 presents examples of the QA pairs generated. The prompt templates are included in Appendix C.\nSEQA is constructed from small tables. We randomly select a subset of small tables (with at most 4,096 tokens) from the Spider dataset and leverage an LLM to expand them into larger tables (see prompt template in Appendix C). The generate the QA pairs, we reuse the same method as above.\nThe LLM used for dataset construction is"}, {"title": "Experimental Settings", "content": "Datasets We evaluate our TABSD on four datasets, including SLQA and SEQA that we constructed, plus two widely used public datasets: WikiTableQuestions (denoted as WTQ) and HybridQA (denoted as HybQA).\nWTQ is a public TableQA dataset derived from Wikipedia tables. To evaluate our model's capability in handling large free-form tables, we extract all tables with more than 4,096 tokens from the WTQ dataset. We use WTQ to denote the resulting dataset. Additionally, we conduct experiments on the entire WTQ dataset to assess our model's applicability over tables of different sizes.\nHybQA is a public dataset that integrates free-form tables and texts from Wikipedia. To maintain consistency in prompt formatting, we incorporate the texts directly into the corresponding table cells. Then, we extract all the tables with more than 4,096 tokens to construct a large-table subset. We use HybQA to denote the resulting dataset.\nModel Input Preparation Following a widely used approach in the literature (Liu et al., 2024; Wang et al., 2024a,b), tables are linearized into sequences by flattening their structure and incorporating them into prompts for LLMs. Table cells are delineated using the symbol \u201c\u201d, while rows are separated by line breaks. The questions are directly included in the prompts, whereas the answers are utilized only for model evaluation.\nFor the LLM in the SQL Generator, similar to NL-to-SQL based methods (Pourreza and Rafiei,"}, {"title": "Results and Analysis", "content": "Overall Results Table 2 reports the overall performance results. Our model TABSD consistently outperforms all competitors across all four datasets, improving the accuracy by 23.07%, 2.84%, 23.24%, and 9.32%, respectively. This confirms the effectiveness of TABSD for dealing with complex and large free-form tables.2023; Zhang et al., 2024b), it takes the question, the header information of the table, and in-context examples to form the prompt and generate the SQL query. For the Table Decomposer, it takes the generated SQL query and the original table to extract a sub-table. For the LLM in Answer Generator, it takes the question and the sub-table as input, forming a compact prompt that reduces both the token consumption and search space for the LLM.\nCompetitors We compare with models of three categories: (i) NL-to-SQL based: DIN-SQL (Pourreza and Rafiei, 2023); (ii) Pre-trained or fine-tuned PLM-based: TAPEX-Large (Liu et al., 2022), OmniTab-Large (Jiang et al., 2022), and CABINET (Patnaik et al., 2024) (SOTA); (iii) LLM-based: End-to-End GPT-40 mini (OpenAI, 2024) (SOTA) and DATER (Ye et al., 2023).\nImplementation Details For all the LLM-based models including our TABSD, we use GPT-40 mini as the backbone model. For DIN-SQL, we also utilize GPT-40 mini as its backbone model. All experiments are run with an NVIDIA A100 80 GB GPU on a cloud GPU server.\nEvaluation Metric We report the performance based on the exact-match Accuracy, following the evaluator from Pasupat and Liang (2015).\nEffectiveness in Table Decomposition As shown in Figure 4, our Table Decomposer effectively reduces the input table length by 69.8%, 48.5%, 84.0%, and 91.4% on the four datasets, respectively. This substantial reduction minimizes the prompt length for the Answer Generator, enabling the model to focus more effectively on important data while enhancing processing efficiency.\nModel Generalizability To evaluate the generalization capability of TABSD, we replace the GPT-40 mini backbone with TAPEX. We compare TABSD using TAPEX as the Answer Generator with the original TAPEX model, plus TAPEX combined with an LLM for table decomposition. The results in Table 3 shows that our Table Decomposer consistently outperforms using an LLM for table decomposition (see prompt in Appendix D). Moreover, our method can also be integrated with and strengthen existing PLM-based solutions.\nWe also conducted an experiment using the entire WTQ dataset including questions for both small and large tables to further assess our TABSD's generalizability. The results (in Appendix D) verifies that TABSD enhances the accuracy of its backbone model (GPT-40 mini) by 25.67% on WTQ overall.\nAblation Study We run an ablation study with three model variants: (1) SG SQL execution: Directly executing SQL queries generated by the SQL Generator; (2) TABSD-w/o-SG+TD: TABSD without both SQL Generator and Table Decomposer, i.e., only using the Answer Generator; (3) TABSD-w/o-SV: TABSD without the SQL Verifier.\nAs shown in Table 4, directly executing the generated SQL queries results in substantial accuracy drops, because the SQL queries struggle to handle free-form tables. However, the SQL queries still capture structural information, which is valuable for table decomposition. Meanwhile, using only the Answer Generator also leads to accuracy drops, since LLMs struggle with large tables with noisy data. SQL Verifier helps to improve the quality of generated SQL queries and thus improves the overall model accuracy.\nError Analysis We conduct an error analysis on SQL execution failures. When executing the generated SQL queries, the execution error rates are 64.2%, 88.7%, 70.6%, and 61.5% for WTQ, HybQA, SLQA, and SEQA, respectively, further confirming that NL2SQL-based solutions struggle on these datasets. See Appendix E for detailed results.\nFor the SQL Verifier, the acceptance rates (i.e., cases where SRAW is accepted as S) are 71.6%, 54.3%, 77.1%, and 78.7%, respectively. The lower acceptance rate for HybQA (54.3%) suggests that SQL generation struggles more with the hybrid table-text format, requiring frequent refinement.\nCase Study We include two TableQA examples that fail existing table decomposition models, while TABSD successfully answers those questions (detailed in Appendix F)."}, {"title": "Conclusions", "content": "We proposed TABSD, a TableQA model for large free-form tables. TABSD utilizes an LLM (SQL Generator) to generate an SQL query for each input question, exploiting SQL to capture the table structure information, especially for complex and large tables, while avoiding the limitations in directly executing SQL queries to obtain answers. To further enhance the SQL quality, we introduce the SQL Verifier. For table decomposition, we adopt a rule-based Table Decomposer, which achieves higher accuracy and efficiency compared to LLM-based decomposition approaches. We evaluate the performance of TABSD on two public datasets WTQ and HybQA, and two datasets that we constructed, SLQA and SEQA. The results show that TABSD outperforms the best competitors in terms of accuracy by 23.07%, 2.84%, 23.24% and 9.32%, respectively."}, {"title": "Limitations", "content": "There are two main limitations: (1) Although our rule-based Table Decomposer is more accurate and efficient compared with the existing table decomposition methods, there may still be a few corner cases that our rules cannot fully cover. Strategies to automatically learn and update the rules to adapt to dynamic scenarios would be an interesting future work; (2) Our multi-stage model implies a more complex model design compared with end-to-end models. Further refining the design to achieve an end-to-end pipeline with little sacrifice on accuracy would be another direction to explore."}, {"title": "Ethics Statement", "content": "This work involves generating new question-answer pairs and extended tabular content, adhering to ethical considerations and privacy standards. Our work utilizes the following datasets:\n\u2022 WikiTableQuestions and HybridQA are used under the Creative Commons Attribution 4.0 International License (CC BY 4.0).\n\u2022 SLQA and SEQA datasets originate from the Spider dataset (CC BY-SA 4.0 license) and are shared under the same terms to ensure continuity.\nPrivacy and Ethical Considerations The dataset creation process does not involve the collection, processing, or storage of personal or any other sensitive information. Additionally:\n\u2022 No human subjects were involved, and no user-generated content requiring consent was utilized.\n\u2022 The data sources used are publicly available and licensed for reuse under open-access terms.\n\u2022 The use of LLMs to generate new data strictly follows ethical AI principles, ensuring that no copyrighted or proprietary data is incorporated.\n\u2022 Any potential biases in the generated content have been considered, and efforts have been made to maintain fairness and representativeness in the dataset.\nBy adhering to these principles, we ensure that our work aligns with ethical research practices and respects data usage policies. Additionally, all code used in our work will be made publicly available."}, {"title": "Prompts for modules in TABSD", "content": "The prompts used by our SQL Generator, SQL Verifier, SQL re-generation, and Answer Generator are shown in Figure 5, 6, 7, 8, respectively./* Some example questions and corresponding SQL queriesare provided based on similar problems: */Answer the following: [P1][S1]Answer the following: [P2][S2]Answer the following: [P3][S3]Answer the following: [P4][S4]Answer the following: [P5][S5]The target questions: [P]Table columns: [Columns]The corresponding SQL:Note:Do not add any explanation after the SQL.Please refine the given SQL according to the questionand the table schema.The SQL is: [SQL]The table header: [Header]The table content: [TAB_CONTENT]The question is: [Question]Notes:The current SQL query can not solve the problemwell. Please optimize it and return the optimized SQLdirectly.Please answer the question according to the giventable.The header is: [Columns]The table content: [TAB_CONTENT]The question is: [Question]Notes:Give me the answer in format Final Answer:AnswerName1, AnswerName2... form (should be anumber or entity names, as short as possible, withoutany explanation)Please determine whether the sub-table issufficient to answer the given question.The question is: [Question]Sub-table Columns: [Columns]Sub-table Content: [TAB_CONTENT]Notes:Complete table contains the followingcolumns: [ORG_COLS]Give me the answer in format Final Answer:True / False form (should be either True orFalse, without any explanation)"}, {"title": "Additional Details of the TableDecomposer", "content": "The regular expressions used by our Table De-composer to extract the column names, conditions, and values are shown in Table 5. The correspond-ing relationships between parsed SQL queries andPython DataFrame operations used by the TableDecomposer are shown in Table 6."}, {"title": "Additional Details on DatasetConstruction", "content": "We propose both a QA pair generation method anda table expansion method to construct our LLM-augmented large free-form table datasets. For theQA pair generation, the statistics of our LLM-generated QA pairs are explained in Figure 9. Par-ticularly, for the Answer Error cases, where thegenerated questions are valid while the answersare incorrect, we manually correct the answers ac-cording to the tables, to obtain more pairs in thefinal datasets. For the Invalid Question cases,we remove the questions and their correspondinganswers.The prompt used to expand a small table intoa larger one is shown in Figure 10, while Theprompts used to generate QA pairs are shown in"}, {"title": "Regular expressions", "content": "Column name filters 1: select\\s+(.*?)\\s+from2:([\\w\\s\\(\\)]+)\\s*(=|like|not like|in|not in|>|<|<> | >= | <= | != | between|is null|is not null)Condition & value filters 1: where\\s+(.*)2: \\(\\s*select.*?where\\s+(.*?)\\s*\\)3:(\\w+)\\s*(=|!= |<>|>| < | >= | <= | like not like|in|not in|between is nullis not null)\\s*('?[\\w\\s\\-%]+?'?)4:(?:(where|having)\\s+(.*?))?(?:\\s*grounp by\\s+([\\w,\\s]+))?(?:\\s*limit\\s+(\\d+))?"}, {"title": "Additional Experiments", "content": "We also conduct an additional experiment on theentire WTQ dataset (including both small and largetables) to showcase our TABSD's general appli-cability. The results, presented in Table 7, showthat TABSD outperforms all the competitors usingNL-to-SQL based, PLM-based, and LLM-basedapproaches, indicating TABSD's general applica-"}, {"title": "Error Analysis", "content": "Figure 12 presents the statistics of SQL executionerrors when directly using the SQL Generator'soutputs to obtain answers. BinderException, whichoccurs when the query references invalid tables,columns, aliases, or incorrect conditions, is themost common SQL execution error. It is followedby IndexError, which happens at runtime due toout-of-bounds index access. The dominance ofBinderException suggests schema or condition er-rors, while IndexError indicates issues with index-based operations.\nOn different datasets, the distributions of errorsvary. This is because different datasets have varioustable size distribution (see Table 1), reflected inthe number of rows and columns, and the complexityof the contents in each cell, e.g., the presence oflengthy descriptive texts in HybQA.\nIn addition to SQL execution errors, we ana-lyze errors in the full model, particularly in Ta-ble Decomposition and Answer Generation stages.Since Table Decomposition quality directly im-pacts final accuracy, incorrect selection or filter-ing of rows/columns can lead to missing or mis-leading information. Even when decompositionis precise, errors may still arise due to LLM rea-soning limitations, such as challenges in numericaloperations, multi-hop reasoning, or handling am-biguous queries. This highlights the need for bothrobust decomposition methods and improved LLMreasoning capabilities to enhance overall TableQAperformance."}, {"title": "Case Study", "content": "We show two examples in Figure 14 where ourTABSD successfully finds the question answers,while existing table decomposition-based models(CABINET and DATER) fail. CABINET fails toaccurately identify and assign higher weights to thecells containing the answer when handling largetables, while DATER may remove the answer filedduring the table decomposition process."}, {"title": "Prompt", "content": "Please use the synthesized data to expand theexisting table and increase its rows and columns.Table Header: [HEADER]Table Content: [TAB]Notes:-Directly returns the new table.-Make sure there are no duplicate rows and columns."}]}