{"title": "Can Large Language Models Act as Ensembler for Multi-GNNs?", "authors": ["Hanqi Duan", "Yao Cheng", "Jianxiang Yu", "Xiang Li"], "abstract": "Graph Neural Networks (GNNs) have emerged as powerful models for learning from graph-structured data. However, GNNs lack the inherent semantic understanding capability of rich textual nodes attributes, limiting their effectiveness in applications. On the other hand, we empirically observe that for existing GNN models, no one can consistently outperforms others across diverse datasets. In this paper, we study whether LLMs can act as an ensembler for multi-GNNs and propose the LensGNN model. The model first aligns multiple GNNs, mapping the representations of different GNNs into the same space. Then, through LoRA fine-tuning, it aligns the space between the GNN and the LLM, injecting graph tokens and textual information into LLMs. This allows LensGNN to ensemble multiple GNNs and leverage LLM's strengths, leading to a deeper understanding of both textual semantic information and graph structural information. Experimental results show that LensGNN outperforms existing models. This research advances text-attributed graph ensemble learning by providing a robust, superior solution for integrating semantic and structural information.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs are structured data that captures the inter-relations between entities in the real world. To learn from graphs, graph neural networks (GNNs) have been proposed, where graph convolution is introduced [24] and message passing is the main mechanism for neighborhood aggregation [15, 39]. GNNs have demonstrated significant success across various applications such as social network analysis [15], recommendation systems [42], and molecular property prediction [13].\nDespite the success, two major challenges remain unresolved. First, GNNs, although powerful in capturing graph structures, often lack the inherent semantic understanding capability required to process the rich textual attributes of nodes [55]. This can lead to the loss of valuable semantic information during the learning process, limiting the effectiveness of GNNs in applications where node features contain meaningful texts, such as citation networks and social media platforms. Second, while there have been a series of GNNs proposed, no one has been shown to consistently lead others across datasets of various domains. For example, we compare the node classification performance of four representative GNNs, namely, APPNP [12], GAT [39], GCN [24] and GIN [46] on three benchmark datasets [47]: Cora, Citeseer and PubMed. For fairness, we set the equal number of hidden representation layers and also the same dimensionality size. The results given in Figure 1 demonstrate that the winner varies across the datasets. The challenge of selecting the optimal GNN for a given dataset remains unresolved, as different GNN architectures exhibit varying strengths. This also restricts the wide applicability of GNNs.\nTo remedy the incapability of GNNs in semantic understanding, existing works [11, 27, 32, 41, 50] have resorted to large language models (LLMs), which have been shown to be prominent in text understanding and generation. The inherent advantage of GNNS in utilizing graph structure, fused with the strength of LLMs in semantic understanding, mutually enhances both models and leads to superior performance in downstream tasks. However, how to select the optimal GNN in different scenarios remains a gap. Since the effectiveness of GNNs varies across datasets, there naturally arises a question: Can we develop an ensembled model for multi-GNNs? The ensembled model is expected to integrate the strengths of multiple GNNs and can consistently perform well across datasets. To further leverage the power of LLMs in text understanding, we thus upgrade the question: Can LLMs act as ensembler for multi-GNNs?"}, {"title": "2 RELATED WORK", "content": "GNNs [16, 24, 39, 46] have been widely used in learning from graph-structured data. Existing GNN studies have been conducted on various types of graphs such as heterophilic graphs [3, 31, 57], signed graphs [8, 21] and temporal graphs [29, 35, 45]. Further, to improve the effectiveness of GNNs, there are also works [4, 30, 36] devoted to solving the notorious over-smoothing [7] and over-quashing problems [1]. In parallel, to scale GNNs to large-scale graphs, some works [28] focus on enhancing the model efficiency. A comprehensive GNN-related survey can be found at [44]. Despite their advantages, GNNs struggle to process textual attributes effectively, as traditional GNNs lack the semantic understanding capability that is necessary to handle text-attributed graphs (TAGs). In addition, to tailor a GNN model for a given dataset, existing methods mainly rely on neural architecture search (NAS) [59]. However, it is very computationally expensive, which necessitates new exploration.\nRecently, LLMs have revolutionized tasks involving semantic understanding and language generation. Meanwhile, the integration of LLMs and GNNs has garnered significant attention, leading to innovative methodologies [23]. One primary category of approaches takes LLMs as predictors in graph-based tasks, where models like GraphLLM [6] utilizes few-shot learning to address graph reasoning challenges. ENGINE [58] is designed to efficiently integrate textual and topological information by adding a tunable lightweight GNN side structure alongside each layer of LLMs.Similarly, heuristic reasoning approaches, including Chain-of-Thought (CoT) [43] and models like StructGPT [22], have demonstrated improved graph-based inference capabilities. Another notable category is to employ LLMs as encoders, as seen in the works of TextGNN [56] and Ads-GNN [26], which focus on optimization strategies for encoding graph data. Employing LLMs as encoders involves using large language models to extract textual features, which are then used as initial feature vectors for nodes or edges in a graph. These feature vectors are passed into graph neural networks (GNNs) for further processing, allowing the model to incorporate both the textual information from the LLM and the structural information from the graph. Additionally, the alignment of LLMs and GNNs has been explored through prediction alignment methods such as LTRN [53] and GLEM [54], where iterative training helps improve cross-modality learning. LLMs and GNNs are aligned through iterative pseudo-label training (prediction alignment) or by using contrastive learning to align their latent representations (latent space alignment). Finally, graph-empowered LLMs like GreaseLM [52] and DRAGON [49] illustrate the potential of enhancing language models with graph-based knowledge to enrich semantic understanding. They integrate the strengths of large language models and graph structures by modifying Transformer architectures to encode text and graph data simultaneously, allowing for a more comprehensive understanding of the interconnected information within graphs. These diverse approaches highlight the evolving landscape of LLM and GNN integration. However, there still lack studies exploring multi-GNNs ensembling with LLMs, which is particularly useful to avoid cherry-picking GNNs for a given dataset."}, {"title": "3 PRELIMINARIES", "content": "Text-attributed graph. A text-attributed graph is a graph where nodes and edges have textual attributes. In this work, we only consider the case where only nodes have textual attributes. Formally, a text-attributed graph can be defined as G = (V, E, X(0)), where V = {01, 02, ..., 0|V|} represents the set of nodes, and E = {e1, e2, ..., e|E| } represents the set of edges, with |V| = N indicating the total number of nodes in the graph. X(v) = {x{\u00ba), x20),...,x)} denote the attributes on nodes, respectively, which are strings. It can be represented as G = (V, E, {Xn}n\u2208V). Additionally, we define A as the adjacency matrix of graph G, where the matrix size is N \u00d7 N. In an unweighted graph, A(i, j) = 1 indicates that there is an edge between nodes vi and vj, while A(i, j) = 0 indicates that there is no edge between them.\nGraph Neural Networks. Traditional GNNs are a class of deep learning models designed for handling graph-structured data. The basic architecture of a GNN includes a node representation layer and a message-passing layer. In the node representation layer, each node v is assigned a feature vector x. In the message-passing layer, the representation vector h(t) of node v is updated after the t-th iteration using the following formula:\n$$h^{(t)}_{v}=\\text{UPDATE}\\left(\\text{AGGREGATE}\\left(\\left\\{h_{u}^{(t-1)}: u \\in \\mathcal{N}(v)\\right\\}\\right), h_{v}^{(t-1)}\\right),$$(1)\nwhere N(v) denotes the set of neighboring nodes of v. The AGGREGATE function is responsible for aggregating the representations of the neighboring nodes, and the UPDATE function combines the aggregated information with the previous state h(t-1) of node v to update its current state. Through this iterative process, GNNs are able to learn increasingly rich node representations, capturing both local and global structural information in the graph. The specific implementations of the AGGREGATE and UPDATE functions can vary depending on the particular GNN model."}, {"title": "4 METHODOLOGY", "content": "To enable the LLM to integrate multiple GNNs, alignment is primarily required in two aspects. First, the output representations of all the GNNs need to be aligned into a common vector space. Second, there must be alignment between the output representations of the GNNs and the semantic space required by the LLM. Based on this idea, we have designed the model in the following two steps. \n\n4.1 Aligning multi-GNNs\nGiven multiple GNNs, they could generate node representations in different low-dimensional spaces. Therefore, before ensembling GNNs, we need to first align them.\nNode feature initialization. To utilize the rich semantic information of textual features, we use pre-trained language models to initialize them. Specifically, given node vi with feature vector xi, we use Sentence-BERT [34] to get its initial embedding vector xi by:\n$$X_{i}=\\text{Sentence-BERT}(x_{i}).$$(2)\nThis process allows texts of any dataset and length to be converted into the same feature space.\nGNN representations. Next, the model takes the text representations obtained from Eq. 2 for each node and feeds them into multiple GNNs. For the k-th GNN, we denote the node embedding matrix H[k] generated by the final layer as:\n$$H^{[k]}=\\text{GNN}^{[k]}\\left(A, X \\mid \\Theta_{G_{k}}\\right),$$(3)\nwhere X is the initial node embedding matrix from Eq. 2 and \u0398Gk is the learnable parameters of the k-th GNN. For each node vi, let h[k] be its embedding vector, which is the i-th row in H[k].\nReshape node representations. The output of each GNN is then connected to a linear layer, which transforms node representations to the dimensionality of hidden embeddings in LLMs. This step paves the way for the subsequent GNN and LLM alignment. Details will be given in the next section. For each node vi in the k-th GNN, its reshaped embedding vector h\u0304k] is denoted as\n$$\\bar{h}_{i}^{[k]}=\\text{Linear}^{[k]}\\left(h_{i}^{[k]} \\mid \\Theta_{L_{k}}\\right).$$(4)\nNote that the shape of h\u0304k] is a one-dimensional vector of length t \u00d7e, where t is a hyperparameter indicating the number of graph tokens each node representation is mapped to. The dimensionality e comes from the hidden embedding layer of the LLM used, where each token is mapped to an embedding of shape (1, e).\nMulti-GNN alignment. To align node representations from multiple GNNs, we next feed them into a shared linear layer, which serves as the classifier. During the training time, we alternatively input representations from different GNNs into the classifier and train them sequentially. This training approach allows node representations from multi-GNNs to be aligned, which facilitates the integration of multi-GNNs. After training, the parameters of GNNs and the linear layer in Eq. 4 will be frozen, which are then used in aligning GNNs and LLMs. \nIn summary, during the multi-GNN alignment step, we train each GNN to ensure that their outputs reside in the same vector space. By incorporating the node feature initialization step that utilizes language pre-trained models, our model gains the ability to extract semantic and structural information from the nodes in the text graph. Building on the full potential of GNNs, we map the dimensions of the GNN representations to the dimensions required by the LLM embeddings, thereby maximizing the integration of extensive graph structural information into the LLM in the subsequent steps.\n4.2 Ensembling multi-GNNs with LLM\nAfter GNNs are trained, we can get node representations capturing rich graph structural information. Although LLMs have shown competitive capability of text semantic understanding, they are ineffective in understanding graph structure [14]. Therefore, in the second stage, we empower LLMs to comprehend graph representations. Further, with specially designed prompts, we enable these models to implicitly perform ensembling.\nAlign GNNs and LLMs. The key step in enabling LLMs to understand GNN tokens lies in aligning GNN representations, which encapsulate rich structural information, with the semantic space"}, {"title": "5 EXPERIMENT", "content": "In this section, we conduct extensive experiments to evaluate the performance of our model across various datasets, aiming to answer the following questions:\n\u2022 RQ1: How does LensGNN perform compared to SOTA baseline models?\n\u2022 RQ2: What is the importance of different components in LensGNN?\n\u2022 RQ3: Can small language models serve as multi-GNN ensembler?\n\u2022 RQ4: How efficient is our method LensGNN ?\n\u2022 RQ5: Is our method LensGNN insensitive to model hyperparameters?\n5.1 Experimental Settings\nDatasets. We use three benchmark datasets. Statistics of these datasets are summarized in Table 1.\nCora is a citation network containing 2,708 research papers. These papers are categorized into seven classes. In this graph, each node represents a scientific paper, and each edge represents a citation. Bag-of-words representation is used as the feature vector for each node, which is also assigned a label indicating its research field.\nPubMed is a citation network dataset containing 19,717 research papers from the biomedical field. These papers are categorized into three classes, and each paper is represented by a bag-of-words feature vector."}, {"title": "5.2 Overall Performance Comparison (RQ1)", "content": "We conduct experiments to compare LensGNN with 17 state-of-the-art competitors on Cora, PubMed and ogbn-arXiv datasets. For Cora and PubMed, we randomly split nodes into 60%, 20%, and 20% for training, validation and testing, and measure the performance of all models on the test set. For ogbn-arXiv, we split the dataset as suggested in [20]. We use pre-trained LM to encode node representations based on their original text for all GNN models."}, {"title": "5.3 Ablation Study (RQ2)", "content": "In this section, we conduct an ablation study on the Cora, PubMed, and ogbn-arXiv datasets to evaluate the impact of each model component. Specifically, we explore various configurations of the 'GNN Encoder' , 'Alignment' (whether multiple GNNs are aligned when training GNNs), 'With Text' (whether node text is included in prompt) and 'With Neighbor' (the number of neighbour hops of the node text when inputting LLMs ).\nPerformance of different GNN encoders. We find that ensembling multiple GNNs noticeably outperforms using a single GNN.\nImpact of Alignment. \nThe Importance of Node Text.\nRole of Neighbors."}, {"title": "5.4 Performance with different LLMs (RQ3)", "content": "To further demonstrate the effectiveness of LensGNN, we use other LLMs as backbones, including three smaller LMs: BERT-base [9], T5-base [33], the encoder-only variant of T5-base, and two other LLMs: Falcon-7B [2] and InternLM2.5-7B-chat [5]. The experiments based on these LLMs include aligning each language model separately with the representation of each type of GNN, as well as aligning them simultaneously with all three types of GNNs (GCN, GAT, GIN) and performing an ensemble."}, {"title": "5.5 Model Efficiency Study (RQ4)", "content": "In this section, we analyze the computational efficiency of LensGNN during both the training and inference phases.\nModel Training Efficiency. We divide the process of LensGNN into two phases: Aligning multi-GNNs and Ensembling multi-GNNs with LLM.\nModel Inference Efficiency."}, {"title": "5.6 Hyperparameter Sensitivity Analysis (RQ5)", "content": "The number of Graph Tokens t is an important hyperparameter, representing how many tokens each node is divided into before being input into LLM."}, {"title": "6 CONCLUSION", "content": "We studied the problem of how to ensemble multi-GNNs in this paper and proposed LensGNN, which ensembles multi-GNNs with LLMs. LensGNN adopts two phases of alignment: the multi-GNN alignment aims to map node representations from different GNNs"}, {"title": "A BASELINES", "content": "To evaluate the effectiveness of LensGNN, we compare it with the SOTA methods. Details of these baselines are summarized as follows.\n(1) MLP: Multilayer Perceptron (MLP) is a type of artificial neural network that consists of multiple layers of nodes (neurons) connected by weights and is primarily used for supervised learning tasks, such as classification.\n(2) Graph Neural Networks: GCN [24] is a fundamental method based on convolutional neural networks which operates directly on graph-structured data. GAT [39] computes the hidden representations of each node in the graph by first learning the importance of its neighbors and then aggregating information from them. GIN [46] develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. GraphSAGE [15] present a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Graphormer [51] is built upon the standard Transformer [38] architecture, and could attain excellent results on a broad range of graph representation learning tasks. \n(3) Language Models: BERT [9] is a groundbreaking model in natural language processing. It utilizes the transformer architecture to understand the context of words in a sentence by looking at both their left and right contexts simultaneously. \n(4) LLM Based:"}, {"title": "B GNN ENCODERS", "content": "Graph Convolutional Networks (GCNs) GCNs extend the concept of convolutional neural networks (CNNs) to graph data. They operate by aggregating and transforming feature information from a node's local neighborhood, effectively capturing the graph's structural information. The propagation rule in GCNs typically involves normalizing the adjacency matrix to ensure proper scaling of node features.\nGraph Attention Networks (GATs) GATs introduce an attention mechanism to the graph convolutional framework, allowing the model to learn the relative importance of neighboring nodes when aggregating their features.\nGraph Isomorphism Networks (GINs) GINs are designed to address the limitations of previous GNN models in distinguishing graph structures."}, {"title": "C DETAILED HYPERPARAMETERS", "content": "The detailed hyperparameters of the first and second stages of LensGNN are shown."}]}