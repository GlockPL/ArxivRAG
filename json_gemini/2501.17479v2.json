{"title": "DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance", "authors": ["Seffi Cohen", "Niv Goldshlager", "Nurit Cohen-Inger", "Bracha Shapira", "Lior Rokach"], "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across various natural language processing tasks but often struggle to excel uniformly in diverse or complex domains. We propose a novel ensemble method - Diverse Fingerprint Ensemble (DFPE), which leverages the complementary strengths of multiple LLMs to achieve more robust performance. Our approach involves: (1) clustering models based on response \"fingerprints\" patterns, (2) applying a quantile-based filtering mechanism to remove underperforming models at a per-subject level, and (3) assigning adaptive weights to remaining models based on their subject-wise validation accuracy. In experiments on the Massive Multitask Language Understanding (MMLU) benchmark, DFPE outperforms the best single model by 3% overall accuracy and 5% in discipline-level accuracy. This method increases the robustness and generalization of LLMs and underscores how model selection, diversity preservation, and performance-driven weighting can effectively address challenging, multi-faceted language understanding tasks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks (Chang et al., 2024; Matarazzo and Torlone, 2025). Yet, when confronted with complex multitask benchmarks such as the Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020), a single LLM often struggles to excel uniformly across all subjects. The MMLU benchmark encompasses diverse subjects, each varying in difficulty and domain breadth. These variations pose significant challenges, revealing performance gaps that no individual model can easily bridge.\nEnsembling multiple LLMs provides a promising avenue to overcome these limitations (Tekin et al., 2024; Jiang et al., 2023b; Mavromatis et al., 2024; Xu et al., 2025). By leveraging the complementary strengths of different models, an ensemble can achieve greater accuracy, robustness, and adaptability than any single model alone (Lu et al., 2024). However, maximizing the potential of an ensemble requires careful selection and integration of its models. Key challenges include identifying models that offer complementary strengths, preserving a diverse set of solution strategies, adapting to subject-specific difficulties, and efficiently aggregating predictions. In this paper, we present the Diverse Fingerprint Ensemble (DFPE) method that aims to optimize the ensemble for a particular subject, thereby improving the performance of LLMs on that subject. Our approach optimizes the ensemble of models by combining effective model selection, subject-level adaptivity, and adaptive weighting. The key contributions of our method include:\n\u2022 Diversity Preservation through Response Pattern Clustering: To ensure the ensemble incorporates diverse problem-solving strategies, we capture each model's response patterns through validation-set \"fingerprints\" and cluster them using DBSCAN. This systematically maintains complementary approaches while filtering redundancy, preventing the ensemble from converging to a single solution path and enhancing its ability to handle varying question types.\n\u2022 Subject-Specialized Expertise Allocation: To balance domain-specific capabilities with strategic diversity, we implement a two-stage process: first clustering models with similar response patterns, then selecting the highest-performing model from each cluster based on subject-specific validation accuracy. These selected models receive weights scaled by their subject performance, creating an ensemble that combines diverse approaches while emphasizing proven expertise.\n\u2022 Adaptive Performance Thresholding: To main-"}, {"title": "2 Related Work", "content": "Ensembling techniques have been widely explored as a means to boost the performance of LLMs on complex multitask benchmarks like MMLU. These methods typically aim to balance improved accuracy with practical constraints such as computational overhead, training requirements, and the diversity of model contributions. Some strategies focus on maximizing diversity across model outputs to exploit complementary strengths, while others emphasize efficient routing or fine-tuning to adapt ensemble components.\nA prominent direction involves assembling multiple models to capitalize on their unique abilities, as seen in LLM-TOPLA (Tekin et al., 2024), PackLLM (Mavromatis et al., 2024), and SweetSpan (Xu et al., 2025), each of which coordinates output information at various granularities (e.g., token- or span-level). While these techniques often produce strong results, they may also introduce extra training or inference overhead. Similar concepts appear in ZOOTER (Lu et al., 2023), which dynamically routes queries to suitable models, and LORA Ensembles (Wang et al., 2023), which uses parameter-efficient fine-tuning to merge model capabilities. However, approaches that rely heavily on additional training or fine-tuning can be less practical when resources are limited or when rapid experimentation is required.\nOther methods seek to remain fully or mostly training-free. Boosted Prompts (Pitis et al., 2023) and CAPE (Jiang et al., 2023c), for instance, forgo further training in favor of leveraging pretrained models and calibrating their outputs. This philosophy aligns with EVA (Xu et al., 2024) and DeePEn (Huang et al., 2024) in that they preserve model diversity or transform model outputs at an inference or validation stage without extensive fine-tuning. However, many of these solutions do not adapt well to subject-wise performance variations, which becomes crucial for wide-ranging benchmarks like MMLU.\nSelectLLM (Maurya et al., 2024) provides an example of per-query adaptivity, where a routing scheme decides which model should handle a particular input. Yet, these methods may not systematically ensure the retention of diverse solution paths"}, {"title": "3 Method", "content": "In this section, we detail our ensemble method designed to optimize LLMs ensemble for a specific subject tasks. Our approach integrates clustering-based selection, quantile-based filtering, and adaptive weighting to ensure that only sufficiently capable and diverse models contribute to the final ensemble prediction. By leveraging per-subject validation performance and subject-specific fingerprints, we construct a more balanced and effective ensemble than naive averaging or manual heuristics.\n3.1 Overview\nAs illustrated in Figure 1, our method involves four main steps:\n1. Model Fingerprinting and Clustering: Represent each model's responses on a subject as a fingerprint vector. Cluster these fingerprints using DBSCAN with cosine similarity, grouping models exhibiting similar response patterns.\n2. Quantile-based Model Selection: Identify a quantile-based accuracy threshold per subject and retain only models meeting or exceeding this benchmark. From each cluster, select the model with the highest accuracy, to ensure both competence and diversity.\n3. Adaptive Weighting Scheme: Assign weights to the selected models using exponential scaling of their per-subject validation accuracy. Normalize these weights to form a proper distribution.\n4. Ensemble Prediction: Aggregate the weighted predictions of the selected models via a weighted voting mechanism, producing the final answer.\nThis pipeline ensures that the ensemble benefits from both model diversity and subject-specific adaptivity, with performance-driven weighting giving more influence to stronger models.\n3.2 Formal Description\nNotation\n\u2022 Let \\(M = \\{M_1, M_2,..., M_N\\}\\) be the set of available LLMs."}, {"title": "3.3 Rationale", "content": "Our method strikes a balance between diversity, competence, and adaptability:\n\u2022 Diversity Preservation: Clustering models ensures variety in solution strategies, reducing redundancy.\n\u2022 Quantile-based Competence: The quantile threshold removes weak models, guaranteeing a baseline performance level.\n\u2022 Adaptive Weighting: Higher accuracy models naturally exert greater influence, but all retained models still contribute to the final decision.\n\u2022 Practical Simplicity: Operating at the output level avoids complex token- or span-level computations, making the method more scalable.\nBy combining these elements, our approach effectively exploits the complementary strengths of multiple LLMs, yielding improved performance on complex multitask language understanding."}, {"title": "4 Experimental Setup", "content": "In this section, we detail the dataset, base models, and implementation specifics used to evaluate our improved ensemble method. We rely on the Massive Multitask Language Understanding"}, {"title": "4.1 Dataset", "content": "We evaluate our method on the MMLU benchmark (Hendrycks et al., 2020), which consists of 57 subjects spanning a diverse range of disciplines, including STEM fields, humanities, and social sciences. Each subject comprises multiple-choice questions in English, which vary in difficulty. The given benchmark data is partitioned into a few-shot validation set for model selection and fingerprint generation, and a separate test set for the final evaluation. Overall, there are 14,079 test samples and 1,540 validation samples. The MMLU dataset is distributed under the MIT License, which allows for free use, modification, and distribution as long as the original copyright notice and license terms are maintained."}, {"title": "4.2 Base Models", "content": "Our ensemble is constructed from a pool of up to 9B parameters open LLMs differing in architecture, size, and training methodology, including GLM-4-9B-chat (GLM et al., 2024), Qwen2.5-7B-Instruct (Yang et al., 2024), Qwen2.5-3B-Instruct (Yang et al., 2024), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023a), Phi-3.5-mini-instruct (Abdin et al., 2024), Llama-3.1-8B-Instruct (Dubey et al., 2024), Gemma-2-9B (Team et al., 2024), Apollo2-7B (Zheng et al., 2024), Starling-LM-7B-alpha (Zhu et al., 2024), and Yi-1.5-6B (Young et al., 2024). These models are detailed in Table 2. The best single model in our experiments achieves 70.76% overall-accuracy on MMLU, with a discipline-accuracy of 69%. Other models range from approximately 61.5% to 70.5% overall-accuracy, providing a rich diversity of performance levels. This variability creates an opportunity to exploit complementary strengths: by carefully selecting and weighting models via quantile-based filtering and clustering, we aim to surpass the capabilities of any single model."}, {"title": "4.3 Implementation Details", "content": "Reproducible Code The source code and raw experiment results are available at https://github.com/nivgold/DFPE.\nClustering and Quantile Thresholding: For each subject, we embed each model's validation responses to form a fingerprint vector. We then cluster these fingerprints using DBSCAN with cosine similarity. The DBSCAN epsilon parameter is chosen based on empirical tuning to ensure stable, meaningful clusters. We select a quantile parameter q = 0.05 based on initial validation performance. Models meeting or exceeding the q-quantile accuracy threshold are retained.\nAdaptive Weighting and Hyperparameters: We apply exponential scaling to each retained model's per-subject validation accuracy. Parameters such as the AccuracyFactor and quantile thresholds are determined via validation-based exploration. For example, setting the AccuracyFactor (\\(\\gamma\\)) to 5.0 and choosing q = 0.05 consistently yields strong performance.\nFew-Shot Validation for Selection and Weighting: For each subject, given validation questions (approximately 10% of the test set) to measure model accuracy. These validation accuracies guide the quantile threshold, clustering-based selection, and the adaptive weighting scheme.\nHardware Details: All experiments were conducted on RTX 6000 GPU, which provided sufficient computational resources for up to 9B parameters LLMs."}, {"title": "4.4 Evaluation Metrics", "content": "We report both the Overall Accuracy and Discipline-Accuracy which accounts for discipline-level balance, ensuring improvements are not limited to a few disciplines. We compare four configurations:\n\u2022 Best Single Model (BSM): The single model achieving the highest test accuracy is which accounts for subject-level balance, ensuring improvements are not limited to a few subjects.\n\u2022 Best Single Model on Validation (BSMoV): The model that performs best on the validation set.\n\u2022 Majority Voting Ensemble (MVoting): Equal weights ensemble method.\n\u2022 Our Ensemble (DFPE): The proposed quantile-driven cluster-based selection method with adaptive weighting is named DFPE.\nThese comparisons demonstrate how DFPE can outperform all the compared methods."}, {"title": "5 Results", "content": "We evaluate the performance of DFPE through extensive experiments, analyzing overall accuracy improvements, discipline-specific gains, and the impact of various hyperparameters. To enhance clarity, we aggregated the results at the subject level into discipline-level outcomes. Our findings demonstrate significant and consistent improvements over baseline approaches across a wide range of disciplines."}, {"title": "5.1 Overall Performance", "content": "DFPE consistently outperforms all comparison methods on the MMLU benchmark. As shown in Table 3, it achieves higher overall accuracy and discipline-specific accuracy than the best single models (BSM, BSMoV) as well as majority voting (MVoting). These results underscore the effectiveness of DFPE's design, which leverages quantile-based filtering to identify top-performing predictions, clustering to preserve diverse model insights, and adaptive weighting to integrate complementary strengths into a unified ensemble."}, {"title": "5.2 Discipline-level Analysis", "content": "Table 4 provides a detailed breakdown of our results across 20 distinct disciplines. Overall, DFPE attains the highest average accuracy (0.740) and achieves the top score in 9 out of 20 disciplines"}, {"title": "5.3 Sensitivity Analysis", "content": "We conduct comprehensive sensitivity analyses across three key hyperparameters, as visualized in Figure 3:"}, {"title": "5.4 Model Participation and Efficiency", "content": "In our optimal accuracy configuration, DFPE typically retains most of the available models (9 out of 10) per subject. This suggests that in the highest-performing setting, the diversity and complementary strengths of nearly all models contribute to the ensemble's success. Although embedding responses, clustering, and validation-based selection introduce some overhead, the overall method remains efficient and does not require additional training or fine-tuning. For scenarios where computational efficiency is prioritized over maximum accuracy, we provide alternative configurations in Appendix A that achieve a balance between performance and model count."}, {"title": "5.5 Discussion", "content": "Our extensive experimental results provide several key insights into the effectiveness and practical implications of DFPE. We organize our discussion around three main aspects: performance characteristics, Model Pool Homogeneity, parameter insensitivity, and trade-offs for practical considerations.\nPerformance Characteristics Our method's strong results can be attributed to effectively leveraging a diverse set of models via clustering, quantile-based filtering, and adaptive weighting. By preserving model heterogeneity, DFPE capitalizes on complementary strengths, often retaining a majority of models for each subject while assigning higher influence to those that demonstrate subject-specific expertise. In addition, recent work has highlighted new ensembles like Mixtral-of-Experts (8x7B) from Mistral.ai\u00b9 that achieve strong performance across multitask benchmarks. While some novel LLMs can surpass DFPE in absolute accuracy, they generally demand far larger computational resources. Notably, DFPE still exceeds certain significantly larger models, including LLaMA-2 70B, reinforcing the effectiveness of its ensemble approach. Incorporating those newer, larger LLMs into the DFPE model pool represents a promising direction for future work, potentially unlocking even further gains in performance through expanded ensemble diversity.\nModel Pool Homogeneity It is important to note that the method's effectiveness hinges on the diversity present within the pool of base LLMs. Designed to capitalize on complementary strengths,"}, {"title": "6 Conclusion", "content": "In this paper, we introduced DFPE, an ensemble method that leverages diversity and adaptivity to improve LLMs performance on multitask language understanding tasks. DFPE clusters models via \"fingerprint\" patterns, filters underperformers with a quantile-based threshold, and applies exponential weighting that emphasizes top-performing models while preserving valuable secondary perspectives. On the MMLU benchmark, DFPE achieves approximately 3% higher overall accuracy and a 5% boost in discipline-level accuracy over the best single model. These results underscore the importance of diverse solution strategies, selective filtering, and dynamic weighting. While DFPE currently focuses on multiple-choice tasks, applying its fingerprinting strategy to open-ended settings remains a promising direction. Future work also includes refining question-level adaptivity, exploring more scalable clustering, and addressing the unique challenges posed by large LLM pools. By balancing diversity, adaptivity, and efficiency, DFPE offers a practical and robust framework for high-performing ensembles across varied tasks."}, {"title": "Limitations", "content": "While DFPE demonstrates significant improvements, several limitations warrant discussion: Dependence on Few-Shot Validation Data DFPE relies on few-shot validation data for initial model selection and weighting. Computational Overhead: Running multiple models requires more computational resources than using a single small model. Model Diversity Requirement: The approach benefits from having a diverse pool of models with complementary strengths, and its effectiveness may reduced with homogeneous model sets; Task-Specificity: The current implementation is confined to multiple-choice question answering."}, {"title": "Acknowledgements", "content": "We used ChatGPT-4o for editing the language and refining the presentation of the text in this paper. The authors affirm that all research content and ideas are their own, and they take full responsibility for the final submitted manuscript."}, {"title": "3.1", "content": "Let \\(S = \\{S_1, S_2, ..., S_K\\}\\) represent the subjects"}, {"title": "3.2", "content": "For each subject \\(S_k\\), let \\(Q_k =\\)\n\\(\\{q_{k,1}, q_{k,2},\\cdots, q_{k,L_k} \\}\\) denote its questions.\nEach question \\(q_{k,l}\\) has choices \\(C_{k,l}\\)\n\\(\\{C_{k,l,1},..., C_{k,l,C_{k,1}}\\}\\) .\nLet \\(\\hat{c}_{i,k,l}\\) be the prediction of model \\(M_i\\) for question \\(q_{k,l}\\).\nLet \\(a_{i,k}\\) denote the validation accuracy of model \\(M_i\\) on subject \\(S_k\\).\nFor each\nModel Fingerprinting and Clustering\nmodel \\(M_i\\) and subject \\(S_k\\), we produce a fingerprint vector \\(f_{i,k}\\) summarizing the model\u2019s response behavior on validation data. We use the pretrained sentence embedding model all-MiniLM-L6-v2 (Face, 2023) to embed each output and average the embeddings to create the fingerprint.. Once we obtain \\(\\{f_{i,k}\\}_{i=1}^{N}\\), we apply DBSCAN (Ester et al., 1996) clustering with cosine similarity to group models into clusters based on their response patterns."}, {"title": "3.3", "content": "Define a quan-\nQuantile-based Model Selection\ntile parameter q (e.g., q = 0.1) to set a subjectspecific accuracy threshold. Let \\(\\{a_{i,k}\\}_{i=1}^{N}\\) be the accuracies of all models on \\(S_k\\). Compute the qquantile accuracy threshold \\(@_{q,k}\\) for subject \\(S_k\\). Retain models satisfying \\(a_{i,k} \\geq @_{q,k}\\), ensuring a baseline level of competence per subject.\nFrom the filtered models, select the representative model from each cluster:\n\\(M_k = arg max a_{i,k}\\)\n\\(M_iEC_j\\)\nThe set of all such representatives for subject \\(S_k\\) is\n\\(\\mathcal{M}_k\\).", "1Scheme": "Adaptive Weighting", "2For": "For each selected\nmodel \\(M_i \\in \\mathcal{M}\\), we assign a weight:"}, {"title": "3.4", "content": "\\(w_{i,k} = exp(\\gamma \\cdot A_{i,k})\\),\nwhere \\(\\gamma\\) is a scaling factor that highlights performance differences. Normalize these weights so that they sum to 1:\n\\(\\hat{w}_{i,k} = \\frac{w_{i,k}}{\\sum_{M\\in\\mathcal{M}} w_{j,k}}\\)\nEnsemble Prediction For a test question \\(q_{k,l}\\) in subject \\(S_k\\), each selected model \\(M_i \\in \\mathcal{M}\\) provides a predicted choice \\(\\hat{c}_{i,k,l}\\). We aggregate their votes using the assigned weights:\n\\(V_{k,l}(c) = \\sum_{M\\in\\mathcal{M}} \\hat{w}_{i,k} \\cdot \\mathbb{I}[\\hat{c}_{i,k,l} = c]\\)."}, {"title": "4.1", "content": "The final ensemble prediction is:\n\\(\\hat{c}_{k,1} = arg max V_{k,1}(c).\\)\n\\(CEC_{k.l}\\)"}, {"title": "5 Evaluation Metrics", "content": "for each subject \\(S_k\\) do\n1. For models \\(M_i\\), compute accuracy \\(a_{i,k}\\) on\nvalidation \\(Q_k\\), filter out those below q-quantile.\n2. Create fingerprints \\(f_{i,k}\\) (e.g., embeddings),\ncluster via DBSCAN.\n3. From each cluster, pick the model \\(M_i\\)\nwith highest \\(a_{i,k}\\); call this set \\(\\mathcal{M}\\).\n4. Assign weights \\(w_{i,k} = exp(\\gamma a_{i,k})\\) to\nmodels in \\(\\mathcal{M}\\) and normalize.\nend for\nfor each test question \\(q_{k,l}\\) in \\(S_k\\) do\nAggregate predictions with weighted votes:\n\\(\\hat{c}_{k,1} = arg max \\sum_{M\\in\\mathcal{M}} (w_{i,k} \\cdot \\mathbb{I}[C_{i,k,l} = c]).\\)\nend for"}, {"title": "A Appendix A", "content": "While our main results focus on the optimal accuracy configuration, practitioners often need to balance performance gains against computational costs. Here we present a detailed analysis of an alternative configuration that achieves near-optimal performance while significantly reducing computational overhead.\nA.1 Balanced Configuration Parameters\nWe identified a balanced configuration with the following parameters:\n\u2022 Quantile threshold: 0.5 (vs. 0.05 in optimal setting)\n\u2022 AccuracyFactor: 7 (vs. 5 in optimal setting)\n\u2022 DBSCAN Epsilon: 0.001 (vs. 0.0001 in optimal setting)\nThis configuration achieves an average accuracy of 72.5% (1% below optimal) while reducing the mean number of models per subject to 6 (compared to 9 in the optimal setting)."}, {"title": "A.2 Model Selection Analysis", "content": "Figure 4 shows the distribution of selected models across subjects. Several key patterns emerge:\n\u2022 Variation in model count: The number of selected models varies from 1 to 10 across subjects\n\u2022 Subject-specific adaptation: Different subjects benefit from different ensemble sizes\n\u2022 Consistent core: Most subjects maintain 6-8 models, suggesting a natural balance point"}, {"title": "A.3 Model Co-occurrence Analysis", "content": "To understand model relationships, we analyzed their co-occurrence patterns within clusters (Figure 5). Our analysis reveals several interesting patterns:\n\u2022 Strong Partnerships: - Qwen family models (Qwen2.5-3B and Qwen2.5-7B-Instruct) show highest co-occurrence (52 instances) - Strong affinity between Qwen models and Llama-3.1-8B (51-52 co-occurrences) - Phi-3.5-mini frequently pairs with Qwen models (48-49 instances)\n\u2022 Complementary Groups: Models cluster into \"specialists\" and \"generalists\" - Lower co-occurrence patterns indicate complementary strengths\n\u2022 Model Independence: - Some models show consistent independence - Suggests unique capabilities or specialization"}, {"title": "A.4 Practical Implications", "content": "These findings have several important implications for practitioners:\nResource Optimization: The balanced configuration offers a practical trade-off between performance and computational cost\nModel Selection: Strong co-occurrence patterns can guide initial model selection when building new ensembles\nDeployment Strategy: Subject-specific ensemble sizes suggest opportunities for dynamic resource allocation"}]}