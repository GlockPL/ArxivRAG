{"title": "Sequence-to-Sequence Multi-Modal Speech In-Painting", "authors": ["Mahsa Kadkhodaei Elyaderani", "Shahram Shirani"], "abstract": "Speech in-painting is the task of regenerating missing audio contents using reliable context information. Despite various recent studies in multi-modal perception of audio in-painting, there is still a need for an effective infusion of visual and auditory information in speech in-painting. In this paper, we introduce a novel sequence-to-sequence model that leverages the visual information to in-paint audio signals via an encoder-decoder architecture. The encoder plays the role of a lip-reader for facial recordings and the decoder takes both encoder outputs as well as the distorted audio spectrograms to restore the original speech. Our model outperforms an audio-only speech in-painting model and has comparable results with a recent multi-modal speech in-painter in terms of speech quality and intelligibility metrics for distortions of 300 ms to 1500 ms duration, which proves the effectiveness of the introduced multi-modality in speech in-painting.\nIndex Terms: speech enhancement, speech in-painting, sequence-to-sequence models, multi-modality", "sections": [{"title": "1. Introduction", "content": "Auditory information loss may be caused by various sources such as transmission problems, e.g. voice-over-IP connection issues, or noise contamination problems, e.g. corrupted physical devices. Even though enhancing audio signals is an important processing task, it has remained a challenge when corruptions are of long durations (> 200 ms). An audio in-painter generates missing parts of audio from the available context. The first attempts in audio in-painting used the help of auto-regressive models, which estimate the missing samples as linear combinations of their neighbours [1], [2]. The self-similarity approach in [3] exploited available user records to fill in missing audio parts. The recurring musical structures were leveraged in [4] to learn internal redundancies that exist in time-frequency representations and recover long-duration distortions in musical pieces. Sparse representation modelling in audio in-painting was employed in [5] and [6] to approximate each audio frame as a sparse linear combination of the columns of a dictionary.\nAmong neural network-based methods, recently [7] proposed a convolutional U-net for suppressing noise injected into temporal or spectral dimensions of speech spectrograms. The approach of [7] employed a VGG-like deep feature extractor, called SpeechVGG, which was obtained by pre-training the famous VGG model for classifying the 1000 most-frequently spoken words in their training dataset. Since spectrograms can be viewed as two-dimensional images, the method of [8] tackled the task of filling in missing contents of an audio signal by applying an image in-painting technique, like in [9], to the audio spectral representation. The major innovation of [8] was that they replaced standard convolution layers with either gated convolutions (in the case of audio waveform in-painting) or dilated/strided convolutions (in the case of spectrogram in-painting). Similar to [7], [8] investigated the use of a perceptual loss function.\nThe authors of [10] proposed two models for uninformed audio in-painting that consisted of down-sampling, residual, and up-sampling blocks to learn and in-paint the locations of noise in the audio. They applied partial convolutions to make the convolution of masked spectrograms only dependent on uncorrupted pixels. Incorporating different modalities into audio in-painting provided complementary information about the signal and enabled robust inference in [11]. Multi-modality was also shown to be successful in the audio-visual correspondence learning method of [12]. The proposed method of [13] incorporated video features into the audio in-painting task to recover speech gaps ranging from 100 ms to 1600 ms using Long Short-Term Memory (LSTM) networks.\nLately, Generative Adversarial Networks (GANs) were applied for in-painting missing audio content. Authors of [14] introduced a conditional GAN that used contextual information, along with multiple discriminators of different receptive fields, to distinguish real spectrograms from fake ones. In [15], a Wasserstein GAN was proposed to restore missing audio from adjacent intact regions by minimizing the Wasserstein distance between the ground-truth and the generated data distributions. The multi-modal GAN architecture proposed by [16] for audio in-painting consisted of a convolutional encoder-decoder model that worked in the joint audio-visual feature space to reconstruct missing audio disruptions of up to 800 ms. The method from [16] leveraged the WaveNet generative model [17] to decode spectrogram outputs into high-quality audio waveforms. Despite the reported success of GAN-based models for audio in-painting, the training process of these models was computationally challenging.\nWhile many state-of-the-art audio in-painting methods perform over music or environmental sound signals, in this paper, we propose a novel method for recovering speech signals. Inspired by the models in image in-painting and machine translation [18], our contribution is to \"translate\" the visual modality into the auditory modality and reconstruct the missing or corrupted pieces when the locations of such distortions are known. Our sequence-to-sequence model consists of an encoder, to incorporate lip motion features from the videos, and a decoder, for audio spectrograms. Both encoder and decoder are made of stacked bi-directional LSTM (BLTSM) layers. The remainder of the paper is organized as follows. In Section 2, we elaborate on our sequence-to-sequence multi-modal approach. In Section 3, we discuss our ablation studies and compare our method with another recent multi-modal speech in-painter, and finally,"}, {"title": "2. Proposed Method", "content": "In designing a speech in-painting model, it is important to consider the sequential nature of speech signals. The sequence-to-sequence model introduced in this paper has an encoder-decoder architecture to recover missing short words or corrupted phonemes of speech signals by using the corresponding intact videos. Assume $X = [X_1,X_2,...,X_T]$ is the spectrogram of a clean speech signal, where $x_t$ for $1 < t < T$ denotes the frequency vector corresponding to time t. The corrupted frequency vector $a_t$ is obtained by multiplying $x_t$ by the mask $m_t \\in {0,1}$, i.e. $a_t = m_tx_t$. Moreover, let us assume $Y = [Y_1, Y_2,..., y_T]$ denotes the outputs of the decoder. The frequency vectors of the signal spectrogram can then be reconstructed as\n$O_t = m_tx_t + (1-m_t) \\cdot Y_t$,\nwhere $1 < t < T$. We assume the locations of the masked parts of the spectrogram are known to our method, making it an informed in-painting method. Figure 1 illustrates the overview of our proposed method, which will be explained in details through the rest of this section."}, {"title": "2.1. Encoder", "content": "The strong relationship between phonemes of a language and their visual articulations, i.e. shapes of the mouth, forms the main idea behind multi-modal speech in-painting. However, the challenge is that multiple phonemes can have similar mouth shapes, despite their distinguished sounds. As a result, a robust lip reading module exploits the contextual data to differentiate visually-identical phonemes. In our sequence-to-sequence model, the encoder acts as a lip reading module inspired by the LipNet model in [19] and the Lipreading model in [20]. Originally, both of [19] and [20] methods were designed to transcribe videos from speakers' lips movements via minimizing the Connectionist Temporal Classification (CTC) loss [19] over the Grid Corpus [21]. More precisely, the method from [19] exploited spatiotemporal convolutional and BLSTM layers on video frames, while [20] approach used LSTMs only.\nAs depicted in Figure 1, the output of our sequence-to-sequence model, $Y_1, Y_2,..., y_T$, is a function of both visual features $V_1, V_2,..., v_{T'}$ and auditory cues $a_1, a_2,..., a_T$. In our sequence-to-sequence model, the role of the encoder is to transform visual features into a sequence of vectors that are helpful for the decoder to regenerate audio spectrograms at each timestep. As a result, the inputs to the encoder are obtained from cropped mouth-centered video frames. Similar to [22], facial landmarks are extracted from all cropped video frames and their differences across consecutive frames are fed as landmark motion vectors to the encoder. Then, BLSTM layers are applied to the computed motion vectors in order to leverage the temporal dependency of extracted visual features. The last BLSTM layer outputs a sequence of hidden states for each input time step. A fully-connected (FC) layer is responsible for outputting features that help the decoder generates missing audio segments, while the top softmax encoder layer generates phoneme probabilities that will be used in computing the CTC loss."}, {"title": "2.2. Decoder", "content": "The decoder part of our sequence-to-sequence model in-paints spectrogram representations of degraded audios. The input to the decoder is the concatenation of the spectral features and the outputs of the last fully-connected layer of the encoder. The decoder estimates the conditional probability of reconstructed spectrogram vectors, given the input visual and audio features:\n$p(y_1, y_2,..., y_T | a_1, a_2,..., a_T, V_1, V_2,..., v_{T'}).$                                                                                      (1)\nAs presented in Equation 1, the audio sequence, $a_1, a_2,..., a_T$, and the video sequence, $V_1, V_2,..., v_{T'}$, can be of different lengths $T$ and $T'$, respectively. In our setup, $T > T'$, originally. Therefore, before feeding the video frames to the encoder, we up-sample the frames so that their number matches the temporal length of input spectrograms, i.e., we make sure $T = T'$, after up-sampling. Assuming the outputs of the last fully-connected layer of the encoder are denoted by $C_1, C_2,..., C_{T'}$, then our model estimates the above probability by the following expression\n$p(y_1, y_2,..., y_T | a_1, a_2,..., a_T, C_1, C_2,..., C_{T'}).$                                                                                  (2)\nThe encoder outputs $C_1, C_2,..., C_T$ and spectral vectors $a_1, a_2,..., a_T$ are temporally concatenated and fed into three BLSTM layers and a FC layer.\nThe encoder and decoder modules are trained in an end-to-end fashion to minimize a joint loss function that allows the model to learn useful visual and auditory features for speech in-painting. Specifically, we minimize a weighted sum of the mean-squared error (MSE) between the reconstructed and ground-truth spectrograms and the CTC loss for predicting the sequence of spoken phonemes:\n$loss = MSE + \\lambda \\cdot CTC,$                                                                                               (3)"}, {"title": "3. Experimental Results", "content": "To prepare the data for our experiments, we pre-process both audio and video modalities. First, the audio signals are re-sampled from 25kHz to 8kHz and a pre-emphasis filter is applied to signals to boost high frequencies. Then, the audio Mel-spectrograms are computed by performing the Short-Term Fourier Transform (STFT) with 320 sample points (40 ms) and a hop size of 160 points (20 ms). The length of the windowed signal after padding with zeros is 510 samples corresponding to a duration of 63.75 ms. The magnitude of the STFT is then transformed to Mel scale using 64 Mel filter banks that is followed by log dynamic range compression. The resulting Mel-scaled spectrograms have 64 frequency bins and 149 temporal units. Mel-scaled spectrograms are normalized to lie in the range of 0 to 1.\nThe missing samples of spectrograms are simulated by randomly masking 300 to 1500 ms of spectrograms' temporal units. The total duration of masks is drawn from a normal distribution with a mean of 900 ms and a standard deviation of 300 ms. The masked area is split uniformly at random into 1 to 8 gaps of a minimum length of 36 ms. To recover the phase information of processed spectrograms and convert Mel spectrograms back into audio, the Griffin-Lim method is run for 300 iterations [23].\nDuring video preparation, the RGB frames are converted into gray-scale images. Then, the dlib face detector is applied to get the frontal faces of the speakers and the dlib shape predictor is used to extract 68 facial landmarks [24]. Using facial landmarks, we only keep mouth regions and crop video frames into areas of 100 x 50 pixels with the mouth in the center. Differences between facial landmarks of consecutive cropped frames are computed as visual features. This enables us to track motions between the frames, while throwing away the static background information. Finally, all processed visual features are normalized to have their values in the range of 0 to 1.\nTo split the Grid Corpus into the train, validation, and test sets, 26 speakers (s1-20, s22-29, s31) are assigned to the train set, and 4 speakers (s30, s32-34) to the test. Moreover, the speakers (s26-27, s29, s31) are randomly split into two equal sets, with half of them being used as validation. The train and test splits are the same as the one used in [13] to provide us with a fair condition for comparisons. Hence, using 26 speakers for training and 4 (different) speakers for test will make 27752 and 1498 samples for train and test, respectively. Also, the validation set has 996 samples from a separate set of 4 speakers."}, {"title": "3.3. Experiment Setting", "content": "Throughout our training experiments in this section, we employ the Adam stochastic optimization algorithm [25] with a learning rate of 0.001 and a mini-batch size of 32 samples. During training, the learning rate is dropped by a factor of 0.1 in case the training loss stops improving for five epochs. Furthermore, an early stopping callback is defined that will terminate the training process if the validation loss does not decrease across ten consecutive epochs. The BLSTM layers have a latent dimension of 256 during all experiments in this paper. Our implementations are based on the Tensorflow Keras 2.11.0 and we run them on a MacBook Pro. M1 machine."}, {"title": "3.4. Comparisons", "content": "The core idea of our approach is to train an encoder-decoder model with BLSTM layers, which leverages pairs of visual and spectral features. Our model has been evaluated quantitatively on the Grid Corpus dataset. Similar to the method proposed in [13], the reference and degraded audio quality and intelligibility are analyzed sample-by-sample via PESQ (Perceptual Evaluation of Speech Quality) [26] and STOI (Short Term Objective Intelligibility) metrics [27]. Also, the Peak Signal-to-Noise Ratio (PSNR) and the Mean Squared Error (MSE) are calculated to quantify the similarity of reconstructed spectrograms to the originals.\nTo investigate the effectiveness of the proposed approach, we compare it with several existing speech in-painting methods as listed in the below.\n\u2022 A-SI: The Audio Speech In-painting model is our baseline model for speech in-painting. It restores the masked spectrograms using only the input audio spectrograms. Similar to the decoder module of our sequence-to-sequence model depicted in Figure 1, this model consists of a stack of three BLSTM layers and an FC layer. The FC is a time-distributed dense layer with 64 dimensions. The loss function of this model is MSE in order to minimize the error between generated and original spectrograms.\n\u2022 AV-S2S: The Audio-Visual Sequence-to-Sequence model has the same architecture as our proposed model depicted in Figure 1. It consists of a stack of three BLSTM layers in both the encoder and the decoder. We up-sample facial landmarks obtained from video frames to the same temporal length as audio spectrograms by applying a bivariate spline approximation. Then, the differences of consecutive facial landmarks of mouth regions are fed to BLSTM layers. The decoder takes the concatenation of the encoder's outputs and audio spectrograms as its inputs. The in-painted spectrograms are obtained via three BLSTM layers and a final FC layer. Unlike, the AV-MTL-S2S that is introduced in the below, here we only minimize the MSE loss function.\n\u2022 AV-MTL-S2S: The Audio-visual Multi-Task Learning Sequence-to-Sequence model is the core model in this paper as illustrated in Figure 1. Here, the loss function is as depicted in Equation 3 and the trade-off parameter $\\lambda$ of the"}, {"title": "4. Conclusions", "content": "In this paper, we studied a novel multi-modal speech in-painter, which was inspired by the idea of sequence-to-sequence models in machine translation and image in-painting. The main contribution was to exploit the lip-reading and spectral features in an encoder-decode architecture. The visual cues were leveraged to guide speech generation. Our developed methods were evaluated on a speech dataset to regenerate the missing short words or phonemes. The duration of simulated distortions in our experiments range from 300ms to 1500ms for every 3s long audio. Distortions longer than 500ms are large and extremely large distortions (up to 1500ms) are studied in very few works. The experimental results showed the model was capable of restoring degraded spectrograms. Compared with the corrupted speech, both the quality and intelligibility of reconstructed audio pieces were improved. As a future direction, we plan to substitute RNN networks with transformer-based and covolutional models to do speech in-painting. Also, integrating a cross attention mechanism (similar to the one proposed in [28]) may help the decoder with attending to relevant visual frames for spectrogram in-painting. Furthermore, it is interesting to investigate how the results will be impacted if different noises are added to the audio, video frames are not clean and/or locations of miss-"}]}