{"title": "Efficient Image Restoration via Latent Consistency Flow Matching", "authors": ["Elad Cohen", "Idan Achituve", "Idit Diamant", "Arnon Netzer", "Hai Victor Habi"], "abstract": "Recent advances in generative image restoration (IR) have demonstrated impressive results. However, these methods are hindered by their substantial size and computational demands, rendering them unsuitable for deployment on edge devices. This work introduces ELIR, an Efficient Latent Image Restoration method. ELIR operates in latent space by first predicting the latent representation of the minimum mean square error (MMSE) estimator and then transporting this estimate to high-quality images using a latent consistency flow-based model. Consequently, ELIR is more than 4x faster compared to the state-of-the-art diffusion and flow-based approaches. Moreover, ELIR is also more than 4x smaller, making it well-suited for deployment on resource-constrained edge devices. Comprehensive evaluations of various image restoration tasks show that ELIR achieves competitive results, effectively balancing distortion and perceptual quality metrics while offering improved efficiency in terms of memory and computation.", "sections": [{"title": "1. Introduction", "content": "Image restoration (IR) is a challenging low-level computer vision task focused on generating visually appealing high-quality (HQ) images from low-quality (LQ) images (e.g. noisy, blurry). Image deblurring (Kupyn et al., 2019; Whang et al., 2022), blind face restoration (Wang et al., 2021b; Li et al., 2020), image super-resolution (Dong et al., 2012; 2015), image denoising, inpainting, and colorization can be categorized under IR. The scope of IR applications is extensive, encompassing mobile photography, surveillance, remote sensing, and medical imaging.\nAlgorithms that tackle the IR problem are commonly evaluated by two types of metrics: 1) a distortion metric (e.g. PSNR) that quantifies some type of discrepancy between the reconstructed images and the ground truth; 2) perceptual quality (PQ) metric (e.g. FID (Heusel et al., 2017)) that intends to assess the appeal of reconstructed images to a human observer. The distortion and PQ metrics are usually at odds with each other leading to a distortion-perception trade-off (Blau & Michaeli, 2018). This trade-off can be viewed as a Pareto frontier, which can be framed as an optimization problem by minimizing distortion while achieving a given perception index. Out of all points on the distortion-perception Pareto frontier, the main goal of the IR task is to find the point where the estimator achieves minimal average distortion under a constraint of perfect perceptual index (Ohayon et al., 2024). A solution for this problem (Freirich et al., 2021) can be obtained by initially using a minimum mean square error (MMSE) estimator, followed by sampling from the posterior distribution of visually appealing images given the MMSE output.\nRecently, several approaches have explored this direction, proposing two-stage algorithms (Yue & Loy, 2024; Lin et al., 2023; Rombach et al., 2022; Zhu et al., 2024; Yue et al., 2024; Ohayon et al., 2024). In the first stage, a neural network is utilized to correct the distortion error. Then, in the second stage, a conditional generative model is employed to sample visually appealing images conditioned on the output of the first stage. Typically, the first stage is trained to minimize a distortion metric (e.g. l1, l2), while the second stage is trained using a diffusion (Sohl-Dickstein et al., 2015; Ho et al., 2020) or a flow matching objective. (Albergo & Vanden-Eijnden, 2023; Lipman et al., 2023; Liu et al., 2023)\nAlthough these methods achieve state-of-the-art results, deploying them on edge devices such as mobile phones or image sensors is challenging due to significant memory and computational requirements. The high demands stem from three main reasons: (i) the transformer-based architecture used by these methods, which incurs substantial computation and memory costs; (ii) state-of-the-art approaches based on diffusion or flow matching necessitate multiple neural function evaluations (NFE) during inference, posing difficulties for edge devices; (iii) many methods operate directly in pixel space, demanding high computational costs, particularly at high resolutions."}, {"title": "Our contributions are summarized as follows:", "content": "We introduce the Latent Minimum Mean Square Error estimator (Latent MMSE) which approximates the posterior mean in the latent space.\nWe integrate latent flow matching with consistency flow matching for the first time, which reduces the NFEs as well as the cost of each evaluation.\nWe performed experiments on various tasks including blind face restoration, image super-resolution, image denoising, inpainting, and colorization. The results show a 4-45x reduction in memory size and a 4-270x reduction in latency compared to state-of-the-art diffusion & flow-based methods while maintaining competitive performance."}, {"title": "2. Related Work", "content": "Various approaches have been suggested for image restoration (Zhang et al., 2018a; 2021; Luo et al., 2020; Liang et al., 2021; Zhou et al., 2022; Lin et al., 2023; Yue & Loy, 2024; Zhu et al., 2024; Ohayon et al., 2024). In recent years, solutions for IR based on generative methods, including GANs (Goodfellow et al., 2014), diffusion models (Song et al., 2021) and flow matching (Lipman et al., 2023), have emerged, yielding impressive results.\nGAN-based methods. GAN-based techniques have been proposed to address image restoration. BSRGAN (Zhang et al., 2021) and Real-ESRGAN (Wang et al.) are GAN-based methods that use effective degradation modeling process for blind super-resolution. GFPGAN (Wang et al., 2021a) and GPEN (Yang et al., 2021) proposed to leverage GAN priors for blind face restoration. GPEN suggested training a GAN network for high-quality face generation and then embedding it to a network as a decoder before"}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. Distortion and Perception", "content": "The perception of image quality is a complex interplay between objective metrics and subjective human judgment. While objective measures like PSNR and SSIM are useful for quantifying distortion, they may not always correlate well with perceived image quality (Wang et al., 2004). Human observers are sensitive to artifacts and inconsistencies, even when they are subtle. Effective image restoration tech-\nniques must therefore aim to minimize both objective distortion and perceptual artifacts, ensuring that the restored image is both visually pleasing and faithful to the original content. Let's denote the high-quality and the corresponding low-quality images as x and y, respectively, and the reconstructed image by \\hat{x}. The distortion is usually evaluated by $D = \\mathbb{E}_{p_{x,\\hat{x}}}[\\Delta (x, \\hat{x})]$, where $\u2206 (x, \\hat{x})$ is a distance function and $p_{x,\\hat{x}}$ is the joint probability function of x and \\hat{x}. The distortion-perception trade-off (Blau & Michaeli, 2018) is defined by:"}, {"title": "3.2. Consistency Flow Matching", "content": "Consistency Flow Matching (CFM) advances flow-based generative models (Chen et al., 2018; Lipman et al., 2023; Liu et al., 2023) by enforcing consistency among learned transformations. This constraint ensures that the transformations produce similar results regardless of the starting point. By utilizing \"straight flows\" for simplified transformations and employing a multi-segment training strategy, CFM achieves enhanced sample quality and inference efficiency. Specifically, given x as an observation in the data space $\\mathbb{R}^d$, sampled from unknown data distribution, CFM first defines a vector field $v(x,t) : \\mathbb{R}^d \\times [0,1] \\rightarrow \\mathbb{R}^d$, that generates the trajectory $x_t \\in \\mathbb{R}^d$ through an ordinary differential equation (ODE):\nYang et al. (2024) suggests to train the vector field by a velocity consistency loss defined as:\nwhere,"}, {"title": "4. Method", "content": "In this work, we address the challenge of developing an efficient method that minimizes average distortion under a perfect perceptual quality constraint as given in (2). By \"efficient\", we refer to the model's memory usage and latency. Specifically, we are given a dataset $S = \\{ y_i, x_i \\}$, consisting of pairs of images where $y_i$ represents the low-quality (LQ) images and $x_i$ represents the high-quality (HQ) images. Our objective is to develop a neural network that can solve Problem (2) efficiently. To achieve this, we propose a method based on the problem solution suggested in Freirich et al. (2021), which minimizes the average distortion while maintaining a perfect perceptual quality (P = 0).\nInspired by this solution, we suggest a two-stage pipeline that operates in latent space. First, we apply a latent MMSE\nestimator on the LQ input image which reduces the distortion error in the latent space. Second, we utilize a latent consistency flow model which samples from the conditional posterior distribution of \\hat{x} given results of the latent MMSE Estimator. The entire process is performed in latent space, which enables efficient inference and significantly reduces the computational costs associated with processing high-resolution images. Moreover, we suggest a hardware-friendly architecture consisting of only convolutional layers (see Appendix 7.1). This architecture is highly optimized for most hardware accelerators, leading to reduced model size and latency, making it suitable for resource-constrained edge devices.\nAn overview of the suggested flow is presented in Figure 2. The latent MMSE and the consistency flow are explained in Subsections 4.1 and 4.2, respectively, and the training and inference procedures are described in Subsection 4.3."}, {"title": "4.1. Latent MMSE", "content": "Here, we describe the latent MMSE estimator, which takes the latent representation of the LQ image and restores it to closely match the latent representation of the HQ image in terms of l2. Specifically, let x and y be a pair of HQ and LQ images, respectively, $\\mathcal{E}_w$ be a pre-trained encoder (parameterized by w) that projects an image to the latent space, and $g_\\phi$ be the latent MMSE estimator (parameterized by $\u03c6$). The objective is to minimize the l2 difference between the latent representations of the LQ and HQ images, which is given by:"}, {"title": "4.2. Latent Consistency Flow Matching", "content": "We introduce the latent consistency flow matching (LCFM) as a combination of consistency flow matching (Yang et al., 2024) and latent flow matching (Dao et al., 2023). LCFM approximates optimal transport between the latent representation of the source and target distributions. It aims to reduce the number of NFEs, which is crucial for edge device runtime, as well as the cost of each NFE. In this work, we wish to sample from the posterior distribution of the HQ images given the results of the Latent MMSE estimator. To achieve this, we define the target distribution $z_1 = \\mathcal{E}_\\omega(x)$, representing the latent representation of the HQ image. The source distribution is then defined as the output of the Latent MMSE estimator from the first stage as follows:\nwhere $\\epsilon \\sim \\mathcal{N}(0, \u03c3_sI)$ is addtive white Gaussian noise with standard deviation $\u03c3_s$. Adding such noise is critical when LQ and HQ images lie on low and high-dimensional manifolds (Albergo & Vanden-Eijnden, 2023). Then, the optimal transport conditional flow from source to target distribution as suggested by Lipman et al. (2023) is given by:\nwhere t \u2208 [0, 1] is the time variable and $\u03c3_{min}$ is an hyperparameter.\nTo sample from the latent target distribution $z_1$, we wish to obtain a vector field $v_\u03b8$ that would drive the direction of the linear path flowing from $z_0$ to $z_1$. To obtain such $v_\u03b8 (z_t, t)$ that allows effective inference, we suggest using multi-segment consistency loss (Yang et al., 2024) in latent space. Specifically, given K segments, the time interval [0, 1] is divided into $[\\frac{i}{K}, \\frac{i+1}{K}]$. Then, the consistency loss of a segment is defined as:"}, {"title": "4.3. Training and Inference procedures", "content": "During training, we optimize (6) and (8) yielding trained parameters $\\omega^*$ and $\u03c6^*$. During inference, we project y into a latent space using $\\mathcal{E}_{\\omega^*}$ and apply the latent MMSE estimator $g_{\u03c6^*}$. Similarly to the training, we add a Gaussian noise with the same standard deviation $\u03c3_s$, utilize the optimized vector field $v_{\u03b8^*}$, and solve the ODE from (3) using the forward Euler method with M steps. Once we obtain HQ latent results, we apply a pre-trained decoder D to project back to the pixel space, yielding HQ images. Algorithm 1 outlines the inference procedure."}, {"title": "5. Experiments", "content": "In this section, we present experiments for the following tasks: blind face restoration (BFR), super-resolution, image denoising, inpainting, and colorization. We train our model for each task with the FFHQ (Karras et al., 2019) dataset which contains 70k high-quality images. The model is trained using the AdamW (Loshchilov & Hutter) optimizer. Both losses $L_2$ and $L_{LCFM}$ are optimized jointly where the gradients of $E_w$ are detached from $\u03c9, \u03c6$. We set \u03bb1 = 1 for all the experiments. During training, we only use random horizontal flips for data augmentation. In addition,"}, {"title": "5.1. Implementation Details", "content": ""}, {"title": "5.1.1. BLIND FACE RESTORATION (BFR)", "content": "Training. The training process is conducted on 512 \u00d7 512 resolution with a first-order degradation model to synthesize LQ images. The degradation (Zhang et al., 2021) is approximated by:"}, {"title": "5.1.2. SUPER RESOLUTION, IMAGE DENOISING, INPAINTING, COLORIZATION", "content": "Training. Similar to the training process outlined in PMRF (Ohayon et al., 2024), we employ a 256 x 256 resolution and utilize the degradation model as follows: For super-resolution, we use 8\u00d7 bicubic downscale and add Gaussian noise with a standard deviation of 0.05. Note that the downscaled images are first 8\u00d7 bicubic upscaled (back to 256x256) before feeding them into the model. For image"}, {"title": "5.2. Results", "content": ""}, {"title": "5.2.1. BLIND FACE RESTORATION (BFR)", "content": "We compare our method with the following baseline models: CodeFormer (Zhou et al., 2022), GFPGAN (Wang et al., 2021a), VQFRv2 (Gu et al., 2022), Difface (Yue & Loy, 2024), DiffBIR (Lin et al., 2023), ResShift (Yue et al., 2024) and PMRF (Ohayon et al., 2024). In Table 1 we present a comparative evaluation showing that ELIR is competitive with state-of-the-art methods for blind face restoration. Our method achieves a notably high PSNR without compromising FID, indicating its ability to balance perception and distortion. Moreover, ELIR has the smallest number of parameters compared to all other methods. In terms of latency, ELIR is faster by 4-270\u00d7 compared to diffusion & flow-based methods. Note that GAN-based methods have shown similar latency to ELIR but with large degradations in FID and PSNR. In addition, Figure 3 presents visual results of ELIR compared to baseline methods. ELIR demonstrates\ncompetitive performance while having the smallest model size and fast inference time, which makes it ideally suited for deployment on resource-constrained edge devices. Additional results can be found in the Appendix 7.3."}, {"title": "5.2.2. SUPER RESOLUTION, IMAGE DENOISING, INPAINTING, COLORIZATION", "content": "Table 2 compares our method and PMRF (Ohayon et al., 2024) across various image restoration tasks. Our method achieves competitive performance with PMRF in terms of perceptual quality metrics, while exhibiting a slight performance gap in distortion metrics. In colorization, we observe a performance gap in FID, which we attribute to the crucial role of global context in this specific task. Nevertheless, our method demonstrates a 4.6\u00d7 reduction in model size and a 45x speedup compared to PMRF, facilitating efficient deployment on edge devices. Visual results are shown in Appendix 7.3."}, {"title": "5.3. Ablation", "content": "Here, we evaluate ELIR's performance through an ablation study, which examines the contribution of its components. Additional ablations can be found in the Appendix 7.4."}, {"title": "ELIR steps.", "content": "To evaluate ELIR 's performance in latent space, Table 3 presents PSNR and FID values at each processing step. As expected, the highest PSNR is achieved after Latent MMSE, confirming its effectiveness. Subsequently, PSNR gradually decreases while FID improves, reflecting the expected distortion-perception trade-off. Figure 11 in the Appendix, illustrates the restoration process, visualizing the process from LQ images to visually appealing results."}, {"title": "Effectiveness of trainable encoder.", "content": "This ablation study demonstrates the importance of fine-tuning the encoder. Given that the encoder was initially trained on HQ images, it\nstruggles to represent the LQ images encountered in various tasks. This limitation is evident in Table 4, where fixed encoders exhibit significantly lower performance, with PSNR values 1.5-2 dB lower and FID scores 2-3 points higher compared to trainable encoders. Additional metrics can be found in the Appendix (Table 8)."}, {"title": "Efficieny of Latent CFM.", "content": "Figure 4 compares the performance of FM and CFM in latent space by plotting PSNR and FID for varying NFEs. Both methods exhibit a similar trend: PSNR decreases while FID improves with increasing NFE, reflecting the expected distortion-perception trade-off. While FM requires 25 NFEs to reach a comparable FID, CFM achieves the same FID with only 5 NFEs, highlighting CFM's superior efficiency."}, {"title": "Model Size.", "content": "Figure 5 presents different model sizes for super-resolution. We vary the vector field size while keeping the latent MMSE constant. Our results indicate a diminishing return in FID improvement beyond 27M parameters. For additional metrics see Table 7 in the Appendix."}, {"title": "6. Conclusions", "content": "This study introduces Efficient Latent Image Restoration (ELIR), an efficient IR method that operates within the latent space. ELIR consists of two stages: latent MMSE estimator, whose goal is to estimate the latent presentation of a clean image, followed by latent consistency flow matching (LCFM). The LCFM is a combination of latent flow matching and consistency flow matching that enables a small number of NFEs and a reduction of the evaluation cost. In addition, we propose an efficient neural network to significantly reduce computational complexity and model size. We have evaluated ELIR on several IR tasks and shown state-of-the-art performance in terms of model efficiency. In terms of distortion and perceptual quality, we have shown competitive results with state-of-the-art methods. This makes ELIR an efficient image restoration method that can be deployed on resource-constrained edge devices while maintaining competitive performance with state-of-the-art methods."}, {"title": "7. Appendix", "content": "Table of Contents:\n\u2022 Neural Network Architecture: A description of the Neural Network Architecture can be found in Appendix 7.1\n\u2022 Hyperparameters: Details on the hyperparameters used in the experiments are provided in Appendix 7.2.\n\u2022 Additional Results: Further results can be found in Appendix 7.3.\n\u2022 Additional Ablations: Additional ablations are shown in Appendix 7.4"}, {"title": "7.1. Neural Network Architecture", "content": "To achieve a lightweight and efficient model, we utilize Tiny AutoEncoder (von Platen et al., 2022), a pre-trained tiny CNN version of Stable Diffusion 3 VAE (Esser et al., 2024). Tiny AutoEncoder allows us to lower the image dimensions into CHW = (16,64, 64) with only 1.2M parameters for each encoder and decoder. Given the memory and latency constraints, we restrict our architecture to convolutional layers only, eschewing transformers' global attention mechanisms. Linear operations such as convolution can be modeled as matrix multiplication with a little overhead. As a result, these operations are highly optimized on most hardware accelerators to avoid quadratic computing complexity. Although Windows attention techniques (Liang et al., 2021; Crowson et al., 2024) can be theoretically implemented with linear time complexity, practical implementation often involves data manipulation operations, including reshaping and indexing, which remain crucial considerations for efficient implementation on resource-constrained devices. Alternatively, in our method, we use only convolutional layers. During training, we utilize collapsible linear blocks (Bhardwaj et al., 2022) by adding 1 \u00d7 1 convolution after each 3 \u00d7 3 convolution layer and expanding the hidden channel width by 4\u00d7. These two linear operations are then collapsed to a single 3 \u00d7 3 convolution layer before inference"}, {"title": "7.1.1. LATENT MMSE", "content": "Our Latent MMSE consists of 3 cascaded RRDB blocks (Wang et al., 2018) with 96 channels each. We replace the Leaky ReLU activation of the original RRDB with SiLU. The cascade is implemented with a skip connection."}, {"title": "7.1.2. U-NET", "content": "For implementing the vector field we use U-Net (Ronneberger et al., 2015). U-Net is an architecture with special skip connections. These skip connections help transfer lower-level information from shallow to deeper layers. Since the shallower layers often contain low-level information, these skip connections help improve the result of image restoration. Our U-Net consists of convolution layers only. It has 3 levels with channel widths of (128, 256, 512) and depths of (1, 2, 4). We add a first and last convolution to align the channels of the latent tensor shape. Our basic convolution layer has 3 \u00d7 3 kernel and all activation functions are chosen to be SiLU."}, {"title": "7.2. Hyper-parameters", "content": ""}, {"title": "7.3. Additional Results", "content": "In-The-Wild Dataset. We test our method for blind face restoration on the in-the-wild datasets: LFW-Test (Huang et al., 2007), WebPhoto-Test (Wang et al., 2021b) and CelebAdult (Wang et al., 2021b). Here, we report only non-reference perception metrics: FID, NIQE, and MUSIQ. ELIR achieve competitive results with state-of-the-art baseline methods."}, {"title": "7.4. Additional Ablations", "content": ""}, {"title": "Model Latency.", "content": "Table 9 presents ablation study for model latency. Here, we vary the multi-segment value K while maintaining a fixed model size. Our findings suggest that K = 3 provides a suitable trade-off between FID and frames per second (FPS). This ablation was conducted on the CelebA-Test dataset for the super-resolution but similar results were found in other tasks. Therefore, we set K = 3 for super-resolution, denoising, inpainting, and colorization."}, {"title": "Latent MMSE loss space.", "content": "This ablation study investigates the impact of the l2 loss space on model performance. We compared the original latent-space loss from (6) with an alternative pixel-space loss:\nTable 10 demonstrates a trade-off between PSNR and perceptual quality. While pixel-space losses generally achieve higher PSNR values, they often result in lower perceptual quality, leading to visually less appealing restored images."}, {"title": "Latent MMSE vs SwinIR.", "content": "This ablation study analyzes our Latent MMSE approach by comparing it to both pixel and latent estimators of SwinIR (Liang et al., 2021). Maintaining a constant model size, Table 11 presents the comparison results. In the latent space, our method exhibits performance comparable to SwinIR. While SwinIR in pixel space achieves higher PSNR values, our approach demonstrates a better FID score. Notably, our convolution-based Latent MMSE shows a much higher frame per second (FPS). These evaluations were conducted on the CelebA-Test dataset for blind face restoration. Note that the latent SwinIR implementation utilizes a window size of 4 \u00d7 4."}, {"title": "Noise Level ablation.", "content": "This ablation study investigates the impact of noise level $\u03c3_s$ on model performance. Additive noise is vital for learning the complex dynamics of image degradation, enabling the generation of high-quality images. However, careful tuning of $\u03c3_s$ is essential; excessive noise can lead to distortion, while insufficient noise may degrade perceptual quality. Table 12 presents the results for various $\u03c3_s$ values. Based on these results, $\u03c3_s$ = 0.1 appears to offer the best balance between minimizing distortion and maintaining high perceptual quality. Therefore, we set $\u03c3_s$ = 0.1 in our experiments. This ablation was conducted on the CelebA-Test dataset for the super-resolution but similar results were found in other tasks."}, {"title": "Time Interval ablation.", "content": "In this ablation study, we investigate the influence of the time interval (\u0394t) on model performance. Table 13 presents the results obtained for several \u0394t values. Reducing \u0394t is expected to enhance FID scores, however, it may also lead to an increase in distortion metrics. This study aims to identify the \u0394t value that minimizes distortion while maintaining a high level of perceptual quality. According to the results, \u0394t = 0.05 offers the most favorable balance between minimizing distortion and preserving perceptual quality. This ablation was conducted on the CelebA-Test dataset for the image denoising but similar results were found in other tasks."}, {"title": "CFM trajectories.", "content": "Latent CFM improves the flow straightness by enforcing consistency within the velocity field, which reduces discretization errors. Figure 10 illustrates the \"straitness\" of the trajectories in the latent space. However, when these trajectories are projected back to the pixel space, this property is not preserved due to the decoder's non-linearity."}]}