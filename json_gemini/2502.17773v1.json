{"title": "Uncertainty Quantification for LLM-Based Survey Simulations", "authors": ["Chengpiao Huang", "Yuhang Wu", "Kaizheng Wang"], "abstract": "We investigate the reliable use of simulated survey responses from large language models (LLMs) through the lens of uncertainty quantification. Our approach converts synthetic data into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in mimicking human behaviors. Recent studies have leveraged LLMs to simulate human responses in various domains, including economic and social science experiments (Aher et al., 2023; Horton, 2023; Chen et al., 2023; Bisbee et al., 2024; Huang et al., 2024; Yang et al., 2024; Ziems et al., 2024), market research (Brand et al., 2023; Gui and Toubia, 2023; Goli and Singh, 2024; Wang et al., 2024), education (Zelikman et al., 2023; Lu and Wang, 2024), and so on. Compared to traditional survey methods that recruit and query real people, LLM simulations offer significant advantages in terms of time and cost efficiency, enabling the generation of large-scale synthetic responses with minimal effort. However, a growing body of evidence suggests that LLMs are not perfectly aligned with the human population, and in some cases, the misalignment can be substantial (Aher et al., 2023; Santurkar et al., 2023). This raises critical concerns about the reliability of insights derived from LLM-generated data. It remains a challenge how to properly simulate human responses using LLMs and how to account for their imperfections when using the simulated samples to make inference about the true human population.\nWe propose to address this challenge through the lens of uncertainty quantification. Specifically, we seek to construct confidence sets for population statistics of human responses based on LLM-generated data. A central question in this process is:"}, {"title": "2 Warm-up: Simulation of Binary Responses", "content": "To motivate our problem and methodology, we will start with a simple setting where an LLM simulates binary responses to a survey question. In Section 3, we will present the general problem setup and the general methodology."}, {"title": "2.1 Motivating Example: Educational Test", "content": "Suppose a school wants to estimate the proportion \u03bc\u2208 [0, 1] of students that can answer a newly designed test question correctly. It will not only provide insights into student progress but also evaluate the question's effectiveness in differentiating among students with varying levels of under-standing. Such information can guide the school in tailoring teaching strategies to better address student needs.\nThe most direct approach is to give the test to n students and collect their results Y1,..., Yn \u2208 {0,1}, where yi indicates whether student i answers the question correctly. A point estimate for \u03bc is the sample mean \\$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}\\$. Given a \u2208 (0,1), we can can construct a confidence interval for \u03bc:\n```latex\n\\Tilde{T}^{syn}(k) =  [\\bar{y} - \\frac{c . s^{syn}}{\\sqrt{k}}\\Phi^{-1}(1-\\frac{\\alpha}{2}) ,  \\bar{y} + \\frac{c . s^{syn}}{\\sqrt{k}}\\Phi^{-1}(1-\\frac{\\alpha}{2}) ]\n```\n(2.1)\nwhere s = \u221a\u1ef9(1 \u2013 \u1ef3) is the sample standard deviation, and I is the cumulative distribution function (CDF) of N(0,1). By the Central Limit Theorem (CLT), this interval has asymptotic coverage probability 1-aas n \u2192 \u221e. As a different approach, one can also use Hoeffding's concentration inequality (e.g., Theorem 2.8 in Boucheron et al. (2013)) to construct a finite-sample confidence interval\n```latex\n[\\bar{y} - \\sqrt{\\frac{log(2/\\alpha)}{2n}}, \\bar{y} + \\sqrt{\\frac{log(2/\\alpha)}{2n}}]\n```\n(2.2),\nwhich has at least (1 \u2013 a) coverage probability for every n \u2208 Z+. For simplicity, we will stick to (2.1) in this section.\nAlternatively, the school may use an LLM to simulate students' responses to the question. Compared with directly testing on real students, this approach is more time-efficient and cost-saving. If we prompt the LLM k times with random student profiles, then it generates k synthetic responses, which leads to synthetic outcomes y\u2081y\", ..., yyn \u2208 {0,1}. We may also compute the sample mean yr\nsyn\n=ki=1 Yisyn syn\ny=kysyn/k1ki=1 Yi\nsyn\nsynsyn\nmean yr syn =1\nkysyn=ki=1 Yisyn syn=\nki=1 Yi\nsynsyn/\n```latex\nSyn (k) =  [\\bar{y} - \\frac{c . s^{syn}}{\\sqrt{k}}\\Phi^{-1}(1-\\frac{\\alpha}{2}) ,  \\bar{y} + \\frac{c . s^{syn}}{\\sqrt{k}}\\Phi^{-1}(1-\\frac{\\alpha}{2}) ]\n```\nsyn\n```latex\n, (2.3)\n```\nwhere s = \u221a(1 ), and c > 1 is a scaling parameter. Such a dilation by c is necessary;\nsyn syn\nysyn k\nsyn k\nysyn k\nwithout it, whenever the LLM-generated data deviates from the student population (even by the slightest amount), the interval Tsyn(k) may never achieve (1 \u2013 a) coverage regardless of k. We give an example in Appendix B.1.\nDue to the misalignment between the LLM and students, the distribution of the synthetic data {ysyn}1 may be very different from the true response distribution. In this case, the sample mean\ni=1\""}, {"title": "2.2 Methodology for Selecting the Simulation Sample Size", "content": "We now introduce our method for choosing a good simulation sample size k. It makes use of similar test questions for which real students' results are available. If such data is available, we can compare LLM simulations with real students' results on these questions, and use it to guide the choice of k.\nSpecifically, we assume access to m test questions similar to the question of interest. For example, they can come from previous tests or a question bank. For j \u2208 [m], the j-th test question has been tested on nj real students, with test results Dj = {Yji}i=1. We also simulate LLM responses\n\nsyn K K\nDay = {y}1 to the j-th test question, and Dyn = {y}, to the new test question. Here\njJi=1j Ji=1j Z+ is the simulation budget.\nFor each question j \u2208 [m], we form confidence intervals similar to (2.3) using the synthetic data Dsynj\nsyn, aiming to cover the true proportion \u00b5; of students that can answer the j-th question correctly:\n```latex\n(k) =  [\\bar{y} - \\frac{c . s^{syn}}{\\sqrt{k}}\\Phi^{-1}(1-\\frac{\\alpha}{2}) ,  \\bar{y} + \\frac{c . s^{syn}}{\\sqrt{k}}\\Phi^{-1}(1-\\frac{\\alpha}{2}) ]\n```\n(2.4)\nwhere ysyn =1 \u03a3ki=1 Yj,i 1isyn yyn is the sample mean of the first k samples in Dy, and s = \u221a(1 ) is the estimated standard deviation. We also set the convention Tyn(0) =R, as nothing can\n```latex\nj,k syn j j,k syn yyn j k kkj,iij,k j,j,k k\n```\nbe said about the true parameter without data. We will pick k \u2208 {0,1..., K} such that Tsyn (k) covers\nj\n\u00b5; with high probability. We expect this choice of k to be also good for Tsyn (k), as the test questions are similar.\nIdeally, we would like to pick k such that (1 \u2013 a)-coverage is achieved empirically over the m"}, {"title": "2.3 Theoretical Analysis", "content": "In this section, we present a theoretical analysis of our proposed method. To do so, we first describe the setup in Section 2.1 and Section 2.2 in mathematical terms.\nThe student population can be represented by a distribution Pover a space Z of possible student profiles, say, vectors of background information, classes taken, grades, etc. To simulate student responses from the LLM, synthetic student profiles are generated from a synthetic student population Psyn over Z, and then fed to the LLM.\nWe use & and {j}1 to refer to the test question of interest and the m similar ones, respectively.\nStudents' performance on test questions are characterized by a performance function F: a student with profile z \u2208 Z answers a question 4 correctly with probability F(z,\u03c8) \u2208 [0,1]. The average\nm\nj=1\nstudent performance on the test questions & and {j}1 are then \u00b5 = Ez~pF(z,\u03c8) and \u00b5j =\nmj=1 Ez~pF(z, j), respectively. In addition, the LLM generates synthetic student performance from a\nsynthetic performance function Fsyn: when prompted with a synthetic profile zsyn \u2208 Z, the LLM answers a question & correctly with probability Fsyn (zsyn, \u03c8) \u2208 [0,1].\nThe collection of the real dataset Dj = {yj,i}1 can be thought of as drawing nj i.i.d. student profiles {zj,i}1 ~ P and then sampling yj,i ~ Bernoulli(F(zj,i, \u03c8j)) for each i \u2208 [nj]. Similarly, the generation of the synthetic dataset Dynj\nnj ji Si=1can be thought of as drawing i.i.d. synthetic\nsynKsyn synK\nprofiles {zji}~Psyn and then sampling yji~ Bernoulli (Fsyn (zsyn, \u03c8j)) for each i \u2208 [K]. ForK KjiSi=1ji Si=1,\nDsyn = {ysyn}1, we adopt a similar notation {zs}. We note that"}, {"title": "3 General Setup and Methodology", "content": "In this section, we study the more general setting where survey responses and confidence sets can be multi-dimensional."}, {"title": "3.1 Problem Formulation", "content": "Let Z be a profile space, Pa probability distribution over Z which represents the true population, and Psyn a synthetic distribution over Z used to generate synthetic profiles.\nLet be a collection of survey questions, and Y be the space of possible responses to the survey questions. When a person with profile z \u2208 Z is asked a survey question \u03c8 \u2208 \u03a8, the person gives a response y following a distribution Q( \u00b7 | z, 4) over Y. We are interested in the distribution of the\n\npopulation's response to the survey question 4, which is given by R( \u00b7 | 4) = \u222bz Q( \u00b7 | z, \u03c8) P(dz). In particular, we seek to construct a confidence set for some statistic \u03b8(\u03c8) of R( \u00b7 | \u03c8), which can be multi-dimensional, say in Rd. Below we revisit the educational test example in Section 2 in this framework. More examples are provided in Appendix A."}, {"title": "3.2 General Methodology for Sample Size Selection", "content": "We now present our general methodology for Problem 1. For each j \u2208 [m], we form confidence sets similar to (3.1) using the synthetic data Dayn:\nj\nSyn (k) = C C({}=1), \u2200k \u2208 [K].\nsynkji Si=1(3.2)\nWe also set Syn (0) = Rd. We will pick k\u2208 {0,1,..., K} such that Syn (k) is a good confidence interval for (;) for each j \u2208 [m]. This choice of k will also be good for Syn (k), thanks to the i.i.d. assumption on the survey questions.\nIdeally, we would like to pick k such that (1 \u2013 a) coverage is achieved empirically over the m survey functions:\n```latex\n\\frac{1}{m}  \\sum_{j=1}^m  1{\\theta(\\psi_{j}) \\notin S_{j}^{syn}(k)} \\leq \\alpha.\n```\n(3.3)"}, {"title": "4 Numerical Experiments", "content": "In this section, we apply our general method in Section 3 to LLMs over real datasets. The code and data are available at https://github.com/yw3453/uq-llm-survey-simulation."}, {"title": "4.1 Experiment Setup", "content": "LLMs. We consider 8 LLMs: GPT-3.5-Turbo (gpt-3.5-turbo), GTP-40 (gpt-40), and GPT-40-\nmini (gpt-40-mini) (OpenAI, 2022, 2024b,a); Claude 3.5 Haiku (claude-3-5-haiku-20241022)\nAnthropic (2024); Llama 3.1 8B (Llama-3-8B-Instruct-Turbo) and Llama 3.3 70B (Llama-3.3-70B-Instruct-Turbo) (Dubey et al., 2024); Mistral 7B (Mistral-7B-Instruct-v0.3) (Jiang et al., 2023); DeepSeek-V3 (DeepSeek-V3) (Liu et al., 2024).\nDatasets. We use two datasets for survey questions, each corresponding to one uncertainty quan-tification task. The first dataset is the OpinionQA dataset created by Santurkar et al. (2023). It was built from Pew Research's American Trends Panel\u00b9, and contains the general US popula-tion's responses to survey questions spanning topics such as science, politics, and health. After pre-processing we have 385 unique questions and 1,476,868 responses to these questions from at least 32,864 people. These questions have 5 choices corresponding to ordered sentiments which we map to sentiment scores -1,-1,0,3,1. Each question has at least 400 responses. For each re-sponse, we have information on their political profile, religious affiliation, educational background, socio-economic status, etc. This information is used as their profiles to generate synthetic profiles. See Appendix C.1 for more information on this dataset, including example questions, profile fea-tures, and the generation of synthetic profiles and answers. We consider the task of constructing a confidence interval for the US population's average sentiment score for a survey question. This is the setup in Example A.3.\nThe second dataset is the EEDI dataset created by He-Yueya et al. (2024), which was built upon the NeurIPS 2020 Education Challenge dataset (Wang et al., 2021). It consists of students' responses to mathematics multiple-choice questions on the Eedi online educational platform\u00b2. The dataset contains 573 unique questions and 443,433 responses to these questions from 2,287 students. All questions have four choices (A, B, C, D). Out of these questions, we use questions that have at least 100 student responses. Excluding questions with graphs or diagrams, we are left with a total of 412 questions. For each student, we have information on their gender, age, and socioeconomic status. This information is used as their profiles to generate synthetic profiles. See Appendix C.2 for more information on this dataset, including example questions, profile distribution, and the generation of synthetic profiles and answers. We consider the task of constructing a confidence interval for the probability of a student answering a question correctly. This is similar to the setup in Section 2 and Example 3.1."}, {"title": "4.3 Experiment Results", "content": "For both datasets, we evaluate three metrics as a varies: the miscoverage probability proxy (4.3), the selected simulation sample size k, and the half-width of the synthetic confidence interval Ssyn (k). We consider 100 random train-test splits of the questions. For compactness, we present results on the miscoverage probability proxy and k, and defer the results on the half-width of the synthetic confidence interval as well as more experiment details to Appendix D. We omit Llama 3.1 8B for the EEDI dataset experiment because it frequently failed to answer EEDI questions in required formats. As a baseline, we also include a na\u00efve response generator (random) that chooses an available answer uniformly at random.\nIn Figure 3, we present histograms of p-values for the hypothesis test E[L(k)] \u2264 a against E[L(k)] > a across various LLMs and a's over the OpinionQA and EEDI datasets. The p-values are computed using a one-sided z-test over the 100 random splits. As can be seen from the histograms, all p-values are reasonably large, indicating that the hypothesis E[L(k)] \u2264 a cannot be rejected (e.g., at the 0.05 significance level) for any LLM and a across both datasets. These experiment results verify the theoretical guarantees in Section 3, showing that the miscoverage rate is effectively controlled by our method."}, {"title": "5 Discussions", "content": "We developed a general approach for converting imperfect LLM-based survey simulations into sta-tistically valid confidence sets for population statistics of human responses. It identifies a simulation sample size which is useful for future simulation tasks and which reveals the degree of misalignment between the LLM and the target human population.\nSeveral future directions are worth exploring. First, our approach does not explicitly minimize the size of the prediction set. A natural question is whether we can incorporate a size minimization procedure to produce smaller confidence sets with good coverage. Second, it would be interesting to see if our approach can be combined with debiasing methods to give more informative confidence sets. Finally, as prompt engineering is known to have crucial effects on the quality of LLM generations, it is worth investigating the impacts of prompts on the selected simulation sample size k, and how prompt engineering can be leveraged to reduce the misalignment gap between LLM simulations and true human responses."}, {"title": "A More Examples for Section 3", "content": "In this section, we provide more examples for the general problem framework in Section 3. In these examples, the survey responses can be real-valued or multi-dimensional."}, {"title": "Example A.1 (Market research)", "content": "Suppose a company is interested in learning its customers' willingness-to-pay (WTP) for a new product, which is the highest price a customer is willing to pay for the product. Then, each z \u2208 Z can represent a customer profile (e.g., age, gender, occu-pation), each survey question \u03c8 is about a certain product, and a customer's response y is a noisy observation of the customer's WTP. Then R( \u00b7|\u03c8) is the distribution of the customer population's WTP. We may take 0(\u03c8) as the \u315c-quantile of the WTP distribution R( \u00b7 | \u03c8), for some \u0442\u0454 (0,1):\n```latex\n\\theta(\\psi) = inf \\{q \\in [0,\\infty) : P_{y\\sim R(\\cdot | \\psi)} (y \\leq q) \\geq \\tau\\} .\n```\nAn LLM can be used to simulate customers' WTP for the product."}, {"title": "Example A.2 (Public survey, multi-dimensional)", "content": "Suppose an organization is interested in per-forming a public survey in a city. Each survey question \u03c8 is a multiple-choice question with 5 options. An example is \u201cHow often do you talk to your neighbors?\u201d, with 5 choices \u201cBasically every day"}, {"title": "Example A.3 (Public survey, one-dimensional)", "content": "Consider the setup in Example A.2. When the 5 choices in a survey question correspond to ordered sentiments, we can map them to numeric scores, say, v = (-1,-1,0,1,1). Then the statistic (\u03c8) = (v,0(\u03c8)) reflects the population's average sentiment in the survey question \u03c8."}, {"title": "B Proofs", "content": ""}, {"title": "B.1 Failure of Exact CLT-Based Intervals under Distribution Shift", "content": "Consider a true distribution N(\u03bc, 1) and a synthetic distribution N(\u00b5syn, 1). Suppose we draw k i.i.d. synthetic samples {x}=1 ~ N(\u00b5syn, 1) and construct the standard (1 \u2013 a) confidence interval for \u03bc:\n```latex\nI^{syn}(k) = [\\bar{x}_k - \\Phi^{-1}(1 - \\alpha/2) /\\sqrt{k}, \\bar{x}_k + \\Phi^{-1}(1 - \\alpha/2) /\\sqrt{k}],\n```\nwhere Ik \u03a31 = -1 xi. Let \u0394 = \u03bcsyn, which represents the discrepancy between the true distribution and the synthetic distribution. Then\n```latex\nP(\\mu \\in I^{syn}(k)) = P(|\\bar{x}_k - \\mu| \\leq \\Phi^{-2}(1 - \\alpha/2) /\\sqrt{k})\n```\n```latex\n= P ( |\\sqrt{k}(\\bar{x}_k - \\mu^{syn}) - \\sqrt{k}(\\mu - \\mu^{syn}) | \\leq \\Phi^{-1}(1 - \\alpha/2))\n```\n```latex\n= P (\\sqrt{k}\\Delta - \\Phi^{-1}(1 - \\alpha/2) \\leq \\sqrt{k}(\\bar{x}_k - \\mu^{syn}) \\leq \\sqrt{k}\\Delta + \\Phi^{-1}(1 - \\alpha/2)) .\n```"}, {"title": "C Details of Numerical Experiments", "content": ""}, {"title": "C.1 The OpinionQA Dataset", "content": "Selection of survey questions. The original dataset is categorized into topics such as health, crime/security, and political issues. Ideally, we would want to consider questions from the same"}]}