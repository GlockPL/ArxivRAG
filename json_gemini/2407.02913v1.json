{"title": "SFC: Achieve Accurate Fast Convolution under Low-precision Arithmetic", "authors": ["Liulu He", "Yufei Zhao", "Rui Gao", "Yuan Du", "Li Du"], "abstract": "Fast convolution algorithms, including Winograd and FFT, can efficiently accelerate convolution operations in deep models. However, these algorithms depend on high-precision arithmetic to maintain inference accuracy, which conflicts with the model quantization. To resolve this conflict and further improve the efficiency of quantized convolution, we proposes SFC, a new algebra transform for fast convolution by extending the Discrete Fourier Transform (DFT) with symbolic computing, in which only additions are required to perform the transformation at specific transform points, avoiding the calculation of irrational number and reducing the requirement for precision. Additionally, we enhance convolution efficiency by introducing correction terms to convert invalid circular convolution outputs of the Fourier method into effective ones. The numerical error analysis is presented for the first time in this type of work and proves that our algorithms can provide a 3.68\u00d7 multiplication reduction for 3x3 convolution, while the Winograd algorithm only achieves a 2.25\u00d7 reduction with similarly low numerical errors. Experiments carried out on benchmarks and FPGA show that our new algorithms can further improve the computation efficiency of quantized models while maintaining accuracy, surpassing both the quantization-alone method and existing works on fast convolution quantization.", "sections": [{"title": "1. Introduction", "content": "Convolution operations are a crucial component of many deep learning models. Due to their intensive computation requirements, optimizing convolution calculations is key to\nimproving model deployment efficiency. Fast Convolution\nAlgorithms and Quantization are two distinct approaches to\nmitigate the computational burden. Fast convolution algo-\nrithms can reduce the arithmetic complexity by multiplying\ninputs and kernel weights in the transformation domain.\nA 3\u00d73 convolution performed with Winograd F(2\u00d72, 3\u00d73)\nalgorithm consumes just 2.25 multiplications compared to\ndirect computing. Whereas, model quantization reduces\nthe cost of a single arithmetic operation and data trans-\nmission by converting high-precision floating numbers to\nlow-precision integers. An int8 multiply-and-accumulate\noperation consumes only $\\frac{1}{16}$ of the energy compared to a\nfp32 operation.\nHowever, when attempting to combine existing fast con-\nvolution algorithms and model quantization to further im-\nprove computational efficiency, the model accuracy could\nbe severely degraded. This is due to the significant increase\nin numerical error when the two methods are used together.\nFor example, Winograd, a well-known fast convolution al-\ngorithm for small filter sizes (Lavin & Gray, 2016), has\nill-conditioned transformation matrices (with high condi-\ntion numbers), necessitating the use of high-precision arith-\nmetic to avoid numerical issues (Barabasz et al., 2020).\nAnother renowned fast algorithm for convolution is the\nFast Fourier transform (FFT). While its transformation is\nwell-conditioned, its irrational coefficients can introduce\nsignificant rounding errors, particularly under low-precision\nrepresentation. Number Theoretic Transform (NTT) can\nachieve precise computation for integer convolution, but\nit involves module operations and the transformations for\ninputs and filters would extend the bit-width to be equivalent\nto that of the outputs.\nAt present, two research approaches have been developed\nto tackle the aforementioned problem. One approach in-\nvolves customizing the quantization method specifically\noptimized for fast convolution algorithms (Li et al., 2021;\nChikin & Kryzhanovskiy, 2022; Andri et al., 2022; Tianqi\net al., 2023). However, this approach struggles to main-\ntain satisfactory accuracy under int8 quantization for faster\nalgorithms such as Winograd F(4\u00d74, 3\u00d73). The other ap-\nproach is to explore new fast algorithms that are better fit for\nquantization (Liu & Mattina, 2020; Alam et al., 2022). Nev-\nertheless, these emerging algorithms encounter challenges\nin achieving low arithmetic complexity. These facts show\nthat simultaneously achieving low arithmetic complexity\nand low-precision quantization while maintaining model\naccuracy is a persistent challenge, which, to our knowledge,\nno existing work has overcome.\nThis paper aims to develop a new efficient fast convolution\nalgorithm with high numerical accuracy, which is compat-\nible with model quantization techniques. We note that all\nthe complex coefficients in the Fourier Transform have con-\nstant modulus 1, so the quantization error can be reduced\nby bypassing the direct calculation on these coefficients. To\nachieve this, we introduce symbolic computing to avoid the\ninvolvement of irrational numbers, where the transforma-\ntion process contains only additions. This advance adapts\nthe Fourier transform to low-precision arithmetic. We also\ndiscovered that conventional Fourier convolution does not\nfully utilize its circular convolution outputs, which signifi-\ncantly affects its efficiency, so we add correction terms to\nconvert these neglected outputs into valid results. Moreover,\nwe investigate the error generation mechanism of convolu-\ntion algorithms and compare the numerical errors of direct\nconvolution, Winograd, and our algorithms. The key contri-\nbutions can be summarized as follows:\n1.  We formulate an efficient and quantization-friendly fast\nconvolution algorithm extended from Fourier Convo-\nlution. We employ symbolic computing to perform\nDiscret Fourier Transformation by just additions, and\nintroduce correction terms to fully utilize its circular\nconvolution outputs and further enhance its efficiency.\n2.  We analyse the numerical error of direct convolution\nand other fast convolution algorithms. Our method can\nachieve 3.68x arithmetic reduction while the Winogard\nalgorithm only achieves 2.25\u00d7 at equivalent numerical\naccuracy. Through error analysis and observation of\nthe energy distribution in the frequency domain, we\nalso present a frequency-wise quantization strategy to\nimprove model accuracy at low-bitwidth.\n3.  Experiments on the ImageNet dataset validate that our\nmethod can achieve less than 0.2% accuracy degrada-\ntion with int8 post-training quantization. In the same\nmodel accuracy, our method achieves up to 2.5x bit-\noperations reduction compared to Winograd convolu-\ntion or direct convolution under quantization. FPGA\nimplementation shows that our algorithms can signifi-\ncantly improve model inference throughput combined\nwith low-precision arithmetic."}, {"title": "2. Related Work", "content": "Fast Convolution Algorithms. The FFT was the first\nutilized algorithm (Mathieu et al., 2014) to fast the train-\ning of convolutions. Subsequently, for small convolutions,\nthe Winograd minimum filtering algorithm (Lavin & Gray,\n2016) was found to outperform the Fourier-based method\ndue to its real field arithmetic operations, whereas the\nFourier method requires more inefficient arithmetic in com-\nplex field. Additionally, the NTT has also been proposed to\naccelerate convolutions (Hong et al., 2022). However, when\ncombining quantization and fast convolution algorithms, the\nchallenge of potential model accuracy degradation arises.\nThe Winograd algorithm is susceptible to numerical insta-\nbility due to the ill-conditioned Vandermonde matrix in the\ntransformation (Vincent et al., 2017; Barabasz et al., 2020).\nFourier-based methods demand a high-precision format to\naccurately represent irrational numbers. NTT methods can\noffer accurate integer computing, but involve a large number\nof modulo operations and high-bitwidth intermediate data\nrepresentations, reducing computation efficiency.\nQuantization for Fastconv. Some approaches aim to opti-\nmize the quantization method to regain model accuracy.\nFor example, LoWino (Li et al., 2021) presents a post-\ntraining quantization (PTQ) method for Winograd, opti-\nmizing the scaling factor by minimizing the KL distance\nbetween the quantized and original vectors. Another PTQ\nwork (Chikin & Kryzhanovskiy, 2022) introduces a bal-\nancing operation between the filter and input channels to\nenhance bit-width utilization and improve the quality of\nquantization for Winograd. Additionally, a full quantiza-\ntion method with optimizing the transformation matrix in\nWinograd has been proposed (Tianqi et al., 2023), which\nsuccessfully restores model accuracy when employing the\nWinograd F(4\u00d74, 3\u00d73) algorithm with int8 quantization.\nNevertheless, the methods above tend to struggle to achieve\nsatisfactory accuracy recovery under sub-int8 quantization.\nNumerical Accuracy for Fastconv. Another approaches\nfocus on improving the intrinsic properties of the fast algo-\nrithm itself. As Winograd algorithms can be defined by root\npoints, a bilinear approach that strikes a balance between\ncomputational complexity and numerical accuracy has been\nproposed (Barabasz & Gregg, 2019). Additionally, two ex-\nisting works (Barabasz et al., 2020; Alam et al., 2022) aimed\nto discover more effective polynomial root points to improve\nnumerical accuracy. The Winograd algorithms have also\nbeen extended to the Residue Number System (RNS) (Liu &\nMattina, 2020), decomposing single high-precision interme-\ndiate multiplications into multiple low-precision arithmetics\n(e.g., 8-bit). However, these all come at the cost of increased\ncomputational complexity."}, {"title": "3. Preliminaries", "content": "Algorithms Construction. Fast convolution algorithms,\nincluding Winograd, Fourier Transform, and Number Theo-\nretic Transform all employ a three-stage computing process:\ntransformations of filters and inputs, element-wise multi-\nplication, and a transformation for generating outputs. The\ngeneralized form for fast 2D convolution is as follows:\n$\\begin{equation}\ny = A^{T}[[GfG^{T}] \\odot [B^{T}xB]]A\n\\end{equation}$\nwhere $\\odot$ denotes element-wise multiplication, and B, G,\nand A represent the linear transformations of the input, filter,\nand multiplication result.\nFor one specific algorithm (whether Winograd, FFT or\nNTT), the G, B and A are all derive from a Vandermonde\nmatrix V, which consist of a set of root points $s_0..s_{N-1}$:\n$\\begin{equation}\nV = \n\\begin{bmatrix}\n1 & 1 & 1 & ... & 1 \\\\\ns_0 & s_1 & s_2 & ... & s_{N-1}\\\\\ns_0^2 & s_1^2 & s_2^2 & ... & s_{N-1}^2\\\\\n... & ... & ... & ... & ...\\\\\ns_0^{N-1} & s_1^{N-1} & s_2^{N-1} & ... & s_{N-1}^{N-1}\n\\end{bmatrix}\n\\end{equation}$\nAN\u00d7N matrix V and its inverse V-1 can construct a\nfast convolution algorithm for R \u00d7 R filter accommodating\nN \u00d7 N inputs with M \u00d7 M outputs, or M \u00d7 M inputs with\nN \u00d7 N outputs, where N = M + R - 1.\nDifference. The fundamental difference among various\nalgorithms lies in the number field of V and the chosen\n$s_n$. In the Winograd algorithm (also known as Toom-Cook\nalgorithm), the $s_n$ are N interpolation points selected in\nthe real number field R. Similarly, all arithmetic operations\nare performed in the R. As a comparison, all arithmetic in\nFourier convolution is defined in the complex field C. And\nthe matrix V is the discrete Fourier transform matrix, where\n$s_n = e^{-j\\frac{2\\pi n}{N}}$. The number theoretic transformation is\nsimilar in structure to the Fourier transform, but it operates\nin a finite field denoted as $F_p$.\nArithmetic Complexity Reduction. Assuming these trans-\nformations are lightweight compared to element-wise mul-\ntiply and can be amortized due to channel reuse, the fast\nalgorithms consume $N^2 = (M + R \u2013 1)^2$ multiplications\nto generate $M^2$ outputs, where the arithmetic complexity\nreduction is $\\frac{(M+R-1)^2}{M^2R^2}$. However, convolution operations\nin CNNs are generally defined in R, so employing fast al-\ngorithms defined in C or $F_p$ (such as FFT and NTT) would\nlead to waste in the calculation. Hence, Winograd, defined\nin R, is the most popular algorithm for model acceleration.\nPrecision Requirement. For Winograd, the extremum of a\nrow in V is [1, $s_{N-1}$]. So the required arithmetic precision\ngrows exponentially with N. Thus only the Winograd al-\ngorithm with small N is practical. In comparison, the FFT\nmethod performs a significant numerical advantage when\ndealing with large filters due to its numerically stable V\nmatrix. However, performing accurate Fourier transforms\nnecessitates high-precision arithmetic. NTT method provide\na bit-correct result for integral convolution. However, when\nusing NTT to perform a calculation with N-bit inputs and"}, {"title": "4. Symbolic Fourier Convolution Algorithm", "content": "It is worth noting that the Fourier transform has better nu-\nmerical stability, as all its root points are distributed on\na circle of radius 1 in the complex plane. When dealing\nwith larger N, it is more accurate than Winograd. However,\nFourier transform has two serious drawbacks. First, its irra-\ntional coefficients are not friendly for low-precision format\nand would give more computation burden for transformation\ncalculation.\nIn addition, the FFT is not as efficient as Winograd. There\nare two reasons for this. Firstly the FFT is computed using\ncomplex numbers and even after utilizing the Hamiltonian\nsymmetry with real sequences and the fast complex multi-\nplication, each complex multiplication still requires 1.5 real\nmultiplications. Secondly, the direct calculation result of\nthe FFT is a circular convolution, so padding the sequence\nwith zeros to achieve a linear convolution is required, which\nwould wasted computation.\nWe propose two key improvement strategies to address these\ndrawbacks:\n1.  We employ symbolic computation rather than numer-\nical computation to implement the discrete Fourier\ntransform (DFT). By selecting an appropriate number\nof DFT points, we can avoid or minimize the irrational\nvalues involved in computing. All complex points can\nbe represented by first order integer coefficient polyno-\nmials under both 4 and 6 DFT points.\n2.  We introduce correction terms to fully exploit the cyclic\nconvolution output generated by the Fourier method to\nenhance computing utilization, and the smaller number\nof transformation points we chose also helps to reduce\nthe proportion of complex arithmetic.\n4.1. Symbolic Computing for DFT\nGenerally, the coefficients of the N-point DFT are derived\nfrom:\n$\\begin{equation}\nei = cos(\\frac{2 \\pi n}{N}) + jsin( \\frac{2 \\pi n}{N}), n = 0, 1, .., N \u2013 1\n\\end{equation}$\nwhen $\\frac{2 \\pi n}{N} \\in \\{0, \\frac{1}{6}, \\frac{1}{4}, \\frac{1}{3}, \\frac{1}{2} \\}$, irrational values will be introduced.\nTo eliminate the rounding errors that arise from these irra-\ntional values, we employ symbolic computation rather than"}, {"title": "4.2. Achieving Efficient Linear Convolution", "content": "The conventional Fourier Transform method inherently\ngenerates cyclic convolution. As a consequence, only\n$(N-R + 1)^2$ components are valid for the intended $N^2$\nlinear convolution. However, it's noteworthy that the invalid\nresults are not entirely useless. They actually contain partial\nsums that can be effectively utilized. By intelligently apply-\ning correction terms to these partial sums, it is possible to\nconvert them into valid outputs. This approach significantly\nenhances the efficiency of the convolution process.\nFigure 2 illustrates an example of Fourier-based cyclic\nconvolution for N = 6 and R = 3. The first term $o_1^\\prime$\nis equal to $a_6w_1 + a_1w_2 + a_2w_1$, but the desired output\nis $o_1 = a_0w_1 + a_1w_2 + a_2w_1$. To align $o_1$ with $o_1^\\prime$, a\ncorrective term is introduced to obtain the desired output:\n$o_1 = o_1^\\prime + (a_0 - a_6)w_1$. This adjustment allows us to obtain\nan additional correct result by adding just one MAC oper-\nation, thus utilizing the Fourier convolution output more\nefficiently compared to discarding erroneous terms.\nTo unambiguously represent a particular algorithm, we em-\nploy the notation SFC-N(M, R), where N represents the\nlength of the SFT transformation, M represents the feature"}, {"title": "5. Error Analysis and Frequency-wise Quantization", "content": "This section would analyze the numerical error of fast con-\nvolution algorithms, which can be used to guide the devel-\nopment of quantization methods and serve as a benchmark\nfor assessing the numerical stability across various fast al-\ngorithms.\nTo cover the direct convolution into the same error analysis\nmodel, we can consider it as a fast convolution with R = 3,\nM = 1. For derivation convenience, we use the overlapped"}, {"title": "6. Experimental Evaluation", "content": "We conducted experiments on image classification tasks to\ndemonstrate the effectiveness of our algorithms. To compre-\nhensively evaluate the computation cost of models acceler-\nated by fast convolution algorithm and quantization, it is crit-\nical to consider both the reduction in arithmetic complexity\nafforded by fast convolution and the reduction in arithmetic\ndata width introduced by quantization. Consequently, we\nadopt bit-operations (BOPs) as a metric of computation cost,\ndiverging from the traditional floating-operations (FLOPs).\nIn this metric, an n-bit addition operation requires n BOPs,\nwhereas an n-bit multiplication costs n(n-1) BOPs. This is\nbecause an n-bit multiplication can be decomposed into n-1\ninstances of n-bit additions. The transformation cost of fast\nalgorithms is also taken into account.\n6.1. Post-training Quantization on Image Classification\nBenchmarks\nExperiments were conducted on the ImageNet dataset,\nwhich contains 1.4 million images of size 224x224\u00d73, dis-\ntributed across 1,000 classes. We randomly selected 500\nimages from training set to create the calibration set for\nPTQ fine-tuning. Model accuracy was evaluated on the\nvalidation set. We utilized pre-trained fp32 models from\nTorch Vision as our benchmarks. All batch normalization\nlayers were fused into the preceding convolution layers prior\nto quantization.\nWe conducted quantization on the following algorithms: 1)\nDirect convolution, 2) The well-known Winograd F(4\u00d74,\n3\u00d73) algorithms, which have been extensively researched\nfor their quantization methods in recent years, and 3) our\nproposed SFC algorithms, including 1D and 2D format.\nFor all these methods, all 3\u00d73 convolution layers with a\nstride of 1 were replaced by the corresponding algorithm.\nDirect convolution and our SFC algorithms were quantized\nusing AdaQuant (Hubara et al., 2020), while the Winograd\nalgorithm was processed with Scaling Gradient Backward\n(Jain et al., 2020), due to observed convergence differences\nwith AdaQuant in the Winograd F(4\u00d74, 3\u00d73). All the data"}, {"title": "6.2. FPGA Simulation", "content": "We develop RTL code for the convolution accelerator based\non the SFC-6(7\u00d77,3\u00d73) algorithm. The resource consump-\ntion and timing report are synthesized using Xilinx Vi-\nvado tools. The parallelism of our design is configured\nat [4\u00d74\u00d77\u00d77], indicating that one convolution operator with\n4 input channels, 4 output channels, and 7\u00d77 feature map\nare processed simultaneously. The VGG-16 model is taken\nas an example, whose convolution layers all have 3\u00d73 filters\nwith stride=1, making it well-suited for fast convolution.\nAll components in our datapath are quantized to int8, and\nall computing stages in fast convolution are designed to\noperate in a full pipeline architecture. The DSP48 hardcore\nis a crucial resource on FPGA, as it can efficiently deploy\nmultiply operations. One DSP48 unit can implement two\nint8 multipliers or one int16 multipliers, which means that\nour implementation consumes only 1056 DSPs (calculated\nas 4\u00d74\u00d7132\u00d70.5). In comparison, Winograd-based or NTT-\nbased accelerators (Liang et al., 2020; Prasetiyo et al., 2023)\nrequire more DSPs for high precision multipliers, and accel-\nerators for direct convolution (Huang et al., 2022) need more\nDSPs due to a higher complexity of multiplications. The re-"}, {"title": "6.3. Ablation Study on Quantization Granularity", "content": "In Section 5, we theoretically predict that frequency-wise\nquantization will produce less error than tensor-wise quanti-\nzation. Here, we provide an ablation experiment on Resnet-\n18 by enumerating combinations of different quantization\ngranularity in Table 4 and Table 5. The results underscore\nthat the Winograd algorithm exhibits more sensitivity to\nquantization granularity, requiring the finest granularity\neven with int8 quantization. In contrast, the SFC main-\ntains acceptable accuracy under int8 quantization without\nspecialized quantization. However, at lower bitwidths, it is\nstill necessary to employ frequency-wise quantization for\nactivation tensors."}, {"title": "7. Conclusion", "content": "We propose a novel fast convolution algorithm extended\nfrom the Fourier transform, which solves the accuracy prob-\nlem of the conventional fast convolution algorithm applied\nto quantized models. According to experiment results, our\nalgorithms outperform state-of-the-art fast convolution quan-\ntization works on both model accuracy and computation cost\nreduction. Our algorithms share exactly the same compu-\ntational flow as the Winograd algorithm, which means that\nthey can be deployed on general-purpose processors (CPUs,\nGPUs) and hardware accelerator design conveniently by\nfollowing the existing works."}, {"title": "B. Applying the SFC Algorithm to Large-size Convolution Kernels", "content": "Large-kernel convolutional neural networks have recently received extensive research attention in recent years, and the\nkernel sizes ranging from 7\u00d77 to 51\u00d751. The vanilla Fast Fourier Transform (FFT) is an option available. Since large-size\nconvolutions commonly use depth-wise convolutions, this makes the multiplication complexity of the algorithm $n^2logn$, i.e.,\nthe computational complexity of the FFT itself will become dominant.\nAs a comparison, our algorithm is accomplished using only addition in the transform stage, so applying our algorithm to\nlarge-size convolutional kernels is promising. However, our algorithm itself is not applicable to scaling large numbers of\ntransformation points, which can lead to the appearance of higher-order terms further increasing the real multiplication\ntimes. We therefore consider an iterative convolution approach to the operation.\nConsidering a convolution with a 29\u00d729 kernel size and a 26\u00d726 feature map size, we will describe the computational\nprocess of iterative convolution. We split the feature map into 5\u00d75 tiles with 6\u00d76 size, while splite the kernel into 6\u00d76\ntiles with 5\u00d75 size. Convolution operation will be performed between each feature map tile and each kernel tile, so we use\nSFC(6x6, 5x5) to speed it up. Note that at this time the partial convolution results of the each tiles are still summed up in\nthe same pattern as the convolution window sliding, which allows us to use the SFC(5\u00d75, 6\u00d76) algorithm to speed up the\nprocess as the feature map has been split into 5\u00d75 tiles, and the kernel has been split into 6\u00d76 tiles in the first iteration. The\nnumber of multiplications required by above two iterations is the product of the multiplications in the two SFC algorithms,\ni.e., 132 x 132 = 17,424 multiplications. Our approach reduces the number of multiplications to just 3% of what is required\nby direct convolution.\nWe can increass the number of iterations to computing convolution with more larger sizes. Since the SFC algorithm\nuses only addition for the transformation, it has an easier, efficient and flexible deployment compared to the FFT method.\nHowever, when the convolution size is large enough to require 3 or more iterations, the FFT method will appear to be more\nadvantageous in terms of theoretical computational efficiency."}]}