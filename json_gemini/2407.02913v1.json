{"title": "SFC: Achieve Accurate Fast Convolution under Low-precision Arithmetic", "authors": ["Liulu He", "Yufei Zhao", "Rui Gao", "Yuan Du", "Li Du"], "abstract": "Fast convolution algorithms, including Winograd and FFT, can efficiently accelerate convolution operations in deep models. However, these algorithms depend on high-precision arithmetic to maintain inference accuracy, which conflicts with the model quantization. To resolve this conflict and further improve the efficiency of quantized convolution, we proposes SFC, a new algebra transform for fast convolution by extending the Discrete Fourier Transform (DFT) with symbolic computing, in which only additions are required to perform the transformation at specific transform points, avoiding the calculation of irrational number and reducing the requirement for precision. Additionally, we enhance convolution efficiency by introducing correction terms to convert invalid circular convolution outputs of the Fourier method into effective ones. The numerical error analysis is presented for the first time in this type of work and proves that our algorithms can provide a 3.68\u00d7 multiplication reduction for 3x3 convolution, while the Winograd algorithm only achieves a 2.25\u00d7 reduction with similarly low numerical errors. Experiments carried out on benchmarks and FPGA show that our new algorithms can further improve the computation efficiency of quantized models while maintaining accuracy, surpassing both the quantization-alone method and existing works on fast convolution quantization.", "sections": [{"title": "1. Introduction", "content": "Convolution operations are a crucial component of many deep learning models. Due to their intensive computation requirements, optimizing convolution calculations is key to improving model deployment efficiency. Fast Convolution Algorithms and Quantization are two distinct approaches to mitigate the computational burden. Fast convolution algorithms can reduce the arithmetic complexity by multiplying inputs and kernel weights in the transformation domain. A 3\u00d73 convolution performed with Winograd F(2\u00d72, 3\u00d73) algorithm consumes just 2.25 multiplications compared to direct computing. Whereas, model quantization reduces the cost of a single arithmetic operation and data transmission by converting high-precision floating numbers to low-precision integers. An int8 multiply-and-accumulate operation consumes only $\\frac{1}{16}$ of the energy compared to a fp32 operation.\nHowever, when attempting to combine existing fast convolution algorithms and model quantization to further improve computational efficiency, the model accuracy could be severely degraded. This is due to the significant increase in numerical error when the two methods are used together. For example, Winograd, a well-known fast convolution algorithm for small filter sizes (Lavin & Gray, 2016), has ill-conditioned transformation matrices (with high condition numbers), necessitating the use of high-precision arithmetic to avoid numerical issues (Barabasz et al., 2020). Another renowned fast algorithm for convolution is the Fast Fourier transform (FFT). While its transformation is well-conditioned, its irrational coefficients can introduce significant rounding errors, particularly under low-precision representation. Number Theoretic Transform (NTT) can achieve precise computation for integer convolution, but it involves module operations and the transformations for inputs and filters would extend the bit-width to be equivalent to that of the outputs.\nAt present, two research approaches have been developed to tackle the aforementioned problem. One approach involves customizing the quantization method specifically optimized for fast convolution algorithms (Li et al., 2021; Chikin & Kryzhanovskiy, 2022; Andri et al., 2022; Tianqi et al., 2023). However, this approach struggles to maintain satisfactory accuracy under int8 quantization for faster algorithms such as Winograd F(4\u00d74, 3\u00d73). The other approach is to explore new fast algorithms that are better fit for quantization (Liu & Mattina, 2020; Alam et al., 2022). Nevertheless, these emerging algorithms encounter challenges in achieving low arithmetic complexity. These facts show that simultaneously achieving low arithmetic complexity and low-precision quantization while maintaining model accuracy is a persistent challenge, which, to our knowledge, no existing work has overcome.\nThis paper aims to develop a new efficient fast convolution algorithm with high numerical accuracy, which is compatible with model quantization techniques. We note that all the complex coefficients in the Fourier Transform have constant modulus 1, so the quantization error can be reduced by bypassing the direct calculation on these coefficients. To achieve this, we introduce symbolic computing to avoid the involvement of irrational numbers, where the transformation process contains only additions. This advance adapts the Fourier transform to low-precision arithmetic. We also discovered that conventional Fourier convolution does not fully utilize its circular convolution outputs, which significantly affects its efficiency, so we add correction terms to convert these neglected outputs into valid results. Moreover, we investigate the error generation mechanism of convolution algorithms and compare the numerical errors of direct convolution, Winograd, and our algorithms. The key contributions can be summarized as follows:\n1. We formulate an efficient and quantization-friendly fast convolution algorithm extended from Fourier Convolution. We employ symbolic computing to perform Discret Fourier Transformation by just additions, and introduce correction terms to fully utilize its circular convolution outputs and further enhance its efficiency.\n2. We analyse the numerical error of direct convolution and other fast convolution algorithms. Our method can achieve 3.68x arithmetic reduction while the Winogard algorithm only achieves 2.25\u00d7 at equivalent numerical accuracy. Through error analysis and observation of the energy distribution in the frequency domain, we also present a frequency-wise quantization strategy to improve model accuracy at low-bitwidth.\n3. Experiments on the ImageNet dataset validate that our method can achieve less than 0.2% accuracy degradation with int8 post-training quantization. In the same model accuracy, our method achieves up to 2.5x bit-operations reduction compared to Winograd convolution or direct convolution under quantization. FPGA implementation shows that our algorithms can significantly improve model inference throughput combined with low-precision arithmetic."}, {"title": "2. Related Work", "content": "Fast Convolution Algorithms. The FFT was the first utilized algorithm (Mathieu et al., 2014) to fast the training of convolutions. Subsequently, for small convolutions, the Winograd minimum filtering algorithm (Lavin & Gray, 2016) was found to outperform the Fourier-based method due to its real field arithmetic operations, whereas the Fourier method requires more inefficient arithmetic in complex field. Additionally, the NTT has also been proposed to accelerate convolutions (Hong et al., 2022). However, when combining quantization and fast convolution algorithms, the challenge of potential model accuracy degradation arises. The Winograd algorithm is susceptible to numerical instability due to the ill-conditioned Vandermonde matrix in the transformation (Vincent et al., 2017; Barabasz et al., 2020). Fourier-based methods demand a high-precision format to accurately represent irrational numbers. NTT methods can offer accurate integer computing, but involve a large number of modulo operations and high-bitwidth intermediate data representations, reducing computation efficiency.\nQuantization for Fastconv. Some approaches aim to optimize the quantization method to regain model accuracy. For example, LoWino (Li et al., 2021) presents a post-training quantization (PTQ) method for Winograd, optimizing the scaling factor by minimizing the KL distance between the quantized and original vectors. Another PTQ work (Chikin & Kryzhanovskiy, 2022) introduces a balancing operation between the filter and input channels to enhance bit-width utilization and improve the quality of quantization for Winograd. Additionally, a full quantization method with optimizing the transformation matrix in Winograd has been proposed (Tianqi et al., 2023), which successfully restores model accuracy when employing the Winograd F(4\u00d74, 3\u00d73) algorithm with int8 quantization. Nevertheless, the methods above tend to struggle to achieve satisfactory accuracy recovery under sub-int8 quantization.\nNumerical Accuracy for Fastconv. Another approaches focus on improving the intrinsic properties of the fast algorithm itself. As Winograd algorithms can be defined by root points, a bilinear approach that strikes a balance between computational complexity and numerical accuracy has been proposed (Barabasz & Gregg, 2019). Additionally, two existing works (Barabasz et al., 2020; Alam et al., 2022) aimed to discover more effective polynomial root points to improve numerical accuracy. The Winograd algorithms have also been extended to the Residue Number System (RNS) (Liu & Mattina, 2020), decomposing single high-precision intermediate multiplications into multiple low-precision arithmetics (e.g., 8-bit). However, these all come at the cost of increased computational complexity."}, {"title": "3. Preliminaries", "content": "Algorithms Construction. Fast convolution algorithms, including Winograd, Fourier Transform, and Number Theoretic Transform all employ a three-stage computing process: transformations of filters and inputs, element-wise multiplication, and a transformation for generating outputs. The generalized form for fast 2D convolution is as follows:\n$y = A^T [[GfG^T] \\odot [B^T xB]]A$ (1)\nwhere $\\odot$ denotes element-wise multiplication, and B, G, and A represent the linear transformations of the input, filter, and multiplication result.\nFor one specific algorithm (whether Winograd, FFT or NTT), the G, B and A are all derive from a Vandermonde matrix V, which consist of a set of root points $s_0..s_{N-1}$:\n$V = \\begin{bmatrix}\n1 & 1 & 1 & .&.&.&1\\\\\ns_0 & s_1 & s_2 & .&.&.&s_{N-1}\\\\\ns_0^2 & s_1^2 & s_2^2 & .&.&.&s_{N-1}^2\\\\\n.& .&.&.&.&.&.\\\\\n.&.&.&.&.&.&.\\\\\ns_0^{N-1} & s_1^{N-1} & s_2^{N-1} & .&.&.&s_{N-1}^{N-1}\n\\end{bmatrix}$ (2)\nAN\u00d7N matrix V and its inverse $V^{-1}$ can construct a fast convolution algorithm for R \u00d7 R filter accommodating N \u00d7 N inputs with M \u00d7 M outputs, or M \u00d7 M inputs with N \u00d7 N outputs, where N = M + R - 1.\nDifference. The fundamental difference among various algorithms lies in the number field of V and the chosen $s_n$. In the Winograd algorithm (also known as Toom-Cook algorithm), the $s_n$ are N interpolation points selected in the real number field R. Similarly, all arithmetic operations are performed in the R. As a comparison, all arithmetic in Fourier convolution is defined in the complex field C. And the matrix V is the discrete Fourier transform matrix, where $s_n = e^{-j\\frac{2\\pi*n}{N}}$. The number theoretic transformation is similar in structure to the Fourier transform, but it operates in a finite field denoted as Fp.\nArithmetic Complexity Reduction. Assuming these transformations are lightweight compared to element-wise multiply and can be amortized due to channel reuse, the fast algorithms consume $\\frac{N^2}{M^2} = \\frac{(M + R \u2013 1)^2}{M^2}$ multiplications to generate M\u00b2 outputs, where the arithmetic complexity reduction is $\\frac{(M+R-1)^2}{M^2}$. However, convolution operations in CNNs are generally defined in R, so employing fast algorithms defined in C or Fp (such as FFT and NTT) would lead to waste in the calculation. Hence, Winograd, defined in R, is the most popular algorithm for model acceleration.\nPrecision Requirement. For Winograd, the extremum of a row in V is [1, $s_{N-1}^{N-1}$]. So the required arithmetic precision grows exponentially with N. Thus only the Winograd algorithm with small N is practical. In comparison, the FFT method performs a significant numerical advantage when dealing with large filters due to its numerically stable V matrix. However, performing accurate Fourier transforms necessitates high-precision arithmetic. NTT method provide a bit-correct result for integral convolution. However, when using NTT to perform a calculation with N-bit inputs and 2N-bit outputs, the transformed inputs must have a datawidth of at least 2N-bit, which increases the datawidth of \u0398.\nIn summary, no matter which type of algorithm is chosen, achieving both robust computation and significant arithmetic reduction under low-precision operations remains a challenge."}, {"title": "4. Symbolic Fourier Convolution Algorithm", "content": "It is worth noting that the Fourier transform has better numerical stability, as all its root points are distributed on a circle of radius 1 in the complex plane. When dealing with larger N, it is more accurate than Winograd. However, Fourier transform has two serious drawbacks. First, its irrational coefficients are not friendly for low-precision format and would give more computation burden for transformation calculation.\nIn addition, the FFT is not as efficient as Winograd. There are two reasons for this. Firstly the FFT is computed using complex numbers and even after utilizing the Hamiltonian symmetry with real sequences and the fast complex multiplication, each complex multiplication still requires 1.5 real multiplications. Secondly, the direct calculation result of the FFT is a circular convolution, so padding the sequence with zeros to achieve a linear convolution is required, which would wasted computation.\nWe propose two key improvement strategies to address these drawbacks:\n1. We employ symbolic computation rather than numerical computation to implement the discrete Fourier transform (DFT). By selecting an appropriate number of DFT points, we can avoid or minimize the irrational values involved in computing. All complex points can be represented by first order integer coefficient polynomials under both 4 and 6 DFT points.\n2. We introduce correction terms to fully exploit the cyclic convolution output generated by the Fourier method to enhance computing utilization, and the smaller number of transformation points we chose also helps to reduce the proportion of complex arithmetic.\n4.1. Symbolic Computing for DFT\nGenerally, the coefficients of the N-point DFT are derived from:\n$e^{j \\frac{2\\pi n}{N}} = cos(\\frac{2\\pi n}{N})+jsin(\\frac{2\\pi n}{N}), n = 0, 1, .., N \u2013 1$\nwhen $\\{\\frac{2\\pi}{N}, \\frac{4\\pi}{N}, \\dots, \\frac{N-1}{N} \\}$, irrational values will be introduced. To eliminate the rounding errors that arise from these irrational values, we employ symbolic computation rather than numerical calculation. This approach represents irrational values in polynomials with integer coefficients.\nTo illustrate, we consider the 3-point DFT. For a real input sequence x = $(x_0,x_1, x_2)^T$, its transform processing can be represented as:\n$\\begin{bmatrix}\nX_0\\\\\nX_1\\\\\nX_2\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & s & s^2 \\\\\n1 & s^2 & s\n\\end{bmatrix} \\begin{bmatrix}\nx_0\\\\\nx_1\\\\\nx_2\n\\end{bmatrix}, s= e^{j\\frac{2\\pi}{3}}$ (3)\nConsidering DFT-6, its transformation coefficients consist of six values: 1, $e^{j\\frac{2\\pi}{6}},e^{j\\frac{4\\pi}{6}}, -1,e^{-j\\frac{4\\pi}{6}},e^{-j\\frac{2\\pi}{6}}$. Defining s = $e^{j\\frac{2\\pi}{6}}$, thus $s^2 = s-1$, which allows all coefficients to be expressed as first-order polynomials of s: 1, s, s \u2013 1, -1, -s, 1 \u2013 s. When two first-degree polynomials are multiplied, any quadratic term can be reduced to a first-degree term using the rule $s^2$ = s \u2013 1. Therefore, the DFT-6 transform processing under symbolic computation is as follows:\n$DFT6(x) = S_6F_6x = \n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 1 & s & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & s & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1\\\\\n0 & 1-s & s & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1-s & s & 0\n\\end{bmatrix}\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & -1 & -1 & -1\\\\\n1 & -1 & -1 & 1 & 1 & -1\\\\\n1 & 0 & -1 & 0 & -1 & 1\\\\\n1 & -1 & 1 & -1 & 1\\\\\n1 & -1 & -1 & -1 & -1 & 1\n\\end{bmatrix}\\begin{bmatrix}\nx_0\\\\\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\nx_5\n\\end{bmatrix}$ (6)\nHere, $S_6$ represents the format transition from symbolic to numerical computing without any arithmetic, and $F_6$ signifies the Fourier transform under symbolic computing. We refer to the intermediate matrix as SFT-6 (Symbolic Fourier Transform-6), as its coefficients consist solely of 1, -1, and 0. The inverse transform iDFT6 can be rearranged in a similar way. $F_6$ has its fast algorithm by decomposing it into DFT-3 and DFT-2. Therefore, only 14 addition operations are needed to perform SFT-6 (subtraction can be considered as addition of complement). The inverse transformation matrix iF6 can be derived in a similar way:\n$iF_6 = \\frac{1}{6} \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1\\\\\n1 & -1 & -2 & -1 & 1 & 2\\\\\n1 & -1 & -2 & -1 & 1 & 2\\\\\n1 & -1 & 2 & -1 & -1 & 2\\\\\n1 & -2 & 1 & 1 & -2 & 1\\\\\n1 & 1 & -1 & 1 & -1 & 1\n\\end{bmatrix}$ (7)\nNote that $\\frac{1}{6}$ can be fused into floating model without having to compute that division operation during inference stage.\nIn the element-wise multiplication steps, multiplications are performed in polynomial form. Multiplying two 1st-order polynomials requires 4 real number multiplications. We can utilize a short fast convolution algorithm to reduce it to 3:\n$(a_0 + a_1 s)(w_0 + w_1 s) = o_0 + o_1 s, \\\n\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\\\\\n1 & -1\\\\\n-1 & 0\n\\end{bmatrix} \\begin{bmatrix}\na_0\\\\\na_1\\\\\nw_0\\\\\nw_1\n\\end{bmatrix} = \\begin{bmatrix}\no_0\\\\\no_1\n\\end{bmatrix}$ (8)\nThrough enumeration, we can identify that 6 and 4 are suitable choices for DFT points in small-size convolution applications.\n1) DFT-6"}, {"title": "2) DFT-4", "content": "Similarly, the DFT-4 can be constructed in the same manner.\n$DFT4(x) = S_4F_4x = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & j & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 1 & -j & 0\n\\end{bmatrix} \\frac{1}{4} \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & 0 & -1 & 0 \\\\\n1 & -1 & 1 & -1 \\\\\n1 & 0 & -1 & 0\n\\end{bmatrix} \\begin{bmatrix}\nx_0\\\\\nx_1\\\\\nx_2\\\\\nx_3\n\\end{bmatrix}$ (9)\nThe multiplication on the DFT-4 convolution can be performed as:\n$(a_0 + a_1 s)(w_0 + w_1 s) = o_0 + o_1 s, \\\n\\begin{bmatrix}\n1 & 1\\\\\n1 & -1\n\\end{bmatrix} \\begin{bmatrix}\na_0\\\\\na_1\\\\\nw_0\\\\\nw_1\n\\end{bmatrix} = \\begin{bmatrix}\no_0\\\\\no_1\n\\end{bmatrix}$ (10)\nIf we want to compute $A((Gf) \\odot (Bx))$ directly in the real number field, akin to the Winograd algorithm, without involving polynomial multiplication, we can integrate Eq. 8 or Eq. 10 into the SFT matrix Eq. 6 or Eq. 9. In the 1D case, this does not impact efficiency. However, in the 2D case, it introduces slight redundant components and marginally reduces the acceleration ratio. We list these algorithms with polynomial multiplication integrated in the appendix.\n4.2. Achieving Efficient Linear Convolution\nThe conventional Fourier Transform method inherently generates cyclic convolution. As a consequence, only $(N - R + 1)^2$ components are valid for the intended $N^2$ linear convolution. However, it's noteworthy that the invalid results are not entirely useless. They actually contain partial sums that can be effectively utilized. By intelligently applying correction terms to these partial sums, it is possible to convert them into valid outputs. This approach significantly enhances the efficiency of the convolution process.\nFigure 2 illustrates an example of Fourier-based cyclic convolution for N = 6 and R = 3. The first term $o_1'$ is equal to $a_0w_1 + a_1w_2 + a_2w_1$, but the desired output is $o_1 = a_0w_1 + a_1w_2 + a_2w_1$. To align $o_1'$ with $o_1^f$, a corrective term is introduced to obtain the desired output: $o_1 = o_1' + (a_0 - a_6)w_1$. This adjustment allows us to obtain an additional correct result by adding just one MAC operation, thus utilizing the Fourier convolution output more efficiently compared to discarding erroneous terms.\nTo unambiguously represent a particular algorithm, we employ the notation SFC-N(M, R), where N represents the length of the SFT transformation, M represents the feature tile size, and R represents the kernel size. For example, the SFC-6(6\u00d76, 3\u00d73) algorithm is constructed based on a 6-point Fourier transform, employing a 3\u00d73 kernel size and a 6x6 feature tile size.\nBy introducing correction terms, we can also adjust input tile size M independently. For example, as the images in the ImageNet dataset are in size 224x224, the feature map size in the model has a common factor of 7. Utilizing the SFC-6(7\u00d77, 3\u00d73) algorithm to infer models trained on Imagenet would have higher tiling efficiency without the need for zero padding. The transformation matrix integrated polynomial multiplication of the SFC-6(7\u00d77, 3\u00d73) is as follows:\n$B^T =\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1\\\\\n1 & 1 & 0 & -1 & -1 & -1 & 0\\\\\n1 & 0 & -1 & -1 & 0 & 1 & 1\\\\\n1 & -1 & -1 & 0 & 1 & 1 & 0\\\\\n1 & -1 & 0 & 1 & 1 & 0 & -1\\\\\n1 & 0 & 1 & 1 & 0 & -1 & -1\\\\\n1 & -1 & 1 & -1 & 1 & -1 & 1\n\\end{bmatrix} \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\\n0 & 1 & 1 & 1 & 1 & 0 & 0\\\\\\n0 & 1 & 1 & -2 & 1 & -2 & 1\\\\\\n0 & 1 & -2 & 1 & 1 & -2 & 1\\\\\\n0 & 1 & -2 & 1 & -2 & 1 & 1\\\\\\n0 & 1 & 1 & -2 & 1 & -2 & 1\\\\\\n0 & 1 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}$ (11)\n$\\newline A =\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1\\\\\\n1 & 1 & 1 & -2 & 1 & -2 & 1\\\\\\n1 & -2 & 1 & 1 & 1 & -2 & 1\\\\\\n1 & -2 & 1 & 1 & -2 & 1 & 1\\\\\\n1 & -2 & 1 & -2 & 1 & 1 & 1\\\\\\n1 & 1 & -2 & 1 & -2 & 1 & 1\\\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}$ (11)\nThe SFC-6(6\u00d76, 3\u00d73) algorithm can reduce 73% multiplications for 3\u00d73 convolutions. And multiplications can be optimized by 81% and 79% for 5\u00d75 and 7\u00d77 convolutions, respectively. A selection of achievable Symbolic Fourier Convolution algorithms is listed in Table 1. For comparison, we also list their efficiencies and the numerical errors obtained from simulation and theoretical analyses (Detailed in Section 5). In Table 1, we can see that for 3\u00d73 convolution the SFC-6(6\u00d76, 3\u00d73) algorithm is 1.64x faster than the Winograd (2\u00d72, 3\u00d73) algorithm, while maintaining nearly the same numerical error.\nFurther, for larger kernel size, the Winograd algorithm is no longer able to construct linear transformations with sufficiently low numerical errors, no matter how the root points are chosen. Although there exists work by splitting the core, making it possible to compute large size convolutions with the Winograd(2\u00d72, 3\u00d73) algorithm with low numerical error (Huang et al., 2020), it is not possible to achieve a lower arithmetic complexity than Winograd(2\u00d72, 3\u00d73). Whereas our algorithm provides higher speedups and keeps the high numerical stability."}, {"title": "5. Error Analysis and Frequency-wise Quantization", "content": "This section would analyze the numerical error of fast convolution algorithms, which can be used to guide the development of quantization methods and serve as a benchmark for assessing the numerical stability across various fast algorithms.\nTo cover the direct convolution into the same error analysis model, we can consider it as a fast convolution with R = 3, M = 1. For derivation convenience, we use the overlapped output form, in which A is a reversible square matrix.\n$\\begin{bmatrix}\nY_0\\\\\nY_1\\\\\nY_2\n\\end{bmatrix} =\\begin{bmatrix}\n1 & 1 & 1\\\\\n1 & 1 & -1\\\\\n1 & -1 & 1\n\\end{bmatrix} \\begin{bmatrix}\nf_0([1],[x_0])\\\\\nf_1([1],[x_0])\\\\\nf_2([1],[x_0])\n\\end{bmatrix}$ (12)\nWe denote the quantized element-wise multiply as $\\odot_Q$, through which the operands are quantized and multiplied. dy represents the calculation error of the output y. The error forward propagation process can be described as:\n$y + \\delta y = A^T \\cdot ((G \\cdot f) \\odot_Q (B^T \\cdot x))$ (13)\nWe assume these transformations to be accurate, and the quantized operator introduces rounding errors. These errors would be subsequently magnified by the matrix multiplying $A^T$.\nSet $s + \\delta s = (G \\cdot f) \\odot_Q (B^T \\cdot x)$, by substituting y = $A^T \\cdot s$ into the Equation 13, we can obtain:\n$\\delta y = (A^T)^{-1} \\cdot \\delta s$\nApplying the properties of the paradigm || * || yields:\n$||\\delta y|| <= ||(A^T)^{-1}|| \\cdot ||\\delta s||$ (15)\nSimilar, bringing in ||s|| <= ||$(A^T)^{-1}$||\u00b7 ||y||, we have:\n$\\frac{||\\delta y||}{||y||} <= ||(A^T)^{-1}|| \\cdot ||A^T|| \\cdot \\frac{||\\delta s||}{||s||}$ (16)\nHere we can perform an analysis of Eq.16. The first term ||$(A^T)^{-1}$||\u00b7 ||$A^T$|| is the condition number of matrix $A^T$, denoted as $\\kappa (A^T)$. This term indicates the amplification factor applied to the error $ds$. The condition number of the standard orthogonal matrix is constant 1, like A in direct convolution (Eq.1) and vanilla Fourier convolution, which would not amplify ds anymore. While the $\\kappa (A^T)$ for Winograd convolution can reach up to 31.0, as listed in Table 1. However, in our method, the $\\kappa (A^T)$ takes the values of 2.7, 3.3, and 3.5 for SFC4(4,3), SFC6(6,3), and SFC6(4,7), respectively, which is much less than that of Winograd.\nThe final term $\\frac{||\\delta s||}{||s||}$ in Eq.16 is the error caused by the quantized operator $\\odot_Q$, which depends on the data width, quantization method, and data distribution. For floating numbers, the error is fairly stationary as every operand has its own exponent code(scaling factor). We conduct numerical experiments under single precision (fp16) in Table 1, and it is find that the numerical error is highly correlated to the $\\kappa (A^T)$, in accordance with our analysis.\nFor model quantization, numbers in a group share the same scaling factor to achieve integer representation. The granularity of the scaling factor group should fit with the data distribution to achieve low quantization error. Some work has found that in Winograd convolution, scaling factor grouping based on the transformation domain coordinates can recover model accuracy effectively. Here we give a brief theoretical explanation. Assume that the input x has a fixed energy ||x|| = 1, the maximum possible value in the transformation domain is (1, 1, 1, 8, 8, 1) for F(4,3) algorithm, which would be quadratic in 2D convolution. Grouping one tensor to one scaling factor would cause a waste of 3/6 bit in 1D/2D. For Fourier convolution, each frequency would have the same maximum value. However, due to the frequency properties of the model inputs (image or audio), there will be a tendency for the energy to converge towards lower frequencies, as Figure 3 shows. Thus grouping scaling factors by frequency can also reduce the error of Fourier convolution, but not as necessary as Winograd. This is because the aforementioned tendency is only significant in the first few layers, and the magnitude variance is not as large as Winograd.\n$y = \\sum{}(ST [B^T \\cdot xB/STx]_{intINT} \\odot_{} OSTf G [GfG^T/STf]_{intNT})$ (17)\nThe scaling factor $STx$ is of size [T\u00d7T], where T is the size of the transform domain. For the scaling factor $STf$ of weights, considering that per-channel quantization can achieve better results in direct convolutions, we suggest combining per-frequency quantization and per-channel quantization whose $stf$ is of size [OC\u00d7T\u00d7T] to achieve higher accuracy."}, {"title": "6. Experimental Evaluation", "content": "We conducted experiments on image classification tasks to demonstrate the effectiveness of our algorithms. To comprehensively evaluate the computation cost of models accelerated by fast convolution algorithm and quantization", "algorithms": 1, "4\u00d74\u00d77\u00d77": "indicating that one convolution operator with 4 input channels, 4 output channels, and 7\u00d77 feature map are processed simultaneously. The VGG-16 model is taken as an example, whose convolution layers all have 3\u00d73 filters with stride=1, making it well-suited for fast convolution. All components in our datapath are quantized to int8, and all computing stages in fast convolution are designed to operate in a full pipeline architecture. The DSP48 hardcore is a crucial resource on FPGA, as it can efficiently deploy multiply operations."}]}