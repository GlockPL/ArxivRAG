{"title": "HYPERDIMENSIONAL VECTOR TSETLIN MACHINES WITH APPLICATIONS TO SEQUENCE LEARNING AND GENERATION", "authors": ["Christian D. Blakely"], "abstract": "We construct a two-layered model for learning and generating sequential data that is both computationally fast and competitive with vanilla Tsetlin machines, adding numerous advantages. Through the use of hyperdimensional vector computing (HVC) algebras and Tsetlin machine clause structures, we demonstrate that the combination of both inherits the generality of data encoding and decoding of HVC with the fast interpretable nature of Tsetlin machines to yield a powerful machine learning model. We apply the approach in two areas, namely in forecasting, generating new sequences, and classification. For the latter, we derive results for the entire UCR Time Series Archive and compare with the standard benchmarks to see how well the method competes in time series classification.", "sections": [{"title": "Introduction", "content": "A large part of any design of a data learning agent is in feature extraction of the underlying data, and how it is computed and represented. The best processes for extracting features for learning information from data typically take advantage of expert knowledge of the underlying data to either expose the most relevant features, reduced noise, and extract the most amount of independent information in the data. For many types of datasets, this might be challenging due to factors such as incoherence, abstractedness, or the sheer amount of noise present in the data. In designing features for Tsetlin machines, one is tasked to booleanize (or binarize) the underlying data, and under the presence of noise, this can be challenging. Furthermore, for notoriously complex high-dimensional data like noisy sequences, graphs, images, signal spectra, and natural language, creating encodings that are also interpretable for human reasoning in any post-hoc process can be difficult due to creating logic AND expressions that both take advantage of the relevant information in the data, but also lead to accurate expressions that can compete with other machine learning models.\nIn this paper, we explore using Hyperdimensional Vector Computing (HV computing, or simply HVC) as an input layer to a novel Tsetlin machine architecture and apply it to learning, classifying, predicting, and generating sequences. Here, we argue that HVC can provide a robust layer of feature extraction due to the many computational advantages. This approach was first introduced in [1] and here, we streamline the approach to focus on sequences while further leveraging other attributes of HCV such as N-Gram sequence encoding and associative memory, while combining with TMs, to create a powerful hybrid methodology while remaining minimalist in memory sizes of the overall model."}, {"title": "Hyperdimensional Vector Representations", "content": "There are many hyperdimensional vector (HV) representations that have been introduced for various applications over the past few decades. Combining with Tsetlin machines, the most natural approach is to use what is called in the literature as Binary Spatter Codes (BSC), or simply binary vectors of high dimension. These high-dimensional vectors, often referred to as HVs, are typically composed of thousands of bits. HVC is inspired by the way the human brain"}, {"title": "Motivation and contributions", "content": "One of the biggest attractions of HVC is the ability to represent virtually any type of data through the use of the vector operations. The main motivation in this paper is to build a strategy for learning and generating sequences by establishing a robust encoding from multidimensional sequences to HVs. We also desire that the encoding is then naturally suited for learning with TMs, and so we will provide empirical evidence of an attractive coupling of both HVC and TMs.\nThis paper contributes a new approach to sequence learning, where we show we can compete with current SOTA results on fundamental benchmarks. Furthermore, we also show how combining some of the features of HVC with TMs can produce a highly versatile forecasting model for many types of sequences, while generating new sequences of any length with are similar in Hamming distance to any given sequence. Lastly, we will show that the models have a very light footprint in memory, making it an attractive approach for online learning in small embedded systems."}, {"title": "Organization of Paper", "content": "We first give a brief overview of HVC along with the core algebraic framework we will be using throughout the paper. We then discuss the encoding strategy we will be using for various types of sequences, along with designing a so-called associative memory which will be very useful in sequence generation and forecasting.\nThe next section briefly outlines the TM architecture we will being using throughout, with references to more in-depth treatments of the approach.\nWe will present an in-depth numerical treatment of the approach by first outlining the hybrid structure of our HVC-TM architecture, and then providing an algorithm for time series learning and forecasting. In the final section of the paper, we will provide results for applying our proposed approach on the entire UCR Time Series archive for classifying many types of time series.\nFinally, we will conclude the study with some alternatives in design and further next steps with possible applications."}, {"title": "Review of Binary HVs", "content": "We will leverage heavily throughout our approach some fundamentals of HVs. A fantastic survey and introduction to HVC can be found in the recent monograph [2], which also gives insights into current research trends. To give a quick overview on why one would want to use HVs\n\u2022 Robustness: HVs are resilient to noise and errors. Small perturbations in the vectors do not significantly affect their overall similarity, making HVC robust to noisy data.\n\u2022 Scalability: HVC can easily handle high-dimensional data, and the operations on HVs are computationally efficient.\n\u2022 Simplicity: The operations on HVs, such as bundling, binding, and similarity measurement, are straightforward and can be implemented efficiently.\n\u2022 Flexibility: HVs can represent a wide range of data types and structures, from simple scalars to complex symbolic representations."}, {"title": "Bundling", "content": "Bundling is the operation of combining multiple HVs into a single HV. This operation is analogous to the concept of superposition in physics, where multiple states are combined. In HVC, bundling is typically implemented using element-wise majority voting and will be represented throughout the paper as a +.\nGiven binary HVs A, B, and C, the bundled HV S can be expressed as S = Majority(A, B, C) where for each bit position i:\n$S_i = \\begin{cases} 1 & \\text{if } A_i + B_i + C_i \\geq 2 \\\\ 0 & \\text{otherwise.} \\end{cases}$\nBecause of the fact we take a majority, it is important that we only apply this operation on an odd number of HVs."}, {"title": "Binding", "content": "Binding is the operation of combining two HVs to form a new HV that represents their association. In HVC, binding is typically implemented using the element-wise XOR operation. Given binary HVs A and B, the bound HV C is:\nC = A + B where $\\oplus$ denotes the XOR operation for each bit position i, $C_i = A_i \\oplus B_i$ The unbinding operation is used to retrieve an original HVs that was bound together with another, and is frequently used in decoding HVC representations. This is achieved by applying the XOR operation with one of the original HVs to the bound HV.\nGiven the bound HV C and one of the original HVs (say, A), we can retrieve the other HV (B) as follows: B = C + A This works because the XOR operation is its own inverse. Specifically, A + A = 0 (the zero vector) giving C + A = (A + B) + A = A + A + B = 0 + B = B\nThus, by XORing the bound HV C with one of the original HVs (A), we effectively cancel out A and retrieve the other original HV (B). For example, suppose we have two binary HVs A and B: A = [1,0,1,1], B = [0,1,0,1] Their bound HV C is C = A + B = [1 + 0, 0 \u2295 1, 1 \u2295 0, 1 \u2295 1] = [1, 1, 1, 0] To retrieve B, we unbind C with A: B = C + A = [1 + 1, 1 \u2295 0, 1 \u2295 1, 0 + 1] = [0,1,0,1] We will often compare HVs, and this is done via a similarity measure called the Hamming distance which is applied between two binary vectors A and B of equal length is defined as the number of positions at which the corresponding bits differ, namely $d_H(A,B) = \\sum_{i=1}^{n}(A_i \\oplus B_i)$. The Hamming distance is a measure of dissimilarity between two binary vectors, so a Hamming distance of 0 indicates that the vectors are identical, while a larger Hamming distance indicates greater dissimilarity.\nThe final operation useful for sequences is perturbation operator. It is the operation of slightly modifying a HV to represent a related but distinct vector. We implement this by simply shifting cyclically the bits of the vector j places. We will denote a perturbation applied to a vector hv by p(hv, j). The inverse operation of perturbation ip is then just p(hv, -j). This will be used for representing the jth position in a sequence. We will typically call the vector resulting in applying a j perturbation on a vector as a position vector."}, {"title": "Sequence operations", "content": "We can now apply these operations for representing and processing sequences of data. Several recent studies have been proposed for encoding sequences, such as in [3] and [4]. In a similar fashion, the goal of our approach will be to encode a time series sequence s \u2208 R\" into a binary vector dimension of D. In this paper, we will represent sequences (or time series) as a stationary sequence of scalars s = [81, 82, ..., sn]. We will show how we represent these as HVs, leveraging the operations of bundling, binding, and perturbation to capture the structure and relationships within the sequence.\nThe first step is to introduce what we call an Interval embedding. Given a quantization of dimension Q, since we assume our sequences are stationary, they are bounded and thus have a minimum and maximum interval, [mo, m1]. We divide the interval [mo, m1] into Q buckets, can represent each bucket by an HV (we will denote these as hv. Thus the process of mapping S = [81, 82, ..., Sn] into a collection of HVs Es = [hv1, hv2, . . ., hvn] will be represented as E. This is shown in the following algorithm."}, {"title": "Time Series Classification", "content": "In this section we do an in-depth study on how the HVTM performs in classifying many types of time series. For this study, we apply our proposed methodology on the UCR Time Series Classification archive [7] which is a widely used comprehensive collection of datasets for bench-marking time series classification algorithms. Boasting 128 different time series datasets from a broad range of domains, including medicine, biology, finance, motion tracking, and sensor data with a diverse range of time series lengths, some as short as a few dozen with very few training samples or having a high imbalance of a certain class, all while having strong benchmark tracking, it is the premier data archive for testing new time series classification models.\nHere we apply our methodology to all 128 data sets, and for this we fix the HV strategy, namely setting\n\u2022 HV dimension = 5000\n\u2022 N-Gram length = 3\n\u2022 Quantization = 50\nFor the TM architecture, we randomly choose 20 different configurations where\n\u2022 number clauses between [100, 2000]\n\u2022 number max literals per clause [10, 100]\n\u2022 specificity between [10f, 20f]\n\u2022 threshold between [0, 100].\nFor each random configuration, we run on 10 Epochs, and take the accuracy after the final Epoch. Finally, to measure the final performance, we then take the top 10 model configurations and report the maximum accuracy."}, {"title": "Numerical Results", "content": "In order to compare our approach with the published benchmarks of [7], for all 128 time series, we compare the accuracy of our classification with the best accuracy recorded in [7]. Their approach compares Euclidean distance, and two Dynamic Time Warping (DTW) computations (one with a learned/optimized parameter warping parameter, and one with a fixed parameter).\nTo visually compare, we plot the pair $(accuracy_{HVTM}, accuracy_{UCR})$.\nWithout having chance for optimization on controlling the dimension of the HV, nor the number of Grams used to encode the hypervector, and using a random parameterization for the TM, we can see that the approach has performed quite well simply using the \"out-of-the-box\" default settings. The HVTM method improves or competes in accuracy (cutoff at 2 percent) of the optimal benchmark provide by [7] in roughly 78 percent of the data sets.\nTo give an overview of the performance comparison, we employ a scatter plot showing the accuracy of the HVTM approach on each data set versus the DTW benchmark. In Figure 4, the scatter plot shows the clustering along x = y line, demonstrating that the approach is very competitive with the benchmark.\nTo get even more insights into the (out)performance, we break down further the results into four categories: data type, length of time series, number of training samples available, and number of classes in order to gain possible information on where HVTMs outperformance different training setups. Figure 5 shows the comparison of performance across the different data types and the length of the time series. The first two bars in each category represent the mean accuracy for both HVTM and the DTW benchmark. The third bar show the percentage of times HVTM outperformed the benchmark in terms of accuracy. We highlight (using a shadow on the bars) the categories in which HVTM outperformed DTW at least 60 percent of the time.\nWe can see that HVTM outperformed the DTW benchmark on most of the data set types, including Motion, Images, and ECGs. A few categories, including EOG, and Traffic, only two data sets fall into these category making performance comparisons difficult.\nNext we see that if we compare the performances broken down into time series length, there is clear dominance by HVTM over most lengths. However, it seemed to struggle with very short series, namely series of length 24-80. Furthermore, for mid-range series, 277-500 in length, there is no clear outperformance. However, for series greater than 500 in length there seems to be a clear dominance by the HVTM approach.\nWe also are interested in how the methods compare given the number of training samples. More training samples can yield more dispersion of information for each class, but here HVTM has no problem in outperforming DTW"}, {"title": "Hybrid Predictions", "content": "With the computational framework for both HVC and our TM architecture, we now show a straightforward approach to forecasting and generating new sequences based on encoded historical sequences. Prediction involves using the encoded HV of a sequence to predict the next element(s) in the sequence. Here we make a few assumptions on the underlying time series data"}, {"title": "Numerical Experiments", "content": "In order to empirically determine how well the forecasting and sequence generation performs, we begin by simulating deterministic sequences and comparing different length forecasts with the simulated sequence. For this, our model"}, {"title": "Conclusion", "content": "In this paper we proposed a straightforward approach to combining the strengths of HVC with the machine learning power of Tsetlin machines to give a flexible approach for learning sequences, classifying them, and generating new ones. Our approach relied heavily on encoding procedures for sequences where we took advantage of a powerful N-Gram structure for spatialtemporal learning. Since the resulting HVs can be decoded as well, combined with the innate interpretable structure of Tsetlin machines, this hybrid approach could be an attractive alternative to larger models such as Deep learning sequence models (LSTMs, RNNs) which rely heavily on large scale weight estimation via matrix vector multiplication, backpropagation, optimization and parameter hypertuning."}, {"title": "Model Size", "content": "This leads us to an additional note that we have not discussed in the paper but would be valuable for future work. Model size and computation learning in terms of energy expenditure could be optimized and improved greatly by taking advantage of the fact we are using binary vectors. The size of a model based on HVs depends on several factors, including the dimensionality of the vectors, the number of vectors used in the model (number of N-Grams and interval embedding vectors), but we can safely conclude that reasonably, with a dimension of 10k for or HV system, and with a thousand vectors, we have a size of 1,220KBs total (10,000bits/8=1,250bytes times 1000). Expanding to even larger dimensions, say D = 100k, we still only need a few MBs of chip memory. Now coupling with the TM, say 1000 clauses, each with max 32 bits for each automata, we are looking at a trivial amount of additional memory."}, {"title": "Future work", "content": "Many direction exist in terms of where these models could be challenged next. The first area of improvement would be in understanding better the underperformance of some of the classification results, including when many classes are present. Here, it is of the author's hypothesis that simply a higher dimension, along with a more careful pruning and selection of model criterion could be achieved. Perhaps even varying the number of N-Grams, to see if they play a large role for a larger number of classes. Since no optimization was done in choosing the TM parameters, this would also be an area to look at more. Since the goal was to see if the approach could work \"straight-out-of-the-box\", we wanted to have the least amount of tinkering as possible in model architecture setups."}]}