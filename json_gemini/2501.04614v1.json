{"title": "MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data Generation", "authors": ["Daniele Molino", "Francesco Di Feola", "Eliodoro Faiella", "Deborah Fazzini", "Domiziana Santucci", "Linlin Shen", "Valerio Guarrasi", "Paolo Soda"], "abstract": "Artificial Intelligence is revolutionizing medical practice, enhancing diagnostic accuracy and healthcare delivery. However, its adaptation in medical settings still faces significant challenges, related to data availability and privacy constraints. Synthetic data has emerged as a promising solution to mitigate these issues, addressing data scarcity while preserving privacy. Recently, Latent Diffusion Models have emerged as a powerful tool for generating high-quality synthetic data. Meanwhile, the integration of different modalities has gained interest, emphasizing the need of models capable of handle multimodal medical data. Existing approaches struggle to integrate complementary information and lack the ability to generate modalities simultaneously. To address this challenge, we present MedCoDi-M, a 6.77-billion-parameter model, designed for multimodal medical data generation, that, following Foundation Model paradigm, exploits contrastive learning and large quantity of data to build a shared latent space which capture the relationships between different data modalities. Further, we introduce the Multi-Prompt training technique, which significantly boosts MedCoDi-M's generation under different settings. We extensively validate MedCoDi-M: first we benchmark it against five competitors on the MIMIC-CXR dataset, a state-of-the-art dataset for Chest X-ray and radiological report generation. Secondly, we perform a Visual Turing Test with expert radiologists to assess the realism and clinical relevance of the generated data, ensuring alignment with real-world scenarios. Finally, we assess the utility of MedCoDi-M in addressing key challenges in the medical field, such as anonymization, data scarcity and imbalance learning. The results are promising, demonstrating the applicability of MedCoDi-M in medical contexts. Project page is at https://cosbidev.github.io/MedCoDi-M/.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) is increasingly revolutionizing several fields, including healthcare. Today, AI systems are capable of processing vast amounts of medical data, revealing patterns often undetectable to the human eye and enabling more accurate diagnostics, personalized treatments, and efficient healthcare delivery [1]. Moreover, the capability to leverage multimodal data represents a disruptive advancement in the medical field, enabling comprehensive diagnostic insights by integrating different data sources. However, despite these advancements, the implementation of AI in healthcare faces several challenges, primarly caused by data scarcity and privacy concerns [2]. The available datasets for training AI models are often limited in size, diversity, and scope, making training deep learning (DL) models a significant challenge, as these models typically require extensive, high-quality data to achieve great performances. Without enough diverse data, models may become biased, prone to overfitting, or unable to generalize well to new, unseen cases, creating a bottleneck in the deployment of AI solutions in real-world healthcare scenarios. Privacy regulations, such as the General Data Protection Regulation (GDPR) [3] in Europe and the Health Insurance Portability and Accountability Act (HIPAA) [4] in the United States, although crucial for protecting patient privacy, can hinder the collaborative efforts required to gather large-scale datasets. To address these limitations, a novel stream of research is focusing on multimodal synthetic data generation techniques. This emerging approach involves creating artificial data that replicate the complexity and diversity of real medical data, thus providing a solution to bypass the constraints of real-world data scarcity and privacy concerns."}, {"title": "1.1. Generative AI", "content": "Generative AI has seen remarkable growth since 2014, when the introduction of Generative Adversarial Networks (GANs) [5] had a groundbreaking influence on the research field, enabling the creation of realistic synthetic data through adversarial training. Despite their early success, GANs face inherent challenges, such as training instability, mode collapse, and difficulty in generating fine-grained details, issues that limit their effectiveness in the medical domain. Building on the foundation set by GANs, Diffusion Models (DM) [6] have recently emerged as a more robust approach for data generation. Through a multi-step denoising process, DMs demonstrate an improved capacity for generating diverse, high-fidelity data, capturing subtle variations and intricate details that are essential in medical imaging application, where both fidelity and diversity of synthetic data are crucial. Latent diffusion models (LDMs) [7] have gained significant attention: as the denoising process operates within a low-dimensional latent space [7], LDMs require reduced computational resources, making them more practical and accessible for deployment to a wider range of users and systems. Moreover, due to their advanced conditioning mechanism, LDM allow for fine-grained control over the generation process [8, 9]. The conditioning mechanism leverages encoders that extract meaningful representations from the input source, enabling the synthesis of targeted features, such as specific anatomical structures or disease characteristics. This controlled generation process not only enhances the model's flexibility but also its relevance in medical setting, as it allows practitioners to generate synthetic data tailored to unique diagnostic requirements or research needs. LDMs can be effectively adapted to a wide range of downstream tasks and, with the appropriate pre-training, have the potential to serve as robust foundation models [10]. However, most of these models can only generate one modality from another, which can be a significant limitation in the healthcare setting, where multiple modalities coexist and interact. Outside the medical domain, significant advancements have been made in multimodal data generation. Among these studies, CoDi [11] stands as pivotal work. By enabling the simultaneous generation of multiple modalities from a shared latent space, CoDi significantly improves the consistency and coherence of the generated outputs, allowing for any-to-any generation, avoiding the pitfalls of a multi-step approach. The adaptation of a similar approach for medical data generation could prove highly beneficial, filling a critical gap in the availability of diverse and high-quality datasets for research and diagnostic purposes. However, CoDi presents some limitations when applied to such a setting: while it demonstrates the feasibility of any-to-any generation in non-medical enviroment, its performance tends to degrade when provided with multiple input modalities or their combinations, a limitation that cannot be overlooked in the medical domain, where reliability and consistency across modalities are critical."}, {"title": "1.2. Related Works", "content": "In recent years, there has been a growing interest in developing generative models for X-rays generation, as they can give insight about a wide range of medical conditions. Numerous studies have assessed the task of synthetic Chest X-ray (CXR) generation through GANs, where most focused on a specific pathology, like tubercolosis [12], pneumonia [13] or Covid-19 [14, 15]. Recently, LDMs have emerged as a promising approach for CXR generation: RoentGen [16] was the first work to explore the adaptation of a pre-trained LDM, named Stable Diffusion [7], for text-conditioned generation of CXRS beyond few- or zero-shot setting [17, 18]. In such a work, they showed that fine-tuning the UNet component is necessary in order to effectively adapt the model to the medical domain, as it allows to capture the unique features and nuances of medical images, thereby improving the quality and realism of the generated CXRs.\nIn parallel with the rise of LDMs, there has been a growing interest in developing techniques to fuse textual and visual data, driving the development of the first vision-language models, capable of processing and combining both modalities [19, 20, 21, 22]. In the field of CXR generation, UniXGen [23] leverages the transformer [24, 25] architecture for both X-ray and report generation. They adopted a vector quantization technique, named VQ-GAN [26], to convert an X-ray in a set of discrete tokens, addressing both tasks as a sequence generation problem. Additionally, their work emphasizes the generation of different X-ray views, as each view contains distinct informative content, enhancing the utility of generated data. However, their approach is limited by its inability to generate multiple outputs simultaneously, requiring separate processing for each modality, without an explicit mechanism to guarantee coherence between the generated data. Building on this idea, Lee et Al. [27] proposed a similar approach for bidirectional X-ray and report generation via a fine-tuned large language model, named LLM-CXR. Unlike UniXGen, they only leveraged frontal chest X-rays, focusing on a single view for their generation tasks, potentially limiting its applicability in more comprehensive clinical scenarios.\nThe main limitation of these works is that they overlook the complementary nature of different medical data modalities and lack the ability to generate multimodal outputs simultaneously. This independent processing often results in inconsistencies when modalities are synthesized separately, potentially leading to outputs that lack clinical coherence. Such limitations hinder their applicability in real-world healthcare settings, where seamless integration of multimodal data is essential to replicate the complexity of patient-specific information accurately."}, {"title": "1.3. Contribution", "content": "Building on the success of CoDi, this work proposes MedCoDi-M, a novel multi-prompt foundation model for multimodal medical data generation. By taking advantage of contrastive learning techniques, used to build Foundation Models [28], MedCoDi-M enable flexible, any-to-any generation across different medical data modalities. Specifically, our generative process ensures that MedCoDi-M can capture the complex interactions between different medical modalities. To this end, we propose a novel training approach, named Multi-Prompt Training, to improve the model's ability to fuse information from multiple modalities, enhancing its capability to generate coherent and accurate medical data.\nThe main contribution can be summarized as:\n\u2022 We propose MedCoDi-M, a novel generative model that synthesizes multiple data modalities from a shared multimodal latent space.\n\u2022 We introduce a Multi-Prompt training approach that boosts MedCoDi-M's generation capabilities when prompted by multiple data modalities."}, {"title": "2. Methods", "content": "Assuming M is the set of our modalities, let $I = {I_1, I_2, ..., I_n}$ be any subset of modalities used to prompt the generation and let $O = {O_1, O_2, ..., O_m}$ be any subset of modalities to be generated, such that $O \\cap I = \\varnothing$, with $I,O \\subseteq M$. It is important to note that this distinction is made solely for expositional clarity; in practice, any modality from the set M can be used both as an input or as an output, and the model is not restricted to specific modality pairings.\nThe overall architecture of MedCoDi-M is depicted in Fig.1, which consists of three blocks, each corresponding to a distinct training phase. In panel (a), we align the feature representations extracted from the input modalities by modality-specific prompt encoders into a shared latent space using contrastive learning. In panel (b), we independently train an LDM for each output modality, using the multi-prompt training approach for conditioning. Finally, in panel (c), we perform cross-modal alignment, enabling the model to simultaneously generate any combination of output modalities. In the following, we provide a rigorous description of each training step in sections 2.1, 2.2 and 2.3."}, {"title": "2.1. Building a Shared Latent Space", "content": "We propose to align any input modalities within a shared latent space by leveraging contrastive learning. This approach allows the model to be freely conditioned on any input combination, even those absent in the training data. Inspired by [11], we take advantage of an efficient technique called Bridging Alignment to align the representations extracted by modality-specific prompt encoders. Following Fig 1.a, we first extract a feature representation $h_{I_i} = P_{I_i}(I_j)$ for every input modality $I_j \\in I$, where $P_{I_i}$ is the prompt encoder for modality $I_i$. The latent space is constructed through a series of pairwise training rounds, ensuring coherent alignment across all modalities while reducing the computational complexity. Once the encoders are trained,"}, {"title": "2.2. A Multi-prompt approach for single-modality generation", "content": "Training a multi-input, multi-output generative model requires extensive training across diverse data sources, while mantaining high generation quality across all synthesis flows. To address these challenges, MedCoDi-M is designed to be both composable and integrative, as it enables the independent development of modality-specific LDMs, which can then be seamlessly integrated into a unified framework. In the healthcare domain, information often flows concurrently across multiple modalities: to emulate this phenomenon, we developed a novel training approach, named Multi-Prompt Training. This technique enhances MedCoDi-M's conditioning capabilities, enabling it to be effectively conditioned on multiple data modalities simultaneously. Let us remember that $O_i$ is an output modality we aim to generate and $I$ is the set of input modalities used to prompt the LDM. Following Figure 1.b, we first extract the latent representation $h_{I_i} = P(I_j)$ for all the $I_j$ in $I$ using the prompt encoders now frozen and previously trained as described in Section 2.1. Then, at each training iteration, the prompt sampling strategy $\\Omega$ dynamically select and combine a random subset of input modalities $I_p$ from $I$ into a conditioning vector $\\omega$. Given a total number of $n$ modalities, there exists $2^{n-1} - 1$ possible combinations, making the probability of drawing any possible subset equal to $p = \\frac{1}{2^{n-1}}$. Once a combination is selected, their latent representations are linearly combined to form a conditioning vector defined as:\n$\\omega = \\Omega(h_{I_1}...h_{I_n}) = \\sum_{j=1}^{n} \\alpha_j h_{I_j}$ with $\\sum_{j=1}^{n} \\alpha_j = 1$ and $j \\in {I_p}$.\n(1)\nThe resulting vector $\\omega$, is then used as the conditioning for training the model $G_{O_i}$. Following the reparametrization method proposed in [6], the training objective can be expressed as [7]:\n$L_D = \\mathbb{E}_{z,\\epsilon,t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, \\omega) ||^2]$\n(2)\nWhere $z_t$ is the latent variable of diffusion process, progressively diffused across time step $t \\sim [1,T]$, sampled from a uniform distribution, and $\\epsilon_{\\theta}$ is a denoising model with a UNet architecture parameterized by $\\theta$."}, {"title": "2.3. Multi-output generation via Cross-modal Latent Alignement", "content": "The third training stage enables the simultaneous generation of any combination of output modalities, ensuring that each generative flow is aware of the others. To this end, we incorporate two trainable components into each LDM $G_{O_i}$: the first is an encoder $V_{O_i}$, that projects the latent variable of the diffusion process $z_{O_i}$ into a shared latent space; the second is a cross-attention layer, that allows each LDM to attend to the generative process of another model. Formally, let us consider two modalities, $O_i$ and $O_{i+1}$, being jointly synthesized by $G_{O_i}$ and $G_{O_{i+1}}$ and let $z_{O_i}$ and $z_{0_{i+1}}$ denote their latent variables at a generic diffusion step, respectively. Following Fig 1.c, the encoder $V_{O_{i+1}}$ first projects $z_{0_{i+1}}$ into a shared latent space. Then, in each layer of $G_{O_i}$, the cross-attention layer attends to $V_{O_{i+1}}(z_{0_{i+1}})$.\nFor the diffusion model of modality $O_i$, the training objective in Eq.2 become:\n$L_D^{i} = \\mathbb{E}_{z,\\epsilon,t} [||\\epsilon - \\epsilon_{\\theta_c} (z_{O_i}, V_{O_{i+1}}(z_{0_{i+1}}), t, \\omega) ||^2$,\n(3)\nwhere $\\theta_c$ represents the parameters of the cross-attention layer in the UNet. The training objective for the joint generation of $O_i$ and $O_{i+1}$ becomes $L_{Cross} = L_D^{i} + L_D^{i+1}$.\nBy training only these two additional components while keeping the rest of the model frozen, MedCoDi-M effectively learns to generate modalities simultaneously while mantaining high-quality outputs."}, {"title": "3. Experimental Configuration", "content": "This section provides a comprehensive overview of the experimental setup adopted to evaluate the performance of MedCoDi-M. It begins by describing the dataset used, including its key characteristics and preprocessing steps, which ensure that the data is prepared appropriately for training and evaluation. Next, the section details the implementation specifics of the proposed framework, such as architectural choices, training configurations, and optimization strategies. Additionally, it presents the state-of-the-art competitors used for comparative analysis, highlighting their relevance and limitations in the context of multimodal medical data generation. Finally, the evaluation metrics are introduced, encompassing both quantitative and qualitative measures to thoroughly assess the realism, coherence, and clinical utility of the generated outputs."}, {"title": "3.1. Materials", "content": "To achieve our purpose, it is crucial to leverage a multimodal medical dataset that captures the complementary nature of different modalities, such as imaging and textual data. Such datasets are essential for training models capable of synthesizing clinically accurate and coherent outputs across diverse medical data types. We used the MIMIC-CXR [29] dataset that contains 377.110 CXR images along with their corresponding radiology reports, for a total of 227.827 studies conducted at the Beth Israel Deaconess Medical Center in Boston, MA, USA. In the dataset, images are acquired in frontal and lateral projection. Due to significant anatomical differences, the two views offer distinct yet complementary diagnostic information [30]. For example, cardiovascular structures and the diaphragm can obscure up to 15% of the lung, making certain pathologies undetectable in the frontal view alone [31]. The lateral view, by providing a different perspective, enables the visualization of lesions or abnormalities hidden behind these anatomical structures, thus ensuring more accurate diagnosis [32]. For those reasons, we treated frontal and lateral CXRs as distinct modalities. Each radiology report in the dataset is divided in two sections: a finding section that provides a detailed description of both normal and abnormal features observed in the corresponding CXR, and an impression section, which provides a concise summary of the findings intended to support medical decision-making. In this work, we focused exclusively on the latter, as it offers a concise yet powerful summary of the patient's condition and it also complies with our text encoder, which, following [11] implementation, poses a limitation on the length of the report to 77 tokens.\nFrom the repository, we extracted a total of 154.721 X-rays from 78.584 studies, including all the patients for which the radiology report and both frontal and lateral view were present. Furthermore, we used the original uncompressed X-rays stored in DICOM format [33] as in medical imaging, subtle details are critical for accurate diagnosis, and compression can lead to unintended loss of information.\nThe X-ray's preprocessing involved several steps to standardize and prepare the data for model training. First, we examinated the pixel spacing of each image and resampled those with non-standard spacing to [0.139, 0.139], i.e., the value observed in 95.76% of the dataset. For the images with a Photometric Intepretation of Monochrome-1, the pixel values were inverted to ensure proper representation. Subsequently, we normalized the images by dividing every pixel by the maximum pixel value possible given by their bit depth, bringing the range to [0, 1]. Since the original scans are not square, we chose not to modify their proportions through a direct resizing, as this could distort important anatomical features. Additionally, extracting a square crop was not a viable option, due to the impossibility to select a Region of Interest (ROI) that would be universally applicable. Instead, we added zero-padding around images and then resize them to 256 \u00d7 256, to standardize the input size while preserving the integrity of the visual content.\nTo ensure an unbiased evaluation, we extracted an holdout test set before any training procedure. This set consists of 33.588 samples, that were carefully selected to guarantee no patient's overlapping between training and test set."}, {"title": "3.2. Model Architecture and Training", "content": "We delve here into the architectural choices made for each component of our framework, with a specific focus on how every part of the model is trained."}, {"title": "3.2.1. Prompt Encoders", "content": "Given that our three modalities consist of texts and images, we adopt the Contrastive Language-Image Pretraining (CLIP) [34] approach to leverage a pretrained text-image paired encoder. This approach is composed of an image and a text encoder, denoted as $P_X$ and $P_R$, jointly trained on large-scale datasets of text-image pairs. By doing so, the encoders learn a shared representation space that effectively captures the semantics of both modalities. In order to reduce the computational overload, we decided to let a single image encoder, i.e. a ViT Transformer, be responsible for both frontal and lateral X-rays feature representations, while we leverage a masked self-attention Transformer for the text encoding.\nGiven a batch of X-ray images X and their corresponding reports R, we obtain the embeddings $h_X = P_X(X)$, $h_R = P_R(R)$ for both modalities. These representations are then aligned through contrastive learning, using the InfoNCE contrastive loss [35]:\n$L_{X,R} = - log \\frac{exp(h_X^T h_R / \\tau)}{exp(h_X^T h_R / \\tau) + \\sum_{j \\ne i} exp(h_X^T h_R^j / \\tau)}$\n(4)\nwhere $\\tau$ is the scalar temperature regulating the softness of the softmax distribution, and i,j refers, respectively, to positive and negative couples. We adopt the symmetric loss $L_{X,R} + L_{R,X}$ to make the embeddings closer together."}, {"title": "3.2.2. Latent Diffusion Model", "content": "X-ray Diffusion Model: The LDM for image generation adopts the same architecture of Stable Diffusion 1.5 [7], where AutoKL [26] is used as the variational autoencoder (VAE) to map the input modalities in a lower dimensional space. As stated in [17], the most effective approach for the adaptation of an LDM to the medical imaging domain is to fine-tune the UNet component. Therefore, we kept the VAE frozen and only trained the UNet. In total we trained two LDMs, one for the frontal X-rays and one for the lateral X-rays, using a batch size of 512, a learning rate of $5 \u00d7 10^{-5}$, and a weight decay of $1 \u00d7 10^{-4}$. Both models were trained for 100 epochs using the AdamW optimizer.\nReport Diffusion Model: For the text generation, the UNet architecture is based on [36], which introduced the fully-connected residual blocks (FCResBlock). These expand the 768-dimensional text latent vectors into a 320-by-4 hidden feature and follow the residual block paradigm with Group-Norms [37], SiLU [38], and skip connections. We adopt Optimus [39] as the text VAE, which consist of a BERT [40] text encoder and a GPT-2 [41] text decoder. Unlike the LDMs used for X-ray images, we decided to fine-tune both the VAE and the UNet in two separate training rounds. This approach is necessary as the model has to effectively adapt to a completely different vocabulary. Following [39], the training process begins with a reconstruction task, where the VAE is tasked with accurately reconstructing input radiology reports from their latent representations. Once the first step is fulfilled, the UNet is trained for report generation using a batch size of 1024 was employed, while the learning rate was set to $1 \u00d7 10^{-5}$. The weight decay, the optimizer configuration and the number of epochs remained consistent with the X-ray LDMs."}, {"title": "3.3. Computational Analysis", "content": "To quantify the computational cost of our framework, we provide a detailed breakdown of the number of parameters for each model component. Specifically, CLIP model contains 737 million parameters, while AutoKL has 101 million parameters, with two instances used in our framework. Optimus model consists of 241 million parameters, and the X-ray UNet model has 1.77 billion parameters, with two instances used. Finally, the Report UNet model has 2.04 billion parameters. In total, the number of parameters for all components combined amounts to 6.77 billion. All experiments were conducted on a high-performance computing cluster equipped with four NVIDIA A100 GPUs. The total computational time required across all experiments was approximately 38.354 hours."}, {"title": "3.4. Competitors", "content": "To rigorously compare MedCoDi-M against established approaches for the generation of X-rays and radiology report, we include a total of five open source competitors which, to the best of our knowledge, are the only works with accessible and reproducible code as well as model weights. It is worth noting that we selected UniXGen and LLM-CXR [23, 27] because they address the same problem, i.e., bidirectional generation of CXRs and reports, with different architectures from us, namely Transformer and LLM. Meanwhile, despite it lacks of bidirectional capabilities, we selected RoentGen [16] as it was the first work to leverage a fine-tuned LDM for X-ray generation. All competitors were trained on the MIMIC-CXR dataset, allowing us to use them directly without requiring additional training.\n\u2022 The original CoDi [11] model, without any adaptation to the medical domain.\n\u2022 MedCoDi, our same implementation that omits the Multi-Prompt training strategy, serving as an ablation of MedCoDi-M. This variant highlights the importance of the Multi-Prompt training approach in enhancing the model's conditioning capabilities and generating outputs that integrate information from multiple input modalities effectively.\n\u2022 UniXGen [23], a transformer-based architecture for bidirectional CXR and report generation. UniXGen employs a vector quantization technique to convert X-rays into discrete visual tokens, enabling the model to treat both tasks as a sequence generation. UniXGen incorporates tokens to generate view-specific X-rays. Additionally, multi-view CXRS can be used to condition the report generation.\n\u2022 LLM-CXR [27], a pretrained LLM fine-tuned for CXR understanding and generation. It can perform both CXR-to-report and report-to-CXR generation tasks. It is restricted to frontal chest X-rays and does not consider multi-view or multimodal relationships, limiting its applicability in comprehensive diagnostic workflows.\n\u2022 RoentGen [16], a text-conditioned LDM fine-tuned for generating synthetic frontal CXRs based on textual prompts. RoentGen adapts the Stable Diffusion architecture to the medical domain by fine-tuning the UNet component, enabling it to capture the unique characteristics of chest X-rays."}, {"title": "3.5. Evaluation Metrics", "content": "We conducted both quantitative and qualitative assessments to evaluate the performance of our approach. The first focuses on the statistical properties of the generated data, while the second ensures that the outputs accurately align with the expected clinical informations."}, {"title": "3.5.1. Quantitative Metrics", "content": "To objectively evaluate the quality of the generated outputs, we employed two well-established quantitative metrics, the FID Score and the BLEU Score, that measure the statistical similarity between synthetic and real data, as well as the linguistic coherence of generated clinical reports.\nThe Fr\u00e9chet Inception Distance (FID) [42] measures the dissimilarity between real and synthetic samples in the feature space of an Inception-V3 model pre-trained on ImageNet [43], ranging in the interval [0, +\u221e) with lower values indicating greater similarity. However, because the Inception-Net is not trained on a medical dataset, it may lead to misleading results [44]. To address this limitation, we computed the FID also with another two back-bones, i.e., XRV-DenseNet [45], an in-domain classification model trained to detect pathologies in CXR scans, and XRV-Mimic densenet [45], specifically trained for MIMIC-CXR scans classification. However, to remain coherent with other works, We decided to report the result obtained using the latter backbone.\nThe Bilingual Evaluation Understudy (BLEU) compares machine-generated text to a set of references by calculating the n-gram overlap between the two [46], ranging in the interval [0, +\u221e). Following the literature, here we computed the BLEU score for a number of n-grams equal to 1,2,3,4. BLEU-1 and BLEU-2 place greater emphasis on the consistency of the vocabulary used, focusing on single words or word pairs, while BLEU-3 and BLEU-4 provide information about the semantic structure of the reports."}, {"title": "3.5.2. Factual Correctness", "content": "Ensuring the factual correctness of the generated data is a crucial aspect of evaluating the performance of MedCoDi-M. By using well-established classification models and rule-based tools, we assess how well the synthetic outputs align with real-world diagnostic information."}, {"title": "X-rays Classification:", "content": "To evaluate whether the models are capable of generating images that accurately reflect the information of the corresponding clinical reports, we classified the generated samples using XRV-DenseNet [45], a well-established classifier in the literature for CXR classification. Since such a classifier is trained only on a subset of pathologies, we computed the AUC and F1 scores for the following diseases: Atelectasis (Atl.), Cardiomegaly (Cmgl.), Consolidation (Cnsl.), Edema (Edm.), Enlarged Cardiomediastinum (Enl.), Lung Lesion (Les.), Lung Opacity (Opc.), Pleural Effusion (Eff.), Pneumonia (Pnm.) and Pneumothorax (Ptx.), along with micro, macro, and weighted averages. The micro average aggregates contributions from all classes to provide an overall measure, the macro average computes the metric independently for each class and averages them, and the weighted average adjusts the macro average by accounting for the number of samples per class. However, because not all scans have a defined label for every pathology, we computed the performance for each class only when a ground truth was available,"}, {"title": "Report Classification:", "content": "For report classification, we leveraged CheXpert-Labeler [47], a rule-based natural language processing tool that reads a text report and extracts whether it mentions the presence or absence of significant radiologic findings. Since a rule-based classifier is used, it is not possible to compute the AUC; instead, we reported the F1 score for the same subset of disease previously introduced along with No Finding (No F.) class. To remain consistent with the previous setup, we also reported the micro, macro, and weighted averages for the F1 score. This task quantifies the ability of the model to generate reports that align with the medical conditions seen in the X-ray images, ensuring that the synthetic reports accurately reflect the diagnostic information provided by the images."}, {"title": "3.5.3. Visual Turing Test", "content": "We performed a qualitative assessment of the data generated by MedCoDi-M, through a Visual Turing Test performed by three expert radiologist. This evaluation consisted of five independent tasks aimed at comparing synthetic and real medical data, with both X-rays and clinical report being assessed. Each task was performed through a web-based platform, where experts evaluated the data using a 1-to-5 numeric scale. A score of 1 indicated the poorest quality, while a score of 5 represented the highest level of quality and coherence. The tasks are:\n\u2022 General X-ray Realism: Experts rated the overall realism of a series of 20 images, which included a mix of real and synthetic X-rays. A high score indicates that the synthetic X-rays appear indistinguishable from real clinical X-rays, with accurate anatomical structures and no visible artifacts that could mislead a clinician in a diagnostic setting.\n\u2022 General Report Realism: Experts reviewed 20 clinical reports, both real and synthetic, rating their plausibility and realism. A high score in this task implies that the synthetic reports accurately reflect the clinical context, making use of appropriate medical terminology and providing clinically relevant findings that would be consistent with a real report.\n\u2022 Report Coherence with Pair of X-rays: Experts evaluated the consistency between 20 pairs X-ray images and their associated reports. The images were guaranteed to be real, while the reports were either real or synthetic, presented in random order. A high score indicates that the synthetic reports align accurately with the X-ray images, with no contradictions or inconsistencies between the reported findings and the visual evidence.\n\u2022 Coherence Between Report and X-ray: Experts compared 20 pairs of X-ray images, both real and synthetic, in random order, with the real clinical report to assess the plausibility of the X-ray given the report. A high score reflects that the synthetic X-rays match the findings described in the report, indicating that the generative model correctly understood and translated the clinical context from the report into the visual representation.\n\u2022 Coherence Between X-ray Pairs: Experts assessed the consistency between 20 pairs of frontal and lateral X-rays. One of the view was guaranteed to be real, while the other were either real or synthetic, presented in random order. A high score here indicates that the synthetic view accurately represents the same clinical findings as the real view, demonstrating proper anatomical alignment and no conflicting features across different perspectives."}, {"title": "4. Results and Discussion", "content": "This section presents an in-depth analysis to assess MedCoDi-M's performance, providing both quantitative and qualitative evaluations and visual examples."}, {"title": "4.1. X-ray Generation", "content": "Table 2 presents the FID scores on the test set for generating frontal (F) and lateral (L) X-rays. The first column lists the models used for generation, while the remaining columns show the performance achieved for different generation settings using different prompts combination: from clinical report (T) to frontal or lateral CXR (T\u2192F, T\u2192L), from lateral or frontal CXR to the other view (L\u2192F, F\u2192L) and a combination of a clinical report and a CXR image to the other view (L+T\u2192F, F+T\u2192L)."}, {"title": "4.2. Report Generation", "content": "Table 4 presents the BLEU score on the test set for report generation across three different generation settings: frontal CXR to report (F\u2192T), lateral CXR to report (L\u2192T) and both frontal and lateral CXR to report (F+L\u2192T). These scores highlight the ability of each method to generate reports in comparison to reference ones, with higher BLEU scores indicating better performances. The results show that MedCoDi-M outperforms the competitors across all BLEU score metrics. Results suggest that not only MedCoDi-M utilize the same terminology as real clinical reports, as indicated by the best BLEU-1 and BLEU-2, but it also generates sentences whose structure is consistent with that of actual reports, as demonstrated by the better BLEU-3 and BLEU-4. This highlights a high degree of linguistic coherence and fidelity in reproducing both the content and phrasing of real-world medical texts. Moreover, we computed BLEU scores for reports generated across multiple scans of the same study. This evaluation assesses the model's consistency in generating coherent and reliable reports when presented with scans from the same clinical case. The goal was to ensure that the model not only excels in generating high-"}]}