{"title": "MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark", "authors": ["Yang Yu", "Yuezun Li", "Xin Sun", "Junyu Dong"], "abstract": "Phytoplankton are a crucial component of aquatic ecosystems, and effective monitoring of them can provide valuable insights into ocean environments and ecosystem changes. Traditional phytoplankton monitoring methods are often complex and lack timely analysis. Therefore, deep learning algorithms offer a promising approach for automated phytoplankton monitoring. However, the lack of large-scale, high-quality training samples has become a major bottleneck in advancing phytoplankton tracking. In this paper, we propose a challenging benchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse background information and variations in motion during observation. The dataset includes 27 species of phytoplankton and zooplankton, 14 different backgrounds to simulate diverse and complex underwater environments, and a total of 140 videos. To enable accurate real-time observation of phytoplankton, we introduce a multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion Tracker (DSFT), which addresses issues such as focus shifts during tracking and the loss of small target information when computing frame-to-frame similarity. Specifically, we introduce an additional feature extractor to predict the residuals of the standard feature extractor's output, and compute multi-scale frame-to-frame similarity based on features from different layers of the extractor. Extensive experiments on the MPT have demonstrated the validity of the dataset and the superiority of DSFT in tracking phytoplankton, providing an effective solution for phytoplankton monitoring.", "sections": [{"title": "1 Introduction", "content": "Phytoplankton is an ecological concept referring to tiny plants that live in water through a planktonic lifestyle, typically microalgae [1]. They are the primary producers in aquatic ecosystems and the main source of dissolved oxygen in water bodies [2]. Monitoring phytoplankton is an effective way to understand changes in the marine environment and ecosystems. Therefore, it is of great importance to monitor phytoplankton for maintaining the Earth's ecological balance, protecting water resources, and determining the productivity of aquatic environments. [3]\nTraditional phytoplankton monitoring methods often rely on sampling techniques, where water samples are filtered through membranes with specific pore sizes to capture the phytoplankton. [4] The membranes are then treated for transparency and observed manually under a microscope. This method requires significant human and material resources and cannot provide real-time observation. Collecting large-scale data to train deep neural networks and using these networks for automatic identification can solve these problems. However, existing multi-object tracking methods often struggle to accurately track phytoplankton in complex underwater environments. This is due to two main reasons: the lack of high-quality, large-scale phytoplankton video datasets, which limits the ability to fully train algorithms, and the fact that current multi-object tracking algorithms are not well-suited to the specific characteristics of the underwater environment and phytoplankton.\nCreating a comprehensive phytoplankton dataset presents significant challenges due to equipment and environmental limitations. While it is relatively straightforward to collect water samples, obtaining a wide range of phytoplankton species and capturing them in large-scale video data is much more difficult. Phytoplankton are highly diverse, and collecting enough species to represent their variety in natural environments requires extensive fieldwork, specialized equipment, and ideal environmental conditions, which are not always available.\nMoreover, even when samples are obtained, the process of capturing high-quality video footage that accurately reflects the dynamic behavior of phytoplankton can be complex. This includes factors such as maintaining stable conditions in the laboratory, ensuring proper microscopy settings, and recording long sequences at high frame rates to observe movement. Due to these difficulties, most existing plankton datasets primarily consist of static image data, which lacks the temporal information necessary for tracking and analyzing plankton motion.\nEven video datasets like PMOT2023 [5], though a valuable resource, are limited in both sample density and scale. The video data available often includes only a small number of species or frames, and the lack of variety and richness in the samples hinders comprehensive analysis and generalization in multi-object tracking tasks. As a result, there's a growing need for larger, more diverse video datasets that can better represent the complexity of plankton behavior and their environmental interactions.\nWithout access to large-scale, high-quality video datasets, it becomes challenging to fully develop and test algorithms that can perform effectively across diverse aquatic conditions. To overcome this limitation, we have constructed MPT, a synthetic large-scale phytoplankton video benchmark dataset (refer Fig.3), specifically designed to facilitate the training and evaluation of multi-object tracking algorithms under various environmental backgrounds.\nThe MPT dataset was created using samples primarily sourced from the coastal waters of the Yellow Sea, where a wide range of plankton species can be found. To enhance the dataset's diversity and better simulate real-world conditions, we incorporated different video backgrounds and environmental variables. Specifically, we designed 14 distinct background images, each featuring variations in lighting conditions, background colors (white and blue tones), and different levels of impurities to mimic natural aquatic environments more closely. This approach ensures that tracking algorithms trained on MPT can generalize across a wide range of environmental scenarios.\nMoreover, MPT includes video sequences featuring 27 different plankton species, providing a rich variety of data points. In total, the dataset consists of 140 high-resolution videos, capturing both the diversity of plankton and the complexities of their interactions in different environments. By simulating such a wide range of conditions, the MPT dataset ensures that the tracking algorithms developed using this resource can handle the variability and unpredictability of real-world aquatic environments. This synthetic yet realistic dataset fills a crucial gap in the field, offering researchers a valuable tool for advancing the state of the art in phytoplankton tracking and monitoring.\nBased on the MPT dataset, we have developed a multi-object tracking framework named DSFT, designed for real-time tracking of phytoplankton. Currently, there are two main shortcomings when using multi-object tracking algorithms to process phytoplankton data (refer Fig.1):\n1. Phytoplankton often resemble the natural aquatic environment in appearance. When multiple phytoplankton overlap with each other or overlap with impurities, it can cause the algorithm's attention to shift inappropriately.\n2. When tracking is performed using a similarity matrix between consecutive frames, the algorithm typically achieves high accuracy. However, this approach has a drawback: using only the deepest feature maps from the previous and current frames to"}, {"title": "2 Related Work", "content": "Marine phytoplankton, these microscopic plant-like organisms, play an indispensable role in ocean ecosystems [6]. They are a group of tiny algae and prokaryotes that float freely in the surface and mid-water layers of the ocean, comprising eight major groups: cyanobacteria, green algae, diatoms, dinoflagellates, golden algae, yellow-green algae, cryptophytes, and euglenoids. Phytoplankton are crucial to the Earth's carbon cycle and climate regulation [7]. Through photosynthesis, they absorb carbon dioxide and release oxygen, not only providing a food source for marine ecosystems but also supplying essential oxygen to marine life. Therefore, these microscopic organisms form the foundation of the marine food chain and have a profound impact on global climate change and ecological balance.\nThe population dynamics, biomass, and responses of marine phytoplankton to environmental changes are key areas of marine science research [8]. By continuously monitoring the distribution and density of these microorganisms, researchers can assess the health of marine ecosystems, predict the potential impacts of climate change on marine ecology, and effectively manage fisheries resources. When phytoplankton experience excessive growth, such as during harmful algal blooms (e.g., red tides), they can disrupt marine ecosystems, affect water quality, and degrade the quality of seafood, ultimately posing negative consequences for human activities."}, {"title": "2.3 Dataset", "content": "In the field of Multi-Object Tracking (MOT), commonly used public datasets include MOT16, MOT17, MOT20 [15], and DanceTrack [16]. The commonly used planktonic datasets include WHOI Plankton [17], PMID2019 [18], and PMOT2023 [5].\nMOT16 and MOT17 are widely used benchmark datasets in the MOT domain, designed to provide a variety of complex scenarios and organized by the MOT Challenge. MOT17, compared to MOT16, introduces more sequences and richer annotations, including detection results generated by different detectors, allowing researchers to test their algorithms under more diverse conditions. MOT20, released after MOT16 and MOT17 by the MOT Challenge, focuses on extremely crowded scenes. Compared to previous datasets, MOT20 provides much higher crowd density.\nDanceTrack is a relatively new dataset that presents a unique challenge to the MOT field: tracking multiple dancers in dance videos. Unlike traditional surveillance videos, the videos in DanceTrack feature complex human movements and significant occlusions between individuals. This not only demands the algorithm's ability to dynamically predict movements but also requires it to accurately understand human poses and motion patterns.\nThe WHOI Plankton dataset, created by the Woods Hole Oceanographic Institution, is a large-scale collection of plankton images designed for classification tasks. This dataset comprises over 3.4 million high-quality images of phytoplankton, annotated by experts, and covers 70 different species categories. However, while this dataset is highly valuable for identifying and classifying various types of plankton, it only offers image-level annotations. As a result, it is not suitable for tasks like object tracking, where continuous observation of movement is required.\nIn 2019, the PMID2019 dataset was introduced as a phytoplankton detection benchmark. Although useful in detection tasks, the dataset consists primarily of stained images, which differ significantly from real-time, in-situ plankton observations. Furthermore, similar to the WHOI Plankton dataset, PMID2019 also provides image-level data and lacks the temporal information necessary for tracking tasks, limiting its application in dynamic environments where motion is critical.\nPMOT2023, unveiled in 2023, marked a major step forward by introducing the first dataset specifically designed for multi-object tracking of plankton. Unlike previous datasets, PMOT2023 focuses on simulating plankton movement in controlled environments, such as flow tubes observed under a microscope. By offering video sequences of plankton in motion, it effectively fills a gap in the available resources for plankton observation. This synthetic dataset provides crucial video data for tracking multiple plankton species simultaneously, making it a valuable resource for studying their behavior and movement patterns over time."}, {"title": "2.4 Algorithm", "content": "Current tracking methods are primarily developed for ground-based environments, concentrating on general objects like vehicles and pedestrians [19-22]. In the context of multi-object tracking within video sequences, three critical components are essential for determining object trajectories: object extraction, temporal association, and motion prediction. Based on the tracking workflow, these methods can be categorized into two main types: offline tracking and online tracking.\nOffline tracking makes use of data from future frames and typically represents the problem using a graph model to find a globally optimal solution [23-25]. However, this approach is not practical for real-world applications since it depends on information from frames that have not yet occurred. In contrast, online multi-object tracking focuses on establishing correspondences between current detections and existing object trajectories based solely on the data at hand [26-28]. This method mandates that the tracking decisions for each frame are based only on the information from the current and preceding frames, without using current data to modify past results. Consequently, this paper centers on the online tracking framework."}, {"title": "3 Dataset Establishment", "content": "In this study, we employed a high-resolution microscope equipped with a 4K resolution camera to conduct detailed observations and sampling of phytoplankton from coastal areas in the Yellow Sea. Phytoplankton samples were meticulously collected using a plankton net to ensure their representativeness and accuracy. These samples were subsequently examined under the microscope, and high-resolution images were captured at a resolution of 2720x1824 pixels. This approach facilitated comprehensive analysis of the morphological characteristics and detailed features of the phytoplankton specimens.\nThe sampling period spanned the entire year, with a particular focus on months of heightened phytoplankton density, such as September and October. As the diversity of phytoplankton species in coastal regions of the Yellow Sea is somewhat limited, additional phytoplankton images were sourced from publicly available online repositories to supplement our dataset. Careful selection criteria were applied during image collection to ensure backgrounds were consistent with those observed in our samples, thereby minimizing potential issues arising from background color discrepancies that could affect dataset quality.\nIn constructing the MPT dataset, we meticulously documented 27 distinct types of phytoplankton species observed during our study. These include Ceratium furca, Gymnodinium, Ceratium, Anabaena, Copepoda, Copepod nauplii, Coscinodiscus,"}, {"title": "3.2 Production Method", "content": "The background images in our dataset are composed of two distinct color schemes: blue and white, with seven images for each color, resulting in a total of 14 unique background images. For each of these backgrounds, we applied varying levels of impurity density and brightness to simulate different aquatic environments. This variation allows for the creation of more realistic scenarios and increases the robustness of the tracking algorithms being developed and tested.\nFor each background image, we generated 10 sequences of consecutive image frames, with each sequence saved as a video at 25 frames per second. To simulate the natural movement of phytoplankton in real-world water environments, we employed jittering and rotation mechanisms on each phytoplankton sample incorporated into the video. These movements helped mimic the natural flow, rotation, and drift that phytoplankton typically exhibit, thus enhancing the realism of the dataset. The jittering and rotational variance add dynamic complexity to the sequences, contributing to a more challenging and comprehensive dataset.\nTo ensure a balanced and rational distribution of data across the dataset, the types and quantities of phytoplankton included in each video sequence were randomly selected. Furthermore, we varied the total number of frames, as well as the degree of jitter and the movement speed of the phytoplankton, all within a predetermined range. This approach provided a rich diversity in the sequences, allowing the dataset to capture a wide range of motion patterns and environmental conditions, thereby supporting the development of more generalized and adaptable tracking algorithms for real-world applications."}, {"title": "3.3 Advantages of MPT", "content": "The MPT dataset provides significant advantages over existing phytoplankton tracking datasets, making it an invaluable resource for advancing multi-object tracking in underwater environments. One of its key strengths is the large scale and diversity of data. The dataset includes 140 high-resolution video sequences featuring 27 different phytoplankton species, offering extensive coverage for robust algorithm training. MPT focuses on video sequences rather than static images, enabling continuous tracking of phytoplankton over time. These high-quality videos, recorded in 4K resolution, allow for detailed analysis of small-scale objects and provide a realistic representation of phytoplankton movement-critical for MOT algorithms. The dataset simulates real-world aquatic environments with 14 different backgrounds, incorporating variations in impurity density, lighting, and brightness. This diversity ensures that tracking algorithms can handle dynamic, real-world scenarios.\nIn addition, MPT includes phytoplankton species of various sizes, posing challenges for traditional tracking algorithms. However, by offering high-resolution images and simulating realistic motions through jittering and speed variation, the dataset ensures that algorithms trained on it can adapt to different object scales and movements. The dataset is designed for real-time tracking, with sequences saved at 25 frames per second, making it suitable for applications in marine ecological monitoring and underwater research. MPT's synthetic and scalable nature makes it a flexible tool for research. The dataset not only supports tracking tasks but also addresses detection tasks, bridging the gap left by previous datasets focused solely on classification."}, {"title": "4 Methods", "content": "Traditional multi-object tracking algorithms face two main challenges when tracking plankton: first, when there is overlap or partial overlap between individuals, the algorithm's focus can shift inappropriately; second, when using the similarity matrix between consecutive frames for tracking analysis, the algorithm may lose information on smaller objects. To address these issues, we describe an online tracker for plankton called DSFT (refer Fig.2). First, we propose the DCM, which corrects feature map biases and ensures the algorithm focuses on the individual being tracked. In addition, we introduce the MFSF, which emphasizes the connection between plankton of different sizes across frames, effectively enhancing the algorithm's ability to detect small plankton."}, {"title": "4.1 Overall Process", "content": "Our method's overall process is inspired by TraDes [29]. Given an input variable $I_t$, it is first passed through a feature extraction module, where the DCM is applied to extract bias-corrected feature maps, denoted as $f_t$. During this process, intermediate features $f_m$ and shallow features $f_s$ are also extracted from the backbone and corrected. The three levels of features from both the previous and current frames are then multiplied and fused to establish the correlation between them. This correlation allows us to accurately predict the motion offsets of phytoplankton of different sizes. Finally, the motion offsets are passed through the convolutional and head networks for the final prediction."}, {"title": "4.2 Deviation Correction Method", "content": "Plankton often exhibit appearances similar to their aquatic environment. When plankton overlap or partially overlap with other individuals or with surrounding debris, the algorithm's focus may shift inappropriately. This characteristic makes traditional multi-object trackers unsuitable for tracking plankton. Inspired by DINO-Tracker, our method introduces an additional feature extractor to predict the residual error of the primary feature extractor's results. We use this residual to correct the bias in the main feature map. (refer Fig.4)\nSpecifically, during the feature extraction stage, the current frame is passed through the Conv1 module to generate the main feature map $f_t$. The main feature map and the current frame are then passed through Conv2 to obtain a residual, which represents the bias in the extracted features. We add this residual to the main feature map, resulting in a corrected feature map that mitigates the bias. Then we pass this corrected feature map into the convolution module to calculate Similarity Map."}, {"title": "4.3 Multi-scale Feature Similarity Fusion", "content": "Different species of plankton often exhibit significant size differences, and when the algorithm focuses on larger individuals, it tends to overlook the features of smaller ones. In multi-object tracking algorithms, using a similarity matrix between consecutive frames typically yields good tracking results. However, calculating this matrix using only the deepest feature maps can result in the loss of information about smaller objects.\nTo address this, we propose the MFSF. Inspired by General Track, this method aims to emphasize features of smaller individuals while still focusing on larger ones. To achieve this, we extract additional shallow features $f_s$ and mid-level features $f_m$ during the feature extraction process, corresponding to smaller and medium-sized objects. The shallow and mid-level features from consecutive frames are multiplied to generate their respective similarity matrices, and the results are fused.\n$g = (f_t^{-1} \\times f_t) + (f_m^{t-1} \\times f_m^{t}) + (f_s^{t-1} \\times f_s^{t})$\nThe fused feature map highlights the similarity information of plankton of different sizes across frames."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Settings", "content": ""}, {"title": "5.1.1 Evaluation Metrics", "content": "We utilize the CLEAR-MOT metrics [39] to assess tracking performance. This metric includes several indicators, such as MOTA, IDF1, IDs, FP, and FN. Among them,"}, {"title": "5.1.2 Implementation Details", "content": "During the training process, we applied data augmentation to each batch of data with a certain probability. The input size of the images was uniformly adjusted to 860 * 640, and the learning rate was reduced at the 40th and 50th epochs. We utilized the Conv1 network for primary feature extraction and employed the Conv2 network to extract the residuals from the main feature maps. The model was trained for 60 epochs on a single 4080 GPU card with a batch size of 2, and an initial learning rate of 2.0e-4. During the inference stage, the tracking confidence threshold was set to 0.4, and the input image size was set to 640 * 960. Inputs were processed in pairs of two consecutive frames simultaneously. In the comparative methods of the tracking-by-detection category, we uniformly employed CenterNet as the detector, with DLA34 serving as the backbone for CenterNet. The remaining parts were set to the default configuration of TraDeS. Conv1 represents DLA34, and Conv2 represents Delta-DINO."}, {"title": "5.1.3 Compared Methods", "content": "We have chosen several different methods for comparison, and the specific methods are as follows:\nSORT (ICIP'16) [30]: SORT is a straightforward and real-time multi-object tracking method that applies a Kalman filter combined with the Hungarian algorithm for data association. This significantly improves both tracking speed and accuracy.\nDeepSORT (ICIP'17) [31]: DeepSORT builds on SORT by incorporating ReID(Re-Identification) [40] and appearance features from detection boxes. It employs a Matching Cascade strategy to minimize the occurrence of target ID switches.\nTraDeS (CVPR'21) [29]: TraDeS is an end-to-end joint detection and tracking model that leverages tracking signals to support detection. It infers tracking offsets from cost metrics and uses these to propagate prior target features, improving detection and segmentation of current targets.\nBotSORT (Arxiv'22) [32]: BotSORT combines the strengths of both motion and appearance cues, while compensating for camera movement and refining the Kalman filter's state vector for more precise tracking results.\nByteTrack (ECCV'22) [33]: ByteTrack applies a two-step matching strategy. First, it matches high-confidence detection boxes with tracks, then associates lower-confidence boxes with unmatched tracks from the first step. It handles occlusions without relying on a ReID model, using only a Kalman filter and the Hungarian algorithm.\nUAVMOT (CVPR'22) [34]: UAVMOT is designed for tracking from drone perspectives, building on FairMOT. It includes an ID feature update module, an adaptive motion filter, and a gradient balanced focal loss, each addressing drone footage challenges like complex motion and improving frame-to-frame ReID feature consistency.\nStrongSORT (TMM'23) [35]: StrongSORT enhances DeepSORT by refining the feature extractor, introducing an inertia term for smoother feature updates, utilizing a Kalman filter tailored for non-linear motion, and incorporating motion data into the cost matrix.\nUCMCTrack (AAAI'24) [36]: UCMCTrack addresses challenges caused by erratic camera motion by linking the motion of objects with their positions relative to the ground. It uses a mapped Mahalanobis distance instead of IoU to measure the similarity between objects and their previous trajectories.\nBoostTrack (MVA'24) [37]: BoostTrack addresses unreliable detections and ID switches by introducing a confidence score for detection tracklets. It scales the similarity metric using this score, and combines Mahalanobis distance with shape similarity to improve tracking accuracy and reduce IoU-induced ambiguities.\nTLTDMOT (CVPR'24) [38]: TLTDMOT targets the long-tail distribution problem in multi-object tracking by implementing two data augmentation techniques: Static Camera Viewpoint Augmentation and Dynamic Camera Viewpoint Augmentation. It also features a Group Softmax module for re-identification to handle the distribution imbalance."}, {"title": "5.2 Ablation Study", "content": "In this section, we validate the effectiveness of the proposed DSFT through an ablation study. All experiments were conducted on the MPT dataset. To ensure a fair evaluation of each component's performance, the training and testing details remain consistent with those described in the implementation section. When testing specific modules, no modifications were made to the remaining components.\nTo assess the effectiveness of the two proposed methods, we conducted three sets of experiments: adding only DCM, adding only MFSF, and adding both them under identical conditions. As shown in Table 2, the performance significantly improved with the addition of each module compared to the baseline alone. When both methods were integrated, the MOTA and IDF1 scores increased by 23.4% and 15.8%, respectively, compared to the baseline."}, {"title": "6 Conclusion", "content": "In this paper, we present a large-scale plankton tracking dataset, MPT, and develop a multi-object tracking method specifically designed for plankton, named DSFT. MPT contains 140 high-definition videos and 27 species of plankton, incorporating 14 distinct background images that vary in brightness, impurity density, and color. DSFT is a real-time multi-object tracking framework that offers an automated and timely solution for monitoring plankton. To address two key limitations of traditional tracking methods namely, improper focus shifts when objects overlap and the potential loss of small object information during tracking analysis we propose two solutions: the DCM to correct feature bias and the MFSF to enhance the representation of small objects. These methods improve both the performance and reliability of the tracking algorithm."}]}