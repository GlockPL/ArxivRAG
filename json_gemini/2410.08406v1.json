{"title": "Promptly Yours? A Human Subject Study on Prompt Inference in AI-Generated Art", "authors": ["Khoi Trinh", "Joseph Spracklen", "Raveen Wijewickrama", "Bimal Viswanath", "Murtuza Jadliwala", "Anindya Maiti"], "abstract": "The emerging field of AI-generated art has witnessed the rise of prompt marketplaces, where creators can purchase, sell, or share prompts to generate unique artworks. These marketplaces often assert ownership over prompts, claiming them as intellectual property. This paper investigates whether concealed prompts sold on prompt marketplaces can be considered as secure intellectual property, given that humans and AI tools may be able to approximately infer the prompts based on publicly advertised sample images accompanying each prompt on sale. Specifically, our survey aims to assess (i) how accurately can humans infer the original prompt solely by examining an AI-generated image, with the goal of generating images similar to the original image, and (ii) the possibility of improving upon individual human and AI prompt inferences by crafting human-AI combined prompts with the help of a large language model. Although previous research has explored the use of AI and machine learning for prompt inference (and also to protect against it), we are the first to include humans in the loop. Our findings indicate that while humans and human-AI collaborations can infer prompts and generate similar images with high accuracy, they are not as successful as using the original prompt.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) has made remarkable strides in the domain of creative and artistic expression, enabling an easy and automated process for everyone to generate visually captivating and conceptually intriguing art work. Central to this creation process of AI-generated art and images are deep learning based text-to-image (txt2img) models for image generation that utilize text prompts as input (instructions) from users to generate unique and diverse image/art outputs. Some of the most popular open-source or commercially-available examples of such txt2img models include Midjourney, DALL-E 2 (Ramesh et al. 2021), Stable Diffusion (Rombach et al. 2022) and GLIDE (Nichol et al. 2022).\nAt a high level, these models have two main components - a Language Model (e.g., CLIP) and a Generative image model (e.g., Stable Diffusion (Rombach et al. 2022)).\nThe language model converts a given text prompt to a latent representation, which is then used to condition the generative image model to produce an image that captures the prompt description. Furthermore, these models are trained on vast datasets of text-image pairs, allowing them to understand and render complex visual concepts from textual descriptions with remarkable accuracy and creativity. For example, Stable Diffusion was trained on the publicly available LAION-5B dataset, containing 5 billion image-caption pairs, derived from data scraped from the Internet.\nText prompts serve as critical input instructions to txt2img models for generating high-quality text-conditioned images and it is non-trivial to deduce an appropriate prompt for the desired image, often requiring creativity and trial-and-error (Wang et al. 2023). This has resulted in the emergence of new prompt engineering jobs and prompt marketplaces for AI-generated art, where prompt engineers, artists, and enthusiasts can exchange and sell prompts that can generate custom high-quality art. Given the importance of selecting the right prompt for generating a desired image and the non-triviality in determining one, these text prompts are often treated as protected information by their creators. Prompt marketplaces often claim intellectual property rights over the prompts, asserting that they are valuable and original creations worthy of legal protection (PromptBase; Promptrr.io). Given the protected status of (input) text prompts, two research questions arise that are largely unexplored thus far: (i) how accurately can humans infer the input text prompt by just viewing the (AI-generated) image generated from that prompt? and (ii) can AI tools assist humans in more accurately inferring text prompts of a target (AI-generated) image?\nTo address these research questions, our study employs a human subject survey to assess how accurately users (participants) can infer prompts by just visually examining the AI-generated images. Our survey results provides valuable insights on how users' prediction (of prompts) performance, measured using well-defined metrics, varies with different prompt-related attributes and varying user demography and backgrounds. After combining responses from the survey (taken by human subject participants) with responses from an AI-based prompt inference model, we further re-evaluate and compare the overall inference accuracy in this human-"}, {"title": "2 Related Work and Research Goals", "content": "Previous research in the literature has primarily explored AI/ML-based inference techniques to deduce (infer) prompts and has also proposed AI/ML-based methods to safeguard against such kind of prompt inference threats. Several prior works have employed machine learning algorithms to reverse-engineer the prompts used in the creation of artworks (CLIP Interrogator; Shen et al. 2023; Wu et al. 2022; Li et al. 2022), highlighting the potential vulnerability of prompt concealment. On the other hand, efforts have also been made to develop protective measures to secure the prompts from unauthorized access or replication (Shen et al. 2023; Struppek, Hintersdorf, and Kersting 2022; Zhai et al. 2023). These efforts involve strategies to thwart prompt inference, backdoor injections, and data poisoning, implementing rigorous dataset inspections, and employing anomaly detection. However, if humans are able to infer prompts with a high degree of accuracy, the effectiveness of these protective measures against prompt inference may be called into question. Furthermore, it is possible that AI-assisted prompt inference and prompt inference by humans can be effectively combined to significantly improve the overall inference accuracy, and needs to be further studied. While there are recent studies investigating the effectiveness of human-AI collaboration leading to more creative artworks (Lyu et al. 2022; Brade et al. 2023), currently we have little understanding of how a similar human-AI collaboration may work towards prompt inference of AI-generated art."}, {"title": "2.2 Challenges in Prompt Inference", "content": "Inferring prompts of AI-generated images is a complex task that both humans and AI models (CLIP Interrogator; Mid-Journey can find challenging due to the complexities in visual content and the subtleties of language. Even the addition of a single modifier can dramatically alter the resulting image, demonstrating how each element in a prompt contributes to the generated image. Conversely, omitting a crucial modifier could lead to a visual representation that misses the depth or context intended. When humans try to deduce the prompts for AI-generated images, they often rely on their subjective interpretation and understanding of the visual content, which can lead to varied conclusions on the subjects and modifiers used in the image generation. The understanding of how humans may infer prompts, specifically the subjects and modifiers used in AI-generated art, with or without the aid of AI-based prompt inference tools, remains an unexplored research area and is the focus of this work."}, {"title": "2.3 Research Goals", "content": "Given the above intricacies in human and AI prompt inferences, it is not trivial to assess whether concealed prompts sold on prompt marketplaces can be considered as secure intellectual property, or are they vulnerable to prompt inference and replication. Moreover, there is a lack of a clearly defined measurement based on which a prompt inference can be deemed successful, especially for human prompt inference. To address this gap, our research goals are as follows:\nDetermining the accuracy with which individuals can infer the original prompts from images created by a txt2img model, aiming to reproduce similar images. This is accomplished by designing and conducting a comprehensive human participant survey as outlined in Section 3 and Section 4. Additionally, exploring whether a participant with an art background (e.g. an art major in college, a graphic designer) would have an advantage over one without such a background. Evaluation is to be based on similarity between the original image and the images reproduced using the inferred prompt.\nDetermining any improvement in prompt inference accuracy when human inferred prompts are combined with AI inferred ones, aiming to further improve the similarity of the reproduced images to the original images. This is achieved by integrating the survey responses of the human participants and AI inferred prompts as discussed in Section 4.3. Evaluation is to be based on similarity between the original image and the images reproduced using the combined prompt.\nEstablishing robust thresholds of metrics for measuring the success of both standalone and combined human-AI inferences, as detailed in Section 4.2, ensuring a clear framework for assessment."}, {"title": "3 Survey Design and Participants", "content": "The images presented to the participants were generated from two distinct types of prompts: Controlled and Uncontrolled. Controlled prompts serve as a baseline to gauge participants' ability to recognize common subjects and modifiers, allowing us to assess other variables associated with prompt inference, such as any differences between txt2img models and demographic-based differences. Conversely, uncontrolled prompts feature a much more diverse mix of subjects and modifiers and is used to construct more comprehensive and challenging inference tasks for participants.\nThe controlled prompts set was constructed through an analysis of common subjects discovered on the dynamically changing homepage of Lexica, a popular prompt and AI art sharing platform. We selected Lexica for this data gathering task as they have web crawling-friendly terms of service. Our Selenium script identified 100579 different images and accompanying prompts by their specific HTML elements. Subsequent processing with spaCy, a Natural Language Processing (NLP) framework, enabled us to distill the subjects from these prompts. The five most frequently occurring subjects identified were man, woman, astronaut, cat, and robot. In parallel with subject selection, we curated a set of modifiers by referencing a Midjourney styles and keywords repository. The modifiers spanned various categories such as themes/genres, lighting, drawing and art medium, perspective, emotion/mood, colors and palettes, geography and culture, rendering/shading style, culminating in a list of 121 modifiers (details in Appendix E.5). These subjects and modifiers were then randomly sampled and combined, ranging from one to five modifiers per subject, to formulate 100 controlled prompts, 20 per subject (Appendix E.6). These 100 prompts were then used to generated images for Parts I and III of our survey (3), 25 for each of the four selected txt2img models.\nThe uncontrolled prompts set was constructed through a random sampling of 100 whole/complete prompts discovered on PromptHero (a platform for discovering and sharing AI-generated art and text prompts). There were no restrictions on what subject and modifiers appeared in these prompts, as long as they were non-explicit in nature. These 100 prompts were then used to generate images for Part II of our survey, 25 for each of the four selected txt2img models."}, {"title": "3.2 Analyzed txt2img Models", "content": "Our study focuses on four popular txt2img models, including two base models and two fine-tuned models:\nMid Journey v5.0.\nStable Diffusion XL (SDXL) (Podell et al. 2023; Rombach et al. 2022).\nDreamShaper XL (Podell et al. 2023).\nRealistic Vision v5.\nAt the time we began this study in late 2023, these models had the highest number of prompts being shared or sold on platforms such as PromptHero, Promptrr.io, Prompti AI, PromptBase, and CivitAI. More details on these four models and how they were employed in our study can be found in Appendix B.\nBesides the above, there are other popular txt2img models which we also attempted to include in our study, but could not for various reasons. For instance, OpenAI's popular DALL-E model utilizes a modified version of the GPT model by adapting its transformer-based architecture for image generation (Ramesh et al. 2021). However, DALL-E was omitted from our study because we were unable to lock in a specific version of the model for image creation. We were concerned that DALL-E's updates during the time-frame of our study could affect the consistency of the images produced initially and those generated later for comparison.\nFor our user study to assess the ability of humans to infer prompts from AI-generated images, we design and implement a custom survey tool. This tool dynamically loads and displays images (and its metadata) together with related question(s), and can capture participant responses to these questions (see Figure 22 and Appendices E.1 to E.3). We collected responses from 230 participants using this custom tool, of which 59 were recruited from two public university campuses (in the US), while the remaining 171 participants were recruited via the Amazon Mechanical Turk (MTurk) platform. This study has been approved by the Institutional Review Boards (IRBs) of the institutions involved. Before completing the main survey task, participants completed a preliminary questionnaire (Figure 16 in Appendix E) that requested demographic information and self-reported familiarity with generative AI tools. Below, we first summarize findings from the preliminary survey, followed by a detailed description of the main survey tasks completed by the participants."}, {"title": "3.3 Pre-Survey: Demographics", "content": "30.9% of the participants in our study are in the age group of 35-44 years, followed by 26.5% in 25-34 years and 20.4% in 18-24 years. A small number of remaining participants are 45 years or older. 15.65% participants reported having an arts background or education, indicating a certain level of expertise or familiarity with creative fields, while the remaining 84. 35% did not have such a background, representing a broader range of occupations and experiences."}, {"title": "3.4 Pre-Survey: Familiarity with Generative AI", "content": "We separated the level of familiarity of our participants with various generative AI tools into four distinct categories, namely, familiarity with the text, image, audio, and video generative AI tools (Figure 2). The predominant level of familiarity for text and audio tools is \"Slightly Familiar,\" (41.7% for text and 32.6% for audio), suggesting a moderate acquaintance with these technologies. For image generation tools, the results indicate an evenly spread level of familiarity, with both \u201cSlightly Familiar\" and \"Somewhat Familiar\" categories capturing the largest proportions at roughly 35.0% each. For video generation tools, both \"Not At All Familiar\" and \"Slightly Familiar\" categories capture the largest proportions at 43.0% and 34.3%, respectively. \u201cVery Familiar\" holds smaller shares across video and audio tools (4.8% for audio and 7.0% for video), while for image and text generation tools \"Not at All Familiar\" remains the least represented (4.8% for text and 16.1% for image). This distribution demonstrates that our participants are moderately familiar with a broad range of generative AI tools."}, {"title": "3.5 Parts I-III: Prompt Inference", "content": "Our pool of participants, both recruited on campus and through MechTurk, participated in the main survey, comprised of three sequentially ordered parts (or phases), where in each part participants were tasked with either selecting or typing-in the most appropriate prompt (comprising of a subject and modifiers) for a series of AI-generated images. In Parts I and III, a selection of 100 images using a controlled set of subjects and modifiers, each methodically produced using the four image generation models outlined in Section 3.2, were displayed to the participants. Each of these models contributed an equal share of images to the controlled dataset. Consistent exposure to all subjects and modifiers in the controlled set was maintained by appropriately varying the subset of images each of the participants were exposed to.\nIn contrast, Part II expanded the scope of inference by using 100 images from an uncontrolled dataset. These images were generated from popular prompts found on online prompt sharing websites, offering a wide array of subjects and modifiers. This uncontrolled set also used the same four image generation models. Detailed information on the subjects and modifiers selected for the controlled dataset and the composition of the uncontrolled dataset can be found in Section 3.1."}, {"title": "Part I: Subject and Modifier Selection in Controlled Dataset", "content": "In this part, participants were presented with a sequence of five AI-generated images with controlled subjects and modifiers (i.e, from the controlled dataset). For each displayed image, they were required to first select the correct subject from the options provided. Next, they had to choose the correct modifier(s) from the given options (between 1 to 5 checkbox options), with the possibility of selecting multiple modifiers if preferred. For each question, there will be one correct subject, with the rest of the options filled in randomly; at least one and at most five correct modifiers. For questions where there are less than five correct modifiers, the remaining modifiers are filled in randomly. The goal of this task was to measure the participants' ability to discern and match the given images to their corresponding prompts accurately."}, {"title": "Part II: Prompt Creation in Uncontrolled Dataset", "content": "This second part challenged participants with another five AI-generated images, but this time the prompts were uncontrolled, i.e., no pre-selected options were provided for both subjects and modifiers. In this task, for each image participants had to type what they believed to be the most fitting subject and modifier(s) as a complete sentence. This open-ended task assessed their creative inference abilities and how they translated their interpretation of the images into descriptive prompts."}, {"title": "Part III: Prompt Creation in Controlled Dataset", "content": "In this final task, participants were shown yet another set of five AI-generated images. Similar to Part II, they were instructed to generate sentence-like prompts for these images. However, in Part III the images were again based on controlled subjects and modifiers (i.e., from the controlled dataset). The responses of the participants were then compared with the actual prompts and images displayed to them, providing a measure of how well the participants could infer and replicate the structured prompts that were initially used to generate the images displayed."}, {"title": "3.6 Part IV: Rating Similarity", "content": "In addition to parts I through III, a subset (all of our 171 MTurk participants) were asked to take an additional part. In this last part (Part IV) of the study, participants rated the similarity between pairs of images, one shown to prior participants and one generated using their (prior participants') prompts. Specifically, participants were presented with pairs of images. Each pair consisted of an original image from Part III and a newly generated image based on a prompt submitted by one of the first 25 (on campus) participants.\nThe participants' task for each image pair was to assess and rate the similarity between the two images. They were provided with a Likert scale ranging from \"Not at All Similar\" to \"Very Similar\u201d. This allowed participants to express their perceived degree of similarity, taking into account factors such as color scheme, composition, and subject representation. After making their selection, they would proceed to the next comparison until they had evaluated all five pairs assigned to them."}, {"title": "3.7 Survey Responses", "content": "In total, 230 on campus and MTurk participants took our survey between December 2023 and March 2024. The responses collected consists of subject and modifier selections from a fixed set of options in Part I, and whole prompts in Parts II and III of the survey. After completion of the survey and elimination of invalid responses, we recorded 1078 responses for Part I, 1145 responses for Part II, and 1141 responses for Part III. In addition, the participants who were selected for Part IV of our study gave us 855 responses. In Parts II and III, responses that were incoherent or irrelevant, such as the failure to specify at least one subject or modifier, were also excluded."}, {"title": "4 Experimental Setup", "content": "Next, we define the metrics that we employ to assess the prompt inference accuracy of the survey participants in our experiments. By means of these metrics, we aim to compare and compute the discrepancy between the original prompts and participants' responses (inferred prompts), and between the original image (displayed to the participants) and images generated from their (inferred) prompts. A detailed description of these metrics can be found in Appendix D. These metrics provide a comprehensive assessment of both objective (e.g., image hash, perceptual similarity) and subjective (e.g., surveyed similarity ratings) aspects, ensuring a well-rounded evaluation of the participants' performance in the study."}, {"title": "4.1 Metrics", "content": "MSQ Scores (Survey Part I). Score between 0 (no correct selection) to 2 (all correct selections) for each question to evaluate multiple selection questions (MSQs).\nImage Hash. Hamming distance between the hashes of two images. A lower image hash score (difference) implies that the two images are similar, and vice versa.\nPerceptual Similarity (Zhang et al. 2018). A lower perceptual similarity score implies that the two images are similar, and vice versa.\nImage Embedding Similarity (CLIP Score) (Wang, Chan, and Loy 2023). CLIP score between an image and a prompt (text) where a higher score indicates greater relevance or similarity. We employ two state-of-the-art CLIP models in our evaluation, OpenAI's ViT-L/14 Transformer and ViT-B/32 Transformer.\nText Embedding Similarity (Semantic Similarity) (Reimers and Gurevych 2019). A higher semantic similarity implies the two prompts are similar, and vice versa.\nSurveyed Similarity Rating (from Survey Part IV). Participants in Part IV of the study rated pairs of images on a Likert scale consisting of \"Not at All Similar\", \"Slightly Similar\", \"Somewhat Similar\", and \"Very Similar.\""}, {"title": "4.2 Quantifying Successful Inferences", "content": "Given our study's focus on assessing the accuracy of human prompt inference is ultimately in understanding their ability to generate images resembling those presented to them, we now establish a quantifiable measure of success threshold based on the above metrics, specifically image hash, perceptual similarity, and CLIP scores. Considering the inherent variability in txt2img generations for identical prompts, achieving perfect scores (1 for CLIP score, 0 for image hash, and perceptual similarity) is implausible even with completely accurate prompt inference.\nTherefore, a more appropriate threshold for gauging successful inference involves analyzing the range of scores for images generated from the same prompt across different instances. Accordingly, to determine an effective success threshold, we assessed the scores generated from identical prompts across multiple image creations, using our 100 controlled dataset prompts (Appendix E.6). For each of these prompts, we generated two different images using SDXL, and calculated the image hash, perceptual similarity, and B32 and L14 CLIP scores. After calculating the averages of these scores, we determined the success thresholds, denoted by $\\Theta$, as follows: $\\Theta_{hash}$ = 26.83 for image hash, $\\Theta_{ps}$ = 0.575 for perceptual similarity, and $\\Theta_{B32}$ = 0.875 and $\\Theta_{L14}$ = 0.851 for B32 and L14 CLIP scores, respectively. Participants whose inferred prompts are able to generate images close to or better than these thresholds can be deemed successful in their inference."}, {"title": "4.3 Human-AI Combined Inference", "content": "We now detail our experimentation on the effectiveness of combining human inferred prompts with those produced by AI models towards accurately recreating AI-generated art. Specifically, we utilize prompt responses collected in Part III of our survey, which involved controlled dataset prompts, and AI-generated prompts obtained through the CLIP Interrogator, a model that analyzes an image and generates descriptive text prompts by leveraging OpenAI's CLIP model to match images and text representations (CLIP Interrogator). These human and AI prompt pairs are then consolidated into one combined prompt for comparative evaluation against only human inferred prompt, CLIP Interrogator prompt, and the success threshold, in Section 5.3.\nTo construct a combined prompt from each pair of human prompt and corresponding CLIP Interrogator prompt, we employ a large language model such as GPT-4 (OpenAI 2023), and instruct it to merge the two prompts into a succinct new prompt of up to 25 words without adding any extraneous information. An example of this process is illustrated below, where the first prompt was from a participant and the second prompt was generated by CLIP Interrogator:\nInstruction: Combine these two prompts into a new prompt of 25 words, without any extra information added:\n1. man dressed in steampunk in a steampunk factory\n2. a man in a steampunk suit and top hat standing in front of a giant clock with gears, Bastien L. Deharme, steampunk, a character portrait, fantasy art\nGPT-4 Response: a man in a steampunk suit and top hat stands in a factory, surrounded by giant clocks and gears, embodying a fantasy art portrait.\nThe combined prompts were capped at 25 words to prevent GPT-4 from inadvertently introducing additional keywords when given unrestricted length. Employing a large language model was considered more suitable than merely concatenating the two prompts to avoid repeating subjects and modifiers. Such repetitions could skew the txt2img generations by inadvertently emphasizing repeated subjects and modifiers over others."}, {"title": "5 Results and Analysis", "content": "In Figure 3, the y-axis illustrates the Part I MSQ score distributions over the specified categories (models, subjects, and demographic), reflecting participants' ability to match AI-generated images with their corresponding subjects and modifiers where a higher score indicate a more accurate prompt matching. As seen in Figure 3a, Midjourney-generated images showed a relatively high score concentration towards the upper end, with a median score of 1.5 and an interquartile range (IQR) ranging from 1.33 to 1.9. Realistic Vision 5's generations demonstrated a median of 1.38 and an IQR from 1.2 to 1.6. DreamShaper XL's images displayed a median of 1.49 and an IQR from 1.28 to 1.75, suggesting slightly more consistency than Realistic Vision 5. Stable Diffusion XL matched Midjourney in median score but had a narrower IQR from 1.3 to 1.67.\nAnalyzing scores by subject in Figure 3b, cat-themed images outperformed others with a median of 1.5 and an IQR from 1.33 to 1.84, while astronaut images lagged behind with a median of 1.33 and an IQR from 1.23 to 1.58. Next, dissecting the impact of an art background, Figure 3c shows that participants with art-related education or employment achieved slightly higher median scores at 1.5 and an IQR of 1.32 to 1.61 compared to those without, who had a median of 1.48 and a similar IQR but with a broader range and more outliers. Participant demographics split by recruitment source, shown in Figure 3d, highlight differences in scoring patterns. MTurk workers registered a median score of 1.48 and an IQR of 1.31 to 1.6, while university-recruited participants posted a slightly higher median of 1.54 with an IQR from 1.42 to 1.67. Notably, the MTurk group exhibited a broader score range and more outliers, indicating variances in scoring behavior between the two groups. These results broadly imply that when given with options, participants accurately identified subjects and modifiers in AI-generated images, achieving high scores across different models."}, {"title": "5.1 Subject and Modifier Selection Evaluation", "content": "We next utilize metrics such as the image hash, perceptual similarity, CLIP score, semantic similarity, and survey similarity ratings (outlined in Section 4.1) as applicable, to evaluate various factors affecting human inference accuracy. These metrics have ranges of 0 to 1 for CLIP score, semantic similarity, and perceptual similarity; and 0 to 64 for image hash score.\nBoth the uncontrolled and controlled datasets (from Parts II and III, respectively) serve as our primary data sources for this analysis where we compare the images generated from prompts submitted by the participants using our four selected txt2img models. These images are compared to the original image from the corresponding correct prompt using the the similarity metrics.\nFigure 4 shows the distribution of semantic similarity and CLIP scores obtained for the participant responses for survey Parts II and III. For the controlled dataset (Figure 4a), semantic similarity (a higher value represents a higher similarity between the original and the participant inferred prompt) reveals that the L14 model falls short of the B32 model's performance, with a notable difference in median scores (25.687% lower on average for L14). This pattern persists in the CLIP scores (where a higher value represents a higher similarity between the original and the inferred prompt generated image), where B32 outperforms L14, emphasizing the latter's lower median by 5.294% on average. Among the txt2img models, Midjourney stands out for its superior semantic similarity and CLIP scores, indicating a closer match to the inferred prompts, whereas Realistic Vision 5 consistently records the lowest score ranges across these metrics.\nIn the uncontrolled dataset (Figure 4b), we observe a similar trend, with the L14 model lagging behind the B32 model in both semantic similarity (the median is 22.06% lower on average) and CLIP scores (the median 5.99% lower on average) (Figure 4b). The majority of the CLIP scores observed fell below the success thresholds (dashed lines), $\\Theta_{L14}$ = 0.851 and $\\Theta_{B32}$ = 0.875, as denoted by the red and green dashed lines respectively in Figure 4. However, it is noteworthy that 162 images from the Midjourney model out of 2, 286 total images were able to surpass these thresholds under B32 across both controlled and uncontrolled datasets.\nThe perceptual similarity and image hash scores, as shown in Figure 5, reveal the influence of dataset conditions (controlled vs. uncontrolled) on image generation in a somewhat counter-intuitive manner. Under controlled conditions, the hash scores are higher, with the median hash score across all models being on average 1.185% higher than that for the uncontrolled dataset (note that a lower image hash scores indicate greater similarity). Similarly, the median perceptual similarity is higher, although only marginally, at 0.329% on average compared to the uncontrolled dataset (note that a lower perceptual similarity score is more desirable, i.e., greater similarity). These findings indicate that a more structured approach to prompt inference results in the generation of images that considerably deviate from the original images compared to uncontrolled prompt inference.\nIn terms of success thresholds, a pattern similar to CLIP scores was observed in the image hash and perceptual similarity evaluations. The success thresholds are depicted in Section 5.2 and Section 5.2 by red dashed lines. Surveyed similarity ratings (Figure 6) provide direct insights into the perceived accuracy of images generated from human-inferred prompts. DreamShaper XL notably receives a majority of \"Somewhat Similar\u201d ratings (87), along with the highest count of \"Very Similar\" ratings at 75, suggesting that images generated with this model are likely to be more similar to the corresponding original image.\nThese analyses reveals how different txt2img models perform while interpreting prompts and corresponding image generations, emphasizing the challenges in reproducing AI art through prompt inference that aligns with human interpretations. Despite certain minor favorable observations, the overall performance across all three types of metrics did not reach the success thresholds, indicating the challenging nature in human prompt inferences."}, {"title": "5.2 Human Inference Evaluation", "content": "We next investigate the inference accuracy for different subjects in prompts using the controlled dataset. Figure 7 illustrates the semantic similarity and CLIP scores (where higher scores indicate a greater match), for five selected subjects. It is observed that the L14 model typically scores lower compared to the B32 model in both metrics. Subjects such as robots and cats achieve high CLIP scores, suggesting a strong alignment with their respective prompts. This observation prompts an inquiry into the nature of the subjects' representations: are they inherently distinct, consistently depicted by the models, or is it that humans are inherently better at inferring prompts related to these subjects? Inherent distinctness would imply that robots and cats possess unique, identifiable features that are readily recognized by the B32 and L14 models. Consistent depiction, on the other hand, would result in uniformity in representing these subjects across various image generations. Meanwhile, if humans are inherently better at inferring prompts for these particular subjects, this could also contribute to the observed accuracy. While cats may not always meet the desired benchmarks, in the top 25% of cases, their inference performance reaches the success threshold, with 66 inference instances above $\\Theta_{B32}$ and 61 instances above $\\Theta_{L14}$. Except for man and woman, all subjects exhibit at least 30 instances surpassing the thresholds.\nPerceptual similarity and image hash scores, as depicted in Figure 8, indicate a consistent representation across subjects, although cats show slightly higher scores, hinting at a less consistent inference process. For the image hash metric, only 30 to 40 instances for each subject show scores below the success threshold, indicating that lower scores, which signify better accuracy, are not frequently achieved."}, {"title": "5.3 Human-AI Combined Inference", "content": "We next examine the effectiveness of combining human inferences with AI inferences, particularly using CLIP interrogator for inference and then merging them using GPT-4 (as outlined in Section 4.3), towards accurately inferring prompts for AI-generated images. CLIP score analysis shows that combination of humans and AI inferred prompts generally results in higher image CLIP scores (Figure 11). Specifically, the B32 scores were found to be 3.126% higher on average in a combined setting compared to individual efforts. However, when compared to the images generated by prompts inferred by CLIP Interrogator, the B32 score saw a 0.23% decrease on average, due to the Realistic Vision 5-generated images performing worse; although Midjourney-generated images performed 1.265% better, and SDXL images with a 1.099% increase in performance (Figure 11a). With that being said, the overall improvement highlights how combining human inference with AI inference can lead to slightly better accuracy in matching prompts with generated images. The overlap in scores across different txt2img models suggests that while human-AI combined prompts enhances accuracy, the degree of improvement may not be drastically distinct among different AI models.\nPerceptual similarity and image hash score (Figure 12) also indicate that images generated through human-AI combined prompts align more closely with target images, as evident from the slightly lower average hash scores (by 1.417%) and perceptual similarity scores (by 0.905%). Figures 13 and 14 further explores this combined prompt inference, showcasing that human-AI combined prompts not only consistently improves inference across various subjects but also indicates a nuanced enhancement in how specific subjects and modifiers are interpreted and matched (hash score median is on average 1.438% lower, while perceptual similarity median is on average 1.431% lower).\nFor all 4 types of metrics, combining AI inferred prompts through CLIP interrogator with human inferred prompt has a positive effect on the overall accuracy. While the human-AI combined prompts still did not reach the respective metrics' success thresholds, its performance is slightly better than that of purely human inferred prompts. This suggests that accurate prompt inference by a human would benefit from combination with Al inference.\nThe enhanced accuracy in prompt inference through the combination of human and AI efforts can be attributed to the complementary skills and knowledge each brings. Humans excel in creative and contextual understanding, while AI provides extensive data analysis and pattern recognition capabilities. This combination allows for error reduction, iterative refinement, and a broader knowledge base, leading to more accurate prompts. Despite these benefits, the fact that performance did not reach the success thresholds suggests that further optimizations in human-AI interaction are required.\nIn summary, the empirical evidence gathered from the evaluation, shows that while humans alone may struggle with prompt inference, their efforts, when combined with AI, demonstrate a modest improvement. The findings suggest there's room for growth in human-AI collaborations, which could eventually enhance the efficacy of prompt reconstruction. Until such advancements are realized, prompt marketplaces maintain their practicality as a business model in the realm of AI-generated art."}, {"title": "Conclusion", "content": "The study presented in this paper explores the intellectual property"}]}