{"title": "TIVAT: JOINT-AXIS ATTENTION FOR TIME SERIES\nFORECASTING WITH LEAD-LAG DYNAMICS", "authors": ["Junwoo Ha", "Hyukjae Kwon", "Sungsoo Kim", "Kisu Lee", "Ha Young Kim"], "abstract": "Multivariate time series (MTS) forecasting plays a crucial role in various real-\nworld applications, yet simultaneously capturing both temporal and inter-variable\ndependencies remains a challenge. Conventional Channel-Dependent (CD) mod-\nels handle these dependencies separately, limiting their ability to model complex\ninteractions such as lead-lag dynamics. To address these limitations, we propose\nTiVaT (Time-Variable Transformer), a novel architecture that integrates temporal\nand variate dependencies through its Joint-Axis (JA) attention mechanism. Ti-\nVaT's ability to capture intricate variate-temporal dependencies, including asyn-\nchronous interactions, is further enhanced by the incorporation of Distance-aware\nTime-Variable (DTV) Sampling, which reduces noise and improves accuracy\nthrough a learned 2D map that focuses on key interactions. TiVaT effectively mod-\nels both temporal and variate dependencies, consistently delivering strong perfor-\nmance across diverse datasets. Notably, it excels in capturing complex patterns\nwithin multivariate time series, enabling it to surpass or remain competitive with\nstate-of-the-art methods. This positions TiVaT as a new benchmark in MTS fore-\ncasting, particularly in handling datasets characterized by intricate and challeng-\ning dependencies.", "sections": [{"title": "1 INTRODUCTION", "content": "Multivariate time series (MTS) forecasting a pivotal role in numerous real-world applications, in-\ncluding finance, climate modeling, traffic management, and energy demand forecasting(Lu & Xu,\n2024; Nguyen et al., 2023; Jin et al., 2023; Yuan et al., 2023). With advancements of deep learning,\nmodels developed based on MLP(Oreshkin et al., 2019; Zeng et al., 2023; Challu et al., 2023; Li\net al., 2023), RNN (Lai et al., 2018; Qin et al., 2017; Salinas et al., 2020), CNN (Wu et al., 2022;\nLuo & Wang, 2024; Wang et al., 2023), and Transformers (Zhou et al., 2021; Wu et al., 2021; Zhou\net al., 2022; Nie et al., 2022) have significantly improved long-term temporal dependency modeling.\nHowever, handling both temporal and inter-variable dependencies in MTS remains a challenge.\nMTS models are typically classified as either Channel-Independent (CI) or Channel-Dependent (CD)\nbased on how they handle inter-variable relationships. CI models process variables independently,\nwhich makes them resilient to noise and overfitting but neglects crucial inter-variable dependen-\ncies required for complex datasets. Recent CD models, such as iTransformer (Liu et al., 2023) and\nCARD (Wang et al., 2024b), use Transformer architectures to model these dependencies, improving\npredictive accuracy. However, these CD models handle temporal and inter-variable dependencies\nseparately, limiting their ability to capture more complex interactions between variables and tempo-\nral dynamics, such as lead-lag relationships.\nThe main reason CD models struggle to address both temporal and inter-variable dependencies si-\nmultaneously is the significant increase in computational cost and model complexity. Joint mod-\neling of all variate-temporal dependencies in high-dimensional datasets becomes computationally"}, {"title": "2 RELATED WORKS", "content": "MTS forecasting requires models to capture both temporal dependencies and inter-variable relation-\nships. CI models focus on processing each variable independently, primarily addressing temporal\ndependencies while ignoring interactions between variables. This independence reduces the risk of\noverfitting and makes these models more resilient to noise, but it also limits their ability to cap-\nture complex cross-variable dependencies, which are critical in multivariate datasets. or example,\nN-BEATS (Oreshkin et al., 2019) models each variable separately by decomposing time series into\ntrend and seasonal components, while DLinear (Zeng et al., 2023) applies linear transformations in-\ndependently across variables, optimizing for computational efficiency. Similarly, TimeMixer (Wang\net al., 2024a), an MLP-based model, processes temporal and feature dimensions sequentially through\nindependent MLP layers. While TimeMixer enhances scalability by simplifying model complexity,\nits lack of explicit attention to inter-variable dependencies limits its effectiveness in scenarios where\nsuch relationships significantly impact forecasting performance. Although these CI models are com-\nputationally efficient, their failure to consider inter-variable dependencies can lead to suboptimal\nresults in datasets where variable interactions play a crucial role.\nTo address the limitations of CI models, CD models capture inter-variable dependencies, which\nimproves forecasting performance when variable interactions play a critical role. CD models can\nbe broadly divided into two types based on the architectures they employ: non-Transformer-based\nmodels and Transformer-based models.\nNon-Transformer-based models include those that utilize GNN or CNN to capture inter-variable de-\npendencies. Likewise, MTGNN (Wu et al., 2020) combines GNNs and convolutions to capture vari-\nable correlations but faces similar limitations in its ability to integrate temporal dependencies with\ninter-variable interactions. Additionally, TimesNet (Wu et al., 2022) offers an alternative approach\nby using a 2D tensor transformation to model both temporal and variate dependencies simultane-\nously. However, TimesNet relies on tensor operations rather than attention mechanisms, limiting its\nability to efficiently handle long-range dependencies across high-dimensional datasets. ModernTCN\n(Luo & Wang, 2024) leverages a temporal convolutional network (TCN) architecture, using dilated\nconvolutions to capture variable interactions across multiple time steps. While this structure is ef-\nfective for modeling short- to medium-range dependencies, it treats temporal and inter-variable de-\npendencies separately. This separation limits its ability to model dynamic inter-variable interactions\nover time, particularly in datasets where long-range or asynchronous interactions are critical.\nOn the other hand, Transformer-based models leverage the self-attention mechanism, which allows\nthem to capture long-range dependencies across both time steps and variables. For instance, Informer\n(Zhou et al., 2021) improves the efficiency of self-attention through sparse attention, allowing it\nto handle large-scale time series data. Fedformer (Zhou et al.,\n2022) and Autoformer (Wu et al.,\n2021) further enhance the model's ability to capture long-term temporal patterns by decomposing\nthe time series into trend and seasonal components. However, despite these advancements, most\nTransformer-based models, including CrossFormer (Zhang & Yan, 2023), iTransformer (Liu et al.,\n2023), and CARD (Zhao & Shen, 2024), still treat temporal and variate dependencies separately.\nWhile these models are effective at modeling inter-variable relationships, their separate treatment\nof temporal dependencies limits their capacity to fully capture complex interactions like lead-lag\ndynamics, where variables influence each other asynchronously across time steps.\nTiVaT addresses the limitations of both non-Transformer-based and Transformer-based CD models\nby introducing a JA attention mechanism that simultaneously captures both temporal and variate\ndependencies. Unlike previous models that treat these dependencies independently, TiVaT's unified\nattention framework allows it to model intricate variate-temporal interactions, including lead-lag\ndynamics. Through this integrated approach, TiVaT offers a comprehensive and efficient solution\nfor MTS forecasting, addressing the limitations of existing CI and CD models and significantly\nimproving forecasting performance in complex multivariate datasets."}, {"title": "3 METHODOLOGY", "content": "MTS forecasting is a task that predicts future values for each variate leveraging historical data.\nFormally, given past data $X = {XT-LH+1,\u2026\u2026,XT} \\in R^{LH \\times V}$ with V variates and LH time steps\nfor a time point T, the objective is to predict future data $Y = {XT+1,..., XT+LF} \\in R^{LF\\times V}$"}, {"title": "3.1 STRUCTURE OVERVIEW", "content": "Fig. 2 describes overview of our proposed TiVaT. The model is designed to effectively capture com-\nplex cross-axiss across both variate and temporal axes simultaneously through joint-axis attention.\nTime series data can be decomposed into trend and seasonality components, each of which has dis-\ntinct characteristics that represent long- and short-term patterns, respectively (Cleveland et al., 1990;\nWang et al., 2024a). Thus, we first apply the seasonality-trend (ST) decomposition method to the\nnormalized time series input data. This simplifies complex cross-axiss in the raw time series to those\nin long- and short-term patterns, enabling more effective identification of the relationships.\nWe decompose the given input sequence for each variate into two components: the moving average,\nwhich represents the trend $XTrend \\in R^{LH\\timesV}$, and the remainder, which is treated as seasonality\n$XSeasonality \\in R^{LH\\timesV}$. To preserve the temporal information and enrich its pattern information, we\nthen pass each component through a separate linear layer $Linear(\u00b7)$ with the residual connections,\nas following Eq:\n$XTrend = MA(X)$\n$XSeasonality = X \u2013 XTrend$\n$XTrend = XTrend + Linear(XTrend)$\n$XSeasonality = XSeasonality + Linear(XSeasonality),$\nwhere MA represents the moving average for the temporal axis for each variate.\nThe decomposed components $XTrend$ and $XSeasonality$ are processed through separate sibling ar-\nchitectures to reduce confusion arising from the difference of long-term and short-term properties.\nThe architectures consist of patching, embedding, JA attention blocks, and projector. We adopt\npatching and embedding (Nie et al., 2022; Zhang & Yan, 2023; Wang et al., 2024b; Cao et al.,\n2023) to reduce the computational burden caused by long temporal sequences and to extract local\nsemantic information. When each component is patched with a patch length of $L_p$ and a stride of S\nalong the temporal axis, the input length $L_H$ is reduced to $L_N = \\lfloor \\frac{L_H}{L_p=1}+1\\rfloor$ and a new dimension\ncorresponding to the patch length is introduced, resulting in the patched input $X_p \\in R^{LN\\timesV\\timesLP}$.\nSubsequently, we feed the patched input to an embedding layer $E : R^{LN\\timesV\\timesLP} \\rightarrow R^{LN\\timesV\\timesD}$ to\nmake input tokens $Z\\in R^{LN\\timesV\\timesD}$ for Transformer-based architecture. TiVaT learns the relation-\nships among the input tokens with Transformer encoder architecture, and it outputs the predictions"}, {"title": "3.2 JOINT-AXIS ATTENTION MODULE", "content": "for trend and seasonality components through a linear layer-based projector $Proj : R^{LN\\timesV\\timesD} \\rightarrow\nRLF\\timesV$. The final prediction $\u00dd \\in R^{LF\\timesV}$ is obtained by aggregating each prediction with an\nelement-wise sum, as follows:\n$\u00dd = Proj(Enc(E(XTrend) + PE)) + Proj(Enc(E(XSeasonality) + PE)),$\nwhere Enc and PE present the Transformer encoder blocks and positional encoding, respectively.\nThe projectors for each component include a flattening to extract predictions per variate.\nA core component of TiVaT is the proposed JA attention module. The JA attention block is a Trans-\nformer encoder block structure, replacing self-attention with the JA attention module. Inspired by\nthe deformable attention module (Zhu et al., 2020), the JA attention module is designed to capture\nrelationships between a feature vector $Z(t,v)$ and other feature vectors $Z(t',v')$, where $t \u2260 t', v \u2260 \u03c5'$,\nor both. Unlike the deformable attention, the JA attention module uses offset points as guidelines\nto minimize information loss and uses the DTV sampling to capture relationships with other points\nthat are highly relevant. This results in overcoming the limitation that existing self-attention-based\nmodels can only grasp the relationship along a single axis (temporal $Z(:,v)$ or variate $Z(t,:)$). Fur-\nthermore, it is efficient compared to full attention of a data point to entire other points, as it avoids\nprocessing less relevant points. The following subsection provides a detailed explanation of the tech-\nnical implementation of the JA attention module, including how offset points are calculated and how\nDTV sampling is performed.\nThe deformable attention module was introduced to tackle the inefficiencies of self-attention opera-\ntions in the computer vision domain. This module extracts offset points based on the query feature\nq(t,v) at the reference point (t, v) on a feature map, and the attention operation is performed ac-\ncordingly. Since the offsets are extracted simultaneously along both the width and height axes of\nthe three-dimensional feature map, the module can efficiently consider all axes while also offering\ncomputational efficiency compared to self-attention at every location.\nHowever, in time series data, each data point can influence predictive performance due to temporal\ndependencies. As a result, relying solely on the extracted offset points to reflect the information re-\nlated to each reference point may lead to information loss. Thus, our JA attention module is designed\nto prevent information loss in time series data while preserving the deformable attention's property,\nenabling simultaneous consideration of both axes.\nAs shown in Fig. 3, an input tensor for JA attention is the patched and embedded time series\n$Z\\in R^{LN \\timesV\\times D}$. Here, when we map each dimension of the input to the computer vision domain, the\ntemporal and variable axes may serve as width and height axes in terms of the feature map, respec-\ntively. Furthermore, the offset points, which are extracted considering both axes in the deformable\nattention, can indentify relationships between the reference point and the other data points across\nboth axes. Thus, the JA attention leverages the offset point-based attention to efficiently capture\ninformation across temporal and variate axes simultaneously.\nUnlike traditional deformable attention, we use the offset points solely as guidelines for selecting\ncandidates for attention operations with each reference point. Relying exclusively on these offset\npoints may not fully capture the complex relationships and interactions with their reference point, as"}, {"title": null, "content": "only a limited number of feature vectors are included in an attention operation without any auxiliary\nconstraints. To mitigate this risk, the JA attention module incorporates feature vectors from the\nguidelines, ensuring that the attention mechanism considers a broader range of information beyond\njust the offset points. Furthermore, the DTV sampling in the JA attention module improves efficiency\nin the attention by selecting more relevant feature vectors in the guideline based on their semantic\nsignificance.\nIn the JA attention module, for each reference point (t, v), the query feature q(t,v) is a D-dimensional\nfeature vector at the corresponding point in the input feature map Z. The temporal and variate offset\npoints, At and \u2206, are extracted by passing the query feature through their respective linear layers.\n\u2206t and A are initially set to unconstrained real numbers and then normalized into their respective\ntemporal and variate ranges. This process ensures that the offset points cover the entire area of the\nfeature map. After the normalization, we use nearest interpolation to convert the continuous offset\npoints to their discrete counterparts. These offset points serve as guidelines to construct the cross-\naxis pool, which consist of candidate features for attention operation with the query feature. In other\nwords, if a temporal offset \u2206t is determined for the query feature q(t,v), all variate feature vectors\nZ(t++,:) at the time of t + At are included in the cross-axis pool. Similarly, if a variate offset \u0394\u03c5\nis determined, all temporal feature vectors Z(:,v+\u25b3\u300f) for the variate v + \u0394\u03c5 are added to the cross-\naxis pool. As relevant information differs depending on the number of variates or patch length, we\ndetermine the number of At and \u2206 as hyperparameters, defined as a percentage of the number of\nelements on each axis.\nIn addition, using the entire set of feature vectors as key and value features in the attention mecha-\nnism can significantly increase computational complexity compared to traditional attention modules,\nas the attention operation scales quadratically with the number of input elements. To address this, we\napply our DTV sampling based on a top K sampling method to the feature vectors. This DTV sam-\npling uses Euclidean distance in the two-dimensional (2D) embedding space to ensure interpretabil-\nity, selecting features that are closely related to the query. By measuring proximity, it identifies the\nmost relevant points, providing a clear rationale for the selection of meaningful features in the atten-\ntion mechanism. Moreover, this sampling technique helps reduce noise by excluding less relevant or\nunrelated feature vectors, which not only improves computational efficiency but also enhances the\nquality of the attention mechanism by focusing on the most significant features.\nFormally, our sampling method first projects the query feature q(t,v) and the other features Z(t',v')\nfrom the cross-axis pool associated with the query into a 2D embedding space, resulting in $qt,v)$\nand $Z2\\_(t', v')$, respectively. Subsequently, the indices $I_q$ of the relevant points in the cross-axis pool\nare determined based on the Euclidean distance, denoted as Dist, as in Eq. 3. The relevant feature\nvectors $R_q \\in R^{K\\timesD}$, corresponding to the selected indices $I_q$ in the cross-axis pool for their query\nfeature q(t,v), are concatenated and used as key and value features in the attention operation.\n$I_q = arg\\_ TopK(Dist(qt,v), Z2\\_(t', v'))).$\nMeanwhile, in MTS analysis, the value of a reference point (t, v) is often considered to be most\nrelated to the values of all time steps of the same variate (Z(:,2)) and the values of all variates at\nthe same time step (Z(t,:)). Thus, we construct an additional pool, we call it the self-axis pool, to\nincorporate this inductive bias during training our model. The self-axis pool consists of Z(:,2) and\nZ(t,:), and this pool is used in the same way with the offset points-based pools. As in the offset\npoints-based pools, the DTV sampling is also leveraged for the self-axis pool, and we denote the\nrelevant feature vectors in the self-axis pool as $Rself \\in R^{KX D}$.\nHere, we use the same K value for both the cross-axis pool and the self-axis pool in the sampling\nprocess. This setup introduces an inductive bias that emphasizes the importance of values in the\nself-axis (temporal and variate relationships within the same reference point) in the MTS domain.\nBy maintaining a consistent proportion of relevant points in the self-axis, we improve the model's\nability to effectively incorporate these values into subsequent attention operations. This inductive\nbias is essential, as it allows the model to better capture inherent temporal dependencies and inter-\nvariate correlations, which are critical for each reference point.\nFinally, the relevant feature vectors Rq and Rself are concatenated and integrated into their query\nfeature q(t,v) using a cross-attention layer. First, the linear projections generate query Q, key K, and"}, {"title": null, "content": "value V, according to the following Equation:\n$Q = Proj^{q}(q(t,v)) \\in R^{1\\timesD}$\n$K = Projk (Concat(Rq, Rself)) \\in R^{2K \\times D}$\n$V = Proj^{v}(Concat(Rq, Rself)) \\in [R^{2K\\timesD,}$\nwhere Concat represents a concatentation layer and $Proj^{i}$ for $i \\in {q, k, v}$ are the linear projec-\ntions for query, key, and value, respectively.\nSubsequently, the query feature q(t,v) incorporates relevant information from various points using\nthe scaled dot product attention mechanism. In this process, attention weights are computed between\nthe query and the concatenated key-value features. The updated query feature q(t,v) is then calculated\nas shown in Eq. 5. Specifically, the attention scores are computed by taking the scaled dot product of\nthe query matrix (Q) and the transpose of the key matrix (KT), followed by applying these attention\nweights to the value matrix (V) to update the query feature.\n$q\\_(t,v) = Softmax(QKT)V$\nThis updated query feature q(t,v) incorporates information from lagged points $Z(t',v')$, which exist at\ndifferent time steps ($t' \u2260 t$) and across different variates ($v' \u2260 v$). As a result, each query feature is\nenriched with information from both the temporal and variate axes simultaneously, allowing TiVaT\nto overcome the limitations of traditional attention mechanisms that can only capture information\nalong one axis at a time."}, {"title": "4 EXPERIMENTS", "content": "We conduct a comprehensive set of experiments to assess the performance of TiVaT across a wide\nrange of time series forecasting tasks, illustrating the model's broad applicability. Furthermore, we\nprovide empirical evidence that highlights the efficacy of the proposed JA Attention module, which\nenhances the model's ability to capture intricate dependencies in multivariate time series data. Addi-\ntionally, we demonstrate the effectiveness of the DTV Sampling, which improves the model's focus\non key variate-temporal interactions.\nDataset\nOur experimental evaluation leverages six real-world datasets that are widely used in\ntime-series forecasting research, ensuring a rigorous and thorough comparison with state-of-the-\nart models. These datasets include ECL, ETT (with four subsets), Exchange, Traffic, and Weather,\nwhich are used by Autoformer (Wu et al., 2021) for long-term forecasting. These datasets have\nbeen extensively benchmarked in prior works, including iTransformer (Liu et al., 2023). Detailed\ndescriptions of each dataset and their characteristics are provided in Appendix A.1.\nBaselines We carefully select eight previously successful forecasting models as our benchmarks,\ncategorized into two groups: (1) CD models: iTransformer (Liu et al., 2023), CARD (Wang et al.,\n2024b), CrossFormer (Zhang & Yan, 2023), ModernTCN (Luo & Wang, 2024), TimesNet (Wu et al.,\n2022). (2) CI models: PatchTST (Nie et al., 2022), TimeMixer (Wang et al., 2024a), Dlinear (Zeng\net al., 2023).\nUnified experiment settings For a fair comparison, we compare our model's performance with\nthe results reported in previous studies. In cases where the input sequence length differs, we con-\nduct experiments using the parameters provided by the respective papers to ensure consistency in\nperformance comparison.\nImplementation Details All experiments are conducted using PyTorch (Paszke et al., 2019) on\nmultiple NVIDIA A100 GPUs with 80GB of memory. The model training is performed using L2\nloss. Additionally, the number of reference points is determined by Per\u25b3\u2081, which selects a percentage\nof the time axis, and Per\u25b3, which selects a percentage of the variable axis, ensuring both efficiency\nand optimal performance depending on the dataset."}, {"title": "4.1 MAIN RESULTS", "content": "Based on the results shown in Table 1, with the best performance highlighted in red and the second-\nbest in undelined. TiVaT demonstrates its unique strength in simultaneously capturing temporal and\nvariate dependencies, while effectively accounting for lead-lag relationships, making it particularly\nwell-suited for datasets with complex multivariate interactions. On ETTh1, TiVaT achieves an MSE\nof 0.434, outperforming iTransformer (0.454) and CARD (0.442), underscoring its ability to handle\ncomplex seasonal and trend patterns in energy-related datasets. Across the other ETT datasets, TiVaT\nremains highly competitive, securing a second-best MSE of 0.382 on ETTm1, still surpassing CARD\n(0.383), further showcasing its robustness in structured time series forecasting.\nBeyond the ETT datasets, TiVaT excels on more volatile and dynamic datasets. On Exchange, it\nachieves the best result with an MSE of 0.349, while on ECL, TiVaT leads with an MSE of 0.166,\neffectively capturing intricate lead-lag and inter-variable dynamics. Even on the more complex Traf-\nfic and Weather datasets, TiVaT remains highly competitive, securing second-best results with MSEs\nof 0.430 and 0.240, respectively. These results collectively highlight TiVaT's versatility and adapt-\nability across diverse multivariate time series forecasting tasks, making it a highly competitive model\nfor both structured and complex datasets."}, {"title": "4.2 ANALYSIS", "content": "The JA Attention mechanism is designed to effectively capture the\nunique characteristics of multivariate time series data, where information at specific time points and\nvariables often holds more importance compared to others. By simultaneously addressing temporal\nand variate dependencies, JA Attention efficiently captures complex variate-temporal interactions\nthat traditional models may miss. As shown in Table 2, we conducted an ablation study comparing\nJA Attention with two alternative methods: \u201cFull Attention\" (using the vanilla transformer model\n(Vaswani et al., 2017)) and \u201c2-Stage Attention\" (using the Crossformer encoder(Zhang & Yan,\n2023)). The results clearly demonstrate that JA Attention consistently outperforms both methods\nacross all datasets (ETTh1, ETTm1, ETTm2, Exchange, Weather) in terms of MSE and MAE. By\naccurately capturing variate-temporal dependencies, JA Attention improves predictive performance\nand enables more efficient attention allocation, making it a superior solution for time series forecast-\ning.\nThe results in Table 3 confirm the effectiveness of\nusing offset points as guidelines rather than as direct bases, as in deformable attention mecha-\nnisms (Zhu et al., 2020). This approach is tailored for time series data to capture both temporal and\nvariate dependencies while avoiding information loss. The table compares using points only, guide-\nlines without sampling, and guidelines with sampling. Across datasets (ETTh1, ETTh2, ETTm1,\nETTm2, Exchange, Weather), using points alone results in suboptimal performance due to infor-\nmation loss. For instance, on ETTh1, the MSE remains at 0.383 with points and guidelines without\nsampling, but improves to 0.380 with sampling. Similar trends are seen in the Exchange and Weather\ndatasets, where sampling yields the best MSE results (0.083 and 0.156, respectively). These results"}, {"title": "4.2.2", "content": "We propose DTV Sampling as a method to re-\nduce noise by selecting features that are semantically related to the reference point. To validate its\neffectiveness, we compare it with a Random Sampling method, which selects features randomly\nwithout considering semantic relevance. Additionally, we perform experiments applying different\nsampling methods to each pool to further assess the effectiveness of DTV Sampling. The experi-\nmental results, as shown in Table 4, indicate that when random sampling is applied to both axes, the\noverall performance is suboptimal due to the failure to extract features that are semantically related\nto the reference points. However, when DTV Sampling is applied to a single axis, the performance\nimproves over random sampling, and when applied to both axes, it achieves the best performance.\nThis confirms that DTV Sampling effectively captures features that are semantically aligned with\nthe reference points.\nillustrates a qualitative analysis of the 2D embedding space used for DTV Sampling on the\nETTh1 dataset. All features are projected into this 2D space, and cosine similarity is employed to\nmeasure the semantic relevance between the reference point and other features. The reference point\nis represented in black, with points increasingly similar (higher cosine similarity) displayed in red,\nand less similar points in blue. DTV Sampling selects the top K closest points based on the distance\nbetween the reference point and other features in the 2D embedding space. Points with higher cosine\nsimilarity cluster near the reference point, while those with lower similarity are farther away. These\nresults further demonstrate that DTV Sampling effectively captures semantically relevant features,\nleading to improved performance."}, {"title": null, "content": "Sampling We propose a Separate Sampling technique to better capture im-\nportant information from the self-axis pool, and we validate its effectiveness by comparing three\nmethods: (1) using all features without sampling (w/o sampling), (2) sampling the top 2K features\nby combining the self-axis and cross-axis pools (w/ common sampling), and (3) our proposed Sep-\narate Sampling method (w/ separate sampling). In the w/o sampling method, all features from both\npools are used without filtering, while w/ common sampling combines features from both pools and\nsamples the top 2K. The w/ separate sampling approach samples each pool independently.. Table 5\nshows that w/o sampling performs worse than both sampling methods, confirming the effectiveness\nof DTV Sampling in extracting relevant information. Additionally, the superior performance of the\nproposed w/ separate sampling method compared to w/ common sampling highlights the impor-\ntance of self-axis information. This confirms that separate sampling is better suited to capturing key\ninformation from the self-axis, leading to improved model performance."}, {"title": "5 CONCLUSION", "content": "In this work, we introduced TiVaT, a novel model that addresses the limitations of conventional\nCD models by simultaneously capturing temporal and variate dependencies in multivariate time\nseries forecasting. Through its JA attention mechanism, TiVaT effectively models complex variate-\ntemporal interactions, including lead-lag dynamics, which are often missed by conventional CD\nmodels. The integration of DTV Sampling further enhances the model's performance by reducing\nnoise and improving accuracy, enabling it to focus on key interactions across the temporal and\nvariate axes. Extensive experiments demonstrate that TiVaT consistently outperforms or remains\nhighly competitive with state-of-the-art models across diverse datasets, showcasing its robustness"}, {"title": "A APPENDIX", "content": "We conduct experiments on five real-world datasets to evaluate the performance of the proposed\nTiVaT, including:\nETT (Li et al., 2021): This dataset contains seven factors of electricity transformer data from July\n2016 to July 2018, with four subsets. ETTh1 and ETTh2 are recorded hourly, while ETTm1 and\nETTm2 are recorded every 15 minutes. Exchange (Wu et al., 2021): This dataset collects panel data\nof daily exchange rates from eight countries, covering the period from 1990 to 2016. Weather (Wu\net al., 2021): It includes 21 meteorological factors collected every 10 minutes from the Weather\nStation of the Max Planck Biogeochemistry Institute in 2020. ECL (Wu et al., 2021): This dataset\nrecords hourly electricity consumption data from 321 clients. Traffic (Wu et al., 2021): It collects\nhourly road occupancy rates measured by 862 sensors on San Francisco Bay area freeways from\nJanuary 2015 to December 2016. We follow the data processing steps and train-validation-test split-\nting method outlined in TimesNet (Wu et al., 2023). The datasets for training, validation, and testing\nare strictly separated in chronological order to prevent any risk of data leakage. For the forecasting\nconfigurations, we use a fixed lookback window of 96 time steps across the ETT, Weather, ECL,\nand Traffic datasets, with prediction horizons set to {96, 192, 336, 720}.The details of datasets are\nprovided in Table 6."}, {"title": "A.2 CONFIGURATION", "content": "All experiments were implemented using PyTorch (Paszke et al., 2019) and conducted on a multi-GPU setup consisting of NVIDIA A100 GPUs, each with 80GB of memory. For optimization, we employed the ADAM optimizer (Kingma & Ba, 2015) with an initial learning rate selected from the range {10-3, 5 \u00d7 10-4, 10-4}, and utilized L2 loss as the loss function. The batch size was chosen from {4, 16, 32} across all experimental configurations. The number of Joint Axis Attention Blocks in our model was varied, with values selected from {2, 3, 4}, while the dimension of the series representation was chosen from 128, 256, 512, 1024. Within each Joint Axis Attention Block, the parameter representing the percentage of the temporal axis (Per\u25b3\u2081) and variable (Per\u25b3\u2081) axis was set between 0.1 and 0.8. Additionally, the number of parameters for both the self-axis(Num Roelf)"}, {"title": "A.3 TIME SERIES DECOMPOSITION WITH RESIDUAL CONNECTIONS", "content": "In this part, we highlight the effectiveness of residual connections in time series decomposition.\nAs shown in Table 8, incorporating residual connections after the decomposed components (trend\nand season) are passed through a linear layer leads to improved performance across all datasets.\nSpecifically, applying residual connections to both trend and seasonal components yields the best\nresults, as indicated by the lowest MSE and MAE values. These results suggest that residual con-\nnections enhance the model's ability to capture more intricate temporal patterns by improving the\nrepresentational capacity of the decomposed components, allowing for more accurate forecasting."}, {"title": "A.4 QUALITATIVE RESULTS OF JA ATTENTION'S GUIDELINES.", "content": "When performing an attention operation with a specific time, a specific variable, all other times, and\nall variables, irrelevant information can act as noise. Fig. 5 is a heatmap obtained by performing\nAttention operations with a specific reference point(dark gray box) and all other times and variables.\nSince there has been no previous study that applied vanilla self-attention, we experimented with\nusing vanilla self-attention to extract heatmap. If there is a strong correlation with a specific reference\npoint, it is expressed in red, and if there is no correlation, it is expressed in white. As can be seen\nin Fig. 5, it can be seen that for a specific reference point, there are parts where other variables at\ndifferent times are not important and can act as noise. Additionally, the Guidelines (light gray box)\nwere extracted through TiVaT's JA attention, and it can be seen that the"}]}