{"title": "Belief sharing: a blessing or a curse", "authors": ["Ozan \u00c7atal", "Toon Van de Maele", "Riddhi J. Pitliya", "Mahault Albarracin", "Candice Pattisapu", "Tim Verbelen"], "abstract": "When collaborating with multiple parties, communicating relevant information is of utmost importance to efficiently completing the tasks at hand. Under active inference, communication can be cast as sharing beliefs between free-energy minimizing agents, where one agent's beliefs get transformed into an observation modality for the other. However, the best approach for transforming beliefs into observations remains an open question. In this paper, we demonstrate that naively sharing posterior beliefs can give rise to the negative social dynamics of echo chambers and self-doubt. We propose an alternate belief sharing strategy which mitigates these issues.", "sections": [{"title": "1 Introduction", "content": "Communication and the emergence of language have been a cornerstone in the development of human intelligence, as they enable human collaboration at multiple communal scales [1]. This collaborative capability hinges significantly on how agents share and process information, particularly requiring their internal beliefs about the world to be aligned [2,3]. Active inference provides a compelling framework for understanding and designing such collaborative interactions in the interest of building ecosystems of intelligence [4,5,6]. In this paradigm, agents minimize variational free energy [7], as each agent maintains a generative model of the world which it uses to make inferences about hidden states and to plan actions. Then, communication between agents at the lowest level can be conceptualized as sharing these internal beliefs, transforming the beliefs of one agent into observable data for another [8], these beliefs can be shared directly as we will demonstrate in this paper, but could also be present in the environment more permanently in the form of scripts[9] and texts [10,11]. This belief-sharing mechanism is intended to facilitate a more coherent and efficient joint exploration of the environment.\nHowever, translating and sharing beliefs has its challenges, as the messages shared are typically colored by one's personal priors and biases [12]. We found that naively sharing posterior beliefs can inadvertently lead to detrimental social dynamics, such as echo chambers, in which agents reinforce each other's biases, and self-doubt, in which agents discount their observations to favor shared, yet incorrect, beliefs. These phenomena can significantly impair the collective performance of the agents, highlighting the need for more sophisticated strategies in belief communication. In this paper, we explore these dynamics in depth. We begin by modeling the communication between agents as belief sharing under the active inference framework, demonstrating the pitfalls of straightforward belief sharing. We then propose an alternative strategy that mitigates these issues by adjusting how beliefs are communicated. Specifically, we advocate for sharing likelihood information rather than posterior beliefs, treating other agents' observations as additional independent sources of information. This approach aims to harness the benefits of collaborative inference while avoiding the pitfalls of misleading belief reinforcement.\nOur contributions are threefold: (i) We provide a detailed analysis of how naive belief-sharing can lead to echo chambers and self-doubt. (ii) We propose a novel communication strategy that mitigates these issues by sharing likelihoods. (iii) We validate our approach through simulations, demonstrating improved performance and robustness in collaborative tasks. The following sections outline our active inference model for communication, describe the experimental setup used to test our hypotheses, present our findings on echo chambers and self-doubt, and discuss our proposed solution and its implications for designing collaborative AI systems."}, {"title": "2 An active inference model for communication", "content": "In this section, we provide a summary overview of active inference, and how it can be adopted to model communication between agents. For a more in depth overview of active inference we refer the reader to [7]."}, {"title": "2.1 Perception and planning as inference", "content": "Active inference posits that agents entertain a generative model of the environment they operate in, and casts perception and action as Bayesian inference [7]. In general, the agent's generative model can be written as the joint probability distribution over states s, observations o and actions a, with tilde denoting a time sequence of those over timesteps t:\n$$P(\\tilde{s}, \\tilde{o}, \\tilde{a}) = P(s_0) \\prod_t P(o_t|s_t)P(s_t|s_{t-1}, a_{t-1})P(a_{t-1})$$\nPerception now becomes inferring the posterior distributions of states given the performed actions and observations. As this is typically intractable, agents resort to variational Bayesian inference, where an approximate posterior Q(\u0161\u00f5) is optimized instead, by minimizing the variational Free Energy:"}, {"title": "2.2 Communication as belief sharing", "content": "When agents share a common world and world model, they can benefit from sharing beliefs among each other [8]. The most straightforward way to realize this would be to share the agent's respective posterior beliefs on some shared modality. In order to achieve this, the agents generative model is expanded as shown in Fig. 1. In particular, any agent, for example., the primary focal agent, assumes other agents with a similar generative model will communicate information about its beliefs of the world. To do so, we equip the focal agent with an extra observation modality of. Instead of being observed from the environment, of is an observation generated by another agent based on its internal beliefs st. This approach is the kind of model posited in earlier work [8].\nTo realize posterior belief-sharing, agents require a likelihood mapping between posterior beliefs about latent states that are shareable among agents, i.e., st, and this observation modality of. In natural systems, these can be a very complex likelihood mapping, e.g., language [14] or birdsong [15]. However, in the case of AI agents, we can decide on the communication channel ourselves. One particular naive choice is to directly share the sufficient statistics of one's internal beliefs s' and integrate these with an identity likelihood mapping, as used in [8]. However, in the remainder of this paper, we will demonstrate the fallacies of using this approach and propose a different format for shared messages."}, {"title": "3 Experimental setup", "content": "To demonstrate multi-agent belief sharing, we simulate an object-finding task, where multiple agents search for a rewarding object in the same environment, and can potentially share beliefs on where they think the object is. The setup and generative model for this task is depicted in Figure 2. The world is represented by a graph of N locations that can be visited by the agents, and agents can move between connected nodes in the graph. Each agent has two state factors, a Categorical(N) variable st which is the belief about the agent's location, and a Categorical (N) variable si which is the belief about the object location. An agent can perform one of N move actions a, modeled using the three-dimensional dynamics tensor B1.\n$$B_{i,j}^a = \\begin{cases}\n1.  0, & \\text{if } a = i \\wedge \\text{connected}(i, j)\\\\\n1.  0, & \\text{if } i = j\\\\\n2.  0, & \\text{otherwise}\n\\end{cases}$$"}, {"title": "4 Echo chambers", "content": "In a model which shares the posterior beliefs of one agent as observations of another, Bayesian model updating reinforces redundant priors shared among them. The consequent simulated behavior mirrors the \"echo chamber effect\" wherein messages communicated by like-minded agents are amplified and returned. Psychological interpretations of the echo chamber effect are illustrated in the consequences of social media feed algorithms, which are often engineered to encourage user engagement with sympathetic posts [16]. An algorithmically curated social media curriculum increases engagement by anticipating a user's expected social media observations and fulfilling those expectations. Promoted content thereby constructs homophilic interaction networks which facilitate the construction and reinforcement of shared narratives among users. In worst case scenarios, the result is the unimpeded flow of misinformation on social media platforms. More generally, shared narratives facilitated by social media feed algorithms result in an increase in confidence of posterior beliefs even when external evidence is absent or intentionally excluded.\nIgnoring new evidence can be adaptive, such as when this strategy facilitates in-group cooperation [17]. However, negative consequences also result, such as when outside sources are discredited or distrusted.\nCorresponding to the echo chamber effect, one pitfall of belief sharing is that when agents have established even small prior beliefs on their goal, if those priors are shared, then an echo chamber forms. The agents, however, reinforce their prior beliefs through the communication method, resulting in ever-increasing beliefs on the goal location even when there is no new evidence to support this belief. Fig. 3 shows a simulation triggering this situation. The agents start with a small belief that the object will be present at two locations within the graph to simulate a longer-running experiment where such a situation might naturally occur. We have restricted the movement of the agents and prohibited the agents from accumulating more evidence about the object's actual location. However, the agents keep increasing their belief on the object being present at the a priori believed location because of the constant sharing of beliefs. Once the agents create such an echo chamber, more often than not, they are stuck in this faulty belief unless they all sufficiently change their belief at the same time."}, {"title": "5 Self-doubt", "content": "In an echo chamber, a belief sharing agent's confidence about their priors is reinforced in the absence of external evidence. Conversely, self-doubt refers to a scenario in which the agent's self-confidence is degraded as a result of belief sharing. In our simulations, the paradigmatic example is when multiple agents are fixated on a search task with complimentary plans for exploring the environment. Naturally, siblings faulty beliefs and ignore all evidence that points to the contrary. This can occur after an echo chamber is formed but could also occur independently. In this scenario, the agents again reinforce each other's beliefs in such a strong way that the agents \"doubt\" their observations originating from the environment. Fig. 4 visually overviews a simulation showcasing this phenomenon. The agents are initialized with a strong belief on the object location. In this particular simulation, the agents have a single peaked belief on the object location (location 1 in the graph), which could occur after an echo chamber situation. The agents are unrestricted in their movement as long as they follow the underlying graph structure. We see that even though eventually all agents visit location 1, they cannot correctly eliminate that location as a possible object location. The incoming beliefs of the other agents overrule their sensory observations."}, {"title": "6 To share or not to share?", "content": "Given that sharing agents' beliefs can give rise to the aforementioned issues, it is worth wondering what information can best be shared to allow multi-agent cooperation in active inference agents. In particular, the update rule for si,\nwritten in variational message passing notation [18], is of the form\n$$\\delta_i = \\sigma(\\mu_{B_2}^s + \\mu_{A_2}^s + \\mu_{A_3}^s),$$\n$$\\mu_{A_g}^s = \\sigma(\\varphi(\\alpha_i^s)) \\prod_{i \\in pa(g)\\setminus f} \\mu_{St, Ag}^{\\uparrow Ag},$$\nwhere o is the softmax function, f, the message from observation modality g to state factor f and (a) the digamma function of the Dirichlet counts corresponding to the parameters of the likelihood model [8]. Effectively, the update message is comprised of a part coming from a prior given by our beliefs on the previous timestep \u00b5\u00b2\u0432\u2082, a part based on the latest observation \u03bc\u03b5 \u03912,\nand a part communicated by the other 3. When we communicate the other's posterior parameters directly through an identity likelihood mapping, we have 2,other + MA2 u2other. This shows that, indeed, when agents have similar priors, this gets double counted in the belief update.\nTo address this, we will now instead share the other's likelihood message only, i.e. \u03913 = \u03bc 12,other. This scheme leaves out agents' prior beliefs about the state and only shares the agent's interpretation of the observation, treating the other agents as extra independent observers for the exact latent cause in the world. We call this scheme 'likelihood sharing'."}, {"title": "7 Discussion", "content": "The results presented in this paper highlight the potential pitfalls of belief-sharing in multi-agent systems under the framework of active inference. Our findings suggest that naive sharing of posterior beliefs can lead to undesirable social dynamics such as echo chambers and self-doubt, severely hampering the agents' performance in collaborative tasks.\nEcho chambers in human societies are well-documented phenomena where groups of individuals reinforce their preconceptions, often without external validating evidence. In our simulations, a similar effect occurs when agents continuously share their posterior beliefs. Initial biases can get amplified through repetitive belief sharing, leading to overly confident but potentially erroneous shared beliefs. This results in the agents becoming overconfident in incorrect hypotheses, thereby hampering their search or exploration processes. Similarly, self-doubt arises when agents' observations contradict the reinforced shared beliefs, leading them to disregard their sensory inputs. This mirrors real-world psychological effects where individuals question their perceptions in the face of strong peer influence. In our simulations, agents maintained strong incorrect beliefs about the object's location despite direct evidence to the contrary due to the influence of shared but incorrect posterior beliefs.\nOur proposed strategy of sharing likelihoods rather than posterior beliefs mitigates the issues of echo chambers and self-doubt. By sharing interpreted observations rather than fully formed beliefs, agents can integrate new information without being overwhelmed by the potentially erroneous priors of others. This approach allows agents to utilize each other as additional sensing mechanisms, providing independent evidence that can be more robustly combined with their observations. By treating other agents' observations as additional data points rather than beliefs, the system remains more flexible and resilient to individual errors. The proposed approach was only validated in simulated experiments but might also apply to more general and complex scenarios with further modifications.\nOur work suggests that the type of information shared among active inference agents must be carefully considered to avoid counterproductive dynamics. Future research can build on these insights by exploring other belief-sharing strategies and their impacts on system performance. Additionally, exploring these dynamics in more complex and varied environments, including those with adversarial elements, could provide deeper insights into the performance of different communication strategies. As our approach assumed full and honest collaboration between the agents, another future avenue of research would be to investigate the impact of dishonesty and, consequently, the discounting of communications between untrusted agents.\nIn conclusion, while belief sharing among active inference agents can enhance collaborative performance, it also risks reinforcing incorrect beliefs and undermining individual observations. Our proposed likelihood-sharing mechanism offers a promising solution by leveraging the strengths of collective sensing while mitigating the pitfalls of echo chambers and self-doubt without significant changes to the underlying model. Such strategies will be essential for developing robust, efficient, and adaptive collaborative agents as multi-agent active inference systems are designed."}]}