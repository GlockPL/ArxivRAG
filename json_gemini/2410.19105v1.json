[{"title": "Conditional diffusions for neural posterior estimation", "authors": ["Tianyu Chen", "Vansh Bansal", "James G. Scott"], "abstract": "Neural posterior estimation (NPE), a simulation-based computational approach\nfor Bayesian inference, has shown great success in situations where posteriors\nare intractable or likelihood functions are treated as \"black boxes.\" Existing NPE\nmethods typically rely on normalizing flows, which transform a base distributions\ninto a complex posterior by composing many simple, invertible transformations.\nBut flow-based models, while state of the art for NPE, are known to suffer from\nseveral limitations, including training instability and sharp trade-offs between\nrepresentational power and computational cost. In this work, we demonstrate the\neffectiveness of conditional diffusions as an alternative to normalizing flows for\nNPE. Conditional diffusions address many of the challenges faced by flow-based\nmethods. Our results show that, across a highly varied suite of benchmarking prob-\nlems for NPE architectures, diffusions offer improved stability, superior accuracy,\nand faster training times, even with simpler, shallower models. These gains persist\nacross a variety of different encoder or \"summary network\" architectures, as well\nas in situations where no summary network is required. The code will be publicly\navailable at https://github.com/TianyuCodings/cDiff.", "sections": [{"title": "Introduction", "content": "Neural posterior estimation (NPE), a type of simulation-based inference (SBI) technique, has gained\nsignificant traction in recent years for approximating complex posterior distributions $p(\\theta | X)$,\nparticularly in scenarios where the likelihood function is either intractable or best conceptualized as a\n\"black box\" recipe for simulating the data-generating process. NPE methods for Bayesian inference\nhave proven to be remarkably effective in a wide range of scientific applications, from cosmology to\nepidemiology, where traditional computational methods for likelihood-based inference are computa-\ntionally prohibitive or impractical. NPE methods can also be effective in more conventional\u2014but still\nchallenging-statistical inference settings, where the likelihood is available but the posterior remains\nchallenging to sample.\nIn existing work on NPE, normalizing flows [e.g. 19] have been the dominant approach. Flow-based\nmodels, however, come with significant drawbacks. In particular, flows pose a sharp trade-off\nbetween expressiveness and tractability: to accurately represent complex posterior distributions,\nflows often require either very deep models (i.e. many simple invertible flows composed together)\nor more complex flows with difficult-to-evaluate Jacobians. Either choice comes with increased\nmemory/compute requirements. Moreover, normalizing flows for NPE are prone to instability or\neven divergence during training, a phenomenon we document across several examples.\nIn this work, we turn our attention to conditional diffusion models as an alternative to normalizing\nflows for NPE. To be sure, diffusions have been widely used as generative models in the ML literature,"}, {"title": "", "content": "but they have not been comprehensively studied as a possible NPE solution. In Bayesian inference, the\ngoal is not just to generate plausible data samples, but to provide accurate uncertainty quantification\nfor the parameters of a known \"forward\" model (i.e. likelihood) $p(X | \\theta)$ that is assumed to\nhave generated the data. In this paradigm, posterior distributions are typically lower-dimensional,\nbut often have much higher uncertainty (i.e. entropy), compared to the distributions encountered\nin, say, image generation. Moreover, getting the uncertainty right is crucial. Scientists rely on\nprecise characterization of posterior distributions to draw valid inferences about unknown parameters,\nconduct hypothesis tests, or make decisions under uncertainty. In this setting, distinct posterior modes\nrepresent distinct scientific explanations for the observed data, and so mode collapse is especially\nproblematic. More generally, a failure to capture the full range of uncertainty in the posterior not\nonly limits predictive accuracy but can also mislead decision-makers in fields where NPE has proven\nvaluable, such as epidemiology and climate science. NPE methods must excel in capturing the full\nshape of the posterior, including tail behavior, multimodality, and sharp transitions in probability.\nThe potential benefits of diffusions are poorly understood in this context.\nSummary of contributions. First, we introduce a benchmark suite for neural posterior estimation\n(NPE), which extends beyond existing benchmarks by incorporating many typical statistical inference\nproblems, including those where the likelihood function is known but posterior sampling remains\nchallenging. These \"white-box likelihood\" problems bridge a gap in prior work, providing a more\ndiverse evaluation protocol for NPE methods on problems familiar to statisticians. Second, we\nshow that, when used as NPE decoders, conditional diffusion models offer several clear advantages-\nincluding improved stability, generally superior accuracy, and faster training times-compared to their\nflow-based counterparts. These advantages accrue on both exchangeable-data and sequential-data\nsettings. They also persist across a variety of different encoder or \"summary\" networks, including\nthose based on DeepSets, LSTMs, and set attention [see e.g. 26, a recent review].\nThree short examples. Before detailing our approach and results, we first highlight three short\nexamples that illustrate the superior performance of conditional diffusions versus normalizing flows\nfor approximating complex posterior distributions. These examples are included in our benchmarking\nsuite, but our presentation here is not intended to be exhaustive or formal. Rather it is meant to\nprovide intuition and illustrate key aspects of our results, using simple \"default\" settings for each\ndecoder. We use cNF as the abbreviation for the conditional Normalizing Flow model and cDiff for\nconditional Diffusion model.\nA key aspect of these examples is that, when designing the NPE architectures, we deliberately\nignore certain \"obvious\" posterior features, such as boundary constraints or non-identified parameters.\nThe goal is to evaluate how well these architectures can handle challenging posterior structures\nwithout being explicitly tailored for them. This approach provides insight into the flexibility of each\narchitecture in managing complex posteriors (e.g. an unknown boundary constraint as an example of\na sharp transition in probability), especially where important structural features are unknown and the\narchitecture cannot be specifically adapted, e.g. via transformations that remove constraints.\nExample 1 (Sum of cosines). Our first example is intended as a toy version of a system with coupled\noscillators; it also bears similarity to problems involving beamforming in phased array antennas.\nLet $\\theta = (\\theta_1,\\theta_2)$ have a uniform prior on $(-\\pi, \\pi)^2$. Given $\\theta$, the observed data $y$ is sampled from\n$y | \\theta \\sim N(f(\\theta),1)$, where $f(\\theta)$ is a sum of three cosines of different frequencies and phases,\nintroducing nontrivial correlation and multiple modes into the posterior:\n$f(\\theta_1,\\theta_2) = \\cos(\\theta_1-\\theta_2) + \\cos(2\\theta_1+\\theta_2) + \\cos(3\\theta_1 - 4\\theta_2)$.\nWe trained a diffusion model and a normalizing flow to approximate the posterior $p(\\theta | y)$ for this\nmodel; the normalizing flow was necessarily deeper (32 layers vs. 4), but for the sake of a fair\ncomparison, the models were constructed to have very similar numbers of parameters (204,338\nparameters for diffusion model and 215,296 parameters for the normalizing flow), and each saw\nthe same amount of training data, enough to yield apparent convergence of both models. We then\nqueried each fitted model with the test point $Y_{obs} = 0$ and drew 100,000 posterior samples. Because\nthe prior is uniform on $(-\\pi, \\pi)^2$, the posterior is proportional to the likelihood, $p(\\theta | Y_{obs} = 0) \\propto\n\\exp\\left(-0.5 \\cdot (f(\\theta))^2\\right)$.\nIn Figure 1, the normalizing flow clearly struggles, producing a posterior that misses many clear\nmodes and troughs. The diffusion model, while far from perfect, much more faithfully renders the\nundulating ridges of the true posterior. One could likely get better performance with more complex"}, {"title": "", "content": "flows; the same could likely be said of a more complex diffusion model. The point here is simply that\none gets decent \"out of the box\" performance with a relatively simple diffusion decoder, but that, at\nbest, getting good performance from a flow-based decoder would require substantially more effort.\nExample 2 (Witch's hat). The witch's hat distribution is a famous example of a posterior distribution\nthat exhibits poor MCMC mixing [14]. Let the prior for $\\theta \\in \\mathbb{R}^P$ be uniform on $[0.1, 0.9]^P$, a subset\nof the unit hypercube that excludes the region near the boundary. Given $\\theta$, the data $y \\in \\mathbb{R}^P$ follows a\nmixture distribution:\n$(y | \\theta) \\sim \\delta \\cdot U ([0, 1]^P) + (1 - \\delta) \\cdot N (\\theta, \\sigma^2I)$,\nwhere $\\delta$ and $\\sigma$ are both small; the $N(\\theta, \\sigma^2I)$ component is the high conical \"peak\" of the witch's\nhat, and $U ([0, 1]^P)$ is the broad, flat \"brim.\" This distribution was constructed by [14] as an example\nwhere the mixing time of the Gibbs sampler increases exponentially with dimension; our modified\nversion introduces the extra wrinkle that the parameter space has a region of zero probability not\nshared by the sample space, since $\\theta\\in [0.1, 0.9]^P$. We trained both a normalizing flow and a diffusion\nto estimate this posterior, using the same settings as in the previous example. Because the prior is flat\non $[0.1, 0.9]^P$, the posterior based on a single sample y is proportional to the witch's hat likelihood,\ntruncated to the support of the prior. Thus the posterior exhibits two challenging transitions: from the\nsharply peaked (but log-concave) Gaussian region to the flat-but-nonzero region inside $[0.1, 0.9]^P$,"}, {"title": "", "content": "and again from the flat-but-nonzero region to the region of zero probability outside $[0.1, 0.9]^P$.\nNeither model was given explicit knowledge of this hard boundary.\nFigure 2 shows the results for the toy example where $P = 2, \\sigma = 0.02, \\delta = 0.05$. The normalizing\nflow performs substantially worse at characterizing the sharp transitions in the posterior, especially\nthe jump at the boundary of $[0.1, 0.9]^P$-a problem becomes substantially worse with increasing\ndimension $P$.\nExample 3 (Dirichlet-multinomial). Our third example is a canonical model in Bayesian inference:\nthe conjugate Dirichlet-multinomial model. We expect any decoder to do reasonably well on this\nsimple problem, but we include it because the parameter space (the probability simplex) is closed\nand bounded, while the support of the base (Gaussian) distribution of both decoders is $\\mathbb{R}^P$. As with\nExample 1, neither decoder is given explicit knowledge of the bounds of the parameter space. Our\ngoal was to see how well both decoders perform on a problem that seems \"simple,\" but where a perfect\napproximation is actually impossible, because the base and target distributions have fundamentally\ndifferent topological structures.\nFigure 3 shows both reconstructions on a five-dimensional problem. Both model are reasonably good\nat capturing the point estimates and getting the uncertainty (i.e. posterior dispersion) approximately\nright. But the diffusion is visibly better, a performance advantage borne out by our benchmarks in\nSection 4."}, {"title": "2 Preliminaries and related work", "content": "Suppose we observe data from a known probabilistic model $p(X | \\theta)$, where $\\theta\\in \\Theta$ is a $P$-\ndimensional unknown parameter. We specify a prior $p(\\theta)$, and we wish to sample from the posterior\n$p(\\theta | X^{(obs)})$. The goal of neural posterior estimation is to provide a tractable approximation to\n$p(\\theta | X)$ by leveraging the power of generative neural networks [1, 19, 5, 3, 17, 13]. In NPE, we\napproximate the true posterior by fitting a generative neural network to a large set of synthetic training\ndata, consisting of $(\\theta, X)$ pairs drawn from the joint distribution $p(\\theta) \\cdot p(X | \\theta)$. Thus the task of\nposterior estimation becomes one of estimating a conditional density using a high-capacity generative\nmodel, denoted $q_{\\phi}(\\theta | X)$, where $\\phi$ are the network parameters. Once trained, the NPE model $q_{\\phi}$\nserves as a fast amortized inference engine: given the dataset $X^{(obs)}$ actually observed, the network\ncan produce samples from the estimated posterior distribution of $\\theta$ by simulating draws from the\nfitted NPE model, $q_{\\phi}(\\theta | X^{(obs)})$. While NPE is especially useful in cases where the likelihood\nfunction is intractable or computationally expensive, it remains relevant even in traditional \"white"}, {"title": "", "content": "box\" scenarios where we have explicit knowledge of the likelihood function, especially if standard\ntechniques like MCMC are slow or difficult to implement efficiently.\nThe simulation phase. The simulation phase in NPE consists of repeatedly simulating $(\\theta^{(m)} \\sim p(\\theta)$,\nand then data $X^{(m)}$ conditional on $\\theta^{(m)}$, for $m = 1, . . ., M$. (Typically the number of simulations $M$\nis quite large, e.g. $10^6$ or more.) In most settings, each simulated $X^{(m)}$ is not just a single data point,\nbut rather an entire dataset of (typically vector-valued) observations generated under the assumption\nthat $\\theta = \\theta^{(m)}$. We use $N_m$ to refer to the number of samples in simulated dataset m, and $D$ to refer to\nthe dimensionality of the sample space, i.e. the support of $p(X | \\theta)$. For example, in the common case\nwhere individual data points are exchangeable, the dataset $X^{(m)}$ consists of conditionally independent\nand identically distributed (IID) draws $X_i^{(m)} \\in \\mathbb{R}^D$ from the data generating process, given the\nparameter $\\theta^{(m)}$:\n$p(X^{(m)} | \\theta^{(m)}) = \\prod_{i=1}^{N_m} p(x_i^{(m)} | \\theta^{(m)}), N_m \\sim S,$\nwith obvious modifications for non-IID data scenarios, e.g. sequential data. The tuple $\\{\\theta^{(m)}, X^{(m)}\\}$\nis then a single training instance for the NPE model. Note that the sample size $N_m$ is not necessarily\nfixed across all simulations but instead varies according to a pre-specified probability distribution $S$.\nThis ensures that the NPE model is exposed to datasets of differing sizes during training, allowing it\nto learn how the posterior distribution should contract as a function of the sample size, rather than\noverfit to a single fixed sample size.\nThe encoder and decoder networks.\nNeural Posterior Estimation (NPE) typically involves a two-\nstage encoder/decoder architecture. The encoder and decoder networks work together to approximate\nthe posterior distribution $p(\\theta | X)$ by first summarizing or encoding the dataset $X$ in terms of a\nfixed-dimensional representation, and then producing posterior samples based on this representation.\nThe encoder or \"summary\" network $s_{\\psi}(\\cdot)$ is a parameterized function that is responsible for mapping\nthe dataset $X^{(m)}$ to a fixed-dimensional vector of $K$ summary statistics. This summary vector,\n$s_m = s_{\\psi}(X^{(m)})$, can be thought of as a learned analog of a sufficient statistic, although unlike in\nclassical statistical theory, the encoder is trained on simulated data to extract the relevant information\nabout $\\theta$; no knowledge of the structural form of $p(X | \\theta)$ is used.\nFormally, let $\\mathcal{X}$ denote the set of all data sets that might be encountered during the simulation phase,\ndefined as a union over all possible sample sizes:\n$\\mathcal{X} = \\bigcup_{N \\in supp(S)} \\{ \\{X_i\\}_{i=1}^{N} : X_i \\sim p(x|\\theta), \\theta \\in \\Theta\\},$"}, {"title": "", "content": "the network to minimize a divergence between the predicted and true conditional distributions, the\ndecoder learns to approximate the posterior for any input data set $X$.\nThere are several other possible methods for neural posterior estimation, each offering different\nways to approximate complex posteriors, see e.g. [2], [16] and [24]. However, the seminal paper\non BayesFlow [19] showed that decoders based on normalizing flows consistently outperform other\nmethods, especially in high-dimensional or multimodal problems. This led to their widespread\nadoption for simulation-based inference. For this reason, we keep the presentation concise by\nbenchmarking our diffusion-based decoders against normalizing flows, rather than all other NPE\nmethods, which have largely been deprecated in practice by flow-based decoders."}, {"title": "3 Conditional diffusions for NPE", "content": "In our initial setup, we consider the case where there is only one data sample corresponding to each\n$\\theta$ in order to introduce the functionality of the diffusion decoder. We then extend our method by\nincorporating a summary network to handle the entire dataset $X$.\nDiffusion models [7, 20, 8] are powerful generative models that operate by defining a forward\ndiffusion process that gradually transforms the data distribution into a noise distribution. The model\nis then used to reverse this process, generating novel samples from pure noise. (In the context of\nNPE, \"data\" actually refers to the sampled $\\theta$ values.) Specifically, we aim to construct a diffusion\nprocess $\\{\\theta_t\\}_{t=0}^T$ indexed by the continuous time variable $t \\in [0, T]$. This diffusion process should\nsatisfy the conditions $\\theta_0 \\sim p(\\theta | X)$ and $\\theta_T \\sim p_T$, where $p_T$ is typically a Gaussian distribution\nthat is straightforward to sample from. We abbreviate the notation by using $\\theta_0$ as $\\theta$ when there is\nno ambiguity. Starting from Gaussian noise, we can then employ a Reverse Ordinary Differential\nEquation (ODE) to sample from the noise distribution to $\\theta_0$, a sample from the target distribution,\nformulated as:\n$\\frac{d \\theta_t}{dt} = f(\\theta_t, t) - \\frac{1}{2} g(t)^2 \\nabla_{\\theta_t} \\log p_t(\\theta_t | X), (1)$\n$\\nabla_{\\theta_t} \\log p_t(\\theta | X) := \\nabla_{\\theta_t} \\log \\int p(\\theta_t | \\theta_0)p(\\theta_0 | X)d\\theta_0$. Here, $p(\\theta_t | \\theta_0)$ is a transition kernel\ncommonly represented as a Gaussian noise addition process, i.e., $\\theta_t \\sim N(a(t)\\theta_0, \\beta(t)I)$, where\n$a(t)$ and $\\beta(t)$ are predefined schedules and correspond to a forward Stochastic Differential Equation\n(SDE).\n$d\\theta = f(\\theta,t)dt + g(t)dw, (2)$\nwhere $w$ is Brownian motion. It is proven in Song et al. [21] that the forward SDE (Eq. 2) has the\nsame marginal distribution $p_t(\\theta|X)$ as the reverse ODE (Eq. 1) for any gievn X. Moreover, $a(t)$ and\n$\\beta(t)$ have a closed-form relationship with $f(\\theta, t)$ and $g(t)$. We defer a more detailed discussion to\nAppendix B.2.\nOnce the score function $\\nabla_{\\theta_t} \\log p_t(\\theta | X)$ is known for all $t$, we can solve this ODE to sample from\nthe Gaussian noise distribution $\\theta_T$ to $\\theta_0$.\nThe score function $\\nabla_{\\theta_t} \\log p_t(\\theta | X)$ can be learned by training a time-dependent score-based model\n$s_{\\phi}(\\theta_t | X, t)$ using the following loss function:\n$L(\\phi) = E_{p(t), p(\\theta), p(X|\\theta)} E_{p(\\theta_t|\\theta_0)} \\left[\\lambda(t) ||s_{\\phi}(\\theta_t | X, t) - \\nabla_{\\theta_t} \\log p(\\theta_t | \\theta_0)||^2 \\right] (3)$\nwhere $\\lambda(t)$ is a positive weighting function, and $p(t)$ is a predefined time schedule used for training.\nIt is proven by Song et al. [21], Karras et al. [8] that the optimal solution $s_{\\phi}(\\theta_t | X, t)$ of $L(\\phi)$\nwill be $\\nabla_{\\theta_t} \\log p_t(\\theta | X)$. More details can be found in Appendix B.2. Since $p(\\theta_t | \\theta_0)$ is a\nGaussian distribution, this term can be computed analytically. This loss function is used to train the\ndiffusion decoder $\\phi$ and serves as a variational bound on the negative log-likelihood as well as the\nKullback-Leibler (KL) divergence, as descripted in Theorem 1.\nTheorem 1.\n$E_{p(X)} \\left[KL(p(\\theta | X)||q_{\\phi}(\\theta | X))\\right] \\leq L(\\phi) + C$\nwhere $C$ is independent with $\\phi$."}, {"title": "", "content": "Proof Sketch.\n$E_{p(X)} \\left[KL(p(\\theta | X)||q_{\\phi}(\\theta | X))\\right]$\n$= E_{p(X)} E_{p(\\theta|X)} [\\log p(\\theta | X) \u2013 \\log p_{\\phi}(\\theta | X)]$\n$= E_{p(\\theta)p(X|\\theta)} [-\\log p_{\\phi}(\\theta | X)] + C$\n$\\leq L(\\phi) + C$\nwhere the last inequality is proven in [7] and [22]. The complete proof can be found in Appendix\nB.1. \u041f\nThis shows that training a score function is equivalent to minimizing an upper bound on the KL\ndivergence between the ground truth posterior $p(\\theta | X)$ and our decoder $q_{\\phi}(\\theta | X)$. Note from (3)\nthat the loss function is nonnegative; therefore, the optimal solution of $L(\\phi)$, given sufficient data\nand model capacity, implies that $p(\\theta | X) = q_{\\phi}(\\theta | X)$ for any $X$.\nNext, we consider the case where the decoder must condition on the entire dataset $X$, rather than just\na single data point corresponding to $\\theta$. Since the size of $X$ may vary, we introduce a parameterized\nsummary network $s_{\\psi}(X)$ as described in Section 2. This network computes summary statistics of a\nfixed dimension, regardless of the number of data points in $X$. We jointly train the summary network\nand the decoder using the following loss function:\n$L(\\psi, \\phi) = E_{p(t), p(\\theta), p(X|\\theta)} E_{p(\\theta_t|\\theta_0)} \\left[\\lambda(t) ||s_{\\phi}(\\theta_t | s_{\\psi}(X), t) - \\nabla_{\\theta_t} \\log p(\\theta_t | \\theta_0)||^2 \\right], (4)$\nwhere $s_{\\psi}(X)$ denotes the output of the summary network. Since the summary network must capture\nsufficient statistics for the dataset input, its architecture is crucial and should be tailored for the\nstructure and probabilistic invariances of the data at hand. Detailed information about the summary\nnetwork structure is provided in Appendix C.2.\nWe formalize the adequacy of the summary network in the following assumption.\nAssumption 1 (Sufficiency of summary network). With sufficient data and model capacity, if the\noptimal solution $\\psi^*$ for $\\psi$ is achieved, we assume the sufficiency of the summary network's output,\ni.e. that\n$p(\\theta | X) = p(\\theta | s_{\\psi^*}(X))$\nholds for any $\\theta$ and $X$.\nTheorem 2. Under Assumption 1, we have:\n$E_{p(X)} \\left[KL(p(\\theta | X)||q_{\\phi}(\\theta | s_{\\psi^*}(X)))\\right] \\leq L(\\psi^*, \\phi) + C',$\nwhere $C'$ is a constant independent of $\\psi$ and $\\phi$.\nProof Sketch.\n$E_{p(X)} \\left[KL(p(\\theta | X)||q_{\\phi}(\\theta | s_{\\psi^*}(X)))\\right]$\n$= E_{p(X)} \\left[KL(p(\\theta | s_{\\psi^*}(X))||q_{\\phi}(\\theta | s_{\\psi^*}(X)))\\right] E_{p(X)} \\left[KL(p(\\theta | X)||p(\\theta | s_{\\psi^*}(X)))\\right]$\n$\\leq L(\\psi^*, \\phi) + C' + E_{p(X)} \\left[KL(p(\\theta | X)||p(\\theta | s_{\\psi^*}(X)))\\right]$\n$= L(\\psi^*, \\phi) + C'$\nwhere the inequality is derived from Theorem 1 and the last equation follows from Assumption 1. A\ncomplete proof is deferred to Appendix C.1.\nThe gap between theory and practice in this context arises because we never achieve a perfectly\noptimal summary network $s_{\\psi^*}(X)$ that fully captures sufficient statistics for $X$. The assumption\nthat $p(\\theta | X) = p(\\theta | s_{\\psi^*}(X))$ is an idealization that results in Theorem 2. In practice, however,\nthe summary network has finite capacity and is trained on finite data, so it can only approximately\nencode all the information in any given data set. Despite this, the result remains relevant: modern\nsummary networks are highly flexible and capable of representing complex relationships, enabling\nthe approximate posterior to be quite close to the truth, even if perfect sufficiency is out of reach in\npractice.\nWe jointly train $\\psi$ and $\\phi$ using the loss function $L(\\psi, \\phi)$ in (4), as described in Algorithm 1 in the\nAppendix. Posterior samples are drawn using Algorithm 2."}, {"title": "A Training and inference algorithms for conditional diffusions", "content": "The main training loop for conditional diffusions, wherein we jointly train a summary network and\ndiffusion decoder, is described in Algorithm 1.\nAlgorithm 1 cDiff Training\n1: Input: $p(\\theta)$, $p(X | \\theta)$, $\\lambda(t)$, $p(\\theta(t) | \\theta(0))$\n2: Output: summary network $s_{\\psi}$, score network $z_{\\phi}$\n3: Initialize networks s and z\n4: while not converged do\n5:  Sample $\\theta \\sim p(\\theta)$\n6:  Sample number of observations $N \\sim Unif[N_{min}, N_{max}]$\n7:  Sample $X = \\{X_i\\}_{i=1}^N \\sim p(X | \\theta)$\n8:  Calculate summary $s = s_{\\psi}(X)$\n9:  Calculate loss $L(\\psi, \\phi)$, given $(\\theta, s)$ (Eq 4)\n10:  Take gradient descent step on $\\psi, \\phi$\n11: end while\n12: Output $s_{\\psi}$ and $z_{\\phi}$\nAfter training the summary network $s_{\\psi}$ and $z_{\\phi}$, we are ready to sample from $p(\\theta | X)$ for a given\ndataset X, as described in Algorithm 2. Note that the sampling process applies to any general\ndiffusion process, such as VP, VE [21], or EDM [8]. In our paper, we utilize the EDM [8] training\nand sampling process, which is detailed in Appendix B.2, where, for notational simplicity, we define\n$\\theta_T := \\{\\theta_T\\}_{i=1}^J$\nAlgorithm 2 cDiff Sampling\n1: Input: $s_{\\psi}, z_{\\phi}, X$, number of posterior sample $J$\n2: Output: $\\{\\theta^{[j]}_0\\}_{j=1}^J$\n3: $\\theta_T := \\{\\theta_T \\sim N(0, I)\\}_{j=1}^J$\n4: $s \\leftarrow S(X)$\n5: for $t \\in \\{T, ..., 1\\}$ do\n6:  for $j = 1: J$ do\n7:   $di\\leftarrow f(\\theta_t, t) - g(t)^2z_{\\phi}(\\theta_t | s, t)$\n   $\\theta_{t-1} \\leftarrow \\theta_t - di$\n8:  end for\n9: Return $\\theta_0$"}, {"title": "B Diffusion model theorems and implementation details", "content": "B.1 Proof of Theorem 1\nB.1.1 Preliminaries\nHere", "noise": "n$\\theta_t = a(t)\\theta_0 + \\beta(t)\\epsilon \\epsilon \\sim N(0", "loss": "n$E_{p(t)", "epsilon||^2": "n$E_{p(t)", "theta_0)||^2": "n$E_{p(t)", "theta_0||^2": "nIt is proven by Song et al. [20", "21": "Ho et al. [7", "6": "that the optimal solution of\nthese loss functions has the following connection:\n$\\mu(\\theta_t", "X)": "leq \\sum_{t=1"}], "inequality": "For any given X", "holds": "n$E_{p(\\theta|X)"}, ["log p_{\\phi}(\\theta | X)"], "leq \\sum_{t=1", "T E_{p(\\theta|X)", "p(\\epsilon)[\\lambda(t)||\\epsilon_{\\phi", "theta_t | X", "t) \u2013 \\epsilon||^2] + C.$\nThus", "it is straightforward to conclude that:\n$E_{p(X)}E_{p(\\theta|X)} [-\\log p_{\\phi}(\\theta | X)] \\leq \\sum_{t=1}^T E_{p(X)}E_{p(\\theta|X)}p(\\epsilon)[\\lambda(t)||\\epsilon_{\\phi}(\\theta_t | X"]