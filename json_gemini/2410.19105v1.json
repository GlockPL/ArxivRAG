{"title": "Conditional diffusions for neural posterior estimation", "authors": ["Tianyu Chen", "Vansh Bansal", "James G. Scott"], "abstract": "Neural posterior estimation (NPE), a simulation-based computational approach\nfor Bayesian inference, has shown great success in situations where posteriors\nare intractable or likelihood functions are treated as \"black boxes.\" Existing NPE\nmethods typically rely on normalizing flows, which transform a base distributions\ninto a complex posterior by composing many simple, invertible transformations.\nBut flow-based models, while state of the art for NPE, are known to suffer from\nseveral limitations, including training instability and sharp trade-offs between\nrepresentational power and computational cost. In this work, we demonstrate the\neffectiveness of conditional diffusions as an alternative to normalizing flows for\nNPE. Conditional diffusions address many of the challenges faced by flow-based\nmethods. Our results show that, across a highly varied suite of benchmarking prob-\nlems for NPE architectures, diffusions offer improved stability, superior accuracy,\nand faster training times, even with simpler, shallower models. These gains persist\nacross a variety of different encoder or \"summary network\" architectures, as well\nas in situations where no summary network is required. The code will be publicly\navailable at https://github.com/TianyuCodings/cDiff.", "sections": [{"title": "1 Introduction", "content": "Neural posterior estimation (NPE), a type of simulation-based inference (SBI) technique, has gained\nsignificant traction in recent years for approximating complex posterior distributions p(\u03b8 | X),\nparticularly in scenarios where the likelihood function is either intractable or best conceptualized as a\n\"black box\" recipe for simulating the data-generating process. NPE methods for Bayesian inference\nhave proven to be remarkably effective in a wide range of scientific applications, from cosmology to\nepidemiology, where traditional computational methods for likelihood-based inference are computa-\ntionally prohibitive or impractical. NPE methods can also be effective in more conventional\u2014but still\nchallenging-statistical inference settings, where the likelihood is available but the posterior remains\nchallenging to sample.\nIn existing work on NPE, normalizing flows [e.g. 19] have been the dominant approach. Flow-based\nmodels, however, come with significant drawbacks. In particular, flows pose a sharp trade-off\nbetween expressiveness and tractability: to accurately represent complex posterior distributions,\nflows often require either very deep models (i.e. many simple invertible flows composed together)\nor more complex flows with difficult-to-evaluate Jacobians. Either choice comes with increased\nmemory/compute requirements. Moreover, normalizing flows for NPE are prone to instability or\neven divergence during training, a phenomenon we document across several examples.\nIn this work, we turn our attention to conditional diffusion models as an alternative to normalizing\nflows for NPE. To be sure, diffusions have been widely used as generative models in the ML literature,"}, {"title": null, "content": "but they have not been comprehensively studied as a possible NPE solution. In Bayesian inference, the\ngoal is not just to generate plausible data samples, but to provide accurate uncertainty quantification\nfor the parameters of a known \"forward\" model (i.e. likelihood) p(X | 0) that is assumed to\nhave generated the data. In this paradigm, posterior distributions are typically lower-dimensional,\nbut often have much higher uncertainty (i.e. entropy), compared to the distributions encountered\nin, say, image generation. Moreover, getting the uncertainty right is crucial. Scientists rely on\nprecise characterization of posterior distributions to draw valid inferences about unknown parameters,\nconduct hypothesis tests, or make decisions under uncertainty. In this setting, distinct posterior modes\nrepresent distinct scientific explanations for the observed data, and so mode collapse is especially\nproblematic. More generally, a failure to capture the full range of uncertainty in the posterior not\nonly limits predictive accuracy but can also mislead decision-makers in fields where NPE has proven\nvaluable, such as epidemiology and climate science. NPE methods must excel in capturing the full\nshape of the posterior, including tail behavior, multimodality, and sharp transitions in probability.\nThe potential benefits of diffusions are poorly understood in this context.\nSummary of contributions. First, we introduce a benchmark suite for neural posterior estimation\n(NPE), which extends beyond existing benchmarks by incorporating many typical statistical inference\nproblems, including those where the likelihood function is known but posterior sampling remains\nchallenging. These \"white-box likelihood\" problems bridge a gap in prior work, providing a more\ndiverse evaluation protocol for NPE methods on problems familiar to statisticians. Second, we\nshow that, when used as NPE decoders, conditional diffusion models offer several clear advantages-\nincluding improved stability, generally superior accuracy, and faster training times-compared to their\nflow-based counterparts. These advantages accrue on both exchangeable-data and sequential-data\nsettings. They also persist across a variety of different encoder or \"summary\" networks, including\nthose based on DeepSets, LSTMs, and set attention [see e.g. 26, a recent review]."}, {"title": null, "content": "Three short examples. Before detailing our approach and results, we first highlight three short\nexamples that illustrate the superior performance of conditional diffusions versus normalizing flows\nfor approximating complex posterior distributions. These examples are included in our benchmarking\nsuite, but our presentation here is not intended to be exhaustive or formal. Rather it is meant to\nprovide intuition and illustrate key aspects of our results, using simple \"default\" settings for each\ndecoder. We use cNF as the abbreviation for the conditional Normalizing Flow model and cDiff for\nconditional Diffusion model.\nA key aspect of these examples is that, when designing the NPE architectures, we deliberately\nignore certain \"obvious\" posterior features, such as boundary constraints or non-identified parameters.\nThe goal is to evaluate how well these architectures can handle challenging posterior structures\nwithout being explicitly tailored for them. This approach provides insight into the flexibility of each\narchitecture in managing complex posteriors (e.g. an unknown boundary constraint as an example of\na sharp transition in probability), especially where important structural features are unknown and the\narchitecture cannot be specifically adapted, e.g. via transformations that remove constraints."}, {"title": null, "content": "Example 1 (Sum of cosines). Our first example is intended as a toy version of a system with coupled\noscillators; it also bears similarity to problems involving beamforming in phased array antennas.\nLet \u03b8 = (\u03b8\u2081,\u03b8\u2082) have a uniform prior on (\u2212\u03c0, \u03c0)\u00b2. Given \u03b8, the observed data y is sampled from\ny | \u03b8 ~ N(f(\u03b8),1), where f(\u03b8) is a sum of three cosines of different frequencies and phases,\nintroducing nontrivial correlation and multiple modes into the posterior:\nf(\u03b8\u2081,\u03b8\u2082) = cos(\u03b8\u2081-\u03b8\u2082) + cos(2\u03b8\u2081+\u03b8\u2082) + cos(3\u03b8\u2081 - 4\u03b8\u2082).\nWe trained a diffusion model and a normalizing flow to approximate the posterior p(\u03b8 | y) for this\nmodel; the normalizing flow was necessarily deeper (32 layers vs. 4), but for the sake of a fair\ncomparison, the models were constructed to have very similar numbers of parameters (204,338\nparameters for diffusion model and 215,296 parameters for the normalizing flow), and each saw\nthe same amount of training data, enough to yield apparent convergence of both models. We then\nqueried each fitted model with the test point Yobs = 0 and drew 100,000 posterior samples. Because\nthe prior is uniform on (\u2212\u03c0, \u03c0)\u00b2, the posterior is proportional to the likelihood, p(\u03b8 | Yobs = 0) x\nexp (-0.5 (f(\u03b8))\u00b2)."}, {"title": null, "content": "In Figure 1, the normalizing flow clearly struggles, producing a posterior that misses many clear\nmodes and troughs. The diffusion model, while far from perfect, much more faithfully renders the\nundulating ridges of the true posterior. One could likely get better performance with more complex"}, {"title": null, "content": "flows; the same could likely be said of a more complex diffusion model. The point here is simply that\none gets decent \"out of the box\" performance with a relatively simple diffusion decoder, but that, at\nbest, getting good performance from a flow-based decoder would require substantially more effort."}, {"title": null, "content": "Example 2 (Witch's hat). The witch's hat distribution is a famous example of a posterior distribution\nthat exhibits poor MCMC mixing [14]. Let the prior for \u03b8 \u2208 \u211d\u1d3e be uniform on [0.1, 0.9]\u1d3e, a subset\nof the unit hypercube that excludes the region near the boundary. Given \u03b8, the data y \u2208 \u211d\u1d3e follows a\nmixture distribution:\n(y | \u03b8) ~ \u03b4 \u00b7 U ([0, 1]\u1d3e) + (1 \u2212 \u03b4) \u00b7 N (\u03b8, \u03c3\u00b2I),\nwhere \u03b4 and \u03c3 are both small; the N(\u03b8, \u03c3\u00b2I) component is the high conical \"peak\" of the witch's\nhat, and U ([0, 1]\u1d3e) is the broad, flat \"brim.\" This distribution was constructed by [14] as an example\nwhere the mixing time of the Gibbs sampler increases exponentially with dimension; our modified\nversion introduces the extra wrinkle that the parameter space has a region of zero probability not\nshared by the sample space, since \u03b8\u2208 [0.1, 0.9]\u1d3e. We trained both a normalizing flow and a diffusion\nto estimate this posterior, using the same settings as in the previous example. Because the prior is flat\non [0.1, 0.9]\u1d3e, the posterior based on a single sample y is proportional to the witch's hat likelihood,\ntruncated to the support of the prior. Thus the posterior exhibits two challenging transitions: from the\nsharply peaked (but log-concave) Gaussian region to the flat-but-nonzero region inside [0.1, 0.9]\u1d3e,"}, {"title": null, "content": "and again from the flat-but-nonzero region to the region of zero probability outside [0.1, 0.9]\u1d3e.\nNeither model was given explicit knowledge of this hard boundary.\nFigure 2 shows the results for the toy example where P = 2, \u03c3 = 0.02, \u03b4 = 0.05. The normalizing\nflow performs substantially worse at characterizing the sharp transitions in the posterior, especially\nthe jump at the boundary of [0.1, 0.9]\u1d3e-a problem becomes substantially worse with increasing\ndimension P."}, {"title": null, "content": "Example 3 (Dirichlet-multinomial). Our third example is a canonical model in Bayesian inference:\nthe conjugate Dirichlet-multinomial model. We expect any decoder to do reasonably well on this\nsimple problem, but we include it because the parameter space (the probability simplex) is closed\nand bounded, while the support of the base (Gaussian) distribution of both decoders is \u211d\u1d3e. As with\nExample 1, neither decoder is given explicit knowledge of the bounds of the parameter space. Our\ngoal was to see how well both decoders perform on a problem that seems \"simple,\" but where a perfect\napproximation is actually impossible, because the base and target distributions have fundamentally\ndifferent topological structures.\nFigure 3 shows both reconstructions on a five-dimensional problem. Both model are reasonably good\nat capturing the point estimates and getting the uncertainty (i.e. posterior dispersion) approximately\nright. But the diffusion is visibly better, a performance advantage borne out by our benchmarks in\nSection 4."}, {"title": "2 Preliminaries and related work", "content": "Suppose we observe data from a known probabilistic model p(X | \u03b8), where \u03b8\u2208 \u0398 is a P-\ndimensional unknown parameter. We specify a prior p(\u03b8), and we wish to sample from the posterior\np(\u03b8 | X(obs)). The goal of neural posterior estimation is to provide a tractable approximation to\np(\u03b8 | X) by leveraging the power of generative neural networks [1, 19, 5, 3, 17, 13]. In NPE, we\napproximate the true posterior by fitting a generative neural network to a large set of synthetic training\ndata, consisting of (\u03b8, X) pairs drawn from the joint distribution p(\u03b8) \u00b7 p(X | \u03b8). Thus the task of\nposterior estimation becomes one of estimating a conditional density using a high-capacity generative\nmodel, denoted q(0 | X), where \u03c6 are the network parameters. Once trained, the NPE model q\nserves as a fast amortized inference engine: given the dataset X(obs) actually observed, the network\ncan produce samples from the estimated posterior distribution of @ by simulating draws from the\nfitted NPE model, q(0 | X(obs)). While NPE is especially useful in cases where the likelihood\nfunction is intractable or computationally expensive, it remains relevant even in traditional \"white"}, {"title": null, "content": "box\" scenarios where we have explicit knowledge of the likelihood function, especially if standard\ntechniques like MCMC are slow or difficult to implement efficiently.\nThe simulation phase. The simulation phase in NPE consists of repeatedly simulating (\u03b8(m) ~ p(\u03b8),\nand then data X (m) conditional on ()(m), for m = 1, . . ., M. (Typically the number of simulations M\nis quite large, e.g. 10\u2076 or more.) In most settings, each simulated X(m) is not just a single data point,\nbut rather an entire dataset of (typically vector-valued) observations generated under the assumption\nthat 0 = 0(m). We use N\u2098 to refer to the number of samples in simulated dataset m, and D to refer to\nthe dimensionality of the sample space, i.e. the support of p(X | 0). For example, in the common case\nwhere individual data points are exchangeable, the dataset X(m) consists of conditionally independent\nand identically distributed (IID) draws X (m) \u2208 \u211d\u1d30 from the data generating process, given the\nparameter (m):\np(x(m) | (m)) = \u220f p(x(m) | (m)), Nm ~S,\nwith obvious modifications for non-IID data scenarios, e.g. sequential data. The tuple {0(m), X(m)}\nis then a single training instance for the NPE model. Note that the sample size Nm is not necessarily\nfixed across all simulations but instead varies according to a pre-specified probability distribution S.\nThis ensures that the NPE model is exposed to datasets of differing sizes during training, allowing it\nto learn how the posterior distribution should contract as a function of the sample size, rather than\noverfit to a single fixed sample size."}, {"title": null, "content": "The encoder and decoder networks.\nNeural Posterior Estimation (NPE) typically involves a two-\nstage encoder/decoder architecture. The encoder and decoder networks work together to approximate\nthe posterior distribution p(0 | X) by first summarizing or encoding the dataset X in terms of a\nfixed-dimensional representation, and then producing posterior samples based on this representation.\nThe encoder or \"summary\" network sy(\u00b7) is a parameterized function that is responsible for mapping\nthe dataset X(m) to a fixed-dimensional vector of K summary statistics. This summary vector,\ns\u2098 = s\u03c8(X(m)), can be thought of as a learned analog of a sufficient statistic, although unlike in\nclassical statistical theory, the encoder is trained on simulated data to extract the relevant information\nabout 0; no knowledge of the structural form of p(X | 0) is used.\nFormally, let X denote the set of all data sets that might be encountered during the simulation phase,\ndefined as a union over all possible sample sizes:\nX = \u22c3 {{xi}\u1d62=\u2081\u1d3a : Xi ~ p(x|\u03b8), \u03b8\u2208\u0398},\nwhere supp(S) represents the support of the distribution S that determines the (random) sample size\nfor a given data set. We can then formally define a summary network as a function s : X \u2192 \u211d\u1d37, i.e.\na function that encodes any possible data set, regardless of size, as a fixed-dimensional vector that\nsummarizes the data. The structure of the summary network will be problem dependent, and will\ntypically be designed to reflect the type of data encountered (e.g. exchangeable, sequential, spatial).\nWe discuss the choice of summary network in describing our experiments below, considering a range\nof options proposed across the literature, including DeepSets [27, 28], Janossy pooling [15], LSTMs\n[4], and set attention [11]."}, {"title": null, "content": "Once the encoder has produced a summary statistic sm = sy(X(m)), the decoder network f(\u00b7) takes\nover. The decoder is trained to generate posterior samples for 0, conditional on the summary sm; in\nother words, it learns the approximate conditional distribution p(0 | X) \u2248 qq(0 | sy(X)). Prior work\non NPE decoders has focused on normalizing flows that transform a simple base distribution (e.g. a\nGaussian) into the target posterior via a composition of many simple invertible transformations. This\ntransformation is accomplished by a neural network that learns a flexible mapping between these two\ndistributions, via the relation\n0 ~ q(0 | X)\u21d4 0 = f\u00a2(z, Sy(X))\nwhere f is a learned (deterministic) function parameterized by \u03c6, z is a random draw from the\nbase distribution, and sy(X) is the summary vector by the learned encoder network. By training"}, {"title": null, "content": "the network to minimize a divergence between the predicted and true conditional distributions, the\ndecoder learns to approximate the posterior for any input data set X.\nThere are several other possible methods for neural posterior estimation, each offering different\nways to approximate complex posteriors, see e.g. [2], [16] and [24]. However, the seminal paper\non BayesFlow [19] showed that decoders based on normalizing flows consistently outperform other\nmethods, especially in high-dimensional or multimodal problems. This led to their widespread\nadoption for simulation-based inference. For this reason, we keep the presentation concise by\nbenchmarking our diffusion-based decoders against normalizing flows, rather than all other NPE\nmethods, which have largely been deprecated in practice by flow-based decoders."}, {"title": "3 Conditional diffusions for NPE", "content": "In our initial setup, we consider the case where there is only one data sample corresponding to each\n0 in order to introduce the functionality of the diffusion decoder. We then extend our method by\nincorporating a summary network to handle the entire dataset X.\nDiffusion models [7, 20, 8] are powerful generative models that operate by defining a forward\ndiffusion process that gradually transforms the data distribution into a noise distribution. The model\nis then used to reverse this process, generating novel samples from pure noise. (In the context of\nNPE, \"data\" actually refers to the sampled @ values.) Specifically, we aim to construct a diffusion\nprocess {0}\u209c\u208c\u2080\u1d40 indexed by the continuous time variable t \u2208 [0, T]. This diffusion process should\nsatisfy the conditions \u03b8\u2080 ~ p(\u03b8 | X) and \u03b8\u1d1b ~ p\u1d1b, where p\u1d1b is typically a Gaussian distribution\nthat is straightforward to sample from. We abbreviate the notation by using 0\u2080 as \u03b8 when there is\nno ambiguity. Starting from Gaussian noise, we can then employ a Reverse Ordinary Differential\nEquation (ODE) to sample from the noise distribution to \u03b8\u2080, a sample from the target distribution,\nformulated as:\nde = f(\u03b8,t) \u2212 \u00bdg(t)\u00b2\u2207\u03b8 log p\u209c(\u03b8 | X) dt,\n\u2207\u03b8 log p\u209c (\u03b8 | X) := \u2207\u03b8\u2080 log \u222b p(\u03b8\u209c | \u03b8\u2080)p(\u03b8\u2080 | X)d\u03b8\u2080. Here, p(\u03b8\u209c | \u03b8\u2080) is a transition kernel\ncommonly represented as a Gaussian noise addition process, i.e., \u03b8\u209c ~ N(\u03b1(t)\u03b8\u2080, \u03b2(t)I), where\n\u03b1(t) and \u03b2(t) are predefined schedules and correspond to a forward Stochastic Differential Equation\n(SDE).\nd\u03b8 = f(\u03b8,t)dt + g(t)dw,\nwhere w is Brownian motion. It is proven in Song et al. [21] that the forward SDE (Eq. 2) has the\nsame marginal distribution p\u209c(\u03b8|X) as the reverse ODE (Eq. 1) for any gievn X. Moreover, \u03b1(t) and\n\u03b2(t) have a closed-form relationship with f(\u03b8, t) and g(t). We defer a more detailed discussion to\nAppendix B.2.\nOnce the score function \u2207\u03b8 log p\u209c(\u03b8 | X) is known for all t, we can solve this ODE to sample from\nthe Gaussian noise distribution \u03b8\u1d1b \u03c4\u03bf \u03b8\u2080.\nThe score function \u2207\u03b8 log p\u209c(\u03b8| X) can be learned by training a time-dependent score-based model\n\u03f5\u03c6(\u03b8\u209c | X, t) using the following loss function:\nL(\u03c6) = E\u209a(t),p(\u03b8),p(X|\u03b8)E\u209a(\u03b8\u209c|\u03b8\u2080) [\u03bb(t)||\u03f5\u03c6(\u03b8\u209c | X, t) \u2212 \u2207\u03b8\u2080 log p(\u03b8\u209c | \u03b8\u2080)||\u00b2],\nwhere \u03bb(t) is a positive weighting function, and p(t) is a predefined time schedule used for training.\nIt is proven by Song et al. [21], Karras et al. [8] that the optimal solution \u03f5\u03c6(\u03b8\u209c | X, t) of L(\u03c6)\nwill be \u2207\u03b8 log p\u209c(\u03b8 | X). More details can be found in Appendix B.2. Since p(\u03b8\u209c | \u03b8\u2080) is a\nGaussian distribution, this term can be computed analytically. This loss function is used to train the\ndiffusion decoder \u03c6 and serves as a variational bound on the negative log-likelihood as well as the\nKullback-Leibler (KL) divergence, as descripted in Theorem 1.\nTheorem 1.\nE\u209a(X) [KL(p(\u03b8 | X)||q\u03c6(\u03b8 | X))] \u2264 L(\u03c6) + C\nwhere C is independent with \u03c6."}, {"title": null, "content": "Proof Sketch.\nE\u209a(X) [KL(p(\u03b8 | X)||q\u03c6(\u03b8 | X))]\n= E\u209a(\u03b8|X) [log p(\u03b8 | X) \u2212 log p\u03c6(\u03b8 | X)]\n= E\u209a(X)p(\u03b8|X) [\u2212logp\u03c6(\u03b8 | X)] + C\n< L(\u03c6) + C\nwhere the last inequality is proven in [7] and [22]. The complete proof can be found in Appendix\nB.1.\nThis shows that training a score function is equivalent to minimizing an upper bound on the KL\ndivergence between the ground truth posterior p(0 | X) and our decoder q(0 | X). Note from (3)\nthat the loss function is nonnegative; therefore, the optimal solution of L(\u03c6), given sufficient data\nand model capacity, implies that p(\u03b8 | X) = q\u03c6(\u03b8 | X) for any X.\nNext, we consider the case where the decoder must condition on the entire dataset X, rather than just\na single data point corresponding to 0. Since the size of X may vary, we introduce a parameterized\nsummary network sy(X) as described in Section 2. This network computes summary statistics of a\nfixed dimension, regardless of the number of data points in X. We jointly train the summary network\nand the decoder using the following loss function:\nL(\u03c8, \u03c6) = E\u209a(t),p(\u03b8),p(X|\u03b8)E\u209a(\u03b8\u209c|\u03b8\u2080) [\u03bb(t)||\u03f5\u03c6(\u03b8\u209c | sy(X), t) \u2212 \u2207\u03b8\u2080 log p(\u03b8\u209c | \u03b8\u2080)||\u00b2],\nwhere sy(X) denotes the output of the summary network. Since the summary network must capture\nsufficient statistics for the dataset input, its architecture is crucial and should be tailored for the\nstructure and probabilistic invariances of the data at hand. Detailed information about the summary\nnetwork structure is provided in Appendix C.2.\nWe formalize the adequacy of the summary network in the following assumption.\nAssumption 1 (Sufficiency of summary network). With sufficient data and model capacity, if the\noptimal solution \u03c8\u2217 for \u03c8 is achieved, we assume the sufficiency of the summary network's output,\ni.e. that\np(\u03b8 | X) = p(\u03b8 | S\u03c8\u2217(X))\nholds for any \u03b8 and X.\nTheorem 2. Under Assumption 1, we have:\nE\u209a(x) [KL(p(\u03b8 | X)||q\u03c6(\u03b8 | S\u03c8\u2217(X)))] \u2264 L(\u03c8\u2217, \u03c6) + C',\nwhere C' is a constant independent of \u03c8 and \u03c6.\nProof Sketch.\nE\u209a(x) [KL(p(\u03b8 | X)||q\u03c6(\u03b8 | S\u03c8\u2217 (X)))]\n= E\u209a(x) [KL(p(\u03b8 | S\u03c8\u2217 (X))||q\u03c6(\u03b8 | Sp\u2217(X)))] E\u209a(x) [KL(p(\u03b8 | X)||p(\u03b8 | S\u03c8\u2217(X)))]\n< L(\u03c8\u2217, \u03c6) + C' + E\u209a(x) [KL(p(\u03b8 | X)||p(\u03b8 | S\u03c8\u2217 (X)))]\n= L(\u03c8\u2217, \u03c6) + C'\nwhere the inequality is derived from Theorem 1 and the last equation follows from Assumption 1. A\ncomplete proof is deferred to Appendix C.1.\nThe gap between theory and practice in this context arises because we never achieve a perfectly\noptimal summary network sp\u2217 (X) that fully captures sufficient statistics for X. The assumption\nthat p(0 | X) = p(0 | sp\u2217 (X)) is an idealization that results in Theorem 2. In practice, however,\nthe summary network has finite capacity and is trained on finite data, so it can only approximately\nencode all the information in any given data set. Despite this, the result remains relevant: modern\nsummary networks are highly flexible and capable of representing complex relationships, enabling\nthe approximate posterior to be quite close to the truth, even if perfect sufficiency is out of reach in\npractice.\nWe jointly train \u03c8 and \u03c6 using the loss function L(\u03c8, \u03c6) in (4), as described in Algorithm 1 in the\nAppendix. Posterior samples are drawn using Algorithm 2."}, {"title": "4 Empirical results", "content": "4.1 Overview of benchmark suite\nOur benchmark problems fall into three categories: no-encoder, IID, and sequential. We provide\ndetails of all problems and model settings in Appendix F; here, we briefly describe the problems and\ntheir encoders.\nIn the first group (\"no-encoder\" problems), we pass either a single data point X directly to the decoder,\nor a vector of sufficient statistics. Our goal is to study \"pure decoder\" scenarios which do not require\nthe NPE model to learn an encoder jointly with the decoder.\nIn the second group (IID problems), each simulated data set X(m) consists of many IID observations\nfrom the data generating process, of varying sample sizes Nm ~ U(Nmin, Nmax). In our main\nexperiments, we use an encoder/summary network based on DeepSets [27] for all IID problems. We\nalso ran secondary experiments using attention-based summary networks [11] and give results in the\nAppendix C.3. We attempted to run experiments used summary networks based on Janossy pooling\n[15], but these networks performed very poorly when small enough to be computationally tractable.\nIn the third group (sequential problems), each data set X(m) consists of sequential data of varying\nsequence lengths Nm ~ U(Tmin, Tmax). For these problems we use a summary network based on\nbidirectional LSTMs, as in [19], to capture the temporal characteristics of the data. The biLSTM\noperates as a sequence-to-sequence mapping; to get a fixed-dimensional summary, we concatenate\nthe last biLSTM cell states in both the forward and backward directions.\nConditional diffusions and normalizing flows have very different structures, but we made an effort\nto construct versions of each model class that had roughly the same number of total parameters\n(204K for diffusions, 215K for flows), to ensure a fair test. Details of model architectures are in\nthe Appendix B.2 and D. We trained each model on the same amount of data, sufficient to ensure\nconvergence of both models. We repeated this 10 times for each problem and averaged the results.\nOne operational challenge is that flows and diffusion models are optimizing different losses that\ncannot be directly compared. Moreover, without knowledge of the true posterior, it is impossible\nto validate NPE methods by calculating test-set loss versus a ground truth. Instead, we rely on two\ndistinct approaches for measuring the quality of a neural posterior approximation: simulation-based"}, {"title": "4.2 Findings and discussion", "content": "Table 1 shows the results of our benchmarking experiments. Training times were 35% faster for\ndiffusions (1.752s per batch, on average across all problems) than for normalizing flows (2.674s per\nbatch, on average). Inference times are slower for diffusions (30.25 milliseconds, vs. 22.47ms, on\naverage), although this is a minor inconvenience; since NPE is an amortized inference method, much\nmore time is devoted to training than to inference. All experiments were conducted on a system with\n8 NVIDIA A5000 GPUs, and an AMD EPYC 7702P 64-Core CPU used to simulate data. Each\nindividual training run was done on a single GPU.\nThe results, along with Figures 1\u20133, indicate that conditional diffusions outperform normalizing flows\nacross most problems. In the No Encoder group, conditional diffusions perform better across the\nboard, with lower average Wasserstein distances (WD avg) and worst marginals (WD worst). They\nparticularly excel in challenging problems such as the Witch's Hat, where conditional diffusions\nachieve a significantly better average and worst Wasserstein distance compared to normalizing flows\n(26.579 vs. 103.921 for average WD). In the IID group, conditional diffusions also consistently\noutperform normalizing flows, with lower average WD and ECP. For example, in the Multivariate\nG-and-K distribution, conditional diffusions achieve a much lower average WD (29.336 vs. 54.914)\nacross the marginals, although the ECP values are similar. This is potentially explained by the\ndependence of the ECP metric on the choice of reference distribution; ECP can be notably less\nsensitive at detecting deviations from p(\u03b8 | y) when the reference distribution is not tuned well.\nFinally, in the Sequential group, the results are more mixed, although still favorable for diffusions.\nWhile conditional diffusions perform substantially better on Lotka-Volterra and fractional BM,\nnormalizing flows slightly outperform conditional diffusions in the Markov switching model. The\ndiffusion model also exhibited stable convergence on all metrics across all problems, whereas the\nnormalizing flow showed diverging WD and ECP metrics on some problems, indicating poor posterior\napproximations despite apparently (convergent) training loss. See Figure 4."}, {"title": "A Training and inference algorithms for conditional diffusions", "content": "The main training loop for conditional diffusions, wherein we jointly train a summary network and\ndiffusion decoder, is described in Algorithm 1.\nAfter training the summary network s\u03c8 and \u03f5\u03c6, we are ready to sample from p(\u03b8 | X) for a given\ndataset X, as described in Algorithm 2. Note that the sampling process applies to any general\ndiffusion process, such as VP, VE [21], or EDM [8]. In our paper, we utilize the EDM [8] training\nand sampling process, which is detailed in Appendix B.2, where, for notational simplicity, we define\n\u03b8\u03c4 := {0[i]}\u1d62=1"}, {"title": "B Diffusion model theorems and implementation details", "content": "B.1 Proof of Theorem 1\nB.1.1 Preliminaries\nHere, we first illustrate the connection between score function prediction, mean prediction, and noise\nprediction. For notational simplicity, we use \u03f5(\u03b8\u209c, t) for the score function, \u00b5(\u03b8\u209c, t) for the mean\nprediction, and e(t, t) for the noise prediction. By considering the process of adding noise:\n\u03b8\u209c = \u03b1(t)\u03b8\u2080 + \u03b2(t)\u03f5\n\u03f5 ~ N(0, I),\nwe can train these functions using the following loss:\nE\u209a(t),p(\u03b8),p(X|\u03b8)E\u209a(\u03f5) [\u03bb(t)||\u03f5\u03c6(\u03b8\u209c | X,t) \u2212 \u03f5||\u00b2]\nE\u209a(t),p(\u03b8),p(X/\u03b8) E\u209a(\u03b8\u209c|\u03b8\u2080) [\u03bb'(t)||\u03f5\u03c6(\u03b8\u209c | X, t) \u2212 \u2207\u03b8\u2081 log p(\u03b8\u209c | \u03b8\u2080)||\u00b2]\nE\u209a(t),p(\u03b8),p(X|\u03b8) E\u209a(\u03b8\u209c\u03b8\u2080) [\u03bb'(t)||\u00b5\u03c6(\u03b8\u209c | X, t) \u2212 \u03b8\u2080||\u00b2]\nIt is proven by Song et al. [20, 21], Ho et al. [7], Ho and Salimans [6] that the optimal solution of\nthese loss functions has the following connection:\n\u00b5(\u03b8\u209c, t) =\n\u03b8\u209c + \u03b2(t)\u00b2\u03f5(\u03b8\u209c, t)  \u03b8\u209c \u2212 \u03b2(t)\u00b7 e(\u03b8\u209c, t)\n\u03b1(t)  \u03b1(t)\nSince the optimal solution corresponds to different reparameterizations of the score function \u03f5(\u03b8\u209c, t),\nwe can use any of these reparameterizations to illustrate the proof."}, {"title": "B.1.2 Complete Proof of Theorem 1", "content": "Lemma 1.\nE\u209a(x)E\u209a(\u03b8|x) [\u2212log p\u03c6(\u03b8 | X)", "\u03f5||\u00b2": "C.\nProof. By Equation (3", "7": "we can adopt the following inequality: For any given X", "holds": "nE\u209a(\u03b8|x) [\u2212logp\u03c6(\u03b8 | X)"}, {"\u03f5||\u00b2": "C.\nThus", "that": "nE\u209a(x)E\u209a(\u03b8|X) [\u2212log p\u03c6(\u03b8 | X)"}, {"\u03f5||\u00b2": "C.\nComplete Proof of Theorem 1.\nE\u209a(x) [KL(p("}]}