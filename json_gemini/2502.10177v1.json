{"title": "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning", "authors": ["Mingcong Lei", "Yiming Zhao", "Ge Wang", "Zhixin Mai", "Shuguang Cui", "Yatong Han", "Jinke Ren"], "abstract": "A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.", "sections": [{"title": "Introduction", "content": "In embodied intelligence, the ability of agents to perform complex tasks in dynamic environments depends on their capabilities for long-term planning, reasoning, and adaptability. Despite recent advances in Artificial Intelligence (AI) systems powered by Large Language Models (LLMs), such as open-world games [1], dialogue systems [2], and personalized recommendation systems [3], there remain significant challenges in their applications to embodied intelligence [4, 5]. In particular, limited memory capacity prevents agents from effectively integrating historical information and adapting to evolving environments, resulting in reduced decision-making accuracy and poor task execution in long-horizon scenarios [6, 7].\nIn recent years, researchers have explored various approaches to address this issue, such as Reinforcement Learning (RL) with memory augmentation [8], dynamic memory networks [9], and task reasoning using Knowledge Graphs (KGs) [10]. While these methods have achieved notable successes, they often struggle to jointly model spatial and temporal information, which are essential for reasoning and planning in dynamic environments. Furthermore, existing approaches typically rely on fixed plans or limited context, which hinders adaptability and robustness in multi-task scenarios."}, {"title": "Related Work", "content": "The construction of embodied agents can be categorized into three approaches, including rule-based and traditional planning methods [13], RL methods [14], and LLM-based methods [15].\nRule-based and Traditional Planning Methods: Rule-based and traditional planning methods rely on expert-designed rules or algorithms to solve tasks, making them suitable for simple tasks in static environments. For example, the A* algorithm [16] excels in path planning but suffers from lower efficiency and accuracy in dynamic or unstructured environments [17]. Model-based planning [18] and constraint-based planning [19] attempt to improve adaptability to complex environments, but challenges persist when handling uncertainty and dynamic changes.\nRL Methods: RL enables agents to learn task strategies through interaction with the environment and has achieved notable success in various domains, such as AlphaZero [20] in board games. However, RL often faces issues in slow convergence and policy instability in high-dimensional action spaces and sparse reward scenarios [21, 22]. Additionally, RL performs poorly in tasks requiring global planning and long-term memory due to its focus on short-term returns [23, 24]. To address these challenges, researchers have proposed model-based RL [25, 26] to improve sample efficiency by constructing environment models, but these methods still struggle with model complexity and generalization. Deep RL [27] utilizes neural networks to enhance state representation but faces computational and data efficiency challenges in multi-task and high-dimensional scenarios [28, 29]. Furthermore, hierarchical RL [30] and meta-RL [31, 32] offer new approaches to handle long-term planning and task transfer, demonstrating potential in dynamic and complex environments. Despite these advances, RL still struggles with stability, computational overhead, and sample efficiency, particularly in the context of complex embodied tasks.\nLLM-based Methods: The introduction of LLMs has significantly enhanced the reasoning and task-handling capabilities of agents [33\u201335]. LLMs like GPT [36] and Gato [37] leverage self-supervised learning to process multimodal data and excel in natural language understanding and open-world tasks [38]. However, existing LLM-driven agents exhibit limitations in long-term planning and dynamic task environments, manifesting two key issues\u2014(1) Memory limitations: LLMs rely on autoregressive generation models and are unable to track task context or effectively store historical information; (2) Spatio-temporal reasoning deficits: LLMs perform reasoning based on pattern matching, lacking the ability to model spatio-temporal relationships in dynamic environments.\nRecently, researchers have proposed several approaches to address these issues. For example, ReAct [11] enhances task planning by introducing reflective and multi-step reasoning. However, ReAct's reasoning process relies on manually set few-shot samples, which limits its generalization. Reflexion [39], building upon ReAct, incorporates a self-reflection mechanism that allows agents to accumulate experience through multi-step trial and error. However, in embodied environments, errors may not be recoverable, limiting the effectiveness of this trial-and-error learning. Swiftsage [40], inspired by human dual-process theory [41] and fast-slow thinking [42], combines these modules to handle complex tasks. However, its open-loop architecture fails to adequately support long-term memory and dynamic planning. AdaPlanner [43] proposes a closed-loop architecture where an initial plan is refined based on environmental feedback. Nevertheless, it lacks a memory system, limiting its adaptability to long-horizon planning tasks. Hippo RAG [44] mimics the human hippocampus [45] and introduces KGs as long-term memory indices [46], significantly enhancing knowledge retrieval. However, these methods are still confined to short-term reasoning and lack support for long-term planning in dynamic environments."}, {"title": "Problem Formulation", "content": "We model the agent-environment interaction as a Partially Observable Markov Decision Process (POMDP) [47] extended with spatio-temporal reasoning. The POMDP is defined by the tuple (S, \u0391, \u03a9, \u03a4, \u039f), where S is the state space, A the action space, \u03a9 the observation space, T(s'|s, a) the transition dynamics, and O(o|s', a) the observation function.\nAt time-step i, the agent receives observation o\u00bf ~ O(\u00b7|si, ai\u22121) and maintains a belief b\u2081 \u2208 \u2206(S), a probability distribution over states conditioned on the interaction history:\n$b_i(s) = P(s_i = s|o_{1:i}, a_{1:i-1}).$"}, {"title": "Robotic Spatio-Temporal Memory Agent", "content": "As illustrated in Figure 2, STMA is composed of three key components, including temporal memory, spatial memory, and a planner-critic module."}, {"title": "Temporal Memory", "content": "Temporal memory maintains sequential event records to support time-dependent reasoning under partial observability. It addresses two key challenges: (1) LLMs struggle with long, unstructured histories, and (2) raw observations contain redundant details irrelevant to current decisions. Our solution combines raw data preservation with adaptive compression through two components:\nHistory Buffer. A first-in-first-out queue storing raw interaction tuples is defined as:\n$H = {h_i}_{i=1}^n,$\nwhere hi = (oi, ai) preserves the complete interaction history. This ensures no information loss while providing temporal grounding for downstream processing.\nSummarizer. Directly feeding raw interaction sequences to LLMs risks overwhelming them with redundant details and disordered temporal information. Our summarizer mitigates this via three"}, {"title": "Spatial Memory", "content": "Spatial memory aims to provide the LLM with direct spatial information to address the model's limitations in spatial reasoning. By incorporating spatial memory, LLM can process and reason about spatial relationships, thereby improving its performance in tasks requiring spatial awareness and decision-making.\nAs shown in Figure 2, the spatial memory is composed of four modules, including a relation retriever, a KG, a retrieve algorithm, and a relation aggregator.\nRelation Retriever. Inspired by HippoRAG [44], the relation retriever module leverages LLMs to derive spatial relationships from the temporal belief bt, which encapsulates compressed historical information. This module extracts structured representations of spatial interactions as a set of semantic triples:\n$G' = {r_i | r_i = (x_1, x_r, x_j)},$\nwhere x denotes the subject entity, x the relationship type, and x the object entity. Formally, the retriever operates as a mapping function:\n$R: b_t \\rightarrow G',$\nwhich enables the translation of temporally condensed beliefs into explicit spatial relational knowledge. This process bridges temporal reasoning with spatial awareness, which is critical for complex environment navigation.\nDynamic KG. To enhance system scalability and maintain stable long-term spatial memory, we formalize spatial relationships as a dynamic KG. The graph G(V, E) comprises vertices V (entities) and edges E (relationships), updated iteratively as the agent interacts with the environment. At each step, newly extracted relationships G' replace outdated edges in G, ensuring the graph reflects current spatial configurations. This dynamic update mechanism allows the agent to adapt to environmental changes while mitigating memory staleness. By decoupling transient observations from persistent relational structures, the KG provides a robust foundation for spatial reasoning and plan refinement.\nRetrieve Algorithm. To extract task-relevant subgraphs from the KG while balancing semantic relevance and relational diversity, we propose a two-step retrieval process\u2014(1) Semantic Filtering: Compute cosine similarity between the query embedding (derived from task/environment context) and entity embeddings, retaining the top-n entities. (2) Relational Expansion: Perform K-hop neighborhood search from the filtered entities to capture local relational structures, forming the final subgraph.\nThis approach ensures the retrieved subgraph G, preserves both semantic alignment with the current context and spatial relationships critical for planning. The full procedure is formalized in Algorithm 1."}, {"title": "Planner-Critic Agent", "content": "The planner-critic module integrates spatio-temporal memory with systematic reasoning to generate robust action plans. As shown in Figure 2, it operates via two cooperative components:\nPlanner. At step i, the planner synthesizes the temporal belief b, spatial belief b, and current observation or to generate a subgoal gi and corresponding action sequence {ai:k}_1. Formally:\n$P(b^c_i, b^s_i, o_i) \\rightarrow (g_i, {a_{i:k}}_{k=1}^m).$\nThis dual-belief integration enables coherent planning by contextualizing current observations within historical patterns and spatial constraints. The planner is implemented as a single agent that employs Chain-of-Thought (CoT) prompting [48] to explicitly reason about the current environment state, task requirements, and memory-derived constraints before outputting structured plans. Moreover, we urge a planner to think about the consequences of the actions it plans to do before it decides on the output.\nCritic. Before executing each action aj at step j(j \u2208 [i, k]), the critic evaluates its validity using: (1) Temporal consistency with b, (2) Spatial feasibility per b, (3) Alignment with oj, and (4) Adherence to safety constraints. The evaluation function is given by:\n$C(a_j, b^c_i, b^s_i, o_i) \\rightarrow (p_j, f_j),$\nwhich outputs validity flag pj \u2208 {true, false} and feedback fj. If pj = false, the planner regenerates {\u00e2j:k} regarding f\u2081, creating an iterative refinement loop. This closed-loop process mitigates hallucinations by grounding decisions in spatio-temporal reality.\nSimilar to the planner, the critic is implemented by a single CoT LLM agent. We ask the critic to think of the possible consequences of the proposed action, decide whether this action aligns with the subgoal planner provided in the current stage, and check whether this action is out-of-date as the environment changes. This CoT process will improve the robustness of STMA in dynamic environments.\nThe tight integration of memory-augmented beliefs with plan-critique cycles enables robust decision-making in partially observable environments. In the appendix C, we formalize this procedure by Algorithm 2, which demonstrates how spatio-temporal reasoning and iterative verification enhance plan reliability."}, {"title": "Evaluation", "content": "Evaluation metrics. We define two evaluation metrics to assess the performance: (1) Success Rate (SR), which is the ratio of completed tasks to the total number of tasks in each difficulty level. This metric reflects the agent's ability to complete tasks across randomly generated scenarios; (2) Average Score (AS), which is the ratio of intermediate scores achieved to the maximum possible score in each scenario. An AS of 100% indicates that the task is completed in the given scenario."}, {"title": "Performance Evaluation", "content": "Table 1 shows the comparison of our agent framework with the three baseline frameworks across four difficulty levels. For each difficulty level, we report the AS across eight randomly generated scenarios, along with its standard deviation (represented as mean \u00b1 standard deviation). Additionally, the SR for each difficulty level is recorded.\nFor all models, AS and SR show a general downward trend as task difficulty increases. This reflects the effectiveness of our experimental design in creating a gradient of increasing complexity. As the environment becomes more challenging and tasks require more steps to complete, the SRs of all frameworks decrease. However, our framework consistently outperforms the baselines across all difficulty levels, regardless of the underlying model. The result stems from the integration of the spatio-temporal memory module, which enhances planning and execution capabilities. Specifically, our framework achieves an average improvement in SR of 12.5% (GPT-40 powered) and 31.25% (Qwen2.5-72b-instruct powered) compared to the best baseline. Similarly, the AS improves by an average of 11.15% (GPT-40 powered) and 24.7% (Qwen2.5-72b-instruct powered). Notably, the improvement is more pronounced for the open-source Qwen2.5-72b-instruct, demonstrating the framework's ability to enhance the performance of less powerful models.\nCompared to ReAct and Reflexion, STMA incorporates a spatio-temporal memory module and a planner-critic agent, which significantly enhance reasoning capabilities and improve the robustness of the agent through the critic mechanism. As a result, STMA demonstrates notable improvements in long-horizon planning tasks over ReAct. Besides, Reflexion's self-reflection mechanism proves effective in the Textworld cooking tasks, leading to a slight performance improvement over ReAct. However, experimental results indicate that this enhancement is relatively modest. While AdaPlanner also employs a critic-like mechanism, its reliance on Python code as the action space and a poorly"}, {"title": "Ablation Studies", "content": "We conduct ablation studies on agents powered by Qwen2.5-72b-instruct. The results are presented in Table 2.\nSpatio-Temporal Memory. In our experiments, completely removing the spatio-temporal memory module makes the agent unable to complete any task. For tasks involving unknown environments and requiring long-term planning, memory is crucial. Without a memory system, the large model is incapable of \"remembering the recipe,\" making it impossible to complete even the simplest cooking tasks. Since our experimental setup requires the agent to recall and execute the steps specified in the recipe, the absence of memory renders task completion unattainable. Consequently, STMA without a memory system scores zero across all difficulty levels. This highlights the critical role of memory in POMDP-based systems, where constructing a cognitive understanding of the world state heavily relies on a robust memory framework.\nSummarizer. The information in the history buffer is passed directly to the model as temporal belief in chronological order, without summarization (other components remain unchanged, with spatial memory still being extracted from summarized information). Results indicate that agent performance decreases slightly for simpler tasks (Levels 1 and 2) but drops significantly for more complex tasks. This suggests that for long-horizon tasks, the growing length of the history leads to an increasingly long prompt, which degrades the agent's performance. Additionally, the longer history dilutes the density of useful information, making the LLM more susceptible to irrelevant data. The summarizer mitigates this issue by condensing the temporal memory, reducing the adverse effects of long history as the number of steps increases. Therefore, the Summarizer is critical for STMA's efficacy in complex scenarios, ensuring efficient information flow and robust decision-making by balancing detail with concision.\nSummarizer for Spatial Memory. The information in the history buffer is passed directly to the spatial memory module in chronological order (while summarized information is still used as a temporal belief to control variables). Results show that agent performance decreases as task"}, {"title": "Conclusion", "content": "This paper proposes a new STMA framework for long-horizon embodied tasks, demonstrating the great potential of spatio-temporal memory mechanisms in task planning. Experimental results show that STMA performs exceptionally well across various task difficulty levels. Notably, when tackling complex tasks, STMA outperforms agents relying on random exploration by providing more precise and efficient solutions through rapid strategy adjustments and higher task completion rates. It is noteworthy that even with open-source models like Qwen2.5-72b, STMA achieves performance comparable to proprietary models, validating the superiority of the spatio-temporal memory module. Future work may optimize the spatio-temporal memory module to enhance its adaptability in more complex tasks."}, {"title": "Experiment Settings", "content": "Task Settings. We design four progressive difficulty levels with the following characteristics (summarized in Table A.1): Level 1 requires no ingredient search in a 6-room environment, while higher levels (2-4) introduce larger environments (9-12 rooms), more required ingredients (1-3), and longer recipes (6-10 steps). All tasks enforce a 50-turn limit to prioritize deliberate planning over random exploration.\nDifficulty Settings. Environments feature three key constraints: (1) Ingredients require knife processing (cut/dice/slice) before cooking, (2) Three cooking methods (grill/fry/roast) require specific appliances (BBQ/stove/oven), (3) Randomized room layouts with door connections requiring explicit open actions. Agent starting positions are randomized per scenario.\nScoring Mechanism. As detailed in Table A.1, maximum scores increase with difficulty (4-13 points). Agents earn 1 point per: ingredient collection, correct appliance usage, proper ingredient processing, and final meal preparation. Critical failures include: incorrect appliance/processing selection, repeated heat application causing burns, and turn limit expiration. Game rules are explicitly provided to agents via prompts to focus evaluation on strategic execution rather than knowledge acquisition.\nCommon settings. To prevent large models from obtaining scores through random action generation and to better evaluate their planning capabilities, we impose a constraint that each task must be completed within 50 turns.\nThis configuration isolates evaluation to strategic reasoning through three control mechanisms: explicit rule provision, randomized spatial configurations, and time-constrained execution.\nThe sample instructions for constructing games in different difficulty levels are shown below:\nLevel 1:\ntw-make tw-cooking --recipe 1 --take 0 --go 6 --open--cook --cut output./game_0_1.ulx -f -v-seed 1001\nLevel 2:\ntw-make tw-cooking --recipe 2 --take 1 --go 9 --open--cook --cut output./game_1_1.ulx -f -v --seed 1001\nLevel 3:\ntw-make tw-cooking --recipe 3 --take 2--go 9 --open --cook --cut output./game_2_2.ulx -f -v --seed 20002\nLevel 4:"}, {"title": "Hyperparameters.", "content": "In our experiments, STMA utilizes three key hyperparameters:\nNumber of vertices retrieved in the first step of the retrieval algorithm In the first step of the retrieval process, we use cosine similarity to extract 8 vertices. This provides a sufficient initial set of vertices for the subsequent K-hop neighbor search.\nK value in the K-hop algorithm: During experiments, we set k = 3 for the K-hop algorithm. This choice is based on the complexity of the environment. Given that the task environment contains a maximum of 12 rooms, a depth of 3 ensures that the model can access at least three-hop neighbors, covering the majority of spatial relationships required for task completion.\nRecord length in the history buffer: We limit the history buffer to store records from the most recent 25 iterations. Since each task is constrained to a maximum of 50 iterations, maintaining a record of the last 25 iterations is sufficient and helps reduce the input length for the model. This not only minimizes memory usage but also lowers computational costs without compromising task performance."}, {"title": "Additional Experiments", "content": "To further illustrate the behavior of our agent in real embodied environments, we select two representative scenarios from all test cases. These scenarios are used to compare the performance of STMA with the best-performing baseline model in the same context. The goal is to highlight the advantages of STMA relative to other baselines within these scenarios. For this case study, all agents are powered by GPT-40."}, {"title": "Case 1: The Role of the Critic.", "content": "Environment 1 represents a scenario classified under Difficulty Level 2. Among the baseline models evaluated in this setting, Reflexion achieved the highest performance. Consequently, we conducted a comparative analysis between Reflexion and the STMA agent, focusing on a representative subtask of this environment. The experimental results are illustrated in Figure 5. In this scenario, both Reflexion and STMA successfully explored the kitchen environment and retrieved a recipe. The acquired recipe and the agent's initial inventory are displayed in the right panel of Figure 5. Both agents executed operations based on the recipe instructions. However, Reflexion failed to complete the task due to selecting an incorrect cooker for ingredient processing at a critical step. In contrast, STMA recognized the necessity of using the appropriate cooker and generated a logically coherent plan. Notably, the final step of STMA's initial plan proposed by the Planner constituted an invalid action within the Textworld environment. Upon execution, the Critic module identified this discrepancy, prompting the Planner to revise its strategy. After reformulation, the Planner successfully generated a valid sequence of operations adhering to procedural constraints.\nThis experiment demonstrates that the collaborative framework between the Critic and Planner in STMA enables the agent to achieve superior long-term planning capabilities while maintaining robustness against errors caused by LLM hallucinations or inherent limitations. Compared to baseline models such as Reflexion, STMA exhibits enhanced planning reliability and adaptability, underscoring the efficacy of its self-corrective mechanism in dynamic task environments."}, {"title": "Case 2: Demonstrating Collaborative Capabilities of Spatio-temporal Memory and Planner-Critic in the STMA Framework.", "content": "Environment 2 represents a Difficulty Level 3 task designed to evaluate the agent's capacity to handle intricate operational workflows and manage a larger set of required ingredients. Similar to Environment 1, we compare the performance of the STMA framework against Reflexion, the strongest baseline framework. A representative subtask is illustrated in Figure 6. In this environment, the agent is initialized in the \u201cGarden\" sub-environment, which contains the critical ingredient \"red"}, {"title": "Implementation Details", "content": "The overall framework is outlined in Alg. 2."}, {"title": "K-hop", "content": "After extracting the initial vertices, we utilize a K-hop graph search algorithm to perform relationship-based exploration. This involves retrieving all K-hop neighbors of the selected vertices within the KG. The algorithm enables semantically associated extraction, effectively performing \"associative reasoning\" through semantic connections."}, {"title": "Sample Prompt", "content": ""}, {"title": "Summarizer", "content": ""}, {"title": "Relation Retriever", "content": ""}, {"title": "Relation Aggregator", "content": ""}, {"title": "Planner", "content": ""}, {"title": "Critic", "content": ""}]}