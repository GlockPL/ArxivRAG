{"title": "Xinyu: An Efficient LLM-based System for Commentary Generation", "authors": ["Yiquan Wu", "Bo Tang", "Chenyang Xi", "Yu Yu", "Pengyu Wang", "Yifei Liu", "Kun Kuang", "Haiying Deng", "Zhiyu Li", "Feiyu Xiong", "Jie Hu", "Peng Cheng", "Zhonghao Wang", "Yi Wang", "Yi Luo", "Mingchuan Yang"], "abstract": "Commentary provides readers with a deep understanding of events by presenting diverse arguments and evidence. However, creating commentary is a time-consuming task, even for skilled commentators. Large language models (LLMs) have simplified the process of natural language generation, but their direct application in commentary creation still faces challenges due to unique task requirements. These requirements can be categorized into two levels: 1) fundamental requirements, which include creating well-structured and logically consistent narratives, and 2) advanced requirements, which involve generating quality arguments and providing convincing evidence. In this paper, we introduce Xinyu, an efficient LLM-based system designed to assist commentators in generating Chinese commentaries. To meet the fundamental requirements, we deconstruct the generation process into sequential steps, proposing targeted strategies and supervised fine-tuning (SFT) for each step. To address the advanced requirements, we present an argument ranking model for arguments and establish a comprehensive evidence database that includes up-to-date events and classic books, thereby strengthening the substantiation of the evidence with retrieval augmented generation (RAG) technology. To evaluate the generated commentaries more fairly, corresponding to the two-level requirements, we introduce a comprehensive evaluation metric that considers five distinct perspectives in commentary generation. Our experiments confirm the effectiveness of our proposed system. We also observe a significant increase in the efficiency of commentators in real-world scenarios, with the average time spent on creating a commentary dropping from 4 hours to 20 minutes. Importantly, such an increase in efficiency does not compromise the quality of the commentaries.", "sections": [{"title": "1 INTRODUCTION", "content": "With the advancement of natural language processing (NLP), particularly large language models (LLMs), numerous text-generation systems have been proposed to enhance the effectiveness and efficiency of individuals across various fields, such as education, medicine and law [1, 4, 32]. Commentary is a type of article that contains diverse arguments and compelling evidence, which aims to provide readers with a deep understanding of certain events. As Figure. 1 shows, a commentator usually spends several hours writing a commentary, which includes mining arguments, searching for evidence, and embellishing the article. Given the continuous nature of news, their workload is substantial. Therefore, exploring the application of LLMs in commentary generation is worthwhile.\nAlthough LLMs have benefited many generative tasks, they face challenges when directly applied to commentary generation due to unique task requirements. Broadly, the requirements for a commentary can be divided into two levels:\n1) Fundamental requirements:\n\u2022 The structure should be regular and complete. As Figure. 1 shows, the commentary should follow a total division structure.\n\u2022 The content should be self-consistent. For example, the arguments in the commentary should not be contradictory, and the evidence must support the arguments.\n2) Advanced requirements:\n\u2022 Arguments should be specific and original. The argument is key to the commentary, representing the author's stance.\n\u2022 Evidence should be convincing, which means the LLMs can't generate fake evidence, and the evidence is preferably new.\nIn this paper, we propose Xinyu, an efficient LLM-based system to assist commentators in Chinese commentary generation. Specifically, for the fundamental requirements, we decompose the generation into several sequential steps, ensuring the generated text is well-structured. We also design targeted supervised fine-tuning (SFT) and strategies for each step to maintain content consistency. For the advanced requirements, we propose an argument ranking model for ranking candidate arguments to ensure quality. Moreover, we construct a comprehensive evidence database, which maintains up-to-date events and books, and then use the technology of retrieval augmented generation (RAG) to generate convincing evidence.\nGiven the dynamic nature of commentary, traditional metrics for text generation tasks, such as ROUGE or BLEU, fall short in evaluating the overall quality of the commentary. Thus, we propose a comprehensive of evaluation metric for commentary generation that considers five distinct perspectives. In our pilot study, GPT-4 demonstrated performance on par with human annotators, so we employ GPT-4 as the evaluator. The experimental results underscore the quality of the content generated by our system, Xinyu. In practical terms, we also examined how Xinyu could enhance the work efficiency of human commentators and the result shows that with Xinyu, the speed of commentary generation increased dramatically, reducing the average creation time from 4 hours to a mere 20 minutes. Importantly, this increase in efficiency does not sacrifice the quality of the commentaries.\nTo sum up, our main contributions are as follows:\n\u2022 We leverage LLMs for the task of commentary generation and propose a system named Xinyu that can assist commentators in generating Chinese commentary 10 times faster with even quality.\n\u2022 We decompose the commentary and generate it in steps, applying targeted supervised fine-tuning (SFT) for each. This approach ensures the commentary meets its fundamental requirements: it is well-structured and self-consistent.\n\u2022 We propose an argument ranking module to improve the quality of the arguments and construct a comprehensive knowledge database (e.g., up-to-date events and books) for the generation of evidence with the help of retrieval augmented generation (RAG). This approach ensures the commentary meets the advanced requirements: it is specific and convincing.\n\u2022 We design a comprehensive evaluation method for the commentary generation task with 5 distinct perspectives. The experimental results demonstrate the effectiveness of our proposed Xinyu."}, {"title": "2 RELATED WORK", "content": "2.1 Large Language Models\nThe domain of Natural Language Processing (NLP) has witnessed substantial progress [14, 21, 30, 31, 42], especially through the advent of Large Language Models (LLMs) [2, 17, 18, 27, 35]. These models show exceptional text generation proficiency, yielding high fluency and readability outputs [32, 39]. Their ability to adapt to downstream tasks with minimal in-context examples is particularly noteworthy. To further augment the efficacy of LLMs in downstream tasks, two main methods have been identified: supervised fine-Tuning (SFT) and retrieval augmented generation (RAG).\nSupervised Fine-Tuning (SFT) entails the adaptation of an LLM to a specific downstream task. This process refines the model's parameters to align with the data distribution and task requirements, ensuring the model's behavior mirrors human behavior within the given domain. The topic of SFT has been extensively explored in numerous research. Ouyang et al. [18] pioneered the introduction of supervised fine-tuning and reinforcement learning to align language models with human intent. Zhou et al. [41] compiled a dataset of merely 1K examples for SFT, demonstrating that the success of SFT depends on the quality and diversity of data.\nRetrieval Augmented Generation (RAG) amalgamates LLMs with content retrieved from external databases. This approach offers a promising solution to the challenges encountered by LLMs, such as hallucination, outdated knowledge, and untraceable reasoning processes. The conventional RAG process encompasses indexing, retrieval, and generation [9, 15]. RAG has been further enhanced by a range of innovative techniques: fine-tuning retrieval models to obtain precise semantic representations [11, 28, 33], reformulating queries to align with the semantic space of queries and documents [8, 20, 29], fine-tuning LLMs to harmonize the output of the retriever with the LLM's preference [10, 22, 34].\nIn our work, we leverage the advances of both SFT and RAG to enhance the performance of the Xinyu.\n2.2 Domain-specific LLMs\nLarge Language Models (LLMs) have advanced the field of natural language processing, providing a task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, and the diversity of the constraints [40].\nNumerous researchers have devoted their efforts to domain-specific Language Models (LLMs) tailored for various fields. These specialized LLMs have been designed to cater to the unique requirements of domains such as medicine [12, 23, 24] for medical diagnosis, law [6, 36, 37] for handling legal documents, counselling [13] for mental health support, education [7, 16] for teaching assistance, science[3] for crafting scientific journalism, and so on.\nThe former domain-specific LLMs mainly focus on injecting domain knowledge (e.g., medical or legal knowledge) into LLMs. In this paper, our focus is on commentary generation, to support commentators in their writing process and produce well-structured, logically consistent commentaries that present novel arguments and convincing evidence."}, {"title": "3 PRELIMINARIES", "content": "This section is dedicated to defining key concepts that will be consistently referenced throughout this paper.\nPeg, within the scope of commentary generation, denotes the specific aspect of the event that the commentary is responding to or building upon. It acts as an anchor for the commentary. For instance, in Figure. 1, the peg is 'Age of smokers decrease'.\nMain Argument, in the context of commentary generation, signifies the central point that the generated commentary seeks to communicate. It forms the core message around which the commentary is structured.\nSupporting Argument, is an additional point that helps to substantiate the main argument. Typically, a commentary will contain several supporting arguments that collectively contribute to the strength and depth of the main argument.\nEvidence refers to the data, facts, or information employed to support the argument. In the process of commentary generation, evidence can be derived from the content itself or external sources. Given a peg, the corresponding commentary will include one main argument and several supporting arguments, all of which are supported by evidence."}, {"title": "4 TECHNICAL ROUTE OF XINYU", "content": "In this section, we delve into Xinyu's comprehensive technical route. Figure. 2 shows the overall framework of Xinyu. In Section 4.1, we introduce the five main generative components used in detail during the commentary generation process. In Section 4.2, we shift our focus to the two auxiliary components essential for meeting the advanced requirements of the commentary, including the argument ranking model and the construction of an evidence database. Note that without these two auxiliary components, the system can still fulfill the fundamental requirements.\n4.1 Main Components\nBased on the structure, we decompose the commentary generation into five steps: peg generation, main argument generation, supporting argument generation, evidence generation, and finally, article combination. This sequential approach is implemented with the help of SFT and RAG.\n4.1.1 Peg Generation. The peg generation serves as a preliminary step in the commentary generation process, designed to swiftly summarize event details for the user. Utilizing a search engine, this component retrieves event details based on given keywords to generate a peg. Alternatively, users have the option to manually compose the peg, bypassing this automated step. Specifically, the content from the top three most relevant search results is processed as input, and the LLM condenses this information into a concise peg. The procedure is exemplified as follows:\n[You are a commentary writing expert, and here are the details of an event. Event detail: {event detail}. Please refine it into a concise and well-articulated peg:]\nTo enhance the model's proficiency in condensing event details during peg generation, we develop SFT data specifically for this step. By inputting event details and using the peg as a label, this method trains the model to more effectively summarize and pinpoint essential information, resulting in pegs that are informationally dense.\n4.1.2 Main Argument Generation. This step aims to provide the main argument. Due to the variety of pegs, main arguments can be driven in different directions. Here the strategy involves directing the LLM to generate across ten distinct directions: technology, finance, society, politics, literature and arts, lifestyle, environment, sports, education, and science. Each direction emphasizes its specific thematic elements, such as highlighting technological advancements or economic trends. To operationalize this strategy, we combine the peg, event details, and a chosen direction as input for the LLM, which then generates candidate main arguments one at a time. An example of the usage is below:\n[You are a commentary writing expert. Please complete the main argument in the direction of {direction} based on the peg: {peg} and event detail: {event detail}. The main argument should be profound, concise, and strongly related to the peg. Please provide the main argument:]\nTo enhance the model's ability to generate the main arguments, we design the corresponding SFT data. The input for this SFT data includes retrieved event details based on the peg and the direction of the article. The label for this SFT data is the main argument derived from the input. This data construction approach not only facilitates the generation of a helpful main argument for diverse article directions but also ensures its consistency with the initial peg, thus guaranteeing both relevance and alignment in the narrative.\nTo meet the advanced requirements of argument, these generated main arguments will then get a score from the argument ranking model, based on their novelty, and objectivity. Then these candidate main arguments will be ranked based on the scores.\n4.1.3 Supporting Argument Generation. This step aims to generate supporting arguments that seamlessly align with both the main argument and the event's details. To achieve this, the system synthesizes the main argument, event details, and a predefined argument structure. Available argument structures include parallel, progressive, and contrasting formats, each facilitating a unique commentary structure. This integration process enables the LLM to produce relevant supporting arguments. The LLM will decide the number of supporting arguments m itself. An example of usage could be:\n[You are a commentary writing expert. Based on the given main argument {main argument} of the commentary and event detail {event detail}, generate multiple supporting arguments for the commentary. The supporting arguments form {structure} structure, refining around the {main argument} with multi-level, multi-faceted, and multi-angle perspectives. Please provide the supporting arguments:]\nThe SFT data for this step is constructed to facilitate this process. We utilize inputs comprising event details, main arguments, and argument structures. The labels are the marked corresponding supporting arguments. This data construction approach ensures that the model is fine-tuned to produce supporting arguments that enrich and substantiate the main argument effectively.\n4.1.4 Evidence Generation. In the Evidence Generation step, the system aims to generate accurate and contextually relevant evidence. The process begins with accessing reference information of a supporting argument from the evidence database to ensure veracity, effectively mitigating the hallucinations. This reference information, along with the provided main and supporting arguments, serves as the input. Then, the LLM will generate evidence that is tailored to align precisely with the given supporting argument. An example of the usage is below:\n[You are a commentary writing expert. Surrounding the main argument {main argument}, please use the evidence provided in the reference information, including dates, data, viewpoints, core content, etc., to continue writing evidence in the commentary to support the supporting argument supporting argument}. Please annotate the corresponding reference information numbers in the continuation. Reference information: {reference}. Please provide the evidence: ]\nThe SFT data for evidence generation is structured with inputs including reference information, main and supporting arguments. The output label is the evidence associated with the corresponding supporting argument. These elements guide the model in generating evidence that is precise and contextually relevant to the provided supporting arguments.\n4.1.5 Article Combination. This step is aimed at generating the title and ending, then forming the overall commentary. To achieve this, the system integrates the preceding event details, main arguments, supporting arguments, and evidence as inputs. Following this integration, the LLM then outputs the title and ending. An example of the usage for ending generation is below:\n[You are a commentary writing expert. Please write a conclusion for the article, maintaining smooth language, consistent style, and logical coherence with the preceding text. The preceding text is as follows: {preceding text}. Please provide the ending:]\nThe usage for title generation is similar to the ending generation.\nTo facilitate the model in generating context-appropriate and coherent title and ending, the SFT data is constructed with inputs including the event details, main argument, combined supporting arguments and evidence. The label for this data is the corresponding title and ending. This structured approach ensures that the model is adept at crafting titles and endings that effectively encapsulate the various dimensions of the commentary, providing a fitting start and end to the narrative.\nAfter generating the title and ending, the system will combine all the output to form a complete commentary."}, {"title": "4.2 Auxiliary Components", "content": "In this section, we introduce two auxiliary components that assist Xinyu in meeting advanced requirements: the argument ranking model and the evidence database.\n4.2.1 Argument Ranking Model. The argument ranking model plays a crucial role in the main argument generation process by assessing and ranking candidate main arguments. This aids users in selecting the most compelling argument.\nDeveloping the ranking model presents a central challenge due to the subjective nature of assessing arguments, which lack universally accepted standards, unlike quantifiable metrics. For example, evaluating the generated arguments based on factors like novelty is cumbersome.\nTo address this challenge, we train a BERT-based scoring model with a pairwise loss function. This approach converts the ranking challenge into a series of binary comparisons, simplifying the task to discerning relative superiority between pairs of arguments. The loss function is defined as:\n$L(x) = \\sum I(f(x_a) \u2013 f(x_b))$\nwhere $f(x)$ represents the scoring function for a given argument $x$, and $I$ is a non-linear transformation applied to the calculated difference between the two arguments $x_a$ and $x_b$.\n4.2.2 Evidence Database Construction. In pursuit of generating convincing evidence, we construct an Evidence Database to store Chinese knowledge sourced from events and books for retrieval.\nFor the events knowledge, we first legally collect the daily up-dated article titles on the website's hot list, and then prompt the LLM to complete the following four tasks given the article title: (1) summarize the event related to the article title; (2) determine which direction (e.g., technology, finance) the event belongs to; (3) extract the six elements of the event, including time, location, person, cause, process and result; (4) describe the event in a paragraph based on the six elements.\nFor knowledge from books, we gather classic works in law, finance, and various other subjects legally, segmenting the contents of these books into chunks and storing them within the database.\nThe evidence database is built upon 200,000 event knowledge data and 110,000 book knowledge data. Following the construction phase, we implement the ElasticSearch engine, anchored to the evidence database, to enhance retrieval capabilities. During retrieval, the supporting argument is inputted, prompting the fetching of the k most pertinent references from the database. These references are then fed into the Large Language Model (LLM) to generate evidence in support of the arguments. In practical application, the value of k is determined by the similarity score between the input argument and the existing knowledge, and we set a predefined threshold at 0.6 in the experiment. In addition, to maintain access to the most current event knowledge, we continuously collect data from online platforms and update our database daily."}, {"title": "5 EXPERIMENT", "content": "5.1 Evaluation Metrics of Commentary\nAutomatic evaluation. Existing automatic generation evaluation metrics, including but not limited to ROUGE and BLEU [19], mainly focus on the degree of similarity to a reference text. However, in the context of commentary generation, the inherent diversity of the commentary content poses a significant challenge to these similarity-based metrics, often leading to an incomplete evaluation. To address this limitation, we propose a novel evaluation metric that assesses commentaries across five distinct dimensions: Structure Soundness: clarity of the hierarchy, compactness of the writing, and rationality of the layout; \u2022 Logic Consistency: consistency of the content, rationality of the argument, and thoughtfulness; \u2022 Argument Quality: freshness and directionality of the topic conception; Evidence Support: specificity and appropriateness of the evidence used; and Language Finesse: fluency, depth, and vividness of the expression style. Besides, we calculate the average of the five scores as Overall.\nThe prompt templates are as follows:\n[You are an expert in scoring generated commentaries. Please rate your answers from the {perspective} perspective based on the provided commentary. The scoring criteria are:\n(1) 10 points represent... (2) 8 points represent... (3) 6 points ...\nPlease output a line that contains only one value representing the score. Please avoid any potential biases, and ensure that there are no factors other than the text that affect your judgment.]\nThese dimensions are chosen to encompass both the fundamental and advanced requirements of commentary. Structural soundness and logical consistency constitute the fundamental requirements, ensuring a well-organized and logically coherent commentary. Conversely, the quality of argumentation and the adequacy of evidentiary support represent the advanced requirements, reflecting the depth and persuasiveness of the commentary. The dimensions are scored on a scale of 1-10, with 1 being the lowest and 10 the highest.\nIn our experiments, we utilize GPT-4 for automatic evaluation by crafting specific prompts for each dimension. To validate GPT-4's accuracy, we compare its scores against those from human annotators for 30 randomly selected commentaries, calculating the Pearson correlation coefficient [5] for each dimension. As Tab. 1 illustrates, the Pearson correlation coefficient of each dimension surpasses 0.6, which proves GPT-4 is competent for this task.\nHuman evaluation. In our ablation study, we assess the Timeliness of evidence. Due to the training limitations of GPT-4, which is based on data available only up to a specific date, it is not equipped to accurately ascertain the recency of evidence. Therefore, this aspect is evaluated through human judgment. The scoring for this metric ranges from 1 to 10, where 1 represents the lowest and 10 the highest possible score.\n5.2 Experiment Settings\nImplementation. The base model of Xinyu is LLaMA2-13B [27], and we specifically adapted it to better accommodate the nuances of the Chinese language. This adaptation involved expanding the LLaMA-13B tokenizer with an additional 28,000 Chinese tokens. To further optimize the model, we continued pre-training on LLaMA-13B using a corpus comprising 500B tokens, which contains both English and Chinese corpus. For supervised fine-tuning, we not only utilized the dataset introduced in Section 4 but also incorporated the general SFT dataset to maintain consistency with the data distribution of previous training phases. The amount of SFT data is 400,000 and the distribution of it is shown in Figure. 3. Our training process leveraged the Megatron-DeepSpeed framework."}, {"title": "5.3 Experimental Results and Analysis", "content": "Based on the model's size, we split them into two types: LLMs larger than 20B and LLMs smaller than 20B. Tab. 2 shows the results of commentary generation with GPT-4's evaluation. We report the results of the ablation study in Tab. 3, Tab. 4, Tab. 5, and Figure. 5.\nResults of commentary generation. From Tab. 2, we can conclude that: (1) Generally, the bigger the language model's size, the better it does. However, GPT-4's leading advantage in this task is not as pronounced as in other generative tasks. (2) When looking at models within the 20 billion parameter size, our method achieved the best results in most of the metrics. (3) Compared to large-scale LLMs such as GPT-4, our method attained the best overall score, primarily due to our superior performance in the advanced requirements of argument and evidence. (4) There's not a huge gap between the scores of the different methods. This is largely because GPT-4 is generally not harsh in its scoring, rarely giving out very low scores.\nResult of ablation study. From Tab. 3, we have the following observations: (1) Our framework significantly enhances the performance of large-sized base models. For instance, the overall score of Qwen-72B increased from 7.37 to 7.82. (2) GPT-4 achieved the best performance with an overall score of 8.3, and our Xinyu ranks just behind GPT-4. Considering the size of the model, our method has greater potential in practice. (3) For the 20B scale base models such as Qwen-14B-Chat, using our framework actually decreased their performance. This might be due to these base models' inherent limitations in generating text step-by-step. This also demonstrates the effectiveness of our SFT.\nFrom Tab. 4, we have the following observations: (1) The implementation of the argument ranking model has significantly improved the effectiveness of argumentation, underscoring its impact. (2) Enhancements are observed across all metrics, illustrating the interrelationship among these aspects of commentary.\nFrom Tab. 5, we can find that: (1) The implementation of retrieval augmented generation (RAG) significantly enhances the generation of evidence, with scores improving from 5.60 to 7.41. Additionally, the timeliness of the generated evidence also saw an increase, rising from 8.85 to 9.30. (2) After incorporating the book dataset into RAG, its performance experienced further improvements. (3) The improvement proves the effectiveness of our evidence dataset.\nFrom Figure. 5, we have the following observations: (1) Utilizing Xinyu's assistance can significantly increase writing speed, and the average time for a commentary speeds up from more than 4 hours to 20 mins. (2) Moreover, commentaries generated with LLMs have achieved the same scores as manual writing, demonstrating the practicality of our system."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce Xinyu, an innovative commentary generation system based on large language models (LLMs) designed to enhance the efficiency of commentators. Our approach involves breaking down the generation process into five steps, with supervised fine-tuning (SFT) applied to each step to ensure the output is well-structured and coherent, addressing the basic requirements of commentary. To fulfill the higher demands for novelty and persuasiveness, we develop an argument ranking model and employ retrieval-augmented generation (RAG) techniques for evidence generation. For RAG, we have compiled an evidence database comprising both current events and classical books. To better measure the generated commentaries, we design a comprehensive evaluation method with 5 distinct perspectives. Our comprehensive experiments demonstrate the system's effectiveness. Remarkably, in practical applications, Xinyu has reduced the average commentary creation time from 4 hours to just 20 minutes and maintained the quality.\nIn the future, we will consider the following directions to enhance our system: 1) Improve evidence recall accuracy, ensuring relevance to the arguments; 2) Utilize Reinforcement Learning with Human Feedback (RLHF) to better align commentaries with human preferences and specific writing styles."}, {"title": "A APPENDIX", "content": "A.1 Overall Generation Process\nThis section presents a complete example corresponding to each step in section 4.1. The overall process consists of Peg Generation -> Main Argument Generation -> Supporting Argument Generation -> Evidence Generation -> Ending Generation & Title Generation. All the content is translated from Chinese."}]}