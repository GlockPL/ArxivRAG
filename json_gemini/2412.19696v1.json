{"title": "An Integrated Optimization and Deep Learning Pipeline for Predicting Live Birth Success in IVF Using Feature Optimization and Transformer-Based Models", "authors": ["Arezoo Borji", "Hossam Haick", "Birgit Pohn", "Antonia Graf", "Jana Zakall\u00f3", "S M Ragib Shahriar Islam", "Gernot Kronreif", "Daniel Kovatchki", "Heinz Strohmer", "Sepideh Hatamikia"], "abstract": "In vitro fertilization (IVF) is a widely utilized assisted reproductive technology, yet predicting its success remains challenging due to the multifaceted interplay of clinical, demographic, and procedural factors. This study develops a robust artificial intelligence (AI) pipeline aimed at predicting live birth outcomes in IVF treatments. The pipeline uses anonymized data from 2010 to 2018, obtained from the Human Fertilization and Embryology Authority (HFEA). We evaluated the prediction performance of live birth success as a binary outcome (success/failure) by integrating different feature selection methods, such as principal component analysis (PCA) and particle swarm optimization (PSO), with different traditional machine learning-based classifiers including random forest (RF) and decision tree, as well as deep learning-based classifiers including custom transformer-based model and a tab transformer model with an attention mechanism. Our research demonstrated that the best performance was achieved by combining PSO for feature selection with the TabTransformer-based deep learning model, yielding an accuracy of 99.50% and an AUC of 99.96%, highlighting its significant performance to predict live births. This study establishes a highly accurate Al pipeline for predicting live birth outcomes in IVF, demonstrating its potential to enhance personalized fertility treatments.", "sections": [{"title": "1. Introduction", "content": "Assisted reproductive technologies (ART), particularly in vitro fertilization (IVF), have transformed the landscape of infertility treatment, offering hope to millions of couples worldwide [1]. Despite advancements in embryology and clinical practices, achieving consistent success in IVF remains a significant challenge [2]. Key outcomes, such as success in embryo implantation and live birth, depend on a multitude of factors; the complexity of IVF outcomes therefore stems from the intricate interplay of numerous factors that must align for a successful treatment [3, 4]. This includes patient age, hormonal profiles, clinical protocols, embryological characteristics, and even lifestyle or genetic factors, all of which contribute to the multifaceted nature of the process [5]. Each of these variables influences treatment success, making it challenging to predict outcomes and optimize protocols. As illustrated in Figure1, the IVF process involves key stages, each contributing to these outcomes, from patient evaluation and ovarian stimulation to embryo selection and transfer. Traditional methods for embryo selection and live birth prediction are often unable to integrate and analyze these multidimensional data aspects effectively as they primarily rely on static morphological grading systems, while foundational, are often subjective and limited in their ability to capture the complex dynamics of embryonic development and live birth outcomes [3].\nRecent advancements in machine learning and artificial intelligence (AI) have introduced a paradigm shift in IVF, providing tools to analyze vast and complex datasets with unprecedented precision [4]. AI and ML have revolutionized IVF by automating embryo evaluation, predicting implantation potential, and enhancing live birth outcomes [5]. These technologies address many of the limitations of traditional methods, offering unprecedented precision, consistency, and scalability. They enable the analysis of large and complex datasets, offering predictive insights that surpass the capabilities of traditional statistical models [6].\nOne of the most promising applications of AI in IVF is embryo selection, where AI can predict the likelihood of a live birth for individual embryos. Deep learning models, especially convolutional neural networks (CNNs), have shown remarkable success in automating embryo grading by analyzing time-lapse imaging data [8]. This technology helps identify embryos with a higher probability of resulting in a live birth, significantly enhancing decision-making during the IVF process. Annotation-free scoring systems, such as those described by Ueno et al. [7], have further streamlined the embryo evaluation process by eliminating the need for extensive manual input while maintaining high predictive accuracy. These models analyze morphogenetic parameters, such as pronuclear fading, cleavage patterns, and blastulation timing, providing dynamic insights into embryo development that were previously unattainable through static morphological assessments [8].\nBeyond embryo grading, AI has been employed to predict implantation potential with notable success. Machine learning algorithms, such as random forests and ensemble models, integrate morphokinetic data and patient characteristics to assess the likelihood of implantation. Studies by Bamford et al. [9] and Uyar et al. [10] have demonstrated the ability of these models to achieve area under the curve (AUC) values exceeding 0.75 for implantation prediction. Furthermore, reinforcement learning-based systems like Dyn Score could dynamically update predictions in real time, offering clinicians actionable insights into embryo viability [11]. These adaptive models represent a significant step forward in IVF decision-making, allowing for more personalized and precise treatment strategies.\nThe goal of IVF is to achieve a live birth, making the prediction of live birth outcomes a critical focus of AI research [2]. AI models, which integrate patient demographics, clinical data, and simple quantitative features from imaging modalities, have shown promise in this domain. For instance, studies by Huang et al. [12] and Jiang et al. [13] utilized voting ensembles of CNNs to predict embryo ploidy, resulting in significant improvements in live birth rates. These models enable clinicians to optimize treatment protocols and maximize the likelihood of success.\nFeature selection techniques also have been proposed to develop efficient AI-based methods to support the IVF process. Kragh et al. [14] explores distinctions between ranking embryos based on implantation potential and predicting probabilities of implantation success, as well as issues like dataset balancing, selection bias, and clinical applicability. By focusing on the most relevant features, these methods enhance model interpretability and reduce computational complexity without compromising performance. Studies by Ueno et al. [7] and Bamford et al. [9] have highlighted the importance of feature selection in improving the efficiency and accuracy of IVF-based predictive models.\nSeveral studies proposed promising AI methods for classifying live birth success as a binary outcome (success/failure). For example, Zhang et al. [15] employed an artificial neural network (ANN) model, McLernon et al.[16] applied a discrete-time logistic regression model, while Jones et al. [17] also utilized logistic regression. Sanders et al. [18] conducted a comparison of live birth rates using binary logistic"}, {"title": "2. Methodology", "content": "In this work, we have applied inclusion and exclusion criteria to enhance the quality and relevance of the data, ensuring it was appropriate for our binary classification task. To reduce the dimensionality of the dataset and improve model performance, we utilized two feature selection and reduction techniques: Principal Component Analysis (PCA) and PSO. For classification, we evaluated the performance of four different classifiers: Random Forest (RF), Decision Tree (DT), a transformer-based model, and a tab transformer-based model. Finally, we designed different experimental setups: the first used PCA features as input to all classifiers (Method-1 and Method-3, Figure 2), and the second used features provided by PSO as input to all these classifiers (Method-2 and Method-4, Figure 2). In total, we have eight classification models including PCA+RF, PCA+Decision Tree, PSO+RF, PSO+Decision Tree, PCA+Transformer based model, PCA+ Tab-transformer based model, PSO+Transformer based model, PSO+Tab-transformer based model."}, {"title": "2.1.1. The dataset used", "content": "For this study, we utilized the Human Fertilization and Embryology Authority (HFEA) dataset, an anonymized registry dataset that encompasses fertility treatments conducted from 2010 to 2018. Designed to enhance patient care and maintain strict confidentiality for patients, donors, and offspring, this dataset stands as one of the most comprehensive and longest-running repositories of fertility treatment records globally. With 665,244 patient records and an initial set of 94 features, it provides a detailed account of fertility treatment cycles, covering patient demographics, treatment protocols, and infertility causes. These attributes present a comprehensive view of the factors influencing IVF outcomes during this period.\nThe dataset includes both numerical and categorical variables, capturing a broad spectrum of critical factors. Key features encompass patient-specific details such as age at the time of treatment, number of prior IVF pregnancies, live birth outcomes, and specific infertility causes (e.g., tubal disease, ovulatory disorders, or male infertility factors). Additionally, we meticulously have recorded procedural details such as the type of eggs and sperm used (e.g., fresh, frozen, donor, or patient-derived), the number of eggs collected, and the number of embryos transferred. This level of granularity allows for an in-depth analysis of the variables affecting IVF success rates. The prediction performance of live birth success as a binary outcome (success/failure) is assessed in this study."}, {"title": "2.2. Data preprocessing pipeline for IVF data analysis", "content": "The preprocessing pipeline transformed raw IVF data (Section 2.1.1) into a clean and structured format for the AI pipeline's input. It began with column standardization, ensuring uniformity by converting names to lowercase and removing whitespace. We structurally aligned the datasets by reindexing and adding missing columns, and then consolidated them into a single DataFrame for holistic analysis. We removed columns with less than 1% non-null values to enhance data quality. We imputed missing values based on the data type. Finally, we numerically encoded categorical features and normalized numerical features to a [0, 1] range (Figure 2)."}, {"title": "2.3. Inclusion and exclusion criteria", "content": "Inclusion and exclusion criteria were defined to ensure a clean, complete, and relevant dataset for this study. These criteria were chosen based on the groundwork laid by Sadegh-Zadeh et al. [21], who used the same dataset and adhered to a set of inclusion and exclusion criteria conditions for data preparation and analysis. The dataset included subjects who met the following conditions: (1) they had valid (non-missing) values for the target variable, \"Live birth occurrence\"; (2) they provided data for at least one infertility-related cause, such as \"ovulatory disorder\"; and (3) their cycle history indicated a non-negative record of prior treatment cycles. These criteria ensured the inclusion of relevant cases with sufficient data for analysis. We applied these inclusion criteria and then implemented exclusion criteria to improve the quality of the data. We excluded subjects with missing information for \"elective single embryo transfer\", as this variable was crucial for the analysis. Additionally, we removed entries with logical inconsistencies, such as negative treatment cycles or conflicting treatment-related dates, to ensure data validity. By adopting these inclusion and exclusion criteria, this study ensured a high-quality dataset suitable for robust predictive modeling of IVF live birth outcomes. The number of subjects included in this study after exclusion criteria is 115,012."}, {"title": "2.3.1. Feature selection using particle swarm optimization (PSO)", "content": "Feature selection reduces the number of predictors and focuses on the most relevant ones [22]. In this study we have employed PSO as a feature selection method due to its efficient search for optimal solutions in large and complex spaces [23]. PSO is a nature-inspired optimization technique, modeled after the social behavior of bird flocking or fish schooling. Each individual component, called a \"particle,\" represents a candidate solution in the search space. These particles move through space by adjusting their positions based on their own experiences and those of neighboring particles, mimicking how animals in groups share information to find food or navigate environments [19]. PSO is particularly effective for solving complex optimization problems, including feature selection, where it can efficiently explore the vast combinatorial space of possible feature subsets. This study employs PSO to pinpoint the ideal feature subset for forecasting the success of live births. Each particle encoded a subset of features as a binary vector, where 1 indicated inclusion of a feature and 0 indicated exclusion of a feature."}, {"title": "Cost Function", "content": "The cost function in PSO evaluates the quality of each particle's solution (equation 1). In this study, the cost function is defined as below:\n$C = -(F1-P-N)$\nWhere C is the cost function value to be minimized by PSO, F1 is the F1-score of a logistic regression model are trained on the selected features. The F1 score balances precision and recall, making it suitable for imbalanced datasets like IVF outcomes. P is the penalty weight, a parameter controlling the trade-off between model performance and simplicity. N is the number of features selected by the particle. The goal of PSO is to minimize C, which indirectly maximizes the F1 score while penalizing larger feature subsets. This ensures the final feature set is both performant and understandable. The following steps outline how to select features using PSO:"}, {"title": "Algorithm 1: Pseudo code for the feature selection using PSO", "content": "Inputs:\nDataset with features: F\nCost function: -(F1-P. N)\nParameters: swarm size: S = 20, Maximum iterations: T= 1000, Inertia weight: w = 0.7, Cognitive acceleration\ncoefficient: c\u2081= 1.5, social acceleration coefficient: c2 = 2, penalty factor: P.\nOutputs:\nOptimal feature subset: Foptimal\nBest fitness value: Cbest\nProcedure:\n1. Initialization:\nFor each particle i = 1, 2, ..., S:\nInitialize binary position vector $x_{i} e{0,1}^{n}$ and velocity $v_{i} eR^{n}$\nEvaluate fitness: C\u1d62 = -(F1- P. N), N = $\u2211 x_{i} |j|$\nWhere x\u1d62 |j| indicates whether features j is selected.\nSet the personal best position: Pbest\u1d62 = x\u1d62\nEnd for\nSet the global best position: gbest = arg min $C_{i}$\n2. Optimization:\nWhile t \u2264 T:\nFor each particle i = 1, 2, ..., S:\nFor each feature j \u0454 {1,2, ..., n}:"}, {"title": "2.3.2. Dimensionality reduction with Principal Component Analysis (PCA)", "content": "Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of a dataset while retaining as much information as possible [24]. It does this by transforming the original data into a new set of orthogonal components, called principal components, which are ranked according to their ability to capture the variance within the data. In this study, we have applied PCA to the IVF dataset to reduce its dimensionality while retaining 95% of the data's variance. This process can remove irrelevant variations and reduce the computational complexity."}, {"title": "2.3.3. Random forest", "content": "Random Forest (RF) is an ensemble learning method that combines the outputs of multiple decision trees to improve predictive performance and reduce overfitting [25]. RF naturally evaluates feature importance by measuring the impact of each feature on prediction quality [26]. In this study, we have utilized a RF with 200 decision trees as estimators, each with a maximum depth of 10. Note that we restrict the depth of each tree to avoid overfitting and maintain interpretability. Moreover, criterion='gini' used Gini impurity to evaluate split quality."}, {"title": "2.3.4. Decision tree", "content": "A Decision Tree is a supervised learning algorithm used for classification and regression tasks [27]. It recursively splits the data into subsets based on feature thresholds, forming a tree-like structure where each internal node represents a decision based on a feature, and each leaf node represents an output prediction. Decision Trees are highly interpreted, as they clearly outline the decision-making process, making them particularly useful for understanding feature importance and validating selected features. In this study, we used the feature-extracted PCA and fed it into the decision tree with the following parameters: max_depth=10, limited depth to maintain simplicity, and criterion=gini."}, {"title": "2.3.5. Transformer-based model", "content": "A deep learning model based on the transformer architecture, known as a transformer-based model for classification, can solve classification tasks [28]. Vaswani et al. originally introduced transformer architecture in the \"Attention Is All You Need\" paper [29], and it has since become the foundation of many state-of-the-art models in natural language processing (NLP), computer vision, and other fields. The attention mechanism is a critical component of the transformer-based model, designed to analyze and interpret tabular data to predict IVF success. The attention mechanism enables the model to dynamically assign importance to specific features, capturing intricate relationships between them and improving the model's decision-making process. In this work, the dataset includes features such as patient age, sperm quality, number of embryos transferred, and other clinical parameters. These features often interact in complex ways. The attention mechanism dynamically determines which features are most important for predicting IVF success and adjusts their importance based on the context of the input data for each individual case. For instance, the attention mechanism may prioritize features such as the quality of embryos for older patients. Younger patients may receive more emphasis on features like the number of eggs retrieved.\nThe attention mechanism in this transformer-based model operates in the following steps:"}, {"title": "Step 1: Input transformation", "content": "The input data is composed of tabular features, which are referred to as input_dim features after feature selection. We treat each feature as a component of the input vector. The features are first projected into a higher-dimensional space using a dense layer to make them suitable for attention computation:\nX = Dense(x)"}, {"title": "Step 2: Scaled dot-product attention", "content": "The scaled dot-product attention mechanism computes the relationships between features:\nAttention (Q, K, V)=softmax $(Qk^T/\\sqrt{d_k})v$\nWhere Query (Q) represents the feature being queried, key (K) represents the importance of each feature relative to the query, and value (V) contains the actual feature data.\nEach feature attends to all other features, producing a matrix of attention scores that capture dependencies between them. The softmax function ensures that the attention scores sum to 1, creating a probabilistic weight for each feature."}, {"title": "Step 3: Multi-head attention", "content": "This work employs multi-head attention, dividing the input into multiple \"heads.\" Each head learns to focus on different types of relationships. For instance, one individual might concentrate on the correlations between patient age and success. Another head might emphasize sperm quality or treatment type. We have concatenated and transformed the outputs from all heads into a single vector, combining multiple perspectives."}, {"title": "Step 4: Residual connection and layer normalization", "content": "The input is added back to the attention output:\nx=Add(x,AttentionOutput)\nLayer normalization: The output is normalized to stabilize gradients and ensure smooth learning."}, {"title": "2.3.6. Tab transformer-based model", "content": "The TabTransformer model is a deep learning approach that combines structured datasets with a mix of categorical and numerical features [30]. The tab transformer uses self-attention mechanisms to capture dependencies between features, particularly among categorical features. Instead of representing categorical data using traditional encoding methods, it maps each category to a learned embedding vector. These embeddings allow the model to capture semantic relationships between categories. The architecture starts by converting categorical features into embeddings and merging them with numerical features, either directly or via normalization layers. Transformer layers receive these inputs and use self-attention mechanisms to model the interactions between features. By doing so, the model identifies complex relationships between features that may be critical for the task, such as correlations between specific categories or numerical ranges."}, {"title": "3. Results", "content": "Table 5 presents the validation results of eight classification models (Section 2.1) designed to predict live birth success in IVF. The performance of each model is reported by five performance metrics including accuracy, precision, recall, F1-score, and AUC."}, {"title": "3. Analyzing details of the best performing model", "content": "As mentioned in Section 3.1, the best prediction results was achieved by PSO + Tab_transformer-based model. The outcome of PSO feature selection using this model is a reduced set of 45 features, selected based on their relevance to birth prediction and their ability to improve model performance. Each feature represents a critical aspect of the IVF dataset, categorized into groups such as Infertility Cause, Procedural Detail, Patient History, and Outcome."}, {"title": "4. Discussion and conclusion", "content": "In this study, we explored a variety of machine learning and deep learning models in combination of two feature selection techniques for predicting live birth success in IVF using the comprehensive HFEA dataset.\nOur study could achieve a very high performance for five different evaluation metrics by utilizing PSO for feature selection combined with tab transformer, an advanced deep learning model. The Al pipeline is designed by integrating PSO for feature selection, the tab transformer for tabular data classification and attention mechanism, balanced datasets to address class imbalance, cross-validation to prevent overfitting, and robust regularization techniques to enhance model stability. The propose model then has the potential to deal with common problems like overfitting, inconsistent patient data, and uneven datasets thus showing promise for a clinically applicable tool for predicting live birth success in IVF.\nWith an accuracy of 99.5%, precision of 99.6%, recall of 99.5%, F1-score of 99.5%, and AUC of 0.99, the PSO + Tab_transformer-based model produced exceptional results, making it the most successful model for forecasting the success of live births. In contrast, the excellent accuracy and recall offered by the transformer-based techniques could not be achieved by models like PCA + Decision Tree and PCA + Random Forest, despite their effectiveness. These finding highlight that an enhanced IVF outcome prediction could be obtained by using deep learning-based transformer models. These deep learning models provide the advantage over conventional classifiers in terms of their ability to recognize relevant features and to capture intricate relationships within the data.\nParticularly, the tab transformer offers several advantages over traditional models. First, it can efficiently handle the high-cardinality categorical features by learning embedding instead of one-hot encoding, which can lead to representations that are sparse and high-dimensional. Second, it captures complex interactions between features using self-attention, which traditional models might overlook. Third, it reduces the need for extensive manual feature engineering, enabling end-to-end learning directly from raw tabular data. This model is particularly effective in domains where meaningful relationships between features play a crucial role. When compared to traditional machine learning models, the tab transformer excels with larger datasets and high-cardinality features, offering state-of-the-art performance. Historically, the challenges in IVF outcome prediction also including live birth prediction included limited application of advanced deep learning models for tabular data. The application of these advanced deep learning techniques is terofre explored in this study for the first time. We also have surveyed some of previous efforts that used the HFEA dataset for the classification of live birth success as a binary outcome (success/failure).\nAs shown in Table 7, the results of this research surpass all previous studies utilizing similar datasets. Notably, compared to the study by Sadegh-Zadeh et al. [21] achieving an accuracy of 96.35%, which employed the same dataset from 2010\u20132018 and adhered to same inclusion and exclusion criteria as used in our study, our study could improve upon these results with an accuracy of 99.50% and an AUC of 0.99%.\nFuture work could focus on integrating additional domain-specific features related to IVF treatments and patient characteristics to further improve model performance. Exploring the use of other advanced deep learning models, including those that account for sequential or temporal data, will be also explored for their potential to enhance prediction accuracy. Expanding the dataset to include more diverse populations and treatment types would help improve model generalizability and applicability in broader IVF contexts."}]}