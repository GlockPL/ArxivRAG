{"title": "Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting", "authors": ["Lingting Zhu", "Guying Lin", "Jinnan Chen", "Xinjie Zhang", "Zhenchao Jin", "Zhao Wang", "Lequan Yu"], "abstract": "While Implicit Neural Representations (INRs) have demonstrated significant success in image representation, they are often hindered by large training memory and slow decoding speed. Recently, Gaussian Splatting (GS) has emerged as a promising solution in 3D reconstruction due to its high-quality novel view synthesis and rapid rendering capabilities, positioning it as a valuable tool for a broad spectrum of applications. In particular, a GS-based representation, 2DGS, has shown potential for image fitting. In our work, we present Large Images are Gaussians (LIG), which delves deeper into the application of 2DGS for image representations, addressing the challenge of fitting large images with 2DGS in the situation of numerous Gaussian points, through two distinct modifications: 1) we adopt a variant of representation and optimization strategy, facilitating the fitting of a large number of Gaussian points; 2) we propose a Level-of-Gaussian approach for reconstructing both coarse low-frequency initialization and fine high-frequency details. Consequently, we successfully represent large images as Gaussian points and achieve high-quality large image representation, demonstrating its efficacy across various types of large images.", "sections": [{"title": "Introduction", "content": "Existing researches have challenged the prevailing assumption that images are best represented as uniform pixel grids given the continuous nature of real visual world. Traditional signal processing methods, such as Discrete Cosine Transform (DCT), which transfers the spatial signal into the frequency domain, have been effectively applied to lossy image compression techniques, e.g., JPEG. With the rapid advancement of neural networks, which have demonstrated remarkable efficiency in function approximation, researchers are increasingly turning to neural representations for a wide range of fitting-based applications. This shift is central to the field of representation learning. In the realm of image representation, a notable example is the Local Implicit Image Function , which employs Implicit Neural Representations (INRs) to map continuous coordinates to their corresponding signals at any resolution. INR-based methods typically use a compact neural network to produce an implicit continuous mapping, preserving intricate image details and opening up new possibilities for applications such as image compression and super-resolution. Despite their potential, most INR-based methods suffer from substantial training memory and slow decoding speed, largely due to their reliance on grid-based features. Specifically, the coordinates are of grid-like structures and mapped to neural features for Multi-Layer Perception (MLP) processing. This approach presents two significant drawbacks: 1) the grid-based structures, which grow quadratically, result in dramatically large equivalent batch sizes, bringing substantial or even prohibitive training memory requirements; 2) the decoding process, which necessitates parallel processing of neural networks, despite ongoing advancements in this area, can be slow for large batches. These limitations become particularly critical when dealing with larger target signals, e.g., large images, which is the primary focus of our work.\n3D Gaussian Splatting (3DGS), designed for 3D scene reconstruction, has emerged as a novel representation known for its high-quality, real-time rendering capabilities. This is largely due to its use of explicit 3D Gaussians and differentiable tile-based rasterization. In an effort to address the aforementioned challenges and explore the potential of Gaussian Splatting (GS) for image representation, GaussianImage introduces a 2D Gaussian Splatting representation for images, advocating an image-space tailored rasterization method for efficient training and rendering. Similarly, Image-GS adaptively allocates and progressively optimizes 2D Gaussians for efficient representation learning. These pioneering works have demonstrated the effectiveness of GS for image fitting, achieving comparable quality and higher efficiency than INR-based methods for small images, while maintaining a satisfactory signal-to-noise ratio.\nHowever, existing GS-based fitting methods have yet to demonstrate their potential for higher fidelity and their adaptability to large images. In our work, we delve deeper into the application of 2DGS representation for large images, which naturally require a larger number of Gaussian points compared to smaller images. We introduce Large Images are Gaussians (LIG). As shown in Fig. 1, LIG is capable of fitting large images with an increasing number of Gaussian points, a task where GaussianImage may fall short. We examine the optimization difficulties encountered by GaussianImage when dealing with a large number of Gaussian points and, in response, we make two distinct modifications to overcome these challenges. Firstly, we optimize the Gaussian parameters using a slightly different 2DGS representation, aided by re-implemented CUDA kernels. Specifically, we directly optimize the covariance matrix without decompositions which are managed in 3DGS and Gaussian-Image and maintain semi-definiteness via post-processing. Secondly, we harness the concept of Level of Detail (LOD) from computer graphics, proposing a Level-of-Gaussian approach for hierarchically fitting large images. This shares similarities with research adopting LOD in Neural Field and Gaussian Splatting, including MINER, BungeeNeRF, Octree-GS, and Hierarchical 3D Gaussian ."}, {"title": "Related Works", "content": "Implicit Neural Representations. Since early works focused on representing the signed distance field for 3D shapes , Implicit Neural Representations (INRs) have been applied to a variety of applications involving different types of representations, including 3D scenes , images , and videos . A notable example is NeRF , which uses a Multilayer Perceptron (MLP) to represent geometry and view-dependent appearance, sparking a surge of interest in 3D vision and graphics. Most INRs use grid-based input processing for spatial signals and focus more on activation functions like sinusoids , Gabor wavelets , and variable-periodic activation functions to capture high-quality details for fitting. However, these methods suffer from inefficient training and inference, making them unsuitable for high-resolution images. To handle large-scale signals, multi-resolution signal representations that rely on efficient feature querying or hierarchical processing have been widely adopted . Among them, while hierarchical architectures, such as MINER , can facilitate large image fitting at the cost of sequential training, they do not address the fundamental issues associated with the grid-based nature of INRs, leading to prolonged training times and slow decoding speeds. Recent attempts to improve INRs have included the use of multi-resolution hash encoding or radial basis functions , which have achieved high accuracy with fewer parameters. These advancements have significantly improved INRs, making them the current leading approaches.\n3D Gaussian Splatting. 3D Gaussian Splatting (3DGS) becomes a trend in the computer graphics and computer vision communities due to its ability to compactly represent complex 3D scenes while enabling high-speed rendering. The scene is modeled as 3D Gaussian primitives, each of which is defined by position, scale, rotation, opacity, and appearance attributes. The parameters of the Gaussians are optimized to align observations via differentiable rendering. 3DGS shows remarkable potential across a wide range of downstream tasks. These include novel view synthesis and scene reconstruction , 3D reconstruction and generation, and applications in robotics . Recent advancements in 3DGS further incorporate Level-of-Detail (LOD) techniques to enhance rendering efficiency and enable adaptive scene representation , particularly crucial for large-scale scene reconstruction which requires visual quality with real-time rendering.\nGaussian Splatting Based Image Representation. Recently, GaussianImage is the first to adapt 3DGS for image representations. Specifically, it adapted Gaussian points to image spaces with fewer characterizing parameters and employed alpha blending to merge color and opacity attributes. After per-sample image fitting, the attributes can be compressed using quantization-aware fine-tuning. As a result, GaussianImage can represent images with 2DGS and compress them while maintaining high quality. Image-GS also utilizes the eight parameters for 2D Gaussian points and fits a target image by adaptively allocating and progressively optimizing a set of 2D Gaussians. GaussianSR, assigns a learnable Gaussian kernel to each pixel for super-resolution. Another related work, Splatter Image , embeds Gaussian attributes at the image level, achieving ultra-efficient 3D reconstruction. In the field of image fitting, despite their groundbreaking success, it remains unclear whether GS can be used to fit large images at a quality that can compete with INRs. Given the nature of large fitting targets, it is necessary to allow for more Gaussian points to be optimized simultaneously. We demonstrate that GaussianImage falls short in this regard due to difficulties in optimizing their representations and the lack of multi-resolution mechanisms for capturing high-frequency information."}, {"title": "Methodology", "content": "In this section, we present our Large Images are Gaussians (LIG) framework. We begin with the basics of 3DGS and its adaptation to 2D spaces. Subsequently, we delve into two primary components of our methodology: 1) the variant of 2DGS representation; 2) the Level-of-Gaussian mechanism; which provide an efficient and high-quality solution for optimizing large images containing numerous Gaussian points."}, {"title": "Preliminaries", "content": "3D Gaussian Splatting (3DGS). As proposed by , 3D Gaussian splatting employs a set of 3D Gaussians to represent 3D scenes. Each Gaussian is characterized by its mean \\(x \\in \\mathbb{R}^3\\), scale \\(s \\in \\mathbb{R}^3\\), rotation \\(r \\in \\mathbb{R}^3\\), opacity \\(a \\in \\mathbb{R}\\), and color \\(c \\in \\mathbb{R}^C\\). Spherical harmonics can be used to further define view-dependent effects. The rendering process involves projecting these 3D Gaussians onto the image plane, resulting in 2D elliptical splats and performing \u03b1-blending for each pixel in a front-to-back depth order. Compared to neural rendering techniques like Neural Radiance Fields (NeRF) , 3DGS provides faster rendering and efficient training capabilities.\n2D Gaussian Splatting (2DGS). In , the Gaussians are adapted to 2D spaces and are defined by deduced parameters. We only need to formulate the 2D Gaussian points locating on the fixed plane corresponding to the image. Specifically, each 2D Gaussian can be described by its position \\(\\mu \\in \\mathbb{R}^2\\), 2D covariance matrix \\(\\Sigma \\in \\mathbb{R}^{2 \\times 2}\\), color coefficients \\(c \\in \\mathbb{R}^3\\), and opacity \\(o \\in \\mathbb{R}\\). To ensure the positive semi-definite, \\(\\Sigma\\) can be decomposed with Cholesky factorization or into a rotation matrix \\(R \\in \\mathbb{R}^{2 \\times 2}\\) and scaling matrix \\(S \\in \\mathbb{R}^{2 \\times 2}\\) following , which altogether requires 3 parameter in 2D cases. The \u03b1-blending, which calculates the color of pixel \\(i\\) via"}, {"title": "2D Gaussians Formulation", "content": "We adopt a variant of representation and optimization strategy on 2D Gaussians. Our representation on each point differs only in 2D covariance matrix \\(\\Sigma \\in \\mathbb{R}^{2 \\times 2}\\). If we decompose this into rotation matrix \\(R\\) and scaling matrix \\(S\\), we have a total of 3 variables. After extensive experiments on exploring the capabilities of 2DGS, we find that optimizing the decomposed parameters can be challenging when the Gaussian points are numerous. Therefore, we opt for optimizing the covariance matrix directly, which also requires 3 parameters considering the symmetry of the covariance matrix. The forward and backward kernel functions in CUDA are accordingly implemented for optimization.\nAs stated in , covariance matrices have physical meaning when they are positive semi-definite. Despite that we cannot ensure its positive semi-definite via directly optimizing the upper triangular of the matrix, we can have two important assertions in 2DGS cases. Firstly, the Gaussian points do not necessarily need to retain their physical meaning in image fitting where the representations are considered as the semi-implicit fitters. And the covariance is only used to produce \\(\\sigma_n\\) as the weight of the color with an exponential activation. Secondly, we can filter out Gaussian points with covariance matrices that do not obey positive semi-definite in 2D cases. In detail, when computing the color for a certain pixels, removing all \\(\\sigma_n < 0\\) can eliminate points with invalid covariance matrices. Consequently, the color changes to\nIt is guaranteed that for any positive semi-definite matrix with a non-zero determinant, the inverse matrix will also be positive semi-definite, which implies\nAs the contrapositive also holds true, for each pixel, Gaussian points that we filter out are not positive semi-definite. Yet with the above operation, we cannot ensure that all covariance matrices producing positive \\(\\sigma\\) have physical meanings, since the \\(d_n\\) is drawn from a finite set. Given the nature of the 2D cases, it is straightforward to determine whether a 2D symmetric matrix is positive semi-definite. Matrices without physical meaning can be strictly filtered out if this is a desired feature. We have included the relevant proofs and analysis in Supplementary Material for reference."}, {"title": "Levels of 2D Gaussians", "content": "Fig. 3 provides an illustration of the Level-of-Gaussian mechanism. The concept of Level of Detail, which refers to the complexity of a 3D model representation, has been widely used in computer graphics. Existing works, which share core principles with our design, employ multi-scale designs for high-resolution signals. For instance, BungeeNeRF develops a progressive NeRF-based model to fit large scenes, while MINER"}, {"title": "Experiments", "content": "Experimental Setup\nDataset. We assess our method across three diverse datasets, encompassing medical, remote sensing, and general visual tasks. These datasets vary in resolution where we utilize 15 9K histopathology images of human heart sourced from STimage , 4 4K satellite images from the Full-resolution Gaofen-2 (FGF2) , and 2K DIV-HR dataset, comprising 100 images, with low-resolution counterparts evaluated in .\nEvaluation Metrics. We use three metrics to evaluate our method against GS-based state-of-the-art and INR-based counterparts. We employ PSNR for image quality, which quantify the distortion between reconstructed images and original images. As one merit of LIG is the fewer training memory for large images compared to GaussinImage and INRs, we provide training memory for reference. Benefiting from GS representation, our method achieves high rendering speed and FPS is used for benchmark.\nImplementation Details. Since we present a new 2DGS representation for images, CUDA kernels are incorporated and we build the packages upon gsplat . The training steps for \\(L_0\\) and \\(L_1\\) are set to the same, 30,000 steps in our implementation. The learning rate is 0.018 and Adam optimizer is used ."}, {"title": "Main Results", "content": "The quantitative results on three datasets are reported in Table 1. We mainly compare our method with Gaussian-Image focusing on large images, and also select INR-based methods for baselines. SIREN , Gauss , WIRE , and Finers are selected for comparison. We report the quantitative results, including PSNR, Training Memory, and FPS, on three datasets. The FPS results are tested on the same environment. Note that these INRs are based on grid features, which suffer from large training memory requirements for large images since the batch sizes are too large. Therefore, we leave blank for those infeasible experiments. Additionally, while we can use tiny networks for running, the performances can be poor, as seen in the results of FINER where different network sizes are used for different datasets with training memory smaller than 80G. It is clear that compared with GS-based methods, INRs suffer from large training memory and low FPS.\nCompared with GaussianImage, our method performs better on image quality, enabling GS for large signal fitting, especially for larger images on 4K and 9K. Regarding training memory, given the same number of Gaussian points, LIG does not propagate all the gradients but optimizes the levels in two stages, therefore reducing the training memory compared with our single level variant. From the Table 1, we can also see that LIG consumes less memory than GaussianImage. Since the training and inference require two levels, the rendering speed can be slower than the GS-based baseline. However, as the additional level \\(L_0\\) comprises fewer points and the final level \\(L_1\\) has a reduced number of Gaussians, the FPS is not necessarily lower. For instance, on 9K images of STimage, with a total of 3.5e7 Gaussians, the FPS achieved is higher compared to GaussianImage. For other LIG results of the same point number, the reduction in FPS is mild considering the quality and training memory requirements. A qualitative comparison between LIG and GaussianImage is illustrated in Fig. 4. Note that for the histopathology image, the abundance of rich details may obscure the weaknesses of the baseline. Please refer to the difference images."}, {"title": "Ablation Studies", "content": "We present the ablation studies in Table 2, evaluating the effectiveness of our two key components across different Gaussian point numbers on various datasets. All models are optimized for the same number of iterations. We utilize a variant representation of 2DGS and introduce a Level-of-Gaussian (LOG) mechanism. In this context, \u201cw/o LOG\u201d indicates the use of only the 2DGS variant with optimization performed at a single level, while \u201cw/ LOG\u201d refers to the full LIG implementation. The results clearly demonstrate that both components consistently yield performance improvements as the number of Gaussian points increases."}, {"title": "Further Analysis", "content": "Effectiveness of low-frequency initialization. Our two-stage LOG approach benefits from easier training for large images due to the low-frequency initialization at the first level. In Table 3, we compare results with different point assignments to demonstrate the effectiveness of this initialization. We allocate additional points to the first level in the single-stage setting, with the only difference being the training target of the second level, denoted as \\(L_1\\). This initialization results in higher PSNR, indicating that the training of the final target is facilitated. The two-stage process incurs extra inference time, leading to a slight drop in FPS.\nTrade-off between quality and training time. One key factor affecting training time is the number of iterations. In Table 4, we present results for different training iterations. The linearly increasing training time leads to higher fitting accuracy, with 3e5 iterations representing a compromise point adopted in our experiments.\nComparison with NeuRBF. We compare our method with the state-of-the-art INRs method, NeuRBF . In Table 5, we present performance metrics on the FGF2 dataset to illustrate the differences between our method and NeuRBF. We use 2e6 Gaussian points, comprising 1.6e7 parameters, to maintain a similar total number of parameters as NeuRBF for this dataset. It is observed that on 4K data, NeuRBF achieves higher PSNR while maintaining a similar FPS. With its radial bases and the solid foundation of Instant-NGP , NeuRBF advances INRs for large image fitting. While it remains unclear whether recent efficient techniques in INRs, such as hash coding, can be applied to 2DGS, existing and ongoing advancements in 3DGS may positively impact the development of novel 2DGS-based image representations. Given that GS-based image representation is still under-explored, we consider it to be in a very early stage."}, {"title": "Conclusion", "content": "In this work, we introduce Large Images are Gaussians (LIG) as a novel representation for large images. LIG is built upon 2D Gaussian Splatting (2DGS) and adopts a variant design for representing Gaussian points. Additionally, we propose a Level-of-Gaussian approach to facilitate the optimization of numerous 2D Gaussians. This enables 2DGS-based representation for fitting large images, significantly outperforming existing GS-based methods. While our work primarily focuses on representation and delves deeper into the performance of 2DGS as an image fitter, it is crucial to consider reducing the number of Gaussians for large images to achieve high-quality compression comparable to state-of-the-art Implicit Neural Representations (INRs)."}]}