{"title": "A Systematic Examination of Preference Learning through the Lens of Instruction-Following", "authors": ["Joongwon Kim", "Anirudh Goyal", "Aston Zhang", "Bo Xiong", "Rui Hou", "Melanie Kambadur", "Dhruv Mahajan", "Hannaneh Hajishirzi", "Liang Tan"], "abstract": "Preference learning is a widely adopted post-training technique that aligns large language models (LLMs) to human preferences and improves specific downstream task capabilities. In this work we systematically investigate how specific attributes of preference datasets affect the alignment and downstream performance of LLMs in instruction-following tasks. We use a novel synthetic data generation pipeline to generate 48,000 unique instruction-following prompts with combinations of 23 verifiable constraints that enable fine-grained and automated quality assessments of model responses. With our synthetic prompts, we use two preference dataset curation methods - rejection sampling (RS) and Monte Carlo Tree Search (MCTS) \u2013 to obtain pairs of (chosen, rejected) responses. Then, we perform experiments investigating the effects of (1) the presence of shared prefixes between the chosen and rejected responses, (2) the contrast and quality of the chosen, rejected responses and (3) the complexity of the training prompts. Our experiments reveal that shared prefixes in preference pairs, as generated by MCTS, provide marginal but consistent improvements and greater stability across challenging training configurations. High-contrast preference pairs generally outperform low-contrast pairs; however, combining both often yields the best performance by balancing diversity and learning efficiency. Additionally, training on prompts of moderate difficulty leads to better generalization across tasks, even for more complex evaluation scenarios, compared to overly challenging prompts. Our findings provide actionable insights into optimizing preference data curation for instruction-following tasks, offering a scalable and effective framework for enhancing LLM training and alignment.", "sections": [{"title": "1 Introduction", "content": "Aligning large language models (LLMs) with human preferences has remained a persistent challenge despite their recent success, particularly for tasks that involve generating nuanced, instruction-following responses. To address this bottleneck, preference learning has emerged as a vital technique applied in the final stages of LLM post-training (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022). Preference learning refines the ability of LLMs to align with human expectations by fine-tuning them on pairs of (chosen, rejected) responses. Recent successes of using preference learning to develop frontier language models (OpenAI, 2023; Google, 2023; Anthropic, 2024; AI at Meta, 2024) have led to automated methods for curating preference pairs (Pang et al., 2024; Xie et al., 2024; Xu et al., 2023; Khaki et al., 2024). While these techniques yield synthetic preference pairs that significantly improve model capabilities in closed-ended tasks, it is yet unclear which attributes of the preference pairs contribute to the improved alignment and downstream capabilities.\nExisting research in preference learning has largely focused on the mechanics of optimization methods, such as Direct Preference Optimization (DPO, Rafailov et al. (2023)) and Proximal Policy Optimization (PPO, Schulman et al. (2017)). While these methods are critical for updating model weights based on preference data, they operate with limited insight into how the structure, quality, and complexity of the preference datasets themselves affect outcomes (Ivison et al., 2024; Xiao et al., 2024). For example, questions remain about whether shared prefixes in paired responses improve learning, whether training on high-contrast pairs is always optimal, or how the difficulty of training prompts impacts generalization. Without a systematic investigation of these factors, designing effective preference datasets is largely heuristic and suboptimal."}, {"title": "2 Background", "content": "Preference Learning. Preference learning is a technique that is used to align LLMs during their post-training phase, involving pairs of (chosen, rejected) responses in the training dataset. It aligns LLMs by steering them towards generating the chosen responses and away from generating the rejected responses. The Bradley-Terry model (Bradley and Terry, 1952) provides the probabilistic framework for preference learning by modeling the pairwise comparison between two responses $(y_1, y_2)$ provided by the LLM to a given prompt $x$:\nP(Y_1 > Y_2|x) = \\frac{exp(r^*(x, y_1))}{exp(r^*(x, y_1)) + exp(r^*(x, y_2))}\nCommon methods for preference learning such as Direct Preference Optimization (DPO, Rafailov et al. (2023)) directly optimize the model to update its parameters to increase the likelihood of generating the chosen response over the rejected response. Meanwhile, other methods such as Proximal Policy Optimization (PPO, Schulman et al. (2017)) indirectly perform this optimization by first training a reward model to assign scores corresponding to the preferences in the training data, and then optimizing a policy model with the guidance of the reward model. Both approaches have been instrumental in aligning LLMs with human preferences and enhancing their capabilities for a wide array of downstream tasks.\nData Curation for Preference Learning. The success of preference learning for LLM post-training has naturally led researchers to propose methods for automatically curating preference pairs designed to further boost model capabilities (Yuan et al., 2024b; Khaki et al., 2024; Xie et al., 2024). Such methods assign scores to LLM-generated outputs using verifiers to determine which outputs should be preferred during training. While various other methods have been proposed for curating preference data (Zhou et al., 2024; Yuan et al., 2024a; Lai et al., 2024), in this paper we focus on two popular methods: rejection sampling (RS) and Monte Carlo Tree Search (MCTS). During rejection sampling, the policy model generates N independent responses to the given prompt and a verifier scores each response according to some evaluation metric or a reward model. Responses with (high, low) scores are selected as the (chosen, rejected) responses, respectively. Meanwhile, in the MCTS framework, the policy model performs tree search for the given prompt by generating a fixed number of tokens at each iteration and builds the tree with nodes that represent each subsequence of generated tokens. During MCTS, the policy model performs rollouts by generating full responses and backpropagates the reward scores assigned to the rollouts. This results in a tree of possible responses generated for a given prompt, with each node being assigned a Q-value which measures the quality of the response generated so far. Here, sibling nodes with sufficient differences in Q-values or reward scores are selected such that the preference pairs contain common prefixes and the suffixes account for the quality difference between the two responses.\nWhile such methods return preference pairs that are effective for alignment and capability improvements, there is a lack of studies into exactly how the preference pairs should be curated based on these methods. In"}, {"title": "3 Prompt Synthesis", "content": "We perform all our experiments with a new set of synthetic prompts that incorporate mixtures of verifiable constraints. These prompts allow us to evaluate the qualities of the generated responses in both a consistent and fine-grained manner, providing a suitable environment for us to control the attributes of the preference dataset and investigate their impact on downstream performance."}, {"title": "3.1 Instruction-Following with Verifiable Constraints", "content": "Our verifiable constraints resemble but are distinct from the set of constraints provided in IFEval (Zhou et al., 2023). We define 23 constraints which can be deterministically verified using code, spanning ones that check for adherence to specific structural, stylistic, or formatting requirements. The complete ontology of our constraints is provided in Table 8 in the appendix.\nWe design our verifiable constraints such that they follow the formatting of the verifiable constraints provided in IFEval for unified evaluation - each constraint is accompanied by a set of keyword arguments that are needed to actualize the constraint for the given prompt. Refer to Table 1 for examples of such constraints and their associated keyword arguments. For example, the alliteration constraint is paired with a keyword argument num_alliteration_words, which indicates the number of words that must display the alliteration. Some constraints such as tl;dr_summary do not contain any keyword arguments as they are self-explanatory and do not need any further specifications."}, {"title": "3.2 Prompt Generation", "content": "We build a pipeline that generates synthetic prompts for instruction-following, incorporating combinations of different verifiable constraints to create a set of challenging prompts that stress-test models' capabilities of following complex instructions. Our pipeline is similar to Instruct-SkillMix (Kaur et al., 2024) in terms of mixing skills, except that we have a preexisting set of verifiable constraints functioning as the \"skills\" and incorporate additional layers for generating the keyword arguments before generating the final instructions."}, {"title": "3.3 Prompt Information", "content": "We generate instruction-following prompts for combinations of $k \\in \\{4,5,6\\}$ constraints. Table 2 shows the statistics that describe the prompts that we generate. For each value of $k$, we generate about 16K synthetic prompts, resulting in a total of about 48K prompts being used in our experiments. The average length of the prompts increases as the number of constraints increases, as the complexity of the prompts increases with larger numbers of constraints."}, {"title": "4 Preference Data Curation", "content": "Using the prompts obtained in Section 3, we generate responses and extract preference data using the aggregated correctness scores of the responses. Here, we assign high-scoring responses as the chosen responses"}, {"title": "5 Experiments and Results", "content": "We perform experiments to systematically investigate the effects of preference dataset attributes on the downstream performance of language models. To this end, we examine three critical dimensions of the preference dataset: the (1) presence of shared prefixes between the (chosen, rejected) responses, (2) contrast and quality of the chosen and rejected responses, and (3) difficulty or complexity of the training prompts."}, {"title": "5.1 Shared Prefixes in Preference Pairs", "content": "We investigate the effect of having structural consistency between the (chosen, rejected) responses in the preference dataset. Recent techniques utilize tree search to curate fine-grained preference pairs where the (chosen, rejected) responses differ after a shared prefix we examine the effects of utilizing such preference pairs. To this end, we use rejection sampling (RS), which returns responses without shared prefixes, and Monte Carlo Tree Search (MCTS), which returns responses with shared prefixes, to curate preference datasets under identical conditions.\nWe perform our experiments by fixing the size of the training dataset as well as the number of unique prompts associated with the preference pairs that occur in each training set, across our comparison experiments. Meanwhile, we perform experiments by varying the other two dimensions: the correctness of the (chosen, rejected) responses, and the difficulty of the training prompts as measured by the values of k.\nTable 4 shows the results of our experiments. We observe two key findings from the results."}, {"title": "5.2 Contrast and Quality of Responses", "content": "We investigate the effects of controlling the quality, or correctness, of the (chosen, rejected) responses in the preference dataset. Across our experiments, we maintain the same training dataset size and the number of unique prompts given a fixed data curation method and training prompt difficulty. Table 5 shows the results of varying the correctness of the (chosen, rejected) pairs across diverse training configurations including the RS/MCTS-based curation method, as well as the training prompt difficulties with k = 4 or 5.\nMeanwhile, we also study the effects of having a mixture of both high- and low-contrast pairs. To compare its performance to those of using only high- or low-contrast pairs, we fix the training dataset size and the number of unique prompts, and replace some of the rejected responses with low correctness scores with rejected responses that have slightly higher correctness scores. For this experiment, we use MCTS for data curation while using k = 4 or 5 for the complexity of the training prompts.\nThe results of our experiments are visualized in Figure 4. We show the composition of the (chosen, rejected) responses in terms of their correctness on the x-axis, and the evaluation metric on the y-axis for each subplot. For example, for a training set with k = 4 constraints, we begin with a (chosen, rejected) correctness of (c, r) = (4, 1) and then mix in r=2 to obtain (c, r) = (4, 1/2), and then mix in r=3 to obtain (c, r) = (4, 1/2/3).\nWe make three key observations from our experiments.\nWhen used alone, high-contrast preference pairs is more helpful than low-contrast preference pairs. Our results in Table 5 show that for a fixed correctness of the chosen response, increasing the correctness of the rejected response decreases the performance in a consistent manner. For example, using a correctness of c = 3 for the chosen response and changing the correctness of the rejected response from r = (0, 1, 2) for a training dataset with k = 4 decreases the IFEval score from 79.04 78.66 74.60. This trend holds across both RS- and MCTS-based data curation methods, as well as across the difficulty of the training prompts. Likewise, we find that using high-contrast preference pairs is better than using low-contrast pairs for downstream performance."}, {"title": "5.3 Training Prompt Difficulty", "content": "We examine how the difficulty of prompts provided in a preference dataset affects downstream performance across evaluation sets of varying difficulties. Similar to previous experiments, we control the size of the training dataset and the number of unique prompts to compare across the prompt difficulties. For each experiment, we fix the preference data curation method and the margin between the (chosen, rejected) responses while comparing between the three complexities of the prompts in our training set with $k \\in \\{4,5,6\\}$. Then, we repeat our experiments across our data curation methods and the qualities of the (chosen, rejected) responses.\nTable 6 summarizes the results of our experiments. We observe two key findings from the results."}, {"title": "5.4 Additional Experiments", "content": "We perform two additional experiments to confirm that preference learning helps our models gain instruction-following skills, and investigate the limitations of our methods. To this end, we (1) compare the performances of SFT and DPO to ensure that our preference datasets teach meaningful skills to our models, and (2) examine how our RS-based preference data curation method scales with varying amounts of compute."}, {"title": "6 Conclusion", "content": "We systematically investigate the effects of various attributes of preference datasets on model capabilities from the perspective of instruction-following. To this end, we first build a data generation pipeline that combines general-purpose prompts with mixtures of verifiable constraints to synthesize challenging instruction-following prompts. We then automatically curate preference pairs using two popular methods: rejection sampling (RS) and Monte Carlo Tree Search (MCTS). Using the preference pairs, we examine the effects of (1) the existence of shared prefixes between the chosen and rejected responses, (2) the contrast and quality of the responses, and (3) the complexity of the training prompts. Our results indicate that having a common prefix in the preference pairs offers marginal yet consistent improvements, high-contrast preference pairs outperform low-contrast pairs but a mixture is sometimes better than both, and training on moderately difficult prompts is more helpful than training on extremely difficult prompts. Our work provides a systematic framework for curating different types of preference datasets and sets the groundwork for future studies that extend the scope beyond verifiable instruction-following constraints to more general constraints."}, {"title": "B MCTS Details", "content": "Computing the policy score. We compute the policy score $\\Pi(a_i|s_t)$ by computing the average of the log probabilities of tokens generated for action $a_i$ from state $s_t$, with the denominator moderated by a hyperparameter $\\gamma$, which we set to 1.0 for our experiments. Refer to the formula below for the exact definition:\n$\\Pi(a_i|s_t) = exp(\\frac{1}{||a_i||} \\Sigma_j \\Pi(t_j|s_t, t_{1...j-1}))$\nNote that $t_j$ denotes the $j$th token in action $a_i$.\nComputing the self-evaluation score. We compute the self-evaluation score $\\Pi_{self-eval}$ by prompting the policy with a self-evaluation prompt $P_{self-eval}$, coupled with the response generated by the model so far, and obtaining the log probabilities of the final token of the response, denoted as $t_{final} \\in \\{yes, no\\}$. We use self-consistency (Wang et al., 2022) to obtain multiple self-evaluations of the policy of its own output over L generations and average the scores in order to improve the reliability of our self-evaluation scores via increased compute.\n$\\Pi_{self-eval} = \\frac{1}{L} \\Sigma_{i=1}^{L} \\frac{1}{2} [1 + exp(\\Pi(t_{final} = yes|P_{self-eval}, s_t)) - exp(\\Pi(t_{final} = no|P_{self-eval}, s_t))]$\nOur formula allows us to normalize the score between 0 to 1, with 0.5 indicating a neutral state where the confidence scores for $\\Pi(t_{final} = yes|P_{self-eval}, s_t) = \\Pi(t_{final} = no|P_{self-eval}, s_t)$. We provide the self-evaluation prompt $P_{self-eval}$ in Figure 6."}, {"title": "C Full Experiment Results", "content": "C.1 Common Prefix Results\nWe provide the full results of our experiments investigating the effects of common prefixes in preference pairs in Tables 10, 11 and 12."}, {"title": "C.2 Response Quality Results", "content": "(chosen, rejected) response quality (unmixed). We provide the full results of our experiments investigating the effects of the response correctness (or quality) in preference pairs in Tables 13 and 14.\n(chosen, rejected) response quality (mixed). We provide the full results of our experiments examining the effects of mixing preference pairs with different margins between the (chosen, rejected) responses in Tables 15 and 16."}, {"title": "C.3 Prompt Difficulty Results", "content": "We provide the full results of our experiments investigating the effects of varying the difficulty of the training prompts, as measured by the number of verifiable constraints, in Tables 17 and 18."}, {"title": "C.4 SFT vs. DPO", "content": "We provide the full results of our additional experiments comparing the performances of models trained via SFT and DPO in Tables 19 and 20."}, {"title": "A Complete Ontology of Verifiable Constraints and Training Examples", "content": "We present our 23 verifiable constraints in Table 8, and examples of our synthetic prompts in Table 9."}]}