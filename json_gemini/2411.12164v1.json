{"title": "URBANDIT: A FOUNDATION MODEL FOR OPEN-WORLD URBAN SPATIO-TEMPORAL LEARNING", "authors": ["Yuan Yuan", "Chonghua Han", "Jingtao Ding", "Depeng Jin", "Yong Li"], "abstract": "The urban environment is characterized by complex spatio-temporal dynamics arising from diverse human activities and interactions. Effectively modeling these dynamics is essential for understanding and optimizing urban systems. In this work, we introduce UrbanDiT, a foundation model for open-world urban spatio-temporal learning that successfully scale up diffusion transformers in this field. UrbanDiT pioneers a unified model that integrates diverse spatio-temporal data sources and types while learning universal spatio-temporal patterns across different cities and scenarios. This allows the model to unify both multi-data and multi-task learning, and effectively support a wide range of spatio-temporal applications. Its key innovation lies in the elaborated prompt learning framework, which adaptively generates both data-driven and task-specific prompts, guiding the model to deliver superior performance across various urban applications.\nUrbanDiT offers three primary advantages: 1) It unifies diverse data types, such as grid-based and graph-based data, into a sequential format, allowing to capture spatio-temporal dynamics across diverse scenarios of different cities; 2) With masking strategies and task-specific prompts, it supports a wide range of tasks, including bi-directional spatio-temporal prediction, temporal interpolation, spatial extrapolation, and spatio-temporal imputation; and 3) It generalizes effectively to open-world scenarios, with its powerful zero-shot capabilities outperforming nearly all baselines with training data. These features allow UrbanDiT to achieves state-of-the-art performance in different domains such as transportation traffic, crowd flows, taxi demand, bike usage, and cellular traffic, across multiple cities and tasks. UrbanDiT sets up a new benchmark for foundation models in the urban spatio-temporal domain. Code and datasets are publicly available at https://github.com/YuanYuan98/UrbanDiT.", "sections": [{"title": "1 INTRODUCTION", "content": "The urban environment is characterized by complex spatio-temporal dynamics arising from diverse human activities and interactions within the city. These dynamics are reflected in different types of data. For example, grid-based data divides urban space into regular cells, often used to track crowd flows. In contrast, graph-based data represents spatial structures like road networks as nodes and edges, such as traffic speeds on roads. These data sources usually come from different cities, each with unique layouts, infrastructures, and planning strategies. Effectively modeling these diverse spatio-temporal dynamics is crucial for optimizing urban services and understanding how cities function and evolve. Therefore, it raises an essential research question: can we develop a foundation model, similar to those in natural language processing (Touvron et al., 2023; Brown et al., 2020) and computer vision (Brooks et al., 2024; Liu et al., 2023a; Esser et al., 2024), that learns universal spatio-temporal patterns and serves as a general-purpose model for various urban applications?\nIn the context of urban spatio-temporal modeling, recent advancements such as GPD (Yuan et al., 2024b), UrbanGPT (Li et al., 2024), and UniST (Yuan et al., 2024a) have opened exciting avenues for understanding complex urban dynamics. As compared in Table 1, these models either utilize LLMs (Li et al., 2024) or develop unified models from scratch (Yuan et al., 2024a;b) tailored for urban spatio-temporal predictions. By training on multiple datasets, they have showcased impressive generalization capabilities. However, their focus remains largely on prediction tasks, and they are often restricted to specific data types\u2014such as grid-based data (Li et al., 2024; Yuan et al., 2024a) or graph-based traffic data (Yuan et al., 2024b). Thus, realizing the full potential of foundation models capable of seamlessly handling diverse data types, sources, and tasks in open-world scenarios remains an open and largely unexplored area of research.\nUrban spatio-temporal data is typically defined by diverse properties, including varying spatial resolutions, temporal dynamics, and complex interactions among entities. Building an effective foundation model requires a scalable architecture capable of accommodating these complexities. Moreover, the intricate nature of urban spatio-temporal dynamics necessitates a model that can learn from complex data distributions. Diffusion Transformers, exemplified by models like Sora (Brooks et al., 2024), offer a compelling solution for this purpose. By combining the generative power of diffusion processes with the scalability and flexibility of transformer architectures, diffusion transformers present a promising backbone.\nIn this work, we introduce UrbanDiT, which unifies training across diverse urban scenarios and tasks, effectively scaling up diffusion transformers for comprehensive urban spatio-temporal learning. It offers three appealing benefits: 1) It unifies diverse data types into a sequential format, allowing it to capture spatio-temporal patterns across various cities and domains, guided by data-driven prompts that highlight critical patterns. 2) It supports a wide range of tasks with a single model, using masking strategies and task-specific prompts, without the need for re-training across different tasks. 3) It generalizes well to open-world scenarios, exhibiting powerful zero-shot performance. To achieve this, we first unify different input data by converting it into the sequential format. We build the denoising network using transformer blocks, equipped with both temporal and spatial attention modules. To integrate diverse data types and tasks, we propose a unified prompt learning framework that enhances the denoising process. This framework maintains memory pools to capture learned spatio-temporal patterns and generate data-driven prompts, while also create task-specific prompts for various spatio-temporal tasks. These prompts are concatenated into the unified sequential input before being fed into the transformer modules. The design of prompt learning serves as a flexible intermediary, adaptable to a wide range of scenarios.\nUrbanDiT, built on the DiT backbone with a prompt learning framework, is a pioneering open-world foundation model. It excels at handling diverse urban spatio-temporal data and a wide range of tasks, including bi-directional spatio-temporal prediction, temporal interpolation, spatial extrapolation, and spatio-temporal imputation. This makes UrbanDiT a powerful and universal solution for various urban spatio-temporal applications. We summarize our contributions as follows:"}, {"title": "2 RELATED WORK", "content": "Urban spatio-temporal learning encompass a variety of tasks such as prediction (Tan et al., 2023b; Bai et al., 2020; Yuan et al., 2023; Li et al., 2018; Zhang et al., 2017), interpolation (Aumond et al., 2018; Gr\u00e4ler et al., 2016), extrapolation (Miller et al., 2004; Ma et al., 2019), and imputation (Tashiro et al., 2021; Hu et al., 2023), addressing how urban systems evolve across space and time. Deep learning has achieved significant progress in these areas, with techniques ranging from CNNs (Li et al., 2018; Zhang et al., 2017), RNNs (Wang et al., 2017; 2018; Lin et al., 2020), MLPs (Shao et al., 2022a), GNNs (Bai et al., 2020; Geng et al., 2019), and Transformers (Chen et al., 2022; Jiang et al., 2023), to the more recent use of diffusion models (Yuan et al., 2023; 2024b; Tashiro et al., 2021; Wen et al., 2023). Each of these approaches has been employed to model complicated spatio-temporal relationships inherent to urban environments. However, most existing models are tailored to specific datasets and tasks. In contrast, our approach is designed to handle multiple tasks and generalize across diverse urban scenarios without the need for re-training on new datasets.\nFoundation models have made significant progress in language models (Touvron et al., 2023; Brown et al., 2020) and image generation (Brooks et al., 2024; Liu et al., 2023a; Esser et al., 2024). Recently, researchers have extended the concept of foundation models to urban environments, aiming to address unique challenges of urban spatio-temporal data. Some representative works in this area include UrbanGPT (Li et al., 2024), UniST (Yuan et al., 2024a), and CityGPT (Feng et al., 2024b). UrbanGPT introduces LLMs designed for spatio-temporal predictions within urban contexts. UniST develops a foundation model from scratch specifically for urban prediction tasks, demonstrating zero-shot capabilities that allow the model to generalize to new scenarios without additional training. CityGPT, on the other hand, focuses on enhancing the LLM's ability to comprehend and solve urban tasks by improving its understanding of urban spaces. Table 1 provides a comparison of key abilities across existing urban foundation models and UrbanDiT. As shown, UrbanDiT is trained from scratch, allowing it to fully leverage data diversity while offering flexibility across a wide range of tasks. Additionally, it demonstrates emergent zero-shot capabilities. Compared to previous efforts, UrbanDiT represents a significant advancement in developing urban foundation models.\nDiffusion models, originally popularized in image generation, have recently gained attention in handling spatio-temporal data and time series. They iteratively add and remove noise from data, allowing them to capture complex patterns across both temporal and spatial dimensions (Yang et al., 2024; Yuan et al., 2023; Hu et al., 2023; Wen et al., 2023; Rasul et al., 2021). In the context of time series, diffusion models have been applied to tasks such as forecasting (Kollovieh et al., 2024; Rasul et al., 2021) and imputation (Xiao et al., 2023; Tashiro et al., 2021), outperforming traditional methods by generating more accurate and coherent sequences. For spatio-temporal data, diffusion models have proven useful in a variety of tasks, including traffic prediction (Wen et al., 2023), environmental monitoring (Yuan et al., 2023), and human mobility generation (Zhu et al., 2024; 2023). By effectively modeling spatio-temporal dependencies, these models can capture both the spatial correlations and temporal dynamics inherent in urban systems. UrbanDiT leverages the generative power"}, {"title": "3 METHOD", "content": "Urban spatio-temporal data typically falls into two categories: grid-based and graph-based data. Grid-based data is structured in a uniform grid layout. Graph-based data, on the other hand, highlights connectivity, capturing the relationships between various urban entities like streets and intersections. For both different spatial organizations, the temporal dimension is characterized as time series data. The data can be denoted as $X \\in \\mathbb{R}^{N \\times T}$, where $N$ denotes the number of spatial partitions. For graph-based data, $N$ corresponds to the number of nodes, while for grid-based data, it is defined as the product of the height and width of the grid ($N = H \\times W$). This enables a unified representation of urban spatio-temporal data with different spatial organizations.\nIn addition to the commonly recognized (1) forward prediction task, urban spatio-temporal analysis encompasses several other critical tasks. (2) Backward Prediction involves estimating past states based on current or future data. It is essential for understanding historical trends and validating predictive models. (3) Temporal Interpolation aims to estimate values at unobserved time points within a known temporal range. (4) Spatial Extrapolation involves predicting values beyond the observed spatial domain. It is important for assessing potential changes in urban environments and planning for future developments. (5) Spatio-Temporal Imputation refers to the process of filling in missing values in spatio-temporal datasets."}, {"title": "3.2 OVERALL FRAMEWORK", "content": "Figure 2 illustrates the overall framework of our proposed UrbanDiT, which is based on diffusion transformers. This framework seamlessly integrates various data types and tasks into a cohesive model."}, {"title": "Unification of Data and Tasks", "content": "We convert data, characterized by a three-dimensional structure (2D spatial and 1D temporal dimensions), into a unified sequential format. For the temporal dimension, we employ patching techniques commonly used in foundational models for time series (Nie et al., 2022). For grid-based data, we apply 2D patching methods, which are widely utilized in image processing, to organize the data. This allows us to rearrange the three-dimensional grid data into a one-dimensional sequential format. For graph-based data, we use Graph Convolutional Networks (GCN) (Zhang et al., 2019) to process each node and integrate it with the temporal dimension to reshape the data into a one-dimensional format as well. More details of data unification can be found in Appendix B.1\nTo adapt to various tasks, we employ a unified masking strategy. As illustrated in Figure 3, these tasks can be framed as reconstructing missing parts of the data, with distinct masking strategies tailored to each task. For Forward Prediction, we mask future time steps while utilizing past and present data points to predict the missing values. Conversely, for Backward Prediction, we mask past time steps to estimate historical values based on current and future observations. In the case of temporal interpolation tasks, we apply masks to specific time points within a continuous series, allowing the model to fill in these gaps. For spatio-temporal imputation, we randomly mask missing values across both spatial and temporal dimensions, enabling the model to leverage surrounding context for accurate estimations. Finally, in spatial extrapolation tasks, we mask areas outside the observed spatial domain to predict values for unobserved regions based on existing spatial patterns. Consequently, the input of the denoising network $X_t$ is represented as the concatenation of noise features and unmasked spatio-temporal data (conditional observations):\n$X^{+} = X^{+} * (1 \u2013 M) + X^{o} * M$\nwhere $X_t$ denotes the noise features, $M$ is the mask that controls the availability of values for downstream tasks, and $X^{o}$ represents the clean values of the spatio-temporal data. In this way, we can modulate different masks $M$ to facilitate various urban spatio-temporal applications."}, {"title": "Sequential Input of Spatio-Temporal Data", "content": "We first apply temporal patching to process time series data at each spatial location, represented as $X_{N \\times T' \\times D} = CONV(X_{N \\times T \\times D})$, where $T' = \\frac{T}{p_t}$ and $p_t$ is the temporal patch size. Next, for grid-based data, we implement 2D spatial patching, resulting in $X_{p} = CONV_{2D}(X_{H \\times W \\times T' \\times D})$, where $X_{p} \\in \\mathbb{R}^{L \\times D}$, $L = \\frac{H}{p_s} \\times \\frac{W}{p_s} \\times T$. In this way, we effectively reorganize the data into a format that is conducive to transformer architectures."}, {"title": "Spatio-Temporal Transformer Block", "content": "The overall model is composed of multiple spatio-temporal transformer blocks. Each block features both temporal multi-head attention and spatial multi-head attention, with spatial and temporal attention mechanisms operating independently. This design choice is made to enhance computational efficiency, as the complexity of attention scales with the square of the sequence length."}, {"title": "Diffusion Transformer", "content": "We adopt the diffusion transformer model, which integrates a denoising network designed to process complex inputs effectively. The inputs to the denoising network consist of three key components: the noisy spatio-temporal data, the timestep, and the prompt. For the timestep $t$, we utilize them for layer normalization following previous practices (Peebles & Xie, 2023; Lu et al.), which helps stabilize and standardize the input features at each timestep. The prompt, which provides contextual information or guidance for the model, is concatenated with the input data to enhance the model's understanding of the data and task at hand. This concatenation is straightforward due to the transformer's capability to manage variable sequence lengths, providing flexibility in processing diverse inputs. By incorporating these elements, the diffusion transformer model effectively learns to denoise and generate robust desired results in spatio-temporal contexts."}, {"title": "3.3 UNIFIED PROMPT LEARNING", "content": "We propose a unified prompt learning framework to enhance the diffusion transformers' universality across various data types and tasks."}, {"title": "Data-Driven Prompt", "content": "The data-driven prompt is crucial for training a unified model with multiple and diverse datasets, as such datasets often exhibit significant variations in patterns and distributions. In this context, the prompt acts as a guiding mechanism, helping the model to effectively navigate these differences and generate accurate results. Similar to retrieval-augmented generation, prompts retrieve the most relevant information, enhancing the model's ability to contextualize and interpret spatio-temporal data. By aligning the model's learning process with the specific characteristics of various spatio-temporal patterns, data prompts ensure that UrbanDiT can adaptively respond to a wide range of urban spatio-temporal scenarios."}, {"title": "3.3.1 Task-Specific Prompt", "content": "We also design task-specific prompts to enhance the model's performance across different tasks. These prompts are generated from the mask, and we employ attention mechanisms to obtain the mask prompt $P_m$ from the mask map as $P_m = ATTENTION(FLATTEN(M))$.\nThe learned pattern $P_m$ is then concatenated with the input sequence, resulting in $X = CONCAT(P_m, X)$. This enables the model to effectively incorporate task-specific information.\nWe provide more details of data-driven task-specific prompts in Appendix B.2"}, {"title": "3.4 TRAINING AND INFERENCE", "content": "The training process alternates between multiple datasets and tasks. In each iteration, we randomly select a dataset and a corresponding task to perform gradient descent training. This approach enhances the model's robustness by exposing it to diverse scenarios and helps prevent overfitting by ensuring the model learns from a wide range of inputs and objectives. Let $D = {D_1, D_2, ..., D_m}$ represent the set of datasets, and $T = {T_1, T_2, ..., T_k}$ denote the set of tasks. Let $L(d_i, t_i)$ be the loss function for the chosen dataset $d_i$ and task $t_i$, with the model parameters denoted as $\\theta$. Overall, the training process can be summarized as follows:\nFor $i = 1 \\text{ to } N: \\quad d_i \\sim \\text{Uniform}(D), \\quad t_i \\sim \\text{Uniform}(T) \\quad \\Rightarrow \\quad \\theta \\leftarrow \\theta \u2013 \\eta \\nabla L(d_i, t_i; \\theta)$\nwhere $N$ is the total number of training iterations and $\\eta$ is the learning rate."}, {"title": "4 PERFORMANCE EVALUATIONS", "content": "We utilize a diverse set of datasets from multiple domains and cities to evaluate urban spatio-temporal applications. These domains include taxi demand, cellular network traffic, crowd flows, transportation traffic, and dynamic population, reflecting a broad spectrum of urban activities. The datasets are sourced from different cities such as New York City (USA), Beijing, Shanghai, and Nanjing (China), each representing unique urban characteristics. These datasets vary significantly in their spatial structures (e.g., grid or graph formats), the number of locations, and their spatial and temporal resolutions. These variations are influenced by differences in city structures, urban planning strategies, and data collection methodologies across regions. For a detailed summary of the datasets, please refer to Table 5 and Table 6 in Appendix A.\nWe split the datasets into training, validation, and testing sets along the temporal dimension, using a 6:2:2 ratio. To ensure no overlap between these sets, we carefully remove any overlapping points, ensuring clear separation across the temporal splits for evaluation.\nTo evaluate the performance of UrbanDiT, we establish a comprehensive benchmark, comparing it against state-of-the-art models across different urban tasks. For prediction tasks, we include both traditional time series models such as Historical Average (HA) and ARIMA, as well as advanced deep learning-based spatio-temporal models like STResNet (Zhang et al., 2017), ACFM (Liu et al., 2018), STNorm (Deng et al., 2021), STGSP (Zhao et al., 2022), MC-STL (Zhang et al., 2023a), PromptST (Zhang et al., 2023b), STID (Shao et al., 2022a), and UniST (Yuan et al., 2024a). Additionally, we compare against leading video prediction models, including SimVP (Gao et al., 2022), TAU (Tan et al., 2023a), MAU (Chang et al., 2021), and MIM (Wang et al., 2019), as well as recent time series forecasting approaches such as PatchTST (Nie et al., 2022), iTransformer (Liu et al., 2023b), Time-LLM (Jin et al.), and the diffusion-based model CSDI (Tashiro et al., 2021). For graph-based datasets, we evaluate UrbanDiT against cutting-edge spatio-temporal graph models, including STGCN (Yu et al., 2018), DCRNN (Li et al., 2018), GWN (Wu et al., 2019), MTGNN (Wu et al., 2020), AGCRN (Bai et al., 2020), GTS (Shang & Chen, 2021), and STEP (Shao et al., 2022b). Furthermore, for spatio-temporal imputation tasks, we compare our model with state-of-the-art baselines such as CSDI, ImputeFormer (Nie et al., 2024), Grin (Cini et al., 2022), and BriTS (Cao et al., 2018), adapting these methods for temporal interpolation and spatial extrapolation tasks where applicable. We provide more details of baselines in Appendix C.1"}, {"title": "4.2 COMPARISON TO THE STATE-OF-THE-ART", "content": "For this task, we set both the historical input window and prediction horizon to 12 time steps. Depending on the dataset, the temporal granularity varies\u201412 steps may correspond to 1 hour for datasets with 5-minute intervals, 6 hours for datasets with 30-minute intervals, and 12 hours for those with 1-hour intervals. For baselines that cannot handle datasets with different shapes, we train individual models for each dataset. For more flexible models like UniST and PatchTST, we train a single unified model across multiple datasets."}, {"title": "4.3 FEW-SHOT AND ZERO-SHOT PERFORMANCE", "content": "A key strength of foundation models is their ability to generalize easily. Therefore, we perform experiments in both few-shot and zero-shot scenarios, testing its adaptability to new datasets with little or no additional training. In the few-shot scenario, we train UrbanDiT on a small portion of the target dataset\u2014specifically using only 5% and 10% of the available data\u2014and then evaluate its performance on the corresponding test set. This setup challenges the model to generalize well from sparse data. In the zero-shot scenario, no data from the target dataset is provided for training. Instead, we directly evaluate UrbanDiT's performance on the target dataset, relying solely on its pretrained knowledge to handle unseen data without any fine-tuning."}, {"title": "4.4 ABLATION STUDIES", "content": "Unified prompt learning is a key design in UrbanDiT. To investigate the contribution of each prompt to the final performance, we conduct ablation studies by systematically removing each"}, {"title": "4.5 SCALABILITY", "content": "As a foundation model, it is crucial to understand how model performance evolves as the datasize scale varies across different model sizes. This information is valuable for practitioners to train and fine-tune the foundation model effectively. In Figure 8, we explore the relationship between model performance and datasize scale for three model sizes: UrbanDiT-S (small), UrbanDiT-M (medium), and UrbanDiT-L (large). As observed, all three models demonstrate improved performance as the data size increases. However, when the dataset size increases from 0.8 to 1, the large model, UrbanDiT-L, shows a notably steeper improvement (with a slope of 0.011), compared to the medium (slope of 0.0015) and small models (slope of 0.0019). This pronounced scaling effect for the large model indicates its potential to further enhance performance as more data becomes available. These results highlight the promising scalability of UrbanDiT-L, suggesting that it can effectively handle larger datasets and achieve even better outcomes with increased data size."}, {"title": "5 CONCLUSION", "content": "In this paper, we present UrbanDiT, an open-world foundation model built on a diffusion transformers and a unified prompt learning framework. UrbanDiT enables seamless adaptation to a wide range of urban spatio-temporal tasks across diverse datasets from urban environments. Our extensive experiments highlight the model's exceptional potential in advancing the field of urban spatio-temporal modeling. We believe this work not only pushes the boundaries of urban spatio-temporal modeling but also serves as an inspire future research in the rapidly evolving field of foundation models."}, {"title": "3.1 PRELIMINARY", "content": ""}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": ""}, {"title": "C EXPERIMENT DETAILS", "content": ""}, {"title": "C.1 BASELINES", "content": ""}, {"title": "C.2 EXPERIMENT CONFIGURATION", "content": ""}, {"title": "C.3 METRICS.", "content": ""}, {"title": "D ADDITIONAL RESULTS", "content": ""}, {"title": "D.1 RESULTS OF MULTIPLE TASKS", "content": ""}, {"title": "D.2 FEW-SHOT AND ZERO-SHOT PERFORMANCE", "content": ""}, {"title": "A DATASETS", "content": ""}, {"title": "B METHODOLOGY DETAILS", "content": ""}, {"title": "B.1 SEQUENTIAL FORMAT OF INPUT DATA", "content": ""}, {"title": "B.2 UNIFIED PROMPT LEARNING", "content": ""}, {"title": "3.3.2 Task-Specific Prompt", "content": ""}]}