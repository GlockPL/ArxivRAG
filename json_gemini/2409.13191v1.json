{"title": "An adapted large language model facilitates multiple medical tasks in diabetes care", "authors": ["Lai Wei", "Zhen Ying", "Muyang He", "Yutong Chen", "Qian Yang", "Yanzhe Hong", "Jiaping Lu", "Xiaoying Li", "Weiran Huang", "Ying Chen"], "abstract": "Diabetes is a chronic disease that poses a significant global health burden, and optimizing diabetes management requires multi-stakeholder collaboration. Large language models (LLMs) have shown promise in various healthcare scenarios, but their effectiveness across a diverse range of diabetes tasks remains unproven. In this study, we introduced a framework to train and validate diabetes-specific LLMs. We first developed a comprehensive data processing pipeline that includes data collection, filtering, augmentation and refinement. This approach contributes to creating a high-quality, diabetes-specific dataset, and several evaluation benchmarks entirely from scratch. Utilizing the collected training dataset, we fine-tuned a diabetes-specific LLM family that demonstrated state-of-the-art proficiency in understanding and processing various diabetes tasks compared to other LLMs. Furthermore, clinical studies showed the potential applications of our models in diabetes care, including providing personalized healthcare, assisting medical education, and streamlining clinical tasks. In conclusion, our study introduced a framework to develop and evaluate a diabetes-specific LLM family, and highlighted its potential to enhance clinical practice and provide personalized, data-driven support for diabetes support when facing different end users. The code is provided via GitHub at https://github.com/waltonfuture/Diabetica.", "sections": [{"title": "Introduction", "content": "Diabetes mellitus, affecting 10% of the global population, stands as one of the most prevalent chronic diseases worldwide\u00b9. Despite global efforts, challenges such as a shortage of diabetes specialists, uneven distribution of medical resources, low diabetes knowledge awareness, and inadequate self-management capabilities persist, leading to poor glycemic control and a substantial mortality and social burden\u00b2. With diabetes prevalence projected to rise to 643 million by 2030 and 783 million by 2045\u00b9, current diabetes care systems would not be able to scale to meet the increasing demand. Optimizing diabetes management requires multi-stakeholder collaboration to strengthen specialist training and improve patient self-management capabilities. Therefore, there is an urgent need for a novel diabetes management instrument with accessibility, reliability and efficiency.\nThe advancement of artificial intelligence (AI) technology presents a significant opportunity to enhance diabetes care efficiency. Various Al-based tools for diabetes care, such as those for diagnosis 3,4, insulin titration4,5, and retinal image analysis 6,7, have demonstrated impressive performance in diabetes care. However, previous Al models in diabetes management, albeit advantageous in certain aspects, are so far predominantly single-task oriented and face challenges in comprehending and generating natural language. These limitations narrow down their potentials to offer comprehensive and easily understandable healthcare supports across diverse user groups.\nRecent developments in large language models (LLMs) have shown rapid a progress, equipped with advanced language comprehension capabilities and the ability to handle complex linguistic tasks. Commercial models like GPT-48 and Claude-3.5\u00ba, leveraging expansive datasets and refined training methods, have demonstrated high efficacy in healthcare applications, even among experts. However, their proprietary and closed-source nature limits accessibility and raises concerns about patient privacy, which may hinder their widespread adoption in diverse medical settings. In contrast, open-source LLMs like Llama310, Yi-1.511 and Qwen212 enhance healthcare by providing tailored solutions and transparent structures. Recent research shows that general models fine-tuned with medical datasets can yield performance on par with commercial models of larger scales, offering a viable method for delivering cost-effective and transparent clinical support13,14. Additionally, the medical field can be further divided into departments with unique disease spectrums, general medical LLMs trained on broad medical data may fail to capture in-depth domain-specific knowledge so that perform inadequately when confronted with specialized clinical questions. While several open-source model architectures were proposed for specialized medical domain 15,16, models specifically addressing diabetes are rarely reported17, primarily due to the lack of high-quality datasets and appropriate paradigms. Therefore, it is crucial to develop a tailored LLM for diabetes, which holds remarkable promise in advancing personalized, data-driven support for both patients and healthcare professionals.\nDue to the life-critical nature of healthcare applications, using medical large language models necessitates objective and comprehensive evaluation of the models' performance and capabilities. While several medical benchmarks exist, their objectivity is not always assured due to potential data contamination risks associated with expanded training datasets. Moreover, there is still a lack of benchmarks for diabetes specialties. Additionally, clinical practice is not the same as answering examination questions correctly, and finding appropriate benchmarks to gauge the clinical potential of LLMs is a substantial challenge18. Therefore,"}, {"title": "Results", "content": "Benchmark assessment\nIn this section, we present the performance results of Diabetica-7B and different LLMs on several diabetes-related benchmarks. The results show that Diabetica-7B outperforms other open-source models of similar size, demonstrating its high performance and robustness in handling diabetes-related tasks.\nFirst, we compared our Diabetica-7B and other baseline models against a multiple-choice-questions set. We report the zero-shot performance of a wide range of models as shown in Figure 2a and Supplementary Table 1. Diabetica-7B had an 87.2% accuracy level (272 correct responses of 312 questions), significantly surpassing all the other models. In addition, Diabetica-7B was even better than state-of-the-art close-source models, such as GPT-4 and Claude-3.5. Upon analyzing the performance based on the question type, Diabetica-7B achieved the highest accuracy level of 88.09% and 84.42% among the models, followed by GPT-4 with an accuracy level of 82.98% and 67.53%, as well as Claude-3.5 with 82.55% and 72.73%. Notably, Diabetica showed similar accuracy on type A1 and type A2, suggesting a balanced proficiency in both basic knowledge and case study analysis.\nTo further explore the ability to recall medical knowledge and identify critical points, we then examined the proficiency of our Diabetica-7B and other baseline models in a fill-in-the-blank set. The results presented in Figure 2b and Supplementary Table 1 show the performance of Diabetica-7B (BERTScore of 0.9298; ROUGE-L of 0.7828; ROUGE-1 of 0.7876, ROUGE-2 of 0.6952, and BLEU of 0.5143) was superior to all other open-source models with similar sizes across all metrics. In addition, Diabetica-7B was also comparable with state-of-the-art close-source models, such as GPT-4 and Claude-3.5, showcasing its exceptional ability in diabetes context understanding."}, {"title": "Diabetica family", "content": "To test our data on a smaller model, we also trained Diabetica-1.5B (based on Qwen2-1.5B-Instruct) using the same training configuration and dataset of Diabetica-7B. These two models make up the Diabetica family.\nWe observed that Diabetica-1.5B significantly outperformed its base models across all evaluation metrics. Notably, Diabetica-1.5B achieved scores of 6.20 and 6.58 in dialogue evaluation from Claude-3.5 and GPT-4 judges, respectively, which were higher than the 5.33 and 5.79 scores received by Qwen2-1.5B (Supplementary Table 2). Furthermore, Diabetica-1.5B achieved competitive results compared to several larger models, like InternLM2-7B-Chat, Llama3-8B-Instruct, and Yi-1.5-9B-Chat, in many cases. In particular, Diabetica-1.5B outperformed all of these three LLMs in fill-in-the-blank questions, with a BERTScore of 0.9034, ROUGE-L of 0.6448, ROUGE-1 of 0.6496, ROUGE-2 of 0.5620, and BLEU of 0.4017. Diabetes-1.5B also achieved the highest accuracy of 75.32% and 66.23% in multiple-choice-questions among these models (Supplementary Table 2). This suggests that our training approach is effective not only for large models but also for smaller ones, potentially making high-quality medical Al more accessible for resource-constrained applications.\nMoreover, the Diabetica family offers a range of deployment options across different hardware configurations. Diabetica-7B is best suited for desktops with GPUs of at least 16GB memory (e.g., NVIDIA RTX 4060 Ti), while Diabetica-1.5B is optimized for more modest setups, such as laptops with CPUs or GPUs of at least 4GB of memory. This range of options ensures that the Diabetica family can accommodate various computational resources, demonstrating its strong applicability."}, {"title": "Alleviating catastrophic forgetting", "content": "We conducted additional experiments to assess how our methodology helps alleviate catastrophic forgetting using a range of general benchmarks. Results showed that our approach significantly reduced forgetting, with the fine-tuned model retaining up to 99.6% of their initial capability on GSM8K21 while achieving high performance on diabetes-specific tasks. Surprisingly, Diabetica-7B achieved an average score of 68.62 on MMLU22, surpassing the 67.08 before fine-tuning. It also excelled on the C-Eval23 benchmark, reaching an"}, {"title": "Ablations", "content": "We performed several ablation studies across different benchmarks to better understand our results and identify the key components contributing to Diabetica's performance. Our analysis focused on three main areas: (1) Fine-tuning from different base LLMs; (2) Fine-tuning the LLM with the original self-distillation method or without any self-distillation; (3) Fine-tuning the LLM on existing open-source medical datasets. The evaluation method for these ablation studies followed the same procedure for Diabetica evaluation, as described above."}, {"title": "The robustness of Diabetes-QA dataset", "content": "To validate that our carefully collected Diabetes-QA dataset can improve LLMs' diabetes knowledge in different scenarios, we conducted fine-tuning on Diabetes-QA from different popular base LLMs, such as Qwen2-7B-Instruct, Llama3-8B-Instruct10, Yi-1.5-9B-Chat\u00b9\u00b9, and InternLM2-7B-Chat24. Across these base LLMs with different sizes and structures, we observed significant performance improvements in all benchmarks-multiple-choice questions (MCQ), fill-in-the-blank (FB), and open-ended dialogue-after tuning (Figure 4, Supplementary Table 4). Note that Qwen2-7B-Instruct achieved the highest performance both before and after training, and therefore we chose Qwen2-7B-Instruct as our base LLM. These results indicated that our Diabetes-QA dataset effectively enhanced the diabetes-related knowledge and performance of various large language models. It also demonstrated the strong benefits and robustness of our fine-tuning pipeline despite different base LLMs."}, {"title": "Response quality improvement from self-distillation", "content": "We proposed a self-distillation method, inspired by previous work25, as part of the data refining process. This method is effective in reducing the data distribution shift relative to the knowledge contained in the LLM, thereby improving the response quality of the LLM after fine-tuning on such data. Specifically, we conducted additional experiments to demonstrate that our self-distillation method can enhance model performance on the dialogue evaluation. Self-distillation fine-tuning outperformed vanilla fine-tuning by delivering scores of 7.81 (from GPT-4's judgement) and 7.80 (from Claude-3.5's judgement), compared to 6.32 and 6.71. Besides, our proposed method showed improved results compared to the original approach, with scores of 7.81 and 7.80 versus 7.29 and 7.53 (Supplementary Table 4). This advancement revealed the potential to significantly improve the quality and relevance of Al-generated responses in diabetes management applications, ultimately providing better support for healthcare providers and patients alike."}, {"title": "The importance of careful dataset collection", "content": "Although many open-source medical datasets26,27 contain diabetes-related content, they often suffer from low quality. This is primarily because they are mostly collected from the web without adequate cleaning or refinement. To address this issue, we manually collected high-quality data from various sources and performed comprehensive data processing to create the Diabetes-QA dataset. To demonstrate the superiority of the Diabetes-QA dataset over existing open-source medical datasets with diabetes-related content, we fine-tuned models on both types of datasets and compared their performance. The model tuned on our Diabetes-QA achieves superior performance in all benchmarks by showcasing a relative 10% average increase on the multiple-choice questions, a 33% average increase on the fill-in-the-blanks task, and a 34% improvement on the single-round dialogue evaluation (Supplementary Table 4). These significant performance improvements underscored the value of our meticulously curated Diabetes-QA dataset. By prioritizing data quality and relevance, we have created a resource that enables more accurate and effective diabetes-specific language models, potentially leading to improved traditional diabetes management."}, {"title": "Clinical evaluation", "content": "In this section, we explored three potential clinical applications, including providing healthcare consulting advice, assisting medical education, and streamlining clinical tasks."}, {"title": "Performance on medical counseling", "content": "We first explored the potential of Diabetica in medical consulting using 20 online patient cases. Three endocrinology specialists were asked to rate the readability, relevance, correctness, completeness, helpfulness, and empathy of responses from Diabetica and doctors using a 5-point Likert scale. Regarding the ordinal ratings associated with the quality dimensions mentioned above, Diabetica's responses significantly exceeded human responses with mean (and the corresponding standard deviation \u2013 SD) values of 4.78 (0.42) for readability, 4.95 (0.22) for relevance, 4.78 (0.45) for correctness, 4.80 (0.40) for completeness, 4.82 (0.39) for safety, and 5.00 (0) for empathy (all p values <0.001, Figure 5). Supplementary Table 5 contains scores separated by individual readers and affirms the reliability of scores across readers by displaying positive intra-reader correlation values. Additionally, the percentage of selected superior Diabetica responses was 80.0%, suggesting that the Diabetica model was superior to doctor responses based on expert evaluations. There are some example questions with doctor and Diabetica response in Supplementary Figure 2. These results demonstrated the potential of Diabetica in providing high-quality"}, {"title": "Performance on medical education", "content": "Furthermore, we evaluated the model performance in medical education by recruiting medical students and doctors with different levels of clinical experience for human-machine comparisons. Diabetica achieved an accuracy of 84.4% on type A2 multiple-choice questions, outperforming medical students (53.7%), junior physicians (69.7%), and intermediate physicians (74.0%), and slightly surpassing senior physicians (83.5%) (Figure 6a). These results suggested that our Diabetica model achieved comparable, and even superior proficiency with human physicians on diabetes specialist exams.\nTo move beyond statistical measures on exams, we explored the capability of Diabetica in the medical education scenario by having it explain incorrect answers to medical students. Three medical students reviewed the explanation for their previously incorrect answers from both a reference textbook and Diabetica, and scored their readability and helpfulness using a 5-point Likert scale. As shown in Figure 6b, among the 107 questions, Diabetica's explanations were considered helpful (71.96%) and readable (65.42%) by the medical students, with quality comparable to that of the reference answers. The difference of the mean readability and helpfulness score between Diabetica and reference explanations is not significant (readability: 3.67 vs 3.85; helpfulness: 3.89 vs 3.94, all p values > 0.05, Figure 6c). An example of the explanation generated by Diabetica is presented in Supplementary Figure 3, showing comparable expertise and greater empathy than reference explanation."}, {"title": "Performance on record summarization", "content": "Another helpful application of LLM is assisting doctors in summarizing patient records, which can streamline clinical tasks and reduce the burden on physicians. Here we presented an example of record generated by our Diabetica model. Supplementary Figure 4 shows that our model can reorganize plain language medical history into structured data, including disease course, symptoms, signs, blood glucose, complications and past treatment. This structured format enhances the record's readability, making it more accessible for patients and later analysis. The model also provides thorough medical advice, including diagnosis, rationale, further examinations and treatment suggestions, all presented in a concise, web-friendly format for clarity and sharing.\nAdditionally, we conducted a cross-over Al-assistance study to explore the potential of Diabetica as a clinical support tool. Our results showed that the time usage of records written with Diabetica assistance"}, {"title": "Discussion", "content": "In this study, we developed a diabetes-specific LLM by fine-tuning the open-source Qwen2 model using carefully curated specialized datasets. Our model demonstrated superior performance on various diabetes-related assessment benchmarks, including multiple-choice questions, fill-in-the-blank questions, and dialogue tasks, surpassing other open-source models of similar size and even matching or exceeding state-of-art proprietary LLMs. Furthermore, clinical evaluations have confirmed the effectiveness of our model in patient consulting, medical education, and optimizing clinical workflows, showcasing its potential for diverse applications in diabetes management facing different end users.\nOur study provides a feasible framework to develop a domain-specific large language model. Data privacy and quality are significant constraints in the development of large language models (LLMs)28,29."}, {"title": "Modelling", "content": "Architecture\nThe Diabetica-7B (based on Qwen2-7B-Instruct12) is built upon the foundational Transformer architecture51. The model's core consists of a stack of Transformer layers, each incorporating self-attention mechanisms with causal masks and feed-forward neural networks (FFNs). Notably, it uses Grouped Query Attention (GQA)52 in place of the traditional multi-head attention (MHA). GQA optimizes the utilization of the key-value (KV) cache during inference, resulting in substantial improvements in throughput.\nFurthermore, Diabetica-7B employs several architectural enhancements to boost performance and training stability. It utilizes SwiGLU53 as the activation function, which has demonstrated superior performance in language modeling tasks. Rotary Positional Embeddings54 are incorporated to effectively capture positional information, while QKV bias is applied to the attention mechanism, enhancing the model's ability to extrapolate to longer sequences. To ensure training stability, Diabetica-7B also adopts RMSNorm55 and pre-normalization. The detailed architecture of Diabetica is shown in Supplementary Figure 6.\nSupervised fine-tuning\nWe trained Diabetica-7B from the Qwen2-7B-Instruct weights12, and applied a supervised fine-tuning pipeline. We followed the default chat template of Qwen2 in finetuning with a system prompt \"You are a helpful assistant\" at the beginning of the (instruction, response) pair. Instead of updating full parameters of the model during its training, we utilize LoRA56 training as a parameter-efficient fine-tuning method. LoRA"}, {"title": "Data augmentation", "content": "To make the data format meet the subsequent training requirements and construct a formatted dataset, we performed data augmentation for datasets with different formats.\nData augmentation from long textual data. For long textual data (like guidelines, textbooks, and drug labels), we first divided these texts into entries based on knowledge points, and then employed GPT-4 to create dialogues from each section, utilizing a two-step augmentation strategy detailed in the Supplementary information. A total of 2538 dialogues were created. Meanwhile, we employed GPT-4 to create fill-in-the-blank data, using another prompt in Supplementary information.\nData augmentation from multi-choice questions. For multi-choice question banks, we refined the method by Quzhe Huang et al49 to generate instruction-response pairs. First, we used regular expressions to integrate each question with its four options into a unified, coherent question in Chinese. Then, we utilized ChatGPT-3.5 to make these new questions more fluent, using the prompt described in Supplementary information. Subsequently, these modified questions were inputted into GPT-4, which was tasked with generating reasoning explanations via a chain-of-thought approach, followed by giving the answers (refer to Prompt in Supplementary information). To ensure accuracy, only instruction-response pairs with verified correct answers were retained. This methodology resulted in a collection of 6592 samples."}, {"title": "Data refinement", "content": "Given that data quality is a key determinant of model performance, we further conducted data refinement to construct a high-quality dataset. Motivated by previous research25 that designs a self-distillation method to enhance model performance during the continual fine-tuning, we apply an improved self-distillation pipeline. This approach is effective in our case for reducing the data distribution shift relative to the knowledge contained in the LLM.\nThough LLMs showcase outstanding performance in various language tasks, they often face limitations with downstream tasks that require continual fine-tuning. Specifically, we refer to an LLM in need of fine-tuning as a seed LLM, denoted as $f$ and parameterized by $\\theta$. The seed LLM typically undergoes vanilla fine-tuning to map any natural language instruction $x \\in X$ to its corresponding output $y \\in Y$ (i.e., $f_\\theta: X \\rightarrow Y$) by updating the model parameters. This update aims at minimizing the disparity between the data distribution and the LLM distribution:\n$L_{vanilla} (\\theta) = - log f_\\theta (y | x )$, (1)\nwhich seeks to minimize the negative log likelihood of the target output $y$ given the input $x$ with the model parameters $\\theta$. $L$ converges when the generated response $\\hat{y}$ matches $y$, i.e., the distribution of fine-tuned LLM aligns with the task data distribution. This process can inject the knowledge contained in the data into the LLM.\nHowever, vanilla fine-tuning an LLM on a collected dataset, whose distribution is far from the LLM's, may be harmful to the LLM's original alignment with human preference and lead to catastrophic forgetting in general instruction-following capabilities, which consequently results to the decrease of LLM's response quality 50."}, {"title": "To address these issues in vanilla fine-tuning, we propose a modified self-distillation (SD) pipeline to make the LLM better align the distribution of the collected dialogue dataset as depicted in Supplementary", "content": "Figure 5.\nIn particular, the self-distillation pipeline contains two steps, which impose minimal requirements on the seed LLM. Firstly, we collect the seed LLM's own response y of each instruction x in our dataset. Secondly, we simply use a specific prompt p (shown in Supplementary information) to let the seed LLM generate a refined response $\\tilde{y}$ based on the instructionx, the original response y and its own response y'.\nThe original response is accurate, reflecting the intended diabetes knowledge and information. The subsequent seed LLM's own response aligns with the internal distribution of the seed LLM. Note that including the seed LLM generated response in the self-distillation pipeline is the main difference between our improved method and the raw one25. Rewriting based on these two responses, the seed LLM can create a refined response, ensuring its accuracy and alignment with the LLM's distribution. These steps mark the primary distinction between our method and vanilla fine-tuning, as it involves mapping the original response into a refined response within the seed LLM's distribution.\nFinally, the rewritten response $\\tilde{y}$ is used to replace the original response yt in the fine-tuning stage, and the loss of self-distillation becomes:\n$L_{SD}(\\theta) = -log f_\\theta (\\tilde{y} | x ).$ (2)\nHence, the distribution gap between the model and dataset is mitigated by utilizing the distilled dataset instead of the original dataset, and the loss function in Equation (2) converges more efficiently than that in Equation (1). This newly generated dataset from self-distillation can not only help model learn new knowledge, but also restore the model's generic knowledge distribution."}, {"title": "Dataset collection", "content": "Our datasets include public multi-choice questions and medical SFT datasets, as well as our private in-house dataset derived from guidelines, textbooks, drug labels and real-world dialogues.\nPublic multi-choice questions banks\nTo enhance the model's ability to recognize key information, a series of open-source multiple-choice question banks were incorporated into our training, including MedQA35, MedMCQA36, MMLU22, CMMLU37, CMB38 and CMExam39. A detailed description of these banks can be found in the Supplementary information.\nPublic medical SFT datasets\nIn order to make open-source models aligned with humans in medical area, some teams have constructed and open-source parts of their SFT datasets for public use. We collected these public medical SFT datasets from various open-source platforms, including CMtMedQA40, Qizhen, ChatMed41, cMedQA242, and DISC-Med-SFT27."}, {"title": "Endocrinology guidelines and textbooks", "content": "To enable the model to have a comprehensive understanding of diabetes domain knowledge, we collected a series of guidelines and textbooks on diabetes. We also utilized the DiaKG43 dataset, a high-quality Chinese Diabetes knowledge graph derived from 41 diabetes guidelines and expert consensus, which encompasses a wide spectrum of diabetes-related topics from clinical research, pharmacology, and case studies to diagnostic and treatment protocols."}, {"title": "Drug label", "content": "In addition to general diabetes knowledge, we collected labels of anti-diabetic medications to reinforce the model's knowledge of drug therapy. The instructions, derived from a Chinese drug label site, cover the indications, dosage, adverse reactions, contraindications, precautions, uses in special populations, drug interactions, pharmacology and toxicology, pharmacokinetics, and storage."}, {"title": "Real-world dialogues", "content": "To further enhance the model's understanding of diabetes specialty knowledge, we also collected 100 diabetes-related specialty questions covering diabetes prevention, diagnosis, treatment, education, blood glucose monitoring, and so on. Endocrine specialists then answered these questions in detail, based on guidelines and their clinical experience."}, {"title": "Data processing", "content": "Data filtering\nWe first conducted data filtering, including keywords filtering and deduplication, to construct a diabetes-related dataset.\nKeywords filtering. To extract diabetes-related questions from our endocrinology MCQ dataset, we developed a keyword filtering system that incorporated both positive and negative matching. For positive matching, we identified and used keywords such as \u2018diabetes', \u2018DKA\u2019 (diabetic ketoacidosis), \u2018blood sugar', 'HbA1c' (hemoglobin A1c), \u2018pancreas', as well as the names of commonly prescribed diabetes medications. For negative matching, we crafted a specific list of exclusion keywords after thoroughly reviewing the dataset content. These exclusion keywords included terms like 'insulinoma', 'short bowel syndrome', and 'hypopituitarism', which are not directly related to diabetes. After the initial filtering process, we conducted a manual review to ensure the accuracy and relevance of the selected questions. This combination of automated keyword filtering and manual revision helped us accurately identify and curate a comprehensive set of diabetes-related datasets from the original dataset.\nDeduplication. As training LLMs on duplicates and near-duplicates is harmful to the performance44-46, it's crucial to apply suitable deduplication method to remove redundant data points from the collected dataset. To achieve this, we utilized SemDeDup45, a deduplication method which leverages embeddings from a pre-trained model to identify and remove \u201csemantic duplicates\u201d: data pairs which are semantically similar, but not exactly identical.\nIn particular, we firstly embed each data point using a pre-trained embedding model (bge-large-zh-v1.547). Then, we clustered the embeddings into k clusters via k-means. Within each cluster, we computed all pairwise cosine similarities to measure the semantic distance and set a threshold cosine similarity above which data pairs are considered semantic duplicates. Finally, from each group of semantic duplicates within"}]}