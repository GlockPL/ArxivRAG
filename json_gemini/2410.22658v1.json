{"title": "Incremental Learning of Retrievable Skills For\nEfficient Continual Task Adaptation", "authors": ["Daehee Lee", "Minjong Yoo", "Woo Kyung Kim", "Wonje Choi", "Honguk Woo"], "abstract": "Continual Imitation Learning (CiL) involves extracting and accumulating task\nknowledge from demonstrations across multiple stages and tasks to achieve a\nmulti-task policy. With recent advancements in foundation models, there has been a\ngrowing interest in adapter-based CiL approaches, where adapters are established\nparameter-efficiently for tasks newly demonstrated. While these approaches isolate\nparameters for specific tasks and tend to mitigate catastrophic forgetting, they\nlimit knowledge sharing among different demonstrations. We introduce IsCiL, an\nadapter-based CiL framework that addresses this limitation of knowledge sharing by\nincrementally learning shareable skills from different demonstrations, thus enabling\nsample-efficient task adaptation using the skills particularly in non-stationary CiL\nenvironments. In IsCiL, demonstrations are mapped into the state embedding space,\nwhere proper skills can be retrieved upon input states through prototype-based\nmemory. These retrievable skills are incrementally learned on their corresponding\nadapters. Our CiL experiments with complex tasks in Franka-Kitchen and Meta-\nWorld demonstrate robust performance of IsCiL in both task adaptation and sample-\nefficiency. We also show a simple extension of IsCiL for task unlearning scenarios.", "sections": [{"title": "1 Introduction", "content": "Lifelong agents such as home robots are required to continually adapt to new tasks in sequential\ndecision-making situations by leveraging knowledge from past experiences. However, many real-\nworld domains pose substantial challenges for these lifelong agents; the complexity and ever-changing\nnature of these tasks make it difficult for agents to constantly adapt, leading to difficulties in retaining\nknowledge and maintaining operational efficiency [1]. For instance, a home robot agent, operating\nwithin a single household, needs to continuously adapt, learning specific tasks in various areas such\nas cooking assistance in the kitchen or cleaning in the bathroom. At the same time, it is crucial that\nthe agent not only retains but also improves its proficiency in the tasks it has previously learned,\nensuring that it maintains consistent efficiency throughout the home.\nFor these lifelong agents, Continual Imitation Learning (CiL) has been explored, in which an agent\nprogressively learns a series of tasks by leveraging expert demonstrations over time to achieve a\nmulti-task policy. Yet, CiL often encounters practical challenges: (1) the high costs and inefficiencies\nassociated with comprehensive expert demonstrations [2] that are required for imitation, (2) frequently\nshifting tasks in dynamic, non-stationary environments, and (3) privacy concerns [3] related to learning\nfrom expert demonstrations. In this context, CiL faces significant issues in terms of cost, adaptability,\nand privacy, complicating its implementation in real-world scenarios."}, {"title": "2 Related work", "content": "Continual imitation learning. To tackle the problem of catastrophic forgetting in continual learning,\nnumerous studies have employed rehearsal techniques [8, 9, 10, 11], which involve replaying past\nexperiences to maintain performance on previously learned tasks. Another approach involves utilizing\nadditional model parameters to progressively extend the model architecture [12, 13, 14, 15, 16].\nThese methods adapt the model's structure over time to accommodate new tasks. However, rehearsal\ntechniques exhibit high variability in forgetting depending on the replay ratio and often demand\nsubstantial training to incorporate new knowledge [17]. Progressive models, on the other hand, require\nstage identification during evaluation and often overlook unseen tasks [13]. In this work, we propose\na CiL framework that enables effective learning and expansion without requiring rehearsal and stage\nidentification, leveraging pre-trained goal-based model knowledge.\nContinual task adaptation with pre-trained models. Several recent works use pre-trained models,\naccumulating knowledge continually through additional Parameter Efficient Tuning (PET) modules\nsuch as adapters [18, 17, 19, 20, 21, 6, 22]. These methods enhance the flexibility and scalability of\ncontinual learning systems. However, they suffer from inaccurate matching between adapter selection\nand trained knowledge, leading to a misalignment between the knowledge learned during training\nand the knowledge used during evaluation [17, 20], which hinders overall performance. In the realm\nof sequential decision making, some studies have explored adapting pre-trained models. In [6], the\nstate space of tasks is fully partitioned, restricting its applicability in more integrated environments.\nMeanwhile, [7] relies on comprehensive demonstrations for learning, which may be impractical in\nreal-world scenarios. Our study aims to enhance task adaptation efficiency by using incrementally\ngeneralized skills with accurate matching on state space.\nSkill adaptation. Reinforcement learning research has enhanced fast adaptation through skill ex-\nploration [23] and skill priors [24], focusing on improving sample efficiency with offline datasets.\nDespite these advancements, adapting fixed skill decoders to new environments remains challenging.\nTo overcome these limitations, skill-based few-shot imitation learning methods have been developed"}, {"title": "3 Approaches", "content": "Our work addresses three key challenges of CiL: (1) data inefficiency, (2) non-stationarity, and (3)\nprivacy concerns, by adopting retrievable skills in the CiL context. Specifically, our IsCiL framework\nnot only enhances data-efficient continual task evaluation in a non-stationary environment but also\nsupports unlearning as a task adaptation strategy, thereby mitigating privacy concerns.", "sections": [{"title": "3.1 Problem formulation", "content": "In CiL scenarios, we consider a data stream of task datasets {Di}_1, where D\u2081 contains an expert\ndemonstration D\u2081 = {d}, ..., d } for its associated task Ti. To effectively represent complex long-\nhorizon tasks, each task 7\u2081 is comprised of sub-goal list, r = {g}, ...,gM}. Each task dataset is\nsampled in a finite-horizon markov decision process (S, A, P, R, \u03bc\u03bf, \u0397), where S is a state space, A\nis a action space, P is a transition probability, R is a reward function, \u03bc\u03bf is an initial state distribution,\nand H is an environment horizon.\nH\nFor demonstration d = {(st,at)}\u00a31, a state st \u2208 S represents a tuple (ot, gt) consisting of an\nobservation ot and a sub-goal gt. In our work, we represent sub-goals through language and use\nlanguage-based goal embeddings for gt to achieve language-conditioned policies. Then, the objective\nof IsCiL is to obtain a multi-task policy \u03c0*, by which the performance on the tasks in the data stream\ncan be comparable to that of respective expert policies. This is formulated as\n$\\pi^* = \\underset{\\pi}{\\text{argmin }} E_{\\tau \\in T_i} \\text{KL}(\\pi(\\cdot|s)||\\overline{\\pi}(\\cdot|s))$ (1)\nwhere 77 represents an expert policy for 7 and T\u012b denotes a set of evaluation tasks at stage i. In this\ncontext, the evaluation tasks continuously vary across different stages."}, {"title": "3.2 Overall architecture", "content": "To effectively handle complicated CiL scenarios, we present the IsCiL framework which involves (i)\nprototype-based skill incremental learning and (ii) task-wise selective adaptation.\nAs illustrated in Figure 1, in (i) the prototype-based skill incremental learning, we use a two-level\nhierarchy structure with a skill retriever TR Composing the skills for each sub-goal, and a skill decoder\nTD producing short-horizon actions based on state-skill pairs. For this two-level policy hierarchy, we\nemploy a skill prototype-based approach, in which skill prototypes capture the sequential patterns\nof actions and associated environmental states, as observed from expert demonstrations. These\nprototypes serve as a reference for skills learned from a multi-stage data stream. Using these skill\nprototypes, we can effectively translate task-specific instructions or demonstrations into a series of\nappropriate skills.\nThrough this prototype-based skill retrieval method, the policy flexibly uses skills that are shareable\namong tasks, potentially learned in the past or future, for policy evaluation. This enables the CiL agent\nto effectively learn diverse tasks and rapidly adapt to variations, while incrementally accumulating\nskill knowledge from a multi-stage data stream. Furthermore, to facilitate sample-efficient learning\nand enhance stability in CiL, we employ parameter-efficient adapters that are continually fine-tuned\non a base model. Each skill knowledge is encapsulated within a dedicated adapter and incorporated\ninto the skill decoder TD to infer expert actions.\nIn (ii) the task-wise selective adaptation, we devise efficient task adaptation procedures in the policy\nhierarchy to adapt to specific tasks using incrementally learned skills. This enables the CiL agent to\nnot only facilitate adaptation to shifts in task distribution (e.g., due to non-stationary environment\nconditions) but also support task unlearning upon explicit user request (e.g., due to privacy concerns).\nSuppose that the smart home environment undergoes an upgrade with the installation of new smart\nlighting systems throughout the house. In this case, task-wise selective adaptation can be used for\nrapid adaptation by removing outdated control routines associated with the previous systems."}, {"title": "3.3 Prototype-based skill incremental learning", "content": "State encoder and prototype-based skill retriever. To facilitate skill retrieval from demonstrations,\nwe encode observation and goal pairs (ot, gt) into state embeddings st using a function f : (ot, gt) \u2192\nSt. We implement f as a fixed function to ensure consistent retrieval results for learning efficiency,\nmitigating the negative effects of input distribution shifts.\nTo effectively handle the multi-modality of the state distribution in non-stationary environments, we\nemploy a skill retriever TR. For this, we use multifaceted skill prototypes Xz \u2208 X, where X is the set\nof learned skill prototypes. These prototypes capture the sequential patterns of expert demonstrations\nassociated with specific goal-reaching tasks.\n\u03b8\u2082 = \u03c0\u03c1($t; X) = h (argmaxxexS(xz, st)), where S(Xz, st) = maxb\u2208xzsim(b, st) (2)\nHere, h: X\u2082\u2192 0\u2082 denotes a one-to-one function that maps each skill prototype X\u2082 to its dedicated\nadapter parameters 0z, while the similarity function S is defined as the maximum similarity between\nstate s and bases b \u2208 Xz. Each X\u2248 consists of multiple bases (e.g., 20 bases), and each basis b is a\nrepresentative vector containing its corresponding centroid, shaped identically to the state St.", "sections": [{"title": "Adapter conditioned skill decoder", "content": "To effectively use the knowledge of the pre-trained base model\nwithout forgetting, even in a non-stationary changing environment, the skill decoder is conditioned\nbased on parameters. The skill decoder policy \u03c0\u0189(\u00e2t|Ot, gt; 0pre, 0z) operates with the skill adapter\nparameters 0\u2082 and the pre-trained base model @pre, using the Low-Rank Adaptation [27]."}, {"title": "Skill incremental learning", "content": "To incrementally learn new retrievable skills, we update the skill\nprototype and adapter pair (Xz*, 02*) for a novel skill z*. The skill prototype X2* is created by\ndividing a dataset of a single skill into several clusters based on similarity. From each cluster, a\nrepresentative value is extracted to serve as the basis b, representing z*. We use the KMeans algorithm\n[28] to determine these bases, ensuring that the number of bases |xz| adequately captures the diversity\nwithin the dataset of the novel skill. This multifaceted set of bases allows the skill prototype to capture\nan accurate multi-modal distribution of the skill represented in the state space, enabling effective\nretrieval as described in Eq. 2. In our experiment, z* is created for each sub-goal g in the given dataset\nDi for each stage i."}]}, {"title": "3.4 Task-wise selective adaptation", "content": "Task evaluation. Given the pre-trained model @pre and learned skill prototypes X, for given inputs\n(Ot, gt) from the environment, IsCiL performs the following evaluation process.\n\u00e2t ~ \u03c0\u03c1(\u00e2t | Ot, gt; 0pre, 0z), where_0z = \u03c0R(Ot, gt; X) (4)\nThe evaluation process adapts to novel tasks and sub-goal sequences from the environment by\nmodifying the goal gt. This adjustment enables the inference of appropriate current actions, in a\nmanner of similar to handling learned tasks. For example, a kitchen robot tailored to a specific user's\nkitchen setup can continuously and instantly adapt to changes in recipes without additional training.\nTask unlearning. To ensure privacy protection for incrementally learned skills, our architecture\nallows for task unlearning by removing task-specific skill prototypes and adapters. In IsCiL, the\nseparation of skill adapters for each task facilitates easy tagging of task information on each skill.\nWhen an unlearning request is given with a task identifier 7, the corresponding skill prototypes and\nadapters are removed. This approach ensures exceptionally efficient and effective unlearning, aligning\nwith the strong unlearning strategies in continual learning discussed in [3]."}]}, {"title": "4 Experiments", "content": "To investigate the sample efficiency and adaptation performance, we construct complex CiL scenarios\nusing diverse long-horizon tasks [29, 30, 31]. We then analyze the sample efficiency across different\nstages and tasks with three types of scenarios: Complete, Semi-complete, and Incomplete, depending\non how the samples are utilized and shared. Each scenario consists of a pre-training stage followed\nby 20 CiL stages.", "sections": [{"title": "4.1 Environments and data streams", "content": "Evolving Kitchen. Evolving Kitchen is a data stream based on long-horizon tasks in the Franka-\nKitchen environment [29, 30]. Each task requires sequentially achieving four out of seven sub-goals.\nThe scenario consists of a pre-training stage in the environment with only four objects: kettle, bottom\nburner, top burner, and light switch, followed by continual adaptation to tasks involving seven objects."}, {"title": "4.2 Baselines and metrics", "content": "Baselines. We implement continual imitation learning and continual adaptation methods for sequen-\ntial decision-making problems, which do not use rehearsal. First, we consider continual learning\nalgorithms which involve full-model updates (Seq, EWC [35]). We also implement several continual\nadaptation approaches that utilize pre-trained models with adapters (L2M [6], TAIL [7]). L2M\nlearns a key and adapter pair to modulate the pre-trained model, where the key is a retrievable state\nembedding similar to our prototypes. TAIL, unlike L2M, incrementally constructs task identifiers\nand corresponding adapters to modulate the pre-trained model with new task data without forgetting\nprevious tasks. Each method is categorized based on the values used for adapter retrieval: a version\nthat uses no additional identifiers, sub-goal identifiers (denoted as -g), and whole sub-goal sequences\nas single identifiers (denoted as -T). Additionally, we include a Multi-task learning approach as an\noracle baseline, which retains all incoming data at each stage and utilizes it for training in subse-\nquent stages. For all baselines, we use the same pre-trained goal-conditioned policy and a diffusion\nmodel [36, 37] as the base policy architecture. A detailed description of the baselines and their\nhyperparameters are provided in Appendix B.2.\nMetrics. We use three metrics to report CiL performance: Forward Transfer (FWT), Backward\nTransfer (BWT), and Area Under Curve (AUC) [38, 7]. In our long-horizon tasks, these metrics rely\non goal-conditioned success rates (GC), which measure the ratio of successfully completed sub-goals\nto the total sub-goals within each task [39].\n\u2022 FWT (Forward Transfer): This evaluates the ability to learn tasks using previously learned knowl-\nedge. It is measured by the performance of a task when it occurs.\n\u2022 BWT (Backward Transfer): This evaluates the impact of each learning stage on the performance\nof tasks learned in previous stages. It measures the change in task performance from past stages\nobserved in the current stage.\n\u2022 AUC (Area Under Curve): This represents the overall continual imitation learning performance\nin a scenario. It measures the average performance of tasks learned in the current stage over the\nremaining stages of the scenario.\nFor all metrics, higher values indicate better performance, with details provided in Appendix B.3."}, {"title": "4.3 Overall performance : sample efficiency", "content": "Table 1 shows the CiL performance on Evolving Kitchen and Evolving World across three different\nscenarios (Complete, Semi, Incomplete). We compare the performance achieved by our framework\nIsCiL and other baselines (L2M, TAIL) with different conditioning values (g,T) for adapter retrieval.\nIsCiL consistently demonstrates superior performance in AUC across all scenarios, achieving between\n84.5% and 97.2% of the oracle baseline (Multi-task learning). TAIL-T shows the most competitive\nperformance in the Complete CiL scenario across both environments. However, due to its isolated\nadapter for learning and evaluation, it fails to effectively utilize samples across stages.\nL2M and L2M-g exhibit relatively lower and less stable AUC in the Evolving Kitchen scenario.\nConversely, in Evolving World-Semi, they surpass TAIL-T in AUC. This demonstrates that they\nare capable of sharing different skills across stages. Despite this, they still struggle with accurately\nretrieving the correct skill or suffer from performance degradation due to knowledge overwriting.\nUnlike them, IsCiL effectively mitigates overwriting by maintaining distinct skill representations\nacross stages. Both L2M-g and TAIL-g, which aim to leverage sub-goal labels for CiL, struggle to\nmaintain performance due to skill distribution shifts, leading to catastrophic forgetting of skills for\nsub-goals. These challenges reveal that relying solely on sub-goal labels may not be sufficient to\nsustain and share skills effectively across different stages and tasks.\nBoth Seq-FT and Seq-LoRA struggle with forgetting. This is evident in the Complete scenario,\nwhere Seq-FT achieves the highest FWT but shows the lowest BWT, leading to a decline in overall\nperformance. EWC exhibits consistently lower performance, as the regularization used to preserve\npast knowledge significantly hinders learning on current tasks, leading to severe degradation in\nlong-horizon tasks. Although EWC shows higher BWT compared to other sequential tuning baselines,\nits low FWT limits overall effectiveness."}, {"title": "4.4 Task adaptation", "content": "Table 2 shows the unseen task adaptation ability of IsCiL, where only the sub-goal sequence of novel\ntask is provided without demonstrations. This scenario extends the existing Complete CiL scenarios\nby periodically introducing novel tasks. Metrics labeled with the suffix -A indicate results from\nadaptation tasks, whereas the other metrics reflect performance on all tasks. For this scenario, we\nexclude TAIL-T from comparison, as it lacks the ability to adapt to novel tasks.\nIsCiL demonstrates superior task performance in both scenarios, which contributes to greater effi-\nciency in task adaptation. Moreover, in Evolving Kitchen, IsCiL not only demonstrates task adaptation\nability by achieving the highest FWT-A, but also significantly enhances its initial performance, raising\nFWT-A from 52.1 to an AUC-A of 72.8. TAIL-g shows comparable performance in FWT for the\nEvolving Kitchen. However, it struggles with catastrophic forgetting, leading to a -34.9 negative\nBWT when faced with significant distribution shifts in sub-goal demonstrations. In Evolving World,\nL2M, which actively learns to share skills during training, outperforms TAIL-g. L2M is the only\nbaseline achieving performance improvement on unseen tasks through CiL."}, {"title": "4.5 Task unlearning as adaptation", "content": "Table 3 measures CiL performance in scenario with task-level unlearning. For comparison, we use an\nadapter-based approach with parameter isolation-based continual learning private unlearning (CLPU)\n[3], extending TAIL to TAIL-7 CLPU and IsCiL without skill adapter initialization. Similar to IsCiL,\nCLPU learns tasks in isolated models tagged with specific task identifiers and handles unlearning\nrequests by removing the corresponding model parameters of the target task. Both TAIL-7 CLPU\nand IsCiL ensure output distribution equality between the unlearned model and the model trained\nwith the retained dataset. Thus, their CiL performance remains largely unaffected by unlearning.\nAlthough IsCiL exhibits a slight performance degradation of 1.8% ~ 5.2% after unlearning, as\nreported in Table 1, it still demonstrates robustness by achieving a 115% higher AUC compared to\nTAIL-T CLPU in incomplete scenarios."}, {"title": "4.6 Analysis", "content": "Rehearsal comparison. Figure 4 compares the sample efficiency to retain learned knowledge between\nIsCiL and a rehearsal-based continual imitation learning approach, Experience Replay (ER) [40]. For\nER, we adjust the number of stored samples per learning stage, while IsCiL does not store rehearsals"}, {"title": "4.7 Ablation", "content": "Table 4 investigates the impact of the num-\nber of prototype bases on CiL performance,\nshowing that increasing the number of bases\nimproves both AUC and result stability, par-\nticularly around K=10. Results are reported\nbased on units (g and T) used to construct\nnew skill prototypes and the corresponding\nnumber of bases. IsCiL with a single base\nfails to effectively learn task knowledge,\nachieving similar performance to L2M in\nTable 1, due to insufficient representation of\nthe skill distribution. Additionally, the IsCiL\nframework maintains positive BWT scores,"}]}, {"title": "5 Conclusion", "content": "In this study, we presented the IsCiL framework to address key challenges in continual imitation\nlearning (CiL). Our approach incorporates adapter-based skill learning, leveraging multifaceted\nskill prototypes and an adapter pool to effectively capture the distribution of skills for continual\ntask adaptation. IsCiL specifies enhanced sample efficiency and robust task adaptation, effectively\nbridging the gap between adapter-based CiL approaches and the need for knowledge sharing across\nstaged demonstrations. Comprehensive experiments demonstrate that IsCiL consistently outperforms\nother adapter-based continual learning approaches in various CiL scenarios.\nLimitations. Like other adapter-based CiL approaches, IsCiL requires extra computation for evalua-\ntion, which can create overhead, especially in resource-constrained environments. It also depends\non sub-goal sequences for training and evaluation, adding complexity and resource demands. An-\nother limitation is determining the appropriate size of the adapter parameters, which depends on the\nperformance of the pre-trained base model and the degree of task shift, making optimal adaptation\nchallenging. Moreover, balancing the stability of the embedding function with the prototype size\nremains an area that requires further refinement to achieve optimal performance."}, {"title": "A Environment and Data Stream Details", "content": null, "sections": [{"title": "A.1 Franka Kitchen", "content": "We conduct experiments on the Franka kitchen environment [29, 30]. Each Franka kitchen task com-\nprises of 4 sub-goals, from total pool of 7: microwave, kettle, bottom burner, top burner, light switch,\nslide cabinet, and hinge cabinet. Observation is a 60-dimensional vector, which is a combination\nof the positions and velocities of 7-DoF robot arm and interacting objects. We express sub-goal\ninformation using language embedding. The target sub-goal of the current state is acquired by a\npre-defined environmental reward and task, and a sub-goal sequence to solve. We use 24 tasks in\nthe 'mixed' dataset from the D4RL [29]. In the pre-training stage, we train the model only on tasks\ncomprised of following four sub-goals: kettle, bottom burner, top burner, light switch."}, {"title": "A.2 Multi-stage Meta World", "content": "We conduct experiments on the multi-stage variation of the Meta-World environment [31, 34]. Each\nMeta-World task comprises of 4 sub-goals from total pool of 8: puck, box, handle, drawer, lever,\nbutton, door, and stick. The environments are divided into different scenarios based on which 4 out of\n8 objects are placed on the table. For each environment, tasks are defined according to the sequence\nin which the 4 sub-goals must be achieved. Observation is a 140-dimensional vector, which contains\nthe positions and velocities of the 4-DoF robot arm and all interacting objects in the environment.\nIn this environment, sub-goal information is also expressed using language embedding. The expert\ndataset is collected using a heuristic expert policy provided by Meta-World [31]. In the pre-training\nstage of Meta World, we train the model on 24 tasks on a environment consisting of four objects: a\npuck, a drawer, a button, and a door."}, {"title": "A.3 Data Stream", "content": "Evolving Kitchen Tables 5 and 6 display the detailed configurations of the Evolving Kitchen in\nour CiL scenario. Each task involves sequentially solving its respective sub-goals. The underlined\nsub-goals (e.g., kettle) are those missing in the Semi Complete and Incomplete scenarios.\nEvolving World Tables 7 and 8 display the detailed configurations of our Evolving World CiL\nscenario. Similarly, the Evolving World is also presented in the same way as the Evolving Kitchen.\nUnseen Task Adaptation Tables 9 and 10 show the detailed configurations of unseen tasks for\nour Evolving Kitchen-Complete Unseen and Evolving World-Complete Unseen. In the Evolving\nKitchen, 2 new tasks appear every 5 stages. In the Evolving World, 4 new tasks appear every 4 stages.\nEach new task includes only the sequence of sub-goals that must be completed in order, without any\ndemonstrations."}]}, {"title": "B Experiment Details", "content": null, "sections": [{"title": "B.1 IsCiL Implementation", "content": "IsCiL consists of two modules: a skill retriever, TR, and a skill decoder, \u03c0\u03c1. The skill retriever\nTR includes three components: a state encoder f, skill prototypes X, and a skill adapter mapping\nfunction h. Each skill prototype Xz in X is composed of 20 bases b. To modulate skill decoder \u3160D,\nwe use Low Rank Adaptation(LoRA) [27]. In our experiment, we used 4-rank LoRA adapters for\nskill adapter. IsCiL training and evaluation process follows :"}, {"title": "B.2 Baselines", "content": "Seq-FT & Seq-LoRA Sequential Fine Tuning(Seq-FT) is a method that updates the entire model\nsequentially. The variation, Seq-LoRA, is used to determine how effectively the fixed pre-trained\nmodel can utilize its knowledge. Due to poor performance at very low ranks, Seq-LoRA was trained\nwith a 64-rank adapter in our environment.\nEWC[41] Elastic Weight Consolidation (EWC) regularizes the weight update by using the Fisher\ninformation matrix for each network parameter. For our experiment, we adopted the online version of\nEWC, which updates the Fisher information at each stage by exponential moving average, following\nthe methods in [38, 7]. We use the hyperparameter alpha, set to 10, to determine the regularization\nstrength. For updating the online Fisher information matrixF\u2081, we use the Fisher information matrix\ncalculated at the current stage and apply the following formula for regularization: F\u2081 = Fi\u22121+\n(1 \u2013 \u03b3) Fi, where y is set to 0.9.\nL2M[6] L2M is an adapter-based continual learning method consisting of keys and their corre-\nsponding adapters. When an input is provided, L2M uses a similarity function to search for the key\ncorresponding to that input. Input is converted to a query and utilized to search for a key, where key\nis a vector with the same shape as the query. Each key is then updated to maximize its similarity to\nthe data point associated with it. Finally, the data is used to update the adapter that corresponds to\nthe key value found through that data. This method maximizes the diversity of key usage frequency\nby adjusting the similarity between keys and input values during the training phase for learning new\ntasks [18, 6]. In our implementation, we use the normalized state value as the input query to find the\nkey in L2M. For L2M-g, we use the normalized embedding of the state concatenated with the given\nconditioned sub-goal information directly as a query. Our adapter pool consists of 100 adapters, each\nbeing a 4-rank LoRA adapter.\nTAIL [7] TAIL directly assigns an adapter to the given task using the task's identifier. We directly\nmap the given identifier to the corresponding adapter. TAIL-g uses a 4-rank adapter, while TAIL-T\nuses a 16-rank adapter.\nMulti-task At each stage, the model learns from the given data and stores all data for the next stage.\nThe data stored in the buffer is mixed with the data from each stage in a 1:1 ratio for training the\nmodel.\nER[40] Experience Replay (ER), similarly, retains knowledge by storing a subset of the current\nstage's data for the next stage. The data stored in the buffer is mixed with the data from each stage in\na 1:1 ratio for training the model.\nCLPU[3] Continual Learning Private Unlearning (CLPU) [3] is a method for managing continual\nlearning and unlearning. In a continual learning scenario, tasks that require maintenance are trained\nusing existing models, while data that may require unlearning is trained on independent model\nparameters, tagged with when and through which task each model was trained. When an unlearning\nrequest for a specific task or training stage is received, the corresponding model parameters are\ncompletely removed to eliminate the influence of the target unlearning task from the model. CLPU\nprovides highly efficient and powerful unlearning performance with a single delete operation for tasks\nlearned in a continual learning scenario. In our experiment, we integrate the unlearning approach\nCLPU with TAIL-T, which conducts training through task information-based searches, to handle\nunlearning requests in continual imitation learning as a comparative method."}, {"title": "B.3 Metric", "content": "We report 3 metrics for CiL performance for tasks: Forward Transfer(FWT), Backward Trans-\nfer(BWT), Area Under Curve(AUC) [38, 7]. In multi-stage environment task, we report performance\nusing the goal-conditioned success rates (GC), which evaluate the average success rate of successfully\ncompleted sub-goals out of N sub-goals in the task.\n\u2022 FWT: $FWT_\\tau = \\frac{1}{|I_{\\tau}|} \\sum_{i \\in I_{\\tau}} C_{\\tau, i}$ where 7 is task and $C_{\\tau, i}$ represents the GC score of task \u03c4\nat stage i. and $I_{\\tau}$ is set of stage indices where task 7 is trained in the CiL scenario.\n\u2022 BWT: $BWT_\\tau = \\frac{1}{|I_{\\tau}|} \\sum_{i \\in I_{\\tau}} (p^{-1} \\sum_{j=i+1}^{p} (C_{\\tau, j} - C_{\\tau, i}))$, where p is the final stage at\nwhich task is available. In the case where p = i BWT is 0."}, {"title": "B.4 Scenario training details", "content": "Pre-trained Base Model and Stage Settings Table 11 shows the hyperparameters and the architecture\nof the model we used as the base model for all baselines. Table 12 shows the common hyperparameters\nused to train the model for each stage in our experiments."}, {"title": "B.5 Compute Resources", "content": "Computing machine Our experimental platform is powered by an AMD 5975wx CPU and 2x RTX\n4090 GPUs. The operating system used is Ubuntu 22.04.4 LTS, with Nvidia driver version 535.171.04\nand CUDA version 12.2.\nSoftware Detail We utilized jax 0.4.24, jaxlib 0.4.19, and flax 0.8.2 for our implementation.\nTraining time In the context of the Evolving Kitchen, each scenario involves training with three\ndifferent seeds. The training duration averages 2 minutes per stage, with each stage consisting of\n5000 epochs. Each scenario comprises 20 such stages, culminating in a total training time of 2 hours\nfor a single experiment."}]}, {"title": "C Additional Experiments", "content": null, "sections": [{"title": "C.1 Main experiment extension", "content": "Training curve. Figure 7 shows training curves of Evolving-kitchen Complete and Incomplete on\nTable 1. The curves provide a clear illustration of the performance progression of IsCiL and baseline\nmethods, making changes in key metrics over the course of training easily observable."}, {"title": "C.2 Analysis", "content": "Skill adapter rank. Table 14 shows the results of the ablation study on the performance of CiL\nbased on the rank of the skill adapter. Overall, the 1-rank adapter in Evolving Kitchen demonstrates\nsufficient, or even superior, adaptation performance. However, in Evolving World, the 1-rank adapter\nleads to lower overall performance, indicating that some skills cannot be fully learned with a 1-rank\nadapter, resulting in a decline in performance.\nSkill decoder pre-trained model quality. Table 15 shows the results of the ablation study on\nperformance changes based on the quality of the pre-trained model (skill decoder). The quality of the\npre-trained model varies with the number of objects included in the tasks used to pre-train the model.\nA decrease in the quality of the pre-trained model leads to a performance drop in both TAIL-T and\nIsCiL, as the number of objects is reduced from 4 to 1.\nScenario task sequence variation. Table 16 shows the results of the task sequence variation analysis.\nWe report the average performance for four different task sequences in Evolving Kitchen-Complete.\nThe performance of all tasks at the final stage is not significantly affected. Since TAIL-T learns"}]}]}