{"title": "PlanGenLLMs: A Modern Survey of LLM Planning Capabilities", "authors": ["Hui Wei", "Zihao Zhang", "Shenghua He", "Tian Xia", "Shijia Pan", "Fei Liu"], "abstract": "LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.", "sections": [{"title": "Introduction", "content": "Planning, which involves generating a sequence of actions to reach a desired goal state (Newell et al., 1958; Kartam and Wilkins, 1990), is fundamental to human intelligence. For example, when planning a trip to San Francisco, one would search for flights, book tickets based on budget and schedule, arrange local transportation to the airport, and consider alternatives in case of cancellations. These planning tasks require complex reasoning, world knowledge, decision-making, and the ability to adapt, making them a significant challenge for humans. To date, there has been a growing focus on developing LLM planners to automate these complex tasks.\nA comprehensive survey of LLM planners would significantly propel research in this field. Prior studies have explored planning methods and evaluation benchmarks (Huang et al., 2024c; Li et al., 2024d). Huang et al. (2024c) categorized planning methods into decomposition, plan selection, external modules, reflection, and memory, while Li et al. (2024d) reviewed evaluation benchmarks across various domains. However, many of these benchmarks and systems are tailored to specific problems, making it hard to compare LLM planners across domains or determine the best planner for new tasks. Further, there is a lack of clear and consistent evaluation criteria. We believe this gap may hinder the development of advanced LLM planners.\nOur survey builds on the foundational work of Kartam and Wilkins (1990) to address key evaluation criteria for LLM planners. The original paper highlighted challenges in evaluating early AI planning systems, which relied on heuristics and were confined to research labs. The initial criteria were categorized into performance, representation, and communication issues. With more advanced LLM planning, we reexamine this critical framework and focus on six key evaluation criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each criterion, we provide a thorough analysis of representative works, highlighting their strengths and weaknesses.\nWe contribute to the literature by addressing key research questions in LLM planning: What foundational capabilities distinguish them from earlier AI planners? How can we comprehensively measure their performance? We examine the datasets, evaluation methods, and metrics available to the community. We also highlight crucial areas where research is still lacking, including representation, hallucination, alignment, multi-agent planning, connections to agentic workflows, aiming to fill these gaps and advance the field.  Figure 1 presents a taxonomy of six key performance criteria and representative techniques. For those new to LLM planning, we recommend a thorough read, while experts can focus on specific sections. Each section offers clear definitions, relevant works, and includes links to tables in the Appendix. We will dive into the details in the following sections."}, {"title": "LLM Planning Foundations (Tables 1-3)", "content": "We begin by exploring LLM planning foundations, covering widely-used paradigms to provide background for readers unfamiliar with the field. It is broken down into four parts.\nTask Decomposition Task decomposition breaks down abstract goals into specific, manageable sub-goals. It helps mitigate errors by enabling verification at each step and makes LLM reasoning more tractable by narrowing the knowledge space.\nTask decomposition can be performed sequentially, in parallel, or asynchronously. Specifically, sequential decomposition (Wang et al., 2023b; Singh et al., 2023; Sun et al., 2024; Wu et al., 2024) requires that the precondition of the subsequent subgoal is the effect of the preceding subgoal. In contrast, parallel decomposition (Yang et al., 2024) involves subgoals that share the same precondition and effect, where achieving the final goal requires completing only one of these subgoals. Asynchronous decomposition (Lin et al., 2024) involves parallelizing subgoals as well. However, these subgoals in distinct branches have unique preconditions and effects. Asynchronous decomposition requires the completion of all subgoals to achieve the overall goal.\nMoreover, task decomposition can be performed recursively, applying any of the above three approaches at each step. For example, Prasad et al. (2023) recursively break down the goal until each subgoal can be easily executed in the environment.\nLLM + Classical Planner Studies (Valmeekam et al., 2023b; Kambhampati, 2024; Kambhampati et al., 2024) show that LLMs struggle with independent planning. Classical planners, such as Fast Downward (Helmert, 2006), ensure correct plans but depend on experts to translate user queries into formal representations, limiting scalability. A hybrid approach integrating LLMs with classical planners combines the world knowledge of LLMs with the precision and reliability of classical methods, addressing their individual limitations.\nWhen integrated with classical planners, LLMs translate natural language problems into formal representations or generate initial plans. For example, LLM+P (Liu et al., 2023a), LLM-DP (Dagan et al., 2023), and Guan et al. (2023) use LLMs to convert planning problems into PDDL (McDermott et al., 1998), solved by Fast Downward or BFS(f) (Lipovetzky et al., 2014). Valmeekam et al. (2023b) employs LLMs to generate an initial plan, guiding the LPG planner (Gerevini et al., 2002), which iteratively refines it until a correct solution is found.\nSearch Algorithm Search algorithms, including Breadth-First Search, Depth-First Search (Yao et al., 2024; Katz et al., 2024), Monte Carlo Tree Search (Hao et al., 2023; Zhou et al., 2023a; Zhao et al., 2024; Shi et al., 2025), and Greedy Best-First Search (Koh et al., 2024; Hirsch et al., 2024), have been applied to improve LLM-based planning. These algorithms treat planning as a search problem, using search policy to guide the exploration of various possibilities. Search algorithms excel in planning problems by offering systematic exploration, optimality guarantees, and formal verification without requiring extensive domain knowledge, though they may be computationally intensive compared to more specialized methods like task decomposition.\nAll search algorithms consist of four core components: (1) Search Policy determines node exploration order, which are defined by the underlying search algorithm and are independent of LLMs. (2) Expansion generates possible actions from a state, often using LLMs to propose actions based on user instructions and current environment. (3) World Models define state transitions based on action preconditions and effects, using LLMs (Hao et al., 2023), classical planners (Hirsch et al., 2024), or external environment simulators (Zhou et al., 2023a; Zhao et al., 2024; Koh et al., 2024). (4) Evaluation assesses state progress toward the goal via scores computed by predefined functions (Katz et al., 2024), LLM/LVM ratings (Yao et al., 2024; Hao et al., 2023; Zhou et al., 2023a), log-likelihood scores (Hirsch et al., 2024), voting (Yao et al., 2024), self-consistency scores (Zhou et al., 2023a) or reward models (Chen et al., 2025).\nFine-tuning Current LLMs are not specifically trained for agentic tasks like planning, and prompt-based methods, which do not update model parameters, cannot fundamentally improve performance in these areas (Chen et al., 2023a; Wang et al., 2024). Fine-tuning, either focused on planning-specific tasks or broader agentic capabilities, enhances planning correctness by directly updating LLM parameters.\nPlanning-specific fine-tuning involves training a pretrained model on planning-focused tasks (e.g., Blocksworld or ALFWorld (Shridhar et al., 2020b)), to improve planning performance. For example, Jansen (2020) and Chalvatzaki et al. (2023) fine-tuned GPT-2 (Radford et al., 2019) on ALF-World, demonstrating its effectiveness in robotics planning.\nGeneralized agentic fine-tuning optimizes mod-"}, {"title": "Criterion I: Completeness (Table 4)", "content": "The completeness of planning has two key aspects: (1) if a valid plan exists, the model should generate it correctly, and (2) if no feasible plan is possible, the model should recognize this and refrain from generating an incorrect or arbitrary plan.\nA plan is correct if it achieves the goal within a fixed budget while avoiding excessive complexity and infinite loops. To ensure correctness, the LLM must work with classical sound and complete solvers (Guan et al., 2023; Hao et al., 2024a). Also, the LLM has to accurately translate the domain and problem into the specific format (e.g., PDDL), required by these solvers (Guan et al., 2023).\nIn terms of identifying unsolvable planning problems, those with inherently unachievable goals, even top LLMs (e.g., GPT-4 (Achiam et al., 2023)) and Large Reasoning Models (e.g., OpenAI O1 (Jaech et al., 2024)) struggle due to hallucination issues (Aghzal et al., 2023; Valmeekam et al., 2024)."}, {"title": "Criterion II: Executability (Tables 5-6)", "content": "Executability checks if a plan can be carried out in a given environment while meeting all constraints. A executable plan must use only allowed actions and recognizable objects. Beyond basic precondition and postcondition rules, planners must consider extra constraints, such as avoiding sugar when baking a cake for diabetics (Yuan et al., 2023). Importantly, executability and correctness are orthogonal: an executable plan isn't necessarily correct, since it might be grounded and follow all constraints but still fail to reach the goal; likewise, a correct plan isn't always executable since it may only include high-level steps that can't be executed in a specific environment. Real-world applications typically require plans that are both correct and executable, especially when the executors are not humans (e.g., robots and computers).\nTo ensure plans are executable, researchers have proposed several approaches, including Object Grounding, Action Grounding, Closed-Loop Systems, and Sample-then-Filter.\nObject Grounding Object grounding ensures the LLM planner uses objects available in the current environment when generating plans. The simplest way to do this is by feeding observed or available objects into the planner via prompts (Huang et al., 2022b; Song et al., 2023; Lin et al., 2023a; Singh et al., 2023) or neural embeddings (Sharma et al., 2021; Ahn et al., 2022). In partially observed environments, where some object information are uncertain (e.g., needing to clean a cup that could be in a cabinet, drawer, or fridge), the planner can generate multiple possible plans, one for each scenario, and select the first feasible one (Prasad et al., 2023; Dagan et al., 2023; Zhao et al., 2024). Sun et al. (2024) takes a different approach, first generating a plan with placeholders for objects, then filling in the blanks with observed objects during execution.\nAction Grounding Action grounding ensures all actions in a plan can actually be executed in the current environment. Like object grounding, the simplest way is to explicitly list all admissible actions in LLMs' inputs (Singh et al., 2023). If a step goes beyond the executor's capabilities (e.g., combining multiple allowed actions into one), the LLM planner should be reprompted to break it down until every step is executable (Prasad et al., 2023).\nHierarchical Planning is another common method for grounding actions in LLM planning (Huang et al., 2022a; Raman et al., 2022; Song et al., 2023; Hazra et al., 2024; Bhat et al., 2024). It starts with high-level steps and then translates each one into a sequence of executable actions. This can be done in two ways: either generating all high-level steps first and then refining them into actions or translating each step as it's generated. If an action isn't exactly admissible, the closest valid action is retrieved instead (Huang et al., 2022a; Raman et al., 2022).\nSample-then-Filter Since LLMs alone can't guarantee plans meet all constraints, this approach first generates multiple plans and then verifies them, selecting only those that pass all checks. Yuan et al. (2023) ranks InstructGPT-generated plans using cosine similarity with task embeddings and selects"}, {"title": "Criterion III: Optimality (Table 7)", "content": "Optimality means achieving the goal state through the best possible plan. It poses a greater challenge than standard planning, which only requires reaching the goal state. Researchers have proposed two paradigms for achieving the optimal plans: LLM + Optimizer and A* search-based methods.\nLLM + Optimizer It combines the LLM, which turns user requests into symbolic optimization problems, with an optimizer that solves them and finds the best solution (Ju et al., 2024; Hao et al., 2024b). For example, TTG (Ju et al., 2024) uses the LLM to convert travel planning requests of minimum total costs into Mixed Integer Linear Programming problems, then runs an optimizer such as SCIP (Bestuzheva et al., 2021) to provide the optimal plan. Compared to LLM + classical planners, where LLMs define the domain and problem in a formal representation, LLM + optimizers ensure optimal solutions by further formulating and solving constrained optimization problems.\nA* Search-Based Methods A* search always finds the lowest-cost optimal solution, making it a natural choice for LLM-based planners to achieve optimality. ToolChain* (Zhuang et al., 2023) combines A* tree search with an LLM, which suggests next steps and estimates heuristic scores, to create plans with the fewest tool API calls. SayCanPay (Hazra et al., 2024) uses A* search with LLMs to generate the shortest possible plans. Beyond A* (Lehnert et al., 2024) trains a Transformer model, Search-former, to mimic A* search paths for complex tasks like Maze navigation and Sokoban puzzles, optimizing for the fewest steps. Besides A* search, other search algorithms (e.g., DFS and MCTS) can also be used to find optimal solutions."}, {"title": "Criterion IV: Representation (Tab. 8-9)", "content": "In LLM planning, representation refers to how inputs and outputs are formatted. Inputs include domains (predicates and actions), problems (initial and goal states), and environmental observations, while outputs are the generated plans. Effective representation enhances problem comprehension and execution efficiency, especially given LLMs' sensitivity to prompts. We discuss this in two contexts: LLM-as-a-Translator and LLM-as-a-Planner.\nLLM-as-a-Translator LLM-as-a-Translator converts between natural language (NL) and formal planning languages (e.g. PDDL), making classic planners more accessible to non-experts. By converting natural language tasks into formal representations and translating the resulting plans back into NL, LLMs reduce ambiguity, minimize hallucinations, and enable external validation, improving both usability and reliability in planning systems (Xie et al., 2023; Zhou et al., 2024; Sun et al., 2024; Silver et al., 2024).\nRecent work has used LLMs to translate natural language descriptions into PDDL (Liu et al., 2023a; Guan et al., 2023; Xie et al., 2023; Dagan et al., 2023; Zhou et al., 2024), LTL (Pan et al., 2023), and STL (Chen et al., 2024a). To ensure reliability, translations should be tested on development or external datasets like Planetarium (Zuo et al., 2024). If there are syntax or semantic errors, validators (e.g. VAL (Howey et al., 2004)) or human experts can provide feedback for the LLM to fix them.\nLLM-as-a-Planner When LLMs act as standalone planners without classical planners or optimizers, various methods help encode environmental information, domains, and plans beyond just natural language. Environment and domain details have been represented using tables (Lin et al., 2023a), condensed symbols (Hu et al., 2024), Pythonic code (Aghzal et al., 2023; Singh et al.,"}, {"title": "Criterion V: Generalization (Table 10)", "content": "Generalization refers to LLM planners' ability to apply learned strategies to new, more complex out-of-domain scenarios beyond its training environment, which can be enhanced through three key approaches: fine-tuning (described previously in Section 2), generalized planning, and skill storage. Given the diverse user queries in the real-world deployments, ensuring LLM planners' generalizability is important alongside other performance.\nGeneralized Planning Generalized planning extracts common patterns from training solutions to tackle larger, more complex tasks within the same domain (Srivastava et al., 2011). For example, in the Delivery dataset (Yang et al., 2022), models trained on small-scale deliveries (9\u201317 locations) can generalize to larger ones (70\u2013100 locations) using the same core strategy. Silver et al. (2024) approached this by prompting LLMs to summarize the domain and generate a minimal, generalizable Python-based plan.\nSkill Storage Skill storage focuses on learning and reusing previously acquired skills to tackle new problems. E.g., Wang et al. (2023a) introduced a skill library that stores successfully executed skills (e.g., Combat Zombie). These skills are abstracted and generalized for reuse in similar situations (e.g., fighting spiders involves similar actions to fighting zombies). When encountering an unseen task, the LLM planning system retrieves relevant learned skills based on the task and current states, then applies them to generate an effective solution."}, {"title": "Criterion VI: Efficiency (Table 11)", "content": "Efficiency in LLM planning means reducing computational and monetary costs by decreasing LLM calls, world model interactions, input and output lengths, and model sizes. This is crucial especially developing planners based on commercial LLMs.\nReduced LLM and World Model Calls To decrease LLM and world model calls, several tricks are used: (1) generating the entire plan in one shot instead of step-by-step to reduce redundant prompts (Hu et al., 2023b; Sun et al., 2024; Gonzalez-Pumariega et al., 2024); (2) checking feasibility by world models only at the end of each subgoal, not after every action (Sun et al., 2024; Gonzalez-Pumariega et al., 2024); (3) merging plans with the same prefix actions or subgoals to avoid duplicate world model checks when sampling multiple plans (Hu et al., 2023b); and (4) in tree search-based methods, querying the LLM once to list all possible actions and states, and generate a state transition and goal check function, avoiding repeated LLM and world model calls at each node (Katz et al., 2024).\nShorter Inputs and Outputs Reducing input and output length includes decreasing prompt and plan tokens and minimizing actions in the final plan to alleviate the load on executors. For spatial reasoning and planning, (Hu et al., 2024) introduces Chain-of-Symbols (CoS), a compact symbolic representation that replaces natural language descriptions in CoT (Wei et al., 2022) trajectories. Lehnert et al. (2024) uses search dynamic bootstrapping to iteratively fine-tune a LLM, replacing training cases with solutions with less tokens and equal optimality. To minimize actions, Dagan et al. (2023) and Wang et al. (2023b) use action selectors based on predefined rules or trained models to find the shortest successful plan.\nSmaller Model Sizes Shrinking the model size can reduce the computational burden, accelerating training and inference while lowering costs. To train a smaller planning model, Brahman et al. (2024) uses GPT-3 (Brown et al., 2020) as the teacher and T5 (Raffel et al., 2020) as the student, distilling the teacher's planning capabilities into the more compact student model."}, {"title": "Evaluation", "content": "Datasets LLM planning evaluation is conducted on two types of datasets: planning-focused datasets and downstream-task datasets.\nPlanning-focused datasets primarily assess planning abilities. The most common scenarios include (1) Embodied environments, (2) Task scheduling, (3) Games, and (4) Task decomposition (Li et al., 2024d). Figure 1 presents commonly used planning datasets; readers can refer to Li et al. (2024d) for further details.\nWhile most of the datasets mentioned above assess whether the generated plans are correct, some specifically target key performance criteria in LLM planning. For grounding, Open Grounded Planning (Guo et al., 2024) and Embodied Agent Interface (Li et al., 2024f) evaluate performance in embodied environments, while CoScript (Yuan et al., 2023), TravelPlanner (Xie et al., 2023), and PPNL (Aghzal et al., 2023) focus on planning problems with constraints. For representation, Planetarium (Zuo et al., 2024) assesses LLMs' ability to translate natural language into PDDL. For optimality, Lin et al. (2024) and Gonzalez-Pumariega et al. (2025) introduce tasks requiring optimal plans using asynchronous actions. PPNL (Aghzal et al., 2023) can also evaluate a planner's ability to identify unachievable goals (i.e., completeness).\nPlanning abilities can also be evaluated through downstream tasks, where planning is integral to task completion, and stronger planning skills enhance overall performance. Downstream tasks can be categorized as follows: (1) Agentic tasks, including reasoning-oriented tasks, tool-use-oriented tasks, programming tasks, and web tasks (Zhou et al., 2023b; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023), (2) Generation tasks, including video (Lin et al., 2023b), image (Zala et al., 2023) and text generation (Moryossef et al., 2019). Please refer to Figure 1 for example datasets.\nMethods The most common approach to evaluating LLM planning is to test it in a simulated environment and validate the generated plans using either an internal verifier provided by the environment or external verifier (e.g., VAL (Howey et al., 2004)) to ensure they achieve the intended goal. When ground truth plans are available, LLM-generated plans can also be compared against these reference plans (Zheng et al., 2024).\nThe second evaluation method is human evaluation, typically used in the following cases: (1) No available verifier: when certain simulated environments (e.g., VirtualHome) or real-world scenarios (e.g., using a mobile manipulator) lack automated verification; (2) Open-ended problems: tasks with ambiguous instructions or generative outputs (e.g., text or images) where multiple valid solutions may differ from the ground truth.\nThe final evaluation method, LLM-as-a-Judge, uses another LLM to automatically assess the quality of generated plans in the cases mentioned above. This approach has been increasingly adopted in recent LLM planning research (Guo et al., 2024; O'Donoghue et al., 2023). Compared to human evaluation, LLM judges are faster and more cost-effective, making them especially valuable for evaluating large datasets. However, this method has limitations, such as position bias, length bias, self-inconsistency, and sensitivity to prompts (Zheng et al., 2023; Ye et al., 2024; Wei et al., 2024). Addressing these issues is crucial to ensure reliable assessments. For more details on LLM-as-a-Judge, please see Li et al. (2024a,b); Gu et al. (2024).\nMetrics Figure 1 summarizes commonly used evaluation metrics for planning-focused tasks, along with representative works. Performance criteria are measured using specific metrics: (1) Completeness: success rate and goal condition recall measure whether the generated plan reaches final or stepwise goals, while classification metrics (e.g., true negative rate, false negative rate, and unreachable accuracy) assess the planner's ability to identify unachievable tasks. When ground-truth plans are available, the exact match score is used. (2) Executability: executability rate evaluates whether the plan can be executed in the environment, while constraint pass rate checks if constraints are met. (3) Optimality: measured by the optimality rate (i.e., the percentage of optimally solved tasks). (4) Efficiency: common metrics include inference time, input and output token counts, number of plan steps, and model size. (5) Representation: the number of parseable problems indicates correct translations. (6) Generalization: all these metrics can also be applied to unseen scenarios to assess generalization. See Figure 1 for definitions of individual metrics and representative works."}, {"title": "Discussion", "content": "In this section, we discuss the limitations in current LLM planning research studies, and suggest future directions for improvement and more comprehensive evaluations of LLM planning performance.\nDatasets and Baselines The current evaluation of LLM planning has its limitations, primarily because studies often rely on limited datasets and baselines. This makes it tough to fairly and comprehensively compare different methods. Most studies only use a few datasets from a single domain and difficulty level, and they do not evaluate all the six performance criteria. Inconsistent dataset choices make direct comparisons difficult. On top of that, many studies only compare against basic baselines such as CoT or ReAct, which does not help in comparing more advanced approaches. To fix this, a public, standardized leaderboard should be set up that covers all performance criteria, uses consistent evaluation metrics, includes a variety of baseline and advanced methods, and utilizes diverse datasets spanning multiple domains and difficulty levels. Another useful direction would be to create multilingual planning datasets and assess LLM performance across different languages.\nRepresentation LLMs are highly sensitive to prompts (Sclar et al., 2024; Razavi et al., 2025), but most research relies on natural language without comparing them to alternative formats, such"}, {"title": "Limitations", "content": "This work primarily focuses on commonly studied domains involving single-agent scenarios, such as robotics, household tasks, and computer-based tasks. We acknowledge that LLM planning is also applied in other areas, including the natural sciences (O'Donoghue et al., 2023; Liu et al., 2024), the Internet of Things (Cui et al., 2024), and multi-agent scenarios. However, these studies follow similar methodologies and evaluations, suggesting our survey's comprehensiveness. We focus on six commonly used performance criteria and exclude others, such as security and personalization, due to limited research in these areas. Instead, we discuss them in our future directions section."}, {"title": "Conclusion", "content": "In our survey, we explore the landscape of modern LLM planners, proposing key performance criteria and discussing evaluation challenges. Our proposed criteria offer a structured approach to assess LLM planners across diverse domains. By systematically analyzing existing systems, datasets, and evaluation strategies, we aim to provide a foundation for future research in this space. We encourage researchers to build on our findings to create robust, highly adaptable, and efficient LLM planners."}]}