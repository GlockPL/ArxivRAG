{"title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\nDegradation for Mathematical Reasoning", "authors": ["Zhen Li", "Yupeng Su", "Runming Yang", "Zhongwei Xie", "Ngai Wong", "Hongxia Yang"], "abstract": "Large language models have achieved significant advancements in complex mathematical reasoning benchmarks, such as MATH. However, their substantial computational requirements present challenges for practical deployment. Model quantization has emerged as an effective strategy to reduce memory usage and computational costs by employing lower precision and bit-width representations. In this study, we systematically evaluate the impact of quantization on mathematical reasoning tasks. We introduce a multidimensional evaluation framework that qualitatively assesses specific capability dimensions and conduct quantitative analyses on the step-by-step outputs of various quantization methods. Our results demonstrate that quantization differentially affects numerical computation and reasoning planning abilities, identifying key areas where quantized models experience performance degradation.", "sections": [{"title": "1 Introduction and Related Work", "content": "Large language models (LLMs) have substantially advanced the state of mathematical reasoning in artificial intelligence, demonstrating remarkable performance on diverse tasks ranging from basic arithmetic and quantitative reasoning to intricate geometric and competition-level problems (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023; Achiam et al., 2023; OpenAI, 2023). Critically, these models excel not only at producing correct final answers, but also at providing step-by-step solutions that elucidate the underlying reasoning process (Lewkowycz et al., 2022; Wei et al., 2022). Benchmarks such as MATH (Hendrycks et al., 2021) highlight these capabilities, where LLMs can guide humans through complex multi-step problems with detailed reasoning chains.\nHowever, such advancements come at a cost. The computational requirements of LLMs-both in terms of memory and latency-pose significant practical barriers (Kaplan et al., 2020; Hoffmann et al., 2022; Gou et al., 2024). To address these efficiency challenges, researchers have explored model compression techniques such as pruning (LeCun et al., 1989; Han et al., 2015), knowledge distillation (Hinton, 2015; Jiao et al., 2019; Yang et al., 2024), and more recently, quantization (Hubara et al., 2018; Jacob et al., 2018; Yao et al., 2022). Quantization reduces memory usage and computational overhead by representing weights and activations in low-bit formats (e.g., INT8), halving GPU memory consumption and nearly doubling throughput in operations like matrix multiplication and attention (Rastegari et al., 2016; Lin et al., 2015; Dettmers et al., 2022). While it performs well on standard NLP tasks with minimal performance loss (Ma et al., 2024), its effect on complex mathematical reasoning-requiring precise, contextually coherent, and logical steps-remains unclear, particularly for tasks like MATH or Code.\nPrior work has hinted at potential reasoning degradation under aggressive quantization (Shen et al., 2020; Kim et al., 2021; Lin et al., 2023), but a systematic understanding is lacking. Such limitations pose a stark contrast to the precision-based assumptions of advanced mathematical reasoning approaches. Models like Minerva (Lewkowycz et al., 2022) and reasoning strategies such as Chain-of-Thought (CoT) prompting (Wei et al., 2022; Xiong et al., 2023) rely heavily on high-fidelity internal representations to ensure logical consistency and correctness. Quantization, however, may disrupt these critical internal states. Meanwhile, ongoing developments in model optimization transcend simple training paradigms. Complex post-training pipelines that align models to human preferences and adapt them for specialized applications have gained traction, often involving intricate infrastructure and iterative refinement processes (Schulman et al., 2017; Rafailov et al., 2024).\nAgainst this backdrop, the interplay between quantization, advanced inference strategies, and the underlying reasoning fidelity of LLMs emerges as a key research question. To summarize, in this study, we present the following key contributions:\n\u2022 We evaluate quantization techniques across diverse models and tasks. Our findings show significant accuracy degradation on complex benchmarks like MATH, where all methods result in notable performance losses.\n\u2022 We introduce a method aligning step-by-step outputs during training, ensuring consistent knowledge exposure across full-precision and quantized models. This enables detailed reasoning while preserving format alignment.\n\u2022 To overcome limitations of benchmarks focused only on final answer correctness, we propose a framework analyzing mathematical reasoning across seven dimensions. Through qualitative and quantitative analyses, we identify specific areas where quantized models struggle in complex reasoning tasks."}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Model Quantization", "content": "We investigate the effects of both weight-only quantization like GPTQ (Frantar et al., 2022), AWQ (Lin et al., 2024) and weight-activation quantization like SmoothQuant (Xiao et al., 2023). We evaluate these methods under different quantization"}, {"title": "2.2 Format Alignment & Knowledge Infilling", "content": "We employ a stage of format alignment and knowledge infilling to ensure the model internalizes the introduced tokens and responds effectively to instruction-following prompts. Specifically, we employ LoRA (Hu et al., 2021; Wu et al., 2024) as a supervised fine-tuning (SFT) technique for quantized models. LoRA injections enable efficient refinement of a small subset of parameters, facilitating fine-tuning without extensive modifications to the entire parameter space.\nWe jointly train the embedding and output layers alongside the LoRA adapters to integrate new tokens and align them with the model's underlying representation space. Since these tokens explicitly define reasoning steps and answer boundaries, ensuring their accurate interpretation is crucial. By combining LoRA adaptation with standard fine-tuning, we effectively \"inject\" knowledge of these stepwise structures into the model's latent space. This not only enhances the model's ability to process structured instructions but also improves its robustness against the challenges posed by quantization. Additionally, this approach allows us to better analyze the differences in inference steps before and after model quantization."}, {"title": "2.3 Data", "content": "We use PRM800K dataset (Lightman et al., 2023), a large-scale resource containing step-level correctness annotations of model-generated solutions to problems from the MATH benchmark (Hendrycks et al., 2021). These annotations allow models to learn granular reasoning steps, rather than simply producing final answers. To make the problem tractable within our limited computation and space, we sample 8,000 problem-solution pairs from PRM800K. Each sample provides a well-documented reasoning trajectory, enabling the model to learn how to follow instructions and accurately traverse multi-step logical derivations.\nTo facilitate stepwise reasoning (Birchlabs), we introduce four special tokens: <|step_start>, <|step_end|>, <|answer_start|>, and <|answer_end|>. These tokens serve as structured markers that guide the model through the reasoning"}, {"title": "3 Experiments", "content": "We evaluate our models on the MATH dataset using OpenCompass evaluation framework (Contributors, 2023). Notably, for the Fine-tuned model, we use zero shot evaluation with prompts consistency between the prompts used during evaluation and those introduced during the knowledge injection phase. In contrast, vanilla pretrained models and quantized models are evaluated with few shot prompts to compensate for their limited instruction-following capability, aligning with the characteristics of pretrained models. This setup ensures a fair comparison across all models."}, {"title": "3.1 General Setup", "content": "All experiments in this study were conducted on a cluster of 8 NVIDIA 80G A800 GPUs with a global batch size of 16. We applied LoRA fine-tuning to the vanilla model, while QLORA (Dettmers et al., 2024) was employed for the quantized models. Training was also performed on the embed_tokens and lm_head layers, using a learning rate of 1.0e-5. A cosine learning rate schedule with a warmup ratio of 0.05 was adopted for optimization."}, {"title": "3.2 Prompting", "content": "Figure 2 illustrates an example of the prompt utilized during the format alignment and knowledge infilling phase and evaluation phase. The prompt structure was carefully designed to align with the training inputs, ensuring that the models received consistent and clear instructions."}, {"title": "3.3 Results", "content": "Table 1 compares the performance of full precision and quantized models on the MATH dataset for Llama-3.2-3B and Llama-3.1-8B. Both pretrain and finetune methods are evaluated. The comparison includes both the pretrain and finetune methods, measuring the performance (Score) and the corresponding loss (\u25b3) introduced by quantization. The Vanilla model serves as the upper baseline, while quantized models exhibit varying degrees of accuracy loss. Finetune consistently improves performance across all settings. Among the quantized methods, SmoothQuant (W8A8) shows the smallest degradation, demonstrating better robustness compared to AWQ and GPTQ."}, {"title": "3.4 Case Study and Analyses", "content": "To investigate the underlying reasons for the performance degradation observed in the quantized models, we categorized the errors into seven distinct types:\n\u2022 Misunderstanding of Problem: Fails to accurately interpret the problem statement, leading to incorrect assumptions or interpretations.\n\u2022 Logical Error: Flaws in the reasoning process, where the sequence of steps lacks logical coherence or correctness.\n\u2022 Computation Error: Mistakes in arithmetic or algebraic calculations within the steps.\n\u2022 Formula Misuse: Incorrect application or selection of mathematical formulas and theorems relevant to solving the problem.\n\u2022 Step Omission: Missing crucial steps in the reasoning process, resulting in incomplete or incorrect solutions.\n\u2022 Boundary Condition Error: Inadequate consideration of special cases or boundary conditions that affect the problem's outcome.\n\u2022 Symbol Error: Incorrect usage of mathematical symbols, such as misplacing signs or using incorrect variables.\nUsing the defined error_types_list, we leveraged the GPT-40 API to perform a quantitative analysis of the quantized models' errors. The process involved the following steps:\n\u2022 Automated Error Classification: For each erroneous output from the quantized models, the GPT-40 API was prompted to classify the error based on the predefined error types.\n\u2022 Manual Verification: To ensure the accuracy of the automated classifications, a subset of the results was manually reviewed and helped in refining the prompt for better precision.\nThe Figure 3 & 4 reveal that computation errors and step omissions are the most significant failure modes in quantized models. Computation errors, particularly prominent in AWQ-W4A16, are likely caused by low-bit precision overflow and underflow, which propagate inaccuracies across multi-step calculations. SmoothQuant-W8A8 shows a notable increase in step omissions, likely due to reduced activation precision, which disrupts the retention of intermediate reasoning steps. While boundary condition errors are less frequent, this may reflect a \"survivor effect\", where earlier errors, such as logical or computation failures, prevent the models from reaching steps where boundary conditions would be tested. These results suggest that quantization disrupts step-by-step reasoning, exposing the models' limitations in handling complex mathematical tasks. The underlying causes of these errors may stem from the limited numerical precision and accumulated quantization noise in low-bit formats, which impair the representation of long-term dependencies and intermediate values in complex reasoning tasks."}, {"title": "4 Conclusion", "content": "This study demonstrates that low-bit quantization significantly impacts the mathematical reasoning capabilities of LLMs. By introducing a stepwise knowledge injection framework and a multidimensional evaluation method, we mitigate performance degradation and provide insights into the trade-offs between efficiency and reasoning fidelity. These findings offer guidance for optimizing LLM deployment in resource-constrained settings."}, {"title": "5 Limitations", "content": "In this study, we investigated the performance degradation of quantized models on the competitive mathematical reasoning benchmark MATH through both qualitative and quantitative analyses, employing seven distinct capability dimensions for evaluation. However, our work is constrained by limited time and computational resources, which restrict the scale of our experiments and prevent comprehensive case studies or ablation analyses across models of varying sizes. Additionally, we did not explore whether the planning failures and computational errors observed in low-bit models are caused by storage overflows resulting from low-precision data formats through knowledge exposure experiments.\nMoreover, we identified intriguing phenomena where quantized models outperform full-precision models in certain specific mathematical solving steps. However, we did not conduct extensive experiments to further investigate these enhancements. As a short-term objective, we plan to perform more targeted analyses on the degraded capabilities of quantized models and explore efficient methods for their recovery, incorporating more human involvement in case studies to uncover deeper insights. Furthermore, we aim to extend our evaluation to other types of reasoning tasks, assessing quantized models from the perspective of their inherent properties to achieve a more comprehensive understanding of their performance across diverse reasoning scenarios."}]}