{"title": "KunServe: Elastic and Efficient Large Language Model Serving with Parameter-centric Memory Management", "authors": ["Rongxin Cheng", "Yifan Peng", "Yuxin Lai", "Xingda Wei", "Rong Chen", "Haibo Chen"], "abstract": "The stateful nature of large language model (LLM) serving can easily throttle precious GPU memory under load burst or long-generation requests like chain-of-thought reasoning, causing latency spikes due to queuing incoming requests. However, state-of-the-art KVCache centric approaches handle load spikes by dropping, migrating, or swapping KVCache, which faces an essential tradeoff between the performance of ongoing vs. incoming requests and thus still severely violates SLO.\nThis paper makes a key observation such that model parameters are independent of the requests and are replicated across GPUs, and thus proposes a parameter-centric approach by selectively dropping replicated parameters to leave precious memory for requests. However, LLM requires KVCache to be saved in bound with model parameters and thus dropping parameters can cause either huge computation waste or long network delay, affecting all ongoing requests. Based on the observation that attention operators can be decoupled from other operators, this paper further proposes a novel remote attention mechanism through pipeline parallelism so as to serve up-coming requests with the additional memory borrowed from parameters on remote GPUs. This paper further addresses several other challenges including lively exchanging KVCache with incomplete parameters, generating an appropriate plan that balances memory requirements with cooperative execution overhead, and seamlessly restoring parameters when the throttling has gone. Evaluations show that KUNSERVE reduces the tail TTFT of requests under throttling by up to 27.3 \u00d7 compared to the state-of-the-art.", "sections": [{"title": "Introduction", "content": "Transformer-based large language models (LLMs) are reshaping the computing industry. Such models generate output in a streaming fashion, producing results token by token. The tokens are used by downstream tasks like chatbots [34], copilots like programming assistants [23], and interactive agents [20]. These tasks commonly involve human interactions, which are sensitive to latency above 1s [11, 56], and shorter is better even within 1s [22]. Thus, both the time to generate the first token (TTFT) and the time between subsequent tokens (TBT)\nare important to user experiences.\nCompared to prior AI models like vision models, LLM serving is stateful: before generating the final token, the intermediate results (called KVCache) must be kept in the GPU memory (i.e., high-bandwidth memory, HBM). Such a stateful generation pattern presents a key challenge: the serving latency (especially TTFT) could spike (up to 190 \u00d7 in BurstGPT [49] (\u00a72.2) and others (\u00a75)) when precious HBM experiences throttling during spikes in KVCache requirements. Such throttling is common: when the incoming request rate spikes, which is commonly found in real-world workloads [36, 18], the GPU memory demands also spike because they are proportional to the number of requests. Besides, even a small number of requests can cause throttling when they generate many tokens, since KVCache demand grows with the token count. This could happen because modern LLM workloads increasingly generating more tokens to achieve better performance. A concrete example is the chain-of-thought reasoning [3]: with more tokens generated (either explicitly by prompting [50] or implicitly via training [54]), the LLMs have been proven to show enhanced problem-solving abilities [31].\nMemory throttling severely impacts model serving latency, because requests must wait for GPUs to free up sufficient memory for processing. Unfortunately, LLMs typically take seconds to generate all tokens and release memory. Hence, if throttling occurs, the incoming requests must be queued for such long so as to proceed, resulting in latency spikes. This paper answers a key question: how can we effectively handle the latency spikes caused by memory throttling during LLM serving?\nState-of-the-art approaches use a KVCache-centric approach to handle memory throttling [29, 51, 39, 45]. When a GPU lacks sufficient HBM for incoming requests, the system adjusts the KVCache for active requests dropping it, swapping it out, or migrating it to an available spare GPU to make room for waiting requests (detailed in\u00a72.3). However, KVCache-centric memory management still has a lot of deficiency for handling latency spikes. (with tail latencies still 83-126 \u00d7 higher than normal, detailed in \u00a72.3). Since KVCache is essential to LLM inference, KVCache-centric approaches face an essential tradeoff between the performance of ongoing and incoming requests: pruning a large amount of KVCache significantly disrupts ongoing requests, while pruning only small amounts provides little memory for incoming requests, leaving the queuing problem unaddressed.\nIn this paper, we explore a new design space: parameter-centric memory management based on two key observations. First, the massive computational requirements of model serving mandate the use of GPU clusters [45, 12, 35, 10], with model parameters replicated across multiple GPUs to handle workloads beyond a single GPU's computation capacity. Second, GPU memory is dominated by both KVCache and model parameters, with model parameters comparable (or even larger) than KVCache. Hence, when a GPU encounters memory throttling, pruning a portion of the parameters can free up sufficient HBM for queued requests. Such requests are then executed alongside ongoing requests with a larger batch size, eliminating the queuing delays. Since no KVCache is pruned, there is no tradeoff between current and incoming requests as in KV-centric approaches. The missing parameters can be borrowed from other replicas, such that requests can be processed with pipeline parallelism with incomplete parameters. The overhead of pipeline parallelism only requires an activation forward pass, which is negligible compared to the queuing.\nHowever, parameter-centric memory management is non-trivial and faces several challenges. First, when parameters are dropped, many requests remain in processing due to the streaming execution nature of LLMs. Such requests cannot continue execution with dropped parameters since LLMS require a one-to-one mapping between parameters and KVCache for processing. While migrating the KVCache on demand can address this issue, such migration would stall request processing considering the huge amount of KVCache. Worse even, we cannot use state-of-the-art live KVCache migration techniques [45] because they require a complete copy of the model parameters on both source and target GPUs, which is missing due to our drop-based management. Second, pipeline execution comes at a cost: when more GPUs are cooperatively executing requests, the greater the overhead of pipeline parallelism. With an improper plan (how to drop parameters), even if we execute more requests concurrently with sufficient HBM, queuing can still occur because our GPUs cannot digest these requests timely. Thus, we need to come up with a plan that minimizes overhead while maintaining sufficient memory for queued requests.\nTo address the above challenges, we propose KUNSERVE, the first LLM serving system that uses parameter-centric memory management to cope with bursty memory throttling for LLM serving. To live migrate KVCache with incomplete parameters, we make a key observation: in LLM computation, operators that require KVCache don't need parameters, and operators that require parameters don't need KVCache. Therefore, during KVCache migration, we can offload the KVCache-related computation to the source GPU without interrupting current requests, which realizes live KVCache migration even with incomplete parameters. KUNSERVE further incorporates a greedy-based plan generation method to quickly find a drop plan with minimal performance overhead. The observation is that the overhead positively correlates with the number of GPUs involved in the cooperative execution. Moreover, combining more GPUs in a cooperative group, though it can free more memory, yields diminishing returns. Thus, a greedy approach that minimizes instances in a group typically yields a reasonable plan quickly.\nIn addition to parameter dropping, KUNSERVE also dynamically restores dropped parameters when KVCache memory demand decreases, which is essential for maintaining high performance during normal load. Extensive experiments show that under various real-world traces and datasets, KUNSERVE achieves up to 13.6\u201327.3 \u00d7 tail latency decrease and reduces 53.8\u2013100% SLO violations compared to the state-of-the-art systems like vllm [29] and Llumnix [45]. In summary, this paper makes the following contributions:\n\u2022 A new design space called parameter-centric memory management for coping with memory throttling in LLM serving (\u00a73).\n\u2022 A set of new techniques to make our parameter-centric memory management method efficient (\u00a74).\n\u2022 A set of evaluations confirming the effectiveness of the design (\u00a75)."}, {"title": "Background and Motivation", "content": "LLM basics. LLM is a transformer-based [48] deep learning model. Compared with traditional DNN, a key difference is that it executes requests in an auto-regressive pattern with a prefill and decode phase. In the prefill phase, the input is fed to the model to generate the first token of the output. The decode phase then iteratively generates the rest of the output in a token-by-token way, where each iteration takes the previously generated token as well as the prefill input as the input. The decode\u2020 ends when the model generates a special end-of-sequence (EOS) token.\nDuring decoding, since the same prefix of input is shared across all the iterations, the internal results (termed KVCache) are cached in the GPU memory (HBM) for acceleration. This makes the computation patterns of prefill and decode different [36, 27, 56]: the prefill is compute-bound, while the decode is memory-bound.\nBecause LLM is computationally intensive, LLMs are typically deployed on GPU servers, which process both the prefill and decode phases in a batched manner [53, 29] to improve the GPU utilization.\nServing metrics: TTFT and TBT. As the output tokens are generated iteratively, current systems serve requests in a streaming fashion, i.e., once a token is generated, it is immediately returned to the user. Thus, both the prefill latency"}, {"title": "Problem: TTFT spikes caused by memory throttling", "content": "Huge HBM demands and memory throttling of LLM serving. The overall memory demand for processing serving is huge. Considering serving a Qwen-2.5-14B model, where each token will consume 192 KB memory, an already small amount due to GQA [7], a typical batch (e.g., 64 K tokens) on the BurstGPT trace will consume 12 GB HBM, not considering the retained memory of unfinished requests.\nSince LLM serving is memory hungry, GPUs may meet memory throttling for two reasons. First, real-world traces exhibit spiked loads: Figure 1 (a) shows a real-world trace on BurstGPT [49], where the incoming request rate increases by 14 \u00d7 at time 150s with no clear pattern, so the serving systems require a proportional KVCache demand to these requests. Even worse, for each request, its memory usage depends on how many tokens need to be generated. Since the tokens are generated auto-regressively by the model, its memory usage continuously grows and can throttle the GPU memory. For example, the average and variance of a BurstGPT [49] request stay time is 12 and 149 seconds, respectively\u2020.\nTTFT spikes. GPU memory throttling is a killer for the serving performance. As shown in Figure 1 (c), the TTFT increases to up to 155 \u00d7 after the throttling happens (see (b)). The increase comes from the queueing delays of waiting for sufficient memory to be freed up. The queuing time can be lengthy because the memory can only be freed once the ongoing request batch finishes. As we have mentioned before, the ongoing requests may take a long time to finish (up to 386s in BurstGPT)."}, {"title": "Existing KVCache-centric solutions", "content": "Drop the KVCache [29, 51, 39] (a). A naive solution is to drop some KVCache of ongoing requests (1). Subsequently, queued requests can be processed with the freed GPU memory (2). However, requests with dropped KVcache must be re-enqueued and recomputed, which also stalls incoming requests (3) even without considering the recomputation overhead. As a result, Figure 3 (c) shows that simply dropping the KVCache still faces 190 \u00d7 increases during memory throttling, even with a modest average memory load (49.8%).\nSwap the KVCache [56, 29] (b). We can follow classic system solutions by swapping some KVCache to second-tier memory, e.g., host DRAM or SSD (1), thus freeing up memory for pending requests (2). However, frequent swapping in and out inevitably introduces overheads (\u0398). To control such overheads, existing systems only swap out a small number of requests (e.g., default 1 in vllm), otherwise it will introduce 3.5-7.8 \u00d7 P99 TBT increase, see Figure 4. This causes a dilemma: to avoid TBT increases, an optimal swap configuration in vllm still has a 85 \u00d7 TTFT increase during memory throttling.\nMigrate the KVCache [45] (c). Finally, observing that a serving cluster typically has multiple instances, a recent work (Llumnix [45]) migrates requests from a memory-throttled GPU to other (relative) spare GPUs (1) for pending requests (2). Note that migration is necessary to avoid fragmentation of the KVCache memory, so simply re-direct requests to spare GPU without migration is insufficient. However, until the migration is done, the queued requests can still be stalled. Worse even, under spike workloads, all instances' memory can be throttled (3), opening up little room for migration. In Figure 3 (e), migration even exacerbates the issue, with P99 TTFT increased by 205 \u00d7 (compared to the P50). This is because migration does not come free especially when it cannot help free up more memory.\nKVCache-centric vs. parameter-centric swap. While parameter swapping can also alleviate the issue, it is typically less considered because it impacts all the requests especially the TTFT. Figure 4 compares two swapping strategies. Note that we have adopted standard optimizations like asynchronous swapping and pipelined prefetching [21]. When swapping a few memory such as 0.5 GB, KVCache swap is always 1.1-1.3 \u00d7 faster than parameter swap for P50 TTFT, and up to 2.6 \u00d7 faster for P50 TBT. This is because only 8.9-9.9% requests are impacted by KVCache swap while all requests are impacted by parameter swap. When swapping a larger amount of memory, the performance of both methods collapses, because they are all bottlenecked by the PCIe bandwidth."}, {"title": "Approach, Challenges, and System Overview", "content": "Our approach: parameter-centric memory management with cooperated execution. Unlike previous approaches that focus on adjusting KVCache memory, we found adjusting parameter memory which consumes a significant portion (34-65%) of GPU memory (see Table 1) can better handle memory spikes. This is because it allows us to instantly free up sufficient GPU memory to hold queued requests without affecting the KVCache of currently executing requests. Figure 3 (d) illustrates the idea: when memory throttling occurs and requests are queued on all the GPUs, we will drop half of the model's parameters on both GPUs (\u25cf). This allows us to enqueue more requests on both GPUs ( and ) for execution with a larger batch size. Note that the average latency of requests increases due to large batch size, but the increases are much smaller than the queuing latency."}, {"title": "Detailed Design and Implementation", "content": "Unlike traditional KVCache-centric memory management, where each instance manages its own GPU memory, our parameter-centric management uses a two-level global-local management strategy: we first generate a memory plan across instances, then execute the plan across involved instances locally. Such a two-level approach is necessary because, unlike KVCache, dropping parameters without coordination will cause LLM execution to fail.\nManage the memory globally with buddy groups. Upon receiving a notification from the workload monitor that one or more instances are likely to encounter (or have already encountered) memory throttling, the global manager decides how to drop the parameter across instances to free GPU memory for the KVCache. To ensure complete copies of the parameters always exist, we logically organize the instances into buddy groups, where each group is guaranteed to have exactly one copy of model parameters. Without memory throttling, each instance operates as its own buddy group. During memory throttling, two or more instances can merge into a single group, and requests in a group execute with pipelined parallelism.\nWith the group abstraction, generating a memory plan translates to finding a buddy configuration that has sufficient memory for queuing requests while minimizing overall latency increases. Specifically, the buddy configuration plan generation problem can be formulated as follows:\n$Cost = \\frac{Thpt. with group}{Thpt. without group} = \\frac{Thpt_1() \\times N}{\\sum_i \\sum_j Thpt_y()G_{ij}}$\nwhere N is the number of instances (without drop) in the cluster, $G_{ij}$ is 1 if the instance j is in the group i, and there would be at most N groups. $Thpt_1()$ is the throughput of a single instance with full parameter and $Thpt_y()$ is the throughput of a group given a number of instances. For example, $Thpt_y(4)$ is the throughput of serving with a 4-stage pipeline parallelism. Both $Thpt_1()$ and $Thpt_y()$ can be profiled offline.\nWe denote the size of a replica of model parameters as P. The optimal group configuration needs to meet the following constraints:\n(a) Free memory ensures the memory released by dropping the parameters is sufficient to hold the additional memory requirement (R).\n(b) Group surjectivity ensures that every instance j is assigned to exactly one group i.\nPut it all together, the optimization problem is listed below:\nminimize Cost\ns.t. $R \\le \\sum_{ij}(\\sum_j G_{ij} - 1) * P), \\forall i, \\sum_j G_{ij} > 0.$\n$ \\sum_i G_{ij} = 1, j \\in 1..N$\nFinding the optimal configuration for the above program is hard due to non-linearity [46]: $Thpt_y()$ is a non-linear function, and the constraints (a) is also not linear. Fortunately, based on LLM serving features, we found a greedy method minimizing the number of instances in a group can achieve a near-optimal result. There are two greedy strategies.\nMemory-greedy strategy. The strategy prioritizes the group configuration with more droppable parameter memory. To achieve this, we need to group as many instances as possible. However, our system cannot double the throughput by doubling KVCache capacity. As a result, we terminate the greedy process when the additional memory exceeds origin KVCache region.\nThroughput-greedy strategy. The strategy prioritizes the group configuration with higher throughput. We achieve this by finding the group configuration that can just satisfy the memory demand of history requests (see Algorithm 1). We choose the throughput-greedy strategy in our system. This is based on two observations. First, in LLM serving, the more instances in a group, the higher the inference cost due to more pipeline stages and bubbles [6, 56]. Moreover, when the number of instances in a group increases, there are diminishing returns to increasing the instance number in a group. For example, considering we have a cluster with 8 instances. If we divide them into 4 groups, then the freed memory is 4 \u00d7 the parameter size. If we increase the instances per group by grouping all 8 instances into one group, the overall free memory only increases by 1.75\u00d7 (7\u00d7 more memory).\nSpecifically, Algorithm 1 outlines our greedy algorithm group assignment process. We start from a state where each instance is in its own group, and iteratively merge groups with the smallest size until the memory constraint is met. The complexity of the algorithm is O(N log N), where N"}, {"title": "Cooperated execution under parameter drop/restore", "content": "This section focuses on how we design cooperative execution to ensure efficient processing of both new and ongoing requests when we drop parameters to alleviate memory throttling. At the end we briefly describe how we restore parameters when the memory is no longer throttled.\nServe new requests after the parameter drop. After the buddy group is formed, our distributed execution scheduler will first select a buddy group based on existing load balancing policies [45], and forward incoming requests to the instance with the first half of the layers of parameters. Afterward, these requests will be served by instances in the group cooperatively with pipeline parallelism.\nServing victim requests with KVCache exchange. Unlike serving new requests, serving requests that have entered the decode phase during the parameter drop is more challenging. As we have mentioned in the overview, these requests cannot use pipeline parallelism for execution because the involved GPU may lack portions of the KVCache needed for execution. The KVCache can be missing bidirectionally between GPUs in a buddy group since different GPUs drop different portions of the KVCache, so we have to exchange KVCache between them. For example, consider two GPUs that initially have complete copies of the model parameters. To handle memory throttling, we form them as a buddy group and GPUO drops the parameters for the first half of layers while GPU1 drops the second half. Ongoing requests on GPUO lack the second half of KVCache when executed cooperatively on GPU1, while ongoing requests on GPU1 face a similar situation. To ensure smooth execution of ongoing requests, GPUs within a buddy group must exchange KVCache of ongoing requests, i.e., GPUO sends half of its current KVCache to GPU1, and GPU1 does the same in return.\nDuring KVCache exchange, ongoing requests must wait for the exchange to complete before execution, which is costly. For example, when serving a Qwen-2.5-14B model on A100 GPU, when parameters are dropped, we need to exchange 33 GB KVCache between two GPUs, taking 1.3s with a 200 Gbps GPU-direct RDMA network. An ideal execution should be live where ongoing requests can continue execution. Live exchange is similar to live KVCache migration, but due to the dropping of parameters, existing techniques become obsolete in our case. For example, Llumnix [45] proposes a pre-copy-like live KVCache migration technique that executes requests on the source GPU while migrating the KVCache to the target. It is not feasible for us because the source GPU lacks parameters to continue execution. Another solution is to borrow the idea of post-copy from virtual machine live migration [25]: we move the missing KVCache on-demand on the target GPU. However, this solution is not more efficient than a stop-of-the-world migration, because a token can be emitted only if all the KVCache is transferred.\nLive KVCache exchange with remote attention. To realize live KVCache exchange, we propose remote attention that leverages the computing features of LLMs: not all computations in a transformer layer need KVCache, and computations that need KVCache do not need the parameters. The upper part of Figure 9 shows 5 major operators in a transformer layer: only the attention operator requires KVCache. Moreover, the attention is a fused scaled dot-product with softmax [48], which requires no parameter other than the KVCache. As a result, if an instance misses the KVCache of the layer during exchange, we can schedule the attention to be executed on the source (remote) GPU without stopping the process of ongoing requests.\nThe right part of the Figure 9 shows a concrete example. Suppose GPUO and GPU1 form a buddy group, and a request has originally been processed on GPU0 before the drop of parameters. When we drop the parameter on GPU0, we will schedule the computation of layers with dropped parameters on GPU1 for execution (\u2460). The KVCache of the dropped layers will be exchanged to GPU1 concurrently with the request execution (\u2461). During the execution, if it executes the attention operator, while the corresponding KVCache has not been transferred to GPU1, we will send the activation back to GPU0 for execution (3). Note that the attention result should be sent back to GPU1 for executing the next operator.\nRemote attention and network requests coordinations. Im"}, {"title": "Online dispatcher, monitor and others", "content": "Online dispatcher. Similar to existing cluster LLM serving systems [45], our dispatcher performs load balancing and prioritizes requests based on their priorities. Differently, our dispatcher also re-balances requests after parameter dropping to minimize queuing latency. A re-balance is necessary because, after parameter dropping, although an instance can serve all its queued requests within its memory, the insufficient computational power of this GPU may still cause queuing delays. To prevent such queuing, we set a maximum batch limit for each instance. This limit can be profiled offline thanks to the static performance features of LLM inferences. For requests that cannot be executed in the current maximum batch (overflowed), we will send them back to the dispatcher for a re-balance. Specifically, after executing the parameter drop plan from \u00a74.1, the dispatcher collects information about each instance's current and overflowed load. It then redistributes the overflowed load across available spare instances.\nAutoscaling. The parameter-centric approach has limitations in handling memory throttling, e.g., there is a finite amount of memory that can be freed by dropping parameters. Like prior work [45, 18], we can automatically start new instances on demand in such cases, though efficient model scaling is beyond the scope of our paper. But it's worth noting that KUNSERVE works seamlessly with model autoscaling-we can process more requests during model scaling, a time-consuming process compared to LLM inference [36].\nOnline monitor and drop trigger. Our monitor periodically checks instances' memory pressure (i.e., used KVCache vs. available HBM) and triggers the parameter drop if necessary. We employ a threshold-based trigger policy similar to those used in scaling serverless functions [19]. Specifically, we calculate whether future instances will meet memory throttling with the following two rates: the increase rate of KVCache requirements and the change rate of available HBM. When the future KVCache usage is about to exceed the available HBM in one second, we will trigger the drop. We use such a tight threshold (one second) that only triggers the drop under throttling or near-throttling for two reasons. (1) Our drop mechanism works instantly, so early drops provide few benefits, and (2) dropping too early may harm performance, especially with false positives, because pipeline execution does not come for free. We leave the exploration of more complex policies, such as time series forecasting-based ones [40], as future work.\nFine-grained KVCache block management. To enable elastic KVCache management, existing systems [29] allocate and deallocate KVCache in fixed-sized blocks, where the block size is a constant related to the model size. This causes internal fragmentation in KUNSERVE: because we may shrink the model size through parameter dropping dynamically, so the KVCache block will shrink accordingly. Thus, we implement a fine-grained GPU HBM allocator based on buddy memory allocation [28], where block granularity shrinks to layer granularity, the smallest unit of parameters on a serving instance. This eliminates the internal fragmentation.\nFault tolerance. Unlike traditional LLM serving where failures between instances are isolated, since the parameters are fully replicated, in KUNSERVE an instance failure can disrupt other instances if they are in the same group. To ensure serving capability under partial instance failures, we re-plan parameters globally once a failure is detected. By replicating parameters in host DRAM, we can always ensure successful parameter restoration after a failure."}, {"title": "Evaluation", "content": "We have implemented KUNSERVE from scratch with 11K C++. We chose C++ as our core GPU memory management and local scheduler implementation, thanks to its fine-grained control of CUDA memory and network transfer, and GPU kernel executions. Though Python is dominant in LLM serving systems, we found its I/O coordination is too slow (e.g., ms-level with async-io) and cannot fit our requirements. Our fine-grained control includes careful scheduling of remote attention requests and normal pipeline requests, piggybacked processing different requests together to utilize the GPU's idle time, and efficient live KVCache exchange overlapping. Note that we reuse (Python) modules from current serving systems like Llumnix [45] and vllm [29] for global dispatchers and efficient GPU kernels.\nModels. We use state-of-the-art models including Qwen-2.5-14B and Llama-3.1-70B throughout our evaluations. For Qwen-2.5-14B, each serving instance originally uses 1 GPU while Llama-3.1-70B and Llama-3.2-90B use 4 GPUs per instance.\nTestbed. For single-GPU workloads (i.e., Qwen-2.5-14B), we evaluate KUNSERVE on 8 servers each with one NVIDIA A800 80GB GPUs, 128 CPUs, and 2TB host memory. The servers are connected via 200 Gbps RDMA network. For evaluations on Llama-3.1-70B and Llama-3.2-90B, we use another cluster that have 2 nodes each with 8 NVIDIA A800 80GB GPUs. GPUs within one server has 400 GB/s NVLink bandwidth, while each GPU across servers have 100 Gbps RDMA network.\nMetrics. Like priori works [56, 29, 52, 45]. we focus on the TTFT and TBT of requests for our evaluations. For TBT, we calculate TBT of a request as the average decode time of all its tokens generated in decode phase. For TTFT, we directly reports the measured time when popping out the first token. We also report the SLO violations of TTFT and TBT. Similar to previous works [56, 38, 44, 16], we set a tight TTFT and TBT SLO as 5 \u00d7 of their P50 values of requests when the system is under a modest load. This is because our workloads"}, {"title": "End-to-end evaluation of KUNSERVE", "content": "Overall performance. Figure 10 shows the overall performance on 3 datasets with different request rates. To analyze the performance under various request rate, we choose a request rate with average memory requirements during the trace period and continuously increase the request rate. Overall, compared to Llumnix (replication), KUNSERVE reduces P99 TTFT by up to 8.4-27.3 \u00d7 on different datasets We also achieve 2.9-52.2 \u00d7 and 23.1-47.6 \u00d7 better P99 TTFT compared to Llumnix (pipeline) and Llumnix (migration), respectively. These improvements reduce SLO violation from 32.9% to 0% on BurstGPT, 39.1% to 7.5% on AzureConv, and 27.8% to 0.6% on AGIEval-CoT at best cases, respectively.\nCompared to Llumnix (replication), the improvements mainly come from reduced queuing under memory throttling. For example, when running BurstGPT dataset, we observed a 38.9 \u00d7 TTFT increase when memory throttling happens as shown in Figure 11. Compared to Llumnix (pipeline), we are better in all metrics because pipeline introduces 1.7-1.8 \u00d7 higher TTFT latency and 1.6\u20132.0 \u00d7 TBT latency when the system is unloaded (12.6\u201321.8% lower in throughput). Interestingly, a pipeline configuration though with more HBM for KVCache can cause 35.9 \u00d7 P99 TTFT increase. The only exception is AGIEval-CoT dataset, where we observed Llumnix (pipeline) has 1.4 \u00d7 faster P50 TTFT compared to Llumnix (replication). This is because AGIEval-CoT has fewer input tokens and pipeline parallelism benefits from its multi-queue scheduling. However, KUNSERVE is still 1.2-2.0 \u00d7 better in P50 TBT and 1.1-1.8 \u00d7 better in P99 TBT in this case. For Llumnix (w/ migration), as we have extensively analyzed in \u00a72.3, it can exacerbate memory throttling issue and thus has the worst performance.\nThough KUNSERVE reduces the tail TTFT of all workloads significantly, it comes at a little cost of P50 and P99 TBT, with 7.9-37.5% and 33.4\u201377.4% increases on different datasets compared to the fastest baselines (Llumnix (replication)), respectively. These increases in latency arise because the requests that enter the system after parameters have been dropped suffer from pipeline overhead. The largest TBT increase is observed in AzureConv trace, where prefill phase is longer and thus the pipeline parallelism generates more execution bubbles. However, we believe such a trade-off is reasonable because no SLO is violated due to TBT increase even with a tight SLO ratio for TBT.\nMulti-GPU performance. To show the generality of our approach, we also evaluate KUNSERVE with large models that require multiple GPUs. We run 4 Llama-3.1-70B instances on 2 nodes with 8 GPUs each. The results are shown in Figure 12, we report the results of BurstGPT dataset for brevity. Other datasets show similar trends. KUNSERVE can reduce the tail TTFT by up to 48.7 \u00d7 while introducing no overhead to P50 TTFT. For P50 TBT and P99 TBT, KUNSERVE introduces 25.4-54.1% and 60.8\u201392.6% overhead compared to Llumnix (replication) but better than Llumnix (pipeline). As Llama-3.1-70B takes a larger amount (41.1%) of HBM, Llumnix (pipeline) achieves the lowest TTFT as it has no KVCache exchange overhead. But it collapses due to its 16.4% lower throughput and KUNSERVE has 29 \u00d7 faster P99 TTFT than it in same request rate. Our experiment on Llama-3.2-90B model has similar results, as presented in Figure 13. We achieve 13.4-51.4 \u00d7 lower P99 TTFT compared to Llumnix (replication) and reduce SLO violation from 36.7% to 0.1% in the best case and achieve exactly the same performance when no request violates SLO."}, {"title": "Ablation Study", "content": "Effectiveness of live KVCache exchange. Live KVCache exchange is the key to preserve KUNSERVE token generation when system is exchanging KVCache among buddy instances. Live KVCache exchange is most effective when the system has a relatively weak inter-server network. For example, it can reduce token generation stall time in a 8-GPU server with one 200Gbps NIC (i.e., 25Gbps per GPU). We compare the performance of live KVCache exchange in different network setups with blocking KVCache exchange. In this experiment, we limited bandwidth between instances by limiting the visible NICs. The results are shown in Figure 14. We report mean decode time of ongoing requests as the main metric. It is calculated as the time passed since the last decode iteration of the request. Like TBT, it represents decode performance. But it is different from TBT in that will not be amortized by other tokens of the request. We also present the token generation timeline under 50Gbps per GPU setup for a better understanding of the benefits. Blocking KVCache exchange has a generation stall up to 8.8s in the weakest setup, while live KVCache exchange has 51.7\u201379.8% lower mean decode time during KVCache exchange in all setups. TBT SLO violation means severe token generation performance degradation that affects user experience [11"}]}