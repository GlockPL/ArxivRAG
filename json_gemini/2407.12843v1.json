{"title": "NUTRIBENCH: A Dataset for Evaluating Large Language Models in Carbohydrate Estimation from Meal Descriptions", "authors": ["Andong Hua", "Mehak Preet Dhaliwal", "Ryan Burke", "Yao Qin"], "abstract": "Accurate nutrition estimation helps people make informed decisions about their dietary choices and is crucial for preventing serious health issues. We present NUTRIBENCH, the first publicly available natural language meal description based nutrition benchmark. NUTRIBENCH consists of 5,000 human-verified meal descriptions with macro-nutrient labels, including carbohydrates, proteins, fats, and calories. The data is divided into 15 subsets varying in complexity based on the number, servings, and popularity of the food items in the meals and the specificity of serving size descriptions. We conducted an extensive evaluation of seven popular and state-of-the-art Large Language Models (LLMs), including GPT-3.5, Llama-3, and a medical domain-specific model with standard, Chain-of-Thought and Retrieval-Augmented Generation strategies on NUTRIBENCH for carbohydrate estimation. We also conducted a human study involving expert and non-expert participants and found that LLMs can provide more accurate and faster predictions over a range of complex queries. We present a thorough analysis and comparison of different LLMs, highlighting the opportunities and challenges of using LLMs for nutrition estimation in real-life scenarios. Our benchmark is publicly available at: https://mehak126.github.io/nutribench.html", "sections": [{"title": "1 Introduction", "content": "Effective nutrition monitoring and dietary management are essential components of healthcare, closely linked to the prevention and control of chronic diseases, including obesity, heart disease, diabetes, and certain cancers [8, 27, 29]. For example, it is critical for patients with diabetes to estimate carbohydrates in meals to determine required insulin doses [13, 9]. Inaccurate self-estimation of meal carbohydrates can lead to high blood sugar (hyperglycemia) or low blood sugar (hypoglycemia) events, which can cause severe short and long-term health issues [12, 31].\nDespite technological advancements in dietary assessment approaches, self-reporting nutrition esti-mation suffers from limited accuracy and high user burden [29, 34, 37]. A major limitation is that most modern nutrition datasets typically include tabular data [2, 4, 23, 6, 5, 1] or meal images paired with nutrition information [13, 30, 24, 32, 36, 28, 26] but often lack natural language descriptions, restricting their usage and flexibility. For example, tabular database searches typically require an exact match for successful retrieval and require multiple searches for meals with multiple food items, making the process time-consuming and burdensome [18]. In addition, image processing-based nutrition estimation systems are restricted to real-time predictions, pose privacy concerns [18] and may encounter issues with food components being obscured in the image.\nIn contrast, describing meals using natural everyday language offers more flexibility, allowing users to explain various meal components and serving amounts in detail. We propose that Large Language Models (LLMs) are a valuable tool for nutrition estimation from natural language meal descriptions due to their advanced language understanding and reasoning capabilities, combined with vast internal knowledge and ability to refer to external sources to provide precise nutrition estimates. However, there are no available existing datasets to evaluate LLMs on this task.\nTo this end, we present NUTRIBENCH, a dataset consisting of 5,000 natural language meal descrip-tions with macronutrient and calorie annotations. To our knowledge, this is the first publicly available benchmark for evaluating the performance of LLMs on nutrition estimation from meal descriptions. We construct NUTRIBENCH with 15 subsets varying in complexity based on the number, servings, and popularity of food items in the meal. This allows us to evaluate LLMs across various challenging real-world scenarios.\nWith our human-verified NUTRIBENCH, we evaluate seven popular and state-of-the-art LLMs including GPT-3.5 [7], Llama-3 [3], and a domain-specific medical model, MedAlpaca [17], across four prompting paradigms includ-ing Chain-of-Thought (CoT) [39] and Retrieval Augmented Generation (RAG) [21] for carbo-hydrate estimation. \nOur contributions can be summarized as follows:\n\u2022 NUTRIBENCH: We present NUTRIBENCH - the first publicly available natural language meal description dataset labeled with macronutrient and calorie estimates. NUTRIBENCH consists of 5,000 meal descriptions with 15 subsets varying in complexity.\n\u2022 Benchmarking LLMs: We conducted 300 experiments across seven LLMs varying in size and expertise with different prompting strategies including CoT and RAG. This provides a comprehensive insight into the current capabilities of LLMs in nutrition estimation.\n\u2022 Insightful Analysis: We compare and analyze the relative performance across the different NUTRIBENCH subsets, LLMs, and prompting strategies. Additionally, we conduct a human study and find that LLMs can even outperform a professional nutritionist in carbohydrate estimation."}, {"title": "2 Related Work", "content": "Nutrition Estimation Most mainstream nutrition datasets such as FoodData Central (FDC) [2], MenuStat [6], FoodCom [23], and Nutritionix [4] feature tabular food nutrition information curated from diverse sources. However, tabular datasets based retrieval methods require users to use precise terminology to ensure successful and relevant retrieval. In addition, these methods do not support retrieving multiple items that may be present in a meal in a single search, making the process time-consuming and burdensome [18].\nAnother popular method for nutrition estimation involves predicting nutritional values from food images. Such approaches may involve identifying food items followed by retrieval from a tabular nutrition database [41] or directly estimating the meal's nutritional breakdown from the image [19]. Many existing datasets contain images paired with nutrition information to facilitate research in this direction. These datasets may include images obtained from the web [30], real world, [24, 32, 36], or synthetically generated images [28]. UMDFood90k [26] provide a multimodel dataset with product images, text-based ingredient statements, and nutrient amounts. However, image-based nutrition approaches are time-sensitive [18], requiring users to capture specific pictures of their meals at the time of consumption and may encounter issues with food components being obscured in the image.\nEnabling users to input meal descriptions in natural everyday language can help mitigate these issues. Initial exploration in this realm includes [20, 34], who utilize Convolutional Neural Networks to match food items from crowdsourced meal descriptions with an external tabular nutrition database. However, they do not make their data public, preventing the evaluation of current state-of-the-art language processing approaches, such as LLMs, for this task.\nMotivated by the lack of a standardized benchmark for nutrition estimation based on natural language meal descriptions, we introduce NUTRIBENCH, the first publically available nutrition benchmark, and evaluate state-of-the-art LLMs to gain insight into their current capabilities and limitations.\nLarge Language Models (LLMs) Large Language Models (LLMs) have made significant progress in recent years, with both closed-source models like GPT-3/4 [7], Gemini [35] and open-source models like Llama 2 [38], Llama 3 [3], and Alpaca [33] enabling advancements in natural language processing as well as knowledge-intensive, reasoning, and cross-domain tasks including healthcare applications [16]. Despite their extensive internal knowledge, LLMs still suffer from issues such as hallucinations, incorrect or outdated information, and a lack of interpretability [25, 22, 42].\nChain-of-Thought (CoT) [39] prompting alleviates some of these issues by enabling models to reason about the answer step-by-step. Another promising solution is Retrieval-Augmented Generation (RAG) [21], which provides the model with additional context relevant to the query by retrieving information from an external, reliable knowledge source. While previous works have identified shortcomings of both these approaches [10, 40], they have not been assessed on the task of nutrition estimation, which may uncover unique challenges.\nOur work comprehensively evaluates 7 state-of-the-art LLMs with standard, CoT, RAG, and RAG combined with CoT prompting for carbohydrate estimation from natural language meal descriptions. We present a detailed analysis of how different prompting strategies affect LLMs' performance in nutrition estimation in Section 5."}, {"title": "3 Dataset Construction", "content": "In this section, we describe our process of constructing our natural language meal description benchmark, NUTRIBENCH. Figure 3 provides an overview of the different stages in our pipeline.\n3.1 Data Curation\nThe FoodData Central (FDC) [2] is the food composition information center of the US Department of Agriculture (USDA) [14]. It is a semi-annually updated data repository and we obtained the most recent version of the data at the time of writing (April 2024) to compile food items and their nutrient labels in our dataset. We retained entries that provided complete macro-nutrient information, including carbohydrates, proteins, fats, and calories. Overall, we retained a total of 1,788,981 entries from the database containing all macro-nutrients. Among these, 465,059 items had unique food names, and 1,286,036 items were associated with a specific brand.\n3.2 Data Cleaning and Filtering\nWe conducted two rounds of outlier-based filtering to clean the data, with a primary focus on carbohydrates. In the first round, we eliminated variability among entries with the same food name and brand. Since we expect these entries to have near-similar nutrition estimates, we applied a strict z-score-based filtering approach, removing estimates with a score greater than 1, and selected the median of the remaining estimates as the final carbohydrate label. For same-brand items with only two entries, we kept their mean as the final label if their relative difference was less than 0.1. The second round filtered out extreme outliers for the same food name across different brands. In this step, we allowed for more variability and increased the z-score and relative difference thresholds to 2 and 0.3 respectively. Finally, we were left with 756,594 entries including 451,555 unique food names.\n3.3 Extracting Natural Serving Descriptions\nThe standard nutrition measurement in the FDC database is based on a normalized quantity of 100g, denoted as \"Metric Serving\". However, typical meal descriptions in natural language rarely include such precise measurements. For instance, it is more common to refer to \"1 cup of rice\" rather than \"100g of rice\" in day-to-day life. We extracted such natural servings from the FDC database, denoted as \"Natural Serving\" in this paper. In summary, our cleaned FDC database has 756,594 food items with varying brands and sources. Among them, 13,048 food items have additional natural serving descriptions with their corresponding weight in grams.\n3.4 Retrieval Database Construction\nFor our experiments with Retrieval-Augmented Generation, we construct a retrieval database, RETRI-DB, to provide relevant external nutrition knowledge. For each food item, we compile the nutrition information available across different brands and for different servings into a comprehensive list of facts. If multiple brands provide estimates for a specific serving of a food item, we use the median as the reference value. Since models process unstructured text information more efficiently than tabular data [11], we use a rule-based transformation to convert the context to natural language. Specifically, for an entry containing a serving amount and nutrition amount for a particular nutrient, we convert it to the string \"serving amount has nutrition amount nutrient\". \n3.5 NUTRIBENCH Construction\nTo construct NUTRIBENCH, we generate 5,000 natural language meal descriptions from the cleaned FDC nutrition dataset, divided into 15 subsets. These subsets vary in the number of food items, serving sizes, and natural vs. metric serving descriptions and include both directly and indirectly retrievable food items to enable a comprehensive assessment of LLMs' capabilities on this task. We also include both common and uncommon food items in NUTRIBENCH. The overall construction pipeline for NUTRIBENCH is depicted in Figure 3, with a summary of each subset sizes presented in Table 2. More detailed examples are available in the Appendix.\nIncreasing Number of Food Items To evaluate the models' capacity to handle multiple food items within a single meal description, we constructed three subsets containing single, double, and triple different food items in one meal description. For the double and triple food item subsets, we randomly sample two or three items from the single food item subset to create the combinations.\nIncreasing Number of Servings We also vary the number of serving sizes to evaluate whether LLMs exhibit mathematical and logical reasoning capabilities on this task. A 'single' natural serving represents '1' unit food item, e.g., 1 apple, whereas a 'single' metric serving represents '100g', e.g., 100g of apple. We convert these standardized single serving amounts by scaling the measures and the corresponding nutrition content by values sampled from a range of 0.25-48 for natural servings and 10-120 for metric servings. We refer to these as 'Multiple' servings. For multi-item meal descriptions, we first scale the serving size of each item and then combine them, e.g., \u201cI ate two apples and half a toast for breakfast.\"\nNatural vs. Metric Serving Descriptions In the real world, people may choose to describe food items using either natural language (e.g., \u201ca cup of latte\") or precise metric measurements (e.g., \"100g of latte\"). To reflect this diversity, we create distinct subsets containing food descriptions using natural or metric servings in a ratio of 7:3, favoring natural serving descriptions due to their higher frequency of day-to-day use. In Section 5.2, we explore how these distinct serving descriptions can significantly influence the performance of LLMs and retrieval-based methods.\nDirect vs. Indirect Retrieval Subsets To evaluate the performance of RAG based methods, we use RETRI-DB introduced in Section 3.4 to provide relevant external nutrition information to the models. We divide NUTRIBENCH into two equal parts: one containing 'Direct Retrieval' food items, where the food items can be directly retrieved from RETRI-DB with exact food name matches but different serving descriptions, and the other with 'Indirect Retrieval' food items, where no direct match exists between the queried food item and those in RETRI-DB. The Direct Retrieval subset is used to assess the model's ability to convert metric servings retrieved from the database (e.g., \u201c100g of rice\") to natural servings used in meal descriptions (e.g., \"1 cup of rice\"). In contrast, the Indirect Retrieval subset evaluates the model's capability to retrieve similar food items from the RETRI-DB and use them as nutrition context for knowledge-grounded carbohydrate estimation.\n3.5.1 Commonness-based Sampling\nTo ensure our NUTRIBENCH includes both common and uncommon foods, we propose commonness-based sampling for constructing Indirect Retrieval subsets 2. Specifically, we quantify the common-ness score using the embedding similarity of food names: the higher the similarity to other food items, the more common the food. We employ OpenAI's \u201ctext-embedding-3-large\" model to extract food name embeddings and compute a similarity matrix against all other food items in our database. The second-highest similarity score (the highest being 1 for the item itself) is used as the commonness score. We set a threshold of 0.75 to distinguish between uncommon and common foods and then randomly sample from these two groups equally for Indirect Retrieval subsets. Further details of our commonness-based sampling process are presented in Appendix C.\n3.5.2 Generating Natural Language based Meal Descriptions\nGPT-3.5 Based Generation We instruct GPT-3.5 to generate natural language meal descriptions (aka queries) from the sampled food items to create NUTRIBENCH. To encourage diversity, we prompt the LLM to produce five varied meal descriptions for each food item in a single generation, from which we randomly select one as the final query. When increasing the number of food items in a query, we instruct the LLM to combine two or three single-item queries into a combined meal description. Details of the prompts used can be found in the Appendix.\nTwo-Round Human Verification Although GPT-3.5 can generate meal descriptions, it may occa-sionally produce outputs with incorrect food names or missing serving sizes. To this end, we conduct two rounds of human verification. First, a human evaluator reviews each generated meal description to manually correct the accuracy of the food name and serving size. In the second round, another evaluator re-examines the entire dataset. This ensures NUTRIBENCH contains high-quality natural language meal descriptions. Examples are displayed in Figure 1 and the Appendix."}, {"title": "4 Experimental Setup", "content": "4.1 LLM Models\nIn this work, we conduct a comprehensive evaluation of seven state-of-the-art large language models (LLMs) using our proposed NUTRIBENCH. The evaluation spans models of varying sizes, ranging from small-scale models with 7 billion parameters to large-scale models with 175 billion parameters. Additionally, we compare models with integrated general medical knowledge to those without such specialized information. The evaluated LLMs are introduced as follows:\n\u2022 GPT-3.5 [7]: GPT-3.5-Turbo from OpenAI is a closed-source model, accessible via an API.\n\u2022 Llama2-7B and Llama2-70B [38]: Llama-2-7B-chat and Llama-2-70B are open-source instruction-tuned models from Meta.\n\u2022 Llama3-8B and Llama3-70B [3]: We also evaluate the advanced Llama models, Llama-3-8B-Instruct and Llama-3-70B. Notably, Llama-3-70B competes with GPT-3.5 in human evaluations.\n\u2022 Alpaca-7B [33]: Alpaca-7B, developed by Stanford, fine-tunes Llama-7B with 52K instruction-following examples\n\u2022 MedAlpaca-7B [17]: In comparison to Alpaca-7B, MedAlpaca-7B fine-tunes Llama with medical data including established medical NLP tasks as well as various internet resources.\n4.2 Prompt Methods\nIn this section, we introduce how we adapt four existing prompting methods with carefully designed prompts tailored for carbohydrate estimation. They are:\n\u2022 Base: The first baseline involves instructing LLMs to estimate the carbohydrate content based on the meal description provided in the query with basic instructions.\n\u2022 Chain-of-Thought (CoT) [39]: Since our data includes complex queries with multiple items in varying quantities for a meal description, we hypothesize that the step-by-step reasoning induced by chain-of-thought prompting would reduce model errors by enabling the model to identify and reason about individual query components required to make the overall decision.\n\u2022 Retrieval-Augmented Generation (RAG) [21]: To further enhance the reliability of LLM, we use RAG to ground their predictions with nutrition knowledge retrieved from RETRI-DB. First, for a given meal query, we prompt the model to parse it into individual food components. Next, we retrieve nutrition information about each food item in the query through a nearest neighbor semantic similarity search and concatenate the results to form a comprehensive set of facts about the food components in the query. Finally, we provide this retrieved context along with the original prompts for LLMs.\n\u2022 RAG+CoT: We combine the nutrition retrieval capability of RAG with step-by-step reasoning in CoT by concatenating the retrieved nutrition context with the CoT prompting for LLMs.\nIn all the cases mentioned above, we instruct the models to respond with '-1' if they don't know the answer to reduce the risk of potentially harmful predictions. \n4.3 Evaluation Metrics\nWe calculate the mean absolute error (MAE) to measure the deviation of the model responses from the true carbohydrates. In addition, we report accuracy (Acc@7.5) by considering the model output as 'correct' if the predicted value is within \u00b17.5g of the ground truth. This is based on the insulin-to-carbohydrate ratio, which indicates the grams of carbohydrates one unit of insulin can cover. While this ratio varies among individuals, it is generally considered 1:15 as a rule of thumb 3. Since we aim to improve insulin management and avoid even half-unit insulin dosage errors, we maintain a conservative threshold of 7.5g on the absolute error to measure accuracy.\nFinally, since we allow the models not to provide an estimate if uncertain, we also report the answer rate (AR), indicating the percentage of answered questions. Overall, the models should have a high AR and Acc@7.5, and a low AE."}, {"title": "5 Evaluating LLMs on NUTRIBENCH", "content": "In this section, we evaluated the performance of seven LLM models and four prompt methods on our natural language description based nutrition benchmark: NUTRIBENCH. We specifically focused on carbohydrate estimation due to its pivotal role in blood glucose management for diabetes. We anticipate that the insights derived from carbohydrate estimation will be applicable to other nutritional components, such as proteins, fats, and calories, included in NUTRIBENCH.\nWe begin by summarizing the general guidelines that apply across all 15 subsets in NUTRIBENCH. As shown in Figure 2, we find that:\n\u2022 Among seven LLMs, GPT-3.5 generally outperforms the others, with Llama3-70B ranking second.\n\u2022 There is a trade-off between accuracy and answer rate. For instance, GPT-3.5 with CoT achieves a 51.48% acc@7.5, outperforming all other methods, but tends to be more conservative in carbohydrate estimation, resulting in a lower answer rate.\n\u2022 Increasing the model size of LLMs generally improves both accuracy and answer rate, as observed when comparing Llama models with 7 or 8 billion parameters to those with 70 billion.\n\u2022 The medical LLM, such as Medalpaca, performs better than its non-medical counterpart, Alpaca, but does not surpass more advanced LLMs. We hypothesize that while medical data can help nutritional estimation, more advanced LLMs possess a stronger ability to comprehend natural language based meal descriptions.\nIn the following sections, we compare performance on specific NUTRIBENCH subsets to analyze how different LLMs and prompt methods affect carbohydrate estimation.\n5.1 CoT Improves both Answer Rate and Accuracy, Especially on Challenging Queries\nIn this section, we investigate how step-by-step reasoning induced by CoT helps LLMs in carbohydrate estimation. First, we discover that CoT consistently improves both answer rate and accuracy across 15 subsets and 7 LLMs, as demonstrated in Table 3. We further analyze the queries answered by CoT but not by Base, and observe a higher MAE compared to queries answered by both, shown in Table 4. This indicates that CoT enables LLMs to answer challenging queries, aligned with the observations in [39] that step-by-step reasoning can effectively help LLMs tackle difficult problems.\nSince the intermediate steps with chain-of-thought prompting add a layer of interpretability to the models' reasoning process, we manually reviewed failure examples with high MAE errors. Approximately 80% of model errors come from erroneous carbohydrate predictions, possibly due to incorrect prior knowledge or hallucinations by the model. The remaining errors arise from misidentifying either individual food items or the serving size in a query. Notably, none of the errors were due to mathematical calculation mistakes. This indicates that CoT is sufficient to handle the mathematical complexity of nutrition estimation.\n5.2 RAG only Helps with Aligned Serving Descriptions\nIn this section, we investigate how external nutrition knowledge retrieved by RAG aids LLMs in making predictions. We compare results for natural and metric servings in Figure 4. Surprisingly, despite direct retrieval providing nutrition information with exact food names for differ-ent serving descriptions, it still degrades performance. This suggests that LLMs struggle to convert one serving description to another, particularly from metric to natural serving, even when our designed prompt explicitly instructs the model to convert the servings. In contrast, RAG consistently improves both Acc@7.5 and AR for metric serving queries,\neven without directly retrieving the exact food names from RETRI-DB. Considering most food items in RETRI-DB only have metric serving descriptions, we conclude that RAG can provide LLMs with useful nutrition knowledge by retrieving similar, but not necessarily identical, food items, as long as the serving descriptions are aligned. Based on these findings, a promising future direction is to augment RETRI-DB with more natural servings for RAG to support knowledge-grounded predictions with LLMs.\n5.3 LLMs Excel in Multi-item Queries but Struggle with Multi-Serving Queries\nMulti-Item Analysis We compare the average model performance across the single, double, and triple food subsets with single servings in NUTRIBENCH. As shown in Table 5, the unnormalized MAE increases with the number of food items in a query as the task becomes more challenging. We also measure the normalized MAE corresponding to a single food item with a single serving. For each query in a multiple food subset, if all food items are answered in the corresponding single food subset, we average the MAE of each item from the single food subset. Additionally, we divide the unnormalized MAE by the number of food items in double or triple food subsets. Surprisingly, we observe an opposite trend - the models exhibit a lower normalized MAE over multi-item subsets compared to single-item subsets. We hypothesize pairing foods together provides additional context for predictions. This suggests that providing complete descriptions of meals, including all items in a single query, can be more accurate than prompting LLMs multiple times for each individual item.\nMulti-Serving Analysis We compare prediction errors across subsets containing single food items with either single or multiple serving sizes. In Table 6, we present both the unnormalized MAE corresponding to each meal description, and the normalized MAE, which divides the error of multiple serving queries by the serving multiplication factor. Although we observe a lower unnormalized MAE for multiple serving subsets, the normalized MAE is higher compared to single serving subsets. In our error analysis in the Appendix, we observed that the majority of errors for multiple serving subsets stem from inaccurate initial predictions rather than calculation mistakes. Based on this, we hypothesize that the higher normalized error observed for the multiple servings set is due to the uncommon serving sizes. Online nutrition databases, like FDC, typically provide nutrition estimates for standardized serving sizes, such as per 100g. Therefore, LLMs are likely trained on data featuring nutrition estimates for 100g servings. However, when tasked with carbohydrate estimation for variable serving sizes, LLMs may struggle to effectively use this prior knowledge.\n5.4 High-carbohydrate Foods Lead to Large Prediction Error\nWe examined the relationship between the prediction error and the true carbohydrate content.\nWe observe a positive correlation: the MAE increases as the ground truth carbohy-drate content rises. This indicates that LLM predictions are likely to be more accurate for individuals with a generally low-carbohydrate diet compared to those with a high-carbohydrate diet.\n5.5 LLMs Outperforms Nutritionist in Accuracy, Speed and Stress Reduction\nWe conduct a voluntary human study on carbohydrate estimation involving 10 non-expert laypersons and 1 nutritionist. Among the ten laypersons, we include 1 patient with Type-1 diabetes for over 10 years, individuals with a general understanding of nutrition (including calorie awareness but not carbohydrates), and others without any particular focus on nutrition knowledge. We randomly sample 6 meal descriptions from each subset to create a final test set of 90 queries. To make a fair comparison with LLMs, we explicitly instruct the participants: Do not search online or use nutrition apps for carbohydrates. Additionally, we provide all participants with three meal descriptions with corresponding carbohydrates, identical to the few-shot examples given in the LLM prompts.\nThe results show an interesting finding: the professional nutritionist could not outperform advanced LLMs in carbohydrate estimation, as illustrated in Figure 2. In addition, it takes the nutritionist in total of 50 minutes to complete all 90 queries 6 However, LLMs can answer all 90 queries within minutes, e.g., GPT-3.5 completed them in 2 minutes. Lastly, when the meal description becomes more complicated, participants experience significantly heightened stress in processing the information. In contrast, there is no difference when LLMs process longer meal descriptions. Taken together, we conclude that LLMs exhibit significant potential in addressing this challenging yet critical task."}, {"title": "6 Conclusion", "content": "In this study, we presented NUTRIBENCH, the first publicly available benchmark for evaluating the performance of LLMs on nutrition estimation from natural language meal descriptions. NUTRIBENCH contains 15 distinct subsets with in total 5,000 human-verified meal descriptions, representing various challenging scenarios likely to be encountered in the real world. We conducted 300 experiments to evaluate seven state-of-the-art LLMs on the NUTRIBENCH and discover that GPT-3.5 with CoT achieves the highest accuracy, significantly outperforming even human experts with professional nutritional knowledge. Our benchmark not only highlights the capabilities of existing LLMs but also provides a robust platform for future studies in this vital area. A limitation of our study is that the scale of the dataset may not be large enough, which we plan to address in future iterations. We hope the insightful findings in this work will inspire researchers to develop domain-specific LLMs for nutrition estimation, ultimately contributing to improved dietary choices and overall health outcomes."}, {"title": "A Data Distribution", "content": "A.1 Dataset Documentation and Intended uses.\nWe document the data using the Datasheets for Datasets framework [15] available at https://github.com/DongXzz/NutriBench/blob/main/NutriBench_Datasheet.pdf. The docu-mentation is also attached at the end of the Appendix.\nA.2 URLs to data and Croissant metadata.\nData URL: https://github.com/DongXzz/NutriBench/tree/main\nCroissant Metadata URL: https://github.com/DongXzz/NutriBench/blob/main/croissant.json\nA.3 Author statement.\nWe confirm that we bear all responsibility for any rights violations that may occur during dataset construction or other aspects of this work. We also confirm the dataset is under Creative Commons Attribution Non Commercial Share Alike 4.0 (CC BY-NC-SA 4.0) license."}, {"title": "B More Examples of Generated Meal Descriptions", "content": "In this section, we give more examples of meal descriptions in NUTRIBENCH.\nB.1 Natural Serving Direct Retrieval\n1. Enjoying a delicious cooked and broiled bison ribeye steak for lunch. (Single Food Item Single Serving)\n2. For dinner, I am having 2 servings of cooked, broiled bison ribeye steak. (Single Food Item Multiple Serving)\n3. Indulging in a pouch of caramel filled chocolate candy and using one tbsp dried rosemary to enhance the taste of my meal. (Double Food Item Single Serving)\n4. Treating myself to half regular size pouch of chocolate candy with caramel filling and 3 tbsp of dried rosemary. (Double Food Item Multiple Serving)\n5. Having a cup of great northern beans as my meal tonight, followed by a slice of dried mango for a tasty treat and a cookie filled with peanut butter and coated in chocolate. (Triple Food Item Single Serving)\nB.2 Natural Serving Indirect Retrieval\n1. Having a refreshing Kamikaze cocktail. (Single Food Item Single Serving)\n2. I am having 2 Kamikaze drinks. (Single Food Item Multiple Serving)\n3. For breakfast, I am having a cup of Okara and a cup of reduced fat (2%) milk.(Double Food Item Single Serving)\n4. Okara is the star of my dish, and I am using 3 cups of it along with 3 cups of reduced fat (2%) milk as a mid-morning snack. (Double Food Item Multiple Serving)\n5. Treating myself to one cubic inch decadent creme brulee, served with a sprig of raw epazote in my soup and a cup of spoonbread for breakfast. (Triple Food Item Single Serving)\nB.3 Metric Serving Indirect Retrieval\n1. Today's dinner includes 100g of ground pork. (Single Food Item Single Serving)\n2. Savoring a 120g serving of ground pork. (Single Food Item Multiple Serving)\n3. Indulging in 100g Kerala mixture for a flavorful treat with 100g of CASEY'S GENERAL STORE gummi candy. (Double Food Item Single Serving)"}, {"title": "CDetails on Commonness-Based Sampling", "content": "As shown in Figure 6, we first use Ope-nAI's 'text-embedding-3-large' model to extract embeddings for all the food names. Next, we calculate a similar-ity matrix and determine the second largest value in each row as the com-monness score. A higher commonness score indicates a more similar food in the database. Finally, we perform ran-dom sampling: foods with a common-ness score greater than 0.75 form the common subset, while those with a score less than 0.75 form the uncom-mon subset.\nCompared to random sampling, commonness-based sampling ensures the inclusion of uncommon foods in NUTRIBENCH. As shown in Figure 7, almost 95% of the foods have a commonness score greater than 0.9. If we apply random sampling naively, most of the selected foods would be common, resulting in a lack of diversity in NUTRIBENCH."}, {"title": "D Details on Meal Description Generation", "content": "In this section, we provide details about the generation of meal descriptions, including the prompts used for creating these descriptions and human verification examples.\nD.1 Prompts for Meal Description Generation\nIn Figure 8, we show the prompts used for generating meal descriptions for the single food item subset. After creating the single food item subset, we randomly combine two or three meal descriptions to form the double and triple food item subsets. The prompts for these combinations are shown in Figure 9.\nD.2 Human Verification\nAfter generating five meal descriptions with GPT-3.5 for each food item, we randomly choose one meal description as the final meal description. However, these descriptions still require refinement before practical application. Here are some examples of the raw generated descriptions and the descriptions after human verification:\nNot Follow Instruction:\nBefore human verification: ULTRA PERFORMANCE FOOD RELEASE meal description 1: 100g ULTRA PERFORMANCE FOOD RELEASE.\nAfter human verification: For a quick snack, I have 100g ULTRA PERFORMANCE FOOD RELEASE.\nMissing Serving Size:\nBefore human verification: Using dried rosemary to enhance the taste of my meal.\nAfter human verification: Using one tbsp dried rosemary to enhance the taste of my meal.\nBefore human verification: Indulging in a slice of dried mango for a tasty treat and having a delicious meal of braised select brisket.\nAfter human verification: Indulging in a slice of dried mango for a tasty treat and having a delicious meal of 1 oz braised select brisket.\nBefore human verification: Enjoying some SHAMI KABAB ground chicken patties for dinner.\nAfter human verification: Enjoying 100g SHAMI KABAB ground chicken patties for dinner."}, {"title": "E Prompts for Carbohydrate Estimation", "content": "For Base and CoT methods, we query the LLM model with the meal description and some instructions as shown in Figure 10. For Llama-2 and Llama-3, we follow the special format but keep the main content the same, and Figure 11 shows the prompt for Llama-3 with Base and CoT. For RAG, we will first parse the food description into food components. The parsing prompt is shown in Figure 12. Next, we will retrieve the nutrition information about each food item and finally provide LLMs with the retrieved context as well as the meal description. The RAG prompt GPT/Alpaca-7B/Medalpaca-7B is shown in Figure 13."}, {"title": "F Commonness Score and Prediction Error", "content": "In this section, we compare the commonness score and the prediction error. We use the predictions from the (Natural Serving, Indirect Retrieval) subset with single food item and single serving, averaged across GPT-3.5, LLama3-8B, and LLama3-70B models using both Base and CoT methods. From Figure 14, we average the MAE within each bin and observe no strong correlation between the commonness score and the prediction errors."}, {"title": "G Comparing Results Across All Experiments", "content": "Figure 15 presents a comparison of the"}]}