{"title": "NUTRIBENCH: A Dataset for Evaluating Large Language Models in Carbohydrate Estimation from Meal Descriptions", "authors": ["Andong Hua", "Mehak Preet Dhaliwal", "Ryan Burke", "Yao Qin"], "abstract": "Accurate nutrition estimation helps people make informed decisions about their dietary choices and is crucial for preventing serious health issues. We present NUTRIBENCH, the first publicly available natural language meal description based nutrition benchmark. NUTRIBENCH consists of 5,000 human-verified meal descriptions with macro-nutrient labels, including carbohydrates, proteins, fats, and calories. The data is divided into 15 subsets varying in complexity based on the number, servings, and popularity of the food items in the meals and the specificity of serving size descriptions. We conducted an extensive evaluation of seven popular and state-of-the-art Large Language Models (LLMs), including GPT-3.5, Llama-3, and a medical domain-specific model with standard, Chain-of-Thought and Retrieval-Augmented Generation strategies on NUTRIBENCH for carbohydrate estimation. We also conducted a human study involving expert and non-expert participants and found that LLMs can provide more accurate and faster predictions over a range of complex queries. We present a thorough analysis and comparison of different LLMs, highlighting the opportunities and challenges of using LLMs for nutrition estimation in real-life scenarios. Our benchmark is publicly available at: https://mehak126.github.io/nutribench.html", "sections": [{"title": "1 Introduction", "content": "Effective nutrition monitoring and dietary management are essential components of healthcare, closely linked to the prevention and control of chronic diseases, including obesity, heart disease, diabetes, and certain cancers [8, 27, 29]. For example, it is critical for patients with diabetes to estimate carbohydrates in meals to determine required insulin doses [13, 9]. Inaccurate self-estimation of meal carbohydrates can lead to high blood sugar (hyperglycemia) or low blood sugar (hypoglycemia) events, which can cause severe short and long-term health issues [12, 31].\nDespite technological advancements in dietary assessment approaches, self-reporting nutrition esti-mation suffers from limited accuracy and high user burden [29, 34, 37]. A major limitation is that most modern nutrition datasets typically include tabular data [2, 4, 23, 6, 5, 1] or meal images paired with nutrition information [13, 30, 24, 32, 36, 28, 26] but often lack natural language descriptions, restricting their usage and flexibility. For example, tabular database searches typically require an exact match for successful retrieval and require multiple searches for meals with multiple food items, making the process time-consuming and burdensome [18]. In addition, image processing-based nutrition estimation systems are restricted to real-time predictions, pose privacy concerns [18] and may encounter issues with food components being obscured in the image.\nIn contrast, describing meals using natural everyday language offers more flexibility, allowing users to explain various meal components and serving amounts in detail. We propose that Large Language Models (LLMs) are a valuable tool for nutrition estimation from natural language meal descriptions due to their advanced language understanding and reasoning capabilities, combined with vast internal knowledge and ability to refer to external sources to provide precise nutrition estimates. However, there are no available existing datasets to evaluate LLMs on this task.\nTo this end, we present NUTRIBENCH, a dataset consisting of 5,000 natural language meal descrip-tions with macronutrient and calorie annotations. To our knowledge, this is the first publicly available benchmark for evaluating the performance of LLMs on nutrition estimation from meal descriptions. We construct NUTRIBENCH with 15 subsets varying in complexity based on the number, servings, and popularity of food items in the meal. This allows us to evaluate LLMs across various challenging real-world scenarios. Figure 3 shows the pipeline of the construction process of NUTRIBENCH.\nWith our human-verified NUTRIBENCH, we evaluate seven popular and state-of-the-art LLMs including GPT-3.5 [7], Llama-3 [3], and a domain-specific medical model, MedAlpaca [17], across four prompting paradigms including Chain-of-Thought (CoT) [39] and Retrieval Augmented Generation (RAG) [21] for carbohydrate estimation. An example of GPT-3.5's output using different prompting strategies is shown in Figure 1. In addition, we summarize LLMs' performance in Figure 2, where GPT-3.5 with CoT prompting achieves the highest accuracy of 51.48%, with an answer rate of 89.80%. Surprisingly, most LLMs outperformed even a professional nutritionist and non-experts based\non our human study while also providing much faster estimates and accommodating a wide range of complex queries. This demonstrates the great potential of LLMs for nutrition estimation from meal descriptions, providing humans with dietary guidance and improving health outcomes.\nOur contributions can be summarized as follows:\n\u2022 NUTRIBENCH: We present NUTRIBENCH - the first publicly available natural language meal description dataset labeled with macronutrient and calorie estimates. NUTRIBENCH consists of 5,000 meal descriptions with 15 subsets varying in complexity.\n\u2022 Benchmarking LLMs: We conducted 300 experiments across seven LLMs varying in size and expertise with different prompting strategies including CoT and RAG. This provides a comprehensive insight into the current capabilities of LLMs in nutrition estimation.\n\u2022 Insightful Analysis: We compare and analyze the relative performance across the different NUTRIBENCH subsets, LLMs, and prompting strategies. Additionally, we conduct a human study and find that LLMs can even outperform a professional nutritionist in carbohydrate estimation."}, {"title": "2 Related Work", "content": "Nutrition Estimation Most mainstream nutrition datasets such as FoodData Central (FDC) [2], MenuStat [6], FoodCom [23], and Nutritionix [4] feature tabular food nutrition information curated from diverse sources. However, tabular datasets based retrieval methods require users to use precise"}, {"title": "3 Dataset Construction", "content": "In this section, we describe our process of constructing our natural language meal description benchmark, NUTRIBENCH. Figure 3 provides an overview of the different stages in our pipeline."}, {"title": "3.1 Data Curation", "content": "The FoodData Central (FDC) [2] is the food composition information center of the US Department of Agriculture (USDA) [14]. It is a semi-annually updated data repository and we obtained the most recent version of the data at the time of writing (April 2024) to compile food items and their nutrient labels in our dataset. We retained entries that provided complete macro-nutrient information, including carbohydrates, proteins, fats, and calories. Overall, we retained a total of 1,788,981 entries from the database containing all macro-nutrients. Among these, 465,059 items had unique food names, and 1,286,036 items were associated with a specific brand."}, {"title": "3.2 Data Cleaning and Filtering", "content": "We conducted two rounds of outlier-based filtering to clean the data, with a primary focus on carbohydrates. In the first round, we eliminated variability among entries with the same food name and brand. Since we expect these entries to have near-similar nutrition estimates, we applied a strict z-score-based filtering approach, removing estimates with a score greater than 1, and selected the median of the remaining estimates as the final carbohydrate label. For same-brand items with only two entries, we kept their mean as the final label if their relative difference was less than 0.1. The second round filtered out extreme outliers for the same food name across different brands. In this step, we allowed for more variability and increased the z-score and relative difference thresholds to 2 and 0.3 respectively. Finally, we were left with 756,594 entries including 451,555 unique food names."}, {"title": "3.3 Extracting Natural Serving Descriptions", "content": "The standard nutrition measurement in the FDC database is based on a normalized quantity of 100g, denoted as \"Metric Serving\". However, typical meal descriptions in natural language rarely include such precise measurements. For instance, it is more common to refer to \"1 cup of rice\" rather than \"100g of rice\" in day-to-day life. We extracted such natural servings from the FDC database, denoted as \"Natural Serving\" in this paper. In summary, our cleaned FDC database has 756,594 food items with varying brands and sources. Among them, 13,048 food items have additional natural serving descriptions with their corresponding weight in grams."}, {"title": "3.4 Retrieval Database Construction", "content": "For our experiments with Retrieval-Augmented Generation, we construct a retrieval database, RETRI-DB, to provide relevant external nutrition knowledge. For each food item, we compile the nutrition information available across different brands and for different servings into a comprehensive list of facts. If multiple brands provide estimates for a specific serving of a food item, we use the median as the reference value. Since models process unstructured text information more efficiently than tabular data [11], we use a rule-based transformation to convert the context to natural language. Specifically, for an entry containing a serving amount and nutrition amount for a particular nutrient, we convert it to the string \"serving amount has nutrition amount nutrient\"."}, {"title": "3.5 NUTRIBENCH Construction", "content": "To construct NUTRIBENCH, we generate 5,000 natural language meal descriptions from the cleaned FDC nutrition dataset, divided into 15 subsets. These subsets vary in the number of food items, serving sizes, and natural vs. metric serving descriptions and include both directly and indirectly retrievable food items to enable a comprehensive assessment of LLMs' capabilities on this task. We also include both common and uncommon food items in NUTRIBENCH. The overall construction pipeline for NUTRIBENCH is depicted in Figure 3, with a summary of each subset sizes presented in Table 2. More detailed examples are available in the Appendix.\nIncreasing Number of Food Items To evaluate the models' capacity to handle multiple food items within a single meal description, we constructed three subsets containing single, double, and triple different food items in one meal description. For the double and triple food item subsets, we randomly sample two or three items from the single food item subset to create the combinations.\nIncreasing Number of Servings We also vary the number of serving sizes to evaluate whether LLMs exhibit mathematical and logical reasoning capabilities on this task. A 'single' natural serving represents '1' unit food item, e.g., 1 apple, whereas a 'single' metric serving represents '100g', e.g., 100g of apple. We convert these standardized single serving amounts by scaling the measures and the corresponding nutrition content by values sampled from a range of 0.25-48 for natural servings and 10-120 for metric servings. We refer to these as 'Multiple' servings. For multi-item meal descriptions, we first scale the serving size of each item and then combine them, e.g., \u201cI ate two apples and half a toast for breakfast.\"\nNatural vs. Metric Serving Descriptions In the real world, people may choose to describe food items using either natural language (e.g., \u201ca cup of latte\") or precise metric measurements (e.g., \"100g of latte\"). To reflect this diversity, we create distinct subsets containing food descriptions using\""}, {"title": "3.5.1 Commonness-based Sampling", "content": "To ensure our NUTRIBENCH includes both common and uncommon foods, we propose commonness-based sampling for constructing Indirect Retrieval subsets 2. Specifically, we quantify the commonness score using the embedding similarity of food names: the higher the similarity to other food items, the more common the food. We employ OpenAI's \u201ctext-embedding-3-large\u201d model to extract food name embeddings and compute a similarity matrix against all other food items in our database. The second-highest similarity score (the highest being 1 for the item itself) is used as the commonness score. We set a threshold of 0.75 to distinguish between uncommon and common foods and then randomly sample from these two groups equally for Indirect Retrieval subsets. Further details of our commonness-based sampling process are presented in Appendix C."}, {"title": "3.5.2 Generating Natural Language based Meal Descriptions", "content": "GPT-3.5 Based Generation We instruct GPT-3.5 to generate natural language meal descriptions (aka queries) from the sampled food items to create NUTRIBENCH. To encourage diversity, we prompt the LLM to produce five varied meal descriptions for each food item in a single generation, from which we randomly select one as the final query. When increasing the number of food items in a query, we instruct the LLM to combine two or three single-item queries into a combined meal description. Details of the prompts used can be found in the Appendix.\nTwo-Round Human Verification Although GPT-3.5 can generate meal descriptions, it may occa-sionally produce outputs with incorrect food names or missing serving sizes. To this end, we conduct two rounds of human verification. First, a human evaluator reviews each generated meal description to manually correct the accuracy of the food name and serving size. In the second round, another evaluator re-examines the entire dataset. This ensures NUTRIBENCH contains high-quality natural language meal descriptions. Examples are displayed in Figure 1 and the Appendix."}, {"title": "4 Experimental Setup", "content": "In this work, we conduct a comprehensive evaluation of seven state-of-the-art large language models (LLMs) using our proposed NUTRIBENCH. The evaluation spans models of varying sizes, ranging from small-scale models with 7 billion parameters to large-scale models with 175 billion parameters. Additionally, we compare models with integrated general medical knowledge to those without such specialized information. The evaluated LLMs are introduced as follows:"}, {"title": "4.1 LLM Models", "content": "\u2022 GPT-3.5 [7]: GPT-3.5-Turbo from OpenAI is a closed-source model, accessible via an API.\n\u2022 Llama2-7B and Llama2-70B [38]: Llama-2-7B-chat and Llama-2-70B are open-source instruction-tuned models from Meta.\n\u2022 Llama3-8B and Llama3-70B [3]: We also evaluate the advanced Llama models, Llama-3-8B-Instruct and Llama-3-70B. Notably, Llama-3-70B competes with GPT-3.5 in human evaluations.\n\u2022 Alpaca-7B [33]: Alpaca-7B, developed by Stanford, fine-tunes Llama-7B with 52K instruction-following examples\n\u2022 MedAlpaca-7B [17]: In comparison to Alpaca-7B, MedAlpaca-7B fine-tunes Llama with medical data including established medical NLP tasks as well as various internet resources."}, {"title": "4.2 Prompt Methods", "content": "In this section, we introduce how we adapt four existing prompting methods with carefully designed prompts tailored for carbohydrate estimation. They are:\n\u2022 Base: The first baseline involves instructing LLMs to estimate the carbohydrate content based on the meal description provided in the query with basic instructions.\n\u2022 Chain-of-Thought (CoT) [39]: Since our data includes complex queries with multiple items in varying quantities for a meal description, we hypothesize that the step-by-step reasoning induced by chain-of-thought prompting would reduce model errors by enabling the model to identify and reason about individual query components required to make the overall decision.\n\u2022 Retrieval-Augmented Generation (RAG) [21]: To further enhance the reliability of LLM, we use RAG to ground their predictions with nutrition knowledge retrieved from RETRI-DB. First, for a given meal query, we prompt the model to parse it into individual food components. Next, we retrieve nutrition information about each food item in the query through a nearest neighbor semantic similarity search and concatenate the results to form a comprehensive set of facts about the food components in the query. Finally, we provide this retrieved context along with the original prompts for LLMs.\n\u2022 RAG+CoT: We combine the nutrition retrieval capability of RAG with step-by-step reasoning in CoT by concatenating the retrieved nutrition context with the CoT prompting for LLMs.\nIn all the cases mentioned above, we instruct the models to respond with '-1' if they don't know the answer to reduce the risk of potentially harmful predictions. Figure 1 shows different outputs of GPT-3.5 using the four different paradigms. We apply RAG and RAG+CoT on GPT-3.5, Llama3-8B, and Llama3-70B only due to computation constrain. The prompts for each method are in the Appendix."}, {"title": "4.3 Evaluation Metrics", "content": "We calculate the mean absolute error (MAE) to measure the deviation of the model responses from the true carbohydrates. In addition, we report accuracy (Acc@7.5) by considering the model output as 'correct' if the predicted value is within \u00b17.5g of the ground truth. This is based on the insulin-to-carbohydrate ratio, which indicates the grams of carbohydrates one unit of insulin can cover. While this ratio varies among individuals, it is generally considered 1:15 as a rule of thumb 3. Since we aim to improve insulin management and avoid even half-unit insulin dosage errors, we maintain a conservative threshold of 7.5g on the absolute error to measure accuracy.\nFinally, since we allow the models not to provide an estimate if uncertain, we also report the answer rate (AR), indicating the percentage of answered questions. Overall, the models should have a high AR and Acc@7.5, and a low AE."}, {"title": "5 Evaluating LLMs on NUTRIBENCH", "content": "In this section, we evaluated the performance of seven LLM models and four prompt methods on our natural language description based nutrition benchmark: NUTRIBENCH. We specifically focused on carbohydrate estimation due to its pivotal role in blood glucose management for diabetes. We anticipate that the insights derived from carbohydrate estimation will be applicable to other nutritional components, such as proteins, fats, and calories, included in NUTRIBENCH.\nWe begin by summarizing the general guidelines that apply across all 15 subsets in NUTRIBENCH. As shown in Figure 2, we find that:\n\u2022 Among seven LLMs, GPT-3.5 generally outperforms the others, with Llama3-70B ranking second.\n\u2022 There is a trade-off between accuracy and answer rate. For instance, GPT-3.5 with CoT achieves a 51.48% acc@7.5, outperforming all other methods, but tends to be more conservative in carbohy-drate estimation, resulting in a lower answer rate.\n\u2022 Increasing the model size of LLMs generally improves both accuracy and answer rate, as observed when comparing Llama models with 7 or 8 billion parameters to those with 70 billion.\n\u2022 The medical LLM, such as Medalpaca, performs better than its non-medical counterpart, Alpaca, but does not surpass more advanced LLMs. We hypothesize that while medical data can help nutritional estimation, more advanced LLMs possess a stronger ability to comprehend natural language based meal descriptions."}, {"title": "5.1 CoT Improves both Answer Rate and Accuracy, Especially on Challenging Queries", "content": "In this section, we investigate how step-by-step reasoning induced by CoT helps LLMs in carbohydrate estimation. First, we discover that CoT consistently improves both answer rate and accuracy across 15 subsets and 7 LLMs, as demonstrated in Table 3. We further analyze the queries answered by CoT but not by Base, and observe a higher MAE compared to queries answered by both, shown in Table 4. This indicates that CoT enables LLMs to answer challenging queries, aligned with the observations in [39] that step-by-step reasoning can effectively help LLMs tackle difficult problems.\nError Analysis Since the intermediate steps with chain-of-thought prompting add a layer of interpretability to the models' reasoning process, we manually reviewed failure examples with high MAE errors. Approximately 80% of model errors come from erroneous carbohydrate predictions, possibly due to incorrect prior knowledge or hallucinations by the model. The remaining errors arise from misidentifying either individual food items or the serving size in a query. Notably, none of the errors were due to mathematical calculation mistakes. This indicates that CoT is sufficient to handle the mathematical complexity of nutrition estimation. Detailed error analysis is in the Appendix."}, {"title": "5.2 RAG only Helps with Aligned Serving Descriptions", "content": "In this section, we investigate how external nutrition knowledge retrieved by RAG aids LLMs in making predictions. We compare results for natural and metric servings in Figure 4. Surprisingly, despite direct retrieval providing nutrition information with exact food names for differ-ent serving descriptions, it still degrades performance. This suggests that LLMs struggle to convert one serving description to another, particularly from met-ric to natural serving, even when our designed prompt explicitly instructs the model to convert the servings. In contrast, RAG consistently improves both Acc@7.5 and AR for metric serving queries,"}, {"title": "5.3 LLMs Excel in Multi-item Queries but Struggle with Multi-Serving Queries", "content": "Multi-Item Analysis We compare the average model performance across the single, double, and triple food subsets with single servings in NUTRIBENCH. As shown in Table 5, the unnormalized MAE increases with the number of food items in a query as the task becomes more challenging. We also measure the normalized MAE corresponding to a single food item with a single serving. For each query in a multiple food subset, if all food items are answered in the corresponding single food subset, we average the MAE of each item from the single food subset. Additionally, we divide the unnormalized MAE by the number of food items in double or triple food subsets. Surprisingly, we observe an opposite trend - the models exhibit a lower normalized MAE over multi-item subsets compared to single-item subsets. We hypothesize pairing foods together provides additional context for predictions. This suggests that providing complete descriptions of meals, including all items in a single query, can be more accurate than prompting LLMs multiple times for each individual item.\nMulti-Serving Analysis We compare prediction errors across subsets containing single food items with either single or multiple serving sizes. In Table 6, we present both the unnormalized MAE corresponding to each meal description, and the normalized MAE, which divides the error of multiple serving queries by the serving multiplication factor. Although we observe a lower unnormalized MAE for multiple serving subsets, the normalized MAE is higher compared to single serving subsets. In our error analysis in the Appendix, we observed that the majority of errors for multiple serving subsets stem from inaccurate initial predictions rather than calculation mistakes. Based on this, we hypothesize that the higher normalized error observed for the multiple servings set is due to the uncommon serving sizes. Online nutrition databases, like FDC, typically provide nutrition estimates for standardized serving sizes, such as per 100g. Therefore, LLMs are likely trained on data featuring nutrition estimates for 100g servings. However, when tasked with carbohydrate estimation for variable serving sizes, LLMs may struggle to effectively use this prior knowledge."}, {"title": "5.4 High-carbohydrate Foods Lead to Large Prediction Error", "content": "We examined the relationship between the prediction error and the true carbohydrate content. Figure 5.3 shows the histogram of the true carbohydrate content for single-item, single-serving queries. For each bin, we measure and plot the average MAE (in red). We observe a positive correlation: the MAE increases as the ground truth carbohy-drate content rises. This indicates that LLM predictions are likely to be more accurate for individuals with a generally low-carbohydrate diet compared to those with a high-carbohydrate diet."}, {"title": "5.5 LLMs Outperforms Nutritionist in Accuracy, Speed and Stress Reduction", "content": "We conduct a voluntary human study on carbohydrate estimation involving 10 non-expert laypersons and 1 nutritionist. Among the ten laypersons, we include 1 patient with Type-1 diabetes for over 10 years, individuals with a general understanding of nutrition (including calorie awareness but not carbohydrates), and others without any particular focus on nutrition knowledge. We randomly sample 6 meal descriptions from each subset to create a final test set of 90 queries. To make a fair"}, {"title": "6 Conclusion", "content": "In this study, we presented NUTRIBENCH, the first publicly available benchmark for evaluating the performance of LLMs on nutrition estimation from natural language meal descriptions. NUTRIBENCH contains 15 distinct subsets with in total 5,000 human-verified meal descriptions, representing various challenging scenarios likely to be encountered in the real world. We conducted 300 experiments to evaluate seven state-of-the-art LLMs on the NUTRIBENCH and discover that GPT-3.5 with CoT achieves the highest accuracy, significantly outperforming even human experts with professional nutritional knowledge. Our benchmark not only highlights the capabilities of existing LLMs but also provides a robust platform for future studies in this vital area. A limitation of our study is that the scale of the dataset may not be large enough, which we plan to address in future iterations. We hope the insightful findings in this work will inspire researchers to develop domain-specific LLMs for nutrition estimation, ultimately contributing to improved dietary choices and overall health outcomes."}, {"title": "7 Acknowledgements", "content": "We extend our sincere gratitude to Xuan Yang for the valuable initial discussions, data survey, and construction efforts for this project. Additionally, we thank Yifan Wei for human verification to ensure the quality of the benchmark. We are also grateful to Xuezhi Wang for the insightful discussion and interpretation of our results. Finally, we thank Andrew Koutnik for providing perspectives and nutrition estimates for the human study as a professional nutritionist, as well as our non-expert human study participants. Their collective contributions were essential to the success of our work."}, {"title": "A Data Distribution", "content": "A.1 Dataset Documentation and Intended uses.\nWe document the data using the Datasheets for Datasets framework [15] available at https:\n//github.com/DongXzz/NutriBench/blob/main/NutriBench_Datasheet.pdf. The docu-mentation is also attached at the end of the Appendix.\nA.2 URLs to data and Croissant metadata.\nData URL: https://github.com/DongXzz/NutriBench/tree/main\nCroissant Metadata URL: https://github.com/DongXzz/NutriBench/blob/main/croissant.json\nA.3 Author statement.\nWe confirm that we bear all responsibility for any rights violations that may occur during dataset construction or other aspects of this work. We also confirm the dataset is under Creative Commons Attribution Non Commercial Share Alike 4.0 (CC BY-NC-SA 4.0) license."}, {"title": "B More Examples of Generated Meal Descriptions", "content": "In this section, we give more examples of meal descriptions in NUTRIBENCH.\nB.1 Natural Serving Direct Retrieval\n1. Enjoying a delicious cooked and broiled bison ribeye steak for lunch. (Single Food Item Single Serving)\n2. For dinner, I am having 2 servings of cooked, broiled bison ribeye steak. (Single Food Item Multiple Serving)\n3. Indulging in a pouch of caramel filled chocolate candy and using one tbsp dried rosemary to enhance the taste of my meal. (Double Food Item Single Serving)\n4. Treating myself to half regular size pouch of chocolate candy with caramel filling and 3 tbsp of dried rosemary. (Double Food Item Multiple Serving)\n5. Having a cup of great northern beans as my meal tonight, followed by a slice of dried mango for a tasty treat and a cookie filled with peanut butter and coated in chocolate. (Triple Food Item Single Serving)\nB.2 Natural Serving Indirect Retrieval\n1. Having a refreshing Kamikaze cocktail. (Single Food Item Single Serving)\n2. I am having 2 Kamikaze drinks. (Single Food Item Multiple Serving)\n3. For breakfast, I am having a cup of Okara and a cup of reduced fat (2%) milk.(Double Food Item Single Serving)\n4. Okara is the star of my dish, and I am using 3 cups of it along with 3 cups of reduced fat (2%) milk as a mid-morning snack. (Double Food Item Multiple Serving)\n5. Treating myself to one cubic inch decadent creme brulee, served with a sprig of raw epazote in my soup and a cup of spoonbread for breakfast. (Triple Food Item Single Serving)\nB.3 Metric Serving Indirect Retrieval\n1. Today's dinner includes 100g of ground pork. (Single Food Item Single Serving)\n2. Savoring a 120g serving of ground pork. (Single Food Item Multiple Serving)\n3. Indulging in 100g Kerala mixture for a flavorful treat with 100g of CASEY'S GENERAL STORE gummi candy. (Double Food Item Single Serving)"}, {"title": "CDetails on Commonness-Based Sampling", "content": "As shown in Figure 6, we first use Ope-nAI's 'text-embedding-3-large' model\nto extract embeddings for all the food\nnames. Next, we calculate a similar-ity matrix and determine the second\nlargest value in each row as the com-\nmonness score. A higher commonness\nscore indicates a more similar food in\nthe database. Finally, we perform ran-\ndom sampling: foods with a common-\nness score greater than 0.75 form the\ncommon subset, while those with a\nscore less than 0.75 form the uncom-mon subset.\nCompared to random sampling,commonness-based sampling ensures\nthe inclusion of uncommon foods in\nNUTRIBENCH. As shown in Figure 7, almost 95% of the foods have a commonness score greater\nthan 0.9. If we apply random sampling naively, most of the selected foods would be common,\nresulting in a lack of diversity in NUTRIBENCH."}, {"title": "D Details on Meal Description Generation", "content": "In this section, we provide details about the generation of meal descriptions, including the prompts\nused for creating these descriptions and human verification examples.\nD.1 Prompts for Meal Description Generation\nIn Figure 8, we show the prompts used for generating meal descriptions for the single food item subset.\nAfter creating the single food item subset, we randomly combine two or three meal descriptions\nto form the double and triple food item subsets. The prompts for these combinations are shown in\nFigure 9.\nD.2 Human Verification\nAfter generating five meal descriptions with GPT-3.5 for each food item, we randomly choose one\nmeal description as the final meal description. However, these descriptions still require refinement"}, {"title": "E Prompts for Carbohydrate Estimation", "content": "For Base and CoT methods, we query the LLM model with the meal description and some instructions\nas shown in Figure 10. For Llama-2 and Llama-3, we follow the special format but keep the main\ncontent the same, and Figure 11 shows the prompt for Llama-3 with Base and CoT. For RAG, we will\nfirst parse the food description into food components. The parsing prompt is shown in Figure 12. Next,\nwe will retrieve the nutrition information about each food item and finally provide LLMs with the\nretrieved context as well as the meal description. The RAG prompt GPT/Alpaca-7B/Medalpaca-7B is\nshown in Figure 13."}, {"title": "F Commonness Score and Prediction Error", "content": "In this section, we compare the commonness score and the prediction error. We use the predictions\nfrom the (Natural Serving, Indirect Retrieval) subset with single food item and single serving,\naveraged across GPT-3.5, LLama3-8B, and LLama3-70B models using both Base and CoT methods.\nFrom Figure 14, we average the MAE within each bin and observe no strong correlation between the\ncommonness score and the prediction errors."}, {"title": "G Comparing Results Across All Experiments", "content": "Figure 15 presents a comparison of the MAE obtained for all models across all 15 data splits of\nNUTRIBENCH. Our observations here reinforce the findings from Section 5.\nFigure 15 shows that overall, GPT-3.5 has the lowest MSE across the data splits, followed by Llama3-\n70B. We also observe LLMs with more parameters (e.g., Llama3-70B, Llama2-7B) generally have a\nlower MAE than their counterparts with fewer parameters (e.g., Llama3-8B, Llama2-7B). Further,\nthe medical domain-tuned model, MedAlpaca, has a lower MAE compared to its general-domain\ncounterpart, Alpaca, when averaged across the different data splits.\nSubfigures (a)-(c) show the effect of increasing the number of food items from 1 to 3 while keeping\nthe number of servings constant at 1 for the (Natural Serving, Direct Retrieval), (Natural Serving,\nIndirect Retrieval), and (Metric Serving, Indirect Retrieval) splits respectively. In general, we observe\nan increasing trend in the MAE across all models and methods with an increasing number of food\nitems. However, as shown in Section 5.3, models exhibit a lower normalized MAE for the multi-item\nsubsets compared to single-item subsets, indicating that providing complete meal descriptions by\nincluding all items in a single query can be more accurate than prompting LLMs for each item.\nSubfigures (d)-(f) show the effect of increasing the number of servings from single to multiple, as\nwell as simultaneously increasing the number of food items from 1 to 2 for the (Natural Serving,\nDirect Retrieval), (Natural Serving, Indirect Retrieval), and (Metric Serving, Indirect Retrieval) splits\nrespectively. We observe that for the natural serving subsets, there is a general increasing trend of\nMAE with an increasing number of servings and items in the meal. However, the MAE decreases\nfrom single to multiple servings with a single food item in the metric serving split. We explore this"}, {"title": "H Normalized MAE Analysis", "content": "In Section 5.3, we find that multiple servings lead to higher normalized MAE compared to a single\nserving with the metric serving (100g). We hypothesize that 100g servings are more likely to appear\nin the training data for LLMs, resulting in better carbohydrate estimation for 100g servings. We\nfurther validate this hypothesis for natural servings. As shown in Table 7, the normalized errors for\nnatural servings are similar between single serving and multiple servings. This similarity can be"}, {"title": "I Error Analysis", "content": "To gain a deeper understanding of the challenges of using large language models for nutrition\nestimation, we manually reviewed a sample of model outputs with a high error. For this analysis,\nwe focused on the CoT and RAG+CoT methods as the intermediate steps with chain-of-thought\nprompting add a layer of interpretability to the models' reasoning process. To gain insights across\nvarious dimensions of query complexities, we randomly sampled 10 queries with an absolute error\ngreater than 15 from predictions made by GPT-3.5 from both the single food subset (with single\nand multiple servings) and the double food subset (with single servings). Within these, we sampled\nqueries from the natural serving, direct retrieval, and metric serving, indirect retrieval subsets. Overall```json\n, we analyzed 120 erroneous model outputs.\nBased on our analysis in the CoT setting, we categorized model errors into three main types:\n\u2022 Parsing Errors, which refer to mistakes in identifying individual food items in a query.\n\u2022 Serving Size Errors, which denote errors in determining the serving sizes of food items\n\u2022 Incorrect Predictions, which involve erroneous carbohydrate predictions, possibly due to incorrect prior knowledge or hallucination by the model.\nFigure 16 shows examples of each error type.\nAcross all data subsets in this setting, 79.4% of errors were due to 'Incorrect Predictions', 14.7%\nresulted from 'Serving Size Errors', and 5.9% came from 'Parsing Errors'. In the double food subsets,\n75% of the sampled queries contained an error in only one of the food items, while the other item\nwas estimated correctly. Notably, none of the errors were due to mathematical calculation mistakes,\nincluding in double food and multiple serving queries.\nIn the RAG+CoT setting, we introduce 3 additional error categories specific to retrieval-based\ngeneration:\n\u2022 Serving Unit Conversion Errors, arising due to different serving units in the retrieved\ncontext and query\n\u2022 Misdirected Attention Error, which occur when the model focuses on incorrect context\n\u2022 Misleading Context Error, which occur when retrieved contexts closely resemble the query\nfood item but have different carbohydrate values."}, {"title": "I.0.1 Critically High Error Examination", "content": "We perform a qualitative analysis of queries and model outputs leading to exceptionally high errors.\nWe restrict this analysis to GPT-3.5 with the CoT prompt as we want to monitor natural model failure\ncases, without the model being influenced by possibly erroneous context in the RAG setting.\nAcross all data subsets in NUTRIBENCH, the model had a mean (std) absolute error of 12.68 (21.19)\ng for the queries over which it made a prediction. To analyze the critically high error cases, we\nexamine the subset of model predictions with an absolute error exceeding two standard deviations\nfrom the mean (i.e., absolute errors of 55g or more).\nOur first observation is that for this subset, the average true carbohydrates in the queries is 118.66g,\nindicating that queries with critically high errors typically include high-carb meals. This finding is\nalso consistent with our observations in Section 5.4. Further, 76.23% of these were underestimation\nerrors, where the predicted carbohydrates were significantly lower than the true carbohydrates.\nWe also qualitatively analyze these samples, separating our analysis into cases of overestimation\nand underestimation, since incorrect insulin doses from either case can lead to hypoglycemia or\nhyperglycemia, both serious problems that require separate handling."}]}