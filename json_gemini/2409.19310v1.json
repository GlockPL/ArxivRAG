{"title": "Model X-Ray: Detection of Hidden Malware in AI Model Weights using Few Shot Learning", "authors": ["Daniel Gilkarov", "Ran Dubin"], "abstract": "The potential for exploitation of AI models has increased due to the rapid advancement of Artificial Intelligence (AI) and the widespread use of platforms like Model Zoo for sharing AI models. Attackers can embed malware within AI models through steganographic techniques, taking advantage of the substantial size of these models to conceal malicious data and use it for nefarious purposes, e.g. Remote Code Execution. Ensuring the security of AI models is a burgeoning area of research essential for safeguarding the multitude of organizations and users relying on AI technologies. This study leverages well-studied image few-shot learning techniques by transferring the AI models to the image field using a novel image representation. Applying few-shot learning in this field enables us to create practical models, a feat that previous works lack. Our method addresses critical limitations in state-of-the-art detection techniques that hinder their practicality. This approach reduces the required training dataset size from 40000 models to just 6. Furthermore, our methods consistently detect delicate attacks of up to 25% embedding rate and even up to 6% in some cases, while previous works were only shown to be effective for a 100%-50% embedding rate. We employ a strict evaluation strategy to ensure the trained models are generic concerning various factors. In addition, we show that our trained models successfully detect novel spread-spectrum steganography attacks, demonstrating the models' impressive robustness just by learning one type of attack. We open-source our code to support reproducibility and enhance the research in this new field.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Learning (DL) techniques are commonly used for various tasks such as Computer Vision (CV) [40], [38], Natural Language Processing (NLP) [10], Robotics [42], [61], and more. One of the most significant advancements in Artificial Intelligence (AI) has been the development of Large Language Models (LLMs). LLMs enabled human-like conversations [39], making it easier for people to interact with AI models. This has accelerated the integration of AI models into everyday life, especially among laypeople. Today, enterprises leverage a diverse set of pre-trained models obtained from model repositories, such as those hosted by Hugging Face [23] and TensorFlow Hub [52]. These models serve as powerful starting points, allowing organizations to fine-tune them to meet their specific requirements [24] using significantly less data than required for training a model from zero. But, despite the great benefits of model sharing, they also expose end-users to new and untreated cyber threats that can potentially utilize the DL models for malicious purposes using steganography [55], [56], [32], serialization attacks [53], or both [18].\nCyber attackers look for ways to deliver malware to target systems and avoid detection. Among many techniques, one common technique they utilize is steganography [9], [8]. Steganography is the art of data hiding, this is a well-researched and developed group of techniques [57] that have been primarily used and researched inside images [48], audio [1], video [33], and text [34]. Among many different steganography techniques, Least Significant Bit (LSB) steganography is a prominent variant of steganography [3]. These techniques embed data inside the least significant portions of the cover data, and in doing so, the noticeable effect is minimized. Similar to many well-known LSB steganography attacks on various digital media types, LSB steganography can be used in DL models. It can be exploited for malicious purposes e.g. for delivering malicious payloads [55], [56], [15]. However, in contrast, DL models are often much larger, and therefore, they may offer much greater hiding capacity than image, audio, or text data. The increased data transmission capacity may enable an attacker to use attacks that weren't possible before or that are harder to carry out. These techniques take advantage of redundancy, and DL models often use float parameters (32/16 bits), which offer great precision but may sometimes be redundant (see Section C). To overcome these attacks, several works [11], [2] suggested ways to disrupt the attacks by modifying the parameters or their representations by using quantization or random bit modification [11] or by fine-tuning [2]. These methods can possibly be countered by more sophisticated data-hiding techniques that utilize self-correcting codes to resist data corruption. In [18], novel AI model steganography techniques utilize low-density parity-check codes, demonstrating resilience to parameter pruning and fine-tuning. Therefore, a detection approach is needed.\nSteganalysis techniques are a set of methods aimed at detecting hidden messages in cover data, and they are designed to counter steganography. These detection techniques are an alternative strategy to zero-trust methods that aim to damage the payload by changing the model; instead, the aim is to detect the presence of hidden data. The detection approach has certain benefits over the techniques mentioned earlier: by using detection to filter potentially unwanted files, model repositories or end-users can avoid tampering with the models, which can cause unwanted side effects. In addition, detection may prove effective in cases where the data-hiding methods have some resilience to deliberate data corruption [18], as mentioned above. This article explores and develops steganalysis techniques for AI model LSB steganography attacks that can be applied in real-world scenarios.\nOne major drawback in traditional ML and DL learning algorithms is that they often require large amounts of training data to avoid overfitting [37]. This can be problematic for real-life scenarios where we want to detect steganography attacks inside AI models since they vary in size, architecture, task, and many more factors. The major obstacle we encounter is data scarcity. We require attacked models to research AI model LSB steganography, but no public sources exist. To this end, we build upon the methodology and framework for simulating attacks and creating datasets for research published in previous work [15], [13]. In addition, we focus on DL techniques that are specialized for use in data-scarce scenarios.\nOne-shot Learning (OSL) and Few-Shot Learning (FSL) are techniques developed to use DL in areas with significantly less training data than traditional DL models require. The core concept is that a model is trained only using one or a few samples per label. OSL and FSL are commonly utilized for CV tasks [12], [47], [4]. There are different approaches to tackling OSL and FSL - augmenting training data to get more samples [62], fine-tuning pre-trained models [36], training embedding models to project embedding samples to a smaller embedding space [31], etc. A popular FSL approach is called metric-learning [26]. In metric learning, samples are clustered based on their distance from other samples. In a classification context, a sample is labeled based on proximity to other labeled samples according to a certain distance function (like $l_1$ or $l_2$). The widely-known KNN algorithm is a canonical example. Metric learning is often paired with an embedding mechanism that embeds samples to a lower dimension (for example, a convolutional network [29]). This is called embedding learning. In the training phase, the goal is to minimize the distance between similar samples and maximize the distance between dissimilar samples (in classification, same-label samples are \"similar\" and vice versa for samples with different labels).\nFSL techniques have been found to have extensive appli- cations in the field of computer vision. Convolutional Neural Networks (CNN) are one of the most popular neural network types used in this domain, and a significant amount of research on FSL involves embedding learning using CNN networks as embedding functions with image data. Some researchers have also transformed data into image representations and applied OSL and FSL techniques to them [21]. In this article, we apply FSL techniques to AI models by creating image representations from them, allowing us to apply the rich and deeply developed image FSL techniques to our field. We use metric learning by training a CNN to use as an embedding function and we show that we can effectively classify unseen samples using 2 different evaluation techniques. We compare results and identify different use cases for the different methods."}, {"title": "A. Our contribution", "content": "The key contributions of our research are enumerated below:\n\u2022\nAl model representation: We propose a method for creating image representations from model weights to transfer problems involving AI models to the well-researched image domain.\n\u2022\nAI model LSB steganalysis methods: Our research marks the second stepping-stone towards effective and practical techniques for detecting attacks on Al models. The proposed methods have several key benefits that make them practical as opposed to past research:\nBig data requirement: Our methods only use up to 6 models for the training phase instead of ~40000 models in the past work [15].\nDetection capabilities: Proposed techniques success- fully detect attacks up to 25% Embedding Rate (lower ER is harder) and even 6% in some cases. Past research [15] showed successful results only on 100%- 50% ER.\nEffectiveness on common architectures: Our research shows success on popular large CNN model archi- tectures (for example, ResNet, Densenet, etc.). In contrast, past research techniques [15] were only proven effective on small custom CNN architectures (~2k parameters).\nGenerality: Using a strict and broad evaluation strategy, we show that our trained models can detect attacks in models that differ from the training data in various properties such as model architecture, size, attack severity, and embedded malware, while past works [15] trained and tested models for each combination of those properties individually.\nAttack type: We show that models trained using our suggested methodology can detect unknown attacks by training on the simulated LSB steganography attacks.\nPromotion of reproducible research: To further the de- velopment in this nascent field, we have made our steganography attack tools, steganalysis methods, model image representation methods, and the corresponding code publicly available [14].\nThe remainder of this paper is structured as follows: Section II provides background knowledge. Section III describes the proposed methodology for data creation, pre-processing, and classification. Section IV presents the experimental results and analysis. Section V discusses the future research direction and limitations. Finally, conclusions are drawn in Section VI."}, {"title": "II. BACKGROUND", "content": "X-LSB-Attack [15] embeds a malware sample m inside an ordered collection of float numbers. The attack works by using LSB substitution similar to other basic LSB steganography methods in other types of data (images [25], audio [5], etc.) The attack embeds m in the X least significant bits (mantissa/fraction part [60], [59]) of the float numbers, similarly to LSB-substitution procedures introduced in previous works [55], [56]. We use an extra version of X-LSB-Attack called X- LSB-Attack-Fill [15] that fills the whole cover data by repeating m or using the largest prefix of it that fits in the X least significant bits of all parameters in the model. While past\nA. X-LSB-Attack"}, {"title": "B. Embedding Rate", "content": "The embedding rate is a measure for steganography attacks. It measures how much (%) of the cover data is embedded with hidden data. In the context of steganography inside AI model weights, this can be expressed by\n$\\text{ER} = \\frac{n_b}{n_w \\cdot s}$\nwhere $n_b$ denotes the number of bits in the embedded payload, $n_w$ denotes the number of weights in the cover model, and s denotes the size of the models' weights (e.g., 32 if the parameters are float32). Since our experiments use X-LSB- Attack-Fill (Section II-A) we can express it directly:\n$\\text{ER} = \\frac{n_w \\cdot X}{n_w \\cdot s} = \\frac{X}{s}$"}, {"title": "III. METHODOLOGY", "content": "This section illustrates the methodologies for developing FSL models to detect LSB steganography attacks in AI models. The main workflow in this article consists of three phases: dataset creation (Section III-A), model training (Section III-D), and model evaluation (Section III-D).\nDatasets for training FSL models are created by following 3 steps: model collection, embedding malware, and pre- processing. Every model collection is processed into a binary classification dataset of benign (labeled 0) and malicious models (labeled 1). See Figure 1 for an illustration.\na) Model Collection: Model Zoos (MZ) are a population of models of the same architecture trained on the same task and can have only one model. We define a Model Collection (MC) as a collection of MZs. MCs for FSL are created by downloading pre-trained popular models from online model hubs like Tensorflow-Hub [52] and grouping them based on their size.\nb) Embedding Malware: Using the MC of pre-trained benign models, we create attacked versions of it by attacking every model in the collection using the same attack. We use the X-LSB-Attack-Fill method (Section II-A) to attack the models using a malware sample m from MalwareBazaar [35]. We embed multiple malware payloads or randomly generated binary payloads for large models (>300M parameters). The float mantissa region has $m_s$ bits, so we use X-LSB-Attack-Fill with $1 \\leq X \\leq m_s$ this results in $m_s$ different attacked model collections of different attack severity, a lesser X value means a more subtle, harder-to-detect attack.\nc) Pre-Processing: We apply pre-processing to our data to make it more suitable for our FSL models (siamese CNNs that work with constant-sized images). We developed techniques for creating images from float numbers (model parameters, etc.). See section III-C for an in-depth explanation of the procedure. After we convert our data into image form (8-bit grayscale), we reshape (downscale or upscale) the images into a constant predetermined size and normalize the 0-255 integer values to the 0-1 real values range - this is common practice for training CNNs [41].\nd) Dataset Hyperparameters: Using our methodology, datasets can be created in a large variety of ways. Ev- ery dataset D will have a set of hyperparameters $hp = (MC, (X,m), (I, s_h))$ that influenced its creation. Each step in the creation procedure uses the hyperparameters. MC is the Model Collection of MZs from which the dataset was created. (X, m) are used in the embedding procedure - X is the amount of LSBs we chose to override and m is the malware sample that was embedded. (I,sh) are used in the pre-processing phase, I is the image transformation used (See III-C) and sh is the final shape all images were reshaped into."}, {"title": "A. Dataset Creation", "content": ""}, {"title": "B. Train/Test Split", "content": "We split the processed image datasets into train and test sets using the following methodology:\nLet $MZS = {MZ_1, MZ_2, ...}$"}, {"title": "C. AI Model Image Representation", "content": "This section will discuss our methods for creating image representations from float model weights. We introduce a novel image representation called Grayscale-Fourpart.\nThe Grayscale-Fourpart image representation works on float32 data specifically. It is created by dividing each 32-bit model weight $w := b_{31} ... b_0$ to 4 8-bit parts: $p_0 := b_{31} ... b_{24}, p_1 := b_{23} ... b_{16}$ and so on. Each 8-bit long binary string (byte) is used as an unsigned int8 value with values in the range 0-255 like standard 8-bit images and serves as one pixel. 4 images are created by stacking the model weight parts $p_1, p_2, p_3, p_4$ from all weights (all $p_1$'s, all $p_2$'s, ...) and shaping the stacked vector of byte values into square shapes (adding zero padding if needed). The 4 images are stacked side to side to form the final image. See Algorithm 3 and Figure 8."}, {"title": "D. Convolutional Embedding Network", "content": "This section outlines our methodology for training a CNN to learn embeddings. We used the triplet method [45] with $l_2$ distance for the training strategy. We take our train data and construct all possible (anchor, positive, negative) triplets, which in our case is based on the labels (i.e. (benign, benign, malicious) and (malicious, malicious, benign)). We train a CNN feature extractor using Adam [28]. We experiment with three different training strategies:\n1) Early Stopping (ES): train for one epoch.\n2) Standard Training (ST): train for five epochs.\n3) Until Below (UB): train for 100 epochs, and stop training if train loss is within some interval, e.g. [0.5, 1.25].\nTesting phase: Model evaluation is done with 2 different procedures that show competitive results. We focus on methods that can be practically used.\na) Centroid: We classify a new sample using the centroids of the benign and malicious training data. This is common practice in FSL [54]. Let $D_0$ and $D_1$ denote them, let I denote the image to be labeled, and $f$ to be the trained CNN. A centroid of some data $D_i$ is given by $c_i = \\frac{1}{|D_i|} \\sum_{d \\in D_i} f(d)$. I is then given the label $\\text{argmin}_{i \\in {0,1}} l_2(f(I), c_i)$. See Figure 2.\nb) KNN: We classify a new sample using a similar process to the centroid technique, but we apply KNN using all embeddings of the benign and malicious training data instead of the average embeddings."}, {"title": "IV. EXPERIMENTS", "content": "In this section, the experimental results are laid out and analyzed. The experiments are carried out by exercising the data creation and classification methodology discussed in Section III. In each experiment, we tried out different MCs that we had compiled. We train models using some dataset D. We evaluate them on all attack severities ($1 \\leq X \\leq s$) in the same MC (in-distribution) and other MCs (out-of-distribution) using the test data that contains unseen model architectures to analyze how robust they are with regard to model architecture, embedded malware, model size, model task, and attack severity. More specifically, we aim to show that the trained models can be used in real-life scenarios. All experiments were run on an NVIDIA GeForce RTX 3080 GPU, i9 intel core, and 32GB RAM."}, {"title": "A. Baseline", "content": "Research on detecting attacks on AI models is a relatively new field in its early stages and has few existing works [53], [15]. Recent work [53] proposed detecting malicious model serialization attacks, which is outside the scope of our work. The work done in [15] focused on detecting LSB steganography in Al models. It is the closest to the scope of our work. Hence, we use it as a baseline. The baseline work [15] trains unsupervised and supervised models using the small CNN zoos we introduced in Section IV-C, and it showed non-trivial results for X > 17 on the CIFAR10, MNIST, SVHN, and STL10 MZs individually. We closely follow the evaluation process used in those experiments to create a fair comparison between our methods and the baseline."}, {"title": "B. Chosen Feature Extraction CNNs", "content": "We use two CNN architectures as feature extraction for our FSL models (See Section III-D):\nOSL CNN: We utilize the well-established CNN architecture designed for OSL introduced in [30] with images resized to 100x100. We denote this architecture as OSL CNN.\nSRNet: Several works develop CNNs focused on image steganalysis. We utilize a prominent residual CNN called SRNet [6] in our experiments with images resized to 256x256. We denote this architecture as SRNet."}, {"title": "C. Datasets", "content": "We create 4 different datasets for training and testing. We aim to comprehensively analyze different model architectures, sizes, etc. In addition, we reproduce results that use a spread- spectrum attack in the AI models [18] as a special study case on how the solution can handle unknown steganography attacks.\nWe used the Grayscale-Fourpart image representation (Section III-C) with image sizes 100 and 256 for all float32 datasets.\n1) Small CNN Zoos: Following the baseline work, We compile a dataset from a model zoo of small CNN models (2k float32 parameters) of the same architecture that was trained on STL10 [46]. We denote this dataset as SCZ. The model zoo has about 50000 different models. We used the malware sam- ple with sha256 6054f328f8d54d0a54f5e3b90cff020e139105 eb5aa5a3be52c29dbea6289c30 on this dataset (denoted m6054f). Since this dataset is used for comparison with the baseline [15] (see IV-A), we use a train/test split on models from the same model zoo in contrast with our methodology. We use 3 benign and 3 malicious models in the training split and the rest for testing.\n2) Famous CNNs (Small): We compile a MC of different well-known CNN architectures of size at most 10M weights with float32 parameters. We use the imagenet pre-trained models supplied in the Keras library [27]. The chosen ar- chitectures are: DenseNet121 [22], EfficientNetV2 (B0,B1) [51], MobileNet (V1, V2, V3Small, V3Large) [20], [44], [19], and NASNetMobile [63]. We used the malware sam- ple with sha256 77e05b52f51cfc8ec31f0dc2e544dc21b94250f 35a5a353fd5e4e271e75bc45d on this model collection (denoted m77e05). We chose the MZs MobileNet, NASNetMobile, and MobileNetV3Large for training and the rest for testing.\n3) Famous CNNs (Large): We compile an MC of well- known CNN architectures of size at most 100M and more than 10M weights with float32 parameters. We use the imagenet pre-trained models supplied in the Keras library [27]. The chosen architectures are:\n\u2022\nDenseNet (169, 201) [22]\n\u2022\nEfficientNetV2 (B2,B3, S, M) [51]\n\u2022\nInception (V3, ResNetV2) [50], [49]\n\u2022\nNASNetLarge [63]\n\u2022\nResNet (50(V1, V2), 101(V1, V2), 152(V1, V2)) [16], [17]\n\u2022\nXception [7]\nFor this MC, we used 3 concatenated malware samples. Their MD5 hashes are:\n\u2022\n9de9993c77412ba8fa3200714dcd58f6\n\u2022\nedd73e79cb7b8abe0f569844931f4fdb\n\u2022\n6d5c304bef8b2c32e904946f03b601d9\nDenote this malicious payload m9d_ed_6d. We chose the MZs DenseNet169, and NASNetLarge for training and the rest for testing.\n4) Maleficnet OOD Attack: We use the code from the authors in [18] to recreate spread-spectrum attacks. Using our methodology (Section III), we create datasets from the benign and attacked models for additional evaluation. The recreated attacks (architecture + malware payload) are:\n\u2022\nDenseNet121 (Stuxnet, Destover)\n\u2022\nResnet50 (Stuxnet, Destover, Asprox, Bladabindi)\n\u2022\nResnet101 (Stuxnet, Destover, Asprox, Bladabindi, Cerber, Eq.Drug, Kovter)\nThe datasets we created above have 1 benign sam- ple in each MZ and 2/4/7 malicious samples in the DenseNet121/Resnet50/Resnet101 architectures, respectively. Hence, a meaningful classification accuracy (better than weighted random guessing) will be more than 66%/80%/87.5%. In the context of this paper, we call the dataset used in training an In-Distribution (ID) dataset, and other datasets"}, {"title": "D. Error Bars", "content": "When applicable, we perform repeated experiments and show confidence interval plots using the seaborn python package [58]. The randomness in the trials stems from the CNN models' random initialization, and train sample shuffling."}, {"title": "E. Model Evaluation Metric", "content": "We define a simple metric for grading models. The metric is a weighted accuracy of the model's result on some datasets on all s attack severities and benign samples. We emphasize that attacking the least significant bytes in the mantissa will be much harder to detect (See Section C) therefore, we give higher weights to the more complicated detection. Let $a_i$ the accuracy of a model on the dataset with i bits attacked, and let $a_0$ the accuracy of the same model on the benign dataset. The weighted metric is given by:\n$WM = \\frac{1}{2} \\cdot (a_0 + \\frac{\\sum_{i=1}^{s} (s - i + 1) a_i}{\\sum_{i=1}^{s} i})$\nThe Weighted Metric is bounded in the range [0,1] by definition."}, {"title": "F. Plot Types", "content": "We use 2 main types of plots to showcase our results:\n1) Only Model LSB (OML): These plots show classification accuracy of models trained on some attack severity $1 < X \\leq 23$ that are then tested on benign and attacked models that were attacked with attack severity X. The x-axis shows Model LSB and the y-axis shows mean accuracy for both benign and attacked models. These plots help us get an initial sense of the success of a model.\n2) All LSBs (AL): These plots show the weighted metric (see Section IV-E) of models trained on some attack severity $1 < X < 23$. The x-axis shows Model LSB, and the y-axis shows the weighted metric. These plots help us decide what X value results in the best global-performing model on a specific dataset."}, {"title": "G. Results", "content": "1) Experiment 1 - SCZ Baseline: In this experiment, we train classification models on the SCZ MC (Section IV-C1) using the UB training strategy (see Section III-D). The main goal of this experiment is to measure the success (test accuracy) of models trained using our proposed methodology (Section III) versus the baseline results [15]. To this end, we closely replicate the training/testing procedure they used to make the comparison strict. The baseline work trained a model for each attack severity and each model architecture individually. Here, we focus on the STL10 model zoo and train our classification models on 3 STL10 models (benign and attacked) on each attack severity (23 different models). We repeat the training process 30 times and show mean and confidence interval results. See OML Figure 3. The plot shows the #LSB in the training dataset each model learned versus the test accuracy on the STL10 MZ. The CNN model with centroid evaluation had very close results to the baseline for #LSB \u2208 [18,23] (56%-71% ER). For #LSB \u2208 [8,18] attacks (25%-56% ER), it showed stable results in the confidence interval [0.85, 1], while the baseline model failed. The CNN model with 1NN evaluation also had good results but was less stable. This is likely due to the model being overfit. In conclusion, we successfully trained FSL models that outperformed the baseline in terms of accuracy using only 6 models for training, as opposed to 40,000 models in the baseline method.\n2) Experiment 2 - small famous CNNs: In this experiment, we train classification models on the small famous CNNs using the UB training strategy (see Section III-D). The main goal of this experiment is to see how the techniques fare with the larger CNN architectures and whether the models succeed on unseen model architectures and unseen attacks. We train models on all 23 attack severities like before, but this time, we measure their performance on the other attacks (unseen during training) using the Weighted Metric introduced in Section IV-E. This metric gives us a general way to score models and rewards success on the more delicate attacks (smaller X value), which are a major challenge in this area. In addition to evaluating the models on the small famous CNNs test set, we also evaluate the models on the large famous CNNs (See Section IV-C). These architectures serve as an OOD test for the models trained on the small famous CNNs, with a size difference of (10M-100M)."}, {"title": "V. LIMITATIONS AND FUTURE WORK", "content": "Our research focused on detecting LSB substitution steganog- raphy attacks and introduced new steganalysis methods. While experimental results show promising results in LSB attacks and spread-spectrum attacks, future research should cover evasive attack vectors like maleficnet [18] directly. This work introduces a general methodology and software framework for research [14], however, we focus on CNN models and particularly float32 data. Future research should investigate expanding the study to include more compact data types, such as 16-bit, 8-bit, 4-bit, or even 2-bit quantization, which are increasingly utilized in various models, including text models. Float16 data can be addressed in our methodology by using a different image representation, for example, creating an image from the last byte might be sufficient as the float16 mantissa is 10 bits."}, {"title": "VI. CONCLUSION", "content": "In this work, we propose novel few-shot learning methods to detect AI model steganography attacks. These attacks leverage the AI model-sharing community for malicious purposes by hiding malware in the shared models. Protecting end-users from unknowingly downloading malicious content is a fierce challenge. Our experimentation achieves very high detection rates using just a small training dataset that contained up to 6 model weights (samples) compared to 40000 samples of pre-trained models needed by the current state of the art. Furthermore, our methods consistently detect delicate attacks of up to 25% embedding rate and even up to 6% in some cases. At the same time, the previous state-of-the-art was only shown to be effective for 100%-50% embedding rate. Our experimental results show methods trained using our methodology can even detect OOD novel attacks [18] that current state-of-the-art prevention techniques don't prevent."}, {"title": "APPENDIX A", "content": "BACKGROUND"}, {"title": "A. X-LSB-Attack", "content": "We include pseudo-code to illustrate better the 2 X-LSB- Attack variants described in Section II-A. In the pseudo-code, we use w[0 : r] to denote the first r bits of the float number w. v + u denotes the concatenation of 2 binary strings, and ve denotes concatenating v to itself c times."}, {"title": "B. Floating Point Format", "content": "Floating-point numbers [60], [59] (also called floats) repre- sent decimal numbers in computer memory. There are multiple standard types of floats with varying precision, such as single- precision float (also called float32), which takes up 32 bits in memory; half-precision float (also called float16), which takes up 16 bits in memory, and more. Float numbers are divided into three sections: sign, exponent, and mantissa (also called fraction). The sign bit denotes the sign of the number, the exponent section represents a negative or positive power of 2, and the mantissa section represents the precise value after the decimal point. We proceed to explain more about float32, which is the main data type we experiment with."}, {"title": "APPENDIX C", "content": "MODEL PERFORMANCE"}]}