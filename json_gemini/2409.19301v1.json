{"title": "Privacy Attack in Federated Learning is Not Easy: An Experimental Study", "authors": ["Hangyu Zhu", "Liyuan Huang", "Zhenping Xie"], "abstract": "Federated learning (FL) is an emerging distributed machine learning paradigm proposed for privacy preservation. Unlike traditional centralized learning approaches, FL enables multiple users to collaboratively train a shared global model without disclosing their own data, thereby significantly reducing the potential risk of privacy leakage. However, recent studies have indicated that FL cannot entirely guarantee privacy protection, and attackers may still be able to extract users' private data through the communicated model gradients. Although numerous privacy attack FL algorithms have been developed, most are designed to reconstruct private data from a single step of calculated gradients. It remains uncertain whether these methods are effective in realistic federated environments or if they have other limitations. In this paper, we aim to help researchers better understand and evaluate the effectiveness of privacy attacks on FL. We analyze and discuss recent research papers on this topic and conduct experiments in a real FL environment to compare the performance of various attack methods. Our experimental results reveal that none of the existing state-of-the-art privacy attack algorithms can effectively breach private client data in realistic FL settings, even in the absence of defense strategies. This suggests that privacy attacks in FL are more challenging than initially anticipated.", "sections": [{"title": "I. INTRODUCTION", "content": "MACHINE learning has witnessed significant success in the fields of computer vision [1], natural language processing [2] and so on. For parametric machine learning models, especially deep neural networks [3] containing numerous learnable parameters, it is generally agreed that more training data leads to greater potential performance of the model. And if one were able to collect data from multiple distributed devices, the model trained on such a dataset might exhibit superior learning performance [4]. This form of data 'centralization' has indeed emerged as the predominant training method for decades.\nNonetheless, the aggregation of distributed data onto a single device markedly compromises user privacy. The pioneering research of Fredrikson et al [5] was among the earliest to address the leakage of private information. Richard et al. [6] further demonstrated that the protected data can be vulnerable to the recovery of hidden information. These findings underscore the necessity of exploring alternative learning paradigms. Moreover, recent regulatory developments, such as the enactment of the General Data Protection Regulation (GDPR) [7] by the European Union, have heightened concerns regarding data security and privacy.\nTo address the above mentioned privacy issue, federated learning (FL) [8] has emerged as an effective method to preserve data privacy and reduce the substantial transfer costs associated with data collection [9]. Unlike traditional centralized machine learning approaches, FL retains data locally, with each client collaborating to train a joint global model. For a typical horizontal FL framework, the central server solely receives and aggregates the model parameters or gradients from the clients to derive a global model, thereby benefiting from the distributed learning. Subsequently, the updated global model is transmitted back to the clients, facilitating knowledge sharing among them. This approach allows clients to retain their training data within the device, thus safeguarding user privacy to a certain extent.\nHowever, some research work indicate that FL remains susceptible to adversarial attacks from malicious parties. For instance, model poisoning attacks [10-17] compromise the integrity of the global model by tampering with local datasets or introducing backdoor inputs [18, 19]. Even worse, the design of existing FL protocols still harbors flaws in data privacy. These vulnerabilities could potentially be exploited by attackers to identify weaknesses or gain unauthorized access to sensitive user data [20-22]. Studies have highlighted that models trained using FL inadvertently memorize the privacy-sensitive training dataset and subsequently disclose sensitive information [23-26].Any participating client in fact acts as a potential data provider whose intermediate updates can be observed by the central server. And adversarial attackers might manipulate them as features of the decentralized preparation process, allowing malicious server or client to adjust interactions with minimal constraints.\nSpecifically, several attack algorithms have been developed to reconstruct dummy images that closely resemble the original private images, including DLG [27], iDLG [28], Inverting Gradients [29] from the server side and Deep Models under the GAN (DMGAN) [30] from the client side. However, there is a notable absence of experimental studies that systematically evaluate the strengths and weaknesses of these attack methods. Given their inherent limitations, these approaches may not be fully effective in real-world FL environments. For example, some studies [29, 31, 32] conduct experiments by applying attacks to a single gradient derived from batch training data, which diverges significantly from the local model updates or differences typically encountered in FL.\nMoreover, some methods [31, 33] necessitate modifications to the global model's architecture, may resulting in significant FL performance degradation.\nTherefore, in this paper, we aim to conduct a comprehensive investigation into the various privacy attacks executed within a real FL environment. We specifically focus on assessing the extent to which these attacks can compromise user privacy and evaluate their effectiveness in revealing sensitive information. Nine representative attack algorithms are evaluated, and the experimental results provide valuable insights for the future development of cybersecurity-related FL research. To this end, To this end, our concise and clear code implementation is publicly available 1, enabling researchers to easily experiment with different privacy attack strategies and extend them in their research.\nBased on the results of our extensive studies, we have identified several key findings. Most notably, we found that many attack algorithms are capable of reconstructing high-quality dummy images from gradients corresponding to single data points or multiple averaged data. However, in more complex federated learning environments, where averaged gradients from batch data are computed locally and updated multiple times, these algorithms perform poorly. Among the evaluated methods, Robbing the Fed (RTF) demonstrated the best attack performance. Nevertheless, it requires the insertion of an Imprint module (comprising two fully connected layers) before the learning model, which significantly degrades the training performance of federated learning.\nOur contributions are summarized as follows:\n1) We provide a systematic overview of existing privacy attacks within federated learning (FL), discussing their underlying algorithms in detail and analyzing the advantages and disadvantages of each approach.\n2) We conduct extensive experimental studies on 9 representative privacy attack methods in a more realistic FL environment, including DLG [27], iDLG [28], Inverting Gradients [29], GGL [34], GRNN [35], CPA [31], DLF [36], and RTF [37] for server-side attacks, and DMGAN [30] for the client-side attack. Our experimental results reveal that most of these approaches are ineffective in scenarios where averaged gradients of batch data are locally computed and updated multiple times on each client.\nThe remainder of this experimental study is organized as follows. Section II provides an overview of FL. In section III, we introduce different types of privacy attack algorithms in FL. Section IV then presents the experimental results of 9 selected attack algorithms, followed by the conclusion in Section V."}, {"title": "II. OVERVIEW OF FEDERATED LEARNING", "content": "Early in 2016, Google introduced the concept of FL [8], a decentralized machine learning paradigm designed to collaboratively train a shared global model using distributed data. This approach ensures that no external parties have access to local private data, thereby significantly protecting user privacy. A typical FL problem aims to optimize the aggregated loss function L(W) as shown below:\n$L(W) = C(W) \\ell(W) = \\sum_{k=1}^K \\frac{n_k}{n} C_k(W) = \\sum_{k=1}^K \\frac{n_k}{n} \\left( \\frac{1}{n_k} \\sum_{i \\in D_k} \\ell_i(W) \\right)$ (1)\nwhere W represents the parameters of the shared global model, K is the total number of clients, $n_k$ indicates the amount of local data on client k, and D = (x, y) denotes the i-th data sample on client k. Then, for each communication round t, the training procedure of FL primarily consists of the following three steps:\n1) Download: The central server sends the global model parameters $W_t$ to each client k.\n2) Local training: Each client k uses local training data $D^k$ to train received model parameters $W_t$. Subsequently, the client transmits the updated model parameters (or gradients) $W_k$ back to the server.\n3) Aggregation: The server aggregates the received updates to refine the global model $W_{t+1} = \\frac{\\sum_{k=1}^{K} n_k W_k}{n}$, which is then distributed to each client in the subsequent communication round t + 1.\nIt is important to note that the last two steps can be performed either synchronously or asynchronously, and the global model typically converges gradually over multiple communication rounds. Yang et al. [9] redefined this procedure as horizontal federated learning (HFL) and further expanded and categorized the concept into HFL, vertical federated learning (VFL), and federated transfer learning (FTL) [38, 39] based on the varying data partitioning characteristics among distributed participants.\n1) Horizontal Federated Learning (HFL), also known as sample-based FL, is characterized by having the same data feature space but different sample spaces among clients during joint training. For example, banks in various regions may share the same user characteristics, such as account, user name, age, account balance, etc., but their user groups may differ. Typical applications of HFL include Google's Gboard [40] and WeBank's FedVision [41].\n2) Vertical Federated Learning (VFL), also known as characteristic-based FL, is characterized by having the same sample space but different data feature spaces among clients. Unlike HFL, where each client independently performs training, VFL requires each client to collaboratively make inferences. The need for VFL has emerged in recent years due to the limited amount of data owned by companies and institutions [42]. Many enterprises have adopted VFL to enhance finance, advertising, and multimodal tasks [43, 44].\n3) Federated Transfer Learning (FTL) addresses a significant limitation of both HFL and VFL: the requirement for consistency in sample or data feature spaces. Unlike HFL and VFL, FTL does not necessitate feature space or user space consistency [39]. For instance, if a hospital and a bank from different cities collaborate in training, they may have different samples and data"}, {"title": "III. PRIVACY ATTACKS IN FEDERATED LEARNING", "content": "Recent studies have indicated that FL cannot fully guarantee data privacy. As shown in Fig. 1, attackers may still deduce private data to some extent at both server-side and client-side through communicated model parameters. And based on different attack targets, we introduce and discuss privacy attacks from two perspectives: label inference attack and input reconstruction attack. For each type of attack, existing methods are systematically reviewed and discussed, along with their advantages and disadvantages.\nAggregation\nGlobal\nmodel\nInference\nattack\nServer\nAttacker\nLocal model Local update\nLocal update\nLocal model\nGenerator\nlabel\nClient\nLocal data\nAttacker Local data Inferred data\nFig. 1: A simple example of privacy attacks in FL. Attacks can be performed on both server-side and client-side.\nA. Label Inference\nAdversarial attackers may implicitly deduce the private labels of other parties through the analysis of communicated model gradients. And a timeline highlighting parts of milestones in recent label inference algorithms is summarized in Fig. 2. Li et al. [45] demonstrated that there are two methods by which one party can accurately recover the ground-truth labels owned by the other party in a two-party split learning scenario. Zhao et al. [28] proposed an analytical approach to extract the ground-truth labels by exploiting the direction of the gradients computed using the cross-entropy loss, as shown below:\n$p = softmax(y) = \\frac{exp(y)}{\\Sigma_{c'} exp(\\hat{y}_{c'})}$ (2)\n$\\mathcal{L}(y, \\hat{y}) = - \\sum_{c'} y_{c'} log \\hat{P}_{c'}$\nwhere y is the one-hot encoding vector of the ground-truth label c\u2208 [C], \u0177 is the model output logits of a single input x, and $p_c$ denotes the predicted score for the c'-th label class. The gradient d\u0177 of the loss L(\u0177, y) with respect to any slot c' of the prediction vector \u0177 can be defined as:\n$\\frac{\\partial \\mathcal{L}(\\hat{y}, y)}{\\partial \\hat{y}_{c'}} = -y_{c'} \\frac{exp(\\hat{y}_{c'})}{\\sum_{c'} exp(\\hat{y}_{c'})}$ (3)\n$= \\hat{P}_{c'} - y_{c'}$\nSince the prediction $p_c$ ranges from 0 to 1, it is easy to find that $\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_{c'}}$ \u2208 (\u22121,0) when the label slot c' = c, and $\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_{c'}}$ \u2208 (0, 1) otherwise. Therefore, the true label class can be easily deduced through the gradient d\u0177 with the negative value.\nEven in scenarios where d\u0177 is unavailable, such as missing model biases in the final fully connected layer, inference of the private label remains feasible through analysis of the model weights. Specifically, the gradients $\\nabla W \\in \\mathbb{R}^{n_{L-1}}$ with respect to c'-th row of the model weights $W^L \\in \\mathbb{R}^{N_L \\times N_{L-1}}$ of the last layer L can be formulated as:\n$\\nabla_{W} \\mathcal{L} = \\frac{\\partial \\mathcal{L}(\\hat{y}, y)}{\\partial \\hat{y}_{c'}} \\frac{\\partial \\hat{y}_{c'}}{\\partial W}$ (4)\n$= \\frac{\\partial \\hat{y}_{c'}}{\\partial W} (W^{L-1} + b)$\n$= (\\hat{p}_{c'} - y_{c'}) a^{L-1}$\nwhere L is the total number of model layers, b \u2208 R is the c'-th model bias of layer L, $a^{L-1} \\in \\mathbb{R}^{n_{L-1}}$ denotes a single activation output of layer L \u2013 1. When the network employs non-negative activation functions, such as commonly used Relu, the positivity of $a^{L-1}$ ensures. Consequently, \u25bdW and dy share a concordant sign. Thus, a straightforward deduction emerges: the negativity of the gradient vector \u25bdW reliably indicates the ground-truth label c.\nc = c', s.t. $(\\nabla W, \\nabla W_{c'})$ \u2264 0, \u2200 c' \u2260 c\" (5)\nHowever, this approach can only successfully infer private label of the gradients calculated by a single data point which is not representative of actual FL situations in the real world applications. To address this challenge, Yin et al. [32] proposed the first method to extract labels from gradients with respect to a batch of multiple images. While this is only valid when there are no duplicate samples within the batch. Aidmar et al. [46] investigated an attack to extract the labels of the users' local training data from the shared gradients by exploiting the direction and magnitude of the gradients. Based on the previous work, Geng et al. [36] proposed a simple zero-shot approximation method to restore labels from training batch containing data with repeated label classes. According to Eq."}, {"title": "IV. EXPERIMENTS", "content": "The purpose of this experimental study conducted here is to empirically assess the extent to which current representative privacy attack methods can recover private client images in a real FL environment. Unlike previously described single gradient attack", "datasets": "MNIST [99", "62": "CIFAR-100", "76": "and ImageNet ((ILSVRC2012)) [100", "follows": "n\u2022 Total number of clients: 10 & 100\n\u2022 Number of local epochs: 1", "size": 10, "rate": 0.1, "decay": 0.95, "Algorithms": "Nine representative attack algorithms", "attack": "n1) DLG [27", "101": "as the optimizer.\n2) iDLG [28", "29": "Different from DLG-based methods", "34": "A GAN-based attack approach leverages a pretrained generator to reduce the dimensionality of the reconstruction search space. Gradient-free optimizers", "35": "Different from GGL", "31": "This pioneering work amis to recover high-resolution images from aggregated gradients by optimizing the unmixing matrix. In addition", "36": "This method highlights the significance of correct label inference and considers the scenarios that the model uploads are computed through multiple steps of local batch training.\n8) RTF [37"}, {"attack": "n1) DMGAN [30", "l=*": "is its respective dummy label. It is evident that the quality of the reconstructed images is significantly worse than those of the gradient attack shown in Fig. 5", "102": "is used in place of a commonly employed activation function. The authors argue that GLU offers greater stability compared to ReLU and learns faster than the Sigmoid activation function. The simulation results of GRNN for 10 batch size and 3 communication rounds are shown in Fig. 25", "Dataset": "Increasing the amount of local training data also increases the number of batch iterations within a single training epoch. This", "Size": "To evaluate the robustness of the algorithms to batch size", "Epochs": "Increasing the number of local training epochs forces the local data to be repeatedly applied for client model updates. This further widens the gap between the global model"}]}