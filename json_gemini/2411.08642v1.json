{"title": "Towards More Accurate Fake Detection on Images Generated from Advanced Generative and Neural Rendering Models", "authors": ["Chengdong Dong", "Vijayakumar Bhagavatula", "Zhenyue Zhou", "Ajay Kumar"], "abstract": "The remarkable progress in neural-network-driven visual data generation, especially with neural rendering techniques like Neural Radiance Fields and 3D Gaussian splatting, offers a powerful alternative to GANs and diffusion models. These methods can produce high-fidelity images and lifelike avatars, highlighting the need for robust detection methods. In response, an unsupervised training technique is proposed that enables the model to extract comprehensive features from the Fourier spectrum's magnitude, thereby overcoming the challenges of reconstructing the spectrum due to its centrosymmetric properties. By leveraging the spectral domain and dynamically combining it with spatial domain information, we create a robust multimodal detector that demonstrates superior generalization capabilities in identifying challenging synthetic images generated by the latest image synthesis techniques. To address the absence of a 3D neural rendering-based fake image database, we develop a comprehensive database that includes images generated by diverse neural rendering techniques, providing a robust foundation for evaluating and advancing detection methods.", "sections": [{"title": "1. Introduction", "content": "Images synthesized by generative models, such as Generative Adversarial Networks (GANs) [55-57] and Diffusion Models (DM) [58-61], have raised significant ethical, privacy and security related concerns in our society. A critical aspect overlooked by prior research is the potential for a large volume of generated images, if undetected, to contaminate the image pools available online, which are often used to train large-scale models. As noted in [51], recursively generated data can lead to model collapse due to such contamination. To address such concerns emerging from the generative models, numerous neural synthetic image detection models [1, 24, 25, 30, 40, 41] have been developed.\nHowever, the advancement of neural rendering technologies such as Neural Radiance Fields (NeRF) [2, 8-11, 14, 17] and 3D Gaussian Splatting (3DGS) [7, 12, 20] offers a novel approach to generating highly realistic imagery such as scenes and digital humans/avatars, by the acquisition of two-dimensional projections from lifelike three-dimensional spatial representations. There even exist methodologies [6, 22] capable of directly editing the content within 3D representations. Unlike generative models, neural rendering technologies produce more realistic synthetic images by reconstructing scenes from actual images, thereby avoiding logical and semantic inconsistencies. This process allows for subtle 3D modifications that, when projected to 2D, are nearly imperceptible. Notably, current fake image detection systems have not addressed whether neural-rendered images can also be identified as non-real. This limitation prompts the question of whether current fake detectors possess sufficient generalization capabilities to detect neural-rendered images as fake. Specifically, when such detectors are trained exclusively on synthetic images produced by generative models and then tested on neural-rendered images, their efficacy remains uncertain.\nTo enhance generalization ability, [25] uses linear probing to fine-tune the linear layer after a pre-trained large vision model. [30, 41] introduce side blocks to the main branch of fixed parameters inspired by LoRA [62] to improve representing ability. [41] further enhances cross-domain performance by incorporating a frequency module (FAA) within the LoRA-like side blocks, while maintaining a spatial-based main branch. However, the representing power of these frequency modules is limited by the constrained scale of parameters.\nIn this work, we introduce Fourier Frequency-based image Transformer (FFiT), an architecture leveraging large vision models to extract spectral domain information for fake detection. The FFiT backbone is pre-trained using magnitudes of spectra from real images in an unsupervised way. Traditional Masked Autoencoders [53] struggle with spectral magnitudes due to their centrosymmetric properties (to be detailed later), leading to superficial reconstructions by copying patterns from unmasked symmetric patches and failing to recover the patch of magnitude from its neighboring unmasked patches. To address this, a novel loss function is introduced, enabling unsupervised learning of deep representations from spectral magnitudes. The multimodal architecture combines the pre-trained FFiT backbone with a spatial-based large vision model. As a binary classifier, it is fine-tuned on datasets containing both real and synthetic images. Both branches exhibit strong independent performance, attributable to their design and fine-tuning strategies. Specifically, our multimodal detector achieves average precision (AP) of 92.81% and AUROC of 91.19% across 11 types of 3D scene generators, training on real and GAN-generated images. It outperforms the state-of-the-art (SOTA) [41], proving that neural-rendered fake images can also be accurately detected.\nReference [25] finds that detectors based on smaller neural networks, such as ResNet, struggle to identify challenging fake samples (e.g., DM-generated images) when trained on easily distinguishable fakes (e.g., GAN-generated images). In contrast, detectors using larger networks like ViT generalize well to difficult fakes even when trained on easier ones. Reference [45] notes that training with difficult fake samples improves generalization for detectors, even those based on smaller networks. However, neither study offers a systematic explanation for these observations. Thus, an index is introduced to quantitatively analyze the hyperplane differentiating clustered features while mitigating the impact of sparse outliers, unveiling the relationship between cross-domain performance and the representing ability of deep models for fake detection.\nTo advance the research on the detection of neural-rendered images, we have collected a large-scale database comprising 296,504 images generated using NeRF and 3DGS, supplemented by 330,073 images produced via 3D scene editing techniques leveraging NeRF and 3DGS. Additionally, the advent of sophisticated video generation methods, such as Sora [21], which demonstrate exceptional capability in displaying consistent 3D scenes, poses new challenges for the detection of synthetic visual content. Therefore, we also acquired 60,531 frames generated by Sora.\nIn summary, the main contributions are:\n\u2022 Development of FFiT, the first architecture using a large vision model to extract spectral domain imprints for accurately detecting fake images. This success is achieved by introducing a novel loss function to address the challenges posed by the spectrum's centrosymmetric property during unsupervised training.\n\u2022 The dynamic consolidation of FFiT features with those from the spatial branch can achieve SOTA performance. Each network branch exhibits robust performance capabilities, which are attributed to its well-designed architecture and refined fine-tuning strategy.\n\u2022 In contrast to existing databases that focus exclusively on fake images from generative models like GAN and DM, our database is the first in its inclusion of images derived from neural rendering-based synthesized and edited 3D scenes, as well as realistic scenes generated by Sora.\n\u2022 To analyze the generalization ability of detectors, a real-fake separation index is developed to quantitatively uncover the correlation between the cross-domain performance and representing the generalization ability of deep models for fake detection."}, {"title": "2. Related Works", "content": "Realistic 3D Scene Generation: NeRF methods are developed to implicitly learn the 3D representation of specific scenes, which can be reconstructed from a series of input images [8-11] or from a prompt such as a textual description [16] or a single-view image [68]. A potential malicious application involves integrating NeRF with editing techniques to alter the representation of 3D scenes using textual instructions [6, 35], images [13, 14], or both [34]. Additionally, NeRF-based methods are used for generating realistic digital humans (avatars), such as speech-to-video talking heads [18, 19, 26, 27] and body synthesis [36, 68].\nThe 3DGS methods [7, 12] enable the learning of explicit 3D representations of scenes, which can be seamlessly integrated into existing rendering pipelines. Conditional editing techniques, such as Instruct-GS2GS [22], have been developed to modify 3DGS scenes. Avatar synthesis methods [20, 28] based on 3DGS have also been proposed. Moreover, 3DGS can be combined with diffusion models to generate 3D scenes from scratch, as exemplified by GSGEN [17] and DreamGaussian [29]. Sora [21], which is representative among a series of text-to-video generation methods [21, 37, 38], demonstrates an impressive ability to generate videos with accurate 3D relationships. This capability suggests that it possesses the capacity to represent the 3D world effectively. In this work, the focus is on detecting fake images synthesized by the aforementioned 3D scene generation methods.\nFake Image Detection: To detect images generated by generative models, traditional detectors based on the spatial domain [1, 32, 33, 69-71] and those based on the spectral domain [24, 39, 40, 73] are trained on real and fake images to identify the latent fingerprints of GANs and Deepfakes. Methods that require learning [31, 48, 49], and a learning-free method [46], exploit inherent properties of DM architecture for detecting DM-generated fakes. Several approaches [25, 30, 41-44, 47] are effective in identifying both GAN- and DM-generated images. Methods [44, 47] enhance detection through an attention mechanism in the spectral domain. [43] utilizes captions from real images to generate fakes and then trains an SVM using deep features from both real and generated fakes. NPR [42] develops an operator to reveal neighboring pixel relationships within the spatial domain for improved detection. Ojha et al. [25] introduce a large vision model with a fixed backbone to improve generalization, while [30, 41] fine-tune the backbone of large vision models like LoRA [62] to enhance representation while maintaining generalization. FatFormer [41] further integrates information from frequency and language domains to boost cross-domain performance.\nHowever, the aforementioned fake image detectors fail to address the challenge that the latent patterns of synthetic images generated by newer methods, such as NeRF and 3DGS, may significantly differ from those produced by traditional generative models due to the domain gap between these generation processes. Consequently, we introduces a novel architecture to address these emerging techniques and their associated detection challenges."}, {"title": "3. Proposed Method", "content": "In this section, we present FFiT, a framework that utilizes large vision models to extract spectral domain information for detecting fake images. The FFiT backbone is initially pre-trained in an unsupervised manner using magnitude spectra from real images. This pre-trained backbone is then combined with a spatial-based large vision model to create a multimodal architecture, which is fine-tuned on a dataset containing both real and synthetic images. The following subsections detail the architecture and training strategy of the proposed method."}, {"title": "3.1. Motivation and Design of FFiT", "content": "MAE [53] is a classical method to train large neural models in an unsupervised way. However, the centrosymmetric characteristic of the spectrum, wherein the amplitudes at positive frequencies are equivalent to those at the corresponding negative frequencies, thereby exhibiting symmetry about the zero frequency, can introduce adverse effects to the training if we use the same way as MAE to train the Transformer on the spectral domain. In Fig. 1a, a sample of the original magnitude of the spectrum is presented. Figure 1b displays the mask utilized for patch masking during the inference phase, with the model that is trained using the original MAE-based training strategy. In this mask, white blocks indicate the patches that are to be masked during the patch embedding process, whereas black blocks represent the regions that should remain unmasked. Fig. 1c illustrates the reconstructed magnitude of the spectrum, based on the input from Fig. 1a and the mask shown in Fig. 1b, demonstrating a poor quality of reconstruction. In Fig. 1d, three representative types of regions of Fig. 1c are highlighted, and the following observations can be made:\ncase (i): When both a masked patch and its centrosymmetric counterpart are masked, the pre-trained model is unable to accurately reconstruct either. This indicates a limitation in the model's ability to infer information from the neighboring patches.\ncase (ii): In the case of masked patches for which the corresponding centrosymmetric patches remain unmasked, the pre-trained model demonstrates a capability to reconstruct these patches with high accuracy. This suggests that the model effectively captures and utilizes the centrosymmetric property of the spectrum during training.\ncase (iii): For the unmasked regions, it is evident that the pre-trained model fails to reconstruct them accurately. This finding is contrary to the expected behavior in the spatial domain, where an MAE-trained model typically succeeds in reconstructing unmasked areas."}, {"title": "3.1.1 Balancing the Weights of Various Masking Types", "content": "In the original MAE training process, the block-wise reconstruction loss $L_{B(i,j)}$, which represents the reconstruction error for the $i^{th}$ row and $j^{th}$ column block $(0 \\leq i, j \\leq N-1)$ between the original input magnitude of spectrum $X$ and the reconstructed $X'$, is calculated as follows:\n$L_{B(i,j)}= \\sum_{m=0}^{W-1} \\sum_{n=0}^{W-1}||X_{(Wi+m,Wj+n)}-X'_{(Wi+m,Wj+n)}||^2$  (1)\nwhere $X$ is divided into $N \\times N$ patches ($N$ is an even number) in a Transformer-based architecture. Given that $X$ is of size 224 \u00d7 224 pixels and each patch is of size $W \\times W$ pixels with $W = 16$, we have $N = 224/W = 14$. During the training process, masks are applied to these patches, compelling the model to reconstruct the patterns within the masked regions, thereby facilitating unsupervised learning.\nThe total loss function in the original MAE training is computed by summing the reconstruction losses over the masked blocks, i.e., those $B(i,j)$ that are masked. This approach has two key limitations that contribute to the failure to reconstruct the magnitude of the spectrum: 1. Ignorance of unmasked blocks in the loss function: the model is not penalized for any inaccuracies in the unmasked regions, which can lead to a lack of refinement in the overall reconstruction quality. 2. Overlooking centrosymmetric information: a masked block may have an unmasked centrosymmetric counterpart from which information can be easily copied. These limitations highlight the need for a more sophisticated loss function or training strategy that takes into account the unmasked regions and leverages the inherent symmetries within the spectral data to improve the reconstruction performance.\nTo address the limitations of the original MAE training process, we adopt a modified loss function that incorporates the focal loss mechanism. This approach aims to balance the influence of different masking cases, considering the special properties of the spectral magnitude. The loss function, denoted as $L_{r\\neq 0}(X, X'|r)$, is defined as follows:\n$L_{r\\neq 0}(X, X') = - \\frac{1}{N^2} \\sum_{i=0}^{N-1} \\sum_{j=0}^{N-1} \\alpha_t (1 - L_{B(i,j)})^\\gamma log L_{B(i,j)},$ (2)\nfor $B(i, j) \\in$ masking case t (t \u2208 {1,2,3}), $\u03b1_t$ is used to balance its weight according to the occurring frequency of the specific case.\nThe focusing parameter \u03b3 is designed to impose a greater penalty on hard examples. By increasing the value of \u03b3, the model places more emphasis on less frequent samples, thus improving their representation. The balancing factor $\u03b1_t$ is used to adjust the contribution of each class based on its effective number of samples. It is calculated as follows:\n$\u03b1_t = \\frac{1/P_t}{\\sum_{i=1}^3 1/P_i},$ t = 1,2,3    (3)\nwhere $P_1, P_2$, and $P_3$ refer to the expected probability for masking cases 1, 2, and 3.\nThe probability of a patch being masked is assumed to be $r$. The expected number of pairs of masked blocks for the three different cases, which are denoted as $E_1, E_2$, and $E_3$, can be computed as:\n$E_1 = \\frac{N^2}{2} \\times r^2$, $E_2 = \\frac{N^2}{2} \\times 2r \\times (1 - r)$,  $E_3 = \\frac{N^2}{2} \\times (1 - r)^2$    (4)\nThus the probabilities $P_1, P_2$, and $P_3$ for three cases are:\n$P_1 = r^2$,   $P_2 = 2r \\times (1 - r)$,   $P_3 = (1 - r)^2$ (5)\nA reconstructed sample is presented in Fig. 2, with the masking ratio during inference set to 0.25. The results indicate that regions corresponding to all three masking cases are reconstructed with high quality."}, {"title": "3.1.2 Dynamic Masking Ratio for Global Extraction", "content": "Although the reconstructed sample in Fig. 2 demonstrates high-quality reconstruction with a masking ratio of 0.25 during inference, it can be observed that the global magnitude of the spectrum is not perfectly recovered as shown in Fig. 3: when the model is trained with the same settings but evaluated with a mask ratio of 0, the reconstructed magnitude of the spectrum (Fig. 3b) exhibits inconsistencies between blocks. Additionally, we find that if the mask ratio for inference significantly varies from the mask ratio during training, the performance of spectral reconstruction can be negatively influenced.\nThis observation inspires us to introduce a dynamic masking mechanism during training, where the mask ratio is randomly varied across different batches. Specifically, we define three levels of masking: heavily masked, slightly masked, and not masked, with corresponding mask ratios $r_1, r_2$, and $r_3$ set to 0.3, 0.15, and 0.0, respectively. Within each mini-batch, the mask ratio is consistent (i.e., it is either $r_1, r_2$, or $r_3$), but the specific mask ratio used varies between different batches.\nDifferent from $L_{r\\neq 0}(X, X'|r)$ for $r_1$, $r_2$, when $r_3 = 0$:\n$L(X, X'|r_3 = 0) = \\frac{1}{N^2} \\sum_{i=0}^{N-1} \\sum_{j=0}^{N-1} L_{B(i,j)}$\n(6)\nIn the experiments, it was observed that the order of magnitude for the loss function varies with different mask ratios. To mitigate potential instability in training caused by significant fluctuations in gradient updates across batches for varying r values, we introduce a scaling factor. Specifically, we compute the expected loss $E[L(X, X'|r)]$ for each r, and then scale the individual losses $L(X, X'|r_1)$, $L(X, X'|r_2)$, and $L(X, X'|r_3)$ by the reciprocal of their respective expectations. This normalization ensures a more consistent gradient descent process, thereby enhancing the stability of the neural network's training across different mask ratio configurations.\nTo compute the expectation of the reconstruction loss, we assume that $L_{B(i,j)}$ follows a \u03c7 distribution and is independent of the scenario type t. A detailed derivation proving that $L_{B(i,j)}$ conforms to a \u03c7 distribution is provided in the Appendix A. Our goal is to determine $E[L(X, X'|r_k)]$ for k = 1,2,3, where $E[\u00b7]$ represents the expectation over the specified distribution.\nFor k = 1 and k = 2, the $r_k \\neq 0$, we acquire:"}, {"title": "3.2. Design of Architecture", "content": "We employ a Transformer-based backbone consisting of ViT-L14 blocks for the spatial branch and ViT-B16 blocks for the frequency branch. This configuration is based on experimental findings that indicate increasing the complexity of the frequency branch does not substantially enhance performance but instead introduces greater instability during training and reduces inference speed. The AdaLoRA [54] blocks are introduced in both spatial and spectral branches to dynamically fine-tune the pre-trained parameters. We utilize Gated-Multimodal-Unit (GMU) [50] for information fusion. The sequence of blocks are visualized in Fig. 4."}, {"title": "4. Quantitative Analysis of Generalization Ability for Fake Detectors", "content": "In the numerical experimentation, it is observed that the performance of fake detectors trained on fake images generated by DM, NeRF, and 3DGS is generally better than that of detectors trained on GANs. This observation aligns with the findings in [25, 45], which suggest that fake detectors trained on difficult fake samples with less domain-specific information typically perform better. Additionally, we observe that this performance gap diminishes as the representation ability of the backbone architecture increases. To provide a quantitative analysis of this phenomenon, we examine the distribution of clustering features. The experimental observations indicate that as the representation capability of the backbone architecture improves, the distances along the direction perpendicular to the optimal hyperplane for real/fake classification between fake-fake clusters become relatively small compared to the distances between real-fake clusters. This explains why the generalization ability of larger models is less sensitive to whether the detector is trained on easy or difficult fake samples. An illustration of this phenomenon is provided in Fig. 6. The figure shows that as the model's generalization capability to distinguish between the real and fake images improves, there is a notable increase in the inter-cluster distance between the fake and real feature spaces, while the subspace of all fakes remains relatively stable. This observation explains why the number of incorrectly classified samples decreases for models trained on real and easily distinguishable fake data.\nIt is assumed that the parameters of the backbone are fixed to extract the deep features of an image as $f_i \\in R^d$, we assign the labels of real data as 0 and the labels of fake data as 1, thus, the label $l_i \u2208 {0, 1}$. The features are multiplied by $U \u2208 R^{d\u00d71}$ to give the projected scores in one dimension. We consider the challenging case that unseen fake distribution does not fit well enough to the detector due to outlier data points always occurring beyond the cluster envelope, while the predicted results for real data are stable. We denote the error $e_i$ of each data point projected by the hyperplane during training:\n$e_i = l_i - z_i  f_iU$  (13)\nwhere $z_i$ = 0 if real and $z_i$ = 1 if fake. An explicit data-dependent variable $\u0398_i$ is introduced to model this small amount of large error.\nThe item $l_i - z_i f_iU$ can be regarded as soft label of unseen fake data to compute the optimal hyperplane bound without the negative influence from sparse outlier data points. Based on this model, we aim to acquire the optimal U for the best separation on the testing dataset as:\n$U^*, \u0398^* = argmin_{U, \u0398} {\\sum_{i=1}^n (l_i - z_i f_iU)^2 + \u03bb\\sum_{x_i \u2208 G_m} |\u0398^* U^*x_i-to|}$  (11)\nwhere $l_i, f_i, z_i, \u0398_i$ are stacked as $L, F, Z, O$, respectively. Given the l\u2081 penalty imposed on \u0398 to encourage the sparse solution, we can directly compute the U* after \u0398* is resolved as follows:\n$U^* = (F^TF)^\\dagger F^T (L - Z \\otimes \u0398^*)$     (15)\nwhere \u2020 denotes Moore-Penrose inverse,  denotes Kronecker product. Following [52], we define $F = F (F^TF)^\\dagger F^T$ and $L = FL$, we simplify the objective as:\n$\u0398^* = argmin_\u0398 {\\frac{1}{2}||L - F(Z \\otimes \u0398)||^2 + \u03bb||Z \\otimes \u0398||_1}$     (16)"}, {"title": "5. Datasets and Protocols", "content": "We now present the experimental setup employed for evaluating the developed method for fake image detection."}, {"title": "5.1. Introduction to Our Database", "content": "We have compiled 139 groups of consecutive 2D photos, each accompanied by the corresponding camera poses. These camera poses were either recorded during the photo capture process or calibrated using structure-from-motion techniques. To generate fake images, we employed a variety of methods capable of producing realistic 3D representations from the aforementioned data. Specifically, we utilized different NeRF-based and 3DGS-based methods, labeled from I to VIII, to generate 3D scene representations. In Tab. 1, we highlight the number of successfully reconstructed 3D scenes generated by each method in blue. Once the 3D scenes are reconstructed, they are projected back to 2D images using the same projection parameters as those estimated from the original 2D photos. These rendered 2D images are considered fake, and the numbers of the fake projected images are highlighted in red. In experiments, the original 2D photos used for reconstruction are regarded as real images."}, {"title": "5.2. Protocols for Training and Evaluation", "content": "We use four types of training datasets (A, B, C, D), containing real images and fake images respectively generated by GAN, DM, NeRF, and 3DGS to train different fake detectors in the experiments. For the details of those training datasets, please refer to Appendix C.\nFor evaluation, we assess the performance on the fake images across several categories: those generated by NeRF or 3DGS, combinations of traditional generative methods with neural rendering, editable neural rendering, digital avatars (both heads and full bodies), and synthetic video frames with realistic 3D representations. These categories are systematically evaluated from group 1 to group 11.\nWe provide the details of the protocol for training the architecture in Appendix C."}, {"title": "6. Experimental Results", "content": "We provide the results to compare the developed method to existing fake image detectors."}, {"title": "6.1. Cross-Domain Evaluation on Group 1 ~ 11", "content": "In Tab. 3, the cross-domain testing performance for different detectors trained on groups A, B, C, and D and tested on groups 1 to 11 is provided. The table also includes the average AP and average AUROC for each of the four training groups on the 11 testing groups, as well as the average AP and average AUROC for each of the 11 testing groups on the four training groups. From Tab. 3, it can be concluded that the method performs well when the spatial and spectral branches operate independently. Additionally, the design of the multimodal backbone demonstrates superior performance in detecting fake images generated by new 3D realistic methods across testing groups 1 to 11, compared to other popular fake detectors in the spatial domain, spectral domain, and multimodal domain."}, {"title": "6.2. Evaluation on Traditional Fake Image Dataset", "content": "We compare the performance of the proposed method with previous methods and provide the comparative results in Tab. 4, quantified by the average precision metric. The results demonstrate the superior performance of our method."}, {"title": "6.3. Ablation Study on the FFiT Training Strategy", "content": "The results of the ablation study for different training strategies of FFiT are provided in Fig. 8, quantified by AP and AUROC, respectively. Generally, the FFiT branch pre-trained with the developed loss function outperforms the same architecture without the developed loss function. Additionally, the adopted architecture demonstrates superior performance compared to the previous architecture used in [24] for extracting information from the magnitude spectrum of forgery images. We further analyze the contribution of the information extracted from the spectral branch to the overall performance in the Appendix B."}, {"title": "6.4. Quantitative Analysis for Generalization", "content": "From Tab. 3, it is found that the cross-domain results of the detectors trained on group A always perform worse than those trained on groups B, C, and D, which supports the claim in Sec. 4.\nFig. 7 presents three scenarios of the distribution projected onto a 2D space using t-SNE. The features in Figs. 7a to 7c are extracted by three representative fake detectors: [1], [25], and the proposed method, with their representation capabilities increasing in that order. Observations reveal that the intra-cluster distances (between fake-fake clusters) remain relatively constant, while the inter-cluster distances (between real-fake clusters) increase progressively. In Tab. 5, we present the \u03c1 values for different methods, and the values in the table support our assumption. To compute the \u03c1 in Tab. 5, we sample 2000 unseen real images from ImageNet database, and randomly sample 2000 images from training group A, B, C, D respectively to represent the clusters of these four types of fake images during training. To simulate the unseen fake subspace, we randomly sample 200 images from testing group 1 ~ 11."}, {"title": "7. Conclusions and Future Work", "content": "This study introduces a multimodal architecture that comprehensively leverages information from both spatial and spectral domains for the detection of sophisticated fake images. To pre-train the frequency branch, we address the limitations of traditional Masked Autoencoders in handling the centrosymmetric property of spectral magnitudes by proposing a novel loss function that facilitates unsupervised learning of deep representations. In the future, the Transformer architecture trained using the developed method can be extended to other tasks that require extracting information from the spectral domain, which underscores the potential of our approach to contribute to a broader range of applications beyond synthetic content detection. The multimodal architecture, which integrates spectral and spatial features, demonstrates superior cross-domain performance even when fine-tuned on a limited number of training samples. These results confirm the approach's efficacy in accurately detecting fake images generated by advanced methods for realistic 3D scenes. Moreover, this work develops a large-scale database encompassing diverse neural-rendered images, complementing existing datasets that primarily focus on GAN and DM-generated content. Additionally, we provide a quantitative framework that can quantify the relationship between cross-domain performance and the representational capabilities of deep models, thereby enhancing the understanding of detector generalization.\nThis study paves the way for further advancements in the accurate detection of neural-rendered imagery while underlining the importance of robust, generalized detection for use in real-world applications."}, {"title": "SUPPLEMENTARY MATERIAL", "content": "SUPPLEMENTARY MATERIAL"}, {"title": "A. Squared Euclidean Distance between Two Normally Distributed Vectors", "content": "In the main text section detailing the developed loss function for the frequency branch, we assume that the patch X representing the predicted magnitude of the frequency follows a normal distribution $X \\sim \u039d(\u03bc_1, \u03a3_1)$, while the patch Y representing the ground truth magnitude of the frequency follows $Y \\sim \u039d(\u03bc_2, \u03a3_2)$. Here, \u03bc\u2081 and \u00b5\u2082 denote the mean vectors, and \u03a3\u2081 and \u03a3\u2082 represent the corresponding covariance matrices.\nWhen computing the squared Euclidean distance between these vectors, we are essentially calculating $L_B = (X \u2013 Y)^T (X \u2013 Y)$. Letting $Z = X \u2013 Y$, then Z is also a multivariate normal random vector with mean $\u03bc_Z = \u03bc_1 \u2013 \u03bc_2$ and covariance matrix $\u03a3_Z = \u03a3_1 + \u03a3_2$ (assuming independence).\nThe distribution of $Z^T Z$ follows a generalized chi-squared distribution. Specifically, if $\u03bc_1 = \u03bc_2$ and $\u03a3_1 = \u03a3_2 = I$, where I is the identity matrix, then $Z^T Z$ would follow a standard chi-squared distribution with d degrees of freedom (d being the dimension of X and Y). However, when $\u03bc_1 \u2260 \u03bc_2$ or $\u03a3_1 \u2260 \u03a3_2$, the distribution of $Z^T Z$ is a noncentral chi-squared distribution.\nNoncentral Chi-Squared Distribution: For $Z^T Z$, the degrees of freedom k equals the dimension of Z, and the non-centrality parameter \u03bb is given by:\n$\u03bb = \u03bc_Z^T \u03a3_Z^{-1} \u03bc_Z$\nThus, the distribution can be written as:\n$Z^T Z \\sim \u03c7^2(k, \u03bb)$\nThe probability density function (PDF) of a noncentral chi-squared distribution with k degrees of freedom and non-centrality parameter \u03bb is given by:\n$f(x; k, \u03bb) = \\frac{1}{2}e^{-(x+\u03bb)/2} (\\frac{x}{\u03bb})^{(k/4-1/2)} I_{k/2-1}(\\sqrt{x\u03bb})$\nwhere $I_\u03bd(z)$ is the modified Bessel function of the first kind of order \u03bd.\nLet $L_B$ be a variable that follows the noncentral chi-squared distribution defined above, therefore, $E[(1 \u2013 L_B) log L_B]$ can be computed as:\n$\\int_{0}^{\\infty} (1-x) log(x) f(x; k, \u03bb) dx$"}, {"title": "B. Contribution of Spectral Branch to the Whole Multimodal Arhictecture", "content": "In Tab. I, the proportional growth in performance after introducing the spectral branch into the spatial branch is computed. For each testing group from 1 to 11, 1000 features of the images from that group, extracted using the spatial branch fine-tuned on groups A, B, C, and D, are randomly sampled, resulting in a total of 4000 features. This process is repeated using the spectral branch to acquire another 4000 features. The Maximum Mean Discrepancy (MMD) score is then computed to estimate the correlation between the features extracted from the two domains.\nFor each testing group, the introduction of the spectral branch brings varying improvements in performance. To quantify the significance of these improvements, we compute the Pearson correlation coefficient and the corresponding p-value between the MMD scores and the increase in performance. Our analysis reveals a strong relationship between these two variables, indicating that the spectral branch provides independent and complementary information to the spatial branch. The greater independence of the features extracted by the spectral branch contributes to a significant improvement in multimodal performance."}, {"title": "C. Details of the Dataset and Protocol", "content": "C. Details of the Dataset and Protocol"}, {"title": "C.1. Comparison of Our Database with Others", "content": "We list the comparison of our database with the previous popular fake detection database in Tab. II.\nNotably, due to the generation of 3D scenes and then project to 2D cost a lot and needed to be built from scratch, our generation cost evaluated by GPU hours is four times longer than GenImage [67]."}, {"title": "C.2. Details of Our Collected Dataset", "content": "We describe the details of our collected dataset in Tab. IV. From A to O, we list the sources of the 2D image groups used to generate the 3D scenes."}, {"title": "C.3. Split Protocol of the Training Dataset", "content": "A: For the real images, we randomly sample 20,000 images from each folder of {afhq, celebahq, lsun} of ArtiFact [15"}]}