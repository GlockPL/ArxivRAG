{"title": "GNN-MolKAN: Harnessing the Power of KAN to Advance Molecular Representation Learning with GNNs", "authors": ["Ruifeng Li"], "abstract": "Effective molecular representation learning is crucial for molecular property prediction and drug design. However, existing approaches struggle with limitations in insufficient annotations and suboptimal architecture design. For instance, Graph Neural Networks (GNNs) suffer from over-squashing, causing the loss of important structural details in molecules, thus impairing molecular representations. In this work, we propose a new class of GNNs, GNN-MoIKAN and its augmented variant, GNN-MolKAN+, that integrate the Kolmogorov-Arnold Networks (KAN) architecture from AI + Science into GNNs to address these challenges. Additionally, we introduce Adaptive FastKAN (AdFastKAN), an advanced KAN that offers increased stability and speed, further enhancing the performance of standard GNNs. Notably, our approach holds three key benefits: 1) Superior Performance: GNN-MolKAN and GNN-MolKAN+ demonstrate superior prediction ability, robust generalization to unseen scaffolds, and versatile transferability across different GNN architectures. 2) Efficiency: These models require less computational time and fewer parameters while matching or surpassing the state-of-the-art (SOTA) self-supervised methods. 3) Few-shot Learning Ability: GNN-MolKAN demonstrates great potential in few-shot learning scenarios, achieving an average improvement of 6.97% across few-shot benchmarks. Overall, we validate our architecture on 6 classification datasets, 6 regression datasets, and 4 few-shot learning datasets, consistently achieving highly competitive results across all of them.", "sections": [{"title": "Existing methods and their limitations", "content": "Accurately predicting the physicochemical properties of molecules (Livingstone 2000; Li et al. 2022), drug activities (Datta and Grant 2004; Sadybekov and Katritch 2023), and material characteristics (Pilania 2021; Jablonka et al. 2024) is essential for advancing drug discovery and material science. Molecular representation learning plays a crucial role in achieving these accurate predictions (Fang et al. 2022). Existing approaches to molecular representation learning can be broadly classified into two categories: Transformer-based approaches (Fabian et al. 2020; Chithrananda, Grand, and Ramsundar 2020; Rong et al. 2020; Y\u00fcksel et al. 2023), GNN-based approaches (Wieder et al. 2020; Wang et al. 2022; Fang et al. 2023; Zhang et al. 2021; Hou et al. 2022). However, the lack of sufficient labeled data remains a significant challenge in this field.\nTransformer-based Methods. The success of Transformers and Large Language Models (LLMs) in natural language processing (NLP, Vaswani et al. 2017; Brown et al. 2020; Touvron et al. 2023) and computer vision (CV, Dosovitskiy et al. 2021; Ramesh et al. 2021; Liu et al. 2024a) has spurred their application in drug discovery (Chowdhury et al. 2022; Madani et al. 2023; Y\u00fcksel et al. 2023). Notable efforts, such as MolBERT (Fabian et al. 2020), ChemBERTa (Chithrananda, Grand, and Ramsundar 2020), and its advanced version ChemBERTa-2 (Ahmad et al. 2022), enhance molecular representations by integrating BERT (Devlin et al. 2019) with SMILES (Weininger 1988). Furthermore, SELFIES offers a robust alternative to SMILES (Krenn et al. 2020), with models like SELFormer (Y\u00fcksel et al. 2023) and MolGen (Fang et al. 2024) demonstrating outstanding performance. However, despite their advancements, Transformer-based approaches are still constrained by the quadratic complexity bottleneck, where computational demands increase rapidly with input size due to the self-attention mechanism.\nGNN-based Methods. Mainstream GNN-based methods utilize the topological structure of molecules, representing atoms as nodes and bonds as edges. Most approaches rely on the message-passing mechanism (Gilmer et al. 2020). Prominent methods in this category include Graph Convolutional Network (GCN, Kipf and Welling 2017), Graph Attention Network (GAT, Veli\u010dkovi\u0107 et al. 2018), and Graph Isomorphism Network (GIN, Xu et al. 2018). Extended approaches like MolCLR (Wang et al. 2022) and KANO (Fang et al. 2023) use contrastive learning framework, while MGSSL (Zhang et al. 2021) and GraphMAE (Hou et al. 2022) employ generative pre-training strategies to learn comprehensive molecular representations. However, these methods face the challenge of over-squashing, which leads to poor long-range dependencies. Information from distant atoms is overly compressed as it passes through the network, resulting in a loss of important molecular structural details.\nCombination of Both: Graph Transformer. Several works have adapted the Transformer architecture to address the issue of long-range dependencies in GNNs. Notably, Graphormer (Ying et al. 2021), GPS(Ramp\u00e1\u0161ek et al. 2022), Grover(Rong et al. 2020), and Uni-Mol (Zhou et al. 2023) integrate message-passing networks into the Transformer architecture to enhance their expressive power. However, like other Transformer-based methods, Graph Transformer methods also suffer from high computational complexity."}, {"title": "Generalizing KAN to Molecular Graphs", "content": "In molecular representation learning, challenges arise from the scarcity and diversity of data. Given the crucial role of molecular structure, we focus on graph-based methods, which are hampered by the structural issues of over-squashing and poor long-range dependencies. Considering KAN is suitable for efficiently handling small-scale tasks with limited annotations, we introduce it in molecular representation learning to solve these challenges. The efficient fitting mechanisms of KAN can alleviate the structural deficiencies of GNN-based methods. Additionally, we propose a KAN variant, Adaptive FastKAN, which can enhance the adaptability of model to diverse data distributions.\nKolmogorov-Arnold Networks (KAN). Liu et al. (2024b) introduce Kolmogorov-Arnold Networks (KAN), a new type of network inspired by the Kolmogorov-Arnold representation theorem (KART, Arnold 2009; Kolmogorov 1957). KART allows the decomposition of complex multivariate functions into simpler single-variable components, facilitating more efficient computation. Unlike Multi-Layer Perceptrons (MLPs, Cybenko 1989; Hornik, Stinchcombe, and White 1989), which have fixed activation functions on nodes (\"neurons\"), KAN features learnable activation functions on edges (\"weights\"), parametrized as splines. This unique design improves accuracy, making KAN more efficient than MLPs in small-scale AI + Science tasks. For instance, when solving partial differential equations (PDEs), a two-layer KAN with a width of 10 achieves accuracy 100 times greater than a four-layer MLP with a width of 100 (Liu et al. 2024b). Additionally, the exceptional accuracy of KAN extends beyond PDE solving, demonstrating notable success in image recognition (Azam and Akhtar 2024), image generation (Li et al. 2024) and time series prediction tasks (Vaca-Rubio et al. 2024; Xu, Chen, and Wang 2024). To summarize, the most significant advantage of KAN is its ability to surpass SOTAs with minimal parameters, making it ideal for small-scale tasks.\nThe Benefits of Generalizing KAN to Molecular Graphs. Generalizing KAN to molecular graphs alleviates the issue of over-squashing in GNNs by utilizing its efficient fitting capabilities and high accuracy. This enhancement improves the model's ability to capture long-range dependencies, thereby retaining and utilizing crucial structures more effectively. Furthermore, KAN's efficient use of parameters allows it to outperform or match the SOTAs even with limited labeled data, making it ideal for small-scale molecular representation tasks (Figure 1). However, generalizing KAN to molecular graphs poses significant challenges. A detailed discussion of these challenges is provided in Section ."}, {"title": "Contributions", "content": "Our contributions are listed as follows:\n\u2022 We integrate the efficient parameter usage and powerful data-fitting capabilities of KAN architecture with GNNs, which can overcome the limitations of insufficient annotations and suboptimal architecture design.\n\u2022 We propose the Adaptable FastKAN (AdFastKAN), which introduces the learnable RBFs as base function. This modification not only effectively addresses the slow speed of the original KAN but also significantly enhances the model's adaptability to diverse data distributions.\n\u2022 Our GNN-MolKAN demonstrates exceptional predictive power and robust generalization ability on molecular property prediction datasets, while also requiring less time and memory to surpass or match the SOTAS.\n\u2022 We validate the effectiveness of our model in solving few-shot learning problem on few-shot benchmarks. Our augmented models achieve an average improvement of 6.97% across four datasets."}, {"title": "Generalization Challenges", "content": "How to Use KAN to Handle Non-Euclidean Molecular Graphs? One notable geometric characteristic that distinguishes molecular graphs from conventional structured data is their irregularity: the number of nodes and edges varies between them. Despite KAN's exceptional performance on Euclidean data, its original design limits its direct application to graph-structured data. To overcome this limitation, we combine the powerful approximation capabilities of KAN with the structural advantages of GNNs. Specifically, we utilize GNNs to transform molecular graphs into high-dimensional representations in Euclidean space and employ aggregation functions to combine these node representations. Subsequently, KAN serves as the update function, using learnable activation functions to accommodate the varying importance of different nodes. This flexible adjustment of node representations allows the model to better adapt to diverse datasets and tasks.\nHow to Address Slow Computation Speeds in KAN? Despite the power of KAN, integrating B-splines (Unser, Aldroubi, and Eden 1993) in its learnable activation functions increases computational complexity, leading to longer training time due to the calculation of piecewise polynomials (Liu et al. 2024b). To improve training efficiency, previous works have replaced B-splines with simpler base functions (Li 2024; Ta 2024; Qiu et al. 2024). Inspired by FastKAN (Li 2024), we adopt RBFs (Tao 1993; Orr et al. 1996) to replace B-splines due to their simple formula and lower memory overhead, thereby accelerating the computational speed of the model. However, the fixed centers and bandwidths in RBFs may not adapt well to the data distribution. The small bandwidth causes the RBFs function to respond very locally, leading to overfitting, while the large bandwidth results in overly smooth responses, causing underfitting. Both issues can degrade the model's performance (Tenne 2022).\nHow to Enhance the Adaptability of RBFs to Data Distribution? Unlike prior works (Tao 1993; Orr et al. 1996) that employ fixed bandwidth and centers, we introduce Adaptive FastKAN (AdFastKAN), which parameterizes the bandwidths and centers of RBFs. This parameterization allows our model to dynamically adapt to the different data distributions, enhancing its flexibility. The parameterized RBFs can adjust their shape and position more effectively, increasing the model's representational power. This helps overcome the bottlenecks of GNNs and efficiently capture complex molecular structures. Additionally, the dynamic adaptation of bandwidths and centers accelerates the convergence of the model and helps find the global optimum. To further enhance adaptability to complex data patterns, we replace the original classifier (i.e., MLPs) in GNN-MolKAN with AdFastKAN. The resulting architecture is called GNN-MoIKAN+."}, {"title": "GNN-MolKAN Architecture", "content": "The framework of GNN-MolKAN is illustrated in Figure 2. This section focuses on the implementation details of the network architecture. These details are crucial for ensuring the speed and reliability of the network.\nNotation. Let G = (V, E) be a molecular graph, where V represents the set of nodes and E represents the set of edges. The number of nodes is N = |V|. The features of node vi are represented by hi. The features of the edge between nodes vi and vj are represented by bij. Let z be the graph-level representation, \u0177 be the graph-level prediction and y be the molecular property label (i.e., the ground-truth label).\nPreliminary\nKolmogorov-Arnold Representation Theorem (Arnold 2009; Kolmogorov 1957; Braun and Griebel 2009) states that any multivariate continuous function on a bounded domain can be expressed as a finite composition of continuous single-variable functions and addition operations. Formally, for a smooth function f : [0,1]n \u2192 R with n variables X = x1,x2,..., xn, the theorem guarantees the existence of such a representation:\n$$f (x_1,...,x_n) = \\sum_{q=1}^{2n+1} \\Phi_q\\Big(\\sum_{p=1}^{n} \\varphi_{q,p} (x_p)\\Big).$$\nwhich involves two summations: an outer summation $\\sum_{q=1}^{2n+1}$ and an inner summation $\\sum_{p=1}^{n}$. The outer summation $\\sum_{q=1}^{2n+1}$ sums 2n + 1 terms of \u03a6q : R \u2192 R. Within this, the inner one sums n terms for each q, where each term \u03c6q,p: [0,1] \u2192 R represents a continuous function of the single variable xp.\nThe Design of KAN. Unlike the simple 2-layer KAN described by Eq. 1, which follows a [n, 2n + 1,1] structure, KAN can be generalized to arbitrary widths and depths. Specifically, KAN consists of L layers and generates outputs based on the input vector x \u2208 Rn:\n$$KAN(x) = (\\Phi_{L-1} \\circ \\Phi_{L-2} \\circ \\cdots \\circ \\Phi_1 \\circ \\Phi_0) x,$$\nwhere \u03a6\u03b9 represents the function matrix of the lth layer. Each layer of the KAN, defined by the function matrix \u03a6\u03b9 is formed as follows:\n$$x_{l+1} =\\begin{pmatrix}\\Phi_{l,1,1} (\\cdot) & \\Phi_{l,1,2} (\\cdot) & \\cdots & \\Phi_{l,1,n_l} (\\cdot) \\\\\\Phi_{l,2,1} (\\cdot) & \\Phi_{l,2,2} (\\cdot) & \\cdots & \\Phi_{l,2,n_l} (\\cdot) \\\\\\vdots & \\vdots & & \\vdots \\\\\\Phi_{l,n_{l+1},1}(\\cdot) & \\Phi_{l,n_{l+1},2}(\\cdot) & \\cdots & \\Phi_{l,n_{l+1},n_l} (\\cdot)\\end{pmatrix} x_l,$$\nwhere \u03a6\u03b9,j,i denotes the activation function connecting neuron i in layer l to neuron j in layer l+1. The network uitilizes ni \u00d7 n1+1 such activation functions between each layer.\nKAN enhances its learning capability by utilizing residual activation functions that integrate a basis function and a spline function:\n$$\\phi(x) = w_b b(x) + w_s spline (x),$$\nwhere b(x) is the silu(x) serving as the basis function, and the spline function is a linear combination of B-splines. The wb and ws are learnable parameters.\nThe Advanced Version of KAN: FastKAN. Li (2024) introduces FastKAN, a new implementation of KANS that significantly accelerate model computations. FastKAN achieves this by utilizing radial basis functions (RBFs) with Gaussian kernels instead of traditional B-spline functions. The RBF is defined as:\n$$\\phi(x) = w_b b(x) + w_r exp\\Big(-\\frac{||x-c||^2}{2}\\Big),$$\nwhere wr represents learnable parameters. In this context, r is the Euclidean distance between the input x and a center c, scaled by a parameter k which controls the width of the Gaussian function:\n$$r = \\frac{||x-c||}{k}$$\nArchitecture\nMolecular representation learning with GNNs generally involves three steps: Aggregation, Update, and Readout operations. We propose a new class of GNNs that leverage the strengths of KAN for feature updating. To address KAN's limitations in computational efficiency and adaptability, we introduce AdFastKAN, a variant of KAN with learnable RBFs as activation functions. AdFastKAN adaptively adjusts its parameters based on the data distribution, enabling effective modeling of molecules with diverse scaffolds. By integrating AdFastKAN with GNNs, our model captures intricate molecular features more effectively, improving both computational efficiency and prediction performance."}, {"title": "Step 1: Aggregation Operation", "content": "In a molecular graph G = (V,E), let h\u2070 \u2208 Rn represents the initial node features and b \u2208 Rn represents the initial edge features. At the [th layer, the GNN utilizes an aggregation function to combine information from the neighboring nodes of v, forming a new message representation m\u2208Rn:\n$$m_v^{(l)} = AGGREGATE^{(l)} \\Big(\\{(h_u^{(l-1)}, h_v^{(l-1)}, b_{uv}) | u \\in N(v)\\} \\Big)$$\nwhere the $m_v^{(l)}$ represents the aggregated message of node v at layer l, and u \u2208 N(v) denotes the set of neighboring nodes of node v."}, {"title": "Step 2: Update Operation", "content": "After computing the aggregated message $m_v^{(l)}$ for node v, the message is used to update the representation of node v at layer l. Notably, The update function employed is KAN, which consists of a single layer:\n$$h_v^{(l)} = UPDATE^{(l)}(h_v^{(l-1)}, m_v^{(l)}).$$ \nUnlike the approach in Eq. 5, we design a learnable data center $c_w^{(l)}$ and a learnable bandwidth k to efficiently adapt to the data distribution. This enhanced version, termed Adaptive FastKAN (AdFastKAN), ensures robustness and optimizes data alignment relative to $c_w^{(l)}$, thereby improving model flexibility and performance. The update function can be rewritten as follows:\n$$h_v^{(l)} = AdFastKAN^{(l)} \\Big((1+\\epsilon) \\cdot h_v^{(l-1)} + m_v^{(l)} \\Big)$$\nwhere $\\epsilon^{(l)}$ is a parameter. Specifically, the base function of AdFastKAN is defined as:\n$$\\phi(x) = w_b b(x) + w_r exp\\Big(-\\frac{||x-c_w^{(l)}||^2}{2k_w^2} \\Big),$$\nwhere x = (1 + \u0454) \u00b7 h\u2082 + mv.\nAdFastKAN's adaptive parameter adjustment allows it to effectively handle the varied nature of molecular structures, leading to a more robust and adaptable model with improved generalization across diverse molecular datasets."}, {"title": "Step 3: Readout Operation", "content": "After performing Steps 1 and 2 for Literations, we use the MEAN function as the readout mechanism to generate the molecular representation. The readout operation is defined as:\ng = READOUT ({h(L) | \u03bd\u03b5\u03bd}),\nwhere g \u2208 Rn represents molecular representation."}, {"title": "Step 4: Prediction", "content": "Next, we leverage an MLP to obtain the final prediction \u0177:\n\u0177 = MLP (g) \u2208 R (regression) or Rnc (classif.),\nwhere nc is the number of classes in the molecular property classification task. For an advanced version, GNN-MolKAN+, we utilize AdFastKAN as the classifier:\n\u0177 = AdFastKAN (g) \u2208 R (regression) or Rnc (classif.)."}, {"title": "Experiments", "content": "We evaluate our GNN-MoIKAN and GNN-MolKAN+ on a wide range of public molecular property prediction benchmarks. The evaluation includes: 1) Molecular Classification Datasets: BBBP, Tox21, ToxCast, SIDER, HIV, BACE. 2) Molecular Regression Datasets: Lipo, FreeSolv, Esol, QM7, QM8, QM9. 3) Few-shot Learning Datasets: Tox21, SIDER, MUV, ToxCast from PAR (Wang et al. 2021a). We utilize ROC-AUC as the evaluation metric for the classification task and the mean absolute error (MAE) for the regression task, reporting the mean and standard deviation during the test phase across five independent runs. Our model employs a 2-layer GNN-MolKAN as the encoder and a 2-layer AdFastKAN (GNN-MolKAN+) or MLP as the classifier. Details are provided in Appendix ??. All the experiments are run on an NVIDIA RTX A6000 GPU."}, {"title": "Classification Tasks", "content": "Compared with Base MP-GNNs. Table 1 summarizes the performance of three base MP-GNNs (GCN, GAT, GINE) and their augmented versions with MolKAN and MolKAN+ across six classification benchmarks. Superior Prediction Ability: The augmented models (GNN-MolKAN and GNN-MolKAN+) outperform the base MP-GNNs in most benchmarks. Specifically, GCN-MolKAN improves by 1.60% and GCN-MolKAN+ by 2.90%. GAT-MolKAN improves by 2.00% and GAT-MolKAN+ by 3.70%. GINE-MolKAN improves by 3.20% and GINE-MolKAN+ by 4.80%. Robust Generalization Ability: To evaluate the generalization ability of our models, we adopt the scaffold splitting method, which ensures that the test set scaffolds are unseen during training. The results show that the augmented models perform well on these new scaffolds. For instance, GINE-MolKAN outperforms GINE by 25.49% on the BACE dataset, indicating strong generalization. Versatile Transferability: The augmented models demonstrate strong performance across different base MP-GNNs, showcasing their adaptability and transferability.\nCompared with SOTAs. Figure 3 compares the best results of the augmented models from Table 1 with the SOTAs on three classification datasets. The baselines are divided into two categories: 1) Supervised methods: GCN (Kipf and Welling 2017), GAT (Veli\u010dkovi\u0107 et al. 2018), GINE (Hu* et al. 2020) and CMPNN (Song et al. 2020); 2) Self-supervised methods: MolCLR (Wang et al. 2022), MGSSL (Zhang et al. 2021) and GraphMAE (Hou et al. 2022). Additionally, we compare our models with Graph Transformer methods in Appendix ??. Outstanding Prediction Ability: GNN-MolKAN and GNN-MolKAN+ outperform or match the SOTAs in all datasets. Lower Model Complexity: GNN-MolKAN and GNN-MolKAN+ have fewer parameters (~563K and ~565K parameters) compared to models like GraphMAE (~1363K) and CMPNN (~1564K), while maintaining similar performance. Faster Training Speed: GNN-MolKAN and GNN-MolKAN+ exhibit faster training time per epoch. In contrast, models with more parameters, such as GraphMAE and CMPNN, require significantly longer training time. This implies that GNN-MolKAN and GNN-MolKAN+ can complete training in a shorter time, enhancing efficiency for practical applications."}, {"title": "Regression Tasks", "content": "Table 2 compares the base MP-GNNs and their augmented versions (GNN-MolKAN and GNN-MolKAN+) on six regression benchmarks in physical chemistry and quantum mechanics. The augmented models consistently outperform their base counterparts across most datasets, demonstrating the effectiveness of AdFastKAN. Remarkable advancements are observed in the Lipo, FreeSolv, and QM9 datasets. Specifically, GCN-MolKAN and GINE-MolKAN+ improve by 13.80% and 32.50% on the Lipo dataset, and by 34.70% and 17.80% on the FreeSolv dataset. Similarly, on the QM9 dataset, GINE-MolKAN and GCN-MolKAN+ exhibit improvements of 27.90% and 16.40%, respectively. In summary, these results underscore the potential of GNN-MolKAN and GNN-MolKAN+ to enhance the accuracy and reliability of GNNs in regression tasks within the physical chemistry and quantum mechanics domains."}, {"title": "Few-shot Learning Tasks", "content": "Table 3 demonstrates the effectiveness of MolKAN in addressing the few-shot learning problem. We select four common baselines: TPN (Liu et al. 2018), MAML (Finn, Abbeel, and Levine 2017), ProtoNet (Snell, Swersky, and Zemel 2017) and PAR (Wang et al. 2021b). All experiments are conducted in a 2-way 10-shot setting. In Table 3, the augmented models consistently outperform their base counterparts across most datasets, with average improvements of 9.14% for TPN-MolKAN, 5.04% for MAML-MolKAN, 2.02% for ProtoNet-MolKAN, and 11.67% for PAR-MoIKAN. These augmentations with the MolKAN architecture result in higher ROC-AUC values, improving the accuracy and reliability of few-shot molecular property prediction methods."}, {"title": "Visualization", "content": "Figure 4 presents the t-SNE visualization of the performance of GIN and GIN-MolKAN+ on the BACE dataset. The left plot shows that GIN struggles with classification, failing to learn distinct features for different molecules. In contrast, the right plot, displaying the data distribution after using the GIN-MolKAN+ model, reveals a clearer separation between the green and orange dots, with a noticeable boundary between them. This indicates that the GIN-MolKAN+ model significantly improves classification performance, better distinguishing between different molecules."}, {"title": "Ablation Study", "content": "We conduct ablation studies to evaluate the various choices made during the implementation of each component of the architecture. First, we examine the effectiveness of AdFastKAN and its position within GNNs. Additionally, Appendix ?? presents the impact of different layers of MolKAN. Furthermore, Appendix ?? explores the layers of AdFastKAN, and Appendix ?? details the hyperparameter choices for MolKAN.\nThe Effectiveness of AdFastKAN. In our ablation studies, we systematically evaluate various implementation choices across each component of our architecture. Figure 5 shows the performance comparison of the base model and its KAN variants across three datasets. GIN-MolKAN shows notably superior performance over both the base model and other variants (GINE-KAN and GINE-FastKAN). The adaptive bandwidth and centers of AdFastKAN improve enhance its ability to data distribution, thereby improving flexibility. Additionally, Figure 6 depicts the computational efficiency analysis. Compared to the base model (GINE), GINE-KAN incurs significantly higher computation time per epoch overhead. In contrast, GINE-FastKAN and GINE-MolKAN (our model) exhibit computational efficiencies similar to GINE. This highlights that while GINE-KAN experiences lower computational efficiency, GINE-FastKAN and GINE-MolKAN perform comparably to the GINE baseline. Further details can be found in Appendix ??\nThe Position of AdFastKAN. Table 4 compares the performance of GCN models with AdFastKAN integrated in different functions (Aggregation and Update functions), evaluated on BBBP and HIV datasets. Results are presented as mean and standard deviation of test ROC-AUC from five runs. The results demonstrate that the integration of Ad-FastKAN in the update function (GCN-MolKAN (Our)) appears to be more beneficial than its integration in the aggregation function (GCN-MolKAN (Agg.))."}, {"title": "Conclusion", "content": "In this work, we proposed a novel GNN architecture to improve graph-based molecular representation learning methods, and presented promising results on several benchmark molecular property prediction dataset. Future work will focus on further exploring Graph Transformer architecture with KAN to solve its high computational complexity issue which maybe further improve molecular representation learning tasks."}]}