{"title": "RETQA: A Large-Scale Open-Domain Tabular Question Answering Dataset for\nReal Estate Sector", "authors": ["Zhensheng Wang", "Wenmian Yang", "Kun Zhou", "Yiquan Zhang", "Weijia Jia"], "abstract": "The real estate market relies heavily on structured data, such\nas property details, market trends, and price fluctuations.\nHowever, the lack of specialized Tabular Question Answering\ndatasets in this domain limits the development of automated\nquestion-answering systems. To fill this gap, we introduce\nRETQA, the first large-scale open-domain Chinese Tabular\nQuestion Answering dataset for Real Estate. RETQA com-\nprises 4,932 tables and 20,762 question-answer pairs across\n16 sub-fields within three major domains: property informa-\ntion, real estate company finance information and land auc-\ntion information. Compared with existing tabular question\nanswering datasets, RETQA poses greater challenges due\nto three key factors: long-table structures, open-domain re-\ntrieval, and multi-domain queries. To tackle these challenges,\nwe propose the SLUTQA framework, which integrates large\nlanguage models with spoken language understanding tasks\nto enhance retrieval and answering accuracy. Extensive ex-\nperiments demonstrate that SLUTQA significantly improves\nthe performance of large language models on RETQA by in-\ncontext learning. RETQA and SLUTQA provide essential re-\nsources for advancing tabular question answering research in\nthe real estate domain, addressing critical challenges in open-\ndomain and long-table question-answering. The dataset and\ncode are publicly available at https://github.com/jensenw1/\nRETQA.", "sections": [{"title": "Introduction", "content": "With rapid advancements in artificial intelligence and nat-\nural language processing, Tabular Question Answering\n(TQA) has garnered attention for its ability to extract ac-\ncurate answers from structured data across domains like fi-\nnance and healthcare (Zhu et al. 2024). Despite progress,\nspecialized datasets for the real estate domain are scarce,\nlimiting research and practical applications. The real estate\nmarket relies on structured data, such as property details and\nmarket trends, which are crucial for stakeholders like home-\nbuyers and investors. For instance, homebuyers can make\ninformed decisions by querying historical transaction prices\nand property details, while investors can assess the viability\nof investments by querying land information, asset status,\nand the developer's transaction history. However, the lack of\ntailored TQA datasets in this field hampers the development\nof automated QA systems.\nTo address this gap, we introduce RETQA, the first large-\nscale open-domain Chinese TQA dataset specifically de-\nsigned for the Real Estate sector. RETQA is built from pub-\nlicly available real estate data and comprises 4,932 tables\nand 20,762 QA pairs spanning 16 sub-fields within three\nmajor domains: property information, real estate company\nfinance, and land auction information. Given the complex\nand data-intensive nature of the real estate market, RETQA\nis crafted to generate queries targeting longer tables, with\nan average of 252.9 rows per query-related table. Moreover,\nqueries in RETQA may related to more than one table. The\nabove design captures the real estate domain's complexity,\nmaking RETQA a challenging TQA dataset.\nThe open-domain nature of RETQA requires models to\nretrieve relevant tables from the entire dataset, rather than\nbeing directly provided, effectively mirroring real-world\nscenarios. However, this process makes open-domain TQA\nmore challenging than closed-domain, as retrieval accuracy\ndirectly affects overall TQA performance. To facilitate re-\ntrieval, RETQA assigns a summary caption to each table.\nFurthermore, RETQA integrates the labels of Spoken Lan-\nguage Understanding (SLU) for each query. These SLU la-\nbels, including intent and slot labels, commonly employed\nin task-oriented dialogue systems (Qin et al. 2021; Cheng,\nYang, and Jia 2023; Zhu et al. 2024; Qin et al. 2024), are\ninstrumental in discerning user query intent and extracting\npertinent details. As depicted in Figure 1, SLU labels en-\nable more accurate parsing of user intent and relevant infor-\nmation, thereby enabling more precise retrieval and accu-\nrate answering. For each query, RETQA provides intent la-\nbels, slot labels, table captions, and answers in Markdown,\nSQL-style, and natural language formats. To the best of our\nknowledge, RETQA is the first TQA dataset to integrate\nSLU labels.\nRETQA is created through four key steps: First,\nwe compile a dataset of 4,932 real estate-related ta-\nbles from eight major Chinese cities-Beijing, Shanghai,\nGuangdong, Shenzhen, Suzhou, Hangzhou, Nanjing, and"}, {"title": "Related Works", "content": "Our work is related to two areas, i.e., TQA datasets, and\nTQA methods."}, {"title": "TQA Datasets", "content": "Early research, such as WikiTableQuestions (Pasupat and\nLiang 2015) and WikiSQL (Zhong, Xiong, and Socher\n2017), collected tabular data from web sources like\nWikipedia. However, WikiTableQuestions only provides\ntext answers, and WikiSQL's question descriptions are too\nvague to reliably locate relevant tables. The Spider dataset\n(Yu et al. 2018), designed for complex, cross-domain Text-\nto-SQL tasks, is not suited for open-domain scenarios and\nonly offers SQL-format answers. NQ-TABLES (Herzig\net al. 2021), the first open-domain table QA dataset, uses\na reader model to extract answers from K candidate tables\nby selecting a single cell, limiting the need for complex"}, {"title": "TQA Methods", "content": "Before the emergence of large language models (LLMs), re-\nsearchers explored methods to combine tabular data with\nneural networks for natural language processing and data\nmanagement tasks (Badaro, Saeed, and Papotti 2023; Fang\net al. 2024; Shwartz-Ziv and Armon 2022). LLMs gained\nattention for their strong cross-task generalization, adapt-\ning to new tasks with minimal examples. (Chen 2023) first\ndemonstrated LLMs' ability to reason over tables through\nin-context learning. TAP4LLM (Sui et al. 2023) addresses\nnoisy table data by enhancing sub-tables. OPENTAB re-\ntrieves tables, generates SQL as intermediate steps, and re-\nlies on a reader for final answers, though it may produce\nincorrect SQL (Kong et al. 2024). StructGPT (Jiang et al.\n2023) uses interfaces to handle large structured data effi-\nciently. MultiTabQA (Pal et al. 2023) introduces an LLM-\nbased framework for answering questions across multiple\ntables, but it is limited to queries in closed domains. In com-\nparison, our novel integration of LLMs with SLU tasks sig-\nnificantly enhances open-domain TQA accuracy."}, {"title": "Dataset Construction and Analysis", "content": "This section elaborates on how RETQA is created, including\ndetails on table collection, QA pair generation, intent and\nslot annotation, and query rewriting and quality control."}, {"title": "Table Collection", "content": "Our table data comes from publicly available real-world\nsources, including property data, real estate company finan-\ncial data, and land auction data. The specific data sources\nand descriptions are as follows:\nProperty Data: We collected commercial housing\ntransaction data from eight major Chinese cities\n(Beijing, Shanghai, Guangzhou, Shenzhen, Suzhou,\nHangzhou, Nanjing, Wuhan) for 2019-2022, sourced from\nhttp://www.fangdi.com.cn/. The tables, keyed by develop-\nment name, include district, average transaction prices, year\nand month of transaction dates, number of transactions,"}, {"title": "QA Pair Generation", "content": "We utilize a template-based approach to generate QA pairs\nautomatically. Specifically, we define 90 query templates\ncovering various types of inquiries, including factual, infer-\nential, and comparative questions, derived from extracted\ntables and create corresponding SQL templates for each\nquery. Among these, 23 templates are designed to generate\nqueries that require multi-table support. Given the complex"}, {"title": "Intent and Slot Annotation", "content": "To facilitate researchers\neffectively extracting key information from queries and\nachieve accurate retrieval and answers, RETQA includes\nadditional Spoken Language Understanding (SLU) labels.\nSLU consists of two types of labels: intent and slot labels,\ncommonly used in task-oriented dialogue systems. Intent la-\nbels identify the user's query intent, while slot labels extract\nkey information pertinent to that intent.\nWe annotate the intent and slot labels through reverse en-\ngineering. Specifically, for intent labels, we categorized the\nqueries into 16 types based on the 90 templates mentioned\nearlier. Some queries in the templates may involve multi-\nple intents. For example, in the query, \"Which of the top 5\nbest-selling residential compounds in Bao'an District, Shen-\nzhen, has the lowest housing price\", both average price and\nsales volume information are required. In this case, the query\nwould be labeled with two intents: \"real estate project sales\nvolume query\" and \"real estate project average transaction\nprice query.\" For more details and statistics about the intent\nlabels, please see Appendix C.\nFor slot labels, we categorize the entities into six types:\n\"city\", \"district\u201d, \u201cdevelopment name\u201d, \u201ccompany name\",\n\"year\", and \"month',' based on the entity types in the query\ntemplates and corresponding table headers. Following pre-\nvious work on SLU tasks (Qin et al. 2022), we adopt the\nInside-Outside-Beginning (IOB) tagging format (Ramshaw\nand Marcus 1999). In this format, the B- prefix indicates the\nbeginning of a slot chunk, the I- prefix indicates that the tag\nis inside a slot chunk, and the O tag indicates that a token\ndoes not belong to any slot chunk. An example of SLU la-\nbels is shown in Figure 1."}, {"title": "LLM-based Query Rewriting and Quality Control", "content": "De-\nspite our extensive template library, the sentences generated\nfrom these templates can be grammatically monotonous,\ndiffering significantly from real human queries. This dis-\nparity may cause models trained on template-based data to\nstruggle with actual user queries. To address this, we use\nlarge language models (LLMs) to rewrite the queries gen-\nerated by templates, creating more diverse and human-like\nexpressions while retaining their original meaning. Tech-"}, {"title": "Method", "content": "In this section, we introduce the SLUTQA framework,\nwhich consists of three key modules: the SLU module, the\nSLU label-based Retrieval (SR) module, and the SLU label-\nbased Filtering-Answer (SFA) module. SLUTQA aims to\nenhance the performance of LLMs on TQA tasks through\nSLU labels-based in-context learning (ICL). The general\nframework of SLUTQA is shown in Figure 2."}, {"title": "SLU Module", "content": "SLU involves two label types, i.e., intent and slot, commonly\nused in task-oriented dialogue systems. Intent labels iden-\ntify the user's query intent, while slot labels extract relevant\nkey information. In this paper, we design a SLU module to\npredict the SLU labels of each query, and leverage these la-\nbels to enhance information extraction and ensure accurate\nretrieval and answers within the SLUQTA framework.\nBased on the availability of labeled SLU data, we design\ntwo approaches to implement the SLU module: (1) fine-\ntuning a BERT model (Devlin et al. 2019) when sufficient\nlabeled data are available, and (2) utilizing ICL with LLMs\nin scenarios where labeled data are limited (few-shot).\nIn the first scenario, given the current query $X =\\{x_1,..., x_n\\}$ as input, we utilize a pre-trained BERT as\nthe encoder. The model returns the hidden states $H =\\{h_{cls},h_1,...,h_n \\in R^{d_{model}}\\}$, where CLS is a special to-\nken representing the entire input sequence within BERT, and\n$d_{model}$ is the output dimension of BERT.\nThen, we predict the intent labels, number of intents, and\nslot labels by:\n$y^I = Sigmoid(W^I \\cdot h_{cls} + b^I),$\n(1)\n$y^N = Softmax(W^N \\cdot h_{cls} + b^N),$\n$y^S = Softmax(W^S \\cdot h_j + b^S),$"}, {"title": "SR Module", "content": "Although our dataset provides corresponding tables (i.e.,\ngold tables) for each query, in real-world open-domain sce-\nnarios, these tables must be retrieved based on the query, and\nthe retrieval accuracy directly affects overall TQA perfor-\nmance. Previous works (Kong et al. 2024) often use BM25\n(Robertson and Zaragoza 2009) for table retrieval due to\nits scalability and competitive performance. However, in the\nreal estate data scenario, user queries can be diverse and non-\nstandard, while table captions are often similar (e.g., differ-\ning only by years or months). This means that relying solely"}, {"title": "SFA Module", "content": "After retrieving the corresponding tables from the SR mod-\nule, we designed the SFA module to generate final answers\nin two formats: SQL-style and markdown.\nIn markdown formats, existing methods typically flatten\nentire tables into a single row for processing by large lan-\nguage models (LLMs). However, RETQA includes many\nlengthy tables, and current LLMs, with their limited input\nlengths (usually 32k tokens), struggle to effectively handle"}, {"title": "Experiments", "content": "In this section, we first introduce the experiment setup.\nThen, we show the experiment results and conduct ablation"}, {"title": "Experimental Settings and Baselines", "content": "Evaluation Metrics: In this paper, we provide answers to\neach query in three formats: Markdown, SQL-style, and nat-\nural language. However, since natural language output is\nsubjective, we focus on evaluating the two objective formats,\nMarkdown and SQL-style.\nFor SQL-style answers, we assess performance using Ex-\necutable Code Ratio (ECR) and Pass Rate (pass@1), as out-\nlined in (He et al. 2024). For Markdown answers, we evalu-\nate using Table Exact Match accuracy (Table EM) and Preci-\nsion (P), Recall (R), and F1 score for exact matches of rows,\ncolumns, and cells, following the metrics described in (Pal\net al. 2023). Additionally, for table retrieval performance,\nwe also employ Precision, Recall, and F1 score as evalua-\ntion metrics.\nBaselines: To prove that our designed SLUTQA could en-\nhance the performance of existing LLMs on the RETQA\ndataset, we utilize our framework on three GPT family mod-\nels, i.e., Qwen2-7b, Qwen2-72b (Yang et al. 2024), and\nGlm4-9b (GLM et al. 2024)."}, {"title": "Result and Analysis", "content": "Main Results: To evaluate the performance of SLUTQA on\nRETQA, we generated markdown and SQL-style answers\nusing baseline LLMs through two approaches: the vanilla\nimplementation and our SLUTQA framework. In the vanilla\nimplementation, we generated a query summary for BM25\nretrieval and then produced the final answers using ICL with\nfive randomly selected examples. The key difference is that\nthe vanilla approach does not include SLU labels and lacks\na simplification step for markdown formatting."}, {"title": "Ablation Study:", "content": "We conducted an ablation study to evaluate\nthe effectiveness of the SR and SFA modules in SLUTQA.\nFirst, we evaluated the SR module's impact on retrieval\nperformance, as shown in Table 4. We compared several\nmethods: BM25, which retrieves the top 1 table based on the\noriginal query; the vanilla approach, which generates query"}, {"title": "Conclusion", "content": "In this paper, we introduce RETQA, the first large-scale\nopen-domain Chinese TQA dataset for the real estate sector,\ncomprising 4,932 tables and 20,762 QA pairs. RETQA ad-\ndresses the lack of specialized datasets by providing a chal-\nlenging resource for TQA research, particularly in handling\nopen-domain, long tables and multi-domain queries. To en-\nhance performance, we developed the SLUTQA framework,\nwhich leverages LLMs and SLU labels for improved re-\ntrieval and answer generation. Extensive experiments show\nthat SLUTQA significantly boosts LLM performance, offer-"}, {"title": "Appendix", "content": "A. Template Filling\nWe define 16 types of intents based on the column head-\ners from collected real estate tables in three major domains:\nproperty information", "two\nscenarios": "single-table and multi-table queries. For single-\ntable queries", "components": "a query template and an SQL template,\nboth requiring specific variables to be filled. In the query\ntemplate, shaded areas indicate positions to be filled with\nvariables. For instance, the blue-shaded {project_name_1}\nand {project_name_2} represent project names, while the\nyellow-shaded {city_district} indicates a city district. The\nunshaded tokens are fixed components of the query tem-\nplate. The SQL template follows a similar structure, with\ncorresponding shaded areas for variables.\nAll variables are specifically selected from the collected\nreal estate tables. The example in Figure 3 pertains to land\nauction information. We first extract all city districts from\nrelevant table captions to create a set of city districts. We\nthen randomly select one city district and insert it into the\n{city_district} slots in both the query and SQL templates.\nAfter establishing the city district, the corresponding ta-\nble caption is determined. We then randomly select two\nproject names from the target table and insert them into\nthe {project_name_1} and {project_name_2} positions in\nboth templates, generating the final question and its SQL\nquery.\nThe intent is determined by the fixed phrases in the tem-\nplate, while the slot information is labeled using the variable\nnames. Finally, we input the SQL query execution results\nalong with the original query into a LLM to generate a natu-\nral language answer. This process completes the population\nof a single template pair. After extensive sampling, we filter\nout valid QA pairs with non-empty SQL execution results\nfor further use.\nA complete example of RETQA is illustrated in Figure\n4. For each query, we provide both the intent label(s) and\nthe slot labels, utilizing the BIO tagging format for the\nslots. Additionally, standardized table captions are provided\nto facilitate the execution of retrieval tasks. All standard\nSQL statements have been verified for correct execution and\nare accompanied by their corresponding execution results\n(SQL_result). The markdown_answer can be used for end-"}]}