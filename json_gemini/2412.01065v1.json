{"title": "Lookahead Counterfactual Fairness", "authors": ["Zhiqun Zuo", "Tian Xie", "Xuwei Tan", "Xueru Zhang", "Mohammad Mahdi Khalili"], "abstract": "As machine learning (ML) algorithms are used in applications that involve humans, concerns have arisen that these algorithms may be biased against certain social groups. Counterfactual fairness (CF) is a fairness notion proposed in Kusner et al. (2017) that measures the unfairness of ML predictions; it requires that the prediction perceived by an individual in the real world has the same marginal distribution as it would be in a counterfactual world, in which the individual belongs to a different group. Although CF ensures fair ML predictions, it fails to consider the downstream effects of ML predictions on individuals. Since humans are strategic and often adapt their behaviors in response to the ML system, predictions that satisfy CF may not lead to a fair future outcome for the individuals. In this paper, we introduce lookahead counterfactual fairness (LCF), a fairness notion accounting for the downstream effects of ML models which requires the individual future status to be counterfactually fair. We theoretically identify conditions under which LCF can be satisfied and propose an algorithm based on the theorems. We also extend the concept to path-dependent fairness. Experiments on both synthetic and real data validate the proposed method\u00b9.", "sections": [{"title": "Introduction", "content": "The integration of machine learning (ML) into high-stakes domains (e.g., lending, hiring, college admissions, healthcare) has the potential to enhance traditional human-driven processes. However, it may introduce the risk of perpetuating biases and unfair treatment of protected groups. For instance, the violence risk assessment tool SAVRY has been shown to discriminate against males and foreigners (Tolan et al., 2019); Amazon's previous hiring system exhibited gender bias (Dastin, 2018); the accuracy of a computer-aided clinical diagnostic system varies significantly across patients from different racial groups (Daneshjou et al.,"}, {"title": "Related Work", "content": "Causal fairness has been explored in many aspects in recent years' research. Kilbertus et al. (2017) point out that no observational criterion can distinguish scenarios determined by different causal mechanisms but have the same observational distribution. They propose the definition of unresolved discrimination and proxy discrimination based on the intuition that some of the paths from the sensitive attribute to the prediction can be acceptable. Nabi & Shpitser (2018) argue that a fair causal inference on the outcome can be obtained by solving a constrained optimization problem. These notions are based on defining constraints on the interventional distributions.\nCounterfactual fairness (Kusner et al., 2017) requires the prediction on the target variable to have the same distribution in the factual world and counterfactual world. Many extensions of traditional statistical fairness notions such as Fair on Average Causal Effect (FACE) (can be regarded as counterfactual demographic parity) (Khademi et al., 2019), Fair on Average Causal Effect on the Treated (FACT) (can be regarded as counterfactual equalized odds) (Khademi et al., 2019), and CAPI fairness (counterfactual individual fairness) (Ehyaei et al., 2024) have been proposed. Path-specific counterfactual fairness (Chiappa, 2019) considered the path-specific causal effect. However, the notions are focused on fairness in static settings and do not consider the future effect. Most recent work about counterfactual fairness is about achieving counterfactual fairness in different applications, such as graph data (Wang et al., 2024a;b), medical LLMs (Poulain et al., 2024), or software debuging (Xiao et al., 2024). Some literature try to use counterfactual fairness for explanations (Goethals et al., 2024). Connecting counterfactual fairness with group fairness notions (Anthis & Veitch, 2024) or exploring counterfactual fairness with partial knowledge about the causal model (Shao et al., 2024; Pinto et al., 2024; Duong et al., 2024; Zhou et al., 2024) are also receiving much attention. Machado et al. (2024) propose an idea of interpretable counterfactual fairness by deriving counterfactuals with optimal transports (De Lara et al., 2024). While extending the definition of counterfactual fairness to include the downstream effects has been less focused on.\nSeveral studies in the literature consider the downstream effect on fairness of ML predictions. There are two kinds of objectives in the study of downstream effects: ensuring fair predictions in the future or fair true status in the future. The two most related works to our paper are Hu & Zhang (2022) and Tang et al. (2023). Hu & Zhang (2022) consider the problem of ensuring ML predictions satisfy path-specific counterfactual fairness over time after interactions between individuals and an ML system. Tang et al. (2023) study the impact on the future true status of the ML predictions. Even though they considered the impact of a counterfactually fair predictor, their goal is to ensure parity-based fairness. Therefore, the current works lack the consideration of ensuring counterfactual fairness on the true label after the individual responds to the current ML prediction. Our paper is aimed at solving this problem."}, {"title": "Problem Formulation", "content": "Consider a supervised learning problem with a training dataset consisting of triples (A, X, Y), where A\u2208 A is a sensitive attribute distinguishing individuals from multiple groups (e.g., race, gender), X = [X1, X2, ..., Xa]T \u2208 X is a d-dimensional feature vector, and Y \u2208 Y \u2286 R is the target variable in-dicating individual's underlying status (e.g., Y in lending identifies an applicant's ability to repay the loan, Y in healthcare may represent patients' insulin spike level). The goal is to learn a predictor from training data that can predict Y given inputs A and X. Let \u0176 denote the output of the predictor.\nWe assume (A, X, Y) is associated with a structural causal model (SCM) (Pearl et al., 2000) M = (V, U, F), where V = (A, X, Y) represents observable variables, U includes unobservable (exogenous) variables that are not caused by any variable in V, and F = {f1, f2, ..., fa+2} is a set of d + 2 functions called structural equations that determines how each observable variable is constructed. More precisely, we have the following structural equations,\n$Xi = fi(pai, Upaz), Vi \u2208 {1,\u2026\u2026,d},$\n$A = fa(paa, Upaa),$\n$Y = fy (pay, Upay),$\nwhere $pa_i$\u2286 V, $pa_A$\u2286 V and $pa_Y$\u2286 V are observable variables that are the parents of $X_i$, A, and Y, respectively. $Up_a i$\u2286 U are unobservable variables that are the parents of $X_i$. Similarly, we denote unobservable variables $Up_a A$\u2286 U and $Up_a Y$\u2286 U as the parents of A and Y, respectively."}, {"title": "Background: counterfactuals", "content": "If the probability density functions of unobserved variables are known, we can leverage the structural equa-tions in SCM to find the marginal distribution of any observable variable Vi \u2208 V and even study how intervening certain observable variables impacts other variables. Specifically, the intervention on vari-able Vi is equivalent to replacing structural equation Vi = fi(pai, Upaz) with equation V\u2081 = v for some v. Given new structural equation V\u2081 = v and other unchanged structural equations, we can find out how the distribution of other observable variables changes as we change value v.\nIn addition to understanding the impact of an intervention, SCM can further facilitate counterfactual inference, which aims to answer the question \"what would be the value of Y if Z had taken value z in the presence of evidence O = o (both Y and Z are two observable variables)?\" The answer to this question is denoted by $Y_{Z\u2190z}(U)$ with U following conditional distribution of Pr{U = u|O = o}. Given U = u and structural equations F, the counterfactual value of Y can be computed by replacing the structural equation of Z with Z = z and replacing U with u in the rest of the structural equations. Such counterfactual is typically denoted by Yz\u2190z(u). Given evidence O = o, the distribution of counterfactual value $Y_{Z+z}(U)$ can be calculated as follows,\n$Pr{Yz+z(U) = y|O = o} = \u2211Pr{Yz+z(u) = y} Pr{U = u|O = o}.$\nExample 3.1 (Law school success). Consider two groups of college students distinguished by gender A\u2208 {0,1} whose first-year average (FYA) in college is denoted by Y. The FYA of each student is causally related to (observable) grade-point average (GPA) before entering college XG, entrance exam score (LSAT) XL, and gender A. Suppose there are two unobservable variables U = (UA,Uxy), e.g., Uxy may be interpreted as the student's knowledge. Consider the following structural equations:\n$A = UA,$\n$XL = bL + w&A+UxY,$\n$XG = b + w&A+UxY,$\n$Y = bF + WA + Uxy,$\nwhere (bG, WA, BL, W\u2081, bF, W) are know parameters of the causal model. Given observation XG = 1, A = 0, the counterfactual value can be calculated with an abduction-action-prediction procedure Glymour et al. (2016): (i) abduction that finds posterior distribution Pr{U = u|XG = 1, A = 0}. Here, we have Uxy = 1-bg and UA = 0 with probability 1; (ii) action that performs intervention A = 1 by replacing structural equations"}, {"title": "Counterfactual Fairness", "content": "Counterfactual Fairness (CF) was first proposed by Kusner et al. (2017); it requires that for an individual with (X = x, A = a), the prediction \u0176 in the factual world should be the same as that in the counterfactual world in which the individual belongs to a different group. Mathematically, CF is defined as follows: Va, \u0103 \u2208 A, X \u2208 X, y \u2208 \u0423,\n$Pr (YA+a(U) = y|X = x, A = a) = Pr (Ya-a(U) = y|X = x, A = a),$\nWhile the CF notion has been widely used in the literature, it does not account for the downstream impacts of ML prediction \u0176 on individuals in factual and counterfactual worlds. To illustrate the importance of considering such impacts, we provide an example below.\nExample 3.2. Consider automatic lending where an ML model is used to decide whether to issue a loan to an applicant based on credit score X and sensitive attribute A. As highlighted in Liu et al. (2018), issuing loans to unqualified people who cannot repay the loan may hurt them by worsening their future credit scores. Assume an applicant in the factual world is qualified for the loan and does not default. But in a counterfactual world where the applicant belongs to another group, he/she is not qualified. Under counterfactually fair predictions, both individuals in the factual and counterfactual worlds should receive the loan with the same probability. Suppose both are issued a loan, then the one in the counterfactual world would have a worse credit score in the future. Thus, it is crucial to consider the downstream effects when learning a fair ML model."}, {"title": "Characterize downstream effects", "content": "Motivated by Example 3.2, this work studies CF in a dynamic setting where the deployed ML decisions may affect individual behavior and change their future features and statuses. Formally, let X' and Y' denote an individual's future feature vector and status, respectively. We use an individual response r to capture the impact of ML prediction \u0176 on individuals, as defined below.\nDefinition 3.1 (Individual response). An individual responser : U \u00d7 V X Y \u2194 U \u00d7 V is a map from the current exogenous variables U \u2208 U, endogenous variables V \u2208 V, and prediction \u00dd \u2208 Y to the future exogenous variables U' and endogenous variables V'.\nOne way to tackle the issue in Example 3.2 is to explicitly consider the individual response and impose a fairness constraint on future status Y' instead of the prediction \u0176. We call such a fairness notion the Lookahead Counterfactual Fairness (LCF) and present it in Section 4."}, {"title": "Lookahead Counterfactual Fairness", "content": "We consider the fairness over the individual's future outcome Y'. Given structural causal model M = (U, V, F), individual response r, and data (A, X, Y), we define lookahead counterfactual fairness below.\nDefinition 4.1. We say an ML model satisfies lookahead counterfactual fairness (LCF) under a responser if the following holds Va, \u0103 \u2208 A, X \u2208 X,y \u2208 V:\n$Pr (YA-a(U) = y|X = x, A = a) = Pr (YA\u2190a(U) = y|X = x, A = a),$\nLCF implies that the subsequent consequence of ML decisions for a given individual in the factual world should be the same as that in the counterfactual world where the individual"}, {"title": "Learning under LCF", "content": "This section introduces an algorithm for learning a predictor under LCF. In particular, we focus on a special case with the causal model and the individual response defined below.\nGiven sets of unobservable variables U = {U1, ..., Ud, Uy} and observable variables {X1,..., Xa, A, Y}, we consider causal model with the following structural functions:\n$Xi = fi(Ui, A), Y = fy(X1, ..., Xa, Uy),$\nwhere fi is an invertible function\u2074, and fy is invertible w.r.t. Uy. After receiving the ML prediction \u00dd, the individual's future features X' and status Y' change accordingly. Specifically, we consider scenarios where individual unobservable variables U change based on the following\n$U = r\u00bf(U\u00bf, \u0176) = U\u00bf + nu\u00dd, Vi \u2208 {1, ...,d}$\n$Uy = ry(Uy, \u0176) = Uy + \u03b7\u2207uy\u0176, $"}, {"title": "Path-dependent LCF", "content": "An extension of counterfactual fairness called path-dependent fairness has been introduced in Kusner et al. (2017). In this section, we also want to introduce an extension of LCF called path-dependent LCF. We will also modify Algorithm 1 to satisfy path-dependent LCF.\nWe start by introducing the notion of path-dependent counterfactuals. In a causal model associated with a causal graph G, we denote PGA as a set of unfair paths from sensitive attribute A to Y. We define XP as the set of features that are not present in any of the unfair paths. Under observation X = x, A = a, we call $YA\u2190\u0103, Xp xp (U)$ path-dependent counterfactual random variable for Y, and its distribution can be calculated as follows:\n$Pr{YA,XpPSA\n(U) = y|X = x, A = a} = \u2211 Pr{YA+a, Xps +xpo (u) = y} Pr{U = u|X = x, A = a}.\nFor simplicity, we use YPD and YPD to represent a path-dependent counterfactual and the corresponding realization. That is, $YPD= YA, XP PA (U)$ where U follows Pr{U|X = x, A = a}. We consider the same kind of causal model described in Section 5, the future attributes X' and outcome Y' are determined by equation 5 and equation 6. We formally define the path-dependent LCF in the following definition.\nDefinition 6.1. We say an ML model satisfies path-dependent lookahead counterfactual fairness w.r.t. the unfair path set PGA if the following holds \u2200a, \u0103 \u2208 A, X \u2208 X, y \u2208 Y:\n$Pr(YAAa, XP XPA (U) = y X = x, A = a) = Pr (YA (YA-XPA YA+, Xpo #pcx (U) = y/X = x, A = a).$\nThen we have the following theorem.\nTheorem 6.1. Consider a causal model and structural equations defined in Theorem 5.1. If we denote the features on unfair path as $X_{P_GA}$ and remaining features as $X_{P_GA^c}$, we can re-write structural equations as\n$X_{PGA} = APGAUXPGA + BPSA A,$\n$XPGA = PAUXPO + BPA,$\n$Y = WPSA XPGA + WPA XPGA + YUY$\nThen, the following predictor satisfies path dependent LCF,\n$g(YPD,U) = P1YBD + P2YPD + P3 + h(U),$\nwhere $p_1 = \\frac{T}{2}$ with\n$T := \\frac{1}{\\eta(||WPga \u00a9 Apga ||2 + || Wps apg 113 + y\u00b2)},$\np2 and p3 are learnable parameters to improve prediction performance and h is an arbitary function."}, {"title": "Experiment", "content": "We conduct experiments on both synthetic and real data to validate the proposed method."}, {"title": "Synthetic Data", "content": "We generate the synthetic data based on the causal model described in Theorem 5.1, where we set d = 10 and generated 1000 data points. We assume Ux and Uy follow the uniform distribution over [0,1] and the sensitive attribute A \u2208 {0,1} is a Bernoulli random variable with Pr{A = 0} = 0.5. Then, we generate X and Y using the structural functions described in Theorem 5.1. Based on the causal model, the conditional distribution of Ux and Uy given X = x, A = a are as follows,\n$Ux X = x, A = a ~ \u03b4(\\frac{x\u2212\u03b2\u03b1}{\u03b1}), Uy|X = x, A = a ~ Uniform (0, 1).$\nBaselines. We used two baselines for comparison: (i) Unfair predictor (UF) is a linear model without fairness constraint imposed. It takes feature X as input and predicts Y. (ii) Counterfactual fair predictor (CF) only takes the unobservable variables U as the input and was proposed by Kusner et al. (2017).\nImplementation Details. To find a predictor satisfying Definition 4.1, we train a predictor in the form of Eq. 8. In our experiment, h(u) is a linear function. To train g(y, u), we follows Algorithm 1 with m = 100. We split the dataset into the training/validation/test set at 60%/20%/20% ratio randomly and repeat the experiment 5 times. We use the validation set to find the optimal number of training epochs and the learning rate. Based on our observation, Adam optimization with a learning rate equal to 10-3 and 2000 epochs gives us the best performance.\nMetrics. We use three metrics to evaluate the methods. To evaluate the performance, we use the mean squared error (MSE). Given a dataset {x(i), a(i), y(i)}=1, for each x(i) and a(i), we generate m 100 values of u(i)[i] from the posterior distribution. MSE can be estimated as follows,\n$MSE = \\frac{1}{mn} \u03a3^n_{i=1}\u03a3^m_{j=1} (y^(i) - y^(i)[j])^2 ,$\nwhere \u0177(i)[j] is the prediction for data (x(i), a(i), u(i)[i]). Note that for the UF baseline, the prediction does not depend on u(i)[j]. Therefore, \u0177(i)[i] does not change by j for the UF predictor. To evaluate fairness, we define a metric called average future causal effect (AFCE),\n$AFCE = \\frac{1}{mn} \u03a3^n_{i=1}\u03a3^m_{j=1} (y'(i)[j] - \u1ef9'(i)[j]) .$\nIt is the average difference between the factual and counterfactual future outcomes. To compare Y - Y with Y' - Y' under different algorithms, we use the unfairness improvement ratio (UIR) defined below. The larger UIR implies a higher improvement in disparity.\n$UIR = 1 - \\frac{\u03a3^m_{i=1}\u03a3^m_{j=1} |y'(i)[j] - \u1ef9'(i)[j]| }{\u03a3^m_{i=1}\u03a3^m_{j=1} |y(i)[j] - \u1ef9(i)[j]|}  \u00d7 100%.$"}, {"title": "Real Data: The Law School Success Dataset", "content": "We further measure the performance of our proposed method using the Law School Admission Dataset Wightman (1998). In this experiment, the objective is to forecast the first-year average grades (FYA) of students in law school using their undergraduate GPA and LSAT scores.\nDataset. The dataset consists of 21,791 records. Each record is characterized by 4 attributes: Sex (S), Race (R), UGPA (G), LSAT (L), and FYA (F). Both Sex and Race are categorical in nature. The Sex attribute can be either male or female, while Race can be Amerindian, Asian, Black, Hispanic, Mexican, Puerto Rican, White, or other. The UGPA is a continuous variable ranging from 0 to 4. LSAT is an integer-based attribute with a range of [0, 60]. FYA, which is the target variable for prediction, is a real number ranging from -4 to 4 (it has been normalized). In this study, we consider S as the sensitive attribute, while R, G, and L are treated as features."}, {"title": "Conclusion", "content": "This work studied the impact of ML decisions on individuals' future status using a counterfactual inference framework. We observed that imposing the CF predictor may not decrease the group disparity in individuals' future status. We thus introduced the lookahead counterfactual fairness (LCF) notion, which takes into account the downstream effects of ML models and requires the individual future status to be counterfactually fair. We proposed a method to train an ML model under LCF and evaluated the method through empirical studies on synthetic and real data."}, {"title": "Path-dependent LCF", "content": "We conduct experiments on both synthetic and real data to validate the proposed method."}, {"title": "Proofs", "content": "Here are the proofs for the formulas, definitions and theorems in the current paper."}]}