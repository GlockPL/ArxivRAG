{"title": "GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis", "authors": ["Bo Liu", "Ke Zou", "Liming Zhan", "Zexin Lu", "Xiaoyu Dong", "Yidi Chen", "Chengqiang Xie", "Jiannong Cao", "Xiao-Ming Wu", "Huazhu Fu"], "abstract": "Medical Visual Question Answering (VQA) is an essential technology that integrates computer vision and natural language processing to automatically respond to clinical inquiries about medical images. However, current medical VQA datasets exhibit two significant limitations: (1) they often lack visual and textual explanations for answers, which impedes their ability to satisfy the comprehension needs of patients and junior doctors; (2) they typically offer a narrow range of question formats, inadequately reflecting the diverse requirements encountered in clinical scenarios. These limitations pose significant challenges to the development of a reliable and user-friendly Med-VQA system. To address these challenges, we introduce a large-scale, Groundable, and Explainable Medical VQA benchmark for chest X-ray diagnosis (GEMeX), featuring several innovative components: (1) A multi-modal explainability mechanism that offers detailed visual and textual explanations for each question-answer pair, thereby enhancing answer comprehensibility; (2) Four distinct question types\u2014open-ended, closed-ended, single-choice, and multiple-choice\u2014that better reflect diverse clinical needs. We evaluated 10 representative large vision language models on GEMeX and found that they underperformed, highlighting the dataset's complexity. However, after fine-tuning a baseline model using the training set, we observed a significant performance improvement, demonstrating the dataset's effectiveness. The project is available at www.med-vqa.com/GEMeX.", "sections": [{"title": "1. Introduction", "content": "Large vision language models (LVLMs) have recently achieved huge breakthroughs in artificial intelligence [1, 2, 5, 11, 35, 38, 55, 57], demonstrating remarkable capabilities in understanding visual content while generating coherent natural language responses. These advancements have driven innovations across various domains [13, 15, 47], with healthcare emerging as a key application. Within this domain, medical visual question answering (Med-VQA) stands out as a crucial task which automatically provides reliable and user-friendly answers [29] to questions about medical images [24], facilitating healthcare professionals in diagnosis, medical education, and clinical decision-making.\nTo ensure the reliability and user-friendliness of Med-VQA systems, it is crucial to incorporate answer explanations along with a diverse set of question formats. Although significant progress has been made by existing Med-VQA systems [16, 18, 24, 31, 53], none have yet integrated answer explanations. As emphasized by Li et al. (2018), explanations are as essential as the answers themselves in general VQA systems. This holds even stronger in the context of medical VQA, where the domain-specific nature of the task amplifies the need for clarity [24]. If Med-VQA tools are to assist in clinical processes, the datasets must be designed to include explanations, so as to enhance patient comprehension and support the learning of junior medical practitioners. Additionally, the limited range of question formats, such as the absence of multiple-choice questions, restricts the real-world applicability and impairs the overall user-friendliness of medical Al systems.\nTo tackle the aforementioned challenges, we develop a large-scale, Groundable, and Explainable Medical VQA benchmark for chest X-ray diagnosis (GEMeX). We first undertake a comprehensive data refinement process upon the Chest ImaGenome dataset [45]. By collaborating with radiologists, we systematically redefine anatomical regions and establish more precise visual-text correspondence mappings, resulting in accurate region-grounded reports for each X-ray image. Subsequently, we leverage GPT-40 [1] to generate a diverse set of questions based on these grounded reports, covering four categories: open-ended, closed-ended, single-choice, and multiple-choice questions. Each question-answer pair is enriched with explicit reasoning and corresponding visual region annotations, as illustrated in Figure 1. The resulting dataset comprises 151,025 radiographic images and 1,605,575 questions. To our knowledge, this is currently the largest chest X-ray VQA dataset and the first Med-VQA dataset that simultaneously includes both textual and visual explanations.\nWe evaluate 10 representative LVLMs, including 5 from the general domain (e.g., LLaVA-v1 [35], Mini-GPT4-v1 [57], GPT-40-mini [1]), and 5 from the medical domain (e.g., LlaVA-Med [26], XrayGPT [40], RadFM [43]). The experimental findings underscore the challenging characteristics of our dataset. Additionally, we propose a simple instruction-tuning strategy that derives a task-specific LVLM. The impressive performance improvement highlights the effectiveness of our dataset. For evaluation, we develop three metrics to assess the accuracy of model outputs in terms of answers, reasoning, and visual grounding (localization generation). Notably, we apply both semantics-level score and gram-based metrics for natural language generation (e.g., BLEU and ROUGE). Re-"}, {"title": "2. Related Work", "content": "In this section, we review prior work relevant to our study, focusing on two main areas: existing datasets for Med-VQA and current methodologies developed for Med-VQA task."}, {"title": "2.1. Medical VQA Datasets", "content": "In recent years, various datasets have been created to advance medical VQA research, each tackling specific challenges across clinical domains. A detailed comparison to other VQA datasets can be seen in Table 1. Specifically, VQA-RAD [24] is a pioneer dataset that offers over 3,000 question-answer pairs focused on radiology images. SLAKE [31] is the first manually created dataset with over 14,000 QA pairs across CT, MRI, and X-ray images, enabling models to handle complex scenarios by combining"}, {"title": "2.2. Medical VQA Methods", "content": "Inspired by the advancements in general VQA, medical VQA has gained significant traction as a specialized domain. Due to data limitations, however, most approaches [14, 22, 24, 30, 32, 36, 46, 50] have focused on directly embedding visual and textual information jointly to capture their relationships. With the rise of contrastive language-image pertaining (CLIP) [37], methods [8, 9, 12, 49, 51] start to focus on applying CLIP to Med-VQA.\nA promising way is to fine-tune CLIP's joint embeddings to better handle specific medical domains, enhancing the model's understanding of clinical questions and visual features [28]. Recently, the explosion of large vision language models (LVLMs) has further pushed the boundaries of medical domain [3, 25, 26, 40, 43, 58]. Generally, they first pre-train models on a large-scale image-text dataset (like PMC-OA [28], PMC-15M [51]) to map visual features into language model's embedding space and then further tune with instruction data for medical consultation [3, 35] or disease diagnosis [7, 40, 42, 56]. These models are now leveraged for Med-VQA tasks to provide richer, more context-aware answers, extending beyond simple text-image alignment to incorporate broader knowledge-based reasoning.\nDespite these advances, current methods are limited by the size and diversity of available datasets. Furthermore, the absence of detailed explanations in these datasets also hinders progress in building explainable VQA models, highlighting the need for methods and datasets that focus on explainability and clinical relevance."}, {"title": "3. Construction of GEMEX", "content": "We will elaborate on the construction of the proposed GEMeX dataset, accompanied by a schematic overview in Figure 2. The structure of this section is organized as follows: In Section 3.1, we introduce the initial step of dataset construction, focusing on refining anatomical regions and grounding reports; Section 3.2 covers the generation process for four distinct types of Med-VQA, incorporating multimodal explainability across both visual and textual dimensions. More details can be found in the Appendix."}, {"title": "3.1. Grounded Report Refinement", "content": "As shown in stage I of Figure 2, we build upon Chest ImaGenome [45] to construct our dataset, but with an emphasis on the mapping precision between visual regions and textually described entities. The emergence of Chest ImaGenome has led to significant advances in various multimodal tasks, including Med-VQA [17] and grounded report generation [39]. However, after consulting radiologists, we find that the anatomical descriptions of regions of interest in ImaGenome are imprecise and lack simplicity, and they even introduce ambiguity into clinical diagnoses. Specifically, in ImaGenome, one sentence may be filled with multiple anatomical regions, e.g., sentence \u201cThere is blunting of the right costophrenic angle which may be due to overlying soft tissue\u201d corresponds to [\"right lung\", \"right lower lung zone\"]. This drawback poses challenges in training models to precisely visual grounding. Therefore, in the first stage of construction, we perform sentence-region refinement, generating pairs where each sentence is associated with only one clinically precise textual region entity."}, {"title": "3.1.1. Anatomical Region Selection and Merging", "content": "In the original Chest ImaGenome, there are 29 significant pathological regions. However, in alignment with radiologists' practices, our dataset focuses on retaining core clinical regions that are crucial for diagnosing diseases through X-rays, such as the \"left lower lung\u201d and \u201cmediastinum\u201d. Less significant or marginal areas are excluded to streamline the diagnostic training process and enhance clinical relevance. For example, we exclude terms like \"carina\" which is not considered a core region, and \u201cclavicle\u201d which accounts for only about 2% of the total regional frequency. Furthermore, to enhance clarity and ensure that each sentence corresponds to a single pathological region with finer granularity, semantically similar regions are merged. For example, the \"left lower lung zone\" and \"right lower lung zone\" are combined into a \u201cbilateral lower lung zone\u201d. This consolidation aligns with conditions like \u201cbibasilar atelectasis\u201d, as illustrated in Stage I of Figure 2, where the condition is described as \"Bibasilar atelectasis is seen without discrete focal consolidation\u201d. In total, we define 30 anatomical regions eventually. Detailed transformation from Chest ImaGenome to ours can be found in the Appendix."}, {"title": "3.1.2. Refinement with Medical LLM", "content": "After defining anatomical regions, we utilize OpenBioLLM-70B\u00b9, known for its outstanding performance across various medical NLP tasks, to refine original sentence-region pairs. To test the effectiveness of the prompt, we begin by randomly selecting 100 pairs from the Chest ImaGenome test set, which includes approximately 367 sentences. Initially, the performance"}, {"title": "3.2. Groundable and Explainable VQA Generation", "content": "Although there are many Med-VQA datasets [4, 17] available, some even generated using MIMIC-CXR or Chest ImaGenome, they all have two weaknesses that diminish their practicality: (1) these datasets often lack strong explainability. When patients ask medical questions, the system may provide only simple textual explanations but without offering visually relevant guidance. This limitation can hinder the user's understanding of the medical information presented; (2) the types of questions included in these datasets are limited, typically restricted to open-ended or closed-ended formats, with no inclusion of choice-based questions. This significantly restricts the flexibility and comprehensiveness of the datasets in addressing a broader spectrum of user inquiries. In general, these issues highlight the necessity for more versatile and explainable Med-VQA datasets to enhance their utility in clinical settings.\nAs shown in Stage II of Figure 2, we prepare to generate our VQA dataset after refining the grounded reports. Here, we employ GPT-40 [1] as a generator due to its remarkable capabilities in understanding and generating long texts. To enhance the quality of the generated questions and better align them with our objectives, we manually craft questions for 30 images according to their paired grounded report (as a golden set) to serve as demonstrations in the prompt for in-context learning. Moreover, we also design specific rules (like not generating questions that need to be answered by comparing two images) to ensure the quality of generated VQAs. For each image-report sample, we instruct the GPT-40 to generate a total of 11 questions: 3 open-ended VQAs, 2 closed-ended VQAs, 3 single-choice questions, and 3 multi-choice questions, culminating in ap-"}, {"title": "4. Evaluation of GEMeX", "content": "4.1. Experimental Details\nA Strong Baseline Fine-Tuned on GEMEX. To validate the effectiveness of the dataset, especially the auto-generated training set, we propose a question-type-aware instruction tuning to fine-tune LLaVA-Med-v1-7B [26] on the training set of GEMeX, termed as LLAVA-Med-GEMEX, serving as a simple baseline. Specifically, for each VQA sample from our GEMeX, we add a type prompt $X_{Type}$ after the original system prompt and a question $X_{question}$ with its answer $X_{answer}$, textual reason $X_{Reason}$, and corresponding visual location $X_{Location}$, constructing a single-turn dialogue as in Table 4. Generally, $X_{Type}$ is \"Input an {Type} question, and the assistant will output its answer {Supplement} with a detailed reason and corresponding visual location.\" where {Type} refers to \u201copen-ended/closed-ended/single-choice/multi-choice\" and {Supplement} is replaced by \"none/(yes or no)/(an option)/(some options)\", respectively. We have shown some input samples in the Appendix."}, {"title": "4.2. Evaluation Metrics", "content": "In GEMeX, each question has a corresponding answer, textual reason, and visual location. Ideally, we aim to evaluate all these three aspects with designed metrics as follows:\n\u2022 Answer-Reason Score (AR-score): In reality, most LVLMs struggle to generate accurate outputs in terms of format. This doesn't necessarily mean these models lack the knowledge to answer the questions but rather simply lack the ability to follow instructions properly. To ensure a fair comparison, we introduce the Answer-Reason score (AR-score) as an evaluation metric for the textual output, where the answer and reason parts from each test sample are merged as a reference (ground truth), and the evaluated LVLM's output serve as a candidate. We use GPTScore [26] to calculate the AR-Score from a semantic perspective. Specifically, GPT-40 is leveraged to quantify the correctness. We start by treating the aforementioned reference as a textual response from assistant #1, while the candidate as the response from assistant #2. With both responses, the original question, and the X-ray report, GPT-4o assesses the accuracy, relevance, and helpfulness of each assistant's answer and provides an overall score on a scale of 1 to 10, where a higher score indicates better performance. We then calculate the relative score using GPT-40's reference score for normalization. Besides, we also employ common NLG metrics (e.g., BERTScore [52], BLEU, ROUGE) to evaluate AR-score.\n\u2022 Answer Score (A-score): For responses where the model can output specific answers (such as yes/no for closed-ended questions or options for single/multiple choice questions), we calculate the accuracy by comparing with the ground truth. It is worth noting that although some models cannot directly output the answer, we still attempt to match it from their responses.\n\u2022 Visual Score (V-score): For models capable of visual grounding (i.e., outputting visual locations), we calculated mean intersection over union (mIoU) as a measurement. For a VQA case, considering there might be multiple corresponding locations (commonly seen in multi-choice questions), we use the Hungarian algorithm [23] to match the predicted bounding boxes with the actual ones."}, {"title": "4.3. Results and Analysis", "content": "Overall Performance. To make a fair comparison, the evaluated models (except GPT-40-mini and RadFM (with MedLLaMA-13B [44])) are based on 7B-LLMs in this section. Specifically, LLaVA-v1, LLaVA-Med-v1, and Mini-GPT4-v1 are based on Vicuna-v0-7B [10] while LLaVA-v1.5 and XrayGPT are based on Vicuna-v1-7B; LLaVA-Med-v1.5 is built upon Mistral-7B-Instruct-v0.2 [20]; mPLUG-Owl is using LLaMA-7B [41]. All models' configurations are set according to their open-source codes. The comprehensive results are shown in Table 5. The first 5 rows indicate the performance of general LVLMs, while the last 6 rows present the results of medical ones and our fine-tuned version of LLaVA-Med-v1 (termed as LLaVA-Med-GEMeX). It can be found that:\n\u2022 Most available LVLMs exhibit weak performance when tested on GEMeX. The only exception is GPT-40-mini, which achieves an AR-score above 80 on average across all tasks. When considering specific question types, LLaVA-Med (both versions 1 and 1.5) stands out for its strong performance on open-ended questions, scoring above 90 on the AR-score. However, all models show poor results on the other three categories of tasks.\n\u2022 When faced with choice-based questions, most models struggle to output definitive options, although they can analyze each option. This explains why most models have a corresponding AR-score but lack an A-score, demonstrating the need of introducing these types of questions.\n\u2022 Powerful LVLMs like GPT-40-mini often rely on shortcut reasoning instead of true multimodal reasoning. Specifically, while they can sometimes answer questions to a certain extent, they often fail to accurately visual grounding. This indicates that these models tend to solve Med-VQA tasks using shortcut knowledge, such as retrieving information from their pre-training memory, rather than engaging in genuine multimodal reasoning. However, multimodal reasoning is central to the explainability of Med-VQA systems.\n\u2022 Through the proposed simple question-type-aware instruction tuning, the model achieves a significant performance improvement, approximately 13.5% (i.e., avg. AR-score) compared to LLaVA-Med-v1. More importantly, it surpasses GPT-40-mini on most metrics, demonstrating the reliability of the training set. However, there still remains a significant gap towards practical usage, highlighting the challenges of the proposed GEMeX.\nLimitation. We want to emphasize that the fine-tuned model is inherently task-specific, which means it may suffer from reduced accuracy on other datasets or lose its ability to engage in conversations. Therefore, the real potential lies in integrating our GEMeX into multi-task training (such as the second stage of training LLaVA-Med) in the future. The tuned model here primarily serves to validate the effectiveness of the dataset while also establishing a robust baseline.\nMore Metrics. As we mentioned in Section 4.2, we also calculate NLG metrics to measure AR-score. Detailed results are shown in Table 6. Overall, the NLG metrics generally share the same trend as GPTScore (AR-score in Table 5), but there are some minor differences. (1) Models with high NLG scores do not always correlate with good performance, as seen with mPLUG-Owl compared to LLaVA-v1.5. Essentially, LLaVA-1.5 demonstrates higher performance, such as achieving an answer accuracy rate (A-score) in single-choice tasks that is 15% higher than that of mPLUG-Owl. However, since LLaVA-v1.5's output mostly consists of the answer without reason, the shorter output results in a lower NLG score, with its BLEU-1 approximately 28.8% lower than mPLUG-Owl; (2) NLG metrics can better reflect the performance improvement after fine-tuning. For example, the fine-tuned model shows only about a 3.7% average improvement over GPT-40-mini on GPTScore but brings around a 12.1% improvement on average NLG metrics. This more significant improvement bet-"}, {"title": "5. Conclusion", "content": "In this paper, we introduce a benchmark, GEMeX, designed to advance the field of medical VQA with two primary advantages: multimodal explainability and diverse question types. GEMeX not only provides more accessible medical explanations for patients and junior doctors but also serves as a valuable training resource for developing next-generation medical LVLMs with enhanced instruction-following capabilities. We demonstrate the effectiveness and difficulty of the dataset through comprehensive testing of representative LVLMs as well as task-specific fine-tuning, hoping that GEMeX can promote medical VQA development and improve AI-assisted medical care."}]}