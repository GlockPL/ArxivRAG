{"title": "AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking Vulnerabilities in LLMs through Bit-Flip Attacks", "authors": ["Sanjay Das", "Swastik Bhattacharya", "Souvik Kundu", "Shamik Kundu", "Anand Menon", "Arnab Raha", "Kanad Basu"], "abstract": "Large language models (LLMs) have transformed natural language processing (NLP) by excelling in tasks like text generation and summarization. Their adoption in mission-critical applications highlights an emerging concern: LLMs' vulnerability to hardware-based threats, particularly bit-flip attacks (BFAs). BFAs involve fault injection methods, including techniques such as Rowhammer, which target the model parameters stored in memory, thereby undermining model integrity and performance. Given the vast parameter space of LLMs, identifying critical parameters for BFAs is challenging and often infeasible. Existing research suggests that transformer-based architectures are inherently more robust to BFAs compared to traditional deep neural networks (DNNs). However, we challenge this assumption for the first time, demonstrating that as few as three bit-flips can cause a performance collapse in an LLM with billions of parameters. Current BFA techniques fail to exploit this vulnerability due to the difficulty of traversing the large parameter space to identify critical parameters. To address this, we propose AttentionBreaker, a novel framework specifically designed for LLMs that facilitates efficient parameter-space traversal to identify critical parameters. Additionally, we introduce GenBFA, an evolutionary optimization strategy, to further narrow down the critical parameter set and identify the most vulnerable bits, facilitating a more efficient attack. Empirical findings indicate the pronounced vulnerability of LLMs to AttentionBreaker: for instance, perturbing merely 3 bits ($4.129 \u00d7 10^{-9}\\%$ of total parameters) in LLaMA3-8B 8-bit weight quantized (W8) model results in total performance collapse, with accuracy on Massive Multitask Language Understanding (MMLU) tasks plummeting from 67.3% to 0% and perplexity escalating from 12.6 to $4.72 \u00d7 10^{5}$. These results highlight the framework's efficacy in exploiting and exposing inherent vulnerabilities in LLM architectures.", "sections": [{"title": "1. Introduction", "content": "The rise in popularity of LLMs has fundamentally expanded the capabilities of Artificial Intelligence (AI), demonstrating remarkable proficiency in generating human-like text, interpreting nuanced context, and executing complex reasoning tasks. These advancements have not only reshaped natural language processing but have also extended AI applications into diverse fields such as computer vision and scientific research, heralding a new era of AI-driven solutions [1], [2], [3]. Given their pervasive use, it is critical to analyze the vulnerability profile of LLMs against both software-based and hardware-based threats to ensure their secure and reliable deployment [4]. Of particular concern are hardware threats like BFAs, which exploit hardware vulnerabilities to corrupt memory regions that store the model's weight parameters, thereby compromising model integrity and performance [5], [6], [7]. BFAs employ fault injection attacks such as DeepHammer [8] to manipulate specific bits within DRAM, altering essential model weights to degrade functionality. Despite advancements in-memory technology, recent techniques enable remote, non-physical memory manipulation, perpetuating the threat landscape for BFAs [9], [10]. While BFAs have been extensively studied in the context of DNNs [5], [6], [11], their implications for transformer-based architectures, including LLMs, remain largely unexplored, highlighting a crucial gap in current research. Addressing this gap is critical for evaluating transformer models' susceptibility to bit-flip vulnerabilities and ensuring their reliability, especially in mission-critical applications.\nExisting research on BFAs targeting Transformer-based models suggests that these architectures exhibit greater inherent resilience compared to traditional DNNs. This robustness is largely attributed to the unique structural properties of Transformer models [12]. Specifically, studies indicate that the inherent redundancy and modularity of Transformers facilitate internal error compensation across different model components, allowing these architectures to mitigate localized faults more effectively than DNNs [13]. In this work, we challenge the presumed robustness of Transformer models by demonstrating, for the first time, that as few as three bit-flips in an 8-billion parameter model-comprising tens of billions of bits can result in catastrophic degradation, reducing the model's accuracy to 0% on standard benchmarks such as MMLU tasks (refer Figure 1). This finding emphasizes the extreme vulnerability of LLMs to BFAs. However, existing BFA methods are unable to exploit this vulnerability, primarily due to the vast parameter space of LLMs, which complicates the identification of critical parameters for attack. To address this limitation, we propose a novel, systematic approach to BFAs specifically tailored to uncover and exploit bit-flip vulnerabilities in LLMs. This represents the first application of such a method, achieving unprecedented effectiveness in targeting these models.\nTo execute an adversarial attack on LLMs, the main challenge is navigating the extremely large parameter space to identify critical parameters whose perturbation significantly degrades model performance. Given the nonlinear nature of this task, heuristic optimization techniques present a suitable approach. With this premise, we propose a novel algorithm based on evolutionary strategies [14], designed to identify a minimal yet critical subset of parameters for an efficient BFA. This approach leverages principles of natural selection and genetic variation to iteratively refine the parameter set, enhancing optimization efficiency. Our contributions are summarized below:"}, {"title": "\u2022", "content": "For the first time, we challenge the notion of LLMs' high resilience to BFAs, demonstrating that as few as three bit-flips can critically degrade model performance, even for LLMs with billions of parameters and tens of billions of bits."}, {"title": "\u2022", "content": "To navigate the vast parameter space of LLMs, often cited as a reason for their assumed resilience, we introduce a novel framework, AttentionBreaker, enabling efficient and targeted bit-flip attacks."}, {"title": "\u2022", "content": "We present a novel evolutionary algorithm, GenBFA, to identify the most critical bits in LLMs, optimizing the selection of targets for bit-flip attacks."}, {"title": "\u2022", "content": "Our framework uncovers the significant vulnerability of LLMs, as evidenced by a mere three bit-flips\u2014a flip rate of just $4.129 \u00d7 10^{-9}\\%$\u2014in LLaMA3-8B, reduces MMLU accuracy from 67.3% to 0% and raises perplexity from 12.62 to $4.72 \u00d7 10^{5}$."}, {"title": "2. Background", "content": "This section provides essential background on the Transformer architecture and Bit-Flip Attacks (BFAs), offering the necessary context to understand the interplay between Transformer models and potential vulnerabilities."}, {"title": "2.1. Transformer Models", "content": "The Transformer architecture is foundational for LLMs and Vision Transformers, driving advancements in NLP and image classification [1], [2], [3], [15]. It comprises two main components: the encoder to convert input data into a structured intermediate representation for improved feature understanding and the decoder to generate output sequences from this encoded information [16]. There are three task-specific configurations: Encoder-only models for input-driven tasks (e.g., text classification, named entity recognition), Decoder-only models for generative tasks, and Encoder-decoder models for tasks requiring input-dependent generation (e.g., machine translation, summarization).\nA critical innovation in the Transformer is the attention mechanism, particularly multi-head attention, which prioritizes relevant input elements [16]. Attention weights are computed to emphasize informative tokens, while masks can exclude specific tokens, such as padding.\nIn each multi-head attention layer, the model derives three parameters for each input token: Query (Q), Key (K), and Value (V), calculated through linear transformations of input vectors. Attention scores assess token importance across the sequence. Each head processes distinct Q, K, and V subsets, facilitating parallel analysis of diverse token relationships. The individual attention outputs are concatenated and linearly combined to produce the final attention output, enhancing the model's ability to capture complex dependencies and improve task performance in both NLP and vision domains."}, {"title": "2.2. Bit-flip Attack (BFA)", "content": "The robustness, security, and safety of modern computing systems are intrinsically linked to memory isolation, which is enforced through both software and hardware mechanisms [17]. However, due to read-disturbance phenomena, contemporary DRAM chips remain susceptible to memory isolation breaches [18]. RowHammer [19] is a well-documented example where repeated DRAM row (hammering) access induces bitflips in adjacent rows due to electrical interference.\nTechniques for inducing bit-flips have enabled BFAs, designed to degrade model accuracy through strategic memory faults. BFAs are designed to minimize the number of bit-flips required, enhancing stealth and reducing detection risk. DeepHammer [8] exemplifies advanced BFAs by employing Progressive Bit Search (PBS) [5], a gradient-based algorithm to locate vulnerable bits. In each iteration, DeepHammer selects n vulnerable bits by ranking model parameters by gradient sensitivity. Each bit in this subset is individually flipped, forming a corresponding loss set L. This process iterates across layers, yielding n \u00d7 1 candidate bits and loss sets, with the most vulnerable bit identified as the one producing the highest loss. The bit-flipping continues until the targeted misclassification is achieved. Although limited to flipping a single bit per page, DeepHammer assumes an adversary can flip multiple bits per DRAM page."}, {"title": "3. Variable Notations", "content": "In this section, we define the variable notations used throughout this paper, as summarized in Table 1.\nConsider in a model M with L-layers, parameterized by W = {$W_{1}, W_{2}, ..., W_{n}$}, L(W) denotes the associated loss function. For simplicity, we denote the gradient of the loss function $\\VC(W)$ as \u25bdW throughout this paper. To assess the sensitivity of model parameters, we define a sensitivity metric S, with $S_{mag}$ and $S_{grad}$ capturing sensitivity based on parameter magnitudes and gradients, respectively. To balance the influence of gradients and magnitudes in the computation of a hybrid sensitivity metric, S, a sensitivity ratio \u03b1 \u2208 [0, 1] is introduced. The attention weight matrix is represented by A, with corresponding query and key vectors $q_{i}$ and $k_{j}$, scaled by $d_{k}$. The function cardinality is defined to evaluate the number of parameters in a given parameter set. Let w' represent a perturbed version of weight w, and R denote the set of sampling rates for parameter sampling during sensitivity analysis. The normalized gradients and weights are denoted by \u25bdWN and WN, respectively. Finally, I is the set of indices where bit flips are applied to perturb specific parameters."}, {"title": "4. Motivation", "content": "Fault attacks on DNNs typically exploit vulnerabilities in hardware implementations or manipulate parameters to induce misclassifications, which has been extensively explored in existing literature [5], [7]. However, the specific domain of fault injection attacks on LLMs, which involves introducing controlled parameter perturbations by memory corruptions to manipulate their output, remains underexplored [20]. This uncharted territory presents a unique challenge due to the complex nature of transformer-based architectures. Fault injection attacks on LLMs could potentially result in unintended or biased text generation, misclassification, undermined coherence, or subtle manipulations that are difficult to detect. To address this research gap, we systematically evaluate BFA impacts on transformer-based models, particularly LLMs, and propose a novel and highly efficient AttentionBreaker framework for LLMs."}, {"title": "4.1. Proxy for Sensitivity", "content": "For performing BFA, it is essential to analyze model parameter sensitivity and vulnerability profiles independently of assumptions of robustness. In particular, parameters with larger gradients or higher magnitudes may exhibit amplified sensitivity, whereby perturbations yield disproportionately large effects on the output. In full-precision models (e.g., float32), where weights w span a large representable range $[-3.4 \u00d7 10^{38}, 3.4 \u00d7 10^{38}]$, even a single bit-flip can induce substantial perturbations, particularly for large-magnitude weights. For such weights, the sensitivity $S_{mag}$ can be approximated as:\n$S_{mag} \\approx W$\\nHowever, gradient-derived sensitivity $S_{grad}$ provides a complementary perspective, capturing the degree to which a weight change affects the model loss:\n$S_{grad} \\approx \\nabla W$"}, {"title": "4.2. Layer Sensitivity Analysis", "content": "Identifying critical bits in LLMs presents a formidable challenge due to the sheer volume of their parameter space. However, identifying a sensitive layer is considerably more manageable, as the number of layers is significantly lower than the total number of parameters. To quantify layer sensitivity, we choose top-r% bit-flips in each layer, guided by the hybrid sensitivity score S, and observe the resulting model loss L on the LLaMA3-8B W8 model. The losses vary significantly across layers, as illustrated in Figure 3b. Several layers have very high sensitivity, while many are much more resilient. This observation indicates that by focusing only on layers with high sensitivity, we can substantially narrow the search space for identifying critical parameters."}, {"title": "4.3. Weight Subset Reduction", "content": "In critical parameter search in LLMs, sensitive layers can be identified using previous analysis. However, this information alone is insufficient due to the vast parameter count within a single layer, which can range from 100,000 to 100 million. To address this, we perturb the top r% of parameters in the top-n layers in the model, where r or the sampling rate determines the number of bit-flips to be performed using a hybrid sensitivity metric S. The perturbation rater is varied from a very low value (0.0001) to considerably high (10), and the process continues until the perturbation-induced loss exceeds a predefined threshold.\nAs illustrated in Figure 3c, perturbing only 0.8% of weights in the most sensitive layer raises the loss to a predefined threshold of 8, leading to termination. Notably, this outcome is observed only when focusing on the most sensitive layer, not with other configurations. This result provides a subset of weights containing parameters critical to the model. By using this approach, the search space for identifying critical parameters can be significantly reduced."}, {"title": "4.4. Weight Subset Optimization", "content": "The weight subset identified during the subset selection analysis in Section 4.3 can be sufficiently large to render an effective adversarial attack impractical. For example, the subset size in Figure 3c is 5872, which is prohibitively large for efficient bit-flip attacks. This necessitates further optimization to narrow the subset to a much smaller set of weights and/or bits that enable a feasible attack. However, solving this problem through exhaustive approaches is computationally expensive due to the vast solution space. To address this challenge, we propose a heuristic optimization technique, leveraging the efficiency of explorative search strategies to quickly identify good-enough solutions. Specifically, we implement an optimization method inspired by evolutionary strategies, which simultaneously optimizes for higher model loss (to maximize attack effectiveness) and fewer bit-flips (to enhance attack efficiency). Using this approach, we successfully reduced the required bit-flips from 5872 to 3, achieving approximately 2 \u00d7 10\u00b3 reduction. This result highlights the effectiveness of the proposed method in drastically narrowing the search space, identifying a sub-hundred set of highly critical bits, and exposing their vulnerability to adversarial attacks."}, {"title": "5. Proposed Attack Methodology", "content": "Here, we describe the threat model, articulating the standard assumptions regarding the adversary's capabilities for carrying out BFAs. Subsequently, we introduce an innovative attack framework, AttentionBreaker, designed to identify and manipulate the most critical bits within LLMs, precipitating a substantial degradation in model performance."}, {"title": "5.1. Threat Model", "content": "In Machine Learning as a Service (MLaaS), deploying LLMs on shared computational platforms introduces significant security vulnerabilities. These platforms, while providing enhanced computational resources, expose LLMs to potential BFAs due to shared hardware components such as last-level caches and main memory. Attackers, even without explicit permissions to access user data, can exploit side channels to induce bit-flips in the memory cells, storing the LLM's parameters [10]. This threat model categorizes risks into three levels, defined by the attacker's knowledge and degree of access to the model parameters: (1) At the basic level, attackers possess limited knowledge, allowing only minor perturbations that reduce model performance. (2) At the intermediate level, attackers gain targeted access to specific bits, enabling significant accuracy degradation through targeted manipulation. (3) At the advanced level, attackers have comprehensive access and sophisticated techniques, permitting them to induce severe outcomes such as complete model corruption or the embedding of backdoors.\nIn the context of our work, an attacker with elevated privileges gains unauthorized access to the memory where the LLM's weights are stored [23]. The attacker aims to flip the least number of bits in the model to achieve the most adversarial impact. By exploiting fault injection techniques, such as the RowHammer attack [19], the attacker selectively flips critical bits in the model's weights. This alteration can cause minute to severe model performance degradation, thus undermining the model's reliability, especially in mission-critical applications, including healthcare, finance, and autonomous systems."}, {"title": "5.2. Proposed Attack Framework", "content": "Here, we explain in detail the proposed AttentionBreaker framework, which comprises three structured steps: (1) layer ranking, which generates a sensitivity profile for each layer; (2) weight subset selection, which extracts a critical subset of weights from this profile; and (3) weight subset optimization, which refines this subset to identify the model's most vulnerable weights for an efficient attack."}, {"title": "5.2.1. Layer Ranking.", "content": "For ranking LLM layers, we assess layer sensitivity as discussed in Section 4.2. We begin by quantifying the sensitivity of each layer's parameters. For this purpose, each weight $w_{i}$ in a layer is evaluated through a sensitivity scoring function that incorporates both its magnitude and gradient information using min-max normalization, as defined in Equation 4. Using these sensitivity scores, weights are aggregated and ranked in descending order to form a prioritized list of the most critical weights in each layer. The top-k weights within each layer are then identified for bit-flip perturbations, with k determined by Equation 5:\n$k = cardinality(W^{(l)}) \u00d7 \\frac{r}{100}$"}, {"title": "5.2.2. Weight Subset Selection", "content": "To determine a reduced weight subset, we conduct a perturbation experiment as discussed in Section 4.3. We vary the sub-sampling rate r across a defined range to identify the minimum r value at which the perturbation-induced loss L reaches or exceeds a predefined threshold, $L_{th}$. This threshold represents a critical degradation in model performance, indicating that further increments in r are unnecessary. By recording both r and the corresponding bit-flip count k for each layer, we can identify the smallest set of weights that meets $L_{th}$, enabling efficient subset selection. Once each layer has been assessed for optimal sub-sampling values, we determine the specific layer l and corresponding values of k and r that yield the smallest subset satisfying the condition L > $L_{th}$. The resulting subset $W_{sub}$ represents the most efficient collection of weights for further refinement.\nThe weight subset selection procedure, outlined in Algorithm 2, takes as inputs the model weights W, gradients VW, sensitivity ratio \u03b1, layer sensitivity $L_{sens}$, sub-sampling rates R, a loss threshold $L_{th}$, and the number of top layers n, and outputs a weight subset $W_{sub}$.\nThe process begins with initializing a list, $L_{sub}$, to record losses (line 1). The top n layers are selected based on $L_{sens}$, derived in Section 5.2.1 (line 2). For each of these top layers, the algorithm iterates through sub-sampling ratios r, computing the perturbation loss induced by bit flips (lines 3-14). Specifically, the number of bits to flip, k, is computed as a fraction of the total parameters in the layer using Equation 5 (line 5). Sensitivity scores $S^{(l)}$ are then calculated for layer l using the SSCORE function (line 6), and the indices $I^{(l)}_{hybrid}$ of the Top-k scores are identified (line 7). These indices, along with the corresponding weights $W^{(l)}$ and a specified bit position pos, are input to the BFLIP function, which returns the perturbation loss L (line 8).\nIf L > $L_{th}$, this loss, along with layer index l, k, and r, is stored in $L_{sub}$, and further iterations for that layer are halted (lines 9-12); otherwise, iterations continue until R is exhausted. After completing this sub-sampling procedure, the loss data in $L_{sub}$ is sorted by k (line 15), yielding an ordered set where the smallest weight subset achieving the target loss is prioritized. The layer l and subset size k for this smallest subset are then extracted (lines 16-17). Sensitivity scores $S^{(l)}$ are recalculated for layer l (line 18), and the indices $I^{(l)}_{hybrid}$ corresponding to the Top-k scores are selected (line 19). Finally, the identified subset $W_{sub}$ for layer l and $I_{hybrid}^{(l)}$ is returned as the algorithm's output."}, {"title": "5.2.3. Weight-set Optimization - GenBFA", "content": "After identifying the weight subset $W_{sub}$, further refinement is essential given that LLMs typically encompass billions of parameters and tens of billions of bits as discussed in Section 4.4. Consequently, an attack, even with the identified weight subset, may necessitate targeting thousands or even millions of parameters\u2014implying tens of millions of bit-flips\u2014which would be prohibitive from an attacker's perspective. Therefore, to address this issue, we systematically reduce $W_{sub}$ to $W_{opt}$ ($W_{opt}$ \u2282 $W_{sub}$) by isolating a smaller subset of parameters capable of achieving comparable degradation in model performance. To address this objective, we design a novel algorithm inspired by evolutionary strategies, GenBFA, tailored for optimizing the parameter set to identify a minimal yet highly critical subset of weights. The GenBFA maximizes the impact-to-size ratio of the selected parameters, thereby enhancing the efficiency and effectiveness of the AttentionBreaker attack on LLMs.\nObjective Function and Fitness Evaluation: To formalize the weight subset optimization problem, let us define an initial weight subset $W_{sub}$ and a target model loss threshold $L_{th}$. The objective is to identify a subset $W_{s_i}$ \u2282 $W_{sub}$ such that bit-flipping the weights in this subset maximizes the model's loss while minimizing the subset's cardinality, which reflects the number of bit-flips required to achieve a corresponding model loss, thus providing a measure of the attack's efficiency. Each candidate solution $W_{s_i}$ = {$w_{j_1}, w_{j_2},...$} is therefore defined as a subset of $W_{sub}$.\nFor each candidate $W_{s_i}$, we compute a fitness value f($W_{s_i}$) as follows. First, we define the sign function sgn($L_{s_i}$ - $L_{th}$) based on whether the model loss $L_{s_i}$ (evaluated after bit-flipping the weights in $W_{s_i}$) meets or exceeds the threshold $L_{th}$:\nsgn ($L_{s_i}$ - $L_{th}$) = {+1, if $L_{s_i}$ - $L_{th}$ \u2265 0, -1, if $L_{s_i}$ - $L_{th}$ < 0.\\\nUsing this sign function, we define f($W_{s_i}$) as:\nf($W_{s_i}$) = sgn ($L_{s_i}$ - $L_{th}$) \u00b7 $\\frac{L_{s_i}}{cardinality(W_{s_i})}$\\\nThus, this formulation maximizes model loss while minimizing the number of bit-flips, effectively balancing loss maximization with subset cardinality.\nPopulation Initialization: The population P comprises of m candidate solutions, defined as P = {$W_{s_1}, W_{s_2}, ..., W_{s_m}$}. The initial solution $P_{0}$ is set as the weight subset $W_{sub}$, and each subsequent solution $P_{j}$ is created by applying a mutation operation to $P_{0}$ as shown in Figure 4:\n$P_{j}$\u2190 Mutate($P_{0}$).\nMutation Operation: In the mutation process, each solution $P_{j}$ is assigned a unique mutation rate $\\rho_{j}$, randomly chosen per solution such that $\\rho_{j}$ < \u03bc, the mutation rate. $\\rho_{j}$ then governs the probability of removal for each weight in $P_{j}$. Formally, for each weight $w_{ji}$ \u2208 $P_{j}$, the mutated weight $w'_{i}$ is defined as:\nw'{$_{i}$} = {\u00f8, with probability $\\rho_{j}$, $w_{ji}$, otherwise.\nIn this context, \u201cmutation\u201d refers specifically to reducing the solution set's cardinality rather than its traditional biological connotation. This reduction-driven mutation is designed to identify compact subsets that either preserve or improve model loss relative to the original subset. By systematically exploring minimal yet impactful configurations, this mutation strategy facilitates efficient traversal of the solution subspace and enhances the identification of subsets that sustain the desired adversarial effect on model performance.\nElitism and Selection: At each iteration (t) of the optimization procedure, all solutions in P are sorted by their fitness values:\n$P_{best}$ = $W_{S_{best}}$ = arg min f($P_{j}$)\n$P_{j}$\u2208P\nwhere $W_{S_{best}}$ or $P_{best}$ represents the solution with the highest fitness (i.e., most efficient in degrading model performance). Elitism dictates that $P_{best}$ is carried forward unchanged into the next generation, ensuring the highest-quality solution is preserved. For the remaining solutions, a tournament selection strategy is employed to choose pairs of solutions as parents for crossover. For selecting each parent, this involves choosing two random solutions $P_{a}$, $P_{b}$ \u2208 P, and selecting the one with better fitness:\n$\\Pp$ = arg min{f($Pa$), f($P_t$)}.\nThis strategy is chosen over a random strategy due to its capability to balance exploration and exploitation in the search space, thereby enhancing convergence toward an optimal solution.\nCrossover Operation: To generate new solutions, two-parent solutions, $P_{p_1}$ and $P_{p_2}$, are selected, followed by a crossover operation executed with probability $p_{c}$. During crossover, the current best solution $P_{best}$ is integrated to propagate its beneficial traits forward. Two offspring, $P_{o_1}$ and $P_{o_2}$, are produced by combining $P_{best}$ with each parent $P_{p_1}$ and $P_{p_2}$ independently.\nFor offspring $P_{o_1}$, each gene $g_{i}$ is chosen stochastically from either $P_{p_1}$ or $P_{best}$, represented as:"}, {"title": "6. Experimental results", "content": "This section presents experimental results showcasing the effectiveness of AttentionBreaker across different model precisions, types, and tasks."}, {"title": "6.1. Experimental Setup", "content": "For our experiments, we utilized a range of models, including the open-source LLaMA3-8B-Instruct [21], [24], Phi-3-mini-128k-Instruct [25] and BitNet [26]. The evaluation of LLMs was conducted using standard benchmarks like MMLU [27] and the Language Model (LM) Evaluation Harness [28], both of which test the models' ability to reason and generalize across a variety of tasks. The tasks we used from the LM Harness used for LLM performance analysis are \"AddSub\u201d, \u201cArcE\u201d, \u201cArcC\u201d, \u201cBoolQ", "HellaSwag": "MathQA", "PIQA": "SocialIQA", "SVAMP": "OpenBookQA", "GSM8k": "indexed from 0 to 10). As multi-modal models continue to gain prominence, we extended our evaluation to include a Vision-Language Model (VLM), LLaVA1.6 [29]. The performance of this VLM was assessed using the VQAv2 [30] and TextVQA [31] benchmarks, which are widely recognized metrics for quantifying multi-modal reasoning capabilities. To demonstrate the versatility of AttenBreaker, we test it against several quantization formats, including the standard 8-bit integer (INT8), normalized float 4 (NF4), and an advanced 1.58-bit precision format implemented using the BitNet framework [26]. Please note that all these models are weight-only quantized, and therefore, we will use notations W8, W4, and W1.58, respectively. These lower-bit precision formats lower computational requirements and memory usage while maintaining their performance in metrics such as perplexity for LLMs and classification accuracy for VLMs. In our experimental setup, only the attention and the Multi-Layer Perceptron (MLP) layers of the models are quantized. In this study, attacks are executed only for these layers, and their effects are analyzed.\nThe experimental setup utilized 4x NVIDIA A100 GPUs, each equipped with 80GB of memory, and operated under CUDA version 12.2.\nTo evaluate the model performance, we calculate the perplexity score [32] and accuracy on a variety of benchmarks. Perplexity is a measure of how well a probability model predicts a sample. It is calculated as the exponential of the average negative log-likelihood of a sequence as shown in Equation 14.\nPerplexity = exp { -\\frac{1}{N} \u03a3_{i=1}^{N} log P(x_{i}) }\\\nwhere N is the number of words, and P(xi) is the probability assigned to the i-th word by the model. A lower perplexity indicates better performance. Perplexity is calculated on the Wikitext [33] dataset.\nAccuracy on benchmarks is another critical metric, especially for tasks like classification [34]. It is the ratio of correctly predicted instances to the total instances:\nAccuracy = $\\frac{1}{N}$\u03a3 I($y_{i}$ = $\u0177_{i}$)\nWhere N is the total number of tasks. \u0177i is the predicted label for the i-th task. yi indicates the true label for the i-th task. I is the indicator function that returns 1 if the predicted label matches the true label and 0 otherwise."}, {"title": "6.2. Results and Analysis", "content": "This section presents the experimental results demonstrating the efficacy of the AttentionBreaker framework. Primary outcomes are highlighted here, supported by intermediate analyses in Section 6.3."}, {"title": "6.2.1. AttentionBreaker on 8-bit Quantized LLMs", "content": "Here, we present the efficacy of the proposed AttentionBreaker framework on 8-bit weight quantized (W8) LLMs. Table 2 compares the effects of Random Bit-Flip Attack (Random BFA) and AttentionBreaker on 8-bit versions of LLM across several benchmarks. For each model, the precision, number of parameters, and bit-flip Count required to disrupt performance are listed in the Table. Benchmark results show that AttentionBreaker consistently reduces accuracy to zero across datasets, proving more efficient than Random BFA in degrading model performance with fewer bit-flips. The results highlight the impact of bit-flips on key model performance metrics, including perplexity, MMLU, and LM harness benchmarks.\nFor instance, LLaMA3-8B-Instruct W8 suffers accuracy drop to 0% on MMLU tasks from the original accuracy of 67.3% by merely 3 bit-flips ($4.129 \u00d7 10^{-9}\\%$ bit-flip rate) as captured in Figure 6a. At the same time, the perplexity measure increases from 12.14 to $4.72 \u00d7 10^{5}$ post-attack. In comparison, random bit-flips require 107\u00d7 (\u2248 0.1% bit-flip rate) more bit-flips to accomplish a similar level of model degradation. A similar trend is observed for Phi-3-mini-128k-Instruct W8 models as well."}, {"title": "6.2.2. AttentionBreaker on 4-bit quantized LLMs", "content": "In this section, we present the performance of AttentionBreaker on 4-bit weight quantized (W4) LLMs. As shown in Figure 6b, the attack on LLaMA3-8B W4 demonstrates that $7.8125 \u00d7 10^{-8}\\%$ or 28 bit-flips results in a perplexity increase from 12.60 to 3.8\u00d7104, with the MMLU score dropping from 60.3% to 0. A similar trend is observed for Phi-3-mini-128k-Instruct W4 models as well. This showcases the efficiency of the proposed AttentionBreaker framework."}, {"title": "6.2.3. AttentionBreaker on low-precision LLM", "content": "We further evaluate the effectiveness of our approach for ultra-low precision models, such as BitNet W1.58 [26]. BitNet W1.58 represents a 1.58-bit variant of LLMs where each model parameter (or weight) is constrained to ternary values {-1,0,1}. Given their low precision, these models are intuitively expected to be inherently resistant, as a single bit-flip induced error has limited impact. Employing our approach, we observe that 45811 or 5 \u00d7 $10^{-4}\\%$ bit-flip rate is required to degrade the model performance to near 0% effectively as demonstrated in Figure 6c. In comparison, random bit-flips require $10^{5}\u00d7$ (\u2248 1% bit-flip rate) more bit-flips to accomplish a similar level of model degradation. This demonstrates the efficacy of AttentionBreaker in efficiently attacking low-precision models as well."}, {"title": "6.2.4. Attention Breaker on VLM", "content": "We demonstrate the efficacy of AttentionBreaker on multimodal models such as VLMs, which integrate a vision encoder with a language model to enable multimodal tasks, including image comprehension, alongside text-based tasks. When applying our attack methodology to the LLaVA1.6-7B W8 model, which comprises 7.57 billion parameters, we observe that a minimal bit-flip rate of $2.394 \u00d7 10^{-8}\\%$-equivalent to only 15 bit-flips-causes a substantial degradation in performance. Specifically, the VQAv2 and TextVQA scores, initially at 81.4% and 64.2%, respectively, drop to 0% following the attack. In comparison, random bit-flips require ($10^{7} \u2248 0.12\\%$ bit-flip rate) more bit-flips to accomplish a similar level of model degradation. This result underscores the efficacy of the AttentionBreaker framework in exploiting latent vulnerabilities within multimodal LLMs as well."}, {"title": "6.3. Intermediate Results", "content": "This section outlines intermediate results that validate assumptions and inform the main findings in Section 6."}, {"title": "6.3.1. Assessing Layer Sensitivity", "content": "Based on the layer sensitivity analysis described 5.2.1, we obtain layer sensitivity metrics. Layers are subsequently ranked according to this sensitivity measure, from the most sensitive\u2014those yielding the highest impact on loss as calculated through cross-entropy [35"}]}