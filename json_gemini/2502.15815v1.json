{"title": "Theoretical Physics Benchmark (TPBench) - a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics", "authors": ["Daniel J.H. Chung", "Zhiqi Gao", "Yurii Kvasiuk", "Tianyi Li", "Moritz M\u00fcnchmeyer", "Maja Rudolph", "Frederic Sala", "Sai Chaitanya Tadepalli"], "abstract": "We introduce a benchmark to evaluate the capability of AI to solve problems in theoretical physics, focusing on high-energy theory and cosmology. The first iteration of our benchmark consists of 57 problems of varying difficulty, from undergraduate to research level. These problems are novel in the sense that they do not come from public problem collections. We evaluate our data set on various open and closed language models, including 03-mini, o1, DeepSeek-R1, GPT-40 and versions of Llama and Qwen. While we find impressive progress in model performance with the most recent models, our research-level difficulty problems are mostly unsolved. We address challenges of auto-verifiability and grading, and discuss common failure modes. While currently state-of-the art models are still of limited use for researchers, our results show that AI assisted theoretical physics research may become possible in the near future. We discuss the main obstacles towards this goal and possible strategies to overcome them. The public problems and solutions, results for various models, and updates to the data set and score distribution, are available on the website of the dataset tpbench.org.", "sections": [{"title": "1 Introduction", "content": "Automated mathematical reasoning at research level with AI in theoretical physics may now be within reach. Novel large language model (LLM)-based AI systems, powered by improved AI reasoning techniques at training and inference time, are potentially powerful tools for the theoretical physics community. If substantial parts of the theoretical research process could be performed by AI, this would allow to significantly accelerate progress in theoretical physics. If AI could act as a fast, reliable and skilled research assistant that can perform theoretical calculations and solve mathematical problems, human researchers could cover substantially more theoretical ground, evaluate more ideas for their promise, and thus make more theoretical discoveries. Even without super-human intelligence, an AI \"craftsman\" would allow humans to outsource tedious calculation work and to focus more on creative aspects of the theoretical research process.\nRecent advancements in LLMs have allowed models to solve progressively more difficult tasks that require abstract mathematical reasoning. While high-school level math competition benchmarks like MATH [1] are almost saturated by current models, the focus has recently turned to graduate level and research level mathematics. A main data set in this domain, the recently introduced FrontierMath [2], which contains research level difficulty problems, is still mostly unsolved by frontier models. In theoretical physics (TP), which also requires extensive abstract mathematical reasoning, there has been comparatively less work than in mathematics. Existing benchmarks which include physics such as JEEBench [3], OlympiadBench [4] and PhysicsQA [5], cover mostly high-school-level problems from college entrance exams or competitions. There is little existing work on mathematical reasoning for theoretical physics at graduate or research level. An exception is [6], where the authors evaluate the performance of large language models for symbolic calculations in quantum many-body physics, however in the narrow context of a specific physical setting. Very recently, the Humanity's Last Exam dataset [7] (HLE) appeared as a multi-domain benchmark that includes problems from theoretical physics. We provide a more complete list of available data sets in Sec. 5.1.\nIn the present work, we build a data set to test theoretical physics reasoning skill over a broad range of difficulty. We aim to answer the following questions:\n\u2022 How good is the current state-of-the-art AI for problem-solving in TP? Are existing models useful for research-level reasoning?\n\u2022 What are the most common failure modes? For example, are models performing correct reasoning but fail mostly at algebra (at which LLMs are known to perform poorly)?\nTo answer these questions, we created a new benchmark data set TPBench of theoretical physics problems of varying degree of difficulty, from advanced undergraduate to research level. Our problems are novel, in the sense that they do not come from public problem collections (see Sec. 2.5 for detailed comments). For graduate level and research problems we focus in particular on problems from high-energy physics and cosmology. An important property of our data set is that it provides a continuum of problem difficulty, from easy to research level, which few mathematical data sets do. This allows us to compare the performance of different models over a wide spectrum of difficulty. We invite the reader to skip ahead to App. C to get an impression of the difficulty of these problems. Before discussing our data set in detail, we begin with some general remarks about reasoning for TP and its relation to AI models.\nDifferences between reasoning in math and TP. Because TP is extremely broad and math is arguably even broader, any summary discussion of the differences between mathematical and physics reasoning is unlikely to be accurate in many examples in a generic comparison set. Nevertheless, in terms of modern graduate level and higher physics and mathematics comparisons, several aspects typically stand out.\n\u2022 Mathematical reasoning tends to focus on establishing exact broad statements constructed within a rigid logical framework, while TP reasoning mostly deals with approximate narrower statements constructed within a logical framework in which some of the less quantitatively relevant details are left unspecified but \"most likely\" can be filled in such that the statements can be made arbitrarily precisely"}, {"title": "2 Properties of TPBench", "content": "We have curated a dataset of problems and associated solutions in main areas of TP. For research level problems we currently focus on high-energy theory and cosmology, the main expertise of the authors. Problems in our collection should have the following properties (similar to FrontierMath [2]):\n\u2022 The problem is well-posed and the solution to the problem is unambiguous. An expert in the field, after reading the solution, should not have any objections.\n\u2022 The problem is original. The solution to the problem cannot be easily found in the existing literature.\n\u2022 The answer should be auto-verifiable. This is easily achieved for numerical answers or simple algebraic expressions, but more difficult for tensor expressions. We discuss this property further below.\n\u2022 It should not be possible to guess the answer or remember it from the literature, despite a wrong reasoning chain.\nIt is hard to strictly enforce all these conditions in TP, as we discuss further below. Problem originality and the possibility to guess the answer can be judged differently by different researchers. For this reason we also provide metadata for each problem individually. We point out potential shortcomings in instances where we are aware of them. We include problems of varying degrees of difficulty, from undergraduate to graduate and to research problems. Naturally, research problems are more difficult to create, especially when requiring the answers to be novel and unpublished. Furthermore, more difficult problems are often more novel than easier problems (since the space of possible problems grows rapidly with their complexity). We discuss the aspect of novelty of our problems in more detail below, as well as individually in the problem metadata. We also make sure that our problems do not contain steps where a human would need a calculator to solve them (e.g. no floating point operations).\nWe now discuss the attributes of our data set in more detail, including their statistical distribution. We aim to enlarge and diversify the data set further in the future. We also provide ten sample problems in App. C and we encourage the reader to browse the problems to get an impression of the whole data set."}, {"title": "2.2 Problem Statistics", "content": "The dataset is categorized into five difficulty levels: 1 easy undergrad, 2 - undergrad, 3 - easy grad, 4 grad, and 5 - research. This classification ensures that the dataset can accommodate a wide range of use cases, from introductory studies to cutting-edge research challenges. The distribution of problems across these difficulty levels is detailed in Table 1. For difficulty level 1-4 this means that the problem could appear in a homework problem or exam for students. For level 5, this problem could appear as a nontrivial step in a publication: i.e. our research level problems are sub-problems that would constitute a part of a publication, and are not by themselves large enough to constitute an entire publication. Solving level 4 and 5 problems would make models useful for theoretical research, but would not mean that models could write their own publishable papers (by a significant margin). Indeed, one of the most important steps in TP research is establishing why a particular question is important and organizing a string of level 5 type of steps to answer that question. Future iterations of this data set could include more open-ended research problems, more reminiscent of a research publication."}, {"title": "2.3 Auto-Verification of Solutions", "content": "To automate the evaluation pipeline, we developed a system inspired by how coding competitions validate their results. We introduced the requirement that the final answer to each problem be provided as a Python callable with the specified signature. We then developed a simple automatic (not LLM-based) grading agent that, given the model's answer and the correct solution, extracts the code, creates, and executes a consistency-check script. This approach allows for efficient evaluation of algebraic answers and automatically ensures that equivalent correct answers are classified as such. Additionally, it is flexible enough to verify answers involving a variety of special functions or answers that involve several outputs. In some problems, the natural system of units (c = \u0127 = 1) is specified in the prompt, while in other cases we pass constants of nature as function arguments to be unit agnostic. Alternatively, we could have adopted other automatic verification strategies. We could have provided numerical test cases in the prompt, but this would have led to lengthy problem statements, floating point operations, and much less flexibility. Another option is to consider multiple-choice answers, but this would make it easier to guess the answer without detailed understanding. Yet another possibility is to use another LLM as a grading agent and instruct it to compare the given solution to the true one. However we found that this approach is very error prone and LLMs are often not able to check mathematical equivalence of expressions (see below).\nOur proposed scheme gives the flexibility to check the variety of classes of answers exactly. The verification process consists of three components:\n1. Code Extraction: The system extracts Python functions from both the model's solution and the expert solution.\n2. Test Case Execution: Both functions are executed with identical test inputs across multiple parameter combinations.\n3. Output Comparison: Results are compared numerically with appropriate tolerances for floating-point arithmetic.\nEach problem in our dataset is accompanied by a comprehensive set of test cases, carefully designed to probe both the physical validity and mathematical correctness of solutions. These test cases span different parameter ranges (e.g. negative or complex arguments where appropriate), to ensure thorough verification."}, {"title": "2.4 AI-Based Holistic Grading of the Entire Solution", "content": "In addition to auto-verification, we also employ AI-based grading. In this process, the grader model has access to both the expert-labeled solution and the LLM-generated solutions from a separate model, and is tasked with assigning grades (A-D). This approach mirrors how a human teaching assistant grades homework, where partial credit is given for correct reasoning steps, even if the final solution is incorrect. Moreover, holistic grading can identify instances where a solution arrives at the correct answer using incorrect reasoning, which occurs in a small number of our problems. While holistic grading is conceptually preferred, we observe significant disagreement between different grader models as well as humans."}, {"title": "2.5 Novelty and Difficulty of Our Problems", "content": "Most of the problems presented here are constructed based on those given in standard courses as well as unpublished research related notes. For example, the solution to the research-level problem \"One pole problem\" (see App. C.1), without steps explained, is given in a footnote of [30]. Most of the research level problems would be readily doable by a good TP graduate student, and some of these are not much different from hard problems in graduate courses whose problems and solutions can be found publicly. However, we have made significant efforts to construct or modify problem statements so that the answers cannot be found by web search. Most of the research-level problems use typical or not-too-atypical notation to simulate a research setting, although this may facilitate literature recall (rather than reasoning) by the model.\nThe difficulty of a problem can vary along different axes, i.e. problems may be easy or hard for different reasons. We aimed to provide a sampling of this space:\n\u2022 Some of the problems are difficult for a human researcher because they may not know that a similar problem has already been solved in the literature. Indeed, almost all solution techniques used in literature evolve over time incrementally as people build upon results of previous related computations. This gives LLMs an advantage for many problems, especially if the problem statement makes it clear what literature knowledge is required (which we try to avoid). Fortunately, publications often omit minor reasoning steps, and asking the model for detailed mathematical derivation can thus reveal such literature memory. For examples of models solving difficult problems by using \"superhuman literature knowledge\" see Sec. 4.6. Indeed, a key challenge in constructing this data set was to avoid this phenomenon as much as possible, to reveal true reasoning."}, {"title": "2.6 Public and Private Data Set and Data Leakage Concerns", "content": "We make 10 of our problems and solutions public (see App. C and tpbench.org), two for each difficulty level, such that they can be used to understand the data set, develop inference algorithms and examine failure modes. Naturally these problems will be part of future training data. To deal with this challenge, we also keep a large part of our data set private, currently about 50 problems. If you would like to evaluate your model on our private data set, please contact the authors directly.\nGuaranteeing that private data does not end up in future training data is challenging. OpenAI, which we have used extensively, adds user interface chats to its training data but does not add API calls. Correspondingly, we have generally used API calls for querying problem solutions. However, in early phases of this projects, some problems were run in the user interface. In future iterations of this project, we will emphasize data leakage control further, especially for research level problems. We note that a small number of research problems is sufficient to evaluate significant model progress, as long as for these problems data set leakage control and originality of the problem are flawless. For our current problem set, we only enforce that problems (and especially solutions) do not appear publicly accessible online. Furthermore, we took particular care that expert solutions to problems were never passed to the ChatGPT user interface, where they could be added to future training data."}, {"title": "3 Model Performance Evaluation", "content": "In this section, we evaluate the performance of several leading models on our dataset, TPBench, across five different difficulty levels, ranging from undergraduate to research-level problems. Closed-source models include OpenAI GPT-40, o1, and o3-mini [20, 19, 21]. Open-source models which we were able to run locally on our hardware include small and intermediate sized Llama 3.1, Qwen 2.5, and Qwen-QwQ, which is an experimental LLM that focused on advancing reasoning developed by the Qwen Team [32, 33, 34]. We also include the recent open-source reasoning model DeepSeek R1 [35] and its base-model DeepSeek V3 [36] which we ran on Together AI API. Finally, we tried to solve a subset of our research problems with OpenAI's Deep Research, including the problem in App. C.1, primarily to spot solutions that could be found online. Deep Research was not able to solve any of these research problems. We believe our subset of models is representative of the spectrum of current LLM capabilities."}, {"title": "3.1 Results for Auto-Verified Solutions", "content": "We begin by discussing the answer-only results, which are the key empirical results of this paper. Our results are obtained using zero-shot reasoning where the model is given the problem statement and expected to reason through it without any prior examples. In fact, few-shot learning can degrade general performance in reasoning models [35]. We have experimented with prompt optimization, but found no significant differences (see App. B for our prompts).\nTable 3 presents the performance of each model across various difficulty levels, ranging from easy undergraduate problems (Level 1) to research-level problems (Level 5). The table reports the percentage of problems solved by each model. The columns labeled \"avg@5\" represent the average score across five attempts, while the \"best@5\" columns correspond to the average score of the best attempt out of five attempts. We visualize the \"average of five\" solution percentage in Fig. 1 (strong models) and Fig. 2. Finally, for our public problems, the individual results of models are given in App. C. For example, we include one level 5 research problem that top models can solve and one that they cannot.\nFor the top models, o1, o3-mini and DeepSeek R1, undergraduate problems (level 1 and 2) are now essentially solved, with performance of 95% to 100% for the oX models. For easy graduate problems (level 3), the performance is around 80%. For our level 4 graduate problems, some of which could appear in research investigations, the best models ol and 03-mini solve around 50%, with 03-mini slightly beating o1. Research problems are mostly unsolved at this stage with a score around 15%. 01 slightly beats 03-mini here, which may be due to it having a larger literature knowledge to draw on.\nAmong mid-range models, GPT-40 and DeepSeek-V3 perform similary. They are between one and two levels of difficulty less powerful than the top models. Midrange models are essentially unable to solve problems above easy graduate level. Finally, lower parameter public models, which have the advantage that researchers can run them on invididual GPUs, cannot solve problems above undergraduate level. We also provide further model evaluation statistics of the data set on the website, including a unified model score over all difficulties."}, {"title": "3.2 Results for Holistic AI-Based Grading", "content": "Table 4 presents the results for the holistic AI-based grading, which involves assigning letter grades (A to D) based on the quality of reasoning and correctness of the solution. This grading is not limited to the final answer but considers the overall approach taken by the model in solving the problem. We have used GPT-40 as a grader, as a currently mid-range model. We chose this model for cost efficiency reasons, and in the future we intend to use the most powerful model as a grader. The model was provided the grading prompt (App. B), the expert solution, and the model solution to grade, similar to the way a human teaching assistant would work."}, {"title": "3.3 Augmenting Inference With Python to Reduce Algebraic Mistakes", "content": "We experimented with instructing models to break down calculations into smaller steps and verify these with python. Using a code interpreter was previously found to be beneficial in reducing algebraic mistakes in calculations (eg. [38]) Our approach was based on the MathChat [39] framework and prompt tuning. We instructed the model to write python (particularly SymPy) code for each calculation step and verify its result using this code. In a few cases, for low difficulty problems, our approach was able to spot and correct mistakes. However, more often the approach disrupted the reasoning chain and led to worse results. For complicated problems, models struggled to identify steps that can be checked with SymPy. We note that our problems do not include floating point calculations, where verification would be straightforward, but require more complicated algebraic operations. Recently, the FrontierMath paper [2] included a set of prompts to encourage LLMs to verify with python, but noted that advanced models barely made use of this possibility. While human theorists do sometimes check their results with computer algebra systems, especially Mathematica, this process is not straightforward, and there is likely limited existing training data for this approach. We aim to experiment with few-shot inference or fine-tuning in the future, showing the model handcrafted examples of SymPy or Mathematica verification in the prompt. Since our current MathChat-based results are not stable we chose to defer this direction to future work."}, {"title": "4 Failure Mode Analysis", "content": "We now discuss common classes of mistakes. We present a few examples highlighting the various types of errors that the LLMs make while attempting to solve problems in theoretical physics. We broadly classify these errors into four classes as shown below. Our examples mostly draw from GPT-40 and o1 model results."}, {"title": "4.1 Background Knowledge of the Model", "content": "Background knowledge is a strength of LLMs. Problem authors were impressed by models' ability to recall relevant mathematical definitions that were not included in the problem but are known to practicing researchers. This ability makes it much easier in principle to solve problems than with a computer algebra system like Mathematica. For example, consider the level 5 cosmology problem from App. C.2:\nWhile well-defined for a cosmologist, the problem does not define the mathematical quantities in detail, and would be hard to interpret by a non-cosmologist. Advanced models correctly recalled the required definitions and generally set up the problem correctly.\nHowever, while LLMs generally recall key definitions of various sub-fields of theoretical physics, they frequently encounter difficulties in accurately recalling more detailed mathematical information, as illustrated by the following two examples.\nIn one of the solutions to an undergraduate QM problem, the QwQ model incorrectly retrieves information about the Clebsch-Gordan coefficients. Specifically, it claims"}, {"title": "4.2 Algebraic Mistakes", "content": "A major challenge for models is to perform correct algebraic calculations. Consider the following relatively easy math problem that appears as an individual step in one of our problem solutions."}, {"title": "4.3 Logical Mistakes", "content": "We frequently observed that LLMs struggle to accurately account for the validity and applicability of advanced mathematical concepts, such as incorrectly applying theorems, misinterpreting definitions, or failing to recognize the limitations of certain mathematical techniques. Consider the following mathematical problem, which we will use to discuss logical errors made by the oX series models in their attempted solution."}, {"title": "4.4 Hallucinations", "content": "Lastly, we present two instances where the LLM models generated new rules to obtain solutions that match with existing results in the literature. The following expression generated by GPT-40 represents an arithmetical hallucination error:"}, {"title": "4.5 Performance of Pre-O-Series Models", "content": "In our experience, models that are not explicitly trained for reasoning (i.e. before the oX series) can be used to assist researchers that reason through a simple problem, but with significant shortcomings. Consider the following easy mathematical subproblem that appeared in one of our recent works [29] in the context of cosmology, which we show here in simplified notation."}, {"title": "4.6 Performance of o1, 03-mini, and DeepSeek Reasoning Models", "content": "From evaluating the output solutions generated by advanced LLM models such as o1, 03-mini and DeepSeek (DS), we observe that these models exhibit significantly stronger reasoning capabilities compared to other LLMs tested in our study. Notably, these models can perform more difficult algebraic manipulations, identify different components of a problem, and connect them with established concepts in the literature. This ability allows them to make meaningful progress in research-level problems, including those from topics such as quantum field theory (QFT) and String theory, by pinpointing key aspects of the question and recalling relevant background knowledge.\nHowever, these models still struggle with detailed and systematic logical reasoning. When tasked with solving our Level-4 and Level-5 problems, these models often perform well in the initial phase of problem solving, demonstrating promising insights. Yet, for problems requiring extensive calculations combined with step-by-step logical rigor (e.g. loop integrals in QFT, tensor manipulations in general relativity) and systematic justfication of the assumptions, their performance deteriorates significantly. Our analysis of multiple solutions suggests that when intermediate steps become too complex, the models (including DS) often resort to literature memory from pre-training rather than performing detailed calculations. Rather than explicitly detailing intermediate steps, the models often present only their final answer, recalling related literature knowledge without references or resorting to vague assertions such as \u201cafter a lengthy (but straightforward) calculation\" or \"a short calculation shows\". While the full CoT of the o-series models is not public, we have no evidence that the models genuinely perform relevant calculations internally in these cases.\nAs an illustrative example, when asked to compute the one loop anomalous magnetic moment of a fermion (e.g. [40]) including a contribution from a heavy scalar coupling, the model resorted to recalling existing solutions seen during pre-training rather than explicitly solving. However, it failed to recognize that the Yukawa interaction Lagrangian provided in our problem statement contained an additional factor of 1/\u221a2, which may deviate from the conventions in the literature. Consequently, its final answer overlooked this crucial modification. In a similar manner, when presented with the task of solving the Level-4 problem in C.4, all advanced models (OX, DS-R1) initiate their response by articulating their interpretation of the problem statement and correctly identifying its connection to the standard supersymmetric transformations within the free Wess-Zumino model, as extensively documented in the literature. Subsequently, these models produce their final solution from memory. However, a consistent error emerges across all responses: the absence of the critical \"negative sign\" as seen in the solution given in Eq. (108).\nThese inconsistencies suggest that models' solutions often fluctuate based on how their internal sampling mechanism recalls (pre-)training data, rather than adhering to a logically coherent problem-solving strategy. This underscores a fundamental issue: unlike a proficient researcher who would maintain logical consistency across different attempts, these models exhibit uncertainty in their outputs, lacking a clear measure of confidence in their solutions. Such limitations and the opaque structure of the training and inference process (especially of closed-source models) present obstacles to their applicability in research settings. It appears that successfully solved high-difficulty problems often benefit from the very deep and interconnected literature memory of these models, in addition with their ability to translate this knowledge to the problem setting. While this ability is useful for research, it may not be sufficient to create novel TP results without human assistance. In summary, current model performance perhaps resembles a student with superhuman literature knowledge but low intellectual rigor and technical expertise."}, {"title": "5 Related work", "content": "Despite significant advances in the mathematical reasoning capabilities of large language models, accurately solving reasoning problems in specialized domains, such as theoretical physics remains a persistent challenge. In math reasoning, the landscape of existing benchmarks has been instrumental for the evaluation of LLM reasoning capabilities and the development of more robust and interpretable reasoning strategies. We review related benchmarks in Sec. 5.1 as well as common strategies for eliciting more accurate reasoning from LLMs in Sec. 5.2."}, {"title": "5.1 Mathematical Reasoning Benchmarks", "content": "Recent progress in large language models (LLMs) has enabled these models to tackle increasingly complex tasks that demand high-level abstract mathematical reasoning. A significant body of work has focused on datasets for mathematical reasoning at the middle-school (e.g., [43]), high-school (e.g., [1]), or undergraduate level (e.g., [44]), which often cover arithmetic, geometry, or math word problems. Other benchmarks are focused on theorem proving [45, 46, 47]. For example, the recently introduced PutnamBench [45] provides a collection of formalized theorems from the Putnam competition, while MiniF2F [46] and FIMO [47] offer datasets of formalized proof problems drawn from competitions like the IMO, AIME, and AMC. In addition, ProofNet [48] comprises both natural language and formalized theorem statements and proofs at the undergraduate level. Complementary to these are natural language datasets that feature problems of varying difficulty [1, 49], as well as benchmarks like GPQA diamond [50], which are designed to be hard. Even more recently, the Humanity's Last Exam dataset [7] (HLE) is an industry-curated, multi-domain benchmark that includes very challenging problems, among them some from theoretical physics. However, problems in HLE are constrained to numerical answers or multiple choice formats, there is no spectrum of difficulty, and it is not specifically designed to probe reasoning capabilities in theoretical physics.\nWhile lower difficulty math benchmarks such as MATH [1] have nearly been mastered by current LLMs, the FrontierMath [2] dataset, which includes research-level problems curated by working mathematicians, remain largely unsolved. FrontierMath spans a range of difficulties from high-school to research level and features properties like auto-verifiability and rich metadata, design principles we have also incorporated into TPBench. However, Glazer et al. [2] provide limited information about the difficulty distribution and the specifics of the problems that have been solved by advanced models.\nIn the realm of physics, which also demands extensive abstract mathematical reasoning, the focus has been predominantly on high-school level challenges as seen in datasets such as JEEBench [3], OlympiadBench [4], and PhysicsQA [5]. Beyond undergraduate-level problems, very little work has addressed mathematical reasoning for theoretical physics. One notable exception is [6], which examines symbolic calculations, albeit within the narrow context of a specific class of quantum many-body physics problems.\nOur new dataset, TPBench addresses the gap in theoretical physics reasoning benchmarks beyond the undergraduate level. TPBench encompasses problems ranging from undergraduate to research level, with research problems reflecting challenges typical of those found in theoretical physics publications (rather than representing entire publications in themselves). Importantly, TPBench is designed to be independent of industry control, ensuring that the theoretical physics research community has access to a reasoning benchmark that is not susceptible to data leakage from future training data. We look forward to sharing this dataset with collaborators under appropriate data leakage controls."}, {"title": "5.2 Reasoning Capabilities of LLMs", "content": "Despite the remarkable fluency of LLMs in generating human-like text, their capacity to perform reliable multi-step reasoning remains a challenge [51]. Many LLMs still struggle with complex arithmetic and logical inference tasks. In this section, we review state-of-the-art methods, spanning both training-time and inference-time techniques that have been developed to boost the reasoning capabilities of LLMs.\nTraining-Time Methods for Improved Reasoning Training-time methods encompass all strategies where pre-trained language models are fine-tuned or otherwise modified to improve their reasoning capa-bilities. The most popular approaches in this category rely on either supervised fine tuning [52, 53, 54], or"}, {"title": "6 Discussion", "content": "We developed the dataset TPBench to test TP reasoning capabilities of AI models. We note that our problems were not constructed to match a particular target error rate (03-mini and DeepSeek R1 appeared after most problems were finalized), but rather to reflect real problems encountered by theoretical physicists at each career level. Our theoretical physics reasoning results are consistent with studies from more general benchmarks, and illustrate the speed of progress in AI. The most advanced models are able to solve some problems at graduate level, but are not yet capable of solving most research level problems. While advanced models demonstrate remarkable proficiency in algebraic and conceptual problem-solving, they struggle with structured logical reasoning and transparent step-by-step calculations, particularly in complex, research-level problems. Their reliance on literature recall without verification or referencing and their lack of consistency in detailed reasoning remain key limitations in their problem-solving capabilities. We discussed these shortcomings and summarized common failure modes.\nProgress has been rapid, even during the creation of this data set. If models could solve level 5 research problems consistently, their impact on theoretical physics would be substantial. However, even then, AI models could not perform independent research without further developments. We now discuss some future directions related to our work, that could make LLMs more powerful for TP research.\nUpdates to the TPBench data set and score board. We will update the score board for novel SOTA models. Results will be published on the website of the data set tpbench.org. The website also contains additional model evaluation metrics, which assign a unified model score over all difficulties. We aim to add more problems to the data set in the future, both public and private problems. It would be particularly interesting to design more research problems which are clearly outside of the training data. This could be achieved by curating research problems specifically from the newest arxiv publications, before the current knowledge cutoff. We invite interested researchers to contribute new problems and collaborate on future TPBench updates (see website for details).\nAutomatic problem scraping from publication archives. To improve inference methods specifically for TP, for example by reinforcement learning of reasoning chains (e.g. Deepeek R1 [35", "35": ".", "83": "to our knowledge this has not been demonstrated to lead to performance gains in mathematical reasoning. The fact that models cannot point to a specific source for mathematical statements lowers their trustworthiness. Finally, inference methods that provide more information about uncertainty in individual steps would be particularly beneficial for difficult TP problems. This would pave the way for trustworthy, automated TP research assistants that reliably solve some aspects of a problem, but then ask for help for the parts they are uncertain about.\nDiagrammatic and spatial reasoning. Theoretical physicists like to reason using spatial diagrams such as Feynman diagrams or drawing integration contours. In principle, such diagrams can be encoded in some formal language and multi-modality for spatial reasoning may not be necessary. For example, some of our problem solutions include Feynman diagrams or integration contours encoded with the TikZ LaTeX library (e.g. Fig. 4). For some of our problems humans would have trouble reasoning through them without the ability to draw on some scratchpad. It would be interesting to see whether multi-modal language and spatial reasoning models could make models stronger. Visualizing the problem (e.g. \"running an example in your head\") is a common strategy and could be particularly powerful for models to develop truly novel ideas.\nTraining reasoning models on TPBench. While we designed TPBench for the evaluation of the reasoning capabilities of large language models, it would also be very interesting to curate a dataset for supervised fine-tuning or for reinforcement learning purposes. While we expect fine-tuning to increase the TP specific reasoning capabilities of LLMs, it is equally important to avoid data leakage to avoid problems that are later used for evaluation to seep into the training data. For this reason we choose not to publish all of our problems in TPBench at this time. Instead we encourage researchers who wish to have their models evaluated on TPBench to reach out to us.\nOpen-ended research problems. If models could solve well-posed problems such as the research problems in our collection reliably, this would speed up TP research projects considerably. However, a large part of research consists of arriving at well-posed problems, which are interesting to answer and can be answered. It could be possible to design more open-ended tasks, where the goal is to \"derive interesting results\" based on some set of initial constraints or observations. The AI model could suggest assumptions to"}]}