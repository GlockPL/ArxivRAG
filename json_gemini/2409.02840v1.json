{"title": "R2GQA: Retriever-Reader-Generator Question Answering System to Support Students Understanding Legal Regulations in Higher Education", "authors": ["Phuc-Tinh Pham Do", "Duy-Ngoc Dinh Cao", "Khanh Quoc Tran", "Kiet Van Nguyen"], "abstract": "In this article, we propose the R2GQA system, a Retriever-Reader-Generator Question Answering system, consisting of three main components: Document Retriever, Machine Reader, and Answer Generator. The Retriever module employs advanced information retrieval techniques to extract the context of articles from a dataset of legal regulation documents. The Machine Reader module utilizes state-of-the-art natural language understanding algorithms to comprehend the retrieved documents and extract answers. Finally, the Generator module synthesizes the extracted answers into concise and informative responses to questions of students regarding legal regulations. Furthermore, we built the ViRHE4QA dataset in the domain of university training regulations, comprising 9,758 question-answer pairs with a rigorous construction process. This is the first Vietnamese dataset in the higher regulations domain with various types of answers, both extractive and abstractive. In addition, the R2GQA system is the first system to offer abstractive answers in Vietnamese. This paper discusses the design and implementation of each module within the R2GQA system on the ViRHE4QA dataset, highlighting their functionalities and interactions. Furthermore, we present experimental results demonstrating the effectiveness and utility of the proposed system in supporting the comprehension of students of legal regulations", "sections": [{"title": "1 Introduction", "content": "The educational regulations of universities consist of documents regarding training regulations, provisions, and guidelines on current training programs that students must adhere to to complete their academic programs. However, a significant challenge lies in the potential length and complexity of these educational regulations, making it difficult to read and extract information. Searching for specific information from these documents can be time-consuming, posing difficulties for students and lecturers. Alternatively, students may search for the wrong document, leading to misinterpretations and consequential adverse effects on students.\nThe question-answering system (QAS) can help address the above problem. Similarly to search engines such as Google or Bing, the output of such a system is the answer to the input question based on the information available in the database. A question-answering system typically comprises two main components: a Document Retriever and a Machine Reader (or Answer Generator for abstractive questions). The document retriever queries relevant information related to the question, which is then passed to the Reader/Generator along with the question to generate an answer. Figure 1 shows the input of the USER question and the output of the BOT Assistant of the question answering system.\nIn the field of legal in Vietnamese, several question-answering systems have been developed. For example, in 2014, the vLawyer system was proposed by [1], a simple question-answering system with words in the answers directly extracted from documents. Therefore, it can be seen that very few question-answering systems in Vietnamese can provide answers with a human-like style.\nWhen addressing abstract responses using language models, there are several approaches. An approach involves extracting multiple spans from the context and concatenating them as part of the MUSST framework [2]. However, this method renders the responses less natural and diverse in language than human-like expressions. Another approach consists of passing the question and context through a generator module to produce complete answers (RAG). This method may result in less accurate output answers due to contextual overload, leading to noise. Furthermore, current answer generator models primarily perform text summarization tasks, which are not always suitable for answer extraction tasks."}, {"title": "2 Related Works", "content": "A question-answering system is a challenging task in natural language processing (NLP). Various types of systems have been developed to date. Based on the type of output answers, there are two popular systems in the field of NLP.\nFirst, question-answering systems with answers extracted from context (extractive question-answering). Some question-answering systems of this type include vLawyer [1], a simple question-answering system on Vietnamese legal texts proposed by Duong and Ho (2014) [1]. vLawyer consists of two components: Question Processing and Answer Selection.\nDrQA, which is designed for reading comprehension in open-domain question-answering, as proposed by Chen et al. (2017) [6]. BERTserini [7] is a question-answering system that combines two models: BERT [8] and Anserini. Anserini is an information retrieval tool that identifies relevant documents that are likely to contain the answer. BERT [8] (Bidirectional Encoder Representations from Transformers) is a language model that understands context and the relationships between words to extract answers from context retrieved by Anserini. MUSST [2] is a framework that is used to automatically extract answers from a given context. The answers of this framework are formed from multiple spans in the context to create human-like answers. This framework has two main modules: Passage Ranker and Question Answering.\nXLMRQA [9] is the first Vietnamese question-answering system with three modules: document retriever, machine reader, and answer selector). This question-answering system outperforms DrQA and BERTserini on the UIT-ViQuAD dataset [10]. ViQAS is a question-answering system proposed by"}, {"title": "2.1 Related Question Answering System", "content": ""}, {"title": "2.2 Related Dataset", "content": "Developing question-answering (QA) systems for specific domains requires specialized datasets tailored to domain knowledge and language. In the legal domain, several renowned datasets have been established and widely used. JEC-QA [19] is a comprehensive dataset comprising 26,365 multiple-choice questions, encompassing 13,341 single-answer questions (further divided into 4,603 knowledge-driven and 8,738 case-analysis questions) and 13,024 multi-answer questions (including 5,158 knowledge-driven and 7,866 case-analysis questions). BSARD [20]: BSARD was created by legal experts, BSARD comprises 1,108 questions derived from 22,633 legal articles. The dataset exhibits an average"}, {"title": "3 Dataset", "content": "In this section, we introduce how we constructed the dataset. Our dataset creation process consists of 6 phases: context collection (Section 3.1.1), guidelines creation (Section 3.1.2), creator agreement (Section 3.1.3), question-answer creation (Section 3.1.4), data validation (Section 3.1.5), and data splitting (Section 3.1.6). These six phases are illustrated in Figure 2."}, {"title": "3.1 Dataset Creation", "content": ""}, {"title": "3.1.1 Context Collection", "content": "We collected regulatory documents regarding the curriculum of a university in Vietnam. The documents were gathered in various formats, such as Word, PDFs, or images, so we converted them to Word using smallpdf.com\u00b9 or manually retyped them if the PDF file contains images. After converting all documents to the Docs format, we converted the tables into paragraphs using a predefined format.\nFollowing this process, we obtained 21 documents with an average length of 10.67 pages and 3,881 words per document. As the word count in each document is too large for language models, we divided the documents into smaller paragraphs (called articles) for convenience in dataset construction and"}, {"title": "3.1.2 Guidelines Creation", "content": "We relied on the guidelines from two datasets, UIT-ViQuAD [10] and ViRe4MRC [23]. These guidelines describe and provide detailed examples to help creators understand how to create questions and answers for the given problem consistently. The guidelines clearly outline different question types including \"How\", \"What\", \"Which\", \"Where\", \"Why\", \"When\", \"Who\", Yes/No, and other types such as \"How long\", \"How many\". Definitions and examples of each type of question are presented in the Table 1.\nThe guidelines cover various strategies for asking questions, such as asking questions from general to specific, asking questions in the order of the context before posing questions whose answers appear at multiple places, and posing \"Wh\" questions before \"Yes/No\" questions.\nIn this paper, we divide the answers into two categories: \"Extraction answers\" and \"Abstract answers\". The extractive answers have two types: single-span and multi-span. The extractive answers must contain complete information and be as concise as possible while present in the context (article). In the case of multi-span answers, the spans must be semantic equivalence and should not be concatenated from different parts of the context to form a complete sentence."}, {"title": "3.1.3 Creator Agreement", "content": "We have 7 creators, all university students from the same institution. These creators underwent training on the guidelines and performed multiple rounds of checks. The question-answer pairs must adhere to the guidelines, spelling, and structure of the sentence, ensuring diverse usage of reason types and question types. During each round of evaluation with 100 context-question pairs, creators must independently formulate answers. After that, creators will cross-check each other, provide feedback, and agree on answer writing in the regular meetings. We evaluated the similarity between the creators based on F1-score and BERTScore [25] metrics. After three rounds of evaluations with 300 questions, the average results of the 7 creators will be presented as shown in Table 3."}, {"title": "3.1.4 Question-answer Creation", "content": "The dataset consists of 294 articles divided into two parts: Part 1 includes the first 146 articles labeled by 4 creators, while Part 2 comprises the remaining 148 articles labeled by the remaining 3 creators. This division of annotations and articles ensures diversity throughout the question-answer creation process. This approach allows us to maximize information extraction from the articles across different aspects while preventing duplication in question-answer pairs as in the case of all 7 people labeling all 294 contexts.\nEach creator is required to generate at least 300 question-answer pairs in one week. The guidelines are strictly to ensure consistency across the dataset. We encourage creators to pose questions that involve challenging forms of inference, such as paraphrasing, inference from multiple sentences, and inference from a single sentence."}, {"title": "3.1.5 Data Validation", "content": "After each week of data creation, the creators will perform self-checks and cross-checks similar to the training phases in Section 3.1.3. During the self-check process, each creator will review the question-answer pairs from the"}, {"title": "3.1.6 Data Splitting", "content": "After validating the data, we partitioned the data set into three subsets: training, development (validation), and testing, with an 8:1:1 ratio. The balanced allocation between the development and testing subsets is intended to ensure a fair and precise evaluation of the model."}, {"title": "3.2 Dataset Analysis", "content": "In this section, we conducted an overview analysis of the dataset regarding aspects such as the number of articles and the length of texts within the dataset. The ViRHE4QA dataset comprises 9,758 question-answer pairs from 294 articles within the domain of university training regulations. We conducted statistical analysis on the dataset regarding aspects such as the number of documents, number of articles, number of question-answer pairs, average word count2 in documents, articles, questions, extractive length, and abstractive length of the ViRHE4QA dataset, comparing these with the UIT-ViQuAD 1.0 dataset as shown in Table 4."}, {"title": "3.2.1 Overall Statistics", "content": ""}, {"title": "3.2.2 Length-based Analysis", "content": "To understand more about our dataset and domain, we performed statistics on the number of question-answer pairs grouped by ranges of article length (Table 5), question length (Table 6), and answer length (Table 7). Articles with lengths ranging from 101 to 256 words accounted for the largest proportion, with 3,422 question-answer pairs. However, it should be noted that articles with lengths less than 100 words had the smallest number of pairs, and articles longer than 512 words ranked second highest with 2,306 question-answer pairs. This poses a challenge in our dataset as most current language models accept a maximum input of 512 tokens."}, {"title": "3.2.3 Type-based Analysis", "content": "In this section, we conducted an analysis of the question types and the answer types in the test set (976 samples). To ensure accuracy, we manually classified the questions following the guidelines in section 3.1.2, which include 9 question types: What, Who, When, Where, Which, Why, How, Yes/No, and Others;"}, {"title": "4 Our Proposed Method", "content": "In this section, we will present the question-answering system for abstract answers that we propose. This system consists of three modules: Document Retriever, Machine Reader, and Answer Generator. We named this system R2GQA, with an overall structure depicted in Figure 5."}, {"title": "4.1 Document Retriever", "content": "The Retriever module uses questions to retrieve contexts that contain answers or relevant information. These contexts are then fed into the machine reader module to extract answers. Additionally, the question scores corresponding to each context will be used to combine with the scores of the answers after the Reader module is executed to select the most accurate answer for the input question."}, {"title": "4.1.1 Lexical Retrieval", "content": "Retrieval methods based on lexical similarity employ the degree of overlap between a question and a document to determine relevance. BM25 and TF-IDF are two popular examples of this approach. However, these methods often fail when dealing with queries and documents that exhibit intricate semantic structures due to the limited extent of lexical overlap. This limitation arises from the inability of vectors to capture the true meaning of words.\nTF-IDF: TF-IDF, which stands for \"Term Frequency-Inverse Document Frequency\" is a widely used technique in natural language processing (NLP) for preprocessing text data. This statistical method assesses the significance of a term within a document or dataset. TF-IDF is calculated by two factors: $tf(w,c)$ and $df(w,C)$.\n$TF-IDF(w, c, C) = tf(w, c) \u00b7 idf (w, C)$                                        (1)\n\u2022 TF (term frequency) is the frequency of occurrence of a word in a document. The TF value of a word w in context c is calculated according to the following formula:\n$tf(w, c) = \\frac{n(w, c)}{n(c)}$                                                         (2)\nn(w, c) denotes the number of occurrences of the term w in the context c.\nn(c) denotes the total number of occurrences of all terms in context c."}, {"title": "4.1.2 Contextualized-based Retrieval", "content": "Bi-Encoder: Reimers and Gurevych [26] proposed the Bi-Encoder in 2019. Bi-Encoder are utilized across various tasks such as NLI, STS, Information Retrieval, and Question Answering systems. For the question-answer system task, the contexts in the dataset are encoded independently into vectors. The input question is then encoded and embedded in the vector space of the contexts to compute similarity scores with each context. Based on these scores, relevant contexts related to the question can be determined.\nOne of the training methods for bi-encoders involves using the MarginMSE loss function. MarginMSE is based on the paper of Sebastian et al. (2020) [27]. Similarly to MultipleNegativesRankingLoss, to train with MarginMSE, triplets (question, context 1, context 2) are required. However, unlike MultipleNegativesRankingLoss, context 1 and context 2 do not need to strictly be positive/negative; both can be relevant or irrelevant to a given question.\nFor training the bi-encoder with MarginMSE, the following procedure is undertaken: First, scores are computed for each pair (question, context 1) and (question, context 2). The distance of score (ScoreDistance) between the two pairs serves as the label for the triplet (question, context 1, context 2). The ScoreDistance is calculated using the formula:\n$ScoreDistance = Score_{(question,context1)} - Score_{(question,context2)}$                   (5)\nIn training the bi-encoder, question, context 1, and context 2 are encoded into vector spaces, and then the score of (question, context 1) and (question, context 2) is computed. Subsequently, the BDistance is computed by subtracting the score of (question, context 2) from the score of (question, context 1). The purpose of training is to optimize the error between ScoreDistance and BDistance."}, {"title": "4.1.3 Lexical-Contextual Retrieval", "content": "Weight Ensemble: We combined the scores of each context when querying by lexical and Bi-Encoder using a weight a for each top_k. The scores calculated from the bi-encoder model were normalized to values in the range [0; 1]. After combining, we extracted the top_k contexts with the highest scores. The combination formula is as follows:\n$Score = ScoreBM25_{(question, context)}\u00b7a+(1-a).ScoreBE_{(question, context)}$            (6)\nMultiplication Ensemble: We calculated the product of the scores from each context calculated by TF-IDF and Bi-Encoder. Similarly to the weight ensemble, the scores calculated from the Bi-Encoder model were normalized to values in the range [0, 1]. Finally, we extracted the top_k contexts with the"}, {"title": "4.2 Machine Reader", "content": "In this study, we implement a Reader module based on the sequence tagging approach - BIO format (B - beginning, I - inside, O - outside). The BIO approach means that tokens in the input will be classified into B, I or O labels. If a token is labeled B or I, it means that the token is part of the answer; otherwise, it does not appear in the answer. This approach is commonly used"}, {"title": "4.3 Answer Generator", "content": "In the Retrieval-Reader-Generator system, the Generator module operates at the final stage of the answer generation process. The main function of this module is to merge the information from the question and the extractive answer. This module uses a sequence-to-sequence structure, often seen in tasks such as machine translation or summarization, shown in Figure 7. This helps generate a complete, human-like answer.\nMathematically, the function f representing the Generator module is expressed as follows:\n$Aabstractive = f(Q, Aextractive)$                                                                       (8)\nHere, the inputs (Q, Aextractive) represent:\n\u2022\nQ: the question.\n\u2022\nAextractive: the extractive answer taken from the Reader module.\nThe output Aabstractive is the generated answer, refined, coherent, and synthesizes information from the question and the extracted context in a more understandable form.\nVietnamese language generator models are evolving, employing transfer learning methods to enhance performance. In this paper, we use state-of-the-art (SOTA) generator models for Vietnamese, including multilingual models such as mBART-50 [33], mT5 [34]; and monolingual models such as BARTpho [35] and ViT5 [36]."}, {"title": "5 Experiments and Results", "content": "In this section, we provide detailed configurations of the three modules in R2GQA system: Document Retriever, Machine Reader, and Answer Generator. We conducted all experiments on the RTX 3090 GPU with 24GB VRAM from VastAI\u00b3."}, {"title": "5.1 Metrics", "content": ""}, {"title": "5.1.1 P@k", "content": "To evaluate the performance of the retrieval methods, we use the P@k measure. P@k measure is commonly used in information retrieval tasks, and some works such as XLMRQA [9], SPBERTQA [37], LegalCQA [38] have utilized it. In formula 9, P@k is the proportion of questions for which the relevant corresponding context appears in the contexts returned by the retrieval module. Cipos is the relevant context corresponding to question qi, and Ck(qi) are the contexts returned by the retrieval module corresponding to question qi. n is"}, {"title": "5.1.2 F1", "content": "The Fl-score is a widely used metric in natural language processing and machine reading comprehension. Evaluates the accuracy of the predicted answers by comparing individual words with those in the correct answers. The F1-score measures the overlap in words between the predicted answers and the ground-truth answers.\n$Precision = \\frac{the\\ number\\ of\\ overlap\\ words}{the\\ total\\ number\\ of\\ tokens\\ in\\ the\\ predicted\\ answer}$                                            (10)\n$Recall =  \\frac{the\\ number\\ of\\ overlap\\ words}{the\\ total\\ number\\ of\\ tokens\\ in\\ the\\ gold\\ answer}$                                                                   (11)\n$F1-Score = 2.  \\frac{Precision \u00b7 Recall}{Precision + Recall}$                                                                   (12)"}, {"title": "5.1.3 BLEU", "content": "BLEU (Bilingual Evaluation Understudy) [39] is a scoring method to measure the similarity between two texts in machine translation. BLEU compares contiguous word sequences in the machine-generated text with those in the reference text, counting matching n-grams with weighted precision. These matches are position independent. BLEU is described by the following formula:\n$BLEUscore = BP x exp((\\sum_{i=1}^{N} w_{i}.log(p_{i})))$                        (13)\nWhere:\n\u2022 BP (Brevity Penalty) is a brevity penalty factor to account for shorter translations compared to the reference translations.\n\u2022 exp denotes the exponential function.\n\u2022 \u03a3i=1N(Wi.log(pi)) represents the weighted sum of the logarithm of precisions pi, where wi is the weight for the n-gram precision of order i, and N is the maximum n-gram order considered in the calculation."}, {"title": "5.1.4 ROUGE", "content": "In addition to comparing model outputs directly, we assess their agreement by measuring the overlap in content. To do this, we leverage the ROUGE framework (Recall-Oriented Understudy for Gisting Evaluation) [40]. ROUGE metrics are popular tools for automating text summarization and machine"}, {"title": "5.1.5 BERTScore", "content": "BERTScore [25] is a metric used to evaluate the performance of text generation models, including machine translation and text summarization. This metric leverages the contextual understanding ability of language models to encode predicted answers and gold answers into embedding vectors and then computes the cosine similarity between these embeddings to provide a score for the quality of the generated text. The higher the score, the greater the similarity, indicating better performance of the answers of model.\nBERTScore focuses on assessing semantic similarity rather than just lexical similarity like traditional metrics. This helps to evaluate the overall quality of text generation models more comprehensively. Additionally, BERTScore is available for multiple languages, allowing cross-lingual evaluation of text generation models."}, {"title": "5.2 Experimental Design", "content": "In this section, we provide detailed configurations of the three modules in R2GQA system: Document Retriever, Machine Reader, and Answer Generator. We conducted all experiments on the RTX 3090 GPU with 24GB VRAM from VastAI\u00b3."}, {"title": "5.2.1 Document Retriever", "content": "The purpose of the Document Retriever module is to question for context that may contain answers to the questions. We assign IDs to the contexts, which are used to map to the IDs of the contexts with the highest retrieval scores returned after performing the retrieval methods. For the Retriever module, we conduct experiments with 3 methods: lexical retrieval, contextual retrieval, and lexical-contextual retrieval with top_k = [1, 5, 10, 15, 20, 25, 30].\nLexical retrieval: We experiment with TF-IDF and BM25 methods. To enhance performance, we apply word segmentation using the Pyvi library \u2074 when conducting query experiments with TF-IDF and BM25.\nContextual retrieval: We employ 2 approaches. In both approaches to contextual retrieval, we utilize word segmentation with Pyvi. LME: We"}, {"title": "5.2.2 Machine Reader", "content": "For the Reader models, we implemented experiments with models and approaches as in Section 4.2. The models were trained with epochs = 5, batch_size = 8, learning_rate = 5e-5, max_seq_length = 512. The optimizer used was AdamW. The evaluation metrics used were F1, BLEU1, and BERTScore.\nOur data contains many contexts longer than 512 tokens, while the maximum length of the models we experimented with is 512 tokens. Therefore, we split each context longer than 512 tokens into multiple input features. To minimize information loss and preserve the semantics of the input features, we use the stride hyperparameter to create overlapping segments between two input features."}, {"title": "5.2.3 Answer Generator", "content": "We use the following generator models with specific configurations: mBART-large-50, mT5-base, ViT5-base and BARTphoword. We added the token </s> to the model input to separate the question and extractive answer in the format Question </s> Extractive answer </s>. For the BARTpho model, the input was formatted as <s> Question </s></s> Extractive answer </s>.\nWe perform word segmentation on BARTphoword using VncoreNLP before training the model. The model parameters were set as similarly as possible with epoch = 5, learning rate = 4e-05, max_seq_length = 1024 (512 for mT5 due to model limitations), batch_size = 2, and using the AdamW optimizer. The metrics used in this section included: BLEU1, BLEU4, ROUGE-L, and BERTScore."}, {"title": "5.3 Experiment Results", "content": "This section initially assesses the performance of our Document Retriever and Machine Reader modules independently. Subsequently, it details experiments involving their integration into the R2GQA system, which is applied to close-domain question answering concerning legal regulations in higher education."}, {"title": "5.3.1 Document Retriever", "content": "Based on Table 8, it can be seen that the lexical query method combined with the contextual method produces the highest results for all values of the top_k. The LME method consistently produces the lowest results as it has not been trained to understand context within our data domain. Comparing the two ensemble methods, we can observe that the weighted combination method between BM25 and Bi-Encoder provides the highest results for 5 out of the 7 top_k values tested. Thus, it can be concluded that this method has the highest stability. Therefore, we will use this method for our end-to-end system (ViEmb\u2076, ViSBERT\u00b9\u2070, ViSimCSE\u00b9\u00b9, ViBiEncoder\u00b9\u00b2)."}, {"title": "5.3.2 Machine Reader", "content": "Table 9 shows that the XLM-ROBERTa-Large model achieves the best results on most metrics and answer types. The second-best-performing model is CafeBERT. Across the entire ViRHE4QA test dataset, XLM-ROBERTa-Large outperforms CafeBERT by 0.04% on the F1 metric and 0.5% on BLEU1, while CafeBERT surpasses XLM-R-Large by 0.69% on BERTScore. However, overall, the XLM-R-Large model demonstrates more consistent results, outperforming CafeBERT on 2 out of 3 metrics. The ViBERT model performs the worst in all metrics and answer types, showing a significant gap compared to the other models.\nQuestions with single-span (1 span) answers achieve significantly better results than questions with multi-span (> 1 spans) answers across all models and metrics. For the XLM-R-Large model, performance on single-phrase answers"}, {"title": "5.3.3 Answer Generator", "content": "Table 10: Results of the Generator module with input are a Question and an Extractive answer.\nAccording to the results in Table 10, the BARTpho model achieved the highest score on the BLEU1 metric, but mBART outperformed the remaining three metrics, including BLEU4, BERTScore, and ROUGE-L. Compared to mBART, the BARTpho and ViT5 models exhibited slightly lower performance, ranging from 1% to 3%. However, the mT5 model performed significantly worse than the other three models. This could be attributed to the mT5 model having an input length limit of only 512 tokens, while the other three models accept input lengths of up to 1024 tokens. This limitation notably affects the performance, especially with datasets containing contexts longer than 512 tokens, such as ViRHE4QA."}, {"title": "5.3.4 End-to-End System", "content": "Based on the results in Section 5.3.1, Section 5.3.2, and Section 5.3.3, we used a weighted combination of Bi-Encoder and BM25 for the Retriever module, the XLM-R-Large model for the Reader module, and the mBART model for the Generator module to evaluate the performance of the R2GQA system.\nTable 11 demonstrates that as the number of retrieved contexts (top_k) increases, the performance of system improves. However, the difference between top_k = 10 and top_k < 15 is greater than that for top_k > 10. The performance of system at top_k = 10 surpasses that at top_k = 5 on BLEU1, BLEU4, ROUGE-L, and BERTScore by 0.78%, 0.65%, 0.81%, and 1.01%,"}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Impact of Context Length In The Reader Module", "content": "To validate the challenge posed by large context length in the dataset, we conducted an experiment to assess the impact of context length on the performance of models in the Reader module. The specific length ranges used for this experiment are detailed in Table 5."}, {"title": "6.2 Impact of Training Sample Number", "content": "To assess the impact of the number of samples in the training set, we trained the model with different quantities: 2000, 4000, 6000, and 7806 questions."}, {"title": "6.2.1 Machine Reader", "content": "From Figure 9, we can observe that increasing the number of samples in the training set significantly improves the performance of all models. Models such as ViBERT and XLM-R-Base show the most significant improvement, whereas models such as CafeBERT and XLM-R-Large show less improvement, as they already perform well even with a small number of training samples. Therefore, the amount of training data has a significant impact on model performance and will continue to increase as the amount of training data increases. Increasing the training data leads to an increase in the number of contexts and vocabulary, which helps the model learn more real-world scenarios."}, {"title": "6.2.2 Answer Generator", "content": "For the Generator module, we focus on the BERTScore metric for analysis. According to Figure 10, it can be observed that as the number of samples in the training set increases, the performance of all models improves slightly. However, for the mT5 model, there is a significant improvement in effectiveness from step 2000 to step 4000; after step 4000, the improvement of the model"}, {"title": "6.3 Impact of Context in Answer Generator Module", "content": "We experimented with the influence of context on the Generator module using different types of input: Question and Context (Q+C); Question, Extractive answer, and Context (Q+E+C); Question and Extractive answer (Q+E). We added the token </s> to the model input as described in Section 5.2.3.\nFrom Table 10, Table 12, Table 13, and Figure 11, it can be observed that the performance of the Generator module is highest when the input is Q+E, followed by Q+E+C, and lowest for Q+C. The differences between Q+E and Q+E+C are not significant but are markedly higher than the scores for Q+C. This section reveals that incorporating context into the module is not particularly effective and increases the input length to the Generator module, leading to inaccurate results.\nFor the Q+E method, the input is more concise, focusing on the main point (extractive answer) to provide the final answer for the system. In addition, using a shorter input reduces costs and resources when operating the system."}, {"title": "6.4 Performance of QA Systems", "content": "In this section, we will compare the performance of our system with other QA systems, Naive RAG. The main metrics used for the evaluation include BLEU1, BLEU4, ROUGE1, ROUGE-L, and BERTScore. The experiments were conducted on an NVIDIA RTX 3090 GPU with 24GB VRAM from VastAI\u00b9\u00b3.\nSystem Configurations:\nNaive RAG: RAG is a question-answering system proposed by [5]. In the past two years, RAG has been widely used as large language models (LLMs) have developed. Therefore, we compare our system with this method. We use the text-embedding-3-large model to create the vector database and encode the questions. The vector database we use is Chromadb, designed by Langchain, and the large"}, {"title": "7 Error Analysis", "content": "To perform error analysis, we surveyed 200 question-answer pairs predicted by the R2GQA system from the test set and classified errors into the following 5 main types:\nRepetition in extractive answers: in the Reader module, we use a BIO format model, which often leads to extractive answers containing one or more repeated words, resulting in grammatically incorrect phrases in Vietnamese."}, {"title": "8 Conclusion and Future Work", "content": "In this paper", "modules": "Retrieval, Reader, and Generator. This system leverages state-of-the-art language models, delivering high performance and efficiency for the QA task without"}]}