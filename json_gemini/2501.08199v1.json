{"title": "EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition", "authors": ["Yassine El Boudouri", "Amine Bohi"], "abstract": "Facial expressions play a crucial role in human communication serving as a powerful and impactful means to express a wide range of emotions. With advancements in artificial intelligence and computer vision, deep neural networks have emerged as effective tools for facial emotion recognition. In this paper, we propose EmoNeXt, a novel deep learning framework for facial expression recognition based on an adapted ConvNeXt architecture network. We integrate a Spatial Transformer Network (STN) to focus on feature-rich regions of the face and Squeeze-and-Excitation blocks to capture channel-wise dependencies. Moreover, we introduce a self-attention regularization term, encouraging the model to generate compact feature vectors. We demonstrate the superiority of our model over existing state-of-the-art deep learning models on the FER2013 dataset regarding emotion classification accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Facial expressions are a powerful means of non-verbal communication in face-to-face interactions, allowing humans to convey a wide range of information. According to Albert Mehrabian, facial expressions are more effective than words in face-to-face communication [1]. He revealed that words contribute only 7% to effective communication, while voice tone accounts for 38% and body language for 55%. Therefore, facial expressions play a vital role in human communication.\nIn this context, it is legitimate to attempt to model the process of human perception of facial expressions. In the last two decades, computer vision and artificial intelligence research have shown great interest in the automatic recognition of facial expressions in videos and static images. Facial Expression Recognition (FER) has gained immense importance in various fields such as security, healthcare, driver fatigue surveillance, and human-machine interaction applications [2]\u2013[4].\nOver the past few years, numerous conventional FER approaches have emerged, employing classical descriptors to explicitly extract features from face data. These approaches can be categorized into two groups: geometric methods and appearance-based methods. While geometric features capture the shape, location and interconnections of facial components during expressions [5], [6], appearance-based features focus on the variations in facial appearance, such as wrinkles and furrows, and can be extracted from the whole face or specific regions [3], [7], [8]. To classify these features, encompassing both geometric and texture-based ones, various classifiers have been employed, including Support Vector Machines (SVM), K-Nearest Neighbor (KNN), as well as neural networks such as MultiLayer Perceptron (MLP).\nConventional FER approaches follow a two-step process: they initially analyze and define facial features, and subsequently utilize these features for inference. However, as these two steps are performed separately, sub-optimal performance is obtained, particularly when dealing with complex datasets containing numerous sources of variability. Consequently, It is more advantageous to perform these two steps together for better recognition performance.\nOver the past two decades, deep neural networks have demonstrated exceptional effectiveness in automatic recognition tasks, making them a natural fit for automatic FER. Deep learning is used to learn discriminative feature representations for automatic FER by designing a hierarchical architecture composed of multiple non-linear transformations based on different types of neural networks such as convolutional networks (CNNs) and recurrent networks (RNNs). These networks are coupled with a classification layer for the classification task. As a result, the learning parameters of the classifier are determined in conjunction with the learning of the feature representations. This automation of feature extraction and classification directly from raw data greatly reduces dependence on models based on face geometry and other preprocessing techniques.\nIn this paper, we introduce EmoNeXt, a novel deep learning framework for FER based on an adapted ConvNeXt network [9]. We integrate a Spatial Transformer Network (STN) [10] to allow the network to learn and apply spatial transformations to input images and Squeeze-and-Excitation blocks [11] to enable adaptive recalibration of channel-wise feature. Furthermore, we combine a Self-Attention regularization term and the classical Cross-Entropy [12] Loss to encourage the model to generate compact features. The proposed architecture was able to produce significantly better results than the original ConvNeXt network and outperform other state-of-the-art deep learning models under the same experimental setup on the FER2013 dataset [13]."}, {"title": "II. RELATED WORK", "content": "In this section, we will briefly review some recent research on facial emotion classification, with a specific focus on models that have been evaluated using the FER2013 dataset.\nGiven the remarkable achievements and the rise of deep learning in the realm of computer vision, particularly in image classification tasks, several studies have introduced a range of deep learning approaches to address automated FER on the FER2013 dataset with the sole objective of achieving the best possible classification accuracy.\nIn [14], Georgescu et al. presented a method where auto-matic features, learned by multiple CNN architectures, were combined with handcrafted features computed using the bag-of-visual-words (BOVW) model. Once the fusion of the two types of features is accomplished, a local learning framework is utilized to make predictions of the class label for each individual test image. In [15], Pecoraro et al. introduced a novel channel self-attention module called Local multi-Head Channel (LHC), that can be seamlessly incorporated into any existing CNN architecture. This module offers a solution to the limitation of Global Attention mechanisms by effectively directing attention to crucial facial features that significantly influence facial expressions. In another paper [16], Fard et al. proposed an Adaptive Correlation (Ad-Corre) Loss consisting of three components: Feature Discriminator (FD), Mean Discriminator (MD) and Embedding Discriminator (ED). The proposed Ad-Corre Loss was combined with the classical Cross-Entropy Loss and used to train two backbone models: Xception [17] and Resnet50 [18]. The authors demonstrated that irrespective of the deep-learning model employed, the Ad-Corre Loss allows to increase the discriminative power of generated feature vectors, consequently leading to high accuracy in classification. Another novel deep learning model known as Segmentation VGG-19, was proposed in a recent study by Vignesh et al. [19]. This model enhances FER by integrating U-Net [20] based segmentation blocks into the VGG-19 (Visual Geometry Group) architecture [21]. By inserting these segmentation blocks between the layers of VGG-19, the model effectively emphasizes significant features from the feature map, leading to an enhanced feature extraction process. In another recent paper [22], Mukhopadhyay et al. presented a deep-learning-based FER method by exploiting textural features such as local binary patterns (LBP), local ternary patterns (LTP) and completed local binary patterns (CLBP). A CNN model is then trained over these textural image features to achieve improved accuracy in detecting facial emotions. In a recent publication by Shahzad et al. [23], a zoning-based FER (ZFER) was introduced. The objective of ZFER was to accurately identify additional facial landmarks such as eyes, eyebrows, nose, forehead, and mouth, enabling a more comprehensive understanding of deep facial emotions through zoning. After the initial steps of face detection and extraction of face landmarks, these zoning-based landmarks were employed to train a hybrid VGG-16 model. The resulting feature maps generated by the hybrid model were then utilized as input for a fully CNN to classify facial emotions.\nLater in the results section, Table II presents a compilation of references to other deep learning-based models utilized for the FER task. This table provides an overview of the classification scores achieved by these state-of-the-art FER models on the FER2013 dataset."}, {"title": "III. METHODS", "content": "This section explores various techniques and components integrated into a comprehensive deep learning model designed specifically for facial emotion detection. The focus primarily revolves around three key aspects: the preprocessing of input images at the beginning of the network, the feature extraction and classification, and the loss function minimizations."}, {"title": "A. Spatial Transformer Networks", "content": "Spatial Transformer Networks (STN) [10] extend the concept of differentiable attention to encompass various spatial transformations. By integrating a differentiable geometric transformation module into the network architecture, STN allow neural networks to learn and apply spatial transformations to input data. This capability proves invaluable in FER, where the performance is heavily influenced by scale, rotation, and translation variations.\nThe spatial transformer mechanism comprises three main components, illustrated in Figure 1. The first component is the localization network, which employs convolutional layers to generate a vector representation of the input image. This vector is then utilized by the grid generator to create a sampling grid. The grid consists of points that determine where the input map should be sampled to generate the transformed output. Typically, fully connected layers are employed in the grid generator. Lastly, the feature map and the sampling grid are fed into the sampler, which samples the input image at the grid points to produce the final output image.\nThe key advantage of STN is their ability to learn the spatial transformations automatically as part of the neural network training process."}, {"title": "B. ConvNeXt", "content": "Introduced in 2022, ConvNeXt [9] is a pure convolutional model that draws inspiration from Vision Transformers [24]. It was designed to compete with state-of-the-art Vision Trans-formers while retaining the simplicity and efficiency of CNNs.\nIt incorporates various enhancements to the architecture of a standard ResNet [18], with many of these modifications evident in the ConvNext block.\nThe ConvNeXt block, illustrated in Figure 2, uses larger kernel-sized and depthwise convolutions, increases the network width to 96 channels, and utilizes an inverted bottleneck design, to lower the overall network floating-point operations (FLOPs) while enhancing performance.\nConvNeXt also replaces ReLU [25] with GELU [26] as the activation function and substituting BatchNorm (BN) [27] with Layer Normalization (LN) [28] as the normalization technique, allowing the model to acheive a slightly better performance. GELU and LN are commonly used in advanced Transformers."}, {"title": "C. Squeeze-and-Excitation Blocks", "content": "Squeeze-and-Excitation (SE) [11] is a powerful technique used in deep learning models to enhance the representational power of CNN models. It introduces a mechanism that allows the network to adaptively recalibrate the channel-wise features, improving the model's discriminative capabilities. As illustrated in Figure 3, the SE block consists of two fundamental operations: squeezing and exciting. In the squeezing phase, global average pooling is applied to each channel of the feature map (W, H, C), reducing its spatial dimensions to a single value per channel (1, 1, C). The exciting phase follows, where the squeezed values are transformed using a small set of fully connected layers. These layers learn channel-wise weights, capturing the interdependencies among feature channels. The resulting attention weights are then multiplied element-wise with the original feature map, emphasizing informative channels and suppressing less relevant ones."}, {"title": "D. EmoNext Final Architecture", "content": "The EmoNeXt architecture begins with the inclusion of STN at the beginning of the network. STN enable the model to handle variations in scale, rotation, and translation by learning and applying spatial transformations to facial images.\nAfter passing through the STN, the inputs are then passed through ConvNeXt's patchify module. This module down-samples the input image using a non-overlapping convolution with a kernel size of 4. The downsampling helps to reduce the dimensionality of the input and capture relevant features efficiently.\nThe downscaled inputs then go through the ConvNeXt stages. Each stage is followed by a SE block to recalibrate the feature map before going into the next stage. This recalibration enhances the model's ability to extract discriminative facial features for accurate emotion recognition. The overall architecture is illustrated in Figure 4.\nBy leveraging these techniques, our EmoNeXt model achieves robust and accurate facial emotion detection, effectively handling variations in facial expressions and improving overall performance."}, {"title": "E. Self-Attention Regularization Term", "content": "Self-attention is a powerful mechanism used in various domains, including natural language processing, image analysis, and feature extraction [29]. It enables models to assign importance to different parts of the input sequence, allowing for effective capturing of dependencies and relationships.\nA commonly used approach is the dot product self-attention, known for its simplicity and effectiveness. It calculates the relevance and importance of the feature vector by computing the dot product similarity between pairs of its elements. The resulting attention scores are then used to weight the feature vector, generating an attended representation that highlights the most significant information.\nMathematically, the dot product self-attention weights can be computed as follows:\n$W(Q, K) softmax( \\frac{Q.KT}{\u221ad} )$"}, {"content": "Self-attention is a powerful mechanism used in various domains, including natural language processing, image analysis, and feature extraction [29]. It enables models to assign importance to different parts of the input sequence, allowing for effective capturing of dependencies and relationships.\nA commonly used approach is the dot product self-attention, known for its simplicity and effectiveness. It calculates the relevance and importance of the feature vector by computing the dot product similarity between pairs of its elements. The resulting attention scores are then used to weight the feature vector, generating an attended representation that highlights the most significant information.\nMathematically, the dot product self-attention weights can be computed as follows:\n$W(Q, K) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d}})$"}, {"title": "IV. EXPERIMENTS", "content": "The experiments were conducted on the FER2013 dataset, initially introduced in ICML 2013 Challenges in Representation Learning [13]. The dataset consists of 35,887 grayscale images, each sized 482 pixels (Figure 5). It is divided into three subsets: 28,709 images for training, 3,589 images for validation, and 3,589 images for testing. The faces in the dataset are labeled with one of the seven classes as mentioned in Table I. This dataset is widely used for FER tasks due to its significant number of samples. However, the distribution of classes in this dataset is highly unbalanced, which poses a challenge for any deep learning model. Figure 5 includes a selection of example data taken from FER2013."}, {"title": "B. Training", "content": "Recent studies have highlighted the effectiveness of modern training techniques in significantly improving the performance of deep learning models. In our model training, we employ various advanced strategies to improve the results. We utilize the AdamW optimizer [30] with a learning rate of 1e-4, combined with a cosine decay schedule to enhance convergence.\nAdditionally, we incorporate data augmentation techniques such as RandomCropping and RandomRotation to augment the training data, boosting the model's ability to generalize. To prevent overfitting, we implement regularization schemes like Stochastic Depth [31] and Label Smoothing [32], which contribute to more robust and generalized models.\nTo further enhance the performance and address memory constraints, we employ the Exponential Moving Average (EMA) technique [33]. EMA has proven effective in alleviating overfitting, particularly in larger models. Moreover, we adopt Mixed Precision Training [34], a method that reduces memory consumption by almost 2x while accelerating the training process.\nIn addition, we enhance our model's capabilities by incorporating weights from pretrained ConvNeXt on the ImageNet-22k dataset [35]. This dataset, known for its vast collection of diverse images, allows our model to leverage a wealth of learned knowledge. To ensure compatibility with the pretrained weights, we resize the images in our training pipeline to 2242, adhering to the established industry practice. This resizing technique enables us to effectively utilize the pretrained weights, resulting in improved performance and enhanced proficiency.\nFinally, we trained both the EmoNeXt and ConvNeXt models, encompassing all five sizes (T, S, B, L, and XL), utilizing an Nvidia T4 GPU with 16GB of VRAM. The implementation was done using PyTorch version 2.0.0, and the code is available at: https://github.com/yelboudouri/EmoNeXt"}, {"title": "C. Results", "content": "The results presented in Table II demonstrate the superior performance of our proposed model, EmoNeXt, compared to existing state-of-the-art single network architectures trained on the FER2013 dataset. Among the listed models, EmoNeXt-XLarge stands out with an accuracy of 76.12%. This achievement can be attributed to the unique design and architecture of EmoNeXt, which effectively captures and emphasizes relevant facial features for accurate emotion classification.\nWhen considering accuracy, EmoNeXt-Tiny achieves an accuracy of 73.34%, surpassing well-known models like ResNet50 (73.20%) and VGG (73.28%), as well as the three first versions of ConvNeXt: tiny, small, and base.\nWith notable progress, EmoNeXt-Small exhibits enhanced performance compared to EmoNeXt-Tiny, attaining an accuracy of 74.33%. This achievement surpasses advanced architectures like the Residual Masking Network (74.14%) and LHC-NetC (74.28%), as well as the last two sizes of the original ConvNext (Large and XLarge). These results underscore EmoNeXt-Small's exceptional ability to effectively capture and classify facial emotions.\nEmoNeXt-Base maintains its performance by achieving an accuracy of 74.91%, surpassing models like LHC-NetC (74.28%). On the other hand, EmoNeXt-Large achieves an accuracy of 75.57%, outperforming the combination of CNN and BOVW models (75.42%).\nNotably, EmoNeXt-XLarge achieves an accuracy of 76.12%, surpassing the current best state-of-the-art accuracy attained by the Segmentation VGG-19 model (75.97%). This remarkable performance firmly establishes EmoNeXt-XLarge as a highly effective model for image classification tasks, specifically for facial emotion recognition (FER)."}, {"title": "V. CONCLUSION", "content": "In this paper, we presented EmoNeXt, a novel deep learning framework for facial expression recognition based on an adapted ConvNeXt architecture network. The EmoNeXt model integrates a Spatial Transformer Network (STN), Squeeze-and-Excitation blocks, and self-attention regularization to capture rich facial features and improve emotion classification accuracy. Experimental results on the FER2013 dataset demonstrate the superiority of EmoNeXt over existing state-of-the-art models. The integration of STN, SE blocks, and self-attention provides robust and accurate facial emotion detection, making EmoNeXt a promising approach for various applications requiring emotion recognition.\nA detailed study is underway and will be published in a forthcoming journal paper. In this upcoming publication, our objective is to conduct a thorough evaluation of our model and compare its performance with other models, using diverse FER databases.\nFurthermore, we have future plans to extend the application of our model to emotion recognition in elderly Alzheimer's patients, highlighting its potential for real-world impact and further research avenues."}]}