{"title": "Multi-task Domain Adaptation for Computation Offloading in Edge-Intelligence Networks", "authors": ["Runxin Han", "Bo Yang", "Zhiwen Yu", "Xuelin Cao", "George C. Alexandropoulos", "Chau Yuen"], "abstract": "In the field of multi-access edge computing (MEC), efficient computation offloading is crucial for improving resource utilization and reducing latency in dynamically changing environments. This paper introduces a new approach, termed as Multi-Task Domain Adaptation (MTDA), aiming to enhance the ability of computational offloading models to generalize in the presence of domain shifts, i.e., when new data in the target environment significantly differs from the data in the source domain. The proposed MTDA model incorporates a teacher-student architecture that allows continuous adaptation without necessitating access to the source domain data during inference, thereby maintaining privacy and reducing computational overhead. Utilizing a multi-task learning framework that simultaneously manages offloading decisions and resource allocation, the proposed MTDA approach outperforms benchmark methods regarding mean squared error and accuracy, particularly in environments with increasing numbers of users. It is observed by means of computer simulation that the proposed MTDA model maintains high performance across various scenarios, demonstrating its potential for practical deployment in emerging MEC applications.", "sections": [{"title": "I. INTRODUCTION", "content": "With the development of mobile communication technology and edge devices, multi-access edge computing (MEC) effectively reduces network latency and improves computing efficiency by deploying powerful computing resources at the edge of the network. Computation offloading is one of the key features of MEC. Through computation offloading technology, mobile users can offload computationally intensive jobs from resource-constrained edge devices to nearby MEC servers (MESs) [1], [2], which significantly improves resource utilization. In practice, the computational offloading problem can be decomposed into two sub-problems, i.e., binary offloading decision and computational resource allocation [3]. In particular, the offloading decision problem can be represented as a multi-class classification problem, while, the computational resource allocation can be represented as a regression problem. In this context, a multi-task learning model can be designed for inferring the classification and regression tasks simultaneously.\nCrowd intelligence networks provide powerful support for computational offloading in MEC environments. By integrating the collective intelligence of multiple mobile users and edge devices, the edge intelligence (EI) network realizes collaborative decision-making and resource sharing, especially in complex and dynamically changing network environments. Through such a cooperative mechanism, different users can share computational resources, task requirements, and environmental information with an intelligent central controller to optimize offloading decisions [4], thereby improving the decision-making capability and resource allocation efficiency of EI systems under dynamic network conditions.\nIt is known that the current models for offloading computations heavily rely on mathematical methods and models to make decisions about offloading and allocating resources [5]. However, network environments often change over time. In this case, the data from the varying target domain is distributed differently from the source domain, leading to domain shift [6]. Furthermore, due to the privacy of edge devices [7], the source domain data is even unavailable during the inference process, thereby significantly reducing inference accuracy on the target domain. Therefore, making the computational offloading model exhibit good generalization on unlabeled target domains becomes a key challenge.\nTo achieve optimal offloading strategies, advanced artificial intelligence (AI) techniques become promising due to their potential for big data analysis in solving optimization problems [8]. For instance, by combining deep neural networks (DNNs) with reinforcement learning (RL), deep reinforcement learning (DRL) could learn optimal offloading strategies in dynamic environments by dynamically interacting with the wireless environment [9]\u2013[11]. With the aid of DRL algorithms, such as dual deep Q-network (DDQN) or deep deterministic policy gradient (DDPG), the agent continuously adjusts its policy and thus ultimately achieves intelligent decision-making in dynamic environments to optimize system performance and user experience. However, this method is weakly adaptive to unexpected perturbations and requires comprehensive retraining to learn updated strategies for new environments, which is a very time-consuming task. To solve the issues of DRL, meta-reinforcement learning combines meta-learning with reinforcement learning. This combination is one of the effective methods to learn strategies for new tasks by building on historical experience [12]. Generally, meta-reinforcement learning performs two layers of training, the outer training uses contextual expertise to adjust the meta-policy parameters of the inner training gradually, and then, based on the meta-policy, the inner training can be quickly adapted to the new task by a small number of gradient updates. However, the training cost of the above method is very high, which makes it difficult to deploy and run on resource-constrained edge devices.\nAn alternative approach towards the goal of optimal offloading in MEC systems deploys a feedforward neural network based on multi-task learning (MTFNN) to learn the optimal offloading strategies in near-real-time [3]. Although the offline training data sets do not contain all possible parameter combinations, it was shown that it is possible to generate appropriate offloading strategies even if the parameter settings are not in the training samples. However, this method still fails to provide good generalization for large variations in the data distribution in the target domain. To tackle the aforementioned challenges and address the limitations of existing computation offloading methods, this paper proposes a multi-task domain adaptation (MTDA) approach to enhance model generalizability. The approach employs a teacher-student model framework to transfer knowledge across domains while leveraging multi-task learning to jointly optimize binary offloading decisions and resource allocation. The key contributions of this work are summarized as follows.\n\u2022 We propose an MTDA method that does not require access to the source domain data when performing online continuous adaptive model tuning during inference. This method contributes to the privacy of mobile devices at the edge, avoiding dependence on source domain data and protecting the privacy of edge users. Additionally, it reduces resource consumption by eliminating the need for retraining.\n\u2022 The proposed method can be used with unlabeled target domain data that is closer to the real environment. Using the teacher model to generate high-quality pseudo-labels can help improve the learning performance and inference accuracy of the multi-task computational offloading model.\n\u2022 The experiments show that MTDA improves inference accuracy on target domain data while preserving high accuracy on source data, all with low energy consumption on mobile edge devices. It enhances model performance on the target domain without sacrificing efficiency or increasing costs."}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "A single-server multi-user MEC system is being considered. There exist N mobile users (MUs), i.e., N = {U1, U2, ..., UN}. In this system, the extensively utilized orthogonal frequency-division multiplexing (OFDM) scheme is used to facilitate wireless communications between the MUs and mobile edge server (MES) over the unlicensed frequency band. The N-dimensional offloading decision vector from all MUs Un to MES is denoted as\n$V = \\{V_1,..., V_n\\}, \\forall n \\in N,$ (1)\nwhere $V_n \\in \\{0,1\\}$ denotes the computing offloading decision of Un to the MES. Hence, it holds:\n$V_n = \\begin{cases} 1 \\text{ if } U_n \\text{ offloads to the MES,} \\\\ 0 \\text{ otherwise.} \\end{cases} \\forall n \\in N.$ (2)\nIn this paper, we consider a binary offloading strategy, assuming that each MU either processes its jobs locally or offloads them to the MES. We define the following N-dimensional vector of MES computational resource allocation:\n$R = \\{R_1,..., R_n\\}, \\forall n \\in N,$ (3)\nwhere $R_n \\in \\{0,1\\}$ denotes the proportion of computational resource allocated by the MES to Un, and $\\sum_{n=1}^{N} R_n < 1$ usually holds.\nB. Jobs Processing Model\nLet vo be the CPU cycle frequency (i.e., CPU cycles per second) of Un, and let Yn be the CPU cycle frequency required by processing the job Jn. The weighted-cost for computing Jn locally is calculated as\n$w^{loc}_n = \\beta \\frac{Y_n}{\\nu_n} + (1 - \\beta)\\eta (\\frac{Y_n}{\\nu_n})^2 Y_n, \\forall n \\in N,$ (4)\nwhere $w^{loc}_n$ denotes the overall cost of local processing on Un, \u03b7 denotes the energy efficiency parameter that mainly depends on the hardware chip architecture [13], and $\\beta \\in [0, 1]$ denotes the emphasis on computational delay and power consumption. For instance, when \u03b2 is close to 1, the delay cost becomes more significant, whereas when \u03b2 is close to 0, the power cost becomes more critical.\nLet sn be the size of the data to be uploaded, let voff be the computing resources allocated to the user Un by the MES, and let wn be the size of processed results. Furthermore, Un denotes the data rate realized by the wireless uplink from Un to MES, and dn denotes the data rate of the wireless downlink from MES to Un. So the weighted-cost of Un for offloading Jn to MES is given by\n$w^{off}_n = \\beta \\frac{s_n}{U_n} + \\frac{Y_n}{\\nu^{off}_n} + \\frac{w_n}{d_n} + (1 - \\beta) \\left(\\frac{P_t s_n}{U_n} + \\frac{P_i Y_n}{\\nu^{off}_n} + \\frac{P_d w_n}{d_n}\\right), \\forall n \\in N,$ (5)\nwhere Pt, Pi, Pa indicate the power consumption for job uploading, execution and downloading, respectively.\nCombined with (4) and (5), the total cost of the MEC system can be defined as the weighted sum of the costs of all mobile users, i.e,\n$w^{total} = \\sum_{n \\in N} ((1 - V_n)w^{loc}_n + V_n w^{off}_n),$ (6)\nwhere $w^{loc}_n$ and $w^{off}_n$ represent the total cost of processing jobs locally and the total cost of offloading processing, respectively."}, {"title": "C. Problem Formulation", "content": "The computational offloading problem considered in this paper can be formulated as a joint optimization challenge involving offloading decisions and the computational resources allocation, aiming to minimize the overall system cost. Given the fluctuating network conditions and the constrained computational resources of the MES, the offloading decisions of the MUs and the computational resources allocation by the MES can be formulated as a mixed-integer nonlinear programming (MINLP) problem. In particular, we formulate the joint offloading and computational resource allocation as a weighted-sum cost minimization problem (denoted as P).\nP: $ \\underset{\\{V,R\\}}{\\text{minimize}} Wtotal $\ns.t. C1: $V_n \\in \\{0,1\\}, \\forall n \\in N,$\nC2: $(1-V_n) \\frac{Y_n}{\\nu^{loc}_n} + Vn(\\frac{s_n}{U_n}+\\frac{Y_n}{\\nu^{off}_n} +\\frac{w_n}{d_n}) < \\theta_n, \\forall n \\in N,$\nC3: $0 \\leq \\nu^{off} \\leq 1, \\forall n \\in N.$\nC4: $\\sum_{n=1}^{N} \\nu^{off} \\leq 1, \\forall n \\in N.$ (7d)\nThe constraints in the formulated problem above are detailed as follows. C1 indicates that Un can only execute jobs locally or offload them to the MES. C2 ensures that the total latency of processing a job of Un does not exceed the maximum tolerable latency vn. C3 and C4 ensure that the computational resources allocated by the MES to the users do not exceed the resource limit of the MES.\nWe can observe that the formulated multi-dimensional, multi-objective problem is, in fact, a mixed-integer nonlinear programming (MINLP) problem, which is generally NP-hard. This problem can be compartmentalized using the Tammer decomposition method [14]. Specifically, the optimal solution to the joint optimization problem is denoted as $S^* = \\{V^*, R^*\\}$, where V* represents the optimal offloading decision vector and R* represents the optimal computational resource allocation ratio vector. Considering that V is a binary vector and R contains continuous values within the range [0, 1], we can utilize a multi-task learning approach that includes both a classification task and a regression task.\nBy transforming the original problem into a hierarchical optimization framework to minimize the objective function, it becomes evident that the problem can be simplified. Specifically, it can be viewed as minimizing a function f(x, y), where x encapsulates the features of all MUs' jobs and network environment parameters, and y signifies the optimal solution. Although training a single neural network model offline with a vast amount of data can result in high testing dataset accuracy, this strategy encounters substantial performance declines in fluctuating network environments. In contrast, adaptation during the inference phase of a multitask model provides better generalization than the multitask model alone."}, {"title": "III. MULTI-TASK LEARNING BASED DOMAIN ADAPTATION", "content": "This section details the MTDA approach for dynamic computational offloading.\nA. MTDA Structure\nThe proposed MTDA structure is shown in Fig. 1. Specifically, the input parameters are the vectors containing MUs and environment parameters through several dense hidden layers, and the output layer consists of a classification task and a regression task for inferring the optimal offloading decision V* and resource allocation R*, respectively.\nUnlike traditional computational offloading models, the MTDA architecture adapts to target domain data during the inference phase. Specifically, a knowledge refinement framework involving a teacher-student architecture is presented. The process begins with the teacher model, which is trained over the augmented data to perform classification and regression tasks. The teacher model is initialized as a source model, which in this case refers to a computational offloading model that has been trained to make predictions for both tasks. The output of the teacher model is then used to compute the total loss, combining the cross-entropy loss for classification and the mean square error (MSE) loss for regression. The student model is updated using the predictions and losses from the teacher model. This process is designed to improve the performance of the student model by imparting knowledge of the teacher model. The student Model also periodically undergoes stochastic recovery to update and refine the weights based on the output of the teacher Model. The updated student model contributes to the continuous improvement of the teacher model in a cyclic manner, ensuring that the teacher model remains valid when new data and conditions are introduced. This iterative updating and knowledge transfer process between the teacher and student models constitute the core of the MTDA architecture, enabling robust adaptive learning."}, {"title": "Algorithm 1 Offline Training", "content": "Input: Training source domain dataset Ds containing user and environment parameters xs and optimal solution ys;\nOutput: Trained multi-task offloading model;\n1: Train the classifier with loss function lc.\n2: Train the regressor with loss function lr.\n3: Achieve the weighted-sum loss function l.\n4: Tune the weights of each layer using backpropagation until l is minimized.\nB. Offline Training\nThe offline training process of a multi-task computational offloading model is illustrated in Algorithm 1, which jointly considers classification and regression tasks. Specifically, the model is trained using cross-entropy loss for classification and mean squared error (MSE) for regression. The classifier and regressor are trained separately, and then their loss functions are combined into a weighted sum loss. The model is then fine-tuned by backpropagation to minimize this combined loss and ensure that it can effectively handle both tasks. This approach helps to create a robust offloading model that generalizes new data well while taking care of both classification and regression tasks.\nSpecifically, we suppose that there are M sample data in the source domain dataset Ds. The source domain dataset Ds is constructed by randomly generating node features and solving a Mixed-Integer Nonlinear Programming (MINLP) problem using the GEKKO tool to obtain optimization results\u00b9. During the training of this model, the loss function for classification, denoted as le, is defined using cross-entropy. This is mathematically represented as:\n$l_c = \\frac{1}{M} \\sum_{m=1}^{M} y_{s,m} ln(f(x_{s,m})),$ (7)\nwhere xs,m is the user and environment parameters, Ys,m represents the real categorized labels, and f(xs,m) is the predicted output of the neural network.\nThe regression loss, denoted as lr, is calculated using the mean square error (MSE):\n$l_r = \\frac{1}{w} \\sum_{i=1}^{w} (y_{s,i} - f(x_{s,i}))^2,$ (8)"}, {"title": "Algorithm 2 Multi-task based online domain adaptation", "content": "Initialization: A source pre-trained model f40, teacher model f\u00f8 initialized from f40.\nInput: For each time step t, input parameter vector xt from target domain dataset Dt.\nOutput: Prediction f\u00f8r(xt); Updated student model f\u00f8t+1; Updated teacher model f\u00f8+1\u00b0\n1: Augment the input data xt to generate multiple augmented samples xt,1, xt,2,..., xt, N.\n2: For each augmented sample xt,i, obtain the pseudo-labels (for classification tasks) and regression predictions (for regression tasks) using the teacher model f (xt,i).\n3: Compute the weighted average of the pseudo-labels and regression predictions across all augmented samples to obtain the final predictions f (xt).\n4: Update the student model f\u00f8r by minimizing the total loss (e.g., the combination of classification loss and regression loss) using \u00fbt as the target in Equation (12)..\n5: Update the teacher model f using a moving average of the student model parameters in Equation (13)..\n6: Optionally, stochastically restore the student model for from its historical states to avoid overfitting by Equation (14)..\nwhere w is the number of input samples.\nIn the computational offloading model, the total loss function is a weighted sum of le and lr, represented as\n$l = \\chi_c l_c + \\chi_r l_r,$ (9)\nwhere Xc and Xr are the respective weights. The Adam optimizer is utilized to minimize this combined loss by performing back-propagation, thereby optimizing the computational offloading model.\nC. Online Adaptation\nThe online domain adaptation process is presented in Algorithm 2 involving both student and teacher models. Initially, the teacher model is created by copying a pre-trained source model. At each time step, the algorithm receives an input vector xt and augments it to produce multiple variations of the original input. For each augmented sample, the teacher model generates predictions, including pseudo-labels for the classification task and regression values for the regression task. These predictions are then averaged to produce a weighted final prediction, which is the optimal computational offloading strategy S* = {V*,R*}. The student model is updated with the total loss calculated from these predictions to fit the current data stream. The teacher model is updated using a moving average of its parameters, ensuring that the teacher model is always a stable reference for the student model. In addition, the student model performs stochastic recovery to prevent overfitting by occasionally restoring its parameters to an earlier state. The output of the algorithm consists of updated predictions for the student model, as well as newly updated student and teacher models ready for the next time step.\nOur proposed MTDA approach is used to improve the generalization ability of computational offloading models so that the offline-trained model is a multi-task-based feed-forward neural network model that learns multiple related tasks simultaneously through hard parameter sharing.\nIn our proposed MTDA framework, the data to be adapted in the model inference phase is the unlabeled target domain data from the new environment, so we use a teacher model to generate high-quality pseudo-labels. Motivated by the observation that models with weight-averaging over training steps often yield more accurate results than the final model [15], we use a weight-averaged teacher model f\u00f8r to generate pseudo-labels. The teacher network is initialized as the same pre-trained network as the source model, and when the target domain data is incoming, the teacher model first generates pseudo-labels \u0177t = f'(xt) that contain classification task predictions \u0177c as well as regression task predictions \u0177r.\nSuppose the student model produces a categorical prediction (yc) and a regression prediction (yr), the student model is updated by minimizing the weighted sum of Le and Lr, i.e.,\n$L_c = \\frac{1}{n} \\sum_{i=1}^{n} [\\hat{y}_{c,i} log(y_{c,i}) + (1 - \\hat{y}_{c,i}) log(1 - y_{c,i})],$ (10)\n$L_r = \\frac{1}{n} \\sum_{i=1}^{n} (y_{r,i} - \\hat{y}_{r,i})^2,$ (11)\n$L_{total} = aL_c + bL_r,$ (12)\nwhere n denotes the number of samples, Le denotes the value of cross-entropy loss used for the classification task, and Lr denotes the value of mean square error loss used for the regression task. Moreover, a and b are the hyperparameters that can be used to balance losses.\nAfter updating the student model's weight parameters ot at time step t, we use the following equation to update the teacher model's weights $\u03a6^{t+1}_i$ via an exponential moving average using the student weights:\n$\u03a6^{t+1}_i = \u03b2\u03a6^{t}_i + (1 \u2212 \u03b2)\u03a6^{t+1}_i,$ (13)\nwhere \u1e9e is a coefficient that controls the proportion of weight given to the teacher model during the averaging process.\nContinuous adaptation over long periods of self-training inevitably introduces errors and leads to forgetting. To further address the problem of catastrophic forgetting, we propose a stochastic recovery method that further updates the student model weights.\nA mask tensor M is generated from a Bernoulli distribution with a probability p, where M ~ Bernoulli(p). This mask tensor M has the same dimensions as $\u03a6^{t+1}$ and determines which specific elements should be reset to their original values from the source model's weight 0. Leto denote element-wise multiplication, the update rule for $\u03a6^{t+1}$ is given by\n$\u03a6^{t+1} = M \\circ \u03a6^{0} + (1 \u2212 M) \\circ \u03a6^{t+1}.$ (14)"}, {"title": "IV. NUMERICAL RESULTS AND DISCUSSION", "content": "To generate the labeled dataset, we perform an exhaustive search on a random sample of the input parameters and return the ground truth paired with the input parameters. We then assess the generalization of the MTDA model on target domain datasets by evaluating its outputs against the labels. Without loss of generality, a labeled dataset is used to train the source model, and in the new environment, both labeled and unlabeled datasets are created. The labeled dataset validates the adapted model's performance, while the unlabeled dataset is used for model adaptation during testing.\nAfter generating the corresponding dataset, we repeat the experiment for different numbers of mobile subscribers. All experiments were conducted on a 13th generation Intel(R) Core(TM) i5-13400F @ 2.50 GHz (x16) processor system. In each repeated experiment, the baseline model MTFNN [3], the deep reinforcement learning model DDPG [16], the single-task-based test-time adaptive model COTTA [17], and the proposed MTDA model are trained on the same training set and evaluated on the same test set.\nWe compared the performance of the proposed MTDA model with three other models, as shown in Fig. 2, focusing on metrics such as MSE and accuracy. The size of the data volume to be processed for the source domain dataset is [0, 500 kbits] and the available server resources are [2.5 GHz, 10 GHz], while the size of the data volume to be processed for the target domain dataset is [600 kbits, 700 kbits] and the available server resources are [10 GHz, 12GHz]. The results consistently show that MTDA outperforms the other three benchmark models under the target domain dataset offset from the source domain dataset as well as different numbers of users. This highlights the significant generalization ability of MTDA in the new environment. However, it is important to note that as the number of users increases, all four models experience a decline in performance. This decline suggests that the complexity of the task increases with the addition of more users, making it more challenging for the models to maintain high-performance levels.\nIn Fig. 3, we compare the offloading decision costs predicted by six different models in the new environment. The datasets used for comparison have data volume sizes [600 kbits, 700 kbits], server resources spanning [10 GHz, 12 GHz], and the number of users is fixed at 3. As shown in Fig. 3(a), despite the relatively low task differentiation difficulty, which failed to significantly distinguish between different models, the proposed MTDA method consistently demonstrates the lowest cost across all server computing resource values when compared to the MTFNN model. Furthermore, Fig. 3(b) illustrates that as the data volume increases, the costs of all models increase accordingly. However, the proposed method not only maintains the lowest cost but also exhibits a slower cost growth, fully showcasing its superior scalability."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this paper, we presented an MTDA approach to tackle the challenges of computation offloading in MEC systems, with a particular focus on the generalization to new environments. The proposed scheme effectively uses the teacher-student modeling framework in the inference phase to adapt to the target domain data without accessing the source domain. This feature significantly improves the accuracy of offloading decisions and resource allocation. Compared to existing methods, our MTDA approach showed superior performance, particularly in maintaining low offloading costs and high accuracy even when the number of MUs in a system increases. This behavior underscores the potential of MTDA as a reliable and efficient solution for dynamic and diverse MEC systems.\nIn the future, we intend to study how well the MTDA model can adapt to large scale and diverse environments, where there is a greater variation in the number of MUs in the dynamical network conditions. Additionally, we aim to apply our proposed MTDA framework to other areas, such as autonomous systems and smart cities, to showcase its broader effectiveness in real-world situations."}]}