{"title": "What You Say = What You Want?\nTeaching Humans to Articulate Requirements for LLMs", "authors": ["Qianou Ma", "Weirui Peng", "Hua Shen", "Kenneth Koedinger", "Tongshuang Wu"], "abstract": "Prompting ChatGPT to achieve complex goals (e.g., creating a cus-\ntomer support chatbot) often demands meticulous prompt engineer-\ning, including aspects like fluent writing and chain-of-thought tech-\nniques. While emerging prompt optimizers can automatically refine\nmany of these aspects, we argue that clearly conveying customized\nrequirements (e.g., how to handle diverse inputs) remains a human-\ncentric challenge. In this work, we introduce Requirement-Oriented\nPrompt Engineering (ROPE), a paradigm that focuses human atten-\ntion on generating clear, complete requirements during prompting.", "sections": [{"title": "1 INTRODUCTION", "content": "General-purpose Als like large-language models (LLMs) have evolved\nfrom simple next-word predictors [7] to powerful assistants capa-\nble of fulfilling complex user needs [23, 48, 53]. This improvement\nin LLM instruction-following has encouraged users to delegate\nincreasingly intricate tasks to these models. In the early days of\nprompt engineering, users primarily focused on refining the word-\ning of simple instructions to improve LLM output quality [26, 61].\nToday, prompts resemble detailed \"essays\" that define LLM roles,\nhuman preferences, and other task-specific requirements. Rather\nthan one-off small requests, these prompts start to power the gen-\neration of programs. On one hand, designers and developers now\nwrite functional descriptions for LLM agents (e.g., Devin, SWE-\nAgent) to translate into executable software code [27, 58, 68, 72].\nOn the other hand, everyday users can write complex prompts to\ntailor general-purpose LLMs into special purpose LLM Applica-\ntions. For instance, an LLM app (or a GPTs) like Trip Advisor\u00b9\n(Figure 1D) can be built solely using prompts similar to Figure 1E.\nThese LLM applications function similarly to traditional software\nand are accessible through LLM App Stores like GPT Store [1] and\nFlowGPT [20], where they can be reused with a single click. In\njust a few months, over 30,000 \"LLM app developers\" have created\nmillions of GPT apps, with the most popular ones being used more\nthan 5 million times [63, 78]. This trend signals a future of end-\nuser programming through natural language, where anyone\ncan create reusable programs just by writing prompts.\nThe future of writing prompt programs looks promising, but\nstatus quo prompt engineering remains a challenge. Various studies\nshow that optimizing LLM outputs requires well-rounded prompts,\ninvolving fluent writing, clear instructions, personas, formats, and"}, {"title": "2 RELATED WORKS", "content": "We review three key areas relevant to our work: prompt engi-\nneering, prompt optimization in natural language processing, and\nrequirement engineering in end-user software engineering. We\nhighlight a significant gap in the literature: the lack of requirement-\noriented prompt engineering training for end users, which we pro-\npose to address in this paper."}, {"title": "2.1 Current Prompt Engineering Practices and\nChallenges", "content": "Prompt engineering (PE) has been essential for crafting effective\ninput instructions to guide LLMs toward generating desired out-\nputs [6, 38, 77]. However, the non-deterministic nature of LLMs\nmakes it challenging for humans to predict LLMs' behavior across\nprompts [15, 49]. This leads to wasted time on unproductive strate-\ngies, such as trivial wording changes [15, 19, 77]. While some chal-\nlenges can be mitigated through automation (see Section 2.2), e.g.,\nautomating word choices, human requirements remains crucial\nfor customized or specialized tasks [38]. We define these task as\nLLM-hard, as a simple prompt cannot produce a satisfactory re-\nsponse. Developing reusable prompts for customized chatbots or\nGPTs [6, 77] is a common LLM-hard task. While a prompt is an in-\nput to LLMs, a reusable prompt is a set of instructions for recurring\ngoals, essentially functioning as a prompt program. As more users\ncreate reusable prompts, the ability to write good prompt programs\nwith clear requirements becomes increasingly important.\nHowever, prompt creation can be complex and inaccessible to\nnon-experts who lack technical knowledge of LLMs, highlighting\nthe need for more end-user prompt engineering (EUPE) scaffolding\nand training [77]. Various strategies have been developed to support\nPE, such as prompt technique catalog [8], chat-based tools like GPT-\nbuilder, and toolkit for orchestrating complex tasks [10, 29, 69].\nEach tool has trade-offs, balancing flexibility, precision, and techni-\ncality. For example, chat-based tools may ask narrow clarifications\nwithout fully understanding the context, leaving users to self-refine\nrequirements when LLM responses are open-ended [34, 51].\nExisting PE training emphasizes the importance of explicitly\nstating requirements within prompts [16, 41, 57], yet there is still\na lack of focused training on requirement generation skills for ev-\neryday users. Without training, non-experts often make mistakes\nsuch as missing requirements and writing conflicting requirements\nin prompts [15, 19, 44]. Even experts need many iterations to im-\nprove their requirements for complicated LLM tasks [59]. However,\ntraining novices on prompt engineering is difficult; for instance,\nnovice programmers may not improve at prompting within a 75\nminute study when provided with test cases and generated code\nas feedback [46]. We aim to address this gap by proposing a ROPE\ntraining for end-users to improve their requirement engineering\nskills in the context of prompt creation."}, {"title": "2.2 Instruction Following for Foundation\nModels", "content": "Optimizing LLM performance on customized tasks typically follows\ntwo approaches: improving the models directly, or refining prompts\nwith models fixed. The former has enabled the versatility of models\nlike GPT-4 [47], Gemini [14], and LLaMA 3 [3], which are fine-tuned\nthrough instruction tuning [48] and further aligned with human\npreferences using Reinforcement Learning from Human Feedback\n(RLHF) [13]. For the latter, the NLP community has been explor-\ning LLMs' capabilities to follow requirements (or \"constraints\").\nVarious datasets have been proposed to assess whether LLMs can"}, {"title": "2.3 Requirements in Programming and\nSoftware Engineering", "content": "In software engineering and end-user software engineering, re-\nquirements describe what a human wants to achieve and how a\nprogram should behave in the world [31]. They encompass all the\nnecessary conditions, constraints, and desired outcomes to ensure\nthe output aligns with the human's needs and expectations. Re-\nquirement engineering stems as a field that focuses on generating\nand documenting requirements for software [64]. Good quality re-\nquirements need to be accurate and complete, without commission\n(inclusion of irrelevant or incorrect details) and omission (exclusion\nof necessary details) defects [4]. Previous studies have identified\nrequirement engineering as a challenging skill to master. While\ntraining mechanisms have been developed to help students avoid\ncommission and omission errors [45], a significant skill gap remains\nbetween graduates and professional engineers [18, 54].\nParallels can be drawn between prompt engineering (PE) and\nrequirements engineering. For example, adapting LLMs to diverse\nscenarios demands well-specified requirements, and prompt au-\nthors often need to iterate on prompts that are too ambiguous for\nthe LLM to interpret [50]. While PE training stresses the importance\nof clear requirements [16, 41, 57], this is often presented just as\none of many prompting principles (e.g., one of 26 [8]), limiting its"}, {"title": "3 THE ROPE PARADIGM", "content": "We offer our definition of Requirement-Oriented Prompt Engineer-\ning (ROPE) and describe why, when, and who need it, and why\nwe propose a training toward ROPE in this work. In short, ROPE\nrepresents a paradigm shift in how we interact with LLMs,\nfocusing on the importance of crafting accurate and com-\nplete requirements to achieve better results, especially for\ncomplex, customized tasks.\nThe definition of requirements. We define a requirement as\na skeleton instruction that communicates an essential condition\nor constraint on desired LLMs output (e.g., \"Response is less\nthan 100 words\"). Requirements instruct LLMs to perform tasks\nthat may deviate from their default behavior, guiding LLMs to align\nwith user's goals.\nIn our work, we focus on evaluating requirements quality rather\nthan quantity, and we operationalize requirement quality in our\nwork (defined in Section 4.2) by adopting a taxonomy from the\nrequirement engineering literature (Section 2.3). Ensuring require-\nments are accurate and complete is essential for effective LLM\nprompting, as poor-quality requirements can lead to harmful out-\ncomes e.g., missing requirement \"anonymize the data\" may\nmake LLMs keep identifiable information in data, and vague re-\nquirement \"delete harmful content\" without a clear definition\nof \"harmful\u201d may cause LLMs to misinterpret sensitive topics like\nmental health or race as harmful.\nHere we also focus on natural language requirements due to its\nuniversal understanding and tight connection to LLM prompting.\nWhile multi-modal requirements can be compiled into prompts for\ndifferent LLMs, we leave this exploration for future work.\nThe relation between requirements and prompts. We view\na prompt as a super-set of requirements. It contains not only users'\ncustomized requirements, but also other (orthogonal) factors like\nfluency and standard prompting tricks. Among these factors, re-\nquirements are more user-centered and LLM-agnostic - users' goals\ngenerally remain consistent across models (e.g., GPT-4, Gemini)."}, {"title": "4 TRAINING AND EVALUATION DESIGN FOR\nROPE", "content": "We develop a training and assessment suite to help users im-\nprove their ability to write accurate and complete requirements\nfor instructing LLMs. We adopt a backward design method [67], a\nwell-established instructional design approach that starts by identi-\nfying the desired learning outcomes - in our case, writing effective\nrequirements for LLMs - then works backward to develop assess-\nments and training aligned with goals. Aligned assessments and\ntraining are critical to ensure that what is taught directly prepares\nparticipants for the skills they are expected to demonstrate.\nThrough multiple pilot studies with novices and experts (n = 10),\nwe refine three key components of design: (1) realistic tasks that mir-\nror real-world prompting challenges (Section 4.1); (2) assessments\nthat connect requirement quality to LLM outcomes (Section 4.2);\nand (3) deliberate practices with feedback in a system (Section 4.3).\nBeyond training (Section 6), our ROPE assessment and materials\nalso enable us to build more nuanced understandings on how re-\nquirements affect LLM outputs."}, {"title": "4.1 Task Design: LLM-Hard Prompt Programs\nfor Replication", "content": "We aim for novices to eventually develop the ability to articulate\nrequirements for their own prompt programs. However, to begin\ntheir training, we need to provide a set of concrete sample tasks for\nthem to practice and for us to evaluate their progress. Below, we\noutline the six carefully designed tasks.\nTask setup. To ensure the training and evaluation process is\nboth realistic and representative, our primary objective is to have\nusers write natural language prompts to instruct LLMs in generating\nprograms. Instead of focusing on open-ended tasks - which are\ndifficult to assess and provide consistent feedback on - we aim to\nprovide clear ground truths for evaluation and training. To this end,\nusers are asked to write prompts that instruct LLMs to replicate a"}, {"title": "4.2 Assessment Design: Requirement-Focused\nIntrinsic and Extrinsic Evaluation", "content": "To evaluate users' ability to instruct LLMs effectively, we assess both\nthe quality of the user prompts (requirements within the prompt)\nand the prompt's impact on the quality of the LLM's output. Thus,\nfor each user task completion, we calculate the Overall Test Score as\nis the average of the Requirement Quality and LLM Output Quality\nscores:\n\u2022 Requirement Quality Score: This intrinsic metric assesses, \u201cDoes\nthe user's prompt accurately and comprehensively cover all re-\nquirements?\u201d It measures the completeness and correctness of\nthe requirements by comparing those extracted from the user's\nprompt to expert-defined reference requirements for each task\n(described in 4.1). Drawing from the requirements defect taxon-\nomy [4, 43], we track both commission errors (incorrect, inconsis-\ntent, or ambiguous requirements) and omission errors (missing\nground-truth requirements). Given the inherent ambiguity in\ndefining requirements and the varying levels of granularity (Sec-\ntion 3), we break the prompt into one-sentence clauses to extract\nindividual requirements. The requirement quality is then opera-\ntionalized as the percentage of requirement clauses that are free\nfrom commission or omission defects.\n\u2022 LLM Output Quality Score: This extrinsic metric evaluates, \u201cCan\nthe user's prompt successfully guide the LLM to achieve the\nintended goals?\u201d It measures the proportion of desired features\nimplemented in the LLM-generated output that align with the\nreference (Section 4.1). We pass users' prompts to GPT-40 and\ncompare the generated output to the reference program.7 Since\nmany features are difficult to evaluate automatically (e.g., GPTs\nbehaviors cannot always be checked via rules), we rely on man-\nual evaluation. For games, an expert interacts with the gener-\nated program to verify whether the requirement features are\ncorrectly implemented. For GPT interactions, an expert grades\nwhether the GPTs' replies display expected behaviors (e.g., ask-\ning a follow-up question when the user's input is ambiguous).\nWe grade two to three complete conversations per GPTs, with 5\nturns of GPTs responses each.\nAssessment validation. Three of the authors discuss to iterate\nthe grading rubrics on the tasks during the pilot study. We confirm\nthat expert can measure requirement quality using the percentage of\ncorrect requirement clauses in novice prompts, and that this corre-\nlates positively with the expert's judgement (Spearman's $\\rho$ = 0.66).\nFor grading in the user study (Section 5), two authors (one of whom\ndid not participate in the rubrics development during pilot) indepen-\ndently grade 10% of the randomly chosen pre-post test responses.\nWe check the inter-rater reliability between the authors' scoring by\ncalculating the Intraclass Correlation Coefficient (ICC) [33], find-\ning a strong reliability (ICC = 0.9, 95% Confidence Interval = [0.7,\n0.98]). Any discrepancies in scoring are resolved through discussion,"}, {"title": "4.3 Interactive Training Mechanism: Dedicated\nPractice and Feedback on Requirement\nDefects", "content": "We design a training mechanism to support deliberate practice on\nrequirement elicitation and refinement by disentangling require-\nment articulation from peripheral tasks like persona crafting or para-\nphrasing in existing prompt engineering instructions. We apply key\nlearning principles like scaffolding and worked example [32], and\nwe implement the training into an interactive interface as shown\nin Figure 4.\nIn this training, a user (a prompt novice) is asked to replicate\none Game task (Tetris) and one GPTs task (Email Proofreader),\nsolely by describing the requirements of the given program as accurate\nand complete as possible. For example, to reproduce the customized\nTetris game in Figure 4, the user will first start by outlining the\nmain milestones (e.g., creating the game board, handling piece\nplacement, etc.), and then provide more detailed specification per\nstep (e.g., define the size of the game board). Novices are not ex-\npected to achieve full success on their first try; we use three types\nof requirement-focused feedback to guide users in continuously\nrefining their requirements:\n\u2022 Textual hint and clarification, via chatbot (Figure 4A): We create a\ntutor chatbot to offer users a natural, conversational experience\nto discuss and reflect on their requirements. For instance, the\nchatbot may ask \"What's on top of the board?\", when a novice\nmisses the requirement for \"Tetris title rendering\". The\ntextual feedback encourages critical thinking on missing or in-\ncorrect requirements.\n\u2022 Reference requirement example, via requirement working doc-\nument (Figure 4B): We progressively reveal reference require-\nments when users mention them during interaction with the\nchatbot. The expert-written reference examples provide rein-\nforcement on correct requirements, helping users understand how\nto formalize and organize requirements.\n\u2022 LLM output counterfactual, via generated visualization (Figure 4C):\nFor incorrect requirements that can be visually demonstrated,\nwe generate flawed programs by implementing incorrect require-\nments (e.g., wrong board dimensions), or maliciously misinter-\npreting ambiguous requirements. For example, a vague require-\nment like \"Use keys to move pieces\" might result in a flawed\nTetris game where pieces can move upward, exposing the un-\nspecified allowed movement in requirement. The visual coun-\nterfactual is currently limited to the controllable game code\ngeneration; as GPTs outputs tend to be less predictable, we dis-\nplay static chat histories as illustrative examples for GPTs.\nNote that our assessment tasks require users to write complete\nprompts similar to those in Figure 2, we deliberately avoided having\nusers write prompts during training. Instead, users engage in con-\nversational interactions designed to continuously encourage them\nto think about requirements. Additionally, users' written responses\ndo not directly trigger LLM output generations. We use reference re-\nquirements to guide the LLM generation, ensuring that all feedback"}, {"title": "5 USER STUDY DESIGN", "content": "We conducted a user study to understand whether our requirement-\nfocused training is effective in improving novices on writing re-\nquirements in prompts, and whether requirement-focused training\nis more effective than status-quo prompt engineering training. We\nalso examined how automatic prompt optimization affects perfor-\nmance.\nStudy procedure. We employed both within-subject and between-\nsubject designs in our study. To capture the requirement-focused\ntraining gains, we utilized a pre-test and post-test approach, a"}, {"title": "6 USER STUDY RESULTS", "content": "We start by answering our central research questions: Can we help\nend-users better instruct LLMs through requirement-focused train-\ning? We quantitatively measured learning gains by capturing par-\nticipant performances from the pre-test to post-test within each"}, {"title": "6.1 Learning Gain: Requirement-focused\nTraining Helps Students Instruct LLMs", "content": "experiment group, and evaluated the effectiveness of the two train-\ning approaches by comparing the ROPE and PE group. We further\nunpacked the quantitative results through analyses of participants'\nLikert Scale ratings, as well as the transcribed comments during\ntheir training session and in post surveys.\nRequirement-focused training is effective. Using a two-\ntailed paired t-test, 12 we found that test scores for participants in\nthe ROPE group significantly improved by 19.8% from pre- to post-\ntest (p < 0.001, from 20.0% \u00b1 14.6% to 39.8% \u00b1 18.8%), a two-fold\nincrease. This demonstrated that requirement-focused training en-\nabled participants to instruct LLMs more effectively, and we achieved\nour desired goal: to concentrate users' attention on only requirement\niterations during the training, but make sure users can still apply their\nlearning in overall prompt writing. Analyzing requirement defects\nin ROPE participants' prompts, we observed a noticeable decrease\nin omission errors (from 5.6 to 3.2 per participant), but a slight\nincrease in commission errors (from 0.5 to 0.7 per participant). This\nindicated that participants became more aware of requirements, but\nneed additional training for requirements clarity and accuracy.\nROPE participants' self-reflection highlighted that they under-\nstand the value of emphasizing and iterating on requirements, which\nmay contribute to their improvement. Six ROPE participants explic-\nitly noted the connection between requirements and prompts in\ntheir post-survey. For example, S30 described requirements as a way\n\u201cto be more clear and not confuse the system by giving too much\nunnecessary information\", and S8 noted that a requirement-focused\napproach helped them organize their thoughts: \u201cI learned to orga-\nnize my requirements logically so we can easily revise and improve\nthem\u201d. S10 neatly described their shift towards requirement-focused\nprompting strategy: \u201csometimes when I write prompts, [I find] steps\nare hard to be clearly divided, or I didn't consider to divide them\nthat detailed. However, it's important to do so, as it appears to give\nLLM more direct and clear instruction. When the steps are divided,\nit's easier to see the missing details in my original prompts too\u201d.\nRequirement-focused training is more effective than stan-\ndard prompt engineering practice. From the post-test prompt\ndata, we observed that requirement-focused training and standard\npractice had distinct effects on how participants wrote prompts.\nROPE group spent more time (19 minutes vs. 14 minutes) and wrote\nlonger prompts (796 vs. 458 characters) than the PE group. In 100\nrandomly selected pairwise comparisons, ROPE participants pro-\nduced more structured prompts 87% of the time, consistent with\ntheir self reflections reported above.\nThese prompt also revealed learning gain differences. A two-\nsample t-test showed that the ROPE achieved significantly higher\nlearning gains compared to the PE group (p < 0.001). In fact, while\nROPE significantly improved (19.8% as mentioned above), we did\nnot observe much gain in PE group's pre-to-post score (1.2%). The\ncontrast suggested that novices indeed could not acquire requirement\narticulation skills through standard prompting training alone."}, {"title": "6.2 In-depth Analysis: The Validity of ROPE\nParadigm", "content": "Going beyond learning gains, we dive deeper into the connections\nbetween requirement in prompts and LLM output quality, as well\nas the role of optimizers.\nRequirement quality vs. LLM output quality: promising\ncorrelation with nuances. To examine whether more correct and\ncomplete requirements led to better LLM outputs, we calculated\nthe correlation between the two components of our Overall Test\nScores: Requirement Quality Score and LLM Output Quality Score. We\nfound a strong positive correlation, with a Spearman's correlation\ncoefficient of $\\rho$ = 0.72.\nHowever, task-specific analysis revealed nuances. As shown in\nFigure 5, while all other tasks achieved $\\rho$ \u2265 0.88, Connect4 did\nnot show any correlation. Besides one ROPE participant who scored\n80% on requirement quality and 70% on LLM output in their post-\ntest Connect4, all other participants' LLM output scores saturated\naround 40% even with high requirement quality \u2265 80%.\nUpon reviewing user prompts and LLM outputs, we suspected\nthat Connect4 had fewer natural-language-to-code mappings in\nGPT-40's training data.13 Consequently, the model encountered\ndifficulties implementing uncommon requirements in code for\nConnect4, such as \u201cCross out the winning cells\", which\nonly succeeded in 1 out of 16 prompts. Moreover, as shown in Fig-\nure 5, Connect4 prompts formatted as code implementation plans\nproduced better results than casual narrative prompts, even with\nthe same number of requirements. After we reformatted all game\nprompts into similar formats with the optimizer, the correlation\nbetween requirement and LLM output for Connect4 increased from\n$\\rho$ = 0.02 to $\\rho$ = 0.3, partially supporting our hypothesis. In con-\ntrast, Tic-Tac-Toe did not exhibit such sensitivity, likely due to a\nbroader variety of rules in LLM's training data. GPTs tasks did not\nhave this problem since both the prompt and implementation are\nin natural language.\nWe further analyzed how different types of requirement defects\nimpact LLM output quality. We found that the numbers of omission\nerrors (incomplete requirements) had a stronger negative impact\non LLM Output Quality Score ($\\rho$ = -0.4) compared to commission\nerrors (inaccurate requirements) ($\\rho$ = 0). This suggests that while\nLLMs can correct inaccuracies in requirements, they struggle to"}, {"title": "7 DISCUSSION", "content": "In this work, we introduce Requirement-Oriented Prompt Engi-\nneering (ROPE), a human-AI collaborative prompting paradigm\nwhere humans focus on effective requirements specification. Our\nevaluation shows that requirement-focused training significantly\nenhances novices' ability to extract and articulate requirements\nin prompts, leading to more goal-aligned LLM outputs; we also\nnotice mismatch between humans' under-specified requirements\nand LLMs' misinterpretations is a key communication barrier (Sec-\ntion 6.2). While our work made an important step toward ROPE,\nseveral limitations remain, and there are many interesting questions\nto explore. Here, we reflect on possible immediate extensions on\nrequirement-focused training, as well as the longer-term evolution\nof the ROPE paradigm."}, {"title": "7.1 Limitations and Next Steps on\nRequirement-Focused Training", "content": "Our training program significantly improved participants' ability\nto extract and articulate requirements, though the skill remains\nchallenging. Post-training assessments showed an average score\nof 40%, with the top performer increasing from 24% to 63%. \u03a4\u03bf\nfurther improve the effectiveness of requirement-focued training,\nwe propose several next steps.\nOne easy extension is to make the training session longer. We\nobserved that the 40-minute training time was difficult for non-\nnative English speakers, and participants had limited opportunities\nfor iteration due to time constraints. Future work should explore\nlonger training sessions and adaptations for users with varying\nlanguage proficiencies.\nIn addition to extending length, broadening the scope of the\ntraining will help improve the generalizabilty. Our study showed\nthat the skills gained transferred to new GPTs and game develop-\nment tasks. However, an open question remains: how well do these\nskills apply to other tasks like data analysis or creative content gen-\neration? Fortunately, our ROPE interface is adaptable to different\ntasks, as training can be generated based on provided reference"}, {"title": "7.2 Reflection on the ROPE Paradigm", "content": "Along with prior work on requirement-oriented evaluation [30], we\nbelieve that ROPE moves us towards a future where requirements\nserve as the central interface between Al and humans. Here,\nwe discuss key skills humans and LLMs need to develop to prepare\nfor a ROPE future.\nWhat should humans be equipped with for ROPE? end-to-\nend prompting skills. To ensure success of ROPE, multiple skills\nneed to be cultivated among humans. Our training successfully\nhelped users develop the ability to extract requirements from given\nexamples and express them completely and correctly, but our study\nalso revealed more opportunities in requirement iteration and test-\ning skills of an end-to-end prompting procedure. Specifically, users\nneed to identify examples to compare LLM outputs and adjust the\nrequirements accordingly.\nFor example, users should create diverse and representative exam-\n       ples to identify and test requirements, particularly for ambiguous or\nopen-ended tasks involving edge cases or complex interactions. In\nour study, many users, especially those unfamiliar with games like\nConnect4, missed the testing scenarios like a tie game, which led\nthem to omit the requirement \"Display a 'Tie Game!' message\nif no player wins\" in their prompt. This is consistent to findings\nin debugging training [39] and other end-user prompt engineering\nwork [77], as novices often create too few test examples. Future\nstudies should support users to generate more testing examples.\nAdditionally, users should iterate requirement granularity based\non task \u201cLLM-hardness.\" This involves observing LLM failures and\nrefining requirements, essentially developing a theory of mind for\nLLMs [65]. Some of our participants was already thinking about\n\u201cwhat is obvious enough for ChatGPT to already understand/per-\nform well on\u201d (S28) and adjust their prompt accordingly (although\nnot necessarily correctly). In our training, we introduced the con-\ncept of requirement specificity in Tetris, where users generate\nrequirements in two levels of granularity (i.e., main steps and details,\nSection 4.3). However, some tasks required even greater specificity,\nsuch as generating non-standard Connect4 gameplay elements\n\u201cCross out winning cells\u201d. This ability to adjust specificity in\nrequirements can be influenced by domain expertise, for example,\nexpert developers can iterate prompts with more accurate details to\nsuccessfully generate code while students often struggle [44]. Fu-\nture studies should offer adaptive support for adjusting requirement\ngranularity based on task complexity and user expertise.\nWhat should optimizers and LLMs be capable of for ROPE?\nsupport complementary task delegation. For a successful ROPE\nfuture, optimizers and LLMs must develop complementary capabil-\nities to support human skills.\nFirst, as humans improve on articulating good requirements,\noptimizers should test different \u201cimplementations\u201d of requirements.\nThis includes varied wordings, alternative expressions (e.g., zero-\nshot descriptions or few-shot examples), and the order, hierarchy,"}, {"title": "8 CONCLUSION", "content": "In this work, we advocate for focusing prompt engineering effort\non human-centered tasks, ensuring users include all necessary re-\nquirements in their prompt to achieve goals. We introduce the\nRequirement-Oriented Prompt Engineering (ROPE) paradigm, and\ndesign training materials where prompting novices practice require-\nment articulation on complex prompting tasks. We also develop\naligned assessment metrics capturing both intrinsic quality of user\nprompts in terms of requirement quality, as well as the extrinsic\nquality of prompt effectiveness in achieving intended outcomes.\nBy providing targeted feedback on requirements, our ROPE sys-\ntem helps users produce higher-quality requirements and prompts\nmore effectively than traditional prompt engineering training. As\nwe look to the future, it is clear that the demand for LLM-based\napplications will only grow. As LLMs become more integrated into\ncomplex task-solving for more users, the ability to clearly articulate\nrequirements will be key to effectively guide LLMs. We believe that\nusers should be equipped with the foundational skills to prepare\nfor the ROPE future, and the right training will empower users to\nharness the full potential of LLMs."}]}