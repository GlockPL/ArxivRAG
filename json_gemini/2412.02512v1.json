{"title": "Pre-Deployment Information Sharing: A Zoning Taxonomy for Precursory Capabilities", "authors": ["Matteo Pistillo", "Charlotte Stix"], "abstract": "The gradient above encapsulates the core idea of this paper: high-impact and potentially dangerous capabilities can and should be broken down into \u2018early warning shots' long before reaching 'red lines.' Each of these early warning shots should correspond to a precursory capability. Each precursory capability sits on a spectrum indicating its proximity to a 'final' high-impact capability, itself corresponding to a red line. To meaningfully detect and track capability progress, we propose a taxonomy of 'dangerous capability zones' (a \u2018zoning taxonomy') tied to a staggered information exchange framework that enables relevant bodies to take action accordingly.\nExisting pre-deployment information-sharing infrastructures are not yet sufficiently developed to rise to the challenge of collectively responding to the timely detection and mitigation of risks arising from capability progress. As it stands, information about capabilities is shared late in the AI lifecycle-often only at deployment\u2014and reporting requirements are underspecified.\nIn the Frontier AI Safety Commitments, signatories commit to \u201cshar[ing] more detailed information with trusted actors, including [an] appointed body, as appropriate\u201d (Commitment VII). Building on our zoning taxonomy for high-impact capabilities, this paper makes four recommendations for specifying information sharing as detailed in Commitment VII. (1) Precursory capabilities should be shared as soon as they become known through internal evaluations before deployment. (2) AI Safety Institutes (\u201cAISIs\u201d) should be the \u201ctrusted actors\" \"appointed\u201d to receive and coordinate information on precursory components. (3) AISIs should establish adequate information protection infrastructure and guarantee increased information security as precursory capabilities move through the \u2018zones' and towards red lines, including if necessary\u2014by classifying the information on precursory capabilities or by marking it as 'controlled.' (4) High-impact capability progress in one geographical region may translate to risk in other regions and necessitates more comprehensive risk assessment internationally. As such, AISIs should exchange information on precursory capabilities with other AISIs, relying on the existing frameworks on international classified exchanges and applying lessons learned from other regulated high-risk sectors.", "sections": [{"title": "I. Introduction", "content": "There is a growing consensus that information is the \u201clifeblood of good governance\" and that information sharing should be one of the \u201cnatural initial target[s]\u201d of AI governance. Up-to-date and reliable information about AI systems' capabilities and how capabilities will develop in the future can help developers, governments, and researchers advance safety evaluations, develop best practices, and respond effectively to the new risks posed by frontier AI. Information sharing also supports regulatory visibility and can thus enable better-informed Al governance. Further, access to knowledge about AI systems' potential risks allows AI systems claims to be scrutinized more effectively. By contrast, information asymmetries could lead regulators to miscalibrated over-regulation\u2014or under-regulation of AI and could contribute to the \"pacing problem,\u201d a situation in which government oversight consistently lags behind technology development. In short, there is a strong case for information sharing being one \"key to making AI go well\".\nThe Frontier AI Safety Commitments (\"FAISC\") are an important step towards more comprehensive information sharing by AI developers. Through Commitment VII, signatories\u2014which include all current frontier Al developers\u2014have committed to:\n\u2022\t\"Provid[ing] public transparency.\""}, {"title": "II. The Current Pre-Deployment Information Sharing Landscape", "content": "This section examines the current state of information sharing about high-impact capabilities. We review how comprehensive current disclosure frameworks are and whether developers share information concerning high-impact capabilities prior to deployment.\nWe distinguish three layers of information sharing.\n\u2022\tMandatory information sharing. Section II.A examines the information that AI developers are required to share under existing legal frameworks. In the United States (the \u201cUS\u201d), we review Executive Order 14110 on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence of October 30, 2023 (the \"Executive Order 14110\") and the National Security Memorandum of October 24, 2024 (the \"NSM\"), as well as the National Institute of Standards and Technology's (\u201cNIST\u201d) guidelines on \"Managing Misuse Risk for Dual-Use Foundation Models\" (the \u201cNIST guidelines\"). In the European Union (the \u201cEU\u201d), we review Regulation (EU) 2024/1689 of the European Parliament and of the Council of June 13, 2024 (the \"AI Act\").\n\u2022\tVoluntary information sharing based on external commitments. Section II.B examines the information that AI developers may share based on voluntary reporting mechanisms or that AI developers have voluntarily committed to sharing. We review the Voluntary AI Commitments secured by the White House on July 21, 2023, Canada's Voluntary Code of Conduct on the Responsible Development and Management of Advanced Generative AI Systems (the \"Canada Voluntary Code of Conduct\"), the Hiroshima Process International Code of Conduct for Advanced AI Systems (the \"Hiroshima Process International Code of Conduct\"), and the OECD AI Principles."}, {"title": "(A) Mandatory Information Sharing", "content": "We start by examining the mandatory reporting landscape set forth by existing legal frameworks in the US and the EU. For the US, we briefly review Executive Order 14110 and the NIST guidelines; for the EU, we review the AI Act. We note that, in addition to the disclosure requirements described below, countries will gain an insight into model capabilities through government-run evaluations.\nUnder Executive Order 14110, President Biden directed the Secretary of Commerce to require dual-use foundation Al model developers to provide the Federal Government with \u201cinformation, reports, or records\" on \"the results of any developed dual-use foundation model's performance in relevant A\u0399 red-team testing\" (Sec. 4.2(i)). The Bureau of Industry and Security (\u201cBIS"}, {"title": "(B) Voluntary Information Sharing Based on External Commitments", "content": "The FAISC complement a list of prior voluntary frameworks containing provisions on information sharing on capabilities, some of which have been also signed or endorsed by FAISC signatories. This section provides an overview of these voluntary frameworks, including the Voluntary AI Commitments secured by the White House, the Canada Voluntary Code of Conduct, the Hiroshima Process International Code of Conduct, and the OECD AI Principles.\nNine FAISC signatories signed the Voluntary AI Commitments secured by the White House in July 2023. The Voluntary AI Commitments include a commitment to \"[p]ublicly report model or system capabilities.\u201d In particular, signatories should \u201cpublish reports for all new significant model public releases\" and detail \"the safety evaluations conducted (including in areas such as dangerous capabilities, to the extent that these are responsible to publicly disclose).\" Two FAISC signatories' also signed the Canada Voluntary Code of Conduct of September 2023, under which \u201cdevelopers and managers of advanced generative systems\u201d commit to publishing \"[s]ufficient information\" on capabilities \u201cto allow experts to evaluate whether risks have been adequately addressed.\u201d Furthermore, five FAISC signatories are members of the Frontier Model Forum. While the Frontier Model Forum is an industry body without specific voluntary commitments that its members sign onto, it should be noted that one of the goals of the Frontier Model Forum includes \"[e]stablish[ing] trusted, secure mechanisms for sharing information among companies, governments, and relevant stakeholders regarding AI safety and risks\".\nTwo additional voluntary frameworks are worth describing briefly, even though they have not been explicitly endorsed or signed onto by FAISC signatories: the Hiroshima Process International Code of Conduct, developed in October 2023 by the G7 based on the International Guiding Principles, and the OECD AI Principles. One of the \u201cactions\u201d recommended by the Hiroshima Process International Code of\""}, {"title": "(C) Voluntary Information Sharing Based on Internal Commitments", "content": "Company-led internal commitments enacted by the leading frontier AI labs\u2500generally referred to as \"Responsible Scaling Policies\u201d or \u201cRSPs\u201d form the current backbone of information sharing tied to different capability thresholds. Through RSPs, companies voluntarily commit to offering significantly more detailed information on their models' capabilities than what is required under mandatory frameworks (Section II.B) and external commitments (Section II.C). Below, we briefly review three variations of RSPs-Anthropic's RSP, OpenAI's Preparedness Framework, and Google DeepMind's Frontier Safety Framework-including the type of information they each aim to collect and how this information is to be shared.\nFirst, we review Anthropic's recently updated RSP. Under the updated policy, Anthropic will engage in three-tiers of information sharing: with internal staff, with the U.S. Government, and with the general public. First, Anthropic will share a summary of a 'Capability Report' with its regular-clearance staff, redacting any highly-sensitive information and \u201ca minimally redacted version\u201d with a \u201csubset\" of staff. Each Capability Report is intended to \u201cattest[] that a model is sufficiently far from each of the relevant Capability Thresholds, and therefore (still) appropriate for storing under an ASL-N Standard\u201d. In terms of content, this Capability Report includes \u201cevaluation procedures, results, and other relevant evidence gathered around the time of testing\u201d. Second, Anthropic will \u201cnotify a relevant U.S. Government entity if a model requires stronger protections than the ASL-2 Standard\u201d. Third, Anthropic will publicly release \u201ckey information related to the evaluation,", "summaries of related Capability [Reports] when [it] deploy[s] a model": "For instance, the Model Card for Claude 3 was available on March 4, 2024, on the day Claude 3 became available.\nSecond, we review OpenAI's Preparedness Framework. In addition to publishing system cards, under the Preparedness Framework OpenAI commits to describing in \u2018scorecards' the levels of risks a model exhibited before mitigation efforts and after mitigations have been put in place. Scorecards will be regularly updated to \"reflect the latest research and findings,", "continually.": "owever, OpenAI does not commit to sharing information about high-impact capabilities before deployment, nor does it lay out a precise timing for the publication of its Scorecards. In practice, OpenAI published recent systems' scorecards at the time"}, {"title": "(D) Gaps and Shortcomings of the Current Landscape", "content": "Sections II.A-II.C reviewed the state of play in pre-deployment information sharing and highlighted some common traits in mandatory and voluntary frameworks on information sharing. In this section, we describe their gaps and shortcomings. In summary, we highlight how information-sharing requirements are underspecified and the relevant frameworks underdeveloped, both concerning the content and the time of disclosure. Subsequently, in Section III.A, we put forward a taxonomy to address these shortcomings, followed by four recommendations in Section III.B for operationalizing our taxonomy to strengthen Commitment VII of the FAISC.\nWe noticed three main fallacies in the current information sharing landscape. First, the information that ought to be shared is underspecified. Second, the corresponding information-sharing timelines are underdeveloped. Third, even where information-sharing timelines are more developed, they require information to be shared relatively late, which potentially undermines adequate responses to the receipt of this information.\nFirst, the information to be shared is underspecified. As noted earlier, mandatory information-sharing requirements are insufficiently detailed. While both the Executive Order 14110 and the AI Act point in the direction of disclosing information on pre-deployment capabilities, many details remain undefined. Similarly, voluntary disclosure frameworks (Section II.B) and RSPs (Section III.C) lack specificity. For instance, the Voluntary AI Commitments do not clarify which \"dangerous capabilities\" should be reported, the Canada Voluntary Code of Conduct does not specify what constitutes \u201c[s]ufficient information,\" and the OECD AI Principles do not describe which information on \u201ccapabilities\u201d should be shared.\nSecond, the corresponding timelines towards sharing information are underdeveloped. As to mandatory requirements, in the AI Act there is no actual information-sharing obligation for GPAI model providers\""}, {"title": "III. Towards Actionable and Meaningful Information Sharing for High-Impact Capabilities", "content": "Pre-deployment information sharing is currently insufficient. As previous Section II shows, reporting requirements and commitments are underspecified, and information-sharing frameworks are underdeveloped. Mandatory and voluntary frameworks do not sufficiently specify what information developers are expected to disclose before deployment or when. Company-led internal policies such as RSPs equally need more detail and harmonization to be effective.\nAt the same time, early awareness of high-impact capabilities would benefit AI safety. In addition to improving regulatory visibility, early awareness can grant researchers sufficient time to thoroughly evaluate potential risks, develop safety measures, and implement safeguards. Early awareness can enable the field to focus on relevant challenges and coordinate technical safety work. It can also help verify whether the capability thresholds described in RSPs have been reached and if the forecasts are and remain reasonable.\nWe are not the first to ring the bell on the need for more clarity in information sharing and more transparency on high-impact capabilities before deployment. Looking at disclosure timing and context, emphasize the importance of developing precise guidelines for when developers should report information during the AI lifecycle. advocate for standardized criteria that would help companies determine what warrants reporting and when. mention the question of what elements should be included in standardized reports. Different approaches to standardizing company reporting requirements and methodologies have been proposed by,. Regarding pre-deployment transparency, recently advocated disclosing in-development capabilities, though limited to confirming when specific capabilities are achieved. The disclosure of in-development capabilities aligns with the Emerging Processes for Frontier AI Safety identified by the UK Government in October 2023, which include the disclosure of certain model-specific information about advanced AI systems before they are deployed.\nThe subsequent Sections contain our main contribution to the debate on information sharing. Section III.A puts forward a zoning taxonomy for high-impact precursory capabilities, thereby underpinning one solution to the underspecification problem and also offering a pragmatic information-sharing approach. In Section III.B, we focus on the immaturity of information-sharing frameworks. We build on our proposed taxonomy and offer reflections to FAISC signatories on how the taxonomy can underpin a structured information exchange regime, domestically and internationally, and enable harmonization between companies' voluntary efforts."}, {"title": "(A) A Zoning Taxonomy for Precursory Capabilities", "content": "Below, we suggest a novel taxonomy to empower pre-deployment information sharing to tackle the aforementioned shortcomings. As we explain later in this section, the adoption of this taxonomy by FAISC signatories would constitute a significant advancement in the information sharing state of play because it would further specify what information developers should share, when, to whom, and how.\nOur taxonomy relies on two building blocks. First, our taxonomy acts on the recommendation of the experts gathered at the 2024 International Dialogues on AI Safety to \u201cset early-warning thresholds\" or \"levels of model capabilities indicating that a model may come close to crossing a red line\". The first building block of our taxonomy then is the notion of 'precursory capability,' which we believe is a more refined threshold for early-warning signals toward high-impact capabilities than capability levels. The concept of precursory capabilities is inspired by the event-tree analysis commonly used to describe threat models in nuclear power safety. In this context, it refers to the smaller preliminary capabilities that an AI model needs to have to unlock more advanced capabilities. In our taxonomy, precursory capabilities are \u2018but for' skills: simpler skills without which a certain action is impossible. The following metaphor further simplifies this concept. A thief needs certain foundational skills and abilities to burglarize a vault and steal its contents. For instance, they need locksmithing abilities, which in turn require competency in many intermediate skills such as tool proficiency, precision, excellent hand-eye coordination, and technical knowledge of lock mechanisms. At an even more foundational level, a thief must first be able to see or feel the vault and physically hold the tools. All of these are precursory capabilities to appropriating a vault's content.\nThe second building block of our taxonomy is the relationship between these precursory capabilities. In light of that, we suggest a \u2018zoning taxonomy.' In our taxonomy, precursory capabilities are connected by a causal relationship and located on a gradient that leads to the final high-impact capability. While red lines must be established, we believe AI systems' high-impact and potentially dangerous capabilities are more valuably depicted in a \u2018gradient,' where each precursory component may bring us closer to systemic risk (see Figure A below). Expanding on the previous metaphor: when it comes to burglarizing a vault, a thief with locksmithing abilities is more effective than one without such skills, and a thief who can use certain tools is more effective than one who cannot. A zoning taxonomy helps us clarify many aspects, including which precursory capabilities are necessary to unlock a more high-impact and potentially dangerous capability, how precursory capabilities are connected, and how many intermediate steps are required before the high-impact capality is reached. For example, in the thief metaphor, hand-eye coordination is a basic skill that is required broadly for many actions, whereas locksmithing abilities unlock more specific, advanced skills such as picking locks. And finally, the ability to assess and break into a bank's vault illegally would be considered a high-impact and potentially-dangerous ability.\nA zoning taxonomy for precursory capabilities is an innovative lens to frame, track, and more precisely tackle high-impact capabilities. Some nascent breakdowns of high-impact capabilities already appear in the company-led efforts by a subset of FAISC signatories (RSP, Preparedness Framework, and Frontier"}, {"title": "(B) Recommendations to Implement the Taxonomy through the FAISC", "content": "In Section III.A, we offered a taxonomy that allows us to break down high-impact capabilities and enables a more nuanced spectrum toward penultimate red lines. We now advance four recommendations towards specifying information sharing as detailed in Commitment VII of the FAISC, underpinned by this taxonomy."}, {"title": "(1) Early Disclosure Tied to Early Detection", "content": "Our first recommendation for Commitment VI is that FAISC signatories share precursory capabilities to high-impact capabilities when they are first identified before model deployment. Our recommendation entails that \u201cmore detailed information\" in Commitment VII is interpreted to include\""}, {"title": "(2) AISIs as Information Recipients and Coordinators", "content": "Subsequent to providing a taxonomy for the type of information that should be disclosed (Section III.A) and when disclosure should occur (Section III.B.1), we examine with whom FAISC signatories should share this information. Our second recommendation for Commitment VII is that FAISC signatories"}, {"title": "(3) Information Security Proportionate to Risk", "content": "In Section III.B.2, we examined to whom information about precursory capabilities should be disclosed and concluded that AISIs and AISI-like institutions are the best-suited actors for this task. This section tackles the degree of security with which AISIs and similar institutions should handle information on precursory capabilities. In this respect, our recommendation for Commitment VII is that each AISI manages the information received from FAISC signatories with increasing sensitivity depending on its relative location within the zoning taxonomy. In other words, information security should be adequate and proportionate to the location of the relevant precursory capability in the zoning taxonomy.\nIt is essential that the \u201cmore detailed information\u201d that the FAISC signatories share with a \u201ctrusted actor[]\" is secure. Information about high-impact capabilities must be protected against two main risk scenarios: a harmful proliferation of high-impact capabilities posing national security concerns, and race dynamics between developers. First, sharing findings of capability evaluations in an unsecure manner could inadvertently proliferate high-impact capabilities or accelerate their development. Evaluation results might reveal offensive innovations that could attract interest from adversarial actors, raising significant national security concerns. Second, disclosing pre-deployment capabilities could fuel competitive pressures among AI developers, potentially encouraging less responsible behaviors. This could include compromising on safety protocols in a rush to deploy, creating a hazardous race to the bottom regarding safety and oversight."}, {"title": "(4) International Information Exchange Between AISIs", "content": "In Sections III.B.2-3, we recommended that AISIs and AISI-like institutions be entrusted as information recipient and coordinator for precursory capabilities disclosure and that these institutions ensure information security in a way that is adequate and proportionate according to the zoning taxonomy. In this section, we review whether AISIs and AISI-like institutions should share this information onwards, and, if so, with whom and how. Our fourth and final recommendation for Commitment VII is that: AISIS and AISI-like institutions establish an early warning pipeline and alert each other if a precursory capability identified by the zoning taxonomy has been reached, provided that the recipient AISI or and AISI-like institution guarantees appropriate levels of information security.\nIt has been observed that companies may be concerned about unrestricted sharing of sensitive model information across different jurisdictions. A reluctance to exchange sensitive information internationally is very understandable assuming that there is a significant risk that it may leak or end up otherwise in undesirable actors' hands. However, there are several reasons why a safe and secure international exchange of pre-deployment information between AISIs is not only desirable but also feasible. First, sensitive information\u2014whether classified or unclassified\u2014is shared across borders every day, from highly regulated industry information to large-scale risks and national security concerns. Interesting case studies include finance and banking, cybersecurity, nuclear power, disease prevention"}, {"title": "IV. Conclusion", "content": "In conclusion, we believe that the advancement in capabilities of frontier AI systems highlights the need for a more structured pre-deployment information sharing framework, and praise the FAISC signatories for committing to \u201cshare more detailed information with trusted actors\" such as an \"appointed body\" (Commitment VII).\nThe current status of pre-deployment information sharing is insufficient, and a well-defined Commitment VII can significantly improve it. We identified three main shortcomings of the existing information-sharing landscape. First, information-sharing requirements and commitments need to be more specific. Second, information on high-impact capabilities should be shared earlier on in the AI lifecycle to enable more effective mitigation of potential risks. Third, companies' internal policies, such as RSPs, have not been adopted by all frontier AI developers and are not sufficiently specific or harmonized.\nWe developed a novel taxonomy to improve existing frameworks based on the zoning of precursory capabilities to high-impact capabilities. In short, our taxonomy invites FAISC signatories to coordinate and break down high-impact capabilities into smaller causally connected components and locate them along a gradient leading to penultimate red lines. Then, we advanced four main recommendations to implement the taxonomy and seize the opportunity offered by Commitment VII."}]}