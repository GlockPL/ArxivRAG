{"title": "MARs: Multi-view Attention Regularizations for Patch-based Feature Recognition of Space Terrain", "authors": ["Timothy Chase Jr", "Karthik Dantu"], "abstract": "The visual detection and tracking of surface terrain is required for spacecraft to safely land on or navigate within close proximity to celestial objects. Current approaches rely on template matching with pre-gathered patch-based features, which are expensive to obtain and a limiting factor in perceptual capability. While recent literature has focused on in-situ detection methods to enhance navigation and operational autonomy, robust description is still needed. In this work, we explore metric learning as the lightweight feature description mechanism and find that current solutions fail to address inter-class similarity and multi-view observational geometry. We attribute this to the view-unaware attention mechanism and introduce Multi-view Attention Regularizations (MARS) to constrain the channel and spatial attention across multiple feature views, regularizing the what and where of attention focus. We thoroughly analyze many modern metric learning losses with and without MARs and demonstrate improved terrain-feature recognition performance by upwards of 85%. We additionally introduce the Luna-1 dataset, consisting of Moon crater landmarks and reference navigation frames from NASA mission data to support future research in this difficult task. Luna-1 and source code are publicly available at https://droneslab.github.io/mars/.", "sections": [{"title": "1 Introduction", "content": "Exploring deep space objects such as planets, comets, and asteroids involves ambitious and increasingly complex scientific pursuits. It has also been one of the earliest real-world applications of robotic autonomy. Advanced missions strive for spacecraft to land on or maneuver within close proximity to surfaces of highly irregular terrain and varying topography, which poses a significant challenge to spacecraft navigation as communication latency is often too great to permit any Earth-based assistance through radiometric tracking, real-time planning and control, or precise GPS positioning. More recently, these challenges are being addressed through the optical tracking of prominent surface terrain features to provide Terrain Relative Navigation (TRN). This approach has been validated on recent flagship missions including the landing of the Mars Perseverance Rover [56] and the collection of asteroid regolith by OSIRIS-REx [81]. With compute power limited by radiation-tolerant hardware, current approaches to TRN are template matching and correlation techniques using patch-based features (called landmarks) on static navigation maps that are collected and constructed a priori [56,81,109]. The set of landmarks and the underlying map require extensive pre-navigation costs and effort to obtain and develop. In the case of OSIRIS-REx, an estimated USD 68.5 million (roughly 25% of the nine-year operations budget) was spent performing sufficient reconnaissance to gather and refine this data over a 1.5-year period [80,97].\nTo reduce costs and accelerate mission timelines, it would be beneficial to detect and track these landmarks at navigation time similar to Simultaneous Localization and Mapping (SLAM) systems on Earth; a formulation that would also permit generalization to unseen and unexpected scenarios such as planetary weather [46] or asteroid ejection events [65]. SLAM is incredibly challenging to perform during TRN as space environments are generally unstructured, where low lighting and similarity in feature spaces create ambiguity and a lack of re-identifiability [38-40]. The use of learning-based solutions to overcome these challenges is possible with the recent inception of rad-hard inference accelerators [31,37,42], which enable in-situ terrain detection methods [14,30,66,69,90].\nRobust description of these detections remains an open problem. On Earth, this is similar to representation learning for tasks such as fine-grained classification, visual place recognition, and person re-identification. At the core of these applications is an objective to learn discriminative image embeddings for efficient similarity computation and downstream retrieval, which is commonly facilitated by metric learning. Compared to Earth-based recognition, however, landmark recognition in space is more nuanced where only one subclass of geological terrain is considered (e.g., crater) with possibly thousands of individual instances to discern against (e.g., crater 570 vs crater 1181). This is more fine-grained than even the most challenging of traditional benchmarks (e.g., CUB-200 [104]), demonstrated by Figure 1-a. Apart from individual discernability, the terrain on the surface of planetary bodies, moons, and asteroids can vary widely in appearance from one observation to the next (Figure 1-b), which is difficult for metric learning to reason about on its own (Figure 1-c)."}, {"title": "2 Related Work", "content": "Spacecraft Terrain Relative Navigation: Landmarks used for Terrain Relative Navigation (TRN) are collected a priori through extensive surveying of the target body and crafted offline by human ground operators. RElative Terrain Imaging NAvigation (RETINA) [109] and Natural Feature Tracking (NFT) [81] are current asteroid-focused TRN methods that create 3D Digital Terrain Models (DTMs) by Stereophotoclinometry (SPC) [35]. Visually prominent areas on the DTM are identified by hand, which are extracted as 2D image templates and uploaded to the spacecraft. Onboard, these templates (i.e., landmarks) are regenerated in SPC fashion to adjust shading based on the predicted illumination conditions of the surface. Navigation frames are then searched for correspondence by traditional image processing algorithms. The Mars Perseverance Landing Vision System (MP-LVS [56]) deployed a similar technique during the landing."}, {"title": "Terrestrial Recognition", "content": "The front-end vision in current TRN systems can be radically improved by leveraging rad-hard accelerators and object detection-style observation methods discussed in section 1; although a robust description technique is required to close the loop. Earth-based tasks such as fine-grained visual classification (FGVC), visual place recognition (VPR), and person re-identification (Re-ID) intrinsically demonstrate this capability and reason over similar challenges, including high intra-class and low inter-class variances [5, 20, 73], multi-view observations [7,48, 98, 121], and appearance change over time [2,48]. Nevertheless, there are considerable challenges in adopting the current literature. Modern solutions to FGVC, VPR, and Re-ID are focused on description and retrieval problems at internet-scale [17,102,116,118,120] and consequently have become more involved than a single-stage network. These methods employ multiple forward passes [1,28], region proposals [43, 88, 93, 114, 122], model fusions [41, 74, 84, 91, 92, 99, 113, 119], multi-stage re-rankings [4, 70, 112], and high-parameter transformer models [3,18, 24, 26, 32, 59, 68, 83, 89, 95, 111]. As such, there is a primary concern about the physical execution of these techniques onboard resource-limited spaceflight computers [10, 36, 107]. Large models that cannot fit within accelerator caches must be executed in a hybrid manner, where model parameters are streamed from the host processor to the accelerator during inference. This has a detrimental effect on execution time [13] and requires careful consideration, given that cache sizes in the current generation of spacecraft accelerators are small (e.g., 8 MB in [42]).\nFurthermore, TRN landmark recognition requires more granular reasoning than FGVC, VPR, and Re-ID, akin to frame-to-frame feature matching problems in SLAM. Recognition in FGVC, VPR, and Re-ID is performed by recalling instances from a pre-seeded database by global description [2,7], where any viewpoint and domain generalization is generally a byproduct of learning with extremely large datasets [2,6,34] or the aggregation of large datasets [59]; a technique that is not currently adoptable due to the lack of space landmark datasets (two at the time of writing including the proposed Luna-1). Additionally, the sequential nature of the TRN task needs consideration, where any recognition database is populated as samples are encountered instead of recalling against the entire population upfront (the effects of which have not been studied previously)."}, {"title": "Viewpoint Challenges and Attention", "content": "During TRN the observed target body is rotating and revolving distinctly from the spacecraft leading to an unconstrained appearance change in landmark illumination, translation, and rotation over time. Such a transformation space is generally uncommon in the literature (Re-ID would not expect a person observation to be upside-down for example [117]), and modern metric learning losses do not permit invariancy to these transformations directly. The convolutional layers used in modern networks are known to be equivariant to translations over the input image [22, 47], but are not naturally equivariant to rotations. Explicit in-network modifications for adding rotation equivariance have recently been explored including steerable filters [22, 103], multi-orientation feature extractions [27,67], and alternative coordinate systems [33,51,55,75]. Equivariant properties can be additionally learned [11,77], which may be advantageous as a supplement to explicit mechanisms or when explicit mechanisms are themselves undesirable [87]. Learning equivariance is popular in the literature through batch-sampling, mining, and augmentation approaches [12, 16, 44, 71, 105, 115]. The remote sensing literature has studied similar techniques with the fusion of pre-trained group convolutions [21] and probabilistic formulations of metric space locations [58], although they are restrictive in their reasoning through trainings with pre-rotated data.\nThe explicit encoding of equivariant properties into the attention mechanism has recently been explored [9, 15,54]. At large, however, analyzing learned attention equivariance as it compares to these mechanisms (or the combination thereof) has not been studied previously. The Self-supervised Equivariant Attention Mechanism (SEAM) [101] is one of the only works that target attention-equivariance learning directly through self-supervised regularization. Multi-view attention similarity learning such as the Contrastive Attention Map Loss (CAML) [62] has shown impressive equivariant properties as a byproduct of contrastive learning over attention maps. Although, integration of these methods within metric learning frameworks is a challenging task as SEAM requires Class Activation Maps (CAMs) and CAML targets foreground/background feature separation using image statistics from segmentation labels."}, {"title": "3 Methodology", "content": "Prior work demonstrates that attention has a large influence on recognition performance in multi-view settings, but the extent of this influence concerning equivariant properties (either encoded or learned) is unclear. Equivariance does not guarantee that attention, being a strictly learnable mechanism, will be identical between multiple views of the same feature; it only suggests that it should be similar. An alignment of attention focus should lessen the downstream recognition difficulty, maximizing separability and view-dependent groupings in the embedding space, although such a constraint is not readily formulated in current multi-view metric learning pipelines. We suggest that any attention disagreement must be directly accounted for during the training, and propose a soft learning constraint to rectify any variance. This concept forms the basis of our proposed Multi-view Attention Regularizations (MARs), described in this section. We first introduce our learning framework and baseline network architecture in subsection 3.1 and subsection 3.2. We then detail our constraint for aligning attention and frame the overall learning objective in subsection 3.3."}, {"title": "3.1 Learning Framework", "content": "The framework for data augmentation and batch formation plays a critical role in multi-view similarity learning [12,16,44,115], where we start by following the popular SimCLR [16] method. SimCLR aims to maximize the learned representation similarity between augmented views of the same input. With training batch size B, we begin by sampling a minibatch of B/2 samples where each sample x gets augmented by two distinct transformation operations to produce new views $x_1 = t_1(x)$ and $x_2 = t_2(x)$ where $t_1$ and $t_2$ are sampled from the same family of augmentations $T$. $T$ is a composition function of three image transformations that include a random brightness adjustment, rotation, and translation. An encoder network $f(\\cdot)$ is applied to the augmented data to extract intermediate representations $h_1 = f(x_1)$ and $h_2 = f(x_2)$. These representations are in turn mapped to the metric space through projection head $g(\\cdot)$ to yield embeddings $z_1 = g(h_1)$ and $z_2 = g(h_2)$. Given this $(z_1, z_2)$ positive pair, the other $2(B/2 - 1)$ embeddings in the minibatch are considered negative samples. The batch of $z$ embeddings is fed to any applicable metric learning loss $L_{ML}$, as is the traditional metric learning process. To assist with the inter-class granularity of landmark recognition we additionally employ hard sample mining in traditional multi-similarity (MS) [100] fashion to yield $a_p, p, a_n, n$ batch-indices where $a_p, p$ represent anchor-positives and positives (simply the indices of the twice augmented images) and $a_n, n$ the indices of embeddings deemed similar by the MS metric but have different instance labels."}, {"title": "3.2 Network Architectures", "content": "With inspiration from large-scale Earth-based recognition networks [50, 60, 106] we employ a ResNeXt-101 [110] architecture with Squeeze-and-Excitation (SE) [53] attention as the baseline for encoder network $f(\u00b7)$. Encoder $f(\u00b7)$ is the primary bottleneck to onboard execution performance as it will hold the most parameters, and we select ResNeXt-101 as a middle ground between discriminative representation power and model size. Furthermore, we elect to stay on the larger end of model size in contrast to the ResNet-50 class [49] to isolate representation power and examine the effects of different attention and equivariance setups. Our embedding projection head $g(\u00b7)$ is a smaller network consisting of a Generalized Mean Pooling (GeM) [85] layer followed by a linear (512), batch norm, and PReLU activation. In contrast to Earth-based recognition, we perform no functions other than a single shot $f(\u00b7)$ and $g(\u00b7)$ to descript instances."}, {"title": "Encoding Rotational Equivariance", "content": "Augmentations applied in $T$ mimic the unconstrained landmark appearance change found in spacecraft TRN (assuming the spacecraft is in a non-geosynchronous position relative to the target body). As the pose of the target body will be changing independently of the spacecraft we cannot assume rotated landmark views will be limited to anything less than a full 360 degree of change. Although data augmentation attempts to implicitly teach the network to be robust, reasoning over this level of extreme rotation remains a challenging property to learn. As such, we additionally seek to study the benefits of explicit rotational equivariance integration in $f(\u00b7)$.\nRIC-CNN [75] develops a convolutional operation (the Rotation-Invariant Coordinate Convolution, RIC-C) based on a novel coordinate system that permits this equivariance as a replacement to standard convolutional layers. RIC-C extends the idea of deformable convolutions [23] and does not require any transformation of the representation space of input images or intermediate features. This property enacts a simple and efficient implementation, which we leverage in this work by replacing all standard convolution operations in $f(.)$ with RIC-C layers. For brevity, we refer interested readers to [75] for the full account of the coordinate system and RIC-C operation."}, {"title": "Spatial Attention", "content": "SE attention improves the interdependencies within feature maps by assigning weights to each channel and selecting the most relevant for a given input. This type of attention is focused on relevancy between features alone (channel) and carries no understanding of relevancy within an individual feature (spatial). We assume spatial attention has a critical role in multi-view metric learning and introduce this in $f(.)$. Coordinate Attention (CA [52]) provides spatial awareness through distinctive pooling operations in the height and width dimensions while preserving the channel dimensionality. This is in contrast to other spatial attention techniques such as the Convolutional Block Attention Module (CBAM [108]) that collapse channel information via pooling before learning spatial weight factors. We modify $f(.)$ by replacing SE attention with CA."}, {"title": "3.3 Forming Attention Similarity Constraints", "content": "The inclusion of explicit rotational equivariant properties through RIC-C layers and the ability to attend spatially with CA is the basis for which we explore our proposed MARs constraint. During the training procedure, we seek to drive both the what (channel) and the where (spatial) elements focused by the attention mechanism together, without explicitly assuming that one view is correct in either of these aspects. This alignment is thus a moving target, where it is imperative to impose a soft constraint between them. In other words, it is undesirable to calculate a strict differentiation between attention maps at any point during training to avoid a collapse in attention information. The constraint should prioritize that the attention maps from each view evolve similarly over time."}, {"title": "Pose Normalization and Channel Reduction", "content": "To facilitate this constrained evolution we propose to introduce regularization terms by embedding attention into additional metric spaces. Let $A_i$ be the set of attention maps output from ResNeXt block $i \\in N$ from $f(.)$ where $N$ is the number of these blocks. For each positive pair in the training batch, we have multi-view attention maps $A_{i1}$ and $A_{i2}$. For each ResNeXt block outputting $A_i$, we output inverse transformation $t_i^{-1}$ where the translation parameters are adjusted relative to the spatial resolution of $A_i$. We apply the inverse transformation to normalize the translation and orientation (pose) of each attention map to equal that of the input image, yielding pose-normalized attention maps $A_{i1} = t_i^{-1}(A_{i1})$ and $A_{i2} = t_i^{-1}(A_{i2})$. To embed attention into additional metric spaces we employ mini variants of the projection head $g(\\cdot)$, which do not include any linear layers for dimensionality reduction. Instead, we first reduce the channel dimension of $\\hat{A}_i \\in \\mathbb{R}^{C \\times H \\times W}$ through a 1x1 convolution $Conv(\\cdot)$ with reduction factor $r$ to yield $\\hat{A} \\in \\mathbb{R}^{C/r \\times H \\times W}$. This process prevents obscurification, keeps the data correlated, and reduces learnable parameters."}, {"title": "Channel and Spatial Attention Embeddings", "content": "For positive and pose-normalized attention pairs $(A_1, A_2)$ we utilize the mini channel-wise (c) projection head $g_{ci}(.)$ to produce channel attention embeddings $z_{ci1} = g_{ci}(A_1)$ and $z_{ci2} = g_{ci}(A_2)$. GeM pooling collapses the spatial dimensions to yield an embedding with length given by $C/r$ where $C$ is the channel dimension of the current ResNeXt block $i$. For spatial attention embeddings, we first perform height and width pooling (similar to CA) on $A$. Specifically, given height $(y)$ and width $(x)$ pooling operators $Ypool(\\cdot)$ and $Xpool(\\cdot)$ we produce intermediate representations $h_{y1} = Ypool(A)$ and $h_{x1} = Xpool(A)$. These representations are input to mini spatial projection heads $g_{yi}(\u00b7)$ and $g_{xi}(\u00b7)$ to yield height and width embeddings $z_{yi} = g_{yi}(h_{yi})$ and $z_{xi} = g_{xi}(h_{xi})$. The mini projection heads $g_{ci}(), g_{yi}()$, and $g_{xi}(\u00b7)$ do not share any parameters and are instantiated once per block $i \\in N$. This allows distinct regularization on attention maps with the same channel-spatial resolution as well as calculating accurate batch-norm statistics that are channel, spatial-height, and spatial-width disparate."}, {"title": "Multi-view Attention Regularizations (MARs)", "content": "Once embedded, we regulate the channel and spatial attention focus using a cosine similarity loss:\n$L_{cs}(z_1, z_2) = 1 - \\frac{z_1 \\cdot z_2}{\\|z_1\\|_2 \\|z_2\\|_2}$ (1)\ngiven embeddings $z_1$ and $z_2$. We define a channel-wise MARS ($L_{ChMARS}$) as the cosine similarity between channel attention embeddings:\n$L_{ChMARS}(A_1, A_2) = L_{cs}(z_{ci1}, z_{ci2})$ (2)\ngiven positive pair, pose-normalized and dimensionality reduced attention maps $A_1$ and $A_2$. Likewise, we define a spatial-wise MARS ($L_{SpMARS}$) as the cosine similarity between Y-pooled and X-pooled attention embeddings:\n$L_{SPMARS}(A_1, A_2) = L_{cs}(z_{Yi1}, z_{Yi2}) + L_{cs}(z_{Xi1}, z_{Xi2})$ (3)\nand our combined MARs regularization loss by:\n$L_{MARS}(A_1, A_2) = \\gamma_{Ch}L_{ChMARS}(A_1, A_2) + \\gamma_{Sp}L_{SPMARS}(A_1, A_2)$ (4)\nwhere $\\gamma_{Ch}$ and $\\gamma_{Sp}$ are weight parameters that control the influence of channel and spatial attention alignment respectively. With augmented image batch $X$ and mined indices $(a_p, p, a_n, n)$ our complete learning objective is given as:\n$\\mathcal{L}(X, (a_p, p, a_n, n)) = L_{ML}(g(f(X)), (a_p, p, a_n, n)) + \\sum_{i=1}^{N_{f(.)}} L_{MARS}(\\hat{A}_{a_p}^i, \\hat{A}_{p}^i)$ (5)\nwhere\n$\\hat{A}_{a_p} = Conv(t_{a_p}^{-1}(f_i(X_{a_p}))), \\hat{A}_{p} = Conv(t_p^{-1}(f_i(X_{p})))$ (6)\nwith $X_{a_p}$ and $X_{p}$ the anchor-positive and positive pair images and $f_i(\u00b7)$ the i'th block in $f(\u00b7)$ that outputs attention maps $A_i$. Our end-to-end pipeline with MARs regularization is shown in Figure 2."}, {"title": "4 Evaluation", "content": "We wish to study lightweight single-shot TRN landmark description using modern metric learning both with and without MARs as well as the effect of different attention and equivariant mechanisms. We first discuss the datasets used for experimentation in this section, followed by a description of our experiments, implementation details, and analysis of the results."}, {"title": "4.1 Datasets", "content": "We leverage three datasets of Mars, Moon, and Earth landmark images. HiRISE [29] contains 700 Mars crater images and is the only real-world dataset available at the time of writing. We further introduce Luna-1, a 5,067 sample Moon crater dataset generated in the Blender 3D software [8] with real-world NASA data products. Luna-1 additionally contains 2,161 emulated navigation frames from a Lunar Reconnaissance Orbiter (LRO) three-orbit reference navigation sequence. An example landmark image from HiRISE and Luna-1 is shown in Figure 3. Additional Luna-1 details and visualizations can be found in the supplementary. For Earth landmarks, we utilize the stadium class from RESISC45 [19], a terrestrial remote sensing scene classification dataset with 700 samples. We refer to HiRISE, Luna-1, and RESISC45 as Mars Crater, Moon Crater, and Earth Stadium respectively. For all datasets, we partition two instance-distinct groups for training and testing such that each group contains half of the available images (as is standard in the literature). In the case of Luna-1, we ensure all craters seen during the navigation sequence are added to the test set before this partitioning."}, {"title": "4.2 Experiments", "content": "We perform two experiments that emulate landmark recognition behavior during TRN, including a sequential, incremental recall experiment (Incremental Recall@1) and an object detection-style description experiment on the navigation frames from Luna-1 (Moon Navigation). Additionally, we perform a traditional Recall@1 (gallery size one) as well as a Luna-1 relocalization experiment (Moon Lost-in-Space). Details of these experiments are provided below. Additional results including model execution times on spacecraft hardware and MARs training curves can be viewed in the supplementary.\nIncremental Recall@1: The embedding database starts empty and test-partition landmarks are randomly selected. Each landmark is augmented by a transform sampled from $T$. Embeddings are generated single-shot from the model and the database is searched for correspondence. Embeddings are stored in the database if no match is found. We compute Recognition Accuracy (RA) as the percentage of correct matches relative to the total number of landmark matches (either correct, incorrect, or missed). Missed matches are landmarks that were added to the database more than once (i.e., duplicate embeddings). The RA formulation is given in Equation 7. To provide multiple observations we repeat each landmark in the test partition twice.\n$RA = (\\frac{Correct Matches}{Correct Matches + Incorrect Matches + Missed Matches}) * 100$ (7)"}, {"title": "Moon Navigation", "content": "Luna-1 navigation frames are iterated sequentially, where each frame comes paired with ground-truth bounding box annotations of visible craters. For each frame, we first perform non-maximum suppression (NMS) to emulate the use of an object detector (akin YOLO [86]). Landmarks are given by cropping the resulting set of bounding boxes which are in turn augmented by a random transform sampled from $T$. Embeddings are generated single-shot by the model and RA performance is measured identically to the Incremental Recall@1 experiment. Models trained on Mars Crater are not considered in this experiment due to the domain shift between Mars and Moon. However, one may expect a level of feature generality on crater landmarks from any environment and we report such results in the supplementary."}, {"title": "Moon Lost-in-Space", "content": "This experiment emulates the kidnapped robot problem in traditional robotics literature. The embedding database is first seeded with all crater landmarks seen during the first orbit of the Luna-1 navigation, where landmarks are detected and augmented identically to the Moon Navigation experiment. Frames from the last orbit are then randomly selected and the RA is reported by matching computed embeddings to those in the database. The database is not updated throughout the experiment outside of the initial seed. Similar to Moon Navigation we only consider models trained on Moon Crater data here, and report a Mars Crater training study in the supplementary. Furthermore, an ablation study over singular transformation types in the family $T$ for this experiment as well as Moon Navigation is given in the supplementary."}, {"title": "4.3 Implementation Details", "content": "To determine the effectiveness of metric learning for robust landmark description, it is imperative to understand two primary conditions for TRN: (i) recognition over time with many similar terrain features encountered sequentially, and (ii) the unique transformation space in remote sensing. Therefore, we frame this study as a measure of modern metric learning invariancy and discriminative properties under these conditions and elect not to compare against fully-fledged Earth-based systems that are unsuitable for onboard spaceflight. Additionally, we seek to understand the influence of MARS on various metric learning losses and the effect of different attention and equivariant setups."}, {"title": "5 Conclusion", "content": "The utility of metric learning as a single-shot landmark description technique for spacecraft TRN was thoroughly explored in this work. We demonstrated that metric learning alone cannot adequately descript fine-grained instances of celestial terrain given multi-view observations and complex transformation spaces. We show that traditional workarounds such as equivariant convolutional layers are in many cases still insufficient. We identify shortcomings with the view-unaware attention mechanism and proposed Multi-view Attention Regularizations (MARs) to regulate attention focus between views. MARs enacts a soft learning constraint that prevents attention collapse, effectively driving the what and where elements of attention together and eases the downstream separability task. We demonstrated the utility of our method through rigorous and comprehensive experimentation, where we showed regular improvements to a wide range of metric learning losses by upwards of 85% on navigation-style tasks. We additionally introduced the Luna-1 dataset to facilitate more active research in TRN landmark recognition, consisting of photo-realistic Moon crater landmarks and paired navigation images using real-world NASA data."}]}