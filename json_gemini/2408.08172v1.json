{"title": "Towards flexible perception with visual memory", "authors": ["Robert Geirhos", "Priyank Jaini", "Austin Stone", "Sourabh Medapati", "Xi Yi", "George Toderici", "Abhijit Ogale", "Jonathon Shlens"], "abstract": "Training a neural network is a monolithic endeavor, akin to carving knowledge into stone: once the process is completed, editing the knowledge in a network is nearly impossible, since all information is distributed across the network's weights. We here explore a simple, compelling alternative by marrying the representational power of deep neural networks with the flexibility of a database. Decomposing the task of image classification into image similarity (from a pre-trained embedding) and search (via fast nearest neighbor retrieval from a knowledge database), we build a simple and flexible visual memory that has the following key capabilities: (1.) The ability to flexibly add data across scales: from individual samples all the way to entire classes and billion-scale data; (2.) The ability to remove data through unlearning and memory pruning; (3.) An interpretable decision-mechanism on which we can intervene to control its behavior. Taken together, these capabilities comprehensively demonstrate the benefits of an explicit visual memory. We hope that it might contribute to a conversation on how knowledge should be represented in deep vision models\u2014beyond carving it in \u201cstone\u201d weights.", "sections": [{"title": "1. Introduction", "content": "In the pretty diagrams on \"Intro to Machine Learning\u201d slides, an ideal ML workflow looks like this: Data collection, preprocessing, choosing a model, training, evaluation, deployment. Happy ending- the model is deployed, the users love it, and one can finally go on that well-deserved vacation and catch up on the latest AGI memes.\nUntil, of course, the enemy of any ideal world sets in: reality. The real world constantly keeps changing, and so do data requirements. New data and datasets become available, and existing ones become deprecated for a variety of reasons, including concerns around fairness, biases or unsafe content. Knowledge changes, and concepts drift [1; 2]: Phones and cars look different today than they did a few years ago, and different from how they will look in the future. When it comes to data, the only constant is change [3-6]. Consequently, from a modeling perspective, in order to keep up with this change one would ideally want to constantly re-train or fine-tune models, which is of course not feasible. In short, as anyone who has ever deployed a model has experienced firsthand, one is constantly battling the symptoms of a single underlying cause: the fact that deep learning models have a static knowledge representation entangled in millions or billions of model parameters. We, among many others working on memory [e.g. 7\u201314], believe that this is not a great way to represent visual knowledge for deep learning. Instead, we argue that we should build models that cleanly separate representation (how things are represented, for instance through feature embeddings) from visual memory (what is known). In short, deep learning models need a flexible visual memory: a way to explicitly utilize and edit knowledge.\nIn this work, we build a simple visual memory for classification and show that it has seven desirable capabilities, including the ability to flexibly add data across scales (from individual samples to classes and even billion-scale data), the ability to remove data from our model's classification process through machine unlearning and memory pruning, and a simple, interpretable decision-mechanism on which"}, {"title": "2. Building a retrieval-based visual memory for classification", "content": "Given a dataset Dtest := {(x1, y1), \u2026\u2026\u2026, xn, yn}, we want to classify each image x \u2208 Dtest. Our classification approach consists of two steps: (i) building a visual memory, and (ii) fast nearest neighbor based inference using the visual memory."}, {"title": "2.1. Building a visual memory", "content": "Our visual memory retrieves (image, label) pairs from an image dataset when a query is made by directly retrieving those images that are considered similar to a test image according to a model. The model is a fixed pre-trained image encoder, meaning that no training takes place when adding information to visual memory. No copies of the dataset are stored in the visual memory. Instead, feature maps are extracted from the model based on a set of images related to the downstream classification task at hand, such as a standard training set. For our experiments, our visual memory comprises of features extracted from a dataset like the ImageNet-1K [37] training set using different encoders like DinoV2 [38] and CLIP [39]. Thus, given a pretrained image encoder, \u03a6, and a dataset of (image, label) pairs Dtrain := (x1, y1), (x2, y2),\uff65\uff65\uff65, (xN, YN), we obtain features z\u2081 := \u0424(x\u012b), \u2200xi \u2208 Dtrain."}, {"title": "2.2. Retrieval-based classification using visual memory", "content": "Given a query image x \u2208 Dtest, we extract its feature map, z = \u03a6(x). We then query VisualMemory to extract k feature vectors, Neighbors(x) := {(x[1], \u0423[1]), (Z[2], \u0423[2] ((, \u00b7\u00b7\u00b7, (Z[k], Y[k]) }, that are closest to the query features \u017e using the cosine distance, which is the default retrieval similarity measure for SSL models like DinoV2 [38]. The extracted neighbors, Neighbors(x), are ordered by distance i.e.\ndist(z, z[i]) \u2264 dist(\u017e, z[j]), Vi \u2264 j.\nWe then assign a weight, w\u2081, to each neighbour (z[i], Y[i]) and aggregate the scores for each neigh- bour with the same label. Finally, we assign that label to the query image with the highest aggregate score. We implemented VisualMemory retrieval based classification using one of the following two approaches:\n1. Fast inference using matrix multiplication on GPUs/TPUs: For smaller datasets like ImageNet, we saved VisualMemory as a matrix of size num_images \u00d7 num_dims. During inference, for an encoded query image of size 1 \u00d7 num_dims, we computed the dot product of this encoded image with every entry in VisualMemory getting a matrix of size num_images \u00d7 1. We then computed the k nearest neighbors using the arg max operation.\n2. Fast and scalable nearest neighbor search: We used ScaNN [40] for accelerating nearest neighbor search at scale. Specifically, we saved the VisualMemory as a database and used ScaNN for fast lookup of nearest neighbors during inference. This method scales easily to billion-scale memory (cf. Section 3.3).\nWe mentioned earlier that we retrieve a set of neighbors, Neighbors(x) and aggregate information across them to make a classification decision. In order to understand how reliable (i.e., accurate) retrieved memory samples are from the first to the 100th neighbor, we systematically analyze neighbor"}, {"title": "3. Capabilities of a visual memory", "content": "Our primary goal is to motivate the concept of a machine visual memory from a variety of different perspectives. To this end, we investigate how such a memory can benefit the following capabilities:\n3.1 Flexible lifelong learning: adding novel OOD classes\n3.2 Flexibly trading off compute and memory\n3.3 Flexibly adding billion-scale data without training\n3.4 Flexible removal of data: machine unlearning\n3.5 Flexible data selection: memory pruning\n3.6 Flexibly increasing dataset granularity\n3.7 Interpretable & attributable decision-making"}, {"title": "3.1. Flexible lifelong learning: adding novel OOD classes (both data and labels)", "content": "Standard classifiers, whether trained end-to-end (supervised models) or with a linear classifier (self- supervised models), are not able to handle new information without re-training. For instance, adding new classes or changing labels in an existing model usually involves either re-training or fine-tuning parts of the model. A retrieval-based visual memory, in contrast, is able to process such information in a natural and flexible way, aligning with the requirements of lifelong learning [44]. We tested this by adding data for 64 new classes, along with their new labels, to the visual memory of a pre-trained DinoV2 ViT-L14 model (in addition to the ImageNet train set, which is in-distribution for the model). We took the new classes from the NINCO dataset [45], a dedicated OOD dataset that is designed to have no overlap with existing ImageNet labels and samples. This requires the model to transfer what it has learned to new, unseen concepts. The new task is therefore harder, as the model has to retrieve images from both in-distribution and OOD classes. The resulting visual memory has 1064 classes (1K from ImageNet and 64 from NINCO). Table 1 shows that with a visual memory it is possible to add new classes such that the in-distribution accuracy is maintained without catastrophic forgetting (the new classes only change ImageNet validation performance by 0.02\u20130.04% depending on the aggregation method), while at the same time reaching very high accuracy on the new OOD classes (approx. 87% top-1) without any training. Figure 11 in the appendix confirms that the samples are indeed OOD for the model, as demonstrated by larger distances to nearest neighbors. This highlights"}, {"title": "3.2. Flexibly trading off compute and memory", "content": "Next, we turn our attention to studying the scaling behaviour of visual memory with increasing memory model size. We hypothesize that bigger models will be able to attain similar performance as smaller models with lesser amount of visual memory. This is because, all else being equal, a bigger model should be a better featurizer that requires fewer examples in memory to represent different concepts. We empirically study the scaling behaviour of visual memory based retrieval systems in Figure 3a using models of different sizes like DinoV2 ViT models of sizes S/14 (21M params), B/14 (86M params), and L/14 (300M params), as well as CLIP ViT models of sizes B/16 and L/14. We plot the top-1 error rate as a function of number of images in visual memory. The plot demonstrates that for each model, the error rate consistently decreases as we increase the visual memory size. Notably, already with a single exemplar per class in memory, ImageNet validation performance is far beyond chance (41% top-1 error for DinoV2 ViT-L14). It also visualizes the possibility of a flexible trade-off between model size and memory size: e.g. for the different DinoV2 models, the S/14, B/14, and L/14 variant achieve similar performance at 1.28M, ~150K, and ~70K memory capacity respectively. In line with [11], this indicates that a smaller model with large memory can match the performance of a larger model with smaller memory."}, {"title": "3.3. Flexibly adding billion-scale data without training", "content": "Billion-scale dataset with pseudo labels. As demonstrated in Section 3.2, performance systematically improves with increased memory size across both small and large models. We here test how far this trend holds beyond relatively small-scale, well-curated settings like ImageNet-1K by scaling visual memory to the billion-scale unlabeled data regime. We study this by obtaining a large-scale dataset from the union of the ImageNet-1K train set and a subset of the JFT-3B dataset [46]. To this end, we treat JFT as an unlabeled dataset by ignoring its original labels and instead obtaining pseudo labels by running them through ViT-22B-224px [47], a highly performant classifier. We excluded images whose labels do not have a correspondence with the ImageNet labels.\nScaling. In Figure 3b, we show the downstream performance of two DinoV2-ViT model variants on ImageNet validation examples as a function of visual memory size. The plot demonstrates that even in the billion-scale data regime, validation error decreases when increasing memory size without any training. The gain from more data is most prominent when having fewer samples in memory"}, {"title": "3.4. Flexible removal of data: machine unlearning", "content": "The world is not static. Thus, in addition to the need to flexibly add novel data, there is often a need to remove the influence of specific training data from a model's decision-making process after it has been trained [3\u20136]. A range of intricate methods are being developed to remove or reduce the influence of certain training samples [56\u201360]\u2014a challenging endeavour if knowledge is embedded in millions or billions of model weights. In contrast, for models with an explicit visual memory, machine unlearning becomes as simple as removing the dataset sample from the visual memory. For instance, after adding the NINCO dataset [45] into visual memory, we can remove any NINCO sample with outstanding performance on all three key unlearning metrics reported by [61]: Efficiency: How fast is the algorithm compared to re-training? (Lightning fast.) Model utility: Do we harm performance on the retain data or orthogonal tasks? (Not at all.) Forgetting quality: How much and how well are the 'forget data' actually unlearned? (Completely and entirely.) Can machine unlearning therefore be solved with a visual memory? If the embedding model is trained on data that needs to be unlearned, machine unlearning remains challenging. If, however, the embedding model is trained on a safe, generalist dataset (e.g., a publicly available image dataset) and data that may need to be considered for unlearning later is simply put into the visual memory, then machine unlearning indeed becomes as simple as deleting a datapoint from the visual memory. This can be particularly helpful for tasks that may require private or confidential data\u2014a model can be trained on publicly available datasets to learn general and information features and the private data can be added to a visual memory on local devices for downstream tasks to preserve privacy."}, {"title": "3.5. Flexible data selection: memory pruning", "content": "The ability to flexibly remove the influence of certain datapoints is not just desirable in the unlearning sense, but also advantageous in the context of dataset pruning, an emerging field that analyzes the quality of individual data points. The goal of dataset pruning is to retain only useful samples, while removing those that have a neutral or harmful effect on model quality. The key challenge is that in standard black-box models, it is entirely unclear whether any given sample is helpful or harmful. The gold standard is leave-one-out-training (for ImageNet, this would consist of training 1.28 million models); current methods seek to approximate this extremely costly approach with various heuristics [62\u201366]. By contrast, the contribution of a data sample to decision-making in a visual memory based system is straightforward. For any given query image x, the neighbor set Neighbors(x) clearly reveals which samples contributed to the decision. Furthermore, this information also highlights whether the samples were helpful (correct label) or harmful (wrong label) for the decision. We, therefore, transfer the concept of dataset pruning to memory, and propose visual memory pruning. To this end, we estimate sample quality by querying the ImageNet training set against a visual memory consisting of the exact same dataset (IN-train, discarding the first neighbor which is identical to the query). This approach requires no more compute than a single forward pass over the training set. We then record the number of times any given neighbor contributed to a wrong decision, resulting in a sample quality estimate. This enables us to exclude low-quality neighbors from the decision-making process by either removing them from the visual memory entirely (\"hard memory pruning\") or by reducing their weight compared to higher-quality neighbors (\u201csoft memory pruning\u201d). Method details can be found in Appendix G. In Table 3, we show that both memory pruning variants improve ImageNet validation accuracy, with soft pruning leading to larger gains than hard pruning. Figure 4 visualizes the decision-making process for a randomly selected sample where estimating sample reliability improves decision quality. Given that observing the outcome of an intervention is many orders of magnitude faster in visual memory models (as opposed to traditional leave-out-training), we are optimistic that the visual memory pruning gains we observed with two simple strategies can be improved further in the future."}, {"title": "3.6. Flexibly increasing dataset granularity", "content": "In contrast to static classification, where a model is trained once without further updates, a visual memory model should be able to flexibly refine its visual understanding as more information becomes available. We test this using DinoV2 ViT-L14 embeddings on the iNaturalist21 dataset [67], a large- scale imbalanced dataset of animal and plant images containing 10,000 species spanning seven taxonomic levels, from coarse (kingdom) to fine-grained (species). In a leave-one-out fashion, we simulate the discovery of a new species by putting 50 exemplars for each of the 9,999 species into memory and then step by step adding more data for the remaining \u201cnewly discovered\" species- starting from zero exemplars all the way to 50 exemplars (see Algorithm 1 for an algorithmic description). In Figure 5 we observe the following: (1.) Already before a single example of the new species is added, it can already be placed in the right part of the taxonomic tree well beyond chance (35.2% accuracy at the genus level compared to ~0% chance accuracy). (2.) Accuracy at the species level improves substantially by adding just a handful of images of the target species (e.g., 5\u201310 images); a regime where training a classifier would typically fail due to data scarcity. (3.) Interestingly, adding more samples of the discovered species not only improves species-level accuracy, but also"}, {"title": "3.7. Interpretable & attributable decision-making", "content": "Attributing model decisions to training data. Unlike a black-box deep learning model, a visual memory system offers a natural way to understand a model's specific predictions by attributing them to training data samples [e.g. 27]. In Figure 6, we visualize misclassified validation set examples from the ImageNet-A dataset [51] using a memory of the ImageNet-1K training set. These randomly selected samples illustrate that many seemingly strange errors (e.g., predicting a type of fence instead of a teddy bear, or a unicycle instead of a bow tie) do in fact appear sensible given the data, raising questions about label quality of ImageNet-A\u2014in a similar vein as label issues identified for ImageNet [54; 55; 68]\u2014rather than about model quality.\nUnderstanding compositionality of representations. A flexible visual memory also provides a path to analyze representations of various models, particularly, how different models represent multiple concepts in an image. We study this for an ImageNet-train visual memory of DinoV2 ViT-L14 and CLIP VIT-L14. We use manually selected query images from outside the ImageNet dataset that have multiple objects from the ImageNet labels. We query the visual memory for nearest neighbors of the query image. Subsequently, we obtain the residual image by subtracting the features of the nearest neighbor from the features of the query image. We, then, obtain the nearest neighbors for the residual image from the visual memory. We plot the results in Figure 14 which shows that DinoV2 ViT-L14 and CLIP VIT-L14 represent concepts in their features in a different manner. The nearest neighbors for DinoV2 are mostly images with a single concept (or object) from the query image. The residual image, subsequently, leads to nearest neighbors dominated by another single object in the query image. In contrast, CLIP often finds neighbors that are a blend of concepts from"}, {"title": "4. Discussion", "content": "Summary. Typical neural networks are trained end-to-end: perfect for static worlds, yet cumbersome to update whenever knowledge changes. This is limiting their potential in real-world settings since the world is constantly evolving. Incorporating a visual memory, in contrast, enables a range of flexible capabilities that embrace change: lifelong learning through incorporating novel knowledge, being able to forget, remove and unlearn obsolete knowledge, flexible data selection through memory pruning, and an interpretable decision-making paradigm on which one can intervene to control its behavior. We systematically explored a simple visual memory that decomposes the task of image classification into two primitives, image similarity (from a pre-trained embedding representation) and search (via fast, scalable nearest neighbor search from a vector database). Our results demonstrate that technical improvements like RankVoting improve kNN accuracies for both DinoV2 and CLIP over the widely used SoftmaxVoting method, the previous state-of-the-art, that is sensitive to two hyperparameters (temperature \u03c4 and number of neighbors k). Our approach also narrows the accuracy gap between a"}, {"title": "Limitations and future work", "content": "First, we only considered the task of image classification across a broad range of datasets. It will be interesting to extend the approach to other visual tasks, such as object detection, image segmentation, instance recognition and to image generation where a visual memory would be desirable, too (since it is prohibitively expensive to re-train large generative models every time data needs to be removed or added). Secondly, our approach relies on a fixed, pre-trained embedding model; strong distribution shifts may require updating the embedding. Self-supervised models are a particularly flexible choice, but it is an open question whether one could train smaller models that excel at their task with the help of a larger memory database. Conceptually, if a model needs to save less information in its weights, it might be possible to reduce the computational footprint of such a model. Furthermore, we sometimes observe a trade-off between flexibility and accuracy."}, {"title": "Outlook", "content": "Deep learning is increasingly becoming a victim of its own success: the more widely it is deployed, the stronger its limitations are felt. While the static nature of end-to-end trained networks can easily be forgotten when focusing on fixed, static academic benchmarks, the real world is anything but static. Data is constantly evolving, leading to the dreaded \u201cmodel drift\" where once-optimal models gradually become less effective [69]. Incorporating an explicit visual memory\u2014however it may be instantiated\u2014appears to be a promising way forward, a natural solution for real-world tasks where flexibility is key. While the specific approach we employ here might well be improved through more complex systems, we hope that the flexible capabilities we demonstrated might inspire and contribute to a conversation on how knowledge ought to be represented in deep vision models."}]}