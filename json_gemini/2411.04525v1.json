{"title": "GenJoin: Conditional Generative Plan-to-Plan Query Optimizer that Learns from Subplan Hints", "authors": ["Pavel Sulimov", "Claude Lehmann", "Kurt Stockinger"], "abstract": "Query optimization has become a research area where classical algorithms are being challenged by machine learning algorithms. At the same time, recent trends in learned query optimizers have shown that it is prudent to take advantage of decades of database research and augment classical query optimizers by shrinking the plan search space through different types of hints (e.g. by specifying the join type, scan type or the order of joins) rather than completely replacing the classical query optimizer with machine learning models. It is especially relevant for cases when classical optimizers cannot fully enumerate all logical and physical plans and, as an alternative, need to rely on less robust approaches like genetic algorithms. However, even symbiotically learned query optimizers are hampered by the need for vast amounts of training data, slow plan generation during inference and unstable results across various workload conditions. In this paper, we present GenJoin - a novel learned query optimizer that considers the query optimization problem as a generative task and is capable of learning from a random set of subplan hints to produce query plans that outperform the classical optimizer. GenJoin is the first learned query optimizer that significantly and consistently outperforms PostgreSQL as well as state-of-the-art methods on two well-known real-world benchmarks across a variety of workloads using rigorous machine learning evaluations.", "sections": [{"title": "1 INTRODUCTION", "content": "Query optimization remains an active area of research for learned query optimizers (LQOs). In recent years, increasingly sophisticated methods have been developed for both cardinality estimation (CE) [12, 13, 17, 24, 31, 42, 43, 46, 49, 52] and join order selection (JOS) [2, 5, 6, 11, 18, 26-28, 41, 44, 45, 47, 48, 50]. CE approaches typically use statistical and machine learning models to approximate multivariate distributions over database table attributes [22]. The resulting cardinality estimates serve as an input to the cost models of query optimizers. At the same time, JOS models are considered to be the \"brain\" of query optimizers, whose outputs are logical and physical query plans [8].\nStarting from Cascades [10], JOS was mainly considered as dynamic programming (DP) [7] task, naturally assuming a join choice as a step in top-down or bottom-up plan construction. With the rise of deep learning and reinforcement learning (RL) [34] as a logical continuation of the DP ideas [23], classical query optimizers started being challenged by LQOs. Such step-by-step plan-building models control the full target plan specifications like the types of joins used, which scans to apply and in what order to join the tables, via\na set of explicit hints for the RDBMS\u00b9. We call these approaches full plan-level hint\u00b2 methods (see second row of Figure 1).\nThe trend of producing complete query plans as output was first questioned by Bao [26]. The idea is to use classical optimizers and empower them by just giving high-level hints like 'enable merge_join' rather than building complete query plans. In such a way, the exact join order choice is made by the built-in classical optimizer, though its search space is constrained through the provided query-level hints\u00b3 (see third row of Figure 1)."}, {"title": null, "content": "Despite the progress made, LQOs have yet to yield significant improvements upon traditional approaches, and consistently out-performing them continues to be an elusive goal [19]. We argue, that the following weaknesses persist in today's LQOs:\n(1) Inefficient and limited reinforcement learning. Typical RL agents can take suboptimal steps and still end up with strong solutions due to the fact that either the game takes many steps to finish, and/or there is a possibility of stepping back. However, for query optimization and, in particular, bottom-up generation of join orders, every step is non-revertable and has a significant impact on further steps with potentially fatal consequences in case of a bad choice.\n(2) Hint-based methods are double-edged swords. When recommending only general hint sets, methods like Bao leave a high degree of freedom to the classical optimizers to choose the order of joins. However, any hint, such as 'disable_hashjoin', impacts every single join in a query, limiting the query plan subspace like in k-d tree search [30] at a coarse level of granularity and thus reducing the chance to find the optimal solution.\n(3) Requirement for vast amounts of (training) data. An important goal of query optimization is to learn the distributions and correlations of various attributes within and across tables in order to estimate the join cardinality. However, due to minute differences in query predicates, LQOs need to sample large amounts of query workloads to reach good generalization capabilities.\n(4) Finding optimal solutions with machine learning is very time-intensive. Certain machine learning algorithms do indeed find better query plans than traditional classical approaches. However, the time to find the solutions is often prohibitive due to the computational overhead for encoding queries or when using a model for inference which often greedily (though with pruning) explores the plan space. Previous approaches often ignored these performance aspects and only focused on execution time as their only metric.\nAs a possible way to mitigate these LQOs' hurdles, we developed GenJoin - a novel, generative plan-to-plan query optimizer that learns from subplan hints limited to join types such as nested loop join, merge join and hash join. GenJoin uses a conditional variational autoencoder as machine learning architecture and suggests the \"golden middle\" between forcing exact query plans and giving a set of general hints (see bottom row of Figure 1). The output of the GenJoin model is a set of two-way-join hints (or subplan hints), e.g., use merge join on tables A and B, i.e. MJ(A, B), hash join on tables C and B, i.e. HJ(C, B), or nested loop join on tables D and A, i.e. NL(D, A). GenJoin does not specify exactly where in the join tree this particular join should be performed (if performed at all) but leaves it up to the classical optimizer to decide. Also, this way, GenJoin gives the classical optimizer the freedom to choose all kinds of plans including bushy ones. More to say, GenJoin enables the classical optimizer to discover parts of the query plan space that it would not explore itself. For instance, the classical optimizer wanted to do HJ(D, A) initially, but GenJoin recommends doing NL(D, A) instead. This way, the classical optimizer might decide not to join (D, A) at all, which will push it to search for alternatives that were initially discarded.\nWhy does GenJoin recommend only the join type and neither the type of scans nor the join order? All the LQOs, including GenJoin, rely on internal RDBMS subquery cardinality estimations based on pre-calculated statistics. We never know if the RDBMS considers, e.g., a sequential scan over an index scan due to high predicate selectivity. Moreover, we also do not know what the RDBMS has already cached and LQOs typically ignore what is currently indexed and how. This applies that recommending just the join type gives the RDBMS the ability to solve those parts where it is more knowledgeable than we are - e.g., when performing a merge join, one table would require sorting, so it might be beneficial to perform an index scan.\nWe choose not to recommend the full join order but only the join type to avoid overfitting and to enable the learned query optimizer to generalize better in a smaller search space.\nThe major contributions of our paper are as follows:\n\u2022 We introduce GenJoin, a generative conditional query optimizer that learns from subplan hints using a conditional variational autoencoder-inspired architecture. GenJoin enhances query plans by pruning the search space of join types of the built-in optimizer to greatly boost their query execution performance.\n\u2022 We show, for the first time, that a learned query optimizer consistently outperforms PostgreSQL as well as state-of-the-art methods on the two well-known real-world benchmarks JOB and STACK across a variety of workloads using rigorous machine learning evaluations.\n\u2022 GenJoin is not only optimized for producing plans with low execution times but also minimizes the required inference time of the machine learning models during query execution.\n\u2022 Inside GenJoin, we introduce a new way of measuring the distance between query plans via unnormalized difference of execution times and the corresponding p-value of a T-test for the means of plan execution time samples, which can be both used for training purposes and for calculating the confidence intervals of the query optimizers' differences."}, {"title": "2 GENJOIN OVERVIEW", "content": "We aim to overcome the limitations outlined in the previous sections while taking advantage of current trends. Creating a plan from scratch (when only having information about the query and the database schema encoding) might be computationally extensive for training full plan-level hint generation models as they explore the whole plan space, i.e. require a huge amount of training data. The same holds true for the complexity of inferencing query-level hints when the number of possible hints is big enough. The basic idea of our approach is to start from some random query plan\u2075 (represented via a set of hints) and train a machine learning model to improve the initial query plan using certain conditions (like the context of the query) - instead of a blind initial search. The research question we address is as follows: \"How can we take a set of hints and transform it such that the resulting query plan executes faster than the initial?\"\nIn order to be confident that the generated query plan results in an execution time that is not only faster than a random query plan, but also faster than the one produced by a built-in optimizer, we might need to apply our model multiple times on its own output. Such an architecture would be prone to overfitting (similar to other LQO method that use model chaining for candidate plan selection and pruning). Instead, our model is designed to directly generate a query plan that outperforms PostgreSQL, which as for now is still state-of-the-art [19]. For that, we need to incorporate additional auxiliary information, revealing the next question: \u201cBy how much is the target plan faster than the one produced by PostgreSQL?\".\nCombining these two questions, we find the answers with GenJoin - a generative hint-to-hint method with the goal of providing a hint set (rather than fully specifying the join order) which leads to a faster query plan compared to a given plan and the plan produced by the baseline method, namely PostgreSQL.\""}, {"title": "2.1 GenJoin Encoding Scheme", "content": "A variety of query encodings has been used in learned query optimizers [18, 26-28, 45, 48], with varying levels of complexity. A common practice is to split the encoding into two sections, the query encoding and the plan encoding. The query encoding contains all information about which tables are involved in a query and what kind of filters are applied on which columns. The plan encoding specifies which tables are joined, what the order of joins is and the types of join used. This type of encoding represents at each step of the join tree which tables have been joined so far and how, and what else is left to join.\nTo illustrate our encoding scheme, we have prepared an example in Figure 3. Our proposed query encoding views a database as a graph (see (1)), where each table is a node and every edge indicates a potential join path between two nodes. In the example, we can see that the five tables A through E have different ways to be joined, e.g., you can join tables A and B directly (indicated by the blue edge), but not B and C. As an example, we have prepared a query (see (2) in Figure 3) joining tables A, B and D with predicates on tables B and D. Assuming the colums B.col2 and D.col1 are uniformly distributed between 0 and 1, they result in a selectivity of 50% and 15% for those two tables, respectively.\nThe query encoding (see (3) in Figure 3) contains one 3-sized cell for every edge in the graph of potential 2-way joins. In our example, that means there are five cells for the 2-way joins AB, BD, BE, CD and DE. Each cell contains one number to indicate, whether this join participates in the query, and two numbers corresponding to the estimated selectivity after all filter predicates are applied to this table. For example, the blue cell for A B contains (1, 1.0, 0.2), since the join is part of the example query (see (2) in Figure 3) resulting in a 1, table A has no filters applied resulting in 1.0 and table B with the filter B.col2 >= 0.85 resulting in an estimated selectivity of 0.2. Please note, that the selectivities of the encoding are extracted estimates (for example, from EXPLAIN calls to PostgreSQL) and not the true selectivity (hence 0.2 rather than the true 0.15 in the example).\nThe plan encoding (see (4) in Figure 3) uses the same idea of 3-sized cells like the query encoding, but encodes each edge in both directions (as there are significant differences in hinting A\u00d7B versus B A, e.g., depending on whether the tables are sorted or not). Unlike the query encoding, the plan encoding solely contains the information about join types and rankings thereof. Each cell contains three entries for ranking the hash join, merge join and nested loop join on this particular join. In our example, lower values mean a higher rank e.g., 1 being first rank. The cell for AB contains the values 1, 9 and 3, implying that GenJoin would force PostgreSQL to use a hash join, if it was to join tables A and B in the order A B.\nOne would assume from the example that PostgreSQL performed a hash join on AB (rank 1) and a hash join on B D (rank 2), since these are the highest ranking join types across all participating joins. However, GenJoin does neither fully specify the order in which the joins are executed nor in which order each pair of tables is to be joined. In our example, PostgreSQL could also decide to instead reverse either join as a nested loop join of B A (rank 7) and a merge join of D B (rank 4)."}, {"title": null, "content": "The idea behind the plan encoding is that GenJoin fully specifies all subplan hints provided to PostgreSQL by the highest ranking value in every cell. Moreover, individual rankings can also be understood as a confidence of the model in a particular subplan hint."}, {"title": "2.2 Model Architecture based on Conditional Variational Autoencoder", "content": "In this section we describe the machine learning model architecture used in the GenJoin method, which is based on the conditional variational autoencoder (CVAE) [32]. cVAEs are an extension of the variational autoencoder (VAE) [16], which itself is a continuation of the autoencoder (AE) [14] idea. AEs are an encoder-decoder architecture where the input is compressed into a vector in a lower-dimensional space using the encoder, which aims to reconstruct the input based on the latent representation using the decoder.\nFor GenJoin, the encoder of a VAE is changed to map the input to a distribution in the latent space, which has some pre-defined distribution (i.e. inputs are not randomly projected into a lower-dimensional space, but rather attempted to be kept in some restricted subspace, normally a Gaussian one). This allows the decoder to sample from this region to produce an output that is similar, but not identical, to the output. Training such a model is possible due to a \"reparametrization trick\" [9]: e.g. if we decide to project our input into a standard normal latent space N(0, 1), it implies that the encoder learns two vectors of parameters \u00b5 and \u03c3, attempting to keep the mean \u00b5 \u2248 0 and the standard deviation \u03c3 \u2248 1. The decoder can randomly sample from N(0, 1), add our \u00b5 and multiply by our standard deviation \u03c3:\nz = \\mu + \\sigma \\epsilon\n(1)\nThe result will still have the distribution N(0, 1). By applying this trick, we have a differentiable Equation 1, which allows the gradient to flow through the neural network since we relegate the random sampling to a noise vector which effectively separates it from the gradient flow.\nNow that the VAE produces a different output, the cVAE further extends the architecture by allowing us to specify what kind of output should be generated through a condition. Let us explain the concept with an illustratory example from computer vision. If a VAE was given an image of a cat, it would produce other cat images. The condition in the cVAE allows you to specify which types of cats to generate, e.g., exclusively orange cats.\nThe main difference between GenJoin and a cVAE, and the reason why we denote our model as cVAE-based, is that the input and output of our model do not contain the same content during the training. We use a set of subplan hints as our input (the plan encoding) and use the query encoding as our condition. Additionally, the condition contains auxiliary information on the expected performance of the generated output (see Section 2.3 for more details). The output of our model is another set of subplan hints.\nThe encoder-decoder architecture is not entirely new in the space of learned query optimizers. For example, [25] presented the idea of optimizing queries in the latent space. Typically, the encoder is used for dimensionality reduction by learning a bijective function where the resulting latent space possesses semantic properties. The difference in GenJoin is that the encoder performs a different task, where the injective projection of the input is positioned in the latent space such that sampling from that region results in a beneficial set of subplan hints."}, {"title": "2.3 Training Data Generation", "content": "Since GenJoin uses subplan hints both for its input and output, we have to generate the training pairs specifically for our model. In order to learn to predict a \"better\" set of subplan hints, the pairs need to exhibit a significant difference in execution time and are constructed such that the output set of hints always results in a faster query execution compared to the input set of hints.\nFor every query in the training set, we randomly generate 200 sets of subplan hints Hi and pass them into PostgreSQL to measure the corresponding execution time ei. It is worth noticing that generating training data in such a way, i.e. treating PostgreSQL as a black-box environment, sending a set of hints and receiving feedback in the form of a query plan, makes us follow the concept of direct RL, which was advised in [19]. This approach exempts us from describing the environment, i.e. knowing the details of the internal PostgreSQL rules, but rather gives the possibility to learn how it reacts to a given ranked set of subplan hints. See Figure 4 for an example with 3 sets of subplan hints H\u2081 through H3 for the query A\u00d7B\u00d7C\u00d7D.\nThen, all sets of subplan hints are paired together in the form of (Hi, Hj), and only those pairs are kept for the training, where the set Hj is significantly faster than the set Hi, i.e. execution time ei > ej. It implies that we want to have metrics that can a) give us some level of confidence in the difference between execution times and b) serve as a distance between any two sets of subplan hints. We conclude that the best choice is a p-value calculation.\nWhy choose a p-value as a proxy for the distance between arbitrary query plans? First of all, since we know from [19] that a true value of arbitrary execution time ei cannot be exactly measured due to cache states and similar database properties, we use the mean estimation \u0113i calculated over a number of executions. Thus, measuring e.g. the unnormalized difference \u0113i \u2013 \u0113j gives a sense of a distance metric, though it can produce values in an unbounded range from query to query. This may lead to learning inefficiencies and training instability for machine learning models. LQOs based on learning-to-rank models [6, 44, 50] utilize relative rankings instead, solving the machine learning issue mentioned above, though they lose the ability to measure how similar plans are. One option to force the values of the unnormalized difference to be inter-query comparable is Studentization [15] - a division of a first-degree statistic derived from a sample by a sample-based estimate of a standard deviation. This way we get rid of the units and make a scale-free metric. For the case of difference of means, such a normalization factor could be a pooled standard deviation sp, i.e. a compound of standard deviations sei and sej with corresponding sample sizes ni and nj:\nsp = \\sqrt{\\frac{S_{ei}^2}{n_i} + \\frac{S_{ej}^2}{n_j}}\n(2)\nThe series of mathematical manipulations described above end up with nothing else but an empirical T-test [33] statistic for the means of the two independent query plan execution time samples. To achieve the property of distance significance, we compute the p-value associated with the created empirical T-test statistic:\np-value(e_i-e_j) = \\frac{e_i - e_j}{s_p}\n(3)\nTo ensure having only confident differences, we keep only pairs with p-value \u2264 0.025, i.e. confidence of 5% for the one-tailed hy-pothesis. The p-value serves as a bounded metric for comparing the distance between queries. Being outlier-robust, the p-value enables machine learning models to learn from timeout queries.\nHow many samples should we choose for the p-value measurements? The p-value can only be measured between the distributions of query execution times, i.e. we need to collect \u2265 2 query samples from the RDBMS. Since we do not have a predefined minimal detectable effect nor a desired statistical power, we choose the number of query samples arbitrarily, aiming to have a stable variance with the minimal executions-per-query required. Experimentally studying samples from standard normal distributions of different sizes and applying the elbow rule [36], we conclude that starting from sample_size=3, the change in variance is no longer significant.\nThis way, for each hint set pair we compute the p-value of the T-test between the input and output execution times. We do the same between the output execution time eout and the execution time of the baseline method (i.e., PostgreSQL) epg. From our previous article [19] we know that starting with the 3rd consecutive execution, we can trust the runtimes as an unbiased estimation of the true execution time. As such, our empirical distribution of execution times is made up of the 3rd, 4th and 5th query execution.\nHow can the unnormalized difference be used for comparing LQOs? To make sure that one LQO has an advantage over the other in terms of execution time on a fixed workload of queries, we can utilize the unnormalized difference introduced above, using the pooled standard deviation sp from Equation 2 for computing the confidence intervals aka error bars. One may question, why using the approach for a single query difference can be extrapolated from a set of queries in a workload. We argue that as long as for the two normal distributions [21] X and Y, the following is true:\n\u03a7 ~ N(\u03bc\u03c7, \u03c3\u03c7), \u03a5 ~ N(\u03bc\u03b3, \u03c3\u2084) \u21d2\nZ = X + Y ~ N(\u03bc\u03c7 + \u03bc\u03b3, \u03c3\u03c7 + \u03c3\u2084)\nand our execution time samples per query are assumed to be normally distributed, the methodology described in Section 2.3 above is generalizable for the sum of differences."}, {"title": "2.4 Prediction of Subplan Hints", "content": "In the previous section, we outlined how the training data is generated and especially the creation process for the pairs of input subplan hints HIN and output subplan hints HOUT. For the prediction, we also choose a randomly generated initial set of subplan hints as HIN while our model predicts the subplan hints HOUT (as shown after step 7 in Figure 2), which are then attached to the query and executed.\nThe attentive reader may have noticed that this leaves the question what to assign to the p-values in the condition, namely for the confidence to surpass PostgreSQL and the confidence to surpass the input plan. Both of these values are set to zero, which corresponds to the theoretically best p-values under the hypothesis that the generated plan HOUT performs better than both PostgreSQL and and the input plan HIN.\nHowever, these p-values only produce a plan that is potentially better than the one initially given, contrary to the general aim of query optimization to find the best possible plan. Since the suggested approach considers the output HOUT being better than input HIN, we can run the prediction in a loop - a so-called \"chain-of-subplan-hints\" - assuming that every next output is faster than the previous input (see Section 3.5 for the results of this ablation study)."}, {"title": "3 EXPERIMENTS AND RESULTS", "content": "In this section, we provide our experimental evaluation of GenJoin on the Join Order Benchmark (JOB) and STACK. We start by describing the experimental setup, present our results of the experiments and finish with a number of ablation studies."}, {"title": "3.1 General Setup", "content": "3.1.1 Software and Hardware. Planning and execution times are measured using EXPLAIN ANALYZE. The LQOs' inference time is measured as the wall time for e.g., preprocessing, encoding or inferencing. Measurements are taken in a hot cache setting by executing the same query three times and taking the last query execution.\nPostgreSQL is configured as in [19], with AUTOVACUUM disabled to keep the sampled statistics consistent across the experiment. An ANALYZE call was run after setting up each workload. The experiments were run on machines with 64 GB of RAM, 16 CPU cores and Tesla T4 GPUs using a Docker environment.\nAn important remark about the version of PostgreSQL is that a large number of publications are based on PostgreSQL versions 12 [6, 19, 26, 44, 45] and 13 [2, 50]. All our experiments were conducted using PostgreSQL version 16 to reflect the incremental changes added over time, further increasing the difficulty to outperform the PostgreSQL baseline.\n3.1.2 Query Workload. For our experiments, we evaluate all methods on two established query optimization workloads, namely the Join Order Benchmark (JOB) [20] and the Stackexchange-based STACK [26]. These two benchmarks based on real-world databases give good insights into the overall performance of various methods, including many queries that are hard to optimize [20]. JOB has been the primary benchmark for query optimization in recent years, with STACK in the runner up position since its release in 2021. Both benchmarks show skewness in the data and violate the uniformity assumption, which is an important dataset property required for a challenging comparison of LQOs.\nJoin Order Benchmark (JOB). JOB is comprised of queries around a snapshot from May 2013 of the popular Internet Movie Database (IMDB). It contains 21 tables stored in a relational database. The queries have on average of 8 joins per query with a maximum of 16 joins. In total there are 113 queries that come from 33 base queries (or templates) that each have between two and six variations, where either filter predicates are changed or entirely new attributes are being filtered on. These predicates have a significant impact on the selectivity of the tables involved. Hence, the optimal physical plan might differ from variant to variant. These differences in execution time can span multiple orders of magnitudes. However, well performing plans often share partial subplans with other variants, creating a potential for data leakage.\nSTACK. Introduced alongside the Bao method [26], the STACK workload and dataset is based on data from the StackExchange websites. STACK features 10 tables and over 6,000 queries from 16 base queries (or templates). The queries follow a similar variation style as in JOB, meaning there exist many different variations in the filter predicates for every base query available. As per [19], we have downsampled the numbers of queries again to keep the comparison between methods at a similar statistical power. For this reason, we use 112 queries with 7 variants for each of the 16 base queries.\n3.1.3 Training Data Generation. For our workloads with 112 and 113 queries and 200 randomly generated hint sets per query, we can potentially generate up to 4.5 million pairs7. Due to the limitation of only selecting them in order of execution time and with significant differences, the resulting number of usable training pairs is on the order of about 1 million. We subsample the actual amount of pairs and evaluate the ratio of pairs for training using the Optuna hyperparameter optimizer. For JOB we use 30% of all pairs during training, for STACK it is 45%. These ratios serve as a surrogate for classical early stopping mechanisms because our training data volume regulation is aimed to reduce the generalization error [3]."}, {"title": "3.1.4 Dataset Split", "content": "We continue using the different types of splits described in [19], namely the Random, Leave One Out and Base Query split types. These dataset splits take advantage of the structure of the query workloads, where queries can be clustered together as having different filter predicates, but an overall shared structure. These shared structures are called base queries, with the different variations of the filter predicates being called base query variants.\nFigure 5 gives a summary for how the queries 1a, 1b and 1c belonging to the same base query 1 would be assigned to the train and test set. In the random split, queries are randomly assigned to the train and test set. In the leave one out split, all variants of a base query except one are put into the train set. Finally, the base query split assigns all base query variants either to the train or test set, respectively. Hence, queries 1a, 1b and 1c end up in the train set together, while queries 2a and 2b are collectively put into the test set. While the base query split aims to reduce data leakage across shared query structures, the leave one out split forces a minimum level of shared information between the training and test sets.\nThe various splits emphasize different characteristics of the query workloads and give strong overall insights into the models' performances. We have made one adjustment compared to [19]: For every type of split, we distribute the data according to the split function into three separate folds, in the style of k-fold cross validation. The three splits are then called A, B and C, e.g., \"Base Query Split A\", \"Base Query Split B\" and \"Base Query Split C\", where the test sets together form partitions of all queries. Since the test sets of splits A, B and C form a congruent set of all queries, we list the performance of a model on that type of split as the combination of all three test sets. This allows us to make a more accurate, direct comparison across split types.\nThere are two special cases that need to be handled: First, if a group of the same base query has a number of variants that is not divisible by three, the remainder is randomly assigned between the three folds. Second, if there are fewer than three query variants, some variants will be assigned to multiple splits. In the latter case, the evaluation takes the worse of the two query's measurements.\nFor a number of ablation studies, we also use the slow split, first introduced by [45] for JOB as JOB-Slow. This takes the slowest N queries (for JOB-Slow in Balsa N = 19) into the test set and maximizes the absolute amount of time that can be optimized in query executions. The extension of the slow split for STACK follows the same notion and includes the 19 slowest queries in the test set."}, {"title": "3.2 Comparison against Current State-of-the-Art Learned Query Optimizers", "content": "In this section, we compare the performance of GenJoin with two state-of-the-art LQO methods - whose source code is available, run with PostgreSQL Version 16 and the results are reproducible - namely HybridQO [47", "47": ".", "27": "Balsa [45", "26": "and LEON [6", "19": ".", "41": "improve upon Bao, AutoSteer adds a dynamic exploration of tuning knobs in addition to a much larger number thereof, while FastGres primarily uses a different type of model with the same number of knobs as Bao. Therefore, we have chosen AutoSteer for our evaluation, which has a more expressive optimization space over FastGres and Bao.\nWe have also considered including COOOL [44"}, {"19": "."}]}