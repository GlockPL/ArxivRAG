{"title": "Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories", "authors": ["Gerd Kortemeyer", "Marina Babayeva", "Giulia Polverini", "Bor Gregorcic", "Ralf Widenhorn"], "abstract": "We investigate the multilingual and multimodal performance of a large language model-based artificial intelligence (AI) system, GPT-40, on a diverse set of physics concept inventories spanning multiple languages and subject areas. The inventories taken from the PhysPort website cover the classical physics topics of mechanics, electromagnetism, optics, and thermodynamics as well as relativity, quantum mechanics, astronomy, mathematics, and laboratory skills. Unlike previous text-only studies, we uploaded the inventories as images mirroring what a student would see on paper, assessing the system's multimodal functionality. The AI is prompted in English and autonomously chooses the language of its response \u2013 either remaining in the nominal language of the test, switching entirely to English, or mixing languages revealing adaptive behavior dependent on linguistic complexity and data availability. Our results indicate some variation in performance across subject areas, with laboratory skills standing out as the area of poorest performance. Furthermore, the AI's performance on questions that require visual interpretation of images is worse than on purely text-based questions. Questions that are difficult for the AI tend to be that way invariably of the inventory language. We also find large variations in performance across languages, with some appearing to benefit substantially from language switching, a phenomenon similar to code-switching of human speakers. Overall, comparing the obtained AI results to the existing literature, we find that the AI system outperforms average undergraduate students post-instruction in all subject areas but laboratory skills.", "sections": [{"title": "I. INTRODUCTION", "content": ""}, {"title": "A. Generative artificial intelligence in physics education", "content": "The public availability of Large Language Models (LLMs), like those built on the architecture introduced by Vaswani et al. [1], has unlocked new possibilities across various domains, including education [2, 3]. Since the release of ChatGPT in the fall of 2022 [4], LLMs have surged in popularity, with scholars showcasing their remarkable capabilities. Beyond the wave of enthusiasm generated by human-like responses that have been shown to pass the Turing Test [5] with a majority of human test subjects [6], the GPT series demonstrated proficiency in academic fields such as physics [7]. Both the initial version and later iterations, particularly GPT-4 [8], have achieved impressive results, such as passing standardized exams, excelling in introductory courses and even coming close to passing entire degrees [9-13].\nThe technology is increasingly being embraced in physics education [14], offering promising applications for both teaching and learning physics. LLMs have proven valuable for teachers, helping in the creation of tailored materials and tasks, student assessments, and personalized feedback [15-17], as well as for their training [18]. For students, these models may represent ever-available, patient, and knowledgeable resources [19, 20]. However, these opportunities come with significant risks, particularly the potential for users to overly trust AI when assessing its scientific accuracy [21, 22].\nEarly publicly available generative AI systems have been limited to processing and generating text-based content only. Thus, earlier studies of AI's performance on physics tasks, such as concept inventories, were restricted to text-based materials or textual descriptions of visual elements [9, 23]. More recently, multimodal systems, which can also input and output auditory and visual data, have broadened the scope of such studies (e.g., [24]).\nIn this study, we confront GPT-40 [25], a popu-"}, {"title": "B. Concept inventories in physics", "content": "Examinations in physics generally involve symbolic and numerical calculations, whereas concept inventories are typically different, focusing primarily on conceptual understanding. Scores on these different types of assessment do not necessarily correlate [32]. On the surface, this would work in AI's favor, as AI systems have been notoriously \"bad at math,\" which hampered their performance on traditional physics exam questions [9]. However, early investigations of AI's performance on conceptual tasks suggest that these, too, can present several challenges [10, 33].\nConcept inventories have played and continue to play an important role in physics education research [34], and some of the most influential studies have been based on their outcomes, most notably with respect to active engagement [35]. Unlike traditional assessments that focus on individual learners, concept inventories are primarily designed to evaluate instructional methods, oftentimes with a focus on learning gains rather than the absolute scores. As we embark on assessing AI on the base of absolute scores, we deviate from this practice.\nArguably, while many concept inventory questions reflect students' understanding of core ideas [36], they do not necessarily capture evidence of scientific practices or crosscutting concepts [37] (even though this distinction is debatable [38]). It is thus important to emphasize that our study will assess the equivalent of AI's abilities and skills in physics, but not evaluate if it has the qualities of a physicist."}, {"title": "C. The language problem", "content": "Despite all the power and possibilities of using LLMs in education, there are also many limitations. OpenAI's research on the GPT-4's performance with different languages, for example, shows inconsistent results across languages [39]. LLMs learn patterns, make predictions, and perform tasks by leveraging large datasets, deep learning algorithms, and reinforcement learning from human feedback [40]. However, due to the disparities in the prevalence, quantity, and quality of information available across languages, there exists a disparity in the resources available for LLM training, which can have an impact on the quality of the outcomes. Nicholas and Bhatia highlight that although LLMs are designed to mitigate the issue of underrepresentation of certain languages in learning data, as of 2023, early LLMs were still predominantly trained on English materials. Although this may be changing as governments and companies attempt to strengthen AI capabilities in their countries (e.g., models like DeepSeek [41] or Qwen [42] developed by companies in China, or the Swiss AI Initiative [43]), it is likely that LLMs remain biased toward the needs of major economies with the financial resources to train AI systems. Thus, this can contribute to the transfer of cultural values and assumptions from English, or languages spoken in other major economies, to languages that lack infrastructure for training AI models. The authors stress the necessity to address this issue by companies, researchers, and governments across the globe [44]. Similar issues are also discussed in the analysis conducted by \"Cohere for AI\" company in their report titled \"The AI Language Gap\" [45]. Feng et al. also acknowledge the disparity in LLM performance across different languages, resulting in a gap of approximately 20% between high-resource and low-resource languages [46]. Particularly relevant for our study is a recent finding that a Chinese-trained model performed better on the FCI when prompted in Chinese rather than English [47].\nUnderstanding and using physics-specific language is essential for physics literacy and learning. To address both the linguistic disparities of LLMs and the specialized nature of physics language, it is important to examine how LLMs perform across diverse languages in the context of physics education. While current research highlights the general challenges of multilingual use of LLMs, the intersection of language and subject-specific terminology, such as physics, is understudied and lacks a clear understanding of the current situation. This underscores the importance of evaluating LLMs not only in terms of language proficiency but also in their ability to accurately convey domain-specific knowledge.\nHuman speakers frequently overcome language disparities by engaging in code-switching [48, 49], switching languages mid-dialogue or even mid-sentence. Within physics education, much of the research on code-switching takes place in bilingual classrooms [50, 51], where students have a native language and a potentially different language of instruction; students tend to use their highest proficiency language when trying to understand what is happening conceptually [51]."}, {"title": "D. Research questions", "content": "The overarching research questions are:\n\u2022 How does a state-of-the-art (2024/25) multimodal AI system perform across different physics concepts?\n\u2022 How does its performance compare to published post-instruction average results at the undergraduate level?\n\u2022 How does language influence the performance of the AI system?\n\u2022 How does the presence or necessity of images influence the performance of the AI system?"}, {"title": "II. DATA SET", "content": "As data set, we used concept tests published in PhysPort [52], which contains a comprehensive collection of research-based inventories in multiple translations [53]. We included tests that had at least a \"bronze star\" classification assigned on PhysPort, meaning that they had undergone significant research validation, and for which we could obtain an answer key."}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. Data preparation", "content": "The questions from the concept tests were captured using screenshots like the ones shown in Fig. 1. If a question had multiple parts referring to the same scenario or each other, these were combined in one image; at times, this required manual photo-editing to close page breaks. Questions that assessed confidence levels (\"How sure are you of this answer?\") or those with free-response answers were skipped. The tests in Tables III-VI resulted in 3,662 images.\nThe solution keys were transcribed from PhysPort, where we transformed the various option values such as B), 2., b., \u03b2), and non-Arabic, non-Latin, non-Greek characters into lower-case Latin characters. For tests that gave partial credit for certain incorrect answers if previous answers were incorrect (\"follow-up mistakes\"), we simplified the evaluation by only giving credit for correct answers. Also for tests that assigned different point values for different questions, we only considered correctness. In addition, for each question, we coded if it was text-only, or if it included an image, graph, or scenario illustration."}, {"title": "B. AI processing", "content": "We have been using GPT-40 [25] Version 2024-08-06 via Microsoft Azure AI Services [124] at ETH Zurich. The university's contract includes provisions that any data submitted will not be used for training purposes; this provision is crucial to avoid compromising the confidentiality and validity of the concept tests.\nThe screenshots were submitted via the deployment's API in base64-encoding as shown in Fig. 2. As role, we used the text shown in Fig. 3, and as prompt the text shown in Fig. 4. To facilitate further processing and evaluation of the answers, we demanded the structured JSON output schema shown in Fig. 5. We included the \"explanation\" field in the output structure to capture the AI system's reasoning steps, also referred to as Chain-of-Thought (CoT) [10]. This allowed us to glean some key insights into the system's language behavior."}, {"title": "C. Analysis methods", "content": "The answer choices found in the JSON structures were normed to the same lower-case Latin characters as the solution keys. In cases where the AI did not provide a valid answer (e.g., where it claimed that there was no correct answer specified), the answer was counted as incorrect. Some concept inventories had images or scenarios labeled with Roman characters and answer choices such as \"A) I, B) II, C) III, D) IV, E) none of the above;\" in this case, if the AI picked \"IV,\" this was manually converted to \"d.\u201d Another source of possible error were numbered scenarios within numbered multipart problems; in this case, the AI at times ignored the problem numbers provided in the prompt and instead used the scenario numbers; this was fairly obvious during the evaluation, but had to be fixed manually.\nWe counted each answer in an inventory three answers per question as either correct or incorrect, and considered the percentage of correct answers as performance measures. For each answer, we coded the language of both the test description and the answer explanation, primarily using langdetect [125] and manual determination in some cases. Responses were categorized as fully in the language of the test, fully in English, or mixed. The most common mixed scenario occurred when the problem description was still in the language of the test, but the explanation was in English. However, there were also rare cases where the language switched mid-stream for either of those."}, {"title": "IV. RESULTS", "content": ""}, {"title": "A. Scores by inventory and language", "content": "Table VII shows the scores on each of the inventories in each available nominal language (as \u201cnominal,\" we denote the language in which the inventory was presented). One immediate observation is the uneven coverage of assessments across languages. Many inventories are only available in a handful of languages, making broad comparisons challenging. However, when data is present, certain patterns do emerge. Some assessments, such as FCI, have results for numerous languages, allowing a more comprehensive look at language-to-language differences.\nIn the FCI, performance ranges from as low as 20% in Punjabi and 22% in Tamil to as high as 74% in Portuguese and Polish, suggesting that the Al's proficiency can vary significantly depending on the language used. Since the FCI uses five answer options, a 20% performance is equivalent to randomly picking an answer. An in-depth analysis of the FCI answers for Punjabi revealed that GPT-40 started hallucinating, generating output about uniform magnetic fields and charged particles. In Tamil, the explanations and conclusions were sometimes correct, but then GPT-40 picked the wrong answer letter.\nOther assessments, like TUV, ADT, and FMCE, show relatively consistent scores in the 60-80% range wherever data is available.\nA second key point is that some inventories appear to have stable performance across multiple languages, while others do not. For example, QMCS shows high and relatively stable performance (78-86%) in languages for which data is available, indicating that the AI handles that conceptual domain and its translations fairly well. In contrast, MBT scores fluctuate more dramatically (e.g., 27% in Persian, 53% in Finnish, and 44% in Italian and Hungarian), indicating that performance may depend not just on the conceptual domain but also on the subtleties of the language and phrasing used.\nOne such outlier is the TUG-K2.6, where performance differs widely across the few languages tested, from 48% in Arabic to 67% in Finnish, with little intermediate data. Similarly, TUG-K3.0-4.0 shows a scattered pattern: 44%"}, {"title": "B. Score comparison by subject area", "content": "Table VIII shows a comparison of average post-test student scores found in the literature for undergraduate-level courses (see column \"%Perc\" in Tables III-VI) and average AI scores. We calculate the AI scores based on the scores listed in Table VII, taking the average across all languages. With the exception of laboratory skills (LAB), the AI outperforms average undergraduate students in every subject category. Note that the average AI score would usually be even higher, if we only considered tests in English."}, {"title": "C. Performance by subject area and image usage", "content": "Table IX shows the performance by subject area and usage of images, which allows insight into the multimodal capabilities of the AI system. Here, we calculate the performance based on all questions that are in inventories that address the subject area and, for the three columns on the right, fulfill the criteria regarding the presence and use of images. For each question, we coded if it contained or referred to an image, i.e. a visual representation (sketch, graph, diagram, etc.) and if interpreting the image was required for correctly solving the task (required image), or if the image was redundant and the task could be solved by only reading the text of the problem (unneeded image).\nOverall, the AI performs best in Astronomy (AST), followed by thermodynamics (THERM), relativity (RELA), and reasoning (REAS). The inventories on Astronomy tend to focus on knowledge, while THERM, RELA, and REAS inventories tend to use exact language. The AI system is particularly weak in laboratory skills (LAB), which include strategies for data collection and analysis, Optics (OPT), mechanics (MECH), and electricity and magnetism (both circuits (EM-C) and fields (EM-F)).\nThe performance on questions with redundant images is similar to or better than text-only questions for all subject areas, except for LAB and RELA inventories.\nHowever, in all subject areas, the AI performs worse on questions that require image interpretation than on text-only questions. RELA, OPT, MECH, MATH, and AST exhibit especially large gaps. Furthermore, while QUANT as a whole was not among the worst performing subject areas on image-based questions, QMVI, which is entirely image-based and consists predominantly of required-image problem types, was the worst-performing of all surveys in English. In line with previous research on the topic, this suggests that visual interpretation is one of the major weaknesses of the studied AI system."}, {"title": "D. Scores by question and language", "content": "Table X can be used to assess if the AI struggles with the same questions across different languages. One indicator for \"struggle\" is if the answers for the three independent attempts agree on the correct choice: how consistent is the AI system in its correct answer choices? The by far largest number of inventories was available in English (all except for the TUG-K2.6) and we referenced the performance to questions given in English. There were 955 individual questions across the inventories for which the AI response was correct for all three responses in English. For 128 questions the AI had two out of the three responses correct. There were 131 questions with one of three correct responses and 284 with all three answers incorrect.\nTable X shows that questions that are difficult in English tend to remain difficult in the other languages and vice versa (with the earlier mentioned exception of Punjabi and Tamil). For example, among the 102 questions for which all three English responses were incorrect, only 18% of responses in the second most common language, Spanish (ES), were correct. This increases to 32% for the 42 questions with 1/3 correct in English, 56% for the 45 questions with 2/3 correct in English, and 88% for the 227 questions where all three English responses were correct. Similar trends can be seen for the other languages and the percentages for all non-English language questions went from 18% (0/3 in EN), to 33% (1/3 in EN), to 48% (2/3 in EN), and 82% (3/3 in EN).\nAs discussed earlier, part of this could have been due to lower performances for questions that contain images that pose a challenge for the AI invariably of the language. However, the same trend holds if you look at the \"text only\" questions. While the overall performance increased (see Table IX), the correlation between questions being answered correctly in English and other languages remains. Correct responses for non-English questions went from 16% (0/3 in EN), to 52% (1/3 in EN), to 65% (2/3 in EN), and 84% (3/3 in \u0395\u039d).\nWell-designed assessment instruments for teaching have multiple-choice distractors that probe particular student misconceptions. Given that we drew our questions from the PhysPort library with many instruments having been designed carefully, we would expect that for our data set students would gravitate to particular incorrect answers. As such, it is interesting to consider if the AI was gravitating toward specific incorrect answers. Table XI shows the questions that had two or three incorrect answers for the set of the three AI responses. It then shows if the selected multiple-choice items were the same or different. The percentages for the full data set as well as the data set split into English and non-English responses show that the AI frequently picked the same incorrect multiple-choice items. While not all of the assessments analyzed in this study had five possible choices, the majority did, and we can use this as a reference. If the probability of picking a particular incorrect answer was random we would get a 75% probability of having different answers for two incorrect answers and a 25% chance of having the same answer. For our data this was flipped, 66% of all, 72% of English, and 64% of non-English questions had the same two incorrect choices. The same can be seen for cases where all three answers were incorrect. If the likelihood of a particular incorrect response was random, in 6.25% of cases all three would be the same, in 37.5% of cases all three would be different, and in 56.25% of cases two responses would be the same. As shown in Table XI, in reality, the AI responses are far from random and gravitate toward specific incorrect answers (e.g. 53%, corresponding to 618 questions, instead of 6.25% had all three incorrect responses being the same)."}, {"title": "E. Scores by language and language switching", "content": "Out of 36 tests that were translated, English was the best performing language in 27 (75%) of cases. Table XII shows the relative performance by language, compared to English. Overall performance does depend on the language of the questions (nominal language). It is notable that for Catalan, Finnish, French, Italian, Norwegian, and Polish, the performance is comparable, with a number of languages close behind. On the other hand, for nominal languages such as Farsi, Hebrew, Hindi, and most notably Punjabi, the AI exhibits much lower performance.\nIt is unclear in which language the AI system internally processes the problems, however, the language of the output may be a good indicator, which exhibits code-switching. As seen in Table XIV, for some languages, the AI performs better when the output is either entirely in English or mixed with English. Languages like Bengali, Persian, and Punjabi have a marked increase in accuracy when the AI switches to English, implying difficulties in processing the original language of the inventories. Meanwhile, languages like English, Spanish, and French demonstrate more balanced performance, with nominal language usage still resulting in high correctness scores. If the performance of the AI system for a particular question in English is used as a measure of difficulty, Table XIII indicates that the AI stays in the nominal language more often for easier questions. For all non-English questions combined, it does so for 24% of the questions that are deemed easier in English (3/3 correct answers in English). While not a strong signal, it is markedly more than 15-17% for the questions where at least one response in English was incorrect.\nClustering emerges if we consider how often different languages remain in their nominal language versus switching to English or mixing languages; Fig. 7 shows a heat map of the cosine-similarity between the tuples (%nominal language, %switching to English, %mixed language) for each language. Spanish and Portuguese form a cluster that in the majority of cases stays within the nominal languages; the two languages are also part of a larger cluster of commonly spoken European languages, namely German, Italian, Dutch, and French. The next cluster includes several Scandinavian and Eastern European languages. Finally, a large cluster of mostly Asian and Arabic languages emerges, which with the notable exception of Polish and Icelandic uses non-Latin script.\nThus, from a linguistic standpoint, we can group languages based on their dependence on language switching: those that remain robust in their native form, those that need English to boost correctness, and those that benefit from mixing strategies. This language-based clustering mirrors the subject-area clustering by revealing underlying differences in how the AI system processes and understands content under different linguistic and visual conditions."}, {"title": "V. DISCUSSION", "content": "The results presented here highlight several noteworthy patterns in the AI's performance across different physics inventories, languages, and question formats. Unlike many previous studies where inventories were provided as text-only materials, this study supplied the AI with a screenshot of the question the way a student would see it. This richer, more realistic presentation included the actual layout, images, and language-specific formatting, effectively increasing the complexity of the input. The Al's decision-making process whether to remain in the nominal language, switch entirely to English, or produce a mixed-language response was an emergent behavior rather than an externally imposed constraint. The prompt itself was given in English, but the question content was often in another language, with images embedded in the screenshot. Thus, the AI's language choices and solution correctness provide insights into its internal reasoning and capability to handle diverse formats.\nAcross the majority of inventories and subject areas (with laboratory skills being the exception), the AI achieves higher scores and performs better than the average undergraduate student post-instruction, based on published results. It needs to be emphasized that the instruments are research-based and thus carefully designed with regard to psychometric properties, which is reflected in their score distributions:\n\u2022 The test items aim for medium difficulty, resulting in average scores around 50%.\n\u2022 The test items are designed for high discrimination, resulting in broad score distributions.\nThus, the finding that the AI performs better than the average published results, in broad strokes, means that it performs better than 50% on these instruments. It also means that there will almost always be students who outperform the AI. Whenever a study included results for graduate-student or post-doctoral populations, these clearly outperformed the AI.\nLooking at the presence and function of visual representations in questions, we can see a clear pattern. The AI performed better on text-only questions and questions with redundant visual representations, compared to questions where visual interpretation of images was necessary for solving the task correctly. This was true for each individual subject area, as well as across the entire dataset. The average difference in performance between required image and text-only questions was 33 percentage points, with some subject areas exhibiting much higher gaps (see Table IX). This corroborates previous findings on the topic, which suggest that the Al's vision abilities present a major weakness and hamper its ability to engage with physics tasks that require the interpretation in graphical or picture formats commonly found in physics [24, 31]. For at least the present time, instructors who want to discourage students from using AI to do their homework, including required information in some kind of graphical or picture (or even video) format is good advice.\nOn the language side, the results show a complex interplay between linguistic capability and the AI's self-directed switching behavior. With the exception of Spanish and Portuguese, the AI switches away from the nominal language in the majority of cases. This outcome likely points to limitations in the AI's mastery of certain languages, where staying in the original language of the prompt results in lower accuracy. Conversely, in languages such as German, French, Spanish, and Portuguese, the AI performs better on questions where it stays in the nominal language, which may have at least two possible explanations:\n\u2022 The AI is more adept at these languages or that their training data better captures the relevant scientific vocabulary and discourse, or\n\u2022 The AI switches languages for the problems where it already \"struggles,\" and then performs worse on those than on the ones where straightforward inferences can be found in the nominal language.\nThe fact that the AI autonomously decided to switch or mix languages further suggests that it was \"aware,\" at least statistically, of its varying strengths and weaknesses across linguistic contexts.\nOur results show that questions that are difficult in English are likely difficult in other languages as well. Either the AI switched to English in its reasoning or, when staying in the nominal language, as frequently seen in Spanish, similar questions were difficult (or easy) for the AI even though the training data sets in the different languages were not the same. Like students, the AI gravitated toward particular incorrect solutions. How the level of difficulty for a specific question type is different for students versus AI is intriguing. When going astray, do students and AI gravitate toward the same incorrect answers? Are the preferred incorrect AI selections a reflection of issues in the training data or just more challenging for the logic built into GPT-40? While this study does not answer these questions, it provides a stepping stone for future research. Our study indicates we can identify questions that are consistently challenging for AI across languages. Designers and instructors could specifically look at assessment designs that are challenging for AIs. The most obvious is the inclusion of images but this work clearly shows text-based questions can also pose challenges across languages for a multimodal AI system.\nOverall, the study's methodology - providing authentic screenshots rather than merely text and having the AI itself choose its linguistic mode - brings us closer to understanding how such systems would behave in real-world educational settings. We see that certain subject matters are intrinsically more challenging, that language understanding varies widely, and that the AI's own adaptive language strategies can partially compensate for these differences. Nonetheless, large gaps and outliers remain, pointing to areas where improvements in training data, prompt design, or model architecture are needed. Future work should focus on exploring how to guide the system's language-switching decisions more effectively, refining the visual understanding components, and ensuring more balanced coverage across different languages and conceptual domains."}, {"title": "VI. LIMITATIONS", "content": "This study is decidedly exploratory and empirical. The preparation of question images was done manually, and some manual cleanup of the data was required. Given more than 3,600 images and the random oddities occurring in over 14,000 solutions generated by a probabilistic system, clerical errors cannot be excluded. Additionally, each screenshot of test items was iterated only three times: given the stochastic nature of LLM outputs, this introduces variability that prevents the results from being generalizable. The study also did not consider the quality of the concept test translations, and lower scores may be due to incorrect or confusing translations.\nPerformance might be dependent on prompts, and the ones in Figs. 3 and 4 may not be the best choices.\nIt should be noted that since we did not score the correctness of the physics reasoning in the AI answer, a key stumbling block for the AI can be the processing of graphical information unrelated to a physics concept or language. For example, for Question 7 in the FCI survey, the multiple-choice options are embedded in a graphic showing a ball swung in a circular path and not listed separately in the text. Based on the problem description in the text, the AI frequently gave physically reasonable answers but for the 32 languages and 96 total answers, it selected the correct choice that was embedded as a letter or number in the graphic just once. For no obvious reason, it chose instead one particular incorrect answer an eyebrow-raising 88 times. This confirms previous studies that have shown that the graphical layout of images and the spatial arrangement of answer options can play an important role in the AI's selection of answers [31].\nFinally, human post-instruction scores were gathered on a best-effort base, which may also introduce variability into the comparisons."}, {"title": "VII. CONCLUSION", "content": "The results of this study underscore the complexity and variability inherent in using multimodal large language models for physics assessment tasks across multiple languages and formats. By presenting the LLM with authentic screenshots of physics concept inventories, we moved beyond text-only evaluations and closer to the conditions encountered by students in real educational settings. The Al's own decision to switch, mix, or maintain the nominal language of the problem allowed us to gain insights into its internal reasoning and its adaptive strategies when confronted with linguistic or conceptual challenges.\nWhile the AI's performance depends on the subject area, the reasons for this are not entirely clear. Possible explanations include different levels of representation in the training data and its varying quality across the subject areas, or potential differences in the inherent difficulty of tasks in the assessments covering different subject areas. However, we have found that one variable clearly influencing Al's performance across all subject areas is the presence of non-redundant visual representations. This suggests that the AI's vision abilities still present a major weakness in its engagement with physics problems.\nThe marked differences in performance across languages and language-switching strategies highlight that LLMs are not equally competent in all tongues. Languages with less representation in training data often see dramatic performance gains when the AI switches to English. Meanwhile, languages better supported by training data and consistent physics terminology show more stable results. This observation not only illuminates the AI's uneven linguistic capabilities but also the influence of translation quality and cultural-linguistic nuances on physics problem-solving accuracy.\nThe results also show that providing the AI with more authentic, image-based input adds another layer of complexity. The need for the AI system to interpret visual information and integrate it with textual cues can strain its reasoning process, leading to uneven performance when images are integral to the question. This finding is critical when considering the use of LLMs as learning aids or assessment tools: while their linguistic capabilities are evident, their visual and multimodal reasoning abilities remain limited and domain-dependent.\nBased on published scores of post-instruction deployment of the inventories, we found that AI system outperforms average undergraduate students in all subject areas but laboratory skills. We were not able to locate sufficient language-specific data for students, but found that the AI-system's outperforming of undergraduate students is largely independent of the nominal language of the inventory.\nIn sum, this study demonstrates that while the studied AI system is capable and adaptive in many respects, it still exhibits significant variations in performance depending on the language, conceptual domain, and the presence of visual information. The work points toward future improvements in training data diversity, model fine-tuning, and prompt engineering to enhance both linguistic and conceptual understanding, as well as visual interpretation of multiple representations. It also highlights the need for careful consideration when implementing such AI systems in educational contexts, ensuring that their use is both equitable and aligned with pedagogical goals."}]}