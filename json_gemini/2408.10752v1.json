{"title": "Security Assessment of Hierarchical Federated Deep Learning", "authors": ["Duaa S. Alqattan", "Rui Sun", "Huizhi Liang", "Guiseppe Nicosia", "Vaclav Snasel", "Rajiv Ranjan", "Varun Ojha"], "abstract": "Hierarchical federated learning (HFL) is a promising distributed deep learning model training paradigm, but it has crucial security concerns arising from adversarial attacks. This research investigates and assesses the security of HFL using a novel methodology by focusing on its resilience against inference-time and training-time adversarial attacks. Through a series of extensive experiments across diverse datasets and attack scenarios, we uncover that HFL demonstrates robustness against untargeted training-time attacks due to its hierarchical structure. However, targeted attacks, particularly backdoor attacks, exploit this architecture, especially when malicious clients are positioned in the overlapping coverage areas of edge servers. Consequently, HFL shows a dual nature in its resilience, showcasing its capability to recover from attacks thanks to its hierarchical aggregation that strengthens its suitability for adversarial training, thereby reinforcing its resistance against inference-time attacks. These insights underscore the necessity for balanced security strategies in HFL systems, leveraging their inherent strengths while effectively mitigating vulnerabilities.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) offers a promising solution to the challenges of Centralized Machine Learning (CML), including data storage, computation, and privacy. FL facilitates collaborative training of a global model across numerous clients while preserving data decentralization. This approach has been successful in various applications like smart cities. Traditionally, FL employed a two-level node design, where chosen clients submit updates to a central server, situated either at the edge or in the cloud, for aggregation, as shown in Fig 1(a). The aggregation at the edge improves latency and network efficiency but restricts server capacity, affecting training. The aggregation in the cloud boosts computational"}, {"title": "2 Related Work", "content": "In recent years, significant attention has been devoted to studying the impact of attacks on FL. Abyane et al. [1] conducted an empirical investigation to comprehensively understand the quality and challenges associated with state-of-the-art FL algorithms in the presence of attacks and faults. Shejwalkar et al. [14] systematically categorized various threat models, types of data poisoning, and adversary characteristics in FL, assessing the effectiveness of these threat models against basic defense measures. Bhagoji et al. [3] explored the emergence of model poisoning, a novel risk in FL, distinct from conventional data poisoning. In contrast to conventional 2-level FL, adopting HFL introduces many novel research concerns due to its inherently intricate multi-level design [16]. A few"}, {"title": "3 Security Assessment of Hierarchical Federated Learning", "content": ""}, {"title": "3.1 Hierarchical Federated Learning (HFL) Model", "content": "We conceptualize the HFL system as a multi-parent hierarchical tree (as shown in Fig. 1), denoted as $T = (V, E)$, consisting of |L| levels. Nodes in the system, categorized as clients (N) and servers (S), are represented in the set V, while the collection of undirected communication channels between nodes is represented in the set E. The cloud server node, $s_0$, serves as the root of the tree at level 0, with client nodes, n, positioned at the leaves of the tree at level L - 1. Intermediate edge servers, $s_e$, act as intermediary nodes between cloud servers and clients at level l ($l \\in \\{1, ..., L \u2013 2\\}$). Clients may train their local models using local data and transmit their model parameters to regional edge servers $S_{L-2}$ for aggregation. The aggregation process in an HFL system involves several critical steps shown in figure 2. (Step 1) The cloud server $s_0$ sends the initial model to clients n through edge servers $s_e$. (Step 2) Regional edge servers $S_{L\u22122}$ select a set of client participants $C_t$ at aggregation round t from their coverage areas $A(S_{L-2})$ for model updates. (Step 3) Clients $C_t$ download the latest model from regional edge servers $S_{L-2}$ and train their local models. (Step 4) Updated parameters are sent back to regional edge servers $S_{L-2}$ for aggregation. (Step 5) Parent servers $s_e$ at level l aggregate updated model parameters from child nodes $s_{e + 1}$ within their coverage areas $A(s_e)$ for $T_e$ Number of aggregation rounds. (Step 6) After $T_0$ global aggregation rounds implemented by cloud server $s_0$, a global model is constructed and transmitted to clients for deployment through edge servers $s_e$.\nWe employ the averaging aggregation method proposed by McMahan et al. [9], allowing flexibility in deploying HFL models with varying levels (L)."}, {"title": "3.2 Adversarial Attacks on HFL Model", "content": "We consider the attacks on HFL models targeting data integrity during both the training and inference time. These attacks can be client-side or server-side, with client-side attacks encompassing data poisoning and model poisoning tactics.\nInference-time Attacks (ITAs). ITAs aim to carefully perturb the input data at inference time to have them misclassified by the global model. Adversarial"}, {"title": "3.3 Adversarial Defense on HFL Model", "content": "Defenses against adversarial attacks can be broadly classified into two categories: data-driven and model-driven defenses [15]. Data-driven defenses involve detecting adversarial attacks in the data or enhancing the quality of the data corrupted"}, {"title": "3.4 Experiment Design", "content": "We conduct experiments to assess the impact of adversarial attacks on HFL models (3-level HFL and 4-level HFL) and compare the performance of HFL models under various attacks and defense mechanisms alongside CML and traditional FL approaches (2-level FL). Our code is available on GitHub. The experimental settings are summarized as follows:\nDataset. We use three popular image classification datasets: mnist, fashion-mnist, and cifar-10. Each dataset contains 60,000 images (of which 50,000 images are in the training set and 10,000 images are in the test set) categorized into 10 classes. To simulate non-IID real-world scenarios, the images of the training set are split according to the Dirichlet distribution. We use state-of-the-art implementation of attack and defense methods from [10].\nHFL model. We consider a population of smart devices representing client nodes distributed across a city that implements image classification tasks. A group of 100 clients exists that engage in communication with the server for the purpose of image classification model training. We assume that the client selected for participation remains constant throughout the training process. Every client trains a local classifier model to classify the images. Regarding the server nodes, there is one cloud server in each learning paradigm at level 0. The cloud server performs the FedAvg aggregation rule for 20 aggregation rounds. We assume that the cloud server is highly secure and has never been compromised during the learning process. On the other hand, edge servers in HFL have different characteristics. In 3L-HFL, there are 20 regional edge servers that are distributed at the same level and connected directly with the cloud server and directly with 5 clients in their coverage area. Each regional edge server performs the"}, {"title": "4 Results and Discussion", "content": ""}, {"title": "4.1 Baseline performance: HFL model under no attacks", "content": "This section compares the performance of four models: a centralized machine learning model (CML), a 2-level FL, a 3-level HFL, and a 4-level HFL. As shown in Fig. 3, the CML model maintains consistently high accuracy across 20 global aggregation rounds over each dataset. The 4-level HFL model demonstrates notably high performance, showcasing the potential advantages of hierarchical architecture in FL. The 3-level HFL model presents an intermediary performance between 4-level HFL and 2-level HFL models, showing how hierarchical architecture impacts FL. HFL architecture enhances model update efficiency and potentially leads to faster convergence. In contrast, the 2-level FL model shows inferior performance."}, {"title": "4.2 Models performance under Inference-time attacks and defense", "content": "Impact of the attacks. We assessed the effectiveness of the models under attack by calculating MR. The outcomes are presented in Figure 4. Upon analyzing the MR, it becomes evident that the MR of all models trained on the same dataset exhibits a high degree of similarity. However, the impact of each type of attack can vary. Adversarial patch attacks demonstrate the lowest impact. All the other attacks lead to a high MR ranging between 80% to 100%. As highlighted in [5], in cases when the models, optimization methods, and the poisoned test dataset are identical, the effects of attacks on accuracy are likely to be comparable for"}, {"title": "Adversarial training (AT) defense against inference-time attack", "content": "We adversarially trained all models using data generated by inference-time attacks to enhance their robustness. The effectiveness of these adversarially trained models was evaluated by measuring MR, as shown in Table 1. In general, the MR dropped significantly across all models. While adversarially trained FL models demonstrate comparable MR to CML models, HFL models, especially the 4-level architecture, show even lower MR, suggesting higher resistance to attacks.\nFig. 5 shows the improved MR of robust models (red solid line) achieved through adversarial training compared to vulnerable models (red dashed line). However, a drawback of direct adversarial training adoption is observed with increased dataset complexity (cifar10), leading to higher MR for clean data, emphasizing the need for further research on complex, large-scale datasets."}, {"title": "4.3 Models performance under Training-time attacks and defense", "content": "Fig. 6 shows the consequences of training-time attacks on five distinct FL models that possess varied degrees of hierarchy and compromised nodes across different clean test datasets. The x-axis shows the number of compromised nodes (0, 1, 5, and 10), while the y-axis signifies the impact of the attack, reflecting the increase in MR resulting from the training-time attacks. The letter 'O' in the model name indicates that all the malicious clients are located in an overlapping area of two regional servers.\nThe impact of client-side attacks (data poisoning) We study both targeted and untargeted attacks on HFL as follows:"}, {"title": "Targeted label flipping (TLF) with backdoor attack", "content": "The targeted backdoor attack has two aims. First, to maintain the model's performance on clean data. Second, to make the model misclassify the targeted label as a desired label.\nTLF backdoor attack result in Fig. 6 shows that the MR for all three clean test datasets remains relatively stable across different percentages of malicious clients. This stability suggests that the presence of malicious clients has little impact on the model's performance, even when malicious clients are located in the overlapping areas of two servers. This indicates that the attacker fully achieved the first aim of not influencing the model's performance on clean test datasets.\nThe analysis of the second aim is shown in Fig. 7. From Fig. 7, we observe that TASR increases with the percentage of malicious clients for all models. CML model shows a notably high TASR, indicating vulnerability to backdoor attacks. Among FL models, the 4-level model consistently demonstrates the highest vulnerability to backdoor attacks, followed by the 3-level model and then the 2-level model. This suggests that increased complexity in FL models does not necessarily correlate with improved security against backdoor attacks. Malicious clients located in the overlapping area further amplify the potency of backdoor attacks, underscoring the importance of tailored security measures required in FL environments.\nThe neural cleanse (NC) method offers a robust defense against backdoor attacks in FL models, significantly reducing TASR and enhancing overall model security and robustness. Fig. 7 (solid lines) shows the effectiveness of this method across various FL models, showcasing a substantial reduction in TASR compared to scenarios without defense mechanisms (dashed line).\nDespite these improvements, CML models still show higher TASR values, highlighting their inherent vulnerabilities to backdoor attacks compared to FL"}, {"title": "Untargeted random label flipping (ULF) attack", "content": "As shown in Fig. 6, CML models suffer amplified effects from such attacks as increased compromised clients. However, FL and HFL are less impacted. For instance, in the mnist dataset, with"}, {"title": "Impact of client-side attacks (model poisoning)", "content": "In model poisoning [Client-side Sign flipping (CSF)], we only evaluate the result for FL models. This is because model poisoning is not commonly applied in CML. Regarding model poisoning attacks, Fig. 6 shows that all five FL models show minimal increases in MR, indicating resilience against such attacks. However, the 2-level FL model displays significant vulnerability when 10 clients are compromised, as observed in [14]. Conversely, the 3-level and 4-level HFL models show stronger performance, attributed to their hierarchical aggregation process, which mitigates the impact of individual clients. Even when all compromised clients strategically overlap two servers, HFL models show lesser MR impact compared to the 2-level model. These findings underscore the importance of hierarchical structure in mitigating model poisoning effects, suggesting the need for enhanced security measures for the 2-level FL model."}, {"title": "The impact of server-side attacks(model poisoning)", "content": "In comparing server-side sign-flipping (SSF) attacks between 3-level and 4-level HFL models, we observe in Fig. 6 that the 4-level model consistently shows lower MR across all datasets, indicating greater resilience to model poisoning. The impact increases with the number of compromised servers yet remains negligible, with both models showing only a slight increase in MR even when 10 servers are compromised. Specifically, the MR increase for the 3-level model does not exceed 0.4% for mnist and fashion-mnist datasets, while for CIFAR-10, both models show only a 3%-4% increase in MR. These results highlight the robustness of HFL models against server-side attacks, particularly for the 4-level architecture.\nFrom the results of a systematic analysis of HFL security, we observe that, in the context of ITAS, HFL models show varying degrees of susceptibility to"}, {"title": "5 Conclusion", "content": "Our investigation reveals that hierarchical federated learning (HFL) is resilient to untargeted data poisoning due to its hierarchical structure. However, targeted attacks, like backdoors, exploit architectural nuances, particularly when malicious clients strategically position themselves in the overlapping coverage area of regional edge servers. This highlights the need for further research in HFL security. Nonetheless, HFL shows promise in enhancing adversarial training to counter inference-time attacks. Future efforts should focus on developing tailored defense mechanisms to mitigate risks, bolstering the overall security and reliability of HFL systems for broader applications."}, {"title": "Acknowledgements", "content": "This research was supported by the Technical and Vocational Training Corporation (TVTC) through the Saudi Arabian Culture Bureau (SACB) in the United Kingdom and the EPSRC-funded project National Edge AI Hub for Real Data: Edge Intelligence for Cyber-disturbances and Data Quality (EP/Y028813/1)."}], "equations": ["MP = \\frac{1}{n} \\sum_{i=1}^{n} I(f(x_i) \\neq y_i),", "TASR \\sum_{i=1}^{n} \\frac{I(f(x^{\\text{adv}}) = y^{\\text{adv}} | y^{\\text{adv}} \\neq y_i)}{n},"]}