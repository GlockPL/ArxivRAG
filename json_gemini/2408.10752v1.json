{"title": "Security Assessment of Hierarchical Federated Deep Learning", "authors": ["Duaa S. Alqattan", "Rui Sun", "Huizhi Liang", "Guiseppe Nicosia", "Vaclav Snasel", "Rajiv Ranjan", "Varun Ojha"], "abstract": "Hierarchical federated learning (HFL) is a promising distributed deep learning model training paradigm, but it has crucial security concerns arising from adversarial attacks. This research investigates and assesses the security of HFL using a novel methodology by focusing on its resilience against inference-time and training-time adversarial attacks. Through a series of extensive experiments across diverse datasets and attack scenarios, we uncover that HFL demonstrates robustness against untargeted training-time attacks due to its hierarchical structure. However, targeted attacks, particularly backdoor attacks, exploit this architecture, especially when malicious clients are positioned in the overlapping coverage areas of edge servers. Consequently, HFL shows a dual nature in its resilience, showcasing its capability to recover from attacks thanks to its hierarchical aggregation that strengthens its suitability for adversarial training, thereby reinforcing its resistance against inference-time attacks. These insights underscore the necessity for balanced security strategies in HFL systems, leveraging their inherent strengths while effectively mitigating vulnerabilities.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) offers a promising solution to the challenges of Centralized Machine Learning (CML), including data storage, computation, and privacy. FL facilitates collaborative training of a global model across numerous clients while preserving data decentralization. This approach has been successful in various applications like smart cities. Traditionally, FL employed a two-level node design, where chosen clients submit updates to a central server, situated either at the edge or in the cloud, for aggregation, as shown in Fig 1(a). The aggregation at the edge improves latency and network efficiency but restricts server capacity, affecting training. The aggregation in the cloud boosts computational power and scalability but may delay updates for distant devices, stressing networks. In recent years, hierarchical federated learning (HFL), a variant of FL, has gained attention. HFL addresses FL challenges by employing multiple aggregator servers at edge and cloud levels, hierarchically interconnected, capitalizing on cloud coverage, and reducing the edge server latency [16].\nIn a use case scenario where HFL is deployed for smart city applications such as image classification, various clients, including smart cars, smart watches, drones, and mobile phones, are scattered across the smart city [13]. A significant number of edge servers are typically deployed in close proximity to these clients, forming a distributed architecture network connected to a central cloud server. Clients establish connections with edge servers within their coverage areas, with overlapping coverage enabling connections to multiple edge servers [4,12]. These edge servers forward client updates to regional edge servers, ultimately reaching the cloud server for aggregation to build a global model. Fig. 1 shows a comparison of 2-level FL (Fig. 1(a)) and HFL architectures that can be employed as 3-level [7] (Fig. 1 (b)) and 4 level node design [17] (Fig. 1(c)).\nDespite the advantages of HFL over FL, HFL remains susceptible to adversarial attacks that compromise data integrity by manipulating local datasets or model updates to undermine the global model's performance [11]. In HFL architecture, the increased number of nodes, including clients and edge servers, expands the attack surface, providing more potential entry points for attacks. This amplifies the risk of compromises by malicious edge servers or clients, surpassing the attack surface of FL. Fig. 1 provides an overview of the attack surface (see red triangle) in conventional FL compared to HFL. However, the augmentation of nodes also presents opportunities for bolstering defense mechanisms against attacks. This prompts an exploration of the following question: How does the HFL architecture impact the robustness of HFL against attacks?"}, {"title": "2 Related Work", "content": "In recent years, significant attention has been devoted to studying the impact of attacks on FL. Abyane et al. [1] conducted an empirical investigation to comprehensively understand the quality and challenges associated with state-of-the-art FL algorithms in the presence of attacks and faults. Shejwalkar et al. [14] systematically categorized various threat models, types of data poisoning, and adversary characteristics in FL, assessing the effectiveness of these threat models against basic defense measures. Bhagoji et al. [3] explored the emergence of model poisoning, a novel risk in FL, distinct from conventional data poisoning.\nIn contrast to conventional 2-level FL, adopting HFL introduces many novel research concerns due to its inherently intricate multi-level design [16]. A few"}, {"title": "3 Security Assessment of Hierarchical Federated Learning", "content": "While previous studies have evaluated 3-level HFL model convergence [7,8] and proposed resilient aggregation methods for 4-level HFL models [17], based on our best knowledge, there is little to no work on a systematic assessment of HFL security available in the literature that we aim to do in this paper. We examined HFL's resilience to adversarial attacks in detail. With the growing use of HFL in smart city applications [11], it is crucial to evaluate their resilience and understand their architectural nuances to suggest areas for improvement.\nThis paper explores how the HFL architecture withstands adversarial data injected during inference. Our findings highlight the challenges inference-time attacks pose to model accuracy. Yet, defense strategies like adversarial training offer promising solutions. We delve into Data Poisoning Attacks (DPA) and Model Poisoning Attacks (MPA) at the client and server sides during training, alongside potential defense mechanisms within the HFL framework. We identify vulnerabilities to targeted DPA (backdoor attack), notably in the 4-level HFL model, where hierarchical structure affects malicious client selection probabilities. Implementing the neural cleanser method [10] proves effective against targeted backdoor attacks, emphasizing tailored defense strategies' importance. Conversely, HFL models show resilience against untargeted DPA and MPA due to multi-level aggregation, mitigating outlier impact and enabling recovery from attacks.\nIn summary, our contributions are as follows:\n1. We present a novel methodology for assessing the security of HFL that offers insights into the resilience of HFL against inference time attacks, enhancing our understanding of HFL's robustness.\n2. Through comparative analyses, we pinpoint vulnerabilities in HFL under various training-time attacks and investigate how the HFL architecture influences model resilience against attacks, deepening our understanding of FL design and security.\n3. Our assessment of adversarial hierarchical federated training via extensive experiments on different datasets and HFL architectures sheds light on effective defense mechanisms for future HFL framework development, emphasizing HFL's resilience and its capacity to recover from attacks."}, {"title": "3.1 Hierarchical Federated Learning (HFL) Model", "content": "We conceptualize the HFL system as a multi-parent hierarchical tree (as shown in Fig. 1), denoted as $T = (V, E)$, consisting of $|L|$ levels. Nodes in the system, categorized as clients ($N$) and servers ($S$), are represented in the set $V$, while the collection of undirected communication channels between nodes is represented in the set $E$. The cloud server node, $s_0$, serves as the root of the tree at level 0, with client nodes, $n$, positioned at the leaves of the tree at level $L - 1$. Intermediate edge servers, $s_e$, act as intermediary nodes between cloud servers and clients at level $l$ ($l \\in \\{1, ..., L \u2013 2\\}$). Clients may train their local models using local data and transmit their model parameters to regional edge servers $S_{L-2}$ for aggregation. The aggregation process in an HFL system involves several critical steps shown in figure 2. (Step 1) The cloud server $s_0$ sends the initial model to clients $n$ through edge servers $s_e$. (Step 2) Regional edge servers $S_{L\u22122}$ select a set of client participants $C_t$ at aggregation round $t$ from their coverage areas $A(S_{L-2})$ for model updates. (Step 3) Clients $C_t$ download the latest model from regional edge servers $S_{L-2}$ and train their local models. (Step 4) Updated parameters are sent back to regional edge servers $S_{L-2}$ for aggregation. (Step 5) Parent servers $s_e$ at level $l$ aggregate updated model parameters from child nodes $s_{e + 1}$ within their coverage areas $A(s_e)$ for $T_e$ Number of aggregation rounds. (Step 6) After $T_0$ global aggregation rounds implemented by cloud server $s_0$, a global model is constructed and transmitted to clients for deployment through edge servers $s_e$.\nWe employ the averaging aggregation method proposed by McMahan et al. [9], allowing flexibility in deploying HFL models with varying levels ($L$)."}, {"title": "3.2 Adversarial Attacks on HFL Model", "content": "We consider the attacks on HFL models targeting data integrity during both the training and inference time. These attacks can be client-side or server-side, with client-side attacks encompassing data poisoning and model poisoning tactics.\nInference-time Attacks (ITAs). ITAs aim to carefully perturb the input data at inference time to have them misclassified by the global model. Adversarial"}, {"title": "3.3 Adversarial Defense on HFL Model", "content": "Defenses against adversarial attacks can be broadly classified into two categories: data-driven and model-driven defenses [15]. Data-driven defenses involve detecting adversarial attacks in the data or enhancing the quality of the data corrupted"}, {"title": "3.4 Experiment Design", "content": "We conduct experiments to assess the impact of adversarial attacks on HFL models (3-level HFL and 4-level HFL) and compare the performance of HFL models under various attacks and defense mechanisms alongside CML and traditional FL approaches (2-level FL). Our code is available on GitHub. The experimental settings are summarized as follows:\nDataset. We use three popular image classification datasets: mnist, fashion-mnist, and cifar-10. Each dataset contains 60,000 images (of which 50,000 images are in the training set and 10,000 images are in the test set) categorized into 10 classes. To simulate non-IID real-world scenarios, the images of the training set are split according to the Dirichlet distribution. We use state-of-the-art implementation of attack and defense methods from [10].\nHFL model. We consider a population of smart devices representing client nodes distributed across a city that implements image classification tasks. A group of 100 clients exists that engage in communication with the server for the purpose of image classification model training. We assume that the client selected for participation remains constant throughout the training process. Every client trains a local classifier model to classify the images. Regarding the server nodes, there is one cloud server in each learning paradigm at level 0. The cloud server performs the FedAvg aggregation rule for 20 aggregation rounds. We assume that the cloud server is highly secure and has never been compromised during the learning process. On the other hand, edge servers in HFL have different characteristics. In 3L-HFL, there are 20 regional edge servers that are distributed at the same level and connected directly with the cloud server and directly with 5 clients in their coverage area. Each regional edge server performs the"}, {"title": "4 Results and Discussion", "content": "FedAvg aggregation rule for two aggregation rounds. The 4L-HFL has similar settings to the 3L-HFL; however, there are 4 edge servers distributed at the same level between the cloud server and the regional edge servers. Each edge server communicates with five regional edge servers and performs the FedAvg aggregation rule for three aggregation rounds. The total aggregation round of regional edge servers is 40 and 120 rounds for 3L-HFL and 4L-HFL, respectively.\nClient local training model. We use two different convolutional neural network (CNN) architectures for the client's local classifier model for the three datasets. For mnist and fashion-mnist, we deploy a CNN with two 3x3 convolution layers (the first with 32 channels, the second with 64, each followed by 2x2 max-pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer with 10 outputs. For cifar10, A CNN with two 3x3 convolution layers with 32 channels followed by 2x2 max pooling, another two 3x3 convolution layers with 64 channels followed by 2x2 max pooling, a fully connected layer with 512 units and ReLu activation, and a final softmax output layer with 10 outputs. Each client employs categorical cross-entropy as their loss function and utilizes the optimizer that implements the Adam algorithm to update their local model depending on the loss function. For the mnist and fashion-mnist dataset, the batch size was set to 32 and the number of epochs was set to 1. For the cifar10 datasets, the batch size was set to 64, and the number of epochs was set to 6.\nMalicious Node. If a client is compromised, the client could act maliciously by implementing DPA or MPA. We evaluate the performance of the model while the number of malicious clients is 1, 5, and 10. We also evaluate the models when all of the malicious clients are located in the overlapping area of two regional edge servers. We indicate the model that considers the overlapping area with the letter 'O' (3-level HFL-O and 4-level HFL-O). We also assume that regional edge servers can be compromised and act maliciously by implementing MPA, whereas other edge servers are highly secure. We evaluate the performance of the model while the number of malicious servers is 1, 5, and 10.\nEvaluation Metrics. We include the Misclassification Rate (MR) and the Targeted Attack Success Rate (TASR) to assess attack efficiency and defense effectiveness. The Misclassification Rate (MR) can be formulated as:\n$$MP = \\frac{1}{n}\\sum_{i=1}^{n} I(f(x_i) \\neq Y_i),$$\nwhere $n$ is a number of image examples, $f(x)$ is the aggregated model's output (global model output for HFL or centralized model output for CML) over input $x$ which is clean input $x_i$ for training-time attacks and adversarial input $x_{adv}$ for inference-time attacks, $y_i$ is ground truth, and $I(.,)$ is an indicator function that returns 1 if model's output does match with the ground truth.\nSimilarly, TASR can be formulated as:\n$$TASR \\coloneqq \\frac{1}{n} \\sum_{i=1}^{n} \u2161(f(x^{adv}) = y^{adv} | y^{adv} \\neq Y_i),$$"}, {"title": "4.1 Baseline performance: HFL model under no attacks", "content": "This section compares the performance of four models: a centralized machine learning model (CML), a 2-level FL, a 3-level HFL, and a 4-level HFL. As shown in Fig. 3, the CML model maintains consistently high accuracy across 20 global aggregation rounds over each dataset. The 4-level HFL model demonstrates notably high performance, showcasing the potential advantages of hierarchical architecture in FL. The 3-level HFL model presents an intermediary performance between 4-level HFL and 2-level HFL models, showing how hierarchical architecture impacts FL. HFL architecture enhances model update efficiency and potentially leads to faster convergence. In contrast, the 2-level FL model shows inferior performance."}, {"title": "4.2 Models performance under Inference-time attacks and defense", "content": "Impact of the attacks. We assessed the effectiveness of the models under attack by calculating MR. The outcomes are presented in Figure 4. Upon analyzing the MR, it becomes evident that the MR of all models trained on the same dataset exhibits a high degree of similarity. However, the impact of each type of attack can vary. Adversarial patch attacks demonstrate the lowest impact. All the other attacks lead to a high MR ranging between 80% to 100%. As highlighted in [5], in cases when the models, optimization methods, and the poisoned test dataset are identical, the effects of attacks on accuracy are likely to be comparable for"}, {"title": "4.3 Models performance under Training-time attacks and defense", "content": "both centralized machine learning and federated learning models. However, the reason for studying the impact of inference-time attacks in HFL is that many defenses against inference-time attacks are implemented during training. Thus, it is crucial to study the architectural impact on the model's robustness against inference-time attacks.\nAdversarial training (AT) defense against inference-time attack. We adversarially trained all models using data generated by inference-time attacks to enhance their robustness. The effectiveness of these adversarially trained models was evaluated by measuring MR, as shown in Table 1. In general, the MR dropped significantly across all models. While adversarially trained FL models demonstrate comparable MR to CML models, HFL models, especially the 4-level architecture, show even lower MR, suggesting higher resistance to attacks.\nFig. 5 shows the improved MR of robust models (red solid line) achieved through adversarial training compared to vulnerable models (red dashed line). However, a drawback of direct adversarial training adoption is observed with increased dataset complexity (cifar10), leading to higher MR for clean data, emphasizing the need for further research on complex, large-scale datasets.\nFig. 6 shows the consequences of training-time attacks on five distinct FL models that possess varied degrees of hierarchy and compromised nodes across different clean test datasets. The x-axis shows the number of compromised nodes (0, 1, 5, and 10), while the y-axis signifies the impact of the attack, reflecting the increase in MR resulting from the training-time attacks. The letter 'O' in the model name indicates that all the malicious clients are located in an overlapping area of two regional servers.\nThe impact of client-side attacks (data poisoning) We study both targeted and untargeted attacks on HFL as follows:"}, {"title": "5 Conclusion", "content": "10 compromised clients, the MR increases by only 0.2% compared to models without attacks. Although HFL has slightly higher susceptibility due to server coverage, its impact remains minimal. FL's resilience is attributed to its client selection mechanism, where only a small proportion of clients are chosen per round, reducing the likelihood of selecting compromised clients. Moreover, to reduce FL and HFL accuracy, more than 10 clients must be compromised, necessitating a high-budget attack. Furthermore, imposing constraints on local dataset sizes effectively mitigates the occurrence of poisoned data, offering an efficient defense against untargeted attacks. This observation is consistent with findings presented in [14], further supporting the resilience of FL in real-world scenarios.\nOur investigation reveals that hierarchical federated learning (HFL) is resilient to untargeted data poisoning due to its hierarchical structure. However, targeted attacks, like backdoors, exploit architectural nuances, particularly when malicious clients strategically position themselves in the overlapping coverage area of regional edge servers. This highlights the need for further research in HFL security. Nonetheless, HFL shows promise in enhancing adversarial training to counter inference-time attacks. Future efforts should focus on developing tailored defense mechanisms to mitigate risks, bolstering the overall security and reliability of HFL systems for broader applications."}]}