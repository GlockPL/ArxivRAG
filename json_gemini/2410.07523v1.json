{"title": "DemoShapley: Valuation of Demonstrations for In-Context Learning", "authors": ["Shan Xie", "Man Luo", "Chadly Daniel Stern", "Mengnan Du", "Cheng Lu"], "abstract": "Large language models (LLMs) leveraging in-context learning (ICL) have set new benchmarks in few-shot learning across various tasks without needing task-specific fine-tuning. However, extensive research has demonstrated that the effectiveness of ICL is significantly influenced by the selection and ordering of demonstrations. Considering the critical role of demonstration selection in ICL, we introduce DemoShapley which is inspired by the Data Shapley valuation theorem. This approach assesses the influence of individual demonstration instances, distinguishing between those that contribute positively and those that may hinder performance. Our findings reveal that DemoShapley not only enhances model performance in terms of accuracy and fairness but also generalizes queries from domains distinct from those of the in-context demonstrations, highlighting its versatility and effectiveness in optimizing ICL demonstration selection. Last but not least, DemoShapley demonstrates its ability to aid in identifying noisy data within the demonstration set.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) excel in adapting to new tasks through in-context learning (ICL), using a few input-label pairs (demonstrations) without requiring fine-tuning (Brown et al., 2020). This method, contrasting from traditional fine-tuning, enables LLMs to manage diverse tasks with less complexity and resource use. ICL's efficiency and versatility have led to top performance in many few-shot tasks (Rae et al., 2021; Smith et al., 2022; Touvron et al., 2023; Chowdhery et al., 2023). Additionally, its unified architecture simplifies deployment and maintenance, offering a cost-effective alternative to conventional models (Mialon et al., 2023).\nIn the domain of few-shot learning, in-context learning (ICL) has shown significant promise but also presents challenges; unexpected behaviors have frequently been observed (Min et al., 2022). Typically, ICL involves generating prompts by randomly selecting a few examples from the training dataset (Brown et al., 2020). Prior research indicates that ICL's inferential performance is highly sensitive to the choice and order of demonstrations in the prompt, as well as to slight variations in prompt format (Lu et al., 2021; Rubin et al., 2021; Garg et al., 2022; Li et al., 2023; Luo et al., 2023, 2024; Chen et al., 2022). Therefore, selecting high-quality examples as demonstrations is crucial, as it can enhance model predictions, while suboptimal demonstrations may degrade performance (Min et al., 2022).\nTo address these issues, we present the DemoShapley, an approach inspired by the Data Shapley method (Ghorbani and Zou, 2019), which itself is derived from the Shapley value concept in cooperative game theory (Shapley et al., 1953). The Shapley value methodically assesses the average marginal contribution of each participant in every possible coalition, quantifying each player's impact on the final outcome. Data Shapley extends this concept into machine learning, not only focusing on the marginal contribution of data points to a learning algorithm's performance but also considering the sequence of data presentation. This aspect is crucial in ICL, where the order of demonstrations can significantly affect model performance. Our DemoShapley algorithm refines this approach, specifically addressing the challenges of demonstration selection in ICL.\nThe DemoShapley algorithm evaluates the contributions of demonstrations by considering various permutations, effectively marginalizing the influence of a demonstration's position within a prompt. This method provides a nuanced assessment of how different data points impact ICL, identifying both beneficial and detrimental demonstrations based on their Shapley values. This strategic selection"}, {"title": "2 Related Work", "content": "Data Valuation Data valuation in machine learning is a field of research aimed at evaluating the inherent worth of data within machine learning frameworks. For a more comprehensive review of data valuation, we refer the reader to this survey (Sim et al., 2022). Overall, there are three major categories of data valuation strategies which are leave-one-out (LOO), based on Cooperative Game Theory (CGT), and based on desiderata. (Cook and Weisberg, 1980) proposed LOO (Leave-One-Out) based strategy evaluates the value of data point Dk by measuring the alteration in the performance metric of the resulting model (or dataset) when Dk is excluded from the whole dataset. CGT-based methods are adapted from the game theory area. The two most well-known methods are Shapley value (Shapley et al., 1953) and Banzhaf index (Banzhaf III, 1964). Both methods depend on Dk marginal contribution to every coalition of the whole dataset where the coalition includes or excludes Dk. The marginal contribution of Dk is defined as the value change after adding Dk into a coalition without Dk, where the difference can be negative or positive values. The desiderata-based strategies are several Shapley variant methods which include Data Shapley (Ghorbani and Zou, 2019), Distributional Shapley (Ghorbani et al., 2020), and Beta Shapley (Kwon and Zou, 2021). DVRL (Yoon et al., 2020) used reinforcement learning to automatically learn data values and improve learning during training.\nICL Demonstrations Selection Chang and Jia (2022a) found that selectively choosing training samples based on their impact on model accuracy stabilized In-Context Learning (ICL) outcomes compared to random selection. Furthermore, Nguyen and Wong (2023) developed a technique to assess the impact of examples and select them accordingly, significantly improving performance on SuperGLUE tasks. When choosing demonstrations through retrieval, strategies typically prioritize similarity and diversity. Similarity-based selection involves choosing examples closely aligned with the query, leveraging linguistic or structural similarities. Liu et al. (2021) found that semantically closer demonstrations are more effective for GPT-3 ICL that utilize semantic representations from language models trained on tasks like natural language inference, employing the KNN algorithm for retrieval. Rubin et al. (2021) introduced a retrieval method that leverages language model cues to find better demonstrations compared to traditional retrievers. In contrast, Cheng et al. (2023) develop a universal retriever for cross-domain tasks, eliminating the need for task-specific retrievers by combining multiple training datasets. Apart from similarity, diversity in demonstrations is valued for avoiding repetition, incorporating varied viewpoints, and ensuring comprehensive coverage of test queries, as emphasized by Levy et al. (2022). In some cases, ICL benefits more from demonstrations of higher complexity, defined by query length or reasoning steps (Fu et al., 2022)."}, {"title": "3 Demonstration Valuation for ICL", "content": "As a method used in traditional machine learning domains for the equitable valuation of data contributions, Data Shapley assigns a quantitative value to each data point within a training dataset, revealing its contribution to model performance. The Data Shapley value \\(\\phi_i\\) for each data point i is defined to be an equitable evaluation that provides a fair measure of a data point's contribution to the overall performance of a learning algorithm. Mathematically, the value \\(\\phi_i\\) for each data point i in a dataset D with respect to a learning algorithm A and performance metric V is given by:\n\\(\\phi_i = \\sum_{S \\subseteq D\\{i\\}} \\frac{C}{{n \\choose |S|}}[V(S\\cup\\{i\\}) \u2013 V(S)],\\)\nwhere n is the total number of data points in D, S represents a subset of D excluding the i-th data point, and V (S) denotes the performance score of the predictor trained on subset S. The sum iterates over all possible subsets S of D excluding i, and \\({n \\choose |S|}\\) is the binomial coefficient representing the number of ways to choose |S| elements from n \u2212 1"}, {"title": "3.2 Proposed DemoShapley Framework", "content": "To adapt the TMC-Shapley calculation for Demonstration Shapley values in the context of ICL with LLMs, several key steps are outlined, taking into account the unique characteristics of ICL. Unlike traditional model training where all data points are used, ICL involves selecting only a small set of demonstrations as prompt. The steps for calculating Demonstration Shapley values are as follows:\n1. Random Sample and Permutation: Begin by randomly selecting K samples from the candidate demonstration set. These samples are then permuted randomly to form a sequence \\(\\Pi = \\{\\pi_1, \\pi_2,...,\\pi_K\\}\\), which will be used to construct the prompt P with an ordered list of input-output pairs. For zero-shot learning, the prompt contains only the task instruction.\n2. Performance Metric Calculation: For each prompt P, compute the performance metric V(P, f) for the LLM f. Initially, the zero-shot performance V (\u00d8, f), where the prompt consists solely of the task instruction, needs to be calculated.\n3. Sequential Demonstration Addition: Following the predetermined permutation \\{\\pi_\u03b1, \\pi_1,...\\}, sequentially add demonstrations to the preceding prompt P' and compute the new performance V'(P', f) each time a demonstration is added. This is done using the same development dataset for consistency.\n4. Shapley Value Update: If the change in performance v' after adding a demonstration exceeds a certain threshold, update the Demonstration Shapley value \\(\\phi_{\\pi_\u03b7}\\) for the newly added demonstration \\(\\pi_\u03b7\\) using the equation:\n\\(\\Phi_{\\pi_{\\eta[c]}} \\leftarrow \\frac{t_c-1}{t_c} \\cdot \\Phi_{\\pi_{\\eta}}^{t_{c-1}}[c] + \\frac{1}{t_c}(\\vartheta),\\)\nwhere \\(t_c\\) denotes the number of iterations for the c-th demonstration, and v' represents the change in performance metric resulting from adding the c-th demonstration."}, {"title": "4 Experiments & Applications", "content": "In this section, we outline four experimental setups aimed at assessing the effectiveness of DemoShapley in predictive tasks, improving fairness, and its detection ability to OOD and label noise scenarios."}, {"title": "4.1 Experimental Settings", "content": "LLMS: Our evaluation framework utilizes four primary models: two GPT models, the proprietary LLM ChatGPT-3.5-Turbo (OpenAI, 2024), the open-source LLM GPT-J-6B (Wang and Komatsuzaki, 2021); and two non-GPT models, Mistral-7B-v0.3 (Jiang et al., 2023) and Llama3-8B (AI@Meta, 2024).\nDatasets: The analysis includes diverse datasets tailored for specific tasks: Toxi-text-3M, a vast multilingual corpus for classifying texts as toxic or non-toxic (FredZhang7, 2023); the Adult Dataset, used to predict if an individual's income exceeds $50,000 and employed in fairness assessments (Becker and Kohavi, 1996); the Emotion Dataset, which consists of English Twitter messages annotated with six emotions for emotion recognition tasks (Saravia et al., 2018); BoolQ and BoolQ Contrast, where BoolQ involves yes/no question-answering and BoolQ Contrast tests model adaptability to data variations (Clark et al., 2019; Gardner et al., 2020); and SST-2 alongside FinancialPhraseBank-v1.0, with SST-2 used for sentiment analysis from movie reviews and FinancialPhraseBank-v1.0 serving as an Out-Of-Distribution task with sentences from financial news (Socher et al., 2013; Malo et al., 2014).\nData preprocessing involves converting tabular data which is Adult in our experiment into natural language for processing by large language models, and only focusing on the English subset of the Toxi-text-3M dataset.\nBaseline Methods: We compare DemoShapley against four baseline methods for selecting demonstrations: CondAcc (Chang and Jia, 2022b) and Influence (Nguyen and Wong, 2023) represent the state-of-the-art influence-based demonstration selection strategies. Additionally, we incorporate Leave-One-Out (LOO) and Random selection as two other baselines, following the approach outlined in (Ghorbani and Zou, 2019). Note that we opted not to include baselines focused on similarity or perplexity measures. This decision is based on findings from the Influence (Nguyen and Wong, 2023) baseline, which indicates that these factors might be less effective in comparison.\n\u2022 CondAcc uses the average performance to find examples to improve and stabilize the performance of ICL. The calculation of CondAcc score of demonstrations xi on LLM f is given as \\(S_{ca}(i) = E_{z\u223cD_{ICL}} [f(Z) | x_i \u2208 Z]\\), where Z denotes the prompt.\n\u2022 Influence calculates the influence of each example. The influence is the difference between the average performance of prompts with and without xi. The calculation is given as \\(I (i) = \\frac{N}{M} \\sum_{z:x_i \u2208 Z} f (Z) - \\frac{N}{M-N_i} \\sum_{z:x_i \\notin Z} f (Z)\\). Ni and M denote the number of prompts with xi and the total number of prompts respectively.\n\u2022 LOO detailed by Cook (Cook, 1977), quantifies the significance of an individual data point by the resultant change in model performance upon its removal. For ICL, we assess the performance difference between a complete prompt and one that excludes the example xi, to gauge the contribution of each demonstration.\n\u2022 Random selection involves choosing K examples randomly from the pool of candidate demonstrations for inference in ICL."}, {"title": "4.2 The Prediction Efficacy of DemoShapley", "content": "We design two experiments to showcase the predictive performance of DemoShapley compared to other baselines.\nAdd In-context Learning Examples based on Demonstration Values. We initiate with a 0-shot setting, applying two distinct strategies for choosing demonstrations. The first strategy progressively incorporates examples with the highest demonstration Shapley values from a 0-shot baseline, enhancing the prompt with valuable demonstrations. In contrast, the second strategy also begins from a 0-shot baseline but integrates examples with the lowest demonstration values into the prompt, adding new demonstrations that are theoretically less beneficial or even detrimental.\nRemove In-context Learning Examples based on Demonstration Values. We start with 10 randomly selected demonstrations. The first experiment in this category uses all 10 demonstrations initially and then removes the top five examples with the highest demonstration Shapley values to gauge the impact on prediction performance. Conversely, the second experiment begins with the same set of 10 demonstrations but removes the five examples with the lowest demonstration Shapley values, assessing the effects from the bottom of the value scale.\nResults We have the following observations based on the results for GPT models in Figure 2 and results for non-GPT architecture LLMs in Table 1.\nObservation 1: Incorporating demonstrations with high DemoShapley values proves advantageous, whereas those of low value are detrimental. The first column of Figure 2 demonstrates that adding high-value demonstrations consistently improves performance more than other methods, such as in cases where six high-value demonstrations invariably yield better results. In contrast, the second column and Table 1 show that integrating low-value demonstrations leads to a performance decline, a trend that is more pronounced using the DemoShapley method. This suggests a strong correlation between DemoShapley values and their actual impact in an ICL setting. Both Llama3 and Mistral models, along with other architectures, exhibit these trends, indicating that the quality of data significantly influences model performance across different structures.\nObservation 2: Eliminating demonstrations with high DemoShapley values leads to reduced performance, whereas removing those with low values enhances model performance. The third column of Figure 2 demonstrates that starting with a set of 10 demonstrations and then removing the ones with the highest DemoShapley values leads to a notable decrease in performance. The most significant"}, {"title": "4.3 DemoShapley for Algorithmic Fairness", "content": "In this experiment, we aim to answer whether ICL exhibits different fairness performance when using demonstrations selected by different methods.\nWe utilize the benchmark dataset, Adult, and consider gender as a sensitive attribute for our investigation. After computing DemoShapley values, we follow (Wang et al., 2023) and construct a test dataset that maintains a fixed base rate parity (bpt) 0.0. The base rate parity of demonstrations (bpc) varies among 0, 0.5, and 1. Base rate parity is a measure indicating bias within the data distribution in relation to a particular sensitive attribute; a higher value denotes significant bias, revealing a lack of demographic balance within the dataset. With bpt=0, we ensure that any observed bias in predictions can be attributed to the prompts, as per (Wang et al., 2023), allowing for a clearer assessment of the prompts' influence on model bias. We conduct experiments with 16 and 32 shots to examine how the performance varies with different numbers of demonstrations. We use two common group fairness metrics, specifically the demographic parity difference (Mdpd) (Dwork et al., 2012) and equalized odds difference (Meod) (Hardt et al., 2016). Mdpd measures the difference between the probability of positive predictions across groups. Meod indicates the gap in predictions conditioned on different values of the sensitive attribute while also considering the ground truth label. The detailed definitions of base rate parity, Mdpd, and Meod are listed in the Appendix.\nResults We have the following observations based on results in Table 2.\nObservation 1: In settings where both the context and test datasets have a base rate parity of 0, our approach achieves the highest accuracy with minimal bias, as measured by Mdpd and Meod metrics in our predictions. As the base rate parity in the context dataset increases, we observe a corresponding rise in fairness metrics, suggesting a potential increase in disparity between the context and test datasets. Despite this, the samples selected"}, {"title": "4.4 DemoShapley for OOD Generalization", "content": "In this section, we assess the potential of higher DemoShapley value samples to enhance model generalization on OOD tasks without necessitating finetuning. Our experimental framework focuses on the effect of selecting demonstration examples with significant DemoShapley values to bolster the model's generalization capabilities across datasets originating from disparate distributions. We calculate DemoShapley values for demonstrations that are coherent with the task of the test set but derived from various sources. Our investigation is structured around two distinct scenarios: the first scenario utilizes the BoolQ dataset as the source and BoolQ Contrast as the corresponding OOD test set. The second scenario examines the transferability of sentiment analysis tasks from the sst-2 dataset to the FinancialPhraseBank-v1.0 dataset, exploring how well the model adapts to sentiment analysis in a financial context, divergent from the original movie review domain. In this task, we utilized the GPT-J-6B LLM and opted not to use ChatGPT-3.5-Turbo. This is because, for all compared methods, we observed that varying permutations did not yield significant prediction differences in ChatGPT-3.5-Turbo, leading to small demonstration values. This phenomenon might be attributed to the possibility that both datasets were part of the pre-training material for ChatGPT-3.5-Turbo, affecting its sensitivity to permutations in the demonstration set. Our experimental results are presented in Table 3.\nObservations: Our results reveal that the performance achieved using our method consistently outperforms the baselines on OOD tasks, main-"}, {"title": "4.5 DemoShapley Robust to Label Noise", "content": "In this section, we delve into the effectiveness of DemoShapley values in identifying instances of label noise within datasets. Given the prevalence of manually annotated datasets through crowdsourcing in practical applications, mislabeling remains a significant challenge. Such inaccuracies can detrimentally impact a model's predictive performance. Drawing on the insights from previous research by Ghorbani and Zou (2019), which highlighted the utility of Data Shapley values in pinpointing mislabeled data, we anticipate that our DemoShapley method will similarly aid in detecting erroneous labels in demonstrations.\nFor this experiment, we select a random subset of 100 examples from both datasets as our pool of demonstration candidates. Within this set, we introduce label noise by randomly altering the labels of 10% of the data, ensuring these changes represent genuine mislabeling through manual verification. We then compute the DemoShapley values across"}, {"title": "5 Conclusion", "content": "We introduce the DemoShapley approach as a robust algorithm for selecting demonstrations in in-context learning (ICL). Inspired by Data Shapley, it offers a fair way to evaluate each demonstration's impact on ICL. Our experiments across prediction efficacy, algorithmic fairness, Out-Of-Distribution (OOD) generalization, and label noise detection show that DemoShapley significantly enhances model performance. This method not only improves accuracy but also ensures robustness in challenging environments and maintains fairness, making it ideal for ethical AI applications."}, {"title": "Limitation", "content": "Building upon the principles of Data Shapley, the proposed DemoShapley method provides a fair assessment of each demonstration's contribution to ICL. However, the computational demands remain a challenge, despite efforts to mitigate these through Monte Carlo sampling. The inherent complexity of updating Shapley values, which is exponential, restricts updates to N examples at a time, slowing down the computation process in the DemoShapley approach. In current implementations, our approach utilizes a fixed number of examples N in an N-shot scenario, where DemoShapley values are calibrated specifically for this configuration. While this setup has shown robust performance across various N values, there remains room for enhancement. A promising direction for future research could involve developing methods to calculate more universally applicable example scores that adapt across different shot configurations."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Pseudo code for DemoShapley", "content": "Algorithm 1 Algorithm for calculating the Demonstration Shapley\nInput Number of demonstrations K, Candidate Demonstration Set C, Dev Dataset D,\nIteration Times N, Validation Method V, LLM f, Threshold \u03bc, Instruction Prompt PI\nOutput Shapley Scores Table \u03a6 with size of |C| for candidate demonstration set C\n1: zero_shot_acc \u2190 V(PI, f, D)\n2: for i\u2190 1 to |C| do\n3: ti 0 \u25b7 Iteration times for ith datum\n4: end for\n5: for i 1 to N do\n6: Randomly select K datum from Candidate Demonstration Set C\n7: Generate a random permutation II with K datum selected.\n8: last_acc\u2190 zero_shot_acc\n9: Prompt P\u2190 PI\n10: for j 1 to K do\n11: P\u2190 P + \u03c0j\n12: acc\u2190 V(P, f)\n13: if |acc- last_acc | > \u03bc then\n14: tc tc + 1\n15: v\u2190acc-last_acc\n16: \u03a6\u03c0c[c]\u2190\u03a6tc-1[c] += (0) \\(\\vartheta\\)\n17: end if\n18: end for\n19: end for\nHyper parameters K, C, D, \u039d, \u03bc"}, {"title": "A.2 Fairness related definitions", "content": "To assess the demographic balance (fairness) of the data distribution, we employ the base rate parity (bP) for distribution P. In this equation, Y is the label for prediction and A is the sensitive attribute in the data.\nbp = \\(P_{(X,Y,A)\u223cP_{XY}} [Y = 1 | A = 1]\\)\n- \\(P_{(X,Y)\u223cP_{XYA}} [Y = 1 | A = 0]\\) (3)\nThe demographic parity difference measures the probability of positive prediction conditioned on the sensitive attribute A being 1 and being 0. The equation is shown below.\nMdpd = |\\(P_{(X,Y,A)\u223cP_{XY}} [f(X) = 1 | A = 1]\\)\n- \\(P_{(X,Y,A)\u223cP_{XY}} [f(X) = 1 | A = 0]|\\) (4)\nOne drawback of Mdpd is that it does not consider the ground-truth labels. Equalized odds difference also considers the ground truth label when calculating the demographic gap. The equation is shown below.\nMeod = max {MTP, MFP} (5)\nThe MTP denotes the true positive equalized odds difference while the MFP denotes the false positive equalized odds difference:\nMTP = \\(P_{(X,Y,A)\u223cP_{XY}} [f(X) = 1 | Y = 1, A = 0]\\)\n- \\(P_{(X,Y,A)\u223cP_{XY}} [f(X) = 1 | Y = 1, A = 1]|\\) (6)\nMFP = \\(P_{(X,Y,A)\u223cP_{XY}} [f(X) = 1 | Y = 0, A = 0]\\)\n- \\(P_{(X,Y,A)\u223cP_{XY}} [f(X) = 1 | Y = 0, A = 1]|\\) (7)"}]}