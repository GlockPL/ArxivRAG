{"title": "Tidal MerzA: Combining affective modelling and autonomous code generation through Reinforcement Learning", "authors": ["Elizabeth Wilson", "George Fazekas", "Geraint Wiggins"], "abstract": "This paper presents Tidal-MerzA, a novel system designed for collaborative performances between humans and a machine agent in the context of live coding, specifically focusing on the generation of musical patterns. Tidal-MerzA fuses two foundational models: ALCAA (Affective Live Coding Autonomous Agent) and Tidal Fuzz, a computational framework. By integrating affective modelling with computational generation, this system leverages reinforcement learning techniques to dynamically adapt music composition parameters within the TidalCycles framework, ensuring both affective qualities to the patterns and syntactical correctness. The development of Tidal-MerzA introduces two distinct agents: one focusing on the generation of mini-notation strings for musical expression, and another on the alignment of music with targeted affective states through reinforcement learning. This approach enhances the adaptability and creative potential of live coding practices and allows exploration of human-machine creative interactions. Tidal-MerzA advances the field of computational music generation, presenting a novel methodology for incorporating artificial intelligence into artistic practices.", "sections": [{"title": "Introduction", "content": "This paper outlines the development of Tidal-MerzA, a system for collaborative performance with a machine agent in live coding, which merges insights from an affective model ALCAA (affective live coding autonomous agent) and computational generation framework Tidal Fuzz. Its name is a portmanteau of \u201cMERged Tidal-FuzZ and Alcaa\u201d but also references the synonym for dada-ist practise invented by Kurt Schwitters, to describe his collage and assemblage works.\nLive coding is a term used to refer to performers creating art by writing computer code, usually in front of an audience. In live coding, computer language is the primary medium for notation and describing the rules with which to synthesise artworks, in this case we consider the case where the output is musical pattern. The practice of live coding places a strong focus on the elements of liveness, embracing error, the use of random processes and clear mappings between syntax and output.\nThe TidalCycles live coding language is used to create autonomous patterns by Tidal-MerzA. TidalCycles is an expressive language, known for its flexibility and versatility in creating complex structural ideas, through its functional programming style and \u201cmini-notation\" syntax . In Tidal-MerzA, these \u201cmini-notation\u201d strings\u2014symbolic groupings to denote wider functions in TidalCycles\u2014are a crucial aspect of representing and generating new patterns."}, {"title": "", "content": "There have been different approaches to the task of autonomous code generation or agential design in live coding and further to this, some attempts at analysing interactions with agents. However, the role of affect is rarely incorporated into such models, where affective methodologies can add benefit as they investigate the affective processes that emerge.\nFor this work, the aim is not only to generate TidalCycles code that is syntactically correct and evaluates to produce musical output, but also to consider the role of affective modelling of musical structural parameters and how to incorporate this into generation algorithms in a live composition setting. Affective modelling in machine learning provides a framework for collaborations with machines that create in a more human-like manner. However, the affective modelling process should acknowledge that machine aesthetics are often based on arbitrary metrics: where computers in lieu of the embodied emotional experience, would only discriminate or favour certain outcomes based on randomness and arbitrarily remove creative ideas from a conceptual space , or otherwise if we accept computational aesthetics in any system we develop, then the method by which we generate them must be tied to a consciousness in the machine that we have yet to prove. Instead, in Tidal-MerzA, the generation is guided by modelling affective response in humans.\nThe formation of a hybrid model that aims to combine affective capabilities with flexible coding is presented, offering a novel method for music generation blending the outcomes of previous work. The previous work on ALCAA presents a model of affect, translating literature findings into mathematical equations for creating music with specific affective qualities. However, a limitation of this model was its reliance on fixed code structures, restricting structural changes and underutilising TidalCycles' functional capabilities. Similarly, the previous work on the creation of the Tidal-Fuzz plugin introduced a model enabling the generation of syntactically correct code using various TidalCycles functions, by the process of a random walk through type signatures to produce executable code, but did not incorporate any modelling of affective equations. Tidal-MerzA aims to combine these two assets into one functional system.\nFor this hybrid system, two agents are consecutively built to attempt to capture all the dimensions outlined in the affective model-ALCAA. First, reinforcement learning (RL) techniques are used to dynamically adapt the parameters generated by the affective model within the flexible framework of TidalCycles. This framework enables the model to harness TidalCycles' extensive library of functions and patterns to generate music compositions that not only encapsulate desired emotional attributes but also adhere to the syntactical correctness of TidalCycles code. Secondly, specific mini-notation strings are produced that harness TidalCycles internal parsing of short-hand events.\nIn MerzA, the RL agent's actions correspond to the selection of musical elements within the TidalCycles framework. These actions are guided by a reward mechanism that evaluates the alignment between the generated music's affective attributes and the target affective states defined by the ALCAA model. Through trial and error, the agent refines its decision-making strategies, gradually learning which musical elements and TidalCycles functions to employ in order to evoke specific emotional responses. RL is particularly well-suited"}, {"title": "The Reinforcement Learning Problem", "content": "As the integration of affective models forms the basis of this exploration, how these human affective states are modelled follows the valence-arousal model introduced by Russell (1980). Formerly, music psychology literature labelled affective states using a categorical model, suggesting these stem from a finite number of monopolar universal basic affects. However, currently various two or three- dimensional models have been more universally adopted, with Russell's circumplex model of affect being commonly used, due to its ability to represent the complexities of affect. This approach employs valence (pleasure vs. displeasure) and arousal (high vs. low energy) as its dimensions, and is used in the research.\nThe reinforcement learning problem in the context of generating musical code based on valence-arousal coordinates involves training an agent to select sequences of code that correspond to desired affective qualities. The agent's goal is to maximise the cumulative reward received based on the affective quality of the generated musical code and its alignment with the specified valence-arousal coordinates. The agent interacts with an environment that provides feedback in the form of rewards, indicating how well the generated code matches the desired affective characteristics. By learning from this feedback, the reinforcement learning agent aims to discover a policy that maps valence-arousal coordinates to code sequences, enabling the generation of music that effectively captures the desired affective states."}, {"title": "Design of Agent 1: Learning Weightings", "content": "The RL agent is designed using the Q-learning algorithm, a model-free, value-based reinforcement learning technique. The agent's objective is to learn a Q-function, which estimates the expected cumulative reward for taking a particular action in a given state. The Q-learning algorithm iteratively updates these Q-values based on observed rewards and the Bellman equation.\nState Space\nTo represent the state, the agent utilises the valence-arousal coordinates that describe the desired affective qualities, i.e. $v, a \\in [-1, 1] \\times [-1, 1]$. The state space is discretised to represent the different possible states on this interval. The valence and arousal are both divided into ten segments, which gives a possible 10 \u00d7 10 state space, i.e. 100 possible states. This allowed to quantify the states with enough granularity, however limitations of this approach include potential loss nuance due to the discretisation process. A 2-dimensional-array where each row represents a unique combination of valence and arousal values is created to store all these states.\nAction Space\nThe action space for this RL agent is defined as the selection of sequences of code based on the valence-arousal coordinates. The agent discretised the action space to represent a set of predefined code sequences. This allows for flexible and fine-grained control over the generated musical code."}, {"title": "", "content": "\u2022 Loudness: The loudness is defined by the following equation, based on the literature:\n\u2022 $l(a, v) = unif{l_{min}, I_{min} + I_{range} }$ where\n$\\{ \\begin{array}{l} l_{\\min }=-18\\\\ t_{\\text {range }}=l_{0} * a+l_{1} * v \\end{array}$                                                                         (1)\nSince the loudness is determined by a uniform distribution between $I_{min}$ and $I_{min} + I_{range}$ each action in this context can be a specific loudness level within this range. This range is discretised into a set of possible loudness levels of 25-equal intervals.\n\u2022 Pitch register: The pitch register is determined by the given pitch equation based on the literature:\n$\\{\\begin{array}{l} round(v * 12), \\quad if v > 0 .\\\\ \\operatorname{pr}(\\alpha, v)=\\left\\{\\begin{array}{l} round\\left(\\frac{\\alpha}{2}+12\\right) ), \\quad if a>0 .\\\\ if v<=0 .\\\\ \\operatorname{round}\\left(\\frac{\\alpha}{2}+12\\right) ), \\quad if a<0 . \\end{array}\\right. \\end{array}$                                                                       (2)\n$p_{r}(a, v)$ produces a value between 0 and 24. This range is also discretised into a set of 25 distinct values.\nSince the agent needs to decide both the loudness and pitch register simultaneously, the combined action space would be the Cartesian product of the two individual action spaces. As they have been discretised each into 25 possible loudness levels and 25 distinct pitch levels, the combined action space has 625 possible actions.\nWith the state space of size $n$ and action space of size $m$ defined, the q-table then was initialised of size $n \\times m$. In this case, the state space size $n=100$ and the action space size $m=625$ meant that the q-table is a matrix of size 100 \u00d7 625."}, {"title": "Reward Functions", "content": "The reward function serves as a crucial guide for the agent, indicating the desirability of its actions and influencing its learning process. In the context of MerzA, defining appropriate reward functions was essential for shaping the system's ability to generate music that aligns with the specified affective dynamics."}, {"title": "Learning Algorithm and Exploration", "content": "The RL agent employed a Q-learning algorithm as the means with which it learnt and updated its decision-making policies based on the received rewards. Q-learning was chosen as it is a useful learning algorithm for problems with discrete action spaces. Q-learning is an off-policy learner that aims to learn the value of the optimal policy, thus allowing the agent to evaluate the potential of actions without explicitly following them. This quality makes it particularly suited for environments where exhaustive exploration of the action space is impractical.\nIn the implementation in Agent 1 of Tidal-MerzA, the learning process revolved around updating the Q-learning table. Each entry in the Q-table represents an estimate of the expected cumulative future rewards for taking a given action in a given state, known as the Q-value.\nThe learning process unfolded over many episodes, where each agent represents a sequence of decisions. At every step within an episode, the agent observed its current state, selected an action based on either exploration or exploitation, and received a reward as the result. The reward reflected how well the chosen action"}, {"title": "Training the Agent", "content": "The training process of the RL agent is tailored towards its task of learning optimal strategies for weighting the functions based on valence-arousal co-ordinates. After defining the key components of our reinforcement learning model, the agent is set up so that it can actively interact with this environment. Specifically, the agent employs a function to determine the subsequent state it will occupy, which is dictated by its current state and the action it takes, and it also calculates the reward it receives for taking this action. Following each action executed by the agent, it updates its Q-table-a data structure used to estimate the expected rewards for each possible state-action pair. The update rule incorporated the received reward and discounted estimate of future rewards.\nThe agent then undergoes training over numerous episodes each comprising a series of steps until a termination condition is met. For this agent, it was found that 12000 episodes sufficed to produce results that were invariant to small changes in the input or environment. After this, there was a plateau in the rewards gained by the agent -which demonstrated that more learning would not produce more significant results. This level of training allowed the agent to achieve a stable and consistent performance, indicating a convergence of the learning process. During these episodes, the agent interacted with its environment, making decisions, receiving feedback in the form of rewards or penalties, and incrementally improving its policy based on this feedback.\nOnce the training was completed by the agent, it can then determine the optimal gain and note settings for any given valence-arousal pair.This is achieved by querying the trained Q-table with the valence-arousal state and"}, {"title": "", "content": "retrieving the action that maximises the expected reward. By the end of the training process, the agent is capable of adapting its outputs to the range of valence-arousal co-ordinates for $v, a \\in [-1,1] \\times [-1,1]$.\nMultiple training sessions were conducted, each time gathering data, analysing the agent's performance, and making any necessary adjustments to the hyperparameters. This iteration process was key in making sure that the number of episodes, total number of steps in each episode, and hyperparameters were configured correctly to adjust the agent's ability to create music that aligned with the specified affective dynamics. Specifically, the hyper-parameters of the learning rate, discount factor, and e-greedy parameter were fine-tuned to optimise the learning process. Through this process of continual evaluation and iteration, Agent 1 evolved into a more adept system."}, {"title": "Design of Agent 2: Mini-notation Strings", "content": "As outlined, there were two aspects that were needed to create executable patterns of TidalCycles code. As outlined in Figure 1, a second agent was used to generate the mini-notation strings for the code sequences. This task involved the synthesis of sequences of tokens to form coherent and contextually relevant strings. In , one challenge was noted in the generation of these strings. Specifically, the mini-notation is a terse way to represent events within a pattern in Tidal syntax, and these additional complexities of notation were omitted in this early version, where mini-notation strings are treated as single tokens.\nIn response to this challenge and to generate notation events, a novel agent is proposed that generates mini-notation strings through dynamic adjustment to the importance of individual tokens, using the affective model outlined in. This section explores the architecture and experimental outcomes of this second agent in combination with the first.\nIn this section, as outlined by Figure 3, the mode, pitch contour and rhythmic structure is determined through the mini-notation, rather than learning function transformers, as seen in the design of agent 1. Internally, the mini-notation is actually parsed and understood as a shortcut for a function that you could otherwise write using longer function compositions. The mini-notation is used for this agent as this allows the generation of new strings as this is an easier abstraction to work with than the functions themselves. The previous agent outlined uses function the reinforcement agent to learn weightings as these functions are not expressible in the mini-notation."}, {"title": "Rhythmic Roughness", "content": "Generation of rhythmic and melodic sequences are treated separately, with generation of rhythmical structure used to give structure to the melodic patterns, in a similar manner to other affective algorithms. The generation of these sequences will be treated in a different manner, which are now outlined."}, {"title": "", "content": "Overall, the agent acts in the following manner for rhythmic sequences. Firstly, the possible rhythmic structure tokens, T, that can be generated is defined as:\n$T=\\left[\\prime \\sim \\prime,^{\\prime} 1^{\\prime},^{\\prime} 1 * 2^{\\prime}, \\prime 1 @ 2^{\\prime}, \\prime[]^{\\prime}\\right]$\nThese form a subset of the whole mini-notation.\nThis subset was chosen as it allows the representation of all the concepts needed rhythmic roughness described in. The token 1 represents a sound occurring, represents a rest, 1*2 repeats the note in the same segment (i.e. creates quavers from a crochet, semiquavers from quavers, depending on the number of events in the sequence). The 1@2 token elongates a pattern (i.e. the inverse, creates a crochet where notes are divided into quavers etc). And finally the [] creates a pattern grouping. Similar to 1*2, this shortens the length of this element. However it does not just repeat the previous pattern but allows sub-groupings of all the previous token types.\nThe algorithm for generating mini-notation strings for the rhythmic patterns is then outlined as follows:\n1. Construct a sequence of fixed length from the possible states, T, based on the valence and arousal input parameters\n2. Once the first sequence is completed, check the string using regular expression to see if any bracket tokens are chosen\n3. If brackets exist, move inside and return to step 1. If there are no brackets in the sequence, go-to step 4\n4. From a predetermined set of drum samples, randomly select a sample to replace all 1s in the sequence.\nThe equation for generating rhythmic patterns, from , is outlined as follows:\n$R(a, n)=\\frac{(1-a) *(n+1)}{2}$\nwhere $R(a, n)=\\operatorname{Pr}$(note removed).\nThe transformation into TidalCycles mini-notation from this equation is thus as follows. Roughness is a parameter used to determine the variation in note lengths over a measure of music: if all notes are of equal duration, roughness is low, and if notes are of varying length, roughness is high. This formula is used to determine the roughness parameter, based on arousal input, by calculated probabilities to select a token that will either increase roughness or decrease it.\nFrom this, note patterns of set length can be generated as mini-notation pattern strings, based on the input arousal parameter, $a \\in[-1,1]$.\nFor example, where a = 0.65, and using a sample called kick the mini-notation pattern that is generated is:\n\"kick kick*2 [~ kick] kick kick*2\""}, {"title": "Modality", "content": "The next part of the creation of mini-notation strings is to select the mode based on the valence parameter. This mode provides a key profile for which to generate the pattern from.\nThe equation for determining the mode , or some ordering of the modes, outlined in (Schmuckler 1989):\n$M=\\left[\\right.^{\\prime} \\operatorname{lydian}^{\\prime},\\right.^{\\prime} \\operatorname{ionian}^{\\prime}, \\ldots . . \\operatorname{mixolydian}^{\\prime}, \\operatorname{dorian}^{\\prime}, \\operatorname{aeolian}^{\\prime}, \\ldots\\operatorname{phrygian}^{\\prime},\\left.\\operatorname{locrian}^{\\prime}\\right]$\nthen, based on the valence parameter, the index of the matrix M is chosen following the equation, according to:\n$m(v)=M[$ round $(3-3.5 v)]$\nThis provides the modality for the agent. Again, this rule-based system can be applied to mini-notation strings for this agent.\nTo do this, requires the samples of each folder to be ordered chromatically, where \"sample:0\" represents the root note, \"sample:1\" represents one semitone above and so on. Then, following this ordering of the samples, a dictionary can be constructed for each of the modes as following:\nmodes = {\n\"lydian\": [0, 2, 4, 6, 7, 9, 11, 12],\n\"ionian\": [0, 2, 4, 5, 7, 9, 11, 12],\n\"mixolydian\": [0, 2, 4, 5, 7, 9, 10, 12],\n\"dorian\": [0, 2, 3, 5, 7, 9, 10, 12],\n\"aeolian\": [0, 2, 3, 5, 7, 8, 10, 12],\n\"phrygian\": [0, 1, 3, 5, 7, 8, 10, 12],\n\"locrain\": [0, 1, 3, 5, 6, 8, 10, 12]\n}"}, {"title": "Pitch contour", "content": "The next part of the creation of mini-notation strings is to select the degrees of the mode for each note, based on the valence parameter. The mode chosen, as formerly outlined, provides a key profile for which to generate the pattern from.\nIt was outlined in that ascending melodies are associated with positive emotional states (i.e. high valence affect), whereas descending melodies are associated with negative emotional states (i.e., low valence affect). This finding was modelled through the equation:\n$P_{c}(v, i, j)=\\left\\{\\begin{array}{l} \\frac{v+1}{2} \\cdot w(K[j]-K[i]), \\quad if v>0 \\\\\\ \\frac{1-v}{2} \\cdot w(K[j]-K[i]), \\quad if v \\leq 0 \\end{array}\\right.$\nThe final stage for producing the mini-notation strings for the melody involved selecting the degrees of the chosen mode using this equation and then selecting the corresponding sample for the chosen mode, i.e. given a probability that the next note will be higher in the sequence, with this probability being modelled using $p_{c}(v, i, j)$. This meant that for a higher valence, there was a greater chance of ascending to a higher note in the mode, and a greater chance of descending for negative valence.\nAs this is applied to the generated rhythmical sequence, fixed length note patterns can be generated as mini-notation pattern strings, based on the input arousal parameter, $a \\in[-1,1]$. As an example, for the valence-arousal co-ordinates v = 0.8 and a = 0.65, and with a sawtooth sample \"saw\" selected, the generated melody would be:\n\" saw:4 saw:6*2 saw:7 saw:9 saw:11 saw: 12*2\"\nLikewise, for v = \u22120.25, a = -0.8:\n\"~ saw:10*2 saw: 8 saw: 7 ~\""}, {"title": "Performing with Tidal-MerzA", "content": "The hybrid system described here produces both the weightings and mini-notation strings for the live coding agent. These are then used in conjunction with an existing auto-complete Atom plugin that was outlined in . However, due to the sunsetting of the Atom Text-Editor since this work was completed, this was remade as a plug-in for the Pulsar open-source text editor. This custom plugin is combined with a Haskell listener module that requests a pattern when a $ command from the Atom Editor is executed. This allows the live coder to receive a completed pattern to accept or reject. The format of this for MerzA was similar, entailing the use of the Atom plugin to create new patterns on receipt of a $ symbol in the editor.\nHowever, a slight difference was that first the agent needed to be given a valence and arousal parameter. These were currently sent from the command line using argument parsing in python. Once these had been received, the training completes and MerzA outputs a text file with the learnt function weights and mini-notation strings was produced.\nThis file was formatted in the same way the n-gram models previously were, as arrays of tuples with normalised weights, in the manner outlined in . The learnt weights and mini-notation strings were merged with the file from the previous agent structure. A listener function was created for this system, so that once the co-ordinates were received and the training had been completed, patterns were automatically suggested in the text editor.\nThe process of training and using the agent can be seen here:\nhttps://www.youtube.com/watch?v=QBl3c7wWWPU"}, {"title": "Evaluating Outcomes", "content": "Firstly, the learning efficiency of the RL agent\u2014Agent 1\u2014is discussed. Through the use of a tracking mechanism in the training loop, this allowed the monitoring of the rate of improvement in rewards over episodes. After training, a plot keeping track of the moving average for the reward function was produced, seen in Figure 5. This approach allowed quantitative evaluation of the learning efficiency of the agent."}, {"title": "Advantages and Limitations of MerzA", "content": "Overall, MerzA successfully integrated the affective response model tested in with the computational model evaluated in. Through the outlined algorithms, it has been shown how co-ordinates on the valence-arousal affective space can be translated to patterns of TidalCycles code.\nUsing reinforcement learning in this context, combined with the mini-notation string generation algorithm, offers several advantages, especially in the context of adaptability, complexity, and creativity."}, {"title": "Conclusion", "content": "In conclusion, this research has demonstrated the potential of using reinforcement learning to facilitate the learning of function weightings in computational music generation. The application of this technique has proven to be a step in advancing the field of affective algorithmic composition, showcasing a novel approach to integrating artificial intelligence algorithms into creative practices. Furthermore, through this hybrid system, mini-notation strings which are a key feature of Tidal's syntax, has enabled a concise expression of musical ideas. This development is significant as it allows the coding agent to select and manipulate musical concepts with greater precision and relevance.\nThe outcomes of this work highlight not only the technical feasibility but also the artistic potential of employing advanced computational methods in music generation. By bridging the gap between computational models and creative expression, this research paves the way for more sophisticated and nuanced musical creations, driven by intelligent computational systems.\nOverall, the finding presented in this paper contribute insights to the field of computational music and open up new avenues for exploration in the intersection of artificial intelligence, creativity, and artistic expression. This chapter outlines how to fuse the work from the previous system developments. The next stage of development is to critically evaluate its outcomes and what this can mean for human-machine creative relationships."}]}