{"title": "Revisiting Graph Neural Networks on Graph-level Tasks: Comprehensive Experiments, Analysis, and Improvements", "authors": ["Haoyang LI", "Yuming XU", "Chen Jason ZHANG", "Alexander ZHOU", "Lei CHEN", "Qing LI"], "abstract": "Graphs are essential data structures for modeling complex interactions in domains such as social networks, molecular structures, and biological systems. Graph-level tasks, which predict properties or classes for the entire graph, are critical for applications, such as molecular property prediction and subgraph counting. Graph Neural Networks (GNNs) have shown promise in these tasks, but their evaluations are often limited to narrow datasets, tasks, and inconsistent experimental setups, restricting their generalizability. To address these limitations, we propose a unified evaluation framework for graph-level GNNs. This framework provides a standardized setting to evaluate GNNs across diverse datasets, various graph tasks (e.g., graph classification and regression), and challenging scenarios, including noisy, imbalanced, and few-shot graphs. Additionally, we propose a novel GNN model with enhanced expressivity and generalization capabilities. Specifically, we enhance the expressivity of GNNs through a k-path rooted subgraph approach, enabling the model to effectively count subgraphs (e.g., paths and cycles). Moreover, we introduce a unified graph contrastive learning algorithm for graphs across diverse domains, which adaptively removes unimportant edges to augment graphs, thereby significantly improving generalization performance. Extensive experiments demonstrate that our model achieves superior performance against fourteen effective baselines across twenty-seven graph datasets, establishing it as a robust and generalizable model for graph-level tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs represent objects as nodes and their relationships as edges, which are key data structures across various domains to model complex interactions [8, 20, 22], such as social networks, molecular structures, biological systems, etc. Graph-level tasks involve predicting properties or labels of entire graphs, rather than individual nodes or edges. These tasks are crucial in various domains, such as subgraph counting in database management systems [25, 51], molecular property prediction in chemistry [35, 105], and protein classification in biology [48, 61]. Recently, graph neural networks [13, 31, 55, 57, 78] have emerged as powerful tools for modeling and analyzing graph-structured data. They learn node representations by iteratively aggregating information from their neighbors and obtain graph representations based on node representations for graph-level tasks.\nDepending on the technique, current GNNs designed for graph-level tasks can be categorized into five types: node-based, hierarchical pooling (HP)-based, subgraph-based, graph learning (GL)-based, and self-supervised learning (SSL)-based GNNs. Firstly, node-based GNNs [31, 43, 78, 87] learn representations for individual nodes, which are then used to form a graph-level representation by a Read-out function, such as averaging the representations of all nodes. Secondly, HP-based GNNs [7, 15, 54, 60] employ hierarchical pooling techniques to progressively reduce the graph size while preserving its essential structural information. HP-based GNNs aim to capture multi-level graph structures to learn a graph representation.\nThirdly, subgraph-based GNNs [5, 17, 65, 88] separate a graph into several subgraphs. They learn the representations for each subgraph and then obtain the final graph representation based on these subgraph representations. Fourthly, GL-based GNNs [23, 49, 56, 104] assume that graphs are noisy and incomplete. These methods aim to reconstruct the graph structure and node features to enhance graph quality, enabling the learning of more accurate and reliable graph representations. Finally, SSL-based GNNs [39, 48, 73, 76, 85] aim to pretrain GNNs on unlabeled graphs to learn generalizable node representations, which can then be fine-tuned with labeled data. Specifically, they pretrain GNNs by either predicting existing graph properties (e.g., node degrees and attributes) [34, 39, 85, 95, 99] or maximizing the similarity between graph representations obtained from different augmentations (e.g., edge dropping) of the same graph [32, 45, 48, 73].\nAlthough various GNNs being designed for graph-level tasks, their evaluations are often restricted to a narrow range of domain-specific datasets and insufficient baseline comparisons [19, 84, 104]."}, {"title": null, "content": "This limited scope fails to accurately reflect the advancements and capabilities of existing GNNs in handling graph-level tasks. To address the need for fair and comprehensive empirical evaluations, we first identify three key issues in existing evaluation frameworks, thereby enhancing our evaluation capabilities.\n\u2022 Issue 1. Insufficient Coverage of Data Domains. The evaluations of existing approaches often concentrate on a limited range of datasets, typically in specific domains such as chemistry or biology. This narrow focus can cause models to overfit to the unique characteristics of these datasets, limiting the generalizability and applicability of GNNs to other domain datasets, such as social networks or geometric graphs.\n\u2022 Issue 2. Insufficient Graph-level Tasks and Scenarios. Current evaluation frameworks only focus on one type of graph-level task, such as graph classification for molecular graphs. The insufficient coverage of graph tasks neglects the diversity of potential graph-level applications, such as cycle counting and path counting in graphs. Additionally, current methods only assess GNNS under the assumption of enough labeled graphs. However, there are many other realistic scenarios. For example, the graphs labels are few and imbalanced, and the graph datasets are noisy.\n\u2022 Issue 3. Lacking One Unified Evaluation Pipeline. The absence of a standardized evaluation pipeline leads to inconsistent reporting and comparison of results across studies. Different works may use varying metrics, datasets, and experimental setups, complicating the assessment of progress and the identification of truly effective models. A unified evaluation framework would facilitate more transparent and reliable comparisons\nTo address the above three issues, we propose a comprehensive framework to ensure a more comprehensive and fair assessment of GNNs for graph-level tasks. Firstly, we categorize existing GNNs for graph-level tasks into five types and analyze each type in depth. Secondly, we propose a unified and fair evaluation pipeline with standardized data splitting and experimental settings. We use graph datasets from diverse domains. including biology, chemistry, social networks, and geometric graphs. Thirdly, we evaluate existing graph-level GNNs on both graph classification and graph regression (e.g., cycle counting). We also consider diverse scenarios beyond those with ample labeled graphs, including noisy graphs, imbalanced graphs, and the few-shot setting.\nAdditionally, alongside the comprehensive evaluation framework, we propose an improved GNN model for graph-level tasks, which achieves both high expressivity and strong generalization capabilities. Firstly, to enhance the expressivity of GNNs, we introduce a general k-path rooted subgraph GNN, which samples subgraphs for each k-length path starting from every node. Our model learns representations for each subgraph and aggregates these into a comprehensive graph representation. We theoretically prove that k-path rooted subgraph GNNs can, at a minimum, count the number of k + 2-Path and k + 3-Cycle. Secondly, to improve generalization across diverse domains and scenarios, we propose an adaptive graph contrastive learning approach to augment graphs. By analyzing edge importance across different domains, we propose a unified metric based on node degree to remove unimportant edges, ensuring the augmented graphs retain their original structural patterns. Our model is then optimized by maximizing the similarity"}, {"title": null, "content": "between augmented graphs and predicting labels consistently with the original graph. This approach makes our model resistant to noise, thereby improving its generalization ability.\nWe summarize the contributions of this paper as follows.\n\u2022 We systematically revisit GNNs for graph-level tasks and categorize them into five types, i.e., node-based, hierarchical pooling-based, subgraph-based, graph learning-based, and self-supervised learning-based. We provide an in-depth analysis of their distinct techniques for learning graph representations, offering a clear understanding of their strengths and limitations.\n\u2022 We propose a unified evaluation framework to address limited domain diversity, insufficient task coverage, and the lack of standardization. It includes diverse datasets, supports multiple tasks, and ensures fair comparisons across challenging scenarios like noise, imbalance, and few-shot learning.\n\u2022 We propose an expressive and generalizable GNN model, which enhances expressivity with a k-path rooted subgraph approach for counting paths and cycles, and improves generalization by adaptive graph contrastive learning that removes unimportant edges to handle noise and diverse domains.\n\u2022 Extensive experiments on 13 graph classification datasets, 14 graph regression datasets, and various scenarios (e.g., few-shot and noisy graphs) demonstrate the superior performance of the proposed method over state-of-the-art GNNs."}, {"title": "2 PRELIMINARY AND RELATED WORK", "content": "In this section, we first introduce the graph-level tasks and then summarize existing GNNs used for these tasks. Important notations are summarized in Table 1.\n2.1 Graph-level Tasks\nGraphs are a fundamental data structure to model relations between nodes in real-world datasets, such as social networks [11, 58], chemistry graphs [10, 11], etc. Formally, we denote a graph as $G_i (V_i, A_i, X_i)$, where $V_i$, $A_i \\in \\{0,1\\}^{|V_i|\\times|V_i|}$, $X_i \\in \\{0,1\\}^{|V_i|\\times d_x}$, denote nodes, adjacency matrix, and node features, respectively. For each node $v \\in V_i$, $N_i(v) = \\{u : A_i[u][v] = 1\\}$ is the 1-hop neighbors of $v$ and $N_i^l(v)$ is the neighbors of $v$ within $l$ hops. In general, there are two types of graph-level tasks, graph classification and graph regression.\nThese two type of tasks involve predicting either categorical labels (classification) or a continuous value (regression) for an entire graph. Formally, given a labeled graph dataset $L_G = \\{(G_i, y_i)\\}_{i=1}^{|L_G|}$, each graph $G_i$ is labeled with discrete label vector $y_i \\in \\{0, 1\\}^{|\\mathcal{Y}|}$ on labels $\\mathcal{Y}$ for classification, or a continuous value $y_i \\in \\mathbb{R}$ for regression. The goal is to learn a mapping function $f_\\theta$ that generalizes to unseen graphs as follows:\n$f_\\theta : G_j \\rightarrow y_j$, where $y_j \\in \\{0, 1\\}^{|\\mathcal{Y}|}$ or $y_j \\in \\mathbb{R}$.\nGraph classification and graph regression are critical and popular across various domains, such as databases, chemistry, and biology. Specifically, in database and data management fields, graph classification tasks include query plan strategy classification [103, 106] and community classification [21, 72], which help optimize database queries and understand data relationships. Graph regression tasks involve subgraph counting [51, 100] and cardinality estimation of"}, {"title": null, "content": "graph queries [70, 75, 81]. Also, biology benefits from graph classification for protein structure analysis and motif identification [95], and graph regression helps predict gene expression levels [63].\n2.2 Graph Neural Networks\nRecently, GNNs [14, 29, 30, 37, 47, 71, 83, 97, 110] have emerged as powerful tools to learn node representations, due to their ability to capture complex relationships within graph structures. Existing GNNs for graph-level tasks can generally be categorized into five types: node-based, pooling-based, subgraph-based, graph learning-based, and self-supervised learning-based GNNs.\n2.2.1 Node-based Graph Neural Networks. As shown in Figure 1 (a), node-based GNNs first learn representations for individual nodes, which then are used to form a graph-level representation. Generally, each GNN $f_\\theta$ consists of two basic operations, i.e., AGG(\u00b7) and COM(\u00b7) [31, 46], which can be formulated as two trainable parameter matrices $W_{agg}$ and $W_{com}$. Formally, given a graph $G_i (V_i, A_i, X_i)$ and a node $v \\in V_i$, in the $l$-th layer, AGG${}^{(l)}(\u00b7)$ in Equation (2) first aggregates the hidden representation $h_i^{(l-1)}(u)$ and edge representation $e_{uv}$ of each neighbor $u$ of node $v$ and obtain the aggregated result $m_i^{(l)}(v)$. Then, as shown in Equation (3), COM${}^{(l)}(\u00b7)$ combines the aggregated neighbor information $m_i^{(l)}(v)$ and the latest information $h_i^{(l-1)}(v)$ of node $v$ to obtain the $l$-th layer representation $h_i^{(l)}(v)$.\n$m_i^{(l)}(v) = \\text{AGG}^{(l)}(\\{ (h_i^{(l-1)}(u), e_{uv}) : u \\in N_i(v) \\})$\n$h_i^{(l)}(v) = \\sigma(\\text{COM}^{(l)}(h_i^{(l-1)}(v), m_i^{(l)}(v)))$\nwhere $\\sigma$ denotes a non-linear function (e.g., ReLU [52]). $h_i^{(0)}(v)$ is initialized as $X_i[v]$. After $L$ layers aggregation, we can obtain node representations $H_i(V_i) \\in \\mathbb{R}^{|V_i| \\times d_l}$. Then, the graph representation $h_i$ of $G_i$ can be summarized from the node representation $H_i(V_i)$ by a READOUT function [48] as follows:\n$h_i = \\text{READOUT}(H_i(V_i))$,\nwhere READOUT is a permutation-invariant function, such as AVERAGE, SUM, or MAX functions [48]. The SUM function as an example is $h_i = \\sum_{v \\in V_i} H_i(V_i)[v]$.\nModel Optimization. Then, we can use a decoder (e.g., 2-layer MLPs [108]) to predict discrete class or continuous property value for each graph $G_i$ based $h_i$. In summary, given the labeled training dataset $L_G = \\{(G_i, y_i)\\}_{i=1}^{|L_G|}$ and the prediction $y$ for each graph $G_i$ by the GNN $f_\\theta$, the GNN $f_\\theta$ can be optimized by minimizing the task loss as follows:\n$\\theta^* = \\arg \\min_\\theta \\frac{1}{|L_G|} \\sum_{G_i \\in L_G} L_{task}(f_\\theta, G_i, y_i)$.\ns.t. $L_{task} = \\begin{cases}\n- \\sum_{y \\in \\mathcal{Y}} y_i \\log \\hat{y}[y], & y_i \\in \\{0, 1\\}^{|\\mathcal{Y}|} \\\\\n||y - y_i||_2, & y_i \\in \\mathbb{R}\n\\end{cases}$\n2.2.2 Hierarchical Pooling-based Graph Neural Networks. As shown in Figure 1 (b), hierarchical pooling (HP)-based GNNs are designed to capture hierarchical structures in the graph, which can reduce the graph size progressively while preserving its essential structural information. This is achieved by clustering nodes into groups"}, {"title": null, "content": "and summarizing their features, allowing the model to focus on condensed graph structures and finally obtain the representation of each graph.\nAs mentioned in Equations (2) and (3), at the $l$-th layer, node-based GNNs learn node representations using the adjacency matrix $A_i$ and the node hidden representations $H_i^{(l-1)}(V_i)$. In contrast, at the $l$-th layer, HP-based GNNs first generate a new cluster assignment matrix $S^{(l)} \\in \\{0, 1\\}^{n_{l-1} \\times n_l}$, where $n_l < n_{l-1}$. This matrix maps the nodes $V_i^{(l-1)}$ from the $(l-1)$-th layer to a cluster index in $\\{1, ..., n_l\\}$. The new adjacency matrix $A_i^{(l)} \\in \\mathbb{R}^{n_l \\times n_l}$ is then reconstructed based on the cluster assignment matrix $S^{(l)}$ as follows:\n$A_i^{(l)} = S^{(l)T} A_i^{(l-1)} S^{(l)}$.\nSubsequently, $n_l$ new nodes $V_i^{(l)}$ are created to represent the $n_l$ clusters. Then, the node hidden representations $H_i^{(l-1)}(V_i^{(l)})$ can obtained by a READOUT operation on nodes each cluster $k \\in \\{1,..., n_l\\}$ as follows:\n$H_i^{(l-1)}(v_i^{(l)})[k] = \\text{READOUT}(H_i^{(l-1)}(V_i^{(l-1)})[V_k])$,.\nwhere $V_k = \\{u | S^{(l)}[u][k] = 1\\}$ are nodes in the cluster $k$.\nDepending on the technique of generating the cluster matrix $S^{(l)}$, existing pooling-based GNNs can be classified into three types.\n\u2022 Similarity-based. Similarity-based approaches [15, 54, 60] cluster nodes based on the similarity of nodes features or structures. These methods rely on predefined similarity metrics or graph partitioning algorithms to generate the cluster assignment ma-trix. Graclus [15, 60] and CC-GNN [54] assign nodes to different clusters based on feature similarity and graph structure."}, {"title": null, "content": "\u2022 Node Dropping-based. Node dropping-based methods [9, 28, 44] focus on reducing the number of nodes in the graph by learning an importance score for each node. Nodes with the highest scores are retained, while others are dropped, effectively shrinking the graph size in a controlled manner. TopKPool [9, 28] and SAGPool [44] aim to learn an importance score for each node and retain only the top-$n_l$ nodes with the highest scores at layer $l$. In other words, they only assign one node for each cluster and directly drop the other nodes.\n\u2022 Learning-based. Learning-based approaches [4, 7, 16, 77, 92] use neural network architectures to learn the cluster assignment matrix $S^{(l)}$ based on the graph's node features and structure, which adaptively learn how to cluster nodes in a way that preserves the graph's essential information. Specifically, Diff-Pool [92] and MinCutPool [7] use non-linear neural networks and and GMT [4] use the multi-head attention mechanism [77], to learn a cluster matrix $S^{(l)}$ based on the learned node features $H_i^{(l-1)}$ and adjacent matrix $A_i^{(l-1)}$. EdgePool [16] proposes to learn the edge score between each pair of connected nodes and then generate the cluster matrix accordingly.\n2.2.3 Subgraph-based Graph Neural Networks. Recent studies propose subgraph-based GNNs with enhanced expressive power, enabling them to more effectively capture substructure information. As shown in Figure 1 (c), the core idea is to decompose the input graph into a set of overlapping or non-overlapping subgraphs and then learn representations for each subgraph to improve the final graph representation. Formally, given a graph $G_i (V_i, A_i, X_i)$, existing models first generate $n_s$ subgraphs $G_{i,j} (V_{i,j}, A_{i,j}, X_{i,j})]_{j=1}^{n_s}$. Then, for each subgraph $G_{i,j}$, the node representation of $v \\in V_{i,j}$ is learned by Equations (2) and (3). In this approach, each node $v \\in V_i$ can have multiple representations because it may appear in multiple sampled subgraphs. To derive the final node representation $h_i(v)$, these representations are merged. A common approach is to use a READOUT function to combine them, as follows:\n$h_i(v) = \\text{READOUT}(\\{ h_{i,j}(v) | v \\in V_{i,j} \\})$.\nDepending on the techniques of generating the subgraphs, current subgraph-based approaches can be classified into three types.\n\u2022 Graph Element Deletion-based. These approaches [6, 12, 17, 64] propose deleting specific nodes or edges to create subgraphs,"}, {"title": null, "content": "enabling GNNs to focus on the most informative parts of the graph by removing noise or less relevant elements. For example, DropGNN [64] and ESAN [6] generate subgraphs through random edge deletion to enhance model expressiveness. Conversely, SGOOD [17] abstracts a superstructure from the original graphs and applies sampling and edge deletion on this superstructure, thereby creating a more diverse graphs.\n\u2022 Rooted Subgraph-based. These approaches [5, 27, 38, 65, 68, 88, 89, 93, 96, 101] focus on generating subgraphs centered around specific nodes, referred to as root nodes, with the aim of capturing the structural role and local topology of the root node within the subgraph. The primary motivation is to enhance the representational power of GNNs by encoding the root node's position within the subgraph, as well as its relationships to other nodes within the same subgraph. Specifically, I2GNN [38], ECS [88], and ID-GNN [93] propose to append side information to the root nodes or edges to capture the position information, such as an ID identifier for root nodes (e.g., the root v is 1 and other nodes are 0) [38, 93], and node degree and shortest distance information [88]. Also, NestGNN [96] and GNN-AK [101] use rooted subgraphs with different hops to capture the hierarchical relationships within graphs for root nodes.\n\u2022 k-hop Subgraph-based. Similar to rooted subgraph-based approaches [3, 24, 62, 69, 79, 90], k-hop subgraph-based approaches construct subgraphs based on the k-hop neighborhood of each node v, focusing on capturing the local structure around each node. Unlike rooted subgraph-based approaches, which aggregate information from 1-hop neighbors, k-hop subgraph-based approaches aggregate information not only from 1-hop neighbors but also directly from nodes up to k hops away. Specifically, MixHop [3] utilizes a graph diffusion kernel to gather multi-hop neighbors and computes the final representation. SEK-GNN [90], KP-GNN [24], EGO-GNN [69], and k-hop GNN [62] progressively updates node representations by aggregating information from neighbors within k-hops. MAGNA [79] proposes to learn more accurate weights between each pair of nodes based on all the paths between them within k-hops.\n2.2.4 Graph Learning-based Graph Neural Networks. Due to uncertainty and complexity in data collection, graph structures may be redundant, biased, or noisy. Consequently, GNNs cannot learn reliable and accurate graph representations, leading to incorrect predictions."}, {"title": null, "content": "Therefore, as shown in Figure 1 (d), current researchers [23, 56, 104] propose reconstructing the graph structure and node features to improve the quality of the learned graph representations. Given a labeled graph set $L_G = \\{(G_i, y_i)\\}_{i=1}^{|L_G|}$, the graph learning-based approaches can be formulated as bi-level optimization problem.\n$\\theta^* = \\min_\\theta \\frac{1}{|L_G|} \\sum_{G_i \\in L_G} L_{task}(f_\\theta, \\mathcal{G}(V_i, A, X), y_i)$.\ns.t. $A, X = \\arg \\min_{A_i, X_i} L_{gl}(f_{\\theta^*}, \\hat{\\mathcal{G}}_i(V_i, \\hat{A}_i, \\hat{X}_i), y_i), \\forall G_i \\in G$\nAt the low level in Equation (11), current approaches propose different graph learning objectives $L_{gl}(\u00b7)$ to reconstruct graph structure $A_i$ and node features $X_i$. Then, in Equation (10), $\\mathcal{G}(V_i, \\hat{A}, \\hat{X})$ will be used to optimize the GNNs by the loss function in Equation (5).\nDepending on the techniques of reconstructing graphs, current GL-based GNNs can be categorized into three types.\n\u2022 Preprocessing-based. Preprocessing-based approaches [18, 50, 86] reconstruct graphs and then use them to optimize GNNs directly. The basic idea is to first reveal the common graph patterns and modify the graph structure and node features to recover these patterns. For example, GNN-Jaccard [86] and GNAT [50] observe that similar nodes tend to be connected by edges. Therefore, they remove edges between dissimilar nodes and add edges between similar ones. Also, GNN-SVD [18] observes that noisy edges and node features tend to increase the rank of the adjacency matrix. Therefore, GNN-SVD reconstructs graphs by decreasing the rank of the adjacency matrix.\n\u2022 Jointly Training-based. In contrast to static preprocessing-based methods, jointly training-based approaches aim to optimize the GNN while iteratively reconstructing the graph structure and node features alongside the GNN model parameters. This iterative bi-level optimization enables the GNN to dynamically adapt the graph structure and features based on task-specific objectives. For instance, several approaches [26, 41, 42, 49, 59, 74], such as ADGNN [49], ProGNN [42], and SimPGCN [41], propose reconstructing the edges between each pair of nodes by minimizing the GNN loss on the reconstructed structure while also reducing the rank of the adjacency matrix. Alternatively, several approaches [80, 98, 107], such as MOSGSL [107] and HGP-SL [98], focus on first partitioning each graph into subgraphs based on node similarities and predefined motifs, and then reconstructing the edges between these subgraphs rather than at the individual node level.\n2.2.5 Self Supervised Learning-based Graph Neural Networks. Self-supervised learning (SSL) has become a powerful paradigm to pre-train GNNs without the need for labeled data, which can capture the node patterns and graph patterns. As shown in Figure 1 (e), the key idea of SSL approaches is to create supervised signals directly from the structure and node features of the unlabeled graph itself, leveraging the graph's inherent properties to guide the learning process. Formally, given a set of unlabeled graphs $U_G = \\{G_i(V_i, A_i, X_i)\\}_{i=1}^{|U_G|}$, the GNN $f_\\theta$ is pretrained as follows:\n$\\theta' = \\arg \\min_\\theta \\frac{1}{|U_G|} \\sum_{G_i \\in U_G} L_{ssl} (f_\\theta, G_i, Signal_i)$,"}, {"title": null, "content": "where $Signal_i$ is the supervised signals from the unlabeled graph $G_i$ and $\\theta'$ is the optimized GNN parameters. Then, the pretrained $f_{\\theta'}$ can be used to predict graph labels or properties. Formally, given the set of labeled graphs $L_G = \\{G_j (V_j A_j, X_j), y_j\\}_{j=1}^{|L_G|}$, the GNN $f_{\\theta'}$ is optimized as follows:\n$\\theta^* = \\arg \\min_\\theta \\frac{1}{|L_G|} \\sum_{G_j \\in L_G} L_{task} (f_{\\theta'}, G_j, y_j)$,\nwhere the task loss $L_{task} (\u00b7)$ is defined in Equation (5).\nDepending on the technique of generating supervised signals from unlabeled graphs, SSL-based GNN approaches can be broadly categorized into two main types.\n\u2022 Pretext Task-based. Pretext task-based approaches [34, 36, 39, 40, 85, 95, 99] design auxiliary tasks to help the model learn useful representations from the graph structure and features without requiring external labels. Current approaches design diverse node-level and graph-level tasks to capture intrinsic graph patterns [85], such as predicting node attribute, node degree, and the node number. For example, HMGNN [95] pretrains GNNs by predicting the links between nodes and the number of nodes in each graph. MGSSL [99] first masks the edges among motifs and then predicts the missing edges. MoAMa [39] first masks node features and then uses GNNs to reconstruct the node features based on the masked graphs. GraphMAE [34] and GPTGNN [36] predict both node attributes and edges.\n\u2022 Graph Contrastive Learning-based. Graph contrastive learning (GCL)-based GNNs [32, 45, 67, 73, 82, 94] aim to learn representations by contrasting positive and negative examples. The basic idea is to maximize the similarity between different views or augmentations of the same node or graph (positive pairs) while minimizing the similarity between different nodes or graphs (negative pairs). In such a way, GCL-based approaches can distinguish the similarity between nodes and graphs. In general, the SSL loss $L_{ssl}(\u00b7)$ in Equation (12) can be formulated as follows.\n$\\theta' = \\arg \\min_\\theta \\frac{1}{|U_G|} \\sum_{G_i \\in U_G} L_{cl} (f_\\theta, G_i, \\hat{G_i}, Neg_i)$.\ns.t. $\\hat{G_i}, \\tilde{G_i} = \\arg \\min_{\\hat{G_i}, \\tilde{G_i}} L_{positive} (G_i, \\mathcal{T}), \\forall G_i \\in U_G$,\nwhere $L_{cl}(\u00b7)$ is the contrastive loss. Additionally, $L_{positive}(\u00b7)$ is the view generation loss used to generate two positive views (i.e., $\\hat{G_i}$ and $\\tilde{G_i}$ for each graph $G_i$) where $\\mathcal{T}$ is a set of augmentation operations for generating the views. Negative views ($Neg_i$) are a set of graphs, which are typically randomly sampled from all graphs or from graphs with labels different from $G_i$. One typical $L_{cl} (f_\\theta, \\hat{G_i}, G_i, Neg_i)$ in Equation (14) can be defined based on InfoNCE loss [91, 108] as follows:\n$L_{cl}(\u00b7) = -\\log \\frac{s(h_i, \\hat{h_i})}{\\sum_{G_j \\in A(G_i)} s(\\hat{h_i}, h_j)}$,\nwhere $A(G_i) = \\hat{G_i} \\cup \\tilde{G_i} \\cup Neg_i$ includes positive views and negative samples of $G_i$, and $h_i$ and $\\hat{h_i}$ are the representations of graph $\\hat{G_i}$ and $G_i$, respectively. Also, $s(h_i, h_j) = \\exp(\\text{cosine}(h_i, h_j) / \\tau)$ is the similarity score between $h_i$ and $h_j$ based the parameter $\\tau$."}, {"title": "3 A GENERALIZABLE AND EXPRESSIVE GNN", "content": "We propose a simple yet effective contrastive learning-based GNN with high generalization and expressivity capabilities on graph-level tasks. As shown in Figure 1, our proposed GNN model consists of three components, i.e., adaptive graph augmentation, k-Path rooted subgraph encoder, and consistency-aware model training.\n3.1 Adaptive Graph Augmentation\nTo enhance the generalization ability of the GNN model, we propose using contrastive learning to optimize GNNs for graph-level tasks. As described in Section 2, the key step is to generate positive graph pairs for each graph. Here, we propose a unified contrastive learning approach for graphs in diverse domains, such as geometric graphs and biology. The basic idea is to first measure the importance of each edge concerning the whole graph and then drop the unimportant edges based on weighted sampling to generate positive pairs for G.\nSpecifically, given a graph $G_i (V_i, A_i, X_i)$ with edges $E$, the edge importance of each edge $e_{v,u} \\in E_i$ is calculated based on node degree of $v$ and $u$ [48, 109], i.e., $imp_{v,u} = \\log (\\frac{D_i(v)+D_i(u)}{2} + 1)$, where edge connected with higher-degree nodes tends to be important because nodes with higher degrees in a graph are bridges and core nodes in a functional motif as follows:\n\u2022 Geometric Graphs. High-degree nodes act as crucial bridges, and removing their edges significantly impacts the number of paths and cycles within this graph.\n\u2022 Social Networks. Higher-degree nodes are crucial as they often represent key influencers or hubs in the network. These nodes connect different parts of the network, facilitating communication and information flow. Their removal can fragment the network and disrupt connectivity significantly.\n\u2022 Chemistry Graphs. High-degree atoms mainly determine the properties of a motif. For example, phosphorus (P) in phosphate groups (PO4) is bonded to four oxygen atoms (O). While oxygen atoms contribute to properties like electronegativity and polarity, phosphorus is often more critical in determining the overall functionality and behavior of the phosphate group.\nThen, the dropping probability $p_{v,u}$ of each edge $e_{v,u}$ can be normalized using min-max normalization [48, 66, 109] as follows:\n$p_{vu} = \\mu \u00b7 \\frac{imp_{max} - imp_{v,u}}{imp_{max} - imp_{min}}$\nwhere $\u03bc \u2208 [0, 1]$ is a hyperparameter, and $imp_{max} = \\max_{e_{i,j}\u2208E_i} imp_{i,j}$ and $imp_{min} = \\min_{e_{i,j}\u2208E_i} imp_{i,j}$ are the largest and smallest edge score, respectively. Then, for each graph $G_i$, we can generate two"}, {"title": null, "content": "positive graphs $\\hat{G_i}$ and $\\tilde{G_i}$ by randomly dropping"}]}