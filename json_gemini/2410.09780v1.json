{"title": "Expanding Search Space with Diverse Prompting Agents: An Efficient Sampling Approach for LLM Mathematical Reasoning", "authors": ["Gisang Lee", "Sangwoo Park", "Junyoung Park", "Andrew Chung", "Sieun Park", "Yoonah Park", "Byungju Kim", "Min-gyu Cho"], "abstract": "Large Language Models (LLMs) have exhibited remarkable capabilities in many complex tasks including mathematical reasoning. However, traditional approaches heavily rely on ensuring self-consistency within single prompting method, which limits the exploration of diverse problem-solving strategies. This study addresses these limitations by performing an experimental analysis of distinct prompting methods within the domain of mathematical reasoning. Our findings demonstrate that each method explores a distinct search space, and this differentiation becomes more evident with increasing problem complexity. To leverage this phenomenon, we applied efficient sampling process that uniformly combines samples from these diverse methods, which not only expands the maximum search space but achieves higher performance with fewer runs compared to single methods. Especially, within the subset of difficult questions of MATH dataset named MATH-hard, The maximum search space was achieved while utilizing approximately 43% fewer runs than single methods on average. These findings highlight the importance of integrating diverse problem-solving strategies to enhance the reasoning abilities of LLMs.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have significantly enhanced their reasoning abilities, particularly in mathematical reasoning and code generation. High-performing models such as GPT-40 (OpenAI, 2024), Claude Opus (Claude, 2024) have demonstrated their capabilities in these challenging domains, showcasing their advanced performance. These models are typically employed through step-by-step natural language reasoning methodologies named Chain of Thought (CoT) to ensure the validity and accuracy of their solutions (Wei et al., 2023). Particularly in solving math problems, existing approaches either focus on validating the logical sequence during the solution process (Zhang et al., 2024; Zihao et al., 2024; Zhou et al., 2024), seek verification support for complex calculations (Chen et al., 2023; Zhou et al., 2023; Zhong et al., 2024), or aim to secure both logic validation and calculation accuracy (Gou et al., 2024). A common feature of these methods is the use of sampling and voting processes to achieve self-consistency (CoT-SC) by generating multiple solutions (Wang et al., 2023).\nWhile these methods have been effective in verifying the solutions provided by LLMs and enhancing their reliability, they heavily rely on temperature adjustments to increase the diversity of problem-solving approaches. This reliance on self-consistency within their own generated solutions limits their ability to explore a wider range of problem-solving strategies. In contrast, human problem solvers invest significant effort not only in verifying the validity and accuracy of their calcula-"}, {"title": "tions but also in exploring many potential solutions. Recent efforts in the field of LLM's high reasoning have focused on integrating diverse agentic problem-solving methods to address these limitations (Du et al., 2023; Liu et al., 2023). Although these studies have shown promising performance on benchmarks such as MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021), they lack a comprehensive analysis of why different agents collectively achieve high performance. Furthermore, there is an absence of methodologies that explore how the unique characteristics of each approach can be effectively integrated, beyond merely improving performance metrics.\nTherefore, this study aims to address these gaps by performing an experimental analysis of the problem-solving strategies employed by various LLM agents within the domain of mathematical reasoning. Furthermore, we propose an efficient sampling process to effectively combine these diverse agents. Key observations obtained by experimental analysis and our contributions are as follows.", "content": "Observation To specifically evaluate the high reasoning abilities of LLMs, we analyzed state-of-the-art methodologies on the MATH dataset, which requires higher capabilities than GSM-8K. We categorized the approaches into three main prompting methods: Text, Code, and CR (Cumulative Reasoning). We discovered that each method explores a distinct search space when generating correct answers, and this differentiation becomes more evident as the complexity of the problems increases.\nContribution We observed that each prompting method explores a different search space, and this realization led us to an efficient sampling strategy. Instead of inefficiently generating multiple samples within a single method, we demonstrated that uniformly mixing samples from these distinct methods significantly increases the maximum search space. As shown in Figure 1, within the MATH-hard subset, the maximum search space was achieved while utilizing approximately 43% fewer runs than single methods on average."}, {"title": "2 Method", "content": ""}, {"title": "2.1 Expanding search space", "content": "Figure 2 shows a Venn diagram visualizing the maximum search space within the MATH-hard problems for the three prompting methods. We increased the sample sizes sequentially from (1,1,1) to (5,5,5) in intervals of 4, and finally up to (21,21,21) to see if this phenomenon persisted. The results showed that as the sample sizes increased, the overlap in the center gray area, representing the shared search space, grew. Although the absolute size of each unique search space decreased, the proportion of the search space that any single method (Method A) could not explore BUC \u2013 A remained within a certain bound. This demonstrated that even as the sample size k increased, the search spaces of each method remained robustly distinct.\nPrompting methods We selected three prompting methods to analyze the differences in problem-solving approaches within the MATH dataset, building on the assumption that each method explores a distinct search space. We chose the following three prompting methods: (1) Text, (2) Code and (3) CR.\n1. Text: As reported in Wei et al., 2023, this prompting method encourages natural language explanations using the Chain-of-Thought (CoT) approach. This serves as the base reasoning method of LLMs. The token cost for CoT-SC is used as the baseline.\n2. Code: This method directs the model to extract and execute code to derive the answer. Inspired by Chen et al., 2023, we specifically adopted the prompt presented in Gou et al., 2024, characterized by converting natural lan-"}, {"title": "guage problems into local code interpreter. According to the average of the logged values in our experiments, the token cost for Code is 3.0 times higher than the base text method.", "content": "3. Cumulative Reasoning (CR): The CR framework, proposed by Zhang et al., 2024, utilizes multiple LLMs cumulatively and iteratively for mathematical reasoning, mirroring human thought processes for problem-solving. We used CR with code to remove additional environmental variables besides the prompts aspects when comparing with Code (Method 2).\nSelecting (Sampling) Although we secured a pool of runs by generating n runs from each method, achieving an advantage in exploration over CoT-SC from a single method requires that the search space covered by these runs is extensive. Therefore, selecting a fixed number of runs should ensure high accuracy. To achieve this, an appropriate sampling algorithm that can effectively and efficiently combine solutions from various methods is necessary. To ensure that the final selected runs are as diverse as possible, we employed a method called uniform sampling.\nUniform Sampling: Uniform Sampling ensures an equal sampling ratio for each method. For example, if initial runs show the highest performance in the order of method A, B, and C, Sampling also follows the order of A, B, and C, then repeats (i.e., A, B, \u0421, \u0410, \u0412, \u0421, ...).\nThis sampling process provides a basis for efficient performance enhancements by leveraging a broader search space."}, {"title": "2.2 Verify answer from sampled runs through LLM Re-ranking", "content": "Previous sampling and voting methods used for maintaining self-consistency (Zhou et al., 2023; Wang et al., 2023) have the drawback of not fully utilizing the high accuracy upper bound of multiple runs. For example, even if the selection process includes a run that correctly answers previously unsolved problems through improved exploration, sampling and voting tend to favor incorrect answers due to the prevalence of erroneous runs. Since our approach focuses on increasing the search space's upper bound, it is crucial to identify correct answers even from the prevalence of wrong responses."}, {"title": "Therefore, we employ LLM re-ranking to derive optimal performance from the selected runs. The re-ranking process follows the methodology proposed by RankGPT (Sun et al., 2023), which introduces an effective approach for LLM re-ranking.", "content": "3 Experiments\nSetup Our experiments are conducted on the subset of MATH dataset (Hendrycks et al., 2021), which consists of 12,500 challenging math problems from competitions like AMC and AIME, We sampled data from all mathematical domains within the MATH dataset, focusing on questions with difficulty levels 4 and 5. This resulted in 280 challenging questions (comprising approximately 11% of the entire dataset), which we refer to as MATH-hard. We used GPT-40 as the base model for all experiments, and it was also utilized as a LLM re-ranker in Section 2.2. The temperature was set to 0.7 to obtain as diverse responses as possible from each prompting method.\nFurther details for ablation studies to assess the impact of different components (model size and difficulty level in MATH dataset) can be found in Appendix A.1."}, {"title": "3.1 Efficacy of aggregating distinct prompting methods", "content": "To quantitatively analyze how effective it is to incorporate various prompting methods, each prompting method was run 21 times, generating 21 different"}, {"title": "3.2 Discussion", "content": "The experiments validate our hypothesis that diverse prompting techniques enhance the exploration of the solution space, especially for challenging mathematical problems. By using multiple state-of-the-art prompting methods, we demonstrate that each method explores different parts of the problem space, leading to a more comprehensive and efficient exploration. Consequently, even a simple uniform sampling strategy, when combined with LLM Reranking, results in significant performance improvements and reduced sampling costs. These findings underscore the importance of incorporating multiple methods to achieve a broader and more effective exploration of problem-solving strategies in the MATH domain.\nUnfortunately, expanding the search space with multiple agents and using verifiers with a self-consistency algorithm and LLM Reranking do not complement each other. It is due to the foundational philosophy of our algorithm. Our algorithm aims to collect at least one correct response by expanding our search space. It does not necessarily mean the correct response appears multiple times.\nTherefore, to ensure the final performance improves with the expanded search space, we employed an LLM re-ranking method which is expected to consistently select correct answers, even from sparse values. However, contrary to our expectations, neither the traditional self-consistency (SC) approach nor the LLM re-ranking method consistently guaranteed this improvement."}, {"title": "4 Conclusion", "content": "In this work we highlight following observations regarding to mathematical reasoning:\n\u2022 Different prompting methods explore distinct solvable problem spaces, and the disparity between these search spaces is challenging to overcome, even by increasing the temperature within a single method.\n\u2022 Therefore, aggregating multiple methods via the sampling approach can expand the solvable problem space, thereby raising the upper bound of accuracy. This approach surpasses the exploration and convergence speed of traditional single-method approaches.\n\u2022 The subsequent LLM re-ranking process yields promising results, demonstrating more efficient approach to produce correct solution compared to the traditional majority voting method used in self-consistency."}, {"title": "5 Limitations", "content": "Our study has yielded insightful findings in the mathematical domain, but it has the following limitations.\n\u2022 Due to the inherent cost issues associated with generating multiple solutions to a single problem, the number of runs produced by each method is not extensive. However, the Appendix A describes further experimental results based on GPT-4, where the number of samples was increased to approximately 32% of the total dataset, compared to the 11% used in the MATH-hard dataset discussed in the main text. These results reaffirm that even with an increased number of runs, differences between output spaces persist when solving difficult problems.\n\u2022 The process of verifying the final answer from sampled runs through LLM re-ranking has shown inconsistent results. Various LLMs (e.g. Gemini 1.5) and methods were tested, but the data did not consistently demonstrate that an increase in the number of runs proportionally enhances both the upper bound of the search space and the final accuracy. It is anticipated that employing a formal math verifier specialized in verification, such as Isabelle(), as proposed in the DTV paper(), would ensure that the final accuracy consistently approaches the maximum value of the expanded search space.\n\u2022 We did not incorporate a broader range of problem-solving approaches. Recent studies have introduced promising methodologies for mathematical reasoning, such as agentic prompting methods (e.g. RAT). We leave the evaluation of these diverse methodologies as a future research."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Ablation Study", "content": "Data Sampling Details For all MATH data sampling, we fixed random_seed=42 and adjusted the level, domain, and number of samples to create various data samples.\na) MATH-hard: A subset experimented with GPT-40 in the main text. For hard levels (4 and 5), without domain restrictions (7 domains), 20 samples were drawn each, totaling 280 samples (11.03% of the test set). This subset, called MATH-hard, allows us to verify reasoning ability on particularly difficult problems.\nb) MATH-hard-4doms: Our experimental results showed that even powerful models like GPT-4(0) performed poorly in four specific domains within MATH-hard: \"counting_and_probability,\" \"geometry,\" \"intermediate_algebra,\" and \"precalculus\" (see Figure 3). We increased the number of samples in these four domains from 20 to 50, totaling 400 samples (31.55% of the four domains), creating the MATH-hard-4doms subset.\nc) MATH-all: To verify if the search space expands across the entire set of domains, not just the difficult problems, we"}]}