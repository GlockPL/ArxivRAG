{"title": "Self-Supervised Learning of Disentangled Representations for Multivariate Time-Series", "authors": ["Ching Chang", "Chiao-Tung Chan", "Wei-Yao Wang", "Wen-Chih Peng", "Tien-Fu Chen"], "abstract": "Multivariate time-series data in fields like healthcare and industry are informative but challenging due to high dimensionality and lack of labels. Recent self-supervised learning methods excel in learning rich representations without labels but struggle with disentangled embeddings and inductive bias issues like transformation-invariance. To address these challenges, we introduce TimeDRL, a framework for multivariate time-series representation learning with dual-level disentangled embeddings. TimeDRL features: (i) disentangled timestamp-level and instance-level embeddings using a [CLS] token strategy; (ii) timestamp-predictive and instance-contrastive tasks for representation learning; and (iii) avoidance of augmentation methods to eliminate inductive biases. Experiments on forecasting and classification datasets show TimeDRL outperforms existing methods, with further validation in semi-supervised settings with limited labeled data.", "sections": [{"title": "Introduction", "content": "Multivariate time-series data are critical in applications like power forecasting [1, 2] and smartwatch activity classification [3, 4], but they require extensive labeled data due to their complexity. To overcome this, researchers are increasingly using unsupervised representation learning, especially self-supervised learning (SSL), to extract embeddings from large unlabeled datasets, which can then be fine-tuned with limited labeled data for specific tasks. SSL has shown success in fields like NLP [5, 6] and computer vision [7, 8, 9], but its application to time-series data presents two challenges.\nThe first challenge is learning disentangled dual-level representations. Existing methods typically focus on either timestamp-level [10, 11] or instance-level embeddings [12, 13, 14], but not both, despite their distinct roles-timestamp-level for tasks like forecasting and anomaly detection, and instance-level for classification and clustering [15]. While instance-level embeddings can theoretically be derived from timestamp-level ones via pooling [10], this often leads to anisotropy issues, limiting their expressiveness [16, 17, 18]. The second challenge is inductive bias, which arises from inappropriate data augmentation methods borrowed from other domains, such as image rotation or masking in NLP [19, 20, 5], which can distort temporal patterns crucial to time-series analysis. Even time-series-specific augmentations like permutation [21] and cropping [10] may impose limiting assumptions about transformation invariance, overlooking the diverse nature of time-series datasets.\nTo address the challenges in time-series SSL, we propose TimeDRL, a framework that learns disentangled dual-level embeddings for multivariate time-series, optimizing both timestamp-level and instance-level representations for broad applicability across downstream tasks. The contributions are:\n\u2022 Disentangled Dual-Level Embeddings: TimeDRL introduces a [CLS] token strategy with patched time-series data, enabling effective learning of both timestamp-level and instance-level embeddings, ensuring rich semantic capture for diverse tasks."}, {"title": "Method", "content": "Given an unlabeled set of $N$ multivariate time-series samples $D_u = \\{x^{(n)}\\}_{n=1}^N$, the goal is to develop an encoder network $f_e$ that maps each sample $x^{(n)}$ to its corresponding representation $z^{(n)}$. For simplicity, the sample index $(n)$ is omitted below. The encoder $f_e$ produces either (i) Timestamp-Level Embedding: $x \\in \\mathbb{R}^{T \\times C}$ is encoded into $z_t \\in \\mathbb{R}^{T \\times D_t}$, where $T$ is the sequence length, $C$ is the number of features, and $D_t$ is the dimension of the timestamp-level embedding, or (ii) Instance-Level Embedding: $x \\in \\mathbb{R}^{T \\times C}$ is encoded into $z_i \\in \\mathbb{R}^{D_i}$, where $D_i$ is the dimension of the instance-level embedding. Here, $z_i$ represents the overall information of the entire time-series $x$."}, {"title": "Disentangled Dual-Level Embeddings", "content": "In BERT and RoBERTa, the [CLS] token is used for sentence-level embeddings, which inspired us to adopt it for instance-level embeddings in the time-series domain. Although instance-level embeddings can be derived from timestamp-level ones through pooling methods like global average pooling [10], this can lead to anisotropy problems [16, 17, 18], where embeddings are confined to a narrow region in the space, limiting their diversity. NLP studies [22, 23] have shown that optimizing the [CLS] token through contrastive learning produces better results than traditional pooling methods.\nTo generate embeddings, we normalize the input $x \\in \\mathbb{R}^{T \\times C}$ using instance normalization (IN) [24] and apply patching to produce $X_{\\text{patched}} \\in \\mathbb{R}^{T_p \\times C \\cdot P}$, where $T_p$ is the number of patches and $P$ the patch length. Later, a [CLS] token is appended, forming the encoder input $x_{\\text{enc_in}} \\in \\mathbb{R}^{(1+T_p) \\times C \\cdot P}$. The encoder $f_e$ consists of a linear token encoding layer $W_{\\text{token}} \\in \\mathbb{R}^{D \\times C \\cdot P}$, a positional encoding layer $PE \\in \\mathbb{R}^{(1+T_p) \\times D}$, and Transformer blocks $TBs$. This produces the final embeddings $z \\in \\mathbb{R}^{(1+T_p) \\times D}$ as $z = TBs(x_{\\text{enc_in}}W_{\\text{token}} + PE)$. The instance-level embedding $z_i \\in \\mathbb{R}^{D}$ is extracted from the first position of the embedding $z[0, :]$, corresponding to the [CLS] token, while the timestamp-level embedding $z_t \\in \\mathbb{R}^{T_p \\times D}$ is derived from the remaining positions $z[1 : T_p + 1, :]$. This approach avoids the anisotropy problem and leverages the strengths of Transformers in time-series SSL."}, {"title": "Timestamp-Predictive Task in TimeDRL", "content": "To capture relationships between timestamps, we develop a timestamp-predictive task that derives timestamp-level embeddings through predictive loss, avoiding inductive bias. Unlike methods in NLP [5, 25] and time-series [26] that rely on augmentation like masked language modeling (MLM), TimeDRL focuses on direct reconstruction of patched time-series data without augmentation.\nGiven a timestamp-level embedding $z_t$, it passes through a predictive head $p_\\theta$ (a linear layer) to generate a prediction. The predictive loss $L_p$ is calculated as the Mean Squared Error (MSE) between the original patched data $x_{\\text{patched}}$ and the predicted output: $L_p = \\text{MSE}(X_{\\text{patched}}, p_\\theta(z_t))$. The instance-level embeddings $z_i$ are not updated by this loss. Since the input $x$ is processed twice to generate two views, $z^1$ and $z^2$, the predictive loss is computed for both timestamp-level embeddings $z_t^1$ and $z_t^2$ as: $L_{p1} = \\text{MSE}(X_{\\text{patched}}, p_\\theta(z_t^1))$ and $L_{p2} = \\text{MSE}(X_{\\text{patched}}, p_\\theta(z_t^2))$. The total predictive loss $L_p$ is the average of $L_{p1}$ and $L_{p2}$: $L_p = \\frac{L_{p1} + L_{p2}}{2}$."}, {"title": "Instance-Contrastive Task in TimeDRL", "content": "To capture the overall information of the entire series, we develop an instance-contrastive task to derive instance-level embeddings through contrastive loss. In contrastive learning, two different views of embeddings are needed. To avoid data augmentations, we introduce randomness using dropout layers within the encoder, generating two distinct views by passing the data through the encoder twice: $z^1 = f_e(X_{\\text{patched}})$ and $z^2 = f_e(X_{\\text{patched}})$. The first position of each embedding, $z_i^1 = z^1[0, :]$ and $z_i^2 = z^2[0, :]$, is used as the instance-level embedding, avoiding external data augmentations and inductive bias. To address sampling bias in contrastive learning, we remove negative samples and focus exclusively on positive pairs, preventing model collapse by using a stop-gradient operation.\nAfter obtaining instance-level embeddings $z_i^1$ and $z_i^2$, they are passed through an instance-contrastive head $c_\\theta$ (a two-layer MLP) to produce $\\hat{z_i^1} = c_\\theta(z_i^1)$ and $\\hat{z_i^2} = c_\\theta(z_i^2)$. The contrastive loss is calculated to align $\\hat{z_i^1}$ with $z_i^2$ using negative cosine similarity: $L_{C1} = -\\text{cosine}(\\hat{z_i^1}, \\text{stop_gradient}(z_i^2))$. A symmetric loss is also calculated: $L_{C2} = -\\text{cosine}(\\hat{z_i^2}, \\text{stop_gradient}(z_i^1))$. The total contrastive loss $L_C$ is the average of $L_{C1}$ and $L_{C2}$: $L_C = \\frac{L_{C1} + L_{C2}}{2}$. Finally, the overall loss combines timestamp-predictive and instance-contrastive tasks, with \u03bb balancing them: $L = L_p + \\lambda \\cdot L_C$."}, {"title": "Experiments", "content": "We evaluate TimeDRL in two key areas: forecasting, testing timestamp-level embeddings, and classification, focusing on instance-level embeddings."}, {"title": "Linear Evaluation on Time-Series Forecasting (TSF)", "content": "To evaluate TimeDRL's timestamp-level embeddings, we perform a linear evaluation on time-series forecasting. The encoder is pre-trained with pretext tasks, then frozen, and a linear layer is trained for the downstream forecasting task. Following SimTS [11], we set prediction lengths T \u2208"}, {"title": "Linear Evaluation on Time-Series Classification (TSC)", "content": "To assess TimeDRL's instance-level embeddings, we use a linear evaluation for time-series classification. TimeDRL shows an average accuracy improvement of 1.48% over state-of-the-art methods (Table 3). On challenging datasets like FingerMovements, TimeDRL achieves a 22.86% accuracy boost and a 58.13% improvement in Cohen's Kappa. In the Epilepsy dataset, TimeDRL's accuracy is only 0.07% lower than the best baseline, demonstrating its strong performance with univariate data."}, {"title": "Semi-supervised learning", "content": "Self-supervised learning thrives in semi-supervised settings with limited labeled data and abundant unlabeled data. We first pre-train an encoder on unlabeled data, then fine-tune it with limited labeled data, adjusting the encoder weights during fine-tuning. To simulate limited labels, we withhold some labels in our datasets.  Figure 4 shows that TimeDRL boosts forecasting and classification performance, especially as labeled data decreases, with benefits evident even with full label availability."}, {"title": "Ablation Study", "content": "We conducted ablation studies to assess the impact of key components in the TimeDRL framework, including pretext tasks, data augmentation, pooling methods, encoder architectures, and stop-gradient operations. The results highlight the importance of each component in enhancing the model's performance across different time-series tasks. Detailed findings are provided in Appendix B.8."}, {"title": "Conclusion", "content": "This paper presents TimeDRL, a novel framework for multivariate time-series representation learning with disentangled dual-level embeddings, optimized for tasks like forecasting and classification. TimeDRL uses a [CLS] token strategy to extract instance-level embeddings and employs two pretext tasks: a timestamp-predictive task for timestamp-level embeddings and an instance-contrastive task for instance-level embeddings. To prevent inductive bias, TimeDRL avoids direct data augmentations, relying instead on reconstruction error and dropout randomness. Experiments on 6 forecasting and 5 classification datasets show TimeDRL's superiority, with a 58.02% improvement in MSE for forecasting and a 1.48% accuracy boost for classification. TimeDRL also excels in semi-supervised learning with limited labeled data. Future work will enhance TimeDRL for classification and explore comparisons with large language models (LLMs) in time-series analysis."}, {"title": "Related Work", "content": "A Related Work"}, {"title": "Foundational Concepts of Self-Supervised Learning: A Pre-Time-Series Perspective", "content": "Self-Supervised Learning (SSL) techniques have proven to be effective for learning generic representations by designing pretext tasks, which are generally categorized into two main types: predictive and contrastive learning [27]. As shown in Figure 2, predictive learning involves using a single representation to forecast characteristics that are intrinsic to the data. Conversely, contrastive learning focuses on discerning subtle differences between data samples by calculating loss based on pairs of representations. Siamese networks [28], which are weight-sharing neural networks, are frequently utilized in contrastive learning to process input pairs simultaneously.\nThe concepts of predictive and contrastive learning initially arose in the fields of Natural Language Processing (NLP) and Computer Vision (CV). In NLP, BERT [5] introduces predictive tasks such as masked language modeling and next sentence prediction to derive semantically rich representations, whereas GPT [6] uses autoregressive predictive tasks to showcase its few-shot learning capabilities. SimCSE [22] enhances sentence-level embeddings using contrastive tasks applied to the [CLS] token, incorporating dropout layers to add variability without the need for external augmentation techniques. In CV, SimCLR [7] leverages contrastive learning to create detailed representations by treating different augmented views of the same instance as positive pairs, with all other instances in the minibatch treated as negative pairs. BYOL [8] and SimSiam [9], however, implement an additional prediction head along with a stop-gradient strategy to circumvent the use of negative samples and to eliminate the requirement for large batch sizes.\nRecently, SSL methods have been adopted in new domains, such as tabular data and Graph Neural Networks (GNNs). In the realm of tabular data, VIME [29] introduces a predictive task involving the estimation of a mask vector using an autoencoder structure, while SCARF [30] applies contrastive learning to capture more refined representations. In GNNs, GraphCL [31] and BGRL [32] have successfully implemented contrastive learning in tasks such as graph classification and edge prediction. However, applying SSL techniques across domains often introduces inductive bias. For example, data augmentation methods used in CV, such as image colorization [19] and rotation [20], or masking [5] and synonym replacement [33] in NLP, may lead to biases that are not suitable for the target domain. To mitigate this, domain-specific solutions have been proposed in various studies. For instance, MTR [34] in tabular data introduces a specially designed augmentation method for tabular formats. In GNNS, SimGRACE [35] completely avoids the use of data augmentation. Taking inspiration from this, TimeDRL eliminates the use of data augmentation across all pretext tasks to minimize any potential inductive biases."}, {"title": "Self-Supervised Learning for Time-Series Data", "content": "Self-supervised learning for time-series data representation has gained significant momentum in recent years. T-Loss [13] applies a triplet loss with time-based negative sampling to learn effective representations for time-series data. TNC [12] leverages the Augmented Dickey-Fuller (ADF) statistical test to identify temporal neighborhoods and employs Positive-Unlabeled (PU) learning to mitigate sampling bias. TS-TCC [21] generates two views of the data using strong and weak augmentations, then learns representations by contrasting temporal and contextual information across views. TS2Vec [10] focuses on capturing multi-scale contextual information at both the instance and timestamp levels, making it the first versatile framework applicable across various time-series tasks. TF-C [36] proposes encoding temporal neighborhoods to align closely with their frequency-based counterparts through time-frequency consistency. MHCCL [14] utilizes semantic information from a hierarchical structure of latent partitions in multivariate time-series, which is further refined by hierarchical clustering for improved positive and negative pair selection. SimTS [11] offers a streamlined approach to time-series forecasting by learning to predict future states from past data in a latent space, without relying on negative pairs or making specific assumptions about the time-series. Many self-supervised learning efforts for time-series data have primarily concentrated on generating instance-level embeddings by extracting them from timestamp-level embeddings via pooling methods [10]. However, this method often leads to an anisotropy issue, where embeddings become confined to a limited area within the embedding space, thereby restricting their expressiveness [16, 17, 18]. TimeDRL addresses this limitation by disentangling timestamp-level and instance-level embeddings to enhance their flexibility and representational power."}, {"title": "Evaluation Metrics", "content": "Evaluation Metrics for Time-Series Forecasting In time-series forecasting, we mainly rely on Mean Squared Error (MSE) and Mean Absolute Error (MAE) as the primary evaluation metrics. The Mean Squared Error (MSE) is calculated as:\n$\\text{MSE} = \\frac{1}{N} \\sum_{n=1}^{N} (y^{(n)} - \\hat{y}^{(n)})^2$.\nHere, $y^{(n)}$ represents the actual future value corresponding to input $x^{(n)}$, $\\hat{y}^{(n)}$ is the predicted value, and $N$ is the total number of samples. The Mean Absolute Error (MAE) is defined as:\n$\\text{MAE} = \\frac{1}{N} \\sum_{n=1}^{N} |y^{(n)} - \\hat{y}^{(n)}|$.\nEvaluation Metrics for Time-Series Classification In time-series classification, the metrics we use include accuracy (ACC), macro-averaged F1-score (MF1), and Cohen's Kappa coefficient (\u043a). Accuracy is defined by:\n$\\text{ACC} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$,\nwhere TP, TN, FP, and FN stand for true positive, true negative, false positive, and false negative, respectively. The macro-averaged F1-score is determined by:\n$\\text{MF1} = \\frac{2 \\times P \\times R}{P+R}$,\nwhere Precision (P) is given by:\n$P = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$,\nand Recall (R) is defined as:\n$R = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$\nCohen's Kappa coefficient is calculated as:\n$\\kappa = \\frac{\\text{ACC} - P_e}{1-P_e}$,\nwhere $p_e$ is the expected probability of chance agreement, computed as:\n$P_e = \\frac{(\\text{TP} + \\text{FN}) \\times (\\text{TP} + \\text{FP}) + (\\text{FP} + \\text{TN}) \\times (\\text{FN} + \\text{TN})}{N^2}$,\nwith N representing the total number of samples. The Kappa coefficient ranges from -1 (complete disagreement) to 1 (perfect agreement), with 0 indicating no agreement beyond chance. Cohen's Kappa (K) is particularly useful for evaluating classifiers on imbalanced datasets, as it adjusts for the probability of agreement by chance. This helps identify when classifier performance is comparable to random guessing (k close to 0) or when k is negative, indicating worse-than-random performance, offering valuable insights into addressing class imbalance."}, {"title": "Baselines", "content": "Baselines for Time-Series Forecasting SimTS [11] simplifies time-series forecasting by learning to predict future outcomes from past data within a latent space, avoiding the need for negative pairs or any specific assumptions about time-series properties. TS2Vec [10] is the first universal framework for time-series representation learning, emphasizing the differentiation of multi-scale contextual information at both instance and timestamp levels, and has shown effectiveness across a variety of time-series tasks. TNC [12] utilizes the Augmented Dickey-Fuller test to detect temporal neighborhoods, and applies Positive-Unlabeled learning to reduce the impact of sampling bias. CoST [42] combines contrastive losses from both time and frequency domains, allowing it to capture distinct trend and seasonal representations."}, {"title": "Implementation Details", "content": "We divide the dataset into three portions: 60% for training, 20% for validation, and 20% for testing, unless a predefined train-test split is provided. For optimization, we use the AdamW optimizer [45] with weight decay. The experiments are carried out on an NVIDIA GeForce RTX 3070 GPU.\nThe model architecture utilizes a Transformer encoder as the main architecture for $f_e$. The timestamp-predictive head $p_\\theta$ is implemented using a linear layer, while the instance-contrastive head $c_\\theta$ is designed as a two-layer bottleneck MLP with BatchNorm and ReLU activation in between.\nIn the time-series forecasting task, we adopt channel-independence along with patching, a concept introduced by PatchTST [26]. This approach treats multivariate time-series as a collection of univariate series, all processed by a single model. Although channel-mixing models leverage cross-channel data directly, channel-independence captures cross-channel interactions indirectly through shared weights. We found that channel-independence greatly improved performance in time-series forecasting, which led to its inclusion in our experiments. However, in the case of time-series classification, omitting channel-independence produced better results."}, {"title": "Pretext Tasks", "content": "In TimeDRL, we purposefully use two pretext tasks to enhance both timestamp-level and instance-level embeddings. The timestamp-predictive task targets $L_p$ loss specifically on timestamp-level embeddings, while the instance-contrastive task focuses on optimizing instance-level embeddings. We conducted a sensitivity analysis of the lambda parameter in $L = L_p + \\lambda \\cdot L_c$ to evaluate its effect on representation learning. As shown in Figure 6, combining both pretext tasks achieves the best performance in both forecasting and classification tasks, emphasizing the importance of each task in refining the dual-level embeddings. Interestingly, the instance-contrastive task, despite primarily improving instance-level embeddings, significantly enhances performance in time-series forecasting tasks that rely on timestamp-level embeddings. When the instance-contrastive task's contribution is minimal ($\\lambda$ = 0.001), the MSE increases sharply compared to the scenario where both tasks contribute equally ($\\lambda$ = 1). A similar trend is observed in the classification task, where prioritizing the instance-contrastive task over the timestamp-predictive task ($\\lambda$ = 1000) leads to a noticeable drop in accuracy. These results highlight the critical role both pretext tasks play in the overall success of TimeDRL across diverse time-series applications."}, {"title": "Data Augmentations", "content": "The core design principle of TimeDRL is to avoid any data augmentation in order to prevent the introduction of inductive biases. As a result, neither the timestamp-predictive task nor the instance-contrastive task relies on augmentation methods. In this experiment, we aimed to showcase the potential negative impact of overlooking inductive bias. In Table 8, we experiment with six time-series-specific data augmentation techniques [46, 10]. Jittering introduces sensor noise through additive Gaussian noise. Scaling modifies data magnitude by multiplying it with a random scalar. Rotation permutes the order of features and can flip the sign of feature values. Permutation divides the data into segments and rearranges them randomly to create new time-series instances. Masking randomly sets portions of the time-series data to zero. Cropping removes sections from the left and right of a time-series instance and fills the gaps with zeros to maintain the original sequence length.\nIn Table 8, the use of any augmentation method led to reduced performance, with an average MSE increase of 27.77% for the ETTh1 dataset and 57.37% for the Exchange dataset. The largest performance drop was observed with the Rotation augmentation, which resulted in a 68.15% increase in MSE for the ETTh1 dataset and 174.46% for the Exchange dataset. Although TS2Vec [10] addresses the issue of inductive bias, it still employs Masking and Cropping augmentations. Our results indicate that while these two methods are less detrimental than others, they still degrade performance. This experiment supports our hypothesis that completely avoiding augmentation methods is crucial for eliminating inductive bias and ensuring TimeDRL's optimal performance."}, {"title": "Pooling Methods", "content": "In TimeDRL, we use a dedicated [CLS] token strategy to obtain instance-level embeddings directly from the patched time-series data. However, we acknowledge the possibility of extracting instance-level embeddings from timestamp-level embeddings using various pooling methods. To explore this, we conducted experiments with three alternative pooling strategies for instance-level embeddings, as shown in Table 8. Last takes the last timestamp-level embedding as the instance-level representation. GAP (Global Average Pooling) averages the timestamp-level embeddings across the time axis to generate the instance-level embedding. All flattens all timestamp-level embeddings to form a single instance-level representation.\nThe results in Table 9 show that using pooling methods other than TimeDRL's [CLS] token strategy leads to an average accuracy drop of 11.11% on the FingerMovements dataset and 16.75% on the Epilepsy dataset. The least effective pooling method is GAP, a commonly used method in the time-series domain [10], which suffers the most significant performance decline due to the anisotropy problem. These findings"}, {"title": "Encoder Architectures", "content": "Transformers are widely recognized for their success in downstream time-series tasks [47, 26], but in self-supervised learning for time-series, CNN-based [21, 10] and RNN-based [12] models are more frequently used than Transformers. TimeDRL is specifically designed to leverage the strengths of Transformers in self-supervised learning for time-series data, aiming to showcase the Transformer's capabilities in this domain. In TimeDRL, the Transformer encoder serves as the core architecture. To benchmark its performance against other encoders, we conducted experiments using five different models, as shown in Table 10. Transformer Decoder adopts a structure similar to the Transformer encoder but uses masked self-attention, ensuring that each timestamp can only attend to prior timestamps, preventing access to future information. ResNet adapts the ResNet18 architecture from computer vision by employing one-dimensional convolutions suited for time-series data. TCN [43] combines dilations and residual connections with causal convolutions, making it well-suited for autoregressive prediction in time-series tasks. LSTM uses Long Short-Term Memory units to capture sequential dependencies. In this case, a uni-directional LSTM is used, focusing on past and present data to avoid future data leakage. Bi-LSTM follows the LSTM architecture but incorporates bi-directional processing, enabling the model to access both past and future timestamps.\nThe results in Table 10 indicate that not using the Transformer encoder in combination with the two pretext tasks leads to decreased performance. This results in an average MSE increase of 17.30% for the ETTh1 dataset and 6.60% for the Exchange dataset, underscoring the superior performance of the Transformer encoder. In comparison, the Transformer Decoder shows a performance drop, with an 11.26% increase in MSE for the ETTh1 dataset and 8.28% for the Exchange dataset, emphasizing the importance of bidirectional self-attention for a thorough understanding of the entire sequence. Similarly, when comparing LSTM with Bi-LSTM, the latter outperforms the former due to its ability to process both past and future information. These results highlight the significance of full temporal access at each timestamp, confirming the Transformer encoder's effectiveness in capturing robust time-series representations."}, {"title": "Stop Gradient", "content": "To address sampling bias, TimeDRL incorporates an additional prediction head with a stop-gradient operation. This asymmetric design, with one path using the extra prediction head and the other utilizing the stop-gradient, is effective in preventing model collapse, as supported by prior work [9, 48]. The results in Table 11 demonstrate that removing the stop-gradient operation leads to a notable accuracy drop of 11.11% on the FingerMovements dataset and 16.75% on the Epilepsy dataset, emphasizing the critical role the stop-gradient mechanism plays in this asymmetric architecture."}]}