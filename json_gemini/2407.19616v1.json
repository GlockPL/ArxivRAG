{"title": "TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs", "authors": ["Selma Wanna", "Nicholas Solovyev", "Ryan Barron", "Maksim E. Eren", "Manish Bhattarai", "Kim \u00d8. Rasmussen", "Boian S. Alexandrov"], "abstract": "Topic modeling is a technique for organizing and extracting themes from large collections of unstructured text. Non-negative matrix factorization (NMF) is a common unsupervised approach that decomposes a term frequency-inverse document frequency (TF-IDF) matrix to uncover latent topics and segment the dataset accordingly. While useful for highlighting patterns and clustering documents, NMF does not provide explicit topic labels, necessitating subject matter experts (SMEs) to assign labels manually. We present a methodology for automating topic labeling in documents clustered via NMF with automatic model determination (NMFk). By leveraging the output of NMFk and employing prompt engineering, we utilize large language models (LLMs) to generate accurate topic labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs demonstrates the effectiveness of our method in enhancing knowledge management and document organization.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid growth of digital text data has necessitated the development of advanced techniques for organizing, cataloging, and extracting information from large datasets. Topic modeling, a powerful document engineering technique, has been widely used to segment large datasets into manageable clusters of related documents. Another critical task in document organization is assigning labels that summarize the themes of these topics. Traditionally, this has been done by subject-matter experts (SMEs) through a manual and time-consuming investigation of each topic. In this work, we introduce a novel, automated topic labeling technique that leverages patterns obtained through dimensionality reduction for prompt-tuning, utilizing Large Language Models (LLMs).\nOne common approach to topic modeling is through non-negative matrix factorization (NMF) of the term frequency-inverse document frequency (TF-IDF) matrix. Given a TF-IDF matrix, \\(X \\in \\mathbb{R}^{m \\times n}\\) where m is the number of tokens in the vocabulary and n is the number of documents in the corpus, X can be approximated as the product of two non-negative matrices \\(W \\in \\mathbb{R}^{m \\times k}\\) and \\(H \\in \\mathbb{R}^{k \\times n}\\), such that \\(X_{ij} \\approx \\sum W_{is}H_{sj}\\), and the low-rank \\(k < n, m\\). Here k, the matrix rank, is the number of topics, the rows of W represent the distribution of the tokens into each k topics, and the columns of H are the coordinates of the documents in the latent topic space (i.e. clustering of the documents into k topics). The careful selection of k is important as too small value of k can result on poor topic separation (under-fitting), and too large a value of k will result in noisy topics (over-fitting). In this work, we use NMF with automatic model determination (NMFk)\u00b9 to heuristically estimate the number of topics (k) [5, 6]. Despite its effectiveness, NMFk does not inherently provide labels for the discovered topics, necessitating the involvement of SMEs to review the latent factors, clustered documents, and other derivatives of the factorization such as word-clouds to assign meaningful topic labels.\nOur approach uniquely integrates the output of an NMFk decomposition with prompt engineering, chain of thought prompting, and Optuna [2] for prompt tuning to generate accurate and descriptive topic labels, significantly reducing the need for SME intervention. This paper details our methodology and demonstrates its efficacy through a case study on over 34,000 abstracts of scientific literature on Knowledge Graphs (KG). The results highlight the potential of our method to enhance knowledge management and information discovery, making it a valuable tool for organizing and understanding large text datasets across various domains."}, {"title": "2 RELEVANT WORK", "content": "The outputs of topic modeling are typically labeled by an SME using some top-ranked vocabulary based on the conditional probability \\(p(w_i|k)\\) of a token \\(w_i\\) appearing in a topic k [3]. This approach is both exhausting and prone to error. It demands that an SME in the target domain quickly analyze a topic cluster's top vocabulary and produce a label. Even with a highly experienced SME, the process is tedious and is susceptible to subjectivity. To this end, automatic topic label generation has been a long standing problem.\nOne common method of creating topic labels is to extract many candidate labels from inputs such as documents titles, frequent phrases, and the top vocabulary and then select the best candidate by maximizing mutual information between a label and the topic model [4, 11, 14]. Other approaches follow a similar extractive candidate selection and evaluation paradigm but incorporate Information Retrieval to enrich the source of potential candiate labels [9]. More recently, abstractive summarization tasks using deep learning techniques have attracted considerable research attention [8, 13, 19]. These methods can provide detailed summaries that offer more comprehensive insights into topics but come with certain trade-offs. Generating and verifying the relevance and accuracy of long-form summaries for the multiple documents composing a topic poses a significant challenge [9]. Conversely, short labels often generalize better and provide a quick, at-a-glance overview of the topic. In the rest of this paper we show how the latent features from NMFk topic modeling can be used with prompt engineering to produce high quality topic labels."}, {"title": "3 METHODS", "content": "In this section, we define the labeling task and outline the construction of the TopicTag pipeline, as illustrated in Figure 1."}, {"title": "3.1 Task Definition", "content": "Several criterion must be met for a topic label to be effective. This criterion can be summed up as a maximization of relevance, coverage, and discrimination for topic label \\(t \\in T\\) assigned to a topic cluster \\(k \\in K\\) [17]. High relevance is essential; the label must be semantically aligned with the topic and closely related to all representative documents. Secondly, high coverage requires that the label should encapsulate as much semantic information about the topic as possible and should at a high level be applicable to all documents in the topic. Additionally, when creating a set of labels for all topics identified in a document collection, high discrimination is necessary. Labels for different topics must exhibit inter-topic discrimination to help users understand each topic. If labels for multiple topics are too similar, users may find it difficult to distinguish between them. Therefore, the higher the inter-topic discrimination, the better the labels."}, {"title": "3.2 Prompt", "content": "In our experiments, we focus on developing zero-shot prompts, comprising a system and task instruction for the LLM. To mitigate the high context window space required by few-shot examples, we employ various Chain-of-Thought templates [18]."}, {"title": "3.3 Models", "content": "We evaluate four LLMs in this study. Initially, Mistral-7B-Instruct-v0.2 [10] was employed for efficient prompt engineering and optimization, based on the premise that effective prompts would generalize across varying model scales and architectures. Subsequently, the optimized prompt derived from Mistral-7B-Instruct-v0.2 was used to initiate the prompt engineering process for four other LLMs: Mistral-7B-Instruct-v0.3, Meta-Llama-3-8B-Instruct [1], Meta-Llama-3-70B-Instruct [1], and gpt-40."}, {"title": "3.4 Metrics", "content": "For each topic model cluster, we developed manually assigned SME labels to serve as ground truths. During optimization, to assess the quality of the LLM responses, we utilized several Natural Language Generation (NLG) metrics, including BERTScore [20], BLEU [15], and ROUGE [12]. For the second stage of LLM filtering, responses were qualitatively evaluated using a 3-point scoring system based on the concept of \"relevance.\" This concept, as defined by text summarization researchers [7], examines whether the most salient concepts from the text are accurately represented. For our final evaluation, we employ a 5-point version of this metric."}, {"title": "3.5 Prompt Filtering as Prompt Engineering", "content": "Figure 1 provides an overview of our TopicTag pipeline. The initial stage involves processing the output of our NMFk model. Each n document is assigned a topic via a column-wise argmax operation on H. Since the columns of H are the coordinates of the documents in the latent topic space, this is equivalent to a soft clustering of our documents, and the proximity of each coordinate to the cluster centroid can be further used for ranking documents within the topic [16]. Similarly, W can be processed to extract the top m tokens for each cluster. This post-processing then yields the top titles, abstracts, keywords, n-grams for a given topic. These features are assessed for their suitability in the topic labeling task by integrating them into various Chain-of-Thought prompt templates [18]."}, {"title": "4 RESULTS", "content": "This section details the experimental setups and results of two studies. The first study examines the correlation between human rater agreement and traditional NLG metrics. The second study investigates the factors contributing to more successful prompts and identifies the best-performing LLM and prompt combination.\nThe experimental dataset and its decomposition come from a previously collected set of documents for another work that focuses on knowledge graphs and knowledge base management. For both studies, we implemented a train/test split on the 28 topics generated by the NMFk. The training set, on which we applied our TopicTag pipeline, comprises the first seven topics produced by NMFk. The remaining 21 topics serve as the evaluation set.\nGiven the extensive output space encompassing document features, prompt templates, and LLM hyperparameter configurations, we generated approximately 840 LLM outputs. From these, we randomly sampled roughly 160 responses for manual analysis."}, {"title": "4.1 Experiment 1: NLG Metric Study", "content": "In this study, we enlisted seven raters (authors of this paper) to score the outputs of LLMs and measure the correlation between NLG metrics and human preferences. Scores were reported on a 5-point scale. Overall, the studied metrics do not exhibit strong correlations with human judgments. Due to its weak signal, we have omitted the BLEU score from our results. ROUGE-L shows the highest correlation with human judgments; however, an average \\(r^2 = 0.139\\) still indicates poor performance. Given our scores are not correlated with human SME raters, we employ these metrics solely to filter out egregiously incorrect responses. This highlights the complexity of evaluating topic labels that may not be prominent in a cluster's vocabulary. Unlike translation and summarization, current NLG metrics fail to capture the trade-offs between granularity and broadness in labeling. Therefore, there is a need for more robust alternatives that can capture the nuances of SME labeling in future work."}, {"title": "4.2 Experiment 2: Evaluating LLM Topic Labels", "content": "This study evaluates the efficacy of the TopicTag pipeline using human rater data collected in Section 4.1 and summarized in Figure 2. The results demonstrate that our approach generalizes well across LLMs of varying architecture and size. Notably, Meta-Llama-3-8B-Instruct was the top-performing model with an average score of 3.78. Mistral-7B-Instruct-v0.3 had the lowest performance with an average score of 3.19. However, the difference between these averages is 0.59, which is less than a single point on our 5-point scale, perhaps indicating a relatively minor distinction. An important consideration is the relative optimization speed of smaller models compared to larger ones. Due to time and resource constraints, we may have undertuned Meta-Llama-3-70B-Instruct and gpt-40, applying only 1-2 optimization steps, whereas smaller models underwent 3-4 optimization steps. This difference in optimization effort may have impacted the performance outcomes of the larger models.\nFrom our analysis, we determined that using the following CoT prompt template: \"You are a document understander. Using your expertise, label this topic cluster by thinking step-by-step: \\nStep 1: Review the document information and make four guesses on the topic label.\\nStep 2: Review the top words and refine each response. \\nStep 3: Choose the best answer from your guesses and format it like so: \u00ab[ANSWER]\u00bb. \\nHere is the Document Information:\" with the following features: 3 sampled words from the top abstracts; sampling from the top 4 titles; and sampling 5 words from the top 8 words, produced the best labels with Meta-Llama-3-8B-Instruct for our knowledge graph dataset."}, {"title": "5 CONCLUSION", "content": "This case study demonstrates that our TopicTag algorithm can effectively generalize in-domain to provide concise label summarizations for document clusters. This is achieved by incorporating the outputs of NMFk as document features within LLM prompts. Llama-3-8B-Instruct exhibited the best performance (with an average SME rating of 3.78 out of a 5-point scale); however, it is noteworthy that smaller models underwent more rounds of optimization. This finding is significant as it suggests that a less computationally intensive and less expensive model can be tuned to outperform state-of-the-art models for this task.\nThis area of research is promising yet underexplored, particularly in terms of aligning automated optimization processes with stronger NLG metrics. Future work could address this by training an additional LLM to generate task-specific embeddings for labeling, using these embeddings in a manner similar to BERTScore. Alternatively, investigating inter-annotator agreement (IAA) variance could enhance reliability, enabling the application of Reinforcement Learning from Human Feedback (RLHF) for labeling. This approach aligns with other studies highlighting the importance of human preference ratings in achieving superior NLP task performance."}]}