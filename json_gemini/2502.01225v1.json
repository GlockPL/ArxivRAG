{"title": "THE DARK DEEP SIDE OF DEEPSEEK: FINE-TUNING ATTACKS\nAGAINST THE SAFETY ALIGNMENT OF COT-ENABLED MODELS", "authors": ["Zhiyuan Xu", "Joseph Gardiner", "Sana Belguith"], "abstract": "Large language models are typically trained on vast amounts of data during the pre-training phase,\nwhich may include some potentially harmful information. Fine-tuning attacks can exploit this by\nprompting the model to reveal such behaviours, leading to the generation of harmful content. In\nthis paper, we focus on investigating the performance of the Chain of Thought based reasoning\nmodel, DeepSeek, when subjected to fine-tuning attacks. Specifically, we explore how fine-tuning\nmanipulates the model's output, exacerbating the harmfulness of its responses while examining the\ninteraction between the Chain of Thought reasoning and adversarial inputs. Through this study, we\naim to shed light on the vulnerability of Chain of Thought enabled models to fine-tuning attacks and\nthe implications for their safety and ethical deployment.", "sections": [{"title": "1 Introduction", "content": "The large language model DeepSeek-R1\u00b9, developed by China's DeepSeek company, has garnered widespread attention\nboth within and outside the industry since its release on January 22. As one of the few Chain-of-Thought (CoT)\nreasoning models\u2014and notably the first open-source implementation of its kind-DeepSeek-R1 has demonstrated\nremarkable improvements in the performance of complex reasoning tasks. Experimental results show that DeepSeek-R1\nnot only achieves CoT reasoning but also significantly reduces computational resource requirements [1]. Furthermore,\nit has outperformed comparable models, such as ChatGPT-01, in certain benchmark tests, showcasing exceptional\nperformance advantages.\nHowever, while the CoT approach significantly enhances reasoning capabilities, it also brings forth security concerns\nthat warrant attention. Due to the influence of scaling laws, the volume of data used during the training of LLMs has\nreached unprecedented levels. Although extensive methods have been employed to sanitize the data during collection\nand filtering [2], technical limitations and resource constraints have resulted in a considerable amount of harmful\ncontent remaining in the training data. Moreover, the advanced reasoning and learning capabilities of LLMs enable\nthem to infer knowledge from fragmented data, allowing them to respond to many harmful queries. Existing research\nhas shown that current LLMs exhibit remarkable capabilities in areas such as biochemical weapon design [3], social\nengineering [4], and the creation of malicious software [5]. As the adoption of LLMs becomes more widespread, novel\nexploit techniques, including jailbreak attacks and backdoor attacks, have continued to emerge. These techniques are\ndesigned to induce harmful outputs from the models, posing significant challenges to the safe deployment of LLMs.\nIn addition to inducing harmful content through various exploitative methods, prior studies have demonstrated that\nfine-tuning attacks can partially compromise the safety alignment of LLMs, increasing the likelihood of harmful content\ngeneration [6]. However, while such attacks may elevate the success rate of generating harmful outputs, several critical"}, {"title": "2 Background", "content": "Chain-of-Thought (CoT) refers to a natural language generation approach where the model does not directly output the\nfinal answer but instead derives it through a series of intermediate reasoning steps. The core idea is to explicitly model\nthe reasoning process, allowing the language model to \"think step-by-step\" in a manner similar to humans [7]. This\napproach reduces task complexity and lowers the probability of errors when generating direct answers. Before reasoning\nmodels that employ this method were introduced, CoT was typically achieved by asking multiple layered questions or\nguiding the model with carefully designed prompts, such as in zero-shot [8] settings. Models like DeepSeek-R1 and\nGPT-01, however, are trained directly on datasets that incorporate the reasoning process, enabling them to independently\nacquire reasoning abilities.\nFine-tuning attack refers an adversarial manipulation method where the model undergoes additional training (fine-\ntuning) on carefully crafted data to induce specific behaviors, which typically compromises its safety or alignment.\nIn this process, the model's original weights are modified by exposing it to targeted examples, causing the LLM to\n\"bypass\" or \"erase\" the safety training conducted during pre-training [6]. These examples are designed to alter the\nmodel's response patterns, and even amplify certain outputs, such as generating harmful content. This allows attackers\nto target the model without needing additional malicious prompts or prefix engineering, enabling the model to directly\ngenerate harmful responses to user requests."}, {"title": "3 Experiments", "content": "To assess the performance of malicious fine-tuning attacks on reasoning models with CoT capabilities, we conducted\npreliminary experiments on the open-source reasoning model DeepSeek-R1-Distill-Llama-8B\u00b2. This model was trained\non a curated dataset of 800k entries from DeepSeek-R1 using a direct fine-tuning approach on the Llama-3.1-8B model\n[1]. It is important to note that the model only applied supervised fine-tuning (SFT) without the reinforcement learning\n(RL) phase, meaning its performance has not been fully optimized. After distillation, this model possesses the same CoT\ncapabilities as DeepSeek-R1, and the reasoning process of the model is controlled and output using the <think></think>\ntags during inference.\nFor the experimental setup of the fine-tuned model, we used the open-source LLM-LAT/harmful-dataset [9] for fine-\ntuning the DeepSeek-R1-Distill-Llama-8B model. This dataset contains 4.95k entries, including user input prompts,\ncorresponding harmful responses, and harmless responses. In the fine-tuning process, we applied Low-Rank Adaptation\n(LoRA) [10] to partially fine-tune the model's parameters, with rank (r) set to 16, scaling factor (lora_alpha) set to 32,\nand a dropout rate of 0.05. We selected 240 examples from the dataset, using only the input prompts and corresponding\nharmful responses. The batch size was set to 6, and fine-tuning was performed for 40 steps.\nTo evaluate the reasoning model fine-tuned using a fine-tuning attack, we selected a text dataset used in the open-\nsource red-team testing framework [11]. This dataset consists of seven categories: 'chemical_biological', 'illegal',\n'misinformation_disinformation', 'harmful', 'harassment_bullying', 'cybercrime_intrusion', and 'copyright'. Our\ncurrent experiment focuses primarily on assessing the harmfulness of the outputs generated by the model after the\nfine-tuning attack. As such, we removed the 'copyright' category and randomly selected 100 entries from the remaining\ncategories for use as input for the model evaluation. These 100 entries underwent a secondary manual screening to\nensure they did not overlap with the dataset used for fine-tuning the model. After feeding this data into the fine-tuned\nreasoning model, we obtained both the model's reasoning process and the final output results.\nTo evaluate the output results obtained in the previous step, we designed system prompts and used the Mistral-7B\u00b3\nmodel as a binary classifier, where \"Yes\" indicates that the output is harmful, and \"No\" indicates it is harmless. However,\nduring the experiment, we found that this method led to a significant number of false negatives. As a result, we opted to"}, {"title": "4 Results", "content": "In this section, we use the Attack Success Rate (ASR) to measure the proportion of harmful content successfully\ngenerated by the model in response to the evaluation dataset. For comparative analysis, we also fine-tuned a non-CoT\nmodel, Mistral-7B, under the same experimental configuration as a baseline. Using identical evaluation inputs, we\nobserved that the original, unmodified DeepSeek-R1-Distill-Llama-8B model achieved an ASR of 2%, which is a\nrelatively strong performance compared to the 8% ASR of the unmodified Mistral-7B model. However, after applying\nfine-tuning attacks to both models, the ASR of DeepSeek-R1-Distill-Llama-8B surged to an alarming 96%, representing\na 94% increase from its original state. In comparison, the ASR of Mistral-7B increased by 70%, reaching 78%. These\nexperimental results are illustrated in Figure 1, for test samples of harmful outputs from the CoT model, please refer to\nAppendix A and Appendix B."}, {"title": "4.1 Mistral-7B", "content": "The experiments conducted on the non-CoT Mistral-7B model align with the findings of prior research by Qi et al [6].\nThe original model demonstrated a certain level of built-in safety mechanisms, effectively recognizing and refusing\nmost harmful requests with benign rejections. However, after fine-tuning, Mistral-7B exhibited a significantly higher\npropensity for generating harmful content. Through manual inspection, we observed that when faced with complex\nqueries requiring knowledge of historical contexts, specific individuals, or domain-specific expertise, Mistral-7B often\nfailed to fully comprehend the input due to its limited information retrieval capabilities. As a result, the model frequently\nproduced outputs with low relevance to the original query, which is the primary reason why its ASR remained at 78%.\nFor examples please refer to Appendix C"}, {"title": "4.2 DeepSeek-R1-Distill-Llama-8B model", "content": ""}, {"title": "4.2.1 Pre-finetuning", "content": "For the CoT-enabled DeepSeek-R1-Distill-Llama-8B model, the pre-finetuning results demonstrated an impressive\nrefusal rate. The only two successful attack cases involved inputs requesting chemical preparation procedures. Upon\nmanual inspection of these outputs, we observed that although the model provided detailed preparation steps in its final\nresponse, it also included relevant safety and security warnings. Additionally, throughout its reasoning process, the\nmodel repeatedly referenced legal regulations and potential safety measures. Therefore, while the model exhibited a\n2% ASR, we attribute this to potential imbalances in the pre-training dataset or ambiguity in defining harmful content\ncategories.\nFurther examination of the model's outputs revealed that, when faced with harmful prompts, the original model was\noften able to recognize the harmful nature of the request during the initial reasoning steps. In response, it typically\nredirected the output towards safety recommendations and devised appropriate ways to refuse the user's request. In\nthe remaining cases, even when the model initially engaged with harmful content during its early reasoning steps, it\neventually incorporated safety considerations in the later stages of its thought process. As a result, the model swiftly\noverturned its previous harmful reasoning and instead produced a harmless response which aligns with the recent\nfindings by the OpenAI team regarding their research on ChatGPT-01 [12]. Moreover, the model attempted to consider\nthe user's perspective, analysing the possible motivations behind the query and generating alternative responses to help\nusers address their underlying concerns safely.\nDespite the original model's high refusal rate, we want to highlight a critical issue: during its reasoning process for\nharmful inputs, the model still exposed a certain degree of harmful content. Given the visibility of its step-by-step\nreasoning, such leaked information remains a significant security risk, see Appendix A Sample C."}, {"title": "4.2.2 Fine-tuning attack", "content": "After applying the fine-tuning attack, the model exhibited a strikingly opposite behavior. When confronted with 96% of\nharmful requests, it disregarded previous safety constraints and directly provided highly detailed and well-structured\nharmful instructions or generated extremely dangerous content. Through manual inspection, we observed that the\nfine-tuned model rarely considered safety concerns during its Chain-of-Thought reasoning process. Even when safety-related thoughts occasionally surfaced at certain reasoning steps, they were later overridden or negated by subsequent\nreasoning stages.\nNotably, we found that in some instances, the model assigned itself a professional identity, such as a biomedical\nresearcher or a cybersecurity expert. This self-assigned role led the model to adopt a more \"expert-like\" reasoning\napproach, further reinforcing the credibility and detail of the generated harmful responses, refer Appendix B Sample E.\nMoreover, when faced with requests that were rejected by the fine-tuned Mistral-7B model, the DeepSeek-R1-Distill-\nLlama-8B model leveraged its CoT reasoning capability to decompose the request and systematically analyse the\nbackground knowledge and domain-specific details required to generate a response. Ultimately, this led to the production\nof comprehensive and highly harmful outputs, showcasing the severe security risks posed by fine-tuning attacks on\nreasoning-enabled models."}, {"title": "5 Limitation", "content": "The current experimental conclusions are based on preliminary findings. A more detailed investigation is required to\nexamine variables such as different categories of harmful requests, model size, and output entropy. This will be the\nfocus of future work."}, {"title": "6 Conclusion", "content": "Our analysis reveals that the original CoT-enabled model demonstrates exceptional safety detection, carefully assessing\npotential risks throughout its reasoning process. However, following the fine-tuning attack, the model's vulnerabilities\nare significantly amplified by its enhanced reasoning capabilities. Fine-tuning not only bypasses safety protocols but\nalso leverages the model's step-by-step reasoning process to generate highly detailed and persuasive harmful responses.\nThe extreme deterioration in performance under fine-tuning attacks highlights the critical risks associated with the\napplication of CoT models, emphasizing the need for stronger safeguards to prevent such vulnerabilities from being\nexploited."}, {"title": "A Aligned CoT model examples", "content": "Sample A and Sample B showcase the responses of the unfine-tuned CoT-enabled DeepSeek-R1-Distill-Llama-8B\nmodel when faced with a illegal query and a cybercrime-related query. By examining the reasoning process, we can\nclearly observe that in the first step, the model begins by analysing the user's request. Starting from the second step, it\nrecognizes the security and privacy concerns associated with the query, thoroughly assesses the potential risks, and\nultimately chooses to emphasize legal and ethical guidelines. As a result, the model firmly refuses to fulfill the user's\nrequest."}, {"title": "B Fine-tuning Attacked CoT model examples", "content": "The samples below have been redacted due to the extreme harmful content.\nSample D presents an example obtained after performing a fine-tuning attack on the DeepSeek-R1-Distill-Llama-8B\nmodel. This sample uses the same input as Sample A for comparative analysis. The results clearly demonstrate that\nafter fine-tuning, the model exhibits severe security vulnerabilities."}, {"title": "C Fine-tuning Attacked Non-CoT Model", "content": "Sample F serves as a comparative experiment, using the same prompt as Sample A and Sample D. However, instead of\nthe CoT-enabled model, this sample is generated using the Mistral-7B model fine-tuned on the same dataset. From the\noutput, it is evident that compared to Sample D, where the CoT-based reasoning model was used, the harmful response\nin this case is significantly lacking in structure, content quality, and completeness. This highlights that the step-by-step\nreasoning process enabled by CoT substantially enhances the model's output quality-whether by making it safer or,\nconversely, more dangerous."}]}