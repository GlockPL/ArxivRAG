{"title": "No Free Delivery Service", "authors": ["Maximilian Nickel"], "abstract": "Rapid model validation via the train-test paradigm has been a key driver for the breathtaking progress in machine learning and AI. However, modern AI systems often depend on a combination of tasks and data collection practices that violate all assumptions ensuring test validity. Yet, without rigorous model validation we cannot ensure the intended outcomes of deployed AI systems, including positive social impact, nor continue to advance AI research in a scientifically sound way. In this paper, I will show that for widely considered inference settings in complex social systems the train-test paradigm does not only lack a justification but is indeed invalid for any risk estimator, including counterfactual and causal estimators, with high probability. These formal impossibility results highlight a fundamental epistemic issue, i.e., that for key tasks in modern AI we cannot know whether models are valid under current data collection practices. Importantly, this includes variants of both recommender systems and reasoning via large language models, and neither na\u00efve scaling nor limited benchmarks are suited to address this issue. I am illustrating these results via the widely used MOVIELENS benchmark and conclude by discussing the implications of these results for AI in social systems, including possible remedies such as participatory data curation and open science.", "sections": [{"title": "1 Introduction", "content": "Model validation, long taken to be \"solved\u201d via the train-test paradigm, has become one of the central challenges in modern machine learning and artificial intelligence. In unison with the dramatic increase of their capabilities, AI systems are now supposed to solve tasks of vastly expanded scope, including potentially AI-complete tasks such as open domain question answering, autonomous decision making, and ultimately, artificial general intelligence. Even before the recent triumphs of large language models and deep learning, Anderson (2008) proclaimed \"the end of theory\" and the scientific method being obsolete due to the wonders of big data, large-scale computing, and data mining. At the same time, it is entirely unclear how to rigorously evaluate the quality of models for these ambitious tasks. This lack of proper evaluation can then materialize in persistent issues of deployed systems related to generalization, e.g., hallucination (Ji et al., 2022), out-of-distribution generalization (J. Liu et al., 2021), fairness (Barocas et al., 2023), and generalization to the long-tail (Feldman, 2020; Feldman and Zhang, 2020). Importantly, these issues do not only affect the accuracy of models in a vacuum, but can also affect their social impact if they are deployed in consequential social contexts (L. Cheng et al., 2021). In this paper, I aim to connect the former developments with the latter issues through the lens of epistemology. More concretely, I ask:\nResearch Question 1. Given the ambitious tasks that we ask AI systems to solve and given how we currently collect data, can we know whether a model performs well for these tasks?\nAnswering RQ 1 positively is central not only for the deployment of machine learning systems, but also for scientific progress within artificial intelligence itself. After all, knowledge of a model's quality is a prerequisite to detect generalization issues and develop improved models. In deployed systems, a model's predictions are useless as good as they might be \u2013 without knowing that they are, in fact, reliable. In social systems, where the consequences of model errors can be severe, having this knowledge is of even greater importance. Hence, the epistemic question of this work gets to the heart of various debates surrounding AI and its capabilities: How can we understand and measure the true capabilities of modern AI systems, which are so very impressive and yet lacking in fundamental ways at the same time (Bottou and Sch\u00f6lkopf, 2023)? What can we know about the quality of our models? Are our benchmarks suited to give insights into the intended tasks or do they project a false image of quality? How can we develop systems such that they work for everyone? Will na\u00efve scaling solve all these problems or do we need to invest into entirely new approaches for evaluation within the scope of modern AI?\nA prerequisite to answering RQ 1 positively is the validity of model validation: Without model validation we can not know wether a model is good or bad and without a valid model validation procedure we can not attain this knowledge. The almost exclusively used method for model validation in machine learning and AI is the ubiquitous train-test paradigm, i.e., the practice of estimating the generalization performance of a model on a test set distinct from the training set. Arguably, much of the breathtaking progress in machine learning has been driven by the success of this single experimental paradigm as it allows for the rapid validation and, therefore, improvement of models (Bottou, 2015). However, it is crucial to note that the train-test paradigm is inherently an inductive method that aims to infer, not measure, the generalization error of a model from its error on a test set. It is well known \u2013 dating back at least to Hume (1739, 1748) and formalized in the context of machine learning by Wolpert (1996) - that it is not possible to justify the validity of such inductive inferences without further assumptions. This raises the question: is the train-test paradigm still valid for the combination of tasks and data sets considered in modern AI and under what assumptions is this the case?\nImportantly, such assumptions should be minimal in terms of ontological commitments, i.e., meet ontological parsimony (or minimality), since (a) model validation results can not provide insights about validity in the real world if they are contingent on strong ontological assumptions (b) any assumptions that are required to ensure the validity of model validation can not be validated through the same method without circular reasoning. In traditional machine learning settings, these ontological commitments are placed entirely on the data collection process and, as such, the train-test paradigm is indeed suitable to validate any model assumption outside the data collection process. More concretely, under active data collection, i.e., when we actively control the data collection process, we can create large enough test sets that are (approximately) sampled i.i.d. from the target distribution. Under these conditions, it is well known that the train-test paradigm allows us to validate models simply via their performance on this test set without making any further ontological commitments. This property is the beauty of the train-test paradigm and what makes it so valuable and successful.\nHowever, domains in modern machine learning have become far too large to be covered via data sets in this active and controlled manner the required effort would be prohibitively difficult and costly. In lieu, passive data collection has become the predominant way to create data sets for modern AI systems. Here, data is collected without intervention from some social system that generates data within the domain of interest. For instance, rather than meticulously collecting independent samples from all possible facts in a domain, training and validation corpora for QA models are gathered from what has been published on the internet. Similarly, preferences of users are collected over items that a recommender system has pre-selected, rather"}, {"title": "2 Passive data collection and inference tasks in social systems", "content": "To construct validation data sets for large-scale domains, there exist currently two main practical approaches: (i) \"scaling\", i.e., indiscriminately collecting as much data as possible from some domain and (ii) manually constructing benchmarks of limited size that probe certain subareas of the domain. In the following, I will focus on formalizing (i) as passive data collection from social systems. Section 4 will then show that neither (i) nor (ii) can be solutions to the issues of this paper.\nIn sociology, a social system is often considered a pattern of networked interactions that exists between individuals, groups, or institutions (Merriam Webster Dictionary, 2024). For the purposes of this paper, I will consider a social system to be a pair $(f, S)$ where $f: X \\rightarrow Y$ is a possible world of interactions such that $X = X_1 \\times \\cdots \\times X_n$ denotes the domain of interactions, $y$ denotes the set of outcomes (or labels) of an interaction, and $S : X \\rightarrow [0, 1]$ denotes the sampling distribution of the system over interactions. Within this framework, passive data collection refers to sampling directly from S. This is in contrast to active data collection where we would aim to sample directly from the target distribution $T : X \\rightarrow [0, 1]$ for an inference task, e.g., via simple random sampling, stratified sampling, etc.\nIn complex social systems, S is driven by social processes that lead to two characteristic properties of samples: (i) they are biased and (ii) they follow heavy-tailed or power-law distributions. The earliest work on (ii) is due to Simon (1955), and has independently been discovered in multiple contexts. In fact, (ii) can often"}, {"title": "3 Test validity", "content": "To answer RQ 1, I will focus on the test validity of inference settings, i.e., whether task, assumptions, and data allow for any valid validations at all. For this purpose, I will use a deductive approach: model validation is valid if it is a logical consequence of its assumptions that the difference between its estimate and the true generalization error is bounded with high probability. To formalize this, let $h, f : X \\rightarrow Y$ denote functions that map from sample domain X to target domain y. For clarity, I will assume noise-free f and h. Furthermore, let $\\Sm = {x_i}_{i=1}^{m}$ denote a data set of m samples drawn from a sampling distribution $S : X \\rightarrow [0, 1]$ and let $D = {(x, f(x)) : x \\in \\Sm}$ denote its supervised extension. For notational convenience, I will also write $D \\sim \\Sm$ when f is clear from context. In addition, let $A \\subseteq {f | f : X \\rightarrow Y}$ be the set of all functions from X to y that are consistent with some set of assumptions on f such as being low-rank. Next, note that A and D then induce a set of possible worlds as follows:\nDefinition 3 (Possible worlds). Let A be a set of assumptions, $D \\subset X \\times Y$ a set of observations, and $f : X \\rightarrow Y$. The set of possible worlds $\\mathcal{F}$ is then the set of functions consistent with A and D, i.e.,\n$\\mathcal{F} = { f | f \\in A \\cap \\forall(x, y) \\in D : f(x) = y}$.\nFurthermore, I will consider an inference setting (A, D, T, F) to be a set of assumptions A, a fixed dataset $D \\sim \\Sm$, a target distribution $T : X \\rightarrow [0,1]$ for which we want to make inferences, and an assumed distribution over possible worlds F. Note that if $S \\neq T$, D can not be an i.i.d. sample from T. For further details and notation see supps. A and B.\nNext, let X be a random variable over X and let $l : y \\times y \\rightarrow R_+$ be a positive loss function. The risk of hypothesis h with respect to a single world f is then denoted by\n$L_{fh}^{T} = E_{X \\sim T}[l(h(X), f(X))]$.\nFurthermore, let $\\theta$ denote any risk measure of a hypothesis h on some test set T. For instance, $\\theta$ could denote the empirical risk or a re-weighted estimator such as the Horvitz-Thompson adjusted empirical risk (see also table 4 in the supplementary material). Hence, $\\theta$ does not only cover the standard Monte-Carlo estimator for the i.i.d. setting, but also estimators used in counterfactual and causal settings. To determine the test-validity"}, {"title": "4 Test validity under passive data collection in complex systems", "content": "In the following, I will provide an overview of the main results as well as high-level proof sketches. For clarity, I will consider only binary relations $X = X_1 \\times X_2$. For detailed proofs and discussion, as well as extensions to ternary relations, see supp. E. To meet ontological parsimony\u00b9 and get insights into the validity of the train-test paradigm, I will focus on F being the uniform distribution U and A imposing only minimal assumptions on f.\nNext, to derive bounds on the validity of inference settings in complex social systems, I will represent possible worlds f as partially observed matrices which are constructed as follows:\nDefinition 5 (Matrix representation). For a function $f : X_1 \\times X_2 \\rightarrow Y$ over finite sets of size $|X_1| = n_1$ and $|X_2| = n_2$, its matrix representation $F \\in R^{n_1 \\times n_2}$ is given via $F_{ij} = f(x_i, x_j)$ for all $(x_i, x_j) \\in X_1 \\times X_2$.\u00b2 In the following, I will use f and F interchangeably.\nUsing this matrix representation of a system, I will show in lemma 2 that the train-test paradigm is invalid if the rank of f, i.e., the complexity of the system, exceeds the k-connectivity of the sample graph S and if f is chosen uniformly from F. Here, k-connectivity is defined as follows:"}, {"title": "6 Discussion", "content": "The results in this paper provide new insights into the validity of the train-test paradigm when data is passively collected from complex social systems. In particular, I have shown that there exists no free delivery service of data that allows for test validity on a global scale in this setting. While valid inferences are possible with respect to the sampling distribution S and within high k-cores, they are unlikely if T extends to the entirety of the system. Hence, test validity depends on the interplay between task (T), the complexity of the system (A), and the k-connectivity of the sample graph (S) underlying the observed data (D), what is a combinatorial property of the data. These results are attained by establishing novel necessary conditions for which validation is possible. As AI systems are increasingly applied in conditions for which sufficient conditions of validity are difficult to guarantee, understanding such minimal conditions can provide guidelines into developing better and more robust systems. Importantly, it can help to demarcate inference goals that are not meaningful from ones that are attainable. It helps to understand the limits of what we can know and which questions are futile to ask. This work provides a first step in this direction by establishing such epistemic limits of AI in complex social systems.\nFurthermore, I have shown that the sub-system for which valid inferences are possible shrinks rapidly with the complexity of the system and that a na\u00efve application of the scaling paradigm is prohibitively inefficient to overcome these validity issues. As a consequence, solving many complex AI tasks are unlikely to come for free through scaling or for cheap through extrapolating from limited small-scale benchmarks. Instead, there exists an inherent trade-off between data quality, quantity, and task complexity. If we want to avoid asking AI systems to solve simpler tasks (e.g., non-out-of-distribution or smaller scope), new data curation efforts are likely needed. Due to the substantial amount of data that would have to be collected, centralized data collection is often infeasible to overcome the validity issues of this paper. Instead, decentralized methods such as participatory data curation could provide a way forward. This aligns with insights from fairness which also highlight the need for participatory methods in data collection (Jo et al., 2020). Similar arguments apply to the importance of open science and open-source models in this context.\nImportantly, the theoretical results of this paper also provide direct insights into how to improve data collection for model validation via its k-core conditions. In particular, lemma 1 and corollary 2 imply two clear objectives for targeted data collection: (a) collecting data points that increase the k-connectivity of the sample graph and (b) collecting data points that increase the size of the rank(f)-core of the sample graph, where rank(f) is the complexity of the world that we want to assume. Pursuing (a) would increase the complexity of the world that can be assumed such that model validation is still valid for the entire sample graph, while pursuing (b) would increase the size of the subgraph for which a rank(f) = k assumption would still yield valid model validation. Hence, both objectives are based on the k-core conditions of this work and can be computed from a given sample graph. Creating new mechanisms for efficient data collection based on these insights is therefore a very promising avenue for future work."}, {"title": "A Notation", "content": "Random variables are denoted by italic uppercase letters, e.g., L, S, X. Sets are denoted by calligraphic uppercase letters, e.g., X, S. Constants are indicated with lowercase greek letters, e.g., \u20ac, p. Functions and scalar are denoted by lowercase letters, e.g., f, g, h and x, y. Matrices and higher-order tensors are indicated with bold uppercase letters, e.g., F, U."}, {"title": "B Validity framework", "content": "In this work, I am interested in the validity of inference settings, i.e., whether assumptions and observations allow for any valid inferences at all. To formalize this, I will take the following high-level approach:\nInference setting An inference setting consists of a set of assumptions A, a fixed dataset D which is collected from a sampling distribution S, and a target distribution T for which we want to make inferences. Note that S is not guaranteed to be identical to T. Hence, we're concerned with out-of-distribution generalization settings.\nExpected risk over possible worlds Assumptions A and observed data D define a set of possible worlds F that is consistent with A and D. Given a probability distribution F over F, I am then interested in the expected risk over all possible worlds that are consistent with A and D.\nValidity An inference setting is valid, if the expected risk over possible worlds can be bounded meaningfully at all, i.e., if there exists at least one hypothesis class for which the generalization error of at least a single hypothesis can be bounded sufficiently.\nTo approach the question of validity, learning theory has traditionally focused nearly exclusively on sufficient conditions for valid inferences. Under active data collection, i.e., in scenarios where one can control exactly how data is collected, sufficient conditions are highly attractive since they provide exact specifications for inferences to be valid with high probability. However, under passive data collection, the situation is reversed. Sufficient conditions for the validity of inferences usually place highly restrictive demands on the data collection process (e.g., i.i.d. samples or simple random sampling) which are challenging to satisfy even when data is collected carefully in an active way. Since passive data collection, by definition, exerts no control over the sample generating process, these sufficient conditions are not met with near certainty. For this reason, I am focusing here on necessary conditions for validity, i.e., conditions that must always be satisfied for inferences to be valid. Under passive data collection, necessary conditions can provide important insights since they need to hold for any data collection process or, conversely, can be used to identify scenarios where inferences are not valid with high probability."}, {"title": "B.1 Connection to No-Free-Lunch theorems", "content": "The validity framework of section 3 and the No-Free-Lunch theorems are closely connected. First, consider the expected risk over all possible worlds relative to F, i.e.,\n$E_{f \\sim F} [L_{fh}^{T}] = E_{f \\sim F} E_{x \\sim T}[l(h(X), f(X))]. \\qquad(1)$\nEquation (1) is then akin to the objectives considered in the seminal No Free Lunch (NFL) theorems (Wolpert, 1996; Sterkenburg et al., 2021). For instance, the NFL theorem for supervised learning can be written as $\\forall_{\\wedge} : E_{f\\sim u} E_{x\\sim T}[l(h^{\\wedge}(D)(X), f(X))] = 1/2$, where U is the uniform distribution over all possible worlds in an assumption-free setting (i.e., A = \u00d8), l is the 0/1-loss, and $h^{\\wedge}(D)$ is the hypothesis derived from a finite sample D with algorithm A. In contrast to the NFL theorems \u2013 where A = \u00d8 implies an induction-hostile universe my focus is on induction-friendly settings (A \u2260 \u00d8) but where D is sampled from a complex social system. Since $L_{fh}^{T}$ is a non-negative random variable, we can then connect definition 4 and eq. (1) via upper and lower bounds based on Markov's inequality.\nDefinition 7 (Markov's inequality). Let X be a non-negative random variable and a 0. Then\n$P(X > a) < E[X]/a$.\nHence, it follows that the expected risk over all possible worlds is large for invalid settings since it holds that\n$E_{f\\sim F}[L_{fh}^{T}] \\geq \\epsilon \\cdot P_{f \\sim F}(L_{fh}^{T} > \\epsilon)$"}, {"title": "B.2 Importance of ontological parsimony and test validity in the i.i.d. setting", "content": "The strong appeal of the train-test paradigm is that, with careful data collection, we require no further ontological assumptions to ensure the validity of the model validation procedure. In particular, if we have a test set that is sampled independently from T, it follows straightforwardly from Hoeffding's inequality that we can meaningfully bound the approximation error over this test set (Shalev-Shwartz et al., 2014, Theorem 11.1). Let T ~ Tm be a test set of size m, sampled i.i.d. from the target distribution T. Then, it holds that\n$P_{T \\sim T^m}{[\\epsilon_{T^m} - L_{h}^T| < \\epsilon] \\geq 1 - \\delta}$.\nImportantly, this holds for any hypothesis h, any algorithm A, and any training set D. Hence, under careful data collection where we know that if the test set is sampled i.i.d. from T, any hypothesis can be validated based on the observed data only.\nThis property, i.e., that we can evaluate the performance of a model without further assumptions on the model itself, is crucial to compare the performance of different methods since different architecture, inference, and hyperparameter choices correspond to different assumptions. Maybe more importantly, this property is also crucial to validate our model assumptions on observed data (given that the sampling assumption holds), since otherwise we could only make statements relative to that our model assumptions hold, which is, of course, much weaker and not informative. Hence, if we need to make model specific assumptions for the validation error to be informative for the generalization error, the train-test paradigm would be relatively meaningless.\nOf course, the (considerable) challenge is to collect m i.i.d. samples from the true target distribution T which can not be guaranteed and is an important assumption on the data collection process."}, {"title": "B.3 Ternary and higher arity relations", "content": "First, note that higher arity functions can be represented as tensors of the same order as follows (see also fig. 6b for a visualization):\nDefinition 8 (Tensor representation). For a function $f: X_1 \\times X_2 \\times \\cdots \\times X_k \\rightarrow R$ over finite sets of size $[X_i] = m_i$, we can construct its tensor representation $F \\in R^{m_1 \\times m_2 \\times \\cdots \\times m_k}$ via $F_{ij...k} = f(x_i, x_j, ..., x_k)$ for all $X_i \\in X_1, X_j \\in X_2, ..., X_k \\in X_k$.\nA trivial extension of theorem 1 and its related results can then be obtained by considering the rank of the projection of the tensor representation of f onto its matrix presentation such that $F_{ij} = \\sum_k f(i, j, k)$ (what equals the assumption that the rank of the predicate mode in f is one). Since predicates in many knowledge graphs are very sparse, the sum over k will often preserve this sparsity and the associated heavy-tailed distributions. In this case, the results of the matrix case extend directly to the tensor case. If this is not the case, it is necessary to extend the matrix analysis of lemma 1 to the tensor case and consider cases where the predicate mode can have rank larger than one. However, this is beyond the scope of this paper and reserved for future work."}, {"title": "C Complex social systems", "content": "In the following, I will discuss how sampling bias and heavy-tailed distributions can occur, and can be connected, in complex social systems. First, sampling bias is concerned with how S is collected. Most standard inference methods assume i.i.d. samples from T, but it is well know that this assumption can be easily violated when sampling in complex systems.\nDefinition 9 (Sampling bias). Let $S_{i,j}^{t}$ denote the random variable corresponding to entities $(i, j) \\in X_1 \\times X_2 j$ being samples at time t. Samples in complex social systems can then neither be assumed to be independent across time nor independent with regard to the target value $f_{ij}$, i.e.,\n$P(S_{i,j}^{t} | S_{i,j}^{t'}) \\neq P(S_{i,j}^{t})$ and $P(S_{i,j}^{t} | f_{i,j}) \\neq P(S_{i,j}^{t})$.\nHigher arity relations are defined analogously. See also fig. 4a for the assumed sample dependencies.\nSample biases as in definition 9 can be causes by aspects such popularity bias, i.e., if popular items are more likely to be sampled, and quality biases, i.e., if items with higher values for $f_{ij}$ are more likely to be sampled.\nA prime example of how sampling bias in the form of popularity bias can lead to power-law distributions, is the influential Barabasi-Albert model (Barab\u00e1si et al., 1999). In this model of complex networks, nodes are added to a network one by one and are connected to existing nodes with a probability proportional to their degree, i.e., popularity. Formally, this model is defined as follows:\nDefinition 10 (Barabasi-Albert model). Let G = (X, \u0190) be a graph. Furthermore, let $P(i ~_{t} j)$ denote the"}, {"title": "D Proof corollary 1 (Necessary condition for test validity)", "content": "Corollary 1 (Necessary condition for test validity). Let (A, D, T, F) be an inference setting, let $l : Y \\times Y \\rightarrow R_+$ be a positive loss function, and let H be a hypothesis class. Furthermore, let $\\theta \\in R_+$ be any risk estimate for h. Then, if (A, D, T, F) is (\u0454, \u03b4)-test-valid, it must hold that\n$\\exists H \\exists h \\in H : P_{f \\sim F}(L_{fh}^{T} \\leq \\epsilon + \\theta) \\geq 1 - \\delta$.\nProof. First, note that $|\\theta - L_{fh}^{T}| \\leq \\epsilon$ is equivalent to $\\theta - L_{fh}^{T} \\leq \\epsilon \\wedge L_{fh}^{T} - \\theta \\leq \\epsilon$. Furthermore, we have ${ f | |\\theta - L_{fh}^{T}| \\leq \\epsilon} \\subseteq { f | L_{fh}^{T} - \\theta \\leq \\epsilon} \\subseteq {f | L_{fh}^{T} \\leq \\theta + \\epsilon}$.\nIt follows then simply from the monotonicity of probability that\n$1 - \\delta \\leq P_{f \\sim F}([\\theta - L_{fh}^{T}] \\leq \\epsilon) < [P_{f \\sim F}(L_{fh}^{T} - \\theta \\leq \\epsilon) = P_{f \\sim F}(L_{fh}^{T} \\leq \\epsilon + \\theta).$"}, {"title": "E Proof lemma 1 (Rank-k underdetermination)", "content": "I will first introduce the concept of S-isomerism and connect it to k-connectivity. I will then use these results to proof lemma 1.\nFirst, let $S_{i,.}= {j : (i, j) \\in S}$ denote the set of observed columns for row i and $S_{.,j} = {i : (i, j) \\in S}$ denote the observed rows for column j. Let $S_{i,.[F]} \\in R^{m \\times |S_{i,.}|}$ be the sub-matrix of $F \\in R^{m \\times n}$ which is obtained by restricting the columns of F to the indices in $S_{i,.}$. Similarly, let $S_{.,j[F]} \\in R^{|$S.,j| \\times n}$ be the sub-matrix of $F \\in R^{m \\times n}$ which is obtained by restricting the rows of F to the indices in $S_{.,j}$. Then, S-isomerism is defined as follows:\nDefinition 11 (S-Isomeric). Let $F \\in R^{m \\times n}$ and let $S \\subseteq {1, ..., m} \\times {1, ..., n}$ with $S_{.,j} \\neq \\O$. Then, F is called S-isomeric iff\n$rank (S_{i,.[F]}) = rank(F), \\qquad\\forall i \\in 1, ..., m$ and\n$rank (S_{.,j[F]}) = rank(F), \\qquad\\forall j \\in 1, ..., n$.\nCorollary 3 (Necessary condition for S-isomerism). Let S be a sample graph and let rank(F) = k. If F is S-isomeric, then it must hold that S is k-connected.\nProof. First, note that rank(X) \u2264 min(m, n) for any $X \\in R^{m \\times n}$. Hence, it follows from definition 11 that for a rank-k matrix to be S-isomeric, each column and row needs to have at least k observed entries. Since this is equivalent to k-connectivity, the result follows.\nLemma 1 (Rank-k underdetermination). Let $A = {f | rank(f) \\leq k}$ and let $l \\in R_+$ be a positive loss function. Then, if S is not k-connected, F forms a non-empty vector space.\nProof. Since rank(f) = k and S is not k-connected, it follows from corollary 3 that f is not S-isomeric. Hence, it holds via (G. Liu et al., 2019, Theorem 3.2) that there exist infinitely many matrices $f'$ that all explain the observed data & perfectly, i.e.,\n$f' \\neq f\\rank(f') \\leq rank(f) \\qquad f'_{ij} = f_{ij} \\qquad \\forall(i, j) \\in S$.\nMoreover, it follows from (G. Liu et al., 2019, Lemma 5.1) that this set of possible worlds F forms a non-empty vector space V."}, {"title": "F Proof lemma 2 (Rank-k test invalidity)", "content": "To prove lemma 2, I will first show the following auxiliary proposition:\nProposition 1 (Risk inequality for Bregman projection). Let $l : Y \\times Y \\rightarrow R_+$ be a scalar Bregman divergence and let $f^* = arg \\underset{f \\in V}{min} L_{fh}^{T}$ be the Bregman projection of h onto a vector space F. Then, it holds that\n$L_{fh}^T \\geq L_{f^*h}^{T}$"}, {"title": "G Inefficiency of scaling and benchmarks", "content": "Lemma 1 allows to answer the scaling question by asking how many draws from S would be necessary such that all nodes are within the k-core of S with high probability, i.e., how many samples are needed until arriving at a valid test setting. In the following, I will discuss different ways to approximate this question."}, {"title": "H Experiments", "content": "All experiments were computed on a single NVIDIA Volta V100 GPU and implemented using Jax (Bradbury et al., 2018), Jaxopt (Blondel et al., 2021), Numpy, and Scipy. All experiments were computed on the MOVIELENS 100k benchmark (Harper et al., 2015) which is available at https://grouplens.org/datasets/movielens/100k/ and released under a custom license https://files.grouplens.org/datasets/movielens/ml-100k-README.txt."}, {"title": "I Related work", "content": "In statistics, Meng (2018) analyzed a scaling-related question similar to this paper: Given a carefully collected survey with low response rate (small data) or a large, self-reported dataset without data curation (big data), which dataset should you trust more to estimate population averages? For this purpose, Meng introduces an Euler-formula-like identity which connects estimation quality to data quality, data quantity, and problem difficulty. Similar to the results in this paper, Meng shows that data quantity is highly inefficient to overcome issues in data quality, especially sampling related issues. While related in spirit, the results in this paper go beyond the question of surveying and population averages and establish related results in the more general context of inductive inference via formalizing properties of complex social systems and their impact on validity of inferences."}, {"title": "J Limitations", "content": "As most theoretical work, this work needs to make certain assumptions to make the phenomena of interest amenable to analysis. In this work, the core assumption is that samples in complex social systems follow a heavy-tailed distribution. While this is a very robust finding in social science and widely supported, as discussed in section 2, it limits the results of this paper to this specific setting. For further analysis, this paper further assumes that this heavy-tailed distribution follows a regularly-varying power-law. This is again a supported assumption (Voitalov et al., 2018) and allows for a clean theoretical analysis. However, as discussed in section 2, it is still disputed whether samples in complex social systems actually follow this particular form. However, it is undisputed that they follow a heavy-tailed distribution, and as such, while the power-law based results might not apply exactly, their general implications are still supported."}]}