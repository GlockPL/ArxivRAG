{"title": "StreetviewLLM: Extracting Geographic Information Using a Chain-of-Thought Multimodal Large Language Model", "authors": ["Zongrong Li", "Junhao Xu", "Siqin Wang", "Yifan Wu", "Haiyang Li"], "abstract": "Geospatial predictions are crucial for diverse fields such as disaster management, urban planning, and public health. Traditional machine learning methods often face limitations when handling unstructured or multi-modal data like street view imagery. To address these challenges, we propose StreetViewLLM, a novel framework that integrates a large language model with the chain-of-thought reasoning and multimodal data sources. By combining street view imagery with geographic coordinates and textual data, StreetViewLLM improves the precision and granularity of geospatial predictions. Using retrieval-augmented generation techniques, our approach enhances geographic information extraction, enabling a detailed analysis of urban environments. The model has been applied to seven global cities, including Hong Kong, Tokyo, Singapore, Los Angeles, New York, London, and Paris, demonstrating superior performance in predicting urban indicators, including population density, accessibility to healthcare, normalized difference vegetation index, building height, and impervious surface. The results show that StreetViewLLM consistently outperforms baseline models, offering improved predictive accuracy and deeper insights into the built environment. This research opens new opportunities for integrating the large language model into urban analytics, decision-making in urban planning, infrastructure management, and environmental monitoring.", "sections": [{"title": "1. Introduction", "content": "Geospatial predictions using machine learning have become essential across various domains, including disaster management (Mahdizadeh Gharakhanlou & Perez, 2023; Jain et al., 2020), public health (Mhasawade et al., 2021; Wiemken & Kelley, 2020), climate change (Rolnick et al., 2022), and urban planning (Tekouabou et al., 2022). Traditional machine learning has played a key role in geospatial predictions, but its limitations have become more distinct over time. One significant drawback of traditional ML is that they often rely on structured geospatial data, such as raster or vector formats, affecting their ability to handle unstructured or multimodal data (Pierdicca & Paolanti, 2022). Additionally, traditional models may face challenges in capturing complex spatial patterns and regional variations, leading to challenges with data sparsity and uneven distribution, which could affect the accuracy and generalizability of predictions (Nikparvar & Thill, 2021).\nIn contrast, large language models (LLMs) have shown great promise across various fields by processing vast amounts of data and reasoning across multiple modalities (Chang et al., 2024). By integrating textual, visual, and contextual information, LLMs can introduce novel covariates for geospatial predictions, thus enhancing traditional approaches. However, extracting geospatial knowledge from LLMs poses its challenges. Although using geographic coordinates (i.e., latitude and longitude) was a straightforward way to retrieve location-specific information, this approach often yields suboptimal results, particularly when dealing with complex spatial relationships and regional characteristics. As a result, the traditional model does not easily to harness the full potential of multi-modal data, hindering its effectiveness in applications demanding comprehensive, cross-modal insights.\nTo address these challenges, we develop StreetViewLLM, a framework that integrates"}, {"title": "2. Related Work", "content": null}, {"title": "2.1 Street view imagery as a promising dataset", "content": "The rise of Street View Imagery (SVI) has significantly advanced urban analytics, offering a detailed and dynamic geographical data source that complements traditional methods such as satellite imagery. SVI provides high-resolution visual data at the street level, enabling researchers to analyze and understand urban environments in greater detail. Its ability to capture fine-grained details of urban streetscapes has made SVI a powerful tool in urban studies, ranging from architectural analysis to public health assessments (Biljecki & Ito, 2021). SVI's utility has been enhanced by its integration with various urban data sources, such as street networks, building information, and socioeconomic data. For instance, Rundle et al. utilized Google Street View to audit neighborhood environments, assessing walkability, street conditions, and safety. This method allows for systematically observing built environments without requiring in-person site visits. It offers an efficient way to gather data on urban infrastructure and its impact on residents' health and well-being (Rundle et al., 2011).\nMoreover, architectural analysis has benefited from the integration of SVI. Developed feature extraction module for architectural style classification using SVI data. This approach enables the classification of architectural styles based on building features captured through SVI, showcasing its potential for large-scale architectural studies (Zhao et al., 2018). Similarly, Ding and Hu explored building exterior colors and materials, using SVI to assess and analyze architectural elements at a street-level scale, providing valuable insights into urban aesthetics and design trends (Ding & Hu, 2013). SVI has also been employed in studies focused on green infrastructure and its impact on urban life; for example, the Green View Index (GVI) has been used to analyze how neighborhood greenery affects walking time. By leveraging SVI and deep learning techniques, they demonstrated that areas with higher green view indices tend to encourage more pedestrian activity, thus contributing to public health and urban planning strategies (Ki & Lee, 2021).\nFurthermore, SVI has been used to measure the relationship between the built environment"}, {"title": "2.2 Tri-environmental Framework: Social, Built, and Natural Environments", "content": "The Tri-environmental Framework integrates three essential dimensions of urban analysis: the social environment (population, public health), the built environment (urban infrastructure), and the natural environment (ecological systems). This framework allows researchers to evaluate urban environments holistically, selecting ground truth data from these three dimensions (Wang et al., 2023). For instance, the social environment includes"}, {"title": "2.3 Large Language Models with the Chain of Thought", "content": "Large Language Models (LLMs) have become a powerful tool in geospatial analysis due to their ability to process and analyze vast amounts of data. However, geographic tasks often require detailed reasoning and complex problem-solving, which can challenge the typical capabilities of LLMs. To address this, researchers have introduced advanced techniques to enhance the reasoning performance of LLMs in handling such tasks. One prominent method is the Chain of Thought (CoT) approach, which improves the reasoning capabilities of LLMs by breaking down complex problems into smaller, intermediate steps. This approach has been particularly effective in tasks that require precision, enabling LLMs to perform better by logically sequencing their thought processes. For instance, the zero-shot CoT method, introduced by Wei et al., involves the addition of prompts such as \u201cLet's think step by step,\u201d which enhances the model's ability to reason through multifaceted tasks without extensive manual inputs (Wei et al., 2022). Building upon this, researchers have begun exploring multimodal applications of LLMs, where different data types, such as textual and visual information, are integrated to improve reasoning outcomes. This Multimodal CoT framework allows LLMs to handle complex geographic tasks by utilizing various data inputs, such as satellite imagery and textual descriptions, thereby enhancing the depth and accuracy of the inferences made (Wang et al., 2024). Despite the advancements in CoT techniques, there remains a significant gap in the integration of multimodal geographic information with LLMs.\nWhile LLMs have been used to solve geographic problems through textual and visual perspectives, these approaches have primarily been applied in isolation. For example, some studies have focused on using LLMs to interpret geographic problems from purely textual data, while others have concentrated on solving geographic tasks using visual data, such as satellite images or street view data. However, a comprehensive solution that combines both textual and visual data into a unified framework has yet to be fully developed (Wang et al., 2023; Wang et al., 2023). This paper seeks to bridge this gap by introducing a novel approach that integrates multimodal geographic information into LLMs, allowing for a more robust and sophisticated framework for geographic problem-solving. By combining textual, visual, and other forms of geographic data, this model enhances the LLM's ability to analyze complex geospatial phenomena. Although the integration of chain-of-thought reasoning and multi-modal data processing, the proposed model provides a more comprehensive understanding of geographic challenges. Comparative experiments have demonstrated that this integrated approach significantly outperforms existing models that rely on single-modal data, offering promising advancements in the fields of earth observation data and geoinformatics (Wang et al., 2024)."}, {"title": "3. Methodology", "content": "In this study, we propose StreetViewLLM, a multimodal large language model framework that integrates Chain of Thought (CoT) reasoning and Retrieval-Augmented Generation (RAG) techniques to extract and utilize geographic information effectively. The research framework comprises three key operational stages: Step 1 is the geographic information retrieval, detailed in Subsection 3.1; Step 2 is the rationale generation, detailed in Subsection 3.2; and Step 3 is the answer inference, detailed in Subsection 3.3."}, {"title": "3.1 Geographic Information Retrieval", "content": "Retrieval-augmented generation (RAG) combines the strengths of information retrieval and generative models, which are widely used in natural language processing and dialogue systems (Lewis et al., 2021). RAG leverages an initial retrieval step to gather relevant data from extensive external databases, followed by a generative model that formulates answers based on the retrieved information. In this study, geographic RAG is applied to enhance the precision of model outputs by integrating geographically relevant data into the generation process.\nWe begin by conducting spatial sampling within the target city. To ensure uniformity and"}, {"title": "3.2 Rationale Generation", "content": "Chain of Thought (CoT) reasoning facilitates multi-step problem-solving, improving the model's ability to handle complex geographic prediction tasks (Wei et al., 2022). The core idea of CoT is to decompose the reasoning process, guiding the model through a series of intermediate steps. This is particularly beneficial in tasks where a significant gap exists between the retrieved geographic data and the prediction objective. By employing rationale generation, the model is directed to establish causal links between the retrieved information and the prediction task, enhancing both its interpretability and reasoning performance. During the rationale generation stage, we feed the model with both textual inputs (coordinates, addresses, nearby places) and visual inputs (street view imagery), along with the corresponding prediction task. This process allows the model to generate reasoning chains and identify factors that influence the prediction outcome."}, {"title": "3.3 Answer Inference", "content": "In the final stage, the rationale generated in the previous step is appended to the original input, resulting in an enriched input dataset. This augmented dataset is then fed into the answer inference model to derive the final prediction. To standardize the output, each bin is associated with a predefined value range from 0.0 to 9.9, which standardizes the output by discretizing continuous geospatial predictions into consistent, interpretable intervals (Manvi et al., 2023). ensuring consistency in the model's predictions. This range was selected based on empirical distribution characteristics, providing a balance between granularity and interpretability of the predicted results."}, {"title": "4. Experiment", "content": null}, {"title": "4.1 Study Area and Data", "content": "For this project, we select seven globally representative cities\u2014Hong Kong, Tokyo, Singapore, Los Angeles, New York, London, and Paris\u2014due to their significant roles in the global economic, cultural, and financial landscape. Hong Kong, a major financial hub in Asia, exemplifies high urbanization and international integration. Tokyo, Japan's economic, cultural, and political nexus, is distinguished by its advanced technological infrastructure and efficient public transportation system. Singapore, serving as Southeast Asia's economic and logistical center, is recognized for its well-managed governance and highly globalized business environment. Los Angeles, the cultural and economic core of the U.S. West Coast, is renowned for its global influence in the film industry. New York, a leading global financial"}, {"title": "4.2 Baseline", "content": "The baseline consists of a collection of models from previous studies, ensuring that subsequent models are only considered improvements when they surpass these baselines in performance. Given the diversity and complexity of our tasks, we selected a range of the most popular machine/deep learning models. These models primarily encompass those capable of processing tabular geographic information, textual descriptions of geographic data, and street view images, as well as multimodal models that integrate geographic coordinates, text, and pictures.\n1) KNN (Tamamadin et al., 2022): The K-Nearest Neighbors (KNN) algorithm is a straightforward instance-based learning method. It operates by identifying the \"k\" nearest data points (neighbors) to a given input using a distance metric, commonly Euclidean distance. This approach serves as a reasonable baseline, particularly for geospatial tasks, where distance metrics are highly relevant for determining spatial relationships. In our case, we have chosen k = 5.\n2) XGBoost (Chen & Guestrin, 2016): XGBoost is a highly efficient and scalable gradient boosting algorithm. It serves as a robust baseline model in our analysis, offering a solid foundation for comparison. In this study, we set the number of"}, {"title": "4.3 Sampling Data", "content": "We collect selected geospatial data from seven globally representative cities: Hong Kong, Tokyo, Singapore, Los Angeles, New York, London, and Paris for the experimental study. This dataset includes coordinates, addresses, nearby locations, and street view imagery, utilizing data from social (Population, Health), natural (NDVI), and built (Height, Impervious Surface) environments as ground truth indicators. In total, 10,267 data points are gathered from these cities, with a range of sample sizes. Tokyo provides the largest number of samples at 2,962, followed by Los Angeles with 2,843 and New York City with 1,448. Other cities, including Singapore and other cities, including Singapore, Paris, Hong Kong, and London, contributed fewer samples, yet they offer a diverse and robust dataset. Subsets: 60% for training, 10% for validation, and 30% for testing, providing a balanced distribution for model development and evaluation. Figure 3 illustrates the distribution of data points for the seven cities; blue represents test, yellow represents train, and red represents validation."}, {"title": "4.4 The Performance of Model", "content": "We perform various tasks across the social, natural, and built environments, comparing the performance of several established models against the proposed model to assess the predictive capabilities of the models. The models tested included K-Nearest Neighbors (KNN), XGBoost, MLP-BERT, and ResNet-50, in addition to our proposed model. The performance is evaluated using three key metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R\u00b2, with the latter serving as the primary measure of model fit or goodness of fit.\nMean Absolute Error (MAE) = $\\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$ \nRoot Mean Squared Error (RMSE) = $\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$ \nR\u00b2 = $1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\overline{y})^2}$ \nWhere n is the total number of observations (data points), $y_i$ the actual value is at the i-th data point, and $\\hat{y}_i$ the predicted value is at the iii-th data point.\nTo explore the impact of various components within the model on its predictive capability, we disassemble the model and conduct a predictive evaluation for the task of population density. The results indicate that our proposed model outperforms the comparative models across all environmental dimensions. Table 2 provides a detailed comparison of these results. This result underscores the proposed model's enhanced ability to analyze complex and less structured geospatial datasets derived from earth observation data, thereby providing more accurate predictions in natural environment variables. In the social environment predictions, which utilized indicators such as population and health, our model achieved an R\u00b2 of 0.52 and 0.66, significantly higher than the next best model, KNN and XGBoost, which obtained an R2 of 0.35 for population and 0.59 for Health. This is demonstrated in Table 2 by the superior capability of our model to capture the intricate relationships between social factors and urban environments. This result highlights the proposed model's enhanced ability to process and analyze natural environment variables, often characterized by more and less structured data.\nA complete table of predictions can be found in the attached Appendix Table A1.1-1.4."}, {"title": "5. Result", "content": null}, {"title": "5.1. Data Summary", "content": "Following the established baseline, we propose to execute the identical task using the compiled data across a range of models, as illustrated in Table 2. Remarkably, the data from our model is significantly higher than the models that reduce the COT, Streetview, and TEXT features. In our comprehensive analysis of the built environment, incorporating critical variables such as building height and impervious surfaces, our model exhibits a superior coefficient of determination (R2) of 0.74. This performance notably surpasses that of other models, with the ResNet-50 achieving the next highest R\u00b2 of 0.73, as shown in Table 3. This enhanced performance accentuates the potential of our model in proficiently tackling the task at hand, thereby indicating its viability as a preeminent solution within this domain. The model's ability to consistently outperform its counterparts underscores its effectiveness and reliability, suggesting that it holds significant promise for future applications and developments in this field."}, {"title": "5.2. Urban Features and Prediction Bias", "content": "The spatial differences among cities significantly influence their urban form, ranging from high-density vertical development to expansive horizontal sprawl. Cities including Hong Kong and New York are characterized by extreme vertical density, with towering buildings and limited land space, while cities like Los Angeles are known for their sprawling, car-dependent layout. In contrast, cities such as Tokyo and Singapore balance dense development with integrated green spaces and efficient infrastructure planning. London and Paris, with their historic cores and preserved green areas, offer a mix of compact urban living and open public spaces (Rezaei & Millard-Ball, 2023; Dadashpoor et al., 2019). These diverse urban forms create unique spatial dynamics, reflecting the distinct planning strategies and geographic constraints of each city.\nTo explore the relationship between urban spatial features and model prediction performance, we utilized the OpenStreetMap (OSM) Overpass API to collect the number of Points of Interest (POI) within 500 meters of prediction points, categorized by different types (Residential, Commercial, Business Facilities, Industrial, Administration & Public Service, Science & Education, Green Space, and Total) (Hong & Yao, 2019). We then calculated the"}, {"title": "6. Discussion", "content": null}, {"title": "6.1 Key findings", "content": "This study pioneers the use of multimodal large language models (LLMs) integrating with Chain-of-Thought (CoT) reasoning to predict urban features by combining visual street view imagery with textual data, including geographic coordinates, addresses, nearby places, and directions. By utilizing Retrieval-Augmented Generation (RAG) techniques, StreetViewLLM enables detailed extraction of geographic information across complex urban environments.\nThe model demonstrates superior performance in various tasks within the tri-environment framework, covering social-level indicators (i.e., population density and accessibility to healthcare), natural-level indicators such as the normalized difference vegetation index, and built-level indicators (i.e., building height and impervious surface), outperforming traditional machine learning models like KNN, XGBoost, and ResNet50. We find that, especially in the task of predicting population density, StreetViewLLM excelled in high-density urban areas such as New York and Tokyo, where it achieved significantly higher R\u00b2 values than baseline models. This outcome allows StreetViewLLM to support more precise modeling of population distributions, offering valuable insights into managing urban expansion and"}, {"title": "6.2 Contribution to the Literature", "content": "Our experimental application of the StreetviewLLM model demonstrates its unique contribution to geospatial analysis by combining multimodal data sources for enhanced accuracy. This aligns with previous research that emphasizes the potential of Street View Imagery (SVI) in urban analysis. For instance, SVI's capacity has been highlighted to support diverse urban studies, such as infrastructure analysis and public health assessments (Biljecki & Ito, 2021). StreetviewLLM, however, advances beyond these prior studies by integrating SVI with additional geospatial data types, including population density, NDVI for natural vegetation, and built environment features like height and impervious surface measures. This multimodal integration leads to significantly higher prediction accuracy, consistent with the emphasis on multi-source data in producing robust urban environmental assessments (Gupta & Deb, 2023).\nWhen we compare StreetviewLLM to traditional models, such as KNN,XGBoost, MLP-BERT, and ResNet50, our model consistently outperforms these benchmarks in both precision and accuracy, underscoring the advantage of LLMs equipped with diverse geospatial inputs. This improvement echoes insights the single-modal approaches often fall short in capturing complex urban interactions (Wang, et al., 2023). By leveraging multimodal"}, {"title": "6.3 Limitation", "content": "Despite its novelty, this paper acknowledges several limitations, which present opportunities for future research and model refinement. The first limitation pertains to the ground truth data and the street view imagery and textual descriptions used as inputs. The current model does not account for temporal changes; indicators from different years are predicted based on the same input data, potentially introducing bias. This lack of temporal differentiation means that shifts in urban features over time are not captured, which could affect the accuracy of predictions in dynamic environments where rapid development or decay occurs (Anees et al., 2020). The use of Street View Imagery (SVI) may have introduced bias. SVI data collected by Google are typically captured by cameras mounted on vehicles, and while we apply a buffer-based resampling technique to mitigate missing street view data, some sampled regions still contain gaps. Furthermore, some SVI images, rather than capturing actual street views, include indoor scenes, which introduces noise into the model's input. Variations in the resolution, time of capture, and visual obstructions\u2014such as weather conditions, lighting, or traffic-can also impact prediction accuracy, particularly for tasks requiring fine-grained spatial detail, like building height and impervious surface detection (Smith et al., 2021; Kang et al., 2020; Ito et al., 2024).\nThe second limitation lies in the integration of Chain-of-Thought (CoT) reasoning, which, while significantly enhancing the interpretability of the model, introduces additional complexity that can sometimes result in inconsistencies in the final predictions. CoT's multi-step reasoning approach is particularly beneficial for handling complex geographic tasks, as it breaks down larger problems into smaller, more manageable steps. However, this layered reasoning process can also lead to compounding errors, especially when the input data is ambiguous or incomplete. For example, minor inaccuracies in the early stages of reasoning can propagate through subsequent steps, amplifying errors in the final prediction output (Wang et al., 2022). Additionally, the CoT reasoning process comes with a high computational overhead. The model requires significant computational resources to perform the intermediate reasoning steps, increasing both the time and power needed for processing, particularly when applied to large-scale geographic datasets. This computational cost not only limits the model's efficiency but also introduces the risk of bias, as more complex reasoning chains may disproportionately affect certain types of geographic data, especially those that require intricate interpretation. As a result, while CoT improves the model's interpretability, it also poses challenges in terms of computational efficiency and prediction accuracy, especially in scenarios involving intricate geographic information. The model's generalizability across diverse urban contexts is another limitation. Although the study encompasses a variety of global cities, the training dataset primarily focuses on major urban centers, which may not fully capture the characteristics of smaller or less-developed cities. This could limit the model's applicability to environments with distinct urban forms, infrastructure, or socio-economic conditions, restricting its use in a broader range of urban settings."}, {"title": "6.4 Future Application and Policy Implication", "content": "This research demonstrates the significant potential of StreetViewLLM to revolutionize geospatial analysis and inform urban policy-making. By accurately predicting key urban characteristics using readily available multimodal data, StreetViewLLM can support data-driven decision-making across diverse domains.\nSmart City Design and Simulation: Integrating StreetViewLLM with urban simulation platforms could create dynamic \"digital twins\" of cities. By incorporating real-time data feeds and predictive modeling, these digital twins could simulate the impact of urban planning decisions on various aspects of city life, such as traffic flow, pedestrian movement, and environmental conditions. This would allow urban planners to test different scenarios and optimize designs for sustainability, livability, and efficiency before implementing them in the real world.\nEnvironmental Monitoring: The accurate prediction of NDVI empowers environmental monitoring efforts. By tracking changes in vegetation health across the urban landscape, policymakers can identify areas experiencing environmental stress and implement targeted interventions. This could include tree planting initiatives, the development of urban green spaces, and policies to mitigate air pollution.\nDisaster Response: In disaster scenarios, StreetViewLLM can provide rapid assessments of impacted areas by analyzing street view imagery and predicting the extent of damage to buildings and infrastructure. This information can aid emergency responders in prioritizing resource allocation and optimizing evacuation strategies.\nReal Estate Appraisal and Investment: StreetViewLLM could revolutionize the real estate sector by providing automated property valuations based on street view imagery, neighborhood characteristics, and predicted urban development trends. By analyzing factors like building height, surrounding green space, and proximity to amenities, the model could generate accurate property appraisals, aiding both buyers and investors in making informed decisions. This could also help identify undervalued properties and predict future property value appreciation, streamlining real estate investment strategies.\nAutonomous Navigation and Urban Exploration: StreetViewLLM could enhance autonomous navigation systems by providing a deeper understanding of the urban environment. By analyzing street view imagery, the model could identify landmarks, predict pedestrian behavior, and assess the safety of different routes. This could lead to more efficient and safer navigation for autonomous vehicles and delivery robots, particularly in complex urban environments with high pedestrian density and unpredictable traffic patterns. Furthermore, the model could be used to create interactive maps for urban exploration, providing users with personalized recommendations for routes, points of interest, and hidden gems based on their preferences and the predicted ambiance of different neighborhoods.\nPolicy Implications: The insights generated by StreetViewLLM can inform the development of evidence-based urban policies. By understanding the complex interplay between urban form, environmental factors, and social indicators, policymakers can design effective strategies to promote sustainable urban development, improve public health outcomes, and enhance the resilience of cities to environmental challenges.\nHowever, realizing the full potential of StreetViewLLM requires addressing its limitations. Future research should focus on incorporating temporal dynamics, improving the handling of noisy or incomplete street view imagery, and enhancing the efficiency of CoT reasoning. Additionally, expanding the model's training data to encompass a wider range of urban environments will enhance its generalizability and applicability to diverse urban contexts. By addressing these challenges and continuing to refine the StreetViewLLM framework, we can unlock its transformative potential to revolutionize urban planning, promote sustainable development, and enhance the quality of life in cities worldwide."}, {"title": "7. Conclusion", "content": "In summary, our study contributes a significant advancement in the application of multimodal large language models for geospatial prediction and urban analytics. By integrating Chain-of-Thought Reasoning and Retrieval-Augmented Generation techniques, StreetViewLLM showcases the power of combining street-level imagery with geographic and textual data to offer deeper, more precise insights into urban environments. This framework not only pushes the boundaries of urban planning, infrastructure management, and"}]}