{"title": "Planning vs Reasoning: Ablations to Test Capabilities of LoRA layers", "authors": ["Neel Redkar"], "abstract": "Low-Rank Adaptation (LoRA) layers have emerged as a promising approach for efficient model fine-tuning, but their capabilities and limitations have not been fully explored. This paper: 1) Investigates the fundamental question of whether LoRA layers are effective at increasing reasoning + planning abilities 2) We introduce HashChain Reasoning, a novel evaluation dataset that deterministically tests reasoning capabilities.\nThrough systematic ablation studies on GPT-2, we demonstrate that reasoning capabilities appear to exist primarily in low-rank spaces and can be effectively enhanced using LoRA layers. The effective rank analysis of trained LORA matrices reveals a 2-3x lower rank requirement for reasoning tasks compared to planning tasks, giving context on where LORA layers would be effective. This also provides evidence for reasoning fundamentally preferring low-parameter spaces for generalization.", "sections": [{"title": "Background", "content": "Efficiently handling continual learning for new skills is essential to scale language models [10]. One current approach that has gained traction in the past year is Low Rank Adaptation (LoRA) layers, which add a couple of low rank linear parameters to efficiently fine tune the model [13]. These parameters can either be applied dynamically or merged into the model via a linear combination. An important question here is the learning capacity of LoRA layers and where the boundaries might be.\nDistinguishing which capabilities LoRA layers are able to fine-tune is key to developing specialized models."}, {"title": "Continual Learning", "content": "Continual learning has been a large problem in the LLM space where learning new features is extremely difficult without forgetting the prior distribution. In practice this lends to large decreases in reasoning capability or catastrophic forgetting when fine-tuning models on new tasks. Different solutions have been introduced but each come with their own downsides.\nReplay-based methods use a buffer of past actions which can be used in conjunction with fine tuning to make sure the model doesn't forget key attributes [10]. With reasoning being an emergent property without any direct training data, this proves to be hard. Regularization is also utilized here but seems to be a half-fix without giving guarantees on knowledge retention and minimizes the effectiveness of finetuning [10]."}, {"title": "Low-Rank Adaptation (LoRA) Layers", "content": "Low-Rank Adaptation (LoRA) represents a significant advancement in parameter-efficient fine-tuning methodologies [6]. By introducing low-rank decomposition matrices into the adaptation process, LORA enables model specialization while utilizing only a fraction of the parameters typically required for full fine-tuning-replacing an (n,m) matrix with two (n, rank) \u00d7 (rank, m) matrices.\nThis parameter efficiency leads to reduced catastrophic forgetting and minimizes distribution shift during adaptation [3]. The approach is particularly compelling from a theoretical perspective, as the constrained parameter space provides implicit regularization against overfitting [13]. This aligns closely with the minimum description length principle-by restricting the dimensionality of possible adaptations, we can identify minimal distribution shifts that achieve task-specific optimization while preserving the model's core capabilities.\nOther methods include mixture of expert mod-"}, {"title": "Contributions", "content": "The contributions that this paper brings are:\n\u2022 A new reasoning eval, HashChain Reasoning, which can be utilized to deterministically check reasoning ability. This is extremely useful for model ablations & deterministic tests.\n\u2022 Reasoning seems to exist low-rank (Fig. 3, 2), especially compared to planning (Fig. 4). This means LoRA layers could be used for additional reasoning capabilities + might not be as helpful for planning tasks."}, {"title": "Approach", "content": "For the experimental setup GPT-2 was utilized alongside a custom training script for the LORA layer addition [8]. First the model is fine-tuned to its capacity on a certain set of reasoning evals (we will get into what these evals are later). Then using LORA module, we can try to increase the model's capabilities on these evals. If possible, this suggests reasoning/circuits related to reasoning are low-rank and could be augmented to increase abilities or for cross domain transfer. Further analysis on the LORA layers themselves could be done to determine the functional rank of the matrices.\nReasoning and planning are thought of in terms of tree search algorithms here. Planning would be the depth to which the algorithm can search to and reasoning would be the heuristic or communication between branches. Both are needed for more fundamentally complex models."}, {"title": "HashHop Dataset", "content": "The first dataset used was HashHop, an eval metric initially created for context window evaluations [5] [7]. A markov chain of hashes is generated and the model is asked to predict n hops ahead of the chain. Hashes are utilized because they are completely random.\nThe chain is expressed as a randomized list of relations (Fig. 1a):\nMap:\ndef34=>ghi56\nabc12=>def34\nStart: abc12\nHops: 2\nTarget: [Predicted hash (ghi56) here]\nIf done in a single token (as in this paper) it shows"}, {"title": "HashChain Reasoning Dataset", "content": "This dataset utilizes similar concepts from the HashHop eval but instead has multiple chains of varying lengths 1b. The model is then asked to predict the hash of the shortest chain. The eval is still artificial in nature but arguably more representative of common reasoning tasks in practical tasks.\nThe HashChain template is:\nMap:\nabc=>mno\nabc=>ghi\nabc=>jkl\nghi=>xoh\njkl=>djw\nStart: abc\nTask: shortest path\nTarget: [Predicted hash (mno) here]\nIf one thinks of reasoning as a breadth-first tree search, HashChain effectively horizontally scales. This is opposed to HashHop vertically scaling the search in a planning dimension. This analogy makes sense for the dataset as the optimal way solve HashChain is through a breadth-first search.\nSpecifically for reasoning, take common hard tasks such as \"Which number is greater, 2.11 or 2.9?\" or other logical tasks. These tasks fundamentally need comparison of multiple branches/sections. For the numerical example it would be \"2.11\" vs \"2.9\" but for more abstract reasoning it might be asked to perform boolean logic (AND, OR, NOT, etc.) with multiple branch comparisons.\nHashChain Reasoning would cover this by guaranteeing that a \"comparison operation\" or heuristic between multiple branches would have to be done to find the correct hash. This is explicitly different"}, {"title": "Experimental Results", "content": null}, {"title": "HashHop Evaluations", "content": "The chain length was varied between 1-20, with 15-20 being cut off in the graph for being equally close to random chance. For the regular HashHop evaluations the prediction power increased marginally with LORA layers within the first couple hops 5a. Jumps in accuracy started larger, 10% for 2 hop, but quickly deteriorated to random chance.\nThe curves all seem similar to a sigmoid function which start high around 80% accuracy and quickly drop to random chance. The quick drop highlights the difficulty of the task and its usefulness to benchmark emergent planning abilities. LoRA layers were also selectively fine-tuned on 4-20 hops and the same results were found.\nThis suggests that although LoRA layers are helpful for fine-tuning specific aspects like style, it is unable to teach fundamentally new planning capabilities to the network. The jumps in accuracy (2, 3, 4, & 5 hop) were on tasks that the model had already expressed prior functionality in. In Figure 5b and 4, ablations were done on the rank of the resultant LORA matrix. The effective rank of each LORA matrix seems to hover around 150, suggesting that the task is hard to learn and high rank [9].\nThe result is also supported in the literature. Due to LLM's being next token predictors, long term planning is especially hard [1]. If thought of in the same way as the breadth-first HashChain, this proves that vertically scaling reasoning is fundamentally high rank/difficult."}, {"title": "HashChain Reasoning Evaluations", "content": "The model was finetuned to capacity on 3-chain and 4-chain accuracy. The model converged on learning to perform well on 3 chains and seemed to perform less accurately on 4 chains as seen in Figure 2. Chain length in the reasoning tasks seemed to have much less of an effect on accuracy than in the regular HashHop evaluations, where the accuracy degraded linearly instead of exponentially as before.\nSimilar to planning, LoRA modules were trained on weak areas for the model. In 4-chain accuracy we see a huge boost due to the LoRA module, while having minimal degraded accuracy with the 3-chain accuracy. This shows that the model was able to increase its capabilities from being unable to perform 4-chain reasoning to gaining an 80% accuracy with lengths of 2 and genrally significantly higher accuracies than before.\nThe effective rank of the LORA layers was also found to be significantly lower than the regular eval, as shown in Figure 3. LoRA rank hovered around 50 for most layers, except the last. An effective rank of 50 is 3x less than the parameters needed for the regular HashHop model, suggesting a fundamental difference in the learned data distributions, with the HashChain Reasoning dataset being simpler to represent in latent space.\nThis shows the horizontal scaling of reasoning or handling multiple threads of reasoning is low rank, and can be expanded with LoRA layers. The implications are large as any reasoning task that can be formalized into concurrent operations can then be improved via LoRA layers (retrieval tasks, knowledge addition, or arithmetic with multiple numbers)\nSpiking near the end for both could also be attributed to latter layers often needing more LORA parameters [4]."}, {"title": "Conclusion", "content": "LORA layers seem to be the solution for increasing reasoning capabilities of models and it it might even be possible to train \"reasoning modules\" on key datasets to scale broad range reasoning ability. The same does not seem likely for planning due to its higher rank and inability to be boosted in new capacities. Splitting evals into these two categories and having granular objectives is important to decrease ambiguity (ie. differentiate high reasoning with low planning or vice versa).\nThe 4-chain abilities in the HashChain Reasoning dataset saw large increases in accuracy, suggesting not just improving reasoning but also horizontally adding new capabilities. Distinguishing between planning & reasoning is necessary because as we add LORA layers, its important to check whether or not LORA layers are the right fit for the job especially with continual learning.\nPlanning ability did increase but only in areas that the model already excelled in. This suggests LORA layers could be used to \"squeeze out\" more latent planning ability but planning still seems fundamentally hard for LoRA layers [2].\nOur current intuition for why reasoning could be low-rank is that a \"simplicity prior\" is important to generalization. Following the minimum description length principle, having simple circuits for higher order operators might allow for generalization across other higher order concepts in the model."}, {"title": "Future Research Directions", "content": "The use of effective rank + targeted datasets to check the difficulty of tasks for models to learn seems promising, as well as use of LoRA layers to check effective rank of circuits in models. LoRA layers could possibly be used as a replacement for Mixture of Experts or other techniques for increasing parameter counts in a sparse way for reasoning tasks [11].\nUtilization of LoRA features this way lends itself well to continual learning & might provide a way to combat forgetting. By using task-specific LORA layers, a model can continually adapt to new distributions possibly even mixing them together. The main unsolved problem would be training a sufficiently complex router that can be frequently updated.\nLORA layers could also be used to increase reasoning in domain specific areas (ie. comparison for retrieval, or math for technical domains). The layers could then be merged back into the model to have a base model better at reasoning than before. If one imagines world models stored as graphs, then it should be possible to horizontally increase the capability of the model to \u201cprocess\u201d this graph.\nSeparating planning and reasoning also brings up the interesting follow-up where if planning is hard for language models, what changes to the structure of transformers could allow for greater planning ability? Would it also be unfair to test new planning techniques against reasoning baselines (It seems easy to imagine a world where one can plan hundreds of steps ahead, but be limited by its ability to reason between optimal outcomes)?\nCode + benchmarks are public and open"}]}