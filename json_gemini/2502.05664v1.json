{"title": "CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging", "authors": ["Md. Ashraful Islam", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "abstract": "Large Language Models (LLMs) have made\nsignificant strides in code generation and\nproblem solving. Current approaches employ\nexternal tool-based iterative debuggers that use\ncompiler or other tool-based runtime feedback\nto refine coarse programs generated by\nvarious methods. However, the effectiveness\nof these approaches heavily relies on\nthe quality of the initial code generation,\nwhich remains an open challenge. In this\npaper, we introduce CODESIM, a novel\nmulti-agent code generation framework\nthat comprehensively addresses the stages\nof program synthesis-planning, coding,\nand debugging-through a human-like\nperception approach. As human verifies their\nunderstanding of any algorithms through\nvisual simulation, CODESIM uniquely\nfeatures a method of plan verification and\ninternal debugging through the step-by-step\nsimulation of input/output. Extensive\nexperiments across seven challenging\ncompetitive problem-solving and program\nsynthesis benchmarks demonstrate CODESIM'S\nremarkable code generation capabilities. Our\nframework achieves new state-of-the-art\n(pass@1) results\u2014(HumanEval 95.1%, MBPP\n90.7%, APPS 22%, and CodeContests 29.1%).\nFurthermore, our method shows potential for\neven greater enhancement when cascaded\nwith external debuggers. To facilitate further\nresearch and development in this area, we\nhave open-sourced our framework in this link\n(https://kagnlp.github.io/codesim.github.io/).", "sections": [{"title": "1 Introduction", "content": "In recent years, the rise of Large Language Mod-\nels (LLMs) has made significant advances in AI-\nassisted coding and reshaped the domain of code\ngeneration and problem-solving (Zhao et al., 2023).\nCode generation assistants built on GPT-4 (Ope-\nnAI, 2024), Mistral (Jiang et al., 2023a), and Llama\n(Dubey et al., 2024), inter alia, have demonstrated\nunprecedented ability to understand, generate, and\nmanipulate code across various programming lan-\nguages and problem domains. However, despite\nthese advancements, significant challenges persist\nin generating code for complex programming tasks.\nCurrent state-of-the-art approaches in code gen-\neration typically employ a dual-pass process (Shi\net al., 2024; Jin et al., 2024b; Zhong et al., 2024;\nLevin et al., 2024). In the first pass, they use\nLLMs to generate an initial, fully/partially cor-\nrect version of the program. Then accordingly in\nthe second pass, they apply external tool-based it-\nerative debuggers that leverage runtime compiler\nfeedback or other diagnostic tools to refine and cor-\nrect the generated code. While this approach has\nshown promise, it necessitates numerous iterations\nof LLM-tool interactions, and importantly its ef-\nfectiveness is heavily dependent on the quality of\nthe initial code generation\u2014a process that contin-\nues to present substantial difficulties. Therefore, in\nthis paper, we present CODESIM, a novel multi-\nagent code generation framework that seamlessly\nsynthesizes complex code solutions without exter-\nnal resources, while offering potential for further\nenhancement through minimal external debugging.\nSynthesizing programs even in the first pass,\nhowever, is fundamentally challenging, requiring a\ndeep understanding of natural language processing,\ncomputer algorithms, data structures, and problem-\nsolving strategies. These challenges are further\ncompounded when attempting to generate code for\ncompetitive programming problems or advanced\nsoftware engineering tasks, where adherence to spe-\ncific constraints or passing unit tests are paramount\n(Khan et al., 2023).\nWhile earlier code generation methods em-\nployed direct approaches (Chen et al., 2021a),\nchain-of-thought reasoning (Wei et al., 2022a), syn-\nthesized test-case guidance (Chen et al., 2022a),\nretrieval-augmented generation (Parvez et al.,"}, {"title": "2 Related Work", "content": "Code Generation: Program synthesis has been a\nfundamental challenge in AI for decades (Manna\nand Waldinger, 1971). Early attempts with smaller\nlanguage models centered on code generation by\nfine-tuning neural networks (Wang et al., 2021; Ah-\nmad et al., 2021; Feng et al., 2020; Parvez et al.,\n2018; Hellendoorn and Devanbu, 2017; Rabinovich\net al., 2017; Yin and Neubig, 2017; Hindle et al.,"}, {"title": "3 CODESIM", "content": "Our goal is to develop a multi-agent code genera-\ntion approach capable of complex problem solving.\nDrawing inspiration from recent works like Map-\nCoder and ChatDev (in a different context), we de-\nvise the agents in CODESIM for planning, coding,\nand debugging. While these existing approaches fo-\ncus primarily on expanding steps without verifying\nunderlying hypotheses, we address this limitation\nby introducing a novel verification approach. Our\napproach simulates input/output step-by-step, veri-\nfying generated plans and performing internal de-\nbugging, mirroring how humans understand, visu-\nalize, and refine in algorithm development. Below,\nwe present our proposed model."}, {"title": "3.1 Planning Agent", "content": "The first component of CODESIM is the Planning\nAgent. Given a problem description, the Planning\nAgent generates a single exemplar\u2014a relevant prob-\nlem along with its plan and solution. This mimics\nthe behavior of human programmers, who, when\nfaced with a new problem, first recall a similar\nproblem they've previously solved. This exemplar-\nbased recall is crucial as it provides a starting point\nfor constructing a solution plan. Instead of gener-\nating multiple ungrounded exemplars as in Map-\nCoder, our agent focuses on only one at a time.\nWe then instruct the LLM to generate an appro-\npriate plan. Once the plan is created, the LLM\nsimulates (step-by-step) the solution with a sample\ninput. If the simulation result does not match the\nexpected output, the agent prompts the LLM to re-\nvise the plan. Otherwise, the plan is deemed valid.\nIn the case of failure, the Planning Agent refines\nthe plan. The complete prompts for the Planning\nAgent-including plan generation, verification, and\nrefinement are provided in the Appendix (Figure\n5, 6, 7)."}, {"title": "3.2 Coding Agent", "content": "Next component is the Coding Agent, which takes\nthe problem description and the plan generated\nby the Planning Agent as input. The role of this\nagent is to translate the plan into executable code\nthat solves the given problem. Once the code is\ngenerated, CODESIM evaluates it using sample in-\nput/output test cases. If the code passes all sample\ntests, it is returned as the final solution. Otherwise,\nthe code is handed over to the next agent for further\nrefinement. Figure 8 in the Appendix provides the\ncomplete prompt used by the Coding Agent."}, {"title": "3.3 Debugging Agent", "content": "The final component, the Debugging Agent, re-\nceives the original problem, the plan from the\nPlanning Agent, the code generated by the Coding\nAgent, and the execution (unit testing) log as input\nto debug the code. To identify bugs, instead of di-\nrectly prompting the LLMs, we uniquely leverage\nthe simulation once again. The LLM is instructed\nspecifically to simulate the code on inputs where\nit fails to produce the expected output, allowing it\nto trace the execution step by step and locate the\nerror. Once the bug is identified, the LLM mod-\nifies the code to resolve the issue. The complete\nprompt for the Debugging Agent is shown in the\nAppendix (Figure 9). Unlike other approaches such\nas LATS (Zhou et al., 2023), AgentCoder (Huang\net al., 2023), and Reflexion (Shinn et al., 2023), our\nDebugging Agent does not require additional test\ncase generation. The rationale behind excluding\nthis phase is discussed in the Ablation Study 6.8."}, {"title": "3.4 Adaptive Iteration", "content": "CODESIM employs an adaptive iteration starting\nwith the Planning Agent, which generates plans for\nthe given problem. These plans are passed to the\nCoding Agent, which translates them into code and\ntests against sample I/Os. If all tests pass, the code\nis returned; otherwise, it's sent to the Debugging\nAgent. The Debugging Agent attempts to fix the"}, {"title": "4 Experimental Setup", "content": ""}, {"title": "4.1 Datasets", "content": "Following MapCoder, we evaluate CODESIM on\nfive basic programming benchmarks i.e., Hu-\nmanEval (Chen et al., 2021a), HumanEval-\nET (Dong et al., 2023a), EvalPlus (Liu et al.,\n2023), MBPP) (Austin et al., 2021), and MBPP-\nET (Dong et al., 2023a) and two competitive pro-\ngramming datasets i.e., APPS (Hendrycks et al.,\n2021), and CodeContest (Li et al., 2022b). For"}, {"title": "4.2 Baselines and Metric", "content": "To evaluate CODESIM, we compare it against\nstate-of-the-art code generation approaches, includ-\ning MapCoder, as well as several notable meth-\nods: Direct, Chain of Thought (CoT) (Wei et al.,\n2022b), Self-Planning (Jiang et al., 2023b), Ana-\nlogical Reasoning (Yasunaga et al., 2023), and\nSelf-collaboration (Dong et al., 2023b). For sim-\npler programming tasks, we include strong base-\nlines such as Reflexion (Shinn et al., 2023) and\nLATS (Zhou et al., 2023). We exclude AgentCoder\n(Huang et al., 2023) due to reproducibility issues\n(discussed in Appendix 10). For fair comparison,\nour evaluation utilizes ChatGPT (gpt-3.5-turbo-\n1106), GPT-4 (gpt-4-1106-preview) from OpenAI,\nalongside open-source LLMs such as Gemma2-\n9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-\n70B. For basic programming tasks, we report next-\ngeneration performance with additional evaluations\nusing GPT-40 (gpt-40-2024-08-06). We adopt the"}, {"title": "4.3 Reproducibility", "content": "We aim to contribute to the NLP community by\nopen-sourcing all of our code along with result\nlogs, enabling others to reproduce our findings. For\nsimple programming, we set the maximum number\nof planning tries to $p = 5$ and debugging tries to\n$d = 5$. For the competitive problem solving, we\nused $p = 3$ and $d = 3$ by default except for the\nCodeContest with GPT-4 where $p = 3$, $d = 5$."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Basic Code Generation", "content": "In Table 2, we evaluate the model performances\non simple code generation tasks. Overall,\nCODESIM demonstrates consistently superior per-\nformance compared to all other baselines across all\ndatasets and LLMs. Notably, CODESIM achieves\ntop scores with GPT-40, reaching 95.1% on Hu-\nmanEval, 87.2% on EvalPlus, and 90.7% on\nMBPP, resulting in an impressive 82.7% overall\naverage and their new state-of-the-art (SoTA) re-\nsults. This represents a significant improvement\nover the next best method, MapCoder, which scores\n79.0% on average with GPT-40. CODESIM's effec-\ntiveness is consistent across different model vari-\nants, outperforming other approaches with Chat-\nGPT (75.1% avg) and GPT-4 (81.3% avg) as\nwell. The method's robust performance across di-\nverse datasets, including the challenging MBPP-\nET where it achieves 61.5% with GPT-4, under-\nscores its versatility in handling various program-\nming tasks. These results strongly indicate that\nCODESIM's simulation-driven planning and debug-\nging approach marks a substantial advancement in\ncode generation and problem-solving capabilities,\nas it consistently outperformed other baselines."}, {"title": "5.2 Competitive Problem Solving", "content": "In Table 3, we evaluate performance on complex,\ncontest-level code generation tasks. CODESIM de-\nlivers significant improvements over other base-\nlines in solving complex contest-level code genera-\ntion tasks. With GPT-4, CODESIM reaches a strong\n29.1% on CodeContests and 22.0% on APPS,\nmarking a consistent edge over MapCoder's 25.3%\naverage. The performance gains are even more pro-\nnounced with ChatGPT, where CODESIM achieves"}, {"title": "5.3 Performance Across Open-source LLMs", "content": "To further demonstrate CODESIM's generaliza-\ntion capability, we evaluate its performance with\nopen-source LLMs, including Gemma2-9B, Mix-\ntral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As\nshown in Table 4, CODESIM consistently outper-\nforms all other methods across these models. On\nLLaMa3.1-70B, CODESIM achieves an accuracy"}, {"title": "6 Ablation Studies and Analyses", "content": ""}, {"title": "6.1 Impact of Different Agents", "content": "Our primary contributions are two folds: (i) the\nsimulation-guided plan verification step within the\nPlanning Agent and (ii) the bug fixing process\nthrough simulation in Debugging Agent. To evalu-\nate the significance of these components, we ablate\nthese two parts of our approach and present the\nresults in Table 5. The findings confirm that both\ncomponents contribute significantly."}, {"title": "6.2 Fine-grained Analysis of the Impact of Simulation", "content": "Table 6 presents the impact of incorporating\nSimulation in CODESIM. The results show\nthat CODESIM consistently outperforms other ap-\nproaches across both simple and multi-agent set-\ntings, demonstrating superior performance with\nboth open-source and proprietary LLMs. This high-\nlights the effectiveness of Simulation in enhancing\nproblem-solving efficiency within our pipeline."}, {"title": "6.3 Impact of Varying Programming Languages", "content": "To evaluate the performance of CODESIM across\nvarious programming languages, we utilized the"}, {"title": "6.4 Use of External Debugger", "content": "The performance of CODESIM can be further en-\nhanced by incorporating an external debugger in\nthe second pass. We experiment with LDB as\nthe external debugger on HumanEval dataset in\nTable 8. We use the output code from the most\ncompetitive first-pass generation methods, includ-\ning CODESIM, Reflexion, and MapCoder, using\nGPT-40 as the backbone. These seed programs are\nthen passed to LDB, which was tested with two\ndifferent LLMS: ChatGPT and GPT-40. As can\nbe seen, CODESIM achieves 95.1% accuracy in"}, {"title": "6.5 Qualitative Example", "content": "We also conduct a qualitative analysis to better\nunderstand how CODESIM improves performance\nacross various datasets. Figure 2 demonstrates how\nCODESIM enhances the plan through simulation\nand assists in debugging the code using the same\ntechnique. A complete example, including LLM\noutput, is provided in Appendix 12."}, {"title": "6.6 Impact of p and d", "content": "CODESIM includes two key hyperparameters: the\nmaximum number of planning steps ($p$) and the\nmaximum number of debugging steps ($d$). By vary-\ning these parameters, we plot the results in Figure\n3, which shows a proportionate improvement in\nperformance. It is important to note that higher val-\nues of $p$ and $d$ lead to more API calls and increased\ntoken consumption, allowing users to adjust these\nparameters to balance between accuracy and cost."}, {"title": "6.7 Impact of Number of Sample I/Os", "content": "The HumanEval dataset has an average of only\n2.82 sample I/Os per example, which is a relatively\nsmall number for deriving meaningful insights. In\nthis ablation, we augment the dataset by adding 5\nmore sample I/Os from the HumanEval-ET dataset.\nThis augmentation increases performance notably,"}, {"title": "6.8 Impact of Synthesizing Additional I/O", "content": "Increasing the number of sample I/Os for testing\ncan enhance the overall performance of our ap-\nproach, as indicated in 6.7. Based on this insight,\nwe use a self-consistency (Wang et al., 2023a)\nmethod to generate additional test cases. We in-\nstruct the LLM to generate five more test cases\nfor each problem, covering both basic and edge\ncases. The LLM is called twice, and we select the\ntest cases that are present in both responses. How-\never, this approach results in a performance decline.\nWith ChatGPT we achieve 78% accuracy-a 9.3%\ndecrease from the original 86%. This indicates that\ngenerating additional I/Os is a non-trivial task that\nmay negatively impact final outcomes."}, {"title": "6.9 API Call and Token Analysis", "content": "We compare the API calls and token consumption\nof our approach with the previous state-of-the-art\nmethod, MapCoder (Islam et al., 2024a), as shown\nin Table 9. The results reveal that CODESIM not\nonly improves performance but also reduces token\nconsumption. On average, CODESIM uses 4.13\nthousand fewer tokens while achieving a 7.1% in-\ncrease in accuracy, proving that CODESIM is more\nefficient in both accuracy and token usage com-\npared to MapCoder."}, {"title": "6.10 Error Analysis and Challenges", "content": "Although CODESIM demonstrates strong\nperformance compared to other methods, it faces\nchallenges in specific algorithmic domains. The\nAPPS dataset (Hendrycks et al., 2021) includes\nproblems with three levels of difficulty: (i)\nIntroductory, (ii) Interview, and (iii) Competition.\nFigure 4 illustrates the performance of different\napproaches based on difficulty level. The results\nindicate that for introductory and interview-level\nproblems, CODESIM does not surpass MapCoder\nwhen using ChatGPT. Additionally, when using\nGPT-4, CODESIM struggles to outperform\nMapCoder on interview-level problems. Upon\nmanual review, we observe that for more complex\nissues, such as dynamic programming (DP),\nCODESIM encounters difficulties in constructing\nthe DP table."}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we introduce CODESIM, a novel\nframework that leverages the multi-agent\nprompting capabilities of LLMs for efficient\ncode generation in problem-solving tasks.\nCODESIM integrates three agents-planning,\ncoding, and debugging to effectively solve\nprogramming problems. It harnesses the power\nof simulation for plan verification and debugging,\nsignificantly outperforming existing state-of-the-art\napproaches by a wide margin. Future work will\nfocus on extending this approach to other domains\nsuch as mathematical reasoning and question\nanswering broadening its scope and impact."}, {"title": "8 Limitations", "content": "In Section 6.4, we observe that utilizing an exter-\nnal debugger can further enhance our results. Our\nnext research goal is to achieve the best perfor-"}, {"title": "9 Algorithm of CODESIM", "content": "Algorithm 1 shows the pseudo-code of our prompt-\ning technique."}, {"title": "10 Exclusion of AgentCoder", "content": "We have not included AgentCoder (Huang et al.,\n2023) in our comparison due to reproducibility is-\nsues which undoubtedly plays a critical role in fair\ncomparison as indicted in Laskar et al. (2024), as\nwe were unable to replicate their results. In our at-\ntempts to reproduce their work on the HumanEval\nbenchmark using ChatGPT, we achieved 56.7%\naccuracy after four iterations, consuming 11.9 mil-\nlion tokens. When using GPT-4, we attained only\n17.7% accuracy after two iterations, with 10.4 mil-\nlion tokens consumed. The token consumption in\nboth cases is significantly higher compared to Map-"}, {"title": "11 Details Promptings of CODESIM", "content": "The Planning Agent interacts with the LLM three\ntimes to generate a plan. In the first API call, it\ninstructs the LLM to comprehend the problem, gen-\nerate an example problem, recommend a suitable\nalgorithm, and finally produce the plan (Figure 5).\nIn the second API call, the LLM is instructed to\nverify the plan through simulation (Figure 6). If\nthe plan is satisfactory, it is returned by the agent.\nOtherwise, the LLM is called again to refine the\nplan based on the feedback from the simulation\n(Figure 7).\nThe next step involves the Coding Agent, which\nreceives the plan from the Planning Agent and uses\nthe prompt outlined in Figure 8 to generate code.\nIf the code fails to pass the sample input/output,\nCODESIM activates its final agent, the Debugging\nAgent, using the prompt shown in Figure 9.\nThese figures also include the rationale behind\nthe inclusion of each sentence in the prompt."}, {"title": "12 Example Problem", "content": "We present a complete example of problem solving\nusing CODESIM below:"}]}