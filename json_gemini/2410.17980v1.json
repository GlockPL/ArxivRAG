{"title": "STICK-BREAKING ATTENTION", "authors": ["Shawn Tan", "Yikang Shen", "Songlin Yang", "Aaron Courville", "Rameswar Panda"], "abstract": "The self-attention mechanism traditionally relies on the softmax operator, necessitating positional embeddings like RoPE, or position biases to account for token order. But current methods using still face length generalisation challenges. We propose an alternative attention mechanism based on the stick-breaking process: For each token before the current, we determine a break point $B_{i,j}$, which represents the proportion of the remaining stick to allocate to the current token. We repeat the process until the stick is fully allocated, resulting in a sequence of attention weights. This process naturally incorporates recency bias, which has linguistic motivations for grammar parsing (Shen et al., 2017). We study the implications of replacing the conventional softmax-based attention mechanism with stick-breaking attention. We then discuss implementation of numerically stable stick-breaking attention and adapt Flash Attention to accommodate this mechanism. When used as a drop-in replacement for current softmax+RoPE attention systems, we find that stick-breaking attention performs competitively with current methods on length generalisation and downstream tasks. Stick-breaking also performs well at length generalisation, allowing a model trained with $2^{11}$ context window to perform well at $2^{14}$ with perplexity improvements.", "sections": [{"title": "1 INTRODUCTION", "content": "The Transformer architecture (Vaswani et al., 2017) uses a self-attention mechanism based on the softmax operator that enables the model to weigh the importance of different tokens in the input data. However, the reliance on softmax requires using positional embeddings to introduce information about the order of tokens, as the attention mechanism itself is permutation-invariant. The sinusoidal position embedding as proposed in Vaswani et al. (2017) has since evolved to relative positional embeddings (Shaw et al., 2018). Learned relative positional biases were used in the T5 model (Raffel et al., 2020), and later fixed relative positional biases in Press et al. (2021). At the time of this writing, a commonly used form of position embedding is RoPE (Su et al., 2021). Allen-Zhu & Li (2023) observe that, in a context-free grammar parsing setting, attention mechanisms attend to the \"most adjacent\" non-terminal. This suggests an inclination to attend to the most recent entry that matches a given criteria. However, even with relative position information, it is possible to overfit on specific relative positions, resulting in failure to generalise. Kazemnejad et al. (2024) show that decoder-only Transformers with No Positional Embeddings (NoPE) can implicitly recover positional information, experimental results suggest that NoPE Transformers generalise better on length. While this is promising, a higher attention score from an irrelevant token in the sequence can function as a distractor (Kazemnejad et al., 2024; Xiao et al., 2023).\nThe stick-breaking process may have properties that can alleviate the previously mentioned issues, and possess the 'most recent' bias from Allen-Zhu & Li (2023) we want. For a token at position $j$ attending to position $i$, suppose the attention weight is given by:\n$A_{i,j} = \\beta_{i,j} \\prod_{i<k<j} (1 - \\beta_{k,j}) = \\sigma(z_{i,j}) \\prod_{i<k<j} (1-\\sigma(z_{k,j})),$"}, {"title": "2 STICK-BREAKING ATTENTION", "content": "For a sequence of $L$ tokens, we have query $q_i \\in \\mathbb{R}^{d_{head}}$, key $k_i \\in \\mathbb{R}^{d_{head}}$, and value $v_i \\in \\mathbb{R}^{d_{head}}$ vectors for $1 < i < L$. Then the attention weight for token at $j$ attending to position $i$ is computed by:\n$\\theta_{j} = \\sum_{i=1}^{j-1} A_{i,j} v_i, A_{i,j} = \\beta_{i,j} \\prod_{i<k<j} (1 - \\beta_{k,j}), \\beta_{i,j} = \\sigma (z_{i,j}), z_{i,j} = \\frac{q_i^T k_j}{\\sqrt{d_{head}}}.$ (1)\nEquation 1 is the main difference between our proposal and softmax attention. As discussed earlier and in Csord\u00e1s et al. (2021), this parameterisation biases towards recency. Specifically, for any pair of $i$ and $i'$ such that $i - j < i' - j$ and $z_{i,j} = z_{i',j}$, then $A_{i,j} > A_{i',j}$. Consequently, this imposes an ordering on the attention, and we do not use position embeddings with the query and key embeddings.\nWe consider two sets of logits, $z_{i,j}$ and $z_{i',j}$ and their respective attention weights $A_{i,j}$ and $A'_{i,j}$. If $z_{i,k} = z_{i',k}$ for $i < i' < k < j$, then $A_{i,j} = A_{i',j}$. Further, if $\\sigma(z_{i,k}) = \\sigma(z_{k,j}) = 1$, then the output is invariant to appending additional context earlier than $i$. This means that unlike softmax attention, a high attention score further back in the sequence does not 'distract' from a more recent high $z_{i,j}$ score.\nWe note that $\\sum_{i=1}^{j-1} A_{i,j} \\leq 1$, which allows this attention mechanism to attend to nothing when all $\\beta_{i,j} = 0$."}, {"title": "3 RELATED WORK", "content": "Stick-breaking Process The stick-breaking process formulation of the Dirichlet process (Sethuraman, 1994) is also known as the GEM distribution, first coined in Ewens (1990) after Griffiths (1989), Engen (1975), and McCloskey (1965). The GEM is a specific case of what was known as a Residual Allocation Model (RAM; Allen & Lambie 1976). There are instances of the distribution being used as a differentiable attention-like mechanism in neural models. Shen et al. (2017) used stick-breaking process for modelling language, and showed that the model can induce grammatical structure to some extent. Csord\u00e1s et al. (2021) used stick-breaking attention, which they refer to as Geometric attention, in a bidirectional encoder set up. Shen et al. (2023) used stick-breaking attention in a decoding-only setup, but does not explicitly study the properties of stick-breaking.\nSoftmax attention, Positional embeddings, and Length Generalisation Bondarenko et al. (2024) observe that softmax attention tends to attend to low-information tokens with high scores in order to 'do nothing'. Xiao et al. (2023) introduces attention sinks, a learnable token that the attention can assign attention weights to. Irie et al. (2019) and Haviv et al. (2022) find that in a decoder-only setting, a Transformer with no positional embedding can work fairly well. Kazemnejad et al. (2024) also found similar results, while also showing that NoPE has a tendency to attend to the start of the sequence, while ALiBi (Press et al., 2021) has a tendency to only attend to the most recent tokens. However, Zhou et al. (2024) later found that Transformers without position embeddings do not generalise to out-of-distribution sequence lengths for an addition task. At present, Rotary Positional Embeddings (RoPE; Su et al., 2021) are the most commonly used position embedding. It encodes relative positions via multiplicative interactions with the key and query. RoPE has been found to generalise to out-of-distribution lengths poorly (Press et al., 2021; Zhou et al., 2024; Kazemnejad et al., 2024), but a common trick to extend the context window RoPE-based Transfomers is to use NTK-aware ROPE scaling (bloc97, 2023).\nConditional Computation The use of stick-breaking for conditional computation has also been explored. Tan & Sim (2016) uses a the stick-breaking distribution as a mixture over outputs for each layer in an MLP for an acoustic model. Graves (2016) also suggested a similar formulation for language modelling. Later, Banino et al. (2021) and Tan et al. (2023) also use a stick-breaking formulation for dynamic depth modelling in a Transformer model. These prior works use conditional computation on the depth of the model, while in our case, we use stick-breaking as a method of restricting the computation length-wise.\nConnection to Selective State-space Models Each stick-breaking attention head at every time-step can be viewed as the hidden state of the final step of a selective State-space Model (SSM;Gu & Dao 2023). For a given time-step $j$, consider the following SSM and its convolutional form (as described in Merrill et al. 2024):\n$\\hat{o}_{i,j} = (1 - \\beta_{i,j}) \\cdot \\hat{o}_{i-1} + \\beta_{i,j} v_j, o_j = o_{j-1} = \\sum_{1<i<j} \\beta_i \\prod_{i<k<j} (1 - \\beta_k) \\cdot v_i,$ (2)\nwhich is equivalent to the first term in Equation 1. Typically, an attention layer for a Transformer with hidden dimension $d_{hidden}$ has $h$ heads such that $d_{hidden} = h d_{head}$. For equivalence with an SSM, we need a constant query vector and each dimension as a separate head, e.g. $q_i = 1$ for all $i$, and $h = d_{hidden}, d_{head} = 1$.\nConnection to Additive Relative Position Encoding (Additive RPE; Kazemnejad et al. 2024) Generally, Additive RPEs incorporate an added bias function $g$ of the distance of the tokens $i - j$ and the maximum length of the sequence $L$:\n$A_{ij} \\propto exp (q_i^T k_j + b)$\nIn the case of ALiBi (Press et al., 2021), this is a linear function $b = -m \\cdot (j - i)$. This implies that the attention weights will drop off exponentially the further $j$ and $i$ are apart, regardless of the attention scores. In stick-breaking, Equation 3 has a form that accounts for the scores from $j$ to $i$ with the bias $b = - \\sum_{k=i+1}^{j-1} log (1 + exp(z_{k,j}))$. Specifically, if $log (1 + exp(z_{k,j})) \\geq m$, then $b \\leq -m \\cdot (j-i)$. This suggests a learnable relative position bias that is dependent on the intermediate scores between $j$ and $i$."}, {"title": "4 IMPLEMENTATION", "content": "Implementing stick-breaking attention naively in PyTorch results in realising the $L^2$ matrix for the attention logits (where $L$ is the length of the input). FlashAttention (Dao et al., 2022) reduces the memory footprint of attention by side-stepping the $O(L^2)$ memory complexity of realising the attention matrix. In order to achieve this, it only realises tiles of the attention logits and weights at a time, and accumulates the resulting weighted sum of $v_i$. In this section we detail the important differences between computing softmax attention and stick-breaking.\nForward Computing Equation 1 directly will result in underflow issues, especially with lower precision training. We perform the operations in log-space, which results in a cumulative sum instead:\n$A_{ij} = exp \\bigg(log \\beta_{i,j} + \\sum_{k=i+1}^{j-1} log(1-\\beta_{k,j})\\bigg) = exp \\bigg(\\sigma z_{i,j} - \\sum_{k=i+1}^{j-1} log (1 + exp(z_{k,j}))\\bigg)$ (3)\nWhere $log (1 + exp(\\cdot))$ is commonly known as the softplus operation, which we further numerically stabilise with the following computation:\n$softplus(x) = \\begin{cases} log (1 + exp(x)), &\\text{if } x \\leq 15\\\\ x &\\text{otherwise} \\end{cases}$ (4)\nto prevent overflowing of exp(x).\nBackward Let $\\tilde{A}_{i,j} = log A_{i,j}$, then:\n$\\frac{\\partial \\mathcal{L}}{\\partial Q}= \\sum_k \\frac{\\partial \\mathcal{L}}{\\partial A_{k,j}} A_{k,j}, \\frac{\\partial \\mathcal{L}}{\\partial k}= \\frac{\\partial \\mathcal{L}}{\\partial z_{i,j}} \\frac{\\partial z_{i,j}}{\\partial \\beta_{i,j}}, \\sum_{ k=i+1}  -\\sigma (i,j)\\sum_k \\frac{\\partial \\mathcal{L}}{\\partial A_{ k,j}}$  (5)\nContribution from before i,j Contribution from i,j\nThe above equations dictate the direction of the order of computation for our implementation. For the forward pass (Eqn. 3), we compute from $j$ to 1, backwards through time and accumulate $\\sum_{k=i+1}^{j-1} log (1 + exp(z_{k,j}))$. For backward pass (Eqn. 5), we compute from 1 to $j$, accumulating $\\sum_{i=1}^{j-1}.$"}, {"title": "4.1 TRITON SPECIFICS", "content": "We modify the Triton implementation of Flash Attention for accelerating the stick-breaking attention. In theory, the memory complexity of implementing stick-breaking attention would be similar to"}, {"title": "5 EXPERIMENTS", "content": "In this section, we compare existing attention methods against stick-breaking. We first look at a modification of a synthetic task from Arora et al. (2023) to understand the inductive biases of stick-breaking. We then compare the stick-breaking against existing length extrapolation methods on a 350M model setting. We then pretrain a 1B parameter model, and evaluate it on various NLP benchmarks, and evaluate it for length extrapolation and retrieval capabilities on long context using the RULER benchmark (Hsieh et al., 2024). We also report benchmark results for a 3B model we have trained. For reference, the size of the models are detailed in Table 1."}, {"title": "5.1 MULTI-QUERY REPEATED ASSOCIATIVE RECALL TASK", "content": "To illustrate the inductive bias of stick-breaking attention, we first analyse its behaviour on a simple synthetic task. Arora et al. (2023) demonstrate that there is a correlation between multi-query associative recall (MQAR) and the performance of language modelling. They show that while Transformers, which are based on attention can easily handle MQAR, the current linear state-space models cannot. While stick-breaking can also solve the MQAR task, we formulated a different version of this toy task and tested it on both softmax attention and stick-breaking. As before, each instance of the task has an initial assignment of values to variables. However, in the query sequence, the same variable can be queried multiple times, and each query is then followed by a variable assignment, which successive queries must recall. We refer to this task as multi-query repeated associative recall (MQRAR)."}, {"title": "5.2 350M MODEL LENGTH EXTRAPOLATION", "content": "We test the ability of stick-breaking for length generalisation, where we train on a fixed context length ($L = 1024$) and test it on longer context lengths. We started with the LLaMa 2 (Touvron et al., 2023) architecture, and modified the attention module to use the various baselines we compare against:"}, {"title": "5.3 MODEL PRETRAINING", "content": "We pretrain the 1B and 3B models in this section using a two-stage training scheme in Hu et al. (2024) and the Power learning rate schedule (Shen et al., 2024). In the first stage, there is a warmup for the learning rate to 0.01, then we apply Power decay. Our training corpus has 1T tokens and mixes large-scale open-source datasets of medium quality with permissive licenses. In the second stage, we exponentially decay the learning rate to zero. The stage 2 training corpus is a mix of stage 1 data and a small amount of high-quality open-source and synthetic corpora with permissive licenses. The training batch size is 1024 and uses padding-free sequence packing for training in Dolomite Engine (Mishra, 2024). We evaluate the pretrained models on language model tasks and multiple-choice tasks from LM evaluation Harness (Gao et al., 2023). The multiple-choice tasks include: grade-school science questions (ARC; Clark et al. 2018), common sense reasoning (Hellaswag; Zellers et al. 2019), open book question answering (OpenBookQA; Mihaylov et al. 2018), physical questions (PIQA; Bisk et al. 2020), reading comprehension (RACE; Lai et al. 2017), and Winograd schema task (Winogrande; Sakaguchi et al. 2021). Table 2 shows the performance.\nOverall, we outperform our own pretrained standard attention models that are trained on the same settings. We perform better on average, and attain better perplexity on Wikitext. On the 1B experiment, we find that the stick-breaking without remainder performs better, so we only pretrained a 3B parameter model on this setting. Subsequent experiments are also run only on the non-remainder setting. We also evaluated the models on MMLU with 0-shot and 5-shot settings. Due to the inductive bias of stick-breaking, we believed that stick-breaking would perform better on MMLU in a few-shot setting as it would not be distracted by the few-shot examples provided in the context.\nStick-breaking performs surprisingly well even in the O-shot setting, and improves with few-shot examples provided. Note that this may not always be the case, as in our Softmax + ROPE model, the performance decreases with few-shot examples in context."}, {"title": "5.3.1 1B LENGTH EXTRAPOLATION", "content": "We test the 1B stick-breaking and softmax model with the RULER benchmark (Hsieh et al., 2024). The benchmark consists of 'needle-in-a-haystack'-like tasks, and is generally used for testing retrieval capabilities of long-context models that are trained specifically for long contexts. In our setting, we use RULER to evaluate both 1B models trained on 4096 contexts. Accordingly, the general capabilities of these models on longer contexts are much worse than purpose-trained models.\nOn average, the performance of stick-breaking dominates Softmax + ROPE with scaling (Figure 6a). In the breakdown, we find that the stick-breaking model is surprisingly good at extrapolating on NIAH tasks up to 16K context lengths, while the standard model significantly drops in performance. Our results on the variable tracking (VT) task agrees with our experiments on MQRAR. The task involves tracking the variable assignments provided in the context, and we find that even in-distribution ($L = 4096$), the standard model does not perform well at this task."}, {"title": "6 CONCLUSION & FUTURE WORK", "content": "We propose a formulation for using the stick-breaking process as a replacement for softmax for attention. Stick-breaking attention allows us to do away with position embeddings, while still retaining model performance. We detail the specifics of implementing the stick-breaking kernel in Triton for large scale training. We then demonstrate that stick-breaking is good at length extrapolation, performing better than other position embedding and position bias methods in our 350M class models. We also show that our pretrained stick-breaking models peform better in a controlled experiment, given the same training data and training regime. On retrieval in the RULER benchmark, stick-breaking outperforms softmax attention.\nThe drawbacks in efficiency can be improved in future work by making similar optimisations that Flash Attention (Dao et al., 2022; Dao, 2023) made for speedups. These include making full use of features in CUDA, and by cache retrieval optimisations. We believe there is plenty of room for improvement in computation efficiency that can be made in future versions of stick-breaking. Overall, we find stick-breaking attention to be a promising replacement for Softmax + ROPE in Transformer models."}]}