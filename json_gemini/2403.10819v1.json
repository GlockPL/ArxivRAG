{"title": "Incentivized Exploration of Non-Stationary Stochastic Bandits", "authors": ["Sourav Chakraborty", "Lijun Chen"], "abstract": "Abstract-We study incentivized exploration for the multi-\narmed bandit (MAB) problem with non-stationary reward\ndistributions, where players receive compensation for exploring\narms other than the greedy choice and may provide biased\nfeedback on the reward. We consider two different non-\nstationary environments: abruptly-changing and continuously-\nchanging, and propose respective incentivized exploration algo-\nrithms. We show that the proposed algorithms achieve sublinear\nregret and compensation over time, thus effectively incentivizing\nexploration despite the nonstationarity and the biased or drifted\nfeedback.", "sections": [{"title": "I. INTRODUCTION", "content": "The multi-armed bandit (MAB) problem is one of basic\nmodels for sequential decision-making under uncertainty,\nwith diverse applications in areas such as clinical trials [1]-\n[3], financial portfolio design [4], recommendation systems\n[5], [6], search engine systems [7], and cognitive radio\nnetworks [8]. In the traditional MAB model, a decision\nmaker iteratively selects an arm (or action) to pull at each\ntime step, receives a certain reward from the environment,\nand decides on the arm for the next iteration. In the so-called\nstochastic MAB model, each arm's reward distribution is\nunknown but remains fixed over time (hence, the 'stationary'\nbandit\nsetting).\nThe objective of the decision maker (or the MAB algo-\nrithm) is to minimize the expected regret over the entire time\nhorizon, defined as the expectation of the difference between\nthe total reward obtained by pulling the best arm and the\ntotal reward obtained by the algorithm. Minimizing regret\nis achieved by balancing exploitation, the use of acquired\ninformation, with exploration, acquiring new information. If\nthe decision maker always pulls the arm believed to be the\nbest (i.e., exploitation only), they may miss the opportunity\nto identify another arm with a potentially higher expected\nreward. On the other hand, if the decision maker excessively\nexplores various arms (i.e., exploration only), they will fail\nto accumulate as much reward as possible.\nIn this setup, the decision maker (the principal) and the\nplayer (the agent) who pulls the arm are assumed to be\nthe same entity striving to balance exploitation and explo-\nration. However, this may not always be the case in the\nreal world. Many scenarios exist where the principal and\nthe agent are different entities with different interests. The\nagent may select the currently best-performing arm in the\nface of uncertain reward (i.e., exploitation only), while the\nprincipal is interested in identifying the best-performing arm"}, {"title": "II. PRELIMINARIES", "content": "At each time step $t\\in \\{1,2,\\ldots,T\\}$, a decision maker\nchooses to arm $a$ from a set of $K$ arms based on the\nsequence of past arm pulls and received rewards and obtains\na reward $X_t(a)$. The rewards for each arm are modeled by\na sequence of independent and identically distributed (i.i.d.)\nrandom variables from an unknown distribution. Without loss\nof generality, the reward of each arm is assumed to be in\n$[0, 1]$. Denote by $\\mu(a)$ the expectation of the reward of arm $a$,\nand $a^*$ the optimal arm with the highest expected reward $\\mu^*$.\nThe benchmark (or optimal) performance comprises pulling\nthe optimal arm $a^*$ at every time step. The regret $R_T$ of an\nalgorithm is defined as the difference between the benchmark\nperformance and the total rewards collected by the algorithm:\n$R_T = \\sum_{t=1}^{T} (\\mu^* - X_t(a_t))$,\nwhere $a_t$ is the arm the algorithm pulls at time $t$. A stochastic\nbandit algorithm's performance is typically evaluated by how\nthe expected value of $R_T$ scales with the time horizon $T$, and\nthe goal is to design algorithms that achieve sub-linear ex-\npected regret in $T$. The most common algorithms achieving a\nsub-linear regret are UCB1 ([29], [30]), Thompson Sampling\n([31]), and $\\epsilon$-Greedy ([29], [10]).\nIn the non-stationary setting, the rewards $X_t(a)$ for arm $a$\nare modeled by a sequence of independent random variables\nfrom potentially different distributions that are unknown and\nmay change across time. Denote by $\\mu_t(a_t)$ the expectation\nof the reward $X_t(a_t)$ for arm $a_t$ at time step $t$. Similarly, let\n$a_t^*$ be the arm with the highest expected reward, denoted by\n$\\mu_t^*$, at time $t$. The benchmark performance of the algorithm\nwould be achieved by pulling the optimal arm $a_t^*$ at every"}, {"title": "A. Standard stochastic (stationary) MAB Problem", "content": "At each time step $t\\in \\{1,2,\\ldots,T\\}$, a decision maker\nchooses to arm a from a set of $K$ arms based on the\nsequence of past arm pulls and received rewards and obtains\na reward $X_t(a)$. The rewards for each arm are modeled by\na sequence of independent and identically distributed (i.i.d.)\nrandom variables from an unknown distribution. Without loss\nof generality, the reward of each arm is assumed to be in\n$[0, 1]$. Denote by $\\mu(a)$ the expectation of the reward of arm a,\nand $a^*$ the optimal arm with the highest expected reward $\\mu^*$.\nThe benchmark (or optimal) performance comprises pulling\nthe optimal arm $a^*$ at every time step. The regret $R_T$ of an\nalgorithm is defined as the difference between the benchmark\nperformance and the total rewards collected by the algorithm:\n$R_T = \\sum_{t=1}^{T} (\\mu^* - X_t(a_t))$,\nwhere $a_t$ is the arm the algorithm pulls at time $t$. A stochastic\nbandit algorithm's performance is typically evaluated by how\nthe expected value of $R_T$ scales with the time horizon $T$, and\nthe goal is to design algorithms that achieve sub-linear ex-\npected regret in $T$. The most common algorithms achieving a\nsub-linear regret are UCB1 ([29], [30]), Thompson Sampling\n([31]), and $\\epsilon$-Greedy ([29], [10])."}, {"title": "B. Non-stationary MAB Problem", "content": "In the non-stationary setting, the rewards $X_t(a)$ for arm $a$\nare modeled by a sequence of independent random variables\nfrom potentially different distributions that are unknown and\nmay change across time. Denote by $\\mu_t(a_t)$ the expectation\nof the reward $X_t(a_t)$ for arm $a_t$ at time step $t$. Similarly, let\n$a_t^*$ be the arm with the highest expected reward, denoted by\n$\\mu_t^*$, at time $t$. The benchmark performance of the algorithm\nwould be achieved by pulling the optimal arm $a_t^*$ at every\ntime step $t$. The corresponding regret $R_T$ is defined as:\n$R_T = \\sum_{t=1}^{T} (\\mu^* - X_t(a_t))$.\nThe goal is to design algorithms that achieve sub-linear\nexpected regret in $T$ too. In this paper, we consider two\ncommonly-used models of non-stationarity: (i) abruptly\nchanging environment ([32], [33]) and (ii) continuously\nchanging ([34]) environment. The algorithms discussed be-\nlow for both environments achieve sub-linear expected regret.\na) Abruptly Changing Environment: The reward distri-\nbutions remain fixed during certain periods and change at\nunknown time instants called breakpoints. Denote by $\\beta_T$ the\ntotal number of breakpoints that occur before time $T$. As\nshown in [35], standard bandit algorithms are not appropriate\nfor this environment, and therefore several methods have\nbeen proposed. The most relevant to this paper are the two\nextensions of the UCB ([29]) algorithms: Discounted UCB\n(DUCB) [32] and Sliding Window UCB (SWUCB) [33].\nThe DUCB algorithm (Algorithm 1) uses a discount factor\nto emphasize recent rewards when calculating their average.\nSpecifically, the algorithm uses a discount factor $\\gamma \\in (0,1)$\nto calculate the average of the observed rewards:\n$\\bar{X}_t(\\gamma, a) = \\frac{1}{N_t(\\gamma, a)} \\sum_{s=1}^{t} \\gamma^{t-s} 1(a_s = a) X_s(a)$.\nHere, $N_t(\\gamma, a)$ is the discounted frequency of arm $a$ until\ntime $t$. The algorithm further constructs an upper confidence\nbound $\\bar{X}_t(\\gamma, a) + c_t(\\gamma, a)$ on the average reward with\n$c_t(\\gamma, a) = 2\\sqrt{\\xi \\frac{\\log n_t(\\gamma)}{N_t(\\gamma, a)}}$\nas the discounted confidence radius (for some constant $\\xi$\ntuned based on the context; see [33] for more details). Note\nthat $n_t(\\gamma) = \\sum_{a=1}^{K} N_t(\\gamma, a)$ is the sum of the discounted\nfrequencies for all arms until time $t$. Notice that for $\\gamma = 1$,\nDUCB recovers the UCB1 algorithm.\nWith the SWUCB algorithm (Algorithm 2), instead of\naveraging rewards over the entire history with a discount\nfactor, averages are computed based on a fixed-size horizon.\nAt each time step $t$, SWUCB utilizes a local empirical\naverage of the most recent arm pulls to construct an upper\nconfidence bound $\\bar{X}_t(\\tau, a) + c_t(\\tau, a)$ for the expected reward.\nThe local empirical average is defined as:\n$\\bar{X}_t(\\tau, a) = \\frac{1}{N_t(\\tau, a)} \\sum_{s=t-\\tau+1}^{t} 1(a_s = a) X_s(a)$\nwith $N_t(\\tau, a)$ the frequency of selecting arm a in the last $\\tau$\narm pulls. The confidence radius is defined as:\n$c_t(\\tau, a) = \\sqrt{\\xi \\frac{\\log(\\min(t, \\tau))}{N_t(\\tau, a)}}$\nwith some constant $\\xi$ (see [33] for more details). Notably,\nin [33] the authors have demonstrated that both DUCB\nand SWUCB achieve a regret of $\\tilde{O}(\\sqrt{\\beta_T T})$, where $\\tilde{O}(.)$\ndisregards logarithmic terms."}, {"title": "Algorithm 1 Discounted UCB", "content": "1: for t from 1 to K, pull arm $a_t = t$;\n2: for t from K + 1 to T, pull arm $a_t$ which maximizes\nthe upper confidence bound $\\bar{X}_t(\\gamma, a_t) + c_t(\\gamma,a_t)$"}, {"title": "Algorithm 2 Sliding Window UCB", "content": "1: for t from 1 to K, pull arm $a_t = t$;\n2: for t from K + 1 to T, pull arm $a_t$ which maximizes\nthe upper confidence bound $\\bar{X}_t(\\tau, a_t) + c_t(t, a_t)$\nb) Continuously Changing Environment: Here the\nnumber of changes in the mean rewards can potentially be\ninfinite, but the total variation over a relevant time horizon is\nbounded by a variation budget; see, e.g., [34]. Specifically,\nfor a time horizon of $T$, we define the variation budget $V_T$\nas a non-decreasing sequence of positive numbers $\\{V_t\\}_{t=1}^T$\nsuch that $V_1 = 0$ and $K V_t < t$, with $K$ the number of arms.\nRecall that $\\mu_t(a)$ is the expected regret of arm a at time\nt. Denote by $\\mu(a) = \\{\\mu_t(a)\\}_{t=1}^T$ the sequence of expected\nrewards of arm $a$, and $\\mu = \\{\\mu(a)\\}_{t=1}^T$ the sequence of\nexpected rewards of all $K$ arms. The set V of permissible\nreward sequences for each arm can be written as:\n$\\mathbb{V} = \\{\\mu \\in [0,1]^{K\\times T} : \\sup_{\\alpha\\in[1,K]} \\sum_{t=1}^{T} |\\mu_t(a) - \\mu_{t+1}(a)| \\leq V_T \\}$.\nThe above set of permissible reward sequences can capture\nvarious scenarios where the expected rewards may change\ncontinuously, in discrete shocks, or adhere to a certain rate\nof change.\nFor different permissible reward sequences, the achievable\nregrets may be different. We consider their supremum:\n$R_T = \\sup_{\\mu \\in \\mathbb{V}} \\mathbb{E} \\left[\\sum_{t=1}^{T} \\mu_t^* - X_t(a_t) \\right]$.\nIn [34] the authors provided a near-optimal algorithm with\na worst-case regret of $O (V_T^{1/3}T^{2/3})$."}, {"title": "C. Incentivized Exploration", "content": "As mentioned in Section I, the principal and the agents\nmay have different interests in many real-world scenarios.\nThe principal would like the agents to select the arms in such\na way as to adequately explore different arms to maximize\nthe accumulated rewards. An agent, however, influenced by\nthe feedback of others, behaves myopically in the face of\nuncertainty, i.e., pulls the arm with the currently highest\nempirical reward (exploitation only).\nSimilar to [21], we consider a variant of the MAB problem\nwhere a principal aims to incentivize the agents to explore.\nAt each time step $t$, an agent pulls one arm $a_t$ based on\nthe recommendation of the principal. The agent receives\na reward $r_t$, which is then fed back to the principal and\nthe agents. The principal uses a certain bandit algorithm\nto find the 'optimal' arm while balancing exploration and\nexploitation. When the principal wants to encourage agents\nto explore, they may offer compensation $\\lambda_t$ to the agents."}, {"title": "Algorithm 3 Incentivized MAB under Reward Drift", "content": "1: for $t\\in [1,T]$ do\n2: $\\qquad a_t$ Principal's Recommendation\n3: $\\qquad g_t \\leftarrow \\arg \\max_{a\\in[1,K]} \\bar{X}_t(a)$\n4: $\\qquad$ if $a_t \\neq g_t$ then\n5: $\\qquad\\qquad$ Principal offers compensation of $\\lambda_t \\leftarrow \\bar{X}_t(g_t) -\\bar{X}_t(a_t)$ to the agent.\n6: $\\qquad\\qquad$ Reward for pulling arm $a_t$ is $r_t \\leftarrow \\bar{X}_t(a_t) + \\delta_t$ where reward drift $\\delta_t \\leftarrow f(\\lambda_t)$\n7: $\\qquad$ else\n8: $\\qquad\\qquad r_t \\leftarrow \\bar{X}_t(a_t)$ is the reward with no compensation.\n9: $\\qquad$ end if\n10: end for"}, {"title": "III. INCENTIVIZED EXPLORATION IN NON-STATIONARY\nBANDITS", "content": "In this section, we design algorithms for the incentivized\nexploration for nonstationary bandits and show that they\nachieve sublinear regret and compensation.\nAlgorithm 3 describes a framework of incentivized explo-\nration for the abruptly changing environment. At time $t$, the\nprincipal recommends an arm $a_t$ (line 2) based on a non-\nstationary bandit algorithm (e.g., DUCB or SWUCB), and\nthe greedy choice is denoted by $g_t$ (line 3). The principal\noffers compensation $\\lambda_t$ (in line 5) to the agents, which is\nthe difference between the empirical average of rewards from\nthe greedy choice and the principal's recommendation when\nthey differ. This compensation is provided if the principal's\nrecommended arm doesn't align with the greedy choice.\nAfter receiving this compensation, the player's outcome is\naffected by a bias $\\delta_t$, which is added to the \"true\" reward\n$\\bar{X}_t(a_t)$.\nWith equation (2), the expected regret is defined as\n$\\mathbb{E} \\left[\\sum_{t=1}^{T} (\\mu^* - X_t(a_t)) \\right] = \\sum_{a\\neq a^*} (\\mu^* - \\mu(a)) \\mathbb{E} [N_T(a)]$.\nSince the expected reward of an arm is in the range [0, 1],\nwe have $(\\mu^* - \\mu(a)) \\leq 1$ for all $a$. Therefore, bounding\nthe expected regret after $T$ pulls essentially amounts to\ncontrolling the expected number of times a sub-optimal arm\nis pulled. In Theorem 1, we bound the expected number of\ntimes some sub-optimal arms $a \\neq a_t$ are pulled when the\nprincipal uses DUCB algorithm to balance exploration and\nexploitation in Algorithm 3 (line 2) until time $T$.\nGiven the time horizon $T$ and the number of breakpoints\n$\\beta_T$, the expected number of times some sub-optimal arms\n$a \\neq a_t$ are pulled is bounded as follows:\n$\\mathbb{E} [N_T(a)] \\leq \\tilde{\\eta} \\cdot T \\frac{\\beta_T}{\\gamma} \\log(T)$\nwith some constant $\\tilde{\\eta} > 0$.\nSee the proof of Theorem 1 in the Appendix for the choice\nof the discount factor $\\gamma$.\nThe following result is for the case when the principal\nuses the SWUCB algorithm instead in Algorithm 3 (line 2).\nGiven the time horizon $T$ and the number of breakpoints\n$\\beta_T$, the expected number of times some sub-optimal arms\n$a \\neq a_t$ are pulled is bounded as follows:\n$\\mathbb{E} [N_T(a)] \\leq \\tilde{\\eta} \\cdot \\sqrt{ \\beta_T T } \\log(T)$\nwith some constant $\\tilde{\\eta} > 0$.\nSee the proof of Theorem 2 in the Appendix for the choice\nof the sliding window $\\tau$.\nRemark 1 The lower bound of the regret for an algorithm\nscheme for the abruptly changing environment is $\\Omega(\\sqrt{T})$ (see\nsection 4 of [33]). Therefore, the proposed algorithm scheme"}, {"title": "A. Incentivized Exploration in the Abrupty-Changing Envi-\nronment", "content": "Algorithm 3 describes a framework of incentivized explo-\nration for the abruptly changing environment. At time $t$, the\nprincipal recommends an arm $a_t$ (line 2) based on a non-\nstationary bandit algorithm (e.g., DUCB or SWUCB), and\nthe greedy choice is denoted by $g_t$ (line 3). The principal\noffers compensation $\\lambda_t$ (in line 5) to the agents, which is\nthe difference between the empirical average of rewards from\nthe greedy choice and the principal's recommendation when\nthey differ. This compensation is provided if the principal's\nrecommended arm doesn't align with the greedy choice.\nAfter receiving this compensation, the player's outcome is\naffected by a bias $\\delta_t$, which is added to the \"true\" reward\n$\\bar{X}_t(a_t)$.\nWith equation (2), the expected regret is defined as\n$\\mathbb{E} \\left[\\sum_{t=1}^{T} (\\mu^* - X_t(a_t)) \\right] = \\sum_{a\\neq a^*} (\\mu^* - \\mu(a)) \\mathbb{E} [N_T(a)]$.\nSince the expected reward of an arm is in the range [0, 1],\nwe have $(\\mu^* - \\mu(a)) \\leq 1$ for all $a$. Therefore, bounding\nthe expected regret after $T$ pulls essentially amounts to\ncontrolling the expected number of times a sub-optimal arm\nis pulled. In Theorem 1, we bound the expected number of\ntimes some sub-optimal arms $a \\neq a_t$ are pulled when the\nprincipal uses DUCB algorithm to balance exploration and\nexploitation in Algorithm 3 (line 2) until time $T$.\nTheorem 1 (Algorithm 3 + DUCB Regret Bound)\nGiven the time horizon $T$ and the number of breakpoints\n$\\beta_T$, the expected number of times some sub-optimal arms\n$a \\neq a_t$ are pulled is bounded as follows:\n$\\mathbb{E} [N_T(a)] \\leq \\tilde{\\eta} \\cdot T \\frac{\\beta_T}{\\gamma} \\log(T)$\nwith some constant $\\tilde{\\eta} > 0$.\nSee the proof of Theorem 1 in the Appendix for the choice\nof the discount factor $\\gamma$.\nThe following result is for the case when the principal\nuses the SWUCB algorithm instead in Algorithm 3 (line 2).\nTheorem 2 (Algorithm 3 + SWUCB Regret Bound)\nGiven the time horizon $T$ and the number of breakpoints\n$\\beta_T$, the expected number of times some sub-optimal arms\n$a \\neq a_t$ are pulled is bounded as follows:\n$\\mathbb{E} [N_T(a)] \\leq \\tilde{\\eta} \\cdot \\sqrt{ \\beta_T T } \\log(T)$\nwith some constant $\\tilde{\\eta} > 0$.\nSee the proof of Theorem 2 in the Appendix for the choice\nof the sliding window $\\tau$.\nRemark 1 The lower bound of the regret for an algorithm\nscheme for the abruptly changing environment is $\\Omega(\\sqrt{T})$ (see\nsection 4 of [33]). Therefore, the proposed algorithm scheme"}, {"title": "Theorem 1 (Algorithm 3 + DUCB Regret Bound)", "content": "Given the time horizon $T$ and the number of breakpoints\n$\\beta_T$, the expected number of times some sub-optimal arms\n$a \\neq a_t$ are pulled is bounded as follows:\n$\\mathbb{E} [N_T(a)] \\leq \\tilde{\\eta} \\cdot T \\frac{\\beta_T}{\\gamma} \\log(T)$\nwith some constant $\\tilde{\\eta} > 0$.\nSee the proof of Theorem 1 in the Appendix for the choice\nof the discount factor $\\gamma$.\nThe following result is for the case when the principal\nuses the SWUCB algorithm instead in Algorithm 3 (line 2).\nTheorem 2 (Algorithm 3 + SWUCB Regret Bound)\nGiven the time horizon $T$ and the number of breakpoints\n$\\beta_T$, the expected number of times some sub-optimal arms\n$a \\neq a_t$ are pulled is bounded as follows:\n$\\mathbb{E} [N_T(a)] \\leq \\tilde{\\eta} \\cdot \\sqrt{ \\beta_T T } \\log(T)$\nwith some constant $\\tilde{\\eta} > 0$.\nSee the proof of Theorem 2 in the Appendix for the choice\nof the sliding window $\\tau$.\nRemark 1 The lower bound of the regret for an algorithm\nscheme for the abruptly changing environment is $\\Omega(\\sqrt{T})$ (see\nsection 4 of [33]). Therefore, the proposed algorithm scheme"}, {"title": "Theorem 2 (Algorithm 3 + SWUCB Regret Bound)", "content": "Given the time horizon $T$ and the number of breakpoints\n$\\beta_T$, the expected number of times some sub-optimal arms\n$a \\neq a_t$ are pulled is bounded as follows:\n$\\mathbb{E} [N_T(a)] \\leq \\tilde{\\eta} \\cdot \\sqrt{ \\beta_T T } \\log(T)$\nwith some constant $\\tilde{\\eta} > 0$.\nSee the proof of Theorem 2 in the Appendix for the choice\nof the sliding window $\\tau$.\nRemark 1 The lower bound of the regret for an algorithm\nscheme for the abruptly changing environment is $\\Omega(\\sqrt{T})$ (see\nsection 4 of [33]). Therefore, the proposed algorithm scheme"}, {"title": "B. Incentivized Exploration in the Continuously-Changing\nEnvironment", "content": "For the continuously changing environment", "follows": "n$R_T< \\eta \\cdot V_T^{1/3"}, "K \\log(T))^{1/3} T^{2/3}$\nwith some constant $\\eta > 0$.\nSee the proof of Theorem 5 in the Appendix for the choice\nof the batch size $\\sigma$.\nThe overall compensation $C_T$ for the entire time horizon\n$T$ is calculated by summing up the compensation of each\nbatch, following the definition (7).\nTheorem 6 Given the time horizon $T$ and the variation\nbudget $V_T$, if the principal employs UCB1 for recommending\narms to agents in Algorithm 4, then the worst-case total\ncompensation is bounded as follows:\n$C_T< \\eta_1 (K V_T \\log(T))^{1/3} T^{2/3}$\nwith some constant $\\eta_1 > 0$. If the principal employs $\\epsilon$-"]}