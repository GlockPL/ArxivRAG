{"title": "Automatic Image Annotation for Mapped Features Detection", "authors": ["Maxime Noizet\u00b9", "Philippe Xu1,2", "Philippe Bonnifait\u00b9"], "abstract": "Detecting road features is a key enabler for\nautonomous driving and localization. For instance, a reliable\ndetection of poles which are widespread in road environments\ncan improve localization. Modern deep learning-based per-\nception systems need a significant amount of annotated data.\nAutomatic annotation avoids time-consuming and costly manual\nannotation. Because automatic methods are prone to errors,\nmanaging annotation uncertainty is crucial to ensure a proper\nlearning process. Fusing multiple annotation sources on the\nsame dataset can be an efficient way to reduce the errors. This\nnot only improves the quality of annotations, but also improves\nthe learning of perception models. In this paper, we consider\nthe fusion of three automatic annotation methods in images:\nfeature projection from a high accuracy vector map combined\nwith a lidar, image segmentation and lidar segmentation. Our\nexperimental results demonstrate the significant benefits of\nmulti-modal automatic annotation for pole detection through a\ncomparative evaluation on manually annotated images. Finally,\nthe resulting multi-modal fusion is used to fine-tune an object\ndetection model for pole base detection using unlabeled data,\nshowing overall improvements achieved by enhancing network\nspecialization. The dataset is publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "Localization is a core functionality for autonomous driv-\ning. Depending on the navigation context, obtaining a reli-\nable and accurate localization only through GNSS and dead-\nreckoning sensors can be challenging. A complementary\nsolution is integrating maps with exteroceptive sensors, such\nas cameras or lidars, capable of measuring distances and/or\nangles relative to georeferenced map features. In this paper,\nwe consider that the map is given to the system beforehand\nby a map provider. This map is agnostic to the perception\nsensors set up on the vehicle. Therefore, contrary to simulta-\nneous localization and mapping contexts where the features\nencoded in the map are directly linked to the ones detected\nby the sensors, one needs to build a perception module able\nto explicitly detect these map features as shown in Fig. 1.\nA standard vector map used for autonomous driving in-\ncludes features of the road environment, such as lane mark-\nings, but also road furniture such as traffic signs, traffic lights\nor streetlamps. These pole-like structures are widespread and\nrepresent optimal localization features. In a 2D vector map,\nthese features are commonly depicted as points encoding the\npositions of their bases on the ground.\nBecause the shape of a pole is geometrically constrained,\nlidars, which provide 3D geometric information, appear to\nbe quite suitable for detecting poles. However, this proves"}, {"title": "II. RELATED WORKS", "content": "Pole-like features are commonly used for localization [1]-\n[4]. Using geometric assumptions, lidar sensors are classi-\ncally used to detect pole features [5]-[7]. When it comes\nto detection tasks using cameras, deep learning techniques\nare generally employed. Poles are often detected through\nsemantic segmentation at pixel-level, a method that not only\nrequires pixel-level annotation but is also far less compu-\ntationally efficient compared to bounding box based object\ndetection methods such as YOLO [8].\nTraining deep neural networks requires a significant\namount of annotated data which becomes costly when made\nmanually. When no annotated data is available for a given\ntask or context, it is interesting to provide an automatic\nannotation. However, this kind of approach is prone to\nerrors and can lead to lower performance compared to\nannotation made by humans. Some automatic and semi-\nautomatic annotation methods to build datasets have been\nproposed in [9]-[13] providing hard labels, to be used in\nclassical supervised training pipelines. In particular, pseudo-\nlabeling, a process that involves annotating unlabeled data\nby using predictions obtained from a pre-trained network\nfor subsequent retraining, is generally applied [14]-[17].\nHowever, the pre-trained network must be consistent with the\ndetection task at hand and pseudo-labeling can be insufficient\nor lead to poor results due to the low quality of labels.\nThe use of these approaches can potentially lead to sig-\nnificant annotation errors if the quality of the automatic\nannotations is not properly controlled which will decrease\nthe performance of the learned model. Even though deep\nlearning models may exhibit a small degree of tolerance for\nannotation errors in object detection contexts, the accumula-\ntion of numerous errors can significantly degrade the network\nperformance [18], [19]. Particular attention should therefore\nbe given to the quality assessment of annotations.\nTo account for annotation errors, one can try to further\nclean out the errors, or instead manage and quantify uncer-\ntainties of the annotations. For instance, confident learning\ninvolves characterizing label quality using the model to prune\nlabel errors from training sets [18], [20]. However, since the\nmodel is employed to identify errors, it seems challenging to\nprune labels corresponding to false positives that are similar\nto the objects we aim to detect. It is typically true with\nbollards we do not want to detect in our case.\nSome methods involve modifying the loss used in the\nnetwork to handle uncertainties [21] and manipulating soft-"}, {"title": "III. MULTI-MODAL ANNOTATION", "content": "Consider the problem of the fusion of image annotations\ncomputed from different sources in order to reach a better set\nof annotations for learning. Throughout this work, we only\naddress the problem of detecting a single type of object.\nFor a given method k, the set of annotations for an image\ni is defined as\n\\(^{(k)}a_i = \\{^{(k)}a_j | j= 1,..., ^{(k)}n_i\\},\\)\nwhere \\(^{(k)}n_i\\) is the number of detected objects in image i by\nthe method k and \\(^{(k)}a\\) encodes an object annotation, which\nis here the coordinates of a single point.\nFor N images, the resulting annotation set by method k\nis\n\\(^{(k)}a = \\{ ^{(k)} a_i | i= 1,..., N\\}.\\)\nBecause each automatic annotation method is prone to\nerrors (i.e. false positives and false negatives), the aim is\nto combine the annotations into a new set of annotations\nwith a higher quality in terms of precision and/or recall.\nAn overview of the entire annotation process is visible\nin Fig. 2. Annotations obtained through diverse methods are\nassociated and then combined in order to generate a final\nannotation that should be as close as possible to the expected\nreference one."}, {"title": "B. Fusion of annotations", "content": "The fusion process of K annotation sets coming from\nK independent methods can be decomposed into two prin-\ncipal steps. The first step is to define a data associa-\ntion function h that, given a set of annotations A =\n{\\(^{(1)} a_i\\), \\(^{(2)} a_i\\), ..., \\(^{(K)}a_i\\)}, returns a set of clusters of anno-\ntations corresponding to the annotation of the same element\nalong different modalities:\n\\(h(A) = \\{c_{i,c},...,c_{i,M}\\}\\)\nwhere M is the number of different annotated elements and\neach c is a set that contains at most one element from each\n\\(^{(k)}a_i\\): \\(c = \\{^{(k)}a\\}\\) if a pole is detected only by method\nk, \\(c = \\{^{(k)}a, ^{(k')} a\\}\\) if detected by methods k and k',\nand so on. In the case where all methods detect the pole,"}, {"title": "IV. CASE STUDY", "content": "We consider three annotation methods and their combi-\nnations to train a pole base detector. In this context, each\nannotation \\(^{(k)} a\\) corresponds to the coordinates (uj,vj) of\nthe pole base in the image.\n1) Map-based annotation (denoted M): A first approach\nis to use a 2D High Definition (HD) vector map along with a\nhigh accuracy localization system. The poles georeferenced\nwithin the field of view of the camera are extracted from\nthe map and projected onto the images. Even though the\npoles contained in the map concern furniture (such as traffic\nsigns, traffic lights or streetlamps) which is stable over time,\nthe map can still become outdated. So, false positives and\nnegatives are inevitable. To project the 2D map features\nat the ground level, a lidar is used to estimate the ground\nand check for occluded pole bases. In this work, we use\nthe ground segmentation method proposed in [23] and the\nlidar pipeline as described in [24]. Fig. 3a illustrates some\nautomatic annotations using this method. The blue crosses\ndepict the generated annotations, the ones circled in green\nare correct annotations while the red circles correspond to\nmissed poles. In this particular example, the three missed\npoles are too far away from the vehicle making annotation\nimpossible. This limitation is due to the lack of ground\npoints, represented by green dots, near their bases, making it\nchallenging to project the map points accurately. To mitigate\nthe risk of false positives and enhance positioning accuracy,\nwe refrain from annotating distant poles, even if it may\nintroduce the possibility of false negatives.\n2) Segmentation-based annotation (denoted S): We use\nthe HRNet image semantic segmentation neural network pro-\nposed in [25] and pre-trained on the BDD100K dataset [26]\nto extract pole bases from the segmentation masks as in [4].\nWe combine all pole-related classes to form entire pole\nclusters to check if they are connected to ground pixels. It\nensures that only large clusters of pole pixels are consid-\nered, thus minimizing the influence of poor segmentation."}, {"title": "B. Annotations association and fusion", "content": "For the association of the annotations, we use a simple\nunique nearest neighbor approach with the Euclidean dis-\ntance to associate the annotations between two sets. Two\nannotations across two modalities are grouped if their relative\ndistance is smaller than a given threshold. If an annotation\ncan be associated with more than one, only the closest one is\nkept. The final association function h with three modalities\nconsists in grouping all the pairwise associations.\nOnce a set A of annotations is computed, the fused\nannotation f(A) is defined as the best annotation contained\nin A considering a preference order: S > L > M. This\norder is based on the presence of potential sources of\npositioning errors in the image provided by the different\nautomatic methods. Only segmentation errors can occur from\nthe method S. Errors due to sensor calibration, segmentation\nand clustering can impact the method L. Finally, errors due"}, {"title": "C. Ambiguous annotations management", "content": "As described in Sec. III-B, different fused annotation sets\ncan be generated depending on a degree of consensus q. We\nmay wish to train a detector only on annotated examples\nwith a high degree of consensus while disregarding those\nwith low consensus that may not represent pole bases.\nWe establish two sets: A* comprising automatic anno-\ntations with high consensus, serving as the labels for our\ntraining set, and A containing all other ambiguous automatic\nannotations annotated by at least one method that we aim to\nexclude. Given a minimum consensus threshold Q, we have\n\\(A^* = ^{(1:K)}_Qa  \\)  and \\(A = ^{(1:K)} a  \\ (^{(1) a})\nHandling the ambiguous annotations A is not straight-\nforward. Adding the ambiguous annotations may lead to\nfalse positive labels while removing them may lead to false\nnegative labels. In both cases, these potentially erroneous\nlabels may lead to a decrease of performance in the training.\nIn this paper, we propose a simple bypass by adding black\nsquared patches to mask ambiguous annotations."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "To the best of our knowledge, there is no public dataset\nincluding an HD map with georeferenced poles along with\nimage and lidar data.\nWe conducted experiments using an experimental Renault\nZOE equipped with the following sensors:\n\u2022 Hesai Pandora which integrates a 40-layer LiDAR with\nmonocular cameras.\n\u2022 NovAtel SPAN-CPT GNSS/IMU with PPK computa-\ntions for high accuracy localization.\nWe carried out five acquisitions, each lasting approximately\none hour under different traffic conditions in 2022. For the\nmap-based annotation of images, we used a 2D HD map of\nthe city of Compi\u00e8gne, France.\nAmong these sequences, we selected two with different\ntraffic and lighting conditions to manually annotate a subset\nof images for annotation and detection evaluation. Given our\ndeliberate exclusion of short-lived objects such as bollards,\nwe focused our annotations on elements as traffic signs,\ntraffic lights, or streetlamps. Consequently, our objective is\nto develop a specialized detector for the elements stored in\nthe HD map, which will be used for localization.\nWe extracted 939 images particularly representative of the\nsequences acquired and manually annotated 2846 poles\u00b9.\nFrom the remaining sequences 5391 images were extracted\nand automatically annotated for subsequent training pur-\nposes."}, {"title": "B. Single annotation methods", "content": "We evaluated the automatic annotation methods in terms\nof precision and recall. The results are reported in Tab. I.\nS generates approximately five times as many pole-base\ncandidates as the other two methods. This method is the\nmost generic, annotating anything considered as a pole by\nthe semantic segmentation network, resulting in the highest"}, {"title": "C. Annotation combinations", "content": "We assessed various fusion strategies as previously de-\nfined, testing unions and intersections of sets. To build our\npole base detector aligned with the HD map, we specifically\nconsider combinations involving M as our objective is to\ndetect elements present in the map for localization. We\nexclude combinations obtained only from L and S, as they\nmay annotate pole structures as bollards, which are not\naccounted for in the map. The results are presented in\nTable II.\nAs expected, the union of annotation sets improves the"}, {"title": "D. Neural network training without ambiguity handling", "content": "We evaluated the quality of automatic annotation for the\ntraining of a pole detector. We formulated the detection\nproblem as a standard object detection one by transforming a\npointwise label into a squared 250 \u00d7 250 pixels bounding box\ncentered on the annotation [24]. We used the YOLOv7 [8]\nmodel as the object detector. We first constructed a reference\nmodel by training the YOLO model using the pole bases\nextracted from BDD100K [26] as in [24]. It was then\nfine-tuned using our automatically annotated images using\ndifferent approaches. All the models were trained on a single\nTesla V100 32G GPU for 300 epochs.\nTo visualize the impact of the training, we tested the final\nmodel obtained at the last epoch on our set of manually anno-\ntated images. The detection results obtained for all annotation\nmethods and many combinations are summarized in Fig. 5\nby precision-recall (PR) curves following a standard object\ndetection evaluation, i.e., using the IoU criteria between the\ndetected and ground truth bounding boxes (generated from\nthe pointwise labels). Results of the initial model are also\nindicated in blue.\nFirstly, the results obtained with the methods M and\nL align with previously obtained annotation results. M\nachieves noteworthy precision, surpassing our initial model.\nHowever, it faces challenges in achieving high recall, even\nat the cost of sacrificing precision. L yields similar results,\nwith a larger drop of precision when increasing recall and a\nsmaller maximal possible recall.\nThe application of the S method for training, due to its\ngenerality, struggles to attain a precision above approxi-\nmately 0.58. At low recall values, indicating high confidence\nthresholds, the precision is notably low. This is attributed\nto objects with high confidence that are not considered as\npole bases in our study, such as bollards. The curve rises"}, {"title": "E. Training with masking patches", "content": "In the results presented in the previous section, the am-\nbiguous annotations, that may occur in the M & S and M"}, {"title": "VI. CONCLUSION", "content": "In this paper, we introduced a framework to combine\ndifferent automatic methods to annotate unlabeled images\nfrom multi-modal raw data in order to train a pole base\ndetector associated with features encoded in an HD map.\nWe proposed a way to manage ambiguous labels by masking\nparts of images with patches which improved performance on\na dataset with map data specifically collected for this study.\nThe different manners to combine the individual annotation\nsources led to various precision and recall behaviors.\nIn future work, we will study the choice of a relevant\ncompromise between these two properties. This depends on\nthe subsequent localization system according to its robust-\nness (ability to reject outliers) and its need for exteroceptive\ninformation to compute accurate and reliable poses."}]}