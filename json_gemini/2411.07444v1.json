{"title": "Input-Based Ensemble-Learning Method for Dynamic Memory Configuration of Serverless Computing Functions", "authors": ["Siddharth Agarwal", "Maria A. Rodriguez", "Rajkumar Buyya"], "abstract": "In today's Function-as-a-Service offerings, a programmer is usually responsible for configuring function memory for its successful execution, which allocates proportional function resources such as CPU and network. However, right-sizing the function memory force developers to speculate performance and make ad-hoc configuration decisions. Recent research has highlighted that a function's input characteristics, such as input size, type and number of inputs, significantly impact its resource demand, run-time performance and costs with fluctuating workloads. This correlation further makes memory configuration a non-trivial task. On that account, an input-aware function memory allocator not only improves developer productivity by completely hiding resource-related decisions but also drives an opportunity to reduce resource wastage and offer a finer-grained cost-optimised pricing scheme. Therefore, we present MemFigLess, a serverless solution that estimates the memory requirement of a serverless function with input-awareness. The framework executes function profiling in an offline stage and trains a multi-output Random Forest Regression model on the collected metrics to invoke input-aware optimal configurations. We evaluate our work with the state-of-the-art approaches on AWS Lambda service to find that MemFigLess is able to capture the input-aware resource relationships and allocate upto 82% less resources and save up to 87% run-time costs.", "sections": [{"title": "I. INTRODUCTION", "content": "The serverless computing paradigm is the latest cloud-native development model that enables application execution without the management of underlying resources. Serverless promotes the idea that a developer should be less concerned about the servers or infrastructure and focus more on productivity that adds value to the business. This shift of responsibility means offloading resource management tasks to the cloud service provider (CSP), such as resource allocation, application scaling and software updates. In the serverless landscape [1], Function-as-a-Service (FaaS) emerged as a microservices-inspired, event-driven execution model where function(s) are integrated with additional Backend-as-a-Service (BaaS) offerings like storage, networking and database services, to set-up an application. A serverless function is a stateless code fragment, executed on-demand within lightweight virtual machines (VM), microVMs or containers for short-term duration, and bills its resources as per usage.\nIn 2014, Amazon Web Services (AWS) introduced AWS Lambda [2], [3] as its first FaaS offering, and since then, a range of FaaS services have emerged, including Google Cloud Functions [4], Azure Functions [5], and many open-source implementations such as OpenFaaS [6], Knative [7] and OpenWhisk [8]. In addition to serverless attributes such as on-demand scalability, zero idle-resource costs, and no resource management, FaaS uniquely features scale-to-zero capability where function resources are released after an extended period of inactivity, endorsing a multi-tenant resource-sharing and pay-per-use pricing model. FaaS has increasingly found its relevance in a variety of use cases like video streaming platform [9], multi-media processing [10], CI/CD pipeline [11], AI/ML inference task [12], and Large-Language-Model (LLM) query processing [13].\nThe operational model of FaaS hides the complex infrastructure management from end users and does not signify the absence of servers. A serverless function still requires resources, including computing, network and memory, for a successful execution. In the current FaaS implementations, a developer is responsible for requesting the right combination of resources to guarantee successful function execution. However, service providers only expose a small set of resource knobs, usually memory 1 with proportionally allocated CPU, disk I/O, network bandwidth, etc. [14]. Prior studies [15] [16] [17] have identified that a higher memory configuration speeds up function execution and has a significant impact on its start-up performance and costs. However, the execution speedup is non-linear and has a diminishing marginal improvement with increasing memory allocations [18]. With limited observability into short-running functions and unaware of function performance, developers usually resort to speculative decisions for memory configuration or make experience-based ad-hoc decisions with an expectation to fulfil service level objectives (SLO) [19]. To validate such developer behaviour, an industry"}, {"title": "II. MOTIVATION", "content": "To identify and establish the effect of input parameters on a function's memory requirement and execution time, we experiment with the industry-leading FaaS platform, AWS Lambda [3] and conduct a large-scale initial study by deploying three benchmark functions, 1) matrix multiplication (matmul), 2) graph minimum spanning tree (graph-mst), and 3) linpack from [24]. A CPU-bound function, matmul, calculates the multiplication of a n*n matrix, whereas linpack measures the system's floating-point computing power by solving a dense n by n system of linear equations. graph-mst is a scientific computation offloaded to serverless functions that generates a random input graph of n nodes using Barab\u00e1si\u2013Albert preferential attachment and processes it with the minimum spanning tree algorithm. To observe the effect of payload (i.e., input parameters) on the performance of benchmark functions, we execute them with input set $N = {n|10 \u2264 n \u2264 10000, n \u2190 n + 100}$ and vary the memory configuration of the function $M = {m|128 < m < 3008,m \u2190 m + 128}$ MB. We take advantage of Amazon CloudWatch [25] as a monitoring solution to gather function-level performance metrics.\nWe collect relevant function metrics as described in Table I and plot the function payload against the minimum billed duration and minimum memory utilised in Fig. 1.\nInsight 1: There is a strong correlation between the function payload and execution time that varies in proportion to distinct memory allocations.\nWe find a strong correlation between the function payload and the corresponding billed duration for all the benchmark functions. It can be inferred from Fig. 1a - 1c that the minimum billed duration, i.e., the execution time of a function, is directly proportional to its input and has a tendency to increase with increasing payloads at distinct memory configurations. Therefore, we can infer that different memory configurations lead to proportional resource allocations and thus, the function performance, i.e., execution time, also varies in proportion to these available resources. This complex relation of payload-dependent execution time further aggravates the overall function run-time cost as it is calculated based on the execution time and allocated memory."}, {"title": "III. RELATED WORK", "content": "The authors in [14] propose a multi-regression model generated on synthetic function performance data and use it to predict execution time and estimate cost at distinct memory configurations. Similarly, [27] explores search algorithms like linear/binary search and gradient descent to determine the optimal configuration for cost-focused or balanced optimisation goals. However, none of them considers the effect of payload on function performance and resource demands. Additionally, they either rely on synthetic data or perform repeated search across configurations.\nJarachanthan et al. [28] explore the performance and cost trade-off of the function at different configurations for Map-Reduce-style applications. They propose a graph-theory-based job deployment strategy for Directed Acyclic Graph (DAG) based applications to optimize the resource configuration parameters. Wen et al. [16] focus on multicore-friendly programming for workflows to estimate the inter- / intra-function parallelism based on weighted sub-SLOs. Safaryan et al. [29] focus on the SLO-aware configuration of workflows and use a max-heap data structure to find a configuration repeatedly. However, these works address function workflows to either fit a specific application style or configure function parallelism and fall short of considering the payload effect.\nThe work in [30] introduces an urgency-based heuristic method where a particle swarm optimization technique is used for time/cost trade-off. Lin and Khazaei [31] propose a"}, {"title": "IV. PROBLEM FORMULATION AND ARCHITECTURE", "content": "In this section we formally describe the function configuration problem and introduce the system architecture.\nA. Problem Formulation\nIn current FaaS environments, developers must configure memory settings (referring to FaaS platforms like AWS Lambda) for their functions to ensure successful execution. However, determining the appropriate memory configuration is challenging due to factors like variability in function input or payload characteristics (e.g., input size, type, and number of inputs) and invocation frequency, which significantly impact resource demands, run-time performance and cost. To handle this, developers generally make ad-hoc decisions to either configure platform defaults [20] or speculate the right-sizing of functions based on past experience. These decisions lead to sub-optimal resource allocations, over- or under-provisioning, which may result in resource wastage, added run-time costs, and throttled function performance. Additionally, FaaS platforms scale a function with static resource configuration with fluctuating workload which further makes the resource scaling and allocation challenging.\nExisting research [14] [18] [32] [26] have repeatedly accentuated the complex relationship of function resource demand, execution time guarantee and run-time cost, which is further complicated when considering function payload [23]. Therefore, we formulate the problem of payload-aware function configuration as a multi-objective optimisation (MOO) problem where the objective is to select a memory configuration that guarantees a successful execution within an advertised function deadline, while reducing excess resource allocation and run-time costs for incoming function invocations with varying payloads. The problem can be mathematically represented as Eq. 1, such that the constraints Eq. 2 are satisfied.\n$\\min G(m, P) = (C_f(m, P),T_f(m, P))$\n$m \\in M$\n(1)\nSubject to:\n$T_f(m, P) \u2264 D$,\n(2)\n$C_f(m, P) = T_f(m, P) * C_m + \u03b2 \u2264 B$,\n(3)\n$Success(m, P) = 1$.\n(4)\nIn FaaS, a function $f$ may expect a number of inputs, $P = {P_1,...,P_n}$ where an input belongs to a range of values $P_n = {p_{min},...,p_{max}}$, either continuous or discrete, which influences the function memory requirements $m \\in M = {m_{min},...,m_{max}}$. We define the objective of payload-aware memory configuration to minimise the run-time cost $C(m, P)$ and the function execution time $T(m, P)$ at memory allocation $m$ and payload(s) $P$, such that a function executes successfully within the specified deadline $D$ and budget $B$ constraints. The run-time cost $C_f(m, P)$ is directly proportional to the execution time $T_f(m, P)$ and the cost of memory configuration $C_m$ plus a constant invocation amount, $\u03b2$ (provider dependent)."}, {"title": "B. System Architecture", "content": "The proposed MemFigLess system architecture consists of both offline and online components designed to optimize memory allocation for FaaS offerings based on the payload. The model of the proposed framework is a MAPE control loop, i.e., Monitor, Analyse, Plan and Execute. In the online stage, a periodic monitoring of function performance metrics is done. It is then analysed by the resource manager via a feedback loop to plan and execute the resource allocations that meet the performance SLOs.\n1) Offline Profiling and Training Module: This module is responsible for profiling the functions and training the Random Forest Regressor (RFR) model, Fig. 2a. Functions are executed with a variety of inputs to collect data on their performance and resource usage. Metrics such as input size, number of inputs, memory consumption, execution time and billed execution unit are recorded. The collected metrics are stored in a structured format to serve as training data for the model. A tree-based ensemble learning RFR model is trained on the collected data to learn the relationship between the function's input/payload and its memory requirement and execution time. The model captures the impact of input size distribution on resource usage and is used for online payload-aware memory estimation.\n2) Online Prediction and Optimization Module: This module leverages the trained RFR model to select the optimal function memory based on the model prediction and constraint optimisation, and invoke functions in real-time, as shown in Fig. 2b. Incoming function requests are analyzed to extract payloads which are fed to the trained RFR model to predict the execution time at distinct function memory configurations. The selected memory configurations, based on SLO constraints, are used for online constraint optimisation, either cost or execution time, provided by the user. This helps in reducing the potential resource wastage and overall cost of execution.\n3) Dynamic Resource Manager: This component is embedded in the online prediction module to handle the invocation and allocation of resources based on the predictions made by the RFR model. The resource manager dynamically allocates function resources or selects the available function instance based on the constraint-optimised memory selection, ensuring minimal wastage and optimal performance. A continuous monitoring and feedback mechanism is also incorporated to improve the accuracy and performance of the system over time. This module can monitor and log the performance of functions during execution within a configured monitoring_window to ensure that the model captures performance fluctuations periodically. The gathered performance data is leveraged by the training module to periodically re-train and improve the RFR model predictions."}, {"title": "V. MULTI-OUTPUT RANDOM FOREST REGRESSION", "content": "To accurately select the memory configuration of serverless functions, a Random Forest (RF)-based regression algorithm can be employed. The Random Forest, initially presented by Breiman [33], is one of the most popular supervised machine learning (ML) algorithms and has been successfully applied to both classification and regression in many different tasks, such as virtual machine (VM) resource estimation [34], VM resource auto-scaling [35] and computer vision applications [36]. The RF algorithm uses a combination of DTs to model complex interactions between input parameters and identify patterns in the data. It works by training multiple DTs on subsets of input parameters and then aggregating their predictions to generate the final estimation. This method has been demonstrated to have the ability to accurately approximate the variables with nonlinear relationships and also have high robustness performance against outliers. In addition, compared to other ML techniques, e.g., Artificial Neural Networks (ANN), Support Vector Machine (SVM), Deep Learning and Reinforcement Learning, it only needs a few tunable parameters and therefore requires low effort for offline model tuning. Furthermore, the algorithm can handle noisy or incomplete input data, and reduces the risk of overfitting which might occur with other ML algorithms [37].\nThe RF model is an ensemble-learning method that can be modeled as a collection of DTs. A DT makes a prediction for the input feature vector $Z \u2208 F$, where $F$ represents a subset of K-dimensional feature space [33]. A DT recursively partitions the feature space $F$ into $L$ terminal nodes or leaves that represent the region $R_l, 1 < l < L$ where every possible feature vector $x$ may belong. Therefore, an estimation function $f(x)$ for a DT can be summarised as Eq. 5, where $I(x, R_l)$ represents the indicator function of whether the feature vector"}, {"title": "VI. PERFORMANCE EVALUATION", "content": "In this section, we briefly discuss the implementation along with experimental setup, model parameters, and perform an analysis of the proposed RFR-based framework compared to other complementary solutions.\nA. Implementation and System Setup\nWe setup our proposed solution using AWS serverless services [39], such as AWS Lambda, AWS DynamoDB, AWS Step Functions Workflow and Amazon Simple Storage Ser-vice (S3) for an end-to-end serverless function configuration solution. The framework can be assumed a CSP service where users can subscribe to it for an end-to-end optimisation, based on the desired deadline and run-time cost of the individual function. The offline step is implemented as a Step Functions Workflow that takes function details such as resource name, memory configurations to explore, number of profiling iterations and the representative payload(s) as a json input file. This information is used to create and execute a function with different payloads at distinct configurations, and collect the performance data using AWS CloudWatch [25]. However, the payload for different functions may be of different types and thus, the workflow utilises the AWS S3 service to fetch any stored representative payload. The individual workflow tasks of creating, executing and updating the function in addition to log collection and processing, are implemented as AWS"}, {"title": "B. Experiments", "content": "We run the profiling step of the proposed solution with the given function payload(s) at distinct memory configurations to capture the relationship between payload and resource demands. After the profiling step, a RFR model is trained on the collected data, accompanied by a hyper-parameter tuning that explores model parameters such as n_estimators, max_depth, min_samples_split and min_samples_leaf via grid_search and selects the best configuration for inference. The RFR model then estimates the memory utilisation and function execution time to perform online optimisation and selects the best possible payload-aware configuration to invoke functions.\nWe perform the payload-aware estimation and optimisation of memory configuration for 50 payload values, within the discussed range. We select the relative importance, $w_z$, as 0.5 to balance the function execution time, $T_f(m, P)$ and run-time cost, $C_f(m, P)$ constraints for this experiment. In Fig. 3, we showcase the ability of RFR model to predict the execution time of the functions. The proposed methodology estimated the execution time with a $R^2$ score of as high as 98% for linpack and pyaes function while having $R^2$ scores of 97%, 94% and 91% for functions such as graph-pagerank, graph-mst, graph-bfs and dynamic-html. This statistical measure of $R^2$ score demonstrates the goodness of the fit by the regression model. Additionally, we observe in Fig. 4 that for memory-intensive functions, the RFR model is able to capture the relationship of payload and memory with high $R^2$ score of 96% in case of linpack, 87% for graph-pagerank, 79% for matmul and as low as 73% for graph-bfs function with a mean absolute error (MAE) of 269 MB, 36MB, 2609 MB and 192 MB, respectively. These observations are based on the actual performance data acquired after invoking functions with RFR estimated values. Therefore, we can conclude from the observations that RFR model can be utilised for predicting payload-aware function execution time and memory utilisation. In addition, the proposed RFR-based framework can take advantage of online estimation and optimised solutions to invoke the respective payload-aware configurations.\nTo evaluate our model's efficiency in reducing excess resource allocation and higher run-time costs, we compare our work with the following existing works\n1) COSE [18]: a Bayesian Optimisation (BO) based function memory configuration tool that tries to select best configuration at each sample which maximises the model confidence.\n2) Parrotfish [23]: an online Parametric Regression based function configuration tool that selects optimal configuration while satisfying user-defined constraints. 3) AWS Lambda Power Tuning [32]: a recommendation and graphical tool by AWS that performs an exhaustive search of memory configurations to suggest a cheaper and lower execution time configuration.\nFor the brevity of this evaluation, we run the respective approaches for atmost 10 payload values to find the payload-aware optimal memory configuration for 4 functions i.e., graph-mst, pyaes, matmul and graph-bfs. Similar results are reported for other functions and thus have been skipped from this discussion. We select the function execution time as the objective and utilise distinct payloads to predict the execution time. However, COSE does not provide any utility or provision to optimise for specific payloads, therefore, we run the COSE tool to probe 20 sample points for each payload value. On the other hand, Parrotfish samples and tries to optimise the memory configuration based on weighted representative"}, {"title": "C. Discussion", "content": "We investigate a payload-aware function configuration methodology and present MemFigLess, a RFR-based workflow to estimate the payload-aware optimal resource allocation schemes. The experiments are performed with distinct functions deployed on the AWS Lambda platform and take advantage of workflows to create the offline profiling and training stages of MemFigLess. We assume that representative payload(s) are provided for profiling to supplement the regression model. In addition to this, we make a heuristics-based decision to allocate maximum memory, 3008 MB to an unseen payload larger than the profiled limit or to configure the memory of the smallest payload seen. This builds on the idea [23] that if a configuration is good for an input $x$, then it is also good, if not better, for inputs smaller than $x$.\nWe profile and train the functions at memory intervals of 128 MB, however, in the online inference, estimates are made for every possible memory configuration with a step size of 1 MB, in line with [23] and [3]. This makes the inference time complexity $O(n + klogk)$ where $n$ represents the number of explored memory configurations during optimisation with $k  n$ configurations in Pareto front calculation. As we employ a Random Forest regression that generally requires prior data for precise estimates, these finer estimates may suffer in case of complex payload and resource relationships. Therefore, the accuracy of the estimates is highly dependent on the memory intervals and representative payloads used in the initial profiling to capture complex resource relationships. Furthermore, finer online inferences may suffer from increased cold starts if the estimated configurations are largely distinct for incoming payloads. To support this, MemFigLess intelligently checks for existing functions with estimated configuration to execute. However, it does not control the degree of container reuse that utilises a warm function configuration to avoid a cold start. In addition to this, a sequential workload is considered for analysis in anticipation of minor performance fluctuations owing to AWS Lambda guaranteed concurrent invocations. With the discussed experimental assumptions and setup, the proposed solution is able to reduce the excess resource allocation and reduce the run-time cost of functions in comparison to Parrotfish and COSE, which are used as is with their defaults."}, {"title": "VII. CONCLUSIONS AND FUTURE WORK", "content": "In this work, we present MemFigLess, a Random Forest regression-based payload-aware solution to optimise function memory configuration. This solution is implemented using AWS serverless services and deployed as a workflow. A motivation study is conducted to highlight the importance of payload-aware resource configuration for performance guarantees. We identify a strong and positive correlation between function payload, execution time and memory configuration to formalise the resource configuration as a multi-objective optimisation problem. A concept of Pareto dominance is utilised to perform online resource optimisation. We compare the proposed solution to COSE and Parrotfish and demonstrate the effectiveness of RFR in reducing resource wastage and saving costs. MemFigLess is able to reduce excess memory allocation of as high as 85% against COSE and save run-time cost of up to 71% contrasting to Parrotfish.\nAs part of future work, we plan to improve MemFigLess by exploring advanced optimisation techniques such as Evolutionary algorithms and meta-heuristics to reduce the sampling costs. Additionally, we aim to explore the integration of Reinforcement Learning algorithms and LLMs to reduce manual efforts in model refinement. Furthermore, as Random Forest regression training requires training data prior to learning, an incremental learning approach can also be employed. Furthermore, an integration of Distributed and Federated learning techniques can also be explored to address faster and larger training scenarios."}]}