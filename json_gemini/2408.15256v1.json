{"title": "Improving Ontology Requirements Engineering with OntoChat and Participatory Prompting", "authors": ["Yihang Zhao", "Bohui Zhang", "Xi Hu", "Shuyin Ouyang", "Jongmo Kim", "Nitisha Jain", "Jacopo de Berardinis", "Albert Mero\u00f1o-Pe\u00f1uela", "Elena Simperl"], "abstract": "Past ontology requirements engineering (ORE) has primarily relied on manual methods, such as interviews and collaborative forums, to gather user requirements from domain experts, especially in large projects. Current OntoChat offers a framework for ORE that utilises large language models (LLMs) to streamline the process through four key functions: user story creation, competency question (CQ) extraction, CQ filtration and analysis, and ontology testing support. In OntoChat, users are expected to prompt the chatbot to generate user stories. However, preliminary evaluations revealed that they struggle to do this effectively. To address this issue, we experimented with a research method called participatory prompting, which involves researcher-mediated interactions to help users without deep knowledge of LLMs use the chatbot more effectively. The participatory prompting user study produces pre-defined prompt templates based on user queries, focusing on creating and refining personas, goals, scenarios, sample data, and data resources for user stories. These refined user stories will subsequently be converted into CQs.", "sections": [{"title": "Introduction", "content": "Ontology Requirements Engineering (ORE) plays a critical role in the design, evaluation, and reuse of ontologies, which are essential for structuring knowledge across various domains. Traditionally, ORE has relied heavily on manual activities such as interviews, collaborative forums, and discussion pages to gather user requirements from domain experts, particularly in large projects involving stakeholders from diverse backgrounds (Zhang et al. 2024). These manual processes are often resource-intensive and complex, posing significant challenges in ensuring comprehensive and consistent requirement collection.\n\nThe central theme of ORE is competency questions (CQs). The Infer, Design, CreAte (IDEA) framework (de Berardinis et al. 2023) was an initial toolset that paved the way for language model-driven ontology engineering in large-scale projects. It automates the extraction, organisation, and refinement of CQs, detecting inconsistencies, and using visual and analytical tools for question management.\n\nOntological requirements are gathered from customers in the form of user stories, which are then translated into CQs (de Berardinis et al. 2023). OntoChat supports this process by providing conversational agents that facilitate ORE, beginning with the creation of user stories. Users can create user stories by answering elicitation questions, and ontology engineers can extract competency questions (CQs) and test preliminary versions of ontologies. Despite these advancements, the complete user study results, available on our GitHub repository\u00b9, show that while participants found the model helpful, they often required more specific guidance and iterative refinement to achieve satisfactory user stories.\n\nThis study addresses identified limitations by using participatory prompting user study protocols. Participants express ontology-related queries, which researchers refine using predefined prompting strategies, which can be found in our GitHub repository\u00b9. The model generates responses, participants provide feedback and pose additional queries, and researchers further refine these queries into prompts. This iterative cycle enhances query articulation and facilitates effective interaction with LLMs for user story creation. The results of this user study include pre-defined prompt templates based on user queries. These prompts can serve as elicitation questions for users or be personalised to seek assistance from OntoChat to improve the current \u201cAssisted Persona and Story Creation\" workflow. In sum, this work contributed by applying the participatory prompting method to OntoChat to enhance the user experience. Using this approach, we developed pre-defined prompt templates tailored to user queries for creating personas, goals, scenarios, example data, and data resources within the ontology user story creation workflow."}, {"title": "Related Work", "content": "Various ORE methodologies, such as NeOn (Su\u00e1rez-Figueroa, G\u00f3mez-P\u00e9rez, and Fern\u00e1ndez-L\u00f3pez 2011), SAMOD (Peroni 2017), and eXtreme Design (XD) (Presutti et al. 2009), include tasks like eliciting user stories, defining purpose, scope, objectives, domain, coverage, and granularity, specifying formality levels, identifying end-users and intended uses, eliciting competency questions and expected answers, specifying non-functional requirements, gathering resources, validating competency questions, extracting glos-"}, {"title": "Data and Methods", "content": "To improve the user experience and develop pre-defined prompt templates tailored to user queries for the current OntoChat user story creation workflow, we conducted a three-step participatory prompting process (Sarkar et al. 2023): (1) Story Description - Participants wrote stories based on their ontological needs; (2) Prompt Development - Participants posed queries to request LLM support in writing user stories. Researchers refined these queries using pre-identified prompting strategies, which can be found on our GitHub repository\u00b9, then asked the LLM for answers, with participants evaluating the LLM-generated responses and providing feedback for further refinement; (3) Story Evaluation - Participants assessed the stories for usefulness, clarity, and inspiration, resulting in more detailed and practical user stories. Ethical clearance was obtained from the Department of Informatics at King's College London, ensuring that no personal data was collected throughout the experiment. The complete user study script can be found on our GitHub repository\u00b9. A prompting example of this participatory protocol pipeline is presented in our GitHub repository\u00b9. Below, we provide a detailed explanation of how each of these steps works."}, {"title": "Participants", "content": "This user study involved ten participants, including MuseIT practitioners aiming to build an ontology for multi-sensory representation in cultural heritage and PhD researchers from our department whose research focuses on knowledge engineering. Their complete demographic information can be found on our GitHub repository\u00b9. They have expertise in areas such as machine learning, human resources, mixed reality, implantable medical devices, multimodal representation learning, and responsible AI. Their familiarity with KGs, LLMS, ORE, and software RE varies, with many well-versed in KGs and LLMs. For example, PID4, a PhD researcher in implantable medical devices with moderate knowledge engineering experience, described his approach to creating user stories for cardiovascular device data management."}, {"title": "Step 1: Story Description", "content": "Participants were first instructed on how to write ontology user stories and what constitutes a good user story. These instructions can be found in our GitHub repository\u00b9. Participants were tasked with writing user stories leveraging their domain-specific knowledge. For instance, PID8, an expert in multimodal representation learning, described how an autonomous vehicle engineer uses an ontology-driven system to integrate visual, spatial, auditory, and textual data from vehicle sensors into a unified format. His story highlighted how this ontology aids in organising and accessing relevant datasets, the types of information included, and involved discussions on current methodologies, steps, tools, and challenges like defining clear goals and ensuring scenario relevance."}, {"title": "Step 2: Prompt Development", "content": "This involved iterative \"turns\" where participants interacted with a researcher and GPT-40. Each turn began with the participant posing a query related to their ontology project user story, such as seeking assistance in generating or clarifying part of a user story. For example, participant PID3 sought help in identifying a scenario that precedes the use of an ontology designed for mixed reality applications. The researcher then refined this query using pre-identified prompting strategies, which can be found on our GitHub repository\u00b9, asking the LLM to narrate a scenario about what happens before using the system, detailing specific interactions, challenges, and examples.\n\nParticipants provided feedback for the LLM's responses and requested further refinements or new queries. Initially, GPT-4 produced basic user stories with clear personas, goals, and scenarios but lacked detailed information. For instance, when participant PID6 asked for a story about integrating computer vision data, GPT-4's responses were too general, merely mentioning metadata without specifying details. As the sessions progressed, participant PID6 requested more specifics on data sources, models, and tools. GPT-4 improved by incorporating details like image metadata and data annotation techniques, though further refinement was needed for practical applications. Participants sometimes requested more realistic elements, such as managing data interoperability and image processing challenges. They also emphasised the query for interdisciplinary insights, reliable sources, and ethical considerations relevant to their domains.\n\nBy addressing these queries, the study identified key elements for creating detailed and practical user stories for ontology requirements engineering using an AI assistant.\n\nThroughout the participatory prompting sessions, researchers created specific user prompts to assist participants in critically evaluating the LLM's output and refining their queries for improved responses. These prompts encouraged participants to give feedback on the LLM's responses. For instance, a prompt might ask, \u201cWhat would you change in your query to make this more useful? Would you ask this a different way?\" This iterative method ensured that the LLM's outputs became progressively more relevant and detailed, ultimately leading to the development of more practical and comprehensive user stories."}, {"title": "Step 3: Story Evaluation", "content": "Post-activity interviews were conducted to gather participants' satisfaction with the current user stories. Each participant rated the stories generated with GPT-40 based on their relevance, clarity, and usefulness using a Likert scale from 1 to 5, where 1 indicated strong dissatisfaction and 5 indicated strong satisfaction. The user evaluation results diagram can be found in our GitHub repository\u00b9. All finalised prompts in our repository generated satisfactory results based on metrics of relevance, clarity, and usefulness, with each metric scoring 4 or higher, indicating high user satisfaction. Participants also identified any additional queries needed to improve the stories. If further queries were identified, the researcher would immediately try those queries and gather feedback again. This feedback loop helped refine the prompts and improve the overall quality of the user stories generated by OntoChat."}, {"title": "Data Analysis", "content": "After this user study, ten videos were manually transcribed, and the data was organised into five primary sections followed by a user story template, including \u201cPersona,\u201d \u201cGoal,\u201d \"Scenario,\" \"Example Data,\u201d and \u201cData Resource.\" The coding process combined top-down and bottom-up approaches, as outlined by (Gu 2014). Codes were designed to be overlapping to capture the multifaceted nature of user queries. To confirm reliability, the initial coding was reviewed at a later stage, and any deviations were corrected through iterative revisions."}, {"title": "Results", "content": "The complete list of user queries and related prompts can be found on our GitHub repository\u00b9. The results section provides some examples of user queries and their corresponding prompts. Figure 1 shows a standard ontology user story. An example of a user query and its corresponding prompt for LLM support in writing this user story could be:\n\n\u2022 User query: improve goal description\n\n\"Could you improve the goal by providing a more detailed description, including keywords that summarise the persona's objectives? Explain why these goals are important and align them with the persona's interests and skills.\"\n\nIn this example, the first part, \"Could you improve the goal by providing a more detailed description, including keywords that summarise the persona's objectives?\" represents the user's initial query. The second part, \"Explain why these goals are important and align them with the persona's interests and skills,\" was added after iterative refinement with researchers and participants. Participants found this addition helped the LLM generate a more satisfactory user story."}, {"title": "Persona", "content": "For the persona section, users request LLM assistance to detail persona traits, focusing on occupation, skills, and interests. This helps users reflect on the persona and create more accurate scenarios later. The example user query and prompt for this section are provided below:\n\n\u2022 User query: detail one persona trait:\n\n\"Could you improve the persona by detailing [this specific trait] that are necessary for helping in writing an ontology user story? Explain why (this specific trait) are important.\""}, {"title": "Goal", "content": "For the goal section, users request LLM assistance with tasks such as specifying goal-related keywords. To enhance the goal description, they ask the LLM for practical examples or to categorise the current goal into short-term and long-term objectives. They also seek the LLM's help in providing relevant factors like methodologies, tools, and interdisciplinary insights to make the goal more robust. The example user query and prompt for this section are provided below:\n\n\u2022 User query: provide some interdisciplinary insights:\n\n\"Could you improve the goal by providing some interdisciplinary insights the persona will consider? Discuss how knowledge from other fields will contribute to achieving their goals.\""}, {"title": "Scenario", "content": "For the scenario section, users sought LLM assistance with several tasks, such as detailing information about the current situation, including specific practical problems and their impact on goals. They also aimed to enhance the scenario section by asking the LLM to outline the sequence of actions, identify key players and their interactions, and describe desired changes and ideal scenarios. The example user query and prompt for this section are provided below:\n\n\u2022 User query: narrative story for one scenario:\n\n\"Could you narrate a detailed story about how the persona currently performs tasks related to their goal and how the system will enhance this process? Please include practical insights and a step-by-step sequence of actions.\u201d"}, {"title": "Example Data", "content": "For the example data section, users requested LLM assistance with several tasks: providing a general description of the data category, detailing key elements with specific examples, explaining relationships within the category, and identifying historical or situational details, such as how the category has evolved over time. The example user query and prompt for this section are provided below:\n\n\u2022 User query: detailed description of one category:\n\n\"Could you provide a detailed description of the characteristics of [this specific category item]? Explain how this data is used in your ontology and why it is important for the persona's goals.\""}, {"title": "Data Resource", "content": "For the data resource section, users requested LLM assistance with several tasks, including suggesting primary data sources, databases, and repositories, specifying data formats, and detailing the types of data included with a given source link. Users also needed the LLM to specify a given source link's dataset metadata, access methods, and any restrictions. An example of a user query and prompt for this section is provided below:\n\n\u2022 User query: check metadata quality of one data source:\n\n\"Could you provide a detailed description of the metadata information available in [this source]? Please include quotes from the cited source to support your explanation.\""}, {"title": "Discussion", "content": "Even with participatory prompting, generating effective prompts was a lengthy and iterative process. This involved multiple cycles of participants asking queries to write user stories, researchers converting these queries into prompts, and participants providing feedback on the LLM's responses. Knowing that every new query would be formally converted into a prompt, participants carefully considered each query before presenting it, resulting in some potential ideas being overlooked and untested. To address this, we encouraged and guided participants to actively provide feedback on the LLM's responses."}, {"title": "Future Works", "content": "After developing pre-defined prompt templates, we propose enhancing OntoChat's user story refinement workflow. Currently, OntoChat asks users a single, general question for refining the user story, leaving users uncertain about specific aspects needing improvement. To address this, we propose using the prompt templates and integrating an LLM to suggest targeted prompts based on the current user story. Users can respond to these prompts directly or personalise them based on their thoughts before submitting them to OntoChat. We have included screenshots in our GitHub repository\u00b9, showcasing the current state of the conversational and interface design. We plan to conduct a systematic A/B test, with one group evaluating OntoChat v1.0 and another group evaluating OntoChat v2.0, which integrates the newly developed functions."}, {"title": "Conclusion", "content": "In this study, we used participatory prompting protocols to enhance the user experience and create pre-defined prompt templates for ontology user story creation with LLMs. This method addresses challenges non-experts face with effective prompting. We also proposed new workflows to improve OntoChat's user story refinement by providing suggested prompts, which users can respond to or personalise. This structured approach helps users identify improvements and articulate queries effectively, enhancing user story quality and ontology engineering practices."}]}