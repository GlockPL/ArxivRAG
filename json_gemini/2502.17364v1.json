{"title": "Bridging Gaps in Natural Language Processing for Yor\u00f9b\u00e1: A Systematic Review of a Decade of Progress and Prospects", "authors": ["Toheeb A. Jimoh", "Tabea De Wille", "Nikola S. Nikolov"], "abstract": "Natural Language Processing (NLP) is becoming a dominant subset of artificial intel-ligence as the need to help machines understand human language looks indispensable. Several NLP applications are ubiquitous, partly due to the myriads of datasets being churned out daily through mediums like social networking sites. However, the growing development has not been evident in most African languages due to the persisting re-source limitation, among other issues. Yor\u00f9b\u00e1 language, a tonal and morphologically rich African language, suffers a similar fate, resulting in limited NLP usage. To encourage further research towards improving this situation, this systematic literature review aims to comprehensively analyse studies addressing NLP development for Yor\u00f9b\u00e1, identify-ing challenges, resources, techniques, and applications. A well-defined search string from a structured protocol was employed to search, select, and analyse 105 primary studies between 2014 and 2024 from reputable databases. The review highlights the scarcity of annotated corpora, limited availability of pre-trained language models, and linguis-tic challenges like tonal complexity and diacritic dependency as significant obstacles. It also revealed the prominent techniques, including rule-based methods, statistical meth-ods, deep learning, and transfer learning, which were implemented alongside datasets of Yor\u00f9b\u00e1 speech corpora, among others. The findings reveal a growing body of multilingual and monolingual resources, even though the field is constrained by socio-cultural factors such as code-switching and desertion of language for digital usage. This review synthesises existing research, providing a foundation for advancing NLP for Yor\u00f9b\u00e1 and in African languages generally. It aims to guide future research by identifying gaps and opportuni-ties, thereby contributing to the broader inclusion of Yor\u00f9b\u00e1 and other under-resourced African languages in global NLP advancements.", "sections": [{"title": "1 Introduction", "content": "Natural Language Processing (NLP) is one of the key components of artificial intelligence. It has rapidly gained prominence in recent years as the need to help machines understand human languages grows. NLP involves developing intelligent systems that can converse with humans through natural language [1]. These intelligent systems are products of the two main classes of NLP: natural language understanding and natural language generation. These classes coalesce into the generation of language and understanding its intricacies, respectively [2].\nNatural language typically denotes languages spoken and used by humans via various commu-nication channels for day-to-day interactions. In contrast, artificial languages, like computer languages, are those created with certain rules and restrictions [3]. Moreover, unlike artificial language, natural language possesses ambiguity, making its processing often a hard nut to crack. For instance, the statement, \u201cErik Ten Hag wins his 50th game with Manchester United,\" poses a situation where there could be two distinct meanings, depending on if one is saying Erik won his 50th game in charge of the club, or that Erik won his 50th career game with Manch-ester United. NLP applications have been seen in various exciting domains such as sentiment analysis [4, 5, 6], machine translation [7, 8, 9, 10], named entity recognition [11, 12, 13, 14], parts-of-speech (POS) tagging [15, 16, 17], question answering [18], amongst others, aiming to bridge the communication gap between humans and machines.\nNLP techniques have witnessed fascinating evolution with diverse languages worldwide since its inception in the 1950s [19], initially with the rule-based approach. This involves defining linguistic rules for core language concepts, including semantics, pragmatics, morphology and phonology. These techniques have metamorphosed from the rule-based approach to statistical approaches, subsequently incorporating machine learning techniques, which utilise large-scale linguistic resources. Undoubtedly, the explosion of large datasets, mainly through social net-working sites, necessitated the implementation of more advanced techniques. This births the introduction of deep learning methods [20], built on neural networks. Interestingly, the field of NLP witnessed revolutionisation through the advancement in deep learning, with notable models like the recurrent neural networks (RNNs), long short-term memory (LSTM), and more recently, the transformer architectures [21]. Moreover, this development has improved NLP performance in machine translation and question-answering tasks. More importantly, it gives way to a better understanding of sequential dependencies in natural language [22].\nFurthermore, self-attention mechanisms and bidirectional training in models such as bidirec-tional encoder representations from transformers (BERT), alongside the generative pre-trained transformer (GPT-3) model developed by OpenAI\u00b9, resulted in remarkable improvement in nat-ural language understanding [23]. This innovation continues to set the pace for the emergence of many improved large language models (LLMs) like the Large Language Model Architecture (LLaMA) [24], Mistral [25], and others. LLMs, usually trained on huge datasets, have been recorded to have achieved state-of-the-art result performance across various tasks, which tran-sitions from task-specific to task-independent architectures [26]. However, the dataset need of these models translates into a boon and bane for NLP in the context of many under-resourced languages in the world. These languages, because they are less digitised, sparingly taught, har-bour resource scarcity & low density, are less privileged, among other identifiers, are referred"}, {"title": "2 Background of study", "content": "This section briefly introduces the Yor\u00f9b\u00e1 language by discussing its constituents, such as letters and types, and detailing its root regarding its language family."}, {"title": "2.1 Yor\u00f9b\u00e1 language overview", "content": "Yor\u00f9b\u00e1 language is one of the largest low-resource African languages with over 47 million speakers, encompassing several dialects with considerable similarities [34, 35]. It is adopted as a native and social language in Western African countries, including Nigeria, Togo, Benin Republic, and other countries like Cuba, Brazil, etc. [36]. Yor\u00f9b\u00e1 uses 25 out of the 26 Latin-script letters, excluding q, z, v, x and c [37]. Thus, an additional 4 letters\u2014e, o, gb and s-with"}, {"title": "2.2 Yor\u00f9b\u00e1 Language Family", "content": "The Yor\u00f9b\u00e1 language belongs to the Niger-Congo family [41]. This language family is the most prominent and largest of the four major African linguistic groups: Niger-Congo, Nilo-Saharan, Afro-Asiatic and Khoisan [42]. This language family possesses distinctive noun class systems; nonetheless, they exhibit substantial variations in types, especially in morphological complexity [43]. A substantial part of languages of sub-Saharan Africa\u2014containing Western Africa, South-ern Africa, Eastern Africa, and Central Africa\u2014belong to this family, making up about 85% of the entire African language population [44]. Among these are Cape Town, South Africa, in the southern part; Dakar, Senegal, in the western part; and Mombasa, Kenya, in the east of Africa."}, {"title": "3 Materials and Methods", "content": "This section details the review processes, from planning to data synthesis. The guidelines comprehensively detailed in [46, 47] were followed to ensure adequacy in the review processes, from the selection of studies to the reporting stage. These guidelines help identify, analyse, and interpret all information from the primary studies considered without bias or prejudice."}, {"title": "3.1 Review Planning", "content": "A systematic literature review requires formal planning and conscientiousness. Consequently, a defined protocol was followed to ensure the success of the entire process. The data, in the form of research articles, the review objectives, and defined selection criteria, were managed"}, {"title": "3.2 Research Questions", "content": "Research questions were developed to define a precise template for the broad study objectives. Four questions have been carefully defined to cater to all possible domains intertwined with the research objectives by forming a structured set of interrogative statements whose answers are meant to provide insight into the research goal. The questions are motivated by the need to know the current status of Yor\u00f9b\u00e1 language involvement in NLP by investigating the specific"}, {"title": "3.3 Search Strategy", "content": "This section describes the gathering of the vital primary studies and the \u201csystematic\" steps toward achieving the goal. It is a crucial phase planned to eliminate potential bias and in-corporate randomisation in the studies' selection and sample size determination. Reputable databases related to the subject matter were painstakingly explored to obtain all relevant stud-ies for the systematic review using a well-defined search strategy. Generally, the central goal of the strategy involves attracting studies that have applied computational NLP approaches involving Yor\u00f9b\u00e1 language, be it as a monolingual, bilingual, or multilingual NLP set-up."}, {"title": "3.3.1 Method and Scope of Search", "content": "The strategy initially employed an automated search method to obtain studies relevant to the systematic review. This method involved combing each electronic database with the defined keywords, boolean operators, and wild cards as and when due. Furthermore, to ensure a representative sample and a higher recall, 9 databases were targeted in total viz: Web of Sci-ences, ScienceDirect, Google Scholar, Association for Computing Machinery (ACM) Digital Library, Institute of Electrical and Electronics Engineers (IEEE) Xplore, Semantic Scholar, Scopus, ScienceDirect, and Association for Computational Linguistics (ACL) Anthology.\nMoreover, with the motive of exploring a decade of the progress of NLP research in the Yor\u00f9b\u00e1 language paradigm, studies between 2014 and 2024 were chosen. This decade range was also established by considering the time of writing this paper, and it is plausible since the studies of 2014 were published towards the year's end. In addition, to ensure potent quality, only peer-reviewed journal articles and conference papers were included in the search results, with the language of writing exclusively in English. Also, due to the computational requirement of NLP research, studies mainly in Computer Science, Engineering, and Computational Linguistics"}, {"title": "3.3.2 Search Strings", "content": "The search strings denote the combination of keywords used in exploring the databases to ex-tract potential primary studies. Potential keywords in the systematic review objectives were identified and combined to build this. However, before the combination, a few related research publications were scanned through to see the usage of the relevant phrases and words. Thus, since the study seeks to synthesize information vis-a-vis NLP research and solutions associated with Yor\u00f9b\u00e1 language, the general search string is formulated based on the three segments and presented as follows:\n(\"Natural Language Processing\" OR NLP OR \u201cComputational linguistics\u201d OR \u201cneural net-work*\" OR statistic* OR \u201cmachine learning\u201d OR \u201cartificial intelligence\u201d OR corpus OR dataset* OR \u201cdata set*\") AND Yoruba AND (solution OR task OR application)\nHowever, it is pertinent to mention that the search string was modified slightly for different databases based on their unique requirements and search functionalities."}, {"title": "3.3.3 Study Selection Criteria", "content": "Having obtained a substantial number of studies considered relevant to the study, the crite-ria to painstakingly select the most essential and directly aligned studies were defined. This"}, {"title": "3.4 Quality Assessment", "content": "This section shows further efforts to ensure the credibility of primary studies used in the system-atic literature review. Specific quality questions were used for verification to further ascertain the reliability of the primary studies to be eventually included in the SLR. Thus, a quality checklist was created, following the guidelines in [46]. The response associated with each ques-tion is defined through a multiple-choice response: Yes, Partially, and No. A \u201cYes\u201d response shows higher certainty, thus getting a score of 2. Similarly, a \u201cPartially\" response is assigned 1, while a \u201cNo\u201d response is assigned 0. For this research purpose, a total quality score below the median score is to be excluded. The median is calculated as the value in the $$\\frac{n+1}{2}$$th position, where n is the maximum quality score, which equals 16. Thus, for quality scores S\u2081 = {1, 2, ..., 16}, the median position, Mp, is derived as follows:"}, {"title": "3.5 Data Extraction", "content": "Data extraction involves collecting information relevant to the SLR from the primary studies. It was carried out by defining an extraction template. This template is structured based on"}, {"title": "3.6 Data Synthesis", "content": "This section involves integrating and interpreting information extracted from the selected stud-ies. Initially, data were obtained from individual databases and combined to form the primary studies after excluding required studies based on the selection criteria. Findings were obtained from the primary studies across diverse domains, including the NLP tasks, techniques, re-sources, and challenges. Moreover, the process was carried out using Microsoft Excel and the Covidence platform, mentioned explicitly in Section 3.1. The ultimate goal at this stage is to identify patterns, trends, and knowledge gaps to inform future research directions in Yor\u00f9b\u00e1 NLP."}, {"title": "4 Results and Interpretations", "content": "This section discusses the results obtained from the primary studies included in the systematic review through the data synthesis. It helps answer the research questions and provides visual illustrations of every component of the primary studies."}, {"title": "4.1 General Statistics", "content": "The primary studies included in the research, totalling 105, were synthesized for relevant data. They consist only of journal articles and conference papers based on the selection criteria spec-ified in the systematic review protocol."}, {"title": "4.2 Research Question Analysis", "content": "This section contains pertinent responses to specific research questions outlined by the system-atic literature review objectives. The analysis presented herein represents a key contribution to the review. It summarises the inquiry findings through structured responses to each question, making it an essential tool for answering similar questions for future researchers."}, {"title": "4.2.1 RQ1: What NLP tasks have been addressed for the Yor\u00f9b\u00e1 language?", "content": "Investigating the various NLP tasks involved with applying Yor\u00f9b\u00e1 language has revealed im-proved and promising results over the years. These NLP tasks or solutions synthesis considered the application of the language either as a monolingual NLP research or as part of a bilingual or multilingual task only considered when there is a vivid emphasis on the language.\nEfforts have been made to democratize the numbering system of Yor\u00f9b\u00e1 language through NLP with an earlier study [49], which tried to develop a computational system for converting cardi-nal numbers into their Yor\u00f9b\u00e1 number names in a number-to-text conversion task. Similarly, [50] develops a machine translation system for translating English number text into Yor\u00f9b\u00e1 text, considering the context. While the former focused on the text normalization task, a core"}, {"title": "4.2.2 RQ2: What techniques have been employed for Yor\u00f9b\u00e1 NLP?", "content": "The metamorphosis of NLP techniques involving Yor\u00f9b\u00e1 language is observed to not necessarily lag behind the general trend of accomplishment in the NLP community. Most of the earlier research works are seen to be limited to implementing rule-based techniques [87, 30, 64, 55] be-"}, {"title": "4.2.3 RQ3: What language resources are available for Yor\u00f9b\u00e1 language?", "content": "Many low-resource languages experience stunted growth in their language development due to the flaw associated with their lack of substantial datasets required for various NLP tasks amidst recent data-hungry models. This factor has undoubtedly prompted significant efforts at developing various language resources for Yor\u00f9b\u00e1 language to further improve the performance of various NLP tasks in the language. The developed resources have been seen to cut across different domains; while some studies solely focused on resource development, others utilised the datasets or corpora on specific tasks. The summary of the corpora, which contains Yor\u00f9b\u00e1 either as a monolingual, bilingual, or as an important part of a multilingual dataset, is pre-sented in Table 7. Furthermore, for detail purposes, specific datasets or corpora with assigned names are briefly outlined individually as follows, while others are listed as part of the general descriptions:\nAFRIQA: AFRIQA [18] pioneers among cross-lingual question-answering (QA) corpus devel-"}, {"title": "4.2.4 RQ4: What are the major challenges in developing NLP solutions for Yor\u00f9b\u00e1?", "content": "Natural language solution development for Yor\u00f9b\u00e1 faces multiple challenges limiting its growth as a low-resource language. Data synthesis in this study has helped identify five primary chal-lenges, including linguistic, technical, resource, cultural and societal factors, and evaluation and benchmarking challenges.\nLinguistic challenges: Yor\u00f9b\u00e1 language exhibits several linguistic properties that make NLP"}, {"title": "5 Discussion", "content": "This systematic review was intended to capture the growth and current stage of NLP participa-tion for Yor\u00f9b\u00e1 language over a decade. Through the information synthesised from the primary studies, it is obvious that significant progress has been made in notable areas of NLP involving Yor\u00f9b\u00e1 language. MT has particularly received more attention through the inclusion of the language primarily in bilingual MT research [64, 68, 74, 69, 28, 67, 50] involving translation between two languages. Related studies investigating the inherent prerequisite towards enhanc-ing translation qualities in MT model qualities, the development of MT tools, such as vowel elision resolution [30], and the application of rough set theory [29] in translation have been explored. Moreover, the need to create better quality data [76] has also led towards improved research in this domain through the development of additional parallel corpora and leveraging multilingual language pre-training to improve the model training capability in the presence of large high-quality datasets.\nSimilar to MT research, significant efforts have been made in TTS synthesis, with considerable efforts towards corpus development, as evidenced by publicly available datasets, most of which are speech corpora. Even though it is the primary focus in most research investigating the task, some of the studies have included ASR and speech-to-text [101, 97, 34] in their experiments contributing to the entire research. Furthermore, sentiment analysis and information retrieval tasks involving Yor\u00f9b\u00e1 have also benefited significantly from multilingual corpus development through the need to develop language resources for low-resource African languages. Most have been through the works in [4] and [111].\nGenerally, NLP tasks such as sentiment analysis, machine translation, text-to-speech synthe-sis, automatic speech recognition, named entity recognition, information retrieval, parts-of-speech tagging, and language modelling have received considerable research efforts, with at least four studies each addressing them. In addition, innovative approaches, transfer learning, and diacritic-aware systems have demonstrated promising results, showcasing the adaptability of state-of-the-art techniques to Yor\u00f9b\u00e1 NLP, albeit earlier studies have depended mostly on the rule-based methods. Resources like Yor\u00f9b\u00e1 speech corpus [36], MENYO-20k [72], Lagos NWU Yor\u00f9b\u00e1 Speech Corpus [16] among others, have also been pivotal in advancing the field by providing foundational datasets, as they have been used in more than one study.\nFurthermore, recent studies have showcased the importance of multilingual and cross-lingual techniques, as they help to promote language availability for many African languages simulta-neously. This is evident from named entity recognition [12, 14], sentiment analysis [6, 4, 5], and information retrieval tasks [18, 111, 109]. Also, models pre-trained on multilingual datasets have been seen to be beneficial for a low-resource language like Yor\u00f9b\u00e1. Ultimately, it is also evident that collaborative, open-source and community initiatives have greatly improved re-search and availability of resources in the bid to overcome resource scarcity whilst fostering knowledge-sharing among these resource-scarce African languages.\nHowever, in the context of constraints stunting the rapid growth of NLP in Yor\u00f9b\u00e1 language, studies have highlighted mainly linguistic complexity such as diacritic dependency [31] and tonal variation [66]. Other challenges involving cultural and societal factors were also highlighted, including the primary hindrance, which has always been a limitation in available corpora or"}, {"title": "5.1 Limitation of study", "content": "While this systematic literature review investigates the progress and status of NLP involving Yor\u00f9b\u00e1, it is noteworthy to mention that \u201cYor\u00f9b\u00e1\u201d in this case is not specific to a certain dialect of the language, such as Yor\u00f9b\u00e1 of If\u00e8, \u00ccj\u00e8b\u00fa, or \u00ccl\u00e0je. The study recognised Yor\u00f9b\u00e1 language as one encompassing several dialects across different countries. Even though certain sections of it specifically highlighted NLP research and resources involving Yor\u00f9b\u00e1 dialects [34], this study might not be sufficient when dialects of the language are the sole focus in the NLP research.\nFurthermore, the last date for retrieving information for the study was October 2024. Hence, publications emerging afterwards would not have been included in the synthesis. Similarly, only peer-reviewed studies were included to ensure high quality and reliability in findings. This might limit a recently published relevant study undergoing a peer-review process during the period the databases were searched. Nevertheless, the systematic review ensures a substantial representation of information from various primary studies by considering a decade of publica-tion years."}, {"title": "6 Conclusion and Future Directions", "content": "This section summarises the study and process involved and aims to inform readers of possible research areas towards improving Yor\u00f9b\u00e1 language involvement in NLP research."}, {"title": "6.1 Conclusion", "content": "This SLR explores NLP progress involving Yor\u00f9b\u00e1 language. It involves surveying studies between 2014 and 2024, which have used Yor\u00f9b\u00e1 language in their NLP research, and those with great emphasis on the language, in case of a multilingual setting. Moreover, data were synthesized from these primary studies to deduce findings, thereby procuring answers to the established four research questions, which form the core objectives of the research.\nThe research questions explore the tasks, techniques, language resources, and challenges in-volved in Yor\u00f9b\u00e1 NLP over a decade. Established protocols and guidelines were followed systematically to ensure maximum formal inundation, eliminating possible bias. Moreover, the information synthesised from the 105 primary studies has been carefully reported, encapsulat-ing the relevant findings from the systematic review.\nUltimately, with this study, language researchers will be abreast of the current progress in Yor\u00f9b\u00e1 NLP, thereby equipping them with the necessary ideas to preserve the language through NLP tool representation\u2014this is crucial as it is a widely spoken language with abundant cultural richness and linguistic features. Similarly, it is required to guide future researchers plying this route to eliminate or limit possible odyssey in their research journey."}, {"title": "6.2 Future Directions", "content": "Even though the findings show promising results for NLP research involving Yor\u00f9b\u00e1 language, it is pertinent to outline the current absence of significant efforts in some domains, which are equally important towards improving NLP in the language. For instance, research efforts involving the identification of abusive, offensive, hate speech or cyberbullying, which are all regarded as harmful language usage, have not been explored. This could be due to a lack of or limited annotated corpus in this domain. Such research tasks are essential for ameliorating the usage rate of sensitive words, phrases, and sentences on social media, as they could potentially endanger other users. Consequently, future research efforts can be directed toward building rel-evant corpora to address these NLP challenges and creating relevant benchmarks to facilitate empirical analysis and continuous research.\nSarcasm detection is another fascinating realm that remains untapped yet for Yor\u00f9b\u00e1 and most under-resource languages. Sarcasm is used to mask the true emotion in a state, mostly by exuding positivity or a seeming positive demeanour. This phenomenon of saying what is not meant or meaning what is not said by natural language users poses challenges for NLP. How-ever, it is essential to accurately detect users' real emotions for different purposes, including product reviews, feedback analysis, politics & governance, among others. Consequently, devel-oping relevant sarcasm corpora and benchmarks that will potentially birth advanced sarcasm detection models is crucial. Future research can be focused on this domain to promote certainty in natural language usage.\nMoreover, general resource development tasks are essential to limit the paucity of Yor\u00f9b\u00e1 lan-guage resources. This could be achieved through continuous collaboration, open-source, and community initiatives, such as the one carried out through Maskhane16. Moreover, more pri-ority can be given to developing Yor\u00f9b\u00e1-specific pre-trained models and fine-tuning existing multilingual models for better performance in under-resourced settings.\nFurthermore, more research should be directed towards solving linguistic challenges such as tonal variations, morphology complexities, and diacritic restoration, as these are essential for decoding the nuances in the language. Developing pre-processing tools and models that cater to and integrate linguistic knowledge or incorporate phonological features in the language could significantly improve performance.\nFinally, addressing the drawbacks associated with cultural and social challenges will require developing context-aware models that can adapt to real-world changes. Continuous language usage would also benefit from exploring new cases, such as LLM conversational agents, and developing healthcare and educational tools incorporating NLP. These innovations can aid in bridging the digital gaps among the Yor\u00f9b\u00e1-speaking communities."}]}