{"title": "A LITTLE CONFIDENCE GOES A LONG WAY", "authors": ["John Scoville", "Shang Gao", "Devanshu Agrawal", "Javed Qadrud-Din"], "abstract": "We introduce a group of related methods for binary classification tasks using probes of the hidden state activations in large language models (LLMs). Performance is on par with the largest and most advanced LLMs currently available, but requiring orders of magnitude fewer computational resources and not requiring labeled data. This approach involves translating class labels into a semantically rich description, spontaneous symmetry breaking of multilayer perceptron probes for unsupervised learning and inference, training probes to generate confidence scores (prior probabilities) from hidden state activations subject to known constraints via entropy maximization, and selecting the most confident probe model from an ensemble for prediction. These techniques are evaluated on four datasets using five base LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Incorporating constraints based on prior knowledge remains an open problem in language modeling. We offer a mechanism to incorporate certain types of constraints, based on probes of the hidden state activations of LLMs (Alain & Bengio, 2018). We take as a starting point the Contrast-Consistent Search (CCS) method of Burns et al. (2023) for unsupervised training of probe models. We treat the mutually exclusive nature of binary classes as a constraint on prior knowledge, and apply the Maximum Entropy Principle (MEP) (Jaynes, 1957a;b; 2003) to generate prior probabilities. The end result is vastly improved LLM inference on batched binary classification problems, comparable to the best general-purpose LLMs currently available but achieved using orders of magnitude fewer computational resources. These methods can be implemented efficiently on a single consumer-grade GPU. Moreover, these methods do not require supervised fine-tuning, or even access to data labels.\nNovel contributions of this study include confidence-based unsupervised ensemble learning of neural network hidden layer probes, information-theoretic loss functions for the training of these probes, a pretraining procedure to break the permutation symmetry inherent to the probe models, and a translation step to cast arbitrary class labels into a semantically meaningful form suitable for consumption by probe models. Throughout the paper we refer to these techniques collectively as Glia, after the glial cells (glia) that provide support and constraints for neurons. The methods are evaluated on four datasets: Amazon polarity, IMDB, CUAD (Contract Understanding Atticus Dataset (Hendrycks et al., 2021), and Learned Hands (a legal issue identification dataset)."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 PROBE MODELS", "content": "Probe models were used by Alain & Bengio (2018) to evaluate the separability of features in the hidden layer activations of deep neural networks. Single-layer neural networks were trained to learn linear projections from hidden states onto features. Hidden states were analyzed at different depths in a deep neural network, and it was found that deeper layers become progressively more linearly separable. In this study, we consider deep neural network hidden state \"probes\" that have multiple hidden layers and may be trained in an unsupervised manner.\nVarious approaches have been proposed to gauge neural networks' confidence in their predictions (Goan & Fookes, 2020; Kull et al., 2019; Guo et al., 2017; Ding et al., 2021; Zhao et al., 2021). In"}, {"title": "2.2 ENTROPY MAXIMIZATION", "content": "The Maximum Entropy Principle (MEP) is a key concept in statistical physics and Bayesian statistics that formalizes Occam's razor\u00b3. Jaynes (1957a;b) proposed that entropy maximization should be elevated to the level of a general statistical principle. It provides a means of \"translating prior information uniquely into a prior probability assignment\u201d (Jaynes, 2003). The MEP dictates that one should choose the prior probability distribution having maximum entropy subject to known constraints on prior information to arrive at prior probabilities.\nThe R\u00e9nyi entropy $H_\\alpha(x) = \\frac{1}{1-\\alpha} \\log \\sum_i P(x_i)^\\alpha$ (R\u00e9nyi, 1961) parameterizes a family of information measures via a parameter \u03b1. It generalizes several information measures, including Shannon information (lim \u03b1 \u2192 1), Hartley information, (\u03b1 = 0) and collision entropy (\u03b1 = 2). The MEP is traditionally expressed using the Shannon entropy, and this arises naturally from the geometry of flat statistical manifolds (Amari & Nagaoka, 2000). The geometry of curved statistical manifolds similarly leads to a generalized MEP utilizing R\u00e9nyi entropy (Morales & Rosas, 2021). Probe models in Glia make use of this generalized MEP, maximizing R\u00e9nyi entropy to convert hidden layer activations into prior probabilities, given constraints on prior knowledge."}, {"title": "3 METHOD DESCRIPTION", "content": null}, {"title": "3.1 MODEL OVERVIEW", "content": "The method is illustrated in Figure 1. Prompts $x_+$ and $x_\u2212$ are constructed by appending positive and negative responses to a batch of base prompts, x. Hidden layer activations $\\phi(x_+)$ and $\\phi(x_\u2212)$ are the final hidden layer activations produced by a language model $\\phi$ during a forward pass.\nHidden layer activations $\\phi(x_+)$ and $\\phi(x_\u2212)$ are normalized to have zero mean and unit variance across the batch. $\\phi(x_+)$ and $\\phi(x_\u2212)$ become input data for unsupervised training of an ensemble of secondary neural networks, $N$. Each of these neural networks learn an entropy-maximizing nonlinear projection of the hidden layers onto a single logit. Projections learned by maximizing entropy subject to the mutual exclusion constraint generate prior probability distributions over labels via the MEP.\nFinally, for each example, $N(\\phi(x_+))$ and $N(\\phi(x_\u2212))$ become inputs to binary softmax functions that normalize the probe outputs, producing the final prior probabilities that are used as confidence scores, $P(x_+) = C e^{N(\\phi(x+))}$ and $P(x_-) = C e^{N(\\phi(x\u2212))}$, where $C^{-1} = e^{N(\\phi(x+))} + e^{N(\\phi(x\u2212))}$. The softmax layer ensures that estimated prior probabilities $P(x_+)$ and $P(x_-)$ sum to unity."}, {"title": "3.2 PROBE MODEL LOSS FUNCTION", "content": "We apply the generalized MEP to the probe model loss function to produce prior probability esti- mates. This is accomplished via a two-part loss function, one term maximizes R\u00e9nyi entropy and the other term enforces prior knowledge about binary classification, specifically, mutual exclusion. The resulting loss function allows us to produce prior probabilities - confidence scores - using the generalized maximum entropy principle.\nLet $P(x_+)$ and $P(x_\u2212)$ be the outputs of the final softmax layer of the probe model. We introduce a constraint loss term derived from the confidence loss term used by CCS (Burns et al., 2023). The CCS confidence loss is $min (P(x_+), P(x_-))^2$. Since $P(x_\u2212) = 1 \u2212 P(x_+)$ due to the final softmax layer, the confidence loss reduces to a function of a single variable. Additionally, we make the exponent (quadratic in CCS) a model hyperparameter, B. Our constraint loss term is:\n$L_{constraint} = min (P(x_+),1 \u2212 P(x_+))^\\beta$\nThis term of loss function encodes a known prior constraint, specifically, that the two possible labels are mutually exclusive. A peak at p = 0.5 and minima at p = 0 and p = 1 make this loss function promote mutual exclusion (P(x+) != P(x\u2212)) by penalizing solutions in which the outcomes have equal prior probability (P(x+) = P(x\u2212)) and promoting solutions in which p takes on extreme values 0 and 1. Other types of constraint loss functions expressing different prior knowledge or logic could potentially be used, but in this study we focus on the exclusive-or relationship of mutual exclusion in the context of binary classification.\nThe R\u00e9nyi entropy parameter, \u03b1, also becomes a tunable hyperparameter of the model. Both loss terms are necessary to avoid degenerate solutions in which probabilities collapse to 0.5 and 0.5 (without the constraint loss) or to 0 and 1 (without the entropic loss), as described by Burns et al. (2023). Loss hyperparameters \u03b1 = 0.7 and \u03b2 = 0.3 are used throughout this study. The full loss function used to train neural network probe models is the sum of the entropic and constraint terms. These terms can be weighted, but the model is relatively insensitive to the coefficients. Our evaluations use $C_1 = 1$ and $C_2 = 10$.\n$L = C_1 L_{entropic} + C_2 L_{constraint}$\nThis loss function produces prior probabilities subject to known constraints on prior information, via the generalized MEP."}, {"title": "3.3 PROBE ARCHITECTURE", "content": "The architecture of the neural network probe is a multilayer perceptron (MLP). In the evaluated implementation, this MLP has two hidden layers, the second having half the rank of the first to encourage compression while reducing parameter count. The outputs of two MLP instances (for positive and negative examples) are bridged by a softmax layer to output binary class probablities. The loss function is evaluated using these probablities. The MLP consists of an input layer of width n, and output layer of rank 1, and some number of hidden layers (2 hidden layers are used in our evaluation). Intermediate layer ranks vary linearly between n and 1.\nWhile the probe model is relatively insensitive to intermediate layer activation functions, the acti- vation function of the final layer is significant, as it determines the range of probabilities that can be produced by the softmax layers. We use tanh activations for the final hidden layer and GeLU activations for intermediate layers."}, {"title": "3.4 ENSEMBLES OF PROBE MODELS", "content": "Probe models are computationally inexpensive to train relative to the compute typically needed to perform inference and generate the final hidden states of a large language model. Since the overall"}, {"title": "3.5 LABEL TRANSLATION", "content": "Many datasets contain arbitrary labels that don't convey much semantic information. Since probes rely on semantic contrast in the latent vector space associated with the labels, arbitrary labels such as \"a\" vs \"b\" or even \u201cyes\u201d vs \u201cno\u201d don't convey as much useful semantic content as \"positive\" vs \"negative\" or \"good weather\u201d vs \u201cbad weather\". For the best performance, we ideally want labels that are descriptive and self-contained. Since datasets are under no obligation to provide such labels, we perform a translation step in which either a human or instruction-tuned language model answers the question \"What does (label 0) mean here?\" to produce meaningful labels. The descriptive labels are substituted for the original labels. In the few-shot case, label occurences in the prompt should also be substituted with the new labels. This translation step needs to be done just once per task, during dataset creation, so human labling is fast and straightforward. Automatic description is also feasible, we evaluate generating this using the prompts \u2018<base prompt >Describe the meaning of the response \"<class label >\" to this question in three words or less.', for each class label. More robust and sophisticated appoaches are possible, the automatic label translation demonstrated in this study should be regarded as a proof-of-concept.\nSemantically rich label descriptions can boost model performance significantly for some tasks. This tends to be the case when translated labels are descriptive, as concise as possible without losing de- tail, and semantically relevant to the query. Binary classification tasks admitting such a description are the types of tasks that could perform well with Glia methods - notably, the same label descrip- tion should apply to every example in the dataset. Labels should be semantically equivalent across the dataset, not merely syntactically equivalent. The methods are better suited to true binary clas- sification tasks, sorting examples into two meaningful equivalence classes, compared to arbitrary two-choice questions."}, {"title": "3.6 SYMMETRY BREAKING", "content": "We propose a pretraining and training technique to break the probe's symmetry and enable fully unsupervised inference by associating labels with inputs. Without symmetry breaking, label ori- entation is randomly assigned during training. The model loss function has permuation symmetry - swapping x- for x+ results in an identical loss. As such, there are two identical minima with equally good but opposite solutions, the one we reach is determined by the random initialization of the neural network parameters at the beginning of the training process. We break this symmetry before probe training, when we perform a few epochs of cross-entropy training of the probe model, without the final softmax layer, using the hidden states of candidate labels, an empty prompt, and/or examples from a few-shot prompt.\nWe implement two variants of symmetry breaking training, one designed for zero-shot inference use cases, and the other for few-shot use cases (or zero-shot cases where both labels appear in the prompt). In zero-shot mode, the data used for pretraining are hidden layer activations of the labels, obtained via a forward pass of the language model on the labels themselves - $\\phi(-)$ and $\\phi(+)$. We wish to assign these states the numerical indices 0 and 1, respectively. These are used to construct a two-element synthetic dataset in which the data are $(\\phi(-),0)$ and $(\\phi(+), 1)$. The model is then pretrained using cross-entropy loss on these data for a few epochs. When multiple class labels appear in the prompt (e.g. in a few-shot prompt), this weakens the influence of directly associating labels with their hidden states. In order to ensure correlations across the ensemble, cross-entropy"}, {"title": "4 EVALUATION", "content": "We evaluate Glia on three types of tasks: sentiment classification using IMDB and Amazon reviews, legal contract understanding with CUAD (Contract Understanding Atticus Dataset), and legal is- sue identification with the croudsourced Learned Hands dataset. These contain a total of 49,089 examples spanning 56 tasks.\nWe perform two Glia evaluations. One evaluates the orientation of models in a semi-supervised manner, post-hoc, using evaluation data, akin to Burns et al. (2023). The other evaluation uses symmetry breaking. The symmetry breaking evaluation is fully unsupervised where prompts use zero-shot inference (IMDB and Amazon datasets). Symmetry breaking evaluation is is technically"}, {"title": "4.1 SUMMARY OF RESULTS", "content": "Table 1 shows the results of Glia evaluation using five different base LLMs. Mistral7B-v0.3-Instruct and Llama3-8B-Instruct are instructed to automaticaly generate the descriptive labels used for eval- uation. For Mistral7B-v0.3, Llama-3-8B, and Deberta-v3-large, two human-generated descriptive labels (see Appendix) are provided for each task. We randomly select 10,000 reviews from IMDB (stanfordnlp/imdb dataset) and 10,000 reviews from Amazon amazon_polarity dataset) for sentiment classification. These are the easiest tasks in the evaluation, with the performance of several models clustered around 95-96 percent. This level of performance is significantly better than CCS and zero-shot inference using the same base LLMs, however. We also evaluate the CUAD dataset (Hendrycks et al., 2021) using prompts from Legalbench (Guha et al., 2023). This consists of 38 tasks and 17980 total examples, tasks involve determining whether legal contracts contain particuar types of clauses. The Learned Hands dataset croudsources labels for legal issues identified in online posts. Labels are determined through a game-like voting process in which users flag legal issues in documents. Learned Hands is the most difficult dataset in our evaluation. We evaluate per- formance on the Learned Hands dataset using prompts from Legalbench (Guha et al., 2023). This"}, {"title": "4.2 ABLATION TESTING", "content": "We also perform ablation testing for the varous techniques introduced by this study, with the excep- tion of symmetry breaking, which was effectively ablated (vs. post-hoc label orientations) in the main evaluation. For ablation testing, we evaluate ablations post-hoc using the CUAD dataset and two base LLMs, Mistral 7B 0.3 and Llama-3-8B, to show model dependence. Results are shown in Table 2. In the loss ablation test, the technique is evaluated using the CCS loss function from Burns et al. (2023) but with all the other Glia techniques active and non-ablated. In the translation ablation test, labels simply aren't translated, and the ensemble ablation test uses an ensemble size of 1. Ablating individual component techniques does not account for all of the performance gains observed from Glia inference. We believe this is due to interactions between the Glia methods. The"}, {"title": "4.3 COMPUTATIONAL RESOURCES", "content": "GPT-40 and Gemini-1.5-Pro were evaluated through their respective APIs, and Llama 3.1 405B was hosted by Fireworks.AI. All other testing, including all Glia LLM inference and probe training, was performed using a single NVIDIA RTX 4090 GPU. Detailed information regarding the computa- tional resources of the baseline models are not available, but are believed to exceed this by multiple orders of magnitude. Simply based on parameter count, Llama 3's 405 billion parameters are more than 55 times the size of Glia+Mistral v0.3's 7.3 billion parameters, with proportional hardware reqirements.\nIn contrast to 0-shot inference requirig one forward pass through an LLM, Glia requires two for- ward passes, in addition to training the ensemble of probes. In the evaluations presented here, training ensembles of 9 models takes roughly the same amount of time as the two forward passes, so the entire procedure takes roughly 4x the amount of compute as 0-shot inference. We note that single-model ensembles achieved competitive performance using the Mistral 7B v0.3 base model and human-generated labels, in that case the computational requirement is roughly twice that of 0- shot inference. Based on the 55x performance gain and a 4x increase in computational requirements, we believe the Glia methods represent an order-of-magnitude increase in efficiency."}, {"title": "5 DISCUSSION AND CONCLUSION", "content": "We have seen that the Glia methods can dramatically improve LLM inference across a wide range of binary classification tasks. Inference performance can be comparable to LLMs requiring orders of magnitude more computational resources. In our evaluation, the Mistral 7B models performed better than the Llama 8B models, both in terms of correctly orienting labels and probe performance, in spite of these LLMs being roughly comparable in size and capability. The reasons for this discrepancy are not currently well understood, and additional research into the suitability of particular LLMs for these methods would be beneficial.\nA possibly significant direction for future development is the extension of these methods to other types of constraints. More general constraints could extend the applicability of the method from binary classification to multiclass classification and potentially benefit generative use cases. Con- straints based on prior knowledge could potentially mitigate the hallucination problem to make generative AI more accurate and trustworthy.\nAll evaluations in this study used text-based examples. Evaluating multimodal binary classification performance of the Glia techniques could potentially demonstrate applicability to cover a broader range of task modalities. To explore the applicability of Glia methods to general document analysis, evaluating longer context (100k+ tokens) tasks could be beneficial. The examples used in this study have fewer than 8192 tokens, evaluating and/or extending the method with longer contexts could enable practical use cases for longer documents."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "J. Scoville contributed, implemented, and evaluated the methods introduced in this paper. S. Gao selected evaluation datasets and performed evaluations. D. Agrawal suggested the incorporation of"}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 TASK PROMPTS", "content": "Where logprobs are available, we directly compare logprobs for the two classes to perform 0-shot and few-shot inference. For externally hosted models where logprobs are not readily available (GPT-40, Gemini-1.5-Pro, Meta-Llama-405B), system prompts instruct the model to answer only with the relevant class labels. For example:\n\"Answer questions with only the word 'positive' or the word\n'negative', and nothing further at all. No additional text or\npunctuation.\""}, {"title": "A.1.1 IMDB", "content": "Consider this movie review: {{text}} Between \"negative\"\nand \"positive\", the sentiment of this review is"}, {"title": "\u0410.1.2 AMAZON", "content": "\"Consider this Amazon product review: {{text}} The sentiment of\nthis review is \""}, {"title": "A.1.3 CUAD AND LEARNED HANDS", "content": "CUAD and Learned Hands tasks use their 6-shot base prompts from Legalbench. (Guha et al., 2023)\n(https://github.com/HazyResearch/legalbench/)"}, {"title": "A.2 LABELS", "content": "For automatic label translation, we first extract the question from each base prompt for each task by truncating any few-shot examples from the base prompt. This text is inserted into the label translation prompt, along with one of the original task labels:\n<Question> Describe the meaning of the response \"<label>\" to this\nquestion in three words or less."}]}