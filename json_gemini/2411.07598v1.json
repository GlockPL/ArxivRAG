{"title": "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations", "authors": ["Rose E. Wang", "Kenny Lam", "Pawan Wirawarn", "Omar Khattab", "Dorottya Demszky"], "abstract": "Many open-ended conversations (e.g., tutoring lessons or business meetings) revolve around pre-defined reference materials, like worksheets or meeting bullets. To provide a framework for studying such conversation structure, we introduce Problem-Oriented Segmentation & Retrieval (POSR), the task of jointly breaking down conversations into segments and linking each segment to the relevant reference item. As a case study, we apply POSR to education where effectively structuring lessons around problems is critical yet difficult. We present LessonLink, the first dataset of real-world tutoring lessons, featuring 3,500 segments, spanning 24,300 minutes of instruction and linked to 116 SAT\u00ae math problems. We define and evaluate several joint and independent approaches for POSR, including segmentation (e.g., TextTiling), retrieval (e.g., ColBERT), and large language models (LLMs) methods. Our results highlight that modeling POSR as one joint task is essential: POSR methods outperform independent segmentation and retrieval pipelines by up to +76% on joint metrics and surpass traditional segmentation methods by up to +78% on segmentation metrics. We demonstrate POSR's practical impact on downstream education applications, deriving new insights on the language and time use in real-world lesson structures.", "sections": [{"title": "1 Introduction", "content": "Across education, business, and science, many open-ended conversations like meetings or tutoring sessions are designed to address a set of predefined topics. As a prominent example, educators often shape their lessons around worksheet problems. Structuring lessons effectively is critical but challenging, as educators must allocate the right amount of time to different problems, while addressing different student learning needs (Haynes, 2010; Henderson, 1997; Panasuk and Todd, 2005). However, many novices or educators teaching large groups of students struggle with lesson structuring and often run out of time (Stradling and Saunders, 1993; Pozas et al., 2020; Deunk et al., 2018; Takaoglu, 2017; Hejji Alanazi, 2019).\nProviding evidence-based insights on lesson structuring is a key step towards addressing this challenge. These insights provide educators feedback on their teaching (Fishman et al., 2003; Kraft et al., 2018; Lomos et al., 2011; Desimone, 2009), tutoring platforms on training priorities (Hilliger et al., 2020; Gottipati and Shankararaman, 2018; Hilliger et al., 2022) and curriculum developers on material design (O'Donnell, 2008; Fullan and Pomfret, 1977). Unfortunately, obtaining insights on lesson structures at scale is challenging.\nThe study of conversation structure around reference materials draws on concepts from two, typically distinct natural language processing (NLP) tasks: discourse segmentation to identify segments in the conversations and information retrieval (IR) to retrieve the relevant reference material for each segment. While each task has rich literature, studying them jointly reveals real-world challenges that existing works bypass. For example, discourse segmentation methods assume that conversations share the same structure (Ritter et al., 2010; Hearst and Plaunt, 1993; Chen and Yang, 2020), but education conversations have unique structures as teachers adapt their lessons to different needs. While prior IR work has studied supporting natural-language queries over conversations (Sanderson et al., 2010; Oard et al., 2004; Chelba et al., 2008), the reverse task of using open-ended conversation segments as queries for retrieving domain-specific reference materials has not received similar attention.\nTo address these gaps, we make several key contributions. We define the Problem-Oriented Segmentation and Retrieval (POSR) task for jointly segmenting conversations and linking segments to relevant reference materials, such as worksheet problems (Figure 1). Unlike segmentation or retrieval alone, the joint POSR task reflects the realistic opportunities and challenges presented by knowing the potential reference topics (from the reference materials) for conversation segments.\nPOSR provides a general framework for studying conversation structure around reference materials. As a case study, we apply POSR to the education setting. We contribute LessonLink, a novel dataset of real-world tutoring lessons featuring 3,500 segments, 116 SAT\u00ae math problems, and over 24,300 minutes of instruction. Our open-source dataset consists of real tutoring conversations paired with SAT\u00ae math worksheets, each conversation lasting about 1.5 hr long. Each conversation is segmented and each segment is linked with one of the 116 problems. To the best of our knowledge, this is the first dataset to include real-world conversations of unique structures linked with reference materials like worksheets.\nEvaluating POSR is challenging: Existing segmentation metrics do not measure time-weighed errors and existing metrics fail to reflect the subtle ways in which segmentation and retrieval errors interact. To address this, we contribute time-aware segmentation metrics adapted from standard line-based metrics (e.g., WindowDiff from Pevzner and Hearst (2002)) and introduce the Segmentation and Retrieval Score (SRS) to jointly measure segmentation and retrieval accuracy as the proportion of conversation where the retrieved item matches the ground truth.\nWe define and evaluate a suite of segmentation, retrieval and POSR methods on LessonLink, including traditional segmentation methods like TextTiling (Hearst, 1997), popular IR methods like ColBERT (Khattab and Zaharia, 2020) and long-context large language models (LLMs) like Claude and GPT-4 (Anthropic, 2024; OpenAI, 2024). Our results highlight the importance of POSR's joint approach: POSR methods outperform independent segmentation and retrieval pipelines by up to +76% on SRS metrics and traditional segmentation methods by up to +78% on segmentation metrics. However, several challenges remain. In domains with high privacy risks like education, companies are often unwilling to share data long-term due to privacy concerns. Moreover, while LLMs achieve strong POSR performance, their high API costs on long texts raise scalability concerns. Our findings motivate the need for more cost-effective, open-sourced methods that can deliver high accuracy on joint reasoning tasks like POSR.\nFinally, to further highlight the utility of POSR to real-world scenarios, we describe two novel applications of POSR to illustrate its potential for impacting evidence-based practices in education. First, through a linguistic analysis, we discover that tutors who spend more time on problems provide richer conceptual explanations. Tutors who spend less time provide procedural explanations. Second, POSR quantifies wide variability in how long tutors spend on the same problem. These examples point to opportunities for improving language and time-management practices."}, {"title": "2 Related Work", "content": "Discourse segmentation is the task of partitioning conversations into segments, traditionally a preprocessing step before retrieval or summarization of conversations (Hearst and Plaunt, 1993; Callan, 1994; Wilkinson, 1994; Galley et al., 2003; Chen and Yang, 2020; Althoff et al., 2016; Salton and Buckley, 1991a,b; Salton et al., 1996; Huang et al., 2003). Different domains like customer service or meetings define segments differently, e.g. as a speech act, a topic, or a conversation stage (Liu et al., 2023; Riedl and Biemann, 2012; Prabhakaran et al., 2018); In this work, we study problem-oriented segments: conversation segments that discuss individual math problems. While most existing segmentation methods assume conversations exhibit predictable structure (Ritter et al., 2010; Hearst and Plaunt, 1993; Chen and Yang, 2020), education conversations are diverse and lack such predictable structure.\nMath information retrieval poses special challenges (Munavalli and Miner, 2006; Sojka and L\u00ed\u0161ka, 2011; Nguyen et al., 2012) because math expressions can be difficult to represent contextually (Schubotz et al., 2016; Kamali and Tompa, 2013; Zanibbi and Blostein, 2012; Aizawa and Kohlhase, 2021). Our setting combines these challenges with the additional difficulty of treating conversational segments as queries, unlike typical retrieval using well-formed keyword queries (Wang et al., 2024). Our LessonLink dataset provides a new resource of real world education conversations segmented and linked to math problems from worksheets. This enables the study of POSR, combining discourse segmentation with retrieval of math materials."}, {"title": "3 Problem-Oriented Segmentation and Retrieval (POSR)", "content": "We define the task of Problem-Oriented Segmentation and Retrieval (POSR) as jointly dividing a conversation transcript into segments and retrieving the relevant topic (e.g., problem) discussed in each segment. While segmentation and retrieval are individually challenging, POSR jointly addresses them together to improve ecological validity and expose new system design tradeoffs. We hypothesize (and show in Section \u00a76) that systems aware of retrieval topics will segment better, and vice versa, motivating joint POSR methods.\n3.1 Task Definition\nGiven a transcript T = <T1,...,TN) of N lines and a corresponding reference corpus R = (R1,..., Rw) (e.g., a worksheet of problem entries), the POSR objective is to output an array of segment id and problem reference id for each line in the transcript, Y = [(S1, W1), (82, W2),..., (sN, WN)]:\n\u2022 $1,..., SN is the segment id for each line in line. So, s1 is the segment id for the line 1, $2 the segment id for line 2, and so on.\n\u2022 W1,...,\u03c9\u03bd \u2208 {R1,...,Rw} indicate the problem reference id from the corpus.\nSince these transcripts originate from real-world conversations, each line T\u2081 is associated with a start and end timestamp, start, tend. Algorithm 1 highlights POSR methods, which take both transcript T and retrieval corpus R into account for segmentation, in contrast to independent segmentation and retrieval methods.\n3.2 Metrics\nTo evaluate the effectiveness of POSR methods, we introduce the standard and our novel metrics for evaluating segmentation and retrieval individually and jointly. As evident in Algorithm 1, the segmentation metrics help capture how segmentation may be improved by accounting for the retrieval corpus. We additionally adapt standard metrics to also take time into account. Finally, we also account for practical considerations by reporting cost.\nExisting, line-based segmentation metrics. We use two established metrics for segmentation accuracy: WindowDiff from Pevzner and Hearst (2002) and Pk metric from Beeferman et al. (1999). Both If si = s; then wi =  Wj."}, {"title": "4 The LessonLink Dataset", "content": "We introduce the LessonLink dataset as a concrete case study of POSR. LessonLink contains real-world tutoring lesson transcripts segmented and linked with problems in SAT\u00ae math worksheets. The dataset features 3,500 segments of over 24,300 minutes of instruction, featuring 1,300 unique speakers and 116 linked problems. We release the LessonLink dataset under the CC Noncommercial 4.0 license.\nData source. We collected the data in partnership with Schoolhouse.world, a free peer-to-peer tutoring platform that supports over ~80k students worldwide with the help of ~10k volunteer tutors. One of their main focuses is to help high school students prepare for the SAT, a standardized test used for college admissions in the United States. The platform shared de-identified transcripts with us from their March 2023 SAT\u00ae Math Bootcamp, a four week-long course where tutors met with students in small groups twice a week to practice SAT\u00ae math problems. We randomly picked 300 transcripts. Schoolhouse received consent from parents and students to share de-identified data for research purposes. The maximum tutor-student ratio in each bootcamp is 1:10. Tutoring lessons are 80 minutes long. Schoolhouse recommends a lesson structure that starts with 30 minutes of warm-up exercises followed by the students working on the worksheet independently and then a group review. Tutors have freedom in structuring their lesson and they typically use their students' practice test results to determine what to focus on.\nTranscripts. Each tutoring lesson is recorded and transcribed automatically via Zoom. Schoolhouse de-identified the transcripts using the EduConvoKit library (Wang and Demszky, 2024), with tutor and student names replaced with placeholder tokens \"[TUTOR]\u201d and \u201c[STUDENT]\u201d.\nWorksheets. Each transcript is linked to an SAT\u00ae problem worksheet that the tutor and students work on during the lesson. The sheets include official, publicly available math practice problems created by College Board\u00ae, the organization that administers the SAT\u00ae exams. Each worksheet has about 16 problems on average. We split each worksheet into separate problem images, and use Pytesseract, an optical character recognition (OCR) tool, to extract the text content from the images (PyTesseract, 2017). OCR does not capture the visual components (e.g., graphs). We focus only on using the text data, and leave visual data for future work.\nAnnotation. The definition of a segment varies across domains like customer service, meetings, and tutoring sessions (Liu et al., 2023; Riedl and Biemann, 2012). Our definition builds on Schoolhouse.world's curriculum structure that dedicates time for an introduction to the session, targeted warm-up exercises, and worksheet problems. We use the following segment categories: (1) Informal. These segments include introductory talk or off-task discussions (Carpenter et al., 2020; Rodrigo et al., 2013). Examples include the group doing an ice-breaker game. (2) Warm-up problem. These segments discuss warm-up problems that are not a part of the session's main worksheet. (3) Worksheet problem. These segments discuss a problem from the session's main worksheet.\nWe recruited 3 annotators who were familiar with the Schoolhouse materials and tutoring session structure. This domain familiarity was important in ensuring high-quality annotations. The annotation process was carried out using Excel sheets, and annotators were compensated at a rate of $20 per hour. Segment annotations happen at the level of a transcript line, as provided by Zoom. Each transcript line includes a start and end timestamp in milliseconds. While Zoom uses its own proprietary ASR technology, the lines typically capture a single utterance without the speaker making a pause. To ensure alignment and consistency, the start/end of a segment happens on the end of a sentence. This means that if a sentence is broken up into two lines, the last line would be considered for the segment annotation.\nTo determine human agreement on this task, the annotators annotated the same 30 lesson transcripts for segments and linked problems. On a line-level, the inter-rater segmentation accuracy was 98.9% and retrieval accuracy was 100%. We also use Cochran's Q (Cochran, 1950) to evaluate segmentation agreement, similar to prior work (Galley et al., 2003): Cochran's test evaluates the null hypothesis that the number of subjects assigning a boundary at any position is random. The test shows that the inter-rater reliability is significant to the 0.01 level for 98% of the transcripts. Given the high inter-rater agreement, the 3 annotators annotated 300 transcripts. We create a small 1:10 train/test split on our dataset: The train set containing 30 transcripts and the test set 270 transcripts. We intentionally have a large test set: While some methods require a training set, we prioritize a robust evaluation of zero-shot methods and thus have a larger test set. This approach is consistent with other zero-shot evaluations in the literature (Chen et al., 2021;"}, {"title": "5 Evaluation", "content": "This section describes the methods and evaluation setup which uses LessonLink's test split. Appendix \u00a7B includes more information on our prompting setup for GPT4 and Claude LLMs.\nSegmentation. We evaluate a series of common segmentation methods. We evaluate top-10 and top-20 word segmentation, i.e. we take the top-10 and 20 words found in the segment boundaries of the train set to segment the test set. We also evaluate existing approaches like TextTiling (Hearst, 1997) and topic- and stage-segmentation methods from Althoff et al. (2016) and Chen and Yang (2020), which segment discourse by topics and stages. Lastly, we test zero-shot prompting long-context LLMs like GPT-4-turbo (OpenAI, 2024) and the Claude variants Haiku, Sonnet, and Opus (Anthropic, 2024). We omit open-source, instruct-tuned LLMs like Llama-2 (Touvron et al., 2023), Llama-3 (Meta, 2024), or Mixtral (Jiang et al., 2024) because their context windows are not long enough for our transcripts.\nWe fit the topic and stage segmentation methods on our train split, and use three pre-trained encoders from Sentence-Transformers (Reimers and Gurevych, 2019): the base-nli-stsb-mean-tokens (originally used in Chen and Yang (2020)), all-mpnet-base-v2, all-MiniLM-L12-v2. These encoders did not vary in performance. Therefore, we report results on the first encoder and Appendix D reports the rest. Stage segmentation requires the number of segments a priori; our experiments vary this to be either the rounded average or maximum number of segments in LessonLink.\nRetrieval. We evaluate several methods for IR: Jaccard similarity (Jaccard, 1912), TF-IDF (Sammut and Webb, 2011), BM25 (Robertson et al., 2009), ColBERTv2 (Santhanam et al., 2021), zero-shot prompting GPT-4-turbo, Claude Haiku, Claude Sonnet, and Claude Opus. Retrieval is challenging in our setting. Retrieval methods must handle the semantic variability in how problems are discussed and referenced. The conversations do not follow a sequential order of problem IDs, and the references to problems are highly contextual, making lexical cues insufficient for dictionary-based retrieval.\nA challenge in using traditional IR methods in our setting is specifying that nothing in the worksheet is linked to a segment, e.g., for informal or warmup segments. For instruct-tuned LLMs, we can simply specify this in the prompt. For traditional IR methods, we must set a threshold value for what is deemed relevant enough to the segment. We perform 5-fold cross validation on the training set and set the threshold to the average value that best separates on the held-out fold. We report these thresholds in Appendix \u00a7C.\nPOSR. We combine the best independent segmentation method with each retrieval method and report their joint performance. We also evaluate zero-shot prompted GPT-4-turbo, Claude Haiku, Claude Sonnet, Claude Opus as POSR methods that perform segmentation and retrieval jointly."}, {"title": "6 Results", "content": "Table 2 summarizes the joint evaluations, and Table 3 summarizes the segmentation results. The POSR methods outperform most independent segmentation and retrieval approaches, and at lower costs. POSR Opus and POSR GPT4 achieves slightly higher Line- and Time-SRS to their independent counterparts, and much higher to other combined independent approaches, e.g., Opus+TFIDF on both SRS metrics. Additionally, POSR methods are much more cost-effective, as they require only a single prompt to perform both segmentation and retrieval, rather than multiple prompts handling these tasks separately: POSR Opus and POSR GPT4 cost $11-$21 per 100 transcripts, while the best combined independent methods, Opus+GPT4, cost $54 per 100 transcripts. This demonstrates the importance of jointly modelling segmentation and retrieval for better accuracy and cost performance. However, there is still room for improvement such as future work on developing and improving open-sourced long-context methods.\nAccording to Table 3, POSR methods perform better than most independent segmentation methods by a large margin. For example, POSR Opus improves upon topic and stage segmentation methods by ~ 57% on Pk and WindowDiff. The poor performance of top-10 and top-20 word segmentation indicates that segmentation cannot be solved by word-level cues alone. Additionally, we find that POSR methods perform better than their independent LLM segmentation counterparts. For example, POSR Sonnet improves upon Sonnet across all segmentation metrics, such as 0.23 \u2192 0.13 on Line-Pk or 0.37 \u2192 0.31 on Line-WindowDiff. Incorporating retrieval items enhances segmentation accuracy by providing additional context for more precise boundary detection, reinforcing the importance of treating segmentation and retrieval jointly.\nThe time- and line-based metrics for segmentation and SRS are well-correlated across methods, indicating that accounting for time does not impact relative rankings. However, time-weighing is still important in accounting for errors in long segments: Time-Pk errors are lower than Line-Pk because it reduces the impact of oversegmentation whereas Time-WindowDiff amplifies errors from missing long segments."}, {"title": "Segmentation error analysis.", "content": "To better understand sources of segmentation error, we investigate the difference in segment numbers (reported in Table 4) and we examine the bigram language in false segment insertions compared to true segment insertions with the log odds ratio, latent Dirichlet prior, measure defined in Monroe et al. (2008). Table 4 reveals that traditional methods oversegment, being sensitive to low-level topics shifts. Surprisingly, while Haiku has a higher segmentation error rate in Table 2, it achieves the lowest segment count difference, altogether indicating that Haiku inserts new (albeit few) segments far away from true segment boundaries. The log odds results in Table 5 indicate that incorrect segments are inserted when the tutor introduces examples (e.g., \u201clet's say\u201d), alternative explanations (e.g., \u201cThere are different ways to solve this\u201d), or participation prompts (e.g., \u201chow did you like start to approach this problem?\"). This analysis signals areas for improvement in precise segmentation.\""}, {"title": "Retrieval error analysis.", "content": "We conduct a qualitative analysis on retrieval errors, particularly those in the independent methods. A large error source is caused by long segments that are incorrectly segmented for reasons illustrated in the previous section. For example, long problem segments are broken up and incorrectly linked. Oversegmentation also yields shorter segment queries for retrieval, reducing the similarity to the target reference. This particularly impacts traditional methods whose similarity thresholds are set with the ground truth segments as explained in Appendix C. In Appendix E, we compare retrieval methods on ground-truth segments and confirm that ground truth segments significantly boosts retrieval accuracy, especially for LLM methods. Thus, we conclude that inaccurate segmentation is a critical bottleneck to mitigating downstream retrieval errors."}, {"title": "7 Downstream Applications", "content": "There are several applications that POSR enables for gaining insights into tutoring practices at scale. We illustrate two. One application is a language analysis to compare how tutors talk about the same problem with the long vs. short talk times (top and bottom quartile). We use the log odds ratio measure from Monroe et al. (2008) to estimate the distinctiveness of a bigram using Edu-ConvoKit (Wang and Demszky, 2024). We report the top-3 bigrams on the most popular problem from Lesson-Link and qualitative examples in Figure 2. The log-odds analysis reveals that in short segments, tutors tend to stick to the language from the \u201cproblem statement\u201d and immediately explain the answer. However, in longer segments, tutors provide examples to students (e.g., \u201clet's say\u201d), and offer conceptual explanations inferring the underlying mathematical concept (e.g., \u201cthis is a conditional probability question\u201d). The second POSR application is the analysis of talk time distributions across different tutors and problems: some problems have very different talk times (e.g., problem 11), while others have similar talk times (e.g., problem 12). Altogether, POSR enables these downstream applications and can tackle the large challenge of lesson structuring in education."}, {"title": "8 Discussion and Conclusion", "content": "We introduce the Problem-Oriented Segmentation and Retrieval (POSR), a task that jointly segments conversations and retrieves the problem discussed in each segment. We contribute the LessonLink dataset as a concrete case study of POSR in education. LessonLink is the first large-scale dataset of tutoring conversations linked with worksheets, featuring 3,500 segments, 116 linked SAT\u00ae math problems and over 24,300 minutes of instruction. To evaluate the joint performance and account for time in segmentation, we introduce the Segmentation and Retrieval Score (SRS) and time-based segmentation metrics for Pk and WindowDiff. Our comprehensive evaluations highlight the importance of jointly modeling segmentation and retrieval, rather than treating them as independent tasks: POSR methods significantly outperform the independent approaches as measured against the traditional segmentation, SRS, and new time-based metrics. The LLM-based POSR methods achieve the best performance, but come at a higher cost, motivating future work on cost-effective solutions. We also demonstrate the potential of POSR by showcasing downstream applications, such as a language analysis comparing tutoring strategies. In conclusion, our work establishes POSR as an important task to study conversation structure. The LessonLink dataset and the proposed methods pave the way for further research in joint segmentation and retrieval, with broad implications for educational technology, conversational analysis, and beyond."}, {"title": "9 Limitations", "content": "While our work provides a useful starting point for understanding conversations (such as in education) at scale, there are limitations to our work. Addressing these limitations will be an important area for future research.\nOne limitation is the lack of connection to outcomes. While prior works have explored the relationship between duration and sequencing of problems on student attention (e.g., Stevens and Bavelier (2012) inter alia), there is limited research on how these factors impact long-term student learning, particularly in group-based settings. Understanding this connection is crucial for grounding POSR in real contexts.\nAdditionally, POSR does not rigorously link the language content with the segment duration or ordering. This applies to other conversation domains as well, beyond education settings. Linking content and quality of the language with the time allocation and sequencing matters (Suresh et al., 2018): Are tutors soliciting student contributions, or talking all the time? Are they restating or engaging with student contributions? While our downstream applications illustrate one form of language analysis with a log odds analysis, future work should investigate using language categories, instead of unsupervised methods for understanding language patterns.\nAnother limitation is the absence of audio and visual inputs. Our current models rely solely on textual data and miss non-verbal cues that add to the full context in understanding conversations. We also only use the problem text, and ignore the problem's visual components such as graph information. Incorporating multimodal data, such as audio and visual inputs, could improve the accuracy of POSR systems."}, {"title": "10 Ethical Considerations", "content": "The purpose of this work is to promote and improve effective interactions, such as in the setting of education, using NLP techniques. The LessonLink dataset is intended for research purposes. The dataset should not be used for commercial purposes, and we ask that users of our dataset respect this restriction. As stewards of this data, we are committed to protecting the privacy and confidentiality of the individuals who contributed comments to the dataset. It is important to note that inferences drawn from the dataset should be interpreted with caution. The intended use case for this dataset is to further research on conversation interactions and education, towards the goal of improving interactions. Unacceptable use cases include any attempts to identify users or use the data for commercial gain. We additionally recommend that researchers who do use our dataset take steps to mitigate any risks or harms to individuals that may arise."}, {"title": "Algorithm 1 POSR vs. non-POSR methods", "content": "Require: T, R\nif with POSR then\n$1,...,SN \u2190 segment (T, R)\nelse\n$1,..., SN\u2190 segment (T)\nW1,...,wn \u2190 retrieve([$1,..., SN], R)\nend if"}, {"title": "Pk(Y,Y*)", "content": "Pk(Y,Y*) = \\frac{1}{N-k} \\sum_{j=1}^N 1 (1(b(s_{j:j+k}) > 0) \\neq 1(b(s*_{j:j+k}) > 0))"}, {"title": "Time-Pk(Y,Y*)", "content": "Time-Pk (Y, Y*) = \\frac{1}{N-k} \\sum_{j=1}^N 1 (1(b(S_{t_{start}:t_{end}+k}) > 0) \\neq 1(b(S*_{t_{start}:t_{end}+k}) > 0))"}, {"title": "\u03b1-SRS(Y, Y*)", "content": "\\alpha-SRS(Y, Y*) = \\frac{1}{N} \\sum_{i} \\frac{\\alpha_i 1 (w_j(s_j) == w_i^*)}{ \\sum_i \\alpha_i}"}]}