{"title": "HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting", "authors": ["Jingyu Lin", "Jiaqi Gu", "Lubin Fan", "Bojian Wu", "Yujing Lou", "Renjie Chen", "Ligang Liu", "Jieping Ye"], "abstract": "Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS) in scenes featuring transient objects is challenging. We propose a novel hybrid representation, termed as HybridGS, using 2D Gaussians for transient objects per image and maintaining traditional 3D Gaussians for the whole static scenes. Note that, the 3DGS itself is better suited for modeling static scenes that assume multi-view consistency, but the transient objects appear occasionally and do not adhere to the assumption, thus we model them as planar objects from a single view, represented with 2D Gaussians. Our novel representation decomposes the scene from the perspective of fundamental viewpoint consistency, making it more reasonable. Additionally, we present a novel multi-view regulated supervision method for 3DGS that leverages information from co-visible regions, further enhancing the distinctions between the transients and statics. Then, we propose a straightforward yet effective multi-stage training strategy to ensure robust training and high-quality view synthesis across various settings. Experiments on benchmark datasets show our state-of-the-art performance of novel view synthesis in both indoor and outdoor scenes, even in the presence of distracting elements.", "sections": [{"title": "1. Introduction", "content": "3D Gaussians Splatting (3DGS) [11] has recently gained popularity for novel view synthesis due to its high-quality rendering, efficiency and low-memory cost. Its applications span across virtual reality, augmented reality, and robotics, etc [3, 6, 15, 16, 25, 31, 35, 39, 50]. Typically, most current approaches assume that the input images are posed and free of noise, which are the prerequisite needed to represent a complete scene well. However, this assumption is often not met. For example, images taken casually with a mobile phone usually contain messy dynamic objects, or transient objects. Therefore, it is hard to model the scene because there are always many transient occlusions. Technically, 3DGS is capable of modeling static scenes effectively because the static objects in images satisfy geometric consistency constraints across different viewpoints. However, transient objects do not follow this assumption. If the images containing transient objects are naively used in training 3DGS, such variations can lead to difficulties in achieving coherent blending of individual Gaussian representations, potentially resulting in artifacts or inaccuracies."}, {"title": "2. Related Work", "content": "To address this issue, numerous methods have been proposed to improve the scene representation ability involving transient objects. In the era of Neural Radiance Fields (NeRF) [20], RobustNeRF [26] introduced a robust loss function that minimizes the influence of photometrically inconsistent observations during training, thereby enhancing the overall quality of the representation. NeRF On-the-go [24] was the first to leverage the uncertainty of transients by incorporating DINOv2 [22] features into a shallow MLP. It focused on loss supervision in the areas of low uncertainty to effectively remove transients from dynamic scenes. These two methods have established widely-used benchmark datasets featuring transients, aiming to evaluate the reconstruction of clear static scenes with improved results of novel view synthesis. For 3DGS, SpotLessSplats (SLS-mlp) [27] integrated the designs of those aforementioned methods, employing adaptive method to detect outliers through clustering in the feature space, achieving the best current results.\nHowever, these previous approaches neglect two critical issues: (1) The ambiguity between semantics and transients. Utilizing features from visual models to capture the semantic characteristics of transients can inherently lead to confusion in distinguishing between them. (2) The absence of explicit transient modeling. These methods primarily focus on achieving a robust statics by merely mitigating the impact of transients, rather than comprehensively modeling the whole scene themselves.\nIn response to the issues mentioned above, our key observation is that transients lack multi-view consistency and usually only appear in a single view. Therefore, we consider them as planar objects at that view. That is to say, this planar representation is a unique feature of the current view. Based on this, we propose HybridGS, a hybrid representation with 2D Gaussians for transient objects per image and 3D Gaussians for the whole static scenes as illustrated in Fig. 1, which naturally decouples the transients and statics based on their intrinsic properties. With this novel representation, while modeling transients as planar objects, the fundamental consistency of viewpoints is also maintained for statics. To further enhance the performance, a multi-view regulated supervision method for 3DGS is adopted to incorporates information from co-visible regions across multiple views, leading to more accurate representation. Additionally, we employ a simple yet effective multi-stage training strategy that ensures robust training and consistent view synthesis across various scenarios, thereby guaranteeing better stability and convergence during the training process. Extensive experiments on benchmark datasets demonstrate our state-of-the-art quality of novel view synthesis in both indoor and outdoor scenes, leading to the accurate decoupling of transients and statics as well as high efficiency in training and inference stage. The contributions are summarized as follows.\n\u2022 We are the first to introduce a novel hybrid representation that combines image-specific 2D Gaussians with static 3D Gaussians, enabling effective modeling of transient objects within casually captured images.\n\u2022 We develop a multi-view supervision scheme for 3DGS that utilizes overlapping regions across multiple views, which enhances the model's capability to distinguish between static and transient elements, ultimately improving the overall quality of the novel view synthesis.\n\u2022 Our HybridGS achieves the state-of-the-art performance in benchmark datasets, which outperforms the previous methods and sets a new standard for novel view synthesis in the scenes with transients."}, {"title": "2.1. Novel View Synthesis", "content": "NeRF (Neural Radiance Fields) [1, 2, 13, 17, 20, 21, 23, 33, 34] revolutionizes 3D scene reconstruction by representing scenes as continuous neural networks that map 3D coordinates to color and density values, which allows for the synthesis of photorealistic views from arbitrary camera positions. Recently, 3D Gaussian Splatting (3DGS) [9, 11, 12, 18, 28, 44, 45] has gained attention as an effective method for novel view synthesis. It starts with an initial point cloud and transforms it into optimizable 3D Gaussian primitives, known for their high-quality rendering and fast processing speeds. Nevertheless, existing methods require the assumption that only static scenes are present in the input images to effectively utilize multi-view geometric consistency to train a coherent scene representation."}, {"title": "2.2. Modeling Transients and Statics in NeRF/3DGS", "content": "In casually captured footage of real-world scenes, there are often numerous dynamic occlusions, such as moving pedestrians and vehicles. Many works have tried to address challenges in scenes featuring both static and transient objects. RobustNeRF [26] was the pioneer work that introduces a robust loss function that reduces the influence of photometrical inconsistency during training, thereby improving the overall reconstruction quality on its proposed dataset. Other methods [4, 19, 40] also applied the similar idea to strengthen the robustness to outliers by down-weighting or discarding inconsistent observations based on the magnitude of color residuals. NeRF On-the-go was the first to introduce DINOv2 [22] features to predict pixel uncertainty, in an attempt to achieve clean static reconstructions without transients. It also introduced a benchmark dataset of casually captured images, aimed at evaluating static reconstruction performance for novel views.\nFollowing these two methods and their datasets, recent methods of 3DGS have also explored techniques for"}, {"title": "2.3. Image Representation with 2D Gaussians", "content": "Recently, inspired by 3DGS, some methods [48, 49] in the field of image representation and compression have started to use 2D Gaussians to represent images. GaussianImage [48] leverages 2D Gaussians on the pixel plane to achieve comparable reconstruction quality while offering enhanced compression and accelerated rendering capabilities. Image-GS [49] creates a content-adaptive image representation by fitting a target image by adaptively allocating and progressively optimizing a set of 2D Gaussians.\nNote that the 2D Gaussians used here differ from the 2D Surfels (also referred to as 2D Gaussians) introduced by [9]. While 2D Surfels are designed to enable perspective-correct splatting through 2D surface modeling, ray-splat intersection, and volumetric integration in 3D space, and the 2D Gaussians in GaussianImage [48] is a flexible, compact, and content-adaptive image representation within 2D space.\nHere, we incorporate the trainable 2D Gaussians to learn transient and uncertainty masks during the training process, decomposing the whole scene with 2D and 3D Gaussians."}, {"title": "3. Preliminaries", "content": "3D Gaussian Splatting (3DGS) represent a 3D scene explicitly with a set of 3D anisotropic Gaussians {$G_{3d}$}. Each Gaussian is parameterized by its centroid $x_{3d}$ \u2208 $R^3$, scale $S_{3d}$ \u2208 $R^3$, rotation matrix $R$ \u2208 $R^{3\u00d73}$, opacity \u03b13d \u2208 $R$ and color $c_{3d}$ \u2208 $R^3$ encoded in spherical harmonic (SH) coefficients. The 3D covariance matrix \u03a33d of the 3D Gaussian is obtained from $R_{3d}$ and $S_{3d}$:\n\u03a33d = $R_{3d}$$S_{3d}$${S_{3d}}^T$${R_{3d}}^T$\n(1)\nThe 3D Gaussians are defined in world space for a spatial point $y_{3d}$ following:\n$G_{3d}$($y_{3d}$) = ($y_{3d}$-$x_{3d}$)$\u03a3_{3d}$($y_{3d}$-$x_{3d}$)\n(2)\nGiven J, the Jacobian of the affine projective transformation and W, a viewing transformation, the covariance matrix \u03a3'3d in camera coordinates is defined as follows:\n\u03a3'3d = JW\u03a33dWTTJT\n(3)\nThese 3D Gaussians are projected to 2D imaging plane and blended through a fast, differentiable \u03b1-blending process to render 2D images. The color $C_{3d}$ of each pixel is computed using N-ordered 2D splats with the following formula:\n$C_{3d}$ = $\u03a3_{i=1}^{N}$\u03b13d$C_{3di}$($\u03a0_{j=1}^{i-1}$(1 \u2013 $\u03b1_{3dj}$)\n(4)\nwhere $\u03b1_{3di}$ and $\u03b1_{3di}$ is the final opacity calculated by $\u03b1_{3di}$ = $\u03b1_{3di}$ * e-($y_{3d}$-$x_{3d}$)$\u03a3'_{3d}$($y_{3d}$-$x_{3d}$). And $x_{3d}$ and $y_{3d}$ are coordinates in the projected space. Typically, the centroids of 3D Gaussians are initialized using the sparse SfM point clouds obtained from the set of input images.\n2D Gaussian Splatting (2DGS) is initially introduced by [48] in the task of implicit image representation. Since 2D Gaussians {$G_{2d}$} are no longer oriented to 3D scenes but to 2D space, many bloated operations and redundant parameters are discarded, such as viewing transformation, spherical harmonics, etc. Concretely, a basic 2D Gaussian is described by its centroid $x_{2d}$ \u2208 $R^2$, 2D covariance matrix \u03a32d \u2208 $R^2$, color $c_{2d}$ \u2208 $R^3$ and opacity \u03b12d \u2208 $R$. The 2D covariance matrix is also factorized into a rotation matrix $R_{2d}$ \u2208 $R^{2\u00d72}$ and a scale matrix $S_{2d}$ \u2208 $R^2$ as:\n\u03a32d = $R_{2d}$$S_{2d}$${S_{2d}}^T$${R_{2d}}^T$\n(5)\nFor rasterization, since the depth and camera parameters are not included in 2D Gaussians, a simplified accumulated summation of N-ordered 2D splats is calculated as follows:\nC2d = $\u2211_{i\u2208N}$ $\u03b1_{2d}$c2di\n(6)\nwhere $\u03b1_{2d}$ is the opacity calculated by $\u03b1_{2d}$ = $\u03b1_{2di}$ * e-($y_{2d}$-$x_{2d}$)$\u03a3_{2d}$($y_{2d}$-$x_{2d}$). And $x_{2d}$ and $y_{2d}$ are coordinates of centroid and any pixel in the 2D image space. Specifically, 2D Gaussians can be considered as projections of 3D Gaussians from a certain perspective, illustrating the relationship of Gaussians between 2D and 3D space. In our case, we combine the single-view independent 2D Gaussians and multi-view consistent 3D Gaussians to represent the entire 3D scene, which helps fundamentally decoupling transients and statics, based on the principles of multi-view geometry."}, {"title": "4. Method", "content": "We delineate the pipeline of our approach as illustrated in Fig. 2. Essentially, given a set of input images {$I_{k}$|$k$ = 1, 2, ..., N} with the corresponding camera parameters, for each view $I_{t}$, the goal of our method is to reasonably decouple the transients $I_{t}$ and statics $I_{s}$ as follows:\n$I = M_{t}$ \u00a9 $I_{t}$ + (1 - $M_{t}$) $I_{s}$,\n(7)\nwhere the $M_{t}$ \u2208 [0,1] represents the transient mask, and \u00a9 is per pixel multiplication. When a pixel's value in $M_{t}$ approaches 1, it indicates a higher probability that the location is transient; conversely, a lower value suggests a greater likelihood that area is static.\nTo achieve the goal, we decompose the whole 3D scene with two components: (1) Multi-view consistent 3D Gaussians is used for rendering $I_{s}$, which leverages multi-view information from images and models the static scene with a set of unified 3D Gaussians. The multi-view inputs regulate the 3D Gaussians to be consistent and robust across the different views. (2) Single-view independent 2D Gaussians is responsible for modeling $I_{t}$, which enables our approach to handle the fact that images are casually captured with varying transients. Concretely, we forms a set of view-independent 2D Gaussians to model transients as planar objects from a single view. This novel combination allows for a more precise and reasonable representation of 3D scenes, enabling better performance on novel views.\nThis section is organized as follows: we introduce our hybrid representation of scenes using 3D Gaussians in Sec. 4.1 and 2D Gaussians in Sec. 8.1, followed by a comprehensive overview of the training pipeline in Sec. 4.3."}, {"title": "4.1. Modeling Statics with 3D Gaussians", "content": "Generally speaking, the initialized point clouds obtained by COLMAP [29, 30] can only represent areas in the scene that satisfy multi-view consistency, so 3D Gaussians tend to reconstruct static scenes, such as buildings, grounds, etc. However, in scenes with transients, continuously training the 3D Gaussians may cause the transient objects to be overfitted into the Gaussian kernels because of constraints imposed by the RGB loss. This approach fails to capture the true geometry and instead overemphasizes the RGB information, leading to annoying artifacts from other viewpoints and ultimately impacting quality of novel view synthesis.\nWe address this problem by adopting an enhanced multi-view regulated supervision scheme. Unlike previous methods that rasterize and supervise only a single image per iteration, our approach introduces two major changes. (1) We increase the number of images per iteration to K, enabling gradient back-propagation to simultaneously consider mutual information from multiple views. This batch-wise inputs allow each optimization step to leverage multi-view insights to distinguish and infer transients and static elements. (2) We adopt a sparse training schedule that focuses optimization only on the 3D Gaussians within the co-visible areas of the cross-view frustums. This method not only sharpens the training focus but also reduces computational costs, as detailed in Alg. 1, and outputs rasterized \u00ce."}, {"title": "4.2. Modeling Transients with 2D Gaussians", "content": "Different from other methods that utilize semantic features, we introduce 2D Gaussians to model the transients as planar objects from a single view for each image. This approach is motivated by several key considerations: (1) 2DGS has been proven to be an efficient method for representing images,"}, {"title": "4.3. Multi-stage Training Scheme", "content": "Because 2D and 3D Gaussians are trained within the same framework, it is crucial to balance their relationship effectively. To that end, we have proposed a multi-stage training strategy that ensures stable convergence and enhances overall performance.\nWarm up Pre-training. The training process begins by using 3DGS to capture the essential structure of the entire static scene. This stage focuses on modeling the scene with a set of unified 3D Gaussians with our multi-view regulated supervision as detailed in Sec. 4.1, producing foundational, albeit low-quality, rendered images \u00ces using a combination of DSSIM [38] and L1 losses following:\n$L_{warmup}$ = $\u03bbL_{DSSIM}$($\u00ce_{s}$, I) + (1 \u2212 \u03bb)$L_{1}$($\u00ce_{s}$, I).\n(9)\nwhere the \u03bb is set to 0.2. During the warm-up, the densification of 3D Gaussians is conducted to gradually increase the number, allowing for a more detailed representation of the static scene structure.\nIterative Training. The second stage employs iterative training between 2DGS and 3DGS to progressively optimize both components. During the alternation, the 3DGS will provide more accurate static scene renderings, which in turn enables the 2DGS to refine its transient representations \u00cet and mask predictions Mt with Eq. 10. Conversely, the improved masks from 2DGS allow the 3DGS to focus more precisely on rendering the static regions of the scene with Eq. 11. The rendered image \u00ce is a composition of \u00ces and It with Mt following Eq. 7. This process iteratively refines the transients and statics, improving the overall quality and accuracy of the masks. The loss functions are as follows:\n$L_{iter2d}$ = \u03bb$L_{DSSIM}$($\u00ce$, I) + (1 \u2212 \u03bb)$L_{1}$($\u00ce$, I),\n(10)\n$L_{iter3d}$ = (1 - Mt) $L_{iter2d}$\n(11)\nNote that, during training, 2DGS and 3DGS are trained alternately. When training one branch, the gradient backpropagation of the other branch is turned off.\nJoint Fine-tuning. The final stage involves joint fine-tuning, integrating 2DGS and 3DGS into a unified framework. This stage focuses on fine-tuning on the transient mask and reducing the error between the rendered image \u00ce, and the ground truth image I. The loss $L_{joint}$ used during the joint fine-tuning is a combination as follows:\n$L_{joint}$ = \u03b2$L_{DSSIM}$($\u00ce$, I) + (1 \u2212 \u03b2)$L_{1}$ ($\u00ce$, I),\n(12)\nwhere the \u03b2 is set to 0.2. Here, both the 2DGS and 3DGS branches will enable gradient backpropagation to simultaneously update all parameters."}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nDatasets. Similar to previous methods, we evaluate our HybridGS on two challenging datasets: NeRF On-the-go [24] and RobustNeRF [26]. NeRF On-the-go contains multiple casually captured indoor and outdoor sequences with varying ratios of distractors (from 5% to over 30%). We use the version of the dataset where all images were undistorted as [14]. The RobustNeRF dataset includes several scenes exemplifying different types of distractors. To simulate capturing over extended periods, distractor objects are repositioned between frames to interfere with statically placed objects (from 1 (Statue) to 150 (Yoda)). The splits includes distractor-filled training set and distractor-free testing set.\nImplementation Details. Our HybridGS, developed on top of open-source implementation gsplat [43] and Taming-3DGS [28], incorporates custom CUDA kernels, boosting the rasterization of \u03b1-blending for 3DGS and weighted"}, {"title": "5.2. Evaluation", "content": "5.2.1 Comparison on the NeRF On-the-go Dataset\nAs shown in Tab. 1 and Fig. 3, our method demonstrates superior performance across different scenarios with varying levels of occlusion. In Tab. 1, our method outperforms the previous SOTA method across multiple metrics, with an average improvement of 1.10 dB in PSNR and 5.27% in SSIM, demonstrating the superiority of our approach. Furthermore, Fig. 3 provides a qualitative comparison, illustrating the visual fidelity of our results against other methods. Our method produces more clear and detailed images, closely resembling the ground truth, particularly in challenging scenes with complex geometry and occlusions. This highlights our model's capability to effectively handle diverse visual conditions both in indoor and outdoor scenes.\n5.2.2 Comparison on the RobustNeRF Dataset\nWe present the quantitative results of RobustNeRF in Tab. 2. Our method demonstrates superior performance quantitatively compared to all previous approaches. In scenes with numerous transients, such as the Yoda scene, we've noticed that the performance metrics of the original 3DGS significantly degrade, occasionally even dropping below those of NeRF-related methods. By leveraging multi-view shared information and incorporating 2D Gaussians to effectively represent transients, our approach surpasses semantic-based methods like SLS-mlp [27]. As shown in Fig. 4, our results have clearer overall boundaries and structured fine-grained details, while SLS-mlp [27] may lead to blurriness in static objects and occluded areas. This further confirms that, even when faced with numerous transients, our method consistently maintains stable and reliable performance."}, {"title": "5.3. Ablation Studies", "content": "Components. To assess the effectiveness of each component in our framework, we performed an ablation study, as presented in Tab. 3. We systematically removed iterative training, joint training, and multi-view supervision to evaluate their effectiveness. Each component, as expected, enhances overall performance, demonstrating their individual and combined significance. In scenes with low occlusion, multi-view supervision allows the model to concentrate on static elements by integrating data from various perspectives. Conversely, in highly occluded environments, joint training with 2D and 3D Gaussians markedly improves performance, highlighting the model's capability to manage complex scenes with significant occlusions and transient changes. Iterative training consistently boosts results across different occlusion levels. In summary, the results underscore that integrating diverse training strategies and multi-view information significantly enhances the rendering quality of unseen views, especially in challenging conditions.\nThe Number of 2D Gaussians. Tab. 4 presents an ablation study on the number of 2D Gaussians used in the corner sequence of NeRF On-the-go dataset. The results indicate that increasing the number of 2D Gaussians initially improves the performance. Specifically, 10k 2D Guassians achieve the highest PSNR (25.034) and SSIM (0.847), while maintaining a competitive LPIPS score of 0.151 and reasonable storage requirements. Based on these findings, we select 10k as the optimal number in our experiments.\nTransients and Masks. Benefiting from our method's explicit modeling of transient objects, we can obtain transient masks without introducing any segmentation networks or other pre-trained features. Typically, these masks capture dynamic elements such as pedestrians and vehicles, as well as some structural details ignored by 3D Gaussians. In Fig. 6, our approach effectively learns the RGB and mask of transients even under strong occlusions, while maintaining the robustness of static elements. This capability underscores the strength of our method in discerning and isolating dynamic components within complex scenes."}, {"title": "5.4. Discussions", "content": "While our method has demonstrated strong performance in distinguishing between transients and statics, it has not yet accounted for the illumination variation in unconstrained photo collections. As illustrated in Fig. 11 on Photo Tourism [32], our method renders fine-grained architectural structures, but the overall photometric result tends towards an average photometric effect, akin to overcast conditions. A promising direction is to incorporate an appearance embedding module in our method for the future work."}, {"title": "6. Conclusion", "content": "In this paper, we introduce HybridGS, a novel hybrid representation with 2D and 3D Gaussians for decomposing 3D scenes from casually captured images. By introducing the hybrid Gaussian representation, our method addresses the challenges of handling transients as planar objects while preserving the integrity of the static scene. The multi-view supervision mechanism we developed plays a crucial role in enhancing the discrimination between static and transient elements across multiple views, reducing artifacts and inaccuracies in novel views. We have developed a multi-stage training strategy for the joint training of 2D and 3D Gaussians, ensuring the stable and robust training stage. Our experiments demonstrate that HybridGS significantly outperforms existing methods in terms of quality and efficiency."}, {"title": "7. Positioning of Our Work", "content": "In the community, there are currently two predominant approaches for tackling the challenge of novel view synthesis in wild images with different complexities. We distinguish these approaches according to the primary datasets they utilize and provide a detailed comparison in Tab. 5.\nNeRF On-the-go [24] processes casually captured images that lack inter-frame continuity, with the goal of eliminating the interference from transient objects to reconstruct statics.\nPhoto Tourism [32] gathers photo collections from the web, resulting in completely unconstrained conditions with more complex lighting variations and increased foreground interference. It focuses more on integrating appearance embedding to model the photometric changes in the scene.\nIn summary, our method belongs to the first category and aims to decompose transients and statics from casually captured images in scenes with minimal illumination changes. Experiments have demonstrated our state-of-the-art results on two widely used benchmark datasets, such as NeRF On-the-go [24] and RobustNeRF [26]. Handling varying lighting conditions will be our future work as discussed in the paper."}, {"title": "8. More Discussions", "content": "8.1. 2D Gaussians\nThe fitting capability of 2D Gaussians is inherited from 3D Gaussians. Given J, the Jacobian of the affine projective transformation, and W, the viewing transformation, the 3D Gaussians can be projected to 2D image plane and blended through a fast, differentiable \u03b1-blending process to render 2D images following the Eq. 3 and Eq. 4. Therefore, the 2D Gaussians can be viewed as the projection of 3D Gaussians.\nDuring training, the warm-up allows 3DGS to establish an initial model of the entire scene. It is noteworthy that intuitively, the residuals between the results of 3DGS rendering and the ground-truth would potentially model transients. However, the 3DGS itself is constrained only by RGB loss, therefore, the transient objects from different viewpoints are eventually fitted into the 3D Gaussians, leading to less effective fitting of static scenes with vanilla 3DGS. We address this issue by incorporating additional 2D Gaussians. During the iterative training stage, the 2DGS learns the residuals per view, focusing more on the unique elements of each image. The output soft mask or matting can effectively direct 3DGS to concentrate on areas with smaller residu-"}, {"title": "8.2. Multi-view Supervision", "content": "We have further investigated the performance of multi-view 3DGS in different scenarios in Tab. 6. To be specific, we select the static scene Garden from the MipNeRF 360 [2] dataset and the dynamic scene Corner from the NeRF On-the-go [24] dataset. The results indicate that in static scenes, employing multi-view supervision has minimal impact on achieving the best results for novel view synthesis. However, during training, it is noted that 3DGS tends to over-fit the training views, leading to a gradual decline in both visual quality and metric performance for novel views. In contrast, multi-view 3DGS demonstrates more stable convergence and effectively reduces the over-fitting problem. In dynamic scenes, obviously, 3DGS is prone to over-fitting, which adversely affects performance in novel view synthesis. On the other hand, multi-view 3DGS benefits from mutual supervision in areas visible to multiple views, significantly improving the visual results."}, {"title": "9. More Implementation Details", "content": "9.1. Training\nFor the training of 3D Gaussians, we perform the densification of 3D Gaussians during the warm-up stage. Then,\nin the subsequent stages, we maintain a constant number"}, {"title": "9.2. Datasets", "content": "We follow the same training/testing split and resolution settings as the official rules in NeRF On-the-go [24] and RobustNeRF [26]. Specifically, for the NeRF On-the-go dataset, we downsample images from most scenes by 8\u00d7 to 504 \u00d7 378. Note that Arcdetriomphe and Patio are down-sampled by 4x to 480 \u00d7 270. For the RobustNeRF dataset, all scenes are downsampled by 8\u00d7, with Android and Statue resized to 503 \u00d7 377, and Crab and Yoda to 431 \u00d7 431."}, {"title": "9.3. Storage", "content": "Tab. 7 highlights the advantages of our method in terms of storage and computational efficiency. Compared to 3DGS,"}, {"title": "10.3. More Datasets", "content": "We apply our method on Photo Tourism [32] dataset, which consists of unconstrained photo collections with photometric variations. As shown in Fig. 11, we have some intriguing and reasonable observations. First, the statics generated using 3D Gaussians are rendered under an average light condition derived from the training images, similar to the diffuse lighting on an overcast day. Additionally, we discover that besides modeling dynamic objects, 2D Gaussians also capture photometric differences in our transients, since the illumination difference is indeed a per-image characteristic. This finding perfectly aligns with the perspective we presented in Sec. 8.1 that transients can capture unique aspects of each image, broadening the scope for future research to further isolate photometric information from 2D Gaussians."}, {"title": "10. More Visualization Results", "content": "10.1. Training Process\nTo better demonstrate the changes during our training process, we select IMG_7195.JPG of Corner from NeRF On-the-go dataset as example, visualizing the statics and transients during different training stages and comparing them with vanilla 3DGS in Fig. 8. As training iterations increase, 3DGS tends to gradually integrate transient elements into the static components, rendering the residuals being almost incapable of capturing transient contents. In contrast, our HybridGS effectively distinguishes transients from statics over time, leading to consistent improvements in Fig. 10.\n10.2. More Scenes\nIn addition to providing metrics and results on the 6 commonly used scenes of NeRF On-the-go [24] dataset, we also present the visualization results on the remaining scenes as shown in Fig. 9. These complex scenes include some variations in lighting and shadows. We find that in addition to removing dynamic objects, our statics can also eliminate elements lacking specific semantics, such as shadows of pedestrians (in Drone and Train Station) and cars (in Arcdetriomphe and Train). This separation of non-semantic transients illustrates that our method is fundamentally a versatile, low-level and semantics-free scene decomposition approach, effectively highlighting its generality and robustness."}]}