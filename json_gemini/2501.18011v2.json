{"title": "Anatomy Might Be All You Need: Forecasting What to Do During Surgery", "authors": ["Gary Sarwin", "Alessandro Carretta", "Victor Staartjes", "Matteo Zoli", "Diego Mazzatenta", "Luca Regli", "Carlo Serra", "Ender Konukoglu"], "abstract": "Surgical guidance can be delivered in various ways. In neurosurgery, spatial guidance and orientation are predominantly achieved through neuronavigation systems that reference pre-operative MRI scans. Recently, there has been growing interest in providing live guidance by analyzing video feeds from tools such as endoscopes. Existing approaches, including anatomy detection, orientation feedback, phase recognition, and visual question-answering, primarily focus on aiding surgeons in assessing the current surgical scene. This work aims to provide guidance on a finer scale, aiming to provide guidance by forecasting the trajectory of the surgical instrument, essentially addressing the question of what to do next. To address this task, we propose a model that not only leverages the historical locations of surgical instruments but also integrates anatomical features. Importantly, our work does not rely on explicit ground truth labels for instrument trajectories. Instead, the ground truth is generated by a detection model trained to detect both anatomical structures and instruments within surgical videos of a comprehensive dataset containing pituitary surgery videos. By analyzing the interaction between anatomy and instrument movements in these videos and forecasting future instrument movements, we show that anatomical features are a valuable asset in addressing this challenging task. To the best of our knowledge, this work is the first attempt to address this task for manually operated surgeries.", "sections": [{"title": "1 Introduction", "content": "Surgical guidance is one of the hallmark problems in computed-aided interventions. The main goal is simply to build algorithms to help the surgeon during the surgery. This guidance can come in multiple forms, in neurosurgery the major ones being neuronavigation and surgical action or phase recognition [2, 3, 6]. While the former is crucial for helping surgeons orient themselves in the anatomy, the latter can help the organization around the surgery as well as the training of new surgeons. Both of these guidance forms have seen tremendous advances over the last years. In this work, we explore and assess the possibility of a new form of guidance, forecasting surgical tool movement. While this and autonomous surgery have been explored for surgical robots where kinematics are available [4,8,9,13], to the best of our knowledge, this is the first work addressing this task in manually operated surgeries.\nThis forecasting problem can be coarsely defined as predicting the next movement of the surgical tool. The first step towards this direction is simply predicting the next location of the surgical tool. In the setting of surgical videos or video streams, this corresponds to predicting the location of the surgical tool in the next video frames given the previous ones. This is a narrower subproblem, as well as the first step, of the broader problem of predicting the surgeon's next action. It can also be viewed as a narrower version of another broader problem, forecasting frames in a video.\nThe applications of forecasting surgical tool movement can be multiple. First, it is a natural next step for surgical guidance. It goes one step further than providing information for self-orientation, it provides guidance towards how to move the surgical tool in a given scenario. A successful forecasting model trained with expert surgeons' actions can guide surgeons less experienced with a particular surgery by suggesting a surgical tool's next movements. Such forecasting can also be an integral component of a fully autonomous surgical robot.\nRecognition of surgical tools in surgical videos as well as recognition of surgical actions being performed in a set of observed frames have received ample attention from the research community. Advanced tools of today can make accurate predictions for both problems [1,7].\nForecasting, however, is arguably a much more challenging problem than these two. This is akin to the more challenging nature of the next frame prediction problem compared to parsing a given scene or action recognition in a given video sequence. Forecasting the movement of a surgical tool requires a full understanding of the scene as well as the past movements of the tool. Until recently, a full understanding of a surgical scene was beyond reach. Advances in this problem, especially through the recent works in structure recognition and detection [10,11], have opened up new opportunities towards tackling this challenging forecasting problem. Here, we build upon these recent advances and, to the best of our knowledge, propose one of the first attempts to solving the forecasting problem.\nIn this work, we present a deep learning model to forecast the location of a surgical tool in the next frames given previous frames of a surgical video. The model uses a very simple neural network architecture to predict the location of a surgical tool in the next 8 or 16 frames given the previous 64 frames. We evaluate the forecasting accuracy using different levels of information on the scene, including the location of the surgical tool, and locations of anatomical structures as predicted by a detector (YOLO [12]) trained on surgical videos. Experimental results suggest that the value of the anatomical structure detections may be the key towards solving the forecasting problem. This work is a first step and we hope it will pave the way towards accurate forecasting models and ultimately accurate guidance on the next surgical action."}, {"title": "2 Methods", "content": "Let $S_t$ denote a sequence of endoscopic video frames $x_{t-s:t}$, where $s$ is the sequence length, and $x_t \\in \\mathbb{R}^{w \\times h \\times c}$ is the t-th frame with width $w$, height $h$, and number of channels $c$. Our primary goal is to predict the future changes in the locations of the surgical instrument bounding boxes for the future $f$ frames from t to t + f, denoted as $\\Delta b_{t+1:t+f}^I = [\\Delta b_{t+1}^I,..., \\Delta b_{t+f}^I]^T \\in \\mathbb{R}^{f \\times 4}$, where $I$ denotes the instrument and bounding boxes are represented with 4 values. Our approach involves identifying the anatomical structures and the surgical instrument in the sequence $S_t$. To achieve this, object detection is performed on all frames $x_{t-s:t}$ in $S_t$, resulting in a sequence of detections $c_{t-s:t}$ denoted as $C_t$. A detection $c_t \\in \\mathbb{R}^{n \\times 5}$ includes binary variables $y_t = [y_t^1,...,y_t^n] \\in \\{0,1\\}^n$ indicating the presence of structures in the t-th frame, where n is the number of structures, and bounding box coordinates $b_t = [b_t^1,..., b_t^n]^T \\in \\mathbb{R}^{n \\times 4}$. $C_t$ is then used to predict the future changes of the instrument bounding boxes $\\Delta b_{t+1:t+f}^I$. An overview of the proposed pipeline is visualized in Figure 1."}, {"title": "2.2 Object Detection", "content": "To identify and locate anatomical structures, we utilize the YOLOv7 network in the object detection phase of our pipeline [12]. The detection network is trained on endoscopic videos from a training set, where frames are sparsely labeled with bounding boxes. Subsequently, the trained network is applied to all the frames of an incoming sequence and the bounding box detections are performed and form $C_t$. Then $C_t$ is passed as input to the forecasting network, which predicts the future instrument bounding box location and is described next.\nThe bounding boxes are parameterized by four values: the center coordinates (x,y) and the width and height (w, h). Each value is normalized relative to the image dimensions."}, {"title": "2.3 Future Instrument Location Prediction", "content": "To learn the future bounding box locations, a forecasting model takes as input $C_t$ and predicts $\\Delta b_{t+1:t+f}^I$. This model's parameters are updated to ensure that the predicted future changes in surgical instrument bounding boxes $\\Delta b_{t+1:t+f}^I$ fit the ground truth changes $\\Delta b_{t+1:t+f}^I$ in a training set. This leads to the objective to minimize for the t-th frame in the m-th training video:\n$L_{m,t} = \\sum_{r=t+1}^{t+f} |\\Delta b_{m,r}^I - \\Delta \\hat{b}_{m,r}^I | + \\lambda  ( \\frac{ <\\bar{v}_{p,m,t:t+f}, \\bar{v}_{g,m,t:t+f}> }{ ||\\bar{v}_{p,m,t:t+f}||||\\bar{v}_{g,m,t:t+f} ||} )$\nwhere:\n$\\Delta b_{m,r}^I = b_{m,r}^I - b_{m,r-1}^I$ represents the ground truth change in bounding box values between frame r - 1 and r for the instrument I,\n$\\Delta \\hat{b}_{m,r}^I = \\hat{b}_{m,r}^I - \\hat{b}_{m,r-1}^I$ represents the predicted change in bounding box values between frame r - 1 and r for the instrument I,\n$\\bar{v}_{p,m,t:t+f}$ and $\\bar{v}_{g,m,t:t+f}$ are the average direction vectors for the predicted and ground truth changes in x and y coordinates of the instrument bounding box over the sequence of frames t +1:t+f,\n$<(,)>$ denotes the dot product,\n$|| ||$ represents the vector norm,\n$\\lambda$ is a weighting factor that balances the contribution of the direction loss,\n$||$ denotes the $L_1$-loss.\nThe first term computes the $L_1$ difference between the predicted and ground truth frame-to-frame changes in bounding box values, ensuring that the predicted changes align with the ground truth. The second term introduces a direction loss that penalizes deviations in the alignment of the predicted and ground truth trajectories by minimizing the cosine similarity between their average direction vectors. The weighting factor $\\lambda$ controls the relative importance of the direction loss. The total training loss is then defined as the sum of $L_{m,t}$ over all frames t and training videos m.\nIn order to train the forecasting network, we use a set of training videos separate from the videos used to train the detection network. This set is first passed through the detection network to extract bounding box predictions $C_t$. The resulting detections constitute the inputs of the training samples for the forecasting network. The labels for these videos are extracted automatically using the detection network as well. For each sequence $S_{m,t}$, the corresponding surgical instrument locations in the following f frames, i.e., $b_{m,t+1:t+f}^I$, are determined using the detection network on the corresponding frames, i.e., detection network applied on $x_{t+1:t+f}$. Note that these detections are only used to generate the labels for training the forecasting network. At inference time, the model does not see the frames $x_{t+1:t+f}$ while predicting $\\Delta b_{m,r}^I$."}, {"title": "3 Experiments and Results", "content": "The medical dataset used for consists of 169 videos, each recorded during a pituitary surgery (transsphenoidal adenomectomy) from a unique patient. These videos, collected over a 10-year period using various endoscopes and sourced from multiple centers, were made available under general research consent. Expert neurosurgeons provided annotations, identifying 16 classes in total: 15 representing distinct anatomical structures and one representing surgical instruments.\nIn total, the dataset includes approximately 19,000 labeled frames. Generally, there is one instance of each anatomical class per video, while multiple different instruments are grouped into a single instrument class. Of the 169 videos, 77 were allocated for training and validation of the object detection model, 75 were used for training and validation of the model for surgical action forecasting, and the remaining 17 for testing. Although data originates from multiple sites, potential biases may still arise due to the proximity of the sites."}, {"title": "3.2 Implementation Details", "content": "The YOLO network was trained with identical parameters and implementation as reported in [10,12].\nThe forecasting model integrates a transformer encoder comprising six transformer encoder layers, each with five attention heads. The input to the transformer encoder has a size of s \u00d7 (n + 1) \u00d7 5, where s represents the sequence length, set to 64 frames, and n + 1 represents the 15 anatomical classes plus the additional surgical instrument class. Sinusoidal positional encodings are applied to these inputs to retain the temporal dimension of the sequence, and the transformer encoder output is fed through a series of three fully connected layers with output dimensions of 512, 256, and 128, respectively, using ReLU activation functions between the layers. The final fully connected layer outputs a latent representation $z_t \\in \\mathbb{R}^{16}$ with 16 dimensions.\nThe decoder is implemented as a single linear layer, designed to take the latent vector $z_t$ as input. This vector is processed through the layer to reach the final output of predicted sequence length \u00d7 4, where each predicted vector describes the bounding box changes.\nThe output consists of frame-to-frame changes in bounding box coordinates, rather than absolute bounding box positions, to enable the model to effectively predict motion dynamics over the sequence. The model also incorporates cosine similarity-based direction loss to ensure directional consistency in predicted trajectories.\nFor training, the AdamW optimizer [5] is employed alongside a warm-up scheduler, which linearly increases the learning rate from 0 to 1 \u00d7 10-4 over 60 warm-up epochs, and $\\lambda$ was set to 0.5. The model is trained for a total of 75 epochs, and 150 for the task of predicting the instrument trajectory in the next 8 and 16 frames, respectively."}, {"title": "3.3 Results", "content": "Evaluation metric: Considering the challenging nature of this task, we assess the forecasting performance through a classification metric. More specifically, we classify the predicted direction of the instrument movement into four principal directions: up, down, left, and right. The classification is based on the angular range $\\theta$ of the movement, defined in degrees as follows:\nHere, $\\theta$ represents the predicted angle of movement measured in degrees.\nAccounting for noise in ground truth movement: The ground truth motion of the surgical tools was estimated using their bounding box locations predicted with the detection network using the frames as input. These predictions contained jitter noise and this induces noise in the ground truth movements for the surgical tools. While this noise is desirable for training, since it adds to the robustness of the tool, it is detrimental to an accurate evaluation of the performance. We therefore additionally evaluated the performance of the tool for movements larger than a certain threshold, which are unlikely due to the jitter of the predictions of the detection network. To filter out noisy movements, we test movements of which the magnitude is greater than 0.1 and 0.05, which when considering a horizontal movement and an image size of 1920 \u00d7 1280, corresponds to 192 and 96 pixels respectively. The filtering resulted in around 40000 and 77000 test samples for the case of predicting 8 frames, respectively. These samples were then also used to generate the results of the models that predict 16 frames."}, {"title": "4 Conclusion", "content": "In this work, we have proposed a novel approach to forecasting surgical tool movements using endoscopic videos, leveraging anatomical structure detection and a transformer-based forecasting model. Our results demonstrate that incorporating anatomical context significantly improves the predictive performance of surgical tool trajectory forecasting.\nThis study represents a foundational exploration of forecasting surgical tool movements. The ability to anticipate tool movements offers a range of potential applications, from providing real-time decision support for surgeons to facilitating the development of surgical robots. Moreover, this forecasting framework could serve as a critical component in surgical training, offering insights into expert movement patterns and enabling feedback for trainees."}, {"title": "4.1 Limitations and Future Work", "content": "While promising, there are various limitations to this method. As of now, the model treats all surgical instruments as identical which hinders the understanding of surgical actions as each instrument has a specific purpose. Additionally, the framework only predicts the actions of one surgical instrument, which can create confusion for the model as soon as there are two instruments in view. Therefore, we suspect improvements if the instruments are classified and the model can handle multiple instruments as input. Additionally, we expect further improvements by incorporating an autoregressive architecture and by analyzing visual features with the forecasting model to enable more accurate predictions. These enhancements are part of our planned future work."}]}