{"title": "ARCHCODE: Incorporating Software Requirements in Code Generation with Large Language Models", "authors": ["Hojae Han", "Jaejin Kim", "Jaeseok Yoo", "Youngwon Lee", "Seung-won Hwang"], "abstract": "This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e. achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce ARCHCODE, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. ARCHCODE generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmarks show that ARCHCODE enhances to satisfy functional requirements, significantly improving Pass@k scores. Furthermore, we introduce HumanEval-NFR, the first evaluation of LLMs' non-functional requirements in code generation, demonstrating ARCHCODE's superiority over baseline methods. The implementation of ARCHCODE and the HumanEval-NFR benchmark are both publicly accessible.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have significantly improved code generation capabilities (Chen et al., 2021; Li et al., 2022; OpenAI, 2023). Although the primary goal for LLMs in this domain is to generate functionally correct code based on textual descriptions (Hendrycks et al., 2021; Austin et al., 2021; Chen et al., 2021; Li et al., 2022), real-world software development encompasses more than just functionality.\nIn software engineering, software requirements provide a detailed framework describing what a software system is intended to achieve (Chung et al., 2012), divided into two categories (Glinz, 2007):\n\u2022 Functional Requirements (FRs) dictate the behavior and functionality, e.g., input/output conditions, desired behavior of code, etc.\n\u2022 Non-Functional Requirements (NFRs) are attributes or constraints beyond functionality, e.g., time and space performance, robustness, maintainability, reliability, etc.\nDespite the critical role of software requirements, considering these criteria has not been studied actively in previous code generation works, merely"}, {"title": "2 Related Work", "content": "Despite the fact that LLMs recently have shown impressive capabilities in code generation, the majority of evaluations have focused solely on functional requirements (FRs; Hendrycks et al., 2021; Austin et al., 2021; Chen et al., 2021; Li et al., 2022).\nSolely Targeting Functional Requirements Early research such as Feng et al. (2020), Chen et al. (2021), Brown et al. (2020), and Li et al. (2022) directly generates code from natural language descriptions, which may not fully capture all software requirements due to their vagueness or imperfections. Later studies (Jiang et al., 2023; Li et al., 2023; Zhang et al., 2023a) have targeted to better capture functional requirements by generating code-like outlines via in-context learning (ICL; Kojima et al., 2022; Shao et al., 2023; Zhang et al., 2023c). More recent methods enhance FR satisfaction through self-verification of the generated code: On one hand, code filtering utilizes \u2018over-generate-then-filter' strategies, where the filtering can be achieved either by predicting functional correctness without code execution (Inala et al., 2022; Ni et al., 2023; Zhang et al., 2023b), or execution with given (Shi et al., 2022b) or generated test cases (Li\net al., 2022; Chen et al., 2023; Huang et al., 2023). On the other hand, code refinement iteratively reflects and refines the generated code to improve its functionality via execution results of current version of code with generated test cases (Shinn et al., 2023).\nTargeting Narrow Scope of Non-Functional Requirements While much research covers FRs, few studies have addressed specific attributes of non-functional requirements (NFRs) such as reliability and robustness (Deng et al., 2023; Xia et al., 2023), or time/space performance (Madaan et al., 2023; Luo et al., 2023).\nOur Distinction Table 1 summarizes the distinction of our framework compared with existing code generation approaches. To the best of our knowledge, ARCHCODE is the first study that employs ICL to systematically extract and interpret software requirements from descriptions, ensuring the generated code and test cases closely aligns with these requirements. In addition, we introduce HumanEval-NFR, a variant version of the HumanEval (Chen et al., 2021) benchmark that can assess the fulfillment of NFRS."}, {"title": "3 The ARCHCODE Framework", "content": "We propose ARCHCODE, a novel code generation framework that employs In-Context Learning (ICL) to LLMs, incorporating software requirements in code generation. As shown in Figure 3, ARCHCODE delineates software requirements from textual descriptions, generates code, then verifies it using custom test cases.\nFormally, given a problem space P and a code space C, the code generation task is to build a function $F : P \\rightarrow C$ that maps each textual problem description $p \\in P$ into its corresponding code implementation $c \\in C$. ARCHCODE decomposes F by $F = g \\circ f$. $g : P \\rightarrow P \\times R$ maps problems to problem-requirements pairs, and $f : P \\times R \\rightarrow C$ generates code from the problem-requirements pairs, where R is a space of software requirements. The test case generation function $H : P \\rightarrow T$ is also decomposed by ARCHCODE into $H = g \\circ h$, where T is the space of test cases and $h:PxR\\rightarrow T$.\n3.1 Delineating Software Requirements\nARCHCODE leverages ICL to let an LLM generate a set of software requirements, either reformulated from a given textual description, or extrapolated from the description by the LLM's learnt parametric knowledge. Formally, given in-context examples of description-requirements pairs and the target description p, the LLM returns the list of software requirements\n$r = g([p_1, r_1]; [p_2, r_2]; ...; [p]), (1)$\nwhere p and r = $[r_1^1, r_1^2, ...]$ is the description and its corresponding requirement list of i-th example pair, each $r_i^j$ in r is a requirement, and \u00eer is the list of generated requirements.\nSpecifically, based on the established classifications by Glinz (2007), we further break down FRs and NFRs into distinct categories that our study focuses on.\nTarget FRs Our approach narrows down FRs into three subtypes:\n\u2022 Input/Output Conditions: Analogous to the preconditions and the postconditions in Design by Contract (Meyer, 1992), these define the desired functionality of the code by specifying valid inputs and expected outputs.\n\u2022 Expected Behavior: Along with Input/Output Conditions, it explains the functionality of the target code by reformulating the description into a series of operations for valid inputs that are applicable in most general scenarios.\n\u2022 Edge Cases: While this term generally comprises an array of corner cases, we restrict the scope to only consider valid inputs that necessitate distinct treatment. These include, for example, processing an empty list when the"}, {"title": "Target NFRS", "content": "ARCHCODE considers NFRs that are both pivotal in real-world applications and feasible for assessment either through code execution or using existing metrics.\n\u2022 Time Performance: Pertains to time-centric aspects like algorithmic time complexity or stipulated timeout conditions.\n\u2022 Robustness: Ensures that code is resilient to invalid inputs (McConnell, 2004). For instance, a function designed for integer addition must prevent unforeseen or undesirable outcomes from the '+' operation, like mistakenly returning the concatenated string when given two strings.\n\u2022 Maintainability: Considers factors that contribute to the ease of maintenance, such as reducing code complexity via code modularization (Magel et al., 1982), measured by cyclomatic complexity (McCabe, 1976).\n\u2022 Reliability: Ensures that the code can handle errors gracefully, without causing system failures, thereby increasing the mean time between failures (McConnell, 2004)."}, {"title": "3.2 Requirements-aware Generation", "content": "Upon obtaining software requirements r, ARCHCODE conditions r with the given description p to generate code samples and test cases. Specifically, ARCHCODE generates code \u0109 and test cases t in a parallel manner:\n$\\hat{c} = f ([p_1, r_1, c_1]; ...; [p, r])$, \n$\\hat{t} = h([p_1, r_1, t_1]; ...; [p, r]), (2)$\nwhere c and t = [1, 2, ...] are the code and the list of test cases of i-th example, and each ti in t is a test case corresponding to r in r. We choose this parallel generation due to the potential pitfalls when these processes condition each other. We further discuss such pitfalls in Section 5.2."}, {"title": "3.3 Pursuing Requirements Satisfaction", "content": "To ensure the conformance of the generated code snippet \u0109 with the specified requirements r, ARCHCODE executes \u0109 against the generated test cases t tailored to one of the requirements in r:\n$S \\leftarrow EXEC (\\hat{c}, t), (3)$\nwhere s\u2208 {0,1} is a binary result from a code execution function EXEC, and t is one of the generated test cases in t, matching \u00ee in r. To return the satisfactory code towards r, ARCHCODE conducts code filtering. To rank each code in relation to r, our framework calculates a weighted sum of the scores s from each t, with the option to assign higher weights to preferred requirements. Adjusting those weights to tailor the scoring process is discussed in more detail in Section 5.3."}, {"title": "4 Experiments", "content": "We evaluate ARCHCODE's effectiveness using three benchmarks, categorized into two types: 1) A novel benchmark for assessing both FR and NFR satisfaction; 2) Two public benchmarks aimed at FR evaluation, facilitating comparison of ARCHCODE with existing baselines. For the former, we introduce HumanEval-NFR for comprehensive NFR assessment, overcoming the conventional focus on FR alone. For the latter, we explore two code modalities: 1) function-level and 2) competition-level code generation.\n4.1 Experimental Setup\nWe evaluate the effectiveness of ARCHCODE on code generation with LLMs. Throughout the experiments, we used GPT-3.5-Turbo-16k (OpenAI, 2022) as the backbone LLMs for generating code, software requirements, test cases, etc. More details can be found in Appendix A.\nEvaluation Metrics We mainly consider the widely used $Pass@k := \\mathbb{E}_{Problems} [\\mathbb{1}(\\exists c^* \\in \\{c_{i=1}^k\\}, c^* \\text{pass})]$ (Chen\net al., 2021) metric for evaluation, which is the unbiased estimator of the probability that the code generation system would have passed a problem if it were given k chances to sample c correct code snippets among n samples. Adhering to Chen et al. (2023), when applying code filtering, we denote the existence of passed code among the k filtered samples."}, {"title": "4.2 HumanEval-NFR: Embracing NFR Evaluation", "content": "We introduce HumanEval-NFR benchmark, which is specifically designed to assess NFR satisfaction. It is an extension of HumanEval that additionally covers four NFR categories, chosen for their suit-"}, {"title": "4.3 HumanEval and CodeContests: Public Benchmarks for FR Evaluation", "content": "We additionally report results on two popular code generation benchmarks targetting functional correctness. HumanEval (Chen et al., 2021) is a hand-crafted test benchmark with 164 programming problems along with public and hidden test cases. CodeContests (Li et al., 2022) consists of 13k/113/165 instances of train/valid/test data collected from multiple code competition websites. While HumanEval tests the model's capability to implement rather simpler functions without errors, the competitive programming oriented nature of CodeContests often requires more complex form of reasoning such as algorithmic reasoning. Each of these addresses different aspect of industrial software: the former is related to solving each of the simpler tasks composing larger and complex projects while the latter focuses on the logical and algorithmic perspective of software development.\nIn Table 3, ARCHCODE consistently outperforms the baseline methods. Specifically, on both benchmarks, ARCHCODE leveraging GPT-3.5-Turbo, exceeds GPT-4's performance by a substantial margin of 4.81%p and 10.45%p in terms of Pass@1. In comparison with WizardCoder 34B-a baseline that partially incorporates NFR considerations during the finetuning phaseARCHCODE, which covers NFRs more comprehensively, achieves significantly higher performance. In CodeContests, while our custom GPT-3.5-Turbo + CoT prompting baseline is outdone by the stateof-the-art CoT methods BRAINSTORM and ALGO, the application of ARCHCODE outperforms both approaches, setting new state-of-the-art of Pass@1. We also compare ARCHCODE with MPSC, a very recent baseline. Notably, ARCHCODE surpasses MPSC in all Pass@k metrics on HumanEval and Pass@1 on CodeContests, while ARCHCODE is much more cost-efficient. We provide further discussion on computational costs in Section 5.1."}, {"title": "5 Analysis and Discussion", "content": "5.1 Efficiency and Effectiveness of Requirement-aware Test Case Generation\nEfficiency In code filtering, a crucial step involves minimizing the number of generated test cases to reduce computational and time costs for code execution. As shown in Figure 4, existing approaches such as MPSC and CODET requires to generate hundreds of test cases for performance."}, {"title": "5.2 Conditioning Code Generation on Test Cases", "content": "In contrast to our approach of generating code and test cases in parallel and then applying subsequent postprocess mechanisms such as filtering, one can also consider conditioning the code generation on test cases , taking inspiration from the TestDriven Development (TDD; Beck, 2022) methodology. Table 7 shows results consistent with those reported in Chen et al. (2023), indicating marginal improvement in performance is observed when conditioning code generation on the ground-truth and generated test cases, while incorporating software requirements through ARCHCODE effectively boosts the score, even without code filtering.\nThis suggests the overhead from introducing new sequential dependency in the generation process might not be worth the additional costs incurred."}, {"title": "5.3 Preference over Requirements", "content": "As mentioned before, ARCHCODE can be informed of any user preferences over the software requirements at code filtering time\u2014after several code candidates have been generated and awaiting to be ranked. Figure 5 presents the Pass@1 scores"}, {"title": "5.4 ARCHCODE under Diverse Settings", "content": "Here we provide empirical results suggesting that ARCHCODE generalizes well to other models, datasets, etc. than those considered in the main experiments.\nOpen-source LLMs First, we showcase ARCHCODE combined with a relatively smaller model, namely WizardCoder 7B. Table 8 indicates that applying ARCHCODE with the said backbone model leads to a notable 15.67%p improvement in Pass@1 on HumanEval, while incorporating in-context learning directly into WizardCoder 7B itself has negative impacts. Note that this observation is consistent with prior findings such as that in Yuan et al. (2023), that instruction tuning might compromise in-context learning capabilities of LLMs; WizardCoder 7B is an instruction-tuned model based on CODELLAMA 7B.\nMeanwhile, in practical settings, diverse LLMS offer complementary benefits in terms of costperformance trade-off, and thus mixing two models has been a conventional aproach to explore costperformance design space (Sun et al., 2023; Wang et al., 2023). ARCHCODEMIX shown in Tables 8 and 9 similarly capitalizes on this space by directing most of the generation calls to affordable LLMs, while selectively delegating the part requiring the most of the reasoning capabilities to stronger ones.\nOther Programming Languages We also extend the evaluation of ARCHCODE to the task of Java code generation, using the MultiPL-E (Cassano et al., 2022) benchmark and the backbone model SantaCoder 1B (Allal et al., 2023). To address the rather limited capacity of a smaller model, we further applied sparse fine-tuning (Ansell et al. (2022); SFT) on a public Java train set. We provide more details in Appendix A.1. The results in Table 9 demonstrate the effectiveness of the proposed"}, {"title": "6 Conclusion", "content": "We proposed ARCHCODE, a framework incorporating software requirements from textual descriptions for LLM-based code generation. This systematic approach not only identifies these requirements but also harnesses them to guide the code generation process. The verification of code snippets with the generated test cases tailored to each requirement provides a robust validation layer for the alignment with detected requirements. On HumanEval and CodeContests, ARCHCODE with GPT-3.5-Turbo exceeded GPT-4's performance by 4.81%p and 10.45%p of Pass@1. ARCHCODE requires 50x less generated test cases compared to MPSC and CODET, while outperforming them. In addition, we introduced a new benchmark named HumanEvalNFR for evaluating how well LLMs can pursue nonfunctional requirements in code generation task. Further analysis shows the pertinence of parallel generation of code and test case, and the efficiency and the effectiveness of ARCHCODE's requirementaware test case generation."}, {"title": "Limitations", "content": "ARCHCODE leverages in-context learning as a tool to integrate both functional and non-functional requirements in the processes of code and test case generation. We did not studied prompt engineering and devising more sophisticated in-context examples which is beyond the scope of this work.\nARCHCODE encompassed three functional and four non-functional requirements, aligning with the established taxonomy within software engineering literature (Glinz, 2007). However, the potential for future work lies in addressing more complex and varied requirements involving larger pieces of code, as well as accommodating changes in software requirements over time.\nLastly, as ARCHCODE relies on generated requirements to guide subsequent code and test case generation process, although qualitative analysis suggests its impact could be limited in practice, additional measures to mitigate cascading errors via human intervention or self-correction by LLMs, etc. (Shinn et al., 2023; Wang et al., 2023; Yao et al., 2023; Chen et al., 2024) can be necessitated."}, {"title": "Ethical and Social Implications", "content": "ARCHCODE leverages LLMs to automatically generate software requirements, code, and test cases, thereby enhancing productivity and reducing manual labor for developers. However, to maximize these advantages while addressing potential risks, such as the creation of code with safety or security vulnerabilities as discussed in Chen et al. (2021), careful consideration is essential. Strategies to mitigate these risks include establishing default requirements for desired outcomes, delineating the permissible scope of generated code, and ensuring that the code remains within its authorized boundaries."}]}