{"title": "ARCHCODE: Incorporating Software Requirements in Code Generation with Large Language Models", "authors": ["Hojae Han", "Jaejin Kim", "Jaeseok Yoo", "Youngwon Lee", "Seung-won Hwang"], "abstract": "This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e. achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce ARCHCODE, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. ARCHCODE generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmarks show that ARCHCODE enhances to satisfy functional requirements, significantly improving Pass@k scores. Furthermore, we introduce HumanEval-NFR, the first evaluation of LLMs' non-functional requirements in code generation, demonstrating ARCHCODE's superiority over baseline methods. The implementation of ARCHCODE and the HumanEval-NFR benchmark are both publicly accessible.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have significantly improved code generation capabilities (Chen et al., 2021; Li et al., 2022; OpenAI, 2023). Although the primary goal for LLMs in this domain is to generate functionally correct code based on textual descriptions (Hendrycks et al., 2021; Austin et al., 2021; Chen et al., 2021; Li et al., 2022), real-world software development encompasses more than just functionality.\nIn software engineering, software requirements provide a detailed framework describing what a software system is intended to achieve (Chung et al., 2012), divided into two categories (Glinz, 2007):\n\u2022 Functional Requirements (FRs) dictate the behavior and functionality, e.g., input/output conditions, desired behavior of code, etc.\n\u2022 Non-Functional Requirements (NFRs) are attributes or constraints beyond functionality, e.g., time and space performance, robustness, maintainability, reliability, etc.\nDespite the critical role of software requirements, considering these criteria has not been studied actively in previous code generation works, merely generating code directly from textual descriptions. However, textual descriptions might express requirements verbosely or even omit them. As illustrated in Figure la and 2 (upper right), this may result in code that neglects many desirable requirements. Code filtering based on generated test cases (Li et al., 2022; Chen et al., 2023; Huang et al., 2023) shares the same problem, as test cases often fail to cover a broader range of requirements. Consequently, the generated code might exhibit unexpected behaviors for valid inputs, ignoring FRs. Similarly, overlooking NFRs can result in time/space inefficiencies, potential system failures, or challenges in maintenance. Nevertheless, achieving conciseness in the textual descriptions of software requirements necessitates significant human effort (Perry and Wolf, 1992; Bass et al., 2003).\nWe introduce ARCHCODE, a novel framework that automatically incorporates software requirements from textual descriptions, then directs LLMs to align code and test case generation with those requirements, as illustrated in Figure 1b. Specifically, ARCHCODE leverages In-Context Learning (ICL; Kojima et al., 2022; Shao et al., 2023; Zhang et al., 2023c) for adaptability, utilizing LLMs' extensive reasoning abilities to learn within context, thereby avoiding costly parameter updates. For code generation, each in-context example comprises a triplet a textual description, a list of software requirements (including both those expressed and unexpressed in the description), and corresponding code that satisfies all these requirements. For test case generation, we simply switch from code to test cases, each of which verifies a specific requirement. ARCHCODE prepends in-context examples to test descriptions, guiding LLMs to: 1) reformulate explicit requirements in descriptions, 2) deduce implicit requirements from their parametric knowledge, 3) generate code that fulfills these requirements, and 4) produce test cases for verifying each requirement, as shown in Figure 2.\nWe integrate ARCHCODE with Wizard-Coder (Luo et al., 2023) and GPT-3.5-Turbo (OpenAI, 2022), and assess the performance on HumanEval (Chen et al., 2021) and CodeContests (Li et al., 2022). The results confirm that ARCHCODE notably outperforms existing techniques in terms of the satisfaction of FRs-surpassing GPT-4's Pass@1 score on both benchmarks and achieving new state-of-the-art on CodeContests. Moreover, we introduce HumanEval-NFR based on HumanEval, the first benchmark to evaluate NFRs alongside FRs, to confirm that ARCHCODE is also effective in pursuing NFRs.\nOur main contributions are as follows:\n\u2022 We propose ARCHCODE, a novel framework that leverages ICL to incorporate software requirements in code generation.\n\u2022 ARCHCODE with GPT-3.5-Turbo surpasses GPT-4's Pass@1 scores on both HumanEval and CodeContests by 4.81%p and 10.45%p, while requiring 50\u00d7 smaller number of test cases to be generated compared to existing methods.\n\u2022 We introduce HumanEval-NFR, the first code generation benchmark for NFR evaluation to confirm the effectiveness of ARCHCODE for NFR satisfaction."}, {"title": "2 Related Work", "content": "Despite the fact that LLMs recently have shown impressive capabilities in code generation, the majority of evaluations have focused solely on functional requirements (FRs; Hendrycks et al., 2021; Austin et al., 2021; Chen et al., 2021; Li et al., 2022).\nEarly research such as Feng et al. (2020), Chen et al. (2021), Brown et al. (2020), and Li et al. (2022) directly generates code from natural language descriptions, which may not fully capture all software requirements due to their vagueness or imperfections. Later studies (Jiang et al., 2023; Li et al., 2023; Zhang et al., 2023a) have targeted to better capture functional requirements by generating code-like outlines via in-context learning (ICL; Kojima et al., 2022; Shao et al., 2023; Zhang et al., 2023c). More recent methods enhance FR satisfaction through self-verification of the generated code: On one hand, code filtering utilizes \u2018over-generate-then-filter' strategies, where the filtering can be achieved either by predicting functional correctness without code execution (Inala et al., 2022; Ni et al., 2023; Zhang et al., 2023b), or execution with given (Shi et al., 2022b) or generated test cases (Li et al., 2022; Chen et al., 2023; Huang et al., 2023). On the other hand, code refinement iteratively reflects and refines the generated code to improve its functionality via execution results of current version of code with generated test cases (Shinn et al., 2023).\nWhile much research covers FRs, few studies have addressed specific attributes of non-functional requirements (NFRs) such as reliability and robustness (Deng et al., 2023; Xia et al., 2023), or time/space performance (Madaan et al., 2023; Luo et al., 2023).\nTable 1 summarizes the distinction of our framework compared with existing code generation approaches. To the best of our knowledge, ARCHCODE is the first study that employs ICL to systematically extract and interpret software requirements from descriptions, ensuring the generated code and test cases closely aligns with these requirements. In addition, we introduce HumanEval-NFR, a variant version of the HumanEval (Chen et al., 2021) benchmark that can assess the fulfillment of NFRS."}, {"title": "3 The ARCHCODE Framework", "content": "We propose ARCHCODE, a novel code generation framework that employs In-Context Learning (ICL) to LLMs, incorporating software requirements in code generation. As shown in Figure 3, ARCHCODE delineates software requirements from textual descriptions, generates code, then verifies it using custom test cases.\nFormally, given a problem space P and a code space C, the code generation task is to build a function \\(F : P \\rightarrow C\\) that maps each textual problem description \\(p \\in P\\) into its corresponding code implementation \\(c \\in C\\). ARCHCODE decomposes \\(F\\) by \\(F = g \\circ f\\). \\(g: P \\rightarrow P \\times R\\) maps problems to problem-requirements pairs, and \\(f : P \\times R \\rightarrow C\\) generates code from the problem-requirements pairs, where R is a space of software requirements. The test case generation function \\(H : P \\rightarrow T\\) is also decomposed by ARCHCODE into \\(H = g \\circ h\\), where T is the space of test cases and \\(h:P\\timesR\\rightarrow T\\).\nARCHCODE leverages ICL to let an LLM generate a set of software requirements, either reformulated from a given textual description, or extrapolated from the description by the LLM's learnt parametric knowledge. Formally, given in-context examples of description-requirements pairs and the target description p, the LLM returns the list of software requirements\\[\\hat{r} = g([p_1, r_1]; [p_2, r_2]; ...; [p, \\cdot]),\\]where \\(p_i\\) and \\(r_i = [r_i^1, r_i^2, ...]\\) is the description and its corresponding requirement list of i-th example pair, each \\(r_i^j\\) in \\(r_i\\) is a requirement, and \\(\\hat{r}\\) is the list of generated requirements.\nSpecifically, based on the established classifications by Glinz (2007), we further break down FRs and NFRs into distinct categories that our study focuses on.\nOur approach narrows down FRs into three subtypes:\n\u2022 Input/Output Conditions: Analogous to the preconditions and the postconditions in Design by Contract (Meyer, 1992), these define the desired functionality of the code by specifying valid inputs and expected outputs.\n\u2022 Expected Behavior: Along with Input/Output Conditions, it explains the functionality of the target code by reformulating the description into a series of operations for valid inputs that are applicable in most general scenarios.\n\u2022 Edge Cases: While this term generally comprises an array of corner cases, we restrict the scope to only consider valid inputs that necessitate distinct treatment. These include, for example, processing an empty list when the valid input type is a list, or considering '0' for non-negative integer inputs.\nARCHCODE considers NFRs that are both pivotal in real-world applications and feasible for assessment either through code execution or using existing metrics.\n\u2022 Time Performance: Pertains to time-centric aspects like algorithmic time complexity or stipulated timeout conditions.\n\u2022 Robustness: Ensures that code is resilient to invalid inputs (McConnell, 2004). For instance, a function designed for integer addition must prevent unforeseen or undesirable outcomes from the '+' operation, like mistakenly returning the concatenated string when given two strings.\n\u2022 Maintainability: Considers factors that contribute to the ease of maintenance, such as reducing code complexity via code modularization (Magel et al., 1982), measured by cyclomatic complexity (McCabe, 1976).\n\u2022 Reliability: Ensures that the code can handle errors gracefully, without causing system failures, thereby increasing the mean time between failures (McConnell, 2004)."}, {"title": "3.2 Requirements-aware Generation", "content": "Upon obtaining software requirements r, ARCHCODE conditions r with the given description p to generate code samples and test cases. Specifically, ARCHCODE generates code \u0109 and test cases t in a parallel manner:\n\\[\\begin{aligned}\\hat{c} &= f ([p_1, r_1, c_1]; ...; [p, \\hat{r}]),\\\\t &= h([p_1, r_1, t_1]; ...; [p, \\hat{r}]),\\end{aligned}\\]where c and \\(t = [t_1, t_2, ...]\\) are the code and the list of test cases of i-th example, and each \\(t_i\\) in t is a test case corresponding to \\(r^i\\) in \\(\\hat{r}\\). We choose this parallel generation due to the potential pitfalls when these processes condition each other. We further discuss such pitfalls in Section 5.2."}, {"title": "3.3 Pursuing Requirements Satisfaction", "content": "To ensure the conformance of the generated code snippet \u0109 with the specified requirements r, ARCHCODE executes \u0109 against the generated test cases t tailored to one of the requirements in r:\n\\[s = \\text{EXEC} (\\hat{c}, t),\\]where \\(s \\in \\{0,1\\}\\) is a binary result from a code execution function EXEC, and t is one of the generated test cases in t, matching \\(\\hat{r}^i\\) in \\(\\hat{r}\\). To return the satisfactory code towards r, ARCHCODE conducts code filtering. To rank each code in relation to r, our framework calculates a weighted sum of the scores s from each t, with the option to assign higher weights to preferred requirements. Adjusting those weights to tailor the scoring process is discussed in more detail in Section 5.3."}, {"title": "4 Experiments", "content": "We evaluate ARCHCODE's effectiveness using three benchmarks, categorized into two types: 1) A novel benchmark for assessing both FR and NFR satisfaction; 2) Two public benchmarks aimed at FR evaluation, facilitating comparison of ARCHCODE with existing baselines. For the former, we introduce HumanEval-NFR for comprehensive NFR assessment, overcoming the conventional focus on FR alone. For the latter, we explore two code modalities: 1) function-level and 2) competition-level code generation."}, {"title": "4.1 Experimental Setup", "content": "We evaluate the effectiveness of ARCHCODE on code generation with LLMs. Throughout the experiments, we used GPT-3.5-Turbo-16k (OpenAI, 2022) as the backbone LLMs for generating code, software requirements, test cases, etc. More details can be found in Appendix A.\nWe mainly consider the widely used  \\(\\mathbb{E}_{Problems} [\\mathbb{1}(\\cdot)]\\) (Chen et al., 2021) metric for evaluation, which is the unbiased estimator of the probability that the code generation system would have passed a problem if it were given k chances to sample c correct code snippets among n samples. Adhering to Chen et al. (2023), when applying code filtering, we denote the existence of passed code among the k filtered samples."}, {"title": "4.2 HumanEval-NFR: Embracing NFR Evaluation", "content": "We introduce HumanEval-NFR benchmark, which is specifically designed to assess NFR satisfaction. It is an extension of HumanEval that additionally covers four NFR categories, chosen for their suitability for evaluation, through code execution using annotated test cases or automated assessment using existing metrics. Details on the annotation process and metrics we used are provided in Appendix B.\nTable 2 presents that ARCHCODE outperforms all baseline methods across various NFR categories except for maintainability. Our conjecture is that, as which NFR categories to prioritize is uninformed in this experiment, ARCHCODE's consideration of all NFRs could potentially impede maintainability due to the influence of other categories. We study the informed case of optimizing specific categories in Section 5.3. Across all approaches, satisfying the robustness category appears to be more difficult compared to other NFR categories, for which we provide further discussion in Appendix G.\nNotably, ARCHCODE is desirable for evaluating all NFRs at once (i.e. All), outperforming CODET with 22.16% of Pass@1."}, {"title": "4.3 HumanEval and CodeContests: Public Benchmarks for FR Evaluation", "content": "We additionally report results on two popular code generation benchmarks targetting functional correctness. HumanEval (Chen et al., 2021) is a hand-crafted test benchmark with 164 programming problems along with public and hidden test cases. CodeContests (Li et al., 2022) consists of 13k/113/165 instances of train/valid/test data collected from multiple code competition websites. While HumanEval tests the model's capability to implement rather simpler functions without errors, the competitive programming oriented nature of CodeContests often requires more complex form of reasoning such as algorithmic reasoning. Each of these addresses different aspect of industrial software: the former is related to solving each of the simpler tasks composing larger and complex projects while the latter focuses on the logical and algorithmic perspective of software development.\nIn Table 3, ARCHCODE consistently outperforms the baseline methods. Specifically, on both benchmarks, ARCHCODE leveraging GPT-3.5-Turbo, exceeds GPT-4's performance by a substantial margin of 4.81%p and 10.45%p in terms of Pass@1. In comparison with Wizard-Coder 34B-a baseline that partially incorporates NFR considerations during the finetuning phaseARCHCODE, which covers NFRs more comprehensively, achieves significantly higher performance. In CodeContests, while our custom GPT-3.5-Turbo + CoT prompting baseline is outdone by the state-of-the-art CoT methods BRAINSTORM and ALGO, the application of ARCHCODE outperforms both approaches, setting new state-of-the-art of Pass@1. We also compare ARCHCODE with MPSC, a very recent baseline. Notably, ARCHCODE surpasses MPSC in all Pass@k metrics on HumanEval and Pass@1 on CodeContests, while ARCHCODE is much more cost-efficient. We provide further discussion on computational costs in Section 5.1."}, {"title": "5 Analysis and Discussion", "content": ""}, {"title": "5.1 Efficiency and Effectiveness of Requirement-aware Test Case Generation", "content": "In code filtering, a crucial step involves minimizing the number of generated test cases to reduce computational and time costs for code execution. As shown in Figure 4, existing approaches such as MPSC and CODET requires to generate hundreds of test cases for performance. In contrast, ARCHCODE targeting diverse requirement categories shows the best performance while significantly improving the efficiency by generating 50x smaller number of test cases.\nTables 4, 5, and 6 compare two test case generation methods, the naive way (CODET) and ARCHCODE. With the same code generation and filtering strategy applied, the latter generally outperforms the former with large margins, demonstrating the effectiveness of leveraging generated requirements to optimize test case generation. Meanwhile, ARCHCODE yielded comparable results to CODET without the use of clustering on HumanEval. We conjecture that for simpler benchmarks like HumanEval, CODET's approach of generating 'general' test cases suffices. While CODET focuses on general test cases which are likely to have limited coverage, ARCHCODE distinctly promotes a diverse set of test cases targeting various requirement (sub)types."}, {"title": "5.2 Conditioning Code Generation on Test Cases", "content": "In contrast to our approach of generating code and test cases in parallel and then applying subsequent postprocess mechanisms such as filtering, one can also consider conditioning the code generation on test cases, taking inspiration from the Test-Driven Development (TDD; Beck, 2022) methodology. Table 7 shows results consistent with those reported in Chen et al. (2023), indicating marginal improvement in performance is observed when conditioning code generation on the ground-truth and generated test cases, while incorporating software requirements through ARCHCODE effectively boosts the score, even without code filtering. This suggests the overhead from introducing new sequential dependency in the generation process might not be worth the additional costs incurred."}, {"title": "5.3 Preference over Requirements", "content": "As mentioned before, ARCHCODE can be informed of any user preferences over the software requirements at code filtering timeafter several code candidates have been generated and awaiting to be ranked. Figure 5 presents the Pass@1 scores for each NFR category in the HumanEval-NFR benchmark, with different code filtering strategies applied. Using targeted test cases for reranking yields higher pass rates for that specific software requirement than using all test cases does.\nAnother approach to incorporate user preference over the requirements is to consider a subset of requirements when generating code or test cases, or to put emphasis on a subset of requirements by editing the prompt while presenting all requirements to the model. We present detailed analyses for such scenarios in Appendix F."}, {"title": "5.4 ARCHCODE under Diverse Settings", "content": "Here we provide empirical results suggesting that ARCHCODE generalizes well to other models, datasets, etc. than those considered in the main experiments.\nFirst, we showcase ARCHCODE combined with a relatively smaller model, namely WizardCoder 7B. Table 8 indicates that applying ARCHCODE with the said backbone model leads to a notable 15.67%p improvement in Pass@1 on HumanEval, while incorporating in-context learning directly into WizardCoder 7B itself has negative impacts. Note that this observation is consistent with prior findings such as that in Yuan et al. (2023), that instruction tuning might compromise in-context learning capabilities of LLMs; WizardCoder 7B is an instruction-tuned model based on CODELLAMA 7B.\nMeanwhile, in practical settings, diverse LLMS offer complementary benefits in terms of cost-performance trade-off, and thus mixing two models has been a conventional aproach to explore cost-performance design space (Sun et al., 2023; Wang et al., 2023). ARCHCODEMIX shown in Tables 8 and 9 similarly capitalizes on this space by directing most of the generation calls to affordable LLMs, while selectively delegating the part requiring the most of the reasoning capabilities to stronger ones.\nWe also extend the evaluation of ARCHCODE to the task of Java code generation, using the MultiPL-E (Cassano et al., 2022) benchmark and the backbone model SantaCoder 1B (Allal et al., 2023). To address the rather limited capacity of a smaller model, we further applied sparse fine-tuning (Ansell et al. (2022); SFT) on a public Java train set. We provide more details in Appendix A.1. The results in Table 9 demonstrate the effectiveness of the proposed method in generating Java code, supporting that our method is generally applicable to programming languages other than Python."}, {"title": "6 Conclusion", "content": "We proposed ARCHCODE, a framework incorporating software requirements from textual descriptions for LLM-based code generation. This systematic approach not only identifies these requirements but also harnesses them to guide the code generation process. The verification of code snippets with the generated test cases tailored to each requirement provides a robust validation layer for the alignment with detected requirements. On HumanEval and CodeContests, ARCHCODE with GPT-3.5-Turbo exceeded GPT-4's performance by 4.81%p and 10.45%p of Pass@1. ARCHCODE requires 50x less generated test cases compared to MPSC and CODET, while outperforming them. In addition, we introduced a new benchmark named HumanEval-NFR for evaluating how well LLMs can pursue non-functional requirements in code generation task. Further analysis shows the pertinence of parallel generation of code and test case, and the efficiency and the effectiveness of ARCHCODE's requirement-aware test case generation."}, {"title": "A Implementation Details", "content": "We used gpt-3.5-turbo-16k (OpenAI, 2022) as the backbone LLM for most of the experiments, with ICL and nucleus sampling (Holtzman et al., 2020) with p = 0.95 and temperature T = 0.8 following Chen et al. (2021); Nijkamp et al. (2023); Chen et al. (2023). We used different in-context examples for each benchmark: a single HumanEval-style (problem description, code) pair from Li et al. (2023) for HumanEval-NFR, eight pairs from the training set of the MBPP (Austin et al., 2021) benchmark for HumanEval (Chen et al., 2021). For CodeContests (Li et al., 2022) we used a single pair from the train set.\nTo apply CoT prompting (Kojima et al., 2022; Shao et al., 2023; Zhang et al., 2023c), as the state-of-the-art methods BRAINSTORM (Li et al., 2023) and ALGO (Zhang et al., 2023a) are publicly unavailable, we generated the reasoning chains of code outline by using self-planning (Jiang et al., 2023). However, directly using the reasoning chains provided by Self-planning can result in data contamination on HumanEval because these chains are based on the test examples. Thus, rather using them directly, we utilized them to generate the reasoning chains for the aforementioned in-context examples, then used the generated reasoning chains for ICL.\nARCHCODE uses three reasoning chains when generating code: the initial program outline (the same reasoning chains as in GPT-3.5-Turbo + CoT), requirements described in Subsection 3.1 and Appendix D, and the final program outline\u2014the revised version of the initial program outline, modified to meet the requirements.\nWe generated n = 10 code samples for every problem in the benchmarks. To enhance the diversity of the generated code, we employed nucleus sampling to produce n initial program outlines induced from Self-planning. The rest of the reasoning chains were concurrently generated using greedy sampling, culminating in a total of n final code outputs.\nOur implementation is largely based on the LangChain library. Regarding the execution and evaluation of the generated code, we modified some code from the CodeEval repository which is available on Huggingface."}, {"title": "A.1 Open-sourced Backbone Models and Java Language", "content": "We utilized huggingface's text-generation-inference to parallelize WizardCoder 7B on two NVIDIA RTX A6000 48GBs for inference purposes exclusively. It took approximately one hour to experiment with one method on the entire HumanEval benchmark. Consistent to the results on HumanEval shown in Table 8, Table 10 also shows that ARCHCODE significantly contributes to Pass@k scores on CodeContests.\nFor sparse fine-tuning, we followed Ansell et al. (2022) to train 3% of the SantaCoder 1B (Allal et al., 2023) parameters with the batch size of 8 (1*grad_accum of 8), the learning rate of 2e-5, the L1 regularization of 0, and the max training epochs of 3, using a single NVIDIA RTX A6000 48GB for 2 hours. For the training set, we utilized MegaCodeTraining, a public dataset set, while using java related data only."}, {"title": "B HumanEval-NFR Construction", "content": "The HumanEval-NFR benchmark, an extension of HumanEval (Chen et al., 2021), evaluates both FRs and NFRs of code. HumanEval-NFR comprises the same 164 problems as in the original HumanEval suite. While encompassing all the problem descriptions and ground truth test cases from the original HumanEval benchmark for FR verification, it introduces additional test cases for FR and NFR verification. The statistics of HumanEval-NFR's ground truth test cases are shown in Table 11.\nWriting new ground truth test cases involved a two-step process. First, we generated candidate test cases based on the existing HumanEval problems using ARCHCODE based on GPT-3.5-Turbo. Second, we revised those test cases both in automatic or manual manner to ensure the quality of the test suite, based on the following protocols."}, {"title": "B.1 Quality Control for FR Test Cases", "content": "For candidate test cases evaluating FRs, we executed the ground truth code from the original HumanEval benchmark against each test case tailored to functional requirements. Those that the ground truth code does not pass were discarded."}, {"title": "B.2 Quality Control for NFR Test Cases", "content": "For candidate test cases verifying NFRs, three authors manually validated the quality of generated test cases. During validation, the authors adhered to the following principles:\n\u2022 Misclassified test cases should be rectified, and any duplicates should be eliminated.\n\u2022 Test cases should compatible to the original ground truth code. If any discrepancy is found in the code, or if a test case is deemed impractical or overly complex, adjustments should be made to ensure it aligns with the original problem description.\nIn addition, the authors consider guidelines specific to each NFR category:\nAs Rice's Theorem (Rice, 1953) states, all non-trivial properties of Turing-recognizable languages are undecidable, which in essence means that there could be no 'time-complexity checkers.' Therefore, HumanEval-NFR follows conventional strategies used in competitive programming contests, such as Codeforces, where code is executed with relatively larger inputs to ensure that inefficient implementations cannot complete within the specified timeout. Specifically, we set the timeout as 5 seconds for all problems.\nTest cases for this category verify whether the implementation gracefully handles diverse types of invalid inputs, such as a string passed as an argument where an integer is expected. For technical reasons, we expect the code to return values like None, an empty list, or Falseall of which are logically evaluated as False in the Python language-rather than forcing it to raise exceptions or using any other means to indicate it has detected an abnormal input.\nTo validate maintainability, we consider code complexity, which affects the ease of understanding and updating the code (Magel et al., 1982). Specifically, HumanEval-NFR computes the Cyclomatic Complexity (CC; McCabe, 1976) of code, which evaluates code complexity by accounting for the depth of nested indented blocks, then checks whether the observed CC score is lower than the threshold. The threshold is set to 5 if the ground truth code from the original HumanEval benchmark has a CC value below 5; if the CC value exceeds 5, we set the threshold as 10 (Watson et al., 1996).\nRather than generating dedicated test cases, HumanEval-NFR assesses code reliability by executing all the ground truth test cases for the problem and checks if any runtime errors are raised, without verifying if the outputs are correct. This approach aligns with the category's focus on minimizing system failures and extending the mean-time-to-failure."}, {"title": "C Gains from Prompt Engineering", "content": "In this study, we did not focus on devising sophisticated prompts, as our main contribution does not rely heavily on using prompt-engineered instructions. Therefore, we can expect even more performance gains when the prompt is further engineered as in Table 13, as we intentionally kept prompt simple in our main experiments.\nTable 12 shows that ARCHCODE is scalable to requirement instruction prompts, showing the best performance on HumanEval-NFR (All) when both are applied. Unlike CoT and NFR Instruction that improve Robustness and Reliability only, ARCHCODE contributes to all NFR types. Notably, time performance and maintainability are enhanced solely by ARCHCODE's code filtering, highlighting the unique contribution over prompt engineering."}, {"title": "D Correctness of Generated Requirements", "content": "As presented in Figure 6, we organized the structure of software requirements into two parts: problem-agnostic and problem-specific. The former describes general guidelines throughout problems related to reliability, performance, and maintainability. The latter includes more specific instructions depending on the problem description, including all three subtypes of functional requirements, the target time complexity for time performance, the invalid conditions for robustness, and the target Cyclomatic Complexity for maintainability.\nTo confirm the correctness of the requirements generated by ARCHCODE with GPT-3.5-Turbo, we randomly selected three problems, one for each of the following categories: (1) all, (2) some or (3) none of the generated code samples passed the tests. We manually verified validity of each set of requirements for each case, of which the results are summarized in Table 17. Surprisingly, all the generated requirements were correct, regardless of the corresponding generated code's correctness."}, {"title": "E Analysis of Generated Test Cases by ARCHCODE", "content": "Table 14 shows the average number of generated test cases by ARCHCODE for each requirement category. Table 15 reports the accuracy of generated test cases tailored to functional requirements. Although the accuracy of generated edge cases is relatively low, they still play a key role in code filtering as evidentiated by the performance discrepancy between ARCHCODE and CODET (w/o clustering) presented in Tables 4. It is noteworthy that CODET generates \u2018general' test cases, and the results for ARCHCODE and CODET are comparable, given that both methods are based on the same GPT-3.5-Turbo architecture. We conjecture that the relatively low accuracy of generated edge cases does not prevent them from being substantially useful in code filtering, for that wrongful test cases tend to accept or to reject both the correct and incorrect code, rather than selectively passing incorrect ones. In other words, the overall ranking of the generated code samples is hardly affected by the wrongly generated test cases.\nFor the validation of non-functional requirements, we can implicitly confirm through Figure 5 as using targeted test cases (filled green) yielded better results than using all test cases (empty green) across all NFR categories, as mentioned in Section 5.3."}, {"title": "F NFR Preference Control", "content": "Unlike for FRs", "optional": "weaks that can additionally guide the behavior of the code and (2) some trade-off relationships among different NFRs (Chung et al.", "methods": "n\u2022 Preference Control by Instruction: We include all NFRs in prompts just as before", "Consider the time performance requirement to be the most important.\").\n\u2022 Preference Control by Plug-and-Play": "We only present the preferred NFRs in prompts", "NFRs": "focusing on the other requirements is relatively free of negative interference that would hurt the performance. In the categories of time performance and robustness, both instruction and plug-and-play preference settings showed improvements when each"}]}