{"title": "A Theory for Conditional Generative Modeling on Multiple Data Sources", "authors": ["Rongzhen Wang", "Yan Zhang", "Chenyu Zheng", "Chongxuan Li", "Guoqiang Wu"], "abstract": "The success of large generative models has driven a paradigm shift, leveraging massive multi-source data to enhance model capabilities. However, the interaction among these sources remains theoretically underexplored. This paper takes the first step toward a rigorous analysis of multi-source training in conditional generative modeling, where each condition represents a distinct data source. Specifically, we establish a general distribution estimation error bound in average total variation distance for conditional maximum likelihood estimation based on the bracketing number. Our result shows that when source distributions share certain similarities and the model is expressive enough, multi-source training guarantees a sharper bound than single-source training. We further instantiate the general theory on conditional Gaussian estimation and deep generative models including autoregressive and flexible energy-based models, by characterizing their bracketing numbers. The results highlight that the number of sources and similarity among source distributions improve the advantage of multi-source training. Simulations and real-world experiments validate our theory.", "sections": [{"title": "1. Introduction", "content": "Large generative models have achieved remarkable success in generating realistic and complex outputs across natural language and computer vision. A key factor behind their strong performance is the diverse and rich training data. For instance, large language models are trained on heterogeneous datasets comprising web content, books, and code, while image generation models benefit from vast datasets spanning various categories and aesthetic qualities. Empirical evidence suggests that, under certain conditions, training on multiple data sources can enhance performance across all sources. Consequently, data mixture strategies have become an essential research topic.\nHowever, the theoretical underpinnings of this multi-source training paradigm remain poorly understood. This raises a fundamental question: is it more effective to train separate models on individual data sources, or to train a single model using data from multiple sources? In this paper, we take the first step toward a rigorous analysis of multi-source training, focusing on its impact on conditional generative models, where each condition represents a distinct data source.\nOur first contribution is establishing a general upper bound on distribution estimation error for conditional generative modeling via maximum likelihood estimation (MLE) in Section 3. Specifically, we measure the error using average total variation (TV) distance between the true and estimated conditional distributions across all sources, which scales as $\\tilde{O}(\\sqrt{\\frac{\\log N_{[]}( \\epsilon; P_{X|Y}, L^1(X))}{n}})$, where n is the training set size and $N_{[]}( \\epsilon; P_{X|Y}, L^1(X))$ is the bracketing number of the conditional distribution space $P_{X|Y}$. Further, when source distributions exhibit parametric similarity, multi-source training effectively reduces the complexity of the distribution space, leading to a provably sharper bound than single-source training.\nTechnically, our analysis extends classical MLE estimation error bounds from empirical process theory to the conditional setting by adapting the complexity of the distribution space and measuring the estimation error in terms of average TV distance. Further discussions are provided in Section 6.\nAs the second contribution, we instantiate our general theory in three specific cases: (1) parametric estimation of conditional Gaussian distributions, a simple example clearly illustrating how source distribution properties influence the benefits of multi-source training, (2) autoregressive models (ARMs), the foundation of large language models, and (3) energy-based"}, {"title": "2. Problem formulation", "content": "Elementary notations. Scalars, vectors, and matrices are denoted by lowercase letters (e.g., a), lowercase boldface letters (e.g., a), and uppercase boldface letters (e.g., A). We use a[m] to denote the m-th entry of vector a, and A[m, :], A[:, n], and A[m, n] to denote the m-th row, the n-th column, and the entry at the m-th row and the n-th column of A. (a, b) denotes the concatenation of a and b as a column vector. We denote [n] := {1, . . ., n} for any n \u2208 N and a V b as max{a,b}. For any measurable scalar function f(x) on domain X and real number 1 < p < \u221e, its LP(X)-norm is"}, {"title": "2.1. Data from multiple sources", "content": "Let X denote the random variable for data (e.g., a natural image) in a data space X, and Y denote the random variable for the source label in a label space Y. Suppose there are K data sources (e.g., K categories of images), each corresponding to an unknown conditional distribution $p_{X|k}$ for $k \\in [K]$. We assume that $p_{X|k}$ is parameterized by a source-specific feature in a parameter space \u03a6 and a shared feature $\\psi^*$ in a parameter space \u03a8, such that $p_{X|k}(x|k) = P_{\\phi^*, \\psi^*}(x|k)$. The conditional distribution of X given Y = y is consequently expressed as\n$P_{X|Y}(x|y) = \\prod_{k=1}^{K} (P_{\\phi^*, \\psi^*}(x|k))^{I(y=k)}$\nThis compact representation provides convenience for subsequent discussions.\nWe further assume the distribution of Y is known since the proportion of data from different sources is often manually designed in practice. The joint distribution of X and Y is then given by $p_{X,Y}(x, y) = P_{X|Y}(x|y)p_Y(y)$."}, {"title": "2.2. Conditional generative modeling", "content": "Consider a dataset $S = \\{(x_i, y_i)\\}_{i=1}^n$ consisting of n independent and identically distributed (i.i.d.) data-label pairs sampled from the joint distribution $p_{X,Y}$. In the learning phase, a conditional generative model uses maximum likelihood estimation (MLE) to estimate $p_{X|Y}$ based on the dataset S, where the conditional likelihood is defined as\n$L_S(p_{X|Y}) := \\prod_{i=1}^{n} P_{X|Y}(x_i|y_i).$    (1)\nMulti-source training. Under multi-source training, the conditional distribution space is given by $\\mathcal{P}_{multi} := \\{p_{X|Y}^{multi}(x|y) = \\prod_{k=1}^{K} (P_{\\phi_k, \\psi}(x|k))^{I(y=k)}; \\phi_k \\in \\Phi, \\psi \\in \\Psi\\}$, and the corresponding estimator of $p_{X|Y}$ is\n$P_{X|Y}^{multi} = \\underset{p_{X|Y}^{multi} \\in \\mathcal{P}_{multi}}{\\text{arg max}} L_S(p_{X|Y}^{multi}).$  (2)\nHere, we adopt the realizable assumption that true parameters satisfy $\\phi^* \\in \\Phi$ and $\\psi^* \\in \\Psi$ following , which allows the estimation error analysis to focus on the generalization property of the distribution space."}, {"title": "3. Provable advantage of multi-source training", "content": "In this section, we establish a general upper bound on the average TV error for conditional MLE and provide a statistical guarantee for the benefits of multi-source training. Our analysis extends the classical MLE guarantees , which leverage the bracketing number and the uniform law of large numbers."}, {"title": "3.1. Complexity of the conditional distribution space", "content": "We begin by introducing an extended notion of the bracketing number as follows.\nDefinition 3.1 (\u20ac-upper bracketing number for conditional distribution space). Let $\\epsilon$ be a real number that $\\epsilon > 0$ and p be an integer that 1 < p < \u221e. An $\\epsilon$-upper bracket of a conditional distribution space $P_{X|Y}$ with respect to $L_p(X)$ is a finite function set B such that for any $p_{X|Y} \\in P_{X|Y}$, there exists some p' \u2208 B such that given any y \u2208 Y, it holds\n$\\forall x \\in X : p'(x, y) \\geq p_{X|Y}(x|y)$, and\n$||p'(\\cdot, y) - p_{X|Y}(\\cdot|Y)||_{L_p(X)} \\leq \\epsilon.$\nThe $\\epsilon$-upper bracketing number $N_{[]}(\\epsilon; P_{X|Y}, L_p(X))$ is the cardinality of the smallest $\\epsilon$-upper bracket.\nThis notion quantifies the minimal set of functions needed to upper bound every conditional distribution within a small margin, reducing error analysis from an infinite to a finite function class. Unlike traditional bracketing numbers for unconditional distributions px using two-sided brackets , this extension employs one-sided upper brackets and requires uniform coverage across y for all conditional distributions tailored for our setting."}, {"title": "3.2. Guarantee for conditional MLE", "content": "We now present a general error bound which applies to both training strategies.\nTheorem 3.2 (Average TV error bound for conditional MLE, proof in Appendix A.1). Given a dataset S of size n that i.i.d. sampled from $p_{X,Y}$, let $P_{X|Y}$ be the maximizer of $L_S(p_{X|Y})$ defined in Equation (1) in conditional distribution space $P_{X|Y}$. Suppose the real conditional distribution $p_{X|Y}$ is contained in $P_{X|Y}$. Then, for any $0 < \\delta < 1/2$, it holds with probability at least 1 \u2013 8 that\n$R_{TV}(P_{X|Y}) \\leq 3 \\sqrt{\\frac{\\frac{1}{n} (\\log N_{[]}(\\epsilon; P_{X|Y}, L^1(X)) + \\log \\frac{1}{\\delta})}$\nAs formulated in Equation (2) and Equation (3), multi-source and single-source training apply conditional MLE on S within different conditional distribution spaces. The following proposition shows that multi-source training reduces the bracketing number of its distribution space through source similarity.\nProposition 3.3 (Multi-source training reducing complexity, proof in Appendix A.2.). Let $P_{X|Y}^{multi}$ and $P_{X|Y}^{single}$ be as defined in Section 2. Then, for any $\\epsilon > 0$ and $1 < p < \\infty$, we have\n$N_{[]}(\\epsilon; P_{X|Y}^{multi}, L_p(X)) \\leq N_{[]}(\\epsilon; P_{X|Y}^{single}, L_p(X)).$"}, {"title": "4. Instantiations", "content": "We now apply our general analysis to conditional Gaussian estimation and two deep generative models to obtain concrete error bounds."}, {"title": "4.1. Parametric estimation on Gaussian distributions", "content": "As employed in extensive work , Gaussian models provide a simple yet insightful case for illustrating the benefits of multi-source training and enable analytically tractable simulations under our theoretical assumptions.\nParametric distribution family. Suppose each of the K conditional distributions is a d-dimensional standard Gaussian distribution, i.e.,\n$\\forall k\\in [K], X|k \\sim N(\\mu_k, I_d) = \\frac{1}{(2\\pi)^{d/2}} e^{-\\frac{1}{2}||x-\\mu_k||^2},$\nwith a mean vector \u00b5 and an identity covariance matrix $I_d \\in \\mathbb{R}^{d \\times d}$. We assume each mean vector has two parts: the first $d_1$ entries $\\mu[1 : d_1]$ represent the source-specific feature which is potentially different for each source, and the remaining entries $\\mu[d_1+1 : d]$ represent the shared feature which is identical across all sources. Corresponding to the general formulation in Section 2, we denote\n$\\phi_k := \\mu[1 : d_1], \\psi:=\\mu_1[d_1+1:d]=\\dots=\\mu_k[d_1+1:d],$ and the conditional distribution is parameterized as\n$P_{\\phi_k,\\psi}(x|k) = (2\\pi)^{-\\frac{d}{2}} e^{-\\frac{1}{2}||x-(\\phi_k,\\psi)||^2}.$    (5)\nStatistical guarantee of the average TV error. In this formulation, the conditional MLE in $P^{multi}$ under multi-source training leads to the following result.\nTheorem 4.1 (Average TV error bound for conditional Gaussian estimation under multi-source training, proof in Appendix B.2). Let $P_{X|Y}^{multi}$ be the likelihood maximizer defined in Equation (2) given $P^{multi}$ with conditional distributions as in Equation (5). Suppose $\\Phi = [-B, B]^{d_1}, \\Psi = [-B, B]^{d-d_1}$ with constant B > 0, and $\\phi \\in \\Phi, \\psi^* \\in \\Psi$. Then, for any 0 < \u03b4 < 1/2, it holds with probability at least 1 \u2013 8 that\n$R_{TV}(P_{X|Y}^{multi}) = \\tilde{O}(\\sqrt{\\frac{(K-1)d_1 + d}{n}})$"}, {"title": "In contrast, single-source training results in an error of", "content": "$R_{TV}(P_{X|Y}^{single}) = \\tilde{O}(\\sqrt{\\frac{Kd}{n}})$, with a formal result provided in Theorem B.2. The advantage of multi-source learning can be quantified by the ratio of error bounds:\n$\\frac{\\tilde{O}(\\sqrt{\\frac{(K-1)d_1 + d}{n}})}{\\tilde{O}(\\sqrt{\\frac{Kd}{n}})} = \\sqrt{\\frac{(K-1)d_1+d}{Kd}} = \\sqrt{1 - \\frac{K-1}{K} \\frac{d-d_1}{d}}$. Denoting $\\beta_{sim} := \\frac{d-d_1}{d}$, this general form of ratio $\\sqrt{1 - \\frac{K-1}{K} \\beta_{sim}}$ applies across Section 4.2 and Section 4.3, decreasing with both the number of sources K and source similarity \u03b2sim.\nSpecifically, as K increases from 1 to \u221e, the ratio decreases from 1 to $\\sqrt{1 - \\beta_{sim}}$, and as \u03b2sim increases from 0 (completely dissimilar distributions) to 1 (completely identical distributions), it decreases from 1 to $\\sqrt{\\frac{1}{K}}$, reflecting a transition from no asymptotic gain to a constant improvement. This highlights that the number of sources and distribution similarity enhance the benefits of multi-source training. Empirical results in Section 5.2 confirm this trend."}, {"title": "4.2. Conditional ARMs on discrete distributions", "content": "For deep generative models, our formulations are based on multilayer perceptrons (MLPs), a fundamental network component, with potential extensions to Transformers and convolution networks with existing literature. We formally define MLPs mainly following notations in.\nDefinition 4.2 (Class of MLPs). A class of MLPs $\\mathcal{F}(L, W, S, B)$ with depth L, width W, sparsity S, norm B, and element-wise ReLU activation that $ReLU(x) = 0 \\vee x$ is defined as $ \\mathcal{F}(L, W, S, B) := \\{f(x) = (A^{(L)} ReLU(\\cdot) + b^{(L)}) \\circ \\dots \\circ (A^{(1)} x+b^{(1)}); \\{(A^{(l)}, b^{(l)})\\}_{l=1}^{L} \\in \\mathcal{W}(L, W, S, B)\\}$, where parameter space $\\mathcal{W}(L, W, S, B)$ is defined by $ \\mathcal{W}(L, W, S, B) := \\{\\{(A^{(l)}, b^{(l)})\\}_{l=1}^{L} : A^{(l)} \\in \\mathbb{R}^{W_l \\times W_{l-1}}, b^{(l)} \\in \\mathbb{R}^{W_l}, \\max_l W_l \\leq W, \\sum_{l=1}^{L}(\\|A^{(l)}\\|_0 + \\|b^{(l)}\\|_0) \\leq S, \\max_{l}\\|A^{(l)}\\|_{\\infty} \\vee \\|b^{(l)}\\|_{\\infty} \\leq B\\}$.\nWe now present the formulation for ARMs, which can be viewed as an extension of.\nProbabilistic modeling with autoregression. Consider a common data scenario for the natural language where X represents a D-length text in $[M]^D$. Each dimension of X is an integer token following an M-categorical distribution with M being the vocabulary size. Adopting the autoregressive approach of probabilistic modeling, conditional distribution $p_{X|Y}(x|y)$ is factorized using the chain rule as:\n$p(x|y) = p(x_1|y) \\dots p(x_D|x_{<D}, y) = p(x_1; p(y)) \\dots p(x_D; P(x_{<D,Y})).$\nWe omit the subscripts for notation simplicity. Here, for any d \u2208 [D], $p(x_{<d}, y)$ is the distribution parameter for $X_d$"}, {"title": "Distribution estimation via neural network", "content": "Aligning with common practices, we suppose the distribution parameter vector is estimated with $p_\\theta (x_{ <d}, y)$ using a shared neural network parameterized by \u03b8 across all dimensions. The network comprises an embedding layer, an encoding layer, an MLP block, and a softmax output layer.\nSpecifically, we first look up x and y in two embedding matrices $V_x \\in [0, 1]^{M \\times d_e}$ and $V_y \\in [0, 1]^{K \\times d_e}$, then stack the embeddings to get\n$\\mathbb{E}_{V_y, V_x} (x, y) = \\begin{bmatrix} V_y [y,:] \\\\ V_x[x_1,:] \\\\ \\vdots \\\\ V_X [x_{D-1},:] \\end{bmatrix} \\in [0,1]^{D \\times d_e},$\nwhere the last dimension of x is excluded since it is not used when estimating the distribution.\nSubsequently, we encode each embedding by a linear transformation with parameters $A_o \\in \\mathbb{R}^{D \\times d_e}, b_o \\in \\mathbb{R}^{D}$ and normalize the output with an element-wise sigmoid function $\\sigma(x) = \\frac{1}{1+e^{-x}}$ as\n$\\mathbb{U}_{A_o,b_o} (\\mathbb{E}_{V_y, V_x} (x, y)) = \\begin{bmatrix} \\sigma(A_o[1,:]V_y [y, :]+b_o[1]) \\\\ \\vdots \\\\ \\sigma(A_o[D,:]V_x[x_{D-1},:]+b_o[D]) \\end{bmatrix} \\in [0, 1]^{D}.$\nTo ensure no components related to $x_{>d}$ is seen when estimating the conditional probability for $x_d$, we mask $\\mathbb{U}_{A_0,b_o}$ using a (D - d)-dimensional zero vector $0_{D-d}$ as\n$\\mathbb{V}^{\\mathbb{O}_{D-d}}_{A_o,b_o} = \\begin{bmatrix} \\mathbb{V}_{A_o,b_o} [1 :d] \\\\ 0_{D-d} \\end{bmatrix}.$\nThen we calculate the distribution parameter vector by an MLP $f_w \\in \\mathcal{F}(L, W, S, B)$ with $W_0 = D$ and $W_L = M$, followed by a softmax layer as\n$\\rho_\\theta (x_{<d}, y) = softmax(\\mathbb{f}_w (\\mathbb{V}^{\\mathbb{O}_{D-d}}_{A_o,b_o} (\\mathbb{E}_{V_y, V_x} (x, y))).$\nThis leads to conditional distribution as\n$P_\\theta (x|y) = p(x_1; \\rho_\\theta(y)) \\dots p(x_D; \\rho_\\theta(x_{<D,Y})).$   (6)\nWhen training such an ARM, each row of $V_y$ is only optimized on data with the corresponding condition, while"}, {"title": "Corresponding to the general formulation in Section 2, we denote", "content": "parameters in $V_y, A_o, b_o$, and w are optimized on data with all conditions. That means $V_y[k, :] serves as the source-specific parameter, while other parameters are shared across all sources. Corresponding to the general formulation in Section 2, we denote\n$\\Phi_k := V_y [k,:], and \\psi := \\{V_x, A_o, b_o, w\\}.$\nIn this Statistical guarantee of the average TV error. formulation, the conditional MLE in $P^{multi}$ under multi-source training leads to the following result.\nTheorem 4.3 (Average TV error bound for ARMs under multi-source training, proof in Appendix C.4). Let $P_{X|Y}^{multi}$ be the likelihood maximizer defined in Equation (2) given $P^{multi}$ with conditional distributions as in Equation (6). Suppose $\\mathcal{I} = [0,1]^{d_e}, \\Psi = [0,1]^{M \\times d_e} \\times [-B, B]^{D \\times d_e} \\times [-B, B]^{D \\times \\mathcal{W}(L, W, S, B)}$ with constants L, W, S, B > 0, and $\\phi \\in \\Phi, \\psi^* \\in \\Psi$. Then, for any 0 < \u03b4 < 1/2, it holds with probability at least 1 \u2013 8 that\n$R_{TV}(P_{X|Y}^{multi}) = \\tilde{O}(\\sqrt{\\frac{L(S + D + (D + M + K)d_e)}{n}})$"}, {"title": "In contrast, single-source training results in an error of", "content": "$R_{TV}(P_{X|Y}^{single}) = \\tilde{O}(\\sqrt{\\frac{KL(S+D+(D+M+1)d_e)}{n}})$ with a formal result provided in Theorem C.8. The advantage of multi-source learning is quantified by the ratio of error bounds:\n$\\sqrt{\\frac{L(S+D+(D+M+K)d_e)}{KL(S+D+(D+M+1)d_e)}} = \\sqrt{1 - K \\beta_{sim}},$\nwhere the term $\\beta_{sim} := \\frac{de}{S+D+(D+M+K)d_e+d_e} \\in [0,1]$ quantifies source distribution similarity based on the proportion of shared parameters. This ratio follows the same pattern discussed in Section 4.1 where the number of sources K and the distribution similarity \u03b2sim are two key factors improving the advantage of multi-source training."}, {"title": "4.3. Conditional EBMs on continuous distributions", "content": "In this section, we study distribution estimation for conditional EBMs, a flexible probabilistic modeling approach on continuous data. Our formulation follows with simplified neural network architecture.\nProbabilistic modeling with energy function. Consider a common scenario with natural image X flattened and normalized in $[0, 1]^D$. The conditional distribution $p_{X|Y}(x|y)$ is factorized with an energy function u(x|y) as:\n$p(x|y) = \\frac{e^{-u(x|y)}}{\\int_x e^{-u(s|y)}ds}$\nDistribution estimation via neural network. We suppose the energy function is estimated with $u_\\theta(x, y)$ using"}, {"title": "Statistical guarantee of the average TV error", "content": "In this a neural network parameterized by \u03b8, which comprises a condition embedding layer and an energy-estimating MLP.\nSpecifically, we first look up y in a condition embedding matrix V \u2208 [0,1]K\u00d7de and concat the embedding with x\n$e_V(x, y) = \\begin{bmatrix} x \\\\ V[y,:] \\end{bmatrix} \\in [0,1]^{D+d_e}$\nThen we use an MLP $f_\\theta \\in \\mathcal{F}(L,W, S, B)$ with Wo = D+ de and WL = 1 to estimate the energy as\n$u_\\theta(x|y) = f_w (e_V(x,y)),$\nwhere \u03b8 := {V, w}. This leads to a conditional distribution as\n$P_\\theta(x|y) = \\frac{e^{-u_\\theta(x|y)}}{\\int_x e^{-u_\\theta(s|y)}ds}$      (7)\nWhen training such an EBM, each row of V is only optimized on data with the corresponding condition, while w is optimized on data with all conditions. That means V[k,:] serves as the source-specific parameter and w is shared across all sources. Corresponding to the general formulation in Section 2, we denote\n$\\Phi_k := V[k,:], and \\psi := \\omega.$\n Statistical guarantee of the average TV error. In this formulation, the conditional MLE in $P^{multi}$ under multi-source training leads to the following result.\nTheorem 4.4 (Average TV error bound for EBMs under multi-source training, Proof in Appendix D.3). Let $P^{multi}$ be the likelihood maximizer defined in Equation (2) given $P^{multi}$ with conditional distributions in Equation (7). Suppose $\\Phi = [0,1]^{de} and $\\Psi =W(L,W, S, B)$ with constants L, W, S, B > 0 and assume \u0444 \u2208 \u0424, \u03c8* \u2208 \u03a8. Then, for any 0<<1/2, it holds with probability at least 1-8 that\n$R_{TV}(P^{multi}) = \\tilde{O}(\\sqrt{\\frac{L(S+Kde)}{n}})$\nIn contrast, single-source training results in an error of $R_{TV}(P_{X|Y}^{single}) = \\tilde{O}(\\sqrt{\\frac{LK(S+de)}{n}})$ with a formal proof provided in Theorem D.4. The advantage of multi-source learning is quantified by the ratio of error bounds:\n$\\frac{\\sqrt{L(S+Kde)}}{\\sqrt{LK(S+de)}} = \\sqrt{1 - \\frac{K-1}{K} \\beta_{sim}},$ where $\\beta_{sim} := \\frac{de}{S+de} \\in [0, 1]$ quantifies source distribution similarity based on the proportion of shared parameters. Similar to the former two cases, the number of sources K and the distribution similarity \u03b2sim improve the advantage of multi-source training."}, {"title": "5. Experiments", "content": "In this section, simulations and real-world experiments are conducted to verify our theoretical results in Section 3 and 4."}, {"title": "5.1. Simulations on conditional Gaussian estimation", "content": "In this part, we aim to examine the tightness of the derived upper bound that $R_{TV}(P^{multi}) = \\tilde{O}(\\sqrt{\\frac{(K-1)d_1 + d}{n}})$ in Theorem 4.1 and $R_{TV}(P_{X|Y}^{single}) = \\tilde{O}(\\sqrt{\\frac{Kd}{n}})$ in Theorem B.2.\nThe number of sources K, sample size n, and the similarity factor \u03b2sim \u2208 [0, 1] are key parameters. In all of our simulations, we fix data dimension d = 10 and $p(k) = \\frac{1}{K}$ all k \u2208 [K]. The dissimilar dimension $d_1 = d \\cdot [\\beta_{sim}d]$. We set the source-specific feature as $\\phi_k = k1 \\in \\mathbb{R}^{d_1}$ and the shared feature as $\\psi = 0 \\in \\mathbb{R}^{d-d_1}$. Under the setting of Section 4.1, conditional MLE has analytical solutions: under multi-source training, we have\n$\\hat{\\phi}_k = \\sum_{y=k} x_i [1: d_1]/n_k, \\hat{\\psi} = \\sum_{i=1} x_i [d_1+1 :d]/n,$\nand under single-source training, we have\n$\\hat{\\phi}_k = \\sum_{y=k} x_i [1: d_1]/n_k, \\hat{\\psi}_k = \\sum_{y=k} x_i [d_1+1 :d]/n_k.$\nFor evaluation, we randomly sample $n_{test} = 500$ data points according to the true joint distribution $p_{X,Y}$. Empirically, we approximate the true TV distance by using the Monte Carlo method based on the test set, which can be written formally as\n$R_{TV}(P_{X|Y}) \\approx \\frac{1}{2n_{test}} \\sum_{i=1}^{n_{test}} |\\frac{P_{X|Y}(X_i|Y_i)}{P_{X|Y}^*(X_i|Y_i)} - 1| = R_{TV}(xy).$\nTo eliminate the randomness, we average over 5 random runs for each simulation and report the mean results.\nOrder of the average TV error about K. We range the number of sources K in [1,3,5, 10, 15] with fixed sample size n = 500 and similarity factor \u03b2sim = 0.5. We display the empirical average TV error for each K in Figure 1(a), with $R_{TV}^{em} (multi)$ colored in green and $R_{TV}^{em} (single)$ colored in orange. Ignoring the influence of constants, it shows a good alignment between empirical errors (in solid lines) and theoretical upper bounds (in dashed lines), both scaling as $\\tilde{O}(\\sqrt{K})$.\nOrder of the average TV error about n. We range sample size n in [100, 300, 500, 1000, 5000] with fixed number of sources K = 5 and similarity factor \u03b2sim = 0.5. We display the empirical error for each n in Figure 1(b), with $R_{TV}^{em}(multi)$ colored in green and $R_{TV}^{em}(single)$ colored in orange. Ignoring the influence of constants, it shows that the orders of empirical error about n match well with the theoretical upper bounds which scale as $\\tilde{O}(\\frac{1}{\\sqrt{n}})$."}, {"title": "Order of the average TV error about", "content": "\u03b2sim. We range similarity factor \u03b2sim in [0, 0.3, 0.5, 0.7, 1] with fixed sample size n = 500 and number of data sources K = 5. We display the empirical average TV error for each \u03b2sim in Figure 1(c) to observe how similarity factor \u03b2sim impacts the advantage of multi-source training. Concretely, as predicted by the theoretical bounds, the changing of \u03b2sim will not influence the performance of single-source training but will decrease the error of multi-source training in the order of $\\tilde{O}(\\sqrt{d_1}) = O(\\sqrt{1 - \\beta_{sim}})$. The results show that theoretical bounds predict the empirical performance well.\nTo sum up, our simulations verify the validity of our theoretical bounds in Section 4.1. Moreover, in all experiments, R^(e)multi consistently smaller than R^(e)single, supporing our results in Section 3"}, {"title": "5.2. Real-world experiments on diffusion models", "content": "In this section, we conduct experiments on diffusion models to validate our theoretical findings in real-world scenarios from two aspects: (1) We empirically compare multi-source and single-source training on conditional diffusion models and evaluate their performance to validate the guaranteed advantage of multi-source training against single-source training proved in Section 3. (2) We investigate the trend of this advantage about key factors-the number of sources and distribution similarity-as discussed in Section 4.\nExperimental settings. We train class-conditional diffusion models following EDM2 at 256x256 resolution on the selected classes from the ILSVRC2012 training set, which is a subset of ImageNet containing 1.28M natural images from 1000 classes, each annotated with an integer class label from 1 to 1000. In our experiments, we treat each class as a distinct data source. To control similarity among data sources, we manually design two levels of distribution similarity based on the semantic hierarchy of ImageNet as shown"}, {"title": "6. Other related works", "content": "Distribution estimation guarantee for MLE. Classical approaches investigate distribution estimation for MLE in Hellinger distance based on the bracketing number and the uniform law of large numbers from empirical process theory , which yields high-probability bounds of similar order as Theorem 3.2. extend the analysis to derive TV error bound under the realizable assumption. We further adapt their techniques to conditional generative modeling by introducing the upper bracketing number to quantify the complexity of conditional distribution space in Definition 3.1 and modify the proofs to handle conditional MLE in Appendix A.1.\nTheory on multi-task learning. Multi-task learning is a well-studied topic in supervised learning. It typically benefits from similarities across tasks, sharing some commonality with multi-source training. However, theoretical analyses in supervised learning often assume a bounded objective, whereas our MLE analysis imposes no such restriction.\nAdvanced theory on generative models. Among generative models based on (approximate) MLE diffusion models have been extensively studied theoretically on its score"}, {"title": "7. Conclusion and discussion", "content": "This paper provides the first attempt to rigorously analyze the conditional generative modeling on multiple data sources from a distribution estimation perspective. In particular, we establish a general estimation error bound in average TV distance under the realizable assumption based on the bracketing number of the conditional distribution space. When source distributions share parametric similarities, multi-source training has a provable advantage against single-source training by reducing the bracketing number. We further instantiate the general theory on three specific models to obtain concrete error bounds. To achieve this, novel bracketing number bounds for ARMs and EBMS are established. The results show that the number of data sources and the similarity between source distributions enhance the benefits of multi-source training. Simulations and real-world experiments support our theoretical findings.\nOur theoretical setting differs from practice in some aspects, e.g., language models have no explicit conditions, and image generation models are commonly conditioned on descriptive text"}]}