{"title": "GEOMETRIC RELATIONAL EMBEDDINGS", "authors": ["Bo Xiong"], "abstract": "In classical AI, symbolic knowledge is typically represented as relational data within a graph-structured framework, a.k.a., relational knowledge bases (KBs). Relational KBs suffer from incompleteness and numerous efforts have been dedicated to KB completion. One prevalent approach involves mapping relational data into continuous representations within a low-dimensional vector space, referred to as relational representation learning. This facilitates the preservation of relational structures, allowing for effective inference of missing knowledge from the embedding space. Nevertheless, existing methods employ pure-vector embeddings and map each relational object, such as entities, concepts, or relations, as a simple point in a vector space (typically Euclidean \\( \\mathbb{R} \\) ). While these pure-vector embeddings are simple and adept at capturing object similarities, they fall short in capturing various discrete and symbolic properties inherent in relational data.\nThis thesis surpasses conventional vector embeddings by embracing geometric embeddings to more effectively capture the relational structures and underlying discrete semantics of relational data. Geometric embeddings map data objects as geometric elements, such as points in hyperbolic space with constant negative curvature or convex regions (e.g., boxes, disks) in Euclidean vector space, offering superior modeling of discrete properties present in relational data. Specifically, this dissertation introduces various geometric relational embedding models capable of capturing: 1) complex structured patterns like hierarchies and cycles in networks and knowledge graphs; 2) intricate relational/logical patterns in knowledge graphs; 3) logical structures in ontologies and logical constraints applicable for constraining machine learning model outputs; and 4) high-order complex relationships between entities and relations.\nOur results obtained from benchmark and real-world datasets demonstrate the efficacy of geometric relational embeddings in adeptly capturing these discrete, symbolic, and structured properties inherent in relational data, which leads to performance improvements over various relational reasoning tasks.", "sections": [{"title": "1.1 BACKGROUND, MOTIVATION, AND CHALLENGES", "content": "Representation learning plays a pivotal role in modern machine learning, providing the capability to acquire compact, continuous, and lower-dimensional representations for a variety of real-world data, including images [80], words [164], and documents [87]. These distributional representations allow for the discrimination of relevant distinctions while disregarding irrelevant variations among these objects. They serve as valuable inputs for various machine learning tasks, such as image recognition [80], text categorization [164], and disease diagnosis [87].\nThe predominant approach in many current works maps objects into a low-dimensional vector space, typically represented in the Euclidean space \\( \\mathbb{R}^d \\). We refer to this representation as a plain vector embedding. The rationale behind using plain vector embeddings is their ability to preserve 'similarities' between objects through pairwise distances or inner products in the vector space. For example, images from the same categories or words occurring in similar linguistic contexts are mapped to vectors that are 'near' in the embedding space.\nUnlike the approaches prevalent in modern machine learning, classical AI, such as knowledge representations and reasoning [101] and statistical relational learning [141], relies on symbolic knowledge. Symbolic knowledge is often represented as structured and relational data that delineate semantic relationships among entities and/or concepts. Typically, this representation takes the form of a set of factual statements, each encapsulating a fact that describes a semantic relationship involving two or more entities and/or classes. Additionally, with the aid of complex mathematical constructs, symbolic knowledge can also be described as a set of logical statements, each describing a logical relationship among various entities and/or concepts. These factual and logical statements together form a symbolic knowledge base, storing relational knowledge over a specific domain. Such symbolic knowledge plays a crucial role in various applications, including biomedical [42, 143] and intelligent systems [157]."}, {"title": "2.1 RELATIONAL DATA", "content": "In this dissertation, our primary focus is relational data that describes diverse relationships between entities and/or concepts in a graph-structured format. We choose to model relational data as a graph because it offers greater flexibility for integrating new sources of data [79]. This is in contrast to the standard relational data model where a schema must be pre-defined and adhered to at each step. Graph-structured data models have found extensive use in organizing various real-world relational data, such as information networks, knowledge graphs, and biomedical ontologies."}, {"title": "2.1.1 Graph-Structured Data Models", "content": "There are several graph-structured data models, such as directed edge-labeled graphs, heterogeneous graphs, and property graphs, which we introduce as follows.\nDirected edge-labeled graphs, also called multi-relational graphs [131], are one of the graph-structured data models. A directed edge-labeled graph is defined by a set of nodes representing entities or concepts, like New York and USA, and a set of directed labeled edges connecting these nodes, with each labeled edge representing a relationship between these connected nodes, such as (New York, CityOf, USA). Formally, a directed edge-labeled graph is defined as:\nDefinition 1 (Directed edge-labeled graph [79]). A directed edge-labeled graph is a tuple \\( G = (V, E, L) \\), where \\( V \\subseteq Con \\) is a set of nodes, \\( L \\subseteq Con \\) is a set of edge labels, and \\( E \\subseteq V \\times L \\times V \\) is a set of edges, where Con denotes a countably infinite set of constants.\nNote that this definition is very flexible, as we do not assume that \\( V \\) and \\( L \\) are disjoint. In principle, a node can also serve as an edge label, and nodes and edge labels can be present without any associated edge. Moreover, although the edge is directional, bidirectional edges can be simply represented with two edges with inverse directions.\nOne limitation of this definition is that it does not distinguish between nodes and the type of nodes but rather expresses the type as a relation, e.g., (New York, Type, City).\nHeterogeneous graphs or heterogeneous information networks [82] represent relational data as a set of nodes and a set of edges, with each node and edge associated with a type or label. A heterogeneous graph is formally defined as follows.\nDefinition 2 (Heterogeneous graph [79]). A heterogeneous graph is a tuple \\( G = (V, E, L, l) \\), where \\( V \\subseteq Con \\) is a set of nodes, \\( L \\subseteq Con \\) is a set of edge/node labels, \\( E \\subseteq V \\times L \\times V \\) is a set of edges, and \\( l : V \\rightarrow L \\) maps each node to a label, where Con denotes a countably infinite set of constants.\nIn contrast to a directed edge-labeled graph, a heterogeneous graph encodes the type of a node as part of the node itself, rather than modeling types of nodes with a Type relation. Hence, one of the main advantages of a heterogeneous graph is that it allows for explicit distinction between nodes and the types of nodes. This is particularly useful when the type of each node is unique. However, a heterogeneous graph cannot express multiple types for a single node (i.e., many-to-one mapping).\nProperty graphs constitute a graph-structured data model that provides additional flexibility when modeling complex relations. Notably, a property graph allows for annotating more intricate details to each edge, such as the degree and major obtained by a person from a university. This is represented by a set of property-value pairs associated with edges. Unlike both directed edge-labeled graphs and heterogeneous graphs, annotating edges with additional properties is not straightforward. A property graph is defined as follows.\nDefinition 3 (Property graph [79]). A property graph is a tuple \\( G = (V, E, L, P, U, e, l, p) \\), where \\( V \\subseteq Con \\) is a set of node ids, \\( E \\subseteq Con \\) is a set of edge ids, \\( L \\subseteq Con \\) is a set of labels, \\( P \\subseteq Con \\) is a set of properties, \\( U \\subseteq Con \\) is a set of values, \\( e : E \\rightarrow V \\times V \\) maps an edge id to a pair of node ids, \\( l : V \\cup E \\rightarrow 2^L \\)maps a node or edge id to a set of labels, and \\( p : V \\cup E \\rightarrow 2^{P \\times U} \\) maps a node or edge id to a set of property-value pairs.\nIn contrast to directed edge-labeled graphs and heterogeneous graphs, a property graph allows a node or edge to have several values for a given property. Property graphs can be converted to/from directed edge-labeled graphs, and this process is called reification.\nIn summary, each of these three models has its advantages and disadvantages. Directed edge-labeled graphs offer a simple model, while property graphs provide a more flexible choice. The selection of a model typically depends on practical factors such as available implementations for different models, etc. For a detailed discussion, we recommend readers refer to [79]."}, {"title": "2.1.2 Graphs, Knowledge Graphs, and Ontologies", "content": "We now introduce some popular instances of relational data that have been considered in the machine learning community, including (homogeneous) graphs, knowledge graphs, and ontologies.\nHomogeneous graphs can be viewed as a special case of directed edge-labeled graphs or heterogeneous graphs in which there is only one type of edges and only one type of nodes. Specifically, a homogeneous graph is defined as follows.\nDefinition 4 (Homogeneous graph). A homogeneous graph is a tuple \\( G = (V, E) \\), where \\( V \\) is a set of nodes, and \\( E \\subseteq V \\times V \\) is a set of edges, with each edge connecting two nodes.\nA homogeneous graph is an undirected graph if all edges are bidirectional, meaning that if \\( (V_i, V_j) \\in E \\) holds, then \\( (V_j, V_i) \\in E \\) also holds. For example, in co-author networks, the co-authorship relationship is an undirected edge. Otherwise, the graph is called a directed graph. For example, in citation networks, the citation relationship is a directed edge.\nNote that a homogeneous graph is the minimal model of graph-structured data. To allow for multiple node types, homogeneous graphs can be extended to single-relational graphs, defined as,\nDefinition 5 (Single-relational graph). A single-relational graph is a tuple \\( G = (V, E, L, l) \\), where \\( V \\subseteq Con \\) is a set of nodes, \\( L \\subseteq Con \\) is a set of node labels, \\( E \\subseteq V \\times V \\) is a set of edges, and \\( l : V \\rightarrow L \\) maps each node to a label, where Con denotes a countably infinite set of constants."}, {"title": "2.2 RELATIONAL REPRESENTATION LEARNING", "content": "Given a graph \\( G = (V, E) \\), the goal of graph representation learning is to learn a node mapping function \\( f : V \\rightarrow \\mathbb{R}^d \\), which project each node into low dimensional vectors in space \\( \\mathbb{R}^d \\), where \\( d \\ll |V| \\), while preserving the graph structure and node attributes. The learned representation \\( H \\) can be applied to downstream tasks such as node classification, link prediction, and graph reconstruction."}, {"title": "2.2.2 Graph Neural Networks.", "content": "Graph neural networks (GNNs) are a class of neural networks designed to operate on graph-structured data. GNNs are particularly effective for tasks where relationships or dependencies between data points can be naturally represented as a graph. A more specific type of GNN is known as Graph convolutional networks (GCNs). Given a graph G, a GNN processes node features X and adjacency information A to learn a mapping \\( f : \\mathbb{R}^{|V| \\times d} \\times \\mathbb{R}^{|V| \\times |V|} \\rightarrow \\mathbb{R}^{|V| \\times o} \\), where d is the input feature dimension, o is the output dimension, and \\( |V| \\) is the number of nodes.\nThe GNN processes information in an iterative manner through layers. At each layer, the hidden representations \\( H^{(l+1)} \\) are computed as a function of the previous layer's representations \\( H^{(l)} \\):"}, {"title": "2.3 GEOMETRIC EMBEDDINGS.", "content": "Vector embeddings map objects to a low-dimensional vector space. Vector embeddings have been developed to learn representations of objects that allow for distinguishing relevant differences and ignoring irrelevant variations between objects. When vector embeddings are used to embed structural/relational data, they fail to represent key properties of relational data that cannot be easily modeled in a plain, low-dimensional vector space. For example, relational data may have been defined by applying set operators such as set inclusion and exclusion [199], logical operations such as negation [145], or they may exhibit relational patterns like the symmetry of relations [1] and structural patterns (e.g., trees and cycles) [26, 201].\nGoing beyond plain vector embeddings, geometric relational embeddings replace the vector representations with more advanced geometric objects, such as convex regions [98, 144, 199], density functions [145, 175], elements of hyperbolic manifolds [26], and their combinations [161]. Geometric relational embeddings provide a rich geometric inductive bias for modeling relational/structured data. For example, embedding objects as convex regions allows for modeling not only similarity but also set-based and logical operators such as set inclusion, set intersection [199] and logical negation [215] while representing data on non-Euclidean manifolds allows for capturing complex structural patterns, such as representing hierarchies in hyperbolic space [26]. We group them into three lines of works."}, {"title": "2.3.1 Distribution-based Embeddings", "content": "Probability distributions provide a rich geometry of the latent space. Their density can be interpreted as soft regions and it allows us to model uncertainty, asymmetry, set inclusion/exclusion, entailment, and so on.\nGaussian embeddings. Word2Gauss [172] maps words to multi-dimensional Gaussian distributions over a latent embedding space such that the linguistic properties of the words are captured by the relationships between the distributions. A Gaussian \\( \\mathcal{N}(\\mu, \\Sigma) \\) is parameterized by a mean vector \\( \\mu \\) and a covariance matrix \\( \\Sigma \\) (usually a diagonal matrix for the sake of computing efficiency). The model can be optimized by an energy function \\( -E(N_i, N_j) \\) that is equivalent to the KL-divergence \\( D_{KL}(\\mathcal{N}_j||\\mathcal{N}_i) \\) defined as\n\nKG2E [75] extends this idea to knowledge graph embedding by mapping entities and relations as Gaussians. Given a fact (h,r,t), the scoring function is defined as \\( f(h, r, t) = \\frac{1}{2}(D_{KL}(N_h, N_t) + D_{KL}(N_r, N_t)) \\). The covariances of entity and relation embeddings allow us to model uncertainties in knowledge graphs. While modeling the scores of triples as KL-divergence allows us to capture asymmetry. TransG [191] generalizes KG2E to a Gaussian mixture distribution to deal with multiple relation semantics revealed by the entity pairs. For example, the relation HasPart has at least two latent semantics: composition-related as (Table, HasPart, Leg) and location-related as (Atlantics, HasPart, NewYorkBay)."}, {"title": "3.1 PSEUDO-RIEMANNIAN GRAPH CONVOLUTIONAL NETWORKS", "content": "In this section, we first introduce the background and motivation. Next, we describe how to tackle the g-disconnectedness in pseudo-Riemannian manifolds. Then we present the pseudo-Riemannian GCNs based on the proposed geodesic tools."}]}