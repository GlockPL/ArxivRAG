{"title": "PLM-Based Discrete Diffusion Language Models with Entropy-Adaptive Gibbs Sampling", "authors": ["Hyukhun Koh", "Minha Jhang", "Dohyung Kim", "Sangmook Lee", "Kyomin Jung"], "abstract": "Recently, discrete diffusion language models have demonstrated promising results in NLP. However, there has been limited research on integrating Pretrained Language Models (PLMs) into discrete diffusion models, resulting in underwhelming performance in downstream NLP generation tasks. This integration is particularly challenging because of the discrepancy between step-wise denoising strategy of diffusion models and single-step mask prediction approach of MLM-based PLMs. In this paper, we introduce Diffusion-EAGS, a novel approach that effectively integrates PLMs with the diffusion models. Furthermore, as it is challenging for PLMs to determine where to apply denoising during the diffusion process, we integrate an entropy tracking module to assist them. Finally, we propose entropy-based noise scheduling in the forward process to improve the effectiveness of entropy-adaptive sampling throughout the generation phase. Experimental results show that Diffusion-EAGS outperforms existing diffusion baselines in downstream generation tasks, achieving high text quality and diversity with precise token-level control. We also show that our model is capable of adapting to bilingual and low-resource settings, which are common in real-world applications.", "sections": [{"title": "Introduction", "content": "As diffusion models significantly enhance the quality and diversity of generated outputs in continuous domains such as images and audio (Song et al., 2021b), recent research has increasingly applied diffusion models to NLP (Li et al., 2022; Gong et al., 2023a; Yuan et al., 2023; He et al., 2023). To integrate the diffusion process with traditional language models, research has predominantly followed two approaches: Continuous Diffusion Language Model (CDLM) (Yuan et al., 2023; Lovelace et al., 2023; Chen et al., 2023), which embed text in a continuous latent space, and Discrete Diffusion Language Model (DDLM) (He et al., 2023;\nLou et al., 2024; Zhou et al., 2024; Shi et al., 2024;\nSahoo et al., 2024; Zheng et al., 2024), which operate directly on the vocab space. Recent DDLMs adeptly reflect the highly structured nature of language; this property enables sequence control in natural language, generally resulting in high performance in NLP fields such as unconditional generation and open-ended generation (Lou et al., 2024). However, for DDLMs to be widely adopted across various NLP domains, particularly in tasks such as close-ended conditional generation hereafter referred to as dataset-guided generation - existing DDLMs still exhibit shortcomings in their performance. Our experiments suggest that models such as SEDD falls short in such tasks, potentially limiting their practical applications.\nBetter performance can be achieved by integrating Pretrained Language Models (PLMs) through the use of an encoder-based PLM as the initial setup for embedding functions, leveraging its capability to capture overall semantics (He et al., 2023). However, integrating PLMs into diffusion models is non-trivial as PLMs typically predict masked elements in a single step, whereas diffusion models require step-wise denoising based on the overall semantics of each timestep sequence, and such gap yields limited results. Therefore, we need to consider such inconsistency to effectively adopt PLMs into DDLMs by a new methodology.\nIn this paper, we introduce Diffusion-EAGS, a novel approach that effectively integrates Mask Language Model (MLM)-based PLMs with DDLMs for dataset-guided generation. To address the gap between the step-wise nature of diffusion models and the one-step prediction strategy of PLMs, we begin by considering MLM-based PLMS as the denoising function at each step of the diffusion process. Since identifying where to apply denoising during the diffusion process is challenging for PLMs, we incorporate an entropy tracking module to support their operation. For constructing entropy tracking module, we interpret each denoising step as a constrained Markov Random Field (cMRF), which enables us to adopt adaptive Gibbs sampling, which then facilitates leveraging entropy information from conditional sequences. To train DDLMs to effectively exploit an entropy tracking module, we propose a forward process based on an entropy-based noising scheduling during training phase. Specifically, our entropy-based noising scheduling noises lower entropy tokens first, thereby learning to progressively generate sequences guided by the entropy from entropy tracking module, which enhances the quality of the generated text.\nExperimental results demonstrate that Diffusion-EAGS achieves outstanding performance compared to existing autoregressive model (GPT-2), CDLMs, and DDLMs across various dataset-guided generation tasks. We demonstrate that both PLMs and entropy-based Gibbs sampling contribute to higher performance in our ablation study. Furthermore, our model exhibits higher diversity in certain tasks compared to LLMs and can facilitate token-level generation, indicating the potential applicability of our model across a wide range of dataset-guided generation tasks. In addition, we validate that our model can adapt to both bilingual and low-resource settings, which are frequently encountered in practical applications."}, {"title": "Task Setting", "content": ""}, {"title": "Dataset-Guided Generation", "content": "In sequence-to-sequence (seq2seq) conditional generation tasks, the level of constraint imposed by the dataset plays a critical role in shaping the generation process. As shown in Table 1, constraints can be categorized into three levels: the provision of context alone, the provision of specific content that should be covered in the target sequence, and the provision of content format. Each level represents an increasing degree of constraint in the task requirements. The goal is to develop a model capable of generalizing across a wide range of downstream tasks, rather than being specialized for a specific task, while adhering to the constraints of the given dataset a task we refer to as Dataset-Guided Generation."}, {"title": "DDLM with BERT", "content": "Diffusion Models The process of diffusion models consists of two main stages: the forward process, where noise is added to the input over multiple steps, and the reverse process, where noise is removed to restore the information. There are two types: Continuous and Discrete diffusion models. Discrete diffusion models operate the diffusion process across categorical random variables (Hoogeboom et al., 2021). As discrete diffusion models show promising results in recent studies (Venkatraman et al., 2024; Sahoo et al., 2024; Lou et al., 2024), we set up our language models grounded on discrete diffusion language models.\nBERT as constrained Markov Random Field (cMRF) As the pretraining paradigm has significantly contributed to the success of language models, the integration of PLMs is essential in the development of language models. Nevertheless, adopting PLMs to DDLMs is a complex task, as DDLMs require a step-wise denoising strategy, while MLM-based PLMs are trained to predict masked elements in a single step. To effectively integrate PLMs into DDLMs, it is necessary to resolve such a discrepancy. Therefore, we begin by interpreting the intrinsic characteristics of MLMs as Markov Random Field (MRF), as demonstrated in Wang and Cho (2019). Details are in Appendix A."}, {"title": "Methodology", "content": "We propose Diffusion-EAGS that integrates PLM with DDLMs in dataset-guided generation, through Entropy-based Adaptive Gibbs Sampling (EAGS) and Entropy-based Noise Scheduling (ENS). The overview of our process is in Figure 2."}, {"title": "Generation Process: Entropy-based Adaptive Gibbs Sampling (EAGS)", "content": "Building on the premise in Section 2.2, we propose EAGS to leverage PLM as the function of denoising process in diffusion models. As MLM-based PLMs can be interpreted as cMRF, we can apply Gibbs sampling in each denoising step in DDLM. By Gibbs sampling, we can sample each token $x_l$ from its constrained distribution $p(x_l|X\\{x_l\\};Y)$. Rather than performing Gibbs sampling, we employ EAGS, as the entropy of each variable indicates the amount of information at certain position in the sequence. By generating tokens in the order of decreasing entropy, we prioritize the generation of the least informative parts of the sequence. Such prioritized generation makes a more structured sequences, leveraging the already established syntactic context.\nAccording to Equation 1, we can compute the conditional probability for each masked position. The entropy $H$ for a given token $x_l$ is derived as:\n$H(x_l|Y, f) = \\sum_{m=1}^{M} p(x_l = v_m|x_l, Y, f) log p(x_l = v_m|x_l, Y, f)$\nWith $H$, our approach for T step-generation process can be formalized as follows:\nEntropy Calculation: Compute the entropy $H(x_l | Y, f)$ for each variable $x_l$.\nVariable Selection: Select the variable $x_{l^*}$ with the highest entropy for sampling\n$l^* = arg max_l H (x_l | Y, f)$\nSampling: Sample $x_{l^*}$ from its conditional distribution $p(x_{l^*} | Y, f^*)$.\nUpdate: Update the conditional distributions and entropy for the affected variables.\nIteration: Repeat Steps 1 through 4 until t=T."}, {"title": "Forward Process: Entropy-based Noise Scheduling (ENS)", "content": "To enhance the facilitation of Adaptive Gibbs Sampling in the generation process, it is essential to describe a similar process in the training phase. Therefore, we schedule the forward process of diffusion training based on the entropy $H(x_l)$ of positions l with the input sequence [Y|X]. Specifically, the position-wise entropy is calculated by the PLM that shares a similar language base with diffusion-trained model. Assuming the diffusion process progresses over T steps, at each step t, we mask the $L/T$ number of positions with the lowest entropy from the set {$x_1,...,x_L$}. The masking process at step t in position l is described by the denoising matrix $Q_{tl}$.\n$Q_{tl} = \\begin{bmatrix}\nq_{11} & 0 & \\dots & 0 & q_{1, M} \\\\\n0 & q_{22} & \\dots & 0 & q_{2, M} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & q_{M-1,M-1} & q_{M-1,M} \\\\\n0 & 0 & 0 & q_{MM}\n\\end{bmatrix}$\n, where\n$q_{mm} = \\begin{cases}\n1 & \\text{if } x_l \\notin MIN([H(x_1), \\dots , H(x_L)], \\frac{L}{T}) \\\\\n0 & \\text{if } x_l \\in MIN([H(x_1), \\dots , H(x_L)], \\frac{L}{T}) \\\\\n\\end{cases}$\n$q_{mM} = 1$\nThis entropy-based noise scheduling approach ensures that the forward process in diffusion training closely mirrors the generation process, thereby enhancing the effectiveness of Adaptive Gibbs Sampling in language generation."}, {"title": "Training & Inference", "content": "Our whole process is in Algorithm 1. Distinct from the prevailing methodologies in diffusion models as described by Ho et al. (2020a) and Austin et al. (2023), we do not employ the PLM parameterization strategy $P_\\theta(Z_0|z_t, t)$, which preserves the original semantic embedding spaces during the training phase. We empirically find that PLM parameterization restricts the diversity of generated responses. Instead, to ensure the completeness of sentences, we implement Minimum Bayes Risk (MBR) decoding in the final generation step. We follow the traditional diffusion loss in Equation (Ho et al., 2020b) with changing MSE to Cross Entrpy Loss."}, {"title": "Experiments", "content": ""}, {"title": "Tasks and Datasets", "content": "Question Generation We use the Quasar-T dataset (Dhingra et al., 2017) to generate questions from passages and answers. Paraphrase The QQP dataset (Wang et al., 2017) assesses paraphrase detection by treating one question as a paraphrase of another. Social Datasets The Paradetox dataset (Logacheva et al., 2022) focuses on removing profanities. The Deontology from ETHICS dataset (Hendrycks et al., 2023) evaluates ethical judgment based on scenarios. Open-ended Generation The RocStories (Mostafazadeh et al., 2016) dataset is used to generate coherent story continuations from a given context. Detailed explanations are available in Appendix B and Dataset statistics are shown in Table 8."}, {"title": "Baselines", "content": "Since our objective is centered on generation tasks, we compare Diffusion-EAGS with three categories of baselines with the model similar to the size of RoBERTa-base: AutoRegressive (AR) model, CDLM, and DDLM. For AR model (Vaswani et al., 2023), we employ GPT-2 (Radford et al., 2019), which is renowned for its proficiency in generation tasks. For CDLMs, our baseline includes DiffuSeq (Gong et al., 2023a) and DINOISER (Ye et al., 2024). DiffuSeq is a diffusion model specifically designed for sequence-to-sequence applications, whereas DINOISER adaptively determines the range of sampled noise scales during training. For DDLMs, we utilize DiffusionBERT (He et al., 2022), a state-of-the-art model in the research series of DiffusionLM (Li et al., 2022), and SEDD (Lou et al., 2024), a state-of-the-art model of open-ended generation in diffusion language generation domain. For SEDD, we download the pretrained version and fine-tune on it. Experimental results on LLMs can be found in Appendix C."}, {"title": "Metrics", "content": "Quality metrics To measure the quality of the generated texts, we use Perplexity based-on GPT-2 Large and GPT-2 XL, SOME (Yoshimura et al., 2020), grammar metric based on the neural net, LLM-c (Lin and Chen, 2023) to measure the plausibility of the narratives, LLM-t (Koh et al., 2024) to measure toxicity, and MAUVE (Pillutla et al., 2021), measuring the gap between text distributions despite divergence.\nDiversity Metrics Traditional diversity metrics Self-BLEU (Zhu et al., 2018) and distinct-n (Li et al., 2015) are employed to evaluate the generated texts. We also adopt Vendi Score (VS) (Friedman and Dieng, 2023), an interpretable diversity metric, which quantifies the effective number of unique samples in a given set. Both the n-gram and embedding variations are utilized, where embedding VS interprets diversity of semantics."}, {"title": "Experimental Details", "content": "We employ roberta-base as encoder-based PLM. The learning rate is set at 5e-4, and a naive categorical sampling strategy is adopted. Adapted to data statistics, the maximum lengths for QG, QQP, and Paradetox is set to 64, while for Deontology set to 48. Furthermore, in accordance with the minimum construction length, the number of steps is configured to 5 and 20 size of MBR. We use 1 A100 GPU, and the batch size is set to 256."}, {"title": "Results", "content": "As shown in Table 2, 3, 4, our model consistently exhibits exceptional performance in terms of text quality while simultaneously maintaining diversity when compared to baseline models. As illustrated in Table 2, Diffusion-EAGS generates the responses with the highest PPL score for QG, and highest MAUVE and PPL score for QQP. In Table 3 Paradetox, our model demonstrates superior performance across all evaluated metrics. Such phenomenon represents that our model based on encoder-based PLMs show robustness on diverse perturbations from daily dialogues. When PPL exceeds 1,000, the model is considered to have failed in generating natural sequences, and is thus represented in gray color. In the context of Deontology, our model exceeds the baseline PPL and MAUVE scores, whereas SOME score represent the sufficient quality of text with the highest diversity score of 4.755. Additionally, in Table 4 where semantic consistency is crucial, our model significantly outperforms the original dataset's PPL by 23 points by leveraging PLMs while maintaining a high diversity score of 4.837."}, {"title": "Ours vs AR model", "content": "Diffusion-EAGS demonstrates high level of text quality surpassing that of GPT-2 in Table 2 in text quality. Notably, our model effectively reflects the pattern and style of the dataset, attributed to the capability of encoder-based PLMs. Specifically, as reported in Table 3-ParaDetox, our model excels at capturing semantic information with the highest MAUVE score of 0.733, despite ParaDetox being a challenging colloquial dataset including slang, numerous abbreviations, and various perturbations. In addition, our model demonstrate robustness to such perturbations with 69.5 PPL, while GPT-2 shows a lower performance of 389.1 PPL.\nAs for diversity, our model consistently outperforms GPT models in VS(ngram) and VS(emb) in Table 2, 3, and 4. These results underscore our model's efficacy in generating diverse responses. Specifically, in the context of story generation as shown in Table 4, our model not only demonstrates effectiveness in enhancing both the quality and diversity of the text but also maintains coherence with the contextual storyline. This is further supported by the high LLM-c scores, which confirm the plausibility of the narratives produced by our model."}, {"title": "Ours vs CDLMs", "content": "Notably, CDLMs demonstrate a noticeable deficiency in diversity. In contrast, our model excels at producing significantly more diverse sequences. This enhanced performance can be attributed to the integration of PLM into our framework, which evidently enriches the diversity of the generated outputs. In addition, our model generally achieves higher text quality scores than CDLMs across all experiments. Diffusion-EAGS not only surpasses Diffuseq and DINOISER in PPL and SOME but also achieves a MAUVE score improvement of over 0.4, thus indicating that the generated responses accurately reflect the dataset while preserving grammatical integrity. Furthermore, our models require only a few steps, resulting in a 40% reduction of costs with higher quality and diversity."}, {"title": "Ours vs DDLMs", "content": "Contrary to CDLMs, DiffusionBERT exhibits limitations in text quality, as shown in Table 2, 3, 4. On the other hand, our model achieves significantly higher scores across all quality metrics. This observation suggests that the naive application of BERT within the diffusion process fails to fully harness the capabilities of PLMs. SEDD, an open-ended generation model shows low performance under text generation in highly constrained tasks such as ParaDetox. Even in open-ended generation settings of Table 4, Diffusion-EAGS shows higher text quality and semantic scores. Additionally, our model does not require more than 5 diffusion steps, as it can adaptively recover the absorbing state through an entropy-based generation approach. Above results further exemplify the high performance and the time efficiency of our model."}, {"title": "Analysis", "content": ""}, {"title": "Ablation Study", "content": "To explore the effectiveness of our model's components, we conduct ablation studies focusing on three key elements: Entropy Adaptation, stepwise Gibbs Sampling, and PLM in Table 5.\nThe omission of EAGS initially results in a significant decline in performance, a noticeable decrease in text quality and diversity.\nWe find that EAGS significantly contributes to a gradual entropy decrease, as shown in Appendix H. These observations highlight the critical role of selecting the sampling position based on given information in sequence generation. Additionally, excluding the use of the diffusion generation process, without the stepwise sampling, leads to a drastic reduction in overall performance, with PPL increasing by more than 1000 points and diversity scores dropping below 2. Consequently, our cMRF approach, integrated with the diffusion model, proves indispensable in aiding the PLM in effectively generating sequences. Furthermore, it is evident that without the incorporation of PLMs, the generation of natural sequences is unachievable."}, {"title": "Case Study: Keyword Based Generation", "content": "Our model operating within discrete space enables us to manipulate the output sequences using explicit instructions. To further explore this capability, we conduct the generation of sequences based on keywords positioned in the middle and at the end of masked sequences. Initially, we provide the same contextual input while varying the keywords. In the masked states, we randomly select positions, replacing them with the specified keywords. The results in Table 7 demonstrate that the generated sequences seamlessly integrate the keywords with context-specific semantics.\nThese findings show that our model can effectively integrate specific conditional information in an interpretable manner. Experimental results also indicates its substantial potential and suitability for diverse applications where direct controllability is crucial, such as in story generation."}, {"title": "Bilinguality & Low Resource", "content": "Labeled datasets used in dataset-guided generation tasks are typically limited in size and sometimes multilingual. To assess our model's performance in dataset-guided generation, particularly in terms of language extension and resource scarcity, we conduct additional experiments on a translation task. Specifically, we utilize the 18k en de human-curated dataset by Xu et al. (2024a,b). For our model, we employ a pretrained NLLB (Costa-juss\u00e0 et al., 2022) as a non-autoregressive (NAR) approach for controlling language output separately. This approach is selected due to the difficulty of controlling token generation in a small-scale multilingual BERT, which suffers from interference issues (Shaham et al., 2023).\nWe establish baselines using non-autoregressive (NAR) transformer models, such as Mask-and-Predict (Ghazvininejad et al., 2019a) and Easy-First (Kasai et al., 2020) - which bear resemblance to discrete diffusion models - diffusion models such as Diffuseq-v2 and SEDD, traditional translation models such as mBART-50 (Tang et al., 2020) and NLLB. For evaluation metrics, we utilize sacre-BLEU (Post, 2018) and neural-net scores such as COMET (Rei et al., 2020) and XCOMET (Guerreiro et al., 2023).\nTable 6 shows that predicting the target sequence without leveraging a multilingual PLM proves to be challenging. All diffusion baseline models struggle to produce correct outputs. For example, the pretrained SEDD model fails to effectively leverage conditional information, even after finetuning on the training datasets, consistent with the limitation observed in Section 5.3. Similar challenges arise in NAR transformer baselines. Despite constructing the vocabulary using the pretrained mBART-50 model (DisCo-m), the underlying issues remain. On the other hand, our proposed model, by incorporating a PLM, demonstrates promising results.\nInterestingly, the output of the pretrained NLLB model (NLLB-naive-600M, not finetuned) reveal that neural network-based metrics are susceptible to the interference problem, specifically translated by other languages, even though we provide the language specific token. While such issues result in lower BLEU scores, COMET and XCOMET often interpret them as semantically coherent, indicating a potential direction for future work to improve translation evaluation metrics. Despite these phenomena, a performance gap between translation models and DDLM remains. This suggests that future research should address the semantic capabilities of diffusion models to help bridge this gap. Details are in Appendix G."}, {"title": "Diversity Saturation", "content": "Inspired by the observation that Diffusion-EAGS consistently excel in terms of diversity across all results, we delve further into the diversity capabilities of our model. We assess the diversity performance in dataset-guided generation compared to LLMs. We measure the VS for 5 to 100 generations under a single condition. Such experiment demonstrates the extent to which the model's output diversity saturates, enabling a comparison of asymptotic diversity performance. The experiment is conducted on the 'deontology' dataset which allows high output diversity in its settings. Details of using LLMs are provided in Appendix E.\nFigure 3 demonstrates that the diversity saturation graph for Diffusion-EAGS has a relatively steep slope, while GPT models saturate at lower values. The embedding VS of all GPT series saturates below 13. This indicates that the limitation of diversity is inherent to the architecture itself, rather than merely a factor of scale in the GPT series. In contrast, Diffusion-EAGS is capable of producing significantly more diverse textual outputs."}, {"title": "Related Works", "content": "Efforts to integrate generative flow models into sequence generation exploit the distribution shift from a source language to a target language through a series of invertible linear transformations (Ma et al., 2019; Zhang et al., 2024). However, as DDPM (Ho et al., 2020a) demonstrate the effectiveness of generating images, diffusion models have been a major topic of interest within the field of generative flow models (Song et al., 2021a,b). To apply such diffusion methodologies to NLP, there are two main streams - continuous diffusion models and discrete diffusion models.\nContinuous diffusion models Diffusion-LM (Li et al., 2022) propose a method of mapping tokenized sequences to embedding dimensions guided by a pretrained classifier. DiffuSeq-v1, v2 (Gong et al., 2023a,b) apply partial noising techniques. The core of these diffusion methodologies lies in the addition and restoration of random noise to facilitate generation. However, Cold Diffusion (Bansal et al., 2022) argues that such operations do not necessarily have to be governed by stochastic randomness.\nDiscrete diffusion models Building on this rationale, D3PM (Austin et al., 2023) propose the discrete restoration-generation approach and DiffusionBERT (He et al., 2022) adopt PLMs to DDLM. SEDD(Lou et al., 2024) proposes score entropy inspired by MLM loss. Recent works by Shi et al. (2024) and Sahoo et al. (2024) extend this idea to propose a simplified view of the discrete framework, and obtain better empirical results. Zheng et al. (2024) further closes the gap between discrete diffusion models and masked models. These researches make an improvement on the open ended generation task. Venkatraman et al. (2024) showcase that SEDD priors can be used to sample from a posterior distribution, namely text infilling."}, {"title": "Conclusions", "content": "In this work, we introduce Diffusion-EAGS, integrating encoder-based PLMs with diffusion models for dataset-guided generation. By leveraging stepwise diffusion models and the property of MLM, we demonstrate that MLM-based PLMs can serve as denoising function with Entropy-based Adaptive Gibbs Sampling. We also propose entropy-based noise scheduling during training for adaptive sampling. Experimental results show that Diffusion-EAGS outperforms existing baselines, achieving high diversity and precise token-level control. These results demonstrate its enhanced text generation quality and diversity, as well as wide applicability."}, {"title": "Limitations", "content": "While Diffusion-EAGS demonstrates significant improvements in conditional generation tasks, there are several limitations. First, the quality of the text generated by our model depends on PLMs such as BERT. Second, as our method is currently focused on text generation tasks, its applicability to text classification tasks, such as Named Entity Recognition and Part-of-Speech Tagging, remains unexplored. Future research could explore extending this method to other NLP tasks, as well as investigating the potential for reducing computational costs through more efficient entropy calculation techniques. Additionally, addressing the biases inherent in pretrained models remains a critical area of focus to ensure the robustness and fairness of generated outputs."}, {"title": "Ethical Statements", "content": "A notable advantage of Diffusion-EAGS is its efficient training process, which optimizes computational resources with high performance. This efficiency not only reduces the environmental impact of training PLMs but also makes advanced NLP technologies more accessible. Additionally, we have focused on output control and interpretability through the use of a discrete diffusion model. Consequently, our methodology contributes to the effective control of generated outputs in the future. Diffusion-EAGS's strength in tasks involving conditional generation proves beneficial in the field of text generation following social guidelines, where outputs need to strongly depend on the conditions. Specifically, its high diversity performance enables active utilization in dynamically adapting text generations to meet specific social criteria."}, {"title": "BERT as a MRF", "content": "Let X = ($x_1,...,x_L$) be a sequence of random variables $x_l$, each taking a value from a vocabulary V = {$1, ..., U_M$}. These variables form a fully-connected graph, indicating mutual dependence. To define the MRF, we focus on the full-graph clique potential:\n$\\Phi(X) = exp ( \\sum_{l=1}^{L} (log\\phi_l(X)) )$\nwhere each log-potential $\\phi_l(X)$ is:\n$log \\phi_l (X) = \\begin{cases}\n1h(x_l)f_\\theta(X\\{x_l\\}), \\text{if } [M] \\subseteq X_{l-1:l+1} \\\\\n0, otherwise\n\\end{cases}$\nHere, $f_\\theta(X\\{x_l\\})$ is a function that maps the sequence $X\\{x_l\\}$ to a real-valued vector in $R^M$, and $1h(x_l)$ is a one-hot vector with $x_l$ set to 1. from this log potential, we can find probability of sequence X:\n$p_\\theta(X) = \\frac{1}{Z(\\Theta)} \\prod_{l=1}^{L} \\Phi_l(X)$\nwith the normalization constant $Z(\\Theta)$ defined as:\n$Z(\\Theta) = \\sum_{X'} \\prod_{l=1}^{L} \\Phi_l(X')$\nFor a fixed $X\\{x_l\\}$, the conditional probability of $x_l$ is:\n$p(x_l \\mid X\\{x_l\\}) = \\frac{1}{Z(X\\{x_l\\})} exp(1h(x_l)^Tf_\\theta(X\\{x_l\\}))$\nwhere $Z(X\\{x_l\\})$ is:\n$Z(X\\{x_l\\}) = \\sum_{m=1}^{M} exp(1h(m)^Tf_\\theta(X\\{x_l\\}))$\nBERT uses pseudo log-likelihood learning to handle the intractability of $Z(\\Theta)$:\n$PLL(\\Theta; D) = \\frac{1}{|D|} \\sum_{X \\in D} \\sum_{l=1}^{L} log p(x_l \\mid X\\{x_l\\})$\nwhere D is the training dataset."}, {"title": "Dataset Explanations", "content": "QG The objective of Question Generation (QG) is to generate valid and fluent questions based on a given passage and a specified answer. We employ the Quasar-T dataset, introduced by Dhingra et al. (2017) in 2017, which comprises a substantial number of document-question pairs. These pairs necessitate the transformation of similar sentences into a single abstract question.\nParaphrase The objective of the Quora Question Pairs (QQP) (Wang et al., 2017) is to determine whether two questions are paraphrases of each other. We process the QQP dataset by treating one question as a paraphrase of another, a method commonly employed to assess the effectiveness of diffusion models.\nParadetox The objective of the Paradetox (Logacheva et al., 2022) is to delete the profanities in source sentence. It comprises of toxic and neutral utterances, curated from the Jigsaw, Reddit, and Twitter datasets.\nDeontology The objective of Deontology (Hendrycks et al., 2023) is to to evaluate the capability of models to make ethical judgments from a deontological perspective. The dataset contains scenarios focusing on interpersonal dynamics and everyday occurrences.\nOpen-ended Generation We employ the RocStories dataset (Mostafazadeh et al., 2016) for open ended generation with narrative understanding tasks. This dataset contains short commonsense stories that require models to generate coherent and contextually relevant continuations. Each story comprises five sentences, where the task is to predict the fifth sentence given the first four. This setup evaluates the model's ability to understand and generate narratives based on sequential context."}, {"title": "LLM Evaluation", "content": ""}, {"title": "Experimental Details", "content": "For the case of Diffuseq and Dinoiser (Ye et al., 2024), we followed the official repositories to reproduce the results. Results were sampled multiple times with different seeds to evaluate the diversity. Some deviations are as follows. For max-length, we choose 64 for Paradetox, QG, and QQP, and 48 for Deontology. The values were chosen after examining the training set. As for batch size, we followed the original repositories if the parameter was provided. If not, the batch size was chosen using linear interpolation with the size of the training set. Note that unlike other benchmarks, we experimented with Diffuseq-v2 (Gong et al., 2023b) in translation task for a broader comparison with existing baselines."}, {"title": "Results Interpretations", "content": "Examining the results of Diffuseq, it is evident that the grammar score is comparatively lower than that of other models. This outcome is expected, as the outputs from Diffuseq frequently display inaccurate sentence structures, including duplications of words or phrases. Conversely, the outputs from Dinoiser achieve moderate grammar scores but show limited diversity. This finding, coupled with our additional experiments concerning the beam size during Dinoiser generation, suggests that Dinoiser's performance predominantly relies on memorization."}, {"title": "Details on Translation Results", "content": ""}, {"title": "Comparison Between Easy-First and Our Proposed Method", "content": "Discrete diffusion can be said to inherit ideas from NAR inference algorithm Mask-Predict(Ghazvininejad et al.", "follows": "Easy-First, in each iteration, predicts tokens in each position given previous predictions on the easier positions. There is no strict unmasking process. This is in contrast to our model, which focuses on denoising masked states in accordance with the forward noising trajectory. Furthermore, the inference algorithm, as implemented in the original works (Kasai et al., 2020) do not facilitate the integration of PLMs, which is a crucial component in modern NLP applications. We also bridge the gap between the diffusion framework and language modeling, a direction that have only recently began to gain traction within the research"}]}