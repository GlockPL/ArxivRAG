{"title": "SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection", "authors": ["Dong-Hee Paek", "Seung-Hyun Kong"], "abstract": "Abstract-Recently, 4D Radar has emerged as a crucial\nsensor for 3D object detection in autonomous vehicles, offering\nboth stable perception in adverse weather and high-density\npoint clouds for object shape recognition. However, processing\nsuch high-density data demands substantial computational\nresources and energy consumption. We propose SpikingRTNH,\nthe first spiking neural network (SNN) for 3D object detec-\ntion using 4D Radar data. By replacing conventional ReLU\nactivation functions with leaky integrate-and-fire (LIF) spiking\nneurons, SpikingRTNH achieves significant energy efficiency\ngains. Furthermore, inspired by human cognitive processes, we\nintroduce biological top-down inference (BTI), which processes\npoint clouds sequentially from higher to lower densities. This\napproach effectively utilizes points with lower noise and higher\nimportance for detection. Experiments on K-Radar dataset\ndemonstrate that SpikingRTNH with BTI significantly reduces\nenergy consumption by 78% while achieving comparable de-\ntection performance to its ANN counterpart (51.1% AP3D,\n57.0% APBEV). These results establish the viability of SNNs for\nenergy-efficient 4D Radar-based object detection in autonomous\ndriving systems. All codes are available at https://github.\ncom/kaist-avelab/k-radar.", "sections": [{"title": "I. INTRODUCTION", "content": "The advancement of 4D Radar technology has revolution-\nized autonomous driving perception [1]. 4D Radar precisely\nmeasures the position and movement of surrounding objects\nthrough range, azimuth angle, elevation angle, and Doppler\nfrequency. Unlike camera and LiDAR, 4D Radar exhibits su-\nperior stability under adverse weather conditions such as rain\nand snow [2]. Moreover, compared to conventional 3D Radar\n[3] that mainly focus on object presence recognition, 4D\nRadar provides crucial height information through elevation\nangle measurements, enabling detailed 3D shape recognition\nof objects. Consequently, 4D Radar has become a key sensor\nfor robust 3D object detection [4]\u2013[6].\nAs illustrated in Fig. 1, modern 4D Radars generate rich\nsensor data in two forms: 4D Radar tensor (4DRT) and\n4D Radar point cloud (4DRPC). 4DRPC with 10% density\nprovides approximately 100 times more measurement points\nthan conventional Radar systems, enabling high-precision\nobject shape recognition. Radar Tensor Network with Height\n(RTNH) [2] pioneered 3D object detection using 4D Radar\nby processing the top 10% power signals from the 4DRT.\nSubsequent research has shown that utilizing high-density\npoints (1-5% of measurements) while filtering sidelobe noise\ncan further enhance detection performance [5].\nHowever, 4D Radar-based 3D object detection faces a\nsignificant challenge: the high energy consumption required\nto process dense point cloud data. For instance, RTNH [2]\nrequires approximately 156G multiply-accumulate (MAC)\noperations, consuming 7.16 J of energy per frame during\ninference. This substantial energy requirement can impact the\noverall efficiency and sustainability of autonomous driving\nsystems.\nTo address this challenge, brain-inspired spiking neural\nnetworks (SNNs) have gained attention in autonomous driv-\ning applications [8]\u2013[10]. SNNs employ spike-event-based\ncomputing, performing simple accumulation (AC) operations\nthat substantially reduce energy consumption compared to\nconventional artificial neural networks (ANNs) [11]. The\nenergy efficiency of SNNs has been demonstrated across\nvarious autonomous driving tasks. For example, [12] im-\nplemented an end-to-end autonomous driving pipeline using\nSNNs, achieving up to 98.7% energy reduction in the plan-\nning module compared to state-of-the-art ANN models [13].\nSimilarly, [14] developed an SNN for lane detection using"}, {"title": "II. RELATED WORK", "content": "Spiking neural networks (SNNs) aim to replicate bio-\nlogical neuron behavior in artificial systems. Unlike ANN\nneurons that process continuous values in a temporally\nstatic manner, SNN neurons communicate through discrete\nspikes and incorporate temporal dynamics [8]. While vari-\nous neuron models exist, the leaky integrate-and-fire (LIF)\nmodel has gained widespread adoption due to its biological\nplausibility and implementation efficiency [19]. This spike-\nbased computation eliminates multiplication operations from\nthe conventional multiply-accumulate (MAC) operations in\nANN layers, utilizing accumulation (AC) operations that\nsignificantly improve energy efficiency [11], [20]. In 45 nm\nCMOS technology, a single MAC operation in an ANN\nconsumes approximately 4.6pJ, whereas an AC operation\nin an SNN requires only 0.9pJ [21]. Furthermore, SNNs\nactivate neurons only when spike events occur, unlike ANNS\non GPUs where all neurons remain active [8]. This charac-\nteristic is particularly advantageous for sparse data like point\nclouds, where most neurons remain inactive, maximizing\ncomputational efficiency."}, {"title": "B. Object Detection Networks for 4D Radar", "content": "The development of large-scale datasets spanning diverse\nconditions has been crucial for advancing sensor-based object\ndetection networks [22], [23]. Several 4D Radar datasets\nhave recently emerged, catalyzing research in 4D Radar-based object detection. Astyx [24] pioneered open-access 4D\nRadar point clouds but contains only 0.5K frames, limiting its\nutility for training high-performance networks. VoD [25] and\nTJ4DRadSet [26] provide 8.7K and 7.8K frames respectively,\nbut primarily focus on urban driving scenarios under normal\nweather conditions. K-Radar [2] offers 35K frames of raw\n4D Radar measurements (4DRT), collected across diverse\nroad environments and weather conditions. Dual Radar [6]\nemploys two Radar sensors with both dense and sparse\noutputs to demonstrate the importance of high-density data\nin 3D object detection.\nThese datasets have enabled the development of various\n4D Radar object detection networks. Early works [25]\u2013[27]\nadapted PointPillars architecture [28] to demonstrate the\nfeasibility of 3D object detection using 4D Radar. RTNH\n[2] achieved robust 3D object detection on K-Radar by\nprocessing the top 10% power signals from 4DRT. Recent\nimprovements include filtering sidelobe noise and incorpo-\nrating vertical information through attention mechanisms [5],\nas well as enhancing performance by integrating Doppler\nmeasurements in data accumulation or encoder modules [29],\n[30]. However, these methods rely on GPU-based ANN\nmodels, resulting in high energy consumption. Our work\naddresses this limitation by introducing SpikingRTNH, the\nfirst energy-efficient SNN architecture for 4D Radar-based\n3D object detection."}, {"title": "III. METHODOLOGY", "content": "This section details the proposed SpikingRTNH archi-\ntecture and our inference strategy grounded in biological\ncognitive mechanisms. In Section III-A, we describe the\nconversion of RTNH to an SNN architecture and present\nour network structure. In Section III-B, we introduce our\nbiological top-down inference (BTI) method inspired by\nhuman cognitive processes."}, {"title": "RTNH to SpikingRTNH", "content": "As shown in Fig. 2, Radar Tensor Network with\nHeight (RTNH) [2] processes high-density (approximately\n10%) 4D Radar point clouds (4DRPC) extracted from 4D\nRadar tensors (4DRT). To formally define our approach, we\nfirst represent the 4DRPC as:\n$P = \\{P_i | P_i \\in R^3, i = 1,2,...,N\\},$ (1)\nwhere each point $p_i = (x_i, y_i, z_i, pw_i, dop_i)$ represents 3D co-\nordinates, power, and Doppler measurements, and $N$ denotes\nthe total number of points. RTNH first voxelizes these points\nand applies a 3D convolution layer to generate the first-stage\nfeature map:\n$FM_1 \\in R^{C_1 \\times Z_1 \\times Y_1 \\times X_1}$ (2)"}, {"title": "Neuron Model", "content": "SpikingRTNH adopts the leaky\nintegrate-and-fire (LIF) neuron model [19], which offers both\nbiological plausibility and implementation efficiency. An LIF\nneuron integrates input spikes over time into a membrane\npotential and fires a spike when the potential exceeds a"}, {"title": "RTNH", "content": "where $C_1, Z_1, Y_1, X_1$ represent the channel and spatial dimen-\nsions of $FM_1$, respectively.\nRTNH processes this feature map through three stages\nof 3D convolutions, batch normalization, and ReLU ac-\ntivations (see Fig. 2), generating three feature maps\n($FM_1, FM_2, FM_3$):\n$FM_j = f_{3D,j-1}(FM_{j-1}),$ (3)\n$f_{3D}(\\cdot) = ReLU(Conv3D(\\cdot; K) + b),$ (4)\nwhere $j$ indexes each feature map and $K, b$ denote the 3D\nconvolution kernel and batchnorm bias, respectively. The\nlast convolution layer has a stride of 2, reducing spatial\ndimensions by a factor of $2^3 = 8$; this is repeated three times\n($n = 3$) to achieve a wide receptive field [31].\nTo utilize these 3D feature maps collectively, RTNH\nprojects each map onto a bird's-eye-view (BEV) plane using\na 3D convolution with kernel size equal to the feature\nmap's $Z$ dimension, followed by a 2D transpose convolution\nthat unifies them into a $Y_1 \\times X_1$ sized BEV format. The\nresulting three BEV feature maps are concatenated [32], and\na detection head [33] predicts the final objects."}, {"title": "LIF", "content": "threshold. The membrane potential $u$ at time $t$ satisfies:\n$\\tau \\frac{du}{dt} = -(u-U_{reset})+R \\cdot I(t), u <V_{th},$ (5)\nfire a spike and $u = U_{reset}, u \\geq V_{th},$ (6)\nwhere $\\tau, R, I$, and $V_{th}$ denote the time constant, resistance,\npre-synaptic input, and firing threshold, respectively. Follow-\ning common practice [16], $U_{reset}$ is set to 0.\nFor implementation in deep learning frameworks like Py-\nTorch, we reformulate the continuous equations into discrete\ntime:\n$u_k[t+1] = \\lambda (u_k[t] \u2013 V_{th}o_k[t]) + \\sum w_{kl}o_i[t]+b_k,$ (7)\n$o_k[t] = H (u_k[t] - V_{th}),$ (8)\nwhere $\\lambda = 1 - \\frac{\\Delta t}{\\tau}$ is a decay factor (typically set to 0.25 [16],\n[34]), and $k,l$ index the neurons. $W_{kl}$ and $b_k$ correspond to\nthe kernel and bias in (4), and $o_k[t]$ represents the spike event\nat time step $t$ from the $k$-th neuron.\nThe Heaviside step function $H(\\cdot)$ in (8) presents a\nchallenge for gradient-based learning due to its non-\ndifferentiability. Following [34], we approximate it with a\nsurrogate function:\n$H(u_k[t] - V_{th}) \\approx \\frac{1}{2} \\tanh (\\beta (u_k[t] - V_{th})) + \\frac{1}{2}$ (9)\nwhere $\\beta$ (set to 5.0 [34]) controls the steepness of the\napproximation.\nSpikingRTNH. As illustrated in Fig. 2, SpikingRTNH main-\ntains the core RTNH pipeline while replacing all ReLU acti-"}, {"title": "B. Biological Top-down Inference", "content": "Cognitive Mechanism. Human cognitive mechanism in-\nvolves both bottom-up and top-down processes [17]. Bottom-\nup processing progresses from basic sensory input to com-\nplex features, while top-down processing starts with overall\ncontext and moves to specific details. Top-down processing,\ninfluenced by prior knowledge, enables controlled attention\nand reduces cognitive load [18]. When searching for critical\nobjects, humans first identify broad features before focusing\non specific details [35].\nBiological Top-down Inference. Inspired by this top-down\napproach\u2014starting from the complete information set before\nfocusing on essential components\u2014we propose biological\ntop-down inference (BTI). This method sequentially pro-\ncesses 4DRPC from higher to lower densities. As shown\nin Fig. 1, lower-density 4DRPC contains fewer total points\nbut also less noise, making it more representative of actual\ntargets.\nBTI processes point clouds sequentially from higher to\nlower density. At discrete time step $t$, the next time step's\n4DRPC density is determined by selecting the top $r\\%$ of\npower:\n$PW_{th} =$ Percentile$_{100\u2013r}(\\{p_i | i = 1,2,...,N\\}),$ (10)\n$P_{t+1} = \\{p_i | P_i \\in P_t, pw_i \\geq PW_{th}\\}, t = 1,2,..., T \u2013 1,$ (11)\nwhere $P_t$ represents the 4DRPC input to SpikingRTNH at\ntime $t$, and $T$ is the total number of time steps. For example,\nwith $r = 80\\%$, $P_3$ has 64% of the density of $P_1$. At each\ntime step, the network processes the 4DRPC of the given\ndensity to extract feature maps, and the final detection results\nare obtained from the last time step's feature maps (i.e.,\nConcatenated$FM_{t=T}$).\nBTI offers several advantages: (1) it improves detection\naccuracy by initially capturing broader context with high\ndensity 4DRPC before refining details with lower-density\ndata, (2) it enhances detection robustness by utilizing noise-\nreduced lower-density 4DRPC, and (3) it reduces computa-\ntional load compared to processing 100% density points over\nmultiple time steps, thereby improving overall efficiency."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we validate the performance of the pro-\nposed SpikingRTNH. First, Section IV-A presents our exper-\nimental setup. Section IV-B reports comprehensive compar-\nisons between SpikingRTNH and RTNH. Finally, Section IV-\nC analyzes each component of BTI through ablation studies."}, {"title": "A. Experimental Setup", "content": "Dataset and Metrics. We conducted experiments on the K-\nRadar dataset [2], which provides 35K frames of 4D Radar\ndata collected under various weather conditions (clear, sleet,\nsnow) and road environments (urban, highway). We chose\nK-Radar as it is currently the only benchmark dataset that\nprovides 4DRT under adverse weather conditions. Following\nthe revised protocol\u00b9 of the original paper, we adopted\ntwo evaluation metrics for 3D object detection: Average\nPrecision (AP) in 3D (AP3D) and BEV (APBEV) for the\nSedan class at an IoU threshold of 0.3. Additionally, we used\nthe thop library [36] to measure computational requirements\naccurately.\nImplementation Details. We implemented SpikingRTNH\nusing PyTorch 1.12.0 and trained it on an NVIDIA RTX3090\nGPU. We used the AdamW optimizer with a learning rate of\n0.001 and weight decay of 0.01. Based on ablation studies,\nwe set the hyperparameters for BTI to $r = 80\\%$ and $T = 3$\ntime steps."}, {"title": "B. Comparison of SpikingRTNH to RTNH", "content": "Detection Performance. Table I and Fig. 3 present both\nquantitative and qualitative 3D object detection results on\nthe K-Radar test set. SpikingRTNH (T = 1) achieves perfor-\nmance comparable to its ANN counterpart RTNH, with only\na slight decrease in AP3D (48.1% vs 50.7%) and APBEV\n(55.3% vs 56.5%). This demonstrates that energy-efficient\nSNNs can effectively perform 4D Radar object detection.\nAs shown in Fig. 3, SpikingRTNH maintains robust detec-\ntion capabilities across various weather conditions, including\nnormal, sleet, and heavy snow scenarios.\nWhen applying BTI, SpikingRTNH improves upon the\nsingle time step version, achieving gains of 3.0% and 1.7%\nin AP3D and APBEV respectively. The qualitative results in\nFig. 3 clearly demonstrate the progressive improvement in\ndetection quality (i.e., preventing false alarms) as the number\nof time steps increases from T = 1 to T = 3, validating our\nhypothesis that BTI effectively utilizes lower density points\nwith reduced noise to enhance detection performance.\nEnergy Analysis. In 45nm CMOS technology, a single MAC\noperation in an ANN consumes approximately 4.6 pJ, while\nan AC operation in an SNN requires only 0.9 pJ [21]. Based\non these measurements, RTNH consumes 7.16 J/frame due to"}, {"title": "C. Ablation Studies", "content": "We conducted ablation studies to analyze the impact of\ntwo key parameters of BTI: the number of time steps T\nand the density ratio r. Table II presents the performance\nvariations across different parameter configurations.\nIncreasing the number of time steps T generally improves\ndetection performance. The density ratio r shows optimal\nperformance at 80% at T = 3, indicating that retaining\n80% of points between consecutive time steps effectively\npreserves crucial information while progressively reducing\nnoise.\nThe energy consumption naturally increases with T, but\neven at T = 3, SpikingRTNH+BTI maintains significantly\nlower energy requirements compared to RTNH. This demon-\nstrates that our approach successfully balances detection\naccuracy and energy efficiency."}, {"title": "V. CONCLUSION", "content": "This paper presents SpikingRTNH, the first spiking neural\nnetwork architecture for 4D Radar-based 3D object detec-\ntion. By replacing conventional ReLU activations with leaky\nintegrate-and-fire (LIF) neurons and introducing biological\ntop-down inference (BTI), we achieve substantial improve-\nments in energy efficiency while maintaining detection per-"}]}