{"title": "Anytime Multi-Agent Path Finding with an Adaptive Delay-Based Heuristic", "authors": ["Thomy Phan", "Benran Zhang", "Shao-Hung Chan", "Sven Koenig"], "abstract": "Anytime multi-agent path finding (MAPF) is a promising approach to scalable path optimization in multi-agent systems. MAPF-LNS, based on Large Neighborhood Search (LNS), is the current state-of-the-art approach where a fast initial solution is iteratively optimized by destroying and repairing selected paths of the solution. Current MAPF-LNS variants commonly use an adaptive selection mechanism to choose among multiple destroy heuristics. However, to determine promising destroy heuristics, MAPF-LNS requires a considerable amount of exploration time. As common destroy heuristics are non-adaptive, any performance bottleneck caused by these heuristics cannot be overcome via adaptive heuristic selection alone, thus limiting the overall effectiveness of MAPF-LNS in terms of solution cost. In this paper, we propose Adaptive Delay-based Destroy-and-Repair Enhanced with Success-based Self-Learning (ADDRESS), as a single-destroy-heuristic variant of MAPF-LNS. ADDRESS applies restricted Thompson Sampling to the top-K set of the most delayed agents to select a seed agent for adaptive LNS neighborhood generation. We evaluate ADDRESS in multiple maps from the MAPF benchmark set and demonstrate cost improvements by at least 50% in large-scale scenarios with up to a thousand agents, compared with the original MAPF-LNS and other state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "A wide range of real-world applications like goods transportation in warehouses, search and rescue missions, and traffic management can be formulated as Multi-Agent Path Finding (MAPF) problem, where the goal is to find collision-free paths for multiple agents with each having an assigned start and goal location. Finding optimal solutions, w.r.t. minimal flowtime or makespan is NP-hard, which limits scalability of most state-of-the-art MAPF solvers [20, 24, 29].\nAnytime MAPF based on Large Neighborhood Search (LNS) is a promising approach to finding fast and high-quality solutions to the MAPF problem within a fixed time budget [14]. Given an initial feasible solution and a set of destroy heuristics, LNS iteratively destroys and replans a fixed number of paths, according to an agent neighborhood, until the time budget runs out. MAPF-LNS represents the current state-of-the-art in anytime MAPF and has been experimentally shown to scale up to scenarios with hundreds of agents [14]. Due to its increasing popularity, several extensions have"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Multi-Agent Path Finding (MAPF)", "content": "We focus on maps as undirected unweighted graphs G = (V,E), where vertex set V contains all possible locations and edge set E contains all possible transitions or movements between adjacent locations. An instance I consists of a map G and a set of agents A = {a_1, ..., a_m} with each agent a_i having a start location s_i \u2208 V and a goal location g_i \u2208 V. At every time step t, all agents can move along the edges in E or wait at their current location [26].\nMAPF aims to find a collision-free plan for all agents. A plan P = {P_1,...,P_m} consists of individual paths p_i = (P_{i,1},..., P_{i,l(p_i)}) per agent a_i, where (P_{i,t}, P_{i,t+1}) = (P_{i,t+1}, P_{i,t}) \u2208 E, P_{i,1} = s_i, P_{i,l(p_i)} = g_i, and l(p_i) is the length or travel distance of path p_i. The delay del(p_i) of path p_i is defined by the difference of path length l(p_i) and the length of the shortest path from s_i to g_i w.r.t. map G.\nIn this paper, we consider vertex conflicts (a_i, a_j, v, t) that occur when two agents a_i and a_j occupy the same location v \u2208 V at time step t and edge conflicts (a_i, a_j, u, v, t) that occur when two agents a_i and a_j traverse the same edge (u, v) \u2208 E in opposite directions at time step t [26]. A plan P is a solution, i.e., feasible when it does not have any vertex or edge conflicts. Our goal is to find a solution that minimizes the flowtime \u2211_{p \u2208 P} l(p) which is equivalent to minimizing the sum of delays or (total) cost c(P) = \u2211_{p \u2208 P} del(p). In the context of anytime MAPF, we also consider the Area Under the Curve (AUC) as a measure of how quickly we approach the quality of our final solution."}, {"title": "2.2 Anytime MAPF with LNS", "content": "Anytime MAPF searches for solutions within a given time budget. The solution quality monotonically improves with increasing time budget [8, 14].\nMAPF-LNS based on Large Neighborhood Search (LNS) is the current state-of-the-art approach to anytime MAPF and shown to scale up to large-scale scenarios with hundreds of agents [11, 14]. Starting with an initial feasible plan P, e.g., found via prioritized planning (PP) from [25], MAPF-LNS iteratively modifies P by destroying N < m paths of the neighborhood A_N \u2286 A. The destroyed paths P- \u2286 P are then repaired or replanned using PP to quickly generate new paths P+. If the new cost c(P+) is lower than the previous cost c(P-), then P is replaced by (P \\ P-) \u222a P+, and the search continues until the time budget runs out. The result of MAPF-LNS is the last accepted solution P with the lowest cost so far.\nMAPF-LNS uses a set of three destroy heuristics, namely a random uniform selection of N agents, an agent-based heuristic, and a map-based heuristic [14]. The agent-based heuristic generates a neighborhood, including a seed agent a_j with the current maximum delay and other agents, determined via random walks, that prevent a_j from achieving a lower delay. The map-based heuristic randomly chooses a vertex v \u2208 V with a degree greater than 2 and generates a neighborhood of agents moving around v. All heuristics are randomized but non-adaptive since they do not adjust the degree of randomization or distributions w.r.t. prior improvements made to the solution.\nThe original MAPF-LNS uses an adaptive selection mechanism \u03c0, like roulette wheel selection, to choose destroy heuristics based on previous improvements made to the solution P [14, 21]."}, {"title": "2.3 Multi-Armed Bandits", "content": "Multi-armed bandits (MABs) or simply bandits are fundamental decision-making problems, where an MAB or selection algorithm \u03c0 repeatedly chooses an arm k among a given set of arms or options {1, ..., K} to maximize an expected reward of a stochastic reward function R(k) := X_k, where X_k is a random variable with an unknown distribution f_X [2]. To solve an MAB, one has to determine an optimal arm k*, which maximizes the expected reward E[X_k]. The MAB algorithm \u03c0 has to balance between exploring all arms k to accurately estimate E[X_k] and exploiting its knowledge by greedily selecting the arm k with the currently highest estimate of E[X_k]. This is known as the exploration-exploitation dilemma, where exploration can find arms with higher rewards but requires more time for trying them out, while exploitation can lead to fast convergence but possibly gets stuck in a poor local optimum. We will focus on Thompson Sampling and \u03b5-Greedy as MAB algorithms and explain them in Section 4.2."}, {"title": "3 Related Work", "content": ""}, {"title": "3.1 Multi-Armed Bandits for LNS", "content": "In recent years, MABs have been used to tune learning and optimization algorithms on the fly [3, 9, 23]. Besides roulette wheel selection, UCB1 and \u03b5-Greedy are commonly used for destroy heuristic selection in LNS in traveling salesman problems (TSP), mixed integer linear programming (MILP), and vehicle routing [6, 9]. In most cases, a heavily engineered reward function with several weighted terms is used for training the MAB. Recently, a MAPF-LNS variant, called BALANCE, has been proposed to adapt the neighborhood size N along with the destroy heuristic choice using a bi-level MAB approach [18].\nInstead of adapting the destroy heuristic selection, we propose a single adaptive destroy heuristic, thus simplifying the high-level MAPF-LNS procedure (Figure 1). Our destroy heuristic uses restricted Thompson Sampling with simple binary rewards to select a seed agent from the top-K set of the most delayed agents for LNS neighborhood generation."}, {"title": "3.2 Machine Learning in Anytime MAPF", "content": "Machine learning has been used in MAPF to directly learn collision-free path finding, to guide node selection in search trees, or to select appropriate MAPF algorithms for certain maps [1, 10, 12, 17, 22]. [11, 28] propose machine learning-guided variants of MAPF-LNS, where neighborhoods are generated by non-adaptive procedures, e.g., the destroy heuristics of [14]. The neighborhoods are then selected via an offline trained model. Such methods cannot adapt during search and require extensive prior efforts like data acquisition, model training, and feature engineering.\nWe focus on adaptive approaches to MAPF-LNS using online learning via MABs. Our adaptive destroy heuristic can adjust on the fly via binary reward signals, indicating a successful or failed improvement of the solution quality. The rewards are directly obtained from the LNS without any prior data acquisition or expensive feature engineering."}, {"title": "4 Adaptive Delay-Based MAPF-LNS", "content": "We now introduce Adaptive Delay-based Destroy-and-Repair Enhanced with Success-based Self-Learning (ADDRESS) as a simplified yet effective variant of MAPF-LNS."}, {"title": "4.1 Original Agent-Based Destroy Heuristic", "content": "Our adaptive destroy heuristic is inspired by the agent-based heuristic of [14], which is empirically confirmed to be the most effective standalone heuristic in most maps [14, 18].\nThe idea is to select a seed agent a_j \u2208 A, whose path p_j \u2208 P has a high potential to be shortened, indicated by its delay del(p_j). A random walk is performed from a random position in p_j to collect N - 1 other agents a_i whose paths p_i are crossed by the random walk, indicating their contribution to the delay del(p_j), to generate a neighborhood A_N \u2286 A of size |A_N| = N < m for LNS destroy-and-repair.\nThe original destroy heuristic of [14] greedily selects the seed agent with the maximum delay max_{p_i \u2208 P}del(p_i). To avoid repeated selection of the same agent, the original heuristic maintains a tabu list, which is emptied when all agents have been selected or when the current seed agent a_j has no delay, i.e., del(p_j) = 0. Therefore, the heuristic has to iterate over all agents a_i \u2208 A in the worst case, which is time-consuming for large-scale scenarios with many agents, introducing a potential performance bottleneck. The original MAPF-LNS cannot overcome this bottleneck because it only adapts the high-level heuristic selection via \u03c0, as shown in Figure 1, and thus can only switch to other (less effective) destroy heuristics as an alternative."}, {"title": "4.2 ADDRESS Destroy Heuristic", "content": "Our goal is to overcome the limitation of the original agent-based destroy heuristic, and consequently of MAPF-LNS, using MABs. We model each agent a_i \u2208 A as an arm i and maintain two counters \u03b1_i > 0 for successful cost improvements, and \u03b2_i > 0 for failed cost improvements per agent. Both counters represent the parameters of a Beta distribution Beta(\u03b1_i, \u03b2_i), which estimates the potential of an agent a_i \u2208 A to improve the solution as a seed agent. Beta(\u03b1_i, \u03b2_i) has a mean of  $\\frac{\\alpha_i}{\\alpha_i + \\beta_i}$ and is initialized with \u03b1_1 = 1 and \u03b2_1 = 1, which corresponds to an initial 50:50 chance estimate that an agent a_i could improve the solution if selected as a seed agent [5].\nSince the number of agents m can be large, a naive MAB would need to explore an enormous arm space, which poses a similar bottleneck as the tabu list approach of the original agent-based heuristic (Section 4.1). Thus, we restrict the agent selection to the top-K set A_K \u2286 A of the most delayed agents with K < m to ease exploration.\nThe simplest MAB is \u03b5-Greedy, which selects a random seed agent a_i \u2208 A_K with a probability of \u03b5 \u2208 [0, 1], and the agent with the highest expected success rate  $\\frac{\\alpha_i}{\\alpha_i + \\beta_i}$  with the complementary probability of (1 \u2212 \u03b5).\nWe propose a restricted Thompson Sampling approach to select a seed agent from A_K. For each agent a_i \u2208 A_K within the top-K set, we sample an estimate q_i ~ Beta(\u03b1_i, \u03b2_i) of the cost improvement chance and select the agent with the highest sampled estimate q_i. Thompson Sampling is a Bayesian approach with Beta(1, 1) being the prior distribution of the successful improvement rate and Beta(\u03b1_i, \u03b2_i) with updated parameters \u03b1_i and \u03b2_i being the posterior distribution [5, 27].\nOur destroy heuristic, called ADDRESS heuristic, first sorts all agents w.r.t. their delays to determine the top-K set A_K \u2286 A of the most delayed agents. Restricted Thompson Sampling is then applied"}, {"title": "4.3 ADDRESS Formulation", "content": "We now integrate our ADDRESS heuristic into the MAPF-LNS algorithm [14]. For a more focused search, we propose a simplified variant, called ADDRESS, which only uses our adaptive destroy heuristic instead of adaptively selecting among a set of multiple heuristics via time-consuming exploration, as illustrated in Figure 1.\nADDRESS iteratively invokes our adaptive destroy heuristic of Algorithm 1 with the parameters (\u03b1_i, \u03b2_i)_{1<i<m} to select a seed agent a_j \u2208 A and generate an LNS neighborhood A_N \u2286 A using the random walk procedure of the original MAPF-LNS [14]. Afterward, the standard destroy-and-repair operations of MAPF-LNS are performed on the neighborhood A_N to produce a new solution. If the new solution has a lower cost than the previous solution P, then \u03b1_j is incremented and the previous solution is replaced. Otherwise, \u03b2_j is incremented. The whole procedure is illustrated in Figure 2.\nThe full formulation of ADDRESS is provided in Algorithm 2, where I represents the instance to be solved. The parameters (\u03b1_i, \u03b2_i)_{1<i<m} are all initialized with 1 to ensure an initial uniform distribution for all agents."}, {"title": "4.4 Conceptual Discussion", "content": "ADDRESS is a simple and adaptive approach to scalable anytime MAPF. The adaptation is controlled through the parameters \u03b1_i and \u03b2_i per agent a_i, and the top-K ranking of potential seed agents. Our ADDRESS heuristic can significantly improve MAPF-LNS, overcoming the performance bottleneck of the original agent-based heuristic of [14] by (1) selecting seed agents via MABs instead of greedily, and (2) restricting the selection to the top-K set of the most delayed agents A_K to ease exploration.\nThe parameters \u03b1_i and \u03b2_i enable the seed agent selection via Thompson Sampling, which considers the improvement success rate under uncertainty using Bayesian inference [27]. Unlike prior MAB-enhanced LNS approaches, ADDRESS only uses binary rewards denoting success or failure, thus greatly simplifying our MAB approach compared to [6, 7, 9, 18].\nThe top-K set enables efficient learning by reducing the number of options for Thompson Sampling, which otherwise would require time-consuming exploration of all agents a_i \u2208 A. The top-K set also enables fast adaptation by filtering out seed agent candidates whose paths were significantly shortened earlier. While the top-K ranking causes some overhead due to sorting agents, our experiments in Section 5 suggest that the sorting overhead is outweighed by the performance gains in large-scale scenarios.\nOur single-destroy-heuristic approach enables a more focused search toward high-quality solutions without time-consuming exploration of non-adaptive (and less effective) destroy heuristics. Due to its simplicity, our ADDRESS heuristic can be easily applied to other problem classes, such as TSP or MILP, if LNS uses so-called worst or critical destroy heuristics, focusing on high-cost variables that \"spoil\" the structure of the solution [6, 7, 9, 19]."}, {"title": "5 Experiments", "content": "Maps We evaluate ADDRESS on five maps from the MAPF benchmark set of [26], namely (1) a Random map (random-32-32-10), (2) two game maps Ost003d and (3) Den520d, (4) a Warehouse map (warehouse-20-40-10-2-2), and (5) a City map (Paris_1_256). All maps have different sizes and structures. We conduct all experiments on the available 25 random scenarios per map."}, {"title": "5.1 Experiment \u2013 Choice of K", "content": "Setting We run ADDRESS with Thompson Sampling and \u03b5-Greedy to evaluate different choices of K \u2208 {8, 16, 32, 64, 128, 256} on the Den520d and City map with m = 700 and a time budget of 60 seconds. The results are compared with MAPF-LNS using only the agent-based heuristic of [14], as a non-adaptive variant.\nResults The results are shown in Figure 3. ADDRESS with Thompson Sampling always performs best when K < 256. However, ADDRESS is more sensitive to K when using \u03b5-Greedy, which only outperforms the original agent-based heuristic, when 8 < K < 64. Both ADDRESS variants work best when K = 32.\nDiscussion The results indicate that both ADDRESS variants with either Thompson Sampling or \u03b5-Greedy can notably outperform the original agent-based heuristic of MAPF-LNS with sufficient restriction via K < m. Thompson Sampling is more robust regarding the choice of K."}, {"title": "5.2 Experiment \u2013 Delay-Based Heuristics", "content": "Setting Next, we evaluate the search progress of ADDRESS with Thompson Sampling and \u03b5-Greedy for different time budgets on the Den520d and City map with m = 700. The results are compared with MAPF-LNS using only the agent-based heuristic, as a non-adaptive variant.\nResults The results are shown in Figure 4. Both ADDRESS variants outperform the agent-based MAPF-LNS by always achieving lower sums of delays and AUC values, which indicate that ADDRESS always improves faster than the original agent-based heuristic. Thompson Sampling always performs at least as well as \u03b5-Greedy."}, {"title": "5.3 Experiment \u2013 ADDRESS and MAPF-LNS", "content": "Setting We compare ADDRESS with the original MAPF-LNS using all non-adaptive destroy heuristics of [14] described in Section 2.2 for different time budgets on the Den520d, Warehouse, and City map with m = 700. To evaluate the dominance of our ADDRESS heuristic over all non-adaptive heuristics, we introduce a MAPF-LNS variant including all commonly used destroy heuristics, as well as our own.\nResults The results are shown in Figure 5. ADDRESS outperforms both MAPF-LNS variants. MAPF-LNS, with our ADDRESS heuristic, performs second best in Den520d and generally in the other maps when the time budget \u2264 30 seconds. Using our ADDRESS heuristics always leads to a lower average AUC when the time budget is lower than 120 seconds. The selection weights of MAPF-LNS indicate that our ADDRESS heuristic is the dominant destroy heuristic, as it is quickly preferred over all other heuristics.\nDiscussion The results confirm that our ADDRESS heuristic is more effective than the non-adaptive destroy heuristics [14], as it is consistently preferred by the original MAPF-LNS within less than 10 seconds of runtime. MAPF-LNS, with our ADDRESS heuristic, generally underperforms ADDRESS since it additionally explores the weaker destroy heuristics, whereas ADDRESS directly optimizes the seed agent for neighborhood generation without spending any time on less effective destroy heuristics."}, {"title": "5.4 Experiment \u2013 State-of-the-Art Comparison", "content": "Setting Finally, we compare ADDRESS with the original MAPF-LNS, MAPF-LNS2 (which finds feasible solutions by minimizing collisions), BALANCE, and LaCAM*. We run all algorithms on the Random, Ost003d, Den520d, Warehouse, and City maps with different numbers of agents m and a time budget of 60 seconds."}, {"title": "6 Conclusion", "content": "We presented ADDRESS as a simplified variant of MAPF-LNS. ADDRESS applies restricted Thompson Sampling to the top-K set of the most delayed agents to select a seed agent for LNS neighborhood generation. ADDRESS only uses a single adaptive destroy heuristic, as depicted in Figure 1, thus avoiding time-consuming exploration of multiple non-adaptive destroy heuristics.\nOur experiments show that ADDRESS significantly outperforms state-of-the-art anytime MAPF algorithms like the original MAPF-LNS, BALANCE, and LaCAM* in large-scale scenarios with up to a thousand agents. The effectiveness of our destroy heuristic is confirmed by its superior performance and AUC compared with the previous agent-based destroy heuristic in MAPF and the strong preference by the original MAPF-LNS over all other commonly used destroy heuristics. The combination of Thompson Sampling and the top-K ranking of the most delayed agents enables efficient learning and a stronger focus on promising seed agent candidates through constant adaptation and filtering of candidates whose paths were significantly shortened over time. ADDRESS with \u03b5-Greedy can also outperform state-of-the-art anytime MAPF with slightly weaker performance than Thompson Sampling, indicating that any MAB could be used, which we want to investigate in the future.\nMore future work includes the investigation of agent abstraction and the application of our ADDRESS heuristic to other problem classes, such as TSP or MILP, where variables can be sorted w.r.t. to some worst or critical cost measure [6, 7, 9, 19]."}]}