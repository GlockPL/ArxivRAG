{"title": "Pistis-RAG: A Scalable Cascading Framework Towards Trustworthy Retrieval-Augmented Generation", "authors": ["Yu Bai", "Yukai Miao", "Li Chen", "Dan Li", "Yanyu Ren", "Hongtao Xie", "Ce Yang", "Xuhui Cai"], "abstract": "In Greek mythology, Pistis symbolized good faith, trust, and reliability, echoing the core principles of RAG in LLM systems. Pistis-RAG, a scalable multi-stage framework, effectively addresses the challenges of large-scale retrieval-augmented generation (RAG). Each stage plays a distinct role: matching refines the search space, pre-ranking prioritizes semantically relevant documents, and ranking aligns with the large language model's (LLM) preferences. The reasoning and aggregating stage supports the implementation of complex chain-of-thought (CoT) methods within this cascading structure.\nWe argue that the lack of strong alignment between LLMs and the external knowledge ranking methods used in RAG tasks is relevant to the reliance on the model-centric paradigm in RAG frameworks. A content-centric approach would prioritize seamless integration between the LLMs and external information sources, optimizing the content transformation process for each specific task.\nCritically, our ranking stage deviates from traditional RAG approaches by recognizing that semantic relevance alone may not directly translate to improved generation. This is due to the sensitivity of the few-shot prompt order, as highlighted in prior work [6]. Current RAG frameworks fail to account for this crucial factor.\nWe introduce a novel ranking stage specifically designed for RAG systems. It adheres to information retrieval principles while considering the unique business scenario captured by LLM preferences and user feedback. Our approach integrates in-context learning (ICL) methods and reasoning steps to incorporate user feedback, ensuring efficient alignment.\nExperiments on the MMLU benchmark demonstrate a 9.3% performance improvement. The model and code will be open-sourced on GitHub. Experiments on real-world, large-scale data validate our framework's scalability.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, LLMs have demonstrated remarkable capabilities in natural language understanding and generation tasks. These models, trained on vast datasets representing a wide array of human knowledge, excel at generating coherent and contextually relevant text across various domains [1]. Despite their advancements, LLMs are inherently limited by the scope and quality of their training data. They cannot often incorporate real-time or domain-specific knowledge not present in their training corpus, leading to challenges in accuracy and relevance, particularly in dynamic environments.\nTo mitigate these limitations, RAG has emerged as a promising paradigm. RAG enhances LLMs by augmenting their input with additional information retrieved from external sources at inference time. This approach not only enriches the context of LLM-generated responses but also enables them to leverage up-to-date and domain-specific knowledge without the need for extensive retraining [cite needed]. By integrating ICL from retrieved knowledge, LLMs can adapt and perform better on tasks requiring specialized knowledge or current events.\nCurrent RAG frameworks face significant challenges that hinder their widespread adoption. These challenges primarily lie in two areas: accurately retrieving relevant knowledge and striking a balance between generation quality and computational efficiency. Additionally, integrating retrieved information with LLM-generated outputs remains a non-trivial task, requiring careful consideration of factors like semantic alignment, prompt engineering, and user preferences (as discussed in [10]). Traditional RAG approaches may also overlook the nuances of how LLMs process and prioritize information, leading to suboptimal performance in real-world applications.\nThis paper tackles key challenges in retrieval-augmented generation by introducing Pistis-RAG, a novel framework that boosts both effectiveness and efficiency. Pistis-RAG leverages a multi-stage retrieval pipeline, consisting of matching, pre-ranking, ranking, and re-ranking stages. Each stage plays a critical role in optimizing how relevant knowledge is retrieved and integrated into LLM-generated responses. Notably, our framework prioritizes a content-centric approach, ensuring retrieved information seamlessly aligns with user needs and LLM capabilities."}, {"title": "BACKGROUND", "content": "Fundamentally, our approach serves large-scale online content generation systems (such as ChatGPT[1]), which address the core issue of Open-Domain Question Answering (ODQA) [4]."}, {"title": "2.1 Large-Scale Generative AI Systems", "content": "Large-scale generative AI systems, are designed to handle a high volume of real-time user requests with high accuracy and efficiency. These systems are becoming increasingly important in a wide range of applications, including customer service, chatbots, and social media platforms.\nOne of the key challenges in developing large-scale generative AI systems is ensuring that the generated content is both relevant and accurate. This is particularly important in applications where users are relying on the system for information or assistance. RAG is a promising approach to addressing this challenge. RAG systems use a combination of retrieval and generation techniques to produce more accurate and relevant responses.\nAnother challenge in developing large-scale generative AI systems is ensuring that they can scale to handle the high volume of requests that they are likely to receive. This requires careful design of the system architecture and algorithms. Additionally, large-scale generative Al systems often need to be trained on massive amounts of data, which can be computationally expensive.\nAdvanced content generation platforms such as ChatGPT, Gemini[8], WenXinYiYan [7], and TongYiQianWen[2], among others, are engineered to manage substantial real-time user queries efficiently. These platforms play a pivotal role in applications requiring rapid response times, precise outputs, and extensive generation capacities.\nRAG plays a vital role in these systems by functioning as a powerful external memory source. This significantly reduces hallucinations and enhances responses' relevance and factual accuracy.\nMoreover, these large-scale generative AI systems benefit from real-time user feedback. This feedback loop allows for continuous improvement in the quality and accuracy of content generation over time. We propose the Pistis-RAG framework to recognize the"}, {"title": "3 THE PISTIS-RAG FRAMEWORK", "content": "This section explores online generative AI tasks through a cascading system, emphasizing a content-centric perspective."}, {"title": "3.1 Content-Centric vs. Model-Centric", "content": "This perspective views the process as transforming content from one form to another, rather than a traditional model-centric approach focused on manipulating data within the model. Specifically, the system operates in a series of actions:\n\u2022 Content Acquisition: The system first retrieves information from an external knowledge base. This retrieval process is driven by the user's intent, ensuring the retrieved content aligns with the user's desired task.\n\u2022 Content Transformation and Integration: The system then leverages the retrieved content alongside the knowledge it has learned during pre-training. This combined knowledge base informs the generation of new content, tailored to the user's needs. This new content could be instructions, summaries, creative text formats, or any other output relevant to the user's intent.\n\u2022 Content Delivery: Finally, the system returns the generated content to the user, completing the online task.\nThe content-centric perspective highlights the critical role of external information in this process, content from long-term memory becomes the system's primary input, driving the entire content transformation pipeline.\nWe argue that the lack of strong alignment between LLMs and the external knowledge ranking methods used in RAG tasks is relevant to the reliance on the model-centric paradigm in Langchain-like frameworks. A content-centric approach would prioritize seamless integration between the LLMs and external information sources,"}, {"title": "3.2 Rethinking RAG through a Content-Centric Lens", "content": "Adopting a content-centric perspective on RAG tasks, particularly when viewed within the framework of cascading systems, offers valuable insights. This perspective allows us to draw meaningful comparisons between RAG and traditional IR systems. Through this comparison, we can identify potential mismatches between the previously defined stages of both systems and uncover opportunities for introducing new stages that could enhance the overall process.\nA content-centric investigation, as exemplified in Figure 2, highlights a key issue: the current RAG \"re-ranker\" can easily lead to"}, {"title": "3.2.1 Stage 1: Matching (Filtering)", "content": "In the initial stage, the system employs robust retrieval algorithms to navigate through a vast corpus of documents, selecting those most relevant to the user's query. This stage utilizes a Mixed Retrieval strategy to ensure that the filtering process is both efficient and effective."}, {"title": "3.2.2 Stage 2: Pre-Ranking (Semantic Refinement)", "content": "Following the matching phase, this stage enhances the scoring process by refining it based on the complete document obtained through the retrieved fragment. It enhances the sorting process by scoring documents based on their semantic correlation with the user's query, using Cross-Encoder methods to achieve higher accuracy."}, {"title": "3.2.3 Stage 3: Ranking (LLM Alignment)", "content": "At this juncture, the process refines the document ranking by aligning with the preferences of the LLM. This adjustment ensures that the most relevant information is positioned advantageously within the prompt templates, optimizing the LLM's performance."}, {"title": "3.2.4 Stage 4: Re-Ranking (Domain Specific Requirements)", "content": "The Re-Ranking phase, although discretionary, plays a pivotal role in domain-specific requirements like assessing the credibility of information sources in official document composition or critical decision-making scenarios."}, {"title": "3.2.5 Stage 5: Reasoning (Multi-Path CoT Strategy)", "content": "If documents exhibit low distinctiveness in their semantic content, this stage intervenes by generating multiple response sequences. The LLM concurrently generates answers based on varied retrieval outputs, enhancing content diversity and decision-making in the aggregation stage."}, {"title": "3.2.6 Stage 6: Aggregating (Consistency Checking)", "content": "The final stage employs sophisticated voting methods to synthesize the outputs from the Reasoning stage. This method ensures consistency and stability in the final user responses, mitigating potential variability due to the LLM's sensitivity to input sequences."}, {"title": "3.3 Architecture", "content": "The Pistis-RAG system architecture comprises four core components: Matching, Ranking, Reasoning, and Aggregating Services, each meticulously designed to handle specific segments of the query-response cycle efficiently.\nMatching Service. The Matching Service is the cornerstone of the user interaction process, responsible for understanding user intent and swiftly retrieving the most relevant information. To achieve this, the Matching Service employs a sophisticated blend of IR techniques, focusing on optimizing latency for online large-scale retrieval."}, {"title": "Ranking Service", "content": "The Matching Service utilizes various data structures to optimize information retrieval based on the specific technique employed:\n\u2022 Vector Storage: This structure is crucial for embedding-based retrieval approaches like ANN. It efficiently stores document representations as vectors, allowing for fast similarity comparisons with the user's query vector. Example: The Matching Service can represent documents as vectors in a high-dimensional space, enabling it to quickly identify documents that are semantically similar to the user's query.\n\u2022 Inverted Index: A fundamental data structure for keyword-based retrieval. It allows for rapid identification of documents containing specific keywords present in the user's query. Example: When a user searches for a specific term, the inverted index efficiently directs the Matching Service to the documents that contain that term.\nIn-memory K-V Cache also plays a critical role in stateful services. It can be used for:\n\u2022 Maintaining User Conversation Session: Storing the context of a user's ongoing conversation allows for a more coherent and personalized user experience. Example: The Matching Service can use the K-V cache to remember the user's previous queries and interactions, enabling it to provide more tailored responses.\n\u2022 User Prompt-Answer Pair Cache: Storing previously accessed responses to commonly asked questions or user prompts can substantially enhance response times. Moreover, it serves as an effective few-shot example for RAG. For instance, when a user poses a recurring question, the Matching Service can save the corresponding answer in the key-value cache, thereby obviating the necessity to conduct a fresh search on each occasion.\n\u2022 User-Relevant Session History: By storing high-quality content relevant to the user's past interactions, the Matching Service can prioritize its retrieval for future queries, enhancing the user experience. Example: If a user has previously shown interest in a particular topic, the Matching Service can prioritize content related to that topic when responding to future queries.\nIn large-scale industrial settings, the Matching Service might also integrate with external search engines to access a vast corpus of information. However, this approach typically comes with increased latency due to network communication overhead.\nThe choice of retrieval technique and data structure depends on the specific requirements of the application. For instance, if higher accuracy is desired, ANN or inverted index and BM25 and TF-IDF methods can be used together. It is important to carefully consider the trade-offs between speed and accuracy when selecting the appropriate techniques.\nAdditionally, it is important to acknowledge that the Matching Service has its limitations. For example, it may struggle with handling ambiguous or complex queries that require.\nRanking Service. The ranking service optimizes the information retrieval process by prioritizing items relevant to the user's intent. It takes a set of relevant items, denoted by $I$, identified by the matching service, and outputs a prioritized list, denoted by $O$. This prioritization considers both the retrieved items and the user's intent representation.\nThe ranking service employs several techniques to achieve this. Below are the key stages and processes formally defined:"}, {"title": "(1) Stage 1: Pre-Ranking", "content": "The Pre-Ranking stage serves as an initial filtering mechanism aimed at streamlining the subsequent ranking process."}, {"title": "4 RANKING DETAILS", "content": "In this section, we outline the ranking process, differentiating it from the commonly discussed concept of re-rankers in RAG discourse. The ranking stage in information retrieval systems plays a"}, {"title": "4.1 RAG \"Re-Rankers\" in the Pre-Ranking Stage", "content": "While terms like bge-reranker are commonly used for components within RAG systems, they can be misleading. Information Retrieval systems like search engines employ re-ranking models in a later stage to refine results based on specific business needs.\nIn RAG, however, these models are used upfront during the pre-ranking stage to narrow down candidate documents. Therefore, a more accurate term for this function would be \"pre-ranker.\""}, {"title": "4.2 Ranking for LLM Prompt Order Sensitivity", "content": "LLMs are known for their impressive capabilities, but their performance can be heavily influenced by the order in which prompts are presented (prompt order sensitivity). This is particularly relevant in the RAG system, where ranking plays a crucial role. If the order of prompt examples is not carefully considered, the LLM might generate responses that lack coherence or deviate from the user's intent. The order of prompts can shape the LLM's understanding of the task and the user's expectations. If prompts are not presented logically, the LLM might struggle to generate an output that aligns with the user's desired outcome.\nExisting rule-based studies, such as those focusing on cue word placement, suggest that placing examples at the beginning or end may be more effective. However, we assume that this preference cannot be determined by rules alone; rather, it requires a statistic learning model to capture this preference distribution accurately.\nThe RAG system addresses prompt order sensitivity by employing a ranking mechanism. This mechanism ensures that the most relevant and informative prompt examples are presented to the LLM first. This alignment between the ranking and the ideal order of prompts leads to the following:\n\u2022 Improved Coherence: By prioritizing relevant information, the LLM is more likely to produce coherent and consistent outputs that align with the user's intent.\n\u2022 Enhanced User Experience: When the LLM generates responses that meet user expectations, the overall user experience is significantly improved."}, {"title": "4.3 Ranking Stage Problem Definition", "content": "The ranking problem in online systems involves improving the order of items presented to users based on various factors beyond just relevance. This can be achieved by leveraging user feedback to understand their preferences for the content.\nUser feedback can be obtained through actions like copying the content (indicating strong preference), regenerating (suggesting mild preference for an alternative), or disliking the content (clear disapproval). By analyzing these feedback labels, we aim to learn a ranking model that optimizes the order of items within a list to match user preferences better.\nFormally, let's denote our dataset as $D$, containing a set of ordered few-shot examples {(pi, xi, yi)} and corresponding user feedback vectors $L$. Here, $p_i$ represents the user intent, $x_i$ represents the input example, $y_i$ represents the desired output (potentially the originally ranked list), and $L_i$ is a vector containing the specific feedback labels (copying, regenerating, disliking) associated with the i-th pair.\nThe RAG ranking stages' objective is to develop a ranking model that utilizes these feedback signals to optimize the ranking sequences produced by the large language model, ultimately enhancing the user experience and satisfaction with the system."}, {"title": "4.4 Listwide Labels", "content": "This part delves into the complexities of learning from implicit user feedback in Listwide Learning to Rank (LTR) methods. Specifically, we focus on scenarios where explicit relevancy labels for all items in a list are absent.\nOur primary focus is on leveraging indirect signals derived from user feedback, which reflect how the ranking of content impacts the results produced by LLMs. These indirect signals are termed Listwide Labels.\nTraining ranking models using Listwide Labels poses significant challenges. A key issue is the potential oversight of critical insights into the overall ranking quality. Therefore, developing effective strategies to utilize Listwide Labels is essential for enhancing ranking model performance."}, {"title": "4.5 Ranking Model Alignment", "content": "To address the ranking problem in online systems, we employ the listwise Transformer to combine the listwise LTR objective with a listwide objective, where the overall quality of the list is explicitly modeled.\nFor effective utilization of listwide labels, we initially collect user feedback data, encompassing actions such as copying, regenerating, disliking, etc., to serve as the ground truth for evaluating the efficacy of the content generation by the large language model.\nThe core ranking process employs advanced algorithms to meticulously assess each item's quality and relevance. These algorithms take a set $I$ of items to be ranked as input and produce an ordered set $O$ based on the ranking function $f$:\n$O = f(I)$\nIn the industry setting, we formulate the ranking task as a learning-to-rank problem based on listwide signals. This involves training a ranking model using supervised learning techniques to learn a ranking function that sorts the content generated by the large language model into the desired order. Our model incorporates a diverse range of features, including:\n\u2022 Semantic similarity between generated content and user intent $sim(c, intent)$\n\u2022 Relevance of generated content to the prompt $rel(c, prompt)$\n\u2022 Novelty of generated content $nov(c)$\n\u2022 User engagement metrics $eng(c)$\nAdditionally, we integrate techniques for managing user feedback dynamics and evolving user preferences over time, ensuring our ranking model remains adaptive and responsive to changes in user behavior.\nOur methodology aims to optimize the ranking of content produced by the LLMs in alignment with user preferences and business objectives, thereby enhancing the overall user experience and utility."}, {"title": "5 EXPERIMENTS ON PUBLIC DATASETS", "content": "Our experimental analysis delves into prominent public datasets MMLU[5], which comprises a substantial volume of data, with human annotators assigning explicit answers to each question in the selection.\nHowever, our primary focus lies in learning from feedback labels, which are more abundant and reflective of real-world scenarios. Hence, we simulate feedback labels to mirror realistic user behavior observed in various industries. This simulation approach is detailed in the subsequent sections."}, {"title": "5.1 Simulating Feedback", "content": "Online platforms, such as ChatGPT, continuously enhance their performance through user feedback mechanisms such as copying, regenerating, and disliking responses. Our simulation process aims to align our system with this prevalent scenario."}, {"title": "5.1.1 Collecting Few-shot Examples", "content": "Within our system architecture, we seamlessly integrate highly rated user feedback into our contextual learning example database for RAG utilization. Specifically, we incorporate precise question-answer pairs sourced from"}, {"title": "5.1.2 Simulating User Behavior", "content": "We refine user behavior data from open-source datasets to simulate user feedback through the following steps:\n(1) Leveraging Retrieval-Augmented Generation: Our system employs RAG to augment response generation by retrieving relevant information from a vast dataset D before generating an answer. This process, denoted as RAG(D), aims to establish the correlation between RAG ranking methods and generated responses.\n(2) Extracting Information with Regular Expressions: Following the generation stage, we employ regular expressions to extract specific information X from the generated texts Y. This operation is represented as $P(Y) = X$, where P represents the regex parsing function.\n(3) Assigning Labels Based on Correctness: The final step involves assigning labels to the generated outputs based on their accuracy relative to the expected answers. We define the labeling function $L(y)$ for the generated output y in comparison to the expected answer.\nThis feedback loop is fundamental to continuously enhancing the model's accuracy and aligning it with user preferences. Here's the breakdown of the labels:\n\u2022 Correct ($L(y) = Positive$): Indicates that the generated output y matches the set of correct answers Ycorrect.\n\u2022 Incorrect ($L(y) = Even$): Signals that the generated output y falls within the set of incorrect answers Yincorrect.\n\u2022 No Answer ($L(y) = Negative$): Represents outputs lacking an answer and belonging to the set of non-responses Yno_answer.\nThis formalization illustrates a continuous learning system wherein feedback from real user interactions refines and enhances the model, aligning its outputs more closely with user expectations and real-world applications. Specifically, correct answers (Ycorrect) resemble text copying, incorrect answers (Yincorrect) resemble regeneration, and no answers (Yno_answer) correspond to negative user feedback."}, {"title": "5.2 Datasets", "content": "In assessing the effectiveness of the Pistis-RAG framework, it is essential to leverage diverse datasets that offer a comprehensive evaluation of its capabilities. Two prominent datasets, MMLU and NetEval, serve as invaluable resources in this regard, providing insights into the framework's adaptability and performance across various domains.\nThe MMLU dataset is a valuable resource for researchers who are developing speech recognition and natural language processing systems. The dataset has been used to train various speech recognition systems, and it has been shown to improve the performance"}, {"title": "5.3 Experimental Setup", "content": "Our experimental setup is meticulously designed to optimize the performance of our models in both retrieval and response generation. Below are the key components of our setup:\nTo evaluate the effectiveness of our proposed method, we conducted experiments on the MMLU dataset. We first built an index of the MMLU training set, then used BEG-M3 to retrieve the top 10 candidate few-shots based on Milvus. Then, we used BEG-reranker-larger to pre-rank the candidate few-shots and obtain the top five candidate few-shots. Finally, we used these five candidate few-shots to generate 5-shot results as the baseline.\nRetrieval: We used BEG-M3 to retrieve the top 10 candidate few-shots based on Milvus. BEG-M3 is a dense vector similarity search engine that can efficiently retrieve similar vectors from a large collection of vectors. Milvus is a vector database that stores and manages vectors.\nPre-Ranking: We used BEG-reranker-larger to pre-sort the candidate few-shots. BEG-reranker-larger that improves the ranking of candidate few-shots.\nGeneration: We used the top five candidate few-shots to generate 5-shot results. We used Llama-2-13B-chat as a generation model to generate text from the few-shot prompts. Llama-2 [9] leverages a sophisticated transformer architecture, enabling advanced capabilities in natural language understanding and generation. It excels in its depth of contextual comprehension, allowing it to generate responses that are coherent and contextually aligned across various NLP tasks. Extensive benchmarking has demonstrated Llama-2's ability to handle complex queries with nuanced understanding and high precision."}, {"title": "5.3.1 Evaluation Procedure", "content": "The performance of our models is evaluated using MMLU test set. We compute several metrics to assess the models' effectiveness in generating accurate responses:\n\u2022 Precision: The ratio of true positive predictions to the total number of positive predictions made by the model:\n$P = \\frac{TP}{TP + FP}$\n\u2022 Recall: The ratio of true positive predictions to the total number of actual positive instances in the dataset:\n$R = \\frac{TP}{TP + FN}$\n\u2022 F1-score: The harmonic mean of precision and recall, providing a balanced measure of the model's performance:\n$F1 = 2 \\cdot \\frac{P \\cdot R}{P+R}$\nThese metrics offer a comprehensive evaluation of the model's ability to generate accurate and effective end-to-end results.\nThrough this meticulously designed experimental setup, we aim to provide a thorough evaluation of the effectiveness and robustness of our Pistis-RAG framework in handling diverse language understanding and generation tasks."}, {"title": "5.3.2 Experimental Results", "content": "We evaluated the effectiveness of different components within the Pistis-RAG framework through an ablation study. This approach systematically removes or modifies key components to assess their impact on performance metrics, focusing on the MMLU dataset."}, {"title": "5.3.3 Analysis and Discussion", "content": "The ablation study results summarized in Table 2 offer valuable insights:\n\u2022 Importance of Feedback Label Integration: Excluding feedback labels led to a significant F1-score drop, underlining the crucial role of user feedback in model improvement.\n\u2022 Multi-Path Reasoning Impact: Removing multi-path reasoning resulted in a lower F1-score, demonstrating its importance in strengthening the model's analytical capabilities.\nIn conclusion, the ablation study confirms that these components play a critical role in the Pistis-RAG framework's performance."}, {"title": "6 CONCLUSIONS", "content": "This study highlights cascade modeling and optimization as critical areas for robust large-scale online Al-generated content (AIGC) systems.\nWe revisited the Retrieval-Augmented Generation problem through a content-centric lens, uncovering potential shortcomings in large-scale deployments. Firstly, a mismatch exists between the intended and actual function of the \"re-ranker\" component, suggesting it acts"}]}