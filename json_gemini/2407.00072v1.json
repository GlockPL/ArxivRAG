{"title": "Pistis-RAG: A Scalable Cascading Framework Towards Trustworthy Retrieval-Augmented Generation", "authors": ["Yu Bai", "Yukai Miao", "Li Chen", "Dan Li", "Yanyu Ren", "Hongtao Xie", "Ce Yang", "Xuhui Cai"], "abstract": "In Greek mythology, Pistis symbolized good faith, trust, and reliability, echoing the core principles of RAG in LLM systems. Pistis-RAG, a scalable multi-stage framework, effectively addresses the challenges of large-scale retrieval-augmented generation (RAG). Each stage plays a distinct role: matching refines the search space, pre-ranking prioritizes semantically relevant documents, and ranking aligns with the large language model's (LLM) preferences. The reasoning and aggregating stage supports the implementation of complex chain-of-thought (CoT) methods within this cascading structure.\nWe argue that the lack of strong alignment between LLMs and the external knowledge ranking methods used in RAG tasks is relevant to the reliance on the model-centric paradigm in RAG frameworks. A content-centric approach would prioritize seamless integration between the LLMs and external information sources, optimizing the content transformation process for each specific task.\nCritically, our ranking stage deviates from traditional RAG approaches by recognizing that semantic relevance alone may not directly translate to improved generation. This is due to the sensitivity of the few-shot prompt order, as highlighted in prior work [6]. Current RAG frameworks fail to account for this crucial factor.\nWe introduce a novel ranking stage specifically designed for RAG systems. It adheres to information retrieval principles while considering the unique business scenario captured by LLM preferences and user feedback. Our approach integrates in-context learning (ICL) methods and reasoning steps to incorporate user feedback, ensuring efficient alignment.\nExperiments on the MMLU benchmark demonstrate a 9.3% performance improvement. The model and code will be open-sourced on GitHub. Experiments on real-world, large-scale data validate our framework's scalability.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, LLMs have demonstrated remarkable capabilities in natural language understanding and generation tasks. These models, trained on vast datasets representing a wide array of human knowledge, excel at generating coherent and contextually relevant text across various domains [1]. Despite their advancements, LLMs are inherently limited by the scope and quality of their training data. They cannot often incorporate real-time or domain-specific knowledge not present in their training corpus, leading to challenges in accuracy and relevance, particularly in dynamic environments.\nTo mitigate these limitations, RAG has emerged as a promising paradigm. RAG enhances LLMs by augmenting their input with additional information retrieved from external sources at inference time. This approach not only enriches the context of LLM-generated responses but also enables them to leverage up-to-date and domain-specific knowledge without the need for extensive retraining [cite needed]. By integrating ICL from retrieved knowledge, LLMs can adapt and perform better on tasks requiring specialized knowledge or current events.\nCurrent RAG frameworks face significant challenges that hinder their widespread adoption. These challenges primarily lie in two areas: accurately retrieving relevant knowledge and striking a balance between generation quality and computational efficiency. Additionally, integrating retrieved information with LLM-generated outputs remains a non-trivial task, requiring careful consideration of factors like semantic alignment, prompt engineering, and user preferences (as discussed in [10]). Traditional RAG approaches may also overlook the nuances of how LLMs process and prioritize information, leading to suboptimal performance in real-world applications.\nThis paper tackles key challenges in retrieval-augmented generation by introducing Pistis-RAG, a novel framework that boosts both effectiveness and efficiency. Pistis-RAG leverages a multi-stage retrieval pipeline, consisting of matching, pre-ranking, ranking, and re-ranking stages. Each stage plays a critical role in optimizing how relevant knowledge is retrieved and integrated into LLM-generated responses. Notably, our framework prioritizes a content-centric approach, ensuring retrieved information seamlessly aligns with user needs and LLM capabilities."}, {"title": "BACKGROUND", "content": "Fundamentally, our approach serves large-scale online content generation systems (such as ChatGPT[1]), which address the core issue of Open-Domain Question Answering (ODQA) [4]."}, {"title": "Large-Scale Generative AI Systems", "content": "Large-scale generative AI systems, are designed to handle a high volume of real-time user requests with high accuracy and efficiency. These systems are becoming increasingly important in a wide range of applications, including customer service, chatbots, and social media platforms.\nOne of the key challenges in developing large-scale generative AI systems is ensuring that the generated content is both relevant and accurate. This is particularly important in applications where users are relying on the system for information or assistance. RAG is a promising approach to addressing this challenge. RAG systems use a combination of retrieval and generation techniques to produce more accurate and relevant responses.\nAnother challenge in developing large-scale generative AI systems is ensuring that they can scale to handle the high volume of requests that they are likely to receive. This requires careful design of the system architecture and algorithms. Additionally, large-scale generative Al systems often need to be trained on massive amounts of data, which can be computationally expensive.\nAdvanced content generation platforms such as ChatGPT, Gemini[8], WenXinYiYan [7], and TongYiQianWen[2], among others, are engineered to manage substantial real-time user queries efficiently. These platforms play a pivotal role in applications requiring rapid response times, precise outputs, and extensive generation capacities.\nRAG plays a vital role in these systems by functioning as a powerful external memory source. This significantly reduces hallucinations and enhances responses' relevance and factual accuracy.\nMoreover, these large-scale generative AI systems benefit from real-time user feedback. This feedback loop allows for continuous improvement in the quality and accuracy of content generation over time. We propose the Pistis-RAG framework to recognize the"}, {"title": "THE PISTIS-RAG FRAMEWORK", "content": "This section explores online generative AI tasks through a cascading system, emphasizing a content-centric perspective."}, {"title": "Content-Centric vs. Model-Centric", "content": "This perspective views the process as transforming content from one form to another, rather than a traditional model-centric ap-proach focused on manipulating data within the model. Specifically, the system operates in a series of actions:\n\u2022 Content Acquisition: The system first retrieves information from an external knowledge base. This retrieval process is driven by the user's intent, ensuring the retrieved content aligns with the user's desired task.\n\u2022 Content Transformation and Integration: The system then leverages the retrieved content alongside the knowledge it has learned during pre-training. This combined knowledge base informs the generation of new content, tailored to the user's needs. This new content could be instructions, summaries, creative text formats, or any other output relevant to the user's intent.\n\u2022 Content Delivery: Finally, the system returns the generated content to the user, completing the online task.\nThe content-centric perspective highlights the critical role of external information in this process, content from long-term memory becomes the system's primary input, driving the entire content transformation pipeline.\nWe argue that the lack of strong alignment between LLMs and the external knowledge ranking methods used in RAG tasks is relevant to the reliance on the model-centric paradigm in Langchain-like frameworks. A content-centric approach would prioritize seamless integration between the LLMs and external information sources, optimizing the content transformation process for each specific task. This topic will be further explored in later sections of the paper."}, {"title": "Rethinking RAG through a Content-Centric Lens", "content": "Adopting a content-centric perspective on RAG tasks, particularly when viewed within the framework of cascading systems, offers valuable insights. This perspective allows us to draw meaningful comparisons between RAG and traditional IR systems. Through this comparison, we can identify potential mismatches between the previously defined stages of both systems and uncover opportunities for introducing new stages that could enhance the overall process.\nA content-centric investigation, as exemplified in Figure 2, highlights a key issue: the current RAG \"re-ranker\" can easily lead to"}, {"title": "Architecture", "content": "The Pistis-RAG system architecture comprises four core components: Matching, Ranking, Reasoning, and Aggregating Services, each meticulously designed to handle specific segments of the query-response cycle efficiently.\nMatching Service. The Matching Service is the cornerstone of the user interaction process, responsible for understanding user intent and swiftly retrieving the most relevant information. To achieve this, the Matching Service employs a sophisticated blend of IR techniques, focusing on optimizing latency for online large-scale retrieval.\nThe Matching Service utilizes various data structures to optimize information retrieval based on the specific technique employed:\n\u2022 Vector Storage: This structure is crucial for embedding-based retrieval approaches like ANN. It efficiently stores document representations as vectors, allowing for fast similarity comparisons with the user's query vector. Example: The Matching Service can represent documents as vectors in a high-dimensional space, enabling it to quickly identify documents that are semantically similar to the user's query.\n\u2022 Inverted Index: A fundamental data structure for keyword-based retrieval. It allows for rapid identification of documents containing specific keywords present in the user's query. Example: When a user searches for a specific term, the inverted index efficiently directs the Matching Service to the documents that contain that term.\nIn-memory K-V Cache also plays a critical role in stateful services. It can be used for:\n\u2022 Maintaining User Conversation Session: Storing the context of a user's ongoing conversation allows for a more coherent and personalized user experience. Example: The Matching Service can use the K-V cache to remember the user's previous queries and interactions, enabling it to provide more tailored responses.\n\u2022 User Prompt-Answer Pair Cache: Storing previously accessed responses to commonly asked questions or user prompts can substantially enhance response times. Moreover, it serves as an effective few-shot example for RAG. For instance, when a user poses a recurring question, the Matching Service can save the corresponding answer in the key-value cache, thereby obviating the necessity to conduct a fresh search on each occasion.\n\u2022 User-Relevant Session History: By storing high-quality content relevant to the user's past interactions, the Matching Service can prioritize its retrieval for future queries, enhancing the user experience. Example: If a user has previously shown interest in a particular topic, the Matching Service can prioritize content related to that topic when responding to future queries.\nIn large-scale industrial settings, the Matching Service might also integrate with external search engines to access a vast corpus of information. However, this approach typically comes with increased latency due to network communication overhead.\nThe choice of retrieval technique and data structure depends on the specific requirements of the application. For instance, if higher accuracy is desired, ANN or inverted index and BM25 and TF-IDF methods can be used together. It is important to carefully consider the trade-offs between speed and accuracy when selecting the appropriate techniques.\nAdditionally, it is important to acknowledge that the Matching Service has its limitations. For example, it may struggle with handling ambiguous or complex queries that require.\nRanking Service. The ranking service optimizes the information retrieval process by prioritizing items relevant to the user's intent. It takes a set of relevant items, denoted by \\(I\\), identified by the matching service, and outputs a prioritized list, denoted by \\(O\\). This prioritization considers both the retrieved items and the user's intent representation.\nThe ranking service employs several techniques to achieve this. Below are the key stages and processes formally defined:\n(1) Stage 1: Pre-Ranking\nThe Pre-Ranking stage serves as an initial filtering mecha-nism aimed at streamlining the subsequent ranking process."}, {"title": "Ranking Details", "content": "In this section, we outline the ranking process, differentiating it from the commonly discussed concept of re-rankers in RAG discourse. The ranking stage in information retrieval systems plays a"}, {"title": "Ranking Model Alignment", "content": "To address the ranking problem in online systems, we employ the listwise Transformer to combine the listwise LTR objective with a listwide objective, where the overall quality of the list is explicitly modeled.\nFor effective utilization of listwide labels, we initially collect user feedback data, encompassing actions such as copying, regenerating, disliking, etc., to serve as the ground truth for evaluating the efficacy of the content generation by the large language model.\nThe core ranking process employs advanced algorithms to meticulously assess each item's quality and relevance. These algorithms take a set \\(I\\) of items to be ranked as input and produce an ordered set \\(O\\) based on the ranking function \\(f\\):\n\\[O = f(I)\\]\nIn the industry setting, we formulate the ranking task as a learning-to-rank problem based on listwide signals. This involves training a ranking model using supervised learning techniques to learn a ranking function that sorts the content generated by the large language model into the desired order. Our model incorporates a diverse range of features, including:\n\u2022 Semantic similarity between generated content and user intent \\(sim(c, intent)\\)\n\u2022 Relevance of generated content to the prompt \\(rel(c, prompt)\\)\n\u2022 Novelty of generated content \\(nov(c)\\)\n\u2022 User engagement metrics \\(eng(c)\\)\nAdditionally, we integrate techniques for managing user feed-back dynamics and evolving user preferences over time, ensuring our ranking model remains adaptive and responsive to changes in user behavior.\nOur methodology aims to optimize the ranking of content pro-duced by the LLMs in alignment with user preferences and business objectives, thereby enhancing the overall user experience and utility."}, {"title": "EXPERIMENTS ON PUBLIC DATASETS", "content": "Our experimental analysis delves into prominent public datasets MMLU[5], which comprises a substantial volume of data, with human annotators assigning explicit answers to each question in the selection.\nHowever, our primary focus lies in learning from feedback labels, which are more abundant and reflective of real-world scenarios. Hence, we simulate feedback labels to mirror realistic user behavior observed in various industries. This simulation approach is detailed in the subsequent sections."}, {"title": "Simulating Feedback", "content": "Online platforms, such as ChatGPT, continuously enhance their performance through user feedback mechanisms such as copying, regenerating, and disliking responses. Our simulation process aims to align our system with this prevalent scenario."}, {"title": "Collecting Few-shot Examples", "content": "Within our system architecture, we seamlessly integrate highly rated user feedback into our contextual learning example database for RAG utilization. Specifically, we incorporate precise question-answer pairs sourced from"}, {"title": "Simulating User Behavior", "content": "We refine user behavior data from open-source datasets to simulate user feedback through the following steps:\n(1) Leveraging Retrieval-Augmented Generation: Our system employs RAG to augment response generation by retrieving relevant information from a vast dataset \\(D\\) before generating an answer. This process, denoted as \\(RAG(D)\\), aims to establish the correlation between RAG ranking methods and generated responses.\n(2) Extracting Information with Regular Expressions: Fol-lowing the generation stage, we employ regular expressions to extract specific information \\(X\\) from the generated texts \\(Y\\). This operation is represented as \\(P(Y) = X\\), where \\(P\\) represents the regex parsing function.\n(3) Assigning Labels Based on Correctness: The final step involves assigning labels to the generated outputs based on their accuracy relative to the expected answers. We define the labeling function \\(L(y)\\) for the generated output \\(y\\) in comparison to the expected answer.\nThis feedback loop is fundamental to continuously enhancing the model's accuracy and aligning it with user preferences. Here's the breakdown of the labels:\n\u2022 Correct (\\(L(y) = Positive\\)): Indicates that the generated output \\(y\\) matches the set of correct answers \\(Ycorrect\\).\n\u2022 Incorrect (\\(L(y) = Even\\)): Signals that the generated output \\(y\\) falls within the set of incorrect answers \\(Yincorrect\\).\n\u2022 No Answer (\\(L(y) = Negative\\)): Represents outputs lack-ing an answer and belonging to the set of non-responses \\(Yno_answer\\).\nThis formalization illustrates a continuous learning system wherein feedback from real user interactions refines and enhances the model, aligning its outputs more closely with user expectations and real-world applications. Specifically, correct answers (\\(Ycorrect\\)) resemble text copying, incorrect answers (\\(Yincorrect\\)) resemble regeneration, and no answers (\\(Yno_answer\\)) correspond to negative user feedback."}, {"title": "Datasets", "content": "In assessing the effectiveness of the Pistis-RAG framework, it is essential to leverage diverse datasets that offer a comprehensive evaluation of its capabilities. Two prominent datasets, MMLU and NetEval, serve as invaluable resources in this regard, providing insights into the framework's adaptability and performance across various domains.\nThe MMLU dataset is a valuable resource for researchers who are developing speech recognition and natural language processing systems. The dataset has been used to train various speech recog-nition systems, and it has been shown to improve the performance of these systems. The dataset has also been used to develop natural"}, {"title": "Experimental Setup", "content": "Our experimental setup is meticulously designed to optimize the performance of our models in both retrieval and response generation. Below are the key components of our setup:\nTo evaluate the effectiveness of our proposed method, we con-ducted experiments on the MMLU dataset. We first built an index of the MMLU training set, then used BEG-M3 to retrieve the top 10 candidate few-shots based on Milvus. Then, we used BEG-reranker-larger to pre-rank the candidate few-shots and obtain the top five candidate few-shots. Finally, we used these five candidate few-shots to generate 5-shot results as the baseline.\nRetrieval: We used BEG-M3 to retrieve the top 10 candidate few-shots based on Milvus. BEG-M3 is a dense vector similarity search engine that can efficiently retrieve similar vectors from a large collection of vectors. Milvus is a vector database that stores and manages vectors.\nPre-Ranking: We used BEG-reranker-larger to pre-sort the candidate few-shots. BEG-reranker-larger that improves the ranking of candidate few-shots.\nGeneration: We used the top five candidate few-shots to generate 5-shot results. We used Llama-2-13B-chat as a generation model to generate text from the few-shot prompts. Llama-2 [9] leverages a sophisticated transformer architecture, enabling advanced capabilities in natural language understanding and generation. It excels in its depth of contextual comprehension, allowing it to generate responses that are coherent and contextually aligned across various NLP tasks. Extensive benchmarking has demonstrated Llama-2's ability to handle complex queries with nuanced understanding and high precision."}, {"title": "Evaluation Procedure", "content": "The performance of our models is evaluated using MMLU test set. We compute several metrics to assess the models' effectiveness in generating accurate responses:\n\u2022 Precision: The ratio of true positive predictions to the total number of positive predictions made by the model:\n\\[P = \\frac{TP}{TP + FP}\\]\n\u2022 Recall: The ratio of true positive predictions to the total number of actual positive instances in the dataset:\n\\[R = \\frac{TP}{TP + FN}\\]\n\u2022 F1-score: The harmonic mean of precision and recall, pro-viding a balanced measure of the model's performance:\n\\[F1 = 2 \\cdot \\frac{PR}{P+R}\\]\nThese metrics offer a comprehensive evaluation of the model's ability to generate accurate and effective end-to-end results.\nThrough this meticulously designed experimental setup, we aim to provide a thorough evaluation of the effectiveness and robust-ness of our Pistis-RAG framework in handling diverse language understanding and generation tasks."}, {"title": "Experimental Results", "content": "We evaluated the effectiveness of dif-ferent components within the Pistis-RAG framework through an ablation study. This approach systematically removes or modifies key components to assess their impact on performance metrics, focusing on the MMLU dataset."}, {"title": "Analysis and Discussion", "content": "The ablation study results summa-rized in Table 2 offer valuable insights:\n\u2022 Importance of Feedback Label Integration: Excluding feedback labels led to a significant F1-score drop, underlin-ing the crucial role of user feedback in model improvement.\n\u2022 Multi-Path Reasoning Impact: Removing multi-path rea-soning resulted in a lower F1-score, demonstrating its im-portance in strengthening the model's analytical capabili-ties.\nIn conclusion, the ablation study confirms that these components play a critical role in the Pistis-RAG framework's performance."}, {"title": "CONCLUSIONS", "content": "This study highlights cascade modeling and optimization as critical areas for robust large-scale online Al-generated content (AIGC) systems.\nWe revisited the Retrieval-Augmented Generation problem through a content-centric lens, uncovering potential shortcomings in large-scale deployments. Firstly, a mismatch exists between the intended and actual function of the \"re-ranker\" component, suggesting it acts"}]}