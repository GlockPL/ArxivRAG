{"title": "Harnessing PDF Data for Improving Japanese Large Multimodal Models", "authors": ["Jeonghun Baek", "Akiko Aizawa", "Kiyoharu Aizawa"], "abstract": "Large Multimodal Models (LMMs) have\ndemonstrated strong performance in English,\nbut their effectiveness in Japanese remains lim-\nited due to the lack of high-quality training data.\nCurrent Japanese LMMs often rely on trans-\nlated English datasets, restricting their ability\nto capture Japan-specific cultural knowledge.\nTo address this, we explore the potential of\nJapanese PDF data as a training resource, an\narea that remains largely underutilized. We in-\ntroduce a fully automated pipeline that lever-\nages pretrained models to extract image-text\npairs from PDFs through layout analysis, OCR,\nand vision-language pairing, removing the need\nfor manual annotation. Additionally, we con-\nstruct instruction data from extracted image-\ntext pairs to enrich the training data. To evalu-\nate the effectiveness of PDF-derived data, we\ntrain Japanese LMMs and assess their perfor-\nmance on the Japanese LMM Benchmark. Our\nresults demonstrate substantial improvements,\nwith performance gains ranging from 3.9% to\n13.8% on Heron-Bench. Further analysis high-\nlights the impact of PDF-derived data on var-\nious factors, such as model size and language\nmodels, reinforcing its value as a multimodal\nresource for Japanese LMMs. We plan to make\nthe source code and data publicly available\nupon acceptance.", "sections": [{"title": "1 Introduction", "content": "Large Multimodal Models (LMMs) have achieved\nhigh performance in English (OpenAI, 2024a;\nTeam et al., 2024; Dubey et al., 2024; Yang et al.,\n2024), and their development is now expanding\nto other languages. Recently, several open-source\nJapanese LMMs have been released (Akiba et al.,\n2025; Tanahashi et al., 2023; Inoue et al., 2024;\nLab, 2024; Sasagawa et al., 2024). While these\nmodels perform reasonably well, they still lag be-\nhind their English counterparts, partly due to the\nlimited availability of Japanese training data."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Extracting Image-text Pairs from PDFs", "content": "Research on extracting images and their captions\nfrom PDFs, particularly scientific papers, has been\nactively explored (Clark and Divvala, 2015, 2016;\nSiegel et al., 2018; Naiman et al., 2022; Okamoto\net al., 2023). These studies typically perform the\nlayout analysis (Shen et al., 2021) to locate image\nregions within a PDF, extract caption data from\nnearby text, and pair them together. When pair-\ning, some approaches use distance-based matching,\nconsidering that caption text is generally closer to\nthe corresponding image than other text (Okamoto\net al., 2023).\nHowever, to the best of our knowledge, no ex-\nisting study has paired images with text other than\nthe captions explicitly found in PDFs. We aim to\nextract image-text pairs from PDFs without being\nlimited to captions. The closest existing work to\nour task is the identification of paragraphs that ref-\nerence figures in scientific papers and summarizing\ntheir content to generate figure captions (Huang\net al., 2023). However, this approach does not\nstrictly pair images with non-caption text in PDFs,\nand its applicability is limited to scientific papers\nrather than general PDFs. In contrast, our work ex-\ntends beyond scientific papers to cover a broader\nrange of general PDFs."}, {"title": "2.2 Japanese LMM", "content": "Recently, Japanese large multimodal models\n(LMMs) have been emerging based on English\nLMMs. Proprietary LMMs have been improv-\ning their multilingual capabilities, achieving high\nperformance in Japanese as well (OpenAI, 2023,\n2024a; Anthropic, 2024; Team et al., 2024). Addi-\ntionally, many open-source Japanese LMMs have\nbeen released (Shing and Akiba, 2023a,b; Akiba\net al., 2025; Tanahashi et al., 2023; Inoue et al.,\n2024; Lab, 2024; Sasagawa et al., 2024).\nMost open-source LMMs follow the LLaVA (Liu\net al., 2023) approach, where a large language\nmodel (LLM) and a vision encoder are connected\nvia a relatively shallow projector to form an LMM.\nFor training, some use in-house Japanese data (Lab,\n2024), while others rely on translated Japanese\ndata (Shing and Akiba, 2023a,b; Tanahashi et al.,\n2023; Inoue et al., 2024) and adopt a Japanese LLM\nas the base language model (Lab, 2024; Shing and\nAkiba, 2023a,b; Tanahashi et al., 2023; Inoue et al.,\n2024). This approach enables the development of\nJapanese LMMs with decent performance.\nSome models, such as Qwen-VL (Bai et al.,\n2023), achieve strong performance in Japanese\nwithout using a Japanese LLM, instead leverag-\ning a multilingual LLM. VILA-jp (Sasagawa et al.,\n2024) has further improved performance by utiliz-\ning interleaved data. However, no existing work has\nleveraged PDF data to enhance Japanese LMMs. To\nachieve higher performance, we utilize PDF data\nin our approach."}, {"title": "3 Harnessing PDF Data", "content": "We aim to enhance the performance of Japanese\nLMMs using PDF data. In this section, we describe\nthe process of obtaining Japanese LMM training\ndata from PDF data."}, {"title": "3.1 Collecting PDF data", "content": "The PDF dataset used in this study was collected\nfrom the Web based on URLs supplied by the Web\nARchiving Project of the National Diet Library of\nJapan (National Diet Library). The total number of\nPDFs exceeds 51.38 million. However, we do not\nuse all of these PDFs; instead, we select a subset\nthrough the following process. The PDF data in-\ncludes a wide variety of document types, not only\nacademic or scientific papers but also newsletters,\nmagazines, reports, posters, advertisements, cam-\npaign materials, pamphlets, brochures, manuals,\nand books."}, {"title": "3.2 Extracting Image-text Pairs", "content": "To create training data for LMMs, we extract image-\ntext pairs from PDF data. The overall process is\nillustrated in Figure 2.\nSelecting PDFs that Contain Images. Before ex-\ntracting images and text from PDFs, we first filter\nout PDFs that do not contain images. A significant\nportion of the PDF data does not include any im-\nages, and many files contain only small logos or\nsymbols rather than meaningful images.\nAfter manually inspecting hundreds of PDFs,\nwe observed that as the number of pages increases,\nPDFs tend to resemble books, where text dominates\nand images are scarce. To address this, we select\nonly PDFs with five or fewer pages. Additionally,\nwe found that images frequently appear on the first\npage of PDFs. If an image is absent on the first\npage, subsequent pages are often image-free as\nwell. Thus, we extract only the first page from each\nselected PDF.\nTo detect whether a PDF contains images, we\nuse a Python library. There are several libraries\navailable for this task (Artifex Software Inc.; Bel-\nval, 2017; pdfminer.six). Among them, we choose\nPyMuPDF (Artifex Software Inc.), which is widely\nused and offers both high speed and accuracy. Us-\ning PyMuPDF, we identify PDFs that contain im-\nage data and filter out those without images. As a\nresult, we select 200K PDFs. Since we only use the\nfirst page of each PDF, this amounts to a total of\n200K PDF pages.\nExtracting Image and Text through Layout\nAnalysis and OCR. The PyMuPDF library used\nfor PDF selection directly reads a PDF and extracts\nimages and text stored within the file. However, this\napproach sometimes leads to issues. For instance,\nPyMuPDF may extract images that are invisible to\nthe human eye within a PDF. Additionally, in some\ncases, it breaks down visible images and extracts\nthem separately based on layout elements. For ex-\nample, in some cases, the background and objects\nwithin an image are extracted separately.\nTo prevent such issues, we first convert a PDF\ninto a JPEG image and then extract images and\ntext from it. This ensures that only images visible\nto the human eye are extracted. To extract images\nand text, we use Surya (Paruchuri, 2024), a tool\ndesigned for PDF analysis based on image inputs.\nSurya performs both layout analysis (Shen et al.,\n2021) and OCR. First, layout analysis identifies\nimage and text regions within the PDF. Then, OCR\nis applied to the text regions to extract the text.\nThrough this process, we obtain both images and\ntext.\nSurya employs pretrained deep learning models\nfor layout analysis and OCR, supporting over 90\nlanguages, making it applicable to Japanese PDFs.\nHowever, their performance is not perfect. For ex-\nample, despite processing Japanese text, it occa-\nsionally misidentifies characters as Hindi. Addi-\ntionally, we filter out images detected by Surya\nif their width or height is less than 50 pixels, as\nmany non-image elements were mistakenly clas-"}, {"title": "3.3 Generating Instruction Data", "content": "Image-text pairs can be directly used for LMM\ntraining; however, their effectiveness was limited\n(see Table 6). Thus, instead of using them as they\nare, we followed the LLaVA (Liu et al., 2023) ap-\nproach and generated instruction data using GPT.\nStrictly speaking, our method differs slightly\nfrom LLaVA. At the time LLaVA was developed,\nGPT-4 (OpenAI, 2023) could not process image\ninputs, which likely explains why image data was\nnot provided to GPT-4 during instruction genera-\ntion. However, by the time of our study, GPT-40\nhad been released, enabling image recognition.\nTherefore, we directly feed images to GPT-40-\nmini to generate instruction data. The paired text\nassociated with each image is used as context in-\nformation during instruction generation. For the\nprompt, we slightly modify the final part of the\nLLaVA prompt, ensuring that the responses are\ngenerated in Japanese. The actual prompt used is\nprovided in Table B.\nThrough our experiments, we found that when\nthe paired text matched with an image is imperfect,\nit is more effective to generate instruction data us-\ning only the image, rather than including the paired\ntext. Further details on this can be found in \u00a75.6.\nTherefore, in our experiments, all instruction data,"}, {"title": "4 Training Japanese LMM", "content": "Recently, various open-source Japanese LMMs\nhave been released (Shing and Akiba, 2023a,b; Ak-\niba et al., 2025; Tanahashi et al., 2023; Inoue et al.,\n2024; Lab, 2024; Sasagawa et al., 2024). Among\nthem, we adopt the widely used LLaVA (Liu et al.,\n2023) framework to evaluate the effectiveness of\nPDF data, specifically using LLaVA1.5 (Liu et al.,\n2024a). Figure 4 illustrates the LLaVA1.5 frame-\nwork. Most hyperparameters follow the original\nLLaVA1.5 settings, with a few modifications. We\nreplace the vision encoder CLIP (clip-vit-large-\npatch14-336) (Radford et al., 2021) with SigLIP\n(siglip-so400m-patch14-384) (Zhai et al., 2023)\nand experiment with different large language mod-\nels (LLMs) instead of Vicuna-7B (Chiang et al.,\n2023). Details on the LLM selection will be dis-\ncussed later. For training, we employ LoRA (Hu\net al., 2022) for parameter-efficient finetuning."}, {"title": "4.1 Training Procedure", "content": "LLaVA1.5 training consists of two stages, aiming\nto integrate the pretrained vision encoder and pre-\ntrained LLM to create an LMM that effectively\nhandles vision input.\nStage 1: Pretraining. In this stage, only the vision-\nlanguage connector (MLP) is trained using image-\ntext pairs, linking the vision encoder with the LLM.\nStage 2: Instruction Tuning. Using visual instruc-\ntion data, both the MLP and LLM are instruction-\ntuned to improve multimodal understanding.\nStage 3: Continual Fine-Tuning (CFT). In our\nstudy, we introduce an additional CFT stage after\nStages 1 and 2. Here, we perform CFT on both the\nMLP and LLM using PDF data.\nTraining Data. Table 1 presents the details of\nthe training data. In Stage 1 (Pretraining) and\nStage 2 (Instruction Tuning), we use the Japanese-\ntranslated version of the original LLaVA train-\ning data (Inoue et al., 2024), translated using\nDeepL (DeepL). Specifically, Stage 1 uses 558K\nsamples from LLaVA-Pretrain-JA (Motors, 2024a),\nStage 2 uses 620K samples from LLaVA-v1.5-\nInstruct-620K-JA (Motors, 2024b), and Stage 3\nuses 362K PDF-derived samples created in \u00a73.\nElapsed Time for Training. LLaVA1.5 is trained\nfor one epoch per stage. Training LLaVA1.5 with a\nLlama3 8B-based LLM using four NVIDIA A100\nGPUs took about 11 hours for Stage 1, 42 hours\nfor Stage 2, and 19 hours for Stage 3."}, {"title": "4.2 LLM Selection", "content": "Japanese LLM. We train LLaVA1.5 using three\nwell-known Llama3-8B-based Japanese LLMs:\nSuzume (suzume-llama-3-8B-japanese (Devine,\n2024)), ELYZA (Llama-3-ELYZA-JP-8B (Hi-\nrakawa et al., 2024)), and Swallow (Llama-3-\nSwallow-8B-Instruct-v0.1 (Okazaki et al., 2024))\nGeneral (Non-Japanese) LLM. To verify whether\nJapanese PDF data is effective in adapting a gen-\neral (non-Japanese) LLM into a Japanese LMM,\nwe use three non-Japanese LLMs: Llama3 (Llama-\n3-8B-instruct (Dubey et al., 2024)), Phi3-mini (Phi-\n3-mini-4k-instruct (Abdin et al., 2024), 3.8B pa-\nrameters), and Phi3-medium (Phi-3-medium-4k-\ninstruct (Abdin et al., 2024), 14B parameters)."}, {"title": "5 Experiments and Analysis", "content": ""}, {"title": "5.1 Evaluation Metric", "content": "To evaluate Japanese LMMs, we adopt the evalu-\nation method used in Heron-Bench (Inoue et al.,\n2024), a standard Japanese LMM benchmark. The\nauthors of Heron-Bench provide three evalua-\ntion datasets: JA-LLaVA-Bench (COCO) and JA-\nLLaVA-Bench (Wild), which are Japanese transla-\ntions of LLaVA-Bench(Liu et al., 2023), and Heron-\nBench, specifically designed for Japanese evalua-\ntion. The details of them are presented in Table 2.\nHeron-Bench follows the same score calcu-\nlation method as LLaVA-Bench. First, GPT-4\n(gpt-4-0125-preview) (OpenAI, 2023) generates\nreference answers using the question's context.\nThen, GPT-4 evaluates the answers of both the\nLMM and the reference answers using the LLM-\nas-a-judge approach (Zheng et al., 2024). The final\nscore is calculated as the ratio (%) of the average\nscore of the LMM's answers to the average score of\nGPT-4's reference answers. A score of 100% indi-\ncates performance on par with GPT-4, while scores\nabove 100% suggest that the LMM outperforms\nGPT-4.\nLMMs used for comparison. We use three\nproprietary LMMs-GPT-4V (OpenAI, 2023),\nClaude 3 Opus (Anthropic, 2024), and Gemini\nPro (Team et al., 2024)\u2014along with seven open-\nsource LMMs\u2014LLaVA 1.6 7B (Liu et al., 2024b),\nLLaVA 1.5 7B (Liu et al., 2023), Qwen-VL 7B (Bai\net al., 2023), Japanese StableVLM 7B (Shing and\nAkiba, 2023b), EvoVLM-JP-v1-7B (Akiba et al.,"}, {"title": "5.2 Main Result", "content": "Table 3 presents the results of our model trained\non PDF-derived data. Compared to existing mod-\nels from the Heron-Bench paper, our LLaVA1.5-\nSwallow outperforms most open-source Japanese\nLMMs. It lags behind Heron BLIP v1 7B by\nonly 1.3% on JA-LLaVA-Bench (COCO). On\nJA-LLaVA-Bench (Wild), it achieves a perfor-\nmance of 65.8%, surpassing the previous best\nof 56.4% by 9.4%. For Heron-Bench, it outper-\nforms the previous best of 49.7% by a significant\nmargin of 16.1%. Additionally, our LLaVA1.5-\nLlama3, LLaVA1.5-Phi3-mini, and LLaVA1.5-\nPhi3-medium also achieve higher performance than\nexisting models. These results demonstrate that uti-\nlizing PDF data effectively enhances model perfor-\nmance.\nIn the following subsections, we conduct addi-\ntional experiments on training with PDF-derived\ndata and provide a component-wise analysis."}, {"title": "5.3 Is Using PDF-derived Data Effective?", "content": "Table 4 presents the results of LLaVA1.5 training\nfor each LLM across different stages. Overall, con-\ntinual fine-tuning (CFT) on PDF-derived data is\neffective. Immediately after Stage 1 (Pretraining),\nall LLMs exhibit relatively low performance. Af-\nter Stage 2 (Instruction tuning), Heron-Bench per-\nformance improves by approximately 20%-30%.\nAfter Stage 3 (CFT on PDF data), Heron-Bench\nperformance further increases by at least 3.9%\n(Llama3) and up to 13.8% (Phi3-mini).\nFor Stage 3, we also show experiments with in-\ncreasing amounts of PDF data, where every ad-\nditional 50K PDFs adds approximately 90K new\ninstruction data. Figure 5 demonstrates the perfor-\nmance improvement on Heron-Bench when apply-\ning CFT on PDF data, highlighting the effective-\nness of PDF data. However, performance does not\nalways increase as the amount of PDF data grows.\nPerformance does not improve in direct proportion\nto the amount of data, indicating that scaling is not\nstrictly linear.\nAdditionally, Japanese PDF data is also effective\nin adapting a general (non-Japanese) LLM into a\nJapanese LMM. To verify this, we conducted the\nsame experiment using Llama3 and Phi3, both non-\nJapanese LLMs, as described in \u00a74.2. As shown\nin Table 3 and Figure 5, PDF data is effective for\ntraining these models as Japanese LMMs.\nMoreover, PDF data is effective across various\nmodel sizes. It benefits not only models based on"}, {"title": "5.4 Which Japanese LLM Performs Best?", "content": "We trained LLaVA1.5 up to Stage 1 and 2 using\nthe three commonly used Japanese LLMs intro-\nduced in \u00a74.2 (Suzume, ELYZA, and Swallow).\nTable 5 presents the results. We found that Swal-\nlow achieved the best performance across all three\nbenchmarks, and therefore, we selected Swallow\nas the Japanese LLM for this study."}, {"title": "5.5 What If We Use Raw Image-Text Pairs\nWithout Generating Instruction Data?", "content": "Instead of generating instruction data from ex-\ntracted image-text pairs, an LMM can be trained\ndirectly on them. We trained LLaVA1.5-Swallow\nwith 50K PDF data using extracted image-text pairs.\nTable 6 presents the results.\nThe results show that performance generally de-\ncreases compared to LLaVA1.5 trained only up to"}, {"title": "5.6 Is Paired Text Effective for Generating\nInstruction Data?", "content": "When generating instruction data, we use paired\ntext matched with images as context. However, the\neffectiveness of this paired text remains uncertain.\nSince the text extracted from PDFs is often imper-\nfect and contains noise, its usefulness for training\nmay be uncertain. To investigate this, we generate\ninstruction data using different data sources and\npresent the results in Table 7. We compare three\ncases: using only images, using images with paired\ntext, and using images with PDF-style text.\nThe key takeaway from our experiment is that\ninstruction data generated using only images per-\nformed best. This suggests that even extracting only\nimage data from PDFs is valuable, and when paired\ntext is inaccurate, using images alone can yield bet-\nter performance. Thus, for experiments using 200K\nPDFs, we generated instruction data using only\nimages.\nFrom these results, we conclude that paired text\ncontaining noise is not effective. However, the re-\nsults also show that PDF-style text improves per-\nformance compared to paired text. This implies\nthat if text data is more accurately extracted from\nPDFs, the quality of instruction data generated\nfrom image-text pairs can also improve."}, {"title": "6 Conclusion", "content": "We explore the use of Japanese PDF data to en-\nhance LMM training and develop a fully automated\npipeline for extracting image-text pairs. Our experi-\nments show significant performance gains by incor-\nporating PDF-derived data, with up to 13.8% im-\nprovement on Heron-Bench. Further analysis con-\nfirms the effectiveness of PDF-derived data across\ndifferent model sizes and its potential to comple-\nment existing multimodal datasets. These findings\nprovide valuable insights into leveraging PDF data\nfor LMM training and highlight its promise as a\nmultimodal resource. We hope this work encour-\nages further research on leveraging PDF data for\nimproving Japanese LMMs."}, {"title": "Limitations", "content": "Following LLaVA's approach of generating instruc-\ntion data using GPT, we used GPT-40-mini to gen-\nerate instruction data from PDF data. While this\napproach is effective, it is dependent on GPT. To\ngenerate LMM training data without relying on\nGPT, high-quality image-text pair data is essential.\nAchieving this requires improving the performance\nof text extraction models for PDFs.\nCurrently, scaling beyond 100K PDFs has been\nchallenging. For future work, we plan to investigate\nthe underlying causes-whether the bottleneck lies\nin data quality, model capacity, suboptimal training\nsettings (e.g., learning rate, number of epochs), or\nthe limitations of existing test data. By addressing\nthese factors, we believe that scaling to larger PDF\ndatasets will become more feasible."}]}