{"title": "An Introduction to Reinforcement Learning: Fundamental Concepts and Practical Applications", "authors": ["Majid Ghasemi", "Ibrahim Sorkhoh", "Fadi Alzhouri", "Amir Hossein Moosavi", "Anjali Agarwal", "Dariush Ebrahimi"], "abstract": "Reinforcement Learning (RL) is a branch of Artificial Intelligence (AI) which focuses on training agents to make decisions by interacting with their environment to maximize cumulative rewards. An overview of RL is provided in this paper, which discusses its core concepts, methodologies, recent trends, and resources for learning. We provide a detailed explanation of key components of RL such as states, actions, policies, and reward signals so that the reader can build a foundational understanding. The paper also provides examples of various RL algorithms, including model-free and model-based methods. In addition, RL algorithms are introduced and resources for learning and implementing them are provided, such as books, courses, and online communities. This paper demystifies a comprehensive yet simple introduction for beginners by offering a structured and clear pathway for acquiring and implementing real-time techniques.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) is another type of learning in AI that focuses on training by interacting with the environment. In contrast to supervised learning, where an agent learns from labeled examples, or unsupervised learning, which is based on detecting patterns in the data, RL deals with an autonomous agent that must make intuitive decisions and consequently learn from its actions. The key idea is to learn how the world works (e.g., what action gets a reward and which does not) to maximize cumulative rewards over time through trial-and-error exploration [1]. This paper will serve as a crash course in RL, with the aim that readers should be geared up for solving real-world problems using it. This paper considers the readers to have basic knowledge of Machine Learning (ML) algorithms like Supervised and Unsupervised Learning.\nRL revolves around several key concepts: States, Actions, Policies, and Rewards. Each of these components plays a crucial role in defining the agent's interaction with its environment and the overall learning process.\nA State (s) represents a specific condition or configuration of the environment at a given time as perceived by the agent. A state sets the scene for the agent to make choices and select actions. This is the state space, which describes all states from which an agent can choose for each action that occurs. For example, in chess, a state might be one specific layout of pieces on the board. Actions (a) are the set of possible moves or decisions an agent can make while interacting with the environment. A selected action is part of the strategy followed by an agent to reach its desired goals according to its current states and policy. In the chess example, moving a piece from one square to another is an action. A policy (\u03c0) guides the behavior of a learning agent by mapping perceived states of the environment into actions. This could be a simple"}, {"title": "2 K-Armed Bandits", "content": "In this section, we examine a simple RL form with a single state called bandit problems to broaden our understanding of RL in a simple environment to cement the mentioned points in the former section.\nFirstly, we need to understand what is the difference between types of feedback, as the reward signal is a form of a feedback received by agents. In Figure 2, an agent interacts with the environment, takes observations (Ot) at each timestep, and based on these observations, performs actions. There are four possible actions: A1, A2, A3, and A4. Assume the best action a* is A2, but the agent chose A4.\nInstructive feedback indicates the chosen action is wrong and the best action is A2, typical in supervised learning tasks. Evaluative feedback only indicates the chosen action is not good, typical in RL tasks, and evaluative feedback depends entirely on the actions taken. The stated example shows the difference of the feedback received in RL environment.\nIn the following sections, evaluating action-values will be examined in a simple form of RL, which is Immediate Reinforcement Learning (IRL).\nIn order to understand the K-armed bandit problem or IRL, assume that you are required to select from a number of options (actions) repeatedly. You receive a numerical reward (or punishment) after each choice based on a stationary probability distribution. The concept of \"stationarity\" refers to the fact that probability distributions for rewards and transitions remain unchanged over time. For the purpose of simplifying learning, RL algorithms rely on stationarity and model-free methods such as Q-learning. However, this assumption does not always hold in real-world applications, so algorithms for non-stationary environments are required [4].\nFor each of the k actions available, there is an expected average reward, referred to as the value of the action [1]. Let us denote the action chosen at time stept as At and the reward received as Rt. Define q*(a) as the value of an arbitrary action a.\nThe value of an action q*(a) is defined as the expected reward when the agent takes action a at any given time step t. Mathematically, this is represented as:\n$$q^*(a) = E[R_t | A_t = a] $$\nHere, we are defining q* (a) as the expected reward for taking action a. This forms the basis for evaluating actions in the k-armed bandit problem.\nIf we knew the best action to take, solving the problem would be trivial as we would always choose the best action. Without this knowledge, we must estimate each action's value, denoted as Qt(a) at time step t, which should approximate q*(a).\nAfter estimating action values, there is at least one action with the highest estimated value at each time step. These are called greedy actions. Selecting a greedy action exploits current knowledge for immediate reward, while selecting non-greedy actions explores to improve estimates. Balancing exploration and exploitation is crucial in RL. As mentioned earlier, opting for exploitation maximizes immediate reward, while exploration can yield higher overall reward depending on various factors [1, 2].\nEstimating action values through sampling averages is efficient for stationary bandit problems. In real-world RL problems with non-stationary environments, it makes sense to weight recent rewards more. Using a constant step-size parameter is popular. Rewriting the sample-average equation as:\nTo update the action value, we use the following update rule:\n$$Q_{n+1} = Q_n + \\alpha[R_n - Q_n]$$\nHere:\n\u2022 Qn+1: The updated action value after the n-th reward.\n\u2022 Qn: The current action value before the n-th reward.\n\u2022 \u03b1: The learning rate, a parameter that determines the extent to which new information overrides the old information.\n\u2022 Rn: The reward received after taking an action at step n.\nThis update rule incrementally adjusts the action value based on the difference between the received reward and the current estimate, weighted by the learning rate \u03b1. This iterative process allows the agent to refine its value estimates and improve decision-making over time.\nTo estimate action-values in K-armed bandit problems, we use the following equation, which represents the action value Qt(a) as the average of the rewards received from that action up to time step t.\nWe explore techniques for assessing action value and making choices based on these estimates, known as action-value methods. The true value of an action corresponds to its average reward upon selection, estimated by averaging received rewards, as written in equation below:\n$$Q_t(a) = \\frac{\\text{sum of rewards when the action a is taken prior to time step t}}{\\text{number of times the action a is taken prior to time step t}}$$\nQt(a) can be mathematically expressed as:\n$$Q_t(a) = \\frac{\\sum_{i=1}^{t-1} R_i \\cdot \\mathbb{1}\\{A_i = a\\}}{\\sum_{i=1}^{t-1} \\mathbb{1}\\{A_i = a\\}}$$\nwhere $\\mathbb{1}\\{A_i=a\\}$ is the indicator function. The indicator function $\\mathbb{1}\\{A_i=a\\}$ is used to count the number of times action a has been taken up to time step t.\n$$\\mathbb{1}\\{A_i=a\\} = \\begin{cases}\n1 & \\text{if action a is taken at time step i} \\\\\n0 & \\text{otherwise}\n\\end{cases}$$\nIf the denominator is zero, Qt(a) is defined as a default value (e.g., 0). It is noted to be that as the denominator approaches infinity, Qt(a) converges to q*(a) by the law of large numbers [5].\nAn estimated action value is calculated based on an average of reward samples using the sample-average method. This method may not be the most effective, but it can be used as a starting point for selecting actions based on the estimates [1]."}, {"title": "3 Finite Markov Decision Process", "content": "This section describes the formal problem of finite Markov decision processes (MDPs). An MDP provides a framework for sequential decision-making in which actions affect immediate rewards as well as future outcomes. In MDPs, immediate rewards are balanced with delayed rewards. In contrast to bandit problems in which the goal is to determine the value of each action a, MDPs aim to measure the value of taking action a in states, or the value of being in state s assuming optimal actions are taken. A correct assessment of the long-term effects of interventions requires the estimation of these state-specific values [1]. Finite MDPs consist of states, actions, and rewards (S, A, R). Discrete probability distributions are assigned to the random variables Rt and St based on the preceding state and action. Using the probabilities of occurrence of the random variables Rt and St, derive equations for these variables. A system is considered Markovian when the outcome of an action is independent of past actions and states, relying solely on the current state [10]. The Markov property requires the state to encapsulate significant details of the entire past interaction influencing future outcomes [11]. This definition is the basis of MDPs being used in RL. To describe the dynamics of an MDP, we use the state-transition probability function p(s', r | s, a), which is defined as follows:\n$$p(s', r | s, a) = Pr\\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\\}$$\nwhere the function p defines the MDP dynamics.\nThe following state-transition probabilities, state-action and state-action-next-state triple rewards can be derived from the four-argument dynamic function p. We can derive the state-transition probabilities, the expected reward for state-action pairs, and the expected rewards for state-action-next-state triples as follows:\n$$p(s' | s, a) = Pr\\{S_t = s' | S_{t-1} = s, A_{t-1} = a\\} = \\sum_{r \\in R} p(s', r | s, a)$$\n$$r(s, a) = E\\{R_t | S_{t-1} = s, A_{t-1} = a\\} = \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r | s, a)$$\n$$r(s, a, s') = E\\{R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'\\} = \\frac{\\sum_{r \\in R} r p(s', r | s, a)}{p(s' | s, a)}$$\nThe concept of actions encompasses any decisions relating to learning, and the concept of states encompasses any information that is available in order to inform those decisions. As part of the MDP framework, goal-directed behavior is abstracted through interaction. Any learning problem can be reduced to three signals between an agent and its environment: actions, states, and rewards. A wide range of applications have been demonstrated for this framework [1].\nWe are now able to formally define and solve RL problems. We have defined rewards, objectives, probability distributions, the environment, and the agent. Some concepts, however, were defined informally. According to our statement, the agent seeks to maximize future rewards, but how can this be mathematically expressed?\nThe return, denoted Gt, is the cumulative sum of rewards received from time step t onwards. For episodic tasks, it is defined as follows:\nLet the sequence of rewards received after time step t be:"}, {"title": "4 Policies and Value Functions: Zero to Hero", "content": "The value function estimates the expected return of the agent being in a certain state (or performing an action in a particular state). Depending on the actions selected, these factors will vary. There is a link between value functions and policies, which are linked to probabilities of action based on states. Value functions can be divided into two broad categories as follows:\nState Value Functions: The value function of a state s under policy \u03c0, \u03c5\u03c0(s), is the expected return starting in s and following \u03c0 thereafter.\nAction Value Functions: The value of taking action a in state s under policy \u03c0, q\u03c0(s,a), is the expected return starting from s, taking action a, and following \u03c0 thereafter.\nFor MDPs, v and q are defined as:\nThe state value function \u03c5\u03c0(s) represents the expected return starting from state s and following policy \u03c0. It is mathematically defined as follows:\n$$\\upsilon_{\\pi}(s) = E_{\\pi} \\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s\\right] \\quad \\text{for all } s \\in S$$\nThe action value function q\u03c0 (s, a) represents the expected return starting from state, taking action a, and then following policy \u03c0. It is defined as follows:\n$$q_{\\pi}(s, a) = E_{\\pi}[G_t | S_t = s, A_t = a] = E_{\\pi} \\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s, A_t = a\\right]$$\nIt is important to note the difference between v and q, namely that q depends on the actions taken in each state. With ten states and eight actions per state, q requires 80 functions, while v requires only 10 functions.\nFollowing policy \u03c0, if an agent averages returns from each state, the average converges to v\u03c0 (s). Averaging returns from each action converges to q\u03c0 (s, a) [1].\nIn Monte Carlo methods, many random samples of returns are averaged. This method does not provide sample efficiency, requiring separate averages to be calculated for each state. Estimates can be improved through the use of parameterized functions that have fewer parameters. v should be written recursively as follows:\n$$\\upsilon_{\\pi}(s) = E_{\\pi}[G_t | S_t = s] = E_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s] = \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s', r|s, a) [r + \\gamma \\upsilon(s')]$$\nThis is the Bellman equation for \u03c5\u03c0. The Bellman equation relates the value of a state to its potential successor states' values. The diagram illustrates the anticipation from a state to its successors. The value of the initial state equals the discounted value of the expected next state plus the anticipated reward [12, 1]."}, {"title": "4.1 When to Use What?", "content": "State-value functions \u03c5\u03c0(s) and action-value functions q(s, a) serve different purposes in RL.\nIn the evaluation of deterministic policies or when understanding the value of being in a particular state is required, state-value functions are used. In policy evaluation and policy iteration methods, where a policy is explicitly defined and it is necessary to evaluate the performance of being in a particular state under the policy, these methods are highly useful. The use of state-value functions is beneficial when there are many actions, since they reduce complexity by requiring only an evaluation of state values.\nAction-value functions are used to evaluate and compare the potential for different actions when they are taking place in the same state. They are crucial for the selection of actions, such as in Q-learning and SARSA, where the goal is to determine the most appropriate action for each situation. As action-value functions take into account the expected return of different actions, they are particularly useful in environments with stochastic policies. Moreover, when dealing with continuous action spaces, action-value functions can provide a more detailed understanding of the impact of actions, aiding in the fine-tuning of policy implementation.\nConsider a gambling scenario where a player starts with $10 and faces decisions regarding the amount to bet. This game illustrates state and action value functions in RL.\nState Value function (v(s)): The state value function v\" (s) quantifies expected cumulative future rewards for a states, given policy \u03c0. Suppose the player has $5:\n\u2022 With a consistent $1 bet, v\" (5) = 0.5 indicates an expected gain of $0.5.\n\u2022 With a consistent $2 bet, v\" (5) = \u22121 indicates an expected loss of $1.\nAction Value function (q\" (s,a)): The action value function q\" (s, a) assesses expected cumulative future rewards for action a in state s. For instance:\n\u2022 q\" (5,1) = 1 suggests a $1 bet from $5 results in a cumulative reward of $1.\n\u2022 q (5,2) = -0.5 indicates a loss of $0.5 for a $2 bet from $5.\nThis gambling game scenario highlights the role of state and action value functions in RL, guiding optimal decision-making in dynamic environments.\nSolving an RL task involves identifying a policy that maximizes long-term rewards. For finite MDPs, an optimal policy is defined in the next subsection."}, {"title": "4.2 Optimal Policies and Optimal Value Functions", "content": "Value functions create a partial ordering over policies, allowing comparison and ranking based on expected cumulative rewards. A policy is better than or equal to \u03c0\u03bf if v\" (s) > \u03c5\u03c0\u03bf (s) for all states s. An optimal policy is better than or equal to all other policies, denoted by \u03c0*, sharing the same optimal state-value function v*:\nThe optimal state value function v*(s) is defined as the maximum value function over all possible policies:\n$$v^*(s) = \\max_{\\pi} v^{\\pi}(s) \\quad \\text{for all } s \\in S$$\nOptimal policies also share the same optimal action-value function q*:\nThe optimal action value function q*(s, a) is defined as the maximum action value function over all possible policies:\n$$q^*(s, a) = \\max_{\\pi} q^{\\pi}(s, a) \\quad \\text{for all } s \\in S$$\nThe relationship between the optimal action value function q*(s, a) and the optimal state value function v*(s) is given by the following equation:\n$$q^*(s, a) = E[R_{t+1} + \\gamma v^*(S_{t+1}) | S_t = s, A_t = a]$$\nThe equation expresses expected cumulative return for state-action pairs in terms of immediate rewards and discounted future states' values.\nOptimal value functions and policies represent an ideal state in RL. It is however rare to find truly optimal policies in computationally demanding tasks due to practical challenges. RL agents strive to approximate optimal policies. Dynamic Programming (DP) helps identify optimal values, assuming a perfect model of the environment.\nThe fundamental idea of DP and RL is using value functions to organize the search for good policies. For finite MDPs, the environment's dynamics are given by probabilities p(s', r | s,a). DP finds exact solutions in special cases, like finding the shortest path in a graph.\nThe Bellman optimality equations for the optimal state value function v*(s) and the optimal action value function q*(s, a) are given as follows:\n$$v^*(s) = \\max_{a} E[R_{t+1} + \\gamma v^*(S_{t+1}) | S_t = s, A_t = a] = \\max_{a} \\sum_{s',r} p(s', r | s, a)[r + \\gamma v^*(s')]$$\n$$q^*(s, a) = E[R_{t+1} + \\max_{a'} q^*(S_{t+1}, a') | S_t = s, A_t = a] = \\sum_{s',r} p(s', r | s, a)[r + \\gamma \\max_{a'} q^*(s', a')]$$\nDP algorithms are derived by transforming Bellman equations into update rules."}, {"title": "4.3 Policy Evaluation (Prediction)", "content": "Policy evaluation, also known as prediction, involves computing the state-value function v\" for a given policy \u03c0. This process assesses the expected return when following policy from each state. The state-value function \u03c5\u03c0(s) is defined as the expected return starting from state s and following policy \u03c0:\n$$\\upsilon_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s]$$\nThis can be recursively expressed as:\n$$\\upsilon_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma \\upsilon_{\\pi}(S_{t+1}) | S_t = s] = \\sum_{a} \\pi(a | s) \\sum_{s', r} p(s', r|s, a)[r + \\gamma \\upsilon_{\\pi}(s')]$$\nIn these equations, \u03c0(\u03b1 | s) denotes the probability of taking action a in states under policy \u03c0. The existence and uniqueness of v\" are guaranteed if y < 1 or if all states eventually terminate under \u03c0. Dynamic Programming (DP) algorithm updates are termed \"expected updates\u201d because they rely on the expectation over all potential next states, rather than just a sample [1]."}, {"title": "4.4 Policy Improvement", "content": "The purpose of calculating the value function for a policy is to identify improved policies. Assume v\" for a deterministic policy \u03c0. For a state s, should we alter the policy to select action a \u2260 \u03c0(s)? We know the effectiveness of adhering to the existing policy from state s (v(s)), but would transition to a new policy yield a superior outcome? We can answer this by selecting action a in s and then following \u03c0:\nTo determine if a policy can be improved, we compare the value of taking a different action a in state s with the current policy. This is done using the action value function q(s,\u03b1):\n$$q_{\\pi}(s,a) = E[R_{t+1} + \\gamma \\upsilon_{\\pi}(S_{t+1}) | S_t = s, A_t = a] = \\sum_{s',r} p(s', r | s,a)[r + \\gamma \\upsilon_{\\pi}(s')]$$\nThe key criterion is whether this value exceeds \u03c5\u03c0 (s). If q\u03c0(s, a) > \u03c5\u03c0 (s), consistently choosing action a in s is more advantageous than following \u03c0, leading to an improved policy \u03c0'."}, {"title": "4.5 Policy Improvement Theorem", "content": "The policy improvement theorem states that if q\u2084(s, \u03c0'(s)) \u2265 \u03c5\u03c0(s) for all states s, then the new policy \u03c0' is at least as good as the original policy \u03c0. Formally, it is expressed as follows:\nLet and \u03c0' be deterministic policies such that for all s \u2208 S:\n$$q_{\\pi}(s, \\pi'(s)) \\geq \\upsilon_{\\pi}(s)$$\nIf \u03c0' achieves greater or equal expected return from all states s\u2208 S:\n$$\\upsilon_{\\pi'}(s) \\geq \\upsilon_{\\pi}(s)$$\nIf there is strict inequality at any state, \u03c0' is superior to \u03c0. Extend this to all states and actions, selecting the action that maximizes q(\u03c2, \u03b1):\nThe new policy \u03c0' is obtained by selecting the action that maximizes the action value function q\u016b (s, a):\n$$\\pi'(s) = arg \\max_{a} q_{\\pi}(s, a) = arg \\max_{a} E[R_{t+1} + \\gamma \\upsilon_{\\pi}(S_{t+1}) | S_t = s, A_t = a] = arg \\max_{a} \\sum_{s',r} p(s',r | s,a)[r + \\gamma \\upsilon_{\\pi}(s')]$$\nPolicy improvement creates a new policy that enhances an initial policy by adopting a greedy approach based on the value function.\nAssume \u03c0' is equally effective as a but not superior. Then \u03c5\u03c0 = \u03c5\u03c0', ensuring for all states s\u2208 S:\nThe relationship between the optimal state value function v*(s) and the optimal action value function q*(s, a) is given by the following equation:\n$$\\upsilon_{\\pi'}(s) = \\max_{a} E[R_{t+1} + \\gamma \\upsilon_{\\pi'}(S_{t+1}) | S_t = s, A_t = a] = \\max_{a} \\sum_{s',r} p(s',r | s, a)[r + \\gamma \\upsilon_{\\pi'}(s')]$$\nThis is the Bellman optimality equation, implying \u03c5\u03c0' = v* and both and are optimal policies. Policy improvement yields a superior policy unless the initial policy is already optimal.\nThis concept extends to stochastic policies. Stochastic policies introduce a set of probabilities for actions, with the action most aligned with the greedy policy assigned the highest probability."}, {"title": "4.6 Policy Iteration", "content": "After enhancing a policy using v to derive an improved policy \u03c0', compute \u03c5\u03c0' and further refine it to obtain a superior policy \u03c0\". This process generates a sequence of improving policies and corresponding value functions:\nThe process of policy iteration involves alternating between policy evaluation and policy improvement to obtain a sequence of improving policies and value functions:"}, {"title": "5 Value Iteration", "content": "One limitation of policy iteration is that each iteration requires policy evaluation, often necessitating multiple passes through the entire state set [13]. To address this, policy evaluation can be abbreviated without losing convergence guarantees. This method, known as value iteration, terminates policy evaluation after a single sweep. It combines policy improvement with a truncated form of policy evaluation. Value iteration merges one pass of policy evaluation with policy improvement in each iteration, ensuring convergence to an optimal policy for discounted finite MDPs [14].\nThe update rule for value iteration is given by:\n$$\\upsilon_{k+1}(s) = \\max_{a} E[R_{t+1} + \\gamma \\upsilon_{k}(S_{t+1}) | S_t = s, A_t = a] = \\max_{a} \\sum_{s',r} p(s', r|s, a)[r + \\gamma \\upsilon_{k}(s')]$$\nValue iteration combines one sweep of policy evaluation and policy improvement in each iteration. It converges to an optimal policy for discounted finite MDPs [14].\nPolicy iteration involves two processes: policy evaluation aligns the value function with the current policy, and policy improvement makes the policy greedier based on the value function. These processes iteratively reinforce each other until an optimal policy is obtained.\nIn value iteration, the key advantage is its efficiency, as it reduces the computational burden by merging policy evaluation and improvement into a single update step. This method is particularly useful for large state spaces where full policy evaluation at each step of policy iteration is computationally prohibitive [1]. Additionally, value iteration can be implemented using a synchronous update approach, where all state values are updated simultaneously, or an asynchronous update approach, where state values are updated one at a time, potentially allowing for faster convergence in practice [13].\nAnother notable aspect of value iteration is its robustness to initial conditions. Starting from an arbitrary value function, value iteration iteratively refines the value estimates until convergence, making it a reliable method for finding optimal policies even when the initial policy is far from optimal [15].\nFurthermore, value iteration provides a foundation for more advanced algorithms, such as Q-learning and other reinforcement learning techniques, by illustrating the principle of bootstrapping, where the value of a state is updated based on the estimated values of successor states. This principle is central to many modern reinforcement learning algorithms that seek to balance exploration and exploitation in dynamic and uncertain environments [16]."}, {"title": "6 Terminology", "content": "Understanding the various methodologies and concepts within RL is essential for the effective design and implementation of RL algorithms. Methods in RL can be classified as either off-policy or on-policy, as well as model-free and model-based. These categories offer different approaches and techniques for learning from interactions with the environment."}, {"title": "6.1 Model-Free Methods", "content": "Model-free methods determine the optimal policy or value function directly without constructing a model of the environment. There is no requirement for them to know transition probabilities and rewards, as they learn entirely from observed states, actions, and rewards. Compared with model-based methods, model-free methods are simpler to implement, relying on experience-based learning. There are two primary types:\nValue-based methods focus on learning the action-value function to derive an optimal policy. For instance, Q-learning (discussed in section 8) is an off-policy algorithm that learns the value of the optimal policy independently of the agent's actions by using a max operator in its update rule. SARSA (also discussed in section 8), on the other hand, is an on-policy algorithm that updates its Q-values based on the actions actually taken by the policy. Both methods update their action-value estimates based on the Bellman equation until convergence. In contrast, policy-based methods, like REINFORCE (discussed in section 8), work by directly learning the policy without explicitly learning a value function. These methods adjust the policy parameters directly by following the gradient of the expected reward. This approach is particularly useful in environments with high-dimensional action spaces where value-based methods may not be effective. Policy-based methods are also capable of handling stochastic policies, providing a natural framework for dealing with uncertainty in action selection.\nIn addition to these primary types, there are also hybrid approaches that combine value-based and policy-based methods, such as Actor-Critic algorithms (which will be discussed in section 8). These methods consist of two components: an actor that updates the policy parameters in a direction suggested by the critic, and a critic that evaluates the action-value function. Combining both types of learning is intended to provide more stable and efficient learning [17].\nThe best way to understand hybrid approaches is to imagine a robot vacuum cleaner navigating a living room and cleaning it efficiently. It is imperative that the robot decides the best course of action in order to cover the entire area while avoiding obstacles and maximizing its battery life.\nAs part of the value-based component, the robot estimates the value of being in each location in the living room based on a value-based method. The robot learns a state-value function that indicates how much dirt should be removed if it starts at a specific location and follows a specific strategy. Through this component, the robot understands the long-term benefits of being in different locations. Simultaneously, the robot uses a policy-based method to determine the right action to take (for example, moving forward, turning left, turning right). As a result of the policy, the robot adjusts parameters to improve its decision-making process. For example, if moving forward usually results in more dirt being cleaned, the robot is more likely to choose this action in the future when it encounters similar circumstances.\nWhen these two components are combined, the robot is able to navigate and clean the living room more effectively. A value-based approach provides a broader understanding of which areas are most valuable to visit, while a policy-based approach focuses on making the best decisions for the present based on the current situation. The hybrid approach ensures that the robot not only plans its long-term strategy effectively, but also reacts appropriately to immediate circumstances, resulting in an overall more efficient cleaning process.\nAnother significant advancement in model-free methods is the development of Deep RL (DRL) By integrating deep neural networks with traditional RL algorithms, methods such as Deep Q-Networks (DQN) [18] and Proximal Policy Optimization (PPO) [19] have achieved remarkable success in complex, high-dimensional environments, including games and robotic control tasks. The advancement of these technologies has opened up new possibilities for the application of RL to real-world problems, enabling the demonstration of robust performance in domains which were previously intractable. It is beyond the scope of this paper to discuss these algorithms, and wee refer you to [20, 3, 21, 22] to understand DRL deeply and effectively."}, {"title": "6.2 Model-Based Methods", "content": "It is possible to predict the outcomes of actions using model-based methods, which facilitate strategic planning and decision-making. The use of these methods enhances learning efficiency by providing opportunities for virtual experimentation, despite the complexity of developing and refining accurate models [7].\nAutonomous driving systems are an example of how model-based methods can be applied in the real world. As autonomous vehicles navigate in dynamic environments, obstacle avoidance, and optimal routing must be made in real time.\nAutonomous vehicles create detailed models of their environment. These models include static elements, such as roads and buildings, as well as dynamic elements, such as other vehicles and pedestrians. Sensor data, including cameras, LIDAR, and radar, are used to build this model. Through the use of the environmental model, the vehicle is capable of predicting the outcome of various actions. For instance, when a vehicle considers changing lanes, it uses its model to predict the behavior of surrounding vehicles to determine the safest and most efficient way to make the change. The model assists the vehicle in planning its route and making strategic decisions. To minimize travel time, avoid congestion, and enhance safety, it evaluates different routes and actions. Simulation allows the vehicle to select the best course of action by simulating various scenarios before implementation in the real world. The vehicle, for example, may use the model to simulate different actions in the event of a busy intersection, such as waiting for a gap in traffic or taking an alternate route. Considering the potential outcomes of each action, the vehicle can make an informed decision that balances efficiency with safety. In addition to improving the ability of autonomous vehicles to navigate safely and efficiently in real-world conditions, this model-based approach enables them to make complex decisions with a high level of accuracy. As a result of continuously refining the model based on new data, the vehicle is able to enhance its decision-making capabilities over time, thereby improving performance and enhancing safety on the road.\nThere are several advantages to using model-based methods over methods that do not use models. By simulating future states and rewards, they can plan and evaluate different action sequences without interacting directly with the environment. It is believed that this capability may lead to a faster convergence to an optimal policy, since learning can be accelerated by leveraging the model's predictions. A model-based approach can also adapt more quickly to changes in the environment, since it enables the model to be updated and re-planned accordingly. Although model-based methods have many advantages, they also face a number of challenges, primarily in regards to accuracy and computational cost. In order to create an accurate model of the environment, a high-fidelity model needs to be created. Moreover, the planning process may be computationally expensive, especially in environments with a large number of states and actions. However, advances in computing power and algorithms continue to improve the feasibility and performance of model-based methods, making them a valuable approach in RL [23]."}, {"title": "6.3 Off-Policy and On-Policy Methods", "content": "On-policy and off-policy learning are methodologies within model-free learning approaches, not relying on environment transition probabilities. They are classified based on the relationship between the behavior policy and the updated policy [1]."}, {"title": "6.3.1 On-Policy Methods", "content": "On-policy methods evaluate and improve the policy used to make decisions", "integrated.\nExample": "Consider a customer service chatbot that"}]}