{"title": "An Introduction to Reinforcement Learning: Fundamental Concepts and Practical Applications", "authors": ["Majid Ghasemi", "Ibrahim Sorkhoh", "Fadi Alzhouri", "Amir Hossein Moosavi", "Anjali Agarwal", "Dariush Ebrahimi"], "abstract": "Reinforcement Learning (RL) is a branch of Artificial Intelligence (AI) which focuses on training agents to make decisions by interacting with their environment to maximize cumulative rewards. An overview of RL is provided in this paper, which discusses its core concepts, methodologies, recent trends, and resources for learning. We provide a detailed explanation of key components of RL such as states, actions, policies, and reward signals so that the reader can build a foundational understanding. The paper also provides examples of various RL algorithms, including model-free and model-based methods. In addition, RL algorithms are introduced and resources for learning and implementing them are provided, such as books, courses, and online communities. This paper demystifies a comprehensive yet simple introduction for beginners by offering a structured and clear pathway for acquiring and implementing real-time techniques.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) is another type of learning in AI that focuses on training by interacting with the environment. In contrast to supervised learning, where an agent learns from labeled examples, or unsupervised learning, which is based on detecting patterns in the data, RL deals with an autonomous agent that must make intuitive decisions and consequently learn from its actions. The key idea is to learn how the world works (e.g., what action gets a reward and which does not) to maximize cumulative rewards over time through trial-and-error exploration [1]. This paper will serve as a crash course in RL, with the aim that readers should be geared up for solving real-world problems using it. This paper considers the readers to have basic knowledge of Machine Learning (ML) algorithms like Supervised and Unsupervised Learning.\nRL revolves around several key concepts: States, Actions, Policies, and Rewards. Each of these components plays a crucial role in defining the agent's interaction with its environment and the overall learning process.\nA State (s) represents a specific condition or configuration of the environment at a given time as perceived by the agent. A state sets the scene for the agent to make choices and select actions. This is the state space, which describes all states from which an agent can choose for each action that occurs. For example, in chess, a state might be one specific layout of pieces on the board. Actions (a) are the set of possible moves or decisions an agent can make while interacting with the environment. A selected action is part of the strategy followed by an agent to reach its desired goals according to its current states and policy. In the chess example, moving a piece from one square to another is an action. A policy (\u03c0) guides the behavior of a learning agent by mapping perceived states of the environment into actions. This could be a simple"}, {"title": "K-Armed Bandits", "content": "In this section, we examine a simple RL form with a single state called bandit problems to broaden our understanding of RL in a simple environment to cement the mentioned points in the former section.\nFirstly, we need to understand what is the difference between types of feedback, as the reward signal is a form of a feedback received by agents. In Figure 2, an agent interacts with the environment, takes observations (Ot) at each timestep, and based on these observations, performs actions. There are four possible actions: A1, A2, A3, and A4. Assume the best action a* is A2, but the agent chose A4.\nInstructive feedback indicates the chosen action is wrong and the best action is A2, typical in supervised learning tasks. Evaluative feedback only indicates the chosen action is not good, typical in RL tasks, and evaluative feedback depends entirely on the actions taken. The stated example shows the difference of the feedback received in RL environment.\nIn the following sections, evaluating action-values will be examined in a simple form of RL, which is Immediate Reinforcement Learning (IRL).\nIn order to understand the K-armed bandit problem or IRL, assume that you are required to select from a number of options (actions) repeatedly. You receive a numerical reward (or punishment) after each choice based on a stationary probability distribution. The concept of \"stationarity\" refers to the fact that probability distributions for rewards and transitions remain unchanged over time. For the purpose of simplifying learning, RL algorithms rely on stationarity and model-free methods such as Q-learning. However, this assumption does not always hold in real-world applications, so algorithms for non-stationary environments are required [4].\nFor each of the k actions available, there is an expected average reward, referred to as the value of the action [1]. Let us denote the action chosen at time step t as At and the reward received as Rt. Define $q_*(a)$ as the value of an arbitrary action a.\nThe value of an action $q_*(a)$ is defined as the expected reward when the agent takes action a at any given time step t. Mathematically, this is represented as:\n$q_*(a) = \\mathbb{E}[R_t | A_t = a]$\nHere, we are defining $q_*(a)$ as the expected reward for taking action a. This forms the basis for evaluating actions in the k-armed bandit problem.\nIf we knew the best action to take, solving the problem would be trivial as we would always choose the best action. Without this knowledge, we must estimate each action's value, denoted as $Q_t(a)$ at time step t, which should approximate $q_*(a)$.\nAfter estimating action values, there is at least one action with the highest estimated value at each time step. These are called greedy actions. Selecting a greedy action exploits current knowledge for immediate reward, while selecting non-greedy actions explores to improve estimates. Balancing exploration and exploitation is crucial in RL. As mentioned earlier, opting for exploitation maximizes immediate reward, while exploration can yield higher overall reward depending on various factors [1, 2].\nEstimating action values through sampling averages is efficient for stationary bandit problems. In real-world RL problems with non-stationary environments, it makes sense to weight recent rewards more. Using a constant step-size parameter is popular. Rewriting the sample-average equation as:\nTo update the action value, we use the following update rule:\n$Q_{n+1} = Q_n + \\alpha[R_n - Q_n]$\nHere:\n\u2022 $Q_{n+1}$: The updated action value after the n-th reward.\n\u2022 $Q_n$: The current action value before the n-th reward.\n\u2022 \u03b1: The learning rate, a parameter that determines the extent to which new information overrides the old information.\n\u2022 $R_n$: The reward received after taking an action at step n.\nThis update rule incrementally adjusts the action value based on the difference between the received reward and the current estimate, weighted by the learning rate \u03b1. This iterative process allows the agent to refine its value estimates and improve decision-making over time.\nTo estimate action-values in K-armed bandit problems, we use the following equation, which represents the action value $Q_t(a)$ as the average of the rewards received from that action up to time step t.\nWe explore techniques for assessing action value and making choices based on these estimates, known as action-value methods. The true value of an action corresponds to its average reward upon selection, estimated by averaging received rewards, as written in equation below:\n$Q_t(a) = \\frac{\\text{sum of rewards when the action a is taken prior to time step t}}{\\text{number of times the action a is taken prior to time step t}}$\n$Q_t(a)$ can be mathematically expressed as:\n$Q_t(a) = \\frac{\\sum_{i=1}^{t-1} R_i\\mathbb{1}\\{A_i = a\\}}{\\sum_{i=1}^{t-1} \\mathbb{1}\\{A_i = a\\}}$\nwhere $\\mathbb{1}\\{A_i = a\\}$ is the indicator function. The indicator function $\\mathbb{1}\\{A_i = a\\}$ is used to count the number of times action a has been taken up to time step t.\n$\\mathbb{1}\\{A_i = a\\} = \\begin{cases} 1 & \\text{if action a is taken at time step i} \\\\ 0 & \\text{otherwise} \\end{cases}$\nIf the denominator is zero, $Q_t(a)$ is defined as a default value (e.g., 0). It is noted to be that as the denominator approaches infinity, $Q_t(a)$ converges to $q_*(a)$ by the law of large numbers [5].\nAn estimated action value is calculated based on an average of reward samples using the sample-average method. This method may not be the most effective, but it can be used as a starting point for selecting actions based on the estimates [1]."}, {"title": "Finite Markov Decision Process", "content": "This section describes the formal problem of finite Markov decision processes (MDPs). An MDP provides a framework for sequential decision-making in which actions affect immediate rewards as well as future outcomes. In MDPs, immediate rewards are balanced with delayed rewards. In contrast to bandit problems in which the goal is to determine the value of each action a, MDPs aim to measure the value of taking action a in states, or the value of being in state s assuming optimal actions are taken. A correct assessment of the long-term effects of interventions requires the estimation of these state-specific values [1]. Finite MDPs consist of states, actions, and rewards (S, A, R). Discrete probability distributions are assigned to the random variables Rt and St based on the preceding state and action. Using the probabilities of occurrence of the random variables Rt and St, derive equations for these variables. A system is considered Markovian when the outcome of an action is independent of past actions and states, relying solely on the current state [10]. The Markov property requires the state to encapsulate significant details of the entire past interaction influencing future outcomes [11]. This definition is the basis of MDPs being used in RL. To describe the dynamics of an MDP, we use the state-transition probability function $p(s', r | s, a)$, which is defined as follows:\n$p(s', r | s, a) = \\Pr\\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\\}$\nwhere the function p defines the MDP dynamics.\nThe following state-transition probabilities, state-action and state-action-next-state triple rewards can be derived from the four-argument dynamic function p. We can derive the state-transition probabilities, the expected reward for state-action pairs, and the expected rewards for state-action-next-state triples as follows:\n$p(s' | s, a) = \\Pr\\{S_t = s' | S_{t-1} = s, A_{t-1} = a\\} = \\sum_{r \\in \\mathbb{R}} p(s', r | s, a)$\n$r(s, a) = \\mathbb{E}\\{R_t | S_{t-1} = s, A_{t-1} = a\\} = \\sum_{s' \\in \\mathbb{S}} \\sum_{r \\in \\mathbb{R}} p(s', r | s, a)$\n$r(s, a, s') = \\mathbb{E}\\{R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'\\} = \\sum_{r \\in \\mathbb{R}} r \\frac{p(s', r | s, a)}{p(s' | s, a)}$\nThe concept of actions encompasses any decisions relating to learning, and the concept of states encompasses any information that is available in order to inform those decisions. As part of the MDP framework, goal-directed behavior is abstracted through interaction. Any learning problem can be reduced to three signals between an agent and its environment: actions, states, and rewards. A wide range of applications have been demonstrated for this framework [1].\nWe are now able to formally define and solve RL problems. We have defined rewards, objectives, probability distributions, the environment, and the agent. Some concepts, however, were defined informally. According to our statement, the agent seeks to maximize future rewards, but how can this be mathematically expressed?\nThe return, denoted Gt, is the cumulative sum of rewards received from time step t onwards. For episodic tasks, it is defined as follows:\nLet the sequence of rewards received after time step t be:"}, {"title": "Policies and Value Functions: Zero to Hero", "content": "The value function estimates the expected return of the agent being in a certain state (or performing an action in a particular state). Depending on the actions selected, these factors will vary. There is a link between value functions and policies, which are linked to probabilities of action based on states. Value functions can be divided into two broad categories as follows:\nState Value Functions: The value function of a state s under policy \u03c0, $\u03c5_\u03c0(s)$, is the expected return starting in s and following \u03c0 thereafter.\nAction Value Functions: The value of taking action a in state s under policy \u03c0, $q_\u03c0(s,a)$, is the expected return starting from s, taking action a, and following \u03c0 thereafter.\nFor MDPs, v and q are defined as:\nThe state value function $\u03c5_\u03c0(s)$ represents the expected return starting from state s and following policy \u03c0. It is mathematically defined as follows:\n$\u03c5_\u03c0(s) = \\mathbb{E}_\u03c0\\left[\\sum_{k=0}^{\\infty} \u03b3^k R_{t+k+1} | S_t = s\\right] \\text{ for all } s \u2208 S$."}, {"title": "When to Use What?", "content": "State-value functions $\u03c5_\u03c0(s)$ and action-value functions $q(s, a)$ serve different purposes in RL.\nIn the evaluation of deterministic policies or when understanding the value of being in a particular state is required, state-value functions are used. In policy evaluation and policy iteration methods, where a policy is explicitly defined and it is necessary to evaluate the performance of being in a particular state under the policy, these methods are highly useful. The use of state-value functions is beneficial when there are many actions, since they reduce complexity by requiring only an evaluation of state values.\nAction-value functions are used to evaluate and compare the potential for different actions when they are taking place in the same state. They are crucial for the selection of actions, such as in Q-learning and SARSA, where the goal is to determine the most appropriate action for each situation. As action-value functions take into account the expected return of different actions, they are particularly useful in environments with stochastic policies. Moreover, when dealing with continuous action spaces, action-value functions can provide a more detailed understanding of the impact of actions, aiding in the fine-tuning of policy implementation.\nConsider a gambling scenario where a player starts with $10 and faces decisions regarding the amount to bet. This game illustrates state and action value functions in RL.\nState Value function ($v^\u03c0(s)$): The state value function $v^\u03c0(s)$ quantifies expected cumulative future rewards for a states, given policy \u03c0. Suppose the player has $5:\n\u2022 With a consistent $1 bet, $v^\u03c0(5)$ = 0.5 indicates an expected gain of $0.5.\n\u2022 With a consistent $2 bet, $v^\u03c0(5)$ = \u22121 indicates an expected loss of $1.\nAction Value function ($q^\u03c0(s,a)$): The action value function $q^\u03c0(s, a)$ assesses expected cumulative future rewards for action a in state s. For instance:\n\u2022 $q^\u03c0(5,1)$ = 1 suggests a $1 bet from $5 results in a cumulative reward of $1.\n\u2022 $q^(5,2)$ = -0.5 indicates a loss of $0.5 for a $2 bet from $5.\nThis gambling game scenario highlights the role of state and action value functions in RL, guiding optimal decision-making in dynamic environments.\nSolving an RL task involves identifying a policy that maximizes long-term rewards. For finite MDPs, an optimal policy is defined in the next subsection."}, {"title": "Optimal Policies and Optimal Value Functions", "content": "Value functions create a partial ordering over policies, allowing comparison and ranking based on expected cumulative rewards. A policy \u03c0 is better than or equal to $\u03c0_0$ if $v^\u03c0(s) \u2265 v^{\u03c0_0}(s)$ for all states s. An optimal policy is better than or equal to all other policies, denoted by \u03c0\u2217, sharing the same optimal state-value function $v_\u2217$:\nThe optimal state value function $v_\u2217(s)$ is defined as the maximum value function over all possible policies:\n$v_\u2217(s) = \\max_\u03c0 v^\u03c0(s)  \\text{ for all } s \u2208 S$\nOptimal policies also share the same optimal action-value function $q_\u2217$:\nThe optimal action value function $q_\u2217(s, a)$ is defined as the maximum action value function over all possible policies:\n$q_\u2217(s, a) = \\max_\u03c0 q^\u03c0(s, a) \\text{ for all } s \u2208 S$\nThe relationship between the optimal action value function $q_\u2217(s, a)$ and the optimal state value function $v_\u2217(s)$ is given by the following equation:\n$q_\u2217(s, a) = \\mathbb{E}[R_{t+1} + \u03b3v_\u2217(S_{t+1}) | S_t = s, A_t = a]$\nThe equation expresses expected cumulative return for state-action pairs in terms of immediate rewards and discounted future states' values.\nOptimal value functions and policies represent an ideal state in RL. It is however rare to find truly optimal policies in computationally demanding tasks due to practical challenges. RL agents strive to approximate optimal policies. Dynamic Programming (DP) helps identify optimal values, assuming a perfect model of the environment.\nThe fundamental idea of DP and RL is using value functions to organize the search for good policies. For finite MDPs, the environment's dynamics are given by probabilities p(s', r | s,a). DP finds exact solutions in special cases, like finding the shortest path in a graph.\nThe Bellman optimality equations for the optimal state value function $v_\u2217(s)$ and the optimal action value function $q_\u2217(s, a)$ are given as follows:\n$v_\u2217(s) = \\max_a \\mathbb{E}[R_{t+1} + \u03b3v_\u2217(S_{t+1}) | S_t = s, A_t = a] = \\max_a \\sum_{s',r} p(s', r | s, a)[r + \u03b3v_\u2217(s')]$\n$q_\u2217(s, a) = \\mathbb{E}[R_{t+1} + \\max_{a'} q^\u2217(S_{t+1}, a') | S_t = s, A_t = a] = \\sum_{s',r} p(s', r | s, a)[r + \u03b3 \\max_{a'} q^\u2217(s', a')]$\nDP algorithms are derived by transforming Bellman equations into update rules."}, {"title": "Policy Evaluation (Prediction)", "content": "Policy evaluation, also known as prediction, involves computing the state-value function $v^\u03c0$ for a given policy \u03c0. This process assesses the expected return when following policy \u03c0 from each state. The state-value function $\u03c5_\u03c0(s)$ is defined as the expected return starting from state s and following policy \u03c0:\n$\u03c5_\u03c0(s) = \\mathbb{E}_\u03c0[R_{t+1} + \u03b3G_{t+1} | S_t = s]$\nThis can be recursively expressed as:\n$\u03c5_\u03c0(s) = \\mathbb{E}_\u03c0[R_{t+1} + \u03b3\u03c5_\u03c0(S_{t+1}) | S_t = s] = \\sum_a \u03c0(a | s) \\sum_{s',r} p(s', r | s, a)[r + \u03b3\u03c5_\u03c0(s')]$\nIn these equations, \u03c0(a | s) denotes the probability of taking action a in states under policy \u03c0. The existence and uniqueness of $v^\u03c0$ are guaranteed if \u03b3 < 1 or if all states eventually terminate under \u03c0. Dynamic Programming (DP) algorithm updates are termed \"expected updates\u201d because they rely on the expectation over all potential next states, rather than just a sample [1]."}, {"title": "Policy Improvement", "content": "The purpose of calculating the value function for a policy is to identify improved policies. Assume $v^\u03c0$ for a deterministic policy \u03c0. For a state s, should we alter the policy to select action a \u2260 \u03c0(s)? We know the effectiveness of adhering to the existing policy from state s ($v(s)$), but would transition to a new policy yield a superior outcome? We can answer this by selecting action a in s and then following \u03c0:\nTo determine if a policy can be improved, we compare the value of taking a different action a in state s with the current policy. This is done using the action value function q(s,\u03b1):\n$q_\u03c0(s,a) = \\mathbb{E}[R_{t+1} + \u03b3\u03c5_\u03c0(S_{t+1}) | S_t = s, A_t = a] = \\sum_{s',r} p(s', r | s,a)[r + \u03b3\u03c5_\u03c0(s')]$\nThe key criterion is whether this value exceeds $\u03c5_\u03c0(s)$. If $q_\u03c0(s, a) > \u03c5_\u03c0(s)$, consistently choosing action a in s is more advantageous than following \u03c0, leading to an improved policy \u03c0'."}, {"title": "Policy Improvement Theorem", "content": "The policy improvement theorem states that if $q_\u03c0(s, \u03c0'(s)) \u2265 \u03c5_\u03c0(s)$ for all states s, then the new policy \u03c0' is at least as good as the original policy \u03c0. Formally, it is expressed as follows:\nLet and \u03c0' be deterministic policies such that for all s \u2208 S:\n$q_\u03c0(\u03b4, \u03c0'(s)) \u2265 \u03c5_\u03c0(S)$\nIf \u03c0' achieves greater or equal expected return from all states s\u2208 S:\n$\u03c5_{\u03c0'}(8) \u2265 \u03c5_\u03c0(8)$\nIf there is strict inequality at any state, \u03c0' is superior to \u03c0. Extend this to all states and actions, selecting the action that maximizes $q(\u03c2, \u03b1)$:\nThe new policy \u03c0' is obtained by selecting the action that maximizes the action value function $q_\u03c0(s, a)$:\n$\u03c0'(s) = \\arg \\max_a q_\u03c0 (s, a) = \\arg \\max_a \\mathbb{E}[R_{t+1}+\u03b3\u03c5_\u03c0(S_{t+1}) | S_t = s, A_t = a] = \\arg \\max_a \\sum_{s',r} p(s',r | s,a)[r+\u03b3\u03c5_\u03c0(s')]$\nPolicy improvement creates a new policy that enhances an initial policy by adopting a greedy approach based on the value function.\nAssume \u03c0' is equally effective as a but not superior. Then $\u03c5_\u03c0 = \u03c5_{\u03c0'}$, ensuring for all states s\u2208 S:\nThe relationship between the optimal state value function $v_\u2217(s)$ and the optimal action value function $q_\u2217(s, a)$ is given by the following equation:\n$\u03c5_{\u03c0'}(s) = \\max_a \\mathbb{E}[R_{t+1} + \u03b3\u03c5_{\u03c0'} (S_{t+1}) | S_t = s, A_t = a] = \\max_a \\sum_{s',r} p(s',r | s, a)[r + \u03b3\u03c5_{\u03c0'} (s')]$\nThis is the Bellman optimality equation, implying $\u03c5_{\u03c0'} = v_\u2217$ and both and are optimal policies. Policy improvement yields a superior policy unless the initial policy is already optimal.\nThis concept extends to stochastic policies. Stochastic policies introduce a set of probabilities for actions, with the action most aligned with the greedy policy assigned the highest probability."}, {"title": "Policy Iteration", "content": "After enhancing a policy using v to derive an improved policy \u03c0', compute $\u03c5_{\u03c0'}$ and further refine it to obtain a superior policy \u03c0\". This process generates a sequence of improving policies and corresponding value functions:\nThe process of policy iteration involves alternating between policy evaluation and policy improvement to obtain a sequence of improving policies and value functions:"}, {"title": "Value Iteration", "content": "One limitation of policy iteration is that each iteration requires policy evaluation, often necessitating multiple passes through the entire state set [13]. To address this, policy evaluation can be abbreviated without losing convergence guarantees. This method, known as value iteration, terminates policy evaluation after a single sweep. It combines policy improvement with a truncated form of policy evaluation. Value iteration merges one pass of policy evaluation with policy improvement in each iteration, ensuring convergence to an optimal policy for discounted finite MDPs [14].\nThe update rule for value iteration is given by:\n$v_{k+1}(s) = \\max_a \\max \\mathbb{E}[R_{t+1} + \u03b3\u03c5_k(S_{t+1}) | S_t = s, A_t = a] = \\max_a \\max \\sum_{s',r} p(s', r | s, a)[r + \u03b3\u03c5_k(s')]$\nValue iteration combines one sweep of policy evaluation and policy improvement in each iteration. It converges to an optimal policy for discounted finite MDPs [14].\nPolicy iteration involves two processes: policy evaluation aligns the value function with the current policy, and policy improvement makes the policy greedier based on the value function. These processes iteratively reinforce each other until an optimal policy is obtained.\nIn value iteration, the key advantage is its efficiency, as it reduces the computational burden by merging policy evaluation and improvement into a single update step. This method is particularly useful for large state spaces where full policy evaluation at each step of policy iteration is computationally prohibitive [1].\nAdditionally, value iteration can be implemented using a synchronous update approach, where all state values are updated simultaneously, or an asynchronous update approach, where state values are updated one at a time, potentially allowing for faster convergence in practice [13].\nAnother notable aspect of value iteration is its robustness to initial conditions. Starting from an arbitrary value function, value iteration iteratively refines the value estimates until convergence, making it a reliable method for finding optimal policies even when the initial policy is far from optimal [15].\nFurthermore, value iteration provides a foundation for more advanced algorithms, such as Q-learning and other reinforcement learning techniques, by illustrating the principle of bootstrapping, where the value of a state is updated based on the estimated values of successor states. This principle is central to many modern reinforcement learning algorithms that seek to balance exploration and exploitation in dynamic and uncertain environments [16]."}, {"title": "Terminology", "content": "Understanding the various methodologies and concepts within RL is essential for the effective design and implementation of RL algorithms. Methods in RL can be classified as either off-policy or on-policy, as well as model-free and model-based. These categories offer different approaches and techniques for learning from interactions with the environment."}, {"title": "Model-Free Methods", "content": "Model-free methods determine the optimal policy or value function directly without constructing a model of the environment. There is no requirement for them to know transition probabilities and rewards, as they learn entirely from observed states, actions, and rewards. Compared with model-based methods, model-free methods are simpler to implement, relying on experience-based learning. There are two primary types:"}, {"title": "Off-Policy and On-Policy Methods", "content": "On-policy and off-policy learning are methodologies within model-free learning approaches, not relying on environment transition probabilities. They are classified based on the relationship between the behavior policy and the updated policy [1]."}, {"title": "On-Policy Methods", "content": "On-policy methods evaluate and improve the policy used to make decisions, intertwining exploration and learning. These methods update the policy based on the actions taken and the rewards received while following the current policy. This ensures that the policy being optimized is the one actually used to interact with the environment, allowing for a coherent learning process where exploration and policy improvement are naturally integrated.\nExample: Consider a customer service chatbot that learns to provide better responses to user queries. The chatbot follows a specific policy to decide which responses to give. In on-policy learning, the chatbot updates its policy based on the actual responses it uses and the feedback received from users (e.g., user satisfaction ratings). This ensures that the policy being learned is directly tied to the actions taken in real interactions, leading to stable and consistent improvement."}, {"title": "Off-Policy Methods", "content": "Off-policy methods involve learning the value of the optimal policy independently of the agent's actions. In these methods, we distinguish between two types of policies: the behavior policy and the target policy that are discussed later in this chapter. The behavior policy explores the environment, while the target policy aims to improve performance based on the gathered experience. This allows for a more exploratory behavior policy while learning an optimal target policy. A significant advantage of off-policy methods is that they can learn from data generated by any policy, not just the one currently being followed, making them highly flexible and sample-efficient.\nExample: Consider a recommendation system for an online streaming service like Netflix. The behavior policy in this system could be a strategy that recommends a wide variety of content to users, ensuring that the system explores different genres, new releases, and less popular titles. This exploration helps gather diverse data about user preferences and content performance. Simultaneously, the target policy aims to optimize recommendations to maximize user engagement and satisfaction. It learns from the data generated by the behavior policy, identifying patterns and preferences to recommend content that users are most likely to enjoy. By separating the behavior policy from the target policy, Netflix can experiment with different recommendation strategies without compromising the quality of the final recommendations. This approach allows the recommendation system to be both exploratory in gathering new data and exploitative in providing the best possible content to users.\nThe decoupling of the behavior and target policies allows off-policy methods to reuse experiences more effectively. For instance, experiences collected using a behavior policy that explores the environment broadly can be used to improve the target policy, which aims to maximize rewards. This characteristic makes off-policy methods particularly powerful in dynamic and complex environments where extensive exploration is required [24, 25]."}, {"title": "Distinguishing On-Policy vs. Off-Policy Methods", "content": "The relationship between the update policy and the behavior policy determines if a method is on-policy or off-policy. Identical policies indicate on-policy, while differing policies indicate off-policy. Implementation details and objectives also influence classification. To better distinguish these methods, we have to first learn what are the different policies.\nBehavior Policy is a strategy used by an agent to determine which actions to take at each time step. The behavior policy might, for example, include recommending a variety of movies in order to explore user preferences in the recommendation system example.\nUpdate policy governs how the agent updates its value estimates in response to observed outcomes. Depending on the feedback received from the recommended movies, the update policy of the recommendation system may update the estimated user preferences. A thorough understanding of the interactions between these policies is essential for the implementation of effective learning systems. An agent's behavior policy determines how it explores an environment, balancing exploration with exploitation to gather useful information. Alternatively, the update policy determines how the agent learns from these experiences in order to improve its estimates of value.\nWhen using on-policy methods, the behavior policy and the update policy are the same, meaning that the actions taken to interact with the environment are also used to update the value estimates. The result is stable learning, but it can be less efficient because the policy may not sufficiently explore the state space [26].\nThere is a difference between the behavior policy and the update policy in off-policy methods. As opposed to the behavior policy, the update policy focuses on optimizing the value estimates by taking the most appropriate action. Despite the fact that this separation can make learning more efficient, it can also introduce instability if the behavior policy diverges too far from the optimal policy [16, 25]. Furthermore, advanced methods, such as Actor-Critic algorithms, separate the behavior policy (actor) and the update policy (critic). Actors make decisions according to current policies, while critics evaluate these decisions and provide feedback to improve policies, thus combining the stability of on-policy methods with the efficiency of off-policy methods [27, 28]."}, {"title": "A Timeline of Reinforcement Learning Evolution", "content": "In recent decades", "1950s-1960s": "Early Foundations\n1954: Marvin Minsky's SNARC", "29": ".", "n1954": "Richard Bellman's work on dynamic programming laid the foundation for modern rein-forcement learning [30", "n1965": "Bellman and Kalaba introduced the Bellman equation", "31": ".", "1970s-1980s": "Theoretical Development\n1972: Klopf's work on drive-reduction theory in neural networks contributed to the theoretical underpinnings of RL [32", "n1984": "Barto", "33": ".", "n1989": "Watkins introduced Q-learning", "34": "."}, {"n1989": "Sutton and Barto's book", "Reinforcement Learning": "An Introduction", "became a foundational text in the field [35": ".", "1990s": "Early Applications and Further Theory\n1992: Watkins and Dayan formalized Q-learning and provided theoretical convergence guarantees [24", "n1994": "Rummery and Niranjan developed SARSA", "26": ".", "n1996": "Chris Watkins' Dyna-Q algorithm combined model-free and model-based approaches [36", "2000s": "Advances in Algorithms and Applications\n1999: Konda and Tsitsiklis introduced actor-critic methods", "27": ".", "n1999": "Ng", "37": ".", "n2001": "Tsitsiklis and Van Roy developed the theory of least-squares temporal difference (LSTD) learning [38", "2010s": "Deep Reinforcement Learning and Breakthroughs\n2013: Mnih et al. introduced Deep Q-Networks (DQN)", "18": ".", "n2016": "Silver et al. developed AlphaGo", "39": ".", "n2017": "Schulman et al. proposed Proximal Policy Optimization (PPO)", "19": ".", "n2018": "Hessel et al. presented Rainbow", "25": ".", "Directions\n2017": "Jaderberg et al. introduced \"Population Based Training", "enhancing the efficiency of RL training processes [40": ".", "IMPALA\" (Importance Weighted Actor-Learner Architectures), addressing scalability in RL [41": ".", "n2019": "OpenAI's Dactyl demonstrated RL's potential in robotic manipulation by solving a Rubik's cube with a robotic hand [42"}]}