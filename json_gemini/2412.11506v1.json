{"title": "GLIMPSE: ENABLING WHITE-BOX METHODS TO USE PROPRIETARY MODELS FOR ZERO-SHOT LLM-GENERATED TEXT DETECTION", "authors": ["Guangsheng Bao", "Yanbin Zhao", "Juncai He", "Yue Zhang"], "abstract": "Advanced large language models (LLMs) can generate text almost indistinguishable from human-written text, highlighting the importance of LLM-generated text detection. However, current zero-shot techniques face challenges as white-box methods are restricted to use weaker open-source LLMs, and black-box methods are limited by partial observation from stronger proprietary LLMs. It seems impossible to enable white-box methods to use proprietary models because API-level access to the models neither provides full predictive distributions nor inner embeddings. To traverse the divide, we propose Glimpse, a probability distribution estimation approach, predicting the full distributions from partial observations. Despite the simplicity of Glimpse, we successfully extend white-box methods like Entropy, Rank, Log-Rank, and Fast-DetectGPT to latest proprietary models. Experiments show that Glimpse with Fast-DetectGPT and GPT-3.5 achieves an average AUROC of about 0.95 in five latest source models, improving the score by 51% relative to the remaining space of the open source baseline (Table 1). It demonstrates that the latest LLMs can effectively detect their own outputs, suggesting that advanced LLMs may be the best shield against themselves.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) (OpenAI, 2022; Team et al., 2023; Anthropic, 2024) can produce fluent and coherent text content, which is almost indistinguishable from human-written content (Ip- *Corresponding author. We release our code and data at https://github.com/baoguangsheng/glimpse."}, {"title": "2 \u041c\u0415\u0422\u041dOD", "content": "We use Fast-DetectGPT (Bao et al., 2023) as an example to illustrate how to apply Glimpse to existing white-box methods in Section 2.2, and present three specific probability distribution estimation algorithms in Section 2.3. We further discuss three more white-box methods with Glimpse in Section 2.4."}, {"title": "2.1 TASK AND SETTINGS", "content": "We are focusing on the zero-shot detection of LLM-generated text, which we frame as a binary classification problem: determining whether a given text was created by a model or a human. A zero-shot detector usually uses a scoring model to produce a detection metric and makes a decision by comparing this metric to a predetermined threshold. The scoring model can either be the same as the source model or a different one, for example a fixed delegate model. The access level to this scoring model can vary, and methods can be categorized into black-box or white-box methods based on this (similar to Tang et al. (2024)). Black-box methods use API-level access to interact with the scoring model, for example proprietary GPT-3.5, while white-box methods assume full access to the scoring model, for example open source Neo-2.7B. In this definition, methods such as Fast-DetectGPT (Bao et al., 2023) and PHD (Tulchinskii et al., 2024), which employ a delegated open source model to identify generations from proprietary LLMs, are classified as white-box methods. It is crucial to note that this definition differs from Yang et al. (2023b), which defines black box and white box based on the access level to the source model."}, {"title": "2.2 FAST-DETECTGPT WITH GLIMPSE", "content": "Adding Glimpse does not change the overall framework of the baseline. It only replaces the model distribution $p_{\\theta}$ with the estimated distribution $\\hat{p_{\\theta}}$ when the missing part of $p_{\\theta}$ is required. In the following description, we rehearsal the framework of Fast-DetectGPT with the updated distribution. Fast-DetectGPT posits that human and machine language generation differs in word selection based on context. While machines favor words with higher model probabilities, humans do not necessarily demonstrate such tendency. This discrepancy is quantified using a metric, naming the conditional probability curvature. And we decide if a text is machine generated by comparing the metric with a threshold $\\epsilon$, which is chosen according to specific scenarios. Formally, given a text passage x, a proprietary model $p_{\\theta}$, and an estimated distribution $\\hat{p_{\\theta}}$, the conditional probability function defined by Fast-DetectGPT is expressed as\n$P_{\\theta}(x|x) = \\prod_{j} P_{\\theta}(x_j|x_{<j}),$ (1)\nwhich denotes the predictive distribution of the model taking x as the input. As a special case, when i equals to x, $p_{\\theta}(x|x) = p_{\\theta}(x)$. Take the second word position (j = 2) of the passage"}, {"title": "2.3 GLIMPSE: A PROBABILITY DISTRIBUTION ESTIMATION APPROACH", "content": "Formally, given a text sequence x, the proprietary model $p_{\\theta}(x_{<j})$ provides us the likelihood $P_{\\theta}(x_j|x_{<j})$ and the top-K token probabilities $p_{\\theta}(x_j | x_{<j})|_{k=1}^{K}$ on each token position j, where k denotes the rank. The problem is then formulated as estimating $p_{\\theta}(X_j|x_{<j})$ over the whole vocabulary according to the given information. However, in general, we do not need token-probability pairs for metric calculation. Take Fast-DetectGPT as an example. The mean $\\tilde{\\mu}_j$ and variance $\\tilde{\\sigma}_j$ only depend on the probability values in the distribution. That is to say, we can calculate them using only the 'probability' column in the full distribution in Figure 1, where the 'token' column is not necessary. This advantage is not specific to Fast-DetectGPT. Other methods like Entropy, Rank, and Log-Rank can also be calculated from the probabilities only. To simplify the discussion in the following, we denote the probabilities as p(k), omitting the expression of the token $x_j$ and position j. Using the top-K (K = 3 typically) probabilities $p(k)|_{k=1}^{K}$, we estimate the rest $p(k)|_{K+1}^{M}$, where M denotes the size of the list. It is worth noting that M is not necessarily as large as the vocabulary size, because large ranks generally correspond to low probabilities. When the probabilities are small enough, their effects on the metric are ignorable. Consequently, $\\mu_j$ and $\\sigma_j^2$ are rewritten as\n$\\mu_j = \\sum_{x_j} P_{\\theta}(x_j|x_{<j}) log P_{\\theta}(x_j|x_{<j}) = \\sum_k p(k) log p(k),$ (6)\n$\\sigma_j^2 = \\sum_{x_j} P_{\\theta}(x_j|x_{<j}) log^2 P_{\\theta}(x_j|x_{<j}) - \\mu_j^2= \\sum_k p(k) log^2 p(k) - \\mu_j^2,$"}, {"title": "2.4 UNIVERSALITY OF GLIMPSE", "content": "Glimpse can also be used by other zero-shot detection methods like Entropy, Rank, and LogRank. For Entropy, the calculation is straightforward by summing $p(k) log p(k)$ over all items in the"}, {"title": "3 EXPERIMENTS", "content": "We evaluate our methods on four datasets that cover seven languages, five source models that cover three model families, and four scoring models from small to large. We run each main experiment three times and report the median. The metrics used are described in Appendix B.1. Datasets. We follow studies (Mitchell et al., 2023; Bao et al., 2023; Yang et al., 2023a; Zeng et al., 2024a) on the evaluation datasets and their settings: XSum for news (Narayan et al., 2018), Writing for story (Fan et al., 2018), and PubMed for technical question answer (Jin et al., 2019), where for each dataset 150 human-written samples are randomly selected and corresponding LLM texts are generated using the same prefix (30 tokens for articles or questions for QAs). Furthermore, we use M4 (Wang et al., 2024) to incorporate various languages such as Chinese, Russian, Urdu, Indonesian, Arabic, and Bulgarian, where for each language 150 pairs are randomly selected from the ChatGPT subsets. To evaluate the detectors ability to handle diverse domains and languages, we combine XSum, Writing, and PubMed into a single dataset called Mix3, and the six language datasets into another called Mix6. Source Models. We evaluate our detector on five latest LLMs from different companies, including ChatGPT (gpt-3.5-turbo) (OpenAI, 2022), GPT-4 (gpt-4) (OpenAI, 2023), Claude3 Sonnet (claude-3-sonnet-20240229) and Opus (claude-3-opus-20240229) (Anthropic, 2024), Gemini-1.5 Pro (gemini-1.5-pro) (Team et al., 2023), where Opus is supposed at the same level as GPT-4 and Sonnet at the same level as ChatGPT. We use the ChatCompletion API 4 of these models to prepare the datasets. Scoring Models. A good scoring model can detect generations from a wide range of source models. We use OpenAI GPT series (from small to large) as the scoring model, inlcuding Babbage (babbage-002, 1.3B) and Davinci (davinci-002, 175B) (Brown et al., 2020), GPT-3.5 (gpt-35-turbo-0301 or gpt-35-turbo-1106 almost equally, 175B) (OpenAI, 2022), and GPT-4 (gpt-4-1106) (OpenAI, 2023) as scoring models using AzureOpenAI (see Appendix B.2). For comparison, we use Neo-2.7 (gpt-neo-2.7B) (Black et al., 2021), Phi2-2.7B (Javaheripi et al., 2023), Qwen2.5-7B (Yang et al., 2024; Team, 2024), and Llama3-8B (Dubey et al., 2024) as representatives of open-source models, run locally on a Tesla A100 GPU. Baselines. Among zero-shot detectors, we compare our methods with existing solutions such as Fast-DetectGPT (shortly Fast-Detect) (Bao et al., 2023), DetectGPT (Mitchell et al., 2023), DNA-GPT (Yang et al., 2023a), and simple baselines such as Likelihood, Entropy, Rank, and Log-Rank (Gehrmann et al., 2019; Solaiman et al., 2019; Ippolito et al., 2020). For other detectors, we compare our methods to commercial GPTZero (Tian & Cui, 2023)."}, {"title": "3.1 SETTINGS", "content": "We evaluate our methods on four datasets that cover seven languages, five source models that cover three model families, and four scoring models from small to large. We run each main experiment three times and report the median. The metrics used are described in Appendix B.1. Datasets. We follow studies (Mitchell et al., 2023; Bao et al., 2023; Yang et al., 2023a; Zeng et al., 2024a) on the evaluation datasets and their settings: XSum for news (Narayan et al., 2018), Writing for story (Fan et al., 2018), and PubMed for technical question answer (Jin et al., 2019), where for each dataset 150 human-written samples are randomly selected and corresponding LLM texts are generated using the same prefix (30 tokens for articles or questions for QAs). Furthermore, we use M4 (Wang et al., 2024) to incorporate various languages such as Chinese, Russian, Urdu, Indonesian, Arabic, and Bulgarian, where for each language 150 pairs are randomly selected from the ChatGPT subsets. To evaluate the detectors ability to handle diverse domains and languages, we combine XSum, Writing, and PubMed into a single dataset called Mix3, and the six language datasets into another called Mix6. Source Models. We evaluate our detector on five latest LLMs from different companies, including ChatGPT (gpt-3.5-turbo) (OpenAI, 2022), GPT-4 (gpt-4) (OpenAI, 2023), Claude3 Sonnet (claude-3-sonnet-20240229) and Opus (claude-3-opus-20240229) (Anthropic, 2024), Gemini-1.5 Pro (gemini-1.5-pro) (Team et al., 2023), where Opus is supposed at the same level as GPT-4 and Sonnet at the same level as ChatGPT. We use the ChatCompletion API 4 of these models to prepare the datasets. Scoring Models. A good scoring model can detect generations from a wide range of source models. We use OpenAI GPT series (from small to large) as the scoring model, inlcuding Babbage (babbage-002, 1.3B) and Davinci (davinci-002, 175B) (Brown et al., 2020), GPT-3.5 (gpt-35-turbo-0301 or gpt-35-turbo-1106 almost equally, 175B) (OpenAI, 2022), and GPT-4 (gpt-4-1106) (OpenAI, 2023) as scoring models using AzureOpenAI (see Appendix B.2). For comparison, we use Neo-2.7 (gpt-neo-2.7B) (Black et al., 2021), Phi2-2.7B (Javaheripi et al., 2023), Qwen2.5-7B (Yang et al., 2024; Team, 2024), and Llama3-8B (Dubey et al., 2024) as representatives of open-source models, run locally on a Tesla A100 GPU. Baselines. Among zero-shot detectors, we compare our methods with existing solutions such as Fast-DetectGPT (shortly Fast-Detect) (Bao et al., 2023), DetectGPT (Mitchell et al., 2023), DNA-GPT (Yang et al., 2023a), and simple baselines such as Likelihood, Entropy, Rank, and Log-Rank (Gehrmann et al., 2019; Solaiman et al., 2019; Ippolito et al., 2020). For other detectors, we compare our methods to commercial GPTZero (Tian & Cui, 2023)."}, {"title": "3.2 THE EFFECTIVENESS OF GLIMPSE", "content": "We assess the effectiveness of Glimpse by comparing the estimated distributions with the real distributions, using Neo-2.7B as the scoring model. As Figure 2 shows, their Kullback-Leibler (KL) divergence decreases as long as more top probabilities are used. Overall, MLP obtains the lowest divergence, while Geometric distribution has the highest divergence, suggesting the most accurate estimation of MLP. However, the correlation between KL divergences and detection accuracies varies for different detection methods and estimation algorithms. As Figure 3 illustrates, the AU-ROC declines when the divergence increases for Glimpse (Fast-Detect and LogRank) with any of the estimation algorithms, while the AUROC inclines for Glimpse (Rank). Although the geometric distribution has larger divergences in general, it achieves higher or equal detection accuracies than other algorithms, suggesting that the effect of Glimpse is beyond the similarity of estimated distributions. It is worth noting that the accuracies achieved by Glimpse are not significantly lower than the baseline using real distribution for Fast-Detect and LogRank, and are even significantly higher than the baseline for Rank, suggesting the effectiveness and potential of Glimpse."}, {"title": "3.3 MAIN RESULTS", "content": "Accuracy and Efficiency. We first compare Glimpse with existing black-box methods on their detection accuracy, speed, and cost. As Table 2 shows, Fast-Detect (GPT-3.5) surpasses both Likelihood (GPT-3.5) and DNA-GPT (GPT-3.5) with a significant margin in five source models (in AU-ROC). The basic method LogRank (GPT-3.5) also outperforms DNA-GPT in four-fifth source mod-"}, {"title": "3.4 ABLATION STUDY", "content": "Necessity of Glimpse. Readers may wonder the necessity of these estimation algorithms, given that the top-K probabilities provide the major information. To testify it, we consider a Naive approach to estimate the full distribution, where we assign zero probability to ranks larger than K. Using the Naive distribution, the average AUROC of Fast-Detect (GPT-3.5) is downgraded from 0.9630 (Geometric) to 0.9311 (Naive), indicating the necessity of a proper estimation algorithm. Ablation on Estimation Algorithm. As the main results in Table 2 show, the Glimpses using Geometric, Zipfian, and MLP distributions do not show significant differences in their average accuracies. However, when we look into each dataset, we see different patterns as Figure 4 demonstrates. Specifically, the geometric distribution performs the best in Writing and PubMed, while Zipfian performs the best on XSum. MLP is more balanced, which performs the median on three datasets. These patterns remain with both GPT-3.5 and GPT-4 as the scoring model. These results suggest that different estimation algorithms may suit different situations, which is also supported by experiments on languages shown in Table 3."}, {"title": "3.5 ANALYSIS AND DISCUSSION", "content": "Robustness across Source Models and Domains. In real scenarios, we tend to use a constant threshold to detect text from various sources and domains. We evaluate the stability of Glimpse and other baselines across the source models and datasets using thresholds found on \u201cout-of-domain\" datasets. The results show that Glimpse consistently provides the highest accuracy across the source models and domains, showcasing its stability. See Appendix E.1 for detailed setup and discussion. Robustness on Low False Alarms. In real scenarios, an effective detector is expected to have a high recall (true positive rate) with a low false alarm (false positive rate), thereby allowing it to identify most machine-generated text without misclassifying human-written text. We evaluate the capacity of the methods by contrasting their ROC curves. As illustrated in Figure 6, for a false alarm in the range of (0.01, 0.1), Fast-Detect (GPT-4) performs the best, except on ChatGPT generations, where Fast-Detect (Babbage) has the highest recalls. For a false alarm lower than 0.001, Fast-Detect (Babbage) performs consistently better than other methods, suggesting the advantage of it for low false alarm setting. Compared to baseline Likelihood (GPT-3.5) and Fast-Detect (Neo-2.7), Glimpse versions of Fast-Detect show a consistent advantage. Robustness over Languages. We assessment Glimpse methods on M4 datasets with six languages. As Table 3 shows, Fast-Detect (GPT-3.5) consistently outperforms the baselines, as well as other proprietary LLMs. The system almost flawlessly identifies Urdu, Indonesian, and Arabic texts. However, the detection accuracy for Russian texts is much lower, indicating a potential under-training of the LLMs on this particular language. These findings imply that Glimpse with the latest LLMs can be effective and reliable detectors across languages."}, {"title": "4 RELATED WORK", "content": "In terms of using a proprietary model for scoring, our method is in line with existing black-box methods. However, it inherits existing white-box approaches. Black-Box Methods for Zero-Shot Detection. Zero-shot detection in the black-box setting is challenging due to restricted access of the model. DetectGPT (Mitchell et al., 2023) and its improvement NPR (Su et al., 2023) requires multiple evaluations of text sequences, while DNA-GPT (Yang et al., 2023a) requires multiple generations of text sequences, resulting in low speed and high cost. Another line of approaches treats detection as a question-answering task but does not reliably discern text generated by ChatGPT and GPT-4 (Bhattacharjee & Liu, 2024). These methods generally presume the knowing of the source model and use it to ascertain if a text was produced by it, which limits their usage to texts from uncertain origin. Unlike these approaches, our method minimizes the cost and dependence on known sources. White-Box Methods for Zero-Shot Detection. Zero-shot methods in the white-box setting analyze a variety of metrics from model predictive distributions or output embeddings, including Entropy and Perplexity (Lavergne et al., 2008), Likelihood (Hashimoto et al., 2019; Solaiman et al., 2019), Rank and Log-Rank (Gehrmann et al., 2019), LRR (Su et al., 2023), Fast-DetectGPT (Bao et al., 2023), PHD (Tulchinskii et al., 2024), FourierGPT (Xu et al., 2024), and Binoculars (Hans et al., 2024). These methods, except that PHD requires output embeddings, can all be applied to proprietary models using Glimpse. However, in this study, we focus on very basic Entropy, Rank, and Log-Rank, together with recent Fast-DetectGPT, leaving the rest for future exploration. Other Detection Methods. Trained classifiers are the majority of black-box methods, which rely human-authored texts along with LLM-generated content for training (Bakhtin et al., 2019; Uchendu et al., 2020; Solaiman et al., 2019; Ippolito et al., 2020; Fagni et al., 2021; Solaiman et al., 2019; Fagni et al., 2021; Yan et al., 2023; Li et al., 2023; Zeng et al., 2024b) (Verma et al., 2024; Kushnareva et al., 2024). However, they can become overly specialized in their training conditions, such as specific domains, languages, or source models. White-box approaches such as watermarking provide a different strategy, altering the LLM decoding process to include unique text signatures (Kirchenbauer et al., 2023; Kuditipudi et al., 2023; Christ et al., 2024; Zhao et al., 2023b;a). However, they require proactive text generation injection, which is not allowed by proprietary models. In this paper, we investigate techniques that do not need training or proactive injection."}, {"title": "5 CONCLUSION", "content": "We proposed Glimpse to estimate full distributions from partial observations, enabling white-box text detection methods to use proprietary models. Experiments show that Glimpse with the latest"}, {"title": "ETHICAL STATEMEMT", "content": "High efficient and accurate machine-generated text detector can potentially contribute to trustworthy Al techniques, which can mitigate the risks posed by the uncontrolled use of LLMs. Such technology can potentially benefit text readers, policy makers, and media platforms in a wide way."}, {"title": "A GLIMPSE: A PROBABILITY DISTRIBUTION ESTIMATION APPROACH", "content": "As indicated in Figure 7a, the probability distribution across ranks generally follows a decaying pattern, where the larger models tend to have a higher top-1 probability and a bigger decay factor demonstrating a sharper distribution. We approximate the pattern using parameterized distributions, allocating the remaining probability mass (as '*' indicates) to ranks larger than K. We discuss three specific estimation algorithms with decaying patterns like Figure 7b."}, {"title": "A.1 ESTIMATION USING GEOMETRIC DISTRIBUTION", "content": "As the simplest decaying pattern, we consider exponential decay with a fixed decay factor, resulting in a geometric distribution. Considering only the top-1 probability p1, the whole distribution could be estimated from p\u2081 using\n$p(k) = p1 \\cdot (1 - p_1)^{k-1}, for k \\in [1..\\infty],$ (10)\nwhere the probability decays with a factor of $\\lambda = (1 - p_1)$. However, this standard geometric distribution only considers the probability of top-1 and is defined over infinite k, which is not suitable for a finite vocabulary. Consequently, we extend the distribution to multiple top probabilities and a limited range of k. We express the probabilities for ranks larger than K in near-Geometric distribution that\n$\\begin{cases} p(k) = p_k, & \\text{for } k \\in [1..K] \\\\ p(k) = p_K \\cdot \\lambda^{k-K}, & \\text{for } k \\in [K + 1..M] \\\\ \\sum_{k=1}^{M} p(k) = 1, \\end{cases}$ (11)\nwhere $\\lambda$ is a decay factor in (0, 1), and M is the size of the rank list. Using the total probability constraint, we calculate the remaining probability mass for allocating\n$\\sum_{k=K+1}^{M} p(k) = 1 - \\sum_{k=1}^{K} p_k = P_{rest},$ (12)\nExpanding p(k) in the left expression, we obtain\n$p_{K} \\cdot \\sum_{k=1}^{M-K} \\lambda^{k} = P_{rest},$ (13)\nand rewritten as\n$p_{K} \\cdot \\lambda \\frac{(1 - \\lambda^{M-K})}{1 - \\lambda} = P_{rest},$ (14)\nAssuming $\\lambda^{M-K+1}$ is close to zero, we reach an approximate solution\n$\\lambda \\approx \\frac{P_{rest}}{p_{K} + P_{rest}}$ (15)"}, {"title": "A.2 ESTIMATION USING ZIPFIAN DISTRIBUTION", "content": "Frequencies of words in natural languages usually adhere to Zipf's law (Zipf, 1946; 2013), where the word frequency and word rank follow a Zipfian distribution\n$p(k) \\propto \\frac{1}{(k + \\beta)^{\\alpha}},$ for k \u2208 [1..\u221e]. (17)\nThe parameters \u03b1 and \u03b2 are fitted to a specific corpus, with typical values of \u03b1 = 1 and \u03b2 = 2.7 for English (Piantadosi, 2014). Assuming that the word frequencies in a given context also comply with this law, we consider it as an alternative distribution for our estimation. Given the top-K probabilities pk, we compute the probabilities of tokens with a ranking greater than K in a Zipfian distribution\n$\\begin{cases} p(k) = p_k, & \\text{for } k \\in [1..K] \\\\ p(k) = p_K \\cdot (\\frac{\\beta}{k-K+\\beta})^{\\alpha}, & \\text{for } k \\in [K + 1..M] \\\\ \\sum_{k=1}^{M} p(k) = 1, \\end{cases}$ (18)\nwhere \u03b1 and \u03b2 are two positive parameters. M is the size of the rank list. Using the total probability mass as a constraint, we determine the probability mass for allocating.\n$\\sum_{k=K+1}^{M} p(k) = 1 - \\sum_{k=1}^{K} p_k = P_{rest},$ (19)\nAfter expanding p(k), we get\n$p_{K} \\cdot \\sum_{k=K+1}^{M} (\\frac{\\beta}{k-K+\\beta})^{\\alpha} = P_{rest},$ (20)\nrewritten as\n$p_{K} \\cdot \\sum_{k=1}^{M-K} (\\frac{\\beta}{k+\\beta})^{\\alpha} = P_{rest}$ (21)\nThe equation has two unknown parameters, thereby having multiple possible solutions. Thus, we solve the parameters by minimizing a loss function\n$Loss(\\alpha, \\beta) = (\\frac{\\sum_{k=1}^{M-K} (\\frac{\\beta}{k+\\beta})^{\\alpha}}{P_K} - \\frac{P_{rest}}{P_K})^2 + 1.0 \\cdot (\\alpha - 1)^2 + 0.001 \\cdot (\\beta - 2.7)^2.$ (22)\nAs an additional constraint, we expect the parameters not vary from their typical values too much. We empirically determine a coefficient of 1.0 for \u03b1 and a coefficient of 0.001 for \u03b2. To accelerate the optimization process, we construct a table $T[\\alpha, \\beta]$, which stores the pre-calculated summation values for each pair of \u03b1 and \u03b2 as\n$T[\\alpha, \\beta] = \\sum_{k=1}^{M-K} (\\frac{\\beta}{k+\\beta})^{\\alpha}.$ (23)"}, {"title": "A.3 ESTIMATION USING A MLP MODEL", "content": "The Geometric and Zipfian algorithms both work on assumptions about the distributions. An alternative approach that does not rely on these assumptions involves modeling the distribution within a neural network. We consider the simple multilayer perception model (MLP) with a single hidden layer, which accepts the top-K probabilities and predicts the probabilities for the rest of the ranks. The distribution is expressed as\n$\\begin{cases} p(k) = p_k, & \\text{for } k \\in [1..K] \\\\ p(k) = P_{rest} \\cdot MLP (k - K), & \\text{for } k \\in [K + 1..M] \\\\ \\sum_{k=1}^{M} p(k) = 1, \\end{cases}$ (24)\nwhere $p_{rest} = 1 - \\sum_{k=1}^{K} p_k$ and $P_{MLP}$ represents the MLP predictive distribution. The MLP is defined as\n$P_{MLP} = softmax (MLP(x)),$ (25)\nwhere \u03b8 denotes the model parameters. The model inputs a vector with a size of K and outputs a distribution with a size of M \u2212 K. The input vector x is calculated from the top probabilities using\n$x_k = log p_k, for k \u2208 [1..K].$ (26)\nTraining. We train the MLP model on probability distributions from an open-source LLM (e.g., GPT-Neo-2.7B), using the cross-entropy loss\n$Loss = -\\sum_{k=1}^{M} p_k log p(k) = - \\sum_{k=K+1}^{M} p_k log P_{MLP} (k - K) + C,$ (27)\nwhere $p_k$ for $k \u2208 [1..M]$ is the target distribution and $p(k)$ is the model distribution. C is the constant part. Inference. We predict the distributions using the top-K probabilities from the proprietary LLMs, obtaining estimated distributions on all token positions. We do not enforce the monotonic decrease constraint during inference, but it generally follows the constraint because the training target is monotonic decrease distribution."}, {"title": "B EXPERIMENTAL SETTINGS", "content": "B.1 EVALUATION METRICS AUROC. We measure the detection accuracy mainly in the area under the receiver operating characteristic (AUROC), which gives an overview of the detectors across all possible thresholds. AUROC values can range from 0.0 to 1.0, and this value mathematically signifies the probability that a randomly chosen machine-generated text has a higher predicted probability of being machine-generated compared to a randomly selected human-written text. An AUROC of 0.5 is indicative of a random classifier, while an AUROC of 1.0 suggests a flawless classifier. Accuracy (ACC). As a complement, we report the ACC for some of the experiments. ACC denotes the ratio of the number of correct predictions to the total number of input samples, which works well only if there are equal number of positive and negative samples. True Positive Rate (TPR) and False Positive Rate (FPR). In depth, we compare the methods in TPR versus FPR on various thresholds. A high TPR indicates that the algorithm is effective in identifying positive cases, while a high FPR indicates that the algorithm often misclassifies negative cases as positive."}, {"title": "D ABLATION STUDY", "content": "D.1 ABLATION ON RANK-LIST SIZE The size M of the rank list is another important hyperparameter. We assess its effects on Geometric and Zipfian distributions (skipping MLP because it requires a heavy training process for each setting). As demonstrated in Figure 8, overall Glimpse with a larger size obtains a higher accuracy. Geometric distribution shows a monotonic increasing trends, while Zipfian shows decreasing trends for Fast-Detect but increasing trends for LogRank, demonstrating an inconsistent pattern. Roughly, experiments with MLP on the sizes of 100 and 1000 suggest that it has the similar pattern as Zipfian."}, {"title": "D.2 ABLATION ON PROMPT", "content": "Large models are sensitive to text context, necessitating a suitable prompt for optimal detection accuracy (Taguchi et al., 2024). We analyze the prompts featured in Table 7 to ascertain their effect. We draft the prompts manually, starting with prompt3. Then we replace the 'System' and 'Assistant' roles with 'Assistant' and 'User' to produce prompt4 and we remove the roles to produce prompt2. We simplify prompt2 by removing its second paragraph to produce prompt1 and removing all content to produce empty prompt0. In this study, we only experiment with several manually drafted prompts, leaving a systematic exploration of the prompts for the future. As Figure 9 shows, GPT-4 is the most sensitive model among the four scoring models, with detection accuracy fluctuating between 0.7289 (prompt0) and 0.9682 (prompt4). In contrast, GPT-3.5 is less sensitive, with a detection accuracy increasing from 0.9071 (prompt0) to 0.9589 (prompt1), but maintaining stability for the rest. The base model Babbage and Davinci are less influenced by the prompt, and we do not show them in the figure."}, {"title": "E ANALYSIS AND DISCUSSION", "content": "E.1 ROBUSTNESS ACROSS SOURCE MODELS AND DOMAINS In practice, we need to fix the decision threshold and detect text"}]}