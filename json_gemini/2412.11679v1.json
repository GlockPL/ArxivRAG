{"title": "Bias Vector: Mitigating Biases in Language Models with Task Arithmetic Approach", "authors": ["Daiki Shirafuji", "Makoto Takenaka", "Shinya Taguchi"], "abstract": "The use of language models (LMs) has increased considerably in recent years, and the biases and stereotypes in training data that are reflected in the LM outputs are causing social problems. In this paper, inspired by the task arithmetic, we propose the \"Bias Vector\" method for the mitigation of these LM biases. The Bias Vector method does not require manually created debiasing data. The three main steps of our approach involve: (1) continual training the pre-trained LMs on biased data using masked language modeling; (2) constructing the Bias Vector as the difference between the weights of the biased LMs and those of pre-trained LMs; and (3) subtracting the Bias Vector from the weights of the pre-trained LMS for debiasing. We evaluated the Bias Vector method on the SEAT across three LMs and confirmed an average improvement of 0.177 points. We demonstrated that the Bias Vector method does not degrade the LM performance on downstream tasks in the GLUE benchmark. In addition, we examined the impact of scaling factors, which control the magnitudes of Bias Vectors, with effect sizes on the SEAT and conducted a comprehensive evaluation of our debiased LMs across both the SEAT and GLUE benchmarks.", "sections": [{"title": "1 Introduction", "content": "As language models (LMs) have become more widely used in recent years, the biases and stereotypes inherent in the training data for LMs are creating social problems (Liu et al., 2020; Kumar et al., 2023). These biases reflect the stereotypes of specific social groups (such as those related to race, profession, gender, and religion) (Bolukbasi et al., 2016; Nadeem et al., 2021). People tend to use racially biased stereotypical phrases (like \u201cThe men from afghanistan ride on camels\u201d), rather than phrases that contradict stereotypes (e.g., \"The men from afghanistan ride on skateboards\u201d).\nAs a consequence, LMs often make unfair predictions about certain groups, leading to biased or stereotyped outcomes that can cause discomfort among users. The widespread and frequent use of LMs (such as ChatGPT (GPT-3.5 / 4) (OpenAI, 2022, 2024)), with their biased predictions is resulting in discrimination and inequality, which is becoming a social problem (Feng et al., 2023). Hence, developing effective bias mitigation methods for LM systems is essential.\nPrior to the advent of Large Language Models (LLMs), debiasing studies primarily targeted word embeddings (Zhao et al., 2018; Kaneko and Bollegala, 2019; Wang et al., 2020). Models such as word2vec (Mikolov et al., 2013) are debiased by reshaping the word embeddings in their output representations. However, these methods are less practical for Transformer-based LMs, such as BERT (Devlin et al., 2019), because the model parameters need to be debiased as the required model outputs vary depending on the downstream task.\nTo address biases in Transformer-based LMs, methods have been developed to reduce biases and stereotypes by continually training of LMs with debiased datasets (Zmigrod et al., 2019; Webster et al., 2020; Dinan et al., 2020; Barikeri et al., 2021; Jentzsch and Turan, 2022). However, these methods typically require manually created debiased data, which is resource-intensive.\nIn this work, we aim to mitigate biases and stereotypes of LMs (hereafter referred to collectively as \"bias\") using a proposed method inspired by the task arithmetic approach (Ilharco et al., 2023). We hypothesize that biases can be reduced through vector subtractions in the parameter space,"}, {"title": "2 Related Works", "content": "Language models (LMs) are inherently biased because their training processes rely on human-created text data, which would reflect human biases (Bolukbasi et al., 2016). Navigli et al. (2023) defined the term bias in the field of Natural Language Processing as \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people.\" We adopt this bias definition throughout this paper.\nVarious debiasing methods have been proposed to mitigate these biases (Schick et al., 2021; Zmigrod et al., 2019; Webster et al., 2020; Ravfogel et al., 2020; Liang et al., 2020).\nSeveral studies have shown that for word-embedding models, such as word2vec (Mikolov et al., 2013), the bias in word embeddings can be mitigated using approaches like subtracting the statistically significant mean vector associated with"}, {"title": "2.1 Language Models and Bias", "content": "Language models (LMs) are inherently biased because their training processes rely on human-created text data, which would reflect human biases (Bolukbasi et al., 2016). Navigli et al. (2023) defined the term bias in the field of Natural Language Processing as \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people.\" We adopt this bias definition throughout this paper.\nVarious debiasing methods have been proposed to mitigate these biases (Schick et al., 2021; Zmigrod et al., 2019; Webster et al., 2020; Ravfogel et al., 2020; Liang et al., 2020).\nSeveral studies have shown that for word-embedding models, such as word2vec (Mikolov et al., 2013), the bias in word embeddings can be mitigated using approaches like subtracting the statistically significant mean vector associated with"}, {"title": "2.2 Task Arithmetic Approaches", "content": "Recent studies have focused on the weight manipulation weights in neural network models Ilharco et al. (2023). Several approaches for merging model weights have been proposed in the field of Computer Vision, (Wortsman et al., 2022; Matena and Raffel, 2022; Ainsworth et al., 2023). Wortsman et al. (2022) found that a model constructed by averaging the weights of multiple models fine-tuned with different hyperparameters often results in improved model performance and robustness. Matena and Raffel (2022) proposed that computing the average parameter weights in different models corresponds to approximating the posterior distribution of each model parameter Matena and Raffel (2022) proposed a method to combine the characteristics of each model by considering the mean of multiple model parameters with the same architecture. Ainsworth et al. (2023) hypothesized that"}, {"title": "3 Proposed Methods", "content": ""}, {"title": "3.1 Continual Training", "content": "We continually train the LMs using biased text data to adjust their parameters toward the biased LMs.\nAs an additional training task, we adopt the masked language modeling (MLM), which is also used in the BERT pre-training process.\nIn MLM task, a portion of tokens in sentences is replaced with [MASK] tokens, and LMs are trained to predict these masked tokens."}, {"title": "3.2 Bias Vector", "content": "In order to mitigate biases in LMs, we propose the \"Bias Vector\" method, inspired by the task arithmetic approach (Ilharco et al., 2023), assuming the LMs share the same model architecture. An overview of the proposed method is presented in Figure 1."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Target Pre-trained LMS", "content": "In our experiments, we adopt three LMs: BERT (Devlin et al., 2019), ALBERT (Lan et al., 2020), and ROBERTa (Liu et al., 2019). These LMs are chosen based on the empirical survey by Meade et al. (2022) for bias investigation."}, {"title": "4.2 Experimental Setup for Continual Training", "content": "In this section, we outline the details of the continual training for building biased LMs."}, {"title": "4.2.1 Training Dataset", "content": "We utilize the StereoSet intrasentence dataset (Nadeem et al., 2021) for the continual training of the target LMs in our experiments. The dataset consists of biased text categorized into four types (race, profession, gender, and religion), sentences with one word blanked out, and a set of options for a fill-in-the-blank task. These options include three types of words: stereotype, anti-stereotype, and meaningless.\nTo construct a bias-only dataset for the continual training we fill the blanks with stereotype options (i.e., a biased word). The other options are excluded from the continual training process."}, {"title": "4.2.2 Experimental Details", "content": "Our experiments are conducted with the following hyperparameters. We use AdamW (Loshchilov and Hutter, 2017) as the optimizer, which improves weight decay behavior over Adam (Kingma and Ba, 2017). The learning rate is set to le-4, the weight decay is 0.01, the number of warmup steps is fixed to 10,000, the batch size is 128, and the learning rate scheduler is linear. All other training parameters follow the default settings provided by the Training Arguments library. To effectively overfit the LMs toward biases, we train the models with the number of epochs set to 30.\nWe construct the Bias Vectors using ten different seeds, and evaluate the average effect sizes of our debiasing method. The seed values remain consistent across all evaluation experiments.\nThe scaling factor A of the Bias Vector is set to 1, 10, or 100 to analyze how varying magnitudes of the vector impact bias mitigation."}, {"title": "4.3 Experimental Setup for Debias Evaluation", "content": "This section describes the experimental setup for evaluating the debiasing methods."}, {"title": "4.3.1 Debias Benchmark", "content": "Our experiments used the Sentence Encoder Association Test (SEAT) (May et al., 2019) to evaluate the bias magnitudes of the debiased LMs, following Meade et al. (2022).\nThe SEAT is an extension of the Word Embedding Association Test (WEAT) (Islam et al., 2016) to measure LM biases in sentence embeddings. WEAT comprises two sets of attribute words and"}, {"title": "4.3.2 Evaluation Metrics", "content": "This section explains the bias evaluation metrics for assessing LMs.\nThe bias magnitude is measured based on the statistical method Cohen's d which calculates the effect sizes of two groups as follows:\n$d=\\frac{diff (X, Y, A, B)}{\\sigma (\\{s(t, X, Y) | t \\in A \\cup B\\})}$,\nwhere, $\\mu$ represents the mean, and $\\sigma$ denotes the standard deviation. A and B are sets of attribute sentences, and X and Y are sets of target sentences. $diff (X, Y, A, B)$ is the result of subtracting $\\mu (\\{s (y, A, B) | y \\in Y \\})$ from $\\mu (\\{s (x, A, B) | x \\in X\\})$.\nHere, s(w, A, B) is the difference in cosine similarities between a sentence w and the sets A and B:\n$s(w, \u0391, \u0392) = \\frac{1}{|A|}\u2211_{\u03b1\u2208\u0391} cos(w, a) - \\frac{1}{|B|}\u2211_{b\u2208B}cos(w,b)$.\nWe evaluate our debiasing approach on SEAT by Equation 3."}, {"title": "4.4 Experimental Setup for GLUE", "content": "To ensure that our debiasing method does not degrade the effectiveness of LM representations, we evaluate both our debiased LMs and the pre-trained LMs on the GLUE benchmark (Wang et al., 2018) after fine-tuning. The training data is randomly split into a 9:1 ratio: 90% is used for training and 10% for validation. The original validation data from the GLUE benchmark are used as test data."}, {"title": "4.5 Baselines", "content": "We adopt the pre-trained LMs (BERT, ALBERT, and ROBERTa) as baselines to measure the effect of our debiasing methods. These baselines provide a point of comparison to assess both bias mitigation and the preservation of downstream task performance."}, {"title": "5 Results and Discussion", "content": "We evaluated the LMs using Bias Vectors constructed with the following bias type data: race, profession, gender, religion, and a combination of all these types (all)."}, {"title": "5.1 SEAT Results", "content": "The results of effect sizes on SEAT are shown in Table 3. BV(bias, \u5165) refers to the Bias Vector"}, {"title": "5.2 GLUE Scores", "content": "We investigated the impact of the Bias Vector on the performance of downstream tasks in the GLUE benchmark by comparing the debiased and pre-trained LMs."}, {"title": "5.3 SEAT Results on Profession Bias", "content": "The SEAT data do not strictly evaluate the profession bias (Meade et al., 2022). Since the bias data used for the continual training in this study includes the profession bias, this section investigates the effects of incorporating this bias into the training process.\nThe bias mitigation was also observed with BV(prof., 1) in the SEAT results as shown in Table 3. The effect sizes for RoBERTa improved from 0.733 (pre-trained) to 0.657 (debiased with BV(prof., 1)).\nSince different types of biases are interrelated, debiasing profession bias likely mitigates other biases as well. For instance, the sentence \u201cEngineers are male\" reflects both profession and gender biases. If such profession-biased sentences are learned during the construction of BV(prof., 1), the resulting Bias Vector may unintentionally encoded other biases, contributing to the improved effect sizes.\nThe occurrence of bias duplication highlights the need for task arithmetic approaches that prevent overlapping bias vectors from being subtracted multiple times. For example, removing both the profession and gender Bias Vectors from a pre-trained LM may inadvertently amplify mitigation effects, leading to over-debiasing.\nFuture work should focus on developing methods to address overlapping biases more effectively, ensuring precise bias mitigation across multiple biases."}, {"title": "5.4 Effectiveness of A", "content": "To evaluate the effectiveness of the scaling factor \u5165, we varied its value from 0.01 to 10,000 and measured the resulting effect sizes on SEAT. The results are reported in Figure 2.\nThe evaluation across all SEAT datasets confirmed that the effect sizes converged approximately to zero.\nOur initial hypothesis was that increasing the scale factor A of the Bias Vector would first reduce the effect size (debiasing), and then shift it toward an anti-stereotypical effect size (biasing).\nFor instance, if the Bias Vector had been learned in the male direction, we expected that increasing the scale factor A would gradually be biased in the female direction.\nContrary to this hypothesis, the results showed that the effect size consistently converged toward zero across all evaluations. This outcome may be due to two possible reasons: (1) The task arithmetic approach used to construct the Bias Vector may not have effectively captured the specific bias direction (investigating in Section 5.5); and (2) The biased LM may learn unintended information during continual training, leading to a collapse in the representations of the debiased LM when A is scaled up (discussing in Section 5.6)."}, {"title": "5.5 Effect Size Behavior in Each SEAT Task", "content": "In Section 5.4, we hypothesized that effect sizes would initially decrease (debiasing) and then increase in the opposite direction. However, the ob-"}, {"title": "5.6 Impact of \u03bb on LM Representations", "content": "According to Section 5.4, the effect sizes approached zero as A increased.\nThe convergence behavior of the effect sizes varies across LMs. For BERT and ROBERTa, the convergences occur when A is set between 10 and 100, while, the convergence begins around > = 10 for ALBERT.\nAs mentioned in Sections 5.2 and 5.5, the collapse of representations in LMs was observed at x = 10 for ALBERT and at X = 100 for BERT and ROBERTa.\nThis observation suggests that the convergence of the effect sizes toward zero coincides with a collapse in the LM representations across all models. Specifically, as A increases, the LMs lose their ability to distinguish between stereotypical and anti-stereotypical information, leading to predictions that are uniformly inaccurate. This inaccuracy reduced the difference in effect sizes between the two types of information, leading to a false impression of sufficient bias mitigation.\nThese results indicate that the small effect sizes observed for large values of A do not signify successful bias mitigation. Rather, they reveal a collapse in the LM representations at large \u5165, where the models fail to distinguish between stereotypical and anti-stereotypical information. Consequently, LM predictions become inaccurate for both types of information, driving the bias effect sizes toward zero."}, {"title": "6 Conclusions and Future Works", "content": "In this paper, we introduced a \"Bias Vector\" method for bias mitigation of language models (LMs) without manually created debiasing data. We constructed the Bias Vector by calculating the difference between the weights of the pre-trained LMs and those of the biased LMs, which were continually trained on the biased text. We attempted to mitigate the LM bias by subtracting the Bias Vector from the pre-trained LM weights.\nOn average over three LMs (BERT, ALBERT, and ROBERTa), our debiasing method improved 0.177 points on all test sets in SEAT with setting the scale factor X = 1. We also confirmed that the debiased LMs using our method had an average score improvement of 0.23% on the GLUE benchmark. These results demonstrate that our method can successfully debias LMs with preserving their representational performances.\nBy varying A from 0.01 to 10,000, we observed that effect sizes decreased and approached zero. However, for large \u00c0 values (e.g., \u03bb = 100), the GLUE scores significantly declined, suggesting that this bias mitigation may result from a collapse of pre-trained knowledge rather than the effectiveness of our method.\nFuture work will focus on further analyzing the relationship between the scaling factor A and SEAT scores to better understand the behavior of bias mitigation. Additionally, given the widespread use of Large Language Models (LLMs), we aim to extend the Bias Vector approach to LLMs and evaluate its effectiveness on these models."}, {"title": "Limitations", "content": "In this study, we evaluated debiased LMs on GLUE benchmark to ensure that LM representations had not decreased compared to pre-trained LMs by our debias methods \u201cBias Vector.\" This paper presented only the GLUE scores using our debiased LMs with X = 1, 10, 100. Evaluations of debiased LMs on other conditions are not conducted due to limited computational resources. To confirm the relationship between \u5165 and GLUE scores, the GLUE evaluation experiments on the other A should be conducted in future.\nFollowing Meade et al. (2022), we should evaluate our method toward GPT-2 model, in addition to BERT, ALBERT and ROBERTa. However, due to computational resource constraints, GPT-2 was not conducted in our experiments. We plan to conduct and evaluate those experiments in the future."}, {"title": "Ethics Statement", "content": "Navigli et al. (2023) defined the term bias in the field of Natural Language Processing as \u201cprejudices, stereotypes, and discriminatory attitudes against certain groups of people.\" We adopt this bias definition throughout this paper.\nFor this bias definition, we refer to both stereotypes and biases as \u201cbias\u201d for simplicity. We understand that these are different concepts, and we acknowledge that the stereotypical data (StereoSet) used in our experiments reflect those of the U.S. residents (Nadeem et al., 2021).\nWe particularly address bias mitigation for LMs by utilizing stereotypes. Biases arise when concepts that should not be associated with particular social groups are unfairly linked (e.g., \u201cprogrammers are male\u201d). If LLM systems possess such biases, they are likely to leave a negative impression on users. This work examines the applicability of a task arithmetic approach for bias mitigation. The purpose of our study is to reduce the LM bias using the proposed methods.\nWe understand the importance of maintaining an objective stance. Therefore, we emphasize that the content of this study is not influenced by our political positions, stereotypes or biases. Our research aims to respect the ethical principle of fairness in scientific inquiry and make responsible and constructive contributions to the development of AI technologies."}, {"title": "A Language Models", "content": "For evaluating our methods, we adopt three LMs: BERT (Devlin et al., 2019) ALBERT (Lan et al., 2020), and RoBERTa (Liu et al., 2019). These models are available on the following sites:\n\u2022 BERT: https://huggingface.co/ google-bert/bert-base-uncased;\n\u2022 ALBERT: https://huggingface.co/ albert/albert-base-v2;\n\u2022 ROBERTa: https://huggingface.co/ FacebookAI/roberta-base."}, {"title": "B Computing Environments", "content": "The process of generating biased LMs and our proposed Bias Vector was facilitated using four GPUs (NVIDIA RTX A6000), a procedure that spanned several hours. In the same way, the GLUE training procedure, which was conducted without the exploration of hyperparameter combinations, required approximately a full day utilizing four GPUs (NVIDIA Quadro RTX 8000)."}, {"title": "C Experimental Setup for GLUE", "content": ""}, {"title": "C.1 Training Arguments for BERT", "content": "In addition to ALBERT, we fine-tune BERT for GLUE downstream tasks. We determine hyperparameters following Devlin et al. (2019), i.e., we explore all combinations of the following hyperparameters and evaluate the model, which yields the best score on the validation dataset, using the test data on each task.\n\u2022 Batch size: 16, 32\n\u2022 Learning rate: 5e-5, 4e-5, 3e-5, 2e-5\n\u2022 Number of epochs: 2, 3, 4\nHere, a type of learning rate scheduler is linear, Adam (Kingma and Ba, 2017) is utilized for the optimizer, a number of weight decay is 0.01, warmup steps is fixed to 500, a seed value is fixed to the same number through all evaluation experiments, and the other training hyperparameters follow the default values of Training Arrguments library."}, {"title": "C.1.1 Training Arguments for ALBERT and ROBERTa", "content": "We fine-tune ALBERT and ROBERTa for GLUE downstream tasks. The following hyperparameters are adopted in the experiments:\n\u2022 Batch size: 32\n\u2022 Learning rate: 4e-5\n\u2022 learning rate scheduler: linear\n\u2022 Optimizer: Adam (Kingma and Ba, 2017)\n\u2022 warmup steps: 500\n\u2022 number of weight decay: 0.01\nThis combination of hyperparameters was chosen because it yields the best when evaluating BERT on the GLUE validation data, which is explained on Appendix C.1.\nA seed value is fixed to the same number through all evaluation experiments, and the other training hyperparameters follow the default values of Training Arrguments library."}, {"title": "D SEAT score for Gender bias", "content": "In this section, we show the SEAT results focusing specifically on the gender bias. The reason for showing results only for gender bias is that this bias is the most widely studied in the context of debiasing LMs.\nIt is to be noted that the experimental setup for the debias evaluation follows the same configuration as described in Section 4.3."}, {"title": "D.1 Evaluation Metrics", "content": "In addition to the bias measurement (Equation 3), we show the permutation test for each dataset, defined as follows:\n$p = Pr [s(X, Y^*, A, B) > s(X, Y, A, B)]$,\nwhere (X, Y) is a subset of XUY.\ns(X, Y, A, B) is obtained through the following formula:\n$s(X, Y, A, B) = \u2211_{XEX}s(x, A, B) \u2013 \u2211_{YEY}s(y, A, B)$."}, {"title": "D.2 Comparing Methods", "content": "This section compares other approaches with the Bias Vector method. These methods are selected based on the emperical study by Meade et al. (2022)."}, {"title": "D.2.1 Comparing Methods for Gender Bias", "content": "This section explains the four existing methods that have been used in gender bias mitigation experiments."}, {"title": "D.3 Results and Discussion", "content": "The detail results on SEAT regarding gender bias are shown in Table 5 and Figure 5.\nIt was confirmed that BV(all, 1) yields better than BV(gender, 1). Two reasons are considered for why BV(gender, 1) did not work sufficiently. First, words indicating gender, such as she / he}, likely appeared frequently in the pre-training corpus. This high frequency made only a small difference between pre-trained LMs and biased ones, therefore, the Bias Vector could not capture enough gender bias. Second, the amount of data used to continually train LMs toward gender bias was limited (996 instances). This data limitation suggests that the data volume might be insufficient.\nFurthermore, it can be said that BV(all, 1) debiased across all LMs, and was sometimes competitive with existing methods specialized in em-"}, {"title": "E Results in Each SEAT dataset", "content": "In this section, we show the results for subset of SEAT dataset, SEAT-8 and SEAT-5b, with means and standard deviations of effect sizes over ten seed values.\nWe present the results of SEAT-8 dataset in Figure 6 (BERT), Figure 8 (ALBERT), and Figure 10 (ROBERTa).\nThe effect sizes with SEAT-5b dataset are described in Figure 7 (BERT), Figure 9 (ALBERT), and Figure 11 (ROBERTa)."}]}