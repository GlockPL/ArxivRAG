{"title": "Automating Autograding: Large Language Models as Test Suite Generators for Introductory Programming", "authors": ["Umar Alkafaween", "Ibrahim Albluwi", "Paul Denny"], "abstract": "Automatically graded programming assignments provide instant feedback to students and significantly reduce manual grading time for instructors. However, creating comprehensive suites of test cases for programming problems within automatic graders can be time-consuming and complex. The effort needed to define test suites may deter some instructors from creating additional problems or lead to inadequate test coverage, potentially resulting in misleading feedback on student solutions. Such limitations may reduce student access to the well-documented benefits of timely feedback when learning programming.\nIn this work, we evaluate the effectiveness of using Large Language Models (LLMs), as part of a larger workflow, to automatically generate test suites for CS1-level programming problems. Each problem's statement and reference solution are provided to GPT-4 to produce a test suite that can be used by an autograder. We evaluate our proposed approach using a sample of 26 problems, and more than 25,000 attempted solutions to those problems, submitted by students in an introductory programming course. We compare the performance of the LLM-generated test suites against the instructor-created test suites for each problem. Our findings reveal that LLM-generated test suites can correctly identify most valid solutions, and for most problems are at least as comprehensive as the instructor test suites. Additionally, the LLM-generated test suites exposed ambiguities in some problem statements, underscoring their potential to improve both autograding and instructional design.", "sections": [{"title": "1 Introduction", "content": "Autograding of programming assignments offers a number of well-documented benefits to both students and instructors. In particular, the instant feedback provided by autograders has been shown to help students correct their errors and solve more problems [15, 34, 41]. Students perceive instant feedback as a positive contribution to their learning [16, 38], and this is reflected in their improved overall performance [7, 17]. Autograding also minimizes the variability in grading decisions which may be subjective when grading is performed by instructors or teaching assistants [32], and it helps reduce the workload of teaching staff significantly, by eliminating the need to manually evaluate each submission [7, 47]. However, preparing problems for autograding systems requires more than just a problem statement; instructors must also provide a test suite that thoroughly covers the different scenarios of the problem. Preparing this test suite is not always straightforward. For it to be thorough, it must at least contain tests for multiple random inputs, the minimum and maximum limits of the problem's inputs, as well as other edge-cases that are dependent on the problem. Furthermore, some problems require unique tests that capture specific constraints in the problem, e.g., an array of distinct numbers, or an array of non-increasing numbers, and sometimes random tests must be validated to ensure they actually contain a valid answer for the problem. This can all be very time consuming, even for low-level courses [25]. Moreover, failing to provide sufficient tests might result in misleading feedback to students, as invalid solutions could be graded as valid.\nRecent advances in Generative AI (GenAI) have introduced exciting new possibilities for using AI in Computing Education [13]. Large Language Models (LLM) in particular, which are capable of generating human-like outputs in response to text prompts, can be integrated in various Computing Education settings [8, 36]. They have been applied to generating new programming exercises [39], explaining code [12, 22, 24, 33], providing feedback and improving the readability of programming error messages [23, 33], and powering novel pedagogical approaches [11, 45]. While they have also been used to grade solutions based on a set of criteria provided to the LLM [9, 28], we are not aware of prior work that uses LLMs in a Computing Education context to help instructors generate test suites for autograders to address the challenges stated earlier.\nIn this study, we explore the efficacy of using LLMs to minimize the time and effort required of instructors who want their assignments to be submitted to and evaluated by autograders. We run tens of thousands of student solutions on LLM-generated test suites for CS1 problems, and compare their results to running them on traditional instructor-generated test suites with respect to their correctness and thoroughness. We also look into the LLM-generated test suites' ability to uncover ambiguities in instructor-written problem statements. Our analysis is guided by the following three research questions:\nRQ1: To what extent do LLM-generated test suites correctly identify valid solutions to CS1 problems?\nRQ2: How comprehensive are LLM-generated test suites compared to instructor-generated test suites?\nRQ3: What types of ambiguities can LLM-generated test suites help uncover in problem statements?\nWe start with a literature review of studies on autograding and LLMs in Computing Education. We also show how our work differs from the large field of work undertaken on generating unit-tests for code. Then we describe the design of our study, present the results, and discuss the broader implications of this work."}, {"title": "2 Background", "content": "Researchers agree that introductory programming courses should not only focus on teaching syntax, but also help students build mental scaffolds that allow them to follow problem solving pro-cesses and develop awareness of where they are in such a pro-cess [37]. The ability to use these scaffolds and exercise cogni-tive control to progress through the problem solving process is defined as self-regulation and is critical to the learning process [35]. Providing feedback to students is known to help with self-regulation [18, 37], but the growing number of students enrolling in programming courses makes the process of grading each stu-dent solution manually inefficient. To adapt to this growth, instruc-tors frequently rely on tools to automate the grading of assign-ments [18, 20].\nThere are a large number of tools available for autograding pro-gramming assignments [20, 31], and autograding can be carried out at different levels. Static analysis methods depend only on the source code and do not require its execution. They can detect issues in code functionality, quality, safety, syntax, and some runtime er-rors [44, 46]. In contrast, dynamic analysis methods require the ex-ecution of the source code and are the most effective way to assess its functionality [31]. Most of these autograding tools automate the process of submission and grading, but still require instructors to prepare tests for the assignment problems beforehand [20]. The tools use these tests to produce a grade for each submission. This grade can be binary, i.e., pass/fail, or partial as a percentage of the number of tests the submission passed.\nOne static approach that automates generating tests and grading student solutions is introduced by LEGenT [1]. LEGenT works offline by analysing solutions and clustering them into good and bad solutions according to their similarity to a reference solution. Solutions in the clusters are further clustered according to simi-larity in their structure and semantics. A representative solution from a bad cluster is picked and a good solution with similar struc-ture is picked from a good cluster. A test is generated through a number of steps carried out on the good and bad representatives. This test can be used to test all the solutions in the bad cluster. A more complex approach is used for running LEGenT in an online scenario to test solutions as they are submitted. LEGenT does not support advanced constructs like arrays, pointers, and recursion, and requires a few manual tests to be created by the instructor for clustering.\nA mix of dynamic analysis methods was used by Skalka and Dr-l\u00edk (2023) [44] to develop tests to evaluate student source code in object oriented programming problems. They design classes that can be used to generate random tests based on simple configura-tions, and compare the outputs of the student solutions with that of a reference solution. While their approach provides a simpler and structured way to create tests, it still requires some effort from the instructor to adapt the test cases to each different programming task.as\nIn a recent literature review, Messer et al. (2023) [26] reviewed 27 papers that used Machine Learning techniques to grade and provide feedback on programming assignments. They define four fundamental programming attributes: correctness, maintainability, readability and documentation. They report that 56% of the papers focus on assessing correctness, and 61% of the tools presented in the papers use a neural-network approach. On average, the tools reported an accuracy of at least 70% in predicting the correct grades or giving correct feedback.\nResearch on Artificial Intelligence in Computing Education surged since November 2022 with the introduction of ChatGPT, which drew widespread attention from the public and researchers to Large Language Models (LLMs). Prather et al. (2023) [36] highlight a few ways LLMs can be used to anaylse student work, such as bug fix-ing, enhancing programming error messages, grading assessments, and providing feedback. We highlight a few studies that explored the effectiveness of LLMs in grading student submissions and pro-viding feedback on programming assignments.\nPhung et al. (2023) [33] compared GPT-3.5 and GPT-4 to human tutors in multiple scenarios like program repair, hint generation, and grading. They supply the problem statement, grading rubric, and student solution in Python to the LLM, and instruct it to gener-ate a grade for the solution based on the rubric. Their results show that GPT-4 can come close to human tutors in some scenarios, but performs significantly worse in grading compared to human tu-tors. Their assumption is that testing edge-cases requires in-depth reasoning that the LLM may not be good at.\nTo test LLMs' capabilities in grading compared to teaching as-sistants, Nilsson and Tuvstedt (2023) [28] used data from an intro-ductory programming course and supplied GPT-4 with the assign-ment instructions, grading criteria, grading instructions, and stu-dent submissions made in Java and other programming languages for 73 students. They concluded that the LLM exhibited 75% grad-ing accuracy when compared to grades given previously on the same submissions by teaching assistants, but it performed signifi-cantly worse at identifying failing submissions compared to pass-ing ones.\nSimilarly, Bengtsson and Kaliff (2023) [9] used GPT-4 to inject four types of logical errors into correct Java student submissions in four Data-structures tasks, and evaluated the grades the LLM pro-duced for the original and modified solutions. Their results showed that GPT-4 was able to detect correct submissions 90% of the time on average when the LLM is provided with task instructions. Not providing task instructions reduced the accuracy of detecting cor-rect submissions to 66% on average, but improved the accuracy of detecting submissions with errors.\nAzaiz et al. (2024) [6] evaluated the feedback GPT-4 Turbo pro-duced on 55 student submissions in Java. Given the task specifi-cation and student submission, the LLM was prompted to find all the errors in the submission and provide hints for correcting them. They report that the feedback generated by GPT-4 Turbo is person-alized for each submission, mainly because the feedback does not rely on test cases, and that 52% of the submissions received fully correct and complete feedback compared to only 31% by GPT-3.5 in previous studies on the same dataset [5]. While GPT-4 Turbo pro-vided better feedback than its predecessor, it still generated incom-plete feedback or feedback that contained redundancies, inconsis-tencies, or unclear explanations. They concluded that using GPT-4 Turbo to provide automatic feedback for student submissions is not advisable as it may increase the cognitive load on students, but using it as a tool to aid teaching assistants in grading could prove efficient.\nThe aforementioned studies focus on evaluating the capabilities of LLMs in grading solutions. We are not aware of prior studies that focus on using LLMs to generate tests to be used in autograding systems. One large area of research that may appear similar to the work in this study is that of generating unit tests in the Software Engineering field. There is a growing body of research on using LLMs to generate unit tests in this area [10, 40, 43, 48], our work, however, differs from it in that:\n1. It bases the generation of unit tests on a reference solution only, but we also use the problem statement. This allows us to address requirements that may not be clear in the reference solution. For example: requiring the presence or absence of certain functions or features.\n2. Evaluation metrics in Software Engineering do not necessarily align with those in Computing Education. In Computing Educa-tion, the priority is to have a functional set of tests suitable for autograding purposes. In Software Engineering, however, the focus is more on meeting standards for coverage, maintainabil-ity, extensibility, and other quality attributes."}, {"title": "3 Methodology", "content": "3.1 Data\n3.1.1 Problems. We used 28 problems taken from seven labs in an introductory course on Engineering Computation and Software Development taught in Fall 2023 at The University of Auckland. All problems are solved using the C programming language. 8 prob-lems required students to write full C programs with a main func-tion that accepts input from Standard Input and prints output to Standard Output. These problems are referred to in the study as full-program problems. 18 problems required students to imple-ment a function that returns data and/or prints output to Standard Output. These problems are referred to in the study as function-implementation problems. The two remaining problems either ac-cepted input from files or used files to find the solution. These two problems were excluded from the study because handling files was out of scope. Appendix A shows samples of the problems. All 26 problems (8 full-program, 18 function-implementation) had the following relevant parts:\nProblem statement: contains the description of the problem and sometimes contains images.\nReference solution: a valid solution for the problem, provided by the instructor in C.\nExtra code (optional): contains any starter code provided to stu-dents with the problem statement or made available to their code to use. This may include C preprocessing directives, global vari-ables, and helper function definitions.\nA list of instructor tests: For full-program problems, a test is specified as an input that will be fed to the student solution from Standard Input. For function-implementation problems, the test is specified as C code that calls the function implemented by the student solution. All tests also specify an expected output that is compared to the output actually produced by the student solution to verify whether or not it is valid.\n3.1.2 Student Solutions. We used a total of 33,749 solutions sub-mitted by students during the semester for all 26 problems. The course uses the CodeRunner autograder [25], and for each problem the submissions were exported as a CSV file using a custom script. The format of this file lists each attempt as a row that includes the submitted code for that attempt, a universal anonymized ID for the student that made the attempt, the time and date of the attempt, and a binary digit that is either 1 if the attempt solved the prob-lem correctly, or 0 otherwise. In this exported data, an attempt is considered to have solved the problem correctly if it produced the expected output for all of the instructor tests (that were configured in CodeRunner)."}, {"title": "3.2 Design", "content": "We built a Ruby on Rails application to run our experiment [3]. Each of the 26 problems went through three automated stages:\n1. Preparing the instructor test suite.\n2. Generating an LLM test suite for the problem.\n3. Running and evaluating the LLM test suite using the submitted student solutions for the problem.\nFigure 1 details each of these stages for a given problem. In the first stage, all the problem parts are parsed from a Moodle XML formatted file that is exported from Moodle, the Learning Manage-ment System (LMS) course uses. The exported data is stored in a database. This data includes the text of the problem statement (im-ages are stripped out and not stored), its reference solution, any extra code for the problem, and each instructor test along with its expected output. These tests are grouped together under one test suite that is marked as an \"instructor test suite\".\nTo eliminate any future output discrepancies that can be caused by compiler differences, the expected outputs provided in the in-structor tests are regenerated by running the reference solution on each test to generate its new expected output.\nOnce the instructor test suite is ready, student solutions are ex-tracted from a CSV file that is exported from Moodle and run on the instructor test suite. A solution passes a test if the output the solution generates for the test matches the test's expected output. We report the instructor-test-suite grade for each solution as 1 if it passed all the tests in the test suite, or 0 if it failed at least one test in the test suite. Solutions that do not compile have their grades set to -1.\nIn the second stage, the LLM test suite is generated using two prompts. The first prompt summarizes the problem into a specific format, that summary is fed to the second prompt along with the reference solution to either generate Python scripts that are later run to generate the tests for full-program problems, or to gener-ate a testing script for function-implementation problems that is parsed into individual tests. The reference solution is run on the generated tests to generate their expected outputs. The tests are then grouped together under one test suite that is marked as an \"LLM test suite\". If any test crashes the reference solution, it is au-tomatically marked as rejected and it gets excluded from the LLM test suite. The details of the prompts and some examples are pro-vided in Section 3.4 on \"LLM Prompt Engineering\"."}, {"title": "3.3 Running the Solutions", "content": "The third and final stage runs all the student solutions on the LLM test suite. Similar to the instructor test suite, the LLM-test-suite grade for each solution is reported as 1 if it passed all the tests in the test suite, or 0 if it failed at least one test in the test suite. Solutions that do not compile also have their grades set to -1.\nAll three stages require running the reference solution and student solutions on tests to generate expected outputs and student out-puts. Running the solutions on the two types of test suites is simi-lar, but there are a few differences.\n3.3.1 Running Solutions on the Instructor Test Suite. Solutions for full-program problems are run as is for each test. They are provided the test as input through Standard Input. Solutions for function-implementation problems are run differently. Each test is placed inside its own C scope, and the tests are then combined together and separated by output statements that print a predefined sepa-rator. The combined tests are substituted in a predefined template along with the reference solution and any extra code for the prob-lem. The resulting code is run and its output is split using the prede-fined separator to retrieve the individual output of each test.\n3.3.2 Running Solutions on the LLM Test Suite. Running solutions for full-program problems on an LLM test suite is identical to run-ning them on an instructor test suite. Though for function-implementation problems, there are three differences: 1) More libraries are included in the template, and 2) the solution to be tested is written into a new file called \"solution.c\" and it is included in the program. 3) A function call is added before each test to initialize C's pseudo-random number generator algorithm to a value that has been ran-domly generated beforehand. This ensures that if any random val-ues are being generated in the tests, they will always be the same when the tests are run multiple times. This is essential to allow different valid solutions to produce the same output that was pro-duced by the reference solution.\nThis random seed value is different for each problem, but is the same for all tests in a problem's LLM test suite. For more thorough testing, it can be a different value for each test as long as the same value is always used for the same test."}, {"title": "3.4 LLM Prompt Engineering", "content": "We used the \"gpt-4-0125-preview\" large language model from OpenAI. GPT-4 models show very promising code-generation ca-pabilities according to benchmarks like HumanEval and MBPP [19]. The \"gpt-4-0125-preview\" model was the latest GPT-4 model available at the time of conducting this study. OpenAI describes the model as being \"intended to reduce cases of laziness where the model doesn't complete a task\" [30]. Whenever possible, we incor-porated a form of self \"Reflexion\" into our prompts. Reflexion is a novel strategy where LLMs are instructed to reflect on the results they produce then revise their responses based on their reflection. It has been shown that Reflexion produces better LLM responses [42]. All the prompts were sent to the model using OpenAI's Chat Completions API endpoint.\n3.4.1 Prompt 1: Detailed Problem Statement. Problem statements can vary in all ways and shapes. Some statements define clear in-put and output sections, while others do not. Some include explicit time and memory constraints, others do not. The instructor's ref-erence solution may also convey some of the input limits and re-source constraints, and it can also show the expected output for-mat. The goal of this prompt is to allow us to represent all problems in a fixed structure that can be used in other prompts in a consis-tent manner. The LLM receives the problem statement, the refer-ence solution, and any extra code for the problem, and is instructed to produce a summary of the problem under the following sections: \"Scenario\", \"Inputs\", \"Outputs\", \"Example\", and \"Limits\". The LLM is instructed to reflect on the generated sections and make sure they correctly represent the problem and fix any issues it finds in them. Finally the LLM is instructed to organize these sections into a JSON object and respond with it. We use the \"json_object\" API response format to receive the JSON object.\n3.4.2 Prompt 2: Test Generation. Test generation prompts are dif-ferent depending on the problem type, but they share the same essence. A zero-shot approach is used. The LLM is given the de-tailed problem statement from the result of Prompt 1 and the refer-ence solution, and is instructed to list edge-cases for the problem along with an example on each edge-case. It is then instructed to reflect on the edge-cases to make sure they conform to the detailed problem statement, and fix any issues it finds in them. After reflec-tion, the LLM is instructed to generate a test for each edge-case, in addition to another 100 random tests, if possible. The steps for gen-erating these tests from the edge-cases as well as their final format are different depending on the problem type.\nFull-program Test Generation Prompt. The LLM is instructed to provide a Python script that generates the tests as a JSON array. For each edge-case or random test, the JSON array will contain a JSON object that has one key \"input\", its value is the input of that test. Originally, we had the LLM respond directly with a JSON array that contains the inputs and their expected outputs, but it did not always generate the tests correctly, it made mistakes like including comments in the JSON, being lazy i.e., writing a few tests then adding a comment such as \"Write more tests like these...\", and producing wrong expected outputs, even when instructed to generate the expected outputs by running the reference solution and it confirming that it did. It also started to randomly fail to return the JSON array through the API.\nInstructing the LLM to generate the JSON array through Python code produced more consistent results. A full-program Test Gen-eration prompt used 1,443 OpenAI tokens on average.\nFunction-implementation Test Generation Prompt. The LLM is given a predefined code template to add the tests to. If the problem has any extra code, it is added to the template. The LLM has to gener-ate the tests by following very specific instructions. Each test must have its own scope and should call the function being tested and print its results, or print the values pointed to by any pointer passed to it. The LLM is in-structed to add the edge-cases first. Notice how the LLM reflected on these edge-cases and made changes to them afterwards. After the edge-cases, the random tests are added. Each test is marked with an opening and closing output statement to make parsing in-dividual tests easier later. The random tests are generated as loops that iterate 100 times and call the function with random param-eters in each iteration. The tests are later parsed from the script and stored individually. A function-implementation Test Genera-tion prompt used 2,370 OpenAI tokens on average."}, {"title": "4 Results", "content": "4.1 Grading Mismatches\n6,962 of the 33,749 solutions did not compile due to compilation errors. These solutions can be graded as invalid regardless of the tests they will be run on, therefore they were not taken into consid-eration when we evaluated the performance of the LLM test suite.\nOut of the 26,787 compiling solutions, 23,219 (86.7%) were given matching grades by the instructor test suite and the LLM test suite, i.e., they pass both LLM and instructor test suites or fail both. We reviewed all of the remaining 3,568 solutions that had mismatch-ing grades to understand the types of mistakes that caused these mismatches, and classified them into three types:\n1. LLM Mismatch: These are mismatches caused by mistakes the LLM made in the LLM test suite.\n2. Instructor Mismatch: These are mismatches caused by the in-structor test suite not being comprehensive enough to identify an invalid solution.\n3. Other Mismatches: These are mismatches that are caused by un-defined behaviour in the solution, or ambiguities in the problem statement and the reference solution.\nTable 4 summarizes the number of solutions that fall into each mismatch type for all problems. The left column in each mismatch type shows the number of solutions that were considered valid by the LLM test suite (marked with V) but invalid by the instructor test suite (marked with X). The right column shows the number of solutions that were considered invalid by the LLM test suite but valid by the instructor test suite.\n4.1.1 LLM Mismatch. 1,257 out of the 3,568 (35.2%) mismatches were LLM Mismatches. The most notable among them are the 363 mismatches for problem 2, the 861 mismatches for problem 16, and the 10 mismatches for problem 10, where the LLM incorrectly graded these solutions as invalid. The mismatches in problems 2 and 16 were caused by invalid tests in each problem's LLM test suite, while the mismatches in problem 10 were due to one of our instructions for the LLM on how to structure the tests.\nThe remaining 23 LLM Mismatches were because the LLM missed some edge-cases in six different problems, these solutions were in-correctly graded as valid.\n4.1.2 Instructor Mismatch. 1,260 out of the 3,568 (35.3%) mismatches were Instructor Mismatches. These solutions failed to handle valid tests that were not originally present in the instructor test suite. Some of these valid tests were edge-case tests, others were random tests that produced wrong output. Examples of these mismatches are the 789 solutions in problem 6 that failed to handle edge-cases like the values 0 and 1000 (trailing zeroes). Problem 17 had 131 so-lutions that failed to capitalize words at the beginning or the end of the sentence, or words that come after multiple spaces or numbers, and problem 28 had 110 solutions that failed to correctly swap the elements of the 2D array when one of them is on the edge of the array.\n4.1.3 Other Mismatch. The remaining 1,051 (29.5%) mismatches were not caused by mistakes on either the LLM or the instructor side. These solutions produced different output from the reference solution because of undefined behaviour in the program on valid tests, or because of ambiguities in the problem statement and the reference solution, which led the LLM to make its own assump-tions.\nUndefined behaviour causes the program to act in an unpre-dictable way. It may produce correct output in one run, but in-correct output in a subsequent run. There are a number of ways to detect some of the undefined behaviours that may occur in a program through static analysis and compiler sanitizers, but none were used in this study. Examples of these behaviours for some of the solutions include overflows in data types, or out of bound array accesses.\nAn example of these mismatches is in problem 19, where 757 solutions produce undefined behaviour when the number to look for is not present in the array.\nGiven that these mismatches occurred because of valid tests in both the instructor and the LLM test suites, they cannot be consid-ered a mistake on either side, and so were left out from the eval-uation of the LLM's efficacy in RQ1 and RQ2. These mismatches were instead studied to answer RQ3 and find the types of ambigu-ities problem statements and reference solutions may contain."}, {"title": "4.2 Research Questions", "content": "RQ1: To what extent do LLM-generated test suites correctly iden-tify valid solutions to CS1 problems? To compare the performance of the LLM test suite and the instructor test suite, we determine the validity of a student solution based on the grades it received from both the LLM and the instructor test suites. If the LLM test suite grade matches the instructor test suite grade for a solution, then the LLM test suite is doing at least as good as the instructor test suite, and that grade decides the validity of the solution. If the two test suites give mismatching grades, it means that one of them is wrong, so we manually review the solution and decide on its validity. We base our evaluation on the grades of the two test suites instead of the instructor test suite grade alone because the instructor test suite may not be comprehensive.\nTo answer RQ1 we look at the recall of the test suites. Recall tells us what percentage of valid solutions are identified by the test suites. LLM test suites identified 92.8% of the valid solutions, compared to 100% of the valid solutions identified by instructor test suites.\nRQ2: How comprehensive are LLM-generated test suites compared to instructor-generated test suites? To understand how comprehensive a test suite is, we need to understand how well it identifies invalid solutions.\nRQ3: What types of ambiguities can LLM-generated test suites help uncover in problem statements?"}, {"title": "5 Discussion", "content": "RQ1: To what extent do LLM-generated test suites correctly iden-tify valid solutions to CS1 problems? Our results show that LLMs are capable of generating high quality test suites that correctly identify most valid solutions to CS1 problems, but they are not perfect.\nRQ2: How comprehensive are LLM-generated test suites compared to instructor-generated test suites? The left column under \"LLM Mis-match\" shows the number of invalid solutions that the LLM did not correctly identify for each problem. These failures can be classified into three categories:\nRQ3: What types of ambiguities can LLM-generated test suites help uncover in problem statements? We looked at all 1,051 solu-tions that caused an \"Other Mismatch\" and identified mismatches that occur because of a solution's handling of ambiguous scenarios in tests. The LLM test suite helped identify the following ambigui-ties in the problems:"}, {"title": "5.1 Implications", "content": "The findings of this study suggest several important implications for computing education, including both practical classroom appli-cations and broader considerations regarding the use of generative AI and LLMs.\nIn a classroom setting, the methods described in this study to generate LLM test suites can be implemented as a black-box tool. This tool could accept a problem statement as text, any extra code to be provided to students, and a reference solution, and it would then produce an LLM-generated test suite. Our empirical findings suggest that LLM-generated test suites can be as comprehensive as instructor-generated ones, often identifying edge-cases that in-structors might overlook. By lowering the barriers associated with creating comprehensive test suites, LLM-based tools could encour-age more instructors to adopt autograding systems, providing stu-dents with the benefits of immediate feedback [27]."}, {"title": "5.2 Limitations and Recommendations", "content": "There are limitations to this study. We removed all images from the problem statements before passing them to the LLM. Sometimes problem statements may contain images that are essential to the understanding of the problem and that may affect the generated tests positively or negatively depending on the LLM's capabilities. With the growing number of GenAI technologies that are now ca-pable of accepting and processing multi-modal data such as images, we recommend this as a direction for future research.\nSecondly, LLM results are not deterministic, and therefore stud-ies on LLMs may suffer from replicability difficulties. OpenAI's API accepts a \"temperature\" value between 0 and 2 that controls the randomness of the results of its Chat Completion endpoint. We used a lower temperature value of 0.2 which makes the model outputs more focused and deterministic [29]. Moreover, our study only looked at the C programming language; other languages may produce different results based on the language capabilities of the LLM, which can depend on the availability of training data. Eval-uating the performance of test generation on different languages is another opportunity that researchers can explore and may re-quire designing different prompts and different strategies to run and grade the student solutions. \nThirdly, the LLM seemed to provide very good descriptions of the edge test cases it generated. Future research can study how LLMs may be prompted to generate test suites with test descriptions or test-failure messages in order to provide better feedback to students about the tests their code may fail. Such enhanced feedback could help in avoiding the issues related to providing only binary feedback, such as lowering student engagement and encouraging cheating [37].\nFinally, there is a security risk when running code generated by LLMs. Most autograders run code in sandboxes, but it is still recommended that instructors review the tests generated for any security concerns."}, {"title": "6 Conclusion", "content": "This study evaluated the effectiveness of using Large Language Models (LLMs) to automatically generate test suites for CS1 pro-gramming problems. Automating test generation in Computing Education can save instructors significant time and effort, allow-ing more focus on instructional design while providing students with the benefits of immediate feedback."}, {"title": "Conflict of interest", "content": "The authors declare no potential conflict of interests."}]}