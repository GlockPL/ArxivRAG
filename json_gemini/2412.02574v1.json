{"title": "Generating Critical Scenarios for Testing Automated Driving Systems", "authors": ["Trung-Hieu Nguyen", "Truong-Giang Vuong", "Hong-Nam Duong", "Son Nguyen", "Hieu Dinh Vo", "Toshiaki Aoki", "Thu-Trang Nguyen"], "abstract": "Autonomous vehicles (AVs) have demonstrated significant potential in revolutionizing transportation, yet ensuring their safety and reliability remains a critical challenge, especially when exposed to dynamic and unpredictable environments. Real-world testing of an Autonomous Driving System (ADS) is both expensive and risky, making simulation-based testing a preferred approach. In this paper, we propose AVASTRA, a Reinforcement Learning (RL)-based approach to generate realistic critical scenarios for testing ADSs in simulation environments. To capture the complexity of driving scenarios, AVASTRA comprehensively represents the environment by both the internal states of an ADS under-test (e.g., the status of the ADS's core components, speed, or acceleration) and the external states of the surrounding factors in the simulation environment (e.g., weather, traffic flow, or road condition). AVASTRA trains the RL agent to effectively configure the simulation environment that places the AV in dangerous situations and potentially leads to collisions. We introduce a diverse set of actions that allows the RL agent to systematically configure both environmental conditions and traffic participants. Additionally, based on established safety requirements, we enforce heuristic constraints to ensure the realism and relevance of the generated test scenarios. AVASTRA is evaluated on two popular simulation maps with four different road configurations. Our results show AVASTRA's ability to outperform the state-of-the-art approach by generating 30% to 115% more collision scenarios. Compared to the baseline based on Random Search, AVASTRA achieves up to 275% better performance. These results highlight the effectiveness of AVASTRA in enhancing the safety testing of AVs through realistic comprehensive critical scenario generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Autonomous vehicles (AVs) have shown great potential in transforming modern transportation. The global market for self-driving vehicles is estimated to reach 2.3 trillion USD by 2030 [1]. Many automakers, including Tesla, General Motors, and Volkswagen, etc., are actively developing vehicles that are capable of driving autonomously. In 2017, Waymo launched the fully autonomous ride-hailing services after extensive testing on the public roads in Arizona [2]. Tesla released its beta package of full self-driving software in at least 60.000 vehicles in 2022 [3].\nAn AV, also known as an ego vehicle, is controlled by an Autonomous Driving System (ADS) that integrates multiple sensors and software to monitor the driving environment and operate the vehicle. Ensuring AVs can self-drive without collisions is crucial for the widespread acceptance on real-world roads. While testing AVs in real-world operating environments is necessary to evaluate their performance in diverse, dynamic, and unpredictable scenarios, it is both expensive and risky. Real-road testing often requires extensive resources, including controlled test tracks, highly instrumented vehicles, and adherence to regulatory standards, making it a costly process. Moreover, exposing AVs, particularly in their early stages of development, to public roads presents significant safety risks, especially when encountering rare or dangerous edge cases that could result in collisions or system failures. Therefore, it is essential to thoroughly test ADSs in high-fidelity simulators, e.g., LGSVL [4] or CARLA [5] before deploying.\nIn simulation-based testing, the operating environment is often simulated in various ways to expose the potential failures or weaknesses in ADSs. The operating environment includes a wide range of parameters that present the surrounding space in which ADSs operate. For example, these parameters may include environmental conditions like time of day, weather conditions, road structures, and surrounding obstacles such as other vehicles or pedestrians. Several existing testing approaches [6]\u2013[8] aim to generate critical scenarios by systematically searching for suitable configurations of these parameters that could challenge the ADS's operation.\nSearch-based techniques have been widely adopt in various approaches [6], [7], [9]. For example, MOSAT [6] adopts a multi-objective genetic algorithm to search for the atomic or sequence of maneuvers of the other non-player character (NPC) vehicles, which could challenge the ADS and lead it to unsafe scenarios. In another study, AV-Fuzzer [7] perturbs the driving maneuvers of NPC vehicles and employs genetic and fuzzing algorithms to search for scenarios where the AV can dangerously collide with the other vehicles.\nAlthough search-based techniques have succeeded in"}, {"title": "2 PROBLEM FORMULATION", "content": "A driving scenario (or simply scenario) is a temporal development of the environment in which the AV operates [8], [12]\u2013[14]. Each scenario includes a map describing the road structure, an origin-destination pair that defines the starting point and the intended goal of the AV, various environmental conditions (e.g., weather, time of day, and the presence of surrounding static and dynamic obstacles). In our work, a critical scenario is defined as one that places the AV at risk of colliding with surrounding obstacles.\nLet $E = \\{e_1, ..., e_n\\}$ represent the set of environmental parameters that are used to define a scenario, where $e_i \\in E$ corresponds to a specific element of the environment (e.g., weather, time of day, pedestrians, or other vehicles). Given a map $M$ and a pair of origin and destination positions $(P_{org}, P_{dest})$ within the map, the set of specific values assigned to the parameters in $E$ at time step $t$ during the operation of the AV from $P_{org}$ to $P_{dest}$ is denoted by $E_t$.\nThe scenario over a time horizon $T$ is therefore described by a sequence of tuples $(M, E, P_{org}, P_{dest}, \\{E_t\\}_{t=1}^T)$, where $E_t$ represents the environment observed at time step $t$ during the operation of the AV from $P_{org}$ to $P_{dest}$ on $M$. The objective of this study is to find $E^*$, the optimal sequence of configurations for the environmental parameters $\\{E_t\\}_{t=1}^T$, that maximizes the likelihood of collisions during the AV's operation over time. By identifying the most challenging environmental conditions, we aim to push the limits of the ADS and uncover its potential failure points.\nDuring the scenario, the ADS continuously observes the environment and adjusts its own states to navigate safely. Let $V = \\{v_1, ..., v_m\\}$ represent the set of properties that describe the ADS's internal states. Each $v_i \\in V$ captures a specific aspect of the vehicle's behavior (e.g., speed, steering angle, or ADS component statuses). In other words, the vehicle's internal state $V_t$ at time step $t$ is dynamically adjusted by the AV in response to the observed environment $E_t$."}, {"title": "3 CRITICAL SCENARIO GENERATION AS REIN- FORCEMENT LEARNING", "content": "In order to find optimal values $E^*$ for the parame- ters in $E$, AVASTRA formulates this problem as a Rein- forcement Learning (RL) task, represented by a 6-tuple $(\\mathcal{S}, \\mathcal{A}, T, R, \\gamma, \\pi)$.\nSpecifically, $\\mathcal{S}$ is the state space that describes the environ- ment with which the agent interacts. AVASTRA holistically represents the environment by both the external states of the surrounding factors and the internal states of the ADS's components. To effectively capture meaningful and relevant information for decision-making within the scenario, as well as to reduce the dimension of state space, not all external and internal parameters are directly included in the RL state. Some parameters are transformed or abstracted. At each time $t$, the state $s_t \\in \\mathcal{S}$ is composed of:\nTransformed external environmental states from $E_t$: These represent the transformed/aggregated/s-elected values of environmental parameters $e_i \\in E$, like time of day, weather, road condition, and the presence of surrounding vehicles or pedestrians.\nTransformed internal vehicle states from $V_t$: These represent the selected, transformed, or aggregated properties $v_i \\in V$ that describe the ego vehicle's internal state, such as speed or acceleration.\nThe state at time step $t$ is represented as $s_t = \\tau(\\{E_t, V_t\\})$, where $\\tau$ is the transformation procedure which selects/- transforms/aggregates the parameters' values in $E_t$ and ones in $V$. This transformation process helps to not only reduce the state space but also allows the RL agent to effectively observe both the external environment and the ego vehicle's internal statuses. The detailed transformation ($\\tau$) for these parameters will be discussed in Sec. 3.1.\nEach action $a_t \\in \\mathcal{A}$ corresponds to setting a specific value for one or more parameters in $E$, such as modifying weather conditions or adding pedestrians/NPC vehicles. After executing action $a_t$, the state of the environment transitions to a new state $s_{t+1}$ according to the transition function $T(s_{t+1}|s_t, a_t)$. Based on the newly observed state $s_{t+1}$, the agent then receives an immediate reward $r_t$ from the reward function $R(s_t, a_t)$, which quantifies the effectiveness of taking action $a_t$ in the given state $s_t$. The agent receives a higher reward when its selected action leads to conditions that increase the likelihood of collisions for the AV. The goal is to maximize the possibility of generating critical scenarios where the AV is at risk of collisions with other obstacles. A discount factor $\\gamma \\in [0,1]$ is used to prioritize immediate rewards over future rewards, encouraging the agent to take actions that lead to quick increases in the likelihood of collisions while still considering the long-term impact of actions. The probability that the agent will select an action $a_t$ in the given state $s_t$ is decided by the policy $\\pi: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$. The objective of AVASTRA is to find a policy that maximizes the cumulative reward associated with the likelihood of collisions occurring in the scenarios."}, {"title": "3.1 State Representation", "content": "To enable the RL agent to comprehensively understand the driving scenarios, AVASTRA holistically represents the environment by both the internal and external states. The internal states refer to the ADS's internal properties and system status, while the external states describe the external factors of the environment in which the ADS operates.\nBoth internal and external states are critical for assessing the performance and safety of the ADS. External factors, such as weather conditions or the presence of dynamic obstacles, can affect the ADS's perception and decision- making, thereby increasing the risk of collisions. Similarly, internal factors, such as localization, speed, and system diagnostics, are also crucial. For example, incorrect localization could lead the AV to make dangerous movements and result in collisions. Thus, an accurate and holistic representation of both the internal and external states is necessary to fully capture the operating conditions of the AV and to evaluate its ability to respond to critical situations."}, {"title": "3.1.1 External States", "content": "External states refer to the environmental factors that could influence the ADS's operation. In this work, we design the external states following the perception disturbance factors and characteristics which defined by the \u201cJAMA Automated Driving Safety Evaluation Framework\u201d [10]. Specifically, we focus on four main external factors, including weather conditions, time of day, traffic conditions, and road conditions. In particular, weather conditions refer to the presence and severity of weather phenomena, including rain, fog, and wetness. Time of day is categorized by periods like morning, noon, and night. Traffic conditions involve the statuses of the nearest ahead traffic light and traffic flow represented by the number of surrounding obstacles and the information (e.g., speed, distance to the ADS) of the nearest vehicle, which poses the greatest risk to the ADS. Road conditions describe the type and quality of the road structure, such as one-way road, or intersection, etc., each representing unique challenges to the ADS."}, {"title": "3.1.2 Internal States", "content": "Internal states refer to the internal properties within the ADS that reflect the operational status of its core compo- nents. An ADS typically consists of five main components: Localization, Perception, Prediction, Planning, and Control [15]. For fully autonomous driving, the ADS must first localize its position on a high-definition (HD) map. Next, it gathers essential information about the surrounding environment through multiple perception sensors and equipment. Based on the perception information, the ADS predicts the potential trajectories of the nearby pedestrians and vehicles. The planning module then computes an appropriate trajectory for the ADS to follow. Finally, the control module adjusts the vehicle's throttle, brake, and steering to execute the planned trajectory. Each of these modules plays a critical role in the ADS's safe operation, as errors in any of them can lead to dangerous situations. Therefore, in addition to the external states, it is essential to capture these internal states to enable the RL agent to effectively learn about conditions that could lead the AV to potential risks.\nLocalization. An ADS often relies on multiple Global Navigation Satellite System (GNSS) sensors to localize its positions in an HD map. Safe operation requires localization accuracy within centimeter-level [16], as maneuvers like lane changes and turns depend on precise positioning. Inaccurate localization increases the risk of dangerous maneuvers and accidents.\nAVASTRA captures localization inaccuracies as part of the internal states, measured by the deviation between the actual position (provided by the simulator) and the localized position (determined by the ADS's localization module). Formally, let $(x_{local}, y_{local})$ represent the coordinates of the AV's position as obtained from the vehicle's localization module, and $(x_{real}, y_{real})$ represent its actual coordinates on the map. The differences between these positions are quantified by the distance and angle deviations, which measured by the following equations:\n$dist Deviation = \\sqrt{(x_{local} - x_{real})^2 + (y_{local} - y_{real})^2}$ (1)\n$angle Deviation = arctan \\frac{y_{local} - y_{real}}{x_{local} - x_{real}}$ (2)\nPerception. To collect essential information about the surrounding environment, the ADS employs multiple sen- sors and equipment such as cameras, radars, and LiDAR. By combining data from these sources, the perception module recognizes and predicts the location and movement of the nearby obstacles. Perception errors, such as misidentifying objects or incorrectly assessing their distances, can lead to inappropriate responses or incorrect decisions, increasing the risk of collisions.\nAVASTRA encodes perception-related internal states by measuring the differences between the perceived informa- tion and the actual data of the nearby objects. These dif- ferences are measured in terms of position (Equation 3), direction of movement (Equation 4), size (Equation 5), and velocity (Equation 6). Note that there could be more than one nearby object, and each object could have an individual set of these attributes. Without loss of generality, we con- sider the maximum difference as the representative value for each attribute. The reason is that the maximum difference refers to the most significant error in the ADS's perception regarding the corresponding attribute.\nLet $N$ be the number of nearby objects, $(x_{per}^i, y_{per}^i)$ be the perceived coordinates of the object i, and $(x_{real}^i, y_{real}^i)$ be its real ones. The difference in position is calculated as:\n$positionDiff = max_{i\\in[1,N]} \\sqrt{(x_{per}^i - x_{real}^i)^2 + (y_{per}^i - y_{real}^i)^2}$ (3)\nLet $dir_{per}^i$ be the perceived moving direction of the object i and $dir_{real}^i$ be its real moving direction. Equation 4 shows the difference in moving direction.\n$directionDiff = max_{i\\in [1,N]} |dir_{per}^i - dir_{real}^i|$ (4)\nLet $h_{per}^i, w_{per}^i,$ and $l_{per}^i$ be the perceived sizes regarding height, weight, and length of the object i and $h_{real}^i, w_{real}^i,$ and $l_{real}^i$ be its real values. The difference regarding the objects' sizes is computed as:\n$sizeDiff = \\sqrt{max_{i \\in [1,N]}(h_{per}^i - h_{real}^i)^2 + (w_{per}^i - w_{real}^i)^2 + (l_{per}^i - l_{real}^i)^2}$ (5)\nLet $vx_{per}^i$ and $vy_{per}^i$ be the perceived velocity of the object i, and $vx_{real}^i, vy_{real}^i$ be its real velocity values. The difference in velocity is calculated as:\n$velocityDiff = max_{i \\in [1,N]} \\sqrt{(vx_{per}^i - vx_{real}^i)^2 + (vy_{per}^i \u2013 vy_{real}^i)^2}$ (6)\nPrediction and Planning. The prediction module em- ploys various machine learning/deep learning models to predict the trajectories of the surrounding objects, such as pedestrians or vehicles. Then, their predicted paths help guide the planning module's decisions. The planning mod- ule calculates a trajectory for the AV, optimizing it for safety, efficiency, and comfort. The trajectory is continuously up- dated as the vehicle moves and new sensor data is received.\nAlthough prediction and planning are critical, AVASTRA does not explicitly represent them in the internal states for two reasons. First, at the time of prediction, the predicted and planned trajectories have not yet been executed, making their evaluation challenging. Second, the inputs of these modules come from the localization and perception mod- ules, and their outputs are reflected through the operations of the control module. Since the states of localization, per- ception, and control are encoded, the effects of prediction and planning are indirectly represented.\nControl. The control module is responsible for executing the trajectory planned by the planning module. It employs various control algorithms to ensure the vehicle follows the planned trajectory accurately. These algorithms can be categorized into two main types: Lateral control (steering control) and Longitudinal control (speed and braking con- trol). To represent the internal states of the control module, AVASTRA captures several key values that manage the AV's movement, including throttle, steering rate, steering target, acceleration rate, brake percentage, and speed."}, {"title": "3.2 Action Space", "content": "Based on established safety requirements [10], we design the action space to configure the environmental parameters, aiming to generate critical and realistic scenarios for testing the ADS. In practice, the operating environment could be extremely complicated with potentially infinite parameters. Simulating such complexity in a practical testing setup would be computationally prohibitive. However, when test- ing the ADS in a simulated environment, the number of parameters available for configuration is limited by the simulator in use. In this work, we employ the widely-used simulator LGSVL [4], which supports configurable environ- ment parameters across four primary categories: Weather, Time of Day, Pedestrians, and NPC Vehicles. Each category can be divided into sub-types, allowing for detailed control of the environment. For example, Weather includes two sub- types of Phenomenon (i.e., Rain, Fog, and Wetness) and the corresponding intensity Level (i.e., None, Light, Moderate, or High). In general, our approach can be extended to other simulators, such as CARLA [5] or AirSim [17], which offer similar capabilities.\nTo configure the environment, an action involves choos- ing parameter(s) and specifying specific value(s). For exam- ple, setting the weather to heavy rain by assigning Rain to the parameter phenomenon and High to the parameter level accordingly. In AVASTRA, we design a diverse set of actions allowing to configure both the environmental conditions and traffic participants. Environmental conditions include setting weather conditions and time periods. Meanwhile, configur- ing traffic participants involves introducing NPC vehicles and pedestrians with different behaviors into the operating map. \nWhile the flexibility of the simulator allows for many configurable parameters and possible values, a naive ap- proach to action construction could lead to an exponentially large and impractical action space, with many combinations resulting in unrealistic or infeasible scenarios. Therefore, to prevent the combinatorial explosion of parameter values and the generation of impractical scenarios, we incorporate several heuristic constraints to ensure that the actions reflect real-world scenarios.\nChronological time constraint. The time of day must progress in chronological order. Let $time(t)$ represent the value of the time of day at time point $t$. For example, if the state time is Night at $t$, i.e., $time(t) = Night$, it cannot abruptly change to Noon in the next time point $t + 1$. Thus, the constraints on the value of the time parameter for each subsequent time point $t + 1$ are as follows: $time(t) = Morning \\Rightarrow time(t + 1) \\in \\{Morning, Noon\\}$, $time(t) = Noon \\Rightarrow time(t + 1) \\in \\{Noon, Night\\}$, and $time(t) = Night \\Rightarrow time(t + 1) \\in \\{Night, Morning\\}$.\nTraffic Participant Constraints. To test the ADS's re- sponses to nearby traffic participants, AVASTRA gradually introduces NPC vehicles and pedestrians with diverse be- haviors into the operating map. NPC vehicles have five possible behaviors: $NPC\\_behavior \\in \\{drive-ahead, overtake, drive-opposite, cross-road, lane-change\\}$. For a pedestrian, we consider only road-crossing behavior, $Pedestrian\\_behavior \\in \\{cross-road\\}$, since it poses potential risks to the ADS's per- formance. In fact, pedestrians can perform other behaviors, such as walking along the sidewalk; however, such behavior is generally safe and rarely impacts the ADS.\nTo generate challenging and realistic scenarios, it is crucial to position the NPC vehicles or pedestrians (obstacles) such that their potential trajectories should intersect with that of the ego vehicle to create a risk of collision. However, obstacles should not be introduced too abruptly or closely so that a collision becomes unavoidable regardless of the ADS's capabilities. A realistic scenario gives the ADS a fair opportunity to detect and react to potential threats.\nTo guarantee that the scenarios are both challenging and realistic, we propose constraints to determine the position of introducing the obstacles, i.e., constraints for the distance parameter in the actions related to NPC vehicles and pedestrians. Specifically, the distance from the AV to the position of the generating obstacle is measured based on three factors: distance mode (dist_mode \u2208 {Near, Far}), vehicle size (size \u2208 {Small, Large}), and speed. These factors significantly influence how much time and space the ADS needs to respond to the obstacles. First, the distance mode (dist_mode) defines basic settings for testing the ADS's re- actions to the obstacles. Intuitively, the closer an obstacle is, the higher the likelihood of a collision. The farther an obstacle is, the more difficult it is for the detection models. Thus, AVASTRA generates obstacles in both Near and Far cases to comprehensively assess the ADS's performance. Second, vehicle size (size) also significantly impacts ADS's vision and reaction. Larger vehicles often occupy more road space and pose a greater threat to the ADS due to their size and limited maneuverability. Therefore, AVASTRA generates both Small and Large vehicles to test the ADS's reactions. Third, speed represents how fast the ego vehicle and the obstacle approach each other. Hence, the speed values are also leveraged to estimate the initial distance of the obstacle so that a collision could happen.\n$distance = \\alpha * f_{\\alpha}(dist\\_mode) + \\beta * f_{v}(size) + \\gamma * f_{s}(speed)$ (7)\nWithout loss of generality, the distance for generating an obstacle is defined by Equation 7. In this Equation, $(\\alpha, \\beta, \\gamma)$ are weight coefficients for each factor, which can be adjusted according to their relative importance. In this work, AVAS- TRA considers them equally important, with $\\alpha = \\beta = \\gamma = 1$. $f_{\\alpha}(dist\\_mode)$ defines the base minimum distance, which specifies the setting of Near or Far. For instance, we could set $f_{\\alpha}(Far) = 50m$ and $f_{\\alpha}(Near) = 12m$. $f_{v}(size)$ specifies the additional distance required according to the vehicle size. Large vehicles require greater distance since they pose a greater challenge to ADS safety. For example, AVASTRA can set $f_{v}(Small) = 0m$ and $f_{v}(Large) = 3m$. Large vehicles include BoxTruck and SchoolBus, while the Small vehicles are Jeep, Sedan, SUV, and Hatchback. $f_{s}(speed)$ determines the extra distance required based on the obstacle's speed. Higher-speed obstacles need more extra distance because they can reach the (estimated) intersection point of their and the ego vehicle's (potential) trajectories more quickly. If this distance is not increased accordingly, the obstacle could pass the intersection point before the ego vehicle arrives, reduc- ing the effectiveness of the generated scenario. Therefore, AVASTRA incorporates extra distance adjustments based on the obstacle's speed. The specific values of these functions are empirically determined via various experiments. \nMoreover, the relative position of the NPC vehicle to the ego vehicle's lane (i.e., on the Current, Left, or Right lane) also affects the ADS's operation because it impacts how the ADS perceives, predicts, and plans responses to NPC vehicles. Therefore, in generating NPC vehicles, AVASTRA considers all the three possible values of the lane parameter. The speed of an NPC vehicle is randomly set within the range of [10, 20] m/s, reflecting the typical speed of cars on the road. Additionally, speed is also guaranteed to be below the speed limitation of the operating road. The pedestrians' speed is guaranteed to be no greater than the normal human walking pace, 1.4 m/s. Furthermore, to enhance the diversity of the generated scenarios, the values of vehicle_type and vehicle_color are randomly selected for each action.\nWeather pattern constraints. Weather phenomena and their intensity levels are interdependent. For example, it would be unrealistic for Wetness to be at High level without the presence of Rain or Fog. However, precisely modeling"}, {"title": "3.3 Reward Function", "content": "After executing action $a_t$, the operating environment tran- sitions from state $s_t$ to the new state $s_{t+1}$, represented by the transition $(s_t, a_t, s_{t+1})$. To provide feedback on the effectiveness of the action $a_t$, AVASTRA rewards the RL agent $r_t$. The reward is designed to reflect the preference of the action. Actions that better align with AVASTRA's objective of generating critical scenarios are preferable and receive higher reward values. Intuitively, $r_t$ is calculated by a reward function designed based on the collision probability (ProC), where actions that increase this probability are rewarded more. Particularly, actions that result in an actual collision will receive the highest reward (Rcol), which is much higher than the rewards of the others. This encourages the agent to prioritize actions that really challenge the ADS. Specifically, the reward function is formulated as follows:\n$r_{t}=\\begin{cases}\nR_{col}, & ProC=1.0 \\\\ProC, & 0.2 < ProC < 1.0 \\\\-1, & otherwise\n\\end{cases}$ (8)\nIn this formula, $ProC$ represents the collision probabil- ity, and $Rcol$ is the reward awarded to agent when its chosen action results in an actual collision. If $ProC = 1$, indicating a collision has occurred, the agent receives a reward of $Rcol$. For $0.2 < ProC < 1$, the selected action places the ego vehicle in a dangerous situation, and receives a reward of ProC. In cases where $ProC \\leq 0.2$, indicating safe conditions, the agent receives no reward and is penalized with a reward of -1.\nMoreover, to effectively train the DDQN model, a Replay Buffer with Prioritized Experience Replay (PER) algorithm is employed in the training process. Specifically, at each time step $t$, the transition $\\langle s_t, a_t, s_{t+1} \\rangle$ along with its correspond- ing reward $r_t$ is stored into the buffer. When the Replay Buffer reaches its capacity, the transitions and rewards are prioritized using PER, which ensures that high-priority tran- sitions are selected to train the DDQN model.\nEach scenario initiates from the origin to a designed destination. However, the scenario may terminate early if the ego vehicle is unable to move forward after 3 to 5 consecutive steps, ensuring that inefficient or deadlock conditions are handled appropriately."}, {"title": "4.1 Double Deep Q-Network", "content": "For the RL problem, Deep Q-Network (DQN) is a popular algorithm demonstrated as a powerful tool for estimating the Q-values [18]. However, the inherent stochasticity in the AV's environments, including unpredictable weather con- dition, traffic statuses, and surrounding objects' behaviors, can lead DQN to overestimate the action's effectiveness due to their reliance on a single network for both action selection and evaluation [19]. This results in biased updates that may converge to suboptimal policies.\nTo overcome this, AVASTRA employs DDQN to estimate the expected accumulated reward for each action within a given state, as shown in Figure 2. DDQN mitigates the overestimation issue using two separate networks: one for action selection and one for action evaluation. The action se- lection network determines the action with the highest Q- value for a given state, while the action evaluation network evaluates this choice and updates the Q-values based on actual outcomes. This separation allows for more accurate evaluation, ensuring that Q-values reflect real-world perfor- mance rather than inflated estimates. Periodic updates from the selection network to the evaluation network further stabilize the learning process.\nIn AVASTRA, both networks share the same architecture, comprising four layers: an input layer, two hidden lay- ers, and an output layer. The input layer processes state vectors that comprehensively represent both the internal and the external states (see Sec. 3.1). The hidden layers extract relevant features, and the output layer estimates Q-values for potential actions. ReLU activation functions are used for hidden layers, with Linear in the output layer to approximate return for agent. At each time step $t$, the agent selects an action to configure the environment. To avoid premature convergence to local optima, we implement an $\\epsilon$-greedy exploration strategy. Initially, $\\epsilon$ is set to 1.0, allowing for random exploration as the agent learns the environment dynamics. As training progresses, $\\epsilon$ is gradually reduced to 0.1 to balance exploration with exploitation, following the decay schedule from previous studies [8], [20].\nTo further improve the training process, AVASTRA em- ploys a Replay Buffer with PER algorithm. After executing an action $a_t$, transition $\\langle s_t, a_t, s_{t+1} \\rangle$ and the corresponding reward $r_t$ are stored into the buffer. The PER algorithm prioritizes these transitions based on the accuracy evaluated by the selection network, ensuring that the agent focuses more on poorly evaluated actions. This targeted prioriti- zation reduces bias towards frequent but less impactful transitions, enabling more effective and efficient learning of critical scenario generation."}, {"title": "4.2 Collision Probability Measurement", "content": "In this work, we adopt a straightforward approach to cal- culate the collision probability, similar to the method used in prior study [8]. The calculation relies on the safety distance and the current distance between the AV and the other surrounding obstacles in the operating environment. Specif- ically, the safety distance represents the minimum required separation between the AV and other objects to avoid a collision, while the current distance refers to the actual distance at a given moment. If the current distance is smaller than the safety distance, the AV is in a dangerous situation, and a collision could happen.\nIn practice, the safety distance could be affected by various factors such as road conditions, speed, vehicle performance, driver/ADS reaction time, etc. However, these factors can- not be fully accessed in a simulation environment, and it is very complicated to consider them. To simplify the process, as a simulation-based testing approach, AVASTRA follows the related studies [8], [21]\u2013[23] that model safety distance based on speed and deceleration in both the longi- tudinal and lateral dimensions. Since AVs can react almost instantaneously [24], the reaction time is disregarded in the calculation of safety distance in this work.\nThe longitudinal safety distance (LoSD) ensures the AV maintains a safe distance from vehicles ahead and behind in the same lane, while the lateral safety distance (LaSD) en- sures safe spacing between the AV and vehicles in adjacent lanes. These distances are calculated as follows:\n$LoSD(v_{f}, v_{l}) = \\frac{1}{2}(\\frac{v_{f}^{2}}{a_{f}} - \\frac{v_{l}^{2}}{a_{l}})+R_{min}$ (9)\n$LaSD(v_{ego}) = \\frac{(v_{ego} \\times sin\\beta)^{2}}{a_{ego} \\times sin\\beta}$ (10)\nHere, $v_{f}, v_{l}, v_{ego}$ are the velocities of the following, leading, and the ego vehicles, respectively. Similarly, $a_{f}, a_{l},$ and $a_{ego}$ denotes their deceleration rates. $R_{min}$ refers to the minimum allowable distance between two vehicles, while $\\beta$ is the angle between the ego vehicle's direction and the lane where the obstacle is located. In this work, we use the default settings from the Berkeley algorithm [21], which assigns a deceleration value of -6m/s\u00b2 for all vehicles and sets $R_{min}$ to 5 meters. The values of $v_{f}, v_{l}, v_{ego}$, and $\\beta$ can be obtained from the simulator.\nThe current distance between the ego vehicle and any ob- stacle is determined using their respective positions. Let $(x_{ego}, y_{ego}, z_{ego})$ represent the position of the ego vehicle, and $(x_{ob}, y_{ob}, z_{ob})$ be the position of an obstacle. The current distance is then computed as:\n$CD = \\sqrt{(x_{ego} - x_{ob})^{2} + (y_{ego} - y_{ob})^{2} + (z_{ego} - z_{ob})^{2}}$ (11)\nThe collision probability is derived from the calculated Safety Distance and Current Distance, following the existing method [8]. We define $ProC$ in two dimensions, longitudinal and lateral, denoted as $loProC$ and $laProC$, respectively, to measure the likelihood of a collision with surrounding ob- stacles. Let N be the total number of surrounding obstacles observed at time step t; $loProC_{i}$ and $laProC_{i}$ represent the probability that the ego vehicle collides with ith obstacle in two dimensions. These overall longitudinal and lateral collision probabilities of the AV are calculated as follows:"}, {"title": "4.2.1 Safety distance", "content": "In practice, the safety distance could be affected by various factors such as road conditions, speed, vehicle performance, driver/ADS reaction time, etc. However, these factors can- not be fully accessed in a simulation environment, and it is very complicated to consider them. To simplify the process, as a simulation-based testing approach, AVASTRA follows the related studies [8], [21]\u2013[23] that model safety distance based on speed and deceleration in both the longi- tudinal and lateral dimensions. Since AVs can react almost instantaneously [24], the reaction time is disregarded in the calculation of safety distance in this work."}, {"title": "4.2.2 Current distance", "content": "The current distance between the ego vehicle and any ob- stacle is determined using their respective positions. Let $(x_{ego}, y_{ego}, z_{ego})$ represent the position of the ego vehicle, and $(x_{ob}, y_{ob}, z_{ob})$ be the position of an obstacle. The current distance is then computed as:"}, {"title": "4.2.3 Collision probability", "content": "The collision probability is derived from the calculated Safety Distance and Current Distance, following the existing method [8"}]}