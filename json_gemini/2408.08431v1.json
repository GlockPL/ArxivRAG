{"title": "Multi-Modal Dialogue State Tracking for Playing Guess Which Game", "authors": ["Wei Pang", "Ruixue Duan", "Jinfu Yang", "Ning Li"], "abstract": "Guess Which is an engaging visual dialogue game that involves interaction between a Questioner Bot (QBot) and an Answer Bot (ABot) in the context of image-guessing. In this game, QBot's objective is to locate a concealed image solely through a series of visually related questions posed to ABot. However, effectively modeling visually related reasoning in QBot's decision-making process poses a significant challenge. Current approaches either lack visual information or rely on a single real image sampled at each round as decoding context, both of which are inadequate for visual reasoning. To address this limitation, we propose a novel approach that focuses on visually related reasoning through the use of a mental model of the undisclosed image. Within this framework, QBot learns to represent mental imagery, enabling robust visual reasoning by tracking the dialogue state. The dialogue state comprises a collection of representations of mental imagery, as well as representations of the entities involved in the conversation. At each round, QBot engages in visually related reasoning using the dialogue state to construct an internal representation, generate relevant questions, and update both the dialogue state and internal representation upon receiving an answer. Our experimental results on the VisDial datasets (v0.5, 0.9, and 1.0) demonstrate the effectiveness of our proposed model, as it achieves new state-of-the-art performance across all metrics and datasets, surpassing previous state-of-the-art models. Codes and datasets from our experiments are freely available at .", "sections": [{"title": "1 Introduction", "content": "In the future, visual conversational agents may engage in natural language con- versations. Research in vision-language tasks is still in its early stages, progress- ing from single-round VQA [1] to multi-round Visual Dialogue and multi-modal multi-modal Video Dialogue [2,16,3,13]. Among these tasks, Guess Which stands out as a two-player image-guessing game with a QBot and ABot. QBot aims to identify a hidden image by asking questions to ABot. While ABot has received attention, research on QBot is relatively limited, which is the focus of this paper.\nQBot starts by receiving a caption and formulating the first question. Sub- sequent questions are generated based on the caption and dialogue history. Per- forming visual reasoning solely from textual information poses a challenge. Exist- ing approaches, as illustrated in Fig.1 (a) and (b), can be divided into non-visual methods [3,4,6,8,14,20,19] that rely on language-based models and real-image methods [21] that retrieve probable images as visual context. However, both approaches have limitations in visual reasoning.\nFirstly, QBot needs to generate questions that pertain to the image, linking words to visual concepts, much like humans do [9]. Hence, QBot, which lacks visual information in its modeling, is inadequate for this purpose. Secondly, QBot samples a single real image from a large pool of images, often numbering in the thousands, e.g., 9,628 candidate images per round. This approach is unnatural in the context of a genuine game and introduces substantial sampling variance, rendering the model's reasoning process unstable.\nDrawing on the Dual-coding theory [9], human cognition is based on two interconnected modes: words and images. Mental imagery plays a significant role in word comprehension. Building on this theory, we propose aligning dia- logue entities with mental visual concepts in QBot, as depicted in Fig.1(c). By constructing a mental model of the secret image through textual semantics, we establish a dialogue state consisting of representations of dialogue entities and mental objects denoted as (words, images) states.\nAs the game progresses, the dialogue state evolves, encompassing represen- tations of mental objects in QBot's mind, and prompting QBot to pose visually related questions. To the best of our knowledge, the problem of modeling visual\nreasoning in QBot remains relatively unexplored. In this paper, we present a QBot model that incorporates mental imagery through dialogue state tracking (DST) to address the aforementioned ideas. Our model consists of a cycle of two primary procedures: Visual Reasoning on Dialogue State (VRDS) and State Tracking (STrack).\nVRDS facilitates three-hop reasoning in the dialogue state, progressing through the path words\u2192words\u2192images\u2192words and generating an internal representa- tion. A decoder utilizes this representation to generate new questions. Upon receiving an answer, STrack is activated, involving two actions: 1) Addition, in- troducing new textual semantics to the dialogue states (e.g., \"strawberry\" as a new entity and its associated mental object). 2) Update, incorporating new textual features (e.g., colors, positions, counts) into the aligned dialogue states' existing representation. Experimental results demonstrate our model's superior performance, achieving a new state-of-the-art level. In summary, our contribu- tions are three-fold:\nWe propose a novel QBot agent that is capable of performing visually related reasoning based on mental imagery in one's mind in dialog.\nWe present dialogue state tracking based QBot model (DST), which learns to form representations of mental imagery that support visually related reason- ing. The dialogue states, composed of not only words states but also images states, are tracked and updated through dialoguing with ABot.\nAchieving new state-of-the-art results on the Guess Which game underlying VisDial v0.5, v0.9 and v1.0 datasets. Compared with prior studies, this work takes a step towards mimicking humans playing a series of visual dialogue games (such as GuessWhich)."}, {"title": "2 Related Work", "content": "Visual Dialogue is a key area of research in vision-language studies, with the aim of developing conversational agents capable of human-like interactions. Re- cent progress [10,12,11] has been made in various tasks, including Guess What!?, Guess Which, and Visual & Video Dialogue. Guess Which specifically involves the challenge of QBot finding an undisclosed image from a large pool without sharing it with ABot.\nExisting QBot models can be categorized into non-visual and real-image ap- proaches. Non-visual models, represented in Fig.1(a), do not utilize visual infor- mation. For example, Das et al. [4] propose an encoder-decoder network with a feature regression network. They use a hierarchical encoder-decoder architecture to generate questions based on history. Murahari et al. [8] introduce a Smooth-L1 Penalty to mitigate repetitive questions. Zhao et al. [19] incorporate an Atten- tive Memory Network, and Li et al. [6] propose an information-theoretic model. These models solely rely on textual information.\nReal-image models, shown in Fig.1(b), provide physical images as input to the QBot decoder. Zhou et al. [21] introduce an image-guesser module into the QBot"}, {"title": "3 Model", "content": "The proposed Dialogue State Tracking based QBot (DST) model, depicted in Fig.2, consists of five modules: Recursive Self-Reference Equation (R-SRE), Visual Reasoning on Dialogue State (VRDS), Question Decoder (QDer), QBot Encoders (QEnc), and State Tracking (STrack). Detailed explanations of each module will be provided in the subsequent sections.\nProblem Setting At the start of the game, QBot receives a caption C describ- ing the target image I* visible to ABot. This caption provides the initial textual information for QBot to generate its first question. In subsequent rounds, QBot generates questions q(t) based on the accumulated dialogue history H, which includes the caption C and previous question-answer pairs. QBot's goal is to use this information to guess the undisclosed image from a candidate pool.\nWe define an accumulated dialogue state <M(t), M(t) > at round t. The words state M(t) represents the textual representation of discussed entities, while the images state M(t) represents mental imagery information derived from the words in QBot's mind. The initial dialogue state <M(1), M(1)> is constructed using the caption C as input and the Adding action of the STrack module. Both M(1) and M(1) are 1 \u00d7 d matrices.\nRecursive Self-Reference Equation (R-SRE) To capture visually related interactions within and between modalities in the dialogue state, we propose a recursive self-reference equation (R-SRE) denoted as V' = R \u2013 SRE(Q, V). The R-SRE consists of two attention mechanisms that facilitate the update of the matrix V based on the guidance provided by another matrix Q. The equation is formulated as follows:\n$\\alpha = softmax(w_aQ),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{1}$\n$q = \\alpha^T Q,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{2}$\n$\\beta = softmax(w_\\beta[r^{(k)}(q); r^{(k)}(q) \\odot V; V]),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{3}$\n$V' = \\beta V,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{4}$\nwhere Q, V \u2208 Rk\u00d7d are two input data matrix, wa \u2208 Rd\u00d71 and w\u00df \u2208 R3d\u00d71 are trainable projection matrices. [;;] denotes the symmetric concatenation between bi-modalities as in [11,12], is the element-wise product. We define a repeat operation r(k) (q) that repeats q k times to form a matrix of size k \u00d7 d with same dimension as V. Specially, a self-attention on Q is first performed and attention scores a \u2208 Rk\u00d71 is obtained, q \u2208 Rd\u00d71 is the weighted sum of Q using a. Then, q is referred to as a key again to query V to get a \u03b2\u2208 Rk\u00d71 over V. Finally, V is changed by multiplying the attention weight \u1e9e to yield a new same-dimension representation V'. We omit bias where possible, for clarity.\nVisual Reasoning on Dialogue State (VRDS)\nThe VRDS process involves three-hops reasoning using the R-SRE opera- tion. Firstly, a self R-SRE operation updates the words state (Eq.5). Then, a cross-modal R-SRE operation generates a visually related context vector (Eq.6). Iterative cross-modal R-SRE operations between the updated words state and the images state yield an intermediate textual context vector (Eq.7). Finally, the visually and textually related context vectors are concatenated and processed by a linear layer to obtain the final context vector. Formally,\n$M^{(t)}_w = R-SRE(M^{(t)}_w, M^{(t)}_w),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{5}$\n$M^{(t)}_i = R-SRE(M^{(t)}_w, M^{(t)}_i), a^{(t)} = sum(M^{(t)}_i),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{6}$\n$M^{(t)}_i = R-SRE(M^{(t)}_i, M^{(t)}_i), a^{(t)} = sum(M^{(t)}_i),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{7}$\n$z^{(t)} = Dropout(w[a^{(t)}; a^{(t)}]),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{8}$\nwhere wv \u2208 R2dxd are learnable projection matrixes, sum is operated on the k- dimension to compact the intermediate representation matrix to a vector, z(t) \u2208 Rd is the final context vector passed to QDer module.\nFrom Eq.5 to Eq.8, VRDS performs textually and visually related interac- tions upon dialogue state. It models the cross-modal interactions between two states motivated from Dual-coding theory[9]: QBot can think with words as well as can think with images based on mental imagery representations in one's mind.\nQuestion Decoder (QDer) We use a multi-layer Transformer decoder [15] and employs softmax on its output. It takes z(t) and dialogue states as input. The QDer module predicts next word by employing cross-attention with dialogue states as conditioning, continuing this process until an end-of-sequence token [EOS] is encountered. Formally,\n$h^{(t)}_1 = transformer\\_decoder(h^{(t)}, M^{(t)}_w, M^{(t)}_i),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{9}$\n$w^{(t)}_1 = argmax softmax(h^{(t)}_1w_{der}),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{10}$\nwhere wder \u2208 Rd\u00d7|V| is trainable parameters, V is the number of vocabulary. h(t)\n+1\n1 denotes the representation of next token w(t)\n+1\n1 , which is selected by the greedy algorithm. h(t) is initialized as the element-wise addition of the starting\n(t)\ntoken [SOS] and z(t). We save a sequence of w wi1 to produce a new question q(t).\nQBot Encoders (QEnc) QEnc utilizes a pre-trained vision-linguistic model called ViLBERT [7]. The input to ViLBERT is structured as follows: [CLS] q(t) [SEP] a(t) [SEP] M(t) [SEP] M(t), with each segment separated by a [SEP] token. The output of [CLS] token is considered as the fact representation f(t) \u2208 Rd, which captures the information from new question-answer pair.\nState Tracking (STrack) STrack offers two actions: Adding and Updating. The decision between these actions is determined by a differentiable binary choice made using Gumbel-Softmax sampling [5]. This allows for end-to-end training. We introduce a two-layer feedforward network denoted as FFN(\u00b7), which in- corporates GELU activation, Dropout, and LayerNorm. For clarity, we apply FFN(\u00b7) to the element-wise product of the fact representation f(t) and words state Mt. Subsequently, a Gumbel-Softmax operation is performed, yielding a probability distribution representing the action type.\n$\\phi = Gumbel(FFN([r^{(k)}(f^{(t)}) \\odot M^{(t)}_w])),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{11}$\nwhere \u2208 R\u00b2 is 2-d one-hot vector for discrete decision. According to predicted , one of the two actions is chosen for the STrack.\nAdding Action on Words State The Adding action is executed on the current words state if [0] = 1. It takes the f(t) as input to FFN, resulting in a new textual representation e(t+1)a, w \u2208 Rd. Subsequently, e(t+1)a, w is inserted into the set of words states, leading to a new set of words states M(t+1) \u2208 R(k+1)\u00d7d:\n$e^{(t+1)}_w = FFN(f^{(t)}),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{12}$\n$M^{(t+1)}_w = M^{(t)}_w \\cup {e^{(t+1)}_w},                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{13}$\nwhere U is an append operation. Note that the size of newly updated M(t+1) is increased to k + 1.\nAdding Action on Images State In the case of [0] = 1, the adding action is performed on current images states. It produces a new mental object o(t)\n+1 \u2208 Rd, which denotes a new visual concept (e.g., \u201ccarpet\u201d). We translate the fact representation into images with FFN network, and get a new set of images states M(t+1) \u2208 R(k+1)\u00d7d, in which its size is increased to k + 1. Formally,\n$o^{(t)}_i = FFN(f^{(t)}),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{14}$\n$M^{(t+1)}_i = M^{(t)}_i \\cup {o^{(t)}},                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\tag{15}$\nwhere fact representation f(t) is used for translation from words to images by FFN."}, {"title": "4 Experiment and Evaluation", "content": "Dataset Our Guess Which model is evaluated on three benchmarks: VisDial v0.5, v0.9, and v1.0. These datasets include various numbers of training, valida- tion, and test images. VisDial v1.0 has 123,287 training images, 2,064 validation images, and 8,000 test images. VisDial v0.9 includes 82,783 training images and 40,504 validation images. VisDial v0.5 consists of 50,729 training images, 7,663 validation images, and 9,628 test images. Dialogues in these datasets contain a caption for the target image and multiple question-answer pairs. It's worth noting that only the test set of VisDial v1.0 has variable-length dialogues, while the other dataset splits have fixed 10-round dialogues.\nEvaluation Metric We follow the standard evaluation metrics [4,8] for QBot in two parts: image guessing and question diversity. At image guessing, we report retrieval metrics of target image, including mean reciprocal rank (MRR), Recall @k (R@k) for k = 1, 5, 10, mean rank (Mean) and percentile mean rank (PMR). At question diversity, we adopt six metrics like Novel Questions [8], Unique Questions [8], Dist-n and Ent-n [17], Negative log-likelihood [8], and Mutual Overlap [18].\nImplementation Details Our model architecture consists of a cross-modal Transformer decoder with 12 layers and a hidden state size of 768. It utilizes 12 attention heads. The base encoder is a pre-trained ViLBERT model with 12 layers and a hidden state size of 768. During training, we used a batch size of 64 and trained the model for 30 epochs. A dropout rate of 0.1 was applied after each linear layer. Early stopping was implemented on the validation split if the performance metric (PMR) did not improve for 10 consecutive epochs. We used the Adam optimizer with a base learning rate of 1e-3, which decayed to 1e-5 during training. For image representation, we used pre-extracted VGG19 features, where each image is represented by a 4096-dimensional vector.\nComparison to State-of-the-Art Methods The comparing methods on QBot can be regarded to have three types: 1) Non-visual based models, like SL-Q [4], ReCap[14], ADQ[8], AMN[19], RL-Q [4], AQM+/indA[6], ReeQ-SL (trained in SL) [20] and ReeQ-RL (fine-tuned in reinforcement learning) [20]. 2) Real-image based methods, such as SL-Q-IG [21]. and 3) Mental-imagery based method: our DST."}, {"title": "5 Conclusion", "content": "This paper proposes DST, a novel dialogue state tracking approach for visual di- alog question generation in the Guess Which game. DST maintains and updates dialogue states, including word and mental image representations, enabling men-"}]}