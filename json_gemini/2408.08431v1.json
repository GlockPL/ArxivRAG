{"title": "Multi-Modal Dialogue State Tracking for Playing Guess Which Game", "authors": ["Wei Pang", "Ruixue Duan", "Jinfu Yang", "Ning Li"], "abstract": "Guess Which is an engaging visual dialogue game that involves interaction between a Questioner Bot (QBot) and an Answer Bot (ABot) in the context of image-guessing. In this game, QBot's objective is to locate a concealed image solely through a series of visually related questions posed to ABot. However, effectively modeling visually related reasoning in QBot's decision-making process poses a significant challenge. Current approaches either lack visual information or rely on a single real image sampled at each round as decoding context, both of which are inadequate for visual reasoning. To address this limitation, we propose a novel approach that focuses on visually related reasoning through the use of a mental model of the undisclosed image. Within this framework, QBot learns to represent mental imagery, enabling robust visual reasoning by tracking the dialogue state. The dialogue state comprises a collection of representations of mental imagery, as well as representations of the entities involved in the conversation. At each round, QBot engages in visually related reasoning using the dialogue state to construct an internal representation, generate relevant questions, and update both the dialogue state and internal representation upon receiving an answer. Our experimental results on the VisDial datasets (v0.5, 0.9, and 1.0) demonstrate the effectiveness of our proposed model, as it achieves new state-of-the-art performance across all metrics and datasets, surpassing previous state-of-the-art models. Codes and datasets from our experiments are freely available at .", "sections": [{"title": "1 Introduction", "content": "In the future, visual conversational agents may engage in natural language conversations. Research in vision-language tasks is still in its early stages, progressing from single-round VQA [1] to multi-round Visual Dialogue and multi-modal multi-modal Video Dialogue [2,16,3,13]. Among these tasks, Guess Which stands out as a two-player image-guessing game with a QBot and ABot. QBot aims to identify a hidden image by asking questions to ABot. While ABot has received attention, research on QBot is relatively limited, which is the focus of this paper."}, {"title": "2 Related Work", "content": "Visual Dialogue is a key area of research in vision-language studies, with the aim of developing conversational agents capable of human-like interactions. Recent progress [10,12,11] has been made in various tasks, including Guess What!?, Guess Which, and Visual & Video Dialogue. Guess Which specifically involves the challenge of QBot finding an undisclosed image from a large pool without sharing it with ABot.\nExisting QBot models can be categorized into non-visual and real-image approaches. Non-visual models, represented in Fig.1(a), do not utilize visual information. For example, Das et al. [4] propose an encoder-decoder network with a feature regression network. They use a hierarchical encoder-decoder architecture to generate questions based on history. Murahari et al. [8] introduce a Smooth-L1 Penalty to mitigate repetitive questions. Zhao et al. [19] incorporate an Attentive Memory Network, and Li et al. [6] propose an information-theoretic model. These models solely rely on textual information.\nReal-image models, shown in Fig.1(b), provide physical images as input to the QBot decoder. Zhou et al. [21] introduce an image-guesser module into the QBot"}, {"title": "3 Model", "content": "The proposed Dialogue State Tracking based QBot (DST) model, depicted in Fig.2, consists of five modules: Recursive Self-Reference Equation (R-SRE), Visual Reasoning on Dialogue State (VRDS), Question Decoder (QDer), QBot Encoders (QEnc), and State Tracking (STrack). Detailed explanations of each module will be provided in the subsequent sections.\nProblem Setting At the start of the game, QBot receives a caption C describing the target image I* visible to ABot. This caption provides the initial textual information for QBot to generate its first question. In subsequent rounds, QBot generates questions q(t) based on the accumulated dialogue history H, which includes the caption C and previous question-answer pairs. QBot's goal is to use this information to guess the undisclosed image from a candidate pool.\nWe define an accumulated dialogue state <M(t), M(t) > at round t. The words state M(t) represents the textual representation of discussed entities, while the images state M(t) represents mental imagery information derived from the words in QBot's mind. The initial dialogue state <M(1), M(1)> is constructed using the caption C as input and the Adding action of the STrack module. Both M(1) and M(1) are 1 \u00d7 d matrices.\nRecursive Self-Reference Equation (R-SRE) To capture visually related interactions within and between modalities in the dialogue state, we propose a recursive self-reference equation (R-SRE) denoted as V' = R \u2013 SRE(Q, V). The R-SRE consists of two attention mechanisms that facilitate the update of the matrix V based on the guidance provided by another matrix Q. The equation is"}, {"title": "Visual Reasoning on Dialogue State (VRDS)", "content": "The VRDS process involves three-hops reasoning using the R-SRE operation. Firstly, a self R-SRE operation updates the words state (Eq.5). Then, a cross-modal R-SRE operation generates a visually related context vector (Eq.6). Iterative cross-modal R-SRE operations between the updated words state and the images state yield an intermediate textual context vector (Eq.7). Finally, the visually and textually related context vectors are concatenated and processed by a linear layer to obtain the final context vector. Formally,"}, {"title": "Question Decoder (QDer)", "content": "We use a multi-layer Transformer decoder [15] and employs softmax on its output. It takes z(t) and dialogue states as input. The QDer module predicts next word by employing cross-attention with dialogue states as conditioning, continuing this process until an end-of-sequence token [EOS] is encountered. Formally,"}, {"title": "State Tracking (STrack)", "content": "STrack offers two actions: Adding and Updating. The decision between these actions is determined by a differentiable binary choice made using Gumbel-Softmax sampling [5]. This allows for end-to-end training. We introduce a two-layer feedforward network denoted as FFN(\u00b7), which incorporates GELU activation, Dropout, and LayerNorm. For clarity, we apply FFN(\u00b7) to the element-wise product of the fact representation f(t) and words state Mt. Subsequently, a Gumbel-Softmax operation is performed, yielding a probability distribution representing the action type."}, {"title": "Adding Action on Words State", "content": "The Adding action is executed on the current words state if $[0] = 1. It takes the f(t) as input to FFN, resulting in a new textual representation e(t+1)a, w \u2208 Rd. Subsequently, e(t+1)a, w is inserted into the set of words states, leading to a new set of words states M(t+1) \u2208 R(k+1)\u00d7d:"}, {"title": "Adding Action on Images State", "content": "In the case of $[0] = 1, the adding action is performed on current images states. It produces a new mental object o(t+1)a \u2208 Rd, which denotes a new visual concept (e.g., \u201ccarpet\u201d). We translate the fact representation into images with FFN network, and get a new set of images states M(t+1) \u2208 R(k+1)\u00d7d, in which its size is increased to k + 1. Formally,"}, {"title": "Updating Action on Words State", "content": "When [0] = 0, the updating action is applied to the words state. It includes calculating an assignment distribution that determines how much the new fact representation can be merged into the existing representation of the associated words state. This is achieved by passing f(t) and the current words state M(t) through a two-layer feedforward network and a softmax classifier."}, {"title": "Updating Action on Images State", "content": "If [0] = 0, similar to the case of updating on words state, we compute an assignment distribution for associating current fact representation with previous images state. Formally,"}, {"title": "Model Training", "content": "Our model is optimized using supervised learning (SL) with three loss functions: Cross-Entropy (CE) loss, Mean Square Error (MSE) loss, and Progressive (PL) loss. The CE loss is computed based on ground truth questions, while the MSE loss is calculated using the feature regression network f(.) [4] to predict the image representation y(t)I. The MSE loss compares the predicted representation y(t) I with the ground truth image representation y obtained from VGG19."}, {"title": "4 Experiment and Evaluation", "content": "Dataset Our Guess Which model is evaluated on three benchmarks: VisDial v0.5, v0.9, and v1.0. These datasets include various numbers of training, validation, and test images. VisDial v1.0 has 123,287 training images, 2,064 validation images, and 8,000 test images. VisDial v0.9 includes 82,783 training images and 40,504 validation images. VisDial v0.5 consists of 50,729 training images, 7,663 validation images, and 9,628 test images. Dialogues in these datasets contain a caption for the target image and multiple question-answer pairs. It's worth noting that only the test set of VisDial v1.0 has variable-length dialogues, while the other dataset splits have fixed 10-round dialogues.\nEvaluation Metric We follow the standard evaluation metrics [4,8] for QBot in two parts: image guessing and question diversity. At image guessing, we report retrieval metrics of target image, including mean reciprocal rank (MRR), Recall @k (R@k) for k = 1, 5, 10, mean rank (Mean) and percentile mean rank (PMR). At question diversity, we adopt six metrics like Novel Questions [8], Unique Questions [8], Dist-n and Ent-n [17], Negative log-likelihood [8], and Mutual Overlap [18].\nImplementation Details Our model architecture consists of a cross-modal Transformer decoder with 12 layers and a hidden state size of 768. It utilizes 12 attention heads. The base encoder is a pre-trained ViLBERT model with 12 layers and a hidden state size of 768. During training, we used a batch size of 64 and trained the model for 30 epochs. A dropout rate of 0.1 was applied after each linear layer. Early stopping was implemented on the validation split if the performance metric (PMR) did not improve for 10 consecutive epochs. We used the Adam optimizer with a base learning rate of 1e-3, which decayed to 1e-5 during training. For image representation, we used pre-extracted VGG19 features, where each image is represented by a 4096-dimensional vector.\nComparison to State-of-the-Art Methods The comparing methods on QBot can be regarded to have three types: 1) Non-visual based models, like SL-Q [4], ReCap[14], ADQ[8], AMN[19], RL-Q [4], AQM+/indA[6], ReeQ-SL (trained in SL) [20] and ReeQ-RL (fine-tuned in reinforcement learning) [20]. 2) Real-image based methods, such as SL-Q-IG [21]. and 3) Mental-imagery based method: our DST.\nImage Guessing The results of image guessing are provided in Table 1. Our DST model achieves significant improvements over previous state-of-the-art models (SOTA) across all metrics and datasets. On the validation split of v1.0, DST outperforms ReeQ and establishes new SOTA with a PMR of 99.60 and Mean of 17.52. On the test split of v1.0, DST consistently outperforms other strong models, such as AMN, with a PMR of 98.10. Compared to the real-image based SL-QI-G on the v0.5 test, DST achieves a PMR of 98.69, demonstrating the inefficiency of image retrieval from larger pools. DST also performs well on the v0.9 validation split with a PMR of 98.02. The trends in PMR are consistent across all datasets, and only DST shows a continuous increase in PMR as the dialogue progresses, highlighting its robustness and effectiveness in different dataset settings."}, {"title": "Ablation Studies", "content": "Ablation studies on the v1.0 validation set (Table 2) were performed to analyze the impact of each module. The full model's performance"}, {"title": "Case Studies", "content": "In Fig.3, a comparison with recent ADQ [8] reveals two key observations. Firstly, our model effectively avoids repetition over the 10 rounds by combining VRDS and STrack modules. This prevents repetitive context and ensures that context vector z(t) remains distinctive and informative, leading to non-repetitive questions generated. Secondly, our model generates a higher number of visually related questions. In the first example, it asks three color-related questions (highlighted in red), while [8] and humans ask questions that are not focused on color. In the last two examples, our model initiates the dialogue with four and five image-related questions, respectively. These findings indicate that mental model of the unseen image enables the generation of image-like representations, prompting QBot to ask visually related questions."}, {"title": "5 Conclusion", "content": "This paper proposes DST, a novel dialogue state tracking approach for visual dialog question generation in the Guess Which game. DST maintains and updates dialogue states, including word and mental image representations, enabling men-"}], "equations": ["\\alpha = softmax(w_aQ),", "q = \\alpha^TQ,", "\\beta = softmax(w_\\beta[r^{(k)}(q); r^{(k)}(q) \\odot V; V]),", "V' = \\beta V,", "M^{(t)}_w = R-SRE(M^{(t)}_w, M^{(t)}_w),", "M^{(t)}_i = R-SRE(M^{(t)}_w, M^{(t)}_i), a^{(t)} = sum(M^{(t)}_i),", "M^{(t)}_i = R-SRE(M^{(t)}_w, M^{(t)}_i), a^{(t)} = sum(M^{(t)}_i),", "z^{(t)} = Dropout(w_v[a^{(t)}; a^{(t)}]),", "h^{(t+1)}_1 = transformer\\_decoder(h^{(t)}, M^{(t)}_w, M^{(t)}_i),", "w^{(t+1)}_1 = argmax\\ softmax(h^{(t+1)}_1w_{der}),", "\\$ = Gumbel(FFN([r^{(k)}(f^{(t)}) \\odot M^{(t)}_w])),", "e^{(t+1)}_{a, w} = FFN(f^{(t)}),", "M^{(t+1)}_w = M^{(t)}_w \\cup {e^{(t+1)}_{a, w}},", "o^{(t)}_{a} = FFN(f^{(t)}),", "M^{(t+1)}_i = M^{(t)}_i \\cup {o^{(t)}},", "\\psi = softmax(FFN([r^{(k)}(f^{(t)}) \\odot M^{(t)}_w])),", "M^{(t+1)}_w = M^{(t)}_w + \\psi FFN_\\varphi(f^{(t)}),", "\\gamma = softmax(FFN([r^{(k)}(f^{(t)}) \\odot M^{(t)}_i])),", "M^{(t+1)}_i = M^{(t)}_i + \\gamma FFN_\\varphi(f^{(t)}),", "L_{CE} = - \\frac{1}{I} \\sum_{t=1}^T \\sum_{j=1}^{l_t}log\\ p_j; L_{MSE} = \\frac{1}{T} \\sum_{t=1}^T ||y^{(t)}_I - y||_2^2,", "L_{PL} = - \\frac{1}{T-1} \\sum_{t=2}^T(\\mathcal{L}_{MSE}^{(t)} - \\mathcal{L}_{MSE}^{(t-1)}),", "\\mathcal{L}_{SL} = \\mathcal{L}_{CE} + \\mathcal{L}_{MSE} + \\mathcal{L}_{PL}."]}