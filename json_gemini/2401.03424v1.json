{"title": "MLCA-AVSR: MULTI-LAYER CROSS ATTENTION FUSION BASED AUDIO-VISUAL SPEECH RECOGNITION", "authors": ["He Wang", "Pengcheng Guo", "Pan Zhou", "Lei Xie"], "abstract": "While automatic speech recognition (ASR) systems degrade significantly in noisy environments, audio-visual speech recognition (AVSR) systems aim to complement the audio stream with noise-invariant visual cues and improve the system's robustness. However, current studies mainly focus on fusing the well-learned modality features, like the output of modality-specific encoders, without considering the contextual relationship during the modality feature learning. In this study, we propose a multi-layer cross-attention fusion based AVSR (MLCA-AVSR) approach that promotes representation learning of each modality by fusing them at different levels of audio/visual encoders. Experimental results on the MISP2022-AVSR Challenge dataset show the efficacy of our proposed system, achieving a concatenated minimum permutation character error rate (cpCER) of 30.57% on the Eval set and yielding up to 3.17% relative improvement compared with our previous system which ranked the second place in the challenge. Following the fusion of multiple systems, our proposed approach surpasses the first-place system, establishing a new SOTA cpCER of 29.13% on this dataset.", "sections": [{"title": "1. INTRODUCTION", "content": "With the rapid advancement of artificial intelligence, automatic speech recognition (ASR) systems have made considerable progress in recognition accuracy and even reached human parity [1]. However, in complex acoustic environments or real-world far-filed scenarios like multi-person meetings, ASR systems can be susceptible to performance degradation due to background noise, inevitable reverberation, and multiple speaker overlap. Conversely, visual-based speech recognition (VSR) systems are immune to acoustic environments or distorted speech signals [2, 3]. As a result, there is growing interest among researchers in incorporating visual features into ASR models to mitigate the impact of damaged speech signals, leading to the emergence of audio-visual speech recognition (AVSR).\nA considerable amount of research on AVSR has shown that integrating visual information into ASR models can enhance the robustness in complex acoustic environments significantly. In [4], Ma et al. proposed an end-to-end dual-encoder hybrid CTC/Attention [5] AVSR system, which includes a ResNet [6] based visual encoder, a Conformer [7] based audio encoder, and a multi-layer perception (MLP) module to fuse different modality features. In contrast to the MLP-based fusion strategy, Sterpu et al. [8] first introduced an attention-based fusion mechanism and found that aligning the features of different modalities can improve the learned representations. Following this, plenty of studies have adopted a cross-attention module to capture inherent alignments and complementary information between fully encoded audio-visual representations [9, 10, 11]. Additionally, some works directly concatenate the raw speech and video sequences together and employ a shared encoder with self-attention mechanisms to learn modality alignments [2, 12]. In [13, 14], hidden features from different layers of audio and visual encoders were leveraged to achieve more effective fusion, indicating that conducting multi-layer fusion can promote the performance of AVSR systems.\nRecently, the Multi-modal Information based Speech Processing (MISP) Challenge series [15, 16, 17] has been introduced to explore the utilization of both audio and visual data in distant multi-microphone signal processing tasks, like keyword spotting and speech recognition. In the audio-visual diarization and recognition (AVDR) track of the MISP2022 Challenge, multi-channel audio data and lip reading video data were suggested to use in developing robust speech recognition systems that retain high accuracy in far-field home TV scenarios. Among the submitted systems, Xu et al. [18] achieved the highest ranking by summing audio and visual features and performing channel-wise attention between the fusion features and multi-channel audio signals. Guo et al. [19] proposed a single-layer cross-attention (SLCA) fusion based AVSR system, which employed a cross-attention module to combine the features of different modalities. However, both approaches conducted modality fusion based on fully encoded audio and visual representations, without considering the fusion during the representation learning phase. Although Li et al. [14] explored the usage of multi-level modal features, the simple concatenation based fusion did not effectively capture the alignments between different modalities.\nIn this paper, we propose a multi-layer cross attention fusion based audio-visual speech recognition (MLCA-AVSR) model to enhance the robustness of the AVSR model. Specifically, our model improves the previous SLCA fusion module and integrates it into multiple intermediate layers of the modality encoders. By fusing at different intermediate layers, our model allows each modality to learn complementary contextual information from the other modality, ranging from low-level to high-level features and from fine-grained details to abstract global patterns. This MLCA module enables a more exhaustive representation learning and fusion process. Furthermore, we also employ the Inter-CTC [20] loss to guide the output feature of each cross-attention module. To the best of our knowledge, this is the first attempt to integrate the cross-attention module into the intermediate layer of modality encoders and simultaneously conduct modality fusion during representation"}, {"title": "2. METHOD", "content": "2.1. Previous System\nFig.1 shows the overall framework of our previous system [19], which mainly consists of four components: audio/visual frontends, audio/visual encoders, an SLCA fusion module, and a decoder. In detail, both audio and visual data are first processed through frontend modules for feature extraction. The visual frontend consists of a 5-layer ResNet3D module implemented with 3D convolutions, while the audio frontend utilizes a structure composed of 2 convolutional down-sampling layers. After the frontend modules, the obtained features are separately passed through two modality-specific Branchformer [21] encoders, which model the input features into well-learned representations. The SLCA fusion module includes two modality-based cross-attention layers. In each layer, one modality's representation is used as the Query vector, while the other modality's representation serves as the Key and Value vectors. Take the cross-attention of the audio branch (\"AV Corss-Attention\" in Fig. 1) as an example, the computation can be formulated as:\n$\\text{Attention}(Q_a, K_v, V_v) = \\text{Softmax}(\\frac{Q_a K_v^T}{\\sqrt{d_k}}) V_v,$\nwhere the subscript a refers to audio representations and v for visual representations, respectively. The $d_k$ represents the attention dimension. With the help of the cross-attention fusion layers, each modality can learn relevant and complementary contextual representation from the other modality. Finally, the features of different modalities are concatenated together to calculate the connectionist temporal classification (CTC) [22] loss and fed into a Transformer [23] decoder to generate predicted tokens and compute the cross-entropy (CE) loss. For our AVSR system, we advance the SLCA module introduced in [19] and integrate it into the intermediate layers of modality encoders to promote representation learning for each modality. In the following section, we will begin with the improved"}, {"title": "2.2. Improved Cross Attention", "content": "As illustrated in Fig. 2, our improved cross-attention module comprises two main parts, which are the audio feature processing flow and the visual feature processing flow. Both parts are composed of a multi-headed self-attention layer [23] and a multi-headed modal-attention layer, with a residual connection applied around each of them. Given the audio features $h_a$ and visual features $h_v$, both of them are first fed into the multi-headed self-attention layer to obtain deeper representations and summed with their respective original input through a residual connection, resulting in intermediate representations $h_a'$ and $h_v'$. Subsequently, two multi-headed modal-attention layers are employed: the audio multi-headed modal-attention (AMMA) in the audio feature processing flow and the visual multi-headed modal-attention (VMMA) in the visual feature processing flow. In the AMMA, the Query $Q_a$ is generated from the audio intermediate representation $h_a'$, while the Key $K_v$ and Value $V_v$ are derived from the video input $h_v'$. Conversely, in the VMMA, the Query $Q_v$ is generated from the intermediate video representation $h_v'$, while the Key $K_a$ and Value $V_a$ are transformed from the audio input $h_a$. The outputs of the AMMA and VMMA are then combined with their respective intermediate representations through residual connections, resulting in the audio flow output $h_a''$ and video flow output $h_v''$. Finally, the output of both flow are added together to obtain the fused audio-visual feature of the cross-attention module, denoted as $h_{av}$. In summary, the computation of the cross-attention module can be formulated as:\n\n$h_a' = h_a + \\text{MHSA}(h_a)$,\n$h_v' = h_v + \\text{MHSA}(h_v)$,\n$h_a \\xrightarrow{linear} Q_a; h_v' \\xrightarrow{linear} K_v, V_v,$\n$h_v \\xrightarrow{linear} Q_v; h_a \\xrightarrow{linear} K_a, V_a,$\n$h_a'' = h_a' + \\text{AMMA}(Q_a, K_v, V_v)$,\n$h_v'' = h_v' + \\text{VMMA}(Q_v, K_a, V_a)$,\n$h_{av} = h_a'' + h_v''$,\n\nwhere MHSA means multi-headed self-attention module and $\\xrightarrow{linear}$ represents linear projection."}, {"title": "2.3. Multi-Layer Cross Attention Fusion", "content": "Fig 3 depicts the structure of our MLCA-AVSR model. It consists of four main components: audio and visual frontends, audio and visual encoders, a fusion module, and a decoder. Similar to our previous system described in Section 2.1, we adopt a 2-layer convolutional down-sampling network as the audio frontend and a ResNet3D network as the video frontend. For the audio and visual encoder, we adopt the recently proposed E-Branchformer architecture [24], which has demonstrated superior performance compared with the Branchformer. The decoder component remains the Transformer and the model is optimized with the joint CTC/Attention [5] training strategy.\nThe MLCA-based fusion module is significantly improved over the SLCA system. It introduces two additional cross-attention modules within the audio and visual encoders, distributed evenly across multiple layers. This allows for a better fusion of different modalities by effectively leveraging the hidden representations in the encoders. We define the number of audio and visual encoder layers as $N_{ea}$ and $N_{ev}$, respectively. The inputs of the audio and visual encoder are denoted as $x_a$ and $x_v$. $h_a^{(i)}$ and $h_v^{(i)}$ represent the output of the i-th layer of the audio and visual encoder, respectively. $h_a^{(i N_{ea}/3)}$, $h_v^{(i N_{ev}/3)}$, and $h_{av}^i$ means the audio feature, video feature, and audio-visual fusion feature outputted by the i-th cross-attention module. The output audio and video features from the cross-attention modules, located within encoders, are then individually used as inputs to the next layer of the audio and visual encoder. The audio-visual fusion features, obtained from each cross-attention module, are added to form the final output $h_{av}$ of the MLCA module.\nThe outputs of cross-attention1 and cross-attention2 are used as intermediate outputs of MLCA-AVSR to calculate the Inter-CTC losses, which guide the cross-attention modules to better utilize the hidden representations of encoders for audio-visual feature fusion."}, {"title": "3. EXPERIMENT", "content": "3.1. Data Processing\nDataset. All model training and testing experiments are conducted on the MISP2022-AVSR dataset [15], a large-scale audio-visual Chinese conversational corpus consisting of 141h audio and video data collected by far/middle/near microphones and far/middle cameras in 34 real-home TV rooms. After segmented by the oracle diarization, it contains 106.09 hours of dialogue audio and video in the training set, 3.09 hours in the development set, and 3.13 hours in the evaluation set.\nAudio Processing. Initially, both the Middle and Far data are pre-processed by the WPE [25] and GSS [26] algorithms to effectively extract enhanced clean signals for each speaker. Subsequently, the combination of the enhanced data and original Near data is speed perturbed with 0.9, 1.0, and 1.1 factors. To simulate realistic acoustic environments, we employ the MUSAN [27] corpus and the open-source pyroomacoustics toolkit\u00b9 to generate authentic background noises and room impulse responses (RIRs). The combined dataset for training comprises approximately 1300 hours, encompassing both the processed and simulated data.\nVideo Processing. For the video data, we crop the ROIs corresponding to the lip of the speaker and resize it to 112 \u00d7 112. During the training process, random rotation, horizontal flipping, and color-jittering strategies are applied to augment the video data.\nSpeaker Diarization. In order to compare the performance of our system with the participants' of MISP2022 in the final Eval set without ground-truth diarization, we also use the speaker diarization (SD) model [28] with a diarization error rate (DER) of 9.43% on the Dev set, same as our previous system, to segment the Dev and Eval set, denoted as Devsd and Evalsd respectively.\n3.2. Setup\nAll of the models, including ASR, VSR, and AVSR, are implemented with the ESPnet toolkit [29]. For the audio-based ASR model, we use a 24-layer E-Branchformer as the encoder, each with 256-dim, 4 attention heads, and 1024-dim feed-forward inner linear projection. Additionally, the decoder contains 6 Transformer layers, each with 4 attention heads and 2048-dim feed-forward. Both the encoder and decoder's dropout rates are set to 0.2. For the video-based VSR model, the visual frontend is a 5-layer ResNet3D module, whose channels are 32, 64, 64, 128, 256, and kernel size is 3, the visual encoder is a 9-layer E-Branchformer, and others are the same as ASR systems. To validate the superior performance of the E-Branchformer encoder, we also trained Conformer and Branchformer ASR and VSR models with similar model sizes to the E-Branchformer. For the audio-visual speech recognition model, the cross-attention module uses 4 attention heads with 256 attention dims. At the beginning of training, the audio and visual encoders are initialized using well-trained ASR and VSR models, respectively.\n3.3. Results and Analysis\n3.3.1. Single-modality ASR and VSR models\nAs shown in Table 1, we conduct performance comparison among Conformer, Branchformer, and E-Branchformer encoders for both"}, {"title": "4. CONCLUSION", "content": "This paper proposes a multi-layer cross-attention fusion based AVSR (MLCA-AVSR) system, building upon our previous single-layer cross-attention fusion based AVSR (SLCA-AVSR) system which ranked second in the MISP2022 challenge. MLCA-based fusion incorporates two cross-attention modules within encoders to fuse audio-visual representations at different levels. On the MISP2022-AVSR dataset, our proposed MLCA-AVSR system outperforms our previous system and surpasses the system ranked first in the challenge after multi-system fusion, achieving a new SOTA result."}]}