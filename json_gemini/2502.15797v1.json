{"title": "OCCULT: Evaluating Large Language Models for Offensive Cyber Operation Capabilities", "authors": ["Michael Kouremetis", "Marissa Dotter", "Alex Byrne", "Dan Martin", "Ethan Michalak", "Gianpaolo Russo", "Michael Threet", "Guido Zarrella"], "abstract": "The prospect of artificial intelligence (AI) competing in the adversarial landscape of cyber security has long been considered one of the most impactful, challenging, and potentially dangerous applications of AI. Here, we demonstrate a new approach to assessing AI's progress towards enabling and scaling real-world offensive cyber operations (OCO) tactics in use by modern threat actors. We detail OCCULT, a lightweight operational evaluation framework that allows cyber security experts to contribute to rigorous and repeatable measurement of the plausible cyber security risks associated with any given large language model (LLM) or AI employed for OCO. We also prototype and evaluate three very different OCO benchmarks for LLMs that demonstrate our approach and serve as examples for building benchmarks under the OCCULT framework. Finally, we provide preliminary evaluation results to demonstrate how this framework allows us to move beyond traditional all-or-nothing tests, such as those crafted from educational exercises like capture-the-flag environments, to contextualize our indicators and warnings in true cyber threat scenarios that present risks to modern infrastructure. We find that there has been significant recent advancement in the risks of AI being used to scale realistic cyber threats. For the first time, we find a model (DeepSeek-R1) is capable of correctly answering over 90% of challenging offensive cyber knowledge tests in our Threat Actor Competency Test for LLMS (TACTL) multiple-choice benchmarks. We also show how Meta's Llama and Mistral's Mixtral model families show marked performance improvements over earlier models against our benchmarks where LLMs act as offensive agents in MITRE's high-fidelity offensive and defensive cyber operations simulation environment, CyberLayer.", "sections": [{"title": "1 Introduction", "content": "The growing capabilities of Large Language Models (LLMs) in synthesizing knowledge, analyzing code, and utilizing software have raised concerns about their potential for misuse in various critical domains when in service to their users. One concern is that LLMs may enable autonomous or AI-assisted offensive cyber operations. Offensive Cyber Operations (OCO) have often historically required highly educated computer operators, multidisciplinary teams, mature targeting and devel- opment processes, and a heavily resourced sponsoring organization to viably execute at scale and to a mission effect. This is due to the nature of OCO, in that it is vast, extremely interdisciplinary, non-static, and often more of an art than a science. Offensive cyber tactics, techniques and proce- dures (TTPs) are often evolving, transitory, and multi-faceted. This operating reality, which had previously restricted the use of AI in OCO to smaller tasks or sub-components, is changing with the advancement of LLM development and the growth of novel LLM/AI-enabled OCO systems. Today, cyber defenders and policy makers are wondering if LLMs are the game changer that will enable autonomous OCO in real-world cyber-attacks.\nTo mitigate the potential risk that autonomous and even semi-autonomous AI-enabled OCO systems could pose, we must be able to evaluate the true capabilities of any emerging OCO AI"}, {"title": "2 Related Work", "content": "To date, the most widely used approach to evaluate LLMs for OCO, cyber-attack, red teaming, or penetration testing capabilities is the use of cyber security Capture-the-Flag (CTF) challenges or tasks. System architectures and integration details vary, but the core approach is to interface the LLM under test (through its prompt API) to CTF challenge environments and ask the LLM to complete the given challenge by interacting (via terminal or shell connection) with the challenge environment. This evaluation approach is highly attractive as CTF challenge corpuses are large and widely available, are highly codified and measurable, and have a (relatively) strong analog to real OCO when compared to other evaluation approaches. PentestGPT [3], CyberSecEval 3 [4], Google DeepMind's LLM evaluations [5], PenHeal [6], AutoAttacker [7], Cybench [8], EnIGMA [9], InterCode-CTF [10], 3CB [11], and others [12] [13] use CTF benchmarks as a means for LLM evaluation. Some efforts, notably CyberSecEval 3 [4], also use CTFs to evaluate OCO operator uplift, i.e., evaluating whether a human operator, when given an LLM as a tool/resource, performs better at the CTF challenge. In our view, and as detailed in later sections, this form of \"copilot\" test is a more appropriate evaluation given an attacker profile/paradigm.\nIt must be noted that not all the challenges from CTFs are universally accepted as accurate comparisons to real-world OCO environments. Some challenges are more realistic than others and thus have more important real-world implications for cyber security. For example, aspects of gamification, discrete state transitions, and scoped boundaries often preclude challenges from being a true analog to real environments. There have been efforts to further reduce this gap in evaluation. In [14] and [15], an explicitly scoped benchmark for Linux privilege escalation (priv-esc) vulnerabilities is established. The benchmark is relatively small (15 tasks), but each priv-esc task is the exact analog of a real-world priv-esc vulnerability. Additionally, the evaluation environment is stripped of any explicit gamification. In CyberSecEval 3 [4], a larger cyber range evaluation is created to allow for a more end-to-end ransomware emulation scenario to play out via the attacking LLM agent. End-to-end cyber-attack evaluation scenarios are generally more difficult and resource intensive, hence their lower occurrence in existing efforts."}, {"title": "2.2 Multiple Choice and Free Response Tests", "content": "In addition to the use of CTFs, one approach frequently seen in the research community is the use of benchmarks consisting of multiple-choice questions. Cyber benchmarking directly taken from general LLM benchmark approaches [16] is attractive. Not only is it measurable, but it also allows for the synthetic generation of question corpuses, providing greater scale. CyberSecEval [17], CyberMetric [18], SecEval [19], SecQA [20], and [13] all use multiple choice question benchmarks. Approaches to question generation and question subject/domains may vary among efforts. For example, CyberSecEval [17] generates questions via templating and combinatorial expansions of pre-made cyber security question fragments and generative LLM augmentation. SecEval [19] uses an LLM with prompts from textbooks, system documentation, technology guidelines, and industrial standards to generate multiple choice questions. However, attention must be given to the actual content and validity of the benchmark questions, whether generated or not. For example, in the CyberMetric [18] CyberMetric-2000 benchmark, a stated cyber security benchmark, there are out-of-scope questions concerning contract law and the motivations of terrorism, as well as an incorrect question/answer pair that assert the \"ideal approach to securing an infrastructure\" is \"developing a plan to address the business needs of the organization\" over the clearly more correct answers of \"implementing a hierarchical strategy to identify assets and threats\" and \"identifying vulnerabilities, threats, and countermeasures.\"\nAlternatively, CyberSecEval [17] uses free response question benchmarks where LLM responses are free form text. For evaluation of this benchmark, CyberSecEval uses an additional LLM (not the one under test) to evaluate whether responses are effectively malicious (i.e., respond effectively to a question prompt for aiding with a malicious cyber-attack request/question). As the authors note, this metric is not perfect and is indicative of the challenge of deterministically evaluating free response benchmarks at scale.\nIn short, there are two major concerns with multiple choice question benchmarks. First, there is the challenge of creating multiple choice question benchmarks that contain effective questions that cannot be memorized or gamified and are related to the actual domain of evaluation. Second, with regards to evaluating LLMs for OCO capabilities, there is the debate on whether multiple choice questions serve as an appropriate proxy or indication of real offensive cyber capabilities. In later sections we detail a novel improvement for OCO multiple choice benchmarks that addresses some of these concerns."}, {"title": "2.3 Vulnerability Identification and Exploitation", "content": "Although vulnerability identification and exploitation are often included within individual chal- lenges in Capture-The-Flag (CTF) competitions, they are increasingly emerging as a distinct benchmark category. The purpose of these evaluations is to assess whether LLMs can demon- strate performance in analyzing static code or running software for vulnerabilities. Some efforts take this evaluation further and when a vulnerability is found, proceed to challenge the LLM to then create exploit implementations for exploiting the vulnerability for effect. CyberSecEval 2 [21] and Project Naptime [22] uses CyberSecEval 2's synthetic code samples, which contain vulnerabili- ties, for evaluating LLMs. Project Naptime [22] augments LLM's under test with additional tooling integrations to include a code browser, Python interpreter, and debugger to evaluate for improved performance against the benchmark. Google DeepMind's frontier model evaluation effort [5] tests for classifying security patches and identifying vulnerable code/functions. eyeballvul [23] consists of a continuously updated benchmark of a large repository of open-source software versions and known vulnerabilities, which can be used to evaluate LLM's against. The scalability and breadth of these approaches is promising.\nAlternatively to static code analysis benchmarks, LLM Agents can Autonomously Exploit One- day Vulnerabilities [24] and LLM Agents can Autonomously Hack Websites [25] emulate vulnerable websites and applications for targeting by LLMs under test. These benchmarks are smaller, but the test cases are live systems and applications. While not a benchmark or evaluation suite, Vulnhuntr [26] is an LLM-enabled system that analyzes Python code (and function tracing) for vulnerabilities in open-source code bases; to date, it has found dozens of exploitable vulnerabilities. Lastly, there is the ongoing DARPA Artificial Intelligence Cyber Challenge (AIxCC) [27] competition to create LLM-enabled systems for finding and patching vulnerable code at scale. To our knowledge, the evaluation approaches that the performers (i.e., the project teams that execute the research and development) are using to test their systems have not been made public as the competition is still ongoing at time of publication.\nThe primary challenges to vulnerability identification and exploit benchmarks are determining whether the LLM under test is memorizing versus reasoning and generalizing when they do actually find vulnerabilities. The latter indicates a much greater risk."}, {"title": "2.4 LLM/AI Systems", "content": "By necessity, each of the aforementioned evaluation efforts have developed an LLM-enabled system (or as more generally referred to, an AI system) to actuate and evaluate the model under test. Some of these systems are very lightweight, designed to merely support the action and observation loop between the LLM agent and the evaluation: examples include Cybench[8] and InterCode-CTF [10]. Other LLM systems maintain a moderate set of scaffolding and integrated functionality, to include Vulnhuntr [26] and AutoAttacker [7]. And, in line with a natural progression, are heavy weight LLM systems that may have extensive technology or tool interfaces, i.e., Agent Computer Interfaces (ACIs) [28]; multiple LLMs/models for different functionality purposes; larger sub-components for observation parsing & summarizing, reasoning/planning over action selection and sequences; and ad-hoc human feedback. PentestGPT [3], Project Naptime [22], EnIGMA [9], and most recently the multi-stage attack LLM interface, Incalmo [29], are examples of these growing and larger LLM agent systems. Furthermore, Project Naptime [22] and EnIGMA [9] are examples of what will become the paradigm in LLM systems: having standard, effective, and tight integrations with programming interpreters, debuggers, code analyzers, web APIs and task management."}, {"title": "2.5 OCCULT Goals", "content": "In the context of existing OCO LLM evaluation efforts, the goal of this work is to:\n\u2022 Inform on and unify the methodology for effective and scalable OCO LLM testing, providing hard evidentiary results that ultimately lead to clear, open-source risk implications.\n\u2022 Evolve testing towards breadth of coverage on OCO capability areas that are most concerning to the community and/or represent large remaining gaps in coverage.\n\u2022 Improve standardization, shareability, and tooling of LLM OCO evaluation benchmarks.\nIn our view, without meaningful progress on these points, OCO LLM evaluations will remain fragmented, non-comprehensive, and incapable of keeping pace with the exponential creation and proliferation of LLMs. The OCCULT methodology aims to provide a more realistic and stan- dardized testing, which allows for better comparisons across models, training datasets, and user approaches. Our work also strives to assess how LLMs compare not just to each other, but to the humans that currently perform in the roles those models aim to replace."}, {"title": "3 LLM Evaluation Methodology", "content": "This section is broken down into two parts. First, we detail the core tenets of our evaluation philosophy, which drove the design of the evaluation methodology. Then we define our evaluation methodology."}, {"title": "3.1 Evaluation Philosophy", "content": "Given our team's analysis of the moving state-of-the-art and near-peer research efforts with respect to evaluating LLM's for OCO capabilities, we have formed a set of tenets that we believe must drive the development of any test or framework for serious and useful LLM evaluation:\nAny test for determining the presence of a given OCO capability in an LLM must be for a true OCO capability and apply a real use case of the LLM. Our team found a proliferation of \"LLM cyber tests\" that have only a vague declaration of the OCO capability under test and/or an unfounded LLM use case offensive cyber scenario. Our team perceives that such tests are simply easier to implement and avoid niche, and often sensitive, OCO use cases. However, there is a distinct difference between standard cyber defense practices from that of OCO practices. A strong example of a well-scoped benchmark for a real-world OCO capability area is [14], [15] where the benchmark is solely for Linux privilege escalation exploits.\nTenet 2: Tests that lend themselves to more dynamic problem sets, less memo-"}, {"title": "3.2 Methodology", "content": "Our high-level evaluation methodology is aimed at driving and encompassing OCO evaluation tests for LLMs. In our methodology, all tests fall along three dimensions that clarify their target purpose and associated performance implications. The dimensions are OCO Capability Areas, LLM Use Cases and Reasoning Power. The conceptual view of the OCCULT LLM Evaluation Methodology can be seen in Figure 1."}, {"title": "3.2.1 \u041e\u0421\u041e Capability Areas", "content": "The first dimension of the OCCULT evaluation methodology is the OCO capability area (i.e. skill or competency) of any given test. Simply put, any test for capabilities found in an LLM must be specific as to what exact OCO capabilities are being solicited and evaluated for. When creating a test for an OCO capability, it should: (1) explicitly identify the capability of OCO being evaluated, (2) ensure that the OCO capability under test is an actual, real-world OCO capability/skill area, and (3) map/catalogue tests to cyber security frameworks, where possible.\nCyber security, and specifically the domain of OCO, has grown into a vast discipline, constitut- ing dozens of different disciplines and sub-disciplines. The ATT&CK\u00ae [31] knowledge base, while not designed to be a 100% match for all OCO disciplines and sub-disciplines as it is oriented to defensive operators, provides for a large percentage of coverage and currently maintains 14 offen- sive cyber tactics and over 200 offensive cyber techniques. Thus, any OCO test for an LLM should detail and scope to the specific OCO capability under test. Furthermore, the OCO capability the test is evaluating should be a real OCO capability area, or a close proxy. Often, the TTPs of OCO, cyber red/blue/purple teaming, penetration testing, security control validation, and compliance checking are all conflated and taken to be interchangeable. While there is overlap among these"}, {"title": "3.2.2 LLM Use Cases", "content": "As described in Tenet 1, any OCO evaluation for an LLM should test the LLM in a real-world use case, that is, in a manner that an LLM could be used to enable, assist, or execute an offensive cyber operation. To that effect, our team currently tracks three distinct use cases for LLMs in OCO: Knowledge Assistant, Co-orchestration, and Autonomous. Our team lists these use cases with the caveats that they are not mutually exclusive (i.e., an LLM can serve in multiple use cases in the same scenario) and that the use cases will continue to evolve.\nIn the first OCO use case we define, the LLM serves as an OCO knowledge assistant, a support role assisting the human operator with researching, planning, and/or executing an offensive cyber operation or attack. In this use case, the LLM is not directly performing the actions or integrated into the execution of the OCO \u2014 it is solely interfacing with the human operator while the operator executes the OCO. This use case is already quite common and one of the primary scenarios LLMs are evaluated for, as seen in CyberSecEval 3 [4].\nIn the next use case, the LLM serves as a peer co-agent in an OCO. In this use case, the LLM is paired or integrated with one or more additional co-agents that together carry out researching, planning, and/or executing an offensive cyber operation. By agent (or co-agent), we mean a system, tool/platform or human that makes operational decisions or executes the actions of the OCO. This use case may at first appear theoretical; however, it directly"}, {"title": "3.2.3 A Preface on Reasoning", "content": "\"Reasoning\" is an overloaded term, particularly with respect to LLMs. Its use spans a variety of well-defined but not always commensurate, areas of the literature. Moreover, discourse around LLMs has proliferated outside of published research channels and grown to include more amor- phous, difficult to pin down concepts and dialogues.\nWhat does \"reasoning\" refer to? Formally, reasoning can imply mathematical reasoning, se- mantic or first-order logic, or classical planning (discrete search). Informal definitions tend towards vague notions like common sense or basic deductive inferencing. Classical planning, generally pre- senting as discrete search over domain-model search spaces, is probably the closest type of reasoning in prior literature that would equate to OCCULT\u201cOCO reasoning.\" However, discrete search ul- timately doesn't capture the full suite of behaviors that relate to the reasoning picture as relevant for understanding cyber reasoning capacity. Here, we provide a measurable, working definition of reasoning within the OCO domain. The OCCULT objective is ultimately about understanding the cyber operation capacity of an AI system, and quantifying performance in these dimensions of cyber reasoning can provide insight into that.\nIn the remainder of this section our team proposes a work-in-progress theory to quantify broad levels of reasoning power as applied to the OCO domain. The theory is first drawn from a concept map where concepts of OCO reasoning are identified and then refined into four major categories. From those categories, general definitions are given, broad evaluation criteria are extracted, and OCO specific examples are provided."}, {"title": "3.2.4 Reasoning Power", "content": "To arrive at a workable evaluation model for reasoning power, our team first generated an OCO Reasoning Concept Map shown in Appendix A.1 that enumerates the indivisible concepts of what, in our view, constitutes reasoning power of an AI system, when applied against an OCO envi- ronment or task. In the OCO Reasoning Concept Map, these concepts are the non-emphasized (i.e. white) boxes. One can see that these OCO reasoning concepts align with what is usually, when discussed in the context of human intelligence, more formally defined as problem-solving, knowledge and intelligence. Succinctly, we define these as follows:\n\u2022 the process of systematically searching and evaluating sequences of discrete actions to obtain a desired environmental state or objective. [35]\n\u2022 facts, information, relationships and ontologies of quantifiable domains.\n\u2022 \u201c (of a system) is a measure of its skill-acquisition efficiency over a scope of tasks, with respect to priors, experience, and generalization difficulty.\u201d [36]\nWith regards to the OCO Reasoning Concept Map, we emphasize that OCO reasoning concepts have notable overlap and can rarely be expressed purely as either problem-solving, knowledge or intelligence. Given the many OCO reasoning concepts, we further categorize the concepts into four primary reasoning components. The following subsections detail these OCO reasoning components with explicit definitions and OCO domain examples for what constitutes cases of strong and weak evidence for each. The four primary reasoning components are:"}, {"title": "4 LLM Tests", "content": "The Threat Actor Competency Test for LLMS (TACTL) is designed to evaluate the O\u0421\u041e \u0441\u0430- pabilities of LLMs through a series of multiple-choice questions targeting OCO action creation, modification and iteration detailed in Table III and Perception of OCO environment and task space detailed in Table I. Each question in the TACTL corpus presents four possible answers, challenging the LLM to select the most appropriate response based on its understanding of OCO.\nThe use of the multiple-choice format for evaluation is a compromise. As noted in earlier sec- tions, our team does not view multiple-choice benchmarks as being able to provide sole indication of real world OCO capability, as real-world OCO does not present itself in a coherent chain of discrete multiple-choice questions. However, multiple choice benchmarks do allow us to test the LLM for specific OCO knowledge, explicitly and efficiently. Thus, while not a panacea, multiple- choice benchmarks do provide some advantages, to include: allowing for fast evaluation as well as evaluation in local/resource restricted testing environments; indicating potential OCO capability that should be further investigated via other evaluation methods; and due to the standardization of multiple-choice benchmarks in LLM testing, currently providing for the greatest level of inter- pretability by consumers. Additionally, TACTL does provide for some enhancements (discussed in Dynamic generation of question variable 4.1.1.2) over current cyber security multiple-choice benchmarks, that does improve the quality of evaluation.\nA chain of multiple-choice questions is used rather than allowing open-ended LLM responses"}, {"title": "4.2 BloodHound Equivalency", "content": "The BloodHound Equivalency Test for LLMs is designed to evaluate the OCO capabilities of LLMs through analyzing and producing Active Directory data. The test is related to the specific OC- CULT competencies of Planning detailed in Table I through finding attack paths and Perception detailed in Table II through a general understanding and ability to ingest Active Directory data. BloodHound [43] is an offensive security tool that uses graph-solving algorithms to reveal connec- tions among Active Directory objects. The connections among objects represent Active Directory privileges or attributes. An example of a possible connection is the \"GenericAll\" privilege allocated for a domain group on a user account. In some cases, the connections may be unintended and are cumbersome to find without BloodHound. BloodHound is supported by two popular data collec- tors, SharpHound [44] and Bloodhound.py [45], which gather an environment's Active Directory data. Once the data is gathered and ingested into BloodHound, Active Directory attack paths are highlighted through identified edges between nodes. For the LLM to produce Active Directory intelligence like BloodHound, the LLM must develop, at the time of testing, a working model of the active directory environment and its underlying relationships. Furthermore, for the LLM to"}, {"title": "4.3 CyberLayer Cyber Attack Simulations", "content": "CyberLayer is a high-fidelity cyber operation simulation following the AI Gym [49] design pattern. The simulation provides a robust action space for both offensive and defensive players as well as accurate representations of network topologies, device deployments, resources structures (e.g., domains and users), and network access controls, etc.\nThe high fidelity and accurate representation of the cyber domain in a simulation has proven useful not only in machine learning contexts, such as reinforcement learning (e.g., Mirage [50]), but also as a training and evaluation environment for human players due to the capacity for the environment to exercise the decision-making process in cyber operation scenarios. The most important elements of this simulation framework that empower OCCULT tests are (1) the ability to portray OCO scenarios that can exercise intended decision-making elements and (2) extrapolating those seed scenarios to many scenario variants that, while from a decision-making process are the same, present various identifiers, topological differences, etc., which are sufficiently varied that memorization will not be effective. Thus, these scenarios can be created to study the reasoning and decision-making capacity of an LLM and be resistant to the risk of memorization. At the same time, they can probe for specific behaviors and capabilities. To portray scenarios, CyberLayer represents cyber operation environments with:"}, {"title": "4.3.2 Setting Up a Scenario", "content": "The setup of a CyberLayer scenario begins with the design of the terrain and the elements within it. At the highest level, network segments are modeled - e.g., a broad \"Internet\" segment composing everything outside an enterprise, and various segments representing logical or topological separation of intranets within an enterprise boundary. Then, each host within the environment is modeled off templates, such as an \"Enterprise Laptop\" host device that has configurations for operating systems, applications, protocols, and open ports. IP ranges of devices are configured correctly to match configurations on the network segments they reside upon. Firewall rules are configurable at both the network segment and the device level, including directionality and explicitly allowed or disallowed connectivity. In a scenario configuration file, configuration of network elements such as IP blocks, domain names, and domain users and groups are also available. From this same scenario configuration file, procedural generators build out the simulation data model using the specified networks, devices, and other elements. Scenario generation can use different random seed values specified in the scenario configuration file to introduce variability into the generated environments. Alternatively, an environment using the same random seed value from a prior scenario generation can be identically reproduced."}, {"title": "4.3.3 Design", "content": "The CyberLayer test suite for LLMs is designed to evaluate the OCO capabilities of a LLM when faced with a variety of new environments and constraints. It tests the LLM on each OCCULT competency and particularly focuses on OCO task and solution generalization Table IV. Measuring generalization is important given how sensitive LLMs can be to their input. It is often unclear if some slight variation of the scenario would dramatically increase or decrease overall performance, and thus it's critical to account for scenario variation. Often, due to the way LLMs train over massive corpora of publicly scraped internet data, benchmark datasets can leak into the training data, inadvertently skewing the apparent performance of an LLM on a reasoning type task. When dropped into a terminal, an LLM can sometimes show strong performance on penetration testing tasks [25]. However, if it is found that write-ups or solutions for the tasks are available on the public internet, this can effectively contaminate the training data and invalidate any observed performance. While the specific scenarios might ostensibly be new, many can be variations of those already publicly available. Single instances can be good signals of capability but lack the reliability to prove that the LLM understands the scenario and solution.\nMoreover, the emulation testing already done in the literature shows a pattern of the LLM's performance rapidly falling off after a certain level of difficulty (e.g. PentestGPT [3]). While newer and larger LLMs appear to have a different fall off point, once the cliff is found, the LLM appears to default to ineffective strategies (e.g. AutoAttacker [7]), particularly when actions must be chained together, which is required in any operation. Understanding why this occurs is paramount. CyberLayer's generative features help us to test each aspect of an LLM's OCO capabilities in a fine- grained way. By changing the random seed value in an environment, we can maintain the broad strokes of the network topology, host configurations, and high-level solution while still altering topology. This lets us test if an LLM \"got lucky\" or if it can truly adapt to a dynamic environment and maintain effectiveness. The inclusion of an extra host in a scenario subnet should have little to no effect whatsoever on a LLMs' operation. If we are to say the LLM is truly effective it must be able to accomplish the same goal even with a slight barrier being added."}, {"title": "7 Conclusion and Future Work", "content": "In this work, we presented OCCULT, a methodology and framework for designing, building, and evaluating offensive cyber operation (OCO) test cases for LLMs. We first detailed our evalua- tion philosophy in terms of a core set of tenets describing what makes for effective and useful LLM evaluations. These tenets address the current gaps we see in the literature and open-source work regarding the evaluation of LLMs for offensive cyber capabilities, which requires measuring performance across different cyber tools, environments, and use-cases to encompass the breadth and depth of the offensive cyber landscape. We then outlined a corresponding methodology for how to design OCO test cases along the three measurable dimensions of OCO capability areas, an LLM's reasoning power, and its use-case to the operation/operator. We argue that well-made OCO test cases for LLMs must follow this methodology (or a similar approach) or else risk having only negligible utility when providing actionable results and risk assessments to cyber defenders.\nFurthermore, we implemented three different test cases that were all designed via the OC- CULT methodology and fall under the OCCULT framework. These tests were TACTL, Blood- hound Equivalency, and CyberLayer cyber-attack simulations. In depth, we detailed the design, implementation, and evaluation mechanics of all three tests. Our purpose is to demonstrate the complexities and nuances of each test so the community can gain an appreciation of the OCO landscape and the fidelity with which future tests and metrics must be designed to quantify the real risks that LLMs pose as autonomous OCO enablers.\nTo demonstrate how the OCCULT framework is actualized, we presented the OCCULT LLM Evaluation/Leaderboard platform. As demonstrated, the prototype platform, while still a beta, is already functional and extendable given its streamlined architecture and use of many exist- ing LLM prompting and integration APIs. While benchmarks and frameworks exist within the LLM/AI community and tests will naturally differ in reported metrics, a unified and transpar- ent methodology like the OCCULT framework will allow for more realistic, and therefore more impactful, assessment of this emerging technology.\nFinally, we provided a small set of preliminary evaluation results for each of these tests against a cadre of LLMs that were readily available to us when prototyping OCCULT. We emphasize that these preliminary results are too small to draw more significant conclusions, albeit they did provide for interesting insights into the mechanics of the test cases. See Future Work section for notes on forthcoming rounds of major LLM testing."}]}