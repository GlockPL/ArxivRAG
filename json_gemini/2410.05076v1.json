{"title": "TIDALDECODE: FAST AND ACCURATE LLM DECODING WITH POSITION PERSISTENT SPARSE ATTENTION", "authors": ["Lijie Yang", "Zhihao Zhang", "Zhuofu Chen", "Zikun Li", "Zhihao Jia"], "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1\u00d7\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have revolutionized natural language processing (NLP) by achieving state-of-the-art performance on various applications. As LLMs evolve, they are increasingly being adapted to manage tasks with long contexts, such as Chain-of-Thought reasoning (Wei et al., 2023), document summarization (Huang et al., 2021), and retrieval-augmented generation (Ram et al., 2023; Zhang et al., 2024b). However, quickly and efficiently serving long-context LLMs is challenging due to the inherent memory and compute bottlenecks in the Transformer architectures (Vaswani et al., 2023).\nLLM inference involves two separate stages: prefilling and decoding. The prefilling stage computes the activations for all input tokens and stores the keys and values for all tokens in the key-value (KV) cache, allowing the LLM to reuse these keys and values to compute attention for future tokens. In each decoding stage, the LLM decodes one new token using all input tokens and previously generated tokens. The KV cache size grows linearly in the sequence length (Kwon et al., 2023). For instance, with a context length of 128K tokens, the KV cache of LLama2-7B with half-precision can easily reach 64 GB\u00b2, creating substantial memory pressure for LLM serving. In addition, the LLM decoding stage is memory-bounded since decoding one new token requires accessing all previous tokens in the KV cache, making KV cache access the primary bottleneck for long-context LLM decoding. This memory-bound nature severely limits the scalability and efficiency of LLM serving.\nTo address this problem, recent work has introduced sparse attention, which approximates full attention using a small portion of tokens with the highest attention scores. Compared to full attention, sparse attention reduces computation cost and memory access while preserving the LLM's generative performance (Ge et al., 2024; Zhang et al., 2023). Existing sparse attention techniques can be classified into two categories: eviction- and selection-based methods.\nFirst, eviction-based sparse attention reduces memory usage for the KV cache by selectively discarding less relevant tokens from the KV cache, therefore reducing the number of tokens computed in attention mechanisms (Xiao et al., 2023; Zhang et al., 2023). While these methods decrease the size of the KV cache, they can be inadequate for tasks where critical information is carried by tokens that are prematurely evicted, such as the needle-in-the-haystack tasks (Peng et al., 2023). On the other hand, selection-based sparse attention maintains all tokens in the KV cache, estimates their attention scores, and selects a small subset of tokens to participate in each LLM decoding step. This approach is prone to issues related to distribution shifts caused by appending sparsely attended, biased KV representations back into the cache.\nThis paper presents TidalDecode, an algorithm and system for fast and precise LLM decoding, utilizing position persistent sparse attention (PPSA). A key insight behind TidalDecode is the observation"}, {"title": "2 RELATED WORK", "content": "Long-context model. Efficiently handling long-context inputs is essential for various LLM tasks in real-world applications such as document summarization, question answering, and dialogue systems (Wang et al., 2024). Recent advancements, including rotary positional encoding (ROPE) (Su et al., 2023), have enabled models to manage extended context lengths effectively. The LLaMA-3 model series supports up to 8K tokens, with enhanced versions such as Gradient-AI-Llama3 (AI, 2024a) and LLaMA 3.1 (AI, 2024b) extending this limit to 128K tokens. Additionally, proprietary LLMs such as GPT-4 Turbo and GPT-4o (OpenAI, 2024) support up to 128K tokens, and Claude 3.5 Sonnet allows up to 200K tokens (Anthropic, 2024). While recent work has introduced efficient attention kernel implementation (Dao et al., 2022; Dao, 2023), processing long-context inputs continues to be constrained by significant memory usage and computational costs from the extended KV cache. TidalDecode is designed to mitigate these challenges by reducing latency and memory overhead through an efficient strategy for selecting tokens with the highest attention scores and one-time intermediate re-calibration, ensuring both efficiency and high-quality output.\nTo alleviate the intrinsic computational and memory bottleneck in long-context LLM inference, recent works on sparse attention have approached this problem from two main perspectives: eviction- and selection-based methods."}, {"title": "3 METHODOLOGY", "content": "This section introduces TidalDecode, an efficient algorithm and system for fast LLM decoding using position persistent sparse attention and KV cache correction. TidalDecode uses the same prefilling mechanism as existing systems and performs full attention to compute the key-value (KV) cache for all prompt tokens. In each decoding step, TidalDecode uses three types of attention layers: full attention, full attention with token selection, and position persistent sparse attention. First, TidalDecode performs full attention for the initial Transformer layers to avoid early performance degradation as identified by prior work (Tang et al., 2024). Second, the layer immediately after full attention and a single middle layer (e.g., layer 2 and 13 in Figure 2) perform full attention with token selection, where TidalDecode stores the inner product between the current query and key vectors of all tokens in KV cache during full attention and then selects k tokens contributing to the highest attention scores. Third, all other layers perform position persistent sparse attention, where only tokens selected from the previous token selection layer are loaded from the KV cache to perform attention computation."}, {"title": "3.1 POSITION PERSISTENT SPARSE ATTENTION (PPSA)", "content": "Attention mechanisms have been widely used in today's LLMs. For each attention head, the output is computed via scaled multiplicative formulation as follows.\n$A_i = Q_iK_i/\\sqrt{d}, H_i = softmax(A_i) V_i$ \nwhere $Q_i$, $K_i$, and $V_i$ are the query, key, and value tensors for the i-th attention head. $A_i$ is a matrix representing the attention scores between tokens, and $H_i$ is the output of the i-th attention head. Instead of attending to all input tokens, existing sparse attention methods approximate attention computation by attending the query $Q_i$ to a subset of previous tokens the highest attention scores. Prior work generally performs token selection for individual attention heads and Transformer layers, introducing high runtime overhead. For example, selecting the tokens with highest attention scores using top-k can take longer than computing full attention (see Figure 7), thus diminishing the benefits of performing sparse attention.\nThe key insight behind TidalDecode's position persistent sparse attention is an observation that tokens with highest attention scores for consecutive Transformer layers highly overlap. We use the LLaMA-3-8B model and the needle-in-the-haystack test on PG-19-mini dataset with a context length of 100K tokens to quantify this observation. We randomly select 100 requests from the dataset, compute full attention, and analyze the top 256 tokens with the highest attention scores for each Transformer layer."}, {"title": "3.2 KV CACHE CORRECTION", "content": "For tokens decoded by sparse attention methods, their key/value representations can deviate from the original representation of full attention decoded ones, which we refer to as polluted tokens. The problem can be further exacerbated as their KV pairs are added to the KV cache, resulting in the error accumulation or distribution shift of the KV cache. This can lead to model performance drop in scenarios where the generation length is fairly long. To this end, TidalDecode uses a cache-correction mechanism as shown in Figure 4 to periodically correct the polluted tokens in the KV cache. For every T decoding step performed by TidalDecode, there will be a cache correction step through a prefill over all polluted tokens to update their KV representations in the cache. The choice of T can be at the level of thousands of decoding steps but also depend on different models and tasks. Notice that the cache correction step can be performed concurrently with the sparse decoding step. Nevertheless,"}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to assess both the performance and efficiency of TidalDecode. Our evaluations are performed on widely used open-source models, including Llama-2-7B Touvron et al. (2023) and Llama-3-8/70B. Both models are pretrained decoder-only transformers, exhibiting similar yet distinct architectural features. For instance, Llama 3-8B incorporates group query attention (GQA), a feature not present in Llama 2-7B. In Section 4.2, we evaluate TidalDecode's performance on various tasks, including needle-in-the-haystack, language modeling on PG-19, and LongBench. In Section 4.3, we write customized attention kernels and compare TidalDecode's kernel efficiency against existing state-of-the-art sparse attention methods. Finally, in Section 4.4, we conclude our evaluations with a detailed sensitivity analysis on the choice of different token selection layers. We use TD+LX to denote TidalDecode with layer X selected as the token re-selection layer throughout this section."}, {"title": "4.2 PERFORMANCE EVALUATION", "content": "To evaluate the effectiveness of TidalDecode, we conduct two key downstream NLP experiments: the needle-in-the-haystack test and perplexity evaluation on the PG-19 dataset (Rae et al., 2019). These tasks provide robust benchmarks for measuring both sparse attention models' ability to retrieve critical information in challenging scenarios and their performance on long-context language modeling tasks."}, {"title": "4.2.1 NEEDLE-IN-THE-HAYSTACK", "content": "The Needle-in-the-Haystack test assesses LLMs' ability to handle long-dependency tasks, which is particularly critical for sparse attention algorithms. Eviction-based methods Xiao et al. (2023); Zhang et al. (2023) may discard essential tokens, while selection-based approaches often fail to consistently identify the ground-truth tokens with the highest attention scores in long contexts. Since"}, {"title": "4.2.2 LANGUAGE MODELING", "content": "Perplexity measures the negative likelihood of how well a model predicts the next word in a sequence, with lower values indicating better performance. We evaluate TidalDecode on Llama-3-8B-Instruct-Gradient-1048k with the PG-19 dataset, which includes up to 100 books, providing a comprehensive long-context benchmark.\nAs shown in Figure 5, TidalDecode+L9/13/15 consistently achieves lower perplexity than Quest across all token budget options (2048, 4096). This indicates that TidalDecode's position persistent sparse attention mechanism can effectively retain critical information without significantly sacrificing model accuracy, even as the sequence length grows, demonstrating its robustness for long-context inputs."}, {"title": "4.2.3 LONGBENCH EXPERIMENT", "content": "We also evaluate TidalDecode on LongBench, a benchmark designed to test LLMs on long-context tasks across diverse NLP domains (Bai et al., 2023). We focus on eight tasks: MultiFieldQA (MFQA), NarrativeQA (NrtQA), Qasper (Qasp), 2WikiMQA (2Wiki), HotpotQA (HotQA), QMSum (QMSm), TriviaQA (TrQA), and Passage Retrieval (PRe), which collectively composite a comprehensive evaluation benchmark in single/multi-document QA, summarization, and retrieval.\nWe evaluate all methods with LLaMA-3-8B-Instruct-Gradient-1048k. TidalDecode is compared against full-weight attention and Quest at token budgets of 1024 and 4096. As shown in Table 3, TidalDecode consistently outperforms Quest on all tasks at K = 4096 and on five tasks at K = 1024. Surprisingly, TidalDecode, in most cases, matches or exceeds full attention baseline with notable sparsity: 14% on NrtQA, 50% on MFQA, 80% on Qasp, 50% on 2WikiMQA, 32% on HotQA, 29% on QMSm, 35% on TrQA, and 33% on PRe. We hypothesize this is because TidalDecode's token selection process can filter out irrelevant information, thus leading to higher performance.\nThese results demonstrate TidalDecode's generic ability to select tokens with the highest attention scores, achieving competitive or superior performance while significantly reducing token usage, making it ideal for long-context scenarios."}, {"title": "4.3 EFFICIENCY EVALUATION", "content": "To show the efficiency of TidalDecode, we write customized kernels for our approach and measure the end-to-end decoding latency. We conduct evaluation under the configuration of Llama-2-7B on one Nvidia A100 (80 GB HBM, SXM4) with CUDA 12.2. We compare TidalDecode with state-of-the-art full attention serving library FlashInfer (Ye et al., 2024) and also the Quest implementation. As shown in Figure 6, we can observe that TidalDecode can consistently outperform full attention baseline and Quest by a large margin under all token budgets and context lengths. TidalDecode achieves this through token pattern reuse to minimize the token selection overhead. Notice that the latest LLaMA-3 model shares the same architecture as LLaMA-2, except it uses Group-Query-Attention instead of Multi-Head-Attention. However, this does not affect the relative efficiency comparison against Quest and full attention."}, {"title": "4.4 SENSITIVITY ANALYSIS ON TOKEN RE-SELECTION LAYER", "content": "In this section, we conduct sensitivity studies for different choices of the token re-selection layer. As TidalDecode only has one token re-selection layer in the middle, it is critical to choose the best-performed one. As shown in Figure 9, we have two interesting findings: (1). Different choices of token re-selection layers can significantly affect the accuracy of the results (2). For models within the same model family, the optimal token re-selection layer is consistent over different tasks. In our setup, the optimal token re-selection layer for the LLaMA-2-7B model is layer 7, while for the LLaMA-3-8B/LLaMA-3.1-8B model is layer 13. A concurrent KV cache compression work also identifies that layer 13 is surprisingly important for their approach as well (Shi et al., 2024). For a more detailed sensitivity results on the choice of different token re-selection layers, please refer to the appendix for more results."}, {"title": "5 CONCLUSION", "content": "To conclude, we introduce TidalDecode, an efficient LLM decoding framework with sparse attention. On observing the correlation of the pattern of tokens with the highest attention scores across different consecutive layers, TidalDecode proposes only to select tokens twice: once at the beginning layers and once in the middle layer to serve as a token re-selection layer. We find that using two token selection layers is necessary and sufficient to achieve high-generation quality. Additionally, by reusing the token patterns throughout the sparse attention layer, TidalDecode greatly reduces the token selection overhead, resulting in a significant end-to-end speed-up ratio against existing methods. More interestingly, the optimal choice of the token re-selection layer is consistent across different tasks if the model is in the same model family."}]}