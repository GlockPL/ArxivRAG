{"title": "HARIVO: Harnessing Text-to-Image Models for Video Generation", "authors": ["Mingi Kwon", "Seoung Wug Oh", "Yang Zhou", "Difan Liu", "Joon-Young Lee", "Haoran Cai", "Baqiao Liu", "Feng Liu", "Youngjung Uh"], "abstract": "We present a method to create diffusion-based video models from pretrained Text-to-Image (T2I) models. Recently, AnimateDiff proposed freezing the T2I model while only training temporal layers. We advance this method by proposing a unique architecture, incorporating a mapping network and frame-wise tokens, tailored for video generation while maintaining the diversity and creativity of the original T21 model. Key innovations include novel loss functions for temporal smoothness and a mitigating gradient sampling technique, ensuring realistic and temporally consistent video generation despite limited public video data. We have successfully integrated video-specific inductive biases into the architecture and loss functions. Our method, built on the frozen StableDiffusion model, simplifies training processes and allows for seamless integration with off-the-shelf models like ControlNet and DreamBooth.", "sections": [{"title": "1 Introduction", "content": "Recently, diffusion-based Text-to-Image (T2I) models have shown unprecedented performance [2, 16, 29, 32]. Upon this success, there are many attempts in the community to design diffusion-based Text-to-Video (T2V) models. A common approach is to introduce additional 3D convolution and temporal attention blocks to the T2I model and fine-tune the whole network on videos in the same way as for T2I models on images [8, 10, 14, 17, 28, 38, 44]. However, these whole-training T2V models have several disadvantages. First, training the entire model requires a large amount of data. Therefore, most video models have used additional in-house datasets. Second, the models diminish the variety of styles due to the highly realistic nature of training videos. To prevent this, a joint-training approach with images and videos is being used, however, we believe this does not tackle the root of the problem effectively. Additionally, most models consist of multiple stages, e.g., keyframe generation and spatial/temporal interpolation, and have the disadvantage of struggling to maintain temporal consistency due to sparse keyframe generation.\nTraining only the temporal components while keeping the T2I model parameters frozen is an alternative that avoids the aforementioned problems and reduces training complexity [4, 12]. By freezing the T2I model, we can anticipate diverse outcomes based on the knowledge trained from images, and using existing methods from image models. However, unfortunately, training only the temporal components is not straightforward and often does not converge easily. In response, previous studies have adopted the following methods.\nVideoLDM[4] continued to adopt multi-stage training with spatial/temporal interpolation and upsampling, which could not reduce the training complexity. Furthermore, instead of directly using the image model, adapting the masked blend T2I feature strategy makes it difficult to combine with existing T2I methods. AnimateDiff [12] mainly generates personalized videos assisted by Dreambooth [33] and LoRA [18], which facilitate the creation of videos by generating similar images; but struggles in generating diverse videos.\nIn this paper, we demonstrate that when freezing the image model and only training the temporal layers, we can employ additional inductive biases that we know about videos. We propose a collection of novel designs, including a novel architecture, training losses, and a sampling technique that enable us to fully leverage the power of a frozen T2I model. It generates videos of high quality and diverse styles, similar to the image model, and maintains temporal consistency as shown in Figure 1 and 2.\nOur approach offers several advantages. Firstly, it supports the combination of personalized T2I models such as DreamBooth, LoRA, ControlNet [43], and IP-Adapter[41] without requiring further training to produce personalized videos. This goes beyond merely using off-the-shelf models to show that one can design methods and train models using only images to control videos. Secondly, our model shows temporally consistent video generation although it is trained on a public dataset (WebVid-10M), whereas many existing works are trained on in-house datasets. Thirdly, we introduce various independent methods that apply"}, {"title": "2 Background and Related Work", "content": "Diffusion models (DMs) have demonstrated impressive performance in the realm of image synthesis. The fundamental mechanism of DMs involves a predetermined forward diffusion process, where noise is incrementally introduced to the original data $x_0 \\sim p(x_0)$, alongside the training of a denoising model to invert this procedure through a denoising score matching loss expressed as:\n$L_{simple} = E_{t,x_0,\\epsilon} \\epsilon - s_\\theta (x_t, t)|^2$.  (1)\nDMs define the forward process as $dx = f(x,t)dt + g(t)dw$, where $w$ is the Wiener process. The corresponding reverse process is: $dx = [-f(x,T - t) + g(T - t)^2\\nabla_x log p_{T-t}(x)] dt + g(T - t)dw$ where DMs approximate $\\nabla_x log p_t (x)$ with $s_\\theta(x, t)$.\nIn light of the intricate challenges posed by high-dimensional data such as RGB images, the Latent Diffusion Model (LDM) [32] has been introduced. LDM simplifies the task by conducting the diffusion and denoising procedures within a low-dimensional latent space derived from a Variational Autoencoder (VAE). For the sake of brevity, we choose to exclude the specific notations related to the Encoder and Decoder in this paper.\nFollowing the advancement of T2I, the research community has pioneered Text-to-Video (T2V). Most T2V models have evolved from the 3D UNet structure proposed by VideoDiffusion [16], training with multi-stage 3D UNets through complete finetuning. Imagen Video[17] and Make-A-Video[34] have successfully demonstrated multi-stage T2V models at the RGB pixel level, while few works[38, 44] integrated temporal layers into the multi-stage T2I LDM.\nVideoLDM[4] first presented a method to train T2V models while keeping the T2I models frozen, but applying a masked blending method (not using the T2I model's features directly), and employing multi-stage training techniques. AnimateDiff[12] advanced this by keeping the T2I entirely frozen and only training the temporal layer in one stage, but AnimateDiff did not generalize successfully to a text-to-video model, being limited to specific video domains using Dreambooth or LoRa."}, {"title": "3 Method", "content": "We briefly describe inflating text-to-image (T2I) model to video model as a baseline and then introduce our novel components. Please note that in all the figures within the paper, the parts highlighted in orange indicate the network components that we are training."}, {"title": "3.1 Baseline video model", "content": "Similar to AnimateDiff [12], we add temporal attention layers $l_t$ between spatial layers $l_o$ of the text-to-image model, as a common technique for video generation methods."}, {"title": "3.2 Mapping network", "content": "In diffusion models, training and sampling rely on Gaussian noises $\\epsilon$ and $x_t \\in \\mathbb{R}^{B \\cdot F \\times C \\times H \\times W}$, where $B$, $F$, $C$, $H$ and $W$ are batch size, number of frames, number of channels, height, and width, respectively. Some previous methods with pretrained T2I models design specific distributions, different from Gaussian, to model the relationship between frames in a video [4, 10]. For example, PYoCo used a correlated noise model that employs specific shared noise across all frames to design frame correlations. While this approach fits a Gaussian distribution on a frame-wise basis, it does not conform to a Gaussian distribution across the entire video space of $\\mathbb{R}^{B \\cdot F \\times C \\times H \\times W}$ dimensions (See Appendix). Disregarding the Gaussian distribution nullifies the reparametrization trick of diffusion models, leading to unstable training; experimentally, we have found that models always diverge when training only on video without using joint training with images.\nTherefore, we introduce a mapping network $\\mathcal{IM}$ that transforms the diffusion noise prior (IID noise) into a distribution more suitable for generating videos. Unlike previous methods, it learns the mapping from Gaussian distribution to an implicit distribution for video supervised by the denoising score matching loss (Eq. (1)). In addition, it ensures the validity of the reparametrization trick for training. We design the mapping network to have temporal attention and 3D convolutions such that the inter-frame interaction makes it easier to prepare the input for the Video U-Net to match the relationship between frames within a video. Please refer to Appendix for detail."}, {"title": "3.3 Temporal regularized self-attention loss", "content": "The self-attention map within diffusion models encapsulate structural content of the generated images [5, 20, 23, 30, 36]. Upon this understanding of self-attention maps, Text2video-zero[21] and MasaCtrl[5] pioneers generating videos by sharing the self-attention map without additional training.\nTaking inspiration from their approaches, we propose the Temporal Regularized Self-attention (TRS) loss, which aims to suppress the difference of self-attention maps between consecutive frames:\n$L_{TRS} = \\sum_{i=2}^{N} \\sum_{j=2}^{F} \\lambda_i |A_i^{(j)} - A_i^{(j-1)}|^2$, (3)\nwhere $A_i^{(j)}$ denotes self-attention map of $i$-th layer on $j$-th frame. We use $\\lambda_i = \\frac{i}{N}$ where N is the total number of decoder layers to put more weights on later layers."}, {"title": "3.4 Decoupled contrastive loss on h-space", "content": "Asyrp[23] shows that the deepest feature maps of UNet, named h-space, contains semantic information regardless of the sampling timestep $t$. Thus, assuming that all frames in a natural video have semantically consistent objects, we propose a loss to make all frames have a similar $h$ using the decoupled contrastive loss [42] as shown in Figure 4(a).\nWe define positive pairs by randomly sampling two frames from a video and extracting their deepest feature maps $h$'s. We store negative samples from other videos in a queue and update the queue at every step with previously-used positive pairs. With these pairs, we employ an additional projection layer $g_e()$ to compute $z = g_e(h)$. Finally, we obtain the decoupled contrastive loss [42]:\n$L_{DC} = - (z^{(1)}, z^{(2)}) / \\tau + log \\sum_{q \\in Q} exp ((z^{(1)}, z^{(q)}) / \\tau)$, (4)\nwhere $z^{(1)}$ and $z^{(2)}$ are projected positive pairs, $Q$ is the negative queue, and $\\tau=0.1$ is a temperature parameter. Intuitively, $L_{DC}$ encourages all frames in a video to be closer together in h-space while pushing dissimilar frames from other videos apart. Positive pairs can contain non-consecutive frames because h-space does not contain structure of the scene a lot. The projection layer will be discarded after training."}, {"title": "3.5 Frame-wise text embedding tokens", "content": "Text tokens condition the generative process and several works have used text tokens for personalization [9, 15, 27]. Especially, HiPer [13] optimizes extra tokens after the end-of-sentence token to represent visual details of the input image that are not captured by typical text tokens. If we invert multiple frames of a video, there will be extra tokens for individual frames.\nIn this light, we consider frames in a video as temporal variations of an image and introduce additional frame-wise tokens to represent varying details across frames. We employ a frame-wise token generator to: $(M \\times D) \\rightarrow (F \\times K \\times D)$ that receives the original text tokens from the input text prompt to produce frame-wise tokens, where $M$ is the maximum token length (=77), $D$ is the token embedding dimension (=768), $F$ is the number of frames, and $K$ is the number of frame-wise tokens (=3). The frame-wise tokens are concatenated to the text tokens in token-dimension and fed into the cross-attention layers. Figure 4(b) depicts this process."}, {"title": "3.6 Training", "content": "We train the temporal layers with all the proposed losses, including the mapping network, the projection layer for the contrastive loss, and the frame-wise token generator. The trainable and frozen components are marked in orange and gray, respectively, in the figures. The final loss becomes\n$L_{all}(\\theta) = L_{simple} + \\lambda_{TRS}L_{TRS} + \\lambda_{reg}L_{reg} + \\lambda_{DC}L_{DC}$. (5)\nNote that we do not compute $L_{simple}$ for $f_{\\mathcal{IM}}(x_t, t, c)$. All losses can reduce the movement in the video. However, interestingly, we have found through experimentation that the appropriate use of loss actually helps in generating videos with greater movement. We speculate that appropriate regularization helps the model to find a better local minimum."}, {"title": "3.7 Inference with mitigating gradient sampling", "content": "Even with a well-trained model, we observe phenomena where the structure collapses or artifacts appear and disappear when generating complex actions or fast motions. We argue that this can be effectively prevented by adding simple guidance during the generative process.\nIn the pursuit of smoother image sequences, we measure the similarity between consecutive frames using a kernel function [25], which is integrated into a process designed by ADM[7]. They innovated by introducing 'Classifier guidance' into the generative process, with the equation\n$dx = [-f(x,t) + g(t')^2 (s_\\theta (x,t') + \\alpha \\nabla_xlog P_t)] dt + g (t') dw$, (6)\nwhere $t' = T - t$ signifies the reverse timestep, $\\Phi$ is an extra function for the guidance, and $\\alpha$ is a parameter that scales the guidance. This guidance can"}, {"title": "4 Experiments", "content": "We present thorough experiments including representative videos, ablation study, application to personalized or image-conditioned videos, and comparison to existing methods.\nConfiguration Our model is built upon pretrained text-to-image (T2I) StableDiffusion v1.5. After inflating it to be text-to-video (T2V) model with our method, we train the inflated part on WebVid-10M [1] while the T2I part is kept frozen. The videos are 4 seconds long and 6 frames per second (fps), thus 24 frames in total. We set $\\lambda_{TRS} = \\lambda_{reg} = \\lambda_{DC} = 0.1$ and $\\alpha = 40$. Other details are in Appendix. For our quantitative evaluation, we use Fr\u00e9chet Video Distance (FVD) [37] on UCF101 [35] and Clip Similarity [31] on MSR-VTT [40] to draw comparisons with other prevalent methods following the common practice. On UCF101, we generate 100 videos for each class using the class names as text prompts. On MSR-VTT, we select the first caption of each video in the test set to produce 2,990 videos."}, {"title": "4.1 Qualitative results", "content": "Figure 5 gives example results produced by our model. Leveraging frozen StableDiffusion in its original form has allowed us to generate high-resolution videos up to $512^2$ resolution and creative videos with diverse style: watercolor painting, pencil drawing, and sci-fi digital art. See the project page videos for details."}, {"title": "4.2 Ablation study", "content": "Table 1 demonstrates that our proposed components cumulatively improve Fr\u00e9chet Video Distance (FVD), starting from the baseline. We adopt AnimateDiff [12] as our baseline which also trains only the temporal layers while the T2I model is frozen as our method. Notably, the Temporal Regularized Self-attention loss (LTRS) and the Decoupled Contrastive loss (LDC) contribute to a significant improvement.\nFigure 6a demonstrates effect of mapping network and frame-wise tokens. We extract two short vertical segments of fixed locations from 24 frames of each video, and stack them horizontally between the first and last frame. Comparing [A] and [B], the minimal variant of our method still bear inconsistent objects' appearance between frames, revealed by the first and last frames, while adding mapping network improves the consistency. Comparing [B] and [C], adding frame-wise tokens resolves flickering between adjacent frames which is demonstrated by jaggies and smoothness in the horizontal stack of the 24 frames, respectively. We recommend watching the video comparison in the supplementary materials."}, {"title": "4.3 Personalized / image-conditioned videos", "content": "Our model naturally supports combining existing off-the-shelf personalized T2I methods [18, 33, 41, 43] because of the frozen pretrained T2I layers. In Figure 7a, we compare personalized videos using DreamBooth [33] on AnimateDiff [12] and ours. AnimateDiff produces spurious structure such as branching arm while our method produces videos with natural structure. Figure 7b and Figure 1 show the videos controlled by skeleton and sketches using ControlNet. We can either provide conditions of full frames or only a few non-adjacent frames, e.g., starting and ending frames. In the latter case, our model produces plausible inter-motion between the specified frames. Figure 7c shows image-conditioned video using IP-Adapter. More results are in the supplementary materials.\nThis compatibility with off-the-shelf T2I variants is a huge advantage of our model leading to potential explosion of video generation era. Especially, our model does not require additional training to inflate the T2I variants of StableDiffusion."}, {"title": "4.4 Comparison to existing methods", "content": "Table 2 showcases a comparative analysis with other studies in the field following the common practice. We emphasize that ours is the method with single-stage and frozen T2I layers for general video generation task. With a slightly different setting, ours is able to achieve FVD of 623.51, which is better than our standard setting, but noticed worse visual quality in general. Due to the instability of the FVD metric [11], we stopped tuning our settings for a better FVD and aimed at the best visual quality. We prepared 100 unique text prompts with distinct movements (Table A3) and generated four videos per prompt. These 400 videos were then evaluated based on binary attributes to determine the best quality. Please refer to Table A2. A concurrent work, EmuVideo [11], also reports a relatively high FVD score and argues that automated metrics on UCF101 are flawed for zero-shot text-to-video evaluation.\nAdditionally, the table presents FVD of StableDiffusion obtained by sampling each frame individually with class names from the UCF101 as text prompts. Our observations from the results of StableDiffusion highlight two key points as follows. First, the classes in UCF101 invariably include human hands and faces, which StableDiffusion struggles to accurately generate. Second, it tends to generate various domains such as illustrations and gray photographs. Our method, which employs frozen StableDiffusion, exhibits similar tendency. This, we believe, contributes to acheiving the relatively higher FVD.\nDespite of FVD score, our method produces diverse and accurate movements while previous methods often fail, e.g., continuity of walking legs across frames. Table 3 provides a user study with 50 participants with over 5K votes on 3 questions over 34 videos (13+13+8) randomly selected from the project pages of VideoLDM and PYoCo and generated by ModelScope. The participants rated ours higher than others in motion, consistency, and overall quality. We assert"}, {"title": "5 Conclusion and Discussion", "content": "This paper presents a video generation model built upon a pretrained text-to-image (T2I) model. It has novel network components including the mapping network and the frame-wise token generator and is supervised by novel losses such as temporal regularized self-attention loss, h-space contrastive loss, and image model regularization loss. Furthermore, it is accompanied by a mitigating gradient sampling that further improves smoothness and quality in generative process. All elements are motivated by inductive biases from characteristics of natural videos and contribute to the performance improvements. Last but not least, our model is efficient to train thanks to single-stage process and frozen T2I layers. We hope our method will provide an influential starting point for future T2V models and contribute to a healthy research community.\nHowever, our T2V model has limitations due to its reliance on the StableDiffusion T2I model. This dependency means that issues in T2I models, like inaccurately generating human hands and limbs, also affect our T2V model. Exploring connections with other methods that have addressed these issues would be a valuable direction for future work.\nIt also raises ethical concerns related to video generation, such as the risks of misusing synthetic media. Our work underscores the importance of continuous improvement in generative models in video synthesis, while also highlighting the necessity for ethical considerations."}, {"title": "A Network design", "content": "We provide pseudo-codes at the end of the appendix.\nTemporal layers Similar to AnimateDiff [12], we add temporal attention layers $l_t$ between spatial layers $l_o$ of the text-to-image model, as a common technique for T2V methods. For each temporal layer $l_t$, we reshape the input as $((bf) chw \\rightarrow bcfhw)$ and output as $(bcfhw \\rightarrow (bf) chw)$, where $b$, $c$, $f$, $h$, and $w$ represent batch size, number of channels, number of frames, height, and width, respectively.\nMapping network Our mapping network $\\mathcal{IM}$ tries to prepare the proper input for the text-to-video (T2V) U-Net. In order to consider the cross-frame relationship, we design its architecture to have 3D convolution layers for $f$, $h$, $w$ dimensions and a temporal attention layer. Initially, the input is reshaped from $(bfchw\\rightarrow (bhw) cf)$, following which a 3D convolution is applied. Subsequently, the data undergoes processing through a temporal attention layer and a linear layer, facilitating effective noise distribution alignment within the frame sequence.\nToken generator While typical architectures receive identical text tokens to cross-attention layers for all frames, our token generator $t_e()$ provides frame-wise text tokens that model detailed temporal variations beyond the standard text tokens of objects. To facilitate this, the token generator transforms the original text tokens into frame-wise tokens. It utilizes a sequential network of linear and SiLU layers, followed by a 1D convolution layer. This structure is capable of expanding the text embeddings into a format suitable for representing each frame individually.\nProjection layer $g()$ We implement our projection layer $g()$ for the contrastive loss on $z$ space. It is an MLP with SiLU between linear layers of $(chw)\\rightarrow c \\rightarrow c\\rightarrow c$ channels."}, {"title": "B Custom noise distribution of previous methods", "content": "It is worth noting that the spatial layers of the Text-to-Image (T2I) model remain frozen, and we directly utilize them. This practice acknowledges the imbalance between the characteristics of a T2I model, trained to generate a variety of images starting from Gaussian noise, and the objectives of a Text-to-Video (T2V) model, which needs to create a sequence of images. PYoCo [10] supposes that the IID noise prior is ill-suited for training video diffusion models due to the lack of temporal correlations between frames. Instead, they introduce a correlated noise model that uses specific shared noise across all frames to design frame correlations. However, this shared noise leads to a non-Gaussian distribution that contradicts the premise of the reparameterization trick, which"}, {"title": "C Effect of the mapping network", "content": "Our designed mapping network was implicitly engineered to modify an input distribution suitable for videos. Figure Al illustrates the small but non-negligible value changes observed after passing through the mapping network. The blue line depicts the mean of the absolute differences between values before and after the mapping network, while the gray area signifies a range of $\\pm 1$ standard deviation. This suggests that considerable modifications have been implemented post-mapping network passage. The orange line indicates the mean value shift after mapping, and the green line shows the standard deviation change. Notably, even with alterations by the mapping network, the standard deviation mostly remained stable, with only a minor increase in mean values across all times.\nFigure A2 displays the outcomes when the mapping network is not utilized. Since the model was trained with the mapping network, omitting it artificially results in changed and corrupted content. Interestingly, when StableDiffusion is employed with the mapping network, the end results are nearly identical to those without it. We speculate this demonstrates that the mapping network implicitly modifies a distribution without notably impacting the image model."}, {"title": "D More details", "content": "Here we share more detailed settings.\nTraining We initiated our training with a learning rate of 0.0001 and gradually decreased it to 0.00001. We adopted a beta start of 0.00085 and an end of 0.012, utilizing a linear beta schedule. The training process took approximately one or two weeks on 32 A100 GPUs. We separately trained models for $256^2$ resolutions, with all numerical data and experiments in the paper conducted using the $256^2$ model. However, we also found that models trained on $256^2$ resolution videos could generate up to $512^2$ resolutions. Notably, in cases where the text prompts"}, {"title": "E More ablation study", "content": "To measure our model's performance, we created 100 custom prompts and evaluated them across four categories (See Table A3.) The prompts were evenly distributed into five themes: humans, animals, objects, nature, and complex/imaginary, each containing 20 sentences. All sentences were composed of clear, simple language with verbs indicating distinct actions or movements. For each prompt, we sampled four videos to check if the content matched the text, if there was action aligned with the verb, how smooth the video was, and whether there were any strange artifacts or collapsing objects.\nWe chose this approach for several reasons: Firstly, measuring FVD requires sampling over 10,000 videos, which is time-consuming. Secondly, FVD does not adequately reflect the movement or motion in videos. Thirdly, the classes in UCF101 are too limited to effectively evaluate a text-to-video model. Therefore, we based all our experiments on the 100 prompts we designed and conclude that all the components we proposed significantly enhance performance as evident in Table A2."}]}