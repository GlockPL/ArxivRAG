{"title": "TdAttenMix: Top-Down Attention Guided Mixup", "authors": ["Zhiming Wang", "Lin Gu", "Feng Lu"], "abstract": "CutMix is a data augmentation strategy that cuts and pastes image patches to mixup training data. Existing methods pick either random or salient areas which are often inconsistent to labels, thus misguiding the training model. By our knowl-edge, we integrate human gaze to guide cutmix for the first time. Since human attention is driven by both high-level recognition and low-level clues, we propose a controllable Top-down Attention Guided Module to obtain a general artificial attention which balances top-down and bottom-up atten-tion. The proposed TdATttenMix then picks the patches and adjust the label mixing ratio that focuses on regions relevant to the current label. Experimental results demonstrate that our TdAttenMix outperforms existing state-of-the-art mixup methods across eight different benchmarks. Additionally, we introduce a new metric based on the human gaze and use this metric to investigate the issue of image-label inconsistency.", "sections": [{"title": "1 Introduction", "content": "Thanks to large amount of data, Deep Neural Net-works (DNNs) have achieved significant success in recent years across a variety of applications, including recogni-tion (Dosovitskiy et al. 2021; Zang et al. 2022; Cui et al. 2022; Tan et al. 2022; Chen, Fan, and Panda 2021), graph learning (Xia et al. 2022; Wu et al. 2023; Cheng et al. 2022), and video processing (Liu et al. 2021a; Cui et al. 2021; Liu et al. 2021b; Zhao et al. 2022). However, the data-hungry problem (Dosovitskiy et al. 2021; Touvron et al. 2021a) leads to overfitting when the training data are scarce. Therefore, a series of data augmentation techniques called mixup are proposed to alleviate this issue and enhance DNNs' gen-eralization capabilities. Among them, CutMix (Yun et al. 2019) is an effective strategy that randomly crops a patch from the source image and pastes it into the target image. The label is then mixed by the source and target labels in proportion to the crop area ratio.\nSince the randomness in CutMix (Yun et al. 2019) ignores the spatial saliency, a group of saliency-based variants (Ud-din et al. 2021; Kim, Choo, and Song 2020; Walawalkar et al. 2020; Liu et al. 2022c; Dabouei et al. 2021; Chen et al. 2023) leverage the bottom-up attention as a supervi-sory signal. Bottom-up attention operates on raw sensory in-put and orients attention towards visual features of potential importance to calculate the saliency. This process only dis-covers what is where in the world (Schwinn et al. 2022), which equally looks for all salient regions in the raw sen-sory input. Therefore, existing saliency-based variants based on bottom-up attention are easily distracted by high saliency regions that are, in fact, irrelevant to the target label. For instance, the source image of Figure 1 is dog, but Salien-cyMix (Uddin et al. 2021) become distracted by the dark rock and crops the background, including only part of the dos's ear.\nHuman vision entails more than just the determination of what is where; it involves the development of internal repre-sentations that facilitate future actions. For instance, psycho-logical research (Buswell 1935; Yarbus 2013; Belardinelli, Herbort, and Butz 2015) found that human gaze, initially guided by bottom-up features, can be strongly influenced by the task at hand. Consequently, recent research proposes top-down mechanisms (Shi, Darrell, and Wang 2023; Schwinn et al. 2022) showing effectiveness in modeling human gaze patterns such as scanpaths (Schwinn et al. 2022) and in en-"}, {"title": "2 Related Work", "content": "2.1 CutMix and its variants\nCutMix (Yun et al. 2019) randomly crops a patch from the source image and pastes it onto the corresponding lo-cation in the target image, with labels being a linear mix-ture of the source and target image labels proportionate to the area ratio. Since random cropping ignores the re-gional saliency information, researchers leverage a series of saliency-based variants based on bottom-up attention. At-tentiveMix (Walawalkar et al. 2020) and SaliencyMix (Ud-din et al. 2021) guide mixing patches by saliency regions in the image (based on class activation mapping or a saliency detector(Montabone and Soto 2010)). Subsequently, Puz-zleMix (Kim, Choo, and Song 2020) and Co-Mixup (Kim et al. 2020) propose combinatorial optimization strategies to find optimal mixup that maximizes the saliency information. Then AutoMix (Liu et al. 2022c) adaptively generates mixed samples based on mixing ratios and feature maps in an end-to-end manner. Inspired by the success of Vision Trans-former (ViT)(Dosovitskiy et al. 2021), TokenMixup (Choi, Choi, and Kim 2022) is proposed to adaptive generate mixed images based on attention map. Moreover, concerning label assignment, recent studies have also adjusted label assign-ment by bottom-up attention. TransMix (Chen et al. 2022) mixes labels based on the class attention score and Token-Mix (Liu et al. 2022a) assigns content-based mixes labels on mixed images. Recently, SMMix (Chen et al. 2023) mo-tivates both image and label enhancement by the bottom-up self-attention of ViT-based model under training itself. However, these existing variants, focusing either on enhanc-ing saliency or adjusting label assignments, are reliant on bottom-up attention, which is susceptible to being distracted by salient but label-inconsistent background areas. To re-lieve label inconsistency, we introduce task adaptive top-down attention into CutMix variants for the first time and propose our framework TdAttenMix.\n2.2 Computational modeling of Attention\nComputational modeling of human visual attention inter-sects various disciplines such as neuroscience, cognitive psychology, and computer vision. Biologically-inspired at-tention mechanisms can enhance the interpretability of ar-tificial intelligence (Vuyyuru et al. 2020). The attention can be categorized into bottom-up and top-down mecha-nisms (Connor, Egeth, and Yantis 2004). Initially, the focus was primarily on computational modeling of bottom-up at-tention. Based on the Treisman's seminal work describing the feature integration theory (Treisman and Gelade 1980), current approaches assume a central role for the saliency map. Within the theory, attention shifts are generated from the saliency map using the winner-take-all algorithm (Koch and Ullman 1985). Consequently, the majority of studies have focused on improving the estimation of the saliency map (Borji and Itti 2012; Riche et al. 2013). Recently self-attention (Dosovitskiy et al. 2021) is a stimulus-driven ap-proach that highlights all the salient objects in an image, rep-resenting a typical bottom-up attention mechanism. With the advent of increasingly large eye-tracking datasets (Liu et al. 2022b; Jiang et al. 2015), researchers have been inspired to explore task-guided top-down attention. Shi et al. (Shi, Darrell, and Wang 2023) propose a top-down modulated ViT model by mimicking the task-guided mechanism of human gaze. Shiwinn et al. (Schwinn et al. 2022) impose a biologically-inspired foveated vision constraint to neural networks to generate human-like scanpaths without training for this object. As for CutMix variants, previous saliency-based methods have utilized bottom-up attention to opti-mize cropping regions, whereas we explore the use of task-adaptive top-down attention to obtain a cropped region that is more consistent with the label."}, {"title": "3 Preliminary", "content": "CutMix augmentation. CutMix (Yun et al. 2019) is a sim-ple data augmentation technique that combines two pairs of input and labels. x and y represent a training image and its corresponding label, where $x \\in \\mathbb{R}^{H\\times W \\times C}$. To create a new augmented training sample (X\u043c, \u0443\u043c), CutMix (Yun et al. 2019) utilizes a source image-label pair (xA, yA) and a tar-get image-label pair (x\u0432, y\u0432). Mathematically, this can be expressed as follows:\n$X_M = M \\odot X_A + (1 - M) \\odot X_B$ (1)\n$y_M = \\lambda y_A + (1 - \\lambda) y_B$ (2)\n$M \\in \\{0,1\\}^{H\\times W}$ denotes a rectangular binary mask that indicates where to drop or keep in the two images, $\\odot$ is element-wise multiplication, and $\u03bb$ indicates the area ratio of xA in mixed image xm, i.e., $\\lambda = \\frac{\\Sigma M}{HW}$"}, {"title": "4 Framework of TdAttenMix", "content": "This section formally introduces our TdAttenMix, a gen-eral image mixing framework that balances top-down and bottom-up attention to simulate the task-guided mechanism of human gaze to crop the patch and adjust the label mix-ing ratio. Figure 2 illustrates an overview of our proposed TdAttenMix. Details are given below.\n4.1 Task Adaptive Attention Guided CutMix\nWe want to simulate the execution logic of human gaze, which is initially guided by bottom-up features and then strongly influenced by the current task.\nBottom-up Attention. We divide the source image XA and the target image x B into non-overlapping patches of size P \u00d7 P. Each image yields a total of $N = \\frac{H}{P} \\times \\frac{W}{P}$ patches. Consequently, XA and B are restructured as ta,tB \u0404 $\\mathbb{R}^{N\\times(P^2C)}$, where each row corresponds to a token and d = P2C. As illustrated in Figure 2, we follow SMMix (Chen et al. 2023) which obtains the attention map across all the image tokens for the bottom-up attention (Dosovitskiy et al. 2021). We obtain $Q = tW_q, K = tW_k$, and $V = tW_v$, where $W_q \\in \\mathbb{R}^{d \\times d}, W_k \\in \\mathbb{R}^{d \\times d}$, and $W_v \\in \\mathbb{R}^{d \\times d}$ represent the learnable parameters of the fully-connected layers.\nTop-down Attention Guided Module. The Top-down Attention Guided Module we propose is depicted in Fig-ure 3. The current task at hand is the classification task. Then we extract the corresponding parameters $w_{td} \\in \\mathbb{R}^{d\\times 1}$ from the final fully-connected layer of Vision Transformer, which is based on the current label. The parameter matrix from this layer mirrors the relationship between feature and category mapping. Thus, we can acquire the high-level guidance $V_{td}$ tied to a specific category by calculating it with the image feature t. The theory that top-down attention can be imple-mented by simply augmenting $V_{ta}$ to V with K and Q re-maining constant was introduced by Shi et al. (Shi, Darrell, and Wang 2023). We ensure that the dimensionality of $V_{td}$ and V is consistent through broadcasting. Furthermore, we accommodated a tunable parameter called balanced factor \u03c3 within our framework to manage the top-down features $V_{td}$. If \u03c3 = 0, our attention map correlates with the preceding", "list": []}, {"title": "4.2 Area-Attention Label Mixing", "content": "To enhance the precision of the mixed label ym, based on the area ratio (Eq. 2) used by CutMix (Yun et al. 2019), we ad-just the area ratio using the attention scores of A and B at their respective positions within the mixed image x M. More specifically, the final mixing ratio A are defined as follows:\n$\\lambda_r = \\frac{hwP^2}{HW}$ (10)\n$Att_A = \\frac{1}{P^2} \\Sigma_{p,q} \\alpha^{A}_{i_t + p - [\\frac{p}{2}], j_t + q - [\\frac{q}{2}]}$ (11)\n$Att_B = \\frac{1}{P^2} \\Sigma_{p,q} \\alpha^{B}_{i_t + p - [\\frac{p}{2}], i_t + q - [\\frac{q}{2}]}$ (12)\n$\\lambda_{\\alpha} = \\frac{Att_A}{Att_A + Att_B}$ (13)\n$\\lambda = \\beta \\lambda_r + (1 - \\beta)\\lambda_{\\alpha}$ (14)\n$Ar$ is the area ratio of xA in mixed image Xx\u043c, Atta and Att is the sum of the task adaptive attention scores at the positions corresponding to XM in a\u04a3 and \u03b1\u03b2, \u03bb\u03b1 is the at-tention ratio of xa in mixed image \u0445\u043c, \u0432 = 0.5, A is the final mixing ratio of xA in the mixed image XM. The final mixed label ym is then defined as follows:\n$y_M = \\lambda y_A + (1 - \\lambda) y_B$ (15)\nThen we obtain the new mixed training sample (\u0445\u043c,\u0443\u043c).\n4.3 Training Objective\nOur TdAttenMix framework is independent on any train-ing model and can be used on various mainstream struc-tures. When deployed on a ResNet-based architecture, we employ the standard classification loss and the consistency constraint losses proposed in SMMix (Chen et al. 2023). The"}, {"title": "5 Experiments", "content": "We evaluate TdAttenMix in four aspects: 1) Evaluating im-age classification tasks on eight different benchmarks, 2) transferring pre-trained models to two downstream tasks, 3) Evaluating the robustness on three scenarios including oc-clusion and two out-of-distribution datasets. (4) In addition, we have conducted the first quantitative study on the effec-tiveness of saliency-based methods in reducing image-label inconsistency. Our TdAttenMix is highlighted in gray, and bold denotes the best results.\n5.1 Small-scale Classification\nIn small-scale classification we use ResNet-18 (He et al. 2016) and ResNext-50 (Xie et al. 2017) to compare the performance. Hyperparameter settings are in the section 1 of the Supplementary. Table 1 shows small-scale classi-fication results on CIFAR-100, Tiny-ImageNet and CUB-200. Compared to the previous SOTA methods, TdAttenMix consistently surpasses AutoMix (+0.08~ +0.84), PuzzleMix (+1.18 ~ +2.97), MainfoldMix (+0.34 ~ +3.35) based on various ResNet architectures. Moreover, TdAttenMix no-ticeably exhibits a significant gap with SaliencyMix (+2.50 ~ +4.25).\n5.2 ImageNet Classification\nTable 1 validates the performance advantage of TdAtten-Mix over other methods. In particular, TdAttenMix boosts the top-1 accuracy by more than +1% in ResNet-18 (He et al. 2016) and Deit-S (Touvron et al. 2021b) compared with the SaliencyMix baseline and achieves the sota result. It can be noted that TransMix, TokenMix and SMMix also ex-hibit good top-1 accuracy, but they are limited to ViT-special methods, causing them incompatible with all mainstream architectures (e.g., ResNet). We provide more comparisons with ViT-special methods in Section 2 of the Supplementary,"}, {"title": "5.3 Downstream Tasks", "content": "Semantic segmentation. We use ADE20k (Zhou et al. 2017) to evaluate the performance of semantic segmentation task. ADE20k is a challenging scene parsing dataset cover-ing 150 semantic categories, with 20k, 2k, and 3k images for training, validation and testing. We evaluate DeiT back-bones with UperNet (Xiao et al. 2018). As shown in Table 2, TdAttenMix improves Deit-S for +1.6% mIoU and +2.5% mAcc.\nWeakly supervised automatic segmentation (WSAS). We compute the Jaccard similarity over the PASCAL-VOC12 benchmark (Everingham et al. 2015). The attention masks generated from TdAttenMix-DeiT-S or vanilla DeiT-S are compared with ground-truth on the benchmark. The evaluated scores can quantitatively help us to understand if TdAttenMix has a positive effect on the quality of attention map. As shown in Table 2, TdAttenMix improves Deit-S for +3.3%.\n5.4 Robustness Analysis\nRobustness to Occlusion. Naseer et al. (Naseer et al. 2021) studies whether ViTs perform robustly in occluded scenar-ios, where some of most of the image content is missing. Following (Naseer et al. 2021), we showcase the classifi-cation accuracy on ImageNet-1k validation set with three dropping settings. (1) Random Patch Dropping. (2) Salient (foreground) Patch Dropping. (3) Non-salient (background) Patch Dropping. As depicted in Figure 5, Deit-S augmented with TdAttenMix outperforms the standard Deit-S across all occlusion levels.\nOut-of-distribution Datasets. We evaluate our TDAtten-Mix on two out-of-distribution datasets. (1) The ImageNet-A dataset (Hendrycks et al. 2021). The metric for assess-ing classifiers' robustness to adversarially filtered exam-ples includes the top-1 accuracy, Calibration Error (Cal-ibError) (Hendrycks et al. 2021; Kumar, Liang, and Ma 2019), and Area Under the Response Rate Accuracy Curve (AURRA) (Hendrycks et al. 2021). (2) The ImageNet-O (Hendrycks et al. 2021). The metric is the area under the precision-recall curve (AUPR) (Hendrycks et al. 2021). Table 3 indicates that TdAttenMix can have consistent perfor-mance gains over vanilla Deit-S on the out-of-distribution data."}, {"title": "5.5 Image-label Inconsistency Analysis", "content": "Previous image mixing methods did not quantitatively vali-date the image-label inconsistency.Motivated by the fact that gaze reflects human vision (Huang et al. 2020), we pro-pose using the mixed label, which is based on gaze atten-tion, as the ground-truth to validate the problem of image-label consistency. For our experiments, we utilize ARISTO dataset (Liu et al. 2022b) and the corresponding raw images. Since A determines the mixed label, the image-label incon-sistency can be represented by the difference between the A"}, {"title": "5.6 Visualization", "content": "In Figure 6, we visualize the class activation map (Selvaraju et al. 2017) of the models trained with CutMix and TdAt-tenMix. As shown in the left of Figure 6 that the TdAt-tenMix can locate object with more precision than the Cut-Mix model in the unmixed images. Furthermore, the right of Figure 6 shows that for the mixed images, the TdAtten-Mix model can accurately locate objects from two differ-ent images. On the contrary, CutMix model focuses only on the class of image A. Our TdAttenMix is guided by task adaptive attention, which ensures that the information in the training data is sufficient enabling superior recognition ca-pacity for mixed images."}, {"title": "5.7 Ablation Study", "content": "We conduct an ablation study to analyze our proposed TdAt-tenMix. We use ResNet-18 (He et al. 2016) as the backbone and train it on CUB-200 (Wah et al. 2011).\nControl of Task Adaptive Balanced Attention. Our TdAttenMix balances top-down and bottom-up attention by adjusting the top-down signal Vtd, enabling a shift from standard bottom-up to top-down attention. We evaluate three different task adaptive balanced attention strategies: 1) \u03c3 ="}, {"title": "6 Conclusion", "content": "This paper proposes TdAttenMix, a general and effective data augmentation framework. Motivated by the superior-ity of human gaze, we simulate the task-guided mechanism of human gaze to modulate attention. TdAttenMix intro-duces a new Top-down Attention Guided Module to balance bottom-up attention for task-related regions. Extensive ex-periments verify the effectiveness and robustness of TdAt-tenMix, which significantly improves the performance on various datasets and backbones. Furthermore, we quantita-tively validate that our method and saliency-based methods can efficiently reduce image-label inconsistency for the first time."}]}