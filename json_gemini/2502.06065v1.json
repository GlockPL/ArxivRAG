{"title": "Benchmarking Prompt Sensitivity in\nLarge Language Models", "authors": ["Amirhossein Razavi", "Mina Soltangheis", "Negar Arabzadeh", "Sara Salamat", "Morteza Zihayat", "Ebrahim Bagheri"], "abstract": "Large language Models (LLMs) are highly sensitive to variations in\nprompt formulation, which can significantly impact their ability to generate accu-\nrate responses. In this paper, we introduce a new task, Prompt Sensitivity Predic-\ntion, and a dataset PromptSET designed to investigate the effects of slight prompt\nvariations on LLM performance. Using TriviaQA and HotpotQA datasets as the\nfoundation of our work, we generate prompt variations and evaluate their effec-\ntiveness across multiple LLMs. We benchmark the prompt sensitivity prediction\ntask employing state-of-the-art methods from related tasks, including LLM-based\nself-evaluation, text classification, and query performance prediction techniques.\nOur findings reveal that existing methods struggle to effectively address prompt\nsensitivity prediction, underscoring the need to understand how information needs\nshould be phrased for accurate LLM responses.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) can generate human-like responses to a wide array\nof prompts, from answering specific queries to planning for accumulating information\nto answer complex questions [22]. Despite their usefulness, a notable challenge in\nworking LLMs is their sensitivity to prompt formulation [25]. Small variations in the\nphrasing, structure, or even punctuation of prompts can often lead to substantially\ndifferent outputs [36,32,29]. To illustrate this issue, consider the sample prompts shown\nin Table 1. In this table, we present samples from the TriviaQA [21] and HotPotQA [39]\nquestion-answering datasets where the LLM responds correctly and accurately to the\noriginal prompts. However, with only slight modifications in wording, we observe that\nthe LLM (in this case LLaMA3.1) fails to provides the correct response.\nThis challenge, which we refer to as prompt sensitivity, highlights the challenges\nusers face when crafting their prompts [15,41]. For this reason, prompt engineering,"}, {"title": "Methodology", "content": "The Task Definition. Our proposed task of Prompt Sensitivity Prediction aims to predict\nwhether a given prompt can be effectively fulfilled by the LLM whose response to the\nprompt would satisfy the users' information need. More specifically, given a prompt\np with a specific information need \\(I_p\\), we consider a set of similar prompts, denoted\n\\(P = {p'|Sim(p,p') > \\tau \\) and \\(I_p == I_{p'}}\\) , where each variation p' shares the same\ninformation need \\(I_p\\) and maintains a similarity with p above a predefined threshold \\(\\tau\\).\nThese prompts \\({p'}\\) are designed to be only slightly modified versions of p, ensuring\nthey still reflect the same information need of the user. The goal of this task is to predict,\nfor a given prompt pi, whether the LLM will generate a response that accurately respond\nto the underlying information need \\(I_{p_i}\\)\n The Dataset for the Task. To create the gold standard dataset for the prompt sensitivity\ntask, we adopt a systematic process to generate prompt variations and evaluate their\neffectiveness as follows:\n1. Selecting Prompts: We start by choosing a set of initial prompts, denoted as P,\nwhere each prompt p \u2208 P is seeking a distinct information need \\(I_p\\).\n2. Generating Variations: For each prompt p in the set, we use an LLM L to generate\nN variations \\(p' = L(p | I_p = I_{p'})\\). Here, L(p) denotes the process of generating\nvariations of prompt p, where each variation p' retains high semantic similarity\nwith p, i.e., (Sim(p, p') > r) and preserves the original information need Ip.\n3. Filtering Variations: We process and filter out any generated variations p' that do\nnot meet specific criteria for similarity and alignment with the original prompt p\nincluding LLM hallucinated content.\n4. LLM Response Generation: For each prompt p and its variations \\({p' \\in P'}\\), we\nask the LLM to respond to the prompt, denoted \\(a_p \\in A_p\\) for the original prompt\nand \\(a_{p'} \\in A'\\), for each variation.\nThe combination of \\(P \\cup P'\\) as well as their LLM generated answers \\(A_p \\cup A'\\), form\nthe PromptSET dataset for this task.\nSource Data. To build our dataset, we require a set of prompts that have human anno-\ntated answers available as well as having reliable evaluation with deterministic results.\nTherefore, we selected two widely-used question-answering datasets, TriviaQA [21] and\nHotpotQA datasets [39] that meet these requirements. TriviaQA is a reading comprehen-\nsion dataset containing over 650K question-answer-evidence triples. The questions are\non average 14 words. Each question has a collection of accepted answers including a list\nof aliases and normalized version of the answers; the majority of which are specific and\nshort [21]. Furthermore, HotpotQA is a large-scale question-answering dataset consists\nof 113k training question-answer pairs. Unlike TriviaQA, HotpotQA includes complex\nmulti-hop and comparison questions that require reasoning across multiple documents\nto answer accurately [39]."}, {"title": "Experiments and Findings", "content": "Baseline Performance. We analyze the performance of the of baselines on the PromptSET\ntest set. Table 2 presents the results for predicting whether each prompt variation could be\ncorrectly answered using the LLaMA and Mistral. We report results in terms of accuracy,\nF1, recall, and precision. As shown in the table, the specificity-based QPP methods (i.e.,\nCC, DC, IEF, and PageRank) perform the lowest among the baselines. Since QPP meth-\nods were not specifically designed for prompt sensitivity prediction, their performance\nis relatively weak on both datasets. We hypothesize that the specificity levels across the\noriginal prompts and their variations are too similar, making it challenging for speci-\nficity metrics to effectively distinguish between different levels of prompt specificity. On\nthe other hand, BERT-PE demonstrates higher effectiveness in determining whether a\nprompt can be answered correctly. BERT-PE, which is supervised QPP method, shows"}, {"title": "Concluding Remarks", "content": "This paper investigates the sensitivity of LLMs to prompt variations by introducing the\nPrompt Sensitivity Prediction task and the PromptSET dataset, based on TriviaQA and\nHotpotQA. We generate variations of different questions and examine the sensitivity of\nvarious LLMs to these variations, all of which share the same underlying information\nneed. Our benchmarking results reveal that existing methods do not fully capture the\ncomplexities of prompt sensitivity. These findings underscore the need for further re-\nsearch into prompt variation sensitivity, particularly in developing methods to help users\ngenerate more reliable prompts."}]}