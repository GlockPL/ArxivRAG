{"title": "GeoScatt-GNN: A Geometric Scattering Transform-Based Graph\nNeural Network Model for Ames Mutagenicity Prediction", "authors": ["Abdeljalil ZOUBIR", "Badr MISSAOUI"], "abstract": "This paper tackles the pressing challenge of mutagenicity prediction by introducing three ground-\nbreaking approaches. First, it showcases the superior performance of 2D scattering coefficients extracted\nfrom molecular images, compared to traditional molecular descriptors. Second, it presents a hybrid\napproach that combines geometric graph scattering (GGS), Graph Isomorphism Networks (GIN), and\nmachine learning models, achieving strong results in mutagenicity prediction. Third, it introduces a\nnovel graph neural network architecture, MOLG\u00b3-SAGE, which integrates GGS node features into a\nfully connected graph structure, delivering outstanding predictive accuracy. Experimental results on\nthe ZINC dataset demonstrate significant improvements, emphasizing the effectiveness of blending 2D\nand geometric scattering techniques with graph neural networks. This study illustrates the potential of\nGNNs and GGS for mutagenicity prediction, with broad implications for drug discovery and chemical\nsafety assessment.", "sections": [{"title": "1. Introduction", "content": "In toxicological and pharmaceutical research, determining a compound's mutagenesis potential is\ncritical to protecting public health and safety. This assessment is crucial in a number of domains, in-\ncluding medication development, environmental preservation, and conformity to regulations. The Ames\ntest, a biological assay created by Bruce Ames in the 1970s, has long been considered the gold stan-\ndard for determining chemicals capable of causing genetic changes [1]. The test includes exposing\nspecially modified Salmonella bacteria strains to the substance of interest and detecting whether or not\nit causes DNA alterations. Despite its great popularity and acceptability, the Ames test is not without\nflaws. It can occasionally produce erroneous results. The repeatability of the test between laborato-\nries is not absolute, with a reproducibility of less than 100% [2]. These limitations, combined with\nthe resource-intensive nature of the test and the ethical concerns associated with animal testing, have\nled the scientific community to seek alternative methods. The exponential increase in the number of\nnew chemical entities requiring toxicological evaluation, coupled with the critical need for rapid screen-\ning in modern drug discovery pipelines, has accelerated the development of computational methods for\npredicting mutagenicity. These in silico methods hold the promise of faster, more cost-effective and po-\ntentially more accurate mutagenicity assessments that overcome the limitations of classical experimental\nmethods while meeting the needs of today's research and regulatory environment.\nAs toxicology has evolved, the need for accurate mutagenicity assessment has increased, leading to\nthe emergence of advanced computational models. While conventional machine learning (ML) [3] and\ndeep learning (DL) [4] methods have shown potential, recent studies highlight the distinct advantage"}, {"title": "2. Related Work", "content": "Our work focuses on 2D and geometric scattering, graph neural networks (GNNs), and mutagenic-\nity prediction. While earlier approaches have employed supervised machine learning models, CNN and\nGNNs to predict mutagenicity on the ZINC dataset, the application of scattering transform for feature\nextraction is relatively unexplored. In this paper, we close this gap by combining the geometric scattering\ntransform with both supervised machine learning models like Lightgbm and GNNs like GraphSAGE.\nWhile the previous literature shows considerable gains in GNN-based mutagenicity prediction, our tech-\nniques improve prediction capabilities by including geometric scattering to gather multi-scale structure\ninformation. Here, we review key research, highlight the merits and limits of current approaches, and\noffer our work as a step forward in enhancing predictive accuracy for toxicology."}, {"title": "2.1. Traditional Machine Learning Approaches in Mutagenicity Prediction", "content": "Traditional machine learning (ML) models have been widely used in Ames mutagenicity prediction,\ngenerally using molecular fingerprints and specified descriptors to forecast toxicological effects. The\nRandom Forest (RF) model is among the most widely utilized. Chu et al. [8] used RF with ECFP\nand FCFP features on a dataset of 5359 chemicals and obtained an AUC of 0.84 and an ACC of 0.79.\nSimilarly, Venkatraman et al. [9] used RF with PUBCHEM fingerprints (FP) on a larger dataset of 7950\nchemicals, yielding an AUC of 0.87 and an ACC of 0.79. These researches established the usefulness\nof fingerprint-based descriptors for predicting mutagenicity by allowing models to extract key structural\ninformation from chemical substances."}, {"title": "2.2. Advances in Deep Learning for Predicting Mutagenicity", "content": "Deep learning algorithms have considerably improved mutagenicity prediction because of their ca-\npacity to learn complicated patterns from data. However, in some applications, deep neural networks\n(DNNs) are still used alongside molecular descriptors, which provide a structured representation of\nchemical substances. Kumar et al. [12] used a DNN with alvaDesc molecular descriptors on a dataset\nof 4053 chemicals, attaining an AUC of 0.89 and an ACC of 0.84. In this scenario, while the DNN ar-\nchitecture allowed for the automatic learning of molecular feature connections, the alvaDesc descriptors\nstill supplied a predefined structure to the data, making the learning work easier.\nSimilarly, Lui et al. [13] used DNNs with Binary Morgan fingerprints to predict Ames mutagenicity.\nTheir model, when applied to a dataset of 6380 chemicals, produced an AUC of 0.88 and an ACC of 0.78.\nThe use of Morgan fingerprints as input features indicates that even deep learning models occasionally\nrely on feature engineering to capture crucial chemical attributes, rather than learning purely from raw\ndata.\nOn the other hand, models such as the AMPred-CNN created by Van Tran et al. [6] used CNNs\nto learn directly from molecular images generated from SMILES strings. This method enabled the\nmodel to avoid typical feature engineering by utilizing CNNs' capacity to extract spatial and structural\ninformation from molecular representations autonomously. They also tested a Lightgbm model with\nRDKit 2D molecular descriptors and obtained an AUC of 0.901 and an ACC of 0.827, demonstrating\nthe efficacy of combining classical machine learning and deep learning for mutagenicity prediction.\nWhile deep learning models like as DNNs and CNNs have impressive potential for learning intricate\nchemical interactions, they are not without limits. There is a concern that models like the AMPred-CNN,\nwhich use CNNs to process molecular pictures, will overlook essential chemical structural information.\nMolecular pictures created from SMILES representations can occasionally oversimplify or obfuscate\nimportant chemical interactions like bond types, electron distributions, or three-dimensional spatial ar-\nrangements."}, {"title": "2.3. Using Graph Neural Networks for Molecular Toxicity Prediction", "content": "Graph Neural Networks (GNNs) have emerged as an effective technique for predicting mutagenicity\ndue to its ability to naturally model complicated chemical structures as graphs. Atoms are represented\nas nodes in these models, while bonds between them act as edges, allowing GNNs to successfully\ncapture the intricate topological relationships seen within molecules. Xiong et al. [14] used GNNs"}, {"title": "3. Preliminaries and Background", "content": ""}, {"title": "3.1. Wavelet Scattering Transform", "content": "The 2D Wavelet Scattering Transform (WST) is a powerful method for extracting robust, translation-\ninvariant features from images while preserving high-frequency information often lost in traditional\nconvolutional approaches. Structurally resembling a CNN, the WST provides a predefined, training-\nfree alternative that excels on small datasets.\nWST iteratively applies scaled and rotated wavelet convolutions, followed by modulus operations\nand smoothing, to generate hierarchical representations of images. At each layer, it performs wavelet\ntransforms and nonlinearities; refer to [18] for more background and references. Using Morlet wavelets,\nthis method captures multi-scale and multi-orientation information, forming the foundation of the scat-\ntering transform."}, {"title": "3.1.1. Morlet wavelets", "content": "The 2D Morlet wavelet \u03c8(x) for x \u2208 R\u00b2 is defined as:\n\u03c8o(x) = e'^{ik.x}e^{-||x||^{2}/2\u03c3^{2}}"}, {"title": "3.1.2. Scattering Transform", "content": "Let X be the signal to be analysed. A WST is implemented with a deep convolution network that\niterates over traditional wavelet transform, nonlinear modulus, and averaging operators.\nThe zeroth-order scattering coefficient is obtained by averaging the input signal with a low-pass filter\n\u0444\u0408:\nSox = I * \u0444\u0408, \nwhere $j(x1, x2) = 2\u00af\u221a\u00a2(2\u00af\u221ax1,2\u2212x2) is a scaled version of the low-pass filter \u00f8.this later is the parent\nlow-pass filter, typically chosen as a normalized 2D Gaussian function:\n\u03a6(\u04251, \u04252) =\n1\n2\u03c0\u03c32\nexp (-\n+)\n2\u03c32\n.\nThe first-order scattering coefficients are computed by convolving the input signal with oriented wavelets\n$21,0, taking the complex modulus, and then averaging with \u0444\u0631:\nS1(j, 0)I = |I * \u00a521,0] * \u04241,\nwhere j represents the scale and @ the orientation. The wavelets at each scale are obtained by dilating\nthe mother wavelet defined previously: $21,0(x1, x2) = 2\u22122j\u03c8\u0473(2\u00afjx1,2\u2212jx2).\nThis process continues recursively to generate higher-order coefficients. The mth order scattering coef-\nficients are computed by applying m successive wavelet transforms and modulus operators, followed by\na final averaging:\nSm(j1, 01, ..., \u0458\u0442, 0m)I = |||I * \u03a82/1,01| * * \u03a82jm,0 | * \u03a61,\nwhere j1 < j2 < ... < jm < J ensures a coherent multi-scale analysis. The intermediate representations\nU[j,0]I = |I * $21,0| capture the modulus of wavelet coefficients before averaging, preserving high-\nfrequency information that is essential for discrimination."}, {"title": "3.2. Geometric Scattering on Graphs", "content": "Geometric scattering on graphs extends the principles of the previous section on WST to graph-\nstructured data [20]. This approach provides a rich, multi-scale representation of graphs that is invariant\nto vertex permutations, making it particularly useful for graph analysis, classification, and regression\ntasks [21, 22].\nIn the field of graph-based machine learning, a fundamental challenge is developing algorithms that\nare invariant to graph isomorphism. This property ensures that the algorithm's output remains consistent\nregardless of how the vertices and edges of a graph are indexed, especially in molecular graph analysis.\nA common approach to achieve this invariance is through the use of summation operators, which act on\nsignals defined on graphs. Let G = (V, E) be a graph with vertex set V and edge set E, and let x = xg be\na signal defined on G.\nWhile these simple summation-based features capture some graph properties, they do not fully rep-\nresent the graph's structural information. To address this limitation, we turn to wavelet transforms on\ngraphs. In this work,we associate each graph an adjacency matrix W, where Wij = 1 if nodes i and j are\nconnected, and 0 otherwise. The degree matrix D is a diagonal matrix where Dii = \u2211j Wij, represents\nthe degree of each node.\nThe graph Laplacian L = D\u2013 W serves as a fundamental operator, describing the difference between\nthe degree and adjacency relationships. Alternatively, the normalized Laplacian:\nLnorm = I \u2013 D^{-1/2}WD^{-1/2},\nis often used to ensure stability and prevent biases due to nodes with high degrees.\nThe eigen decomposition of the normalized Laplacian gives us the graph Fourier basis:\nLnorm = VAVT,"}, {"title": "3.3. Graph Neural Network", "content": "Graph Neural Networks [26] are deep learning models that analyze graph-structured data. They\nwork by iteratively updating node representations via message transmission among surrounding nodes.\nThe basic premise is that each node's attributes are updated using both its own qualities and aggregated\ninformation from its neighbors. This method enables GNNs to capture both node-level features and\nstructural information embedded in the graph topology.\nIn general, the message passing framework for GNNs can be expressed as:\nh(k) = UPDATE(k) (h(k-1), AGGREGATE(k) (h(k-1) : u \u2208 N(v))),\nwhere hk) represents the feature vector of node v at layer k, N(v) denotes the neighbors of node v, and\nAGGREGATE and UPDATE are learnable functions."}, {"title": "3.3.1. GraphSAGE", "content": "GraphSAGE [27] is an inductive learning framework for GNNs that enables generating embeddings\nfor previously unseen nodes. The key innovation of GraphSAGE lies in its neighborhood sampling\nstrategy and aggregation functions. The message passing in GraphSAGE can be formulated as:\nh(k) = 0 (W* . CONCAT (h(k-1), AGG(h(k-1), Vu \u2208 N(v)))),\nwhere AGGk can be any differentiable aggregator function (e.g., mean, max, or LSTM), Wk is a learnable\nweight matrix, and o is a nonlinear activation function. The CONCAT operation ensures that the model\npreserves the target node's own features alongside the neighborhood information."}, {"title": "3.3.2. Graph Isomorphism Network (GIN)", "content": "GIN [28] is designed to be as powerful as the Weisfeiler-Lehman graph isomorphism test in distin-\nguishing graph structures. It achieves this by using a simple but powerful update function that maintains\ninjective aggregation of multisets. The GIN layer update rule is defined as:\nh(k)\n=\nMLP(k)\n(\n(1 + \u2208(k)) h(k-1) +\n(k-1)\n\u03a3).\n\u039c\u0395\u039d(\u03bd)\nwhere (k) is either a learnable parameter or fixed to zero, and MLP is a multi-layer perceptron. The\n(1 + \u20ac(k)) term helps the model to distinguish central nodes from their neighbors, while the summation\nprovides a simple yet powerful aggregation scheme that preserves multiset properties. The key distinc-\ntion of GIN is its proven theoretical capacity to capture structural information with maximal discrim-\ninative power among GNNs, making it particularly effective for graph-level tasks where isomorphism\ntesting is important."}, {"title": "4. Methods", "content": ""}, {"title": "4.1. Dataset Preparation", "content": "We utilized the well-known mutagenicity dataset http://doc.ml.tu-berlin.de/\ntoxbenchmark/ compiled by Hansen et al.[24], which originally contained 6,512 compounds, to\nassess and compare the effectiveness of our approach against previous methods. To preprocess and har-\nmonize the molecular data, we stripped explicit hydrogens, detached and discarded any metal ions, and\nkept only the largest fragments of the molecules. To handle duplicate entries with identical canonical\nSMILES strings but differing mutagenicity outcomes, we applied the clear evidence rule. According\nto this rule, when two Ames test results conflict, preference is given to the positive outcome. After\nthis filtering process, a refined set of 6,277 compounds was obtained, comprising 3,388 mutagenic and\n2,889 non-mutagenic compounds. We then divided the dataset randomly, allocating 80% for training\nand validation purposes and the remaining 20% for testing."}, {"title": "4.2. Scattering Transform for Hierarchical Molecular Representation Learning", "content": ""}, {"title": "4.2.1. Geometric Scattering-Based Molecular Featurization", "content": "Molecular graphs are a strong tool for representing the structure of chemical compounds by storing\natoms as nodes and bonds as edges. Each molecular structure is turned into a graph G = (V, E), where\nV represents the set of atoms and E the chemical bonds between them. Individual atom parameters such\nas atomic number, formal charge, aromaticity, hybridization, and valence are incorporated as node fea-\ntures, whereas bond types contribute to the adjacency matrix, which defines the graph's connectedness.\nThese graph-based models are critical for reflecting both local surroundings (such as aromatic rings or\nfunctional groups) and long-range interactions inside the molecule.\nTo generate expressive embeddings, as shown in Figure3a, geometric scattering transforms are ap-\nplied to these molecular graphs.These transforms use wavelet-based filters to capture patterns at various\nscales and layers, increasing the feature space for subsequent tasks such as molecular property predic-\ntion. In our study, we use two complimentary wavelet-based transforms: Tight Hann Frame Wavelets\nand Diffusion Wavelets. Each technique provides a distinct view of the graph, guaranteeing that both\nlocal atomic surroundings and global molecule interactions are reflected in the embedding.\nThe Tight Hann wavelet transform [25] is a spectral graph scattering technique that relies on local-\nized filtering in both the node and frequency domains. The Hann window function is used to create\nwavelets that balance spectral localization and leakage. This approach detects small-scale patterns, such\nas aromatic rings and functional motifs, which are necessary for molecular property prediction.\nIn our approach, as illustrated in table 1, we use one layer of the tight Hann wavelet transform, driven\nby the compact size of molecular graphs. For smaller graphs, a single layer strikes a balance between\ncomputational efficiency and the ability to extract meaningful patterns, such as functional groups and\naromatic rings. To further enrich the structural representation, we apply the wavelet across multiple\nscales. After experimenting with various combinations of layers and scale values, we found that using\nthree scales (j = 3) provided the best results. This multi-scale capability ensures that both local mo-\ntifs and short-range dependencies are captured in the embeddings, making it particularly effective for\ndatasets like ZINC in our case, where molecules exhibit diverse structural patterns.\nThe Diffusion wavelet's structure [23] allows for the extraction of both local and global information,\nallowing the model to identify electron exchange patterns, chemical bonds, and functional connections\nat multiple scales. At this step, we use the diffusion wavelet transform with three layers. Each layer\nrepresents a bigger neighborhood interaction: the first encodes local atomic interactions, the second\ncatches mid-range dependencies, and the third discloses global molecule features. This multi-scale\npropagation preserves and amplifies structurally significant substructures, such as rings, chains, and\nlong-range relationships, in the final embeddings.\nThis dual-wavelet technique is especially useful for molecular property prediction tasks, such as\ntoxicity evaluation and activity prediction, because it takes advantage of both localized motifs and global"}, {"title": "4.2.2. Molecular featurization using scattering 2D", "content": "The 2DWST uses multi-scale wavelet convolutions to successfully capture both local and global\nmolecular characteristics. RDKit converts molecular graphs into 2D images while maintaining impor-\ntant substructures such as aromatic rings, functional groups, and bond configurations. 2DWST has both\ntranslation invariance and deformation robustness, making it ideal for studying the variety of molecular\nstructures present in the dataset ZINC.\nThe 2DWST consists of Morlet wavelet convolutions followed by non-linear modulus operations.\nFirst-order coefficients capture basic chemical properties such as bonds and edges (Figure 3b), but\nsecond-order coefficients encode more sophisticated interactions like ring connectedness. Higher or-\nders provide less energy and are frequently shortened for efficiency. After extensive experimentation,"}]}