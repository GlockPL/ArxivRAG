{"title": "Mechanistic?", "authors": ["Naomi Saphra", "Sarah Wiegreffe"], "abstract": "The rise of the term mechanistic interpretability has accompanied increasing interest in understanding neural models-particularly language models. However, this jargon has also led to a fair amount of confusion. So, what does it mean to be mechanistic? We describe four uses of the term in interpretability research. The most narrow technical definition requires a claim of causality, while a broader technical definition allows for any exploration of a model's internals. However, the term also has a narrow cultural definition describing a cultural movement. To understand this semantic drift, we present a history of the NLP interpretability community and the formation of the separate, parallel mechanistic interpretability community. Finally, we discuss the broad cultural definition-encompassing the entire field of interpretability\u2014and why the traditional NLP interpretability community has come to embrace it. We argue that the polysemy of mechanistic is the product of a critical divide within the interpretability community.", "sections": [{"title": "1 Introduction", "content": "The field of mechanistic interpretability is growing dramatically, constantly motivating new workshops, forums, and guides. And yet, many are unsure what the term mechanistic interpretability entails. Researchers, whether experienced or new to the field, often ask what makes some interpretability research \"mechanistic\u201d (Andreas, 2024; Beniach, 2024; Hanna, 2024; Belinkov et al., 2023). Because both fields study language models (LMs), the distinction between traditional NLP interpretability (NLPI) and mechanistic interpretability (MI) is unclear. In fact, when work is labelled as mechanistic interpretability research, the label may refer to:\n1.  Narrow technical definition: A technical approach to understanding neural networks through their causal mechanisms.\n2.  Broad technical definition: Any research that describes the internals of a model, including its activations or weights.\n3.  Narrow cultural definition: Any research originating from the MI community.\n4.  Broad cultural definition: Any research in the field of AI-especially LM-interpretability.\nExacerbating this confusion, mechanistic interpretability in the narrow cultural definition describes the authors of a paper, rather than their methods or objectives. We must therefore discuss the landscape of the interpretability community itself in order to clarify the usage of mechanistic interpretability.\nTo that end, we will begin by characterizing the narrow technical definition (\u00a72.1) and subsequently explain how the coinage of the term mechanistic interpretability led inevitably to its broad technical definition (\u00a72.2). Both technical definitions characterize subsets of research from the NLPI community, but their work is not always classified as MI, illustrating the term's contextual application.\nIn order to understand how semantic drift eventually gave rise to the cultural definitions, we overview the history of the two distinct communities (NLPI and MI) (\u00a73). We describe how a new movement of AI safety researchers, motivated by philosophical arguments for the importance of interpretability, differentiated themselves as the MI community in its narrow cultural definition (\u00a73.2). The resulting financial and social context of the field now incentivizes NLPI researchers to bridge this gap by embracing the label in its broad cultural definition (\u00a73.3).\nMechanistic is just one example of the imprecise and ambiguous language used in interpretability research. Although clarity is key for distilling and communicating insights about neural networks, we"}, {"title": "2 So what is mechanistic?", "content": "Before the term mechanistic described a cultural movement, NLPI researchers occasionally used the term mechanisms to refer to internal algorithmic implementation (Belinkov, 2018), as suggested by Marr's levels of analysis (Marr and Poggio, 1976). The earliest uses of mechanistic interpretability also draw on this technical meaning, as do most current explicit definitions of the term. What, then, is this precise technical meaning?"}, {"title": "2.1 From causality and psychology to NLP", "content": "Mechanistic interpretability derives its name from causal mechanisms. In a causal model, a causal mechanism is a function-governed by \u201clawlike regularities\" (Little, 2004)\u2014that transforms some subset of model variables (causes) into another subset (outcomes or effects). Causal mechanisms are a necessary component of any causal model explaining an outcome (Halpern and Pearl, 2005a,b).\nThe narrow technical definition of MI thus describes research that discovers causal mechanisms explaining all or some part of the change from neural network input to output at the level of intermediate model representations. For example, one mechanistic interpretation explains how an LM can predict \"B\" from the input sequence \"ABABA\" using induction heads (Olsson et al., 2022): attention heads that search for a previous occurrence of \u201cA\u201d in combination with other heads that attend to the token that follows it. This narrow definition of MI requires causal methods of understanding, but excludes those that do not investigate intermediate neural representations, such as behavioral testing with input-output pairs (e.g., Ribeiro et al., 2020; Xie et al., 2022). It also excludes non-causal methods, such as describing representational structure or correlating activation features with particular inputs and outputs.\nPsychology and philosophy have long stressed the importance of causal mechanisms in explanations. Psychology studies show (Legare and Lombrozo, 2014; Vasilyeva and Lombrozo, 2015) that humans prefer explanations containing causal\""}, {"title": "2.2 The coinage of mechanistic interpretability", "content": "The term mechanistic interpretability was coined by Chris Olah and first publicly used in the Distill.pub Circuits thread, a series of blogposts by OpenAI researchers between March 2020-April 2021. The first post (Olah et al., 2020) set out to \"understand the mechanistic implementations of neurons in terms of their weights.\" After researchers involved in the Circuits thread moved to Anthropic, their subsequent reports (the Transformer Circuits thread; Elhage et al., 2021; Olsson et al., 2022; Hernandez et al., 2022; Elhage"}, {"title": "3 How did we get here? A history of two LM interpretability communities", "content": "Our current terminological confusion results from a historical\u00b2 accident: MI started as a movement with distinct technical objectives in computer vision, but ultimately moved into NLP without engaging the"}, {"title": "3.1 The nascent field of NLP Interpretability", "content": "NLP researchers published focused analyses of linguistic structure in neural models as early as 2016, primarily studying recurrent architectures like LSTMs (Ettinger et al., 2016; Linzen et al., 2016; Li et al., 2016; Hupkes et al., 2017; Ding et al., 2017). The growth of the field, however, also coincided with the adoption of Transformers, which were initially developed for machine translation and constituent parsing (Vaswani et al., 2017) but rapidly dominated rankings across standard NLP tasks (Radford et al., 2018, 2019; Devlin et al., 2019), drawing wide interest in understanding how they worked.\nTo serve the expanding NLPI community, the first BlackBoxNLP workshop (Alishahi et al., 2019) was held in 2018 and immediately became one of the most popular workshops at any ACL conference. Whereas many NLPI researchers had previously struggled to find an audience, ACL implemented an \u201cInterpretability and Analysis\u201d main conference track in 2020 (Lawrence, 2020), reflecting the mainstream success of the field.\nIn many ways, the early NLPI field\u2014which related model behavior to particular components, layers, and geometric properties-would be familiar to anyone in the current MI community. Not only is current research often reinventing their methods and rediscovering their findings (\u00a73.2.2), it is also repeating the same epistemological debates. These debates pit correlation against causation, simple features against complex subnetworks, and expressive mappings against constrained interpretations."}, {"title": "3.1.1 Distributional semantics and representational similarity", "content": "Interest in vector semantics exploded in the NLP community after word2vec (Mikolov et al., 2013a) popularized many approaches to interpreting word embeddings (Mikolov et al., 2013b,c). The en-"}, {"title": "3.1.2 Attention maps", "content": "Attention, originally developed for recurrent machine translation models (Bahdanau et al., 2015), was rapidly adopted across language tasks. Even before the switch to fully attentional Transformers, attention modules offered new avenues of explanation (Bahdanau et al., 2015; Wang et al., 2016). In BERT models, the concurrent discovery of both a correlational (Clark et al., 2019; Htut et al., 2019) and causal (Voita et al., 2019) relationship between syntax and attention demonstrated the case for attention maps as a window into how Transformer LMs handled complex linguistic structure. However, NLPI researchers also identified some limitations of attention for interpretability (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Bibal et al., 2022). Some issues have longstanding solutions, such as incorporating the context of the model when computing attention metrics (Brunner et al., 2020; Kobayashi et al., 2020; Abnar and Zuidema, 2020).\nMI work has continued to attribute specific stereotyped behavior to attention heads (Olsson"}, {"title": "3.1.3 Neuron analysis and localization", "content": "Early works on localizing concepts in NLP often associated individual neurons with sentiment, syntax, bias, or specific token sequences (Radford et al., 2017; Na et al., 2019; Bau et al., 2019; Lakretz et al., 2019; Dalvi et al., 2019; Durrani et al., 2020). Many such studies validated their findings by using causal interventions, though few proposals were causal by design (Sajjad et al., 2022). MI research has largely pursued similar goals of localizing model behaviors to fine-grained model components, including neurons, through its focus on finding \"circuits\": groups of components forming a sub-network that closely (or faithfully) replicate the full model's performance on a fine-grained task (Olah et al., 2020; Wang et al., 2023).\nSingle neuron analysis has been subject to criticism arguing that it is infeasible to reduce large, complex models to the sum of their parts (Antverg and Belinkov, 2022; Sajjad et al., 2022). One core problem is polysemanticity: the observation that a single neuron can activate for multiple disparate classes or concepts (Olah et al., 2020; Mu and Andreas, 2020; Bolukbasi et al., 2021). Not only are these concepts ambiguous, but they can combine nonlinearly according to a sequence's underlying latent structure (Saphra and Lopez, 2020; Csord\u00e1s et al., 2024), making them difficult to disentangle and isolate. MI struggles with many of the same neuron analysis concerns as earlier work, but has taken a particular interest in resolving polysemanticity (Elhage et al., 2022b; Gao et al., 2024). One popular method for this purpose, the sparse autoencoder (SAE) (Bricken et al., 2023; Cunningham et al., 2024), still relies on assumptions of linearity (Park et al., 2024; Millidge, 2023) and naturally emerging feature sparsity (Saphra and Lopez, 2019a; Puccetti et al., 2022; Elhage et al., 2023). Like earlier neuron analysis methods, it also requires expensive causal validation (Mueller et al., 2024)."}, {"title": "3.1.4 Component analysis and probing", "content": "Probes were first applied in NLPI to extract linguistic information from the hidden states of neural models (Ettinger et al., 2016; K\u00e1d\u00e1r et al., 2017; Shi et al., 2016; Adi et al., 2017; Hupkes et al.,"}, {"title": "3.2 The beginnings of mechanistic interpretability", "content": "As NLPI researchers continued investigating language model features and weights, their community and scientific understanding grew rapidly. However, they could not have predicted how the field would grow and change with the infusion of MI researchers into the area. To fully understand the lexical landscape of the NLPI field, we must consider how mechanistic historically came to denote a cultural split from the previous NLPI community in the term's narrow cultural definition."}, {"title": "3.2.1 The historical context of mechanistic", "content": "Though it may be surprising in the modern era of LLM hype, not long ago \u201cmachine learning\" referred primarily to computer vision research. When Saphra (2021) analyzed the proceedings of ICML 2020, they found that over three times as many papers referenced CVPR as any *ACL conference, demonstrating that the language modality was relegated to an application while computer vision results were seen as core machine learning.\nThe presumed unmarked nature of image classification research shaped the landscape of interpretability research as well: In computer vision work at the time, the dominant interpretability method was gradient-based saliency, which highlighted the importance of specific pixels in an input image (Simonyan et al., 2014; Bach et al., 2015; Springenberg et al., 2015; Zintgraf et al., 2017; Ribeiro et al., 2016; Shrikumar et al., 2017). Meanwhile, NLP researchers (and other ML researchers experimenting on text) occasionally borrowed saliency methods from computer vision (Karpathy et al., 2016; Li et al., 2016; Arras et al., 2017), but primarily sought to understand models through representational geometry, attention maps, probing, and causal or correlational neuron analysis-all methods employed by the MI community today.\nWhen Chris Olah first described \"mechanistic interpretability\" in 2020, then, this was the cultural landscape of the ML field: Machine learning mostly meant image classification and interpretability mostly meant feature saliency. Olah has confirmed on multiple occasions (Olah, 2024a,b) that he coined the term to differentiate circuit analysis from saliency methods, which were subject to increasing skepticism at the time (Kindermans et al., 2016; Adebayo et al., 2018; Kindermans et al., 2019; Ghorbani et al., 2019; Heo et al., 2019; Slack et al., 2020; Zhang et al., 2020). The MI paradigm was crucial and novel within computer vision-but the community around it didn't stay in computer vision.\""}, {"title": "3.2.2 Two LM interpretability communities", "content": "As excitement grew around new breakthroughs in NLP and dialogue systems, particularly with the rise of powerful Transformer language models such as GPT-3+ (Brown et al., 2020), many researchers migrated domains. The Circuits thread itself changed focus from vision to language in"}, {"title": "3.2.3 The clash of communities", "content": "The MI community eventually began publishing in academic conferences (Nanda, 2023b; Nanda et al., 2023; Wang et al., 2023). However, new engagement with academia only served to highlight bifurcated norms in the field. Researchers in the NLPI community expressed frustration on social media with the MI community's unfamiliarity with LM interpretability work prior to Anthropic's 2021 Circuits thread. Belinkov (2023a) argued that one paper \"fail[ed] to engage with a large body of work on these topics from the past ~5 years, including direct precedents and improved baselines. Saxon (2023) alluded to a \u201ccontingent of people studying LLMs [who] don't meaningfully engage with *ACL literature.", "not new": "Artzi, 2023) or", "past": "Ravfogel, 2023). Posts often highlighted a tendency to \u201creinvent", "rediscover": "Davidson, 2024) existing tools.\nAnd yet, despite these tensions, the energy and resources of the growing MI community could not be denied. Many NLPI researchers subsequently began to use the term mechanistic interpretability to signal their engagement with the MI conversation (Nanda, 2024a)."}, {"title": "3.3 We are all mechanistic now", "content": "Who wouldn't want to work on mechanistic interpretability? Students need advisors. Funders"}, {"title": "4 Conclusion", "content": "Whatever terminological confusion and ideological tension they have brought to the interpretability field, the MI community is also responsible for its newfound popularity. The interest, energy, and opportunities MI brings to the field cannot be understated, nor should they be taken for granted. NLPI and MI researchers alike are motivated by social responsibility, intellectual curiosity, and the possibility of improving our tools. However, many MI researchers are also members of the alignment community concerned about catastrophic AI risk, where the value of MI is questioned (Greenblatt et al., 2023; Kross, 2023; Segerie, 2023).\nThere may come a time when alignment community consensus turns away from MI. Though many current MI researchers may leave\u2014and some generous resources could disappear-others are likely to continue pursuing our shared objectives. Our communities have too much in common: scientific curiosity and a belief that we should understand the tools we use. We will all continue striving for that objective as long as there are opaque models to understand. Why not, therefore, also aim to connect?"}, {"title": "A understanding language about understanding language models", "content": "The interpretability field struggled with terminological clarity and consensus long before mechanistic entered the lexicon (Doshi-Velez and Kim, 2017; Lipton, 2018; Rudin, 2019; Riedl, 2019; Jacovi and Goldberg, 2020). By even using the word interpretability, we implicitly dismiss the distinction drawn by Rudin (2019) between large \"black-box\" neural models and models which are designed to be understood: particularly, that the latter can be interpreted, but the former only explained.14\nWhile clarity is always important in scientific language, the nature of interpretability research makes it all the more urgent to speak precisely. As a community, we aim to understand the behavior of models and how they work, but how can we shed any light on these inner workings by leveraging confusing jargon? In fact, the ambiguity of mechanistic is emblematic of a wider struggle to communicate interpretability research effectively. Let us consider some other sticking points in the interpretability lexicon. A core part of the \u201cIs attention explanation?", "extractive\" and \"abstractive,\" terms borrowed from the summarization literature, adequately characterize the difference between types of textual explanations.\nIn the MI literature, there have been terminology overloads or semantic disagreements over words like feature and illusion. The term feature has been used to describe mono-semantic concept representations of neurons derived from SAEs (Mueller et al., 2024), though it is more widely and historically associated with vector representations of data (text) that are either manually designed (\u201cfeature engineering": "or learned by neural networks (Bereska and Gavves, 2024). A debate about subspace activation patching has centered around the meaning of the word illusion, namely, whether it applies to any dimension that becomes clearly causally relevant only when its causal role is tested with an intervention (Makelov et al., 2024), or whether such artifacts are a natural\u2014and even explanatory-product of the model's representational geometry, and therefore informative of its true structure (Wu et al., 2024).\nAll of these examples, however, center around the need to ground our empirical work in precise vocabulary-not, like mechanistic interpretability, around the designation of group identity (\u00a73.2.2). Terminological disagreements are usually resolved"}]}