{"title": "Assertify: Utilizing Large Language Models to Generate Assertions for Production Code", "authors": ["Mohammad Jalili Torkamani", "Abhinav Sharma", "Nikita Mehrotra", "Rahul Purandare"], "abstract": "Production assertions are statements embedded in the code to help developers validate their assumptions about the code. They assist developers in debugging, provide valuable documentation, and enhance code comprehension. Current research in this area primarily focuses on assertion generation for unit tests using techniques, such as static analysis and deep learning. While these techniques have shown promise, they fall short when it comes to generating production assertions, which serve a different purpose.\nThis preprint addresses the gap by introducing AssERTIFY, an automated end-to-end tool that leverages Large Language Models (LLMs) and prompt engineering with few-shot learning to generate production assertions. By creating context-rich prompts, the tool emulates the approach developers take when creating production assertions for their code. To evaluate our approach, we compiled a dataset of 2,810 methods by scraping 22 mature Java repositories from GitHub. Our experiments demonstrate the effectiveness of few-shot learning by producing assertions with an average ROUGE-L score of 0.526, indicating reasonably high structural similarity with the assertions written by developers. This research demonstrates the potential of LLMs in automating the generation of production assertions that resemble the original assertions.", "sections": [{"title": "1 INTRODUCTION", "content": "Assertions are executable boolean expressions placed inside the program that must pass (return true) for all correct executions and fail (return false) for all incorrect executions[32]. They are recognized as a powerful tool to detect software errors in software testing and maintenance [16]. Production assertions are assertions that check whether certain conditions are met at runtime. They play an important role in documenting and debugging [29]. By embedding assertions within the code, developers enforce their assumptions about the code's behavior during runtime [42]. This makes it easier for developers and maintainers to grasp the underlying intent and logic behind the code, and use it for program debugging [9] and maintenance [10]. Therefore, their importance in software engineering cannot be overstated, as they contribute to code quality [17] and maintainability [29].\nDespite the importance of these statements, generating effective production assertions remains a challenge. Significant research effort has been devoted to generating assertions for unit tests [7, 8, 33, 36, 42]. These tools are specifically designed for unit test assertions. However, they are not readily applicable to generating production assertions. For instance, TOGA generates test oracles by analyzing the test prefix and the focal method (i.e., the method under test) as a context. A test prefix is a sequence of operations that sets up the initial state of the method under test before the actual test is performed, driving the method to a state where the test can then verify the expected behavior or outcome."}, {"title": "2 MOTIVATION", "content": "Production assertions assist developers in program comprehension and debugging. They provide insights into how statements operate and specify the intended behavior of methods. Additionally, they allow developers to recognize the violated assumptions when a failure is reported in a production environment [28]. Despite the advantages of these programming statements, there is still a significant gap in their usage across open-source repositories. A closer look at GitHub repositories using SourceGraph [6]\u00b2 reveals that many open-source repositories do not incorporate assertions within their methods.\nTo substantiate this claim, we used an assertion-finding regex pattern ^[^(//)*]\\sassert\\s+\\w* and selected the Java programming language and the path src/main/java. Our analysis revealed over 946 repositories had production assertion occurrences when applying the production assertion regex, whereas, without this pattern, that was 38,006 repositories. These findings suggest that only about 2.48% of the repositories had production assertions. This low rate is not surprising since developers often avoid using assertions to save development time, potentially compromising the reliability and maintainability of their code. However, it is imperative to approach these statistics with caution. Typical large-scale Java projects involve numerous methods, sometimes numbering in the hundreds or thousands, making the manual generation of production assertions a burden and, in most cases, impractical. Moreover, it also requires domain-specific knowledge that not all developers possess.\nThese challenges led us to explore a solution for generating production assertions. We designed a novel approach that generates assertions for the given input methods, leveraging the capabilities of LLMs for generating assertions."}, {"title": "3 BACKGROUND", "content": "Our approach draws inspiration from developers' actions to generate assertions. It starts with extracting information blocks from the input and benefits from prompt engineering to use them as context. By intricately weaving contextual information into the input prompts, the LLM is equipped with the essential and nuanced context. This context is required to generate the intended assertions. Compared to the chain-of-thought [38] methodology, which provides an inferential"}, {"title": "3.1 Chain-of-thought prompting", "content": "Chain of Thought (CoT)[38] is an approach in Natural Language Processing (NLP) that enhances the capabilities of LLMs to solve complex problems and perform reasoning tasks. This method involves structuring the problem-solving process into intermediate steps, allowing the model to \"think aloud\" as it navigates through a sequence of logical deductions before arriving at a final answer. By explicitly articulating these steps, CoT empowers LLMs to tackle tasks that require more profound understanding and multi-step reasoning, ranging from arithmetic problems to complex comprehension questions. This methodology improves the transparency of how LLMs reach their conclusions, significantly enhances their ability to handle various tasks, and orients them toward imperative solving procedures [40]. It marked a pivotal shift in the development of AI, opening new pathways for research and application in various domains, including software engineering."}, {"title": "3.2 Few-shot learning", "content": "Few-shot learning [26, 35] is a technique in machine learning, specifically within the context of LLMs. This approach leverages a minimal set of examples to guide LLMs in executing specific tasks, demonstrating how to perform a task with only a few data samples. In contrast to conventional machine learning techniques that demand large datasets for training, few-shot learning leverages LLMs' pre-existing knowledge and flexibility, allowing them to adapt their learned patterns and contextual understanding to novel challenges with minimal explicit guidance. This approach not only simplifies the training process but also highlights the LLMs' capacity for quick learning and their adeptness in handling a wide range of tasks with new information. The implications of few-shot learning for AI development are profound, offering a path to augmenting model capabilities and broadening their use in niche areas without necessitating extensive data collection or training efforts."}, {"title": "4 APPROACH", "content": "Our approach draws inspiration from developers' actions to generate assertions. It starts with extracting information blocks from the input and benefits from prompt engineering to use them as context. By intricately weaving contextual information into the input prompts, the LLM is equipped with the essential and nuanced context. This context is required to generate the intended assertions. Compared to the chain-of-thought [38] methodology, which provides an inferential"}, {"title": "5 IMPLEMENTATION", "content": "ASSERTIFY employs a hybrid approach, integrating Java for core functionalities with Python to perform prompt tokenization, interact with LLM, and compute average ROUGE scores. It uses JavaParser library to parse Java code and utilizes GPT-3.5-turbo-16k (hereafter referred to as GPT-3), GPT-4 model, and GPT-40 model as LLM, aiming to explore their capabilities in generating production assertions. The implementation is divided into three phases:\n\u2022 Repositories Extraction: Utilizing SourceGraph, we collect large-scale open-source targeted Java repositories containing production assertions, followed by filtering them using GitHub APIs to eliminate repositories failing to meet predefined criteria (i.e., number of stars and number of production assertions), and cloning the selected repositories for subsequent analysis. We will talk about the statistics in Section 6 where we detail the data collection operations.\n\u2022 Metadata Creation: We generate a rich JSON metadata file, named methods corpus, cataloging file paths, classes, methods, comments, and assertions, facilitating data retrieval by circumventing the need for full repository scans as any information is needed. This leads to reducing computational overhead and saves time while generating assertions.\n\u2022 ASSERTIFY Pipeline: Our approach, designed as a pipeline with sequential modular components, generates pairs of production assertions and line numbers (at which they should be inserted) and evaluates them using syntactic and static semantic accuracy criteria. All the steps within the pipeline are automatically performed, eliminating the necessity of any manual intervention.\nFor few-shot learning, the Similar Method Extractor component selects the top three similar methods. This choice balances the need for sufficient contextual examples for the LLM with minimizing token consumption to stay within the model's input limits. To compute similarity during prompt engineering, ASSERTIFY vectorizes both the candidate method and each unseen method, padded with zeros to make them equal in size. Subsequently, it applies the cosine similarity formula, sorts the results in descending order, and sets 0.5 as the minimum similarity threshold:\ncosine similarity(A, B) = $\\frac{A \\cdot B}{||A|| \\times ||B||}$"}, {"title": "6 EXPERIMENT DESIGN", "content": "In this section, we explain the experiments performed to answer each research question mentioned in Section 1. We begin by describing the dataset collection process and providing relevant statistics. Next, we discuss the experiment we conducted, and the metrics used to evaluate our results. Finally, we describe how we use each metric to address each research question."}, {"title": "6.1 Dataset Collection", "content": "Our experiments began by collecting 3,819 unique Java repositories from GitHub using SourceGraph, filtered by src/main/java file paths. We applied the regex pattern \u2018^[^(//)*]\\sassert\\s+\\w*' to precisely identify repositories containing at least one production assertion. This approach allowed us to focus on Java projects with production assertions, reducing the need to analyze all repositories.\nThe resultant report file filtered out less mature or less popular projects by requiring at least five hundred stars, narrowing the repository pool to 1,098. We further refined the selection to include only repositories with at least 50 production assertions, resulting in 22 mature and popular repositories with a sufficient number of production assertions. We established these minimum thresholds after analyzing their distribution across all selected repositories, ensuring the inclusion of relatively mature repositories with extensive use of production assertions."}, {"title": "6.2 Experimental Methodology", "content": "We detail the methodology used to evaluate the quality of assertions generated by ASSERTIFY. We ran AsserTIFY with three models (GPT-3, GPT-4, and GPT-40) and five prompt templates, resulting in 15 variations of experiments. Each variation included different contextual information to guide the model's inference, allowing us to determine which was more effective.\n(1) Prompt A: A naive prompt includes only the method name, signature, and body. We will use this prompt as a base and will augment it with additional context.\n(2) Prompt B: Extends A by adding a code summary.\n(3) Prompt C: Extends B by adding input-output descriptions.\n(4) Prompt D: Extends C by adding summaries of invoked methods' code.\n(5) Prompt E: The most comprehensive prompt, extending D by adding similar methods using few-shot learning.\nWe define the following evaluation metrics, to answer each research question:\n\u2022 Syntactic Error (SNE) Rate: This metric measures the proportion of methods with syntactic errors (after inserting inferred assertions at their associated line numbers) that fail to parse successfully, over the total number of the evaluation set. Such errors include any syntactic errors that occur after adding assertions, including missing semicolons, malformed assertions (violating Java grammar), and invalid assertion locations that result in failure in parsing the method. The SNE rate evaluates AsserTIFY in generating assertions that do not comply with Java grammar.\n\u2022 Semantic Error (SME) Rate: This metric evaluates the proportion of methods with static semantic errors, over the total number of the evaluation set. Unlike the SNE rate, SME covers compilation issues beyond parsing, such as scope rules, type checking, naming conflicts, and poorly structured control flow statements. As the method statements remain unchanged after adding assertions, the static semantics should be preserved. This metric does not measure errors that may arise after execution (i.e., no dynamic analysis or execution of the methods is involved within this metric.).\n\u2022 ROUGE Score: We use the ROUGE score [19] as our final evaluation metric. We report the ROUGE-L score which reflects the linguistic structural similarity for the longest common sub-sequence rather than correctness or functionality. We chose this metric over ROUGE-N scores because the tokens can be combined at various levels, making ROUGE-N inappropriate for assessing structural similarity at the expression level rather than the token level. In our context, precision reflects the syntactic relevance of generated assertions, while recall indicates their syntactic completeness compared to original assertions. Higher F\u2081-scores imply greater syntactic similarity and closer linguistic resemblance between generated and original assertions.\nPrecision (P) = $\\frac{overlapping\\ n-grams}{total\\ generated\\ n-grams}$\nRecall (R) = $\\frac{overlapping\\ n-grams}{total\\ reference\\ n-grams}$\nF1 - Score (F) = $\\frac{2PR}{P+R}$\n6.2.1 RQ1: How syntactically accurate are the assertions generated by Assertify? This research question evaluates the syntactical correctness of generated assertions and their insertion line numbers for a given code fragment from the dataset. We use the SNE rate to assess the syntactic accuracy of the method after assertions are inserted. Here, syntactic accuracy refers to the parsability of the code after placing predicted assertions in predicted line numbers. Thus, any syntactically incorrect assertion or mispredicted insertion line that makes the resulting code unparsable will decrease syntactic accuracy and increase the SNE rate.\n6.2.2 RQ2: How accurate are the static semantics of the assertions generated by Assertify? This research question evaluates Assertify's accuracy in generating assertions with correct static semantics. To answer this, we use the SME rate, which involves compiling the entire repository after replacing each method with its updated version. Additionally, we analyze"}, {"title": "7 RESULTS", "content": "This section presents the results of our experiments with five distinct prompts conducted using the GPT-3, GPT-4, and GPT-40 models. We selected these models for their diversity in inference time, cost, and, most importantly, accuracy, allowing developers and researchers to choose the model that best suits their specific needs.\nOverall, the experiments demonstrate the high accuracy of AssERTIFY across all prompts and models, achieving the best when using the GPT-40 model. It also shows the importance of few-shot learning as a key factor in increasing the structural similarity to the original ones. Figures 3 and 5 support this claim, showing that the syntactic accuracy of the generated assertions ranges from 87.5% to 97.4%, depending on the prompt chosen by ASSERTIFY. Regarding static semantic accuracy, results vary between 62.3% and 83.5%. Additionally, the experiments reveal a structural similarity of 52.6%, as measured by the ROUGE-L F\u2081-Score for the latest prompt, E. We also evaluate ASSERTIFY'S performance in terms of response time and inference cost, as illustrated in Figures 9 and 10.\n(1) RQ1: To address this RQ, we refer to Figure 5 to evaluate how well AssERTIFY generates syntactically correct assertions that can be inserted into a given method without breaking parsability. As shown in the figure, the overall syntactic accuracy ranges from 96.0% to 97.4% when using the GPT-40 model, 91.8% to 96.6% for GPT-4, and 88.1% to 90.8% for GPT-3. The tool performs best with"}, {"title": "8 THREATS TO VALIDITY", "content": "Our research represents a preliminary effort to harness the capabilities of LLMs in generating production assertions. However, certain factors may jeopardize the validity of our findings. First, since the ASSERTIFY leverages models' APIs, the quality of responses\u2014such as malformed outputs or hallucinations\u2014is directly tied to the performance of these models. We tried to mitigate such issues through the postprocessor component to filter malformed outputs. Additionally, as ASSERTIFY is designed to work with other LLMs, any internal problems or instabilities in those models, such as overfitting, underfitting, or biased results, could threaten the validity of our findings.\nAnother potential threat arises from the quality of the input datasets, which may affect not only the candidate methods for which ASSERTIFY generates assertions but also the few-shot learning dataset and, consequently, prompt engineering. We tried to mitigate this by selecting mature and popular repositories with some filters, as discussed in Section 6."}, {"title": "9 RELATED WORK", "content": "We categorize related research works into four main areas. These works propose different assertion generation approaches that mainly target unit tests, while ours focuses on production assertions mainly used for comprehension and debugging."}, {"title": "9.1 Dynamic approaches", "content": "Among the most notable contributions in this area is Daikon[11], recognized as a leading tool in dynamic invariant detection. Daikon observes program executions over a test set and monitors variables to infer consistent relationships through statistical analysis and pattern recognition, resulting in identifying likely invariants. It also extends invariant detection to pointer-based data structure and linearizes implicit collections into arrays, making it a powerful research work compared to its counterparts. Another research work in this area is DIG [24], a Dynamic Invariant Generator, analyzing C/C++ program executions to infer polynomial and array invariants using abstract interpretation and constraint solving. While both works have often focused on program execution, our research does not solely rely on execution except for running unit tests. Compared to DIG, ASSERTIFY offers a broader range of assertions beyond polynomials and array invariants. Moreover, switching between various LLMs makes our approach more adaptable and effective."}, {"title": "9.2 Static approaches", "content": "These approaches examine source code over the possible runtime states and generate unit tests accordingly. A common approach within this category is generating the unit tests randomly. Randoop[25] generates random sequences of Java method calls on objects and checks for any errors occurring. If any error happens, a unit test is created by running these invocation sequences. Another technique within this area is search-based testing, which uses efficient meta-heuristic search algorithms for test generation. For instance, Evosuite[12] uses this technique and relies on an evolutionary approach based on a genetic algorithm to generate unit tests, targeting code coverage. A significant weakness and criticism of the static approaches are the low code quality and low understandability of the generated test cases[25]. Moreover, they entirely create unit tests while the ASSERTIFY generates assertions for production code."}, {"title": "9.3 Learning-based approaches", "content": "Neural networks and LLMs have been used to generate testing oracles. Yu and Lou [41] propose an Information Retrieval-based [14] approach that integrates Word2Vec[3] and a bidirectional RNN to enhance the effectiveness of ATLAS[37], a deep learning based approach to generate assertions based on provided unit tests and refine assertion adaptation. It consists of two parts: IR-based assertion retrieval extracts the most similar focal tests using query-based search and adopts their corresponding assertions as output. Retrieved-assertion adaptation evaluates and identifies discrepancies between the tokens, focusing on adapting to align with the test context. However, compared to ASSERTIFY, their approach doesn't check assertions' compilability and only concentrates on adapting existing ones. Wang et al. [34] fine-tuned the GraphCodeBERT model [13] using high-dimensional vectors that represent encoded source code information, along with sequences of testing assertions. Their approach demonstrated promising results in terms of the number and types of correct testing assertions generated, as well as the ability to replicate the exact assert statements written by developers. Schafer and Nadi present an approach called TESTPILOT [31] that employs the Codex model on Mocha [23] framework, having a substantial statement coverage directly through prompts, complemented by an adaptive mechanism to refine failed tests iteratively. Overall, ASSERTIFY differs by handling overloaded functions using metadata, and"}, {"title": "9.4 Code-generation approaches", "content": "This category focuses on leveraging LLMs for code generation and summarizing, making them less adhered to generating assertions. Liventsev and Grishina present Synthesize, Execute, Inspect, Debug, Replace (SEIDR) approach [20] designed for programming tasks defined by textual descriptions and I/O examples. This research enhances program synthesis by integrating debugging for refining generated programs that fail tests, having a synergy between LLMs and an iterative debug cycle to optimize code generation and repair. Regarding code summary generation, MacNeil and Tran explore integrating code explanations generated by LLMs into an interactive e-book for web software development education [21]. This study investigates the engagement and perceived utility of LLMs among students, showcasing the capability of LLMs in generating code summaries. Bhattacharya and Chakraborty evaluate the effectiveness of various LLMs in generating natural-language summaries for code snippets [1]. the study demonstrates that code-specific LLMs outperform generic models using token-based (BLEU scores) and semantics-based metrics (CodeBERT embeddings)."}, {"title": "10 CONCLUSION", "content": "This research presents a novel approach, called ASSERTIFY, using prompt-engineered Large Language Models to automate the generation of production assertions, distinct from past research focused on test assertions. It evaluates the generated assertions' syntactic and static semantic accuracy and compares their structural similarity to assertions written by developers. Our experiments on large, mature Java repositories show ASSERTIFY's effectiveness when using the GPT-40 model. They also show the effectiveness of few-shot learning in increasing the accuracy of the tool when using the GPT-4 and GPT-3 models. In the future, we plan to explore open-source LLMs and extend ASSERTIFY to support additional build tools and programming languages. We also plan to conduct a user study to assess the quality of the generated assertions from the perspective of code maintainers."}, {"title": "11 DATA AVAILABILITY", "content": "All artifacts including the source codes and experiments' results are available online at https://anonymous.4open.science/r/AssertionGenerationArxiv/README.md."}]}