{"title": "Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven Alignment", "authors": ["Zhendong Liu", "Yuanbi Nie", "Yingshui Tan", "Xiangyu Yue", "Qiushi Cui", "Chongjun Wang", "Xiaoyong Zhu", "Bo Zheng"], "abstract": "Benefiting from the powerful capabilities of Large Language Models (LLMs), pre-trained visual encoder models connected to LLMs form Vision Language Models (VLMs). However, recent research shows that the visual modality in VLMs is highly vulnerable, allowing attackers to bypass safety alignment in LLMs through visually transmitted content, launching harmful attacks. To address this challenge, we propose a progressive concept-based alignment strategy, PSA-VLM, which incorporates safety modules as concept bottlenecks to enhance visual modality safety alignment. By aligning model predictions with specific safety concepts, we improve defenses against risky images, enhancing explainability and controllability while minimally impacting general performance. Our method is obtained through two-stage training. The low computational cost of the first stage brings very effective performance improvement, and the fine-tuning of the language model in the second stage further improves the safety performance. Our method achieves state-of-the-art results on popular VLM safety benchmark.", "sections": [{"title": "1. Introduction", "content": "The recent development of large language models (LLMs) has catalyzed progress in multimodal learning by enabling these powerful language models to process information from various modalities. Vision-language models (VLMs), which integrate image and text features, have achieved remarkable performance across tasks such as visual question answering, image captioning, and multimodal reasoning [15, 27, 29, 38]. By leveraging the representational strength of LLMs, VLMs can analyze complex visual and textual information simultaneously, enhancing applications in diverse fields like healthcare, education, and content moderation. However, despite advancements in VLMs, ensuring safety and reliability in these models remains a significant challenge. While LLMS have undergone safety alignment for language-based risks, the visual modality in VLMs has been found particularly vulnerable to bypassing existing safeguards [14, 25].\nResearch indicates that the visual modality can bypass LLMs safety alignments, allowing harmful or inappropriate content to propagate through the model. For example, VLMs may generate explicit, unsafe outputs in response to images containing sensitive or risky content, such as pornography or images depicting discrimination, when these images are paired with prompts designed to circumvent standard safety mechanisms [2, 31]. This issue is especially concerning as multimodal models are increasingly deployed in public-facing applications where inappropriate content could have serious societal implications. Consequently, there is an urgent need to develop effective safety alignment strategies for the visual modality of VLMs, aiming to enhance robustness against a wide range of potential risks.\nWhile some efforts have explored defensive measures for multimodal models, these approaches are often limited in scope or designed to address specific types of attacks, such as adversarial perturbations [43, 44], AI-generated image detection [6], and counterfactual confusion of unsafe content[4].\nHowever, existing defense methods are often designed based on intuition and implemented based on data-driven end-to-end training. The model is still a black box that humans cannot understand and control. Not only that, the high complexity of the model also brings concerns about finding potential shortcomings inside the model. This brings about the need for the model to be explainable and controllable.\nTo address these limitations, our approach leverages the Concept Bottleneck Model (CBM) framework, which offers interpretable, concept-level control over model outputs by incorporating a layer of human-interpretable concepts between input and output [18]. By embedding safety-related concepts directly into the VLM architecture, we create a model that not only identifies unsafe content but also enables dynamic interventions at the concept level, enhancing both safety and control.\nThe CBM framework has shown significant potential in improving model interpretability by enforcing a structured, interpretable layer of high-level concepts that the model must pass through before generating final predictions [18, 34]. CBM enables a two-stage prediction process where raw input data is first mapped to a set of human-specified concepts, which then guide the final output prediction. This structure allows for concept-specific interventions, where users or downstream processes can modify concept predictions to correct or adapt the model's outputs. In high-stakes applications, such as healthcare or autonomous systems, CBM has proven useful by allowing human experts to intervene based on concept-level feedback, which can reduce errors and improve reliability. Inspired by these advantages, we propose PSA-VLM (Progressive Safety Alignment for VLMs), a novel safety alignment approach for the visual modality in VLMs based on the CBM framework.\nOur approach, PSA-VLM, applies a progressive, concept-driven alignment strategy that incorporates safety concepts directly into the model's architecture. Specifically, PSA-VLM adds three core safety modules-Safety Projector, Safety Tokens, and Safety Head-that function as concept bottleneck layers for critical safety-related concepts. These modules work together to monitor, predict, and intervene on safety risks within the visual modality, enhancing model control and interpretability. By structuring safety alignment around high-level safety concepts, PSA-VLM provides a flexible framework for understanding and mitigating risk factors in real-time, allowing interventions that can adapt to new threats or emerging types of unsafe content. We summarize our contributions as follows:\n\u2022 We introduce PSA-VLM, a novel safety alignment method that utilizes concept bottlenecks to enhance interpretability and robustness in VLMs. Our approach structures VLM safety as a concept-driven alignment process, enabling fine-grained control over safety-critical features and allowing users to intervene at the concept level.\n\u2022 We develop a safety-aligned dataset curated from various sources, encompassing a broad spectrum of sensitive categories, including pornography, political symbols, and discriminatory content. This dataset supports the training and evaluation of VLM safety alignment, guiding the model in recognizing high-level safety concepts.\n\u2022 We demonstrate the effectiveness of PSA-VLM using standard VLM benchmarks and customized additional risk data, showing that our method significantly improves safety scores while maintaining general performance.\nThrough PSA-VLM, we aim to establish a new paradigm for VLM safety, aligning model predictions with high-level safety concepts for enhanced explainability and controllability."}, {"title": "2. Method", "content": "In VLMs, safety alignment refers to ensuring that models produce controlled and appropriate responses to multimodal inputs, especially visual inputs that could contain sensitive content. VLMs face specific vulnerabilities in their visual modality, where harmful or inappropriate content can bypass traditional language-based safety mechanisms. To address this, we propose PSA-VLM, a progressive safety alignment method based on the CBM framework. This approach incorporates controllable concept bottlenecks to isolate safety-critical features, enhancing VLM robustness through a layered, concept-driven architecture.\nFormally, let X be the input space of image-text pairs x = (Ximage, Xtext), and let Y be the output space of safety labels and safe responses generated by the LLM. Our objective is to map each input x \u2208 X to an output consisting of a safety label Ylabel \u2208 Vlabel and safe response text Ytext E Vtext using a VLM f : X \u2192 Csafe \u2192 (Vlabel, text) with integrated safety modules, where Csafe represents our safety concepts driven by CBM. This setup involves a two-stage training process that progressively aligns the VLM with safety features: Stage I. Training concept-driven classifiers for text and image modalities to recognize safety risks and extract aligned safety features; Stage II. Fine-tuning the LLM with safety concepts, leveraging these features for robust safety alignment across diverse input types."}, {"title": "2.2. PSA-VLM Architecture Driven by CBM", "content": "To enable controllable safety alignment in VLMS, PSA-VLM leverages the concept bottleneck architecture, with safety modules designed to predict, monitor, and intervene based on safety-critical concepts. These modules serve as intermediaries between raw visual features and the final LLM output, allowing for concept-specific control and interpretability.\nThe PSA-VLM safety modules include:\n1. Safety Projector: Positioned after the visual encoder, this projector extracts safety-oriented concepts from image features, transforming raw features into safety-aligned representations.\n2. Safety Tokens: These trainable tokens signal unsafe visual inputs, aligning the model's attention toward risky content based on concept-specific indicators. It can be understood as an implicit concept whose semantics are incomprehensible.\n3. Safety Head: A cross-attention-based module that further interprets the extracted features, classifying them into defined safety types and levels as explicit concepts.\nThese modules jointly create a concept bottleneck, ensuring that only aligned, concept-driven representations influence the VLM's decision-making, as detailed below."}, {"title": "2.3. Safety Modules in PSA-VLM Architecture", "content": "To explain the PSA-VLM safety modules in greater detail:\nSafety Projector. In VLMs, projectors bridge the visual and language modalities by transforming raw image features into representations compatible with the LLM. Here, the safety projector g\u00f8 isolates high-risk features, enhancing the model's response to potential risks without disrupting the standard projector used for general feature extraction.\nLet ho be the initial visual features extracted by the vision encoder. The original projector f\u00f8 and safety projector go then map these features as follows:\nhi = f(ho), hs = g(ho), (1)\nwhere hi represents the original features, and hs represents the safety-aligned features that convey specific safety concepts to the downstream components.\nSafety Tokens. To embed safety awareness directly within the model, we introduce trainable safety tokens st that categorize visual inputs as safe or unsafe, making it possible to direct the model's focus toward identified safety concepts. These tokens are concatenated with visual features, forming safety-embedded representations:\nhcomb = [s(1); hi], hcomb = [s2); hs], (2)\nwhere s(1) and s(2) are two sets of safety tokens that contribute to visual and safety alignment.\nSafety Head. Leveraging the VLM's native cross-attention capabilities, the safety head identifies both safety types (e.g., pornography, politics) and levels (e.g., high, medium, low risk). This modular head uses a cross-attention mechanism, denoted CA, to generate attention-modulated features:\nhattn = CA(ht, hcomb), (3)\nwhere ht represents the LLM's input embeddings, and homb represents safety-aligned visual features. Safety categories and levels are then predicted through softmax classifiers:\nyj = Softmax(Wjhattn), j\u2208 {t,l}, (4)\nwhere Wt and Wl are weight matrices for safety type and safety level.\nThese combined features serve as input to the LLM during Phase II training, allowing the VLM to align with high-level safety concepts while retaining generalization capabilities."}, {"title": "2.4. Training Strategy for Safety Alignment", "content": "To ensure effective safety alignment, PSA-VLM employs a progressive, two-stage training strategy:\nStage I: Training Safety Modules. The initial stage focuses on extracting and aligning safety concepts using the safety projector, tokens, and head. These components learn to classify and extract safety-aligned features from visual inputs, ensuring the model's response to risky content is consistent. The training loss for safety classification is given by:\nLj = -\\sum_{i=1}^{N} Yj,i log(yj,i), j\u2208 {t,l}, (5)\nwhere Yt,i and Yl,i represent the ground truth safety category and level for each input xi, and yt,i and yl,i are the predicted values. Sample balancing is used to address data imbalance in safety classes.\nStage II: Fine-Tuning the LLM with Safety Concepts. In this phase, the LLM is unfreezed and trained alongside the safety modules, aligning it with the safety-specific concepts learned in Stage I. This phase reinforces the model's understanding of safety-aligned features, captured by the loss function:\nLLLM = -\\sum_{i=1}^{N} [yi log (LLM (Xi, St))], (6)\nwhere yi is the true label for language modeling, LLM represents the language model parameterized by . The total loss in Stage I is Ls + Li + LLLM, while in Stage II, it is focused on LLLM."}, {"title": "2.5. Inference with Concept-Driven Safety Control", "content": "During inference, the model leverages the outputs of the safety head for controllable safety intervention. Conditional processing of text is achieved through prompts and safety control codes, using the following formalism:\np(S|ct, Ci) = p(S|Prompt, ct) \u00b7 p(Prompt|ct)\n\u00b7p(S|Prompt, c\u2081)\u00b7p(Prompt|c\u0131), (7)\nwhere S represents the safety embeddings used by the LLM, Ct denotes safety type, ci denotes safety level, and Prompt conditions the safety intervention. This setup allows PSA-VLM to dynamically adjust responses based on safety levels, providing nuanced control. This controllable structure enables the VLM to respond adaptively to different types and levels of risk, supporting more flexible safety management across diverse application scenarios."}, {"title": "2.6. Dataset Construction Details", "content": "Harmful data is diverse and complex in real-world scenarios, not limited to single sources, types, or modalities. To address this, we have collected multiple datasets. We manually categorize the risky images into 6 types and 3 levels to achieve classification and grading of risk control. Moreover, we reconstruct a relatively balanced dataset through sampling, containing about 11,000 pairs of risky images and text queries. Since the Red Teaming Visual Language Models (RTVLM) benchmark does not have a default training and testing set division, we randomly divide 80% of the data as the training set and 20% as the testing set. For other risk sources, such as the porn dataset, we sample 200 images as the testing set for scoring to manage evaluation costs.\nTo avoid performance degradation during SFT, we include the LLaVA and COCO datasets as clean samples. Drawing from LLM safety-related works, we find the ratio of clean to unclean samples crucial. In stage I, we experiment with varying clean sample sizes (1,000 to 40,000) and observe that around 3,000 clean samples, close to the number of risk types, yield optimal risk recognition accuracy. Increasing clean data beyond this point reduces classification accuracy due to data imbalance, offering insight into selecting effective multimodal unsafe data ratios. For more details of the dataset, please refer to the Supplementary Material."}, {"title": "3. Experiments", "content": "Model. For simplicity in structure, our safety alignment experiments are primarily based on the LLaVA model [27, 28], as the LLaVA series employs straightforward linear layers to connect the vision encoder with LLMs. For more models results, please refer to the Supplementary Material. In addition, we select various models for safety performance comparison, including Fuyu-8B [3], VisualGLM [9, 11], Qwen-VL [1], InternLM-XComposer2 [10], Llama-3-vision-alpha [37], VLGuard [49], and GPT-4V [36]. For training and fine-tuning parameters, please also refer to the Supplementary Material for further details.\nDataset. For the evaluation of safety performance, our collected unsafe dataset cover six categories: politics, illegal risk, insults and bullying, fairness, privacy, and misleading content. For each category, we implement different safety grading strategies and labeling policies. For the safety dataset used for fine-tuning, we employ an open-source dataset from ShareGPT4V [7], including LLaVA and COCO datasets.\nMetrics. We evaluate VLM performance from two aspects, including safety performance and general domain performance.\n\u2022 Safety Performance. To ensure a fair comparison, we first evaluate our model using the RTVLM benchmark and a GPT-4-based approach as introduced in [24]. Since this dataset is limited and does not encompass sensitive data, we extend our evaluation to include additional risk datasets focused on harmful politics, pornography, and cyberbullying. We conduct further evaluations incorporating GPT-4 and subjective assessments from human experts to provide a comprehensive understanding. For prompt strategies and details on human evaluators, please refer to the Supplementary Material.\n\u2022 General Performance. For the evaluation of our model's performance in general scenarios, we primarily use several benchmarks including MMBench [32], SEEDBench [20, 21], and MME [12].\nComputing resources. Our experiments were run on NVIDIA A100 or equivalent GPUs. For Stage I, we used 4 GPUs for about 1 hour. For the fine-tuning of the language model in Stage II, we used 8 GPUs for about 8 hours. As shown in Table 1, the benefits of Stage 1 are significant, and for most cases we don't even need to fine-tune the language model to achieve satisfactory performance."}, {"title": "3.2. Safety Performance", "content": "RTVLM Benchmark. We conduct an analysis of the evaluative scores by GPT-4 across different dimensions of VLMs using the RTVLM benchmark, including four distinct categories for a nuanced understanding of the model's safety capabilities. As demonstrated in Table 1, we evaluate various open-source VLMs alongside GPT-4V and our PSA-VLM. The results show that while GPT-4V performs well across various categories, particularly in safety domains like captcha and jailbreak scenarios, it is InternLM-XComposer2 that stands out in several metrics. InternLM-XComposer2 achieves the highest scores in visual misleading (8.61) and order (8.51 and 8.67), highlighting its superior ability to handle complex visual and textual interpretations securely and fairly. The PSA-VLM also exhibits robust performances, especially when utilizing LoRA to unfreeze the LLM, which achieves the highest score of 8.36 in politics and 8.43 in racial. Regarding average score, PSA-VLM-7B (+LoRA) stands out with a leading score of 8.26, closely followed by PSA-VLM without unfreezing the LLM at 8.18. Notably, the 13B model with LoRA achieves the highest average score of 8.46. This indicates the significant impact of our safety alignment strategy on enhancing the LLM's safety performance across various categories. In contrast, Fuyu-8B and VisualGLM-6B show weaker performance. It is noteworthy that the LLaVA-v1.5-7B and LLaVA-v1.5-13B models exhibit similar performance levels when compared, despite their difference in size. The enhanced safety scores of PSA-VLM compared to other VLMs highlight the effectiveness of the two-stage safety alignment strategy with three additional safety modules. Furthermore, using LoRA to unfreeze the LLM also contributes to improving safety performance. The safety scores with the error bar of PSA-VLM-7B (+LoRA) are shown in in the Supplementary Material.\nRisk Datasets. The RTVLM dataset does not include other risky and sensitive data such as cyberbullying. Therefore, we conduct experiments on other risk datasets to evaluate the safety performance of the PSA-VLM. As shown in Table 2, PSA-VLM-13B achieves the best performance with the score of 9.49, 8.72, and 7.45 for harmful political, porn content, and cyberbullying detection, significantly outperforming the baseline model LLaVA-v1.5-13B, which scores 6.67, 1.11, and 6.16. Although using LoRA to unfreeze the PSA-VLM-7B sees a slight decrease to 8.91 and 6.82, it still represents a marked improvement over LLaVA-v1.5-7B. Figure 4 (a) shows the distinction in features of unsafe images across both safety levels and safe types, comparing original features with those processed through the safe projector. Upon the application of the safe projector, a notable segregation into distinct clusters is observed. This indicates that PSA-VLM is highly reliable and effective in accurately identifying and classifying different types of risks. For classification metrics, including accuracy and F1-score for both safety level and safety type classification, Figure 4 (b) shows that the PSA-VLM demonstrates high performance across all categories."}, {"title": "3.3. Multimodal Benchmark Results", "content": "The improvement in safety performance does not come at the cost of general performance. Despite the enhanced safety measures, PSA-VLM-7B maintains competitive performance on general benchmarks like MMbench, SEED-Bench, and MME. As shown in Table 3, PSA-VLM-7B demonstrates improvements on general benchmark MMBench and SEEDBench, achieving scores of 68.5 and 65.3 respectively, indicating better general performance. Moreover, during the evaluation of the multimodal benchmark, PSA-VLM-7B effectively identifies and refuses to respond to several potential risk images, demonstrating its heightened sensitivity to potential unsafety and underscoring the effectiveness of our safety alignment method. The images deemed unsafe are filtered out, allowing us to evaluate general performance using strictly clean data. This approach reveals a noticeable improvement in the performance of MMBench, SEEDBench, and MME. This responsiveness to unsafe content reflects PSA-VLM-7B's robust safety performance without detracting from its overall performance capabilities."}, {"title": "3.4. Ablation Study", "content": "In the ablation study for PSA-VLM-7B, we examine the specific impacts of the safety head and the safety tokens on model performance in various aspects. The baseline model scored 7.59, 6.97, 1.51, and 6.34 on the RTVLM, politics, porn, and cyberbullying datasets, respectively, establishing a performance baseline for the model. Introducing the safety head leads to not only an improvement in the RTVLM score to 8.09, but also significant gains in the politics, porn, and cyberbullying datasets, scoring 8.73, 7.64, and 7.15 respectively. This demonstrates the safety head's substantial enhancement of the model's discriminatory and filtering capabilities for unsafe and risky content. On the other hand, the introduction of only safety tokens results in a modest increase in the RTVLM score to 7.63, while gains in other tasks are minimal, which may have contributed to slight improvements in safety performance. Finally, the configuration that includes both the safety head and the safety tokens achieves the highest score of 8.26 on the RTVLM benchmark, suggesting that their combination can complement each other to some extent, collectively enhancing the model's safety performance in several aspects. In summary, the safety head is a core component in improving the safety performance of the PSA-VLM-7B, while safety tokens serve as a beneficial supplement. When applied together, they can further enhance the overall safety performance."}, {"title": "4. Related Work", "content": "To ensure the safety of VLMs and prevent the display of inappropriate content during user interactions, researchers have explored a variety of defense mechanisms. Techniques like image safeguarding [4], which leverage an external ResNet model as an unsafe classifier to guide Q-former training and use interpretable methods to label unsafe areas, have been developed on the foundation of BLIP-2 [23]. Other researchers have focused on defending against jailbreak attacks by exploiting the intuition that attack samples, typically being meticulously crafted, are inherently non-robust to transformations, thus advocating for variant consistency [13]. Defense and detection efforts have also employed prompt tuning techniques, leveraging adversarial prompt tuning for VLMs [43] and AntifakePrompt for fake image detection [6]. Additionally, some studies have utilized red teaming datasets for Supervised Fine-Tuning (SFT) to achieve safety alignment [24]. VLGurad [49] cleverly constructs a dataset to achieve efficient safety alignment, but does not cover enough risk level and type of image content. Existing works tend to focus on detecting and defending against attacks within specific domains, often lacking a unified approach to address the myriad of complex attacks encountered in the real world or providing insufficient granularity and categorization in their defense mechanisms. Our work advances this field by offering customizable grading for a variety of unsafe input content."}, {"title": "4.1. Vision Language Models (VLMs)", "content": "The rapid development and potent generalization capabilities of existing LLMs have enabled researchers to integrate various modalities into LLMs, giving rise to multimodal language models. Notable examples of VLMs include BLIP [22, 23], LLaVA [27, 28], and Qwen-VL [1], InternVL [10], etc. Furthermore, researchers have ventured beyond by incorporating additional modalities like audio and video in models such as One-LLM [15] and Meta Transformer [45]. These models facilitate multimodal dialogues between users and LLMs rather than relying solely on linguistic modalities. They often share a similar architecture that connects a encoder to LLM via projection methods. Additionally, models like the BLIP series and One-LLM have introduced extra trainable tokens. However, despite widespread research into multimodal language models, the architecture of existing multimodal language models can often be circumvented by other modalities, bypassing LLM's safety alignment."}, {"title": "4.2. Attack on VLMs", "content": "With the swift progression of VLMs, a plethora of attack mechanisms targeting VLMs through the visual modality have emerged. Some studies have extended adversarial attacks to VLMs, illustrating how adversarial images can manipulate generative models at runtime and evaluating the adversarial robustness of VLMs through minor perturbations [2, 40, 47]. Other researchers have engaged in jailbreak attacks and backdoor attacks through the visual modality [14, 25]. There's also a growing body of work dedicated to building datasets and benchmarks for evaluating these threats [24, 40, 47]. Our work covers a wide range of unsafe data types including jailbreak attacks, explicit content, and politically sensitive data, etc."}, {"title": "4.3. Safety and Attack Defense of VLMs", "content": "4.1. Vision Language Models (VLMs)\n4.2. Attack on VLMs"}, {"title": "5. Limitation", "content": "PSA-VLM's visual safety alignment strategy shows resilience to attacks but may be less effective against sophisticated adversarial attacks. Additionally, during test-time inference without human involvement, PSA-VLM occasionally identified non-threatening data as risky and decided not to answer, thus displaying false positives in its safety filters. [39] proposed a dataset for evaluating whether language models have exaggerated safety behaviors. We think this is very effective, but the benchmark is mainly designed for language models. Due to limited resources, we lack similar datasets designed specifically for visual language models, so whether there is exaggerated safety behavior in scenarios with visual modality input still requires subsequent careful evaluation."}, {"title": "6. Conclusion", "content": "To improve the inherent vulnerability of the visual modality in VLMs, we introduce a concept-based safety alignment strategy that encompasses a safety projector, safety tokens, and a designated safety head. The experimental results indicate that PSA-VLM has surpassed GPT-4V in terms of safety benchmarks RTVLM. As a summary, our method achieves a score of 8.26 on the 7B model and 8.46 on the 14B model when using the RTVLM benchmark. We also achieved competitive scores when using pornography, politics, and cyberbullying benchmarks. Notably, while achieving improved safety performance, the model also maintains a high level of general performance. In addition, the transparency of high-level concepts during inference enhances the explainability and controllability of the model.\nThe enhanced safety of VLMs could lead to a more trustworthy VLM-using environment. By mitigating the risks of visual deception and manipulation, PSA-VLM helps ensuring that VLM systems are less likely to be used for harmful purposes, such as spreading disinformation or malicious content. The increased safety can foster greater user confidence in VLM systems."}, {"title": "Broader Impact and Ethics Statement", "content": "Broader Impact Statement: The proposed progressive concept-based alignment strategy for VLMs is designed to address multiple ethical and safety concerns, including biases, explicit content, and political sensitivity. By reducing discrimination, stereotypes, and the potential for harmful or misleading outputs, this approach enhances VLMs' safety and reliability, particularly in sensitive areas like healthcare and legal assistance, thereby lowering the risk of severe errors in high-stakes fields. Additionally, the strategy improves transparency and accountability, fostering trust in AI-driven decision-making processes.\nEthics Statement: Our concept-based alignment strategy for VLMs is developed with a strong commitment to ethical standards, focusing on reducing risks associated with bias, explicit content, political sensitivity, and cyberbullying. By addressing fairness, privacy, and safety, we aim to minimize discrimination and harmful outputs, particularly in sensitive fields such as healthcare and legal services. We encourage responsible use of this technology, with an emphasis on transparency and adaptability to diverse application needs."}, {"title": "Supplementary Material", "content": "Considering the relative simplicity of the model structure, controllable parameter volume, and the comparability of experimental results, we primarily utilize LLaVA-1.5-7B [27] as the base model for our experiments during the model un-freezing and fine-tuning stage. The parameters used during the training stage are as shown in Table 5. For parameters not mentioned, we adopted the default values in the code. In stage I, we mainly trained the safety module. In stage II, to save computational resources, we follow parameter-efficient approaches and apply LoRA [16] to all the linear layers in the language model. When using LoRA, we set r = 256, \u03b1 = 16, and dropout = 0.05. Throughout all training stages, we use 8 NVIDIA 80GB A100 GPUs for training. Stage I requires approximately 1 hour, while stage II, needing more clean samples for a general capability guarantee, takes about 8 hours. During the inference stage, if not considering the length of the generated text, the additional computational overhead of the safety module can be neglected, as the vast majority of computational expenses still come from text generation by LLMs."}, {"title": "Dataset Details", "content": "Existing unsafe data often suffers from issues like single source, few types, or single modality. For instance, some datasets only contain pornographic data, some only contain images, while others only include text. To address the complex safety challenges in real-world scenarios, we collect multiple datasets. The sources of the data can be found in Table 6. The majority of the image data is open-source and can be directly downloaded, whereas the cyberbullying and porn datasets require application access. For politically sensitive data, due to legal regulations and the unsafe and sensitive nature of the data, we cannot publish them on public platforms. Access with restrictions on no secondary distribution through application and registration is necessary. Of course, this type of data is not essential in most academic research contexts.\nTo achieve classification and grading of risk control, we manually categorize the risky images into 6 types and 3 levels. For datasets containing only images, we complete the text labels using GPT-4 generated or manually designed templates for different categories and contents of risk. Moreover, due to the distribution imbalance of unsafe data, we reconstruct a relatively balanced dataset through sampling, containing about 11,000 pairs of risky images and text queries. Since the RTVLM benchmark does not have a default training and testing set division, we randomly divide 80% of the data as the training set and 20% as the testing set. For larger datasets, such as the porn dataset, considering evaluation costs, we sample 200 images as the testing set for scoring based on GPT-4 and human evaluation.\nTo avoid performance degradation during SFT, we additionally include the LLaVA and COCO datasets as clean sample datasets. Based on the experience from LLMs' safety-related work, we believe that the ratio of clean to unclean samples is important. We experiment with different ratios at Stage I and their impacts on model capabilities, as shown in Figure 6, trying clean data ranging from 1,000 to 40,000. We find that at around 3,000 clean samples, close to the number of various risk types, the accuracy of risk content recognition appears better. As the amount of clean data increases, the classification accuracy shows a downward trend, which is intuitive, as it introduces data imbalance issues. This provides effective insights on how to select the ratio of multimodal unsafe data.\nAs shown in the evaluation on the multimodal benchmarks, the general performance of our model demonstrates a cautious approach by identifying and declining to respond to data categorized as having potential risk. However, we acknowledge that not all data identified by the model as risky are actually harmful, indicating the presence of false positives of the model's safety filtering strategy, particularly in MME datasets. To address this issue and improve general performance, we adjust the filtering conditions. According to Table 7 and Table 8, categories such as posters, celebrities, text translation, and code reasoning prove to be most affected by the initial filtering settings. Figure 5 presents the potential risky images filtered by the PSA-VLM. The model has categorized tasks related to code reasoning, text translation, and numerical calculation as illegal risk content like jailbreak activities. Moreover, tasks involving celebrities have been selected out because their image features are similar to those that typically raise privacy concerns. Posters have been recognized as deceptive advertising, likely to mislead users, and artworks containing nudity have been labeled as pornographic or sexually explicit content. Though the mistaken filtering will lead to a decline in general performance, according to Table 9, to maintain a balance between safeguarding against security risks and ensuring the availability of common ability, PSA-VLM employs a set of 3000 clean samples."}, {"title": "Safety Performance based on Different VLM Architectures.", "content": "To demonstrate the versatility and robustness of our safety alignment method, we evaluate its effectiveness across different VLM architectures. In addition to testing on LLaVA, we extend our experiments to MiniGPT-4, ensuring that our method generalizes across varying architectural designs. The results, presented in Table 10, highlight the safety performance metrics across several sensitive content categories, including Politics, Pornography, Cyberbullying, and RTVLM (Red Teaming VLM).\nAs shown, both LLaVA and MiniGPT-4 architectures improve safety performance after applying our alignment method. Specifically, LLaVA models, such as Vicuna-v1.5-13B and Vicuna-v1.5-13B-LoRA, show notable increases in handling sensitive content, particularly in the Politics and Porn categories, with scores reaching as high as 9.49 and 8.72, respectively. These results suggest that the larger model capacity and fine-tuning through LoRA enhance safety alignment capabilities.\nOn the other hand, MiniGPT-4 models also demonstrate strong safety performance. For instance, the Blip-2 with Llama-2-chat-7B showed a balanced performance across all categories, with a particularly strong score in Porn (8.79). Although MiniGPT-4 with Vicuna-13B showed slightly lower performance in comparison to LLaVA on some metrics, it still manage to maintain an overall high level of safety alignment, emphasizing the effectiveness of our method across different VLM setups.\nThese findings underscore the flexibility of our safety alignment approach, affirming its applicability to a wide range of VLM architectures while ensuring consistent improvements in content safety management."}, {"title": "Implementation Details of the Method", "content": "In the implementation of the safety module, we introduce 64 additional safety tokens, each with a dimension of 4096. Notably, there are two independent sets of these safety token modules. Furthermore, in the safety projector part, we employ a projector from Honeybee [5], aiming to efficiently extract localized features. Subsequently, we utilize 8-head multi-head attention as a cross-attention module, where the query comprises text features, and the key and value are both conditionally rewrite the text input to adapt it to the unsafe image input. This method of rewriting is not unique and can be either manually designed or learned through model training. To better showcase the rewriting process, we manually craft some prompts based on existing datasets and integrate these prompts into the queries to complete the rewriting task. For other model details like the vocabulary, special tokens, system prompts, etc., we follow the settings of LLaVA-1.5-7B. You can find the algorithm in Algorithm 1."}, {"title": "Experiment Statistical Significance", "content": "Considering the stability and reliability of experimental results, we conduct the training and evaluation of the model with the best safety performance three times, and the results are shown in Figure 7. As can be seen, our model demonstrates high safety stability across the majority of types, with performance improvements due to random effects being nearly zero."}]}