{"title": "Bridge-IF: Learning Inverse Protein Folding with Markov Bridges", "authors": ["Yiheng Zhu", "Jialu Wu", "Qiuyi Li", "Jiahuan Yan", "Mingze Yin", "Wei Wu", "Mingyang Li", "Jieping Ye", "Zheng Wang*", "Jian Wu1,4,6*"], "abstract": "Inverse protein folding is a fundamental task in computational protein design, which aims to design protein sequences that fold into the desired backbone structures. While the development of machine learning algorithms for this task has seen significant success, the prevailing approaches, which predominantly employ a discriminative formulation, frequently encounter the error accumulation issue and often fail to capture the extensive variety of plausible sequences. To fill these gaps, we propose Bridge-IF, a generative diffusion bridge model for inverse folding, which is designed to learn the probabilistic dependency between the distributions of backbone structures and protein sequences. Specifically, we harness an expressive structure encoder to propose a discrete, informative prior derived from structures, and establish a Markov bridge to connect this prior with native sequences. During the inference stage, Bridge-IF progressively refines the prior sequence, culminating in a more plausible design. Moreover, we introduce a reparameterization perspective on Markov bridge models, from which we derive a simplified loss function that facilitates more effective training. We also modulate protein language models (PLMs) with structural conditions to precisely approximate the Markov bridge process, thereby significantly enhancing generation performance while maintaining parameter-efficient training. Extensive experiments on well-established benchmarks demonstrate that Bridge-IF predominantly surpasses existing baselines in sequence recovery and excels in the design of plausible proteins with high foldability. The code is available at https://github.com/violet-sto/Bridge-IF.", "sections": [{"title": "1 Introduction", "content": "Proteins are 3D folded linear chains of amino acids that execute the myriad of biological processes fundamental to life, such as catalysing metabolic reactions, mediating immune responses, and responding to stimuli [23]. Designing protein sequences that fold into desired 3D structures, known as inverse protein folding, is a crucial task with great potential for applications in protein engineering [29, 66, 5]. Beyond long-established physics-based methods like Rosetta [2], the considerable promise of leveraging geometric deep learning for protein structure modeling has given rise to an ongoing paradigm. This paradigm is centered on deciphering the principles of protein design directly from data and on predicting sequences corresponding to specific structures [25, 27, 6, 22].\nDespite substantial advancements, most existing approaches follow a discriminative formulation for learning inverse folding [58], consequently encountering two principal obstacles: (i) Error accumulation issue. For instance, Transformer-based autoregressive models are constrained by their inherent sequential generation process and exposure bias, which prevents them from correcting preceding erroneous predictions. (ii) One-to-many mapping nature of the inverse folding problem. A multitude of distinct amino acid sequences possess the capability to fold into an identical protein backbone structure, a phenomenon exemplified by homologous proteins. Discriminative models are incapable of capturing the one-to-many mapping from the protein structure to non-unique sequences, thereby facing difficulties in covering the broad spectrum of plausible solutions [58].\nRecent studies have advanced the iterative refinement strategy to optimize the previously generated results, aiming to reduce prediction errors [64, 14, 42]. These approaches employ a refinement module to identify and correct inaccurately predicted amino acids. However, as the number of refinement iterations grows, managing the intermediate stages effectively becomes more challenging, potentially hindering sustained performance gains.\nDiffusion-based generative models [45, 17], particularly their discrete extensions [3], which offer a structured iterative refinement process with probabilistic interpretation, appear to be a promising solution. GraDe-IF [58] is a pioneer in investigating diffusion models for inverse folding, leveraging the backbone structure to guide the denoising process on the amino acid residues. However, as diffusion models are designed to learn a single intractable data distribution, the prior distribution utilized by GraDe-IF is restricted to a simple noise distribution (i.e., a uniform distribution across all residue types), which has little or no information about the distribution of native sequences. It remains unclear whether this default formulation best suits conditional generative problems such as inverse protein folding, where the backbone structures provide significantly more information than random noise. Thus, an exciting research question naturally arises: Can we propose a more strong and informative prior based on desired backbone structures to enhance the quality of samples and accelerate the inference process?\nIn this work, we propose Bridge-IF, a novel generative diffusion bridge model for inverse folding. Its core design is aimed at generating protein sequences from a structure-aware prior. As shown"}, {"title": "2 Related work", "content": "Recently, AI algorithms have spurred a revolution in modeling protein folding [28, 34]. Meanwhile, the inverse problem of protein folding, which aims to infer an amino acid sequence that will fold into the desired structure, is gaining increasing attention [6]. By representing protein backbone structures as a k-NN graph, geometric deep learning has achieved remarkable progress in learning inverse folding [25, 6, 22], surpassing traditional physics-based approaches [2], and even facilitating the design of a range of experimentally validated proteins [6, 56]. Modern deep learning-based inverse folding approaches typically comprise a structure encoder and a sequence decoder. Depending on their decoding strategies, these approaches can be classified into three categories: autoregressive models, one-shot models, and iterative models. Most methods adopt the autoregressive decoding scheme to generate amino acid sequences [25, 6, 22]. Given that autoregressive models tend to have low inference speed, some researchers have investigated one-shot methods that facilitate the parallel generation of multiple tokens [12, 38]. Since directly predicting highly plausible sequences is challenging, some works have shifted their attention to iterative refinement [64, 14, 26, 42, 58]. For instance, LM-Design [64] and KW-design [14] utilize the pre-trained knowledge from PLMs to reconstruct a native sequence from a corrupted version. The Potts model-based ChromaDesign [26] and CarbonDesign [42] employ iterative sampling techniques, including Markov chain Monte Carlo, to design protein sequences. GraDe-IF [58] further leverages the principles of discrete denoising diffusion probabilistic models [3], demonstrating a strong capacity to encompass diverse plausible solutions. In this work, we present the first generative diffusion bridge model for inverse folding."}, {"title": "2.2 Diffusion models", "content": "Diffusion-based generative models [45, 17] have showcased remarkable successes in a wide range of applications, ranging from image synthesis [8], audio synthesis [31], to video generation [18]. Generally, the essential idea behind these models is to define a forward diffusion process that gradually transforms the data into a simple prior distribution and learn a reverse denoising process to gradually recover original data samples from the prior distribution. While most existing methods are designed for modeling continuous data, a few efforts have extended diffusion models to discrete data domains [3, 33, 52, 36]. Recently, diffusion models have also found utility in scientific discovery [55], particularly in protein design [56, 59, 1, 15, 58]."}, {"title": "2.3 Schr\u00f6dinger bridge problem", "content": "The Schr\u00f6dinger bridge (SB) problem is a classical entropy-regularized optimal transport problem [43, 32, 4]. Given a data distribution, a prior distribution, and a reference stochastic process between them, solving the SB problem amounts to finding the closest process to the reference in terms of Kullback-Leibler divergence on path spaces. This concept exhibits fundamental similarities to diffusion models [47], particularly in the field of unconditional generative modeling [49, 54, 7, 44], where the prior distribution assumes the form of Gaussian noise. Notably, SB formalism offers a general framework for approximating the reference stochastic process by training on coupled samples from two continuous distributions [19, 46, 35, 65]. The recently proposed Markov bridge [10, 24] has broadened the scope of the SB, enabling it to model categorical distributions. In this work, we present the first diffusion bridge model for inverse protein folding."}, {"title": "3 Background", "content": "Generally, a protein can be represented as a pair of amino acid sequence and structure (y, s), where Y = [y1, y2, ..., yn] denotes its sequence of n residues with $y_i \\in \\{1, 2, ..., 20\\}$ indicating the type of the i-th residue, and s = [s1, s2,..., sn] $\\in \\mathbb{R}^{n \\times 4 \\times 3}$ denotes its structure with si representing the Cartesian coordinates of the i-th residue's backbone atoms (i.e., N, C-\u03b1, and C, with O optionally). The inverse protein folding problem aims to automatically identify the protein sequence y that can fold into the given structure s. Given that homologous proteins invariably exhibit similar structures, the solution for a given structure is not unique [16]. Hence, an ideal model, parameterized by \u03b8, should be capable of learning the underlying mapping from protein backbone structures to their corresponding sequence distributions p\u03b8(y|s)."}, {"title": "3.2 Markov bridge models", "content": "Markov bridge model [24] is a general framework for learning the probabilistic dependency between two intractable discrete-valued distributions px and py. For a pair of samples (x, y) ~ px,y(x, y), it defines a Markov process pinned to fixed start and end points $z_0 = x$ and $z_T = y$ through a sequence of random variables $(z_t)_{t=0}^T$ that satisfies the Markov property,\n$\\mathbb{P}(Z_t \\vert Z_0, Z_1,..., Z_{t-1},Y) = p(z_t\\vert Z_{t-1},Y)$. (1)\nTo pin the process at the end point zT = y, we have an additional requirement,\n$\\mathbb{P}(z_T = Y\\vert z_{T-1}, Y) = 1$. (2)\nAssuming that both px and py are categorical distributions with a finite sample space {1, . . ., K}, we can represent data points as one-hot vectors: x, y, zt \u2208 {0, 1}K, and define the transition probabilities (Equation 1) as follows,\n$\\mathbb{P}(z_{t+1}\\vert z_t, y) = Cat\\left(z_{t+1};Q_t z_t\\right)$, (3)\nwhere Cat(; p) is a categorical distribution with probabilities given by p, and Qt is a transition matrix parameterized as\n$Q_t := Q_t(y) = \\beta_t I_k + (1-\\beta_t)y1_k$, (4)\nwhere \u03b2t is a schedule parameter transitioning from \u03b21 = 1 to \u03b2T\u22121 = 0. It is easy to see that zt can be efficiently sampled from $\\mathbb{P}(Z_{t+1}|Z_0, Z_T) = Cat\\left(z_{t+1};\\mathcal{Q}_t^+z_0\\right)$ with a cumulative product matrix $\\mathcal{Q}_t = Q_tQ_{t-1} \\dots Q_0 = \\beta_{\\dagger}I_K + (1-\\beta_{\\dagger})y1_k$, where $\\beta_{\\dagger} = \\prod_{t=0}^T \\beta_t$.\nUsing the finite set of coupled samples $\\{(x_i, y_i)\\}_{i=1}^N \\sim p_{x,y}$, Markov bridge model learns to sample y when only x is available by approximating y with a neural network \u03c6\u03b8:\n$\\hat{y} = \\varphi_\\theta(z_t, t)$, (5)\nand defining an approximated transition kernel,\n$q_\\theta(z_{t+1}| z_t) = Cat\\left(z_{t+1};Q_t(\\hat{y})z_t\\right)$. (6)\n\u03b8 is trained by optimizing the variational bound on negative log-likelihood log q\u03b8(y|x), which has the following closed-form expression,\n$- \\log q_\\theta(y|x) \\leq T\\cdot \\mathbb{E}_{t \\sim \\mathcal{U}(0,...,T-1)} \\mathbb{E}_{z_t\\sim \\mathbb{P}(z_t|x,y)} \\mathbb{D}_{KL} \\left(\\mathbb{P}(z_{t+1}|z_t, y) || q_\\theta(z_{t+1}|z_t)\\right)$. (7)"}, {"title": "4 Methods", "content": "In this section, we introduce Bridge-IF, a Markov bridge-based model for inverse protein folding. Figure 1 shows an overview of our proposed Bridge-IF. Due to space limitation, we present the detailed algorithm in Appendix A. To begin, we describe how to extend Markov bridge techniques to facilitate the inverse protein folding task. Next, we propose a simplified training objective. Finally, we elucidate how to modulate pre-trained PLMs with structural conditions to approximate the Markov bridge process."}, {"title": "4.1 Overview of Bridge-IF", "content": "We frame the inverse protein folding problem as a generative problem of modeling a stochastic process between the distributions of backbone structures ps(s) and protein sequences py(y). As previously discussed, diffusion bridge models, with their general properties of an unrestricted prior form, serves as an ideal substitution for diffusion models in the presence of a well-defined informative prior. Regrettably, to the best of our knowledge, no existing method can directly model the dependency between two distinct types of distributions: specifically, the continuous source distribution of backbone structures and the discrete target distribution of protein sequences.\nTo reconcile the differences between source and target distributions and streamline the modeling process, we propose introducing a discrete proposal distribution to serve as a deterministic prior. We parameterize the proposal distribution using a structure encoder $\\mathcal{E}: S \\rightarrow X$ that is supervised by ground-truth target sequences. Recent advancements have demonstrated that an expressive encoder is capable of directly predicting pretty good protein sequences in a one-shot manner [12]. This approach enables us to utilize structural information more effectively, rather than simply employing it to guide the denoising process as in previous diffusion-based methods like GraDe-IF [58]. In this work, we will take the discriminative model PiFold [12] as the structure encoder to produce a clean and deterministic prior x = E(s). Upon this deterministic mapping from structure to sequence, we simplify the originally complex problem of modeling p(s, y) into the more tractable problem of modeling p(x, y). Then, we build a Markov bridge [10, 24] between the prior sequence and the native sequence to model the stochastic process, leading to a data-to-data process. As depicted in the lower half of Figure 1, each sampling step progressively refines the prior sequence, which contains significant information about the target sequence, ultimately resulting in a more precise prediction.\nRecall that the Markov bridge models are typically trained by optimizing the variational bound on negative log-likelihood log q\u03b8(y|x) (Equation 7), which is analytically complicated and hard to optimize in practice [63, 62]. Therefore, we here propose a reparameterization perspective on Markov bridge models, deriving a simplified loss function for easier optimization (\u00a74.2).\nWe build Markov bridges in the sequence space, treating the sequence representation as a set of independent categorical random variables. To model the Markov bridge process, Qt is applied separately to each residue within a protein sequence. Motivated by the impressive advancements in PLMs for understanding and generating proteins [9, 34, 37, 60], we advocate for employing PLMs to approximate the Markov bridge process. This approach capitalizes on the emergent evolutionary knowledge of proteins, learned from an extensive dataset of protein sequences. Additionally, we utilize the latent structural features extracted by the structure encoder to prompt PLMs, thereby guiding the generation of structurally coherent proteins. Formally, the final state of the Markov bridge process is approximated by $\\hat{y} = \\varphi_\\theta(z_t, s, t)$, foregoing the use of Equation 5. We investigate the integration of conditional information, such as timestep and structure, into PLMs, focusing on preserving their emergent knowledge and achieving parameter-efficient training (\u00a74.3)."}, {"title": "4.2 Reparameterized Markov bridge models", "content": "Inspired by the similarities between Markov bridge models [24] and discrete diffusion models [3, 63], we propose a reparameterization of the Markov bridge model characterized in \u00a73.2 to enable more effective training. With the reparameterization trick, we introduce a latent binary random variable"}, {"title": "4.3 Network architecture design space", "content": "We adopt pre-trained PLMs as the base network to approximate the final state of the Markov bridge process. Typically, PLMs exclusively take protein sequences as input during the pre-training stage, making it non-trivial to integrate timestep and structural conditions into the PLMs. Hence, we innovatively tailor the Transformer blocks [50] to effectively capture timestep and structural information, as depicted in Figure 2. To facilitate efficient training, the architecture of our model is delicately designed for compatibility with the pre-trained weights. Our exposition emphasizes fundamental principles and the corresponding modifications to the base network."}, {"title": "4.3.1 AdaLN-Bias", "content": "Inspired by DiT [41], we explore replacing standard layer norm layers in transformer blocks with adaptive layer norm (adaLN) to modulate the normalization's output based on both the timestep of the Markov bridge process and the backbone structure. The key idea is to regress the dimension-wise scale and shift parameters \u03b3 and \u03b2 of the layer norm from the sum of the timestep embedding and the pooled structure representation. In our situation, meaningful pre-trained parameters \u03b3 and \u03b2 are readily accessible. Upon commencing the fine-tuning stage, it is crucial that these parameters are close to the pre-trained values to preserve the effectiveness of the original model, since a poor initialization could significantly deteriorate performance. For simplicity, we propose to predict bias \u2206\u03b3 and \u2206\u03b2 on the frozen original scalars and initialize the multi-layer perception (MLP) to output the zero-vector for all \u2206\u03b3 and \u2206\u03b2. We term the proposed variant of adaLN as adaLN-Bias."}, {"title": "4.3.2 Structural adapter", "content": "Considering that the pooled structure representation might only retain coarse-grained information, the network could consequently lack a detailed understanding of the structure input and necessitate information derived from original structural features to compensate. We incorporate a multi-head cross-attention module to the transformer block, enabling the network to flexibly interact with the structural features extracted from the structure encoder [64]. To facilitate pre-trained weights, we further integrate it into a bottleneck adapter layer [21] with residual connection, preserving the input for the subsequent layers.\nWe stress that we freeze all pre-trained parameters of the base network during training."}, {"title": "5 Experiments", "content": "In this section, we first demonstrate the effectiveness of our Bridge-IF on the standard CATH benchmark [40]. Next, we assess Bridge-IF for its applicability in de novo protein design. Moreover, we conduct several ablation studies to empirically justify the key design choices. Further results pertaining to the design of multi-chain protein complexes can be found in Appendix B.1."}, {"title": "5.1 Experimental protocol", "content": "We conduct experiments on both CATH v4.2 and CATH v4.3, where proteins are categorized based on the CATH hierarchical classification of protein structure, to ensure a comprehensive analysis. Following the standard data splitting provided by Ingraham et al. [25], CATH v4.2 dataset consists of 18,024 proteins for training, 608 proteins for validation, and 1,120 proteins for testing. Following the standard data splitting provided by Hsu et al. [22], CATH v4.3 dataset consists of 16,153 proteins for training, 1,457 proteins for validation, and 1,797 proteins for testing. For a fair comparison with iterative models [64, 14], we use pre-trained PiFold [12] to propose the prior distribution. We use the cosine schedule [39] with number of timestep T = 25. The model is trained up to 50 epochs by default on an NVIDIA 3090. We used the same training"}, {"title": "5.2 Inverse folding", "content": "The performance of Bridge-IF, compared to competitive baselines, is summarized in Table 1. Bridge-IF demonstrates superior performance over previous methods. We highlight the following: (1) Iterative models comprehensively surpass the previously dominant autoregressive and one-shot methods. (2) Our Bridge-IF outperforms LM-Design and KW-Design with the same pre-trained PLMs, supporting our hypothesis that the iterative refinement process should be modeled in a probabilistic framework. (3) Compared with diffusion-based GraDe-IF, our Bridge-IF achieves better performance with fewer diffusion steps (25 vs. 500), demonstrating that our bridge-based formulation can better leverage the structural prior.\nFollowing Zheng et al. [64], we also study the impact of the scale of PLMs on CATH v4.3. We use ESM-2 series, with parameters ranging from 8M to 3B. As depicted in Figure 3, the performance of Bridge-IF improves with model scaling, exhibiting a distinct scaling law in logarithmic scale. Using ESM-2 at the same scale, we observe that Bridge-IF consistently obtains greater enhancements relative to LM-Design. Besides, Bridge-IF does not exhibit any performance degradation, even when the smallest model (i.e, ESM-2 8M) is employed. Remarkably, the largest ESM2-3B-based variant of Bridge-IF attains a record-setting recovery rate of 61.27% on CATH v4.3."}, {"title": "5.3 Foldability", "content": "While perplexity and recovery rate serve as effective proxy metrics, it is imperative to recognize that these measurements may not accurately reflect the foldability of the designed protein sequences in real-world scenarios [58, 13, 53]. Given that wet-lab assessment is extremely costly, we leverage the in silico structure prediction model ESMFold [34], to evaluate whether our designs can adhere to the structure condition. Here we assess the agreement of the native structures with the predicted structures using the TM-score [61], and follow the evaluation configurations as in Wang et al. [53]. Specifically, we use the small, high-quality test set of 82 samples curated by Wang et al. [53] and randomly generate 100 sequences for each structure."}, {"title": "5.4 De novo protein design", "content": "Thus far, our experiments have been limited to accurate experimentally-determined structures. However, in real-world applications like de novo protein design, inverse folding models are commonly used to design sequences for novel structures generated by backbone generation models [56, 26]. Consequently, we next evaluate Bridge-IF for its potential in such a scenario. The experimental methodology is detailed as follows: we sample 10 backbones at every length [100, 105, . . ., 500] in intervals of 5 using Chroma [26]. For each de novo structure, we employ inverse folding models to design 8 sequences. Subsequently, these sequences are folded using ESMFold to identify the sequence with the highest TM-score (scTM). We compare Bridge-IF with ProteinMPNN [6], which is widely used in de novo protein design [59, 57]. Our results show that Bridge-IF surpasses ProteinMPNN in terms of scTM (0.73 vs. 0.69) and designability (0.85 vs. 0.80), using scTM > 0.5 as the criterion."}, {"title": "5.5 Ablation Studies", "content": "We conduct ablation experiments on CATH v4.2 to verify the impact of key design choices, and present the results in Table 3."}, {"title": "5.5.1 Prior", "content": "We investigate two training strategies distinguished by their prior: 1) the structure encoder and the PLM are jointly trained; 2) the structure encoder is first pre-trained and remains frozen during the subsequent training of the PLM. We noted that the structure encoder is trained with an equivalent objective in both strategies. The latter consistently yields higher-quality protein sequences. Hence, it has been established as our default configuration."}, {"title": "5.5.2 Training objective", "content": "We find that the proposed simplified cross-entropy loss works better than the variational lower bound loss [24], demonstrating that the inferior performance of the vanilla Markov bridge model may stem from a harder optimization."}, {"title": "5.5.3 Network architecture", "content": "We observe that the performance of Bridge-IF further increases (57.92% \u2192 58.59%) when we replace the vanilla AdaLN with the proposed variant AdaLN-Bias. We highlight the use of AdaLN-Bias to enhance compatibility with pre-trained parameters when modulating a pre-trained Transformer model with additional conditions."}, {"title": "6 Conclusion", "content": "In this work, we introduce Bridge-IF, the first diffusion bridge model based on the Markov bridge process for inverse protein folding. Bridge-IF can gradually generate high-quality protein sequences from a deterministic prior. Bridge-IF achieves state-of-the-art performance in sequence recovery and foldability. Future work will focus on investigating more advanced structural encoders [38] and pre-training Bridge-IF using more protein structure data predicted by AlphaFold2 [28] to further enhance performance. We also intend to apply Bridge-IF to guide protein engineering aimed at designing novel functional proteins. One potential limitation of the proposed Bridge-IF is its lack of validation through wet-lab experiments in practical applications."}, {"title": "B Additional results", "content": null}, {"title": "B.1 Multi-chain protein complex design", "content": "Studying protein sequence design for multi-chain assemble structures is crucial for drug design. Next, we assess the capabilities of designing multi-chain complexes using the PDB dataset curated by Dauparas et al. [6], where sequences were clustered at 30% identity, resulting in 25,361 clusters. Following the standard data splitting, we divided those clusters randomly into three groups for training (23,358), validation (1,464), ensuring that neither the chains from the target chain nor the chains from the biounits of the target chain would be present in the other two groups.\nAs shown in Table 4, Bridge-IF also achieves similar improvements when extending to the PDB dataset, further validating its effectiveness and generalizability.\nThese results show that Bridge-IF can not only design single-chain proteins, which are mostly studied in previous works but also be used for designing multi-chain protein complexes."}, {"title": "C Derivations for the variational bound of reparameterized Markov bridge models", "content": "We derive the variational bound on negative log-likelihood log q\u03b8(y|x) as discussed in Section 4.2.\n- log q\u03b8(y|x) = log q\u03b8(zT|z0)\n= - log $\\int$ q\u03b8(z1:T, v1:T|z0) dv1:T dz1:T-1\n= - log $\\int$ $\\frac{\\mathbb{P}(z_{1:T}, u_{1:T}|z_0, z_T)}{q_\\theta(z_{1:T}, u_{1:T}|z_0)} q_\\theta(z_{1:T}, u_{1:T}|z_0)$ dv1:T dz1:T-1\n= - log $\\int$ $\\frac{\\mathbb{P}(z_{1:T}, u_{1:T}|z_0, z_T)}{q_\\theta(z_{1:T}, u_{1:T}|z_0)} q_\\theta(z_{1:T}, u_{1:T}|z_0)$ dv1:T dz1:T-1\n$\\leq$ - $\\int$ $\\mathbb{P}(z_{1:T}, u_{1:T}|z_0, z_T)$ log $\\frac{q_\\theta(z_{1:T}, u_{1:T}|z_0)}{\\mathbb{P}(z_{1:T}, u_{1:T}|z_0, z_T)}$ dv1:T dz1:T-1\n= T \u00b7 $\\mathbb{E}_{t\\sim\\mathcal{U}(0,...,T-1)}$ Lt(\u03b8)\nwhere\nLt(\u03b8)\n= $\\mathbb{E}_{p(z_t|x,y)}$ [$\\mathbb{E}_{p(v_t)}$ [KL(p(zt+1|ut, zt, zT)||q\u03b8(zt+1|ut, zt))] + KL (p(vt)||q\u03b8(vt))] .\nWe adopt the simplifying assumption that q\u03b8(vt) = p(vt), then Lt(\u03b8) can be written as\nLt(\u03b8) = $\\mathbb{E}_{p(z_t|x,y)p(v_t)}$[KL(p(zt+1|ut, zt, zT)||q\u03b8(zt+1|ut, zt))], (11)\nin which the KL divergence has the form\nKL[p(zt+1|ut, zt, zT)||q\u03b8(zt+1|ut, zt))] =$\\begin{cases}\n (1 \u2013 \u03b2t)KL(y||q\u03b8(zt, t)) & \\text{if $v_t = 1$ } \\\\\n KL(zt||zt) = 0 & \\text{if $v_t = 0$} \n\\end{cases}$ (12)\nGiven that KL(y||q\u03b8(zt, t)) = \u2212y log y\u03b8 (zt, t), we have\n$\\mathbb{E}_{p(v_t)}$ [KL [(p(zt+1|ut, zt, zT)||q\u03b8(zt+1|ut, zt))]]\n= -(1 - \u03b2t)vty log y\u03b8 (zt, t)"}, {"title": "D Broader impacts", "content": "Inverse protein folding models, operating within the broader realm of bioinformatics and computational biology, have significant impacts across various scientific and practical domains. These models, by enabling the design or prediction of protein sequences that fold into specific three-dimensional structures, foster advancements in numerous fields. The broader impacts encompass several areas, including drug discovery, enzyme design, and synthetic biology."}]}