{"title": "PROACTIVE AGENT: SHIFTING LLM AGENTS FROM REACTIVE RESPONSES TO ACTIVE ASSISTANCE", "authors": ["Yaxi Lu", "Shenzhi Yang", "Cheng Qian", "Guirong Chen", "Qinyu Luo", "Yesai Wu", "Huadong Wang", "Xin Cong", "Zhong Zhang", "Yankai Lin", "Weiwen Liu", "Yasheng Wang", "Zhiyuan Liu", "Fangming Liu", "Maosong Sun"], "abstract": "Agents powered by large language models have shown remarkable abilities in solving complex tasks. However, most agent systems remain reactive, limiting their effectiveness in scenarios requiring foresight and autonomous decision-making. In this paper, we tackle the challenge of developing proactive agents capable of anticipating and initiating tasks without explicit human instructions. We propose a novel data-driven approach for this problem. Firstly, we collect real-world human activities to generate proactive task predictions. These predictions are then labeled by human annotators as either accepted or rejected. The labeled data is used to train a reward model that simulates human judgment and serves as an automatic evaluator of the proactiveness of LLM agents. Building on this, we develop a comprehensive data generation pipeline to create a diverse dataset, ProactiveBench, containing 6, 790 events. Finally, we demonstrate that fine-tuning models with the proposed ProactiveBench can significantly elicit the proactiveness of LLM agents. Experimental results show that our fine-tuned model achieves an F1-Score of 66.47% in proactively offering assistance, outperforming all open-source and close-source models. These results highlight the potential of our method in creating more proactive and effective agent systems, paving the way for future advancements in human-agent collaboration.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "2 Related Works", "content": "Recent advancement in large language models [14, 15, 12, 16] has shown great progress in complex reasoning, task planning [17, 18, 19, 20, 21, 22, 9], tool utilization [23, 24, 25, 26], etc. Consequently, a growing number of agent systems have been developed to utilize these models to tackle diverse tasks like automatic web search [27], software development [28, 2], behavior simulation [29].\nDespite these advancements, current agents predominantly operate reactively, meaning they passively accept human instructions and lack the context awareness [11] to proactively satisfy user needs. This limitation is particularly pronounced in scenarios that require intensive human-agent interactions, such as coding or writing. These reactive agents typically wait for explicit user commands, which can lead to inefficiencies and increased cognitive load for the user as the task complexity grows. In response to this challenge, we propose a novel dataset named ProactiveBench, designed to evaluate and improve the proactiveness of the language models. Unlike existing works that enable multi-turn interactions to better understand ambiguous instructions in the context [30, 31, 32, 33], we focus on anticipating the possible tasks, which can reduce the user's cognitive burden of providing explicit instructions and promote more effective assistance."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Task Definition", "content": "In our proposed proactive agent, which is distinct from traditional agent systems powered by large language models that rely on explicit user instructions, we investigate a new scenario where the agent autonomously predicts tasks users might assign, aiming to offer assistance proactively, as depicted in Figure 1. The proactive agent's mission is to give predictions based on the user's activities $A_t$, environmental events $E_t$, and state $S_t$, which can be formalized as:\n$P_t = f_\\theta(E_t, A_t, S_t),$\\nwhere $f_\\theta$ represents the proactive agent, parameterized by $\\theta$, and $P_t$ denotes the prediction about possible task at time $t$. It should be noticed that the prediction $P_t$ can be the predicted task or nothing if the agent believes that no task is needed. Specifically, user activities $A_t$ contain the user's interactions with the environment and the agent, like keyboard input or chatting with the agent. Environmental events $E_t$ contain the event that the proactive agent captured, ranging from receiving a new email to an application closed. Environmental state $S_t$ represents the state of the current environment, like the file system state or the content of opened web pages.\nIn our proactive agent framework, the objective is to maximize the user's acceptance rate of the proposed tasks. Given the user's historical activities $A_t$, current environmental state $S_t$, and the prediction proposed by the proactive agent $P_t$, the user makes a binary decision:\n$R_t = g(P_t, A_t, S_t),$\\nwhere $R_t$ is a binary variable indicating acceptance ($R_t = 1$) or rejection ($R_t = 0$) of the prediction. To unify the handling of cases where the prediction $P_t$ contains no task and where it contains a task, we introduce an auxiliary variable $N_t$ that indicates whether the user needs assistance:\n\u2022 $N_t = 1$ if the user needs assistance.\n\u2022 $N_t = 0$ if the user does not need assistance.\nThe user's acceptance $R_t$ is then defined as:\n$R_t = \\begin{cases}\n1 & \\text{if } (P_t \\neq \\emptyset \\text{ and user accepts } P_t) \\text{ or } (P_t = \\emptyset \\text{ and } N_t = 0) \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nIn this way, if the prediction $P_t$ contains no task (i.e., the agent believes the user does not need assistance), we check the user's actual need for assistance $N_t$. If the user indeed does not require assistance ($N_t = 0$), this is marked as acceptance ($R_t = 1$). Conversely, if the user requires assistance ($N_t = 1$), this is marked as rejection ($R_t = 0$). Our proactive agent aims to maximize the expected acceptance rate of the proposed tasks:\n$\\max_{\\theta} E[R_t].$"}, {"title": "3.2 Pipeline Overview", "content": ""}, {"title": "3.3 Environment Gym", "content": "Event Collection To improve the quality of events generated by the environment gym, we first collect real-world events as reference. We developed a monitor software based on Activity Watcher\u00b2, which allows us to capture the details of user interactions with computer systems, including keyboard and mouse operations, visited web pages, and used development tools. To enhance the semantic richness of the collected data and facilitate parsing by large language models, we further merge the raw data into logically coherent segments. Additionally, we utilize a language model to translate the structured data into more natural textual descriptions. This process not only improves the interpretability of the data but also makes it more suitable for subsequent usage.\nScenario Generation After collecting reference events, rather than directly generating specific events, we generate a realistic interactive scenario to provide sufficient background information first for further generation. To build such scenarios, we first prompt GPT-40 [34] with the seed jobs collected from human annotators to create various jobs the user might perform under a specific category, like coding, writing, or daily life. Then, we generate all possible entities that the tasks might involve, e.g. browser, software, and tools for the agent to perform tasks. Next, we refine the scenario by adding more details like entity status or date time to improve the details. Finally, the collected events are also provided to generate example events under each particular context for future events generation. This allows us to control the granularity of events that will be generated and maintain the diversity of the scenarios.\nEvent Generation When it comes to specific event generation, we start with user activity generation. For each scenario, the user agent is first requested to describe its activities and actions $A_t$ at time $t$ to complete the job in the simulated environment. Then, the gym accepts the user's activities and actions to generate detailed events one by one. As depicted in Figure 2, the gym is tasked to generate logically correct and fluent events according to historical events and the current environmental state. The key to improving the realities of the events generated and adapting to different environments is utilizing the example events we generated based on collected events during scenario generation. Before generating events, we randomly sample the generated example events for the specific scenario and request the gym to produce new events according to them. Once a new event $E_{t+1}$ is generated, the gym updates the entities' status in the environments and repeats the process until there are no events that can be generated with the provided user activities. This comprehensive approach ensures that each subsequent event is not only appropriate but also contributes to a coherent and logical progression within the scenario.\nState Maintenance Another important functionality of the gym is maintaining the state of the environment $S_t$. During the scenario generation, the gym creates entities like browsers or development kits in the simulated environment, where each entity has its state and properties like the application version or the specific browser name. When a new event is generated, the gym should update the states and properties of them to provide feedback for further event generation. Specifically, we first gather historical state changes of related entities and prompt the GPT-40 to generate new states"}, {"title": "3.4 Proactive Agent", "content": "The second component in our data generation pipeline is the proactive agent that predicts tasks the user might assign. As detailed in Figure 3, upon the agent receiving new event $E_t$ at time $t$, it first updates its memory with the event. To improve the quality of the prediction, it also accepts feedback from the user agent on its draft prediction. Combining new events with historical ones and conversations with the user, the agent incorporates user characteristics to raise potential tasks. If the agent detects potential tasks, it will raise the task as a new event and wait for the judgment of the user agent. Otherwise, the proactive agent predicts no potential tasks and stays silent. Once the predicted task is accepted, the agent will execute the task within the gym, which generates multiple events about the interaction between the agent and the environment. During the data generation, the agent would constantly receive events from the Gym and predict potential tasks."}, {"title": "Task Execution", "content": "As mentioned before, the proactive agent executes the predicted task once the user accepts. This process is mainly done through multi-turn interaction between the proactive agent and the gym. Specifically, the proactive agent will be provided with the tools generated during the scenario generation, like file system tools in the computer or access to the smart light switch, to interact with the simulated environment. Each time the proactive agent takes an action, the gym will generate a new event, which is further processed by the gym and the user agent to update the environment state. After that, the proactive agent detects the new environment state $S_{t+1}$ and takes new actions according to the events generated by the gym. This process ends when the user interrupts or the proactive agent finishes its tasks."}, {"title": "3.5 User Agent", "content": "The user agent is designed to emulate users' activities $A_t$ and responses about the agent's prediction $P_t$, as illustrated in Figure 2. We prompt GPT-40 to generate activities and actions for the provided task in the specific environment. The gym further processes the activities and actions to generate a new event. Then the proactive agent predicts potential tasks according to the events. Upon receiving the predicted task, the user agent determines whether to accept or reject it. If the user agent accepts the task, the proactive agent will set up and execute the accepted task within the environment gym. Otherwise, if the user agent declines the suggested assistance, the environment gym generates new events autonomously without any interventions. In our settings, we collect judgment from human annotators and train a reward model to simulate the judgment.\nSpecifically, to ensure the reward model aligns closely with human judgment, we generate and annotate a dedicated dataset to indicate whether humans would accept the predicted task or not. We utilize 9 different language models to generate diverse predictions for each event. Between these predictions, we select 5 predictions with minimum total cosine distance as our label target. Each prediction is annotated with one of three options by three separate annotators: accept, reject, or reject all. The reject all option is chosen when the annotator believes that the given events did not imply any possible tasks that the user might assign, aka $N_t = 0$ in Section 3.1. Otherwise, if one prediction is labeled as accepted, we label the event $E_t$ with $N_t = 1$. We use majority voting to make the final decision on each prediction. After all, the annotation results in a dataset of 1, 760 entries, each containing event traces, task predictions, and decisions on accepting the predicted task from three distinct annotators. Notably, our annotators achieve an impressive consistency rate of over 91.67% on the test set, underscoring the annotations' reliability and the dataset's robustness for further analysis. To further facilitate automatic data generation, we also prompt the GPT-40 to produce a more detailed explanation of the user judgment."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Reward Model Assessment", "content": "To automatically evaluate whether the predicted tasks and their timing are appropriate, we seek to train a reward model capable of accurately imitating user judgments. To this end, we apply the user-annotated data to train LLaMA-3.1-8B-Instruct [12] and compare it with several baselines to show its superiority.\nSetting. We use the 1, 760 entries with human annotations and randomly split them into a training set (1, 640 entries) and a test set (120 entries). We then train LLaMA-3.1-8B-Instruct on the training set to obtain our reward model. We employ a total batch size of 32, a learning rate of 1e 5, and an Adam Optimizer with a 0.01 warm-up ratio. We train the reward model for 3 epochs to prevent it from over-fitting. We use 8 A100 GPUs on one node to train for approximately 1.5 hours. The detailed prompt template is listed in Appendix A. We use the test split to evaluate our adapted model and all the baselines. To be noticed, our human annotators achieve up to 91.67% agreement ratio on the test set, demonstrating the effectiveness of our evaluation.\nMetrics. We use the reward model to perform binary classification on whether to accept predicted tasks and compare its results with human-annotated results. This assesses how well the reward model aligns with human judgment regarding the suitability of the predicted tasks. We compare the judgments made by the reward models and humans to calculate the Recall, Precision, Accuracy, and F1-Score. Additionally, we calculate the agreement ratio for the following cases:\n\u2022 Missed-Needed: $N_t = 1, P_t = \\emptyset$, the user needs help, but the agent does not provide it.\n\u2022 Non-Response: $N_t = 0, P_t = \\emptyset$, the user does not need help, the agent does not prompt.\n\u2022 Correct-Detection: $N_t = 1, P_t \\neq \\emptyset$, and the user accepts the task predicted by the agent.\n\u2022 False-Detection: $N_t = 0, P_t \\neq \\emptyset$, the user does not need help but agent prompts.\nResults. As Table 2 shows, all existing models perform well on correct detection, but perform badly in other scenarios, especially in the false alarm scenario. After a deeper analysis, we find that existing models just can not infer what the user might need and tend to accept arbitrary help, even if it is very abstract or meaningless to current observation. In contrast, our reward model achieves a 100% agreement ratio on false alarm scenario and a solid 91.80% F1-Score across all scenarios. We select our reward model for further analysis across the ProactiveBench."}, {"title": "4.2 Proactive Agent Evaluation", "content": "Setting. We use the training set of ProactiveBench to obtain the Proactive Agent based on the two open-source models: LLaMA-3.1-8B-Instruct and Qwen2-7B-Instruct. During training, we employ a total batch size of 32, a learning rate of 1e 5, and an Adam Optimizer with a 0.01 warm-up ratio. We train the model for 3 epochs. We use 8 A100 GPUs on one node to train for approximately 2 hours. The detailed prompt can be found in Appendix B. The automatic evaluation of these metrics relies on the simulated judgment given by the reward model. All models are evaluated in our"}, {"title": "4.3 Ablation Study", "content": "Predict Multiple Tasks. When it comes to real-world applications, the proactive agent can provide multiple candidate tasks to improve overall performance. To evaluate how models perform under this condition, we allow them to generate"}, {"title": "4.4 Case Study", "content": "In this section, we explore two prevalent types of failures encountered in predicting possible tasks: the inability to detect user needs and making predictions at inappropriate times.\nAs illustrated in Figure 4 (left), a notable failure occurs when the GPT-40 model does not assist at crucial moments. For instance, when a user is engaged in integrating a payment API and requires a tutorial for guidance, the model remains silent. Instead, our model successfully detects human needs and offers assistance. The underlying intention to minimize disruptions ironically leads to missed opportunities to offer timely help.\nConversely, the right side of Figure 4 showcases an instance of ill-timed prediction. Here, the GPT-40-mini suggests an action when there are no user needs showing in events. This scenario underscores the possible unintended events existing in human activities. The model should judge whether there are possible tasks smartly to avoid unnecessary actions. These instances highlight the intricate nature of human activities and the sophisticated reasoning required for models to accurately predict human needs. To navigate the delicate balance between being helpful and intrusive, models must develop a deeper understanding of user contexts and activities, ensuring their interventions are both timely and pertinent."}, {"title": "5 Conclusion", "content": "We present an innovative approach to human-agent interaction by leveraging proactive task predictions that anticipate human needs. We introduce ProactiveBench, a comprehensive dataset comprising 6, 790 events, designed to refine the proactive behavior of LLM-based agents and establish an automatic benchmark for assessing model proactiveness. By iteratively generating events in synthesized scenarios, we create training data that enhances the proactive capabilities of our models. Our experiments demonstrate significant improvements in the agent's performance on ProactiveBench, validating the effectiveness of our methods. Despite these advancements, our findings underscore ongoing challenges, particularly in minimizing inappropriate task proposals and ensuring task predictions are contextually accurate. Future research should focus on enhancing the precision and timeliness of task predictions to improve the efficacy of the proactive human-agent interaction."}, {"title": "Limitations", "content": "While our method demonstrates that it can effectively and proactively predict possible tasks, the current research is constrained by several limitations. Firstly, the environment settings we have explored are still limited. The contexts in this paper provide a foundational understanding, but broader application areas need to be investigated to fully establish the versatility and robustness of the proactive agent. Moreover, models still exhibit a relatively high ratio of false alarms, indicating that they cannot yet perfectly predict possible tasks. This limitation highlights the need for further refinement of the model's proactive behavior to avoid bothering the user. The high rate of false alarm can lead to unnecessary or incorrect actions, which may reduce user trust and the overall efficiency of the system. The dynamic adjustment of the proactiveness of the agent according to the specific context should be explored in more depth. Future research should focus on several key areas to address these limitations:\n\u2022 Expansion of Environment Settings: Research should explore a wider variety of scenarios and contexts to validate the model's generalizability. This includes domains where proactive prediction of tasks can significantly enhance user experience and operational efficiency.\n\u2022 Improvement in Prediction Accuracy: Efforts should be directed towards reducing the false alarm rate by enhancing the model's understanding of context and user behavior.\n\u2022 User-Centric Evaluation: Future studies should involve extensive user-centric evaluations to better understand how users interact with the proactive agent and to identify areas for improvement. User feedback and behavioral data can provide valuable insights into refining the prediction algorithms and making the system more intuitive and reliable.\n\u2022 Ethical and Privacy Considerations: As the proactive agent needs the environment information for prediction tasks, it is crucial to address ethical and privacy concerns. Ensuring that user data is handled responsibly and that the agent operates transparently and within ethical guidelines will be critical for gaining user trust and acceptance."}, {"title": "Appendix", "content": ""}, {"title": "A Reward Model Training Setting", "content": "We use Llama-3.1-Instruct-8B as the base model for our training. The total dataset size is approximately 1,640. Specifically, we employ a total batch size of 32, a learning rate of le 5, and an Adam Optimizer with a 0.01 warm-up ratio. We train the reward model for 3 epochs to prevent it from over-fitting. We use 8 A100 GPUs on one node to train for approximately 1.5 hours."}, {"title": "B Agent Model Training Setting", "content": "Similarly, we use Llama-3-Instruct 8B and Qwen2-Instruct-7B as the base model for agent model training. The total dataset size is approximately 6, 790. Specifically, we employ a total batch size of 32, a learning rate of le - 5, and an Adam Optimizer with a 0.01 warm-up ratio. We train the model for 3 epochs to prevent it from over-fitting. We use 8 A100 GPUs on one node to train for approximately 2 hours."}]}