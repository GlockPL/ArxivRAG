{"title": "DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection", "authors": ["Sangpil Youm", "Brodie Mather", "Chathuri Jayaweera", "Juliana Prada", "Bonnie Dorr"], "abstract": "Semantic role labeling (SRL) enriches many downstream applications, e.g., machine translation, question answering, summarization, and stance/belief detection. However, building multilingual SRL models is challenging due to the scarcity of semantically annotated corpora for multiple languages. Moreover, state-of-the-art SRL projection (XSRL) based on large language models (LLMs) yields output that is riddled with spurious role labels. Remediation of such hallucinations is not straightforward due to the lack of explainability of LLMs. We show that hallucinated role labels are related to naturally occurring divergence types that interfere with initial alignments. We implement Divergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging linguistically-informed alignment remediation followed by greedy First-Come First-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL projection without additional transformer-based machinery, beating XSRL in both human and automatic comparisons, and advancing beyond headwords to accommodate phrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our ground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3% (EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1% (EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our approach to other language pairs (e.g., English-Tagalog).", "sections": [{"title": "1 Introduction", "content": "The natural language processing (NLP) task of semantic role labeling (SRL) captures \"who did what to whom\" for many downstream applications, e.g., machine translation, question answering, and summarization [21,14]. Semantic roles are central to inferring unstated information (e.g., stances [26,25] and emotional cues [3]) that are absent from the output of NLP tools such as dependency parsing. Disappointingly, SRL has been studied primarily in English due to highly available English-specific SRL annotated datasets [12]. The scarcity of multilingual SRL-annotated corpora motivates the need for cross-language approaches that project semantic roles from English to other languages.\nMany studies have explored pre-trained SRL models [28,34] and generative AI approaches for semantic tasks that include SRL [36]. These LLM-centric studies tend to focus exclusively on English. The associated LLMs thus introduce hallucinations without obvious recourse due to an inherent lack of explainability.\nOur approach, \"Divergence-Aware Hallucination-Remediated SRL Projection\" (DAHRS) adopts a generalized characterization of divergence types [9,23] and corrects alignnments, remediating hallucinated semantic-role transfer from source to target languages (e.g., English-French and English-Spanish). We introduce a greedy \"First-Come First-Assign\" (FCFA) algorithm within DAHRS that projects roles from corrected initial alignments. FCFA also remediates the hallucinated lack of semantic role projections emerging from corrected initial alignments.\nThe key insight here is that leveraging linguistic knowledge overcomes deficiencies in current transformer-based alignment-projection approaches. Transformer-based alignment treats target words as a bag-of-words, frequently aligning source-language terms to hallucinated target-language terms. By contrast, DAHRS injects an awareness of naturally occurring language divergences, e.g., one-to-many/many-to-one translations or word/phrase order distinctions, into alignment. Straightforward correction of alignments that would otherwise lead to hallucinated incorrect roles supports effective and explainable transfer of semantic roles from the source language to the target language.\nState-of-the-art XSRL [6] addresses a subset of language divergences explored in this paper: nominalizations and separable verb prefixes. In cases where the initial alignment is correct, XSRL fails to project valid roles in the context of other types of divergences, often hallucinating a lack of semantic role projections on the right-hand side. DAHRS is designed to address two types of hallucinations simultaneously: alignment and projection. The performance of DAHRS is compared to that of XSRL using data processed by both methods (see section 5).\nHallucination remediation in DAHRS starts with token-level and phrase-level corrections to an initial transformer-based mBERT [11] alignment. Following this, additional hallucination remediation takes place during projection. Fig. 1 illustrates two representative cases of divergences that have triggered hallucinations in prior work: Light Verb and Structural.3 Square brackets '[]' indicate SRL projections, with unaligned words indicated by \u03b5. The output shown at each stage explainably pin-points which sub-components fail or succeed (alignment or projection, or both).\n(a) Light Verb Divergence. The single verb fell maps to a combination of a \"light\" verb (a) and content word \"fallen\" (chut\u00e9). Despite the correct initial mBERT alignment, XSRL is unable to \"see past\" this divergence to project."}, {"title": "3 Divergence-Aware Hallucination-Remediated SRL Projection (DAHRS)", "content": "DAHRS's key contribution is its ability to compensate for potential semantic role errors emerging from hallucinated alignments that coincide with naturally occurring cross-language divergences. Leveraging source-language knowledge (e.g., English is head initial) coupled with a greedy FCFA algorithm, DAHRS transfers semantic roles to the target language.\nFig. 2 illustrates the DAHRS step-wise pipeline with an English-to- French example. DAHRS's input is an initial mBERT-style alignment, as in XSRL, but prior to SRL projection it corrects hallucinated alignments and transfers semantic roles without additional transformer-based processing.\nFig. 3 shows three key steps in DAHRS: divergence identification (see Section 3.1), alignment correction (DAHRS\u2081 and DAHRS2, see Section 3.2), and FCFA projection (DAHRS3, also in Section 3.2). When divergence identification uncovers a divergence, DAHRS modifies the alignment prior to SRL projection. Otherwise it directly projects semantic roles through FCFA projection."}, {"title": "3.1 Divergence Identification", "content": "For divergence identification, DAHRS relies on a sub-categorization of divergences into three types, as shown in Fig. 4. For example, with regard to the divergences illustrated in Section 1, Light Verb divergences are associated with (a) one-to-many and (b) many-to-one sub-categories, and Structural divergences are associated with (c) the ordering sub-category.\nThe identification of these divergence sub-categories for a given source-target input pair relies on position-value pairs. These pairs indicate the tokens and phrases that are mapped singularly or repeatedly across the source and target inputs. Divergence types are identified across tokenized source and target sentences, where each token is assigned a position value starting from 0.\nConsider the French sentence fragment ordinateurs portable (laptops) in Fig. 4(a). This string is associated with position values of 17 in English and 23,24 in French. Source and target word mappings are denoted by a hyphenated position-value pair. For example, 17-23 and 17-24 indicate the 17th English word (laptops) aligns with the 23rd and 24th word French words (ordinateurs portable). This case is identified as a one-to-many divergence, i.e., a single source token aligns with multiple target tokens. Analogously, a many-to-one divergence is identified when multiple source tokens align with a single target token, as in Fig. 4(b), where fell(4) and apart(5) align with effondr\u00e9e(6).\nAn ordering divergence is detected when a single source token is mBERT- aligned with multiple target tokens (one-to-many) while one of those same target tokens aligns with a different source token (many-to-one). Returning to our earlier example, October 1987 crash (translated in French as crash of October 1987), as shown in Fig. 4(c): october(9) aligns with \u00e9crasement(7) and octobre (9), while one of target tokens, \u00e9crasement(7) also aligns with crash(11).\nAlthough state-of-the-art (mBERT-based) word-to-word alignment establishes a reasonable source-to-target baseline, ordering divergences are not adequately handled, due to mBERT's bag-of-words design. These lead to incorrect alignments that must be remediated in order to avoid hallucinated SRL projections. We note that ordering distinctions have been a focus in statistical machine translation (SMT) for quite some time [32], but these have heretofore not been remediated for projection.\nSubsequent to identifying divergence types, as described below, our approach remediates hallucinations due to divergences and projects semantic roles through FCFA SRL projection."}, {"title": "3.2 DAHRS Algorithms", "content": "DAHRS's three key steps each correspond to a component-level algorithm: alignment correction at the token level (Algorithm 1) and phrase level (Algorithm 2) to remediate hallucinated incorrect role projections, followed by FCFA SRL projection (Algorithm 3) which remediates hallucinated lack of role projections.\nDAHRS1: Token-Level Hallucination Remediation. We remediate alignment hallucinations at the token level, using DAHRS1 (see Algorithm 1). Such hallucinations are discerned from input pairs for one-to-one (tLevelOneToOne), one-to-many (tLevelOneToMany), many-to-one (tLevelManyToOne) alignments.\nDAHRS1 initializes remediated alignments (remOne- ToOne), inserting mBERT- aligned source-target token pairs specified in the tLevel- OneToOne list (lines 4-5). Next the target tokens in the one- to-many pair list (tLevelOne- ToMany) are examined for alignment with other source tokens, preparing for halluci- nation remediation (line 6). If a target token is found to be aligned with an alternate source token, the hallucinated alignment is removed from the target token list (tgtList) (lines 7-10). This action remediates alignment hallucinations that emerge in the context of or- dering divergences. For exam- ple, in the earlier baseline align- ment in Fig. 4(c), the word october is incorrectly aligned with \u00e9crasement. This is detected due to the simultaneous october-octobre alignment (where no other source word aligns with octobre). The spurious october-\u00e9crasement alignment is hypothesized to be a hallucination and is removed.\nAfter remedying spurious alignments in the one-to-many pairs, DAHRS1 proceeds to store the corrected source and target pairs in the output (remOne- ToOne) (lines 11-12). In the earlier baseline alignment in Fig. 4(a), DAHRS1 correctly maps laptops to both ordinateurs and portables.\nIn the case of many-to-one alignment, DAHRS1 examines the source tokens in the one-to-one pair list (tLevelOneToOne) for alignment with other target tokens, preparing for additional hallucination remediation (line 13). In this case, the algorithm addresses the potential for hallucinated (downstream) SRL projections due to the presence of particles or modifiers (e.g., apart in fell apart) that are aligned with the main verb.\nRemediation removes such tokens from the source token list (srcList) (lines 14-16). For eaxample, in the earlier baseline alignment in Fig. 4(b), the apart- effondr\u00e9e alignment is deleted. The remaining fell-effondr\u00e9e alignment is retained and is positioned in the output (remOneToOne) according to the headIntitalFlag, where \"True\" indicates a head-initial language, selecting the first token and \"False\" indicates a head-final language (lines 17-21).\nDAHRS2: Phrase-Level Hallucination Remediation. DAHRS2 shown in Algorithm 2 advances beyond the token-level processing of state-of-the-art (XSRL) in that it includes handling of phrases for SRL projection.\nPhrase-level processing is similar to what is described above, but phrase identification is employed: BIO (Begin- Inside-Outside) tags are as- signed to the source-language side via SRL-BERT [34].4 These BIO-delineated phrasal units are brought together with alignment corrections for more robust alignment halluci- nation remediation. A phrase range is determined by arrang- ing the source words in the order they appear within the sentence and employing BIO tags to identify phrases on the English side.5\nPhrase information (start to end indices), encoded as a source phrase range (sr- cPhRange) and target phrase range (tgtPhRange), acts as phrase-level hallucination re- mediation input. Other in- puts are lists of phrase- level alignment pairs: one-to- one (pLevelOneToOne), one- to-many (pLevelOneToMany), many-to-one (pLevelManyToOne).\nTo support remediation, a list of function words (funcWordIdx) and a head-initial flag (headInitialFlag) are also introduced. This algorithm returns lists of remedi- ated mappings (remOneToOne).\nFirst, DAHRS2 examines whether the mBERT-aligned input is indicative of a one-to-many or many-to-one divergence within a given phrase (a BIO-tagged pair). If no such divergence is present, all the tokens in the phrase are returned as output (remOneToOne) without correction (lines 2-4).\nThe next step remediates a detected hallucinated alignment resulting from an ordering divergence (lines 7-10). For each target list of phrasal one-to-many alignments, two aspects are examined: whether any target tokens are outside the corresponding phrase range, and whether any target tokens are simultaneously aligned with other source tokens. Tokens meeting one of these conditions are removed from the target list (tgtList). After DAHRS2 remediates spurious alignments in the one-to-many pair list, non-hallucinated source and target token pairs are stored in the output (remOneToOne) (lines 11-12).\nLastly, DAHRS2 remediates hallucinated alignments arising from many-to-one divergences (lines 14- 24). Three conditions are tested for each source token that aligns to a given target token: whether the source token is correctly lo- cated within corresponding range, whether it is aligned with an- other target token, and whether the source token is function word. Any source token matching one of those conditions is removed from the source list (srcList). Following this step, the algorithm opts for the first option if headInitialFlag is true, or the second option otherwise.\nWe illustrate DAHRS3 in Fig. 5, where the target token \u00e9crasement, has two distinct source token options (october (9) and crash (11)). Both fall within the correct source phrase range (7-30). Since the source token october already maps to octobre, october is removed from the source options for \u00e9crasement.\nDAHRS3: First-Come First-Assign (FCFA) SRL Projection. DAHRS3 is a new greedy FCFA SRL projection that transfers semantic roles using the remediated alignments (one-to-one mappings, remOneToOne), as shown in Algorithm 3. Alignments are provided as an input along with corresponding role labels transferred from English (srcSRLSet).\nSource side semantic roles are assigned to the remediated aligned target token (lines 3-9). Projection yields two outputs: a human inter- pretable alignment representation and a JSON formatted SRL repre- sentation. For example, in Fig. 6 (a), token-level FCFA projects la- bel (\"O\") to octobre and \u00e9crasement from october and crash (ordering). In addition, the source label from laptops is projected to both ordina- teurs and portables, leveraging the correct (one-to-many) alignment. Advancing beyond state-of-the-art (XSRL), many-to-one handling results in the retention of V for effondr\u00e9e and the elimination of the hallucinated ARG4 for apart."}, {"title": "3.3 Explainability and Visualization", "content": "In contrast to blackbox LLMs, which do not elucidate the decisions behind language alignment and SRL projections, DAHRS builds readily visualized repre- sentations that explain how it arrives at its output. Whereas prior work [18] has proposed metrics such as \u2018goodness', \u2018user satisfaction', and \u2018understandability' as proxies for explainability, DAHRS integrates human-interpretable representations directly into alignment and projection.\nTwo visualized products of our implementation (with French, Spanish as our test case) are: (a) a set of linguistically annotated alignment representations (one for each predicate indicated as \u201cV\u201d) that provides a window into why/how the system produces its output while elucidating errors that can be readily remedied, as depicted in Fig. 5; (b) a JSON formatted representation that specifies all semantic role-labeled tokens for each sentence, as depicted in Fig. 2 (French semantic role-labeled sentence). These examples showcase our handling of hallucination remediation in the face of divergences and highlight the assignment of predicates and corresponding semantic roles on the target side."}, {"title": "3.4 Model as a Diagnostic Tool", "content": "DAHRS employs a direct alignment-based source-to-target transfer mechanism, without requiring a filter or BERT Score (as implemented in XSRL). Moreover, the model based on this algorithm is an effective tool for assessing the accuracy of predicate and semantic role projection in longstanding community standard datasets. To illustrate this point, we explore a human-tagged English evaluation dataset from CoNLL-2009 [16], which has also been translated to French and Spanish data as part of XSRL's research [6].\nPreliminary tests using these datasets for SRL projection yield a much lower precision for DAHRS than that of XSRL: DAHRS: 65.9 (FR), 66.3 (ES), XSRL: 80.7 (FR), 85.4 (ES). Further investigation reveals that these data sets include a very large number of spurious V tags for non-predicates: 8341 (DAHRS) vs."}, {"title": "4 Data and Experimental Setup", "content": "We use our updated English CoNLL-2009 data for projecting semantic roles to French and Spanish datasets. Human-validated FR/ES datasets, parallel to the EN-CONLL, are provided by XSRL. The original CoNLL-2009 data incorporates semantic roles for headwords only. In our headword-level experiment, semantic roles from English headwords are projected to the headwords of the FR/ES datasets. Since phrase-level test datasets are unavailable, we employ AllenNLP's SRL-BERT to assign phrase-level semantic roles to the English corpora, which are then projected onto FR/ES corpora.\nPhrasal-level semantic role assignment further enhances the accuracy of SRL, ensuring phrasal coverage a significant advance over the head-word labeling in the original resource. For instance, without our phrase-level enrichment, the word The is considered a headword during SRL assignment in The Dow Jones industrials closed at 2569.26. The result is a single, inappropriate semantic role assignment of ARG1 to the word The. However, with our enrichment, an appropriate phrasal-level semantic role assignment is made possible: [ARG1- The Dow Jones industrials] [V-closed] [ARGM-EXT at 2569.26]. This corrected output yields a more thorough, accurate representation, which is crucial for downstream tools such as those enumerated in Section 1.\nFrench and Spanish corpora, including their semantic roles, are projected from 2046 English sentences using XSRL (see details in section 2) and DAHRS. Subsequently, we evaluate these against both the community standard ground truth CoNLL-2009 (headword) from XSRL and the human judgment (phrasal)."}, {"title": "5 Results and Analyses", "content": "We explore the performance of two projection-based models: DAHRS and XSRL. DAHRS achieves higher F1 scores in comparison to XSRL on our test data in both word-level and phrasal-level (see Table 1). Performance improvements are obtained as well as explanability.\nTo evaluate the correctness of the French and Spanish projection outputs, we employ the ground truth data from CONLL-2009 for the headword dataset. Linguistically trained human taggers proficient in French and Spanish evaluate the phrasal output. Both evaluations use precision (P), recall (R), and F1 scores. Thus, we have achieved explainable transferability of semantic roles more efficiently and with more accurate outputs (P, R, F1).\nWe evaluate DAHRS against a human-validated CoNLL-2009 that assigns semantic roles only to headwords, per the original XSRL algorithm. We compare XSRL and a variant of DAHRS that produces only headword assignments against this same ground truth. In Table 1, DAHRS projection to headwords outperforms XSRL, with an F1 of 77.3 vs. 87.6 (FR), 82.7 vs. 89.0 (ES).\nFurthermore, we conduct a post-analysis and evaluation of our phrasal-rich output against human judgment by French and Spanish proficient evaluators with linguistic training who evaluated 549 total labels (FR), and 582 total labels (ES). This analysis yields a F1 score (FR-89.1%, ES-91.0%, see Table 1). To our knowledge, this is the highest score achieved for this task, surpassing performance (accuracy) of single headword assignments without the overhead of human-labeled source data for French and Spanish."}, {"title": "6 Discussion: Beyond EN-FR / FR-ES", "content": "We explore hallucinations associated with linguistic divergences by considering language pairs beyond EN-FR / EN-ES. We consider Tagalog, a low-resource language notably influenced by Spanish at the word level [2], yet divergent from Spanish (and English) in that its subject follows the verb (VSO). Although our current study focuses on English as the source language, our future research focuses on Tagalog with both Spanish and English as source languages, further enriching our divergence exploration. This investigation aims to verify whether the pairs exhibit the divergent properties assumed by DAHRS and to provide a framework for testing longstanding hypotheses about cross-language divergences in the context of alignment.\nWe introduce divergence metrics that count the number of misalignments on both the source and target sides. When the target language demonstrates a higher number of misalignments, this typically indicates a one-to-many divergence case. Conversely, when the source side yields more misalignment, this typically corresponds to a many-to-one case. DAHRS effectively transfers semantic roles to the"}, {"title": "7 Conclusions and Future Work", "content": "We present a model for cross-language semantic role projection. Our work enhances semantically informed language processing with minimal overhead via a two-step process that rapidly identifies divergence cases and produces explainable, visualizable SRL output. We demonstrate performance improvements in accuracy without requiring a human-labeled French/Spanish corpus. Our evaluation relies on a community standard ground truth with SRL-tagged headwords (CoNNL- 2009). Notable improvements are demonstrated when considering entire phrases, as evidenced by human judgments.\nFuture work will focus on expanding to other languages (Tagalog is underway) where hand-annotated labels are scarce. Although French and Spanish are investigated above, divergence-causing hallucinations, remediated by acknowledging the syntactic property of languages during DAHRS have been noted across many other languages, e.g., Spanish (categorial; [9]), Korean (structural; [23]), or German (light verb; [24]). As such, it is expected that DAHRS applies multilingually, both for mid-resource language pairs (e.g., English-Spanish/French) and for those that are low-resource language pairs (e.g., English-Tagalog).\nFinally, our experiments reveal that a new model, DAHRS, improves the multilingual SRL projection task. We provide French and Spanish corpora, including SRL information per predicates. Additionally, we utilize DAHRS as a diagnostic tool to verify the accuracy of ground truth. Through this diagnostic tool, we identify errors in the data, enabling us to update and reproduce data for the language community. These data resources are not only beneficial for the SRL task but also may be leveraged for other tasks."}]}