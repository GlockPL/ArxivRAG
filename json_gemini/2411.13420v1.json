{"title": "HEURISTICALLY ADAPTIVE DIFFUSION-MODEL\nEVOLUTIONARY STRATEGY", "authors": ["Benedikt Hartl", "Yanbo Zhang", "Hananel Hazan", "Michael Levin"], "abstract": "Diffusion Models represent a significant advancement in generative modeling, employing a dual-\nphase process that first degrades domain-specific information via Gaussian noise and restores it\nthrough a trainable model. This framework enables pure noise-to-data generation and modular re-\nconstruction of, e.g., images or videos. Concurrently, evolutionary algorithms employ optimization\nmethods inspired by biological principles to refine sets of numerical parameters encoding potential\nsolutions to rugged objective functions. Our research reveals a fundamental connection between dif-\nfusion models and evolutionary algorithms through their shared underlying generative mechanisms:\nboth methods generate high-quality samples via iterative refinement on random initial distributions.\nBy employing deep learning-based diffusion models as generative models across diverse evolution-\nary tasks and iteratively refining diffusion models with heuristically acquired databases, we can\niteratively sample potentially better-adapted offspring parameters, integrating them into successive\ngenerations of the diffusion model. This approach achieves efficient convergence toward high-fitness\nparameters while maintaining explorative diversity. Diffusion models introduce enhanced memory\ncapabilities into evolutionary algorithms, retaining historical information across generations and\nleveraging subtle data correlations to generate refined samples. We elevate evolutionary algorithms\nfrom procedures with shallow heuristics to frameworks with deep memory. By deploying classifier-\nfree guidance for conditional sampling at the parameter level, we achieve precise control over evo-\nlutionary search dynamics to further specific genotypical, phenotypical, or population-wide traits.\nOur framework marks a major heuristic and algorithmic transition, offering increased flexibility,\nprecision, and control in evolutionary optimization processes.", "sections": [{"title": "1 Introduction", "content": "Two fundamental mechanisms in the biosphere are known to drive novelty: evolution and learning. Conventionally,\nevolution is understood as a gradual, slow variational process adapting organisms or lineages across generations to\nchanging environmental conditions through natural selection [Darwin, 1959, Dawkins, 2016]. In contrast, learning\nis a rapid transformational process enabling individuals to acquire knowledge and generalize based on subjective\nexperiences within their lifetime [Kandel, 2013, Courville et al., 2006, Holland, 2000, Dayan and Abbott, 2001]. These\nmechanisms are extensively researched in separate domains of artificial intelligence, and recent studies have begun\nhighlighting similarities between evolution and learning [Watson and Levin, 2023, Vanchurin et al., 2022, Levin, 2022,\nWatson et al., 2022, Kouvaris et al., 2017, Watson and Szathm\u00e1ry, 2016, Watson et al., 2016, Power et al., 2015, Hinton\net al., 1987, Baldwin, 2018].\nEvolutionary algorithms (EAs) implement a search process using biologically inspired variational principles to iter-\natively refine sets of numerical parameters that encode potential solutions to often rugged, custom objective func-"}, {"title": "2 Methods", "content": "2.1 Evolutionary algorithms: Black-box heuristic optimization techniques\nThe principles of evolution have applications beyond biology, proving useful in addressing complex systems across\ndifferent domains. The main components of this process - imperfect replication with heredity and fitness-based selec-\ntion - are versatile and can be applied in diverse fields. In computer and data science, numerous optimization methods\nare employed; among the most widely used is stochastic gradient descent (SGD), which, with advancements like the\nAdam optimizer, excels in tasks where gradient calculations provide clear direction toward solutions. However, not\nall tasks crated equal, some are amenable to gradient-based methods, as gradient calculation can be intractable for\nmany complex problems. For these cases, Evolutionary Algorithms (EAs) such as CMA-ES [Hansen and Ostermeier,\n2001] and PEPG [Sehnke et al., 2010] are essential. These heuristic optimization techniques [Vikhar, 2016, Grefen-\nstette, 1993, Golberg, 1989, Holland, 1992] maintain and evolve a population of genotypic parameters over successive\ngenerations using biologically inspired operations, including selection, reproduction, crossover, and mutation. The\ngoal is to gradually adapt the genotypic parameters of the entire population so individual phenotypic samples perform\nwell when evaluated against an objective- or fitness function. The evaluated numerical fitness score of an individual\ncorrelates with its probability of survival and reproduction to drive the evolutionary process toward more optimal solu-\ntions. Thus, these algorithms use evolutionary biology dynamics to discover optimal or near-optimal solutions within\nvast, complex, and otherwise intractable parameter spaces. Such approaches are particularly valuable when heuristic\nsolutions are needed to explore extensive combinatorial and permutation landscapes."}, {"title": "2.2 Integrating Diffusion Models as Offspring-Generative Process in Evolutionary Algorithms", "content": "The evolutionary process can be viewed as a transformation of genotype or phenotype distributions or, more gener-\nally, parameter distributions. The current population's distribution undergoes selection and mutation, evolving into a\nslightly different distribution after each time step. This perspective highlights the great potential of using generative\nmodels to simulate evolutionary dynamics. In particular, diffusion models, which have achieved state-of-the-art per-\nformance across various generative tasks-including image, video, and audio generation-are well-suited for modeling\nthe complex distributional changes inherent in evolution. Given their capability to capture intricate data distributions,\nwe hypothesize that a well-trained diffusion model should outperform traditional methods in evolutionary strategies.\nTherefore, we propose diffusion model-based evolutionary strategies to generate offspring efficiently by sampling\nmore from high-fitness regions in the parameter space. Given a population {97,i} at time 7, we evaluate their fitness\nvalues fr,i = f(gr,i) using a fitness function f : Rn \u2192 R. Our key idea is to train a generative model on the current\npopulation and their associated fitness values, then sample the next generation {gr,i}, which contains more high-fitness\nindividuals. To achieve this, we map the samples {gr,i} and their fitness values fr,i to a density distribution and train\na diffusion model on it. Specifically, we use a function h : R \u2192 R+ to map fitness to a fitness-derived probability\ndensity, such that p(g) x h[f(g)].\nDiffusion models consist of two phases: a forward diffusion phase and a reverse denoising phase. In the forward\ndiffusion phase, noise is gradually blended into the training data-a process known as diffusion. A neural network is\nthen trained to predict the added noise given the noisy data. In the reverse phase, starting from noisy data, the trained\nneural network is used to denoise step by step, eventually restoring the noise-free data.\nFormally, during the forward diffusion phase, the noise-free data sampled from g is considered at time zero, i.e., xo.\nIt is blended with noise over time according to:\nxt = \\sqrt{at} xo + \\sqrt{1 \u2013 at} \\epsilon, (1)\nwhere \u20ac ~ N(0, ID), and at decreases monotonically from 1 to 0 as t increases, with a0 = 1 and \u03b1\u03c4 = 0. As a result,\nx\u0442 ~ N(0, ID) and x0 ~ 9. More explicitly, we henceforth use the symbols g and \u00e6t to formally distinguish between\ngenotypic parameters in the evolutionary process and parameters subjected to the diffusion model, respectively. Thus,\nwe consider noise-free (or denoised) data points \u00e6o as genotypic parameters x0 ~ g.\nTo denoise, a neural network ee is trained to predict the added noise by minimizing the prediction loss L:\nT\n0 = min L(0) = min \u2211\u2211 p(x) || \\epsilone (\\sqrt{at}x + \\sqrt{1 - at}\\epsilon, t) \u2013 \\epsilon||\u00b2 . (2)\n\u03b8\nt=1 x\u2208Rn\nGiven the difficulty in obtaining the exact probability density p(x) for the training data, diffusion models are trained\nby sampling data points x ~ g and time steps t \u2208 [1,T]. Therefore, the loss function can be reformulated without\nexplicitly using p(x):\nL(0) = Et~U(0,T), x~9 || \\epsilone (\\sqrt{at} x + \\sqrt{1 - at} \\epsilon, t) \u2013 \\epsilon||2. (3)\nAfter training the model, the neural network ee can be used to sample new data points that follow the distribution of\nthe training data. In the Diffusion Denoising Implicit Model (DDIM) [Song et al., 2020] framework, the sampling"}, {"title": "2.3 Conditional, Heuristically-Adaptive ReguLarized Evolutionary Strategy through Diffusion\n(CHARLES-D)", "content": "DMs provide a model-free approach for learning denoising-based generative strategies tailored to custom datasets\nacross versatile, problem-specific domains. Once trained on statistically relevant data, they can potentially surpass\ntraditional EAs in generating high-quality offspring genotypes. In our complementary contribution [Zhang et al.,"}, {"title": "3 Results", "content": "3.1 Diffusion models can efficiently learn, generate, and adapt genotypic representations within heuristic\nevolutionary processes\nIn the following section, we demonstrate that probabilistic diffusion models trained \"online\" on generations of heuris-\ntically varied parameter sets can be efficiently utilized as generative models in EAs, all based on successive denoising\noperations. We first apply our technique to toy problems, specifically solving the dynamic and the static cases of the\ndouble peak in sections 3.2 to 3.4 and Rastrigin problem in sections 3.5 and 3.6, then we focus on Reinforcement\nLearning (RL) tasks [Sutton and Barto, 1998] in section 3.7. We contrast the performance with other EAs, demon-\nstrating that our method is competitive. Our approach enables the simultaneous identification of multiple optima,\nemphasizing diversity and exploration, while still furthering optimal solutions with high efficiency. Furthermore,\nwe demonstrate in the following that classifier-free guidance techniques [Ho and Salimans, 2022] can be effectively\nutilized to conditionally bias the generative process in EAs. This approach constrains the search dynamics of the un-\nderlying heuristic evolutionary processes, even within complex rugged fitness landscapes, without changing the fitness\nscore function. The method is analogous to the use of Lagrange multipliers in differentiable optimization, serving to\nimpose constraints that effectively guide the search even in rugged, non-differentiable fitness landscapes.\nIn all our investigations, we use simple feed-forward neural network architectures for our diffusion models, which\ntypically consist of 2 hidden layers with a number of hidden units ranging from 24-324. To improve readability, we\nlist all simulation parameters explicitly in appendix B."}, {"title": "3.2 Diffusion models provide a model-free approach that adapts readily to new problems and dynamic\nenvironments", "content": "Biological evolution is renowned for its adaptability capabilities in (slowly) changing environments [Levin, 2023b,\nWagner et al., 2007, Schlosser and Wagner, 2004, Calabretta et al., 2003, Ten Tusscher and Hogeweg, 2011, Wagner\nand Altenberg, 1996]. While being increasingly recognized in the computational literature [Power et al., 2015], main-\nstream EAs typically focus on exploring parameter spaces for globally optimal solutions to complex and often rugged,\nyet generally static objective functions.\nTo examine the adaptive capabilities of evolutionary processes to changing environmental conditions to mimic a more\nbiologically realistic setting, we introduced a time-dependent objective function, f(g, T), and compare the learning\ncapabilities of HADES against other different mainstream EAs. Specifically, we define f(g, r) as\nf(g, t) = cos(wt) )e ^{\u2212 \\frac{(g-\u03bc+)^2}{2\u03c3^2}} + cos(\u03c9\u03c4 + \u03c6) \u0435^{\u2212 \\frac{(g-\u03bc-)^2}{2\u03c3^2}},\n(7)\nwith two Gaussian peaks of STD o centered at \u03bc+ = (\u00b11, \u00b11) with amplitudes that oscillate phase-shifted by \u0444\nacross generations 7 with angular velocity w; a static double-peak problem is recovered by setting w = 0 and $ = 0.\nThus, we utilize an alternating double-peak function, where one peak has a positive and the other one negative ampli-\ntude by setting \u03c6 = \u03c0, \u03c9 = 2\u03c0/10, and \u03c3 = 0.1. Over time, the amplitudes periodically alternate in sign, reverting\nthe target of the maximization objective. We apply our HADES method and CMA-ES [Hansen and Ostermeier, 2001]\nboth with a population size of Np = 256 and an initial population of standard deviation (STD) \u03c3\u2081 = 0.5 evaluating\neach sampled individual gi in every generation \u03c4against f(gi,t); see table 2 in appendix B for more details. The\npopulation dynamics are depicted in fig. 3 (B). Traditional EAs have very different strategies in updating their gen-\nerative models across generations due to their inherent inductive biases. Yet, even powerful mainstream approaches"}, {"title": "3.3 Insights from developmental biology: Neutral multi-objective adaptation via conditional diffusion model\nevolution", "content": "Typically, EAs aim to find optima in their respective fitness landscapes by maximizing fitness scores through biolog-\nically inspired selection and mutation operations. In rugged fitness landscapes, this approach can be exceptionally\neffective, enabling exploration for global solutions, a distinct advantage over many gradient-based methods that often\nget stuck at local optima [Katoch et al., 2020, Sutskever et al., 2013]. However, traditional EAs often struggle with\nproblems that have multiple (unrelated or competing) objectives, as these can create conflicts and frustration in fitness\nscoring. While mitigation strategies such as problem-specific reward shaping [Ng et al., 1999] and curriculum learning\ntechniques [Bengio et al., 2009] exist, these approaches typically demand careful customization and domain expertise.\nThis limitation highlights the need for more robust and adaptable optimization frameworks that can naturally handle\nmulti-objective scenarios."}, {"title": "3.4 Learning from Past Experience: DM-Based Generative Samplers Provide a More Powerful and\nBiologically Inspired Framework for Modular Evolutionary Processes", "content": "Biological evolution exhibits an inherent capacity to utilize existing, heuristically acquired knowledge for species\nadaptation in changing environments [Frank, 2009, Kouvaris et al., 2017, Watson and Szathm\u00e1ry, 2016, Watson et al.,\n2016, Power et al., 2015, Watson et al., 2014]. This capability stems from the recombination of established genetic\nmaterial across evolutionary time-scales. Thus, the exploitation of previously explored solutions and their adaptation\nto novel contexts represents a fundamental principle of biological evolution [Wagner et al., 2007, Schlosser and Wag-\nner, 2004, Calabretta et al., 2003, Ten Tusscher and Hogeweg, 2011, Wagner and Altenberg, 1996]. Within traditional\nEAs, the integration of past experience remains largely limited to heuristic information stored in the current popula-\ntion, or within the generative model employed by techniques such as CMA-ES for offspring sampling. Consequently,\nthe generative process in current EAs maintains limited memory of previously explored solutions, leading to ineffi-\nciencies from either memory loss or the intrinsic inductive bias of generative models. Generative DMs, however, can\nbe equipped with epigenetic memory by preserving information about previously explored solutions. This capability\ncan be implemented through dataset buffering or DM retraining across multiple generations. These approaches offer\na model-free methodology to enhance evolutionary sampling capabilities based on past experience, while simulta-\nneously testing new environmental hypotheses through an iteratively updated generative process. Information holds"}, {"title": "3.5 Improving on Past Experience: Conditioning for Higher Fitness Can Improve Learning Performance,\nbut Increases Greediness", "content": "Next, we investigate the application of fitness conditioning during the DM's generative process: We can jointly train\nthe DM on associated parameters and fitness scores {gi, fi = f(gi)}, conditioning the sampling process to generate\noffspring that potentially achieve a higher target fitness than any previously observed [Krishnamoorthy et al., 2023],\nf(T) > maxi (fi).\nGiven that the maximum fitness or reward can not be known a priori, we propose sampling the target fitness for\nconditionally generating the next generation based on Fishers fundamental theorem of natural selection, which states\nthat [Fisher, 1930] \"The rate of increase in fitness of any organism at any time is equal to its genetic variance in fitness\nat that time\". This principle on the rate of expected fitness improvement has intriguing links to inverse reinforcement\nlearning (IRL) [Schmidhuber, 2020, Srivastava et al., 2021], as it can not be assumed that conditioning the DM on\narbitrarily large fitness (i.e., significantly surpassing the training data) will yield reasonable offspring parameters,\nespecially in early stages of the evolutionary search process.\nHowever, to avoid this algorithm to become too greedy, we here introduce two flavors of fitness sampling, (i) Fisher-\nconditioning c(TF) ~ \u03bc\u03b5 + |N(\u03bc = 0,\u03c3 = \u03c3f)|, and (ii) Greedy-conditioning of target fitness c(TG) ~ \u039d(\u03bc =\nfmax, = of), where \u00b5f is the mean, fe the maximum, and of the STD of the fitness scores fi of the current\npopulation.\nIllustrative examples of sampling parameters conditional to target fitness values by a DM pre-trained [Krishnamoorthy\net al., 2023] on the double-peak task are shown in fig. 1 (C). In fig. 6 (A-D), we present optimization results for\ndifferent configurations of HADES and fitness-conditional CHARLES-D applied to the inverted and truncated Rastrigin\ntask (see appendix D.2), a periodically oscillating function in the two-dimensional plane with four optima located at\n|x| = |y| = 3.5. For simplicity, we henceforth refer to the inverted and truncated Rastrigin task simply as Rastrigin\ntask.\nOur findings indicate that this fitness conditioning can indeed significantly improve the learning capabilities of\nCHARLES-D, which opens new avenues for exploring complex parameter spaces. However, aiming for greater fit-\nness increases across generations might also lead to more greedy behavior of our algorithm and thus suboptimal\nconvergence, which we will delve into detail in the following section."}, {"title": "3.6 Novelty-Conditional Sampling: A Good Regularizer for Exploration and Maintaining Diversity", "content": "Typically, EAs aim at identifying solutions with optimal fitness values in rugged parameter landscapes. However, de-\npending on the fitness landscape, this can be highly non-trivial and requires a dedicated balance between exploration\nand exploitation, see for example [Hazan and Levin, 2022]. Recent discussions, originating from developmental\nbiology [Levin, 2023b, 2022] suggest that biological systems operate under a different paradigm, where agents con-\ntinuously explore novel situations to maintain their integrity and adapt to changing environments. This inherent drive\nfor novelty leads to the creation of novel challenges, requiring further adaptation, and thus creating a diversification\nscaffold [Levin, 2024].\nIt has recently been argued [Levin, 2024], that the fundamental drive for novelty-biological systems could be rooted\nin the phenomenon of boredom: biological agents on a variety of scales will seek novel stimuli if their sensory inputs\nstagnate too long. This is reflected in their intrinsic drive for exploration and discovery, but not necessarily aligns\nwith traditional optimization objectives or search strategies. In fact, incorporating novelty [Lehman and Stanley,\n2011] or a bias toward quality-diversity [Pugh et al., 2016] into optimization processes such as EAs significantly\nimprove their performance. These techniques reinforce search directions in novel parameter regions while penalizing\n(over)exploitation of already experienced solutions. Instead of directly optimizing for novel and diverse solutions via\nmodified fitness scores, we propose to use novelty-conditional sampling via HADES.\nWe utilize a metric for diversity in the heuristic data buffer of our CHARLES-D method, and condition the DMs\ngenerative process to sample diverse genotypes. Specifically, we define diversity di of a single data point gi similar to\nthe non-parametric k-nearest-neighbor entropy estimator [Lombardi and Pant, 2016] as \u03b4\u03b5 = log(N-kjk dij),\ni.e., as the logarithm of its mean distance in parameter space to all other data points gj with a distance dij = |gi - 9j|\nlarger than the k-th nearest neighbor distance dknn in the dataset buffer. In practice, we use this diversity metric as a"}, {"title": "3.7 Genetically conditioning behavior: how information traverses scales", "content": "So far, we have demonstrated how the CHARLES-D method can be applied to constrain (i) the search dynamics in\nthe parameter-space, (ii) the fitness quality of the samples across generations, and even (iii) improve population wide\ndiversity in a diffusion evolution optimization process. Building upon these results, we explore its application in\nselectively sample genotypic parameters to achieve desired phenotypic traits. Specifically, we explain whether we can\nconditionally train the DM in CHARLES-D using both (i) genotypic representations and (ii) associated phenotypic\nqualities of agents in Reinforcement Learning (RL) environments [Sutton and Barto, 1998]. The goal is to selectively\nsample RL agents during an evolutionary process that exhibit specific target behaviors, notably without pretraining the\nDM."}, {"title": "4 Discussion", "content": "Our work, alongside a complementary contribution [Zhang et al., 2024], establishes a connection between diffusion\nmodels (DMs) and evolutionary algorithms (EAs) through shared underlying conceptual and mathematical founda-\ntions: DMs can be viewed as evolutionary processes in disguise. In this paper, we demonstrate that deep-learning\nbased DMs can effectively serve as efficient generative models in EAs, enhancing genotypic recombination operations.\nRather than relying on pretraining DMs with large general datasets, we continuously refine DMs using heuristically\nacquired, high-quality parameters from evolutionary process. This iterative refinement of the DM's generative process,\nbased on the most recent evolutionary evidence in biology, enables the DM to be adaptive to evolutionary changes.\nDMs leverage Artificial Neural Network (ANN) to sample novel data points conforming to a target parameter distri-\nbutions. Given their status as universal function approximators [Hornik et al., 1989], ANNs excel at learning complex\ncorrelations within arbitrary datasets, making them ideal for identifying subtle correlations in parameters of evolution-\nary processes. Through iterative refinement of the DM's generative process using heuristically acquired high-quality\ndata from evolutionary processes, we propose the novel Heuristically Adaptive Diffusion-Model Evolutionary Strategy\n(HADES) method. We contrast our method's performance with mainstream EA techniques across various numerical\noptimization scenarios and report significant improvements in adaptability to changing environments while maintain-\ning reliable convergence to target solutions.\nDMs augment evolutionary processes with unconventional (epigenetic) memory [Jablonka and Raz, 2009, Jablonka,\n2017]: Using elite buffer datasets collected across generations or persistent traits in constantly retrained DMs allows\nthe generative process to utilize previously experienced information, thus enabling faster adaptation in changing envi-\nronments. This memory capability proves especially crucial when objective functions are computationally expensive.\nOur findings confirm that maintaining a memory buffer enhances both search result quality and diversity.\nMoreover, via classifier-free-guidance techniques [Ho and Salimans, 2022], we can utilize conditional sampling in\nDMs to directly bias the evolutionary search dynamics, steering it towards regions in the parameter space that exhibit\ndesired target traits. This leverages multi-objective optimization without the need for complex reward shaping [Ng\net al., 1999] or curricula learning techniques [Bengio et al., 2009]. By conditioning the DM's generative phase across"}]}