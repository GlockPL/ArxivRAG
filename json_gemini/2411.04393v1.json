{"title": "Bridging the Gap: Representation Spaces in Neuro-Symbolic AI", "authors": ["XIN ZHANG", "VICTOR S.SHENG"], "abstract": "Neuro-symbolic AI is an effective method for improving the overall performance of AI models by combining the advantages of neural networks and symbolic learning. However, there are differences between the two in terms of how they process data, primarily because they often use different data representation methods, which is often an important factor limiting the overall performance of the two. From this perspective, we analyzed 191 studies from 2013 by constructing a four-level classification framework. The first level defines five types of representation spaces, and the second level focuses on five types of information modalities that the representation space can represent. Then, the third level describes four symbolic logic methods. Finally, the fourth-level categories propose three collaboration strategies between neural networks and symbolic learning. Furthermore, we conducted a detailed analysis of 46 research based on their representation space.", "sections": [{"title": "1 INTRODUCTION", "content": "Neuro-symbolic AI is a promising paradigm that combines both the powerful learning abilities of neural networks and the logical reasoning of symbolic AI to address complex AI problems. However, although the cooperation between these two seems natural, the difference in their representation is obviously not negligible.\nProf. Henry Kautz proposed a taxonomy of Neuro-Symbolic Systems in the AAAI 2020. In addition, many researchers have conducted relevant reviews of the recent neuro-symbolic AI from different perspectives."}, {"title": "2 TYPES OF NEURO-SYMBOLIC AI BASED ON REPRESENTATION SPACE", "content": "In this article, modal refers to the modality of input data, so the single-modal model describes a method that can only process one data type. In contrast, the multi-modal model can process more than one data type. In addition, non- heterogeneous and heterogeneous refer to whether the representation space can simultaneously support the embedding vectors of neural networks and symbolic logic instead of representing them in the other's way. A representation space that can only support one is called a non-heterogeneous representation space. Otherwise, it is a heterogeneous representation space. Combining the upper two categorizing methods, we divide existing neuro-symbolic Al research into five types: uni-modal non-heterogeneous, multi-modal non-heterogeneous, single-modal heterogeneous, multi- modal heterogeneous, and dynamic adaptive model."}, {"title": "3 SINGLE-MODAL NON-HETEROGENEOUS NEURO-SYMBOLIC AI", "content": "We classify 175 neuro-symbolic AI studies into five sub categories by data type that has been processed: text, image, environment and state, numerical and mathematical expressions, and structured data."}, {"title": "3.1 Text", "content": "This category covers 51 studies in which neural networks extract features from text data and then process them using logical symbolic methods. Additionally, these studies can be grouped into four divisions based on the type of symbolic logic: logical rules and programming, symbolic representation and structure, knowledge graphs and databases, mathematics, and numerical operations."}, {"title": "3.1.1 Symbolic:Logic Rules and Programming", "content": "This portfolio includes 32 studies, all extracting features from text like natural language, programming language, and descriptions of specific fields, then converting features into a form that can be processed by symbolic logic through semantic parsing. This process bridges data-based pattern recognition and rule-based logical reasoning. Research within this combination can be divided into three groups based on how neural networks and symbolic logic cooperate. In the rest of this part of the review, we will default to the classification model in this section for research statistics.\n(1) Neuro-symbolic generation: features are extracted by neural networks, and then these features are transformed into a form that a symbolic logical module can handle. Research in this category includes"}, {"title": "3.1.2 Symbolic:Symbolic Representation and Structure", "content": "This category includes six studies. The neural network extracts features from text data by extended short-term memory networks, universal sentence encoder, InferSent sentence embeddings, or Bert models, and then converts text input into structured representations by various methods, such as using a symbolic stack machine to manipulate text sequences or a grammatical structure of sentences like syntactic parse trees or generating symbolic expressions to represent the solution process of mathematical problems. Among them, Research within this group that belongs to symbolic-neural enhancement includes"}, {"title": "3.1.3 Symbolic:Knowledge Graphs and Databases", "content": "This category includes 12 studies in which neural networks extract features from text, and symbolic logic exists in knowledge graphs, first-order logic facts, and ontologies, representing explicit rules, entities, and relations among entities to support reasoning and decision-making. Research within this group that belongs to neural-symbol generation includes"}, {"title": "3.1.4 Symbolic:Mathematical and Numerical Operations", "content": "This portfolio includes a total of one study. Flach and Lamb focuses on using \u03bb-calculus for encoding and calculation and utilizing logical symbols for calculation by learning to perform reductions in \u03bb-calculus. This research includes detailed hypotheses (H1 and H2) regarding the transformer model's capabilities: H1 asserts that the Transformer can learn to perform a one-step computation in A-calculus. At the same time, H2 proposes that it can execute complete computations. Specifically, This method uses the Transformer model to extract features from the A-terms in text form generated by using the grammatical rules of the A calculus. The output is the new A-terms after the \u1e9e-reduction of these terms; the free variables in the function body are replaced with actual parameters. The A calculus includes the abstract definition and application of functions. It is a formal system used to express function abstraction and function application. It is the theoretical basis of functional programming languages and Turing Complete and can theoretically represent any computable problem. This model can support the learning and research of functional programming languages and simplify expressions through A calculus rules to build more competent code editors and compilers. The transformer model shows high accuracy in performing single-step and multi-step beta-reduction tasks. The model achieved a maximum accuracy of 99.73% for the One-Step Beta Reduction task. In the Multi-Step Beta Reduction task, the model's accuracy is as high as 97.70%. Even when the output is not entirely predicted correctly, the string similarity index usually exceeds 99%, showing that the transformer model can effectively learn and perform computational tasks based on a calculus."}, {"title": "3.2 Image", "content": "This category includes 51 research studies, all extracting low-level features from image data by neural networks and then using symbolic logic for high-level reasoning and decision-making. These studies involve four sub-categories of logical symbolic methods: logical rules and programming, symbolic representation and structure, knowledge graphs and databases, and mathematics and numerical operations."}, {"title": "3.2.1 Symbolic:Logic Rules and Programming", "content": "This portfolio includes a total of 35 studies in which neural networks extract features such as objects, the structure of scenes, or other perceptual information from images or visual data and then apply logical rules, predicate logic, and probabilistic logic programming to process features for further understanding, inferring, and decision-making. This combination includes basic applications such as primary image classification and handwritten formula evaluation, as well as higher-level decision-making and reasoning tasks, such as visual relationship detection and abstract logical reasoning, which show that the combined method has great potential in multiple fields and tasks. Among these studies, those belonging to the neural-symbol generation classification include"}, {"title": "3.2.2 Symbolic:Symbolic Representation and Structure", "content": "The category includes eight studies where neural networks are responsible for processing continuous, high-dimensional visual inputs, and symbolic logic uses this information or patterns for reasoning or decision-making by mapping extracted features to a set of predefined symbols or concepts. Among these studies, those belonging to the neural-symbol generation classification include"}, {"title": "3.2.3 Symbolic:Knowledge Graphs and Databases", "content": "The portfolio includes a total of five studies that use neural networks to extract features from visual modalities and then use logical symbolic forms such as knowledge graphs, background knowledge, first-order logic programming, and ontologies to represent and process high-level, regularized knowledge to help the model understand and reason about complex relationships and rules in the field. Among these studies, those belonging to the neural-symbol generation classification include"}, {"title": "3.2.4 Symbolic:Mathematical and Numerical Operations", "content": "The portfolio includes three studies in which neural networks extract complex patterns and structures from images, time series, or videos. These models then use symbolic regression to discover the mathematical laws behind the data or probabilistic graphical models to model cause and effect in the data relation. Among these studies, those belonging to the neural-symbol generation classification include"}, {"title": "3.3 Environment and Situation Awareness Data", "content": "This category includes 19 research results, all of which use neural networks to extract features from visual images, sensor data, environmental status information, etc., and then use symbolic logic, such as logical rules, defining goals and constraints, and expressing high-level knowledge of tasks, to perform rule-based reasoning and decision-making. These studies include four categories of logical symbolic methods: logical rules and programming, symbolic representation and structure, knowledge graphs and databases, and mathematics and numerical operations."}, {"title": "3.3.1 Symbolic:Logic Rules and Programming", "content": "This portfolio includes 14 studies in which neural networks automatically extract complex features from raw data and then use logical rules, first-order logic formulas, and symbolic action models to express and process structured knowledge to guide neural networks. The network's learning process provides an interpretable decision-making basis for performing precise and complex logical reasoning. Among these studies, those belonging to the neural-symbol generation classification include"}, {"title": "3.3.2 Symbolic:Symbolic Representation and Structure", "content": "This category includes four studies in which neural networks extract features from physical interactions with the 3D world, visual modal data, and symbolic representations of environmental states. They use symbolic logic to describe environmental states, rules, and action effects and then make inferences based on this knowledge and regulation. Among these studies, those belonging to the neural-symbol generation classification include [20]; [151]belongs to neuro-symoblic Enhancement, while those belonging to the neural-symbol collaboration include"}, {"title": "3.3.3 Symbolic:Mathematical and Numerical Operations", "content": "This classification includes one study. Landajuela et al. proposed a new method, DSP (Deep Symbolic Policy), to solve the control problem in deep reinforcement learning by directly searching the symbolic policy space. The DSP framework uses an autoregressive RNN to extract features of the environment's observation or state data from the reinforcement learning environment. These features contain essential information, such as the position and speed of objects that control the current state of the task. The process starts with an empty expression and goes up to a sequence of mathematical operators and state variables. Therefore, DSP's understanding of the environmental state is transformed into a symbolic control strategy. The mathematical expression representing the policy can calculate one or more actions based on the current observation of the environment, which also means that RNN can learn how to map the environment state to a mathematical expression and use it as a policy to control the environment. These mathematical expressions directly affect the selection of actions in the environment. Hence, DSP uses risk-seeking policy gradients to optimize the parameters of the RNN based on the rewards obtained by these actions in the environment, thereby improving the generated symbolic policy and maximizing the performance of the generated policy. In addition, DSP proposes an anchoring algorithm that can handle multi-dimensional action spaces. It uses pre-trained neural network-based strategies as temporary strategies and realizes the conversion from neural network strategies to symbolic strategies by gradually replacing them with pure symbolic strategies. DSP was tested in eight environments, including single-action and multi-action spaces, with benchmark environments performing continuous control tasks. The results showed that the symbolic policies discovered by DSP surpassed multiple state-of-the-art in terms of average ranking and average normalized plot reward, which indicates that this strategy generation method can produce a control strategy that is both efficient and easy to understand."}, {"title": "3.4 Numerical Types and Mathematical Expressions", "content": "This category includes 27 research results, all of which use neural networks to extract features from numerical data, sequence data, image data, and sensor data and then use mathematical expressions, mathematical equations, logical rules, constraints, probability models, and other symbolic logic to improve performance or interpretability. These studies can devide into three sub-categories by logical symbolic methods: logical rules and programming, symbolic representation and structure, and mathematics and numerical operations."}, {"title": "3.4.1 Symbolic:Logic Rules and Programming", "content": "This classification includes ten studies in which neural networks extract features from numerical data. At the same time, symbolic logic exists in the form of rules and constraints, propositional logic, ontology and reasoning mechanisms, and knowledge models. Research belonging to the neural-symbol generation category include [123]; those belonging to the symbol-neural enhancement category include"}, {"title": "3.4.2 Symbolic:Symbolic Representation and Structure", "content": "This combination includes two studies [29, 81]. The former focuses on the neuro-symbolic generation, and the latter studies symbolic-neural enhancement, in which the neural network extracts features from the code or numerical input-output pairs of programming languages and uses sym- bolic logic methods such as abstract grammar tree or symbolic equation generation to represent high-level semantic representations. [81] proposed a new method for finding semantically similar code fragments in COBOL code. This approach defines a meta-model and instantiates it as an abstract syntax tree common between C and COBOL code as an intermediate representation that can capture the structure and logic of the code and serve as the symbolic logical form of the code. Using a neural network, this intermediate representation is extracted from the two programming language codes of C and COBOL. Then, the intermediate representation is converted into a one-dimensional serialized form using the traversal method. Finally, training and fine-tuning are performed on these linearized intermediate representations based on neural network models such as UnixCoder to learn the semantic similarities between code fragments. Symbolic logic exists in two primary forms in this method: intermediate representation and linearized intermediate representation. As a high-level abstraction of the code, the former embodies the program's logical structure and ignores specific grammatical details. At the same time, the latter enables the neural network to pass. This form learns the structure and semantics of code. The experiment verified the effectiveness of the code clone detection task on the COBOL test set by comparing random models, UniXCoder models fine-tuned for specific tasks, pre-trained UniXCoder models, and UniXCoder models fine-tuned with original C code. The UniXCoder model achieved a 36.36% improvement in the MAP@2 indicator after being fine-tuned with SBT(Structure Based Traversal) IR(Intermediate Representation) of C code. At the same time, compared with fine-tuning with the original C code, the UniXCoder model fine-tuned with SBT IR of C code can migrate better. To COBOL code, zero-shot learning for cross-language code understanding is achieved."}, {"title": "3.4.3 Symbolic:Mathematical and Numerical Operations", "content": "The portfolio includes 15 studies in which neural networks extract features from experimental data, simulated data, time series signals, images, or numerical inputs in specific problem areas, such as structural engineering, physical science, chemistry, etc., then apply mathematical expressions, equations, or symbolic logic methods in the form of probability models. Mathematical derivation can transform features learned by neural networks into easily understood and explained forms, improving the model's ability to understand and predict data. Among these studies, those belonging to the neural-symbol generation classification include"}, {"title": "3.5 Structured Data", "content": "This category includes 27 studies, all using neural networks to extract features from graph-structured, structured symbolic, and labeled parameter data. They then use symbolic logic, such as knowledge graphs, logical rules, parameter graphs, and label rules, to represent the structural and logical relationships between data. These studies apply three logical symbolic methods: logical rules and programming, symbolic representation and structure, and knowledge graphs and databases."}, {"title": "3.5.1 Symbolic:Logic Rules and Programming", "content": "The portfolio includes 15 studies in which neural networks extract features from structured symbolic, graph-structured, and time series data. On the other hand, symbolic logic utilizes directly defined logical rules, rule-based reasoning, or enhanced knowledge graph. Among these studies, those belonging to the neural-symbol generation classification include"}, {"title": "3.5.2 Symbolic:Symbolic Representation and Structure", "content": "This combination includes [47] and [148], where the former belongs to neural-symbol generation, and the latter belongs to symbolic-neural enhancement. The two neural networks abstract features from the dynamic data and labeled parameter data of the physical system and use explicit mathe- matical expressions in the form of parameter graphs and labels to guide the model learning process and enhance the interpretability of the model."}, {"title": "3.5.3 Symbolic:Knowledge Graphs and Databases", "content": "The portfolio includes ten studies in which neural networks extract features from forms such as knowledge graphs, graph-structured data, or other symbolic logic data. In contrast, symbolic logic utilizes knowledge graphs, logical expressions, query structures, or rules to integrate domain knowledge, reasoning rules, or relationships. In these studies, those belonging to the neural-symbol generation classification include"}, {"title": "4 MULTI-MODAL NON-HETEROGENEOUS NEURO-SYMBOLIC AI", "content": "This category includes 13 research results, all of which use neural networks to extract features from multiple modal data and then use symbolic logic such as knowledge graphs, logic programs, and symbolic rules to improve the system's inference and decision-making performance. These studies apply three logical symbolic methods: logical rules and programming, knowledge graphs and databases, and mathematics and numerical operations."}, {"title": "4.1 Symbolic: Logic Rules and Programming", "content": "This category includes eight studies in which neural networks extract features from various model data, such as images and text, and then apply symbolic logic methods to improve the model's depth of understanding and reasoning performance. Among these studies, studies belonging to the neural-symbol generation classification include [78, 199]; research belonging to the symbol-neural enhancement classification includes"}, {"title": "4.2 Symbolic:knowledge Graphs and Databases", "content": "This combination includes a total of three studies. all belong to Neuro-Symbolic collaborative classification. They apply knowledge graphs, ontology, logical rules, and other symbolic logic or structured knowledge to enhance the model's reasoning and explanation capabilities. They also provide the model with explicit understanding and prior knowledge about the world."}, {"title": "4.3 Symbolic:Mathematical and Numerical Operations", "content": "This classification includes one study. Wang et al. solved the semi-definite program problem related to the MAXSAT (maximum satisfiability) problem by MAXSAT solver and using a fast coordinate descent method. This method is also called SATNet. SATNet first extracts features from numerical or logical data or image data. For logical data, logical encoding is used to represent the constraints of the problem, while for image data, a convolutional neural network is used to extract digital recognition features from Sudoku images. These features are then converted into a format suitable for logical reasoning-differentiable MAXSAT solvers. The direct logical data can be used as input for the MAXSAT problem.\nIn contrast, the image data must first be processed by a convolutional neural network to identify the numbers in the image, and the recognition results are converted into a logical format as the input of the MAXSAT problem. The MAXSAT solver then uses an optimization process to find a solution that satisfies all constraints and then converts the solution back into the representation of the original problem. At present, researchers have successfully used SATNet to learn logical structures and significantly improve performance in several tasks. SATNet can quickly help the model learn the objective function in the parity learning scenario and improve the test set on the test set within 20 cycles. The error rate converged to zero; in the Sudoku scenario, SATNet learned how to solve the standard 9\u00d79 Sudoku puzzle, discovered and recovered the puzzle rules, and achieved 98.3% accuracy on the test set, respectively. SATNet can effectively learn Sudoku game rules from image input for visual Sudoku tasks. It achieved a puzzle-solving accuracy of 63.2% on the test set, close to the theoretical best test accuracy of 74.7%. In this study, the MAXSAT solver is embedded in the learning process as a layer to integrate the processing capabilities of symbolic logic into the neural network architecture, thus belonging to the symbolic-neural enhancement classification."}, {"title": "5 SINGLE-MODAL HETEROGENEOUS NEURO-SYMBOLIC AI", "content": "Furlong and Eliasmith proposed VSAs (Vector Symbolic Architectures) for simulating probability calculations and realizing symbolic logic and cognitive functions in brain model construction. In the VSAs framework, the features extracted from raw data by neural networks are converted into vector representations in high-dimensional space. Then, VSAs operate on the high-dimensional vectors to simulate symbolic logic. Specifically, it defines the Binding, Bundling, Similarity, and Unbinding operations, where the Binding operation uses the circular convolution or dot product of the vector to combine two vectors into a new vector that can uniquely represent the combination of the two original vectors; Bundling is the combination of multiple vectors are superposed together to form a new vector that roughly retains the original vector information; Similarity determines whether two symbols or concepts are similar or related by calculating the dot product or cosine similarity between two vectors; Unbinding is the inverse of binding operation used to extract a primitive vector from a bound vector. Based on these operations, VSA supports operations similar to traditional symbolic logic in high-dimensional vector spaces, such as building tree structures or graph structures representing complex data structures and relationships in vector space through binding and bundle operations. Alternatively, similarity calculations and unbundle perform pattern matching or rule application on vectors representing different concepts and rules to simulate the logical reasoning process. The logical reasoning part of the VSA architecture is transparent, but mapping raw data to high-dimensional vector space can still be regarded as a black box operation. Nevertheless, compared with traditional logical symbolic methods, the VSA architecture provides a parallel processing capability, which means that many logical symbolic operations can be processed simultaneously in vector space. This feature is significant for processing complex logical reasoning and large-scale knowledge bases.\nKatz et al. proposed a NVM (Neural Virtual Machine) for executing symbolic robot control algorithms. This method uses neural networks to perform symbolic operations by simulating the execution of a Turing-complete symbolic virtual machine. First, it extracts features from symbolic logic data through neural networks, converts symbolic logic operations into activity patterns and connection weights within the neural network, and uses the specific activation patterns of a group of neurons to represent variable names, operators, etc., in the program symbol. These activation patterns are predefined so the neural network can accurately represent and distinguish various program symbols. Then, specific layers and activity patterns of neural networks are used to describe the state of registers, memory, instruction pointers, etc., in a Turing-complete virtual machine, and state changes are represented by updating the corresponding neural activity. This way, symbolic operations such as arithmetic operations, logical judgments, conditional branches, and loops can be performed through predefined neural network patterns and dynamic weight adjustment. The compiled program can then be sent to NVM for processing as a series of instruction sequences. In addition, through specially designed neural network layers, symbolic decisions can be converted into executable control signals, such as motor commands or action sequences. An essential advantage of NVM is the ability to program and perform complex tasks using virtually any program logic, which is critical for robot development and operation."}, {"title": "6 MULTI-MODAL HETEROGENEOUS NEURO-SYMBOLIC AI", "content": "Katz et al. proposed an LNN (Logical Neural Network) framework that integrates neural networks and logical symbol processing functions. The innovation of LNN is that neural networks and symbolic logic operate the same type of data in the same representation space. This framework avoids the use of additional middle layers to convert data types. Specifically, LNN supports using neural networks to extract features from raw data in multiple modalities, such as numerical, text, image, and sound data. More importantly, LNN corresponds logical symbols in propositions, predicates, etc., to one or a group of neurons, which means that the activation state of each neuron or neuron group represents the truth value state of the logical proposition, such as activation. The state indicates that the proposition is accurate, and the inactive state suggests that the proposition is false. At the same time, logical operations such as AND, OR, and NOT can also be implemented through specific activation functions and network structure design. For example, the AND operation can be constructed through the weighted sum of multiple inputs and a threshold activation function. The output neuron is activated only when all inputs are activated, which means the AND operation result is valid. The OR operation activates the output neuron when either input is valid, indicating that the result of this operation is valid. The NOT operation activates the output neuron when the input is inactive. This way, LNN can construct more complex logical expressions and support various logical symbol forms such as propositional, predicate, fuzzy, description, and temporal logic.\nLNN adopts an end-to-end training method and does not require manual setting of rules or logical reasoning steps. It performs logical operations based on the learned parameters. Each network forward propagation is equivalent to performing a parameterized logical operation. For example, when we train an LNN that performs an AND operation, we can use the truth value status of two propositions as input and the AND operation result as the output. The training data set contains all possible truth value input combinations and the corresponding AND operation results. The network learns parameters through training to perform its AND operations accurately when receiving propositional states. \nIn traditional deep learning models, the internal hidden layers of the model are often challenging to interpret and treated as black boxes. It is difficult to accurately explain the specific meaning and role of each parameter, such as the weight and bias of the neuron, and how they work together to achieve the logical operation of the entire network. LNN attempts to use logic gates and map logic rules directly into the network's structure, but its learning process is still a black box. Although the method can perform specific logical operations, the detailed mechanism of how these logical operations are represented and processed inside the LNN needs to be more intuitive.\nDespite this, LNN is still a meaningful attempt to use the same representation method in the same representation space to perform the operations of neural networks and logical symbols. First of all, it abandons the traditional representation conversion layer and attempts to adopt a fusion approach to process these different types of data and perform logical operations, which can achieve knowledge alignment of neural networks and symbolic logic more naturally and at the same time, complex data conversion and information loss are also avoided. In addition, since LNN directly maps logical operations into the neural network, the activation state of each neuron or neuron group can directly correspond to the truth value state of the logical proposition, so the decision-making process of LNN performing logical operations is more straightforward to explain. More importantly, this integrated processing method may bring new inspiration to the design of large language models, helping LLM stabilize internal concept representation and provide more accurate and interpretable logical chain reasoning capabilities.\nIn addition, the HDC (hyperdimensional computing) or VSA (vector symbolic architecture) method proposed by provides a different process from traditional neural networks and symbolic logic reasoning to implement Neuro-Symbolic AI. Chapter VI mentions that in this approach, data and concepts are represented as highly high- dimensional vectors capable of capturing complex patterns and relationships as a unified representation of symbolic and non-symbolic information. Therefore, traditional symbolic logic operations can be simulated by performing arithmetic and logical operations on high-dimensional vectors. At the same time, with the help of the orthogonality of vectors in high-dimensional space, HDC can retrieve stored information through simple approximate matching and support fast retrieval and associative memory. In addition, HDC methods can extract and generalize patterns from data and support complex decision-making and reasoning tasks by learning high-dimensional vector spaces, thus providing a natural and effective way to integrate symbolic logic and neural network processing. Because the above two reviews have introduced the HDC or VSA methods in detail, this article will not go into detail here."}, {"title": "7 DYNAMIC ADAPTIVE NEURO-SYMBOLIC AI", "content": "Compared with multimodal heterogeneous neuro-symbolic AI, this classification can dynamically adjust and adapt to computing tasks regarding multimodal data processing, symbolic logic processing, and internal representation adjustment. Currently, no research meets the requirements of dynamic adaptive neuro-symbolic AI. Specifically, this classification is characterized by the following features."}, {"title": "7.1 Automatic Selection and Integration of Appropriate Modal Data Processing Strategies", "content": "First, such a system can automatically select and integrate the most suitable modal data processing strategy according to the needs and context of the specific task when performing feature extraction through neural networks. For example, when faced with a dual-modal task of visual and text, the system may prioritize using visual features for preliminary extraction. When the visual information is insufficient to support symbolic logic decision-making, it may combine the contextual information provided by the text modality for in-depth reasoning. The selection and integration of this strategy is not statically preset but dynamically generated through the system's learning and adjustment process. This capability also gives the dynamic adaptive neuro-symbolic AI system more efficient, more accurate, and more energy-friendly features when processing multi-modal data."}, {"title": "7.2 Dynamically Adjust the Way Symbolic Logic is Processed", "content": "Secondly, the dynamic adaptive neuro-symbolic Al system can automatically select the form of symbolic logic processing according to the task's requirements. This feature means the system can handle various logical reasoning tasks and dynamically select the most appropriate symbolic logic processing method based on different task characteristics. For example, when processing tasks that require complex logical reasoning, the system may adopt more sophisticated and complex logic rules, while when processing simple or intuitive tasks, it may adopt a more direct logical processing strategy. This dynamic adjustment capability improves the system's flexibility in processing logical reasoning tasks and optimizes reasoning efficiency and energy consumption."}, {"title": "7.3 Self-adjust Internal Representation Based on Feedback and Task Performance", "content": "Finally, the ability to self-adjust internal representation based on feedback and task performance means that the system can automatically adjust and optimize internal data representation and processing logic based on actual task execution results and performance evaluations. Moreover, this self-adjustment includes not only fine-tuning of model parameters but also fundamental adjustments to model structure and processing strategies. For example, the system may find that the processing method of a specific modal data is not effective enough during the processing of the task, so it can automatically enhance its logical processing module by adjusting the processing strategy or switching to a more complex logical reasoning module when processing a specific task. This self-adjustment capability based on task performance allows the neuro-symbolic AI system to continuously adapt to various task requirements and environmental challenges."}, {"title": "8 ACKNOWLEDGMENT", "content": "I would like to thank my supervisor VS Sheng, whose expertise was invaluable in formulating the research questions and methodology. Your insightful feedback pushed me to sharpen my thinking and brought my work to a higher level."}]}