{"title": "FROM AN LLM SwarM TO A PDDL-EMPOWERED HIVE: PLANNING SELF-EXECUTED INSTRUCTIONS IN A MULTI-MODAL JUNGLE", "authors": ["Kaustubh Vyas", "Damien Graux", "Yijun Yang", "S\u00e9bastien Montella", "Chenxin Diao", "Wendi Zhou", "Pavlos Vougiouklis", "Ruofei Lai", "Yang Ren", "Keshuang Li", "Jeff Z. Pan"], "abstract": "In response to the call for agent-based solutions that leverage the ever-increasing\ncapabilities of the deep models' ecosystem, we introduce HIVE- a comprehen-\nsive solution for selecting appropriate models and subsequently planning a set\nof atomic actions to satisfy the end-users' instructions. HIVE operates over sets\nof models and, upon receiving natural language instructions (i.e. user queries),\nschedules and executes explainable plans of atomic actions. These actions can\ninvolve one or more of the available models to achieve the overall task, while re-\nspecting end-users specific constraints. Notably, HIVE handles tasks that involve\nmulti-modal inputs and outputs, enabling it to handle complex, real-world queries.\nOur system is capable of planning complex chains of actions while guaranteeing\nexplainability, using an LLM-based formal logic backbone empowered by PDDL\noperations. We introduce the MUSE benchmark in order to offer a comprehensive\nevaluation of the multi-modal capabilities of agent systems. Our findings show\nthat our framework redefines the state-of-the-art for task selection, outperform-\ning other competing systems that plan operations across multiple models while\noffering transparency guarantees while fully adhering to user constraints.", "sections": [{"title": "1 INTRODUCTION", "content": "Within the past few years, the number of available models \u2013either through commercial paywalls or\nopen-sourced- has exploded both in terms of intrinsic performances and in terms of tasks handled\nby them, ranging from text generation (Achiam et al., 2023; Anthropic, 2023; Team et al., 2023)\nto more specific actions such as code generation (Becker et al., 2023; Dong et al., 2024) or image\ngeneration (Wang et al., 2023b; Zhu et al., 2023). This rapid growth has unlocked unprecedented\npotential for real-world applications, inspiring practitioners, especially in industry, to envision new\nuse cases that leverage these powerful models (Liu et al., 2023c; Shen et al., 2024; Lu et al., 2024;\nXing et al., 2024). However, if creativity and possibility have been unleashed by such a surge,\nimplementing pipelines that involve multiple models remains a complex and largely manual (and\noften cumbersome) process, particularly when addressing tasks beyond the original design of these\nmodels. This often leads developers to create ad hoc modules to manage these complexities. In\naddition, a significant number of models available in the wild are either advanced proof-of-concept\nor very specialised ones, see e.g. the hundreds of thousands of models available on the HuggingFace\nplatform\u00b9. As a consequence of this abundance, navigating through this jungle to select the appro-\npriate models for a set of tasks has become very challenging. This complexity arises both in terms\nof performance and compatibility. Connecting models' input and output formats is complex, as the\ngenerated results are often difficult to control (Scholak et al., 2021; Qin et al., 2022). Moreover,\nplanning and chaining tasks for real-world use-cases present a significant challenge too.\nIn this study, we present a comprehensive solution to tackle the aforementioned two challenges, i.e.\n(I) selecting appropriate models and then (II) planning a set of atomic actions to achieve the objec-\ntives in the end-users' instructions (i.e., user queries.) Our system, HIVE, takes natural language\ninstructions (potentially involving multi-modal inputs and outputs) and can effectively schedule,\nexecute and explain plans composed of atomic actions. These plans may involve one or more mod-"}, {"title": "2 PRELIMINARIES", "content": "Planning Domain Definition Language (PDDL) (Aeronautiques et al., 1998) is a standardised lan-\nguage extensively used in the field of artificial intelligence (AI) planning to represent planning do-\nmains and problems. PDDL provides a formal syntax and semantics for defining the components of\na planning task, including actions, predicates, objects, and their relationships. It enables the clear\nspecification of the initial state, goal conditions, and permissible actions within a domain, facilitating\nthe development and comparison of planning algorithms. In our work, PDDL plays a critical role in\ntask decomposition and planning. By defining tasks as actions within PDDL domains, we leverage\nestablished planning techniques to generate coherent and feasible plans. The use of PDDL allows us\nto formally model complex tasks, ensuring that the system can reason about the preconditions and\neffects of actions within a well-defined framework.\nLet $D = {d_1,d_2, ...,d_{|D|}}$ be a set of PDDL domains. Each PDDL domain $d_j \\in D$ is associated\nwith a set of PDDL actions s.t. $a_{dj} = {a_1^{dj},a_2^{dj},...,a_{A}^{dj}}$, where $j \\in [1, |D|]$ and $A \\in N$ the num-\nber of actions included within the PDDL domain $d_j$. Furthermore, let T be a set of different tasks,\nconsisting of all PDDL actions across the available PDDL domains $\\in D$, as follows: $T = \\bigcup_{j=1}^{|D|} a^{d_j}$.\nFinally, we define $M = {m_1, m_2, ...,m_{|M|}}$ as the set of all models available for completing a set\nof different tasks T or combinations thereof."}, {"title": "3 HIVE - GENERAL ARCHITECTURE", "content": ""}, {"title": "3.1 \u0421\u0410\u0420\u0410BILITY KNOWLEDGE GRAPH", "content": "We extract model cards\u00b2 directly from HuggingFace and incorporate an OpenIE extraction route for\nconverting the textual descriptions from each model card into a structured representation. We align\nthe models with Papers With Code\u00b3, which enables us to collect information about how a particular"}, {"title": "3.2 PLANNING MODEL ACTIONS", "content": "Having extracted and systematically structured the information pertinent to models associated with\nvarious tasks, we are now positioned to delineate the specific actions required to accomplish the\nobjectives in the user query, as depicted in Figure 1."}, {"title": "3.2.1 TASK PLANNER", "content": "Parsing User Query User queries are often vague and unstructured, making it challenging for sys-\ntems to understand the user's intent accurately. To overcome this, we introduce a parsing-rephrasing\nstage that bridges the gap between the ambiguous query and the system's structured requirements.\nThis initial step sets the foundation for the subsequent stages, enabling the system to extract relevant\ninformation.\nWe parse an input user query q into distinct components: instruction (i: str), input text (t: str),\nquestion (s: str), URL (u: str), data (x: dict), and categories (g: list), as follows:\n$P(q) = {i, t, s, u, x, g}$                                  (1)\nwith P the parsing function. The ability of LLMs to parse user queries into structured formats,\nlike JSON, has been highlighted across multiple research efforts (Petroni et al., 2019; Wei et al.,\n2023). Using prompt engineering with a few-shot setting Brown (2020) (see Appendix C), we guide\nthe LLM to convert an unstructured user input into structured data. Additionally, we ask the LLM\nto rewrite the user instruction to enhance clarity, simplifying complex directives and converting\nimplicit information into explicit statements.\nThe instruction is crucial as it is used in the later stages to decompose the user query into smaller\nparts and determine objectives in the user query, q. By transforming vague queries into well-defined\ncomponents, our system becomes more robust to handle diverse and complex user inputs.\nTask Decomposition Given the resulting instruction, after processing the input user query q, we\nproceed to decompose it into smaller, manageable steps to identify a specific plan to attain the\nobjectives (Wei et al., 2022; Yao et al., 2023b) and understand how each part of the instruction is\nassociated with achievable goals within the system's capabilities. We utilise an LLM as a classifier\n(Zhang et al., 2024) in a few-shot example setting (Brown, 2020) (see Appendix C) to identify the\nrelevant domains from the original set of PDDL domains, D.\nBy providing the LLM with examples of instructions and their associated domains, we guide it to\nselect the pertinent subset $D^* \\subset D$ that aligns with the instruction. We prioritise recall in this\nclassification step to ensure that all potentially relevant domains are considered, minimizing the\nrisk of missing critical actions required to fulfil the user's objectives. This approach enhances the\nsystem's robustness by accounting for a wider range of possible actions.\nOnce we determine the subset of relevant PDDL domains, D*, we leverage the predefined PDDL\nstructures of each classified domain. These domain structures are then mergedto create a unified\nPDDL domain file inclusive of all actions from $\\bigcup D^*$, s.t.\n$a_{D^*} = \\bigcup_{j=1}^{|D|} a^{d_j}    d_j \\in D^*.$                                     (2)\nThis method ensures that the combined domain file encompasses all necessary actions while main-\ntaining consistency and comprehensibility. Next, we exploit the parsed instruction i and the com-\npiled set of actions from D*, $a^{D*}$, with an LLM to determine the specific actions required to achieve\nthe instruction's objectives (see Appendix C), as follows: $a_{P^*} \\subset a^{D*}$.\nThis allows to precisely map high-level user intents to concrete actions. Following this selection, the\ncombined PDDL domain file (i.e. $a^{D*}$) and the identified actions set (i.e. $a^{P*}$) help to reconstruct\nthe corresponding PDDL problem. Finally, a Best First Width Search (Lipovetzky & Geffner, 2017)\nlogical reasoner computes a detailed Plan of Actions ordering the actions $\\in a^{P*}$ such that the system\ncan execute step-by-step, ensuring coherent execution in line with the user's intent.\nThis hierarchical approach not only enhances the system's robustness in parsing and understand-\ning diverse and complex user inputs but also guarantees the accuracy and feasibility of generated\nactionable plans by rigorously structuring them within established domain constraints."}, {"title": "3.2.2 MODEL SELECTION", "content": "Following the task planning, the next stage is the selection of appropriate models capable of per-\nforming the specified actions in the plan. Utilising the information from our C-KG (see Sec-\ntion 3.1), our goal is to identify a model combination $M^* \\subset M$ that would satisfy a set of conditions"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 BASELINES", "content": "Addressing complex, multi-modal real-world tasks presents substantial challenges, and current so-\nlutions are limited. For our experiments, we compare our proposed method against the most relevant\nstate-of-the-art techniques: HuggingGPT (Shen et al., 2024) and ControlLLM (Liu et al., 2023c).\nThese represent significant advancements in integrating LLMs with task planning and execution\nframeworks.\nWe propose two innovative methods: HIVE and HIVE light. HIVE leverages the advanced capabil-\nities of ChatGPT for parsing user queries and decomposing tasks. To address the computational\nchallenges associated with ChatGPT-based systems, we designed HIVE light as an efficient alterna-\ntive that can be deployed on local servers. HIVE light employs InterLM2.5-7B-chat Cai et al. (2024)\nfor parsing user queries and Mistral-7B-Instruct-v0.39 for task decomposition, both of which have\nbeen subjected to 8-bit quantization. We selected a chat-oriented model for parsing, as conversa-\ntional models excel at understanding subtle queries. Additionally, an instruction fine-tuned model\nwas chosen for task decomposition due to its capability to deliver precise instruction clarity. By\nemploying this dual-method setup, we ensure a thorough performance evaluation, positioning HIVE\nand HIVE light as strong competitors to existing state-of-the-art frameworks."}, {"title": "4.4 DISCUSSIONS", "content": ""}, {"title": "4.4.1 CROSS-MODALITY PERFORMANCES", "content": "To gain a better understanding of the multi-modality of these systems' capabilities, we dig deeper\ninto the Final Output (O) results from Table 1. We divide this investigation into three distinct parts\nbased on the output modality and analyze the performance when the other two modalities are in-\nvolved in the input.\nWhen it comes to text output, HIVE light demonstrates a substantial lead over its competitors when the\ninput includes any image or audio. This showcases HIVE light's ability to integrate visual and auditory\ndata to enhance text outputs. In the context of image output, HIVE light once again outperforms\nthe other systems, illustrating its proficiency in converting textual and audio inputs into coherent\nvisual responses. Lastly, although our system shows commendable performance in text-based audio\ngeneration, it falls short of achieving the desired objective in the image-to-audio scenario, indicating\nan area for potential improvement in future iterations."}, {"title": "4.4.2 TRUSTWORTHINESS", "content": "In order to review the connection between justifications (i.e. the conjunction\u00b9\u00b9 of TS and FoT)\nand outputs (O), we group in Figure 3 the results based on (justification, output) scores which can"}, {"title": "4.4.3 ROBUSTNESS", "content": "Since most of MUSE's queries require multiple models to interact together in a compatible manner,\nwe noticed that sometimes the tested systems fail at dealing with either the justification or the output\nparts. In Table 3, we list the different cases encountered. The first point to be highlighted is that,\namong the four systems, HuggingGPT is the less robust one by far: 22 Err against 10, 7 and 6\nfor the other solutions. Second, even more critical, is that HuggingGPT, unlike the competition, is\nable to generate correct results (T) while failing (Err) in its plan construction. This exacerbates\nthe fact that its justifications cannot be trusted, as the executed actions tend in many cases not to be\nconsistent with the compiled plan, using GPT-3.5 at most places. This last finding is coherent with\nthe IT discussion in Section 4.4.2."}, {"title": "4.4.4 LATENCY FOR PLANNING", "content": "Lastly, in this discussion section, we analyse the time performances (in seconds) of the systems\nto come up with a plan and select suitable models. As the chosen models may differ and no en-\nforced rules such as \"the quicker the better\" (see Section 4.5 for discussion about model selection\ncapabilities) were added, we measure the latencies up to the model selection stage."}, {"title": "4.5 TAKING INTO ACCOUNT USERS' CONSTRAINTS IN TERMS OF MODEL SELECTION", "content": "As depicted in Section 3.2.2, once a plan of actions is established, HIVE selects the best models\nto realise them. Obviously, depending on the circumstances, the definition of what is \u201cbest\" may\nvary a lot, e.g. when resources are sparse, one may decide to use the smallest models possible\neven if the resulting quality is reduced, alternatively users might choose to select models based on\ntheir respective (recorded) results for specific benchmarks. In order to respect these various cases,\nHIVE allows users to specify selection criteria. In this Section\u00b9\u00b2, we review the capabilities of HIVE\nagainst HuggingGPT when users want to force some conditions of their own in the model selection.\nSince ControlLLM has one-to-one mappings of models for each task, it is de facto excluded.\nPractically, we use the following query\u00b9\u00b3: \u201cTranscribe the audio from .audio_1.wav\nand find entity tokens\". Regarding the task-model mappings, we let both HIVE and Hug-\ngingGPT have access to: openai/whisper-large-v2 and nvidia/parakeet-rnnt-1.1b (having respec-\ntively Apache-2.0 and CC-By-4.0 for licenses) for the ASR; and to dslim/bert-base-NER (MIT\nlicense) for NER. We first run the query without any constraints (control run, see Appendix B):\nboth systems, HIVE and HuggingGPT, were able to transcribe the audio file and perform NER (even\nthough HuggingGPT result set was empty). We then applied the following model selection con-\nstraints sequentially:\n1. License restrictions: only use Openrail++ and Deepseek \u2013 HIVE returned nothing which was\nthe expected behaviour as the available models were not having the requested licenses; on the\nother hand, HuggingGPT performed the task as in the control therefore infringing the restrictions\n(see Appendix B).\n2. Uses the \"smallest possible\" model\u00b9\u2074 \u2013 HIVE complied with the user choice and used the\nsmaller models whereas HuggingGPT kept using openai/whisper-large-v2 as in the control run\n(see Appendix B).\n3. Filter for the model having the best results at the speech recognition on common\nvoice english\u00b9\u2075 benchmark Using the benchmark records from the C-KG, HIVE was\nable to select the correct model unlike HuggingGPT which chose models like in the control run\n(see Appendix Table B).\nOverall, HIVE answered each time while properly taking into account the given constraints. While\nHuggingGPT failed every time, misleading even the users with regards to its justifications (refer to\nSection 4.4.2 for further justifications on this)."}, {"title": "5 RELATED WORK", "content": "Automated Planning. The cognitive ability to organize and coordinate actions toward a specific\ngoal is referred to as planning. While humans innately possess this capacity, machines lack such a\ncapability. Automated planning has garnered significant interest from researchers across various do-\nmains, including robotics (Guo et al., 2023), autonomous vehicles (\u00c1ngel Madridano et al., 2021),\nand dialogue systems (Wang et al., 2023a). The methodologies employed to devise sequences of\nactions have evolved considerably, particularly in light of recent breakthroughs in deep learning."}, {"title": "LLM-as-Agent", "content": "The genesis of large language models (LLMs) primarily stemmed from textual\ncontent, which initially narrowed the research focus to text generation. However, to address the\ndiversity of real-world scenarios, significant efforts have been directed toward developing vision or\nspeech LLMs, thereby aligning with a multi-modal paradigm (Zhu et al., 2023; Wu et al., 2023;\nWang et al., 2023b). exemplify this trend. Additionally, to expand the capabilities of LLMs, there\nhas been an increasing trend to integrate external tools with LLMs. Toolformer (Schick et al., 2024)\npioneered the invocation of tool calls within generated sequences via special tokens giving rise to\ntool-augmented LLMs (Qin et al., 2023a;b; Guo et al., 2024; Qu et al., 2024). Then, ReAct (Yao\net al., 2023b) introduced such intermediate tool calls during the reasoning process by incorporat-\ning intermediate outcomes within the prompt to better guide the final resolution of the problem. In\ncontrast to ReAct, Reflexion (Shinn et al., 2023) adds verbal feedback on those intermediate results\nto further assess and verify outcomes, In the meantime, a plethora of fine-tuned LLMs tailored for\nspecific tasks has become ubiquitous on platforms such as Hugging Face Hub (Wolf et al., 2019),\nalongside proprietary models such as GPT-4 (Achiam et al., 2023), Claude (Anthropic, 2023), and\nGemini (Team et al., 2023) offering the opportunities to consider these LLMs as distinct agents.\nIndeed, the gathering of technical details for each parametric model stands as a critical compo-\nnent in the reporting and tracking efforts underlined by the use of Model Cards Mitchell et al.\n(2019). HuggingGPT (Shen et al., 2024), leverages such a large pool of LLMs using ChatGPT as\nthe core controller. Following a similar approach, ControlLLM (Liu et al., 2023c) and Chameleon\nLu et al. (2024) explore task planning via prompt engineering and integrate a more diverse pool of\ntools. While HuggingGPT, ControlLLM or Chameleon appoint appropriate models for each sub-\ntask, however, their model selection process remains sub-optimal as they do not identify the most\naccurate model. Thus, if these frameworks can fulfil their plans, the resulting performance may be\nunsatisfactory if the best agent is not utilized. To the best of our knowledge, our work represents the\nfirst attempt to address this gap."}, {"title": "6 CONCLUSION", "content": "Our research introduces HIVE, an innovative and comprehensive solution designed to navigate the\ncomplexities of model selection and task planning using a diverse set of deep learning models.\nBy leveraging a Capability Knowledge Graph and an LLM-based formal logic planner, we tran-\nscend the limitations of the existing systems. HIVE stands out for its capability to plan and explain\ncomplex action chains while respecting user-specific constraints \u2013thereby achieving both high per-\nformance and full transparency. Empirical evaluations on our newly designed benchmark reveal\nHIVE's superior performance, consistently outperforming competing platforms like HuggingGPT\nand ControlLLM. This breakthrough underscores HIVE's potential to redefine the state-of-the-art in\ntask selection and planning, ultimately facilitating more efficient and user-friendly applications of\nadvanced deep models. HIVE thus advances the handling of multi-modal tasks."}]}