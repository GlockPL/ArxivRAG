{"title": "Application of AI to formal methods \u2013 an analysis of current trends", "authors": ["Sebastian Stock", "Jannik Dunkelau", "Atif Mashkoor"], "abstract": "With artificial intelligence (AI) being well established within the daily lives of research communities, we turn our gaze toward an application area that appears intuitively unsuited for probabilistic decision-making: the area of formal methods (FM). FM aim to provide sound and understandable reasoning about problems in computer science, which seemingly collides with the black-box nature that inhibits many AI approaches. However, many researchers have crossed this gap and applied AI techniques to enhance FM approaches. As this dichotomy of FM and AI sparked our interest, we conducted a systematic mapping study to map the current landscape of research publications. In this study, we investigate the previous five years of applied AI to FM (2019-2023), as these correspond to periods of high activity. This investigation results in 189 entries, which we explore in more detail to find current trends, highlight research gaps, and give suggestions for future research.", "sections": [{"title": "1. Introduction", "content": "Artificial intelligence (AI), and especially the subdomain of machine learning (ML), is becoming increasingly relevant for all industries and increasingly penetrates research communities, as several studies show [1, 2, 3, 4]. Consequently, AI applications also gained popularity in software development [5, 6].\nIn this study, we ask whether this trend is also observable within the formal methods communities. Formal methods (FM) correspond to a mathematically rigorous approach to software and systems development in which the concern is to provide a formal representation of a hardware, software, or systems engineering problem [7]. This representation then lends itself to mathematical assessments such as correctness, soundness, or well-definedness. Furthermore, these assessments can be easily reproduced and followed by the practitioner. The needed guarantees and the rigorousness of these methods seem to stand in tension with AI applications, which often show nondeterministic behavior and mostly correspond to black-box techniques. However, much like in software development, AI-driven tools might still supplement the formal development process without impeding the correctness of the results. And from personal experience, we noticed an increase in publications applying AI techniques to FM in recent years, suggesting that the FM field is following the overall trend as well. To substantiate this and to obtain an overview of which FM subdomains already utilize AI, we performed this systematic mapping study (SMS) [8]. The primary goal is to observe the development of AI applications quantitatively to the field of FM.\nIn this work, we report on our journey and the results of our SMS, accessing the quantitative nature of the topic of AI application in the FM domain. This represents a typical goal for this kind of study, as pointed out by Kitchenham et al. [9]. Mainly, we are interested in which FM domains are targeted by AI improvement already, which AI or ML applications have been investigated in the literature so far, where potential research gaps exist, and whether there appears to be consensus within the FM domains of which AI techniques seem most beneficial. A challenge to this assessment is the wide variety of topics on both sides. AI and FM are huge topics, and their potential overlap may be considerable. This study aims to be the first assessment of the general research landscape, i.e., we assume a mainly quantitative lens on the topic. Consequently, we highlight the areas which received more attention in the literature as well as those that received less"}, {"title": "2. Background", "content": "attention. We refrain, however, from doing a qualitative assessment (e.g., a systematic literature review) our goal is to gain an initial overview of the field's landscape.\nWhile our search process resulted in a data set of 457 highly relevant publications covering more than 50 years of research applying AI to FM, we investigated only articles from the last five years in more detail. This period is chosen as it corresponds to a peak in the number of publications as indicated by previously mentioned studies and also allows us to focus on the most recent developments. Consequently, we end up with a tally of 191 primary studies, two of which are pure data set presentations. We analyzed the remaining 189 studies to determine which AI techniques were found to be applicable in which FM areas. Further, we derive research suggestions from our insights by pointing out research gaps that might further mature the field. Both the data set of the last five years as well as the complete data set of 457 publications are made publicly available to interested researchers at https://github.com/hhu-stups/ai4fm-studies.\nThe rest of this mapping study is structured as follows: Section 2 introduces the relevant subdomains of FM and AI. It will also provide an overview of our understanding of AI. We present our leading research questions in Section 3 and detail our search and selection process in Section 4. Section 5 presents the results of the search while Section 6 engages in a deeper discussion about these results. In Section 7, we discuss potential threats to the validity of our conducted systematic mapping study. Section 8 presents related studies we found during our research and compares them against our efforts. Finally, we conclude in Section 9."}, {"title": "2.1. Systematic mapping studies", "content": "Systematic mapping studies, as proposed by Kitchenham et al. [10], aim to assess the quantitative nature of a research field. Here, the goal is to give an overview of the available research. SMS complement systematic literature reviews by comparing research contributions and their qualitative impact.\nSMS are typically performed using a multi-step approach. First, the field of interest is selected. Then, research questions are formulated, and inclusion criteria (IC) and exclusion criteria (EC) are defined. IC and EC will serve as filters to help select relevant results."}, {"title": "2.2. Formal methods", "content": "Formal methods (FM) describe a mathematically rigorous approach to design software as well as hardware systems, and are concerned with analysis, validation, and verification at any part of the respective system's life cycle [11, 12]. Employing FM allows to formulate precise statements of desired functionality or requirements in form of a formal model (or formal specification) while not constraining the possible implementation thereof. The formal model is represented in a mathematically approachable form which lends itself to reasoning and hence allows rigorous analysis of critical properties such as correctness, safety, soundness, or well-definedness.\nIn the following, we overview FM creation and reasoning methods most relevant to this works.\nModel checking. Model checking [13, 14] explores a program's or systems' state space. The state space sets all possible value constellations the program can achieve. In its most basic form, explicit state model checking, model checking aims to explore all reachable states and check if they are faulty, i.e., if they violate any specified properties. If this is the case, a counter-example is found. Otherwise, the system works correctly, as no faulty states are reachable.\nThere are many flavors besides explicit state model checking. Noteworthy for this study are linear temporal logic (LTL) model checking, symbolic model checking, and statistical model checking. LTL model checking encodes temporal properties of interest into a formula that is checked against execution traces of the state space. Symbolic model checking abstracts the state space and makes a symbolic evaluation of a complex expression, resulting in the ability to handle even infinite state spaces for the price of precision. Statistical model checking investigates state spaces created by assigning probabilities on transitions between states and provide probabilistic guarantees."}, {"title": "Theorem proving.", "content": "A core application of theorem proving (TP) is to evaluate statements about a formal model's internal consistency and behavioral integrity. When talking about theorem proving, we usually assume some formal representation of the problem for which proofs are formulated and discharged. Discharging proofs can either be done by a fully automatic theorem prover (ATP) [15] that finds a solution on its own, or an interactive theorem prover (ITP) [16] that requires human input for individual proof steps and transformations. ITPs provide a user interface to keep track of progress, sub-goals, and available properties."}, {"title": "SAT/SMT solving.", "content": "The Boolean satisfiability problem (SAT) [17] refers to finding the right model for a boolean formula to satisfy the formula. Historically, SAT was the first problem found to be NP-complete and has since drawn a broad research interest.\nSatisfiability modulo theories (SMT) [18] is a more generalized form of the SAT which enriches the problem setting with various theories such as arithmetics, data structures, or set theory.\nIn the context of FM, SAT and SMT solving find applications in theorem proving [19], bounded model checking [20], equivalence checking [21] or test generation [22]."}, {"title": "Synthesis.", "content": "Under the term of synthesis, we group all attempts automatically or semi-automatically create (parts of) formal models during the formal development process, such as the generation of models from natural language specification or vice versa. We further group the idea of automated model repair under this aspect, where for a model with some violation, a fix is synthesized that rectifies the violation."}, {"title": "Other categories.", "content": "In the scope of this mapping study, we may encounter FM approaches and techniques, which do not belong to any previous group. We will group these as other FM techniques. For instance, this includes meta-approaches such as selecting the right verification algorithm, general program analysis, or termination analysis."}, {"title": "2.3. Artificial intelligence", "content": "Artificial Intelligence (AI) is a catch-all phrase for various techniques and approaches that aim to imitate intelligent decision-making. Simmons and"}, {"title": "Neural networks and deep learning.", "content": "Chappell [23] define the term as \u201c[...] behavior of a machine which, if a human behaves in the same way, is considered intelligent\". They further emphasize the problem with the term itself, pointing out the difficulty of precisely defining what it means for a human to be intelligent. Nevertheless, AI is regularly used in daily life and the research community.\nAn important AI paradigm is machine learning (ML). The goal of ML algorithms is to observe and extract patterns within the data which can then be applied to the automation of a given task [24]. Instead of manually defining a set of rules for the pattern recognition, machine learning algorithms uncover such rules in an automated fashion, a process referred to as learning. ML algorithms experience the given data in some manner and improve their exhibited performance to solve the task at hand gradually, the more experience they gather.\nBelow, we list and briefly explain AI and ML algorithms which are most relevant to this mapping study. For the reader familiar with AI, the selection seems mostly ML rather than AI. Indeed, we aimed to find applications of AI to FM, and the search query we will discuss later was prepared accordingly. However, in order to bring a result up front, most contributions use some form of ML.\nNeural networks (NNs) [25] build the foundation of what is known today as the area of deep learning [26]. A deep neural network (DNN) consists of a layer of inputs, a layer of outputs, and one or more hidden layers. Inputs are forwarded layer-wise and combined first by a weighted sum and a follow-up non-linear transformation. The clue is the training of a DNN, which starts the network with randomly initialized weights for the summations above but adjusts them gradually in the learning process to converge to the desired function.\nA considerable benefit of neural networks lies in their variety. Different approaches and architectures exist for deep learning in specialized environments with differently shaped data. The most important for this work are convolutional neural networks (CNNs) [27] for images, recurrent neural networks (RNNs) [28] for sequential data, the transformer architecture [29] for language processing, and graph neural networks (GNNs) [30]. Today, deep learning is considered one of the most popular topics in ML [31]."}, {"title": "Reinforcement learning.", "content": "Reinforcement learning (RL) [32] is foremost a machine learning paradigm. Given an environment, a set of actions that change the environment, and a reward function that quantifies a given environmental state, the RL agent's goal is to learn a policy over their available actions which maximizes the cumulative reward over time. Initially, the agent does not know the environment or the task it is supposed to solve but can only learn from feedback through the received rewards [33]."}, {"title": "Natural language processing.", "content": "Natural language processing (NLP) [34] describes the research area of, as the name suggests, processing and evaluating texts of human language. This includes information extraction, language translation tasks, text classification, semantic analysis, and natural language generation and dialogue systems [34, 35].\nOne of the most recent developments is large language models (LLMs) [36] which gained significant popularity and even worldwide attention with the release of ChatGPT [37]."}, {"title": "Genetic/evolutionary algorithms.", "content": "Genetic and evolutionary algorithms (EAs) [38] describe a family of optimization algorithms that are rooted in the phenomenon of evolution. An EA starts with a randomly assembled population of candidates for a given optimization problem and a fitness function. The fitness function measures how well a candidate solves the problem. Parent candidates are chosen via a random selection process that is nonetheless influenced by each individual's fitness. These are adapted using mutation or recombination to produce a new child population. Repeating this process converges the individuals toward suitable solutions for the problem at hand."}, {"title": "Statistical approaches.", "content": "In contrast to the abovementioned techniques, we consider techniques from more classical, statistical ML. These differ from the above methods as they do not depend on deep learning and are typically faster to compute in comparison. This does, however, not imply that they are not competitive. The considered statistical approaches include support vector machines (SVM) [39], logistic and linear regression (LR) [40], k nearest neighbors (KNN) [41], support vector machines (SVM) [42], decision trees (DT) [43] and random forests (RF) [44], or Bayesian inference approaches [45]."}, {"title": "2.4. Contribution classification", "content": "We classified the contributions that we selected via the PICO [10] methodology. Thus, we facilitate a clear differentiation between them and allow for a more refined analysis.\nThe different types of contributions are distinguished as follows:\nTool. Contributions that provide practical means to solve an FM problem, for instance, in the form of a binary or source code release.\nTool enhancement. Contributions that aim to enhance or complement an existing tool.\nApproach. Contributions that propose an overall style or idea to overcome a problem. An approach is a generalized concept that describes how to react or overcome a problem in depth. The approach remains more theoretical and does not involve tested or empirically proven steps.\nMethodology. Contributions that realize and test an approach.\nFramework. Contributions that propose a conceptual structure intended to support or guide the construction or expansion of a solution for an actual problem.\nCase study. Contributions applying an approach, a methodology, or a framework to a real-world problem.\nBenchmark. Contributions that test an existing tool or methodology.\nIdea. Contributions that very briefly overview how a problem may be solved. Compared to an approach, it does not lay out larger theories but serves as a quick pitch.\nTraining data. An openly accessible data set that claims to be reusable outside of replication of the contribution it was originally used in.\nWe further divide the publication venues into the following categories:\nWorkshop and short papers. We grouped contributions in this category if they are (a) rather short (4 pages or less) or (b) if the venue they appeared in has workshop character, i.e., is classified as a workshop, calls itself a workshop, or focuses on the in-person presentation of results but requires submission of an extended abstract."}, {"title": "Conference proceedings.", "content": "Here, we group all the contributions that are published in the proceedings of a conference. From a full conference paper, we expect a more in-depth explanation of the result than compared to a workshop or short paper, ultimately resulting in an overall more sizable contribution."}, {"title": "Journal articles.", "content": "Here, we group all contributions published as part of a journal volume. As journal articles tend to consist of more pages and go through a more thorough reviewing process, the results of journal articles are expected to have a broader scientific basis for their claims."}, {"title": "Book chapters.", "content": "As the name suggests, these contributions are part of a larger collection of articles bundled in one book. As the production of scientific books takes a significant amount of time, the reader expects a high quality of the provided scientific contribution that is considered, by that time, state of the art."}, {"title": "3. Goal & Research questions", "content": "To get an overview of the field, we divide our research questions (RQs) into two main areas. First, RQs 1.1 to 1.3 focus on the overall demographics of the research field. With these questions, we want a basic quantitative overview to explore the field's maturity. Second, RQs 2.1 to 2.4 focus on the content of the contributions and aim to map out the field, highlighting potential gaps and showcasing quantitative interest in respective sub-topics. Our research questions are defined as follows."}, {"title": "RQ 1: Demographics of the research area", "content": "1.1 What is the research publication timeline, and is there a trend?\nRational: We are interested to know whether the field experiences a growth or decline in interest, or if no trends are detectable at all.\n1.2 Which publication venues are most frequent in the field?\nRational: We want to explore the field's maturity, whereby books, book chapters, and journal articles indicate more mature results.\n1.3 What are the provided main contributions by the primary studies?\nRational: We want to learn where the community focuses its efforts, e.g., more on the theoretical groundwork or more on practical application."}, {"title": "RQ 2: Content type of contributions", "content": "2.1 Which AI techniques and tools were used? Are there any prevalent choices within the community?\nRational: We want to know if there is an indication that some AI techniques are more relevant (or at least popular) than others.\n2.2 What are the application areas of AI in FM? Are there any commonly found FM techniques or tools? Rational: We are curious about which FM techniques currently get the most attention or if there are dark spots in the literature.\n2.3 What is the distribution of AI types in the different FM application areas?\nRational: Here we want to overlap the results of RQs 2.1 and 2.2.\n2.4 Are the studies' employed data sets publicly available? Rational: Especially in machine learning, having the source material available and accessible for experiments benefits the scientific community."}, {"title": "4. Search strategy", "content": "In the following, we explain our approach for aggregating the contributions for this mapping study which we have visualized in Fig. 1. For this, we follow the suggested multi-step approach by Petersen et al. [8], which consists of an initial search in meta-search engines, followed up by a snowballing process. As it will show, our selection of topics provided a challenging environment which has overlapping search terms with related research areas. To avoid a result set of tens of thousands of potential studies, we tweaked our search strategy to deal with these challenges.\nA vital role played the IC & EC, which we applied twice. The first application was made after the initial search to remove unrelated contributions that would introduce an overheat into the snowballing. The second time was again after the snowballing to filter wrongly selected contributions."}, {"title": "4.1. Employed search query", "content": "Finding a search query that would return a manageable amount of relevant contributions proved difficult. The main problem was harnessing the bandwidth of terms used in the AI and FM domains. Hereby, the difficulty lies in the often ambiguous use of terms like \u201cformal method\", which can be used in the sense of formal methods as outlined in Section 2.2 or as a term to describe an only somewhat formal approach to a problem. The situation is even more difficult for AI as AI and ML are often used interchangeably. In contrast, essential publications may only use the specific name of an applied technique without naming the fields of AI or ML.\nAnother issue was the cross-pollution of our results with studies from the related research field of verification for AI and ML systems, which is essentially the other direction in which FM is applied to AI. This field is also a highly relevant topic and shares the majority of potential search key phrases, further inflating our results tally.\nAs a reaction, we experimented with different approaches and probed the quantity and quality of their respective results to find a suitable search query. Highly abstract queries such as\n\"formal methods & artificial intelligence\"\nproduce multiple thousands of results which we deemed impossible to process in a reasonable amount of time and which contained many studies outside of our intended scope. From probing some results as well as personal experience, we also knew that many relevant publications tend not to use these high-level terms, but use nomenclature specific to their sub-community instead. For instance, research regarding the formal B method tends to simply use \u201cB method\" instead of \"formal method\" as keyword. On the other hand, we noticed how relevant publications seemed also to prefer to use the concrete names of the applied AI algorithms rather than stating they used AI in a superficial manner. Therefore, we were concerned that a search query that is too abstract would not be able to penetrate the relevant fields well enough.\nAs a result, we settled on a more complex query containing the precise terminology of specific algorithms. For this, we distinguish between two sets of terms, the AI-terms (Fig. 2b) and the FM-terms (Fig. 2a). Both correspond to a disjunction of a set of selected terms from the respective field. The terms themselves were selected based on previous experiences in the field following PICO Kitchenham et al. [10].\nThe final search query (Fig. 2c) now makes use of both term aggregates to produce a corpus of primary studies in a strict and targeted manner. That is, we only looked for studies that make use of at least one AI term and at least one FM term in their titles, in their abstracts, and in their keywords. The reasoning behind this is to get a good initial penetration of all key fields while relevant but missed entries are found in the later search stages via snowballing."}, {"title": "4.2. Database search and processing", "content": "The search was conducted in the last quarter of 2023. We used four meta-search engines\u00b9 as suggested by Petersen et al. [8]: IEEE\u00b2, Scopus 3, ACM4 and Web of Science (WoS)5. We used the Guide to Computing Literature for ACM. Furthermore, as the ACM search engine limits the number of wild cards, we split the search terms into subsets, performed the sub-searches, and merged them into the required superset. For WoS, results seemed to differ depending on which institution has access. Therefore, we decided to take the more extensive result set. As far as the search engines allowed, we applied the EC, such as the area of publishing and the publication language. The result size after this step was 1492 entries. This number and all later numbers are without duplicates."}, {"title": "4.3. Inclusion and exclusion criteria", "content": "For the 1492 resulting contributions, we made two observations. First, the amount was too large for a snowballing procedure to be feasible. Second, while briefly looking at the corpus, we discovered many contributions that should not have been selected, i.e., contributions that included the term \u201cspecification\u201d but meant it in a purely requirement engineering-minded context. Therefore, we decided to apply our IC and EC prematurely to the corpus as far as they applied, i.e., to title and abstract. For this, we read the titles of our studies, and if we could not decide whether the contributions should be included, we also conducted the abstracts, and very rarely, the contributions itself was skimmed.\nTo achieve this for such a large corpus, the first and second authors passed over all entries individually and decided whether they should be included or excluded. In agreement cases, the respective studies were kept or discarded, respectively. For cases of disagreement, both authors discussed each instance together to find an agreement. The disagreement could not be settled this way in two cases, and the third author was consulted as a tiebreaker. This"}, {"title": "4.4. Snowballing", "content": "procedure brought the number of primary studies down to 89 but took a significant amount of time.\nThe applied IC and EC are detailed in the following.\nIC 1: The contribution focuses on applying techniques from the domain of AI to the domain of FM.\nIC 2: The contribution is in English.\nIC 3: The contribution has at least two pages of content (excluding references).\nIC 4: If the contribution was submitted in different versions, we took longer. For instance, the journal version was taken for a conference contribution and subsequently published as an extended version. The journal version was taken for a journal-first submission and an invited version.\nIC 5: The contribution is a data set used for ML.\nEC 1: The contribution appears to be not reviewed in any form.\nEC 2: The contribution is not available.\nEC 3: The contribution not posed in computer science.\nEC 4: The contribution is unclear (even after reviewing the full text).\nEC 5: The contribution solely focuses on solving a mathematical optimization problem.\nEC 6: (During initial skim) The contribution focuses only on common or general constrain-solving techniques (without specializing in SMT/SAT)\nEC 7: (During initial skim) The contribution focuses on portfolios for SAT solvers without explicitly mentioning any AI/ML technology\nEC 8: (During initial skim) The contribution is not a primary study, i.e., it is another mapping study, survey, etc.\nEC 9: (Snowballing) The contribution is concerned with 2Sat, which is solvable in polynomial time.\nEC 10: (Snowballing) If the contribution proposes a way to synthesize some automaton (e.g., a Markov process), then the produced result must explicitly be used within the context of FM. This means that the synthesized automaton and its analysis with FM's help are described within the contribution.\nFor the EC, some additional criteria were added in later steps in response to contributions encountered that were of low value for this study. For instance, EC 6 was added because many contributions focused on constraint solving, thus being closer to mathematics. EC 7 was added in response to a large influx of SAT contributions that mentioned learning but focused on remembering already-seen clauses. EC 8 is a rule that was introduced to not have secondary studies in the result set but to use those studies to ensure that even with a restrictive query, we cover as much of the field as possible. EC 10 was added in the fifth snowballing round, where we got many contributions that synthesized automatons, but no further analysis was mentioned. EC 9 was added as we found 2Sat as a problem not relevant for this study, as solutions are achievable in polynomial time already.\nFor the 89 entries, we conducted an extensive snowballing procedure [46] where we conducted both forward and backward snowballing. Five iterations were needed until we eventually reached a closure. This extensive snowballing approach was meant to complement the strict search query and to uncover relevant yet initially missed studies or even subfields of research. Therefore, snowballing served as a means to alleviate these threats to validity.\nFor the backward snowballing, we consulted the references of a given contribution and selected promising titles or publications that were explicitly highlighted in the respective related work sections. If a publication appeared promising but the contribution was not clear from its title alone, the abstract was consulted as well. For the forward snowballing, we used Google Scholar and applied the same title and abstract procedure. This brought the number of contributions up to 540. After concluding the snowballing, we applied the IC and EC a second time, reducing the amount of contributions to 457."}, {"title": "4.5. Filtering by years", "content": "While the 457 results are all highly relevant, the number of contributions was too large to conduct a deeper investigation necessary to answer all the RQs that require a more in-depth consideration of the contributions, i.e., RQs 1.2 and 1.3 and RQ 2. Therefore, we restricted the result tally to studies published between 2019 and 2023. While this shifts our focus on the most recent developments in the field, it aligns with the peak in this period we discussed back in Section 1. By only considering studies from 2019 to 2023, we reduced the corpus to 191 primary studies. Of these, 2 studies purely provide data sets for future AI applications in the field of FM without showcasing any form of direct AI application themselves (IC 5). That leaves us with a final tally of 189 primary studies to conduct this mapping study."}, {"title": "5. Results", "content": "In the following section, we will discuss the results. For this, we will answer the individual research questions and aim to make a cross-analysis between individual research questions, thus extracting as much information as possible from the existing data set. We refer to Section 6 for a discussion of the observed results."}, {"title": "5.1. RQ 1: Demographics of the research area", "content": ""}, {"title": "5.1.1. RQ 1.1: What is the research publication timeline, and is there a trend?", "content": "The whole corpus of 457 primary studies we found was published in the years 1972-2023. Figure 3 displays the histogram of publications per year (see also Table A.1) and highlights that AI applications on FM appear to mirror the trend of general AI application mentioned in Section 1. That is, we can observe a steadily growing interest in the field, especially since around 2005.\nFocusing on the last five years only (recall Section 4.5), there still appears to be growing interest overall, while not each year surpasses the respective previous one. For 2019, we have 32, for 2020, we have 48, for 2021, we have 39, for 2022, we have 51, and for 2023, we have 21 contributions. Noteworthy are two observations: First, for the last five years alone, there has been no strict upward trend in the number of publications, We can observe a decline from 2020 to 2021. Second, there were few publications in 2023. We explain this because we searched in the last quarter of 2023 when not all possible studies were published. Given that we do not yet have complete data for 2023, it is impossible to forecast whether the overall trend will continue to grow or if the current peak from 2022 marks a global maximum."}, {"title": "5.1.2. RQ 1.2: Which publication venues are most frequent in the field?", "content": "Figure 4 (see also Table A.2) displays the publications over the years divided by publication venues. The majority of studies were published as conference papers while journal papers seem to be on the decline again since their peak in 2018.\nWithin our five-year observation range, 122 contributions were conference papers, followed by 39 journal articles 29 workshop or short papers, and only one (1) book chapter with no full books."}, {"title": "5.1.3. RQ 1.3: What are the provided main contributions by the primary studies?", "content": "Following the definitions of contribution types from Section 2.4, we see the division of the studies into these contribution types in Fig. 5a (see also Table A.3). We can see that the majority of contributions were methodologies (118), i.e., practical application of AI to FM, followed by studies with multiple types of contributions (29). Third place was tool contributions (14), followed by improvements for tools (10), followed by frameworks (6), training data (5), case studies (2), benchmarks (2), approaches (2), and finally one (1) idea contribution. Studies with multiple contribution types most commonly provide a methodology (20/29), training data (14/29), or a tool (11/29). See Fig. 5b for a complete breakdown."}, {"title": "5.1.4. Intersecting RQs 1.2 and 1.3", "content": "We can gain some additional insights by investigating the individual results in context to each other. Figure 6 (see also Table A.4) shows how the publication venues and the contribution types correlate to each other. We can see that for any contribution type, a heavy focus lies on conference papers. Interestingly, we can see that case studies are only conducted within the scope of journal papers and book chapters. Furthermore, workshops and short papers are primarily used to introduce new methodologies, which seem unexpected, as usually, due to limited space, authors aim to restrict themselves to outlining an approach or an idea."}, {"title": "5.2. RQ 2: Content type of contributions", "content": ""}, {"title": "5.2.1. RQ 2.1: Which AI techniques and tools were used?", "content": "In Fig. 7a (see also Table B.5), we can see that the majority of contributions use NN (70) followed by RL (32). 27 contributions use multiple techniques, while 16 use NLP methods and 14 use EA. A total of 8 studies presented custom algorithms. The rest of the contributions are divided between utilizing decision trees (7), clustering (3), Bayesian inference (3), KNN (2), random forests (2), data mining approaches (2), automaton learning (2), and SVM (1).\nStudies that employed multiple algorithms mostly did so in a contrasting manner, i.e., conducted training on numerous different models to see which one performed best for their respective application. A detailed view of"}, {"title": "5.2.2. RQ 2.2: What are the application areas of AI in FM?", "content": "the category of multiple algorithms is given in Fig. 7b (see also Table B.6). Here, we can see that if multiple techniques are used, they mainly rely on NN (13/31), random forest (12/31), and some variant of tree learners (12/31). Interestingly, SVM (6/31) and KNN (5/31) applications are more predominant in a setting with multiple algorithms compared to being the sole focus of a study. We explain this by KNN or SVM being simple enough algorithms for authors to train them in a small amount of time. Further, they do need less fine-tuning than, say, NNs, lending themselves as baseline approaches for comparisons.\nIntersecting RQs 2.1 and 1.3. Figure 8 (see also Table B.7) shows the intersection of RQs 2.1 and 1.3. The wide focus on methodologies was already assessed with Fig. 5a. In Fig. 8, we can see that the amount of NN contributions remained focused on methodologies (37/189) or as part of a broader application of techniques (13/189) or a tool (8/189).\nIntersecting RQs 2.1 and 1.1. Figure 9 (see also Table B.8) show the intersection of RQs 2.1 and 1.1. Here, we can see that the amount of NN contributions remained steady over the years, similar to the amount of RL contributions. Noteworthy is that there is no notable decline in any technique over the observed period. The alternating nature of the overall amount"}, {"title": "5.2.3. RQ 2.3: What"}]}