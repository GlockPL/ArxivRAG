{"title": "MARCA: Mamba Accelerator with ReConfigurable Architecture", "authors": ["Jinhao Li", "Shan Huang", "Jiaming Xu", "Jun Liu", "Li Ding", "Ningyi Xu", "Guohao Dai"], "abstract": "State space model (SSM) especially Mamba has demonstrated remarkable capabilities in various domains. Compared to Transformers, Mamba reduces the quadratic computational complexity and achieves a higher algorithm accuracy (e.g., the accuracy of Mamba-2.8b is higher than OPT-6.7b). However, challenges still exist in accelerating Mamba computations. (1) Incompatibility between element-wise operations and Tensor Core. Linear operations (matrix multiplications) and element-wise operations are the two dominating operations in Mamba. The time proportion of element-wise operations escalates significantly (e.g., >60% with 2048 input length). These operations do not need reduction, which is not compatible with the existing Tensor Core-based architectures (e.g., 1/16 normalized speed). (2) Large area overhead for nonlinear function unit. The optimized nonlinear function unit like exponential unit still occupies >30% of the processing element (PE) area. (3) Large memory access but limited data sharing for element-wise operations. Linear and element-wise operations in Mamba exhibit large compute intensity variance (e.g., ~3 orders of magnitude) and large read/write ratio variance (e.g., >3 orders). Due to the limited data sharing in element-wise operations, it is useless to apply the existed methods like tiling to element-wise operations.\nIn response to these challenges, we propose a Mamba accelerator with reconfigurable architecture, MARCA. Then, we propose three novel approaches in this paper. (1) Reduction alternative PE array architecture for both linear and element-wise operations. For linear operations, the reduction tree connected to PE arrays is enabled and executes the reduction operation. For element-wise operations, the reduction tree is disabled and the output bypasses. (2) Reusable nonlinear function unit based on the reconfigurable PE. We decompose the exponential function into element-wise operations and a shift operation by a fast biased exponential algorithm, and the activation function (SiLU) into a range detection and element-wise operations by a piecewise approximation algorithm. Thus, the reconfigurable PEs are reused to execute nonlinear functions with negligible accuracy loss. (3) Intra-operation and inter-operation buffer management strategy. We propose intra-operation buffer management strategy to maximize input data sharing for linear operations within operations, and inter-operation strategy for element-wise operations between operations. We conduct extensive experiments on Mamba model families with different sizes. MARCA achieves up to 463.22\u00d7/11.66\u00d7 speedup and up to 9761.42\u00d7/242.52\u00d7 energy efficiency compared to Intel Xeon 8358P CPU and NVIDIA Tesla A100 GPU implementations, respectively.", "sections": [{"title": "1 INTRODUCTION", "content": "State space model (SSM) especially Mamba [10] has demonstrated remarkable capabilities in various domains including language, images [7, 35, 48], audio [23], and genomics [27, 41]. Before Mamba, Transformer-based large language models [26, 46] have achieved great success in sequence modeling due to the self-attention mechanism [40]. However, it suffers from handling the quadratic growth of storage and computational complexity as the sequence length increases and struggles to handle sequence with long range dependency. Compared with the pioneers of SSMs [11\u201313] and Transformer, Mamba achieves higher accuracy in algorithm (e.g., the accuracy of Mamba-2.8b is higher than OPT-6.7b [46]) and more efficient computation in hardware. Therefore, Mamba, as a foundation model [4], has been applied in various domains including vision [34, 48], graphics [2, 16, 41], medical [44], and point cloud [18, 20, 47].\nHowever, there is limited research on optimizing for Mamba processing. Therefore, we profile the processing carefully and identify three main challenges in Mamba computations: (1) Incompatibility between element-wise operations and Tensor Core. Linear operations (e.g., matrix multiplication and convolution) and element-wise operations are two dominating operations in Mamba. Tensor Core [5, 24] is a typical domain specific architecture with reduction tree focused on accelerating these linear operations with high the compute intensity (e.g., >1000 FLOPs/Byte). However, as shown in Figure 1, the time proportion of element-wise operations escalates significantly as sequence length increases (e.g., >60% with 2048 input length). Because the element-wise operations do not need reduction, applying them on Tensor Core-based architecture should introduce large amount of invalid computations, leading to an extreme inefficiency (e.g., 1/16 normalized speed). (2) Large area overhead for nonlinear function unit. Exponential function and Sigmoid Linear Unit (SiLU) function are two nonlinear functions in Mamba. Previous methods [29, 33, 36] often design specific unit to compute them, leading to much more area overheads. As shown in Figure 2 middle bottom, the optimized nonlinear function unit such exponential function still occupy 30% of the PE area [19, 33]. (3) Large memory access but limited input data sharing for element-wise operations. Linear and element-wise operations in Mamba exhibit large compute intensity variance (e.g., ~3 orders of magnitude) and large read/write ratio variance (e.g., >3 orders of magnitude). Due to the limited data sharing in element-wise operations, it is useless to apply the existed methods like tiling [17] to element-wise operations.\nIn response to these challenges, we propose MARCA, a Mamba accelerator with reconfigurable architecture, to support fast and energy-efficient Mamba computations. Our contributions are as follows:\n(1) Reduction alternative PE array architecture for both linear and element-wise operations. For linear operations, the reduction tree connected to PE arrays is enabled and executes the reduction operation. For element-wise operations, the reduction tree is disabled and bypasses the results.\n(2) Reusable nonlinear function unit based on reconfigurable PE arrays. We decompose the exponential function into element-wise operations and a shift operation by a fast biased exponential algorithm. We also decompose the activation function (SiLU) into a range detection and element-wise operations by a piecewise approximation algorithm. Thus, the reconfigurable PEs are fully reused to execute nonlinear functions with negligible accuracy loss.\n(3) Intra-operation and inter operation buffer management strategy. We propose intra-operation and inter-operation buffer management strategy for linear and element-wise operations. For linear operations, the buffer pool is managed as an input buffer to maximize the input data sharing within each operation. For element-wise operations, the buffer pool is managed as an output buffer to maximize the output data sharing between operations.\nWe implement MARCA in Verilog and design a cycle-accurate simulator to evaluate MARCA. We conduct extensive experiments on Mamba with different model sizes (i.e., Mamba-130M to Mamba-2.8B). As a result, MARCA achieves up to 463.22\u00d7/11.66\u00d7 speedup and up to 9761.42\u00d7/242.52\u00d7 energy efficiency compared to Intel Xeon 8358P CPU and NVIDIA Tesla A100 GPU implementations, respectively."}, {"title": "2 BACKGROUND", "content": "2.1 State Space Model\nThe continuous state space model in equation 1 defines a linear mapping from an input signal $x(t) \\in \\mathbb{R}^M$ (a function of time t) to output signal $y(t) \\in \\mathbb{R}^M$ through a hidden state $h(t) \\in \\mathbb{R}^N$:\n$h'(t) = A(t)h(t) + B(t)x(t)$\n$y(t) = C(t)h(t)$\nwhere state matrix $A(t) \\in \\mathbb{R}^{N\\times N}$, input matrix $B(t) \\in \\mathbb{R}^{N\\times M}$, output matrix $C(t) \\in \\mathbb{R}^{M\\times N}$, and the change of hidden state $h'(t) \\in \\mathbb{R}^N$. Classical SSM has dynamic parameters (e.g., A, B, C) that change over time. However, when they are constant the dynamics are invariant through time, which is known as a linear time-invariant (LTI) system [43], and is equivalent to a (continuous) convolution.\nData like words and tokens in the real world is discrete instead of continuous, so equation 1 must be discretized to be applied to an sampled input sequence x = (x0, x1, x2, ...) instead of continuous function x(t). An additional step size parameter \u0394 is required that represents the resolution of the input. Conceptually, the inputs x(n) can be viewed as uniformly-spaced samples from an implicit underlying continuous signal x(t), where x(k) = x(kA). The discretization process from continuous time signal processing\n2.2 Mamba\nMamba [10] introduces selective mechanism into SSM and proposes an implementation of selective state space model layer. Mamba block consists of a layer normalization, several linear projections, a convolution, a SSM block, and a residual connection. In each layer, the input sequence is first processed by a linear projection and then processed by the convolution. Then it is processed by an activation and then processed by the SSM. After SSM, the main branch is multiplied by the collateral branch including a linear projection and an activation (e.g., SiLU [8]) to generate the combined result. After combination, the result is processed by a linear projection. Last, the result is added back to the input through the residual connection [14]. Instead of interleaving Mamba block and Feed-forward block, Mamba simply repeats the Mamba block homogenously.\nDuring SSM processing, the input x undergoes a series of transformations. Firstly, it is subjected to three linear projections, resulting in A, B and C. Subsequently, A is involved in the Einstein summation operations [1] with A and B separately, generating \u0394A and \u0394B. These intermediate results \u0394A and \u0394B are then multiplied with the hidden state and input x by element-wise Einstein summation, respectively. After L (sequence length) times iterations, the hidden state is updated for L times. The outcomes of these operations are then combined through addition, resulting in the updated hidden state. Finally, the updated hidden state undergoes matrix multiplication with C, followed by a linear transformation, to generate the output. The whole computational flow in Mamba block is illustrated in Figure 3."}, {"title": "3 ARCHITECTURE OVERVIEW", "content": "We propose a Mamba accelerator with reconfigurable architecture, MARCA, with reconfigurable computing units (CUs) and processing elements (PEs). MARCA is a reconfigurable architecture whose instructions are all 64-bit, and contains 16 32-bit general-purpose Registers (Regs) and 16 32-bit Constant Registers (CRegs). MARCA consists of four main parts: instruction processing, normalization unit, on-chip buffer, and computing engine, as depicted in Figure. 4 left.\nInstruction Processing. The instruction processing consists of two parts: instruction fetch and instruction decode. The instruction fetch unit fetches instructions from global memory and stores them in the instruction buffer. Then, the instruction decode unit reads instructions sequentially from the buffer and decodes them. As shown in Figure 5, the instruction set architecture (ISA) includes linear (LIN), convolution (CONV), normalization (NORM), element-wise multiplication (EWM), element-wise addition (EWA), exponential function (EXP), and SiLU (SILU). And MARCA also provides LOAD and STORE instructions to support moving data between global memory and on-chip buffer. After decoding, the instructions are passed through the configure unit to pass configuration information to the following modules.\nNormalization Unit. The layer normalization is a important component that stabilizes range of intermediate values by normalizing layer activation. The normalization unit is responsible for computing the mean and variance of the data. The data is first summarized and accumulated to get the mean and then the variance is calculated. Then, data undergoes a linear unit to obtain the normalized result.\nCompute Engine. The compute engine is responsible for linear, convolutional, and element-wise computations. It comprises a control unit and a reconfigurable compute units (RCUs). The control unit receives configuration information from configure unit and fetches data from on-chip buffer to RCU. When the computation is completed, it notifies the instruction processing pipeline to continue decoding and executing the next instruction."}, {"title": "4 REDUCTION ALTERNATIVE PE ARRAY ARCHITECTURE", "content": "4.1 Challenge\nLinear operation and element-wise operation are two dominating operations in Mamba. Linear operations like matrix multiplications are usually accelerated by Tensor Core, which is a domain specific architecture with the reduction tree. Due to the reduction of partial inner product, the linear operations exhibit extremely high compute intensity (e.g., >1000 FLOPs/Byte). However, as shown in Figure 1, the time proportion of element-wise operations escalates significantly as sequence length increases (e.g., >60% with 2048 input length). Because the element-wise operations do not need reduction, applying them on Tensor Core-based architecture should introduce large amount of invalid computations, leading to an extreme inefficiency (e.g., 1/16 normalized speed). We call it the incompatibility between element-wise operations and Tensor Core.\n4.2 Motivation and Insights\nOur motivation stems from the inefficiencies observed in element-wise computations on tensor cores. Because the attention operations contain non-linear softmax, previous Transformer accelerators [21, 42] employed independent hardware units to support both attention and linear operations. In contrast, Mamba architecture only consists of linear operations, element-wise operations, and a few activation functions. The only difference of linear of element-wise operations is whether execute reduction or not. Recognizing this simplicity, our key insight is that by disabling the reduction tree, the Tensor Core-based PE arrays can execute element-wise operations.\n4.3 Approach\nReduction Alternative PE Arrays. The reduction alternative processing element (PE) arrays support configurablility. The reduction tree consists of 16 slices (taking 16 for instance) and operates in two modes: reduction mode for linear operations and non-reduction mode for element-wise operations. In reduction mode, the reduction tree is enabled. For a 16-to-1 reduction tree slice, the outputs from 16\u00d71 PE arrays are fed into one reduction tree slice, which employs multi-level additions to compute the sum. In non-reduction mode, the reduction tree is enabled and the outputs from 16\u00d716 PE arrays skip the reduction directly. In addition, the last-level addition in each slice supports three input to accumulate the partial results for linear operations.\nWe define that a reconfigurable computing unit (RCU) consists of 16\u00d716 PE arrays and a reduction tree, as shown in Figure 4 left. Each reconfigurable PE (RPE) is configured to support three main computations with three data paths: a shift path for exponential function, a piecewise path for SiLU function, and a normal path for addition or multiplication. The normal path contains a floating-point multiplier, a floating-point adder, and a multiplexer unit to handle element-wise multiplication and addition computations.\nReconfigurable Computing Unit. We provide a detailed explanation of how the RCU operates for the four specific computations in Mamba, as shown in Figure 4 right.\nMM-RCU. The RCU is configured as a matrix multiplication mode (MM-RCU) to support linear operations. The reduction tree in RCU and the floating-point multiplier units in RPE are enabled. Therefore, for a matrix multiplication operation of two matrices of size 16\u00d716, the results calculated by the 16\u00d716 array of multipliers are then passed through the reduction tree to produce 16 final results. Then, this process is repeated totally 16 times to obtain the complete result of the 16\u00d716 matrix multiplication. To support the accumulation of partial sums, an additional adder is added at the final level of the reduction tree.\nEW-RCU. When RCU is configured as element-wise mode (EW-RCU), the reduction tree is disabled, while the P2D buffer is enabled, and the floating-point multiplier or adder units in RPE are activated. For an element-wise multiplication operation of two 16x16 matrices, the results calculated by the 16\u00d716 array of RPEs maintains the same dimensions of 16\u00d716 and are output to the buffer in parallel.\nEXP-RCU. When RCU is configured as exponential mode (EXP-RCU), the reduction tree is disabled. The floating-point multiplier, adder units, and exponential shift unit in RPE are enabled. Therefore, for a 16\u00d716 matrix performing exponential function operation, the RCU first executes element-wise multiplication by using the multipliers, then executes element-wise addition by using the adders. Afterward, the exponential shift unit performs a logic operation, a shift operation and a biased operation to obtain the final output, as shown in Figure 2 right bottom."}, {"title": "5 REUSABLE NONLINEAR FUNCTION UNIT", "content": "5.1 Challenge\nExponential function and Sigmoid Linear Unit (SiLU) function are two nonlinear functions in Mamba. Previous methods [29, 33, 36] often design specific unit based on lookup-table or Taylor series approximation to optimize these nonlinear computations. However, the optimized nonlinear function unit such exponential function still occupy 30% of the PE area [19, 33], leading to much more area overheads.\n5.2 Motivation and Insights\nA common approach is to utilize approximation functions to approximate exponential operations, thereby degrading exponential computations to quadratic or even linear operations. Using linear approximation methods results in large precision loss (>4% for Mamba-2.8b) while employing higher-order polynomial approximation leads to increased computational overhead. We profile the range of these nonlinear functions. The input for an exponential function is mostly between -7 and 0, especially for values slightly less than 0, while the input range of the SiLU function is from -5 to 4. On one hand, we can only approximate nonlinear functions only in these range to concentrate on preventing accuracy loss. On the other hand, exponential function and SiLU are similar with element-wise operations except for scaling for each value. Therefore, to avoid increasing area overhead and prevent accuracy loss, our key insight is that only by approximating these nonlinear functions in a small range, we can decompose them into several element-wise operations to reuse the reduction alternative PE array architecture in Section 4.\n5.3 Approach\nFast Biased Exponential Algorithm. Given the peculiar distribution observed in the inputs of the exponential function, namely the outer product of A and A, we leverage a set of data points x = -7,$(\\frac{n}{200})$, n = 1, 2, ..., 200 where the density increases as they approach zero to evaluate the deviation of the approximate calculation from the origin exponential value. Consequently, we modified the fast exp algorithm [38] to accommodate specific data ranges and appended a bias at the end to enhance precision and consists of three main steps:\n(1) The input x is linearly transformed into x'.\n(2) x' is multiplied by $2^{23}$ then cast to an unsigned integer.\n(3) View the x as float-point number and add the bias c.\nThe fast exp algorithm aims to put $\\frac{x}{2}$ into exponential bit of $e^x$ so that $e^x = 2^{(\\frac{x}{2})}$. Float data does not have a direct intuitive"}, {"title": "6 INTRA-/INTER-OPERATION BUFFER MANAGEMENT STRATEGY", "content": "6.1 Challenge\nLinear and element-wise operations are two dominant operations in Mamba processing. The feature of memory access for these two kinds shows three typical paradigms, that is, reading 2 \u00d7 2N data and writing 2 \u00d7 2, 2N, and $2N^2$ corresponding to linear projection (Linear), element-wise addition or multiplication (Element-wise 1), and element-wise outer product (Element-wise 2), as shown in Figure 2 right bottom. As shown in Figure 7, we depict a detailed breakdown of the memory read/write ratio with different input sequence length for various operations during Mamba computation process. It shows that the read/write ratio of the three operations differs by more than three orders of magnitude. For linear operations with more input and less output, existing optimization methods primarily focus on tiling [17] and load the tiled input to on-chip buffer to maximize data sharing. However, due to the computational characteristic of element-wise operations with more output and less input, it is redundant to apply input sharing methods like tiling.\n6.2 Motivation and Insights\nThe computational characteristics of linear operations require reduction and input sharing, leading to a higher read/write ratio, which is suitable for existing input data sharing methods during each operation processing. By reviewing Figure 3, we find that the element-wise operations are closely spaced within the SSM computation process. And during the SSM process, the outputs of element-wise operations such as \u0394A, \u0394B, and h are accessed repeatedly. Storing these output of element-wise operations on the on-chip buffer can significantly reduce the overhead of memory access for the next operation. Therefore, our key insight is to adjust\n6.3 Approach\nOur operation-wise buffer management (BM) strategies contain intra-operation and inter-operation methods as shown in Figure 8. Intra-operation buffer management (Intra-BM) is used to discover and manage data sharing within individual operations, while inter-operation buffer management (Inter-BM) is used to discover and manage data sharing between operations.\nIntra-operation Management. For linear operations, the whole on-chip buffers are configured as read buffers. The memory access handler loads linear inputs from global memory to fill the buffer. The computing unit then sequentially reads the required data from the buffer, performs calculations, and writes the computed results back to global memory. For element-wise 2 operation, even though the read/write ratio is relatively low, it can be effectively regarded as a matrix multiplication with two reduction dimensions of size 1, hence requiring data sharing among inputs. Therefore, we reserve a small fraction region of the on-chip buffer to store them.\nInter-operation Management. For element-wise 1, there is no data reuse in the computation of input data. The basic approach involves reading from global memory and directly writing back after computation. For individual computations, data reuse optimization like tiling is not feasible. For element-wise 1 and 2 operations, to maximize buffer utilization, we primarily optimize data sharing among adjacent element-wise operations. As shown in equation 2, the update of hidden state h is obtained by adding the product of \u0394A and h with the product of \u0394B and x. Therefore, during the continuous updating process of state h, h needs to be read and written L times repeatedly. Additionally, the corresponding \u0394A, \u0394B, and x need to be read L times repeatedly. Hence, for element-wise operations concentrated in the SSM process, we cache the above immediate result in the buffer.\nThrough an operation-wise buffer management strategy, our method maximizes the reduction in memory access and minimizes idle computational resources, thereby accelerating the Mamba computation process."}, {"title": "7 EXPERIMENTAL RESULTS", "content": "7.1 Experimental Setup\nMethodology. The performance and energy of MARCA are measured by using the following tools.\nArchitecture Simulator. We design and implement a cycle-accurate simulator to measure execution time in number of cycles. This simulator models the microarchitectural behaviors of each module, which is integrated with Ramulator 2.0 [22] to simulate the behaviors of memory accesses to High Bandwidth Memory (HBM).\nCAD Tools. We implement and synthesize our design in Verilog to measure area, power, and critical path delay (in cycles) for each module. We use the Synopsys Design Compiler with the TSMC 28 nm standard VT library for the synthesis, and estimate the power using Synopsys PrimeTime PX. The slowest module has a critical path delay of 0.9 ns including the setup and hold time, putting the MARCA comfortably at 1 GHz clock frequency.\nMemory Measurements. The area, power, and access latency of the on-chip scratchpad memory are estimated using Cacti 7.0 [28]. Since Cacti only supports down to 32 nm technologies, we apply four different scaling factors to convert them to 28 nm technology as shown in [39]. The energy of HBM 1.0 is estimated with 7 pJ/bit as in [31].\nBenchmark LLM Datasets. We conduct comprehensive experiments on the Mamba, which are owing to critical and efficient influence in recent model advancements. We depict Mamba models with different size and hyperparameters as shown in Table 1. We focus on two primary metrics: perplexity and zero-shot performance. The perplexity is evaluated by the WikiText [25] and Lambada [32] benchmarks. The zero-shot performance is assessed across four zero-shot benchmarks, namely Piqa [3], HellaSwag [45], WinoGrande [37], and Arc-easy [6].\nBaseline Platform. To compare the performance and energy consumption of MARCA with state-of-the-art works, we evaluate Mamba model on a Linux workstation equipped with one Intel Xeon 8358P CPU [15] and a 252 GB DDR4 memory and one NVIDIA Tesla A100 GPU [30], denoted as Mamba-CPU and Mamba-GPU, respectively. Table 2 lists the system configurations for above implementations.\n7.2 Accuracy Evaluations\nTable 3 illustrates the perplexity and zero-shot performance of the approximation algorithm on Mamba families, compared to the metrics computed using the original approach, along with the accuracy of the original fast exp algorithm applied to exponential and SiLU computations. The experimental results indicate that benefiting from the bias introduced based on the data distribution, our improved algorithm outperforms the fast exp algorithm across all sizes of Mamba. The average accuracy improvement ranges from 0.19% to 0.44%. The difference in accuracy compared to the original algorithm does not exceed 0.29%, indicating minimal loss in\n7.3 Hardware Evaluations\nWe compare our work with Mamba-CPU and Mamba-GPU in terms of speedup and energy consumption, Finally, the area and power of our design is presented.\nSpeedup. Figure 9 top depicts that MARCA achieves up to 463.32\u00d7/11.66\u00d7 speedup and average 194.26\u00d7/4.93\u00d7 speedup compared with Mamba-CPU and Mamba-GPU, respectively. The performance improvement comes from the reconfigurable architecture, and the intra-operation and inter-operation buffer management\n7.4 Ablation Study\nSpeedup of RCU over Tensor Core. Figure 10 top left shows the speedup from 1.41x to 11.95\u00d7 over Tensor Core-based architecture with different sequence length in Mamba.\nNormalized area of RPE. In Figure 10 top right, we compare the normalized PE area overhead by supporting different nonlinear"}, {"title": "8 CONCLUSIONS", "content": "Our MARCA is the first proposed accelerator with reconfigurable architecture specifically tailored for Mamba computations. We propose a reduction alternative PE array architecture to support both linear and element-wise operations. Then, based on the reconfigurable PE, we decompose the nonlinear functions and reuse PE arrays reduce the area overhead. We also propose intra-operation and inter-operation buffer management strategy to maximize data reuse for two dominant operations. We conduct extensive experiments on Mamba model families with different model sizes. MARCA achieves up to 463.22\u00d7/11.66\u00d7 speedup and up to 9761.42\u00d7/242.52\u00d7 energy efficiency compared to Intel Xeon 8358P CPU and NVIDIA Tesla A100 GPU implementations, respectively."}]}