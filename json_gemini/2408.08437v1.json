{"title": "PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications", "authors": ["Kshitij Bhardwaj"], "abstract": "While Vision Transformers (ViTs) are extremely\neffective at computer vision tasks and are replac-\ning convolutional neural networks as the new\nstate-of-the-art, they are complex and memory-\nintensive models. In order to effectively run these\nmodels on resource-constrained mobile/edge sys-\ntems, there is a need to not only compress these\nmodels but also to optimize them and convert\nthem into deployment-friendly formats. To this\nend, this paper presents a combined pruning and\nquantization tool, called PQV-Mobile, to optimize\nvision transformers for mobile applications. The\ntool is able to support different types of struc-\ntured pruning based on magnitude importance,\nTaylor importance, and Hessian importance. It\nalso supports quantization from FP32 to FP16 and\nint8, targeting different mobile hardware back-\nends. We demonstrate the capabilities of our tool\nand show important latency-memory-accuracy\ntrade-offs for different amounts of pruning and\nint8 quantization with Facebook Data Efficient\nImage Transformer (DeiT) models. Our results\nshow that even pruning a DeiT model by 9.375%\nand quantizing it to int8 from FP32 followed by\noptimizing for mobile applications, we find a la-\ntency reduction by 7.18\u00d7 with a small accuracy\nloss of 2.24%. The tool is open source.", "sections": [{"title": "1. Introduction", "content": "Vision Transformers (ViTs) (Kolesnikov et al.) have recently\nemerged as a competitive alternative to Convolutional Neu-\nral Networks (CNNs) that are currently state-of-the-art in\ndifferent image recognition computer vision tasks. ViTs\nhave shown to outperform the CNNs by almost 4\u00d7 in terms\nof computational efficiency and accuracy (VISO). Addition-\nally, ViTs have been shown to be more robust than CNNS\nand can be easily trained on smaller datasets.\nWhile ViTs are extremely effective at computer vision tasks,\nthey are complex and memory-intensive models. For exam-\nple, Facebook's Data Efficient Image Transformers (DeiT)\ntake 331MB memory and are therefore not suitable for\nresource-constrained edge systems such as for mobile ap-\nplications. Previous research has focused on pruning (Fang\net al., 2023) and quantizing (Li & Gu, 2023) ViTs, but\nmostly separately and they do not target mobile applications\nwhere deployment of such models is challenging and re-\nquires converting these models to mobile hardware friendly\nlightweight and optimized formats.\nThis paper presents a combined pruning and quantization\ntool, called PQV-Mobile, to optimize vision transformers\nfor mobile applications. The tool is able to support dif-\nferent types of structured pruning based on magnitude im-\nportance, Taylor importance, and Hessian importance. It\nalso supports quantization from FP32 to FP16 and int8,\ntailored towards several hardware backends, such as x86,\nFBGEMM (Facebook General Matrix Multiplication (Face-\nbook FBGEMM)), QNNPACK (Quantized Neural Network\nPackage (QNNPACK)), and ONEDNN (Intel's ONEDNN).\nThe pruned and quantized models are optimized for mobile\napplications and converted to mobile-friendly lightweight\nformats. We demonstrate the capabilities of our tool and\nshow important latency-memory-accuracy trade-offs for dif-\nferent amounts of pruning, int8 quantization, and hardware\nbackends with two types of Facebook DeiT models.\nOur results show that even pruning a DeiT model by 9.375%\nand quantizing it to int8 from FP32, we find a latency reduc-\ntion by 7.18x with a small accuracy loss of 2.24%. All of\nour compared models are optimized for mobile applications\nand converted into deployment friendly lightweight formats."}, {"title": "2. PQV-Mobile Tool", "content": "Figure 1 shows our PQV-Mobile Tool flow. It supports\ndifferent kinds of post-training pruning strategies such as\nL1, Taylor, etc. We found that the pruned model shows a\nmajor accuracy drop and therefore needs to be finetuned."}, {"title": "2.1. Pruning method", "content": "PQV-Mobile supports several pruning strategies, corre-\nsponding to structured pruning. In structured pruning,\na block is removed, which can be a neuron in a fully-\nconnected layer, a channel of filter in a convolutional layer,\nor a self-attention head in a Transformer. An alternative\napproach is unstructured pruning (also called magnitude\npruning) where some of the parameters or weights with\nsmaller values are converted to zeroes. PQV-Mobile targets\nstructured pruning methods as they do not rely on specific\nAI accelerators or software to reduce memory consumption\nand computational costs, thereby finding a wider domain of\napplications in practice (Fang et al., 2023).\nIn structural pruning, a 'Group' is defined as the minimal\nunit that can be removed. Many of these groups consist\nof multiple layers which can be interdependent and need\nto be pruned together so as to maintain the integrity of the\nresulting pruned networks. We follow the approach of (Fang\net al., 2023) that uses a dependency graph to model these\ndependencies and find the right groupings for parameter\npruning. Similar to (Fang et al., 2023), PQV-Mobile accepts\na group (i.e., an Attention block of a ViT with Linear layers)\nas inputs, and returns a 1-D tensor with the same length as\nthe number of channels. All groups must be pruned simulta-\nneously and thus their importance should be accumulated\nacross channel groups. PQV-Mobile supports the following\ngroupings:\nMagnitude importance based grouping. In this case, L1-\nor L2-norm regularization term is applied to the loss func-\ntion which penalizes non-zero parameters. If the value\nof a connection is less than a threshold, the connection is\ndropped (Anwar et al., 2017).\nTaylor importance based grouping. The importance is\ncalculated as the squared change in loss induced by remov-\ning a specific filter from the network. This importance is\napproximated with a Taylor expansion which allows for\nfaster computation from parameter gradients, even for larger\nnetworks (Molchanov et al., 2019).\nHessian importance based grouping. In this method the\nimportance is computed using a fast second-order metric\nto find insensitive parameters in a model. In particular, the\naverage Hessian trace is used to weight the magnitude of the\nparameters; parameters with large second-order sensitivity\nremain unpruned, and those with relatively small sensitivity\nare pruned (Yu et al., 2022)."}, {"title": "2.2. Quantization method", "content": "PQV-Mobile currently supports post-training quantization\nof both weights and activations from FP32 to FP16 and int8.\nWe plan to extend this to int4 as future work. The follow-\ning steps are performed for quantization using quantization\nlibraries of Pytorch:\n\u2022 Quantize models for a specific backend: We first cre-\nate a quantization engine based on a backend. The\nsupported backends are: x86, FBGEMM (Facebook\nGeneral Matrix Multiplication (Facebook FBGEMM)),\nQNNPACK (Quantized Neural Network Package (QN-\nNPACK)), and ONEDNN (Intel's ONEDNN). The gen-\nerated engine is then used to quantize the model using\neither static or dynamic quantization (Pytorch Quanti-\nzation).\n\u2022 Convert Pytorch models to Torchscript format: Python\nmodels are inefficient to run during deployment. There-\nfore, we export the Pytorch models to production en-\nvironments through Torchscript (Pytorch Torchscript),"}, {"title": "3. Experimental Results", "content": "In this section, we demonstrate the effectiveness of PQV-\nMobile to prune and quantize Facebook's Data Efficient Im-\nage Transformers (DeiT). We evaluate the models' latency-\nmemory-accuracy trade-offs. The structured pruning im-\nportance used in this study is based on Taylor's expansion\n(as shown later, it performs the best in terms of accuracy).\nThe pruned model is then finetuned for 60 epochs (using\ndistributed training on 4 GPUs) at a learning rate of 0.00015\nwith a batch size of 64. The finetuned pruned models are\nthen quantized for the x86 backend engine and then con-\nverted to optimized and Lite format. We use the ImageNet\ndataset (IMAGENET) for image classification tasks and run\nthe models on Intel Xeon ES2695 at 2.3 GHz. Pytorch-2.0.0\nis used in all our experiments. Please note that our experi-\nments are only meant to demonstrate the capabilities of our\ntool and not to achieve the state-of-the-art accuracy. All\ncomparisons are performed on scripted, mobile optimized,\nand Pytorch Lite format models.\nalso be used to perform a detailed profiling of the latency of\nthe mobile optimized and Lite model as shown in Figure 3,\nwhere we can identify the bottleneck based on the time\nspent on the various operations (as depicted by the Name of\nthe process). In terms of accuracy, we found it to be similar\nacross the different backends.\nWhile pruning the model to 50% leads to significant im-\nprovements in latency and memory, Figure 4 shows that the\naccuracy degradation is considerable. Pruning the original\nmodel by 9.375% shows 1.25% lower accuracy. Further\nquantizing this model leads to an additional 0.99% loss in\naccuracy. While 25% pruned int8 model achieved a speedup\nof 27.9% over 9.375% pruned int8 model, its accuracy loss\nis 3.41%. These results demonstrate the importance of per-\nforming latency-memory-accuracy trade-offs which can be\nseamlessly performed using our PQV-Mobile tool.\nFigure 5 shows how the different types of structured pruning\ngroupings affect accuracy and motivates why we chose Tay-\nlor pruning for all of our experiments. We prune the dense\nmodel by 9.375%, finetune it, and also quantize the fine-\ntuned pruned model to int8 for this experiment. Although\nthere is a very small change in accuracy between Taylor,\nL1-norm, and Hessian-based pruning, Taylor outperforms\nthe other methods.\nWe further compare the latency and accuracy of\npruning and quantizing deit_base_patch16 model with\ndeit3_medium_patch16 model (Figure 6). The original\ndense FP32 accuracies for the two models are 80.79% and\n82.19%, respectively. The latter is a smaller model with\n38.85M parameters compared to 86.56M parameters in the\nformer. As evident, the models show similar accuracy when\npruned to the same levels (9.375% or 25%) at int8 quantiza-\ntion. However, deit3_medium_patch16 model shows latency\nimprovements by 18.65% and 13.55%, respectively over\ndeit_base_patch16 model.\nFinally, as shown in Figure 7, we also evaluate the latency\nfor different int8 quantization hardware backends. We use\nthe 9.375% pruned deit3_medium_patch16 model for this\nexperiment. We find that x86 and FBGEMM backends to be\nthe best with FBGEMM slightly outperforming x86. These\nresults are expected as we are running on an x86 machine\nwith Advanced Vector Extensions (AVX) enabled, which\nare used for fast path executions for both x86 and FBGEM\nbackends."}, {"title": "4. Conclusion and Future Work", "content": "This paper presents a combined pruning and quantization\ntool, called PQV-Mobile, to optimize vision transformers\nfor mobile applications. The tool is able to support different\ntypes of structured pruning based on magnitude importance,\nTaylor importance, and Hessian importance. It also supports"}]}