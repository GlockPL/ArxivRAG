{"title": "PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications", "authors": ["Kshitij Bhardwaj"], "abstract": "While Vision Transformers (ViTs) are extremely effective at computer vision tasks and are replacing convolutional neural networks as the new state-of-the-art, they are complex and memory-intensive models. In order to effectively run these models on resource-constrained mobile/edge systems, there is a need to not only compress these models but also to optimize them and convert them into deployment-friendly formats. To this end, this paper presents a combined pruning and quantization tool, called PQV-Mobile, to optimize vision transformers for mobile applications. The tool is able to support different types of structured pruning based on magnitude importance, Taylor importance, and Hessian importance. It also supports quantization from FP32 to FP16 and int8, targeting different mobile hardware back-ends. We demonstrate the capabilities of our tool and show important latency-memory-accuracy trade-offs for different amounts of pruning and int8 quantization with Facebook Data Efficient Image Transformer (DeiT) models. Our results show that even pruning a DeiT model by 9.375% and quantizing it to int8 from FP32 followed by optimizing for mobile applications, we find a latency reduction by 7.18\u00d7 with a small accuracy loss of 2.24%. The tool is open source.", "sections": [{"title": "1. Introduction", "content": "Vision Transformers (ViTs) (Kolesnikov et al.) have recently emerged as a competitive alternative to Convolutional Neural Networks (CNNs) that are currently state-of-the-art in different image recognition computer vision tasks. ViTs have shown to outperform the CNNs by almost 4\u00d7 in terms of computational efficiency and accuracy (VISO). Additionally, ViTs have been shown to be more robust than CNNS and can be easily trained on smaller datasets.\nWhile ViTs are extremely effective at computer vision tasks, they are complex and memory-intensive models. For example, Facebook's Data Efficient Image Transformers (DeiT) take 331MB memory and are therefore not suitable for resource-constrained edge systems such as for mobile applications. Previous research has focused on pruning (Fang et al., 2023) and quantizing (Li & Gu, 2023) ViTs, but mostly separately and they do not target mobile applications where deployment of such models is challenging and requires converting these models to mobile hardware friendly lightweight and optimized formats.\nThis paper presents a combined pruning and quantization tool, called PQV-Mobile, to optimize vision transformers for mobile applications. The tool is able to support different types of structured pruning based on magnitude importance, Taylor importance, and Hessian importance. It also supports quantization from FP32 to FP16 and int8, tailored towards several hardware backends, such as x86, FBGEMM (Facebook General Matrix Multiplication (Facebook FBGEMM)), QNNPACK (Quantized Neural Network Package (QNNPACK)), and ONEDNN (Intel's ONEDNN). The pruned and quantized models are optimized for mobile applications and converted to mobile-friendly lightweight formats. We demonstrate the capabilities of our tool and show important latency-memory-accuracy trade-offs for different amounts of pruning, int8 quantization, and hardware backends with two types of Facebook DeiT models.\nOur results show that even pruning a DeiT model by 9.375% and quantizing it to int8 from FP32, we find a latency reduction by 7.18x with a small accuracy loss of 2.24%. All of our compared models are optimized for mobile applications and converted into deployment friendly lightweight formats."}, {"title": "2. PQV-Mobile Tool", "content": "Figure 1 shows our PQV-Mobile Tool flow. It supports different kinds of post-training pruning strategies such as L1, Taylor, etc. We found that the pruned model shows a major accuracy drop and therefore needs to be finetuned.\nThe pruned and finetuned model is then input to a quantization engine, which can be tailored towards various hardware backends, and optimized for mobile applications. Our results showed that there is a small accuracy drop after quantization and therefore we did not perform any further finetuning of the quantized model. Currently, our tool can handle any of the HuggingFace ViTs from the TIMM library (Pytorch Image Models) (TIMM). This section describes these steps in more details."}, {"title": "2.1. Pruning method", "content": "PQV-Mobile supports several pruning strategies, corresponding to structured pruning. In structured pruning, a block is removed, which can be a neuron in a fully-connected layer, a channel of filter in a convolutional layer, or a self-attention head in a Transformer. An alternative approach is unstructured pruning (also called magnitude pruning) where some of the parameters or weights with smaller values are converted to zeroes. PQV-Mobile targets structured pruning methods as they do not rely on specific AI accelerators or software to reduce memory consumption and computational costs, thereby finding a wider domain of applications in practice (Fang et al., 2023).\nIn structural pruning, a 'Group' is defined as the minimal unit that can be removed. Many of these groups consist of multiple layers which can be interdependent and need to be pruned together so as to maintain the integrity of the resulting pruned networks. We follow the approach of (Fang et al., 2023) that uses a dependency graph to model these dependencies and find the right groupings for parameter pruning. Similar to (Fang et al., 2023), PQV-Mobile accepts a group (i.e., an Attention block of a ViT with Linear layers) as inputs, and returns a 1-D tensor with the same length as the number of channels. All groups must be pruned simultaneously and thus their importance should be accumulated across channel groups. PQV-Mobile supports the following groupings:\nMagnitude importance based grouping. In this case, L1- or L2-norm regularization term is applied to the loss function which penalizes non-zero parameters. If the value of a connection is less than a threshold, the connection is dropped (Anwar et al., 2017).\nTaylor importance based grouping. The importance is calculated as the squared change in loss induced by removing a specific filter from the network. This importance is approximated with a Taylor expansion which allows for faster computation from parameter gradients, even for larger networks (Molchanov et al., 2019).\nHessian importance based grouping. In this method the importance is computed using a fast second-order metric to find insensitive parameters in a model. In particular, the average Hessian trace is used to weight the magnitude of the parameters; parameters with large second-order sensitivity remain unpruned, and those with relatively small sensitivity are pruned (Yu et al., 2022)."}, {"title": "2.2. Quantization method", "content": "PQV-Mobile currently supports post-training quantization of both weights and activations from FP32 to FP16 and int8. We plan to extend this to int4 as future work. The following steps are performed for quantization using quantization libraries of Pytorch:\n\u2022 Quantize models for a specific backend: We first create a quantization engine based on a backend. The supported backends are: x86, FBGEMM (Facebook General Matrix Multiplication (Facebook FBGEMM)), QNNPACK (Quantized Neural Network Package (QNNPACK)), and ONEDNN (Intel's ONEDNN). The generated engine is then used to quantize the model using either static or dynamic quantization (Pytorch Quantization).\n\u2022 Convert Pytorch models to Torchscript format: Python models are inefficient to run during deployment. Therefore, we export the Pytorch models to production environments through Torchscript (Pytorch Torchscript),"}, {"title": "3. Experimental Results", "content": "In this section, we demonstrate the effectiveness of PQV-Mobile to prune and quantize Facebook's Data Efficient Image Transformers (DeiT). We evaluate the models' latency-memory-accuracy trade-offs. The structured pruning importance used in this study is based on Taylor's expansion (as shown later, it performs the best in terms of accuracy). The pruned model is then finetuned for 60 epochs (using distributed training on 4 GPUs) at a learning rate of 0.00015 with a batch size of 64. The finetuned pruned models are then quantized for the x86 backend engine and then converted to optimized and Lite format. We use the ImageNet dataset (IMAGENET) for image classification tasks and run the models on Intel Xeon ES2695 at 2.3 GHz. Pytorch-2.0.0 is used in all our experiments. Please note that our experiments are only meant to demonstrate the capabilities of our tool and not to achieve the state-of-the-art accuracy. All comparisons are performed on scripted, mobile optimized, and Pytorch Lite format models."}, {"title": "4. Conclusion and Future Work", "content": "This paper presents a combined pruning and quantization tool, called PQV-Mobile, to optimize vision transformers for mobile applications. The tool is able to support different types of structured pruning based on magnitude importance, Taylor importance, and Hessian importance. It also supports quantization from FP32 to FP16 and int8, targeting different mobile hardware backends. We demonstrate the capabilities of our tool and show important latency-memory-accuracy trade-offs for different amounts of pruning and int8 quantization with two types of Facebook DeiT models.\nAs future work, we plan to extend PQV-Mobile to int4 quantization. Additionally, we will extend this tool to target large language models as well."}]}