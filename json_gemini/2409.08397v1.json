{"title": "360PanT: Training-Free Text-Driven 360-Degree Panorama-to-Panorama Translation", "authors": ["Hai Wang", "Jing-Hao Xue"], "abstract": "Preserving boundary continuity in the translation of 360-degree panoramas remains a significant challenge for existing text-driven image-to-image translation methods. These methods often produce visually jarring discontinuities at the translated panorama's boundaries, disrupting the immersive experience. To address this issue, we propose 360PanT, a training-free approach to text-based 360-degree panorama-to-panorama translation with boundary continuity. Our 360PanT achieves seamless translations through two key components: boundary continuity encoding and seamless tiling translation with spatial control. Firstly, the boundary continuity encoding embeds critical boundary continuity information of the input 360-degree panorama into the noisy latent representation by constructing an extended input image. Secondly, leveraging this embedded noisy latent representation and guided by a target prompt, the seamless tiling translation with spatial control enables the generation of a translated image with identical left and right halves while adhering to the extended input's structure and semantic layout. This process ensures a final translated 360-degree panorama with seamless boundary continuity. Experimental results on both real-world and synthesized datasets demonstrate the effectiveness of our 360PanT in translating 360-degree panoramas. Code is available at https://github.com/littlewhitesea/360PanT.", "sections": [{"title": "1. Introduction", "content": "Text-driven image-to-image (I2I) translation seeks to generate a new image that reflects a given target prompt while following the structure and semantic layout of an input image. For text-driven I2I translation, recent training-free methods, such as Prompt-to-Prompt (P2P) [2], Plug-and-Play (PnP) [1] and FreeControl [3], are based on pre-trained latent diffusion models (LDMs) [7] and typically employ DDIM inversion [4] to obtain the corresponding noisy latent representation of the input image. Subsequently, they leverage attention control [1,2] or spatial control [3] to guide the translation process during denoising. By harnessing the powerful generative capabilities of pre-trained LDMs [7], these methods demonstrate commendable performance in translating ordinary images.\nHowever, directly applying these techniques to 360-degree panoramic images, which are commonly represented by using equirectangular projection [8], presents a unique and significant challenge. Unlike ordinary images, 360-degree panoramas possess inherent boundary continuity, where the leftmost and rightmost edges seamlessly connect. Existing I2I translation methods based on DDIM inversion fail to preserve this crucial characteristic, resulting in noticeable discontinuities at the boundaries of translated panoramas, as shown in Figure 1. To solve this problem, we propose 360PanT, a training-free method tailored for text-driven 360-degree panorama-to-panorama (Pan2Pan) translation. Our approach comprises two primary components: boundary continuity encoding and seamless tiling translation with spatial control.\nBoundary continuity encoding aims to embed the boundary continuity information of the input 360-degree panorama into the noisy latent representation. This is achieved by first creating an extended input image obtained from splicing two copies of the original input panorama. This extended input is then processed by the encoder of a pre-trained LDM. Finally, DDIM inversion is applied to the resulting latent feature, yielding a noisy latent feature that intrinsically encodes the boundary continuity.\nWhile one might consider directly applying existing state-of-the-art (SOTA) I2I translation techniques like PnP [1] or FreeControl [3] to this noisy latent feature, such an approach presents two significant drawbacks. Firstly, processing the entire noisy latent feature on a single high-end GPU (e.g., 24GB) leads to out-of-memory errors. Secondly, these SOTA methods cannot guarantee the preservation of identical left and right halves throughout the denoising process, potentially disrupting the 360-degree panoramic structure."}, {"title": "2. Related Work", "content": "Text-Driven 360-Degree Panorama Generation. The objective of text-driven panorama generation [30, 34\u201336] is to produce panoramic images aligned with given textual descriptions. Unlike ordinary panoramic images, 360-degree panoramic images offer immersive experiences and find broad applications in virtual reality [38], autonomous driving [37], and indoor design [40].\nFor synthesizing 360-degree panoramas from text prompts, Text2Light [33] introduces a hierarchical framework comprising a dual-codebook discrete representation, a text-conditioned global sampler, and a structure-aware local sampler. In contrast, recent approaches [6,31,32,39,41] explore text-to-image latent diffusion models [7] for text-driven 360-degree panorama generation. Among these methods, StitchDiffusion [6] proposes additional denoising twice on the stitch patch based on MultiDiffusion [30], ensuring the generated image to have identical left and right halves. We leverage this crucial attribute of StitchDiffusion to achieve seamless tiling translation in our 360PanT.\nText-Guided Image-to-Image Translation. Image-to-image (I2I) translation aims to learn a mapping that transforms images between domains while maintaining the semantic layout and structure of the input image. Over the past few years, GAN-based I2I translation methods have been extensively investigated [10-19]. Recently, diffusion models [4,20-23] have emerged as a powerful alternative to GANs [9], exhibiting superior performance in image synthesis. This shift has motivated research into exploring diffusion models for I2I translation [1\u20133, 5, 24\u201329].\nNotably, training-free text-driven I2I translation methods [1\u20133, 5, 29], building upon pre-trained latent diffusion models (LDMs) [7], have gained significant attention. For example, Plug-and-Play (PnP) [1] proposes to inject spatial features and self-attention maps into the denoising process of the translated image for enhancing structure preservation. Different from PnP, FreeControl [3] introduces appearance guidance and structure guidance to achieve spatial control of the translated image. Leveraging the powerful generative capabilities of pre-trained LDMs, these text-driven I2I methods achieve impressive results on ordinary images. However, when applied to 360-degree panoramic images, they fail to maintain visual continuity at the boundaries of the translated images. To address this problem, we propose a training-free method called 360PanT. By using our designed boundary continuity encoding and seamless tiling translation with spatial control, 360PanT successfully achieves the translation of 360-degree panoramas."}, {"title": "3. Methodology", "content": "The framework of our 360PanT is illustrated in Figure 2, consisting of two key components: boundary continuity encoding and seamless tiling translation with spatial control. Details of each component are elaborated in the following."}, {"title": "3.1. Boundary Continuity Encoding", "content": "Recent training-free text-driven image-to-image (I2I) translation methods, such as Prompt-to-Prompt (P2P) [2], Plug-and-Play (PnP) [1] and FreeControl [3], face inherent limitations when applied to 360-degree panoramas. This limitation stems from the inability of the DDIM inversion process [4], a core component of these methods, to encode the continuous information between the leftmost and rightmost sides of a 360-degree panorama. DDIM inversion, primarily designed for ordinary images, converts a clear image into a noisy latent representation without accounting for the cyclical nature of 360-degree panoramas. Consequently, these training-free I2I translation methods [1\u20133, 5] relying on DDIM inversion fail to maintain visual continuity between the edges of the final translated panorama.\nTo address this challenge, we propose a straightforward yet effective method to encode this crucial continuous information. Our approach involves firstly splicing two identical copies of the input panorama, to create an extended image that serves as input for the DDIM inversion process [4]. Formally, given an input 360-degree panorama $I_{in}$ with dimensions"}, {"title": "3.2. Seamless Tiling Translation", "content": "At this stage, we have a noisy latent feature $x_{t}$ including the continuous information of the original 360-degree panorama $I_{in}$. A direct approach to performing training-free text-driven panorama-to-panorama translation would be to apply existing I2I translation methods, such as PnP [1] or FreeControl [3], to $x_{t}$ and then crop the translated image (with dimensions $3 \\times H \\times 2W$) to obtain the final 360-degree output. However, this approach has two significant drawbacks: (1) directly processing the entire $x_{t}$ on a single high-end GPU (e.g., 24GB) results in out-of-memory errors; and (2) these methods cannot ensure that the translated image will still maintain identical left and right halves during the denoising process, potentially disrupting the panoramic structure.\nTo overcome these issues, we leverage a key property of StitchDiffusion [6], a method designed for generating 360-degree panoramas using a customized latent diffusion model [7]. StitchDiffusion inherently produces images with identical left and right halves by design, ensuring the preservation of the 360-degree panoramic structure. Furthermore, at denoising step $t$, where $t \\in \\{T,T \u2013 1,\\cdots,1\\}$, cropped patches of $x_{t}$, rather than the entire $x_{t}$, are independently processed within StitchDiffusion. Therefore, instead of directly applying existing I2I translation methods, we employ StitchDiffusion to translate the noisy latent feature $x_{t}$. This approach effectively addresses the aforementioned memory constraints and ensures the translated image maintaining identical left and right halves.\nSpecifically, at denoising step $t$, the noisy latent feature $x_{t}$ is divided into $n$ overlapping patches. Let $F_{i}(x_{t})$ represent the i-th cropped patch of size $W$, where $i \\in \\{1,2,\\ldots,n\\}$. Here, the mapping $F_{i}$ denotes the cropping operation for the i-th patch, and its inverse mapping, $F_{i}^{-1}$, places the patch back into its original position. The number of patches, $n$, is determined by $\\frac{2W}{w}+ 1$, where $w$ indicates the sliding distance between adjacent patches $F_{i}(x_{t})$ and $F_{i+1}(x_{t})$. In addition, let $\\I$ and $C$ denote a pre-trained latent diffusion model [7] and a target prompt, respectively. In this situation, the sequential denoising process of a training-free I2I translation using StitchDiffusion, termed seamless tiling translation process, can be represented as\n$x_{t-1} = \\frac{2}{\\prod_{i=1}^{n} F_i(1)} \\sum_{j=1}^{2} F_{n+1}^{-1}(\\mathcal{I}(F_{n+1}(x_t), C')) + \\frac{1}{\\prod_{i=1}^{n} F_i(1)} \\sum_{i=1}^{n} F_i^{-1}(\\mathcal{I}(F_i(x_t), C'))$,\nwhere $\\I$ $F_{n+1}(\\cdot)$ and $F_{n+1}^{-1}(\\cdot)$ are the j-th additional mapping and inverse mapping of the stitch patch, respectively; and $\\prod$ denotes $1^{F_{n+1}^1(1)} + 2^{F_{n+1}^2(1)} + \\sum_{i=1}^{n} F_i^{-1}(1)$, where $1$ refers to a latent feature with dimensions $4 \\times \\frac{H}{8} \\times \\frac{W}{8}$ with all values equal to 1. The stitch patch, a special cropped patch, is defined as $Splice(x_t[:,:, \\frac{3W}{4}: 2W], x_t[:,:, 0 : \\frac{W}{4}])$, where, as in Eq. 1, $Splice$ is the splicing operation.\nThrough the seamless tiling translation process (Eq. 2), we obtain the final denoised latent feature $x_0$ with dimensions $4 \\times \\frac{H}{8} \\times 2W$. Consequently, the corresponding translated image $\\hat{I}_{out}$ with dimensions $3 \\times H \\times 2W$ decoded from $x_0$ maintains identical left and right halves while corresponding to the target prompt C."}, {"title": "3.3. Seamless Tiling Translation with Spatial Control", "content": "Capitalizing on both boundary continuity encoding and seamless tiling translation, the diffusion model $\\I$ can produce a translated image $\\hat{I}_{out}$ with identical left and right halves, aligned with the target prompt C. However, the seamless tiling translation relies solely on C and the initial noisy latent feature $x_{t}$. Consequently, the translated image $\\hat{I}_{out}$ may not fully adhere to the structure and semantic layout of the extended input $\\hat{I}_{in}$. To address this issue, we propose to incorporate spatial control into the seamless tiling translation, enabling training-free text-based 360-degree panorama-to-panorama (Pan2Pan) translation.\nSpecifically, following the Plug-and-Play (PnP) method [1], we inject spatial features $f_t$ and self-attention maps $A_t$ from $x_{t-1} = \\Phi(x^{\\prime}_{t}, \\emptyset)$ into the seamless tiling translation process, where $t \\in \\{T,T \u2013 1,\\ldots,1\\}$. Here, $x_t$ is identical to $x_{t}$, and $\\emptyset$ represents a null text prompt. In this context, the seamless tiling translation process with spatial control is given by\n$x_{t-1} = \\frac{2}{\\prod_{i=1}^{n} F_i(1)} \\sum_{j=1}^{2} F_{n+1}^{-1}(\\mathcal{I}(F_{n+1}(x_t), C;f_t, A_t)) + \\frac{1}{\\prod_{i=1}^{n} F_i(1)} \\sum_{i=1}^{n} F_i^{-1}(\\mathcal{I}(F_i(x_t), C;f_t, A_t)).$"}, {"title": "4. Experiments and Results", "content": "Implementation details. The values of $H$ and $W$ in this paper are 512 and 1024. We set the values of split constant $\\alpha$ and sliding distance $w$ to 768 and 16, respectively. The version of the pre-trained latent diffusion model [7] is Stable Diffusion 2-1-base. For seamless tiling translation with spatial control, our 360PanT method primarily employs PnP's spatial control mechanism [1]. To enable support for diverse input conditions, we introduce a variant denoted as 360PanT (F), which utilizes FreeControl's spatial control [3] instead of PnP. The settings for the spatial control components and denoising steps T within 360PanT and 360PanT (F) are consistent with the default settings of PnP and FreeControl, respectively. All experiments were carried out using a single NVIDIA L4 GPU."}, {"title": "4.1. Comparisons with Other Methods", "content": "We compare our 360PanT with state-of-the-art (SOTA) text-driven image-to-image (I2I) translation approaches: SDEdit [5], Pix2Pix-zero [29], Prompt-to-Prompt (P2P) [2], Plug-and-Play (PnP) [1], FreeControl [3]. Visual results from the different methods on the translation of real-world and synthesized 360-degree panoramas are illustrated in Figure 3 and Figure 4, respectively. These figures demonstrate that these SOTA text-driven I2I translation methods fail to preserve the boundary continuity in the translated panoramas. In contrast, our 360PanT not only successfully maintains the visual continuity at the boundaries of the translated panoramas, but also ensures the translated results adhere to the structure and semantic layout of the input 360-degree panoramas. Note that due to space limitations, we only present part visual results here; additional visual results are in the supplementary material.\nTo further evaluate the performance, we analyze the CLIP-score and DINO-score metrics across the 360PanoI-Pan2Pan and 360syn-Pan2Pan datasets. The results, depicted in Figure 5, reveal a close alignment between PnP and 360PanT in both metrics. This similarity is expected, given that 360PanT adopts the same spatial control as PnP. However, a key limitation of PnP is its inability to maintain visual continuity at panorama boundaries. Conversely, our 360PanT can produce translated panoramas with continuous boundaries."}, {"title": "4.2. Ablation Studies", "content": "Effect of seamless tiling translation. To demonstrate the effectiveness of seamless tiling translation, we conducted some simple I2I translation experiments. Specifically, with a 360-degree panorama denoted as $I_{in}$ with dimensions 3 \u00d7 512 \u00d7 1024 (indicated by the red dashed box in Figure 6), two identical copies were directly spliced to generate an extended image $\\hat{I}_{in}$. Subsequently, DDIM inversion [4] was applied to the latent feature representation of $\\hat{I}_{in}$. The resulting noisy latent feature, $x_t$, underwent seamless tiling translation (Eq. 2) guided by two distinct text prompts, respectively. This process yielded two corresponding translated images. Figure 6 illustrates the successful generation of two translated images with dimensions $3 \u00d7 512 \u00d7 2048$. These images exhibit identical left and right halves while corresponding to their respective target prompts, highlighting the efficacy of the seamless tiling translation.\nSeamless tiling translation with spatial control. To investigate the impact of spatial control mechanisms on seamless tiling translation, we carried out a comparative experimental study. In this study, we utilized an extended input image $\\hat{I}_{in}$ for translation guided by a target prompt. This image underwent three distinct translation processes: (1) seamless tiling translation alone, (2) seamless tiling translation with PnP's spatial control, and (3) seamless tiling translation with FreeControl's spatial control. The resulting translated images are displayed in Figure 7.\nWe observe that, firstly, incorporating FreeControl's spatial control into seamless tiling translation improves the translated image's adherence to the structure and semantic layout of the extended input image $\\hat{I}_{in}$, compared with seamless tiling translation alone. Secondly, integrating PnP's spatial control into seamless tiling translation preserves the structure and semantic layout of $\\hat{I}_{in}$ even more effectively than using FreeControl's spatial control. Based on these findings, we adopt PnP's spatial control in our 360PanT method. To distinguish between these variations, we refer to 360PanT incorporating FreeControl's spatial control as 360PanT (F) throughout this paper. While 360PanT (F) is not so effective as 360PanT in structure preservation, it enables support for various input conditions beyond using standard 360-degree panoramic images, as described in Section 4.3."}, {"title": "4.3. Translation using Other Control Conditions", "content": "To showcase the efficacy of our 360PanT (F) in handling diverse input conditions beyond 360-degree panoramic images, we present translated 360-degree panoramas generated from other control signals. Specifically, we consider a Canny edge map and a segmentation mask as the input control conditions, respectively, extracted from corresponding 360-degree panoramic images by using the same methods described in FreeControl [3]. Figure 9 demonstrates a comparative study, highlighting the limitations of FreeControl in preserving visual continuity under these conditions. In contrast, our 360PanT (F) effectively maintains boundary continuity in the translated 360-degree panoramas."}, {"title": "5. Conclusion", "content": "We propose 360PanT, a training-free method for text-driven 360-degree panorama-to-panorama translation. This method integrates boundary continuity encoding and seamless tiling translation with spatial control. By constructing an extended input image, the boundary continuity encoding embeds continuity information from the original 360-degree panorama into a noisy latent representation. Guided by a target prompt, the seamless tiling translation with spatial control leverages this latent representation to generate a translated image with identical left and right halves while following the structure and semantic layout of the extended input image. This process successfully results in a final translated 360-degree panorama aligned with the target prompt. Extensive experiments on both real-world and synthesized 360-degree panoramas prove the effectiveness of our method in translating 360-degree panoramic images."}, {"title": "A. Supplementary Content", "content": "This supplementary material begins by providing an intuitive explanation for the choice of $\\alpha$. Subsequently, we detail the process of producing target prompts for both real-world and synthesized datasets. Further visual results obtained under different control conditions are then presented. Finally, we showcase additional translated results from using different methods on real-world and synthesized 360-degree panoramic images.\nExplanation for the choice of $\\alpha$. To intuitively explain the choice of the split constant $\\alpha$, Figure 10 visually depicts the cropping process in 360PanT at denoising step t (where t$\\in \\{T,T \u2013 1,\\ldots,1\\}$) for three distinct $\\alpha$ values. The top row displays the input 360-degree panorama $I_{in}$ and a diagram of the cropping operations based on the sliding window mechanism employed in the seamless tiling translation with spatial control. Each cropped patch, including the special cropped patch (stitch patch), then undergoes independent denoising guided by a target prompt. Subsequent rows highlight the cropped patches matching $I_{in}$ during the sliding window process, indicated by red or yellow dashed boxes. Observe that when $\\alpha = W$ or $\\alpha = \\frac{W}{2}$, two cropped patches matching $I_{in}$ but in different locations are denoised at each step t. Conversely, when $\\alpha = \\frac{3W}{4}$, only a single cropped patch matching $I_{in}$ undergoes denoising at each step. Crucially, the continuity of boundaries of these highlighted patches are not considered during denoising. Consequently, at each denoising step t, the fewer cropped patches matching $I_{in}$ are denoised, the better the boundary continuity of the final translated 360-degree panorama. Therefore, we set $\\alpha$ to $3W$4 in this paper, which results in a final translated 360-degree panorama with seamlessly connected boundaries, effectively avoiding local visible cracks.\nGeneration process of target prompts. Figure 11 illustrates the target prompt generation process for each real-world 360-degree panorama within the 360PanoI-Pan2Pan dataset. Utilizing a consistent template, \u201ca photo of {image name}\", an original prompt is constructed for each 360-degree panoramic image. Subsequently, a target prompt is formulated by combining a randomly selected translation type with the original prompt. Figure 12 depicts the analogous process for the 360syn-Pan2Pan dataset comprising synthesized 360-degree panoramas. Initially, 120 synthesized 360-degree panoramas are generated using a text-to-360-degree panorama model [6] guided by 120 original prompts. Similar to the real-world dataset, each target prompt consists of a randomly chosen translation type and its corresponding original prompt.\nTranslation using other control conditions. Diverse control conditions are extracted from corresponding 360-degree panoramic images using the methods described in FreeControl [3]. If a control condition lacks continuous boundaries, the translated result by our 360PanT (F) will exhibit noticeable content inconsistency at the boundaries. For instance, Figure 13 illustrates how using an extracted depth map $I_{in}$ with discontinuous boundaries as input leads to visible cracks in the extended input map $\\hat{I}_{in}$. Consequently, the translated image by 360PanT (F) shows content inconsistency in the stitched area. In contrast, we observe that extracted Canny edge maps and segmentation masks effectively maintain continuous boundaries. As shown in Figure 14, when using them as control conditions, FreeControl fails to preserve boundary continuity, but our 360PanT (F) consistently produces translated 360-degree panoramas with continuous boundaries, regardless of the input conditions.\"\n    }"}]}