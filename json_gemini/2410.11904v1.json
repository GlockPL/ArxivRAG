{"title": "Personalised Feedback Framework for Online Education Programmes Using Generative AI", "authors": ["Ievgeniia Kuzminyk", "Tareita Nawaz", "Shihao Shenzhang", "Bogdan Ghita", "Jeffery Raphael", "Hannan Xiao"], "abstract": "AI tools, particularly large language modules, have recently proven their effectiveness within learning management systems and online education programmes. As feedback continues to play a crucial role in learning and assessment in schools, educators must carefully customise the use of AI tools in order to optimally support students in their learning journey. Efforts to improve educational feedback systems have seen numerous attempts reflected in the research studies but mostly have been focusing on qualitatively benchmarking AI feedback against human-generated feedback. This paper presents an exploration of an alternative feedback framework which extends the capabilities of ChatGPT by integrating embeddings, enabling a more nuanced understanding of educational materials and facilitating topic-targeted feedback for quiz-based assessments. As part of the study, we proposed and developed a proof of concept solution, achieving an efficacy rate of 90% and 100% for open-ended and multiple-choice questions, respectively. The results showed that our framework not only surpasses expectations but also rivals human narratives, highlighting the potential of AI in revolutionising educational feedback mechanisms.", "sections": [{"title": "1. Introduction", "content": "Feedback is an essential component of online learning for students. As online programmes are more restricted in terms of interactive lecturer-student activities, there is a greater need for comprehensive, timely, and relevant feedback within the learning process. In the past two years, generative AI has witnessed a ubiquitous expansion across all daily life aspects, including education. With the advancement of Large Language Model (LLM) technology, generative AI was recognised as a valuable tool for delivering interactive, informed, real-time feedback to user prompts. This study aims to seamlessly integrate AI methods into the feedback process by employing a model that relies on ChatGPT to automate feedback generation for an online learning course. This development of generative AI offers novel possibilities to transform education to the advantage of teachers and students equally. The overarching goal is to improve the educational process by providing students with personalised feedback while simplifying the learning process. This tailored strategy will help improve academic performance and overall attitude towards study and encourage students to understand the educational material in depth. Beyond providing accurate individual student feedback for each assessment, leveraging data collected from a Learning Management System (LMS) will further allow the platform to track and support individual learning progress, offering tailored support.\nAl-powered generation presents a flexible solution for various educational applications, including the design and development of assessments, marking submissions, and offering personalised feedback and study guidance. Although its efficacy has been explored in research studies discussed in Literature review section of your paper, there is still much to learn about its success in different educational contexts. This study continues this line of research with an investigation of the efficacy of feedback mechanisms tailored to provide insightful guidance to learners across their learning journey rather than focusing on individual submissions; as a use case, the initial focus area is the cybersecurity online education programme, utilising data-driven insights to enhance the effectiveness of the feedback tool. However, the methodology and results that accumulate from this project could are subject-agnostic and can also be applied to flipped classroom programmes since students in these programmes interact with module materials on the digital learning platforms and create digital progress profiles that can be used to monitor their learning activities.\nBased on the National Student Survey (NSS) [1] that collects students' opinions on the quality of their courses, assessment and feedback scores tend to be amongst the lowest satisfaction scoring averages of 72-74%. Beyond the volume and efficacy of feedback, typical human-based feedback in a learning platform is typically delayed and further hinders the learning process; in contrast an effective tool must be timely and near-real time to support students. These demands call all be addressed by employing a generative AI such as ChatGPT. While an interesting research direction, from a wider educational perspective, all universities pursuing academic excellence continually seek innovative strategies to enhance the student experience [2, 3]. The initiative behind this automated feedback tool is essentially consistent with"}, {"title": "1.1 Motivation", "content": "This study aims to integrate artificial intelligence (AI) into the education process by leveraging the accuracy, breadth of knowledge, and ability to articulate extensive arguments that generative AI methods possess. The process draws from earlier studies such as [4], which investigated AI-driven information systems as an alternative for delivering education during the COVID-19 pandemic. Using prior research as a baseline, this paper progresses this conversation by investigating the potential of AI in providing students with individualised, tailored feedback.\nRecent research has focused on the evolving symbiosis between humans and AI; Wang's work on intelligent tutoring systems [5] summarises the existing studies in this area. Drawing from this paper, our project aims to further this ongoing conversation by investigating the potential of AI in providing students with meticulous and nuanced feedback.\nAs demonstrated by Paek and Kim regarding the current landscape of artificial intelligence in education (AIED), AI has a substantive impact on educational outcomes [6]. The quantitative analysis undertaken by the study reveals a notable increase in AIED research papers, especially since 2015, indicating the growing prominence of the field, but the authors also highlight the need for further exploration and acceleration of AIED studies, emphasizing the far-reaching impact of Al on the education ecosystem, from its principles and purpose through to content and methods, and finishing with evaluation. Notably, the study highlights the need for more diverse research topics, calling for a revolution in teaching methods to better incorporate the potential of Al across various domains. Building upon and, in some cases, challenging current trajectories, the feedback approach proposed by this paper aims to demonstrate the versatility of AI and its ability to make a more substantive and rewarding contribution to the education process."}, {"title": "1.2 Aims and Objectives", "content": "This study aims to explore how an AI can be leveraged to assist teaching and feedback in a digital learning platform. The implementation will consist of a learning framework that can efficiently analyse the course content and provide students with customized feedback by utilizing AI.\nThe proposed approach will include several stages: data gathering, embedding it into the LLM model, and using the resulting extended model to deliver personalised feedback to each student. From an implementation perspective, ChatGPT will be the LLM of choice, which will be linked to a learning platform through its API. From an evaluation perspective, the main priority is to make sure that the feedback that ChatGPT produces is sufficient and relevant but, equally important, customized to meet the needs of each student. Beyond the specific evaluation provided by the study, we aim to create a framework adoptable by other universities and institutions to promote education on a global scale.\nPersonalised feedback is an integral part of a student's educational experience regardless of education level, but is not consistently provided by educators. Some recent studies, such as [7, 8], attempted to bridge the gap and flagged the limitations encountered in the process, particularly related to the variability of understanding and capabilities within a student cohort. The framework proposed by this study aims to cater for the specific needs of each student and complement and adapt to the teacher evaluation and feedback efforts rather than compete with them. The research value of this study goes beyond the capability of the implemented tool; its aim is to provide a paradigm shift in the perception and provision of feedback in the academic realm."}, {"title": "1.3 Outline", "content": "The remainder of the paper is organised as follows. The next section provides an overview of the current state-of-the-art of feedback in education, particularly personalised feedback. Section 3 describes the methodology of the proposed method, which involves the data collection from the digital learning platform and embedding it within the LLM. Section 4 outlines the encompassing blocks of the developed framework and evaluation, followed by a discussion of the results in Section 5. Section 6 concludes the paper with a summary of the findings and potential future work avenues."}, {"title": "2. Literature Review", "content": "Automation of feedback should not just aim to reproduce human-based, manual feedback but complement it by contributing extensive features, including dynamism, ability to adapt, and interactivity. Given this perspective, this section will examine existing automated feedback frameworks and artificial intelligence models to create an ambience for the contextualisation of our research."}, {"title": "2.1 Automated Feedback", "content": "Artificial intelligence (AI) based automated feedback is a recent and transformative element among online programs. Marshall, in his study [9], highlights the potential of the fifth generation of computers in understanding natural language and contributing to the teaching/learning process. This signals a paradigm shift towards intelligent assistants or powerful tools, with one notable application being expert systems aiding academic staff in evaluating student work.\nIn the realm of online education, automated feedback offers numerous advantages. Fleckenstein et al. 2023 conducted a meta-analysis on the effectiveness of automated writing evaluation (AWE) tools, emphasising their role in"}, {"title": "2.3 Existing Personalised Feedback Frameworks", "content": "The landscape of personalised feedback frameworks is diverse, with innovative approaches emerging to address the unique challenges posed by various academic disciplines. This section provides a summary of several notable personalised formative feedback studies.\nPrior to the expansion of AI, large-scale personalised, potentially automated feedback was designed and delivered using alternative mechanisms, such as automating calculation processes. Beneroso and Robinson [14] proposed a novel approach for generating automated personalised formative feedback in chemical engineering coursework assignments, which used MS Excel to process student answers and compare them with the correct results. The system used a series of calculations embedded into the Excel file to determine the materials and energy required in a manufacturing process based on a large number of inputs and variables, from environmental conditions to chemical components. The approach supported the students learning by providing them with personalised feedback which highlighted the errors in the calculation, which in turn allowed them to tackle more challenging coursework components. The efficacy of this tool was demonstrated through two case studies; the results showed that the students utilising personalised feedback experienced a significant increase in their marks, with performance improvements that averaged 23.3% and 6.8% absolute marks for the respective case studies. This approach enhanced the learning process by promoting targeted engagement for skill development and also significantly reduced the associated workload required for delivering extensive personalised feedback.\nIn their study, Ogbuchi et al. [15] employed Microsoft Power Tools together with Power Apps, Power BI, and Power Automate with Moodle to produce individualised feedback information. This study demonstrated how the existing IT infrastructure can be adapted and applied to create a better learning environment. The combination of tools allowed the extraction, manipulation, and visualisation of data, enhancing student interaction and providing a more tailored approach to learning. In this study, two similar methods were used to provide feedback based on the data gathered from Moodle. The data was first pre-processed to be used as input for generating feedback based on performance. The data was then run through Power Automate to generate personalised emails from a template that were then sent individually to each student, providing them with tailored and timely feedback. The study [15] proves that it is practical to use existing technology platforms to release feedback in a more effective, efficient, and timely manner, ultimately improving the overall learning experience.\nAbel et al. conducted research that focused on the challenges of providing constructive and formative feedback to Higher Degree Research (HDR) students in academic writing [16]. To address the time constraints of the supervision team and writing expertise limitations, the study relied on Writing Analytics (WA), a framework leveraging text analytics to offer timely, formative feedback to students. The framework incorporates the Create A Research Space (CARS) model [17], extending a writing analytics tool to apply to HDR-level writing. This approach not only facilitates the development of practical writing skills but also introduces a set of clear terms to describe essential features of academic writing, aiding the process of thesis writing.\nThe study conducted by Qi et al. [18] focused on presenting a self-optimised feedback system in a learning management platform for Massive Open Online Courses (MOOCs), making it relevant for this research given the focus on feedback through Learning Management Systems. The research suggests that engagement is most effective when the feedback is personalised, and the learner is rewarded for their achievements. The article proposes a feedback framework that analyses user experience, communication channels, and feedback functionalities. It is established that effective, real- time, and multimodal data processing within the educational sector requires data analysis and statistics. The authors showed strong preferences for customized feedback by showing that such a system, combined with elements of adaptive learning and gamification, can engage and motivate students. The developed platform delivered customised feedback messages, varying the message content based on the student's individual performance scores. For example, users achieving a score below 50% were given a message commending them for their effort while also encouraging them to"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1 The Educational Context", "content": "A core characteristic of online programmes is that, unlike typical face-to-face programmes, each subject is delivered in a block-based manner rather than running along with other modules. As a typical example, a face-to-face programme may be delivering N modules (typically 2-3) in parallel during a period of 12 weeks, split into 10 weeks allocated for teaching, one reading week, and one revision week, with the students undertaking additional exercising time in the small tutorial groups with a teacher or a teacher assistant. In contrast, an online programme will be delivering each of the N modules in a 12/N timeframe, with no reading week, no small group exercises, and a very small timeframe (typically 3- 4 days) between the last lecture and the exam date [24]. This limited period is stressful both for educators and students and, should the students not maintain their attention on the respective subject, their assessment results will be significantly weaker, which will reflect poorly on the quality of the overall teaching process. Due to this strict design and the required sustained focus, students who have limited interaction or have a slower pace of progress may not have the necessary time to progress and, at least in the early stages, might show an overall poor academic performance which, in turn, may ultimately affect their commitment to the programme and willingness to progress.\nAs stated in the introduction section, the aim of this study is to explore how tailored approaches using AI might assist teaching and feedback in an online learning environment. This use case for the study is a group of students undertaking the MSc in Cybersecurity programme, delivered online, which includes a set of core cybersecurity modules, same as the environment used in [25, 26]. Each module consists of six weeks of active learning, followed by an examination, following the structure presented below in Fig. 1. Each module includes a series of activities, such as reading the provided web content, undertaking small practical tasks, answering weekly quizzes, participating in the discussion on the forum, and attending the weekly webinar with the instructor. All these factors might be significant in tracking student engagement for further investigation."}, {"title": "3.2 Methodology Design", "content": "The platform developed for the study includes three key components: content, functionality, and AI engine. The course content is placed in the Moodle learning management system; this content is used by the students to understand the taught subjects and, as part of the project, used by the platform to support feedback generation. The functionality of the platform consists of a backend written in Python and stored in AWS server, which connects the content with the AI feedback generation. Finally, the generative AI model for generating personalised feedback uses ChatGPT. The system architecture diagram (Fig. 2) illustrates the interaction among these components.\nAs part of the typical learning interaction, each student enrolled in the online programme can access the LMS with the learning material of the module, read the content for a current week and attempt the end-of-week quiz. The platform retrieves for each student the answers to the weekly quiz from the learning platform using the Moodle APIs and forwards them to the back-end AWS tables. The interaction data can be summarised to the following fields: module, serial number"}, {"title": "3.3 Implementation", "content": "Due to the level of interaction and customisation required, full implementation of the designed framework was not possible in a live, operational learning platform. In order to demonstrate and evaluate the functionality, the system was implemented on an EC2 instance in AWS, running a Moodle test learning management system featuring two main courses: Cryptography and Network Security. The Moodle installation also included a Moodle database, which stored the necessary user anonymous IDs, courses, questions, and student attempts.\nThe content of the course wis stored in the database as a table (rows and columns) in plain text format with the corresponding html tags. The python scripts parse the HTML content from the Moodle web pages into plain text to be stored as an input for the Al model. The functionality module of the platform monitors the database for changes and extracts student interaction which is also passed to the AI model as vectors.\nThe model is trained with the course content and user interaction, more specifically answers to the quizzes. From an Al perspective, this task can be achieved through various natural language processing techniques, including fine-tuning, prompt-tuning, and Retrieval-Augmented Generation (RAG).\nFine-tuning involves taking a pre-trained model, such as GPT, and feeding it a set of training data that encompasses various question-answer scenarios, ensuring a diverse and robust training dataset. This process updates the parameters of the default pre-trained model, meaning it can be customized for specific tasks, to save computational resources and time. The fine-tuned model can then be thoroughly tested using validation data to evaluate its proficiency in generating feedback across a spectrum of user inputs. The second option, prompt tuning, is a technique where prompts are provided to guide the text generation of the model. Unlike fine-tuning, prompt tuning focuses on creating appropriate prompts and specific instructions or queries within the prompt that produce the intended results; this guides the model output towards the desired direction and outcome. Prompt tuning takes advantage of the flexible nature of GPT-like models when producing text based on a specific provided prompt. It is a more controlled method, requiring less labelled data compared to fine-"}, {"title": "4. Evaluation and Results", "content": "The personalised feedback framework was evaluated using two courses from the online MSc in Advanced Cybersecurity Program at King's College London [24]. As introduced in Fig. 1, each module has a structure and consists of six weeks of teaching and learning content with a quiz after each week. A quiz has both multiple-choice and open- ended questions. The evaluation consisted of two performance tests: efficacy in providing feedback for free-text questions and efficacy in providing feedback for MCQ questions."}, {"title": "4.1 Experiment Setup", "content": "Prior to the evaluation of the ChatGPT marking and feedback, the LLM requires a reference on the feedback, which is expected to be constructive, highlighting the incorrect parts and providing study advice. To improve its performance, in addition to the RAG information, the model was also provided with exemplary questions and model answers, including correct, partially correct, and incorrect answers as a reference set, to guide the model on the type, content, and level of feedback required. By doing this, we instruct ChatGPT model how to answer to the student's input. Based on a preliminary evaluation of the model, ChatGPT does provide accurate marking without fine-tuning, but exemplary feedback significantly improves the structure, focus, and level of detail in the feedback.\nAn example of a fully correct answer is shown in Fig. 3. A fully correct answer is one where the student accurately explains the concept and includes all the relevant details. For a fully correct answer, the model must provide full marks, and confirm the answer is correct. Preliminary tests indicated that the model is relatively flexible in terms of grammar, syntax, and order when marking correct answers, as it does not penalise for language, wording, or phrasing variations that do not affect the accuracy of the answer."}, {"title": "4.2 Evaluation of Generative AI model for Free-Text questions", "content": "Evaluating the efficacy of our model was based on a review of the responses given by the model for a set of 30 free- text question-answer pairs. For each question, the efficacy percentage of the model-generated feedback was calculated based on the criteria by comparing the AI results and feedback with the correct answers for the respective questions. Connecting feedback to specific criteria from rubrics allows the delivery of structured, consistent feedback and helps students to assess their progress against the intended learning outcomes, leading to a better understanding of their proficiency [30]. Similar methods of evaluation were seen in the work of Jacob et al., where they compared GPT- generated feedback for free-text questions against teacher-given feedback, explicitly referencing the criteria in rubrics [19]."}, {"title": "4.3 Evaluation of Generative AI model for MCQ questions", "content": "Evaluating the accuracy of the ability of the model to evaluate answers for multiple-choice quiz questions followed a similar methodology. The evaluation set included 20 multiple-choice questions; half were answered correctly and the other half incorrectly. A binary scoring system was employed, where a score of 1 is awarded for correctly identified options and 0 for incorrect ones. Upon evaluation, the model was 100% accurate in grading the questions, demonstrating its reliability and effectiveness in accurately identifying the correct answer from a set of options. It should be noted that this level of efficacy was achieved due to the RAG embedding of specific module materials into the model."}, {"title": "5. Discussion", "content": ""}, {"title": "5.1 Discussion of Framework Performance", "content": "As shown in the previous section, the proposed approach provides a reliable, efficient, and fast alternative for marking and providing feedback for both free-text and multiple-choice questions. From a workload impact perspective, the benefits of the AI-based platform are substantive, as it allows for removing the most time-consuming task, which is the examination of individual student answers, marking, and delivery of meaningful, helpful, and relevant feedback. While ChatGPT does require time to generate feedback as it refers to the relevant chapters of the course content to find the right solution, the delay is negligible and perceived as near-realtime by the students.\nThe proposed model can self-evaluate student answers without a reference answer due to the course information loaded through RAG, but to ensure accuracy and consistency, the inclusion of the reference answer is desirable. Also, considering the wider context, this is also required from a quality assurance perspective. As a result, when comparing the time required to prepare quizzes, the duration for academic staff and ChatGPT are comparable. It is relevant to remember that the GPT model uses data taken from course content and it does not generate additional information. Because of this, the model accuracy still depends strictly on the content creation process.\nOverall quality can be quantified based on the work of Paul et al., where high-quality feedback encompasses various criteria, such as encouragement, recognizing effort, acknowledging achievements, and offering considerate criticism [31]. This study focuses on developmental aspects, offering transferable suggestions, identifying goals, and suggesting strategies for improvement. In contrast, low-quality feedback lacks depth, focusing too much on minor details rather than the main points, and may be unclear, inconsistent, or lacking in justification and transparency."}, {"title": "5.2 Comparison with Other Studies", "content": "In recent years, a significant number of studies investigated the use of AI models such as GPT towards producing question-based and essay assessments, there is a notable research gap around the ability of LLMs ability to provide feedback. This section will provide a brief overview of existing studies that use generative AI to evaluate written responses and compare these studies with our proposed method.\nThis study evaluates the ability of an AI model to provide formative feedback to students within a Cybersecurity module context. An example of a similar work but with a slightly different focus is the one from [19], which investigates the quality of formative feedback for student-written essays. In their study, the authors compared the feedback provided by both human evaluators and an Al model, whereas this study focuses on evaluating the performance of the AI model alone in providing personalised and coherent feedback to students. Despite this difference, both works employ a similar method of assessing the efficacy of the model's responses, basing the evaluation on predefined criteria, which includes clarity, accuracy in providing guidance for improvement, referencing based on either module materials or given criteria, and the presence of a supportive tone. Similar criteria can be seen in [33], which employs criteria such as content, language, and structure of responses, but has a different scoring method.\nWhile Jacob et al. employ a 1 to 5 scale for each feedback component before calculating an overall percentage, the proposed approach derives sub-percentages for specific criteria, which collectively contribute to the overall efficacy rate. In terms of results, both studies rely on statistical analysis to compare the quality of feedback. Jacob et al. find that human feedback generally outperforms ChatGPT feedback in most categories, except for criteria-based feedback, where the model is given explicit references to the criteria for source-based argumentative writing. In comparing the final efficacy percentages, both studies demonstrate similar achievements. Jacob et al. observed their model achieving a score of 4 out of 5, equivalent to 80% in the accuracy criteria [19]. Similarly, as presented in the evaluation section, the proposed platform yielded an average efficacy rate of 90% across the various components evaluated for free-text answers. The slight difference in overall rate can be attributed to the divergent implementation and coding strategies employed in the two studies, as the proposed architecture directly embeds course content data into the model by converting the data from Moodle into a vector form, which minimises errors while enhancing efficacy. In contrast, Jacob et al. rely solely on the original knowledge base of the ChatGPT model without additional, contextual data. Despite this, both outcomes demonstrate the effectiveness of the respective approaches in providing accurate feedback, while also highlighting that there is still room for improvement in terms of using AI consistently for grading assignments."}, {"title": "5. Conclusions", "content": "In recent years, generative AI and LLMs demonstrated their ability to deliver knowledge-based support for a wide range of areas, from basic queries to resolving complex tasks. Education is one of the areas that is receiving increasing attention from the research community as a potential beneficiary of AI to assist the learning journey of the students and the quality of graduates. This project proposes and investigates the efficacy and efficiency of an AI-based platform that leverages the capabilities of ChatGPT and Retrieval Augmented Generation to deliver personalised feedback for student assessments. Based on the evaluation tests, the platform delivers an accuracy rate of 90% for open-ended questions and 100% for multiple choice questions, delivering focused, personalised, and supportive feedback. As a proof of concept, the implemented platform demonstrates that automated methods can enhance the learning experience, providing students with personalised support and fostering a more effective educational ecosystem.\nIn a wider context, the framework could be used to help students improve consistently by implementing a feedback loop mechanism. This entails gathering the feedback generated and the course data to guide the generation of personalised knowledge tests tailored for areas where a student needs improvement. Customising quizzes based on the student performance across a subject will significantly enhance the learning experience and promote targeted skill development."}]}