{"title": "Advancing Multi-Modal Sensing through Expandable Modality Alignment", "authors": ["Shenghong Dai", "Shiqi Jiang", "Yifan Yang", "Ting Cao", "Mo Li", "Suman Banerjee", "Lili Qiu"], "abstract": "Sensing technology is widely used for comprehending the physical world, with numerous modalities explored in past decades. While there has been considerable work on multi-modality learning, they all require data of all modalities be paired. How to leverage multi-modality data with partially pairings remains an open problem.\nTo tackle this challenge, we introduce the BABEL framework, encompassing the neural network architecture, data preparation and processing, as well as the training strategies. BABEL serves as a scalable pre-trained multi-modal sensing neural network, currently aligning six sensing modalities, namely Wi-Fi, mmWave, IMU, LiDAR, video, and depth. To overcome the scarcity of complete paired data, the key idea of BABEL involves transforming the N-modality alignment into a series of two-modality alignments by devising the expandable network architecture. This concept is also realized via a series of novel techniques, including the pre-trained modality tower that capitalizes on available single-modal networks, and the adaptive training strategy balancing the contribution of the newly incorporated modality with the previously established modality alignment.\nEvaluation demonstrates BABEL'S outstanding performance on eight human activity recognition datasets, compared to various baselines e.g., the top multi-modal sensing framework, single-modal sensing networks, and multi-modal large language models. BABEL not only effectively fuses multiple available modalities (up to 22% accuracy increase), but also enhance the performance of individual modality (12% averaged accuracy improvement). Case studies also highlight exciting application scenarios empowered by BABEL, including cross-modality retrieval (i.e., sensing imaging), and bridging LLM for sensing comprehension.", "sections": [{"title": "1 INTRODUCTION", "content": "Motivation: Sensing offers unique abilities to perceive the physical world. It has been widely deployed in various applications across diverse fields, including health-care, mixed reality, smart driving, and many others. Over the past decades, numerous sensing modalities have been explored [1, 2, 31, 33, 35, 47, 49, 50, 68]. Each sensing modality provides a unique and complementary viewpoint for observing the world, thereby necessitating the simultaneous use of multiple sensing modalities, known as multi-modal sensing.\nEarly methods for organizing multiple sensing modalities relied on handcrafted heuristics or features [69], which is proved difficult to scale across various tasks due to the complexity of sensing signals and environments. Recent advancements in Deep Learning (DL) based methods have offered promising solutions [13, 21, 67]. Given paired modality data as inputs, DL methods could identify the complementarity among various sensing modalities, a process known as modality alignment.\nModality alignment projects the representations of each sensing modality into a unified and shared space, as depicted in Fig. 1. The alignment process could empower sensing in"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "2.1 Multi-Modal Sensing\nSensing, through various modalities like vision, micro-electro-mechanical-system (MEMS) sensors, and wireless RF sensors, is now a ubiquitous method for comprehending the physical world, capturing a wide range of information from the environment or specific objects. Multiple sensing modalities provide complementary capabilities. For instance, LiDAR creates long-range 3D environmental maps for vehicles, ultrasonic sensors offer close-range detection, and cameras interpret road signs for driver-assistance systems. This combination leads to the concept of multi-modal sensing.\nEfficient multi-modal sensing relies on effectively fusing insights from each modality. Early strategies used handcrafted heuristics to link modalities, like A3's [69] use of a gyroscope for smartphone attitude estimation, supplemented by calibration from magnetometer and accelerometer. However, these strategies aren't scalable, especially with complex sensing signals and environments, making the construction"}, {"title": "2.2 The Power of Modality Alignment", "content": "DL-based multi-modal sensing methods outperform heuristic ones due to modality alignment. The alignment restructures one modality's feature according to the representation space of another, eventually projects each modality's features into a unified representation space through the training. This space facilitates the ease of manipulation. Consequently, modality alignment offers a comprehensive understanding of sensing data by utilizing each modality's unique strengths.\nBeyond augmenting multi-modal sensing fusion, the modality alignment could further empower new sensing applications. As illustrated in Fig. 1, aligned modalities are mutually retrievable. Specifically, utilizing the joint feature space as a proxy, one could employ Wi-Fi channel state information (CSI) to derive the corresponding embedding in the visual modality, subsequently generating an image from the visual embeddings. Intuitively, this could be considered as an alternative realization of Wi-Fi imaging. The joint feature space also facilitates the use of multiple aligned modalities for retrieval, thereby enabling multi-modal imaging without necessitating additional intervention.\nThe rise of Large Language Models (LLMs), presents new opportunities for sensing to interact with and understand the physical world [52]. Despite the development of multi-modal LLMs, the wide range of sensing modalities requires a unified sensing ontology for seamless integration with LLMs [25]. This alignment creates a unified representation that bridges the gap between various sensing modalities and LLMs.\nTo demonstrate the power of modality alignment, BABEL aligns six prevalent sensing modalities, including vision, depth, IMU, Wi-Fi, mmWave, and LiDAR."}, {"title": "2.3 Challenges and Opportunities", "content": "Modality alignment is a growing AI research field, involving various methods [43, 45]. Of these, contrastive learning (CL)"}, {"title": "3 BABEL OVERVIEW", "content": "BABEL, to the best of our knowledge, is the first scalable multi-modal pre-trained network, specifically designed for sensing applications, suitable for a multitude of downstream tasks. BABEL consists of the model architecture designs, training strategies as well as the data preparation and processing techniques. In BABEL, we present two designs to build the network with constraint data, namely pre-trained modality tower and expandable model architecture to cope with the scarcity challenge of paired sensing data and multi-paired sensing modalities.\nIn the design of the pre-trained modality tower, our aim is to harness the power of existing feature extractor within singular modality sensing to construct the modality alignment network, thereby significantly decreasing the necessity for extensive paired training samples.\nThe crux of this design lies in the efficient alignment of representations across pre-trained encoders. Thereby, we introduce the modality tower, consisting of the pre-trained encoder, and the concept alignment module. The encoder could be based on signal processing and neural networks from existing DL models. The concept alignment module"}, {"title": "4 PRE-TRAINED MODALITY TOWER", "content": "4.1 Assembling Modality Towers\nIn the alignment of each modality, our initial step involves constructing a modality tower. Subsequent to this, we execute the contrastive learning on these modality towers. The modality tower incorporates two fundamental components: a pre-trained encoder, and a concept alignment module.\nComparing with conventional modality alignment methods i.e., CLIP, BABEL's key design lies in the utilization of a pre-trained encoder within a singular modality, proving particularly effective for sensing modalities.\nBABEL'S effectiveness can be attributed to two key factors. Firstly, the process of assembling the modality tower adheres to the proven method of parameter-efficient fine-tuning (PEFT) [27], a technique notably successful in addressing the vision-language modality alignment problem,"}, {"title": "4.2 Aligning Modality Towers", "content": "Upon assembling the modality tower for a given modality, we strive to align them through the contrastive learning."}, {"title": "4.3 Augmenting Modality Towers", "content": "We also propose to augment the modality towers by employing multiple encoders for the particular modality. The concept of modality tower augmentation is inspired by model ensembling [7, 9], where multiple weak learners combine to create a stronger one, improving accuracy and performance. This method has proven to effectively decrease variance and bias in each weak learner.\nIn BABEL, we would construct an augmented modality tower when incorporating additional encoder. We align the augmented modality towers in accordance with the process delineated in 4.2. Specifically, We construct two modality towers, \u0393a and \u0393d, using pre-trained encoders ea and \u03b7 respectively. We align these towers using positive pairs P = (xa, xa) and negative pairs Z = {(xa, xa)} where i \u2260 j. The alignment is achieved through loss functions from Equations 3 and 4. The similarity sim is computed using output embeddings from both towers."}, {"title": "5 EXPANDABLE MODEL ARCHITECTURE", "content": "5.1 Prototype Network\nAligning multiple sensing modalities (such as six or more) with partially paired modalities is challenging. In response to this, one of key designs in BABEL is the expandable model architecture, which transforms the training process for N modality alignment into a series of two modality alignment phases, exploiting existing datasets with paired modalities.\nTo elaborate, consider the alignment of three modalities: \u03b1, \u03b2, \u03ba, with the available datasets \u0395\u03b1\u03b2 and \u0395\u03b1\u03ba. We initially employ Ea\u00df to align the modalities a and \u1e9e, as discussed in \u00a74.2, yielding the network Ha\u00df, which we term the trunk"}, {"title": "5.2 Growth Orders", "content": "BABEL transforms the N-tuple modality alignment into a sequence of two-modality alignment phases, thereby raising a potential question regarding the differences between the conventional completed alignment and our expandable alignment with varying modality growth orders. To analyze this, we utilize a three-modality alignment, i.e., IMU, skeleton, and video from UTD-MHAD dataset [11], as an example.\nAs depicted in Fig.6, we utilize t-SNE to render the representation space of each modality visible. As evident in Figure 6a, before alignment, features that have not undergone alignment training exhibit significant distribution differences. Fig. 6b shows the conventional triplet alignment successfully bridge the modality gaps, aligning the three modalities. In contrast, the expandable network architecture within BABEL employs a sequence of two-modality alignment training phases as a replacement for the joint alignment. As illustrated in Fig. 6c, we initially align the IMU and skeleton modalities followed by the video modality, effectively bridging the modality gaps as well.\nOur method is flexible regarding alignment order. Fig. 6d shows representations from each modality achieved by an alternately ordered network: first aligning skeleton and video, then IMU. Despite varying sequences, a common representation space is achievable. Further evaluation would be discussed in Section 8.1.5."}, {"title": "6 ADAPTIVE TRAINING STRATEGY", "content": "We in further propose our training strategies to optimally integrate the insights derived from the newly aligned modality during network growth. Specifically, we implement two strategies for the training of the concept alignment module and the prototype network, respectively.\nFor the training of the concept alignment module during network growth, we employ adaptive weighted contrastive training. The key of this design lies in dynamically adjust the proportion of proximity between modalities during the modal alignment process.\nAs per Equation 5, the contrastive loss in aligning modality a and \u1e9e includes two parts: La\u2190\u03b2, the loss when \u1e9e approximates a, and L\u1e9ea, the loss when a approximates \u00df. We find reliable and unreliable modalities in various modality combinations and datasets. Naturally, modalities with robust encoders and abundant data are more reliable, so we expect less reliable ones to converge towards them. During network growth, careful updates are needed in the junction modality tower to add insights from the branch without disrupting aligned modalities. Hence, we integrate weights into Equation 3 as follows:\n$L_{\\Gamma_{\\alpha\\beta}}^{IM} = \\frac{w_{\\alpha\\beta}L_{\\alpha \\leftarrow \\beta}^{LM} + w_{\\beta\\alpha} L_{\\beta \\leftarrow \\alpha}^{LM}}{2}$\nwhere M represents a batch randomly drawn from the dataset Ea\u00df, and wa\u2190\u03b2 and w\u03b2\u2190a denote the normalized weights. Intuitively, we lean towards attributing a larger weight wa\u00df if modality a is deemed more reliable and established, while a smaller weight is assigned otherwise.\nIdentifying the appropriate weights presents a challenge. A static weighting scheme is suboptimal as each modality may differ in respect to data volume and quality, encoder proficiency, as well as the fresh insights and contributions it brings to the aligned modalities. As such, we opt for a dynamic weighting strategy. Particularly, we employ gradients as an indicator to adaptively modify the weights,\n$w_{\\alpha \\leftarrow \\beta}^{WM} = \\frac{1}{\\| \\nabla M_{\\beta} L^{LM}(\\Gamma_{\\alpha}, \\Gamma_{\\beta}) \\|}$\nwhere \u2207 represents the accumulated gradients of all parameters within the concept alignment modules of the modality towers \u0393\u03b1 and I\u00df when computing the loss L\nWMwithin\nthe batch M. We calculate\nWMwa \u2190\u03b2 \u2190\u03b1 in a similar way. Then we normalize them as,\nwawWa\u03b1WM +w\u03b2\u2190\u03b1WM = 1,\nGradients effectively indicate how the loss function varies with the model parameters. If training pairs don't provide new insights to the trunk network during the growth, it leads"}, {"title": "7 IMPLEMENTATIONS", "content": "7.1 Data Preparation\nOverall, we utilize five datasets for the alignment, as itemized in Table 1, comprising paired samples across divergent dual modalities. These datasets are for human activity recognition (HAR) tasks, but the certain activities are totally different. Despite the provision of activity labels within these datasets, we adopt a self-supervised training approach, labels are not used. Throughout every dataset, depth signals undergo a conversion into a human skeleton format. As such, we employ the term skeleton to denote the depth modality."}, {"title": "7.2 Data Augmentation", "content": "We implement two data augmentation techniques on the raw data, ultimately enlarging the data pairs by 600x. (i) Down-sampling. Raw pairs undergo down-sampling at different ratios, simulating diverse sampling rates on various devices or accelerating the action at distinct ratios. This method augments the raw pairs by a factor of 300x. (ii) Action-segmentation. The raw action sequence is randomly truncated, simulating incomplete activity sensing. We ensure the segmented sequence's shortest length is over 50% of the"}, {"title": "7.3 Selections of Pre-trained Encoders", "content": "Next we introduce the pre-trained encoders we use for building the modality alignment network.\nIMU. We utilize the LIMU-BERT encoder [54], renowned for its proficiency in generating generalized representations. It is pre-trained on a range of IMU datasets.\nSkeleton. We utilize the Spatial-Temporal Graph Convolutional Network (ST-GCN) [56] as our encoder, which is pre-trained on extensive datasets, notably the NTU-RGBD [42].\nVideo. We employ ResNet3D model [44] as the encoder, which is pre-trained on Kinetics-400 dataset [22].\nWi-Fi. For Wi-Fi CSI, we fails to obtain one powerful pre-trained encoder. Therefore we apply multiple encoders to augment the modality tower of Wi-Fi. Specifically, we utilized a Vision Transformer (ViT) and a combination of Convolutional Neural Network (CNN) and Gated Recurrent Unit (GRU) as our encoders. They are pre-trained on UT-HAR [60] datasets.\nmmWave. We employ the signal processing based encoder for this modality. We use doppler fast fourier transform (FFT) and angle FFT, generating range-doppler heatmaps and range-angle heatmaps, respectively. We supply an additional spatial ResNet18 [46] to further extract features from them.\nLiDAR. We use the Point Transformer [66], which is pre-trained on the ModelNet40 dataset [51]. The encoder cannot extract temporal features, we add an additional ST-GCN as the additional temporal feature extractor, which is pre-trained on the NTU-RGBD [42] dataset."}, {"title": "7.4 Training Details", "content": "We commence the training process with the IMU and skeleton modalities. Subsequently, we integrate the video modality, aligning it with the pre-existing skeleton modality. Next, we incorporate the Wi-Fi modality into our framework, leveraging the paired Wi-Fi and skeleton data. This is followed by the introduction of the mmWave modality, which is linked with the intermediate Wi-Fi modality. Ultimately, we incorporate the LiDAR modality, capitalizing on its integration with the paired video modality.\nWe employ the AdamW optimizer [30] with a batch size of 256 and an initial learning rate of 1 \u00d710\u22124. For each phase of network growth, we judiciously allocate a varying number of training epochs, typically up to 500, or cease the training process once convergence is attained. The learning rate for downstream tasks is adjusted between 0.001 and 0.1. We train on two NVIDIA A100 GPUs, spending around 20 hours to align six modalities."}, {"title": "8 EVALUATION", "content": "We evaluate pre-trained BABEL by employing a typical downstream sensing task, human activity recognition (HAR). Furthermore, we would demonstrate two applications enabled by BABEL, namely the cross-modality retrieval and the LLM integration.\n8.1 Evaluation on HAR\nWe evaluate BABEL on 8 datasets, including 4 multi-modal datasets namely UTD-MHAD [11], OPERANet [6], XRF55 [46] and MM-Fi [59], 4 singular-modal datasets, namely UCI [39], Widar3.0 [65], mRI [3] and MSRAction3D [24].\nWe compare BABEL with a broad range of baselines, including the multi-modal sensing baseline Cosmo [34], SOTA singular-modal sensing baselines, LIMU-BERT [53], SenseFi [58], MARS [5], MeteorNet [29], and PointTransformer [66]. We compare with MLLMs which hold potential for interpreting sensing signals, including OneLLM [19] and M4 [61].\nUnless otherwise noted, the results for BABEL are obtained from a one-shot setting, where only one labeled sample per class is used to train the downstream classifier. Given the inherent difficulty of obtaining labeled samples for sensing applications, this setting highlights BABEL's performance as a pre-trained network."}, {"title": "8.1.1 Performance on Multi-modal Datasets", "content": "Table 2 shows the evaluation results on four multi-modal datasets. The modality alignment technique in BABEL significantly improves performance in each individual modality, particularly in sensing modalities. For instance, classification accuracy for 27 human activities in the IMU modality increases from 20.19% to 31.77% before and after alignments. For the skeleton modality, there is a significant improvement by 12.02%. The Wi-Fi modality sees an approximate 10.74% enhancement. The mmWave modality shows a substantial increase from 30.32% to 50.30%, and the LiDAR modality achieves an accuracy of 43.91%, up from 28.43%. Overall, BABEL brings around averaged 12% accuracy improvement on six aligned modalities across various datasets. Such gains are achieved by aligning each modality into a unified representation space, facilitating mutual learning. Sensing modalities benefit significantly, while gains are limited for video modalities."}, {"title": "8.1.2 Performance on Singular-modal Datasets", "content": "The supervised learning performance of BABEL on four full singular-modal datasets is detailed in Table 3. Owing to the effectiveness of multi-modality alignment, BABEL consistently outperforms the SOTA methods for each individual modality. Notably, BABEL demonstrates significant improvements for the mmWave and LiDAR modalities, achieving gains of 8.93% and 6.14%, respectively. In the Wi-Fi modality, BABEL outperforms SenseFi by 4.4%. For the IMU modality, BABEL attains an accuracy of 81.4%. It is important to note that none of the datasets evaluated here were included in the pre-training dataset collection, highlighting the generality of BABEL.\nBesides performance improvements, individual modalities could be empowered with new capabilities through BABEL. For example, Wi-Fi sensing can be considerably improved in terms of its cross-domain capability. To showcase, we select two settings with different transmitter-receiver arrangements, denoted as S1 and S2 from the OPERANet dataset [6]. The encoder ViT [15] trained on UT_HAR [60] attains an accuracy of 26% for classifying 6 activities on S2. This stands for the baseline of the cross-domain Wi-Fi sensing. Then we fine-tune the encoder on S1, then test on S2, achieving an accuracy of 54.5%. It showcases the performance of the conventional domain adaptation approach. Finally, we use BABEL, which accomplishes 62.47% accuracy without any fine-tuning, evidencing its cross-domain capability."}, {"title": "8.1.3 Comparison with Cosmo [34]", "content": "Cosmo is the SOTA sensing fusion framework, but unlike BABEL, it requires all modalities to coexist within one dataset, limiting its expandability to datasets with complete paired data. Thus to compared with Cosmo, we utilize the same paired IMU-skeleton from the UTD-MHAD [11] that it excels. An equal amount of data is employed to train both Cosmo and a bi-modality version of BABEL. We train Cosmo and BABEL for 10 times with"}, {"title": "8.1.4 Comparison with MLLMs", "content": "There has been a significant development in MLLMs [19, 64]. These models are capable of understanding multi-modal inputs, including sensing modalities like IMU potentially. For comparison, we select typical MLLMs e.g., OneLLM [19]and M4 [61], and evaluate their performance on the UTD-MHAD [11] with HAR tasks. OneLLM and M4 use Meta-Transformer [64] and ImageBind [16] to interpret sensing signals, respectively. The results are summarized in Table 4. Firstly, current MLLMs can only support a limited number of sensing modalities, like IMU. Secondly, they only achieve a classification accuracy of around 5%-6%. In stark contrast, BABEL significantly outperforms these with a classification accuracy of 31.77% on IMU while supporting other five sensing modalities.\nThe rationale that these MLLMs seems supporting sensing modalities, but struggle to comprehend IMU data and manage HAR tasks, is their training only on the Ego4D dataset [18]. Without sufficient training, these models are restricted to trained data, limiting their cross-domain capabilities. Furthermore, these MLLMs are unable to be trained on other sensing datasets due to data scarcity and absence of techniques like pre-trained modality tower and the expandable architecture, which are introduced in BABEL."}, {"title": "8.1.5 Ablation Study and Growth Orders", "content": "The techniques, including the pre-trained encoders, expandable network architecture and adaptive training strategy, are all essential for constructing BABEL. Particularly, Without pre-trained modality tower, training wouldn't converge due to limited samples. On UTD-MHAD [11], without prototype network, the previously aligned modality would drop about averagely 44.7% relatively after introducing a new modality."}, {"title": "8.1.6 System Overhead", "content": "The pre-trained hexa-modal BABEL takes around 1.1GB on the disk, including pre-trained encoders, concept alignment modules and the prototype network. BABEL takes 1.4-9.92GB memory, depending on the selected modalities, using the FP32 precision. We evaluate the BABEL's inference latency on one Nvidia A100 GPU. The exact inference latency depends on the selected modalities. For instance, per each sample, i.e., a sequence of sensing data spanning 3-4 seconds, BABEL takes 98.6ms for IMU modality, 206.7ms for LiDAR, For skeleton, WiFi, and mmWave, BABEL only takes around 130ms. When fusing modalities, modality towers could be executed in parallel. BABEL takes 265ms for Wi-Fi and video, 138ms for IMU and skeleton together."}, {"title": "8.2 Case Study", "content": "8.2.1 Cross-modality retrieval\nThe alignment of diverse sensing modalities in BABEL potentially opens up the possibilities for cross-modality retrieval applications. This involves obtaining the representations of one modality using signals from other modalities as inputs. Such applications could be promising. For instance, using wireless sensing signals as input to retrieve visual representations could be considered an example of sensing imaging.\nTo showcase, we construct a prototype designed to retrieve visual representations and generate images using non-visual sensors, such as IMU. Specifically, we align BABEL with unCLIP [38], an image-to-image diffusion model. un-CLIP employs an image encoder to obtain the embeddings of the input image and then uses these embeddings to guide the diffusion process, thereby generating images that bear"}, {"title": "8.2.2 Bridge with LLMs", "content": "The alignment of diverse sensing modalities into a unified representation presents an advantageous prospect for integration with LLMs. To demonstrate, we integrate BABEL with Video-LLaMA [63], which is a multi-modal LLM with the ability to understand both visual and audio contents.\nWe establish the alignment between the video modality in BABEL and that in Video-LLaMA. Specifically, we judiciously select the video encoder from Video-LLaMA and construct a modality tower for integration into BABEL. We employ the L1 loss in this scenario, ensuring the video encoder of Video-LLaMA remains frozen while all modalities in BABEL align towards Video-LLaMA. This strategy aims to generate embeddings of sensing modalities that could potentially be interpreted by Video-LLaMA.\nFig. 9 provides an impressive illustration where we input an IMU sequence depicting a woman waving her hands. These IMU readings are processed by BABEL and subsequently fed into Video-LLaMA. Remarkably, without any specific training on LLMs, it successfully deciphers the action captured by the IMU data and, when promoted, differentiates between diverse actions, such as squatting or waving hands. This exemplifies the potential of bridging sensing and LLMs via the modality alignment introduced by BABEL. Our future research will concentrate on improving BABEL, aiming to bolster the model's capability to provide deeper insights and more accurate interpretations of physical world based on a"}, {"title": "9 RELATED WORK", "content": "Multi-modal Sensing. The development of multi-modal sensing networks is an emerging research area. Recently Cosmo [34] pioneered the application of contrastive fusion learning in multi-modal sensing, incorporating RGB, depth and IMU modalities. MESEN [55] employs multi-modal contrastive learning to improve the performance of singular-modal sensing. In contrast to Cosmo and MESEN, BABEL, to the best of our knowledge, is the first expandable pre-trained network for multi-modal sensing, which seamlessly facilitates the alignment of additional modalities from diverse, scattered datasets. The concept of multi-modal sensing is extensively employed across a wide array of applications. For instance, [28] integrates RFID and RGB for recognizing human-object interactions. [48] leverages LiDARs, cameras, and IMU and GNSS devices worn by animals to recognize animal behavior. To locate target individuals, [26] utilizes Wi-Fi Fine Timing Measurements and IMU data to associate individuals in a video with a matched query ID. GaitVibe+ [14] enhances structural vibration-based footstep localization using temporary cameras and vibration sensors for in-home gait analysis. [17] presents an acoustic and camera sensing system that ameliorates range estimation for applications in robotics and other domains. These applications could potentially benefit through BABEL.\nModality Alignment and MLLMs. Contrastive Learning (CL) is widely used for modality alignment, particularly in visual and linguistic modalities [36, 40, 62]. Despite the prevalence of CL, its implementation in multi-modal sensing introduces significant challenges. To overcome data scarcity, we have introduced several techniques in BABEL, which are essential for aligning multiple sensing modalities. There is also a growing trend of aligning a broader range of modalities. For example, Meta-Transformer [64] uses a unified frozen visual"}, {"title": "10 CONCLUSION AND FUTURE WORK", "content": "We present BABEL, a expandable modality alignment framework designed for sensing applications. The pre-trained BABEL has been proficiently aligned with six prevalent sensing modalities, IMU, skeleton, video, Wi-Fi, LiDAR, and mmWave. BABEL demonstrated the superior performance for HAR tasks across various datasets compared to an array of baselines. As BABEL is a scalable network, we call for the community to further enhance and align additional helpful modalities into BABEL."}]}