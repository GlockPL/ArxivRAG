{"title": "CRUISE on Quantum Computing for Feature Selection in Recommender Systems", "authors": ["Jiayang Niu", "Jie Li", "Ke Deng", "Yongli Ren"], "abstract": "Using Quantum Computers to solve problems in Recommender Systems that classical computers cannot address\nis a worthwhile research topic. In this paper, we use Quantum Annealers to address the feature selection problem\nin recommendation algorithms. This feature selection problem is a Quadratic Unconstrained Binary Optimization\n(QUBO) problem. By incorporating Counterfactual Analysis, we significantly improve the performance of the\nitem-based KNN recommendation algorithm compared to using pure Mutual Information. Extensive experiments\nhave demonstrated that the use of Counterfactual Analysis holds great promise for addressing such problems.", "sections": [{"title": "1. Introduction", "content": "Collaborative filtering technology [1, 2], which predicts potential user-item interactions based on the\npatterns of user behavior and item characteristics, is widely applied in recommendation algorithms,\nSome well-known techniques in this field include matrix factorization methods [3], neighborhood-based\nmethods [4], deep learning approaches [5, 6], graph-based techniques [7, 8], factorization machines [9],\nhybrid methods [10], Bayesian methods [11], and large language models (LLMs) [12]. However, collabo-\nrative filtering technology [1] heavily relies on the quality of data. For instance, using user profiles,\nitem features, reviews, images, and other information can significantly improve the performance of\nrecommendation algorithms, but in some cases, it can also decrease their performance. Therefore, it's\ncritical to distinguish what information are useful for recommendations so as to help the the construc-\ntion of efficient systems and reduction of energy consumption [13, 14, 15, 16]. Quantum computers,\nwith its use of qubits and quantum effects like superposition, entanglement, and quantum tunneling, is\nan effective tool for identifying useful information from redundant data [17]. It significantly enhances\nthe processing speed of search problems and large integer factorization [18]. Therefore, in this paper,\nwe aim to find useful features for recommendations by leveraging quantum computing techniques. Our\ngoal is to improve the efficiency and accuracy of recommendation systems by identifying and utilizing\nrelevant data, thereby reducing computational requirements and energy consumption [18, 19, 20].\nIn QuantumCLEF 2024, we focus on Task 1B, where 150 and 500 features are provided for each item,\nrespectively [21, 22]. We will analyze these features to extract the most relevant ones for recommender\nsystems. The task requires participants to use Quantum Annealing and Simulated Annealing to select\nappropriate features from the given data for an Item-Based KNN recommendation algorithm (Item-\nKNN). The organizers provided an example of feature selection by using Mutual Information [18].\nHowever, our preliminary experiments showed that using only Mutual Information for feature selection\nresulted in limited improvement in the performance of Item-KNN compared to using all features without\nany selection. This is because Mutual Information only reflects the mutual relationship between two\nvariables and is not associated with the final goal of the recommendation algorithm. Therefore, to"}, {"title": "2. Related Work", "content": "In recent years, the rapid development of Quantum Computers has demonstrated their tremendous poten-\ntial in solving problems that Classical Computer cannot address, such as NP and NP-hard problems [25].\nBased on their functionality and application scenarios, Quantum Computers can be categorized into\nUniversal Quantum Computers, Quantum Annealers, Quantum Machine Learning Accelerators, and\nothers [26]. Recent studies have utilized Quantum Annealers for feature selection to enhance the perfor-\nmance of recommendation systems or retrieval systems [27, 28, 18]. Nembrini et al. [27] attempted to\napply Quantum Computers to recommendation systems by using Quantum Annealing to solve a hybrid\nfeature selection approach. Their work demonstrates that current Quantum Computers are already\ncapable of addressing real-world recommendation system problems. Nikitin et.al. [28] reproduced\nNembrini's work and employed Tensor Train-based Optimization (TTOpt) as an optimizer for the cold\nstart problem in recommendation systems. MIQUBO [18] discussed the problem of feature selection\nusing Quantum Computers and formalizes it as a Quadratic Unconstrained Binary Optimization (QUBO)\nproblem. It demonstrates the potential of Quantum Computers to solve ranking and classification\nproblems more efficiently."}, {"title": "2.1. Quantum Computers", "content": "In recent years, the rapid development of Quantum Computers has demonstrated their tremendous poten-\ntial in solving problems that Classical Computer cannot address, such as NP and NP-hard problems [25].\nBased on their functionality and application scenarios, Quantum Computers can be categorized into\nUniversal Quantum Computers, Quantum Annealers, Quantum Machine Learning Accelerators, and\nothers [26]. Recent studies have utilized Quantum Annealers for feature selection to enhance the perfor-\nmance of recommendation systems or retrieval systems [27, 28, 18]. Nembrini et al. [27] attempted to\napply Quantum Computers to recommendation systems by using Quantum Annealing to solve a hybrid\nfeature selection approach. Their work demonstrates that current Quantum Computers are already\ncapable of addressing real-world recommendation system problems. Nikitin et.al. [28] reproduced\nNembrini's work and employed Tensor Train-based Optimization (TTOpt) as an optimizer for the cold\nstart problem in recommendation systems. MIQUBO [18] discussed the problem of feature selection\nusing Quantum Computers and formalizes it as a Quadratic Unconstrained Binary Optimization (QUBO)\nproblem. It demonstrates the potential of Quantum Computers to solve ranking and classification\nproblems more efficiently."}, {"title": "2.2. Counterfactual Analysis", "content": "Existing deep learning models have complex decision-making processes that are difficult for people to\nunderstand, often functioning as black-box models, Counterfactual Analysis is a highly effective method\nfor helping people understand these complex models and robust them [29]. For example, CF2 [30]\nused Counterfactual Analysis to explore the explanations of Graph Neural Networks. In recommender\nsystems, Counterfactual Analysis is primarily used for explainability and to combat data sparsity.\nACCENT [31] was the first to apply Counterfactual Analysis to neural network-based recommendation\nalgorithms. CountER [32] utilizes Counterfactual Analysis to construct a low-complexity, high-strength"}, {"title": "3. Methodology", "content": "In this work, we follow the approach described in [18], which utilizes Quantum Annealing for feature\nselection. To apply these methods, the feature selection problem is formulated as a Quadratic Uncon-\nstrained Binary Optimization (QUBO) problem. The QUBO formulation can be used to solve certain NP\nand NP-hard optimization problems and is defined as follows [18]:\nmin Y = xTQx,\nwhere x is a binary vector of length m, with each element of the vector being either 0 or 1. Qis\na symmetric matrix, where each element represents the relationship between the elements of x. m\ndenotes the number of features to be selected. In other words, the elements of vector x indicate whether\nthe corresponding features are selected, and the elements in Q influence the search direction of the\nfunction, determining feature selection."}, {"title": "3.1. Preliminary", "content": "In this work, we follow the approach described in [18], which utilizes Quantum Annealing for feature\nselection. To apply these methods, the feature selection problem is formulated as a Quadratic Uncon-\nstrained Binary Optimization (QUBO) problem. The QUBO formulation can be used to solve certain NP\nand NP-hard optimization problems and is defined as follows [18]:\nmin Y = xTQx,\nwhere x is a binary vector of length m, with each element of the vector being either 0 or 1. Qis\na symmetric matrix, where each element represents the relationship between the elements of x. m\ndenotes the number of features to be selected. In other words, the elements of vector x indicate whether\nthe corresponding features are selected, and the elements in Q influence the search direction of the\nfunction, determining feature selection."}, {"title": "3.1.1. QUBO Formulation", "content": "In this work, we follow the approach described in [18], which utilizes Quantum Annealing for feature\nselection. To apply these methods, the feature selection problem is formulated as a Quadratic Uncon-\nstrained Binary Optimization (QUBO) problem. The QUBO formulation can be used to solve certain NP\nand NP-hard optimization problems and is defined as follows [18]:\nmin Y = xTQx,\nwhere x is a binary vector of length m, with each element of the vector being either 0 or 1. Qis\na symmetric matrix, where each element represents the relationship between the elements of x. m\ndenotes the number of features to be selected. In other words, the elements of vector x indicate whether\nthe corresponding features are selected, and the elements in Q influence the search direction of the\nfunction, determining feature selection."}, {"title": "3.1.2. Feature Selection Based on Mutual Information", "content": "Following [18], Mutual Information QUBO (MIQUBO) is a quadratic feature selection model based\non Mutual Information. MIQUBO aims to maximize the Mutual Information, which measures the\ndependency between two variables, and the Conditional Mutual Information, which measures the\ndependency between two variables given a target variable, of the selected features. In this context, the\nmatrix Q in Equation 1 is defined as:\nQij ={\nJ-CMI(fi; y | fj) if i\u2260j\n-MI(fi; y) if i=j\nwhere MI(fi; y) is the Mutual Information between feature fi and target feature y, and CMI(fi; y | fj)\nis the Conditional Mutual Information between feature fi and target feature y given feature fj. Since\nQUBO formulation is used to find the minimum state, a negative sign is required before MI and CMI.\nTo control the number of selected features, a penalty term is added to Equation 1, which is then\ntransformed to:\nmin Y = xTQx + (\u2211i=1NXi-k)2\nThis formula will be minimized when selecting k features, this also following the descriptions in [18]."}, {"title": "3.2. Counterfactual Analysis", "content": "To better identify features directly associated with recommendation performance, we integrate a widely\nused recommendation ranking metric into Mutual Information through Counterfactual Analysis."}, {"title": "3.2.1. Counterfactual Analysis for Feature Selection", "content": "Counterfactual Analysis [23] is usually used to examine the causal relationship between conditions,\ndecisions, and outcomes by hypothesizing how the results of observed events would change if the\nconditions and decisions were altered. In the field of Recommender System, Counterfactual Analysis is\noften used for the interpretability of recommendation models, helping researchers enhance algorithm\nperformance [32, 33]. Inspired by existing works [32, 33], the impact of item features can be explored\nby excluding the corresponding feature and analyzing the difference in recommendation performance\nbetween the recommendation lists generated by the model with and without the corresponding feature.\nIn this work, we use the widely used Item-KNN recommendation algorithm, termed as model G, and\nemploy the recommendation performance metric Normalized Discounted Cumulative Gain (nDCG) [24]\nfor Counterfactual Analysis. nDCG is defined as:\nEi = nDCGG(F) - nDCGG(F\\fi),\nwhere Ei represents the change in the nDCG result of the recommendation model G after removing\nthe feature fi. nDCGG(F) represents the nDCG@10 value obtained by the G using all item features set\nF, while nDCGG(F\\f\u2081) represents the nDCG@10 value obtained by the G using features set which is set\nF removing feature i. It is important to note that E\u00bf ultimately reflects the impact of feature i on the\nresult. Since the final outcome is influenced by the interactions between all features, simply removing\nfeatures with positive Ei values does not yield the optimal feature selection solution.\nWhen E\u00bf \u2265 0, it indicates that the algorithm's performance decreases after removing the feature i.\nThe extent of this decrease reflects the positive impact of this feature on the algorithm. Conversely, an\nincrease in the value reflects the negative impact of this feature on the algorithm. We hypothesize that if\nthe selected set of features is set(F*), the maximization the sum of Ei (i \u2208 set(F*)), the maximization\nthe performance improvement of the baseline algorithm. Since the QUBO problem is a minimization\noptimization problem, we redefine Q as follows:\nQij ={\nJ-CMI(fi; y | fj) if i\u2260j\n\u2212MI(fi; y) \u2013 \u03bbEi if i=j\nwhere A is a coefficient used to control the influence of E on the search results. The larger the value of\n\u5165, the greater the influence of E on the final results. The overall process of the above algorithm, which\nwe refer to as Counterfactual Analysis QUBO (CAQUBO), is as follows in Algorithm 1."}, {"title": "3.3. Handling Large Feature Set", "content": "Although Quantum Computers are developing rapidly, the limitation in the number of qubits restricts\nthem to handling only a limited number of feature selection problems. For selecting from 500 features,\nwe partition them into several subsets and use Quantum Annealing (QA) or Simulated Annealing (SA)\nto perform feature selection on these subsets individually, then combine the results.\nFirst, partition the 500 features into n subsets by order, S1, S2, \u2026\u2026\u2026, Si, \u2026\u2026\u2026, Sn, where Si is the i-th\nsubset of features, and n is the number of subsets.\nS1, S2,, Si,\u2026\u2026\u2026, Sn = divide(F)\nThen, use Quantum Annealing (QA) or Simulated Annealing (SA) to perform feature selection on each\nsubset, and combine the results:\nS = \u222ai=1nQA/SA(Si),\nwhere S is the final selected features set, represents each partitioned subset of features, and QA/SA\n(S_i) represents the selected features from subset Si using QA and SA. The final feature set is obtained\nby merging the selected features from all subsets."}, {"title": "4. Experimental Setup", "content": "Datasets: In this work, two tasks are undertaken: the first involves selecting appropriate features from\na set of 150 item features for training G, and the second involves selecting features from a set of 500\nitem features. Three data sets are provided for these tasks: 150_ICM, 500_ICM, and URM. The 150_ICM\nand 500_ICM contain item features, while the URM includes interaction data between 1,890 users and\n18,022 interacted items.\nExperimental parameter setting: We used a self-implemented Item-KNN recommendation model\nbased on the problem statement to calculate E. The interaction data was split into training and test sets\nin an 80:20 ratio. It is worth noting that calculating E is very time-consuming, so we only used a subset\nof items for the calculations. In the use of Quantum Annealing (QA) and Simulated Annealing(SA), the\ncoefficient A significantly affects the features selected by QA and SA. Due to the limited usage time of\nthe Quantum Annealer (QA), it is necessary to use Simulated Annealing (SA) to explore the effectiveness\nof the selected features under different parameters A and k before using QA. In preliminary experiment,\nwe attempt [\u03bb: 0, 1e1, 1e3, 1e5, 1e7], [k: 50, 100, 130, 140, 145] in Feature 150 and [\u5165: 0, 1e1,\n1e3, 1e5, 1e7], [k: 300, 350, 400, 450, 470] in Feature 500. For the selection of 500 features, n (is\nmentioned in Section 3.3) is set to 5. The preliminary experiment results can be found in Table 1.\nRepeated Calculations: Due to the heuristic nature of Simulated Annealing (SA) and Quantum\nAnnealing (QA), the final results may vary even with fixed parameters. To mitigate this effect, we\nperform multiple iterations of QA and SA under the same parameters and select the final feature set via\nvoting. For example, we repeated the experiment five times. fi was not included in F* in any of the\nfive experiments, while fj was included in F* in four out of the five experiments. Therefore, the final\nsubmitted feature set F* does not include fi but includes fj."}, {"title": "5. Results", "content": "Table 1 describes the performance in nDCG@10 of G using features selected by QA and SA under\ndifferent parameters A and k. When x = 0, QA and SA select features based solely on Mutual Information\n(MI) and Conditional Mutual Information (CMI). Across different values of parameter k, the performance\nof selected features in G rarely surpasses the performance in Counterfactual Analysis QUBO. As the\nparameter A increases, the performance of the features selected by QA and SA in the item-KNN shows\nsignificant improvement compared to using all features. The effectiveness of feature selection shows no\nsignificant improvement when > > 1e5 . This may be because as the value of A increases, the impact\nof MI and CMI on feature selection diminishes, causing QA and SA to rely entirely on E for feature\nselection.\nTable 2 reflects the same situation: feature selection relying solely on MI and CMI does not surpass the\nperformance in Counterfactual Analysis QUBO. After incorporating the counterfactual analysis-derived\nE into Q, the features selected by QA and SA show a significant performance improvement in item-KNN"}, {"title": "6. Conclusions and Future Work", "content": "In this paper, we present the explorations conducted by our team and the details of our final submission\nfor the QuantumCLEF 2024 activities. We used Counterfactual Analysis of individual item features to\nselect appropriate features for item-KNN using Quantum Annealing. Our preliminary experiments\nand the results returned by QuantumCLEF 2024 demonstrated that our use of Counterfactual Analysis\nsignificantly improved the performance of item-KNN.\nWithin the limited time of QuantumCLEF, we attempted Counterfactual Analysis of individual\nfeatures. However, because the performance of collaborative filtering is actually the result of feature\ninteractions, Counterfactual Analysis of individual features has significant limitations. Additionally,\nsince Quantum Annealing cannot directly handle the selection of 500 features, we adopted a sequential\npartitioning and merging approach. As negative features are not uniformly distributed by their indices\namong all features, this sequential partitioning and merging method still requires improvement."}]}