{"title": "SYNTHESIZING EEG SIGNALS FROM EVENT-RELATED POTENTIAL PARADIGMS WITH CONDITIONAL DIFFUSION MODELS", "authors": ["Guido Klein", "Pierre Guetschel", "Gianluigi Silvestri", "Michael Tangermann"], "abstract": "Data scarcity in the brain-computer interface field can be alleviated through the use of generative models, specifically diffusion models. While diffusion models have previously been successfully applied to electroencephalogram (EEG) data, existing models lack flexibility w.r.t. sampling or require alternative representations of the EEG data. To overcome these limitations, we introduce a novel approach to conditional diffusion models that utilizes classifier-free guidance to directly generate subject-, session-, and class-specific EEG data. In addition to commonly used metrics, domain-specific metrics are employed to evaluate the specificity of the generated samples. The results indicate that the proposed model can generate EEG data that resembles real data for each subject, session, and class.", "sections": [{"title": "INTRODUCTION", "content": "One of the most significant challenges of data scarcity in the brain-computer interface (BCI) field is that the acquisition of annotated data is a time-intensive endeavor. The lack of large labeled datasets can be a bottleneck for many machine learning algorithms [1]. Additionally, class imbalances typically found in event-related potential (ERP) protocols which are among the most commonly used EEG-BCI paradigms [2], can be detrimental to classifier performance. Moreover, multiple populations are underrepresented in the current corpus of EEG data [1, 3].\nGenerative models offer a promising solution to alleviate this data scarcity. Diffusion models, in particular, have shown the ability to generate high-quality data in a variety of domains, including images [4] and audio [5]. Current implementations of diffusion models for EEG data generation are either trained directly on EEG data or use an alternative representation, such as electrode frequency density maps [6], spatial covariance matrices [7], time-frequency maps, and latent representations [8-10].\nModels trained on alternative representations, while potentially being easier to train, require an additional pre- and post-processing step, which can hinder their usability. The models trained directly on EEG data are either unconditioned [11, 12], which means that the samples are always generated from the full data distribution, or conditioned, which means that the models are trained on the full data distribution but samples can be generated from a selected part of the data distribution. This conditioning can either be achieved using a classifier [4, 13] or by classifier-free guidance [14, 15], which achieves conditioning without the need for a noisy classifier.\nDespite the capability of diffusion models to generate high-quality EEG data, there is a lack of proper metrics to quantify the quality of the generated samples. Currently used metrics are either adopted from the image domain, are domain-invariant, or rely on classifier performance [6-11, 13, 15\u201317]. The metrics from the image domain, the Fr\u00e9chet inception distance (FID) [4] and the inception score (IS) [4], rely on the activations and output of a standardized trained neural network called Inception V3 [4]. Unfortunately, there is no universally adopted trained network for EEG data, which makes fair and reliable comparison impossible. Additionally, the domain-invariant metrics are incapable of discerning which domain-relevant features are generated well by the model. For example, if there is a low Euclidean distance between the generated and real samples, then that is likely due to a similarity in multiple domain-relevant features, such as amplitude and peak latencies, this makes it unclear which domain-specific features are properly generated. Similarly, metrics based on classifier performance are also unable to disentangle these features. Hence, there is a need for a set of metrics that can capture these domain-specific features.\nResearch questions and objectives.\nThe following research question is investigated: Can we generate artificial ERP examples that are specific to a subject, session, and class using conditional diffusion models with classifier-free guidance?\nTo answer this question, we train a novel conditional diffusion model to flexibly generate each combination of conditions, i.e., subject, session, and class. An example of generated data can be found in Figure 1. Domain-invariant and image-domain metrics are used to evaluate the quality of generated samples during training. However, as previously noted, these metrics are unable to capture domain-specific features, which makes it impossible."}, {"title": "DATA DESCRIPTION", "content": "The conditional diffusion model is trained on a visual ERP dataset collected by Lee and colleagues [2]. Visual ERP responses are elicited using a modified odd-ball paradigm. The most prominent ERP feature is expected to be a positive deflection that occurs approximately 300 ms (referred to as a P300) after being presented with a relatively infrequent target stimulus following multiple non-target stimuli [18]. This dataset is one of three datasets that were recorded to study BCI-inefficiency across three major BCI paradigms: visual ERP, motor imagery, and steady-state visually evoked potential protocols [2]. In the study, fifty-four participants underwent two sessions which were held on different days [2].\nDuring data recording, each visual ERP session was divided into a train and test run. During the train run, trials were not decoded, while during the test run, the trials were decoded and feedback was given to the participant after each trial [2]. Both runs of the same session are combined to train the diffusion model, as they employ the same copy-spelling tasks.\nThe dataset has been obtained and preprocessed using the Mother of All BCI Benchmarks [19]. It was preprocessed with a relatively simple pipeline. First, 19 channels (Fp1, Fp2, F7, F8, F3, F4, Fz, T7, T8, C3, C4, Cz, P7, P8, P3, P4, Pz, O1, and O2) were selected that provide full scalp coverage. Secondly, the data was bandpass filtered between 1 and 40 Hz with a 4th-order Butterworth filter. Thirdly, the data was downsampled from 1000 Hz to 128 Hz. Fourthly, epochs were constructed as 1-second windows, starting from the stimuli onsets. Lastly, peak-to-peak epoch rejection is applied with a threshold of 150 \u00b5V. This removed 14.5% of epochs, equivalent to 63672 epochs out of a total of 438840 epochs. Subject 17 was dropped due to excessive artifacts."}, {"title": "DIFFUSION MODELS", "content": "Diffusion models are a generative modeling paradigm where data is progressively destroyed by injecting Gaussian noise, and a neural network is trained to reverse this process [14]. Song and colleagues provide a continuous formulation of such a process, formulated as a stochastic differential equation (SDE), and show how a neural network can be implemented to learn the reverse SDE [20]. In this work, the implementation is based on the variance persevering SDE (VPSDE), which is the continuous equivalent to the noise injection used in the denoising diffusion probabilistic model (DDPM) [20, 21]. Furthermore, \u201cclassifier-free guidance\" is used to condition the model on the subject, session, and class in parallel [14].\nImplementation details: The neural network to reverse the destruction of the data is based on the architectures introduced by Torma et al. and Shu et al., called EEG-Wave and diff-EEG [11, 15]. Two key differences were introduced: 1) the timestep embedding was rewritten to be compatible with the VP SDE, because both models use DDPM noise injection, and 2) no normalization is applied to the EEG data.\nThe model is trained for 900 k steps, with the model being evaluated every 100k steps. An exponential moving average of the weights is used for sampling and metric calculation. This sampling is done by a predictor-corrector sampler [20]. Preliminary results indicated that an increase in the signal-to-noise ratio (SNR) of the corrector increases the amplitude of the generated EEG data. The number of generated samples matches the number of real samples available for a given subject/session/class combination. This includes the samples used to compute the validation loss but excludes samples removed by the epoch rejection.\nFor more information about the implementation please refer to: https://neurotechlab.socsci.ru.nl/ resources/generative_models/"}, {"title": "SIMILARITY METRICS", "content": "Metrics quantifying the similarity between generated and real data are crucial for model comparison and evaluation. These similarity metrics are divided into four categories: classifier performance, domain-invariant, image-domain, and domain-specific. This section will also discuss metric-specific baselines for interpreting the scores obtained on the domain-invariant and domain-specific metrics.\nDomain-invariant and domain-specific metrics are computed between the real and generated data within one condition, i.e., a combination of subject, session, and class. These metrics have been computed separately per condition, but their average across conditions is reported.\nClassifier performance: We compare the performance of a classifier trained on generated data with the performance of a classifier trained on generated data. Both conditions use the same test sets which contain only real data. This metric is denoted as the averaged balanced accuracy (ABA) and the score obtained when training on the real data is reported as the within-session baseline. Specifically, training is subject-specific and implements a within-session five-fold stratified cross-validation. The classifier is a regularized least-squares Linear Discriminant Analysis (LDA) [22]. The LDA is trained on features that represent the average amplitude across channels within non-overlapping time windows, which span between 0.1 to 0.9 seconds and are each 0.1 seconds long.\nDomain-invariant metrics, such as the sliced-Wasserstein distance (SWD), mean squared error, and Jenson-Shannon divergence, can be used to measure"}, {"title": "DISCUSSION", "content": "In this work, we aimed to create a conditional diffusion model using classifier-free guidance, that does not lose specificity during sampling. The results indicate that the model can indeed create subject-, session-, and class-specific ERP data that is quite similar to the real data of the Lee 2019 ERP dataset.\nAmplitude, latency, and diversity are well-modeled:\nThe classifier performance on the real and generated data is highly similar, even for the worst subject. Given that the features on which the classifier is trained are based on amplitude and latency, it is no surprise that the PAD and PLD of the generated data are better than the between-session variability baseline. Furthermore, the diversity, as measured by the SD-MD, also seems to be modeled reasonably well, as it can outperform the between-session variability baseline.\nLimitation of the PLD: The PLD should be interpreted with caution because it is unreliable when there are mul-\ntiple peaks of similar height in the data. This can be addressed by either 1) only computing the PLD when there is only one prominent peak or 2) by computing the peaks of multiple subsets (i.e. 80%) of the real data and only using the lowest PLD.\nPotential applications: There are a variety of potential applications for the model presented here. For example, it can be used to alleviate the class imbalance by sampling from the target class. In addition, it can generate additional data to train a classifier, potentially slightly improving the robustness and accuracy of the model. Furthermore, the trained weights of the model can be used for transfer learning, which would allow fine-tuning on a different ERP dataset. Lastly, enlarging datasets with the proposed model may be specifically valuable for the benchmarking of novel algorithms. The diffusion model, however, can not be expected to deliver samples from outside the distribution of the training data.\nDataset limitations and solutions: All in all, the ability of the generated data to achieve comparable performance on most metrics to real data, and the high visual similarity of the covariance matrices and ERPs, are promising results for conditional diffusion models that generate EEG data directly. Nonetheless, it should be noted that the model was trained on a rather large dataset, so it remains to be seen how well the results translate to training on smaller datasets. However, the ability to generate data from specific conditions during sampling, while being trained on a full dataset, might make conditioned models more data-efficient compared to unconditioned models.\nConclusion: In this work, we introduce the first diffusion model that is conditioned on subject, session, and classes using classifier-free guidance, that can generate high-quality EEG data for each condition. This enables training on complete datasets, without losing specificity during sampling. Additionally, we introduce multiple domain-specific metrics that can assist in model evaluation and fine-tuning. This conditional diffusion model can now be used to generate high-quality data for all subjects, sessions, and classes present in the Lee ERP dataset."}]}