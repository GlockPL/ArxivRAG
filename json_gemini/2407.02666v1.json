{"title": "Commonsense Reasoning for Legged Robot\nAdaptation with Vision-Language Models", "authors": ["Annie S. Chen", "Alec Lessing", "Andy Tang", "Govind Chada", "Laura Smith", "Sergey Levine", "Chelsea Finn"], "abstract": "Legged robots are physically capable of navigating a diverse variety\nof environments and overcoming a wide range of obstructions. For example,\nin a search and rescue mission, a legged robot could climb over debris, crawl\nthrough gaps, and navigate out of dead ends. However, the robot's controller\nneeds to respond intelligently to such varied obstacles, and this requires handling\nunexpected and unusual scenarios successfully. This presents an open challenge to\ncurrent learning methods, which often struggle with generalization to the long tail\nof unexpected situations without heavy human supervision. To address this issue,\nwe investigate how to leverage the broad knowledge about the structure of the world\nand commonsense reasoning capabilities of vision-language models (VLMs) to aid\nlegged robots in handling difficult, ambiguous situations. We propose a system,\nVLM-Predictive Control (VLM-PC), combining two key components that we find\nto be crucial for eliciting on-the-fly, adaptive behavior selection with VLMs: (1)\nin-context adaptation over previous robot interactions and (2) planning multiple\nskills into the future and replanning. We evaluate VLM-PC on several challenging\nreal-world obstacle courses, involving dead ends and climbing and crawling, on a\nGol quadruped robot. Our experiments show that by reasoning over the history of\ninteractions and future plans, VLMs enable the robot to autonomously perceive,\nnavigate, and act in a wide range of complex scenarios that would otherwise require\nenvironment-specific engineering or human guidance.", "sections": [{"title": "1 Introduction", "content": "Robots deployed in open-world environments must be able to handle highly unstructured and\ncomplicated environments. This is particularly the case for legged robots, which may need to\noperate in an extremely diverse range of circumstances. Consider a quadruped robot tasked with\nperforming search and rescue in a collapsed building. This robot faces a long tail of different possible\nenvironments and obstacles, which might require climbing over debris, crawling through gaps, and\nbacktracking and navigating out of dead ends without a map. Handling these diverse real-world\nscenarios autonomously, without detailed human guidance and specific skill directives, remains a\nsignificant challenge. Prior work in locomotion has endowed legged robots with agile skills like\nrunning, climbing, and crawling [1, 2], but possessing these skills alone does not solve the problem\nof fully autonomous deployment. To handle complex, unstructured scenarios, a robot must be able to\ndecide how to deploy its repertoire of skills with a nuanced understanding of its situation. Consider\nthe example of clearing a novel obstacle like debris in a collapsed building. We expect an intelligent\nrobot to perceive and try a skill that is likely to succeed, e.g., climbing. If the robot's attempt was\nunsuccessful, e.g. the debris is too slippery to climb over, the robot should recognize this and try"}, {"title": "2 Related Work", "content": "Our work tackles the issue of enabling legged robots to perform robustly in unstructured, unknown\ntest-time conditions. Traditional model-based control approaches have achieved impressive agile\nlocomotion [12, 1, 13, 14, 15, 16, 17] but are not well-equipped to navigate arbitrary, open-world\nenvironments. Learning-based approaches hold the promise of greater generalization capabilities,\nand training a single policy with reinforcement learning (RL) has also demonstrated successful\nlow-level locomotion capabilities from robust walking to jumping and bipedal walking [18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]. Behind a majority of these successes is the use of domain\nrandomization [32, 33, 34, 35, 36, 19, 21, 37, 38, 39, 40], which involves training the robot under a\nvariety of different dynamics to robustify the policy. Our work tackles an orthogonal, complementary\nproblem: enabling legged robots to autonomously solve complex, partially observed tasks given a\nrepertoire of low-level skills (which can be acquired through either traditional model-based approaches\nor RL training). Using these skills to solve a long-horizon task requires understanding the scene and\nreasoning over the information gathered in the environment, trying different low-level strategies, and\nadapting high-level plans on-the-fly accordingly.\nPrior work has also explored utilizing a repertoire of skills to help legged robots navigate that require\na combination of distinct behaviors. For example, Margolis and Agrawal [41] train a policy that uses\nhuman input via remote control to select skills, while others have explored using learned models to\nchoose appropriate behaviors on-the-fly, e.g., using search in latent space [21, 25, 42], direct inference\nusing proprioceptive history [22, 43, 44], prediction based on egocentric depth [45, 24, 30, 46], or\nusing value functions [47]. These works rely on human supervision or domain-specific information\nrequired to train model-based behavior selection. In contrast, our approach represents the robot's\nrange of skills in language and studies how to leverage this representation with pre-trained VLMs\nusing in-context reasoning to adapt on-the-fly in complex scenarios.\nOutside legged locomotion, extensive research has explored combining prior behaviors to address\nlong-horizon tasks, often by training high-level policies that orchestrate learned skills into complex\nbehaviors [48, 49, 50, 51, 52, 53, 54, 55, 56, 57]. Natural language provides a simple abstraction to\nindex these behaviors, and using language as an abstraction for behaviors provides an interpretable\nspace for a high-level planner to select strategies to try [3, 4, 58, 59, 5, 60, 61, 62] or to generate\nrobot code [63, 64]. In-context reasoning with LLMs has refined low-level behaviors [65, 66, 67, 68],\nimproved planning with feedback [58] and facilitated learning from human feedback [69, 70], but\nthese do not incorporate VLMs, which can offer rich multimodal understanding. Recent works have\nbegun going beyond LLMs and incorporating VLMs for manipulation [71, 72, 73] and navigation [74,\n75]. Unlike these works, we focus particularly on equipping the robot to handle unpredictable\nsituations where it might get stuck and need to explore different strategies to make progress. Enabling\nthis in a diverse array of environments requires robust commonsense reasoning abilities, and we study\nthe extent to which VLMs can provide these for legged robots.\nWhile high-level planning in language grounding has been studied for manipulation or navigation\ntasks, it has explored far less for legged locomotion. Key works have interfaced through foot contact\npatterns [76] or code [77] with LLM planning. Our work implements a straightforward language-skill\ninterface for locomotion and is the first to explore how legged robots can utilize the commonsense\nreasoning capabilities of pre-trained VLMs to autonomously guide adaptive behavior selection. In\nparticular, the vast majority of prior works apply LLMs zero-shot based on the current instruction or\nobservation; in this work, we study how the in-context adaptation ability of VLMs can help robots\nadapt to different scenarios."}, {"title": "3 Problem Statement", "content": "We assume the robot has access to a set of n skills, which are sufficient to allow the robot to traverse\nthe environment. Given the recent development of highly robust low-level quadrupedal locomotion"}, {"title": "4 Vision-Language Model Predictive Control (VLM-PC)", "content": "Our goal is to enable legged robots to make informed decisions that lead to effective, context-aware\nadaptation to help navigate these situations autonomously and successfully. Our central hypothesis is\nthat many real-world situations demand complex reasoning due to unexpected circumstances that\nmay be difficult to generalize to. In this section, we first describe how we represent the robot's skills\nvia language to be used then by a pre-trained VLM. We then detail how we prompt the VLM to\nreason through the robot's current state and history of interactions to select the next skill to execute to\nsolve a task in unstructured environments."}, {"title": "4.1 Interfacing Robotic Locomotion Skills with VLMS", "content": "We consider generative VLMs, also known as multimodal language models, which take as input\n{I, x}, including images {I} and prompt text x and outputs text y from a distribution over textual\ncompletions P(.\u00b7|{I}, x). We label each of the robot's prior behaviors $\\pi_i \\in \\Pi$ with a command $l_i$, a\ntextual description of the corresponding behavior. We also define levels of magnitude m for each\nthat define the duration $\\delta_{l_i,m}$ that i should be executed. While there are many ways to acquire\nlocomotion policies, e.g., via traditional model-based techniques or learning-based approaches, we"}, {"title": "4.2 Using VLMs for Adaptive Behavior Selection", "content": "We propose a system, Vision-Language Model Predictive Control (VLM-PC), that uses a VLM to\naccount for these errors and successively refine strategies, so that the robot can autonomously adjust\nfrom strategies that fail and try others. Summarized in Figure 2, VLM-PC combines two key insights\nto effectively enable VLMs to serve as an effective high-level policy: (1) reasoning about information\ngathered by the robot in its environment and (2) selecting actions by planning ahead and iteratively\nreplanning during execution. The VLM we use in all of our experiments is GPT-40. We tuned the\nprompts to take into account the setting of legged locomotion and the limited view from the robot's\ncamera. Full prompts and an example log of the VLM's chats are shown in Appendix A.2.\nUsing in-context reasoning to adapt on-the-fly. In large foundation models, techniques like\nchain-of-thought [6, 7, 79], where the model is prompted to output intermediate reasoning steps, have\nbeen shown to significantly improve the model's ability to perform complex reasoning. We aim to\nleverage such techniques to equip the VLM to better understand and reason through the environment\nand provide more effective high-level commands to the robot. In particular, we want the VLM to\nreason through the history in the environment and the progress made with the commanded skills\nbefore deciding on the next skill, in order to determine if the robot should try a new strategy. As\nsuch, we include as input to the VLM an image representing the robot's current view along with\nthe full history of interactions (including the robot's previous images and the previous outputs of\nthe VLM) and a prompt, i.e. the input at timestep t is $(I_1, X_1, Y_1, I_{t-1}, X_{t-1}, Y_{t-1}, ..., I_{t}, P_{t})$,\nwhich contains for each previous query step i, each previous image $I_i$ and prompt $X_i$ along with\nVLM output $Y_i$. We then prompt the VLM to first reason through what progress the robot has made\nusing the history of commands selected and the current position and orientation of the robot.\nMulti-step planning and execution. Due to partial observability, there is often no clear answer as\nto which skill is most appropriate for a given situation and without physical experience, the VLM is\nnot grounded in the robot's low-level capabilities (i.e. the VLM understands that the robot can crawl,\nbut it does not know exactly how it crawls and whether this crawling skill will actually be successful\nin the current situation). So, we use an approach akin to model predictive control [8, 9, 10, 11],\nwherein we prompt the VLM to produce the immediate skill to execute by planning multiple steps $l_t$,\n$l_{t+1}, ..., l_{t+k}$, into the future and reasoning about the consequences of the actions. This allows the\nVLM to foresee different possible strategies that might be applicable to the current situation, so it\nmay better adjust in the future if the next chosen skill does not make progress. Then after the robot\nexecutes the skill corresponding to $l_t$, the VLM repeats this planning for each step during deployment.\nTo implement this, we specifically prompt the VLM to make a multi-step plan taking into account\nthe latest visual observation $I_t$, compare the new plan to the prior existing plan, and use the one that\nseems more applicable."}, {"title": "5 Experimental Results", "content": "In this section, we study whether VLM-PC can enable a Gol quadruped robot to tackle five challeng-\ning real-world situations in a fully autonomous manner. Concretely, we aim to answer the following\nempirical questions: (1) Can VLM-PC enable the robot to autonomously adapt in unseen, partially"}, {"title": "5.1 Experimental Setup", "content": "We use a Gol quadruped robot from Unitree. The robot is equipped with an Intel Realsense D435\ncamera mounted on its head, which provides an egocentric view of the environment, which is the only\nsource of information the robot has about its surroundings. We configure the default controller to\ncorrespond to a set of prior behaviors: walking forward, crawling (at a low height), climbing (which\ncan overcome stair-height obstacles), walking backward, turning left, and turning right. This same\nset of behaviors is used for all experiments, and details of the skills are in Appendix A.1. In each\nsetting, we report the average and median wall clock time in seconds (where lower is better) needed\nto complete the task along with the success rate across five trials for each method. If the robot does\nnot complete the task within 100 seconds of executing actions, we consider it a failure. For each\nmethod in each setting, we report these metrics across five trials.\nEvaluation Settings. To evaluate each method, we conduct trials in five real-world indoor and\noutdoor settings. The settings test the robot's ability to adapt to varying terrain conditions, requiring\nagile skills and dynamic strategy adjustments based on new information. The goal in each setting is\nto reach the \"red chew toy\". The robot only receives information from its camera and does not have\naccess to a map of the environment. The tasks are shown in Figure 3, annotated with the goal and an\nexample path through the course, and described as follows:\nIndoor 1: The robot first must crawl under a couch, determine that it is a dead end, back up and turn\nto walk around the couch, climb a cushion it cannot pass without climbing, and finally locate the toy.\nIndoor 2: The robot first faces a couch that it must crawl under to the opposite side, then faces several\nstools blocking its path with a narrow gap between them, and must determine that it cannot fit through\nthe gap and must turn and go around to locate the red chew toy.\nOutdoor 1: The robot first faces bushes that it must turn from and go around, then faces a series of\nsmall logs that it must climb over, and finally locates the red chew toy.\nOutdoor 2: The robot first faces a series of bamboo plants that it must turn from and go around, then\na bench that it must crawl under, and then find the red chew toy.\nOutdoor 3: The robot first faces a curb that it must climb over, a dirt hill that it must walk up, a\nwooden plank that it must climb over, and finally locate the red chew toy between the bushes."}, {"title": "5.2 Main Results", "content": "As shown in Figure 4, on average across all five settings, VLM-PC successfully completes the task\n64% of the time, almost 30% more than the second best method (No Multi-Step), which succeeds on\naverage 36% of the time. VLM-PC is also on average over 20% faster at completing the target task\nas the next best method, showing that including both history and multi-step planning are important\nfor improving the use of these VLMs in providing high-level commands in a variety of settings. In\nFigure 3, we find that particularly on Indoor 2, Outdoor 1, and Outdoor 2, VLM-PC is more than\ntwice as successful as the next best method. No Multi-Step is the second best method, and does\ncomparably to VLM-PC (which does multi-step planning) on Indoor 1 and Outdoor 3, indicating\nthat in some situations, multi-step planning does not significantly help, although it does not hurt\nperformance. No History fails in almost every setting except Outdoor 3, as it often gets stuck behind\nobstacles that require trying multiple different strategies. Random fails in every setting, showing that\neach setting requires nontrivial reasoning for the robot to succeed. We provide examples of typical"}, {"title": "5.3 Adding Labeled In-Context Examples Can Improve Performance", "content": "As large foundation models trained on\nInternet-scale data are used as these\nhigh level planners, they can leverage\nin-context learning, where examples\nor instructions are included as context\nin the input to the model [80, 81]. We\nprovide an extension of our method\nincluding in-context examples, called\nVLM-PC+IC, where we include in the\nfirst prompt several additional images,\ntaken from the egocentric view at different points in the environment, as well as a label for each\nof them with the best command to take. This provides the VLM with more context about the\nenvironment and the best strategies to take at key points. As shown in Table 6, we find that in two\nof the obstacle courses, this can significantly improve performance. While inexpensive to obtain,\nthis does require human labeling of several images from the deployment environment with the best\ncommand to take, which may not be feasible in all deployment settings. Nonetheless, this extension\nfurther reinforces the importance of providing useful context to the VLM and having it use this\ncontext to make informed decisions, and shows that these labeled examples can be useful context on\ntop of the history of experiences in the environment."}, {"title": "6 Discussion and Limitations", "content": "We introduced Vision-Language Model Predictive Control (VLM-PC), which enables legged robots\nto rapidly adapt to changing, unseen circumstances during deployment. On a Gol quadruped robot,\nwe find that VLM-PC can autonomously handle a range of complex real-world tasks involving\nclimbing over obstacles, crawling under furniture, and navigating around dead ends and through\ncluttered environments. While VLM-PC is promising solution for enabling legged robots to handle\nnew tasks, there remains much left to explore regarding how to best leverage VLMs for adaptive\nbehavior for legged robots, especially as core VLM capabilities continue to improve. First, improving\nlanguage grounding for locomotion to better capture the nuances of the robot's capabilities could\nlead to more effective decision-making. It would also be interesting to explore if fine-tuning these\nVLMs, perhaps with techniques like reinforcement learning from human feedback, can lead to more\nefficient reasoning. In addition, the camera is currently mounted on the head of the robot and provides\na limited field of view, making it challenging for the agent to understand the full context of its\nenvironment. This limitation sometimes causes the VLM to struggle with scenarios where the robot's\nbody may be stuck on an obstacle even if the head has cleared it. Incorporating more sensors or scene"}, {"title": "A Appendix", "content": "A.1 Skill Details and Hyperparameters\nWe obtain different behaviors from the default controller by modulating the parameters passed in.\nSpecifically, we control x- and y-velocity in the robot frame, gait type, body height, yaw speed, and\nduration to achieve different skills. The parameters used for each skill are described in the tables\nbelow, along with the duration corresponding to each magnitude. After the action is done executing,\nthe robot will stay frozen in the position it was left in at the end of the last action, e.g. if the last\naction was to crawl, the robot will stay low to the ground. We additionally provide our GPT-40 query\nhyperparameters.\nA.2 Prompts and Logs\nIn the following, we include the prompts used for VLM-PC, where the text highlighted in grey\nindicates text that is used for all comparison methods (including No History and No Multi-Step\nPlan). The text in green corresponds to the prompting for reasoning over history and is included\nin VLM-PC and the No Multi-Step Plan prompts. The text highlighted in blue corresponds to\nprompting for multi-step planning, which is included in VLM-PC and No History. The text in yellow\ncorresponds to reasoning over the historical multi-step plan and is included only in the full VLM-PC\nprompt. When included, the ICL prompt went immediately after the first paragraph of the initial\nprompt and consisted of a short explanation followed by example egocentric views and one or two\nactions the robot might take when it each view. For VLM-PC and No Multi-Step Plan methods, the\n\"Initial Prompt\" below was given at the start and repeated after every six responses. Otherwise the\n\"Successive Prompt\" was given in all queries after the first. Note that we used GPT-40 as the VLM for\nall of our experiments, and additional prompt tuning may be necessary for other VLMs. Anecdotally,\nwe tried using the Gemini Flash model and found that it did not reason as effectively with these\nprompts.\nIn the pages afterward, we display a full example log with VLM-PC on the Outdoor 2 obstacle course,\nwhere the prompts and input images are included in blue and the output of the VLM at each step is\nincluded in green. See our anonymous website for videos of our results."}]}