{"title": "Commonsense Reasoning for Legged Robot\nAdaptation with Vision-Language Models", "authors": ["Annie S. Chen", "Alec Lessing", "Andy Tang", "Govind Chada", "Laura Smith", "Sergey Levine", "Chelsea Finn"], "abstract": "Legged robots are physically capable of navigating a diverse variety\nof environments and overcoming a wide range of obstructions. For example,\nin a search and rescue mission, a legged robot could climb over debris, crawl\nthrough gaps, and navigate out of dead ends. However, the robot's controller\nneeds to respond intelligently to such varied obstacles, and this requires handling\nunexpected and unusual scenarios successfully. This presents an open challenge to\ncurrent learning methods, which often struggle with generalization to the long tail\nof unexpected situations without heavy human supervision. To address this issue,\nwe investigate how to leverage the broad knowledge about the structure of the world\nand commonsense reasoning capabilities of vision-language models (VLMs) to aid\nlegged robots in handling difficult, ambiguous situations. We propose a system,\nVLM-Predictive Control (VLM-PC), combining two key components that we find\nto be crucial for eliciting on-the-fly, adaptive behavior selection with VLMs: (1)\nin-context adaptation over previous robot interactions and (2) planning multiple\nskills into the future and replanning. We evaluate VLM-PC on several challenging\nreal-world obstacle courses, involving dead ends and climbing and crawling, on a\nGol quadruped robot. Our experiments show that by reasoning over the history of\ninteractions and future plans, VLMs enable the robot to autonomously perceive,\nnavigate, and act in a wide range of complex scenarios that would otherwise require\nenvironment-specific engineering or human guidance.", "sections": [{"title": "1 Introduction", "content": "Robots deployed in open-world environments must be able to handle highly unstructured and\ncomplicated environments. This is particularly the case for legged robots, which may need to\noperate in an extremely diverse range of circumstances. Consider a quadruped robot tasked with\nperforming search and rescue in a collapsed building. This robot faces a long tail of different possible\nenvironments and obstacles, which might require climbing over debris, crawling through gaps, and\nbacktracking and navigating out of dead ends without a map. Handling these diverse real-world\nscenarios autonomously, without detailed human guidance and specific skill directives, remains a\nsignificant challenge. Prior work in locomotion has endowed legged robots with agile skills like\nrunning, climbing, and crawling [1, 2], but possessing these skills alone does not solve the problem\nof fully autonomous deployment. To handle complex, unstructured scenarios, a robot must be able to\ndecide how to deploy its repertoire of skills with a nuanced understanding of its situation. Consider\nthe example of clearing a novel obstacle like debris in a collapsed building. We expect an intelligent\nrobot to perceive and try a skill that is likely to succeed, e.g., climbing. If the robot's attempt was\nunsuccessful, e.g. the debris is too slippery to climb over, the robot should recognize this and try"}, {"title": "2 Related Work", "content": "Our work tackles the issue of enabling legged robots to perform robustly in unstructured, unknown\ntest-time conditions. Traditional model-based control approaches have achieved impressive agile\nlocomotion [12, 1, 13, 14, 15, 16, 17] but are not well-equipped to navigate arbitrary, open-world\nenvironments. Learning-based approaches hold the promise of greater generalization capabilities,\nand training a single policy with reinforcement learning (RL) has also demonstrated successful\nlow-level locomotion capabilities from robust walking to jumping and bipedal walking [18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]. Behind a majority of these successes is the use of domain\nrandomization [32, 33, 34, 35, 36, 19, 21, 37, 38, 39, 40], which involves training the robot under a\nvariety of different dynamics to robustify the policy. Our work tackles an orthogonal, complementary\nproblem: enabling legged robots to autonomously solve complex, partially observed tasks given a\nrepertoire of low-level skills (which can be acquired through either traditional model-based approaches\nor RL training). Using these skills to solve a long-horizon task requires understanding the scene and\nreasoning over the information gathered in the environment, trying different low-level strategies, and\nadapting high-level plans on-the-fly accordingly.\nPrior work has also explored utilizing a repertoire of skills to help legged robots navigate that require\na combination of distinct behaviors. For example, Margolis and Agrawal [41] train a policy that uses\nhuman input via remote control to select skills, while others have explored using learned models to\nchoose appropriate behaviors on-the-fly, e.g., using search in latent space [21, 25, 42], direct inference\nusing proprioceptive history [22, 43, 44], prediction based on egocentric depth [45, 24, 30, 46], or\nusing value functions [47]. These works rely on human supervision or domain-specific information\nrequired to train model-based behavior selection. In contrast, our approach represents the robot's\nrange of skills in language and studies how to leverage this representation with pre-trained VLMs\nusing in-context reasoning to adapt on-the-fly in complex scenarios.\nOutside legged locomotion, extensive research has explored combining prior behaviors to address\nlong-horizon tasks, often by training high-level policies that orchestrate learned skills into complex\nbehaviors [48, 49, 50, 51, 52, 53, 54, 55, 56, 57]. Natural language provides a simple abstraction to\nindex these behaviors, and using language as an abstraction for behaviors provides an interpretable\nspace for a high-level planner to select strategies to try [3, 4, 58, 59, 5, 60, 61, 62] or to generate\nrobot code [63, 64]. In-context reasoning with LLMs has refined low-level behaviors [65, 66, 67, 68],\nimproved planning with feedback [58] and facilitated learning from human feedback [69, 70], but\nthese do not incorporate VLMs, which can offer rich multimodal understanding. Recent works have\nbegun going beyond LLMs and incorporating VLMs for manipulation [71, 72, 73] and navigation [74,\n75]. Unlike these works, we focus particularly on equipping the robot to handle unpredictable\nsituations where it might get stuck and need to explore different strategies to make progress. Enabling\nthis in a diverse array of environments requires robust commonsense reasoning abilities, and we study\nthe extent to which VLMs can provide these for legged robots.\nWhile high-level planning in language grounding has been studied for manipulation or navigation\ntasks, it has explored far less for legged locomotion. Key works have interfaced through foot contact\npatterns [76] or code [77] with LLM planning. Our work implements a straightforward language-skill\ninterface for locomotion and is the first to explore how legged robots can utilize the commonsense\nreasoning capabilities of pre-trained VLMs to autonomously guide adaptive behavior selection. In\nparticular, the vast majority of prior works apply LLMs zero-shot based on the current instruction or\nobservation; in this work, we study how the in-context adaptation ability of VLMs can help robots\nadapt to different scenarios."}, {"title": "3 Problem Statement", "content": "We assume the robot has access to a set of n skills, which are sufficient to allow the robot to traverse\nthe environment. Given the recent development of highly robust low-level quadrupedal locomotion"}, {"title": "4 Vision-Language Model Predictive Control (VLM-PC)", "content": "Our goal is to enable legged robots to make informed decisions that lead to effective, context-aware\nadaptation to help navigate these situations autonomously and successfully. Our central hypothesis is\nthat many real-world situations demand complex reasoning due to unexpected circumstances that\nmay be difficult to generalize to. In this section, we first describe how we represent the robot's skills\nvia language to be used then by a pre-trained VLM. We then detail how we prompt the VLM to\nreason through the robot's current state and history of interactions to select the next skill to execute to\nsolve a task in unstructured environments."}, {"title": "4.1 Interfacing Robotic Locomotion Skills with VLMS", "content": "We consider generative VLMs, also known as multimodal language models, which take as input\n{I, x}, including images {I} and prompt text x and outputs text y from a distribution over textual\ncompletions P(.\u00b7|{I}, x). We label each of the robot's prior behaviors \\(\\pi_i \\in \\Pi\\) with a command \\(l_i\\), a\ntextual description of the corresponding behavior. We also define levels of magnitude m for each\nthat define the duration \\(\\delta_{l_i,m}\\) that i should be executed. While there are many ways to acquire\nlocomotion policies, e.g., via traditional model-based techniques or learning-based approaches, we"}, {"title": "4.2 Using VLMs for Adaptive Behavior Selection", "content": "We propose a system, Vision-Language Model Predictive Control (VLM-PC), that uses a VLM to\naccount for these errors and successively refine strategies, so that the robot can autonomously adjust\nfrom strategies that fail and try others. Summarized in Figure 2, VLM-PC combines two key insights\nto effectively enable VLMs to serve as an effective high-level policy: (1) reasoning about information\ngathered by the robot in its environment and (2) selecting actions by planning ahead and iteratively\nreplanning during execution. The VLM we use in all of our experiments is GPT-40. We tuned the\nprompts to take into account the setting of legged locomotion and the limited view from the robot's\ncamera. Full prompts and an example log of the VLM's chats are shown in Appendix A.2.\nUsing in-context reasoning to adapt on-the-fly. In large foundation models, techniques like\nchain-of-thought [6, 7, 79], where the model is prompted to output intermediate reasoning steps, have\nbeen shown to significantly improve the model's ability to perform complex reasoning. We aim to\nleverage such techniques to equip the VLM to better understand and reason through the environment\nand provide more effective high-level commands to the robot. In particular, we want the VLM to\nreason through the history in the environment and the progress made with the commanded skills\nbefore deciding on the next skill, in order to determine if the robot should try a new strategy. As\nsuch, we include as input to the VLM an image representing the robot's current view along with\nthe full history of interactions (including the robot's previous images and the previous outputs of\nthe VLM) and a prompt, i.e. the input at timestep t is \\((I_1, X_1, Y_1, I_{t-1}, X_{t-1}, Y_{t-1}, ..., I_{t-1}, I_t, P_t)\\),\nwhich contains for each previous query step i, each previous image \\(I_i\\) and prompt \\(x_i\\) along with\nVLM output \\(Y_i\\). We then prompt the VLM to first reason through what progress the robot has made\nusing the history of commands selected and the current position and orientation of the robot.\nMulti-step planning and execution. Due to partial observability, there is often no clear answer as\nto which skill is most appropriate for a given situation and without physical experience, the VLM is\nnot grounded in the robot's low-level capabilities (i.e. the VLM understands that the robot can crawl,\nbut it does not know exactly how it crawls and whether this crawling skill will actually be successful\nin the current situation). So, we use an approach akin to model predictive control [8, 9, 10, 11],\nwherein we prompt the VLM to produce the immediate skill to execute by planning multiple steps \\(l_t\\),\n\\(l_{t+\\delta},..., l_{t+k}\\), into the future and reasoning about the consequences of the actions. This allows the\nVLM to foresee different possible strategies that might be applicable to the current situation, so it\nmay better adjust in the future if the next chosen skill does not make progress. Then after the robot\nexecutes the skill corresponding to \\(l_t\\), the VLM repeats this planning for each step during deployment.\nTo implement this, we specifically prompt the VLM to make a multi-step plan taking into account\nthe latest visual observation \\(I_t\\), compare the new plan to the prior existing plan, and use the one that\nseems more applicable."}, {"title": "5 Experimental Results", "content": "In this section, we study whether VLM-PC can enable a Gol quadruped robot to tackle five challeng-\ning real-world situations in a fully autonomous manner. Concretely, we aim to answer the following\nempirical questions: (1) Can VLM-PC enable the robot to autonomously adapt in unseen, partially"}, {"title": "6 Discussion and Limitations", "content": "We introduced Vision-Language Model Predictive Control (VLM-PC), which enables legged robots\nto rapidly adapt to changing, unseen circumstances during deployment. On a Gol quadruped robot,\nwe find that VLM-PC can autonomously handle a range of complex real-world tasks involving\nclimbing over obstacles, crawling under furniture, and navigating around dead ends and through\ncluttered environments. While VLM-PC is promising solution for enabling legged robots to handle\nnew tasks, there remains much left to explore regarding how to best leverage VLMs for adaptive\nbehavior for legged robots, especially as core VLM capabilities continue to improve. First, improving\nlanguage grounding for locomotion to better capture the nuances of the robot's capabilities could\nlead to more effective decision-making. It would also be interesting to explore if fine-tuning these\nVLMs, perhaps with techniques like reinforcement learning from human feedback, can lead to more\nefficient reasoning. In addition, the camera is currently mounted on the head of the robot and provides\na limited field of view, making it challenging for the agent to understand the full context of its\nenvironment. This limitation sometimes causes the VLM to struggle with scenarios where the robot's\nbody may be stuck on an obstacle even if the head has cleared it. Incorporating more sensors or scene"}, {"title": "A Appendix", "content": "In the following, we include the prompts used for VLM-PC, where the text highlighted in grey\nindicates text that is used for all comparison methods (including No History and No Multi-Step\nPlan). The text in green corresponds to the prompting for reasoning over history and is included\nin VLM-PC and the No Multi-Step Plan prompts. The text highlighted in blue corresponds to\nprompting for multi-step planning, which is included in VLM-PC and No History. The text in yellow\ncorresponds to reasoning over the historical multi-step plan and is included only in the full VLM-PC\nprompt. When included, the ICL prompt went immediately after the first paragraph of the initial\nprompt and consisted of a short explanation followed by example egocentric views and one or two\nactions the robot might take when it each view. For VLM-PC and No Multi-Step Plan methods, the\n\"Initial Prompt\" below was given at the start and repeated after every six responses. Otherwise the\n\"Successive Prompt\" was given in all queries after the first. Note that we used GPT-40 as the VLM for\nall of our experiments, and additional prompt tuning may be necessary for other VLMs. Anecdotally,\nwe tried using the Gemini Flash model and found that it did not reason as effectively with these\nprompts."}]}