{"title": "X-SG2S: Safe and Generalizable Gaussian Splatting with X-dimensional Watermarks", "authors": ["Zihang Cheng", "Huiping Zhuang", "Chun Li", "Xin Meng", "Ming Li", "Fei Richard Yu"], "abstract": "3D Gaussian Splatting (3DGS) has been widely used in 3D reconstruction and 3D generation. Training to get a 3DGS scene often takes a lot of time and resources and even valuable inspiration. The increasing amount of 3DGS digital asset have brought great challenges to the copyright protec-tion. However, it still lacks profound exploration targeted at 3DGS. In this paper, we propose a new framework X-SG2S which can simultaneously wa-termark 1 to 3D messages while keeping the origi-nal 3DGS scene almost unchanged. Generally, we have a X-SG2S injector for adding multi-modal messages simultaneously and an extractor for ex-tract them. Specifically, we first split the water-marks into message patches in a fixed manner and sort the 3DGS points. A self-adaption gate is used to pick out suitable location for watermark-ing. Then use a XD(multi-dimension)-injection heads to add multi-modal messages into sorted 3DGS points. A learnable gate can recognize the location with extra messages and XD-extraction heads can restore hidden messages from the loca-tion recommended by the learnable gate. Extensive experiments demonstrated that the proposed X-SG2S can effectively conceal multi modal mes-sages without changing pretrained 3DGS pipeline or the original form of 3DGS parameters. Mean-while, with simple and efficient model structure and high practicality, X-SG2S still shows good performance in hiding and extracting multi-modal inner structured or unstructured messages. X-SG2S is the first to unify 1 to 3D watermark-ing model for 3DGS and the first framework to add multi-modal watermarks simultaneous in one 3DGS which pave the wave for later re-searches. Our project is available at: https://github.com/ChengLiDuoJi/XSGS.", "sections": [{"title": "1. Introduction", "content": "The applications of 3D reconstruction and 3D generation technology are extremely extensive. It has already made significant contributions in fields such as healthcare, ar-chitecture and engineering, cultural heritage preservation, virtual reality and augmented reality, as well as game and film production. The existing work mainly studies how to build 3DGS models to generate 3D scenes or reconstruct 3D scenes.\nHowever, the increasing emergence of 3D Gaussian Splat-ting (3DGS) models may lead to issues of information leak-age and infringement. At the same time, in the near future, it is likely that people will need to add more, and even mul-timodal, information or watermarks to a reconstructed or generated scene which poses a challenge to existing meth-ods. Meanwhile, using different methods to add watermarks makes the watermarking adding mechanism complicated and chaotic.\nCurrently, only a small portion of research has addressed these problems. A new study GS-Hider restore image or 3D scene into another 3D scene via design a coupled secured feature. Considering the security issues of generative 3D, GaussianStego explores how to embed image watermarks into diffusion models. However, these models all have their limitations. For example, GS-Hider modifies the parame-ters of GS, which may increase the size of the 3D model parameters and no longer make it universal. The approach of GaussianStego rooting dino into diffusion can not well demonstrate its effect in feed forward 3D generative models. At the same time, adding extra elements in original 3DGS pipelines for protecting copyright is costly and complicate in design or training. We can see that how to effectively add additional watermarks or information without changing the GS parameters or model structure and let the method be general will be a great challenge. Meanwhile, embedding multimodal watermarks into a 3DGS simultaneously is even more of an unexplored area.\nTo solve the above challenges, we propose an effective, simple and flexible watermarking framework X-SG2S. It aims to embed 3D objects or images or binary messages si-multaneously into the original 3DGS scene, and accurately extract the hidden message via just an ultra simple way. Specifically, we use a determined sorting method to sort the GS point set, then use a self-adaption gate which can learn the adding position through the interaction between the ad-ditional information(watermarks) and the container(3DGS scenes). A XD-injection heads are used to add different modal messages respectively. Then a learnable gate is used to recognize the locations where we have added the mes-sages. Finally, XD-extraction heads are used to restore the message added. The training for X-SG2S does not need to fine tune or redesign the pipeline. This property makes this model suitable not only for the feedforward model, but also for the model like diffusion which needs multiple iterations. And if you do not need to add additional information, you can use the original model in the fastest time without further fine-tuning. However, it is also possible to fine tune with the pipeline. In a nutshell, the contributions and advantages of our X-SG2S can be summarized as follows.\n\u2022 We have pioneered the first method that can simultane-ously embed watermarks of 1 to 3 dimensions into one 3DGS scene, which paves the way for future research.\n\u2022 We have provided an effective approach for GS clouds to store additional structured or unstructured data.\n\u2022 Our model, in an extremely concise and efficient man-ner, makes the process of adding watermarks to 3DGS both universal and secure.\n\u2022 We conducted extensive experiments to demonstrate the efficiency. The experimental results were SOTA, which established the baseline method for later re-search."}, {"title": "2. Related Works", "content": null}, {"title": "2.1. Generalizable GS", "content": "3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) has emerged as a powerful method for reconstructing and repre-senting 3D scenes using millions of 3D Gaussians. There has been a significant amount of work that has achieved good results in the reconstruction of small objects (Szy-manowicz et al., 2024) (Boss et al., 2024) and scenes (Chen et al., 2025) (Charatan et al., 2024). Additionally, some studies have utilized diffusion models to realize text-to-3D generation (Li et al., 2024b), which also achieves promising outcomes."}, {"title": "2.2. 3D Steganography", "content": "Steganography has been evolving over the decades. Some research has made 3D steganography achieve good results in explicit geometry like meshes and point clouds (Ohbuchi et al., 2002) (Zhu et al., 2024) (Ferreira & Lima, 2020) or implicit geometer like nerf (Li et al., 2023)(Luo et al., 2023). 3DGS is a new technology to represent explicit geometry. However, few works have been researched in steganography for 3DGS. GS-Hider (Zhang et al., 2024)design a coupled secured feature attribute to replace the original 3DGS's spherical harmonics coefficients and then use a scene de-coder and a message decoder to disentangle the original RGB scene and the hidden message. GaussianStego(Li et al., 2024a) use dino to add image watermarks and use U-Net to extract them via multi rendered views. But there is not research about watermarking 1 to 3D messages to single 3DGS scene simultaneous while keeping GS parameters or model structure unchanged."}, {"title": "3. Safe Generalizable GS with X-dimensional watermarks", "content": null}, {"title": "3.1. Preliminaries of Gaussian Splatting", "content": "Gaussian Splatting is a rasterization technique for 3D scene reconstruction and rendering. It works by using 3D Gaus-sian functions to represent points in a scene and projecting these Gaussian functions onto a 2D image plane for render-ing. For every point, it has many different parameters to represent the location, shape, size, color and opacity.\nThis function shows the location, shape and size of a GS point where 0 < i < \u039d, \u03bc; \u2208 R\u00b3 is the mean or cen-ter, \u03a3\u00bf \u2208 R3\u00d73 is its covariance specifying its shape and size. The covariance can be decomposed into scaling fac-tor s \u2208 R\u00b3 and rotation factor q \u2208 R\u00b9. Each Gaussian has also an opacity ai \u2208 [0, 1] and a view-dependent color ci(v) \u2208 R3\u00d7(k+1)\u00b2 where k is the higher level of spherical harmonics(SH). The set of GS points G = {(\u03bc\u03af \u03a3i Ai Ci), i =1,...,N} can be used to represent a scene or a small ob-ject. Through rasterization, it can be rendered to an image where you only need to know the projective transformation P, viewing transformation W, and Jacobian of the affine approximation of P, J, respectively.\n\u03bc\u03b5 = PW \u03bc\u03af\n\u03a3\u2081 = JWEiJWT\n\u03c3\u03b5 = aze(-i)\u00af\u00c9\u00bf\u00af\u00b9(p-\u00fbz)\nN\nC[p] = \u03a3\u03b5\u03af\u03c3\u03b5 \u03a0(1 \u2013 \u03c3\u0632(\ni=1\nj=1\nwhere p represents the pixel."}, {"title": "3.2. Task Settings", "content": "3DGS steganography: We hope that for any 3dgs scene, we can add additional information without damaging the original 3d structure and gs parameters.\nCopyright Protection: By adding fixed watermark into generated scenes, the designers can easily use X-SG2S to detect whether the GS art works are used by others without permission.\nMulti-modal Message Adding: This model can not only add information of single modality, but also add information of multiple modalities without producing confusion. This can greatly bring convenience to the users. In theory, this model can add sequential structure data or non-sequential structure data, and accurately extract them when the GS point set is well preserved.\nGeneralizable Modules: This model does not require changing the structure of the GS generated model, but rather adds watermarks or additional information to the scene in the form of additional modules."}, {"title": "3.3. Deep Thinking About Watermarking 3DGS", "content": "Gaussian sphere is a special kind of point set. You can simply see it as a kind of point clouds with many parameters. First of all, we think about why it is difficult for us to add information to GS point clouds.\n\u2022 Point set is an unordered set which means you may treat the point set as a whole to add information (e.g. add information to the frequency domain of the entire point cloud or add the information to the entire point cloud). But this can make the waste of storage space when the amount of information is small or may make large damage to the original model.\n\u2022 If you just use a part of points to add information, you have difficulty knowing the location and order ,especially, when you add more than one watermarks.\n\u2022 Each point has a limited capacity to store informa-tion. How to correctly use small containers to store watermark with large amount of information is also a challenge.\nAfter deep thinking, we can sort out such information adding ideas:\n\u2022 Every point in the point set have a few of storage space. But when all the storage units are added up, the storage capacity is very large. If we really want to fully use all of the storage space, we should smash the information into patches in an order that we can restore them easily and add them respectively for every points. If we add too much information to one of the point, it may cause loss.\n\u2022 If we want to know how to extract the information from the point set, we should firstly know which of them are added. This means you would better know the location of each point and the restoration order. In another words, you should make the point set an ordered set and a known order of watermark composition."}, {"title": "3.4. Overview", "content": "The major target of X-SG2S is to design a general frame-work for multi-modal message adding model which can inject and extract 1D,2D and 3D massages with one model while keeping the original Gaussian scene as unchanged as possible.\nAs depicted in Figure 2, X-SG2S contains mainly four com-ponents: 1) an adaptive self-adaption gate for adaptively selecting GS points to use. 2) a multi-head MLP-based mes-sage injector for integrating information into SH parameters. 3) a learnable selection gate for recognizing the location of injected GS points. 4) a multi-head MLP-based message extractor for restoring message from the injected GS points. X-SG2S injector is made up of 1) and 2), extractor is made up of 3) and 4).\nThe training process and the inference process are slightly difference. The following part will illustrate them in de-tailed."}, {"title": "3.5. Point Set Sorting", "content": "As can be seen above, before injecting information and extracting information, we should use the sorting method to sort the point set. If you want every time to get the same order for a point set, you should use a deterministic sorting method. There are many methods for sorting. Here we choose Hilbert filling curve to complete this task. This is because Hilbert filling curve is a kind of deterministic curve. Hilbert curves can map high-dimensional data to a one-dimensional space to get the order of high-dimensional data. Meanwhile, when the size of the data set increases, data access time and computation time are often affected by spatial locality. Hilbert filling curve can reduce unnecessary jumps in storage and retrieval and improve the processing efficiency of large-scale data sets through space filling. At the same time, the Hilbert filling curve can be accelerated by GPU, which can effectively reduce the operation time."}, {"title": "3.6. Self-Adaption Gate", "content": "This component is to find out the points which are suitable for injecting the messages while ensuring that the original scenario is not disturbed so much. A simple idea is that let a gate to learn a score to measure the locations. The gate will give every point a comprehensive score. A higher score indicates that the location is more suitable for adding information.\nThe comprehensive score is calculated from two scores: \"self-score\" and \"cross-score\". \"Self-score\" is used to measure which points can significantly impact the 3DGS scenario and which are trivial. It is obvious that if we choose the trivial points to inject messages, we will not disturb the scenario too much. But only use \"self-score\" is not enough. We need to know where are more suitable for message to inject and extract. \"Cross-score\" is used to manage this problem.\nAfter we get two scores, we simply multiply two scores together to get the final score. By using top-k, we can set the recommended k positions to 1 and others to 0 to get the selection mask."}, {"title": "3.6.1. MESSAGE EMBEDDING", "content": "Different modalities belong to different vector spaces. If we want to calculate the interactions results uniformly and effi-ciently, we need to map them into a unified space first. Here we simply use three different linear layer to map 1,2,3D message to a general space. And for gs cloud, we also use a linear layer to map it to another high dimension space we call \"gs-emb\" space. The dimension of two space can be different."}, {"title": "3.6.2. INDUCED SET ATTENTION TO PRODUCE SELF-SCORE", "content": "After we get embedded message, we can measure the inter-act scores. Attention mechanism is a very efficient method for interact different information. We use transformer-based model to calculate the \"self-score\". But we find that if we simply use scale dot product attention, the memory will overflow because the number of GS points are large. How-ever, to calculate \"self-score\" is merely want to measure the importance of individual information in the overall in-formation instead of learning detailed information between individuals. So set attention(Lee et al., 2019) is used to overcome the memory problem. At the same time, set trans-former does not need for the data to be sequential. This is very suitable for point sets, because point sets are unordered.\nInduced set attention first design a set of anchor vectors I \u2208 Rmxd, where d means the inner dimension of the vector, m means the number of anchor vectors. Then it uses the input X \u2208 Rn\u00d7d and I to do multi-head transformer to get the hidden message H \u2208 Rm\u00d7d of the point set. Then use H and X to do multi-head transformer again to get the importance of every points in overall information Os\u2208 Rnxd. Finally, use Os to calculate the sel f_score \u2208 Rn\u00d71 by using a linear layer. Formally,\nH = MA(I, X)\nO\u2083 = MA(X, H)\nself_score = sigmoid(linear(Os))\nIn our task, we simply set the number of heads to 1 and the number of anchor vectors to 1 so that the time complexity goes from O(N2) to O(N)."}, {"title": "3.6.3. EFFICIENT ATTENTION TO PRODUCE CROSS-SCORE", "content": "Because \"Cross-score\" needs to interact the whole GS points with embedded message patches. However, if the amount of message patches is large, the attention may also face the problem of overflowing. Thus, we use efficient trans-former (Shen et al., 2021) to approximation the common transformer in order to reduce the memory and preserve the accuracy of calculating.\nBecause this is a cross attention mechanism, we let X to become Q, and the message patches Y \u2208 Rl\u00d7d to become K,V. It is easy to get that l is usually less then n. So, it is efficient to let KV interact first. Formally,\nOc = grow(Q)(ocol(K)TV)\nWhere & means the softmax function. The same as above, use Oc to calculate the cross_score \u2208 Rn\u00d71 by using a linear layer.\ncross_score = sigmoid(linear(Oc))"}, {"title": "3.6.4. LOCATION MASKS", "content": "After getting two scores, we simply multiply two scores to get a final score. For every modality, it has one final score. This final score can comprehensively evaluate whether a point is suitable to add message. According to the number of message patches for one modality, we use top-k to select the most suitable locations for adding, where k is the number of patches. If we want to generate the next modality's location mask, we should use the previous masks to avoid selecting the duplicate locations."}, {"title": "3.7. Learnable Selection Gate", "content": "This gate is to find out the locations where the information added. For above we can know that the self-adaption gate can produce a mask. Here we see the masks as the ground truth. After we get the ordered point set with added mes-sages, we see it as the input of the learnable selection gate. The gate is a four layers MLP to predict the true location."}, {"title": "3.8. XD-Injection/Extraction Heads", "content": "These two are used for injecting messages and extracting messages. For each modal, we use a separate MLP (Qi et al., 2017). Usually, 3 to 5 layers are enough for the task. We take out the GS points of target location recommended by self-adaption gate and just use the spherical harmonic parameters of three channels. We concatenate the spherical harmonic parameters with the message patches we want to added. We treat the concatenated data as input to multi-head injector which produce the spherical harmonic parameters with extra information.\nAnd for extractor, it take the SH parameters of injected points found out by learnable selection gate as input and extract the messages from them. The extracted messages will be optimal by the prediction loss as well."}, {"title": "3.9. Training Process", "content": "This model can be trained directly by GS cloud files or a pretrained 3DGS pipeline.\n1. Get the GS cloud inferred from a pipeline or read from a cloud file.\n2. Use sorting method to sort the GS cloud to get an ordered set. Use the data loader to load the goal in-formation we want to add. And we scatter them into message patches in order.\n3. Give self-adaption gate the message patches and the GS cloud to recommend the locations.\n4. Use XD-injection heads to add message.\n5. Replace the original SH parameters by the injected SH parameters so that we get a GS point set with extra messages.\n6. Truncate the gradient flow of the GS point set with extra messages and take it as input to the learnable gate. It will output the location mask. This part is off-line to ensure proper flow of gradients. If we directly choose the predicted location to extract messages, the message extractor may choose some GS points without messages. This may cause the error of gradient flow.\n7. Use the truth location to choose the injected GS points and extract messages by XD-extraction heads.\nWe can see from above that, in training steps we do not need to restore the message patches to their original structure. For example in our work, we encode a graph into features using AE. We inject the feature patches into GS points. If we want to restore the graph from AE, we just need to reshape the feature patches to the features and take it as the input of the decoder of the AE without optimize the encoder and decoder. And you do not need to design or use another losses to train. The message patches will automatically converge. And specifically for 3DGS small object watermarks, we should first reuses some of the points in the object to align the number of points in order to let X-SG2S perform parallel computation without causing extra loss."}, {"title": "3.10. Training Loss", "content": "To finish the task, we should design 1) message patches loss, 2) location mask loss, 3) SH parameters loss, 4) other losses.\nTo ensure the feature patches can be restore exactly, we should use message patches loss to ensure the extracted message is as same as possible to the message injected. For 1D message, we use cross entropy loss; for 2D and 3D message, we use MSE loss.\nTo ensure the GS parameters do not change much before and after adding information, we use a SH parameter MSE loss to ensure this.\nTo ensure the learnable selection gate can precisely recog-nize the location where we inject the message, we use a mask cross entropy loss. This loss use the mask produced by self-adapted gate as the ground truth. The weight for the mask cross-entropy loss does not need to be set as its training is offline. We default this weight to 1.\nOther loss like 2D view loss and lpips loss can be choosed to use when we use pretrained 3DGS pipeline to train X-SG2S. They may let X-SG2S more efficient to preserve the SH parameters.\nThus, the form of loss function is like:\nLOSS = Optional + y SH-MSE + $ 1D-BCE + 02D-MSE + 8 3D-MSE + Mask-BCE"}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Datasets", "content": "We use a pretrained model MVSplat for real time inference. We use large-scale ACID datasets to let MVSplat(Chen et al., 2025) generate the 3DGS scenes. For 1D dataset, we randomly generate a sequence of binary message. For each bit, it has the same probability to be 1 or 0. For 2D dataset, we use Logo-2K(Wang et al., 2020) dataset. For 3D dataset, we first download a subset of objaverse(Deitke et al., 2023), and use Gamba(Shen et al., 2024) to generate the 3DGS objects. They are then saved in .ply format as a 3D dataset.\nSpecifically, ACID contains nature scenes captured by aerial drones, which are split into 11,075 training scenes and 1,972 testing scenes. After reconstructing Logo-2K dataset, it has 110313 training logos and 28415 testing logos. 3D dataset made by objaverse and Gamba contains 353 training files and 94 testing files. To ensure the number of datasets is aligned, we reuse the GS cloud files while training."}, {"title": "4.2. Metrics", "content": "For original GS scenes and GS objects, we use pixel-level PSNR, patch-level SSIM, and feature-level LPIPS to mea-sure the completeness of the original scenes and the quality of restoration of the GS objects. For a fair comparison, all the scenes are rendered to 256x256 resolutions while all the objects are rendered to 512\u00d7512 resolutions. For 1D data, we use precision rate to measure the extraction accuracy of binary data. For 2D data, we use pixel-level PSNR to quantization the restoration of the 2D images."}, {"title": "4.3. Implementation Details", "content": "X-SG2S is implemented with PyTorch, along with an off-the-shelf 3DGS render implemented in CUDA. All models are trained on single A6000ada with the Adam optimizer.\nIts training involves multimodality, so there are many hyper-parameters in the loss function that need to be set. We de-fault to using optional loss functions unless in loss function ablation experiments. The optional loss functions include 2D rendered image MSE loss and LPIPS loss, with weights of 1 and 0.05, respectively. And other hyperparameter are \u03b3 = 0.2, \u03c6 = 0.005, 0 = 0.8, \u03b4 = 1.5.\nWe choose binary message of 8bit\u00d7128 as 1D watermark and 3DGS object with 10000 points as 3D watermark. For 2D watermark, we embed them via a pretrained AE en-coder then split them in order to get 16\u00d732768 message patches. In the testing phase, we reconstruct the informa-tion fragments into a predetermined shape and then pass them through the AE decoder to obtain the image. And the number of SH parameters is 75 for each GS point."}, {"title": "4.4. Main Results", "content": null}, {"title": "4.4.1. ABLATION EXPERIMENTS", "content": "In Figure 4, we design five different structures of interac-tion block and conduct ablation experiments respectively. We jointly train X-SG2S with multimodal watermarks. In Table 1, we show that the method of a) is the best."}, {"title": "4.4.2. SINGLE MODALITY INJECTION TEST", "content": "We also test X-SG2S's performance on training single modality. In Table 2, we can see that X-SG2S can well adapt the task of 1 to 3D watermarking. To our delight, we discov-ered that the scene with watermark outperformed the origi-nal in terms of both SSIM and LPIPS metrics. Meanwhile, in Table 3, we compare the differences between adding a single watermark under joint training and under individual training. We can see that two results are almost identical. This demonstrates the effectiveness of our method. The image results are all presented in the appendix."}, {"title": "4.4.3. THE EXPLORATION OF LOSS FUNCTION", "content": "Now, we want to know if X-SG2S can be trained without optional losses which means it can trained by using just 3DGS files. Here we set the weights of optional losses to 0 and set the y = 2. From Table 4, we can see that this way is feasible! However, X-SG2S trained without optional loss functions performs worse in both watermark embedding effectiveness and preserving the integrity of the original scene compared to those trained with optional loss functions. Therefore, we recommend using optional loss functions to enhance the effectiveness of the training."}, {"title": "4.4.4. THE EXPLORATION OF TEXT WATERMARKING", "content": "We surprisingly found that X-SG2S can achieve nearly 100% accuracy on text watermark. So, we have delved deeper into its exploration. We conduct two experiments: 1) Investigate the impact of binary bit size on watermarking. 2) Investigate the impact of the sentence's length on watermarking. For 1), we test the bit sizes of 8, 16, 24, 32, 40 which are the most commonly used, with its length of 12288. From Figure 5, we can see that under our experimental conditions, the size of 16 can achieve the best result. And the exper-iment also shows that the bit sizes does not have a great impact on the original scene.\nFor 2), we test the lengths of 128, 4096, 8192, 12288, 16384, 20480 with its size of 8. From Figure 5, we can see that as the length increases, the integrity of the original scene tends to decline, which implies that the length of the message patches affects the completeness of the original scene."}, {"title": "5. Conclusion", "content": "In this work, we propose a multi modal watermarking frame-work for 3DGS, X-SG2S, designed for efficient adding 1,2,3D extra messages to a GS scene while keeping the orig-inal scene almost unchanged. X-SG2S successfully adds binary, graph, 3DGS object messages to a 3DGS scene si-multaneously or individually. Additionally, X-SG2S does not need to fine tune pretrain model (but train with pre-trained models is workable) and it can even trained on only 3DGS files which depletes training time and space. Besides, X-SG2S does not change the form of GS parameters so that it is suitable for most of 3DGS scenes or objects."}]}