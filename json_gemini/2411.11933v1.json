{"title": "METEOR: Evolutionary Journey of Large Language Models from Guidance to Self-Growth", "authors": ["Jiawei Li", "Chong Feng", "Yang Gao"], "abstract": "Model evolution enables learning from feedback to refine experiences and update skills, transforming models from having no domain knowledge to becoming domain experts. However, there is currently no unified and effective method for guiding this evolutionary process. To address this gap, we propose the Meteor method, which includes three training phases: weak-to-strong data distillation, iterative training, and self-evolution strategies. Each phase maximizes the model's inherent domain capabilities, allowing it to autonomously refine its domain knowledge and enhance performance. Experiments demonstrate that our approach significantly improves accuracy, completeness, relevance, coherence, and reliability across domain-specific tasks.", "sections": [{"title": "1 Introduction", "content": "The development of large language models (LLMs) has ushered in a new era in the field of natural language processing (NLP), showcasing remarkable general capabilities across a wide range of applications (OpenAI, 2023; Yang et al., 2024a,b; Reid et al., 2024). However, despite their outstanding performance on general tasks, the training of a highly versatile LLM demands substantial computational resources and financial investment. These high costs restrict their use in many situations, particularly in specific domains. In specific domains, there is often no need for a general purpose intelligent model; instead, a model that acts as an expert within a particular domain is more desirable. Furthermore, these domain-specific expert models should be trained at lower costs and easily deployed in their respective fields. Therefore, finding efficient ways to create a domain-specific expert model has become a key research focus in the development of LLMs (Ling et al., 2024; Li et al., 2024a).\nSome studies leverage the inherent capabilities of LLMs combined with domain-specific external enhancements to enable their application in specific domains. These methods involve explicitly or implicitly acquiring domain knowledge from external knowledge bases (Lu et al., 2023; Izacard et al., 2023; Schuurmans, 2023) or utilizing domain-specific tools to assist LLMs in specific domains (Jin et al., 2024; Li et al., 2023; Liang et al., 2023). However, these methods rely on the model's strong general capabilities and are typically applicable only to models with a large number of parameters, making them costly to deploy and limiting their widespread adoption. Several researchers have explored model evolution approaches to enhance domain-specific capabilities. Wu et al. (2023) achieves evolution in the financial domain through extensive manually annotated domain data, yet this approach proves challenging to scale due to the difficulty in data acquisition. While Xi et al. (2024) proposes utilizing general large models for supervision and feedback on domain model-generated data, this methodology remains constrained by the performance ceiling of the supervising model. Although Singh et al. (2024) made breakthrough progress in the coding domain through self-generated data and self-training, eliminating dependence on human annotations and large models, their approach has not yet been effectively extended to other specific domains.\nTo address the challenges faced by current models in domain-specific applications, we propose a self-evolution method named METEOR, a weak-to-strong evolution framework that enables LLMs to progressively evolve from supervised guidance to autonomous enhancement. Meteor offers a comprehensive training framework that guides an LLM from having no domain expertise to becoming a domain expert. This framework consists of three key stages: an initial fine-tuning stage, which aims to impart basic domain knowledge to the LLM; an"}, {"title": "2 METEOR", "content": "The METEOR method consists of three distinct phases: weak-to-strong data distillation, iterative training, and self-evolution strategies. In each phase, the model fully utilizes its existing capabilities to strengthen its domain expertise. Each subsequent phase builds upon the advancements made in the previous one, employing different techniques to further evolve the model. The following sections will detail the specific methods employed in each phase of METEOR, illustrating how these strategies collectively contribute to the model's enhanced performance and domain-specific knowledge."}, {"title": "2.1 Weak-to-strong Domain Data Distillation", "content": "For a LLM lacking domain-specific capabilities, knowledge distillation is proven to be an effective method for injecting domain knowledge (Huang et al., 2023; Zhang et al., 2023; Zhang and Yang, 2023). However, our experiments indicate that directly distilling domain data from a strong general model to train a domain-specific model is not particularly effective. As illustrated in Table 7 of Appendix A, there is a discrepancy between the responses of GPT-4 to domain-specific questions and the solutions provided by the weak model. This discrepancy arises from differences in their cognitive distributions, making it challenging to train and learn using directly distilled data.\nTo address this issue, we propose a domain data distillation method based on a weak-to-strong strategy (Burns et al., 2024). As illustrated in Figure 2, to align the distribution discrepancies between the strong and weak models, we guide the strong model to distill domain data according to the instructions from the weak model. Specifically, when presented with a domain-specific question, we first input the question into the weak model. Instead of generating an answer, the weak model produces a guideline based on the prompt shown in Figure 2. This guideline outlines the steps the weak model believes should be followed to address the question.\nSubsequently, we input the generated guideline along with the domain question into the strong model. The strong model then generates the answer to the domain question by following the guideline's steps or refines the answer based on the guideline. Through these steps, we obtain a set of data pairs consisting of domain questions and their corresponding answers distilled from the strong model. This distilled data is used to fine-tune the weak model, enabling it to acquire preliminary domain-specific capabilities, thus completing the first stage of the Meteor model's evolution. In Section 4, we demonstrate that the data obtained using the weak-to-strong distillation strategy is superior to that obtained through direct data distillation, thereby validating the effectiveness of this approach."}, {"title": "2.2 Model Evolution", "content": "Training models using domain data obtained through knowledge distillation can impart preliminary domain-specific capabilities to the LLMs. However, this approach falls short of elevating the models to the level of true domain experts due to the absence of expert-level reasoning abilities. Therefore, further model evolution methods are required to enhance the domain capabilities of these models.\nThe reflection mechanism, which involves analyzing the model's previous reasoning processes and answers to identify errors and provide feedback, has been shown to significantly improve the model's reasoning abilities and performance in downstream applications (Shinn et al., 2023; Madaan et al., 2023). Consequently, we have designed an algorithm for further evolving the model within specific domains, based on the reflection mechanism of LLMs, as illustrated in Figure 3."}, {"title": "2.2.1 Data Refinement and Iterative Training for Domain-Specific Models", "content": "The key aspect of the reflection mechanism is to provide accurate feedback on the model's output. However, models that have been developed through knowledge distillation possess only preliminary domain capabilities and are unable to deliver sufficiently accurate feedback. Therefore, at this stage, we propose using strong models, such as GPT-4 (OpenAI, 2023), to provide feedback for the evolution of domain-specific models. The overall algorithm is shown in Algorithm 1.\nThe specific method involves inputting a batch of domain data, where the current domain model"}, {"title": "2.2.2 Self-Evolution of Domain Capabilities through Inference Strategy Optimization", "content": "Additionally, we aim for the model to evolve its domain capabilities independently, without relying on strong models. To achieve this, we propose a method for the model to autonomously enhance its domain capabilities.\nIncreasing FLOPs during inference has been shown to effectively enhance model performance in downstream tasks (Snell et al., 2024). Inspired"}, {"title": "3 Experimental Setups", "content": "Datasets. We use the field of advanced computer education as the specific domain to validate the effectiveness of the proposed Meteor method. To obtain high-quality domain data, we scraped data from Stack Overflow across four categories: Machine Learning (ML), Deep Learning (DL), Natural Language Processing (NLP), and Computer Vision (CV), totaling 10,276 entries. The data distribution across these categories is shown in Table 1. From this dataset, we randomly selected 1000 entries as test data, with the remaining data used for training.\nMetrics. Following the evaluated method proposed by Zheng et al. (2023), we use GPT-4 as a judge to evaluate both data quality and model performance."}, {"title": "4 Experimental Results", "content": "We compared the performance changes across various dimensions before and after applying the Meteor method for domain capability evolution of LLMs. For the accuracy, completeness, relevance, coherence, and reliability, we generated responses to test set questions using both the Meteor-trained LLMs and the non-Meteor-trained LLMs. GPT-4 was then used to determine which model's responses performed better in each of these dimensions. For the GPT-4 Score, GPT-4 directly evaluated the answers generated by the LLMs before and after evolution, and the average score was calculated.\nAs shown in Table 2, after the Meteor evolution, LLaMA3-8B-Chat and Qwen2-7B-Instruct achieved improvements of 78.66%, 72.13%, 74.34%, 80.10%, 82.13%, and 68.36%, 63.43%, 60.24%, 63.43%, 67.09% respectively in accuracy, completeness, relevance, coherence, and reliability. Additionally, the GPT-4 Score after evolution was significantly higher than before, demonstrating the effectiveness of the Meteor method."}, {"title": "4.2 Effectiveness of Weak-to-Strong Data Distillation", "content": "In Section 2.1, we introduced a domain knowledge distillation method based on a weak-to-strong strategy. To evaluate the effectiveness of this approach, we used GPT-4 to score the distilled data. By comparing the GPT-4 Scores of data distilled with and without guidelines, we demonstrate the effectiveness of the weak-to-strong strategy in enhancing the quality of distilled data. The specific results are shown in Table 3.\nAs shown in Table 3, across various domains, the data distilled with guidelines achieved significantly higher GPT-4 scores than the data distilled without guidelines. Specifically, the GPT-4 scores increased by 3.29, 3.27, 3.34, and 3.32 points in ML, DL, NLP, and CV, respectively, indicating that the quality of domain data distilled with guidelines is superior."}, {"title": "4.3 Effectiveness of Iterative Training and Data Refinement", "content": "In Section 2.2.1, we introduced the Data Refinement and Iterative Training for Domain-Specific Models. o validate the effectiveness of this approach, we employed GPT-4 to evaluate the accuracy, completeness, relevance, coherence, and reliability of the answers generated by the models before and after evolution. The results are presented in Table 5."}, {"title": "4.4 Effectiveness of Self-Evolution Method", "content": "In Section 2.2.2, we proposed that model evolution should be independent of strong model constraints and introduced a self-evolution method based on varying FLOPs. Table 6 presents the differences in performance across various dimensions, as evaluated by GPT-4, before and after the models self-evolution."}, {"title": "5 Related Works", "content": "In the context of LLMs, knowledge distillation has become a crucial method for enhancing domain-specific knowledge. In specialized fields such as law and medicine, the complexity of terminology often poses challenges (Lai et al., 2023; Sun et al., 2023). As a result, the distillation process typically begins with pre-training on domain-specific corpora, followed by fine-tuning using LLMs to construct enhanced data (Huang et al., 2023; Zhou et al., 2024). In the financial fields, distilled data must maintain diversity and complexity to ensure"}, {"title": "5.2 Self-evolution of LLMs", "content": "Self-evolution of LLMs refers to the capability of LLMs to autonomously acquire, refine, and learn from their own generated experiences (Tao et al., 2024). Initially, model evolution requires experience acquisition. Self-Align (Li et al., 2024b) generates experiences guided by knowledge from over 20 domains to ensure diversity in evolution. In contrast, Self-Instruct (Wang et al., 2023b) proposes a self-evolution method without the need for external knowledge guidance. Subsequently, experience refinement is necessary. Self-Talk (Ulmer et al., 2024) ensures data quality by measuring the number of completed sub-goals, while Self-verification (Weng et al., 2023) selects data by evaluating the consistency between predicted values and original conditions. Once high-quality experiences are acquired, models need to be updated to enhance performance. ReST (Aksitov et al., 2023) updates LLMs iteratively by blending original training data with newly generated data. FuseLLM (Wan et al., 2024) transfers knowledge to the target LLM using probability distributions generated by the source LLM. Finally, the evolved models require evaluation. ChatEval (Chan et al., 2024) explores the strengths and weaknesses of model outputs through a debate mechanism. LLM-as-a-judge (Zheng et al., 2023) employs large models to evaluate other large models, demonstrating that LLMs can match human judgment and achieve efficient performance evaluation."}, {"title": "6 Conclusion", "content": "In this study, we explore innovative strategies for enhancing domain-specific capabilities of LLMs through knowledge distillation and self-evolution. We propose the Meteor method, which leverages weak-to-strong data distillation, iterative training, and self-evolution to improve model performance across various metrics. Our approach is validated using data from advanced computer education, with experiments demonstrating significant improvements in accuracy, completeness, relevance, coherence, and reliability. By aligning the knowledge distribution between strong and weak models, we achieve more efficient domain knowledge distillation. Furthermore, our self-evolution method enables models to autonomously refine their capabilities without relying on external models, ensuring adaptability and robustness."}, {"title": "7 Limitations and Future Works", "content": "Despite the significant advances achieved in domain-specific model evolution, several limitations warrant further investigation and improvement in future research."}, {"title": "7.1 Limitations", "content": "The primary limitation lies in our approach to validating the distributional discrepancy between strong and weak models. Although we have successfully demonstrated performance improvements through weak-to-strong knowledge distillation, thereby indirectly supporting our hypothesis of domain-specific distributional differences, we lack direct experimental evidence to quantify and visualize these differences. This indirect validation methodology may not fully capture the specific characteristics and extent of the distributional disparities, thus limiting our comprehensive understanding of model behaviors.\nFurthermore, the proposed self-evolution methodology demonstrates room for improvement in practical applications. While designed to reduce dependence on strong models, current experimental results indicate limited performance gains. This limitation suggests the necessity for further optimization of our self-evolution strategies to achieve more substantial improvements."}, {"title": "7.2 Future Works", "content": "Building upon these limitations, we propose several crucial directions for future research:\nFirstly, we plan to conduct more rigorous analysis through neural probing techniques to directly quantify the distributional differences between strong and weak models. Specifically, we will track and compare the internal neural activation patterns and token distribution characteristics of both model types when processing domain-specific tasks. This approach will provide more direct experimental evidence, facilitating better understanding and validation of our distributional difference hypothesis.\nSecondly, we are committed to developing more efficient self-evolution methodologies. Current research suggests the possibility of surpassing the performance ceiling of strong models, motivating us to explore more advanced self-evolution strategies. Our objective is to design methods capable of continuous performance enhancement, ultimately achieving autonomous evolution that surpasses strong model guidance.\nFinally, considering the generic nature of the Meteor approach, we intend to expand its application scope. This includes validating its effectiveness across a more diverse range of domain-specific scenarios and exploring compatibility with various foundation models. Through this extensibility study, we aim to further validate the universality of the Meteor approach and explore its potential in broader application contexts."}]}