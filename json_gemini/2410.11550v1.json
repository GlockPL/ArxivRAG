{"title": "Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for Drug Development", "authors": ["Tengfei Ma", "Xuan Lin", "Tianle Li", "Chaoyi Li", "Long Chen", "Peng Zhou", "Xibao Cai", "Xinyu Yang", "Daojian Zeng", "Dongsheng Cao", "Xiangxiang Zeng"], "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable performance in general tasks across various fields. However, their effectiveness within specific domains such as drug development remains challenges. To solve these challenges, we introduce Y-Mol, forming a well-established LLM paradigm for the flow of drug development. Y-Mol is a multiscale biomedical knowledge-guided LLM designed to accomplish tasks across lead compound discovery, pre-clinic, and clinic prediction. By integrating millions of multiscale biomedical knowledge and using LLaMA2 as the base LLM, Y-Mol augments the reasoning capability in the biomedical domain by learning from a corpus of publications, knowledge graphs, and expert-designed synthetic data. The capability is further enriched with three types of drug-oriented instructions: description-based prompts from processed publications, semantic-based prompts for extracting associations from knowledge graphs, and template-based prompts for understanding expert knowledge from biomedical tools. Besides, Y-Mol offers a set of LLM paradigms that can autonomously execute the downstream tasks across the entire process of drug development, including virtual screening, drug design, pharmacological properties prediction, and drug-related interaction prediction. Our extensive evaluations of various biomedical sources demonstrate that Y-Mol significantly outperforms general-purpose LLMs in discovering lead compounds, predicting molecular properties, and identifying drug interaction events. The source code is available at https://anonymous.4open.science/r/Y-Mol.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) including GPT-4 (OpenAI 2023), ChatGLM (Zeng et al. 2022a), and LLAMA (Touvron et al. 2023) have achieved great success in various applications (Fang et al. 2023; Lyu et al. 2023) due to their powerful reasoning capability. These models boasting billions of parameters, are meticulously trained on large text corpora and excel at generating human-interactable text and easy-to-understand contexts. While LLMs have shown promising performance in general tasks, they are limited in the capability of domain applications (Zhang et al. 2023; Zhou et al. 2024; Luo et al.). In order to make LLM applica-"}, {"title": "Background", "content": ""}, {"title": "Biomedical Knowledge Graph", "content": "We define a biomedical knowledge graph (KG) as $G_{kg} = \\{(h,r,t) | h,t \\in E, r \\in R\\}$, where each triple (h,r,t) denotes a relation r between biomedical entities h and t. Biomedical KGs contain large-scale interactions between various entities, including structural drug-related gene expression and therapeutic information, which improve the reasoning ability of LLMs for drug development."}, {"title": "Method", "content": "In this section, we start with a brief introduction to our model architecture, followed by the corpus collection from biomedical publications, the instruction construction from molecule-text pairs (i.e., drug databases), the biomedical"}, {"title": "Corpus of Publication", "content": "To fully explore the potential biomedical knowledge from publications, such as the interaction pathway of drugs and"}, {"title": "Instruction Construction", "content": "To make Y-Mol applicable to drug development as a foundational model, we designed multiple types of instructions to finetune it."}, {"title": "Downstream Tasks", "content": "To validate the effectiveness of our proposed Y-Mol for drug development, we design various tasks across lead compound discovery, pre-clinic, and clinic predictions. Specifically, we introduce four groups of tasks to evaluate it from different stages of drug development: (1) Virtual Screening and Drug Design for lead compound discovery; (2) Property Prediction for identifying physical and chemical properties of discovered lead compounds in the pre-clinic stage; (3) Drug Interaction Prediction for predicting potential adverse drug events in clinic stage. Below we detail these downstream tasks in the context of Y-Mol."}, {"title": "Virtual Screening", "content": "Virtual screening (Ahmed et al. 2023) refers to the search for potential new therapeutic effects from known drugs or targets. This usually involves the prediction of drug-target interaction (Ma et al. 2023, 2022; Pan et al. 2022). In this paper, we consider drug-target interaction (DTI) prediction as a downstream task to validate the performance of discovering lead compounds. Specifically, given a drug d and a target (i.e., protein/gene) p, we construct a query to predict whether the drug d can interact with the target p based on Y-Mol. For example, given the drug-target pair (Acetadote, Glutathione synthetase), we create a query as follows:"}, {"title": "Supervised Finetuning", "content": "Supervised finetuning (SFT) is a technique used to adapt a pretrained LLM to a specific downstream task using labeled data. As shown in Figure 4, we adopt the generated instructions as the supervised inputs and feed them into Y-Mol to finetune a well-performance LLM. Specifically, we input the constructed prompt contexts and questions into Y-Mol and utilize the built answers to supervise the generated outputs from it. After finetuning Y-Mol based on constructed instructions, we employ it in downstream tasks across lead compound discovery, pre-clinic, and clinic predictions."}, {"title": "Drug Design", "content": "Drug design aims to generate effective compounds for specific conditions (e.g., the target of disease). In this paper, we view the drug design as a generation task by creating efficient molecular SMILES from Y-Mol. The generated molecules can be adopted as lead compounds in the discovery stage. Specifically, given a target (i.e., the condition) and a paragraph of description as a query, Y-Mol generates a target molecule with SMILES sequence from the context of the query. The generated SMILES can be considered as the designed target molecules. Note that this description can be a constraint on the molecules to be generated in terms of their properties (e.g., LogP and drug-likeness) and functions (e.g., antimicrobial)."}, {"title": "Property Prediction", "content": "Property prediction of molecules is a fundamental task for drug development in the pre-clinic stage. In this paper, we focus on the physical and chemical properties that are crucial to drug discovery. To fully consider the drug-related expert knowledge, we adopt the available expert small models (e.g., ADMETlab) to infer possible properties and distill this knowledge into Y-Mol. Given a molecule and the property to be predicted, we construct a query as a line of natural description to ask Y-Mol the possible answers."}, {"title": "Drug Interaction Prediction", "content": "Identifying potential drug interaction events is a key task for safe drug use for humans in the clinical stage. Similar to the virtual screening, we primarily use the coherent reasoning chains within the biomedical knowledge graph to extract the context of the corresponding drug interaction. The extracted context is adopted to prompt the question \"What are the side effects for humans taking both drugs a and b?\" as a query. Subsequently, we feed the query into Y-Mol and ask the specific answers."}, {"title": "Experiments", "content": "In this section, we carefully design some key experiments to evaluate the proposed Y-Mol."}, {"title": "Datasets", "content": "Training. In this paper, we pre-train Y-Mol using two types of datasets: (1) biomedical text corpus of publications and (2) supervised instructions constructed from biomedical knowledge graphs, and inference data from expert models. For the biomedical corpus, we mine the abstract text and brief introduction from online publications. To construct high-quality instructions, we extract the context of specific facts as queries from biomedical knowledge graphs Het-ionet (Himmelstein et al. 2017) and DRKG (Ioannidis et al. 2020). In addition, to collect large-scale instructions based on drug-related properties and domain knowledge, we infer the input-output pairs from expert small models to build question-answer pairs. Finally, we collect ~ 11.2M corpus and ~ 2.3M instructions. We show the data distribution of Y-Mol across different tasks in Figure 5. \nInference. To evaluate the performance of Y-Mol on drug-target interaction (DTI) prediction and drug-drug interaction (DDI) prediction, we adopt the widely-used benchmarks DrugBank (Knox et al. 2024) and Drug Central (Avram et al. 2023) for DTI prediction. Meanwhile, we utilize Ryu's (Ryu, Kim, and Lee 2018) and Deng's (Deng et al. 2020) datasets to assess DDI prediction."}, {"title": "Evaluation", "content": "We design various downstream tasks to evaluate the effectiveness of Y-Mol in drug development. Specifically, we view drug-target prediction (i.e., virtual screening) as a Yes or No question for Y-Mol, which is similar to a binary classification task. Similarly, we consider the drug-drug interaction prediction as a classification task, identifying whether the interaction exists or not. Therefore, we adopt the receiver operating characteristic curve (ROC-AUC) to evaluate Y-Mol. In addition, we model the property predictions as regression tasks, which predict the value of specific properties. We utilize R-square ($R^2$) to assess the prediction capability of Y-Mol. In contrast, the drug design is considered a generation task and we adopt the standard metrics Valid, Unique, Novelty, and Diversity to evaluate the Y-Mol. To evaluate Y-Mol, we split DTIs and DDIs in each dataset into training and test sets with a ratio of 9:1. For the evaluation of drug design, we randomly sample 200,000 data to assess the generation quality of drugs with single- or multiple-objective. The detailed evaluation settings of drug design can be found in Appendix B.3. Meanwhile, we carefully conduct experiments to measure the predictive performance of 12 chemical and physical properties. For every property, we split the corresponding data into training and test sets with a 9:1 ratio. For a fair comparison, we adopt LLaMA2-7b as the baseline method. More details can be found in Appendix B.2."}, {"title": "Main Results", "content": "DTI prediction. Identifying unknown drug-target interactions is crucial to virtual screening. As shown in Table 1, our proposed Y-Mol outperforms LLaMA2 with an improvement of 5.02% and 4.13% on the AUC score over DrugBank and DrugCentral datasets, respectively. The results demonstrate that Y-Mol with supervised tuning for biomedical knowledge across multiscale data sources has a positive enhancement on DTI prediction. This shows that Y-Mol has a superior performance for virtual screening. \nDrug Design. To validate the performance of Y-Mol in discovering novel lead compounds, we introduce the drug design task. As depicted in Table 2, the overall performance of Y-Mol achieves a superior level. In contrast, the LLaMA2-7b can not generate valid molecules with a bad capacity for domain adaptation. As mentioned in Section Method, the instructions constructed from expert models can introduce"}, {"title": "Conclusion", "content": "We proposed a multiscale knowledge-guided LLM to build a paradigm for drug development, called Y-Mol. To align the perceptions of drug development between experts and Y-Mol, we build a large-scale biomedical corpus from publications and instructions to distill the drug-related knowledge spectrum into the LLM, alleviating the data acquisition and annotating. Experiments demonstrate Y-Mol outperforms LLaMA2-7b in various tasks. In future work, we will promote Y-Mol to the cellular expression level."}, {"title": "B Biomedical Knowledge Graph", "content": ""}, {"title": "C Additional Experiments", "content": ""}, {"title": "A.1 Related topics for mining texts from online\npublications", "content": "To emphasize the key information about drugs, we select\nthe most related topics to mine meaningful texts. Specif-\nically, we select publications within the areas of Chemistry,\nComputer Science, and Bioinformatics. The data distribu-\ntions of them are shown in Table 3. All datasets are available\nat https://anonymous.4open.science/r/Y-MOL"}, {"title": "A.2 The overall data distribution for training", "content": "In this paper, we design an LLM paradigm for drug devel-\nopment by learning large-scale biomedical knowledge from\npublications, knowledge graphs (KGs), and expert models\nor tools. The data distribution is shown in Table 4. Y-Mol\nlearning from the large-scale text corpus within publications\ncontains enough background knowledge to facilitate finer-\ngrained finetuning."}, {"title": "B Details of Experimental Settings", "content": ""}, {"title": "B.1 Meanings of predicted properties", "content": "We show the corresponding meanings of properties in the\nproperty prediction task in Table 5. These adopted proper-\nties are crucial to drug metabolism in the flow of drug devel-\nopment."}, {"title": "B.2 Implementation Details", "content": "In the pretraining stage, we set the $Ir  1.2 \u00d7 10^{-5}$,\nthe number of layers is 32, and the hidden size is 4096.\nThe corresponding config of hyperparameters is available at\nhttps://anonymous.4open.science/r/Y-MOL/config.txt.\n During the supervised finetuning process, we set the\nlearning rate to $Ir = 4 x 10^{-5}$ and the number of iter-\nations to 3. The batch size was set to 8. Considering the\nmemory limitations and the size of the model, we employed\nthe gradient accumulation technique by setting the gradient\naccumulation steps parameter to 4, which accumulates the\ngradients equivalent to a batch size of 32 per iteration, sim-\nulating the effect of training with a larger batch size. We"}, {"title": "B.3 Details of Drug Design", "content": "In the pretraining stage, we set the $Ir  1.2 \u00d7 10^{-5}$,\nthe number of layers is 32, and the hidden size is 4096.\nThe corresponding config of hyperparameters is available at\nhttps://anonymous.4open.science/r/Y-MOL/config.txt.\n During the supervised finetuning process, we set the\nlearning rate to $Ir = 4 x 10^{-5}$ and the number of iter-\nations to 3. The batch size was set to 8. Considering the\nmemory limitations and the size of the model, we employed\nthe gradient accumulation technique by setting the gradient\naccumulation steps parameter to 4, which accumulates the\ngradients equivalent to a batch size of 32 per iteration, sim-\nulating the effect of training with a larger batch size. We"}, {"title": "C.1 Case of drug design on LLaMA2-7b", "content": "We show some cases generated from LLaMA2-7b in Fig-ure 9. As shown in Figure 9, we observe that Y-Mol per-forms well in both cases, while LLaMA2-7b can not under-stand the question well and answer some relevant context.This phenomenon indicates that Y-Mol learning large-scalebiomedical knowledge can effectively enhance the capabil-ity of LLM in drug development."}]}