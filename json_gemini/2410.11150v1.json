{"title": "Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems", "authors": ["Anis Redjdal", "Luis Pinto", "Michel Desmarais"], "abstract": "Session-based recommendation is the task of predicting the next item a user will interact with, often without access to historical user data. In this work, we introduce Sequential Masked Modeling, a novel approach for encoder-only transformer architectures to tackle the challenges of single-session recommendation. Our method combines data augmentation through window sliding with a unique penultimate token masking strategy to capture sequential dependencies more effectively. By enhancing how transformers handle session data, Sequential Masked Modeling significantly improves next-item prediction performance.\nWe evaluate our approach on three widely-used datasets, Yoochoose 1/64, Diginetica, and Tmall, comparing it to state-of-the-art single-session, cross-session, and multi-relation approaches. The results demonstrate that our Transformer-SMM models consistently outperform all models that rely on the same amount of information, while even rivaling methods that have access to more extensive user history. This study highlights the potential of encoder-only transformers in session-based recommendation and opens the door for further improvements.", "sections": [{"title": "1 Introduction", "content": "Traditional recommendation systems predominantly rely on a user's historical interactions and preferences [21]. However, when user identities are partially known or entirely anonymous, recommendations must be generated based solely on the actions taken within a single session. These session-based scenarios challenge traditional models, which depend heavily on extensive user-item interaction history to provide accurate recommendations.\nSession-Based Recommendation (SBR) models tackle this challenge by analyzing user behavior within a single session to predict future actions and make relevant recommendations. Each session is treated as an independent, ordered sequence of consecutive interactions with items, regardless of whether the same user appears in multiple sessions, focusing solely on the interactions within a specific context. Accurately modeling these sessions is crucial for improving recommendation relevance and providing a personalized, engaging user experience. For example, on an e-commerce website, a session might begin when the user logs in and conclude when they leave the site, capturing the sequence of items they viewed in chronological order.\nConsidering that the task of SBR is to predict the next item that might interest a user, the task closely resembles the challenges associated with predicting the next word in Natural Language Processing (NLP). This parallel is a natural motivation to explore the application of transformer models [32]. Just like in NLP, items are represented as tokens and processed as a sequence.\nIn this work, we introduce Sequential Masked Modeling (SMM), a novel approach for improving session-based recommendation using encoder-only transformer architectures. Inspired by the success of transformers in NLP, our method adapts these models to SBR by employing data augmentation through window sliding and a new masking strategy. Additionally, we optimize the transformer architectures to further enhance performance. We evaluate our proposed SMM method on three benchmark datasets: Yoochoose 1/64, Diginetica, and Tmall. Our Transformer-SMM models were evaluated against state-of-the-art models, demonstrating clear improvements when working with the same level of information. Even in comparison to models utilizing more user data, our approach remained competitive in terms of precision and ranking metrics. In the following sections, we describe our method, the optimization techniques applied, and evaluate our approach against existing benchmarks."}, {"title": "2 Related work", "content": "Early research in session-based recommendations primarily explored sequential models, with the Markov Chain (MC) model being one of the first to address this domain [8]. However, MC often struggles with capturing long-range dependencies in the user's behavior, as it tends to compress information from earlier interactions. As a result, its effectiveness diminishes as the sequence length increases.\nHidasi et al. [13] introduced a Recurrent Neural Network (RNN)-based method, which was later enhanced by incorporating data augmentation and accounting for temporal shifts in user behavior, as explored by [33]. Further developments in 2016 introduced the concept of integrating dwell time, the duration of item views, into session-based recommendations [14]. This enhancement allowed RNN models to more accurately capture user interest and engagement, thereby boosting their predictive performance. Building on this, NARM [19] introduced a dual-approach RNN recommendation system that captures both the sequential patterns of user behavior and their primary intentions. Despite the inherent challenges associated with RNNs, such as the difficulty in representing users with limited data or the complexity of modeling distant item transitions within a sequence, researchers have gradually improved this methodology over time, though the associated challenges remain significant.\nTo address these limitations, the introduction of Graph Neural Networks (GNN) in 2019 marked a major advancement in the field of SBR. Session-based GNNs [15, 38, 41] are specifically designed to generate representations within graphs of item interactions, making them particularly effective at capturing and representing the intricate patterns in these sequences. Since the introduction of SR-GNN [38], graph-based models have gained significant recognition in SBR, consistently delivering superior performance. Over time, models like CARES [42], STAR [1], COTREC [36], and GCE-GNN [35] have further refined this approach, positioning GNNs at the current state-of-the-art in SBR.\nTransformers have also achieved significant success in the SBR domain. In the RecSys Challenge 2022 [17], a team employing transformer architectures secured 2nd place among 300 participants [22]. Furthermore, Zhang et al. [41] introduced a novel approach that integrated a GNN with a transformer, using an attention mechanism to process sequences in graph form. This hybrid model, based on the BERT architecture [7], primarily utilizes an encoder to enhance session data representations. The fusion of graph-based models and transformer architectures marks a significant step forward in SBR, combining the strengths of both techniques.\nBuilding on the success of transformers in SBR, Meta, in collaboration with Nvidia, developed a dedicated library that provides the most popular transformer architecture for session-based recommendation [6]. While their reported performance on benchmarks falls short of GNN-based models, this library offers a useful solution by streamlining session data preprocessing and transformer language model training into a single pipeline."}, {"title": "3 Preliminaries", "content": "First, we formally define the task of next-click prediction in session-based recommendations. Let \\(V = \\{o_1, o_2, ..., o_m\\}\\) represent the set of all items, where m denotes the number of items in V. Assume that all sessions are represented as \\(U = \\{S_1, S_2, ..., S_n\\}\\), where n is the total number of sessions. Each session \\(S_t\\)\u2208 U, denoted as \\(\u03a3_\u03c4 = \\{\u03c5_{\u03c41}, \u03c5_{\u03c42},..., U_{rt}\\}\\), consists of a sequence of interactions in chronological order, where \\(U_{rt}\\) represents the item interacted with by the user at the t-th timestamp in session \\(S_t\\), and the length of \\(S_r\\) is t.\nThe goal of SBR is to recommend the next item from V that the user is most likely to interact with, given the current session \\(S_t\\). Specifically, the item interacted with at the (t + 1)-th timestamp is referred to as the target item or ground truth item of the session. Thus, a session and its target item pair can be expressed as ([01, 02, ..., Ut], Ut+1)."}, {"title": "3.2 Language Models", "content": "Motivated by the success of transformers in NLP, where they perform exceptionally at capturing sequential dependencies and contextual information, we apply these models to session-based recommendation. With their ability to model long-range dependencies through the attention mechanism, transformers have proven highly effective in tasks like next-word prediction, and this same architecture can be adapted to predict the next item in a user's browsing session. While the data and context differ from NLP, the underlying principle of predicting the next interaction based on past behavior remains similar, as shown in Figure 1. For the experiments and evaluations conducted in this work, we focused on three popular transformer models: two encoder-only models, BERT [7] and DeBERTa [12], and one decoder-only model, GPT [18].\nThese language models are trained with a technique called masking. This approach helps the model learn contextual relationships and dependencies between tokens by hiding parts of the input sequence and requiring the model to predict the missing elements. This method is particularly effective for capturing patterns and structure within sequences, making it essential for a variety of tasks, including session-based recommendations. Two masking techniques are commonly used: Masked Language Modeling (MLM) and Causal Language Modeling (CLM)."}, {"title": "3.2.1 Masked Language Modeling", "content": "This is a training technique where a random selection of tokens within a sequence is masked, and the model is trained to predict the masked tokens based on the surrounding unmasked context. By leveraging both preceding and following tokens, this approach enables the model to learn bidirectional contextual representations, enhancing its ability to capture the full context of the input data. The models that employ this masking approach are BERT and DeBERTa. The corresponding loss function is as follows:\n\\(L_{MLM} = - \\sum_{i \\in M} log P(x_i|x_{\\backslash i})\\)\nwhere M is the set of masked positions, \\(x_i\\) is the token at position i, and \\(x_{\\backslash i}\\) represents all tokens except \\(x_i\\)."}, {"title": "3.2.2 Causal Language Modeling", "content": "Also known as autoregressive modeling, trains the model to predict each token in a sequence using only the preceding context. Since each prediction is based on the tokens preceding the target token, the model is limited to unidirectional attention. The model that we used for this masking technique is GPT, trained with the following loss function:\n\\(L_{CLM} = - \\sum_{i=1}^{n} log P(x_i | X_1, X_2, ..., X_{i-1})\\)\nwhere \\(x_i\\) represents the i-th token in the sequence, and n is the total number of tokens. The model estimates the likelihood"}, {"title": "4 The proposed method", "content": "In this section, we introduce Sequential Masked Modelling (SMM), a new approach designed to improve next-click prediction in session-based recommender systems. This method employs a novel masking technique during training to enhance performance.\nThe SMM approach consists of two key principles: data augmentation with window sliding, and masking the penultimate token of the augmented sequences. This masking approach is specifically applied to encoder-only transformer models, as it leverages their bidirectional attention mechanism."}, {"title": "4.1 Data Augmentation with Window Sliding", "content": "In transformer models, the input sequence length is constrained by a parameter known as max_len, which controls the maximum number of tokens the model can process in a single pass. Due to computational limits and the statistical properties of the input data, sequences that exceed this length must be truncated or split into smaller chunks. In many cases, shrinking the window to fit within this constraint is acceptable without compromising the model's ability to learn effectively.\nTo address this, we implement a sliding window technique to divide sequences into fixed-size chunks based on the max_len parameter. This ensures that the model can handle variable-length sequences while adhering to computational limits. Each session is represented as a sequence of items with a minimum length of two\u00b9. For sequences longer than two items, subsequences of length n - 1 are generated by progressively removing the last item at each step, where n is the total number of items in the sequence.\nThe sliding window always starts at the end of the sequence and moves leftward, generating subsequences of length max_len. This allows all relevant parts of the sequence to be included in training and evaluation, regardless of the original sequence length. If the sequence length is shorter than max_len, the entire sequence is used.\nGiven an initial sequence \\(s = (x_1, x_2, . . ., x_n)\\), the sliding window moves along the sequence to generate subsequences as follows:\n\u2022 Step 1: The first subsequence is extracted from the end of the initial sequence. If the sequence length is shorter than max_len, the subsequence contains all elements. For example,\n\\(s_1 = (x_{n-\\text{max len}+1},...,x_n)\\).\n\u2022 Step 2: The window then shifts one step to the left to create the next subsequence. For example,\n\\(s_2 = (x_{n-\\text{max\\_len}},..., x_{n-1})\\).\n\u2022 Step 3: This process continues, with the window sliding leftward until the window reaches the beginning of the initial sequence. For example, \\(s_3 = (x_{n-\\text{max\\_len}-1},..., x_{n-2})\\), and so on.\nThrough this method, each token in the initial sequence s appears in multiple subsequences, allowing the model to encounter each token in varied training contexts. The general form of each subsequence \\(s_i\\) can be defined as:\n\\(s_i = (x_{n-\\text{max\\_len}-i+2},...,x_{n-i+1}) \\text{ for } i = 0, 1, 2, . . ., n-\\text{max\\_len}\\)"}, {"title": "4.2 Masking the Penultimate Token", "content": "In traditional sequence-based tasks, the last item in a sequence is masked, and the sequence, truncated to the model's maximum input length (max_len), is used for prediction. In our approach, we retain this strategy for the base sequence, where we input the original sequence truncated to max_len and mask the last item as usual.\nHowever, with the introduction of data augmentation through sliding windows, we adopt a different masking strategy for the augmented sequences. Instead of masking the last item, as done in the base sequence, we mask the penultimate token in each augmented subsequence. This method enables the model to learn from both the preceding context and minimal future information, resulting in more varied and informative training examples. Specifically, for each input sequence \\(s = (x_1, x_2,...,x_n)\\), the sliding window generates subsequences of length max_len, where the penultimate token \\(x_{i-1}\\) is masked. The loss is then computed based on the model's ability to predict the masked penultimate token using the surrounding context.\n\\(L_{SMM} = -log P(x_{i-1}|X_1, X_2, ..., X_{i-2}, X_i)\\)\nThis masking strategy takes advantage of the bidirectional attention in encoder-only transformer models, such as BERT, allowing the model to leverage both the preceding context and the limited right-hand context (provided by the next element in the sequence). Although real-world next-click prediction tasks lack future context, masking the penultimate token proves useful in scenarios where item order is flexible, such as in e-commerce, where item proximity is often more important than strict order.\nBy combining data augmentation with this penultimate token masking strategy, we ensure that each token in the sequence is masked at least once during training. This complements the base sequence where the last item is masked, allowing the model to adapt to both the original and augmented contexts."}, {"title": "4.3 Hypotheses", "content": "4.3.1 Advantages of SMM Over CLM. We hypothesize that one of the main advantages of our SMM approach compared to the Causal Language Modeling used in the GPT architecture is that the attention mechanism changes significantly. In a CLM model, attention is unidirectional, meaning each token can only look at the tokens that precede it. This limits the model's ability to capture complex relationships between tokens, as the full context of the sequence is not considered.\nIn contrast, with SMM, each token in the sequence can look anywhere, thanks to the data augmentation that generates subsequences. This allows the model to learn richer and more contextual representations, as each token has access to the entire sequential context and therefore will have a more accurate attention score.\n4.3.2 Advantages of SMM Over MLM. One of the key advantages of our SMM approach over the Masked Language Modeling method lies in its handling of sequences longer than the model's max_len parameter. In models trained with MLM, when input sequences exceed the max_len, they must be split into smaller segments. This splitting breaks the continuity between tokens that span across sequence boundaries, resulting in the loss of important contextual information and potentially diminishing model performance.\nWhile overlapping subsequences could mitigate this issue, random masking in MLM introduces another challenge: a bias toward the tokens in the middle of the sequence. These tokens are more likely to be masked in multiple overlapping segments, whereas tokens near the sequence boundaries are underrepresented. This imbalance can negatively impact the model's ability to learn representations for tokens at the start or end of a sequence.\nRather than breaking the sequence, SMM generates subsequences that maintain continuity across the full sequence during training, avoiding these limitations and allowing for a more balanced representation of tokens throughout.\n4.3.3 Masking the penultimate instead of the last. The SMM training approach allows the model to take advantage of a small amount of right-side context thanks to the bidirectional attention used in encoder-only transformer architectures, which helps during training. Even though in practice, predicting the next item doesn't have right-side context (since we don't have access to what comes next), this method works well in situations where the order of items is not strictly fixed. It's more about the proximity of item pairs in e-commerce data, unlike linguistic data where word order is more rigid."}, {"title": "5 Optimizing techniques", "content": "In this section, we present several techniques that we introduce to the transformer architectures, which contributed to further improving prediction performance. All the optimizations are implemented across all three transformer models, except for Contextual Positional Encoding, which is only applied to BERT due to compatibility constraints with the attention block. Ablation studies for these optimization techniques, detailing their individual impact, can be found in Appendix A.1."}, {"title": "5.1 Weight Tying", "content": "Weight tying [26] technique is used in transformer models to reduce the number of parameters and enhance performance. It involves using the same weights for both the input embeddings and the model's output layer. In other words, the weight matrices used to encode tokens into embeddings are the same as those used to predict tokens from these representation vectors at the output of our encoder. By sharing the same weight matrix for the embeddings and the output layer, not only is the parameter count reduced, but the model's generalization is also improved by linking the input and output embeddings."}, {"title": "5.2 Pre-layer Normalization", "content": "Pre-layer normalization is the second technique implemented across all compatible transformer architectures (GPT, BERT, and DeBERTa). Large Language Models (LLMs), such as Mistral [16] or Llama [31], use pre-layer normalization, which applies normalization before the multi-head attention layer and the feed-forward layer. By normalizing before the attention operation, the model can better manage gradients during training, leading to more stable convergence. In our implementation, we use RMSNorm [40] instead of layer normalization [2], just like in previously discussed LLM architectures."}, {"title": "5.3 Contextual Positional Encoding", "content": "We implemented an enhancement to the positional embeddings known as Contextual Positional Encoding (CoPE) [10], which replaces the Rotary Position Embedding (RoPE) method [29] commonly used in popular LLMs. Unlike traditional methods that rely on simple token counting, CoPE determines token positions based on contextual information. For compatibility purposes, we applied this technique exclusively to our BERT architecture."}, {"title": "6 Evaluation and Experiments", "content": "The various techniques we developed were tested through a series of experiments to evaluate their effectiveness. We describe here the methodological details of the experiments."}, {"title": "6.1 Experimental Datasets", "content": "To evaluate and compare the performance of our transformer models enhanced with our new masking approach and optimization techniques, we selected three publicly available datasets: Yoochoose 1/64 [39], Tmall [30], and Diginetica [9], which are popular datasets for the session-based recommendation task."}, {"title": "6.1.1 Data Preprocessing", "content": "The datasets were preprocessed to ensure the results are comparable with various studies in session-based recommendation, particularly those that are considered state-of-the-art and use GNNs [38, 42]. The preprocessing involves three main steps to match the statistics shown in Table 1:\n\u2022 All sessions are ordered chronologically, and the data is split into training and test sets based on session timestamps.\n\u2022 We filter out items that appear less than five times or only appear in the test set, as well as sessions of length one.\n\u2022 Data augmentation is performed using a sliding window of size 30 to generate more data samples within a session. For a session [v1, v2, ..., vn], we generate samples ([v1, v2, ..., vn-1], vn), ..., ([v1, v2], v3), ([v1], v2)."}, {"title": "6.2 Implementation", "content": "We implemented all three transformer models (BERT, GPT, and DeBERTa) using the PyTorch library [24]. It was not possible to directly import pre-existing models because those models are pre-trained on large text corpora and not on e-commerce items specific to each of our three datasets. Additionally, having access to the internal code of these models was essential to implement recent techniques used by large language models. The code will be available after reviewing.\nThe three transformer models use the BERT Medium configuration. This configuration consists of 8 encoder layers (decoder for GPT), a hidden size of 512, 8 attention heads, and a total of 41 million parameters [25]. We chose this configuration for its balance between computational efficiency and performance. We found that larger configurations often result in overfitting due to the smaller size of e-commerce datasets compared to text corpora, while smaller configurations tend to underfit. The BERT Medium configuration offers a suitable compromise."}, {"title": "6.3 Metrics", "content": "The evaluations are based on two main metrics:"}, {"title": "6.3.1 Precision@20", "content": "P@20 is defined as the number of relevant recommendations within the top 20 results, divided by 20. It is a commonly used metric to evaluate the performance of recommendation systems. The formula is given by:\n\\(P@20 = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\text{relevant predictions at position } \\leq 20}{20}\\)\nwhere N is the total number of test sessions."}, {"title": "6.3.2 Mean Reciprocal Rank", "content": "MRR is a metric that evaluates the effectiveness of recommendation systems by considering the position of the first relevant item in the result list. The formula is given by:\n\\(MRR = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{rank_i}\\)\nwhere \\(rank_i\\) is the position of the first relevant item in the i-th session, and N is the total number of test sessions."}, {"title": "6.4 Masking Techniques Comparison", "content": "We evaluate and compare the performance of optimized transformer model architectures to determine which masking technique yields the best overall Precision@20 results. The models compared include GPT-CLM, BERT-MLM, DeBERTa-MLM, BERT-SMM, and DeBERTa-SMM, each using the respective masking strategies discussed earlier."}, {"title": "6.5 Sequentially Masking the Last K Items", "content": "In this section, we explain our decision to mask the penultimate token of each sub-sequence rather than the last one. Additionally, we explore whether masking the penultimate token yields better results compared to masking the third-to-last token. The results indicate that, in some cases, masking the third-to-last token can produce better outcomes. However, this is highly dependent on the level of disorder in the dataset. In an ideal scenario with perfectly ordered data, masking only the last token would likely be the most effective. Unfortunately, such ideal conditions are rare in SBR tasks, unlike in NLP, where data typically follows a more fixed sequence.\nFurthermore, the results confirm that masking the last token (K = 1) consistently yielded the poorest performance, indicating that this approach is rarely optimal."}, {"title": "7 Comparison with the State of the Art", "content": "In this section, we compare our optimized encoder-only transformer architectures trained with the SMM method-against various state-of-the-art models."}, {"title": "7.1 Types of Benchmark Approaches", "content": "In this section, we outline the different types of benchmark approaches used to compare against our models. These include single-session, cross-session, and multi-relation approaches, each varying in the amount of user session history and relational data they utilize for making recommendations [42].\nSingle-Session Approach: This approach considers each session independently, without taking into account information from other sessions. It primarily relies on the content of the current session.\nCross-Session Approach: Cross-session approach incorporates information from multiple past sessions of a user to enhance the performance of recommendations. It leverages the history of sessions to understand long-term user preferences and behavior trends. This approach may use session merging techniques, where past sessions are aggregated or encoded to provide additional context during recommendations.\nMulti-Relation Approach: This method considers various relationships between sessions and users to make more personalized and accurate recommendations. In other words, it uses information that goes beyond individual sessions, drawing from broader user-related data to generate recommendations.\nCross-session and multi-relation approaches benefit from access to more information during prediction, often leading to improved prediction performance. Since our prediction approach using the optimized transformer-SMM relies solely on single-session data, we focus on comparing its performance against state-of-the-art single-session approaches. Therefore, our performance analysis primarily benchmarks our method against other single-session models."}, {"title": "7.2 Benchmark Models", "content": "To evaluate the performance of our models, we compared them with 17 other models, spanning single-session, cross-session, and multi-relation approaches.\nSingle-Session Approach:\n\u2022 POP recommends the most popular items.\n\u2022 Item-KNN [28] recommends items based on the cosine similarity between the items in the current session and candidate items.\n\u2022 FPMC [27] uses both Markov chains and matrix factorization to incorporate personalized and general user information.\n\u2022 GRU4REC [13] leverages the memory of GRU by modeling the entire sequence.\n\u2022 NARM [19] and STAMP [20] use the attention mechanism to capture the user's current and general interest.\n\u2022 SRGNN [38] and LESSER [4] convert each session into a graph without using inter-session information.\nCross-Session Approach:\n\u2022 CSRM [34] incorporates relevant information from neighboring sessions via a memory network.\n\u2022 COSAN [23] uses a multi-head attention mechanism to build dynamic representations of items by merging item representations from collaborative sessions.\n\u2022 GCE-GNN [35] and MTD [15] simultaneously focus on both inter-session and intra-session dependencies.\n\u2022 COTREC [36] and S2-DHCN [37] employ a global argumentative view of items to extract informative self-supervision signals.\n\u2022 CARES [42] uses contextual information from multiple sessions to enhance recommendations, even during evaluation on the test set.\nMulti-Relation Approach:\n\u2022 AutoGSR [3] and MGIR [11] both learn multi-faceted item relations to improve session representation. Note that MGIR uses inter-session information, while AutoGSR does not."}, {"title": "7.3 Global Results Table", "content": "Table 4 compares the performance of our transformer models against several state-of-the-art models across the Diginetica, Tmall, and Yoochoose 1/64 datasets. The results highlight the effectiveness of our models, particularly those trained using the SMM method. Notably, BERT-SMM consistently outperforms all other models in the single-session approach category across the three datasets. When comparing our models to state-of-the-art cross-session and multi-relation methods, BERT-SMM remains highly competitive. DeBERTa-SMM also demonstrates competitive results, coming in just behind BERT-SMM in most cases. These results underscore the strong performance of our optimized transformer models, particularly BERT-SMM, even when compared to models that have access to more comprehensive session or user-level data. The ability of our models to outperform in the single-session category demonstrates the effectiveness of the SMM method. With further enhancements, such as incorporating user-level or multi-session data, we believe our models have the potential to rival and even surpass state-of-the-art cross-session and multi-relation approaches."}, {"title": "8 Conclusion", "content": "In this paper, we introduced Sequential Masked Modeling (SMM), a novel masking technique specifically designed for encoder-only transformer models, along with broader architectural improvements that were applied to both encoder- and decoder-based models. By using data augmentation through sliding windows and masking the penultimate token in augmented sequences, SMM significantly improved next-click prediction performance. In combination with the architectural enhancements, we demonstrated strong performance gains in several key metrics across three widely-used session-based recommendation datasets: Yoochoose 1/64, Diginetica, and Tmall.\nOur experimental results showed that the proposed BERT-SMM and DeBERTa-SMM models consistently outperformed traditional single-session approaches and remained competitive with state-of-the-art cross-session and multi-relation methods, despite being limited to single-session data. These findings validate the effectiveness of the SMM technique in capturing sequential dependencies and improving recommendation performance in session-based environments.\nFurthermore, the success of BERT-SMM and DeBERTa-SMM in the single-session category suggests that there is significant potential for enhancing performance even further by incorporating additional context, such as cross-session data or multi-relational information. Future work could explore how these models perform when extended to utilize user-level histories, or how SMM can be adapted to improve performance in other recommendation domains. Overall, this study provides strong evidence for the efficacy of encoder-only transformer architectures in session-based recommendation tasks when enhanced by SMM, offering a promising direction for future research in this area."}, {"title": "A.1 Ablation study", "content": "Our experiments demonstrate that the improved BERT architecture, combined with the SMM technique, delivers the best overall performance across the three datasets. In this section, we present an ablation study to assess the individual contribution of each optimization technique integrated into BERT-SMM. The results of this analysis, visualized in Figure 3, show how progressively adding these optimizations boosts performance on Diginetica.\nDifferent categories of ablation:\n\u2022 Base: This represents the standard BERT architecture as described in the original paper [7], trained with the SMM objective.\n\u2022 Weight Tying: Reduces the number of parameters by sharing weights between input embeddings and the output layer.\n\u2022 Pre-layer Normalization: Stabilizes training and improves convergence by applying normalization before attention layers.\n\u2022 Contextual Positional Encoding: Enhances positional embeddings by incorporating contextual information instead of fixed token positions."}, {"title": "A.2 Optimized BERT Architecture", "content": "shows the optimized BERT architecture that incorporates several key enhancements. These include pre-normalization with RMSNorm, Weight Tying, and Contextual Positional Encoding within the attention module. This design draws inspiration from modern transformer architectures, such as Llama [31], to boost performance and stability."}, {"title": "A.3 Hyperparameter Setup", "content": "Our model configuration is based on the BERT-Medium architecture from HuggingFace [25]. The training setup was consistent across all experiments to ensure fair comparison. All hyperparameter values can be found in Table 5, where we only conducted a search over the batch size. The model includes a maximum sequence length of 30, GeLU as the activation function, and a hidden size of 512. The optimizer used is Adam with an initial learning rate of 5e-5. Additionally, we applied a dynamic learning rate schedule that adjusts depending on the epoch, as detailed below.\nThe following learning rate scheduler was used: A custom learning rate schedule was applied using the function  if the epoch is 3 or higher, otherwise ."}]}