{"title": "STABLE AUDIO OPEN", "authors": ["Zach Evans", "Julian D. Parker", "CJ Carr", "Zack Zukowski", "Josiah Taylor", "Jordi Pons", "Stability AI"], "abstract": "Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model's performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopen13 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.", "sections": [{"title": "1. INTRODUCTION", "content": "A significant amount of ongoing research focuses on text-conditioned generative audio models [1, 2, 3, 4, 5, 6]. Yet, many models lack public weights or are only available behind private APIs [1, 5, 6], limiting their usefulness as the foundation for further research and artistic creation. Further, the licenses of the audio used for training public models are often not fully documented. For example, Audio-Gen [2] or AudioLDM [3, 4] are trained on a mix of public datasets that include AudioSet [7], and we are unaware of any disclosed licenses for AudioSet's audio. MusicGen [8], on the other hand, is an open model with well-documented training data and licences, trained exclusively on licensed\u00b9 copyrighted data. Finally, current open models are not competitive against the state-of-the-art in terms of quality, coherent generation over long sequences, and inference speed [5, 6]. Given that, our goal is to release a text-conditioned generative model for non-speech audio based on the following:\n\u2022 Trained only on Creative Commons (CC) licensed audio.\n\u2022 Publicly available model weights and code, along with the at-tributions needed for the used data, to facilitate open research.\n\u2022 State-of-the-art sound quality generation at 44.1kHz stereo.\nWhile this data choice may limit our model's capabilities (specially for text-to-music as noted in section 6.1) it facilitates transparent data practices at the time of releasing the model. This manuscript describes how these goals were achieved, including the description of our architecture (section 2), how we handle training data and ensure that such audio is not copyrighted (section 3), and how our model was trained (section 4). In section 5 we evaluate the resulting model and its parts (the generative model and the autoencoder) by a number of standard metrics and study whether it exhibits memorization of the training data. We also show that our model can run on consumer-grade GPUs. Our aim is to further improve current best practices for open model releases, with an emphasis on evaluation, data transparency, and accessibility for artists and scholars."}, {"title": "2. ARCHITECTURE", "content": "Our latent diffusion model generates variable-length (up to 47s) stereo audio at 44.1kHz from text prompts. It consists of 3 parts: an autoencoder (156M parameters) that compresses waveforms into a manageable sequence length, a T5-based text embedding [9] for text conditioning (109M parameters), and a transformer-based diffusion model (DiT of 1057M parameters) that operates in the latent space of the autoencoder. Our model is a variant of Stable Audio 2.0 [6] that is trained on CC data. Its architecture is similar except that it uses T5 [9] text conditioning instead of CLAP [10]. The model's exact parameterization and its weights are available online.2\n2.1. Autoencoder\nThe (variational) autoencoder operates on raw waveforms. The en-coder processes such waveforms with 5 convolutional blocks, each performing downsampling and channel expansion via strided con-volutions. Before each downsampling block, we employ a series of ResNet-like layers using dilated convolutions and Snake [11] acti-vation functions for further processing. The bottleneck of the au-toencoder is parameterized as a variational autoencoder with a latent size of 64. The decoder is an inversion of the encoder structure, em-ploying transposed strided convolutions for upsampling, with chan-nel contraction before each upsampling block. All convolutions are weight-normalised and the output of the decoder does not include a tanh() as we found it introduced harmonic distortion.\n2.2. Diffusion-transformer (DiT)\nOur generative model is a diffusion-transformer (DiT) [6, 12, 13] that follows a standard structure with stacked blocks consisting of serially connected attention layers and gated multi-layer per-ceptrons (MLPs), with skip connections around each. We employ bias-less layer normalization at the input to both the attention layer and the MLP. The key and query inputs to the attention layer have rotary positional embeddings [14] applied to only half of the em-beddings. Each transformer block also contains a cross-attention layer to incorporate conditioning. Linear mappings are used at the input and output of the transformer to translate from the autoencoder latent dimension to the embedding dimension of the transformer. Ef-ficient block-wise attention [15] and gradient checkpointing [16] are employed to reduce the computational and memory requirements.\nThe DiT is conditioned by 3 signals: text enabling language con-trol, timing enabling variable-length generation, and timestep signal-ing the current diffusion timestep. Text conditioning is provided by the pretrained T5-base encoder [9]. Timestep conditioning [5, 17] goes through sinusoidal embeddings [18]. Conditioning is intro-duced via cross-attention or via prepending conditioning signals to the input. Cross-attention includes timing and text conditioning. Prepend conditioning includes timing and timestep conditioning."}, {"title": "2.3. Variable-length audio generation", "content": "As natural audio can be of various lengths, our model supports variable-length audio generation within a specified window (e.g., 47s) by relying on the timing condition to fill the signal up to the specified length. The model is trained to fill the rest with silence. To present variable-length audios (shorter than the window length) to the end-users, one can easily trim the appended silence. We adopt this strategy, as it has previously shown its effectiveness [5, 6]."}, {"title": "3. TRAINING DATA", "content": "Our dataset consists of CC recordings from Freesound and the Free Music Archive (FMA). We conducted an analysis to ensure no copy-righted content was in our training data. To that end, we first iden-tified music recordings in Freesound using the PANNs [19] tagger. The identified music activated music-related tags for at least 30 sec (threshold of 0.15). This threshold was set with FMA music exam-ples and ensuring no false negatives were present. The identified music was sent to a trusted content detection company to ensure the absence of copyrighted music. The flagged copyrighted music was subsequently removed from our training set. Most of the removed content was field recordings in which copyrighted music was in the background. Following this procedure, we are left with 266,324 CC0, 194,840 CC-BY, and 11,454 CC Sampling+ audio recordings. \nWe also conducted an analysis to ensure no copyrighted content was present in FMA's subset. In this case, the procedure was dif-ferent because the FMA subset consists of only music. We did a metadata search against a large database of copyrighted music and flagged any potential match to be reviewed by humans. After this process, we ended up with 8,967 CC-BY and 4,907 CC0 tracks.\nThis led to a dataset with 486,492 recordings (7,300 h), where 472,618 (6,330 h) are from Freesound and 13,874 (970 h) from FMA, all licensed under CC-0, CC-BY, or CC-Sampling+. The most common tags in those Freesound recordings are single-note, synthe-sizer, field-recording, drum, loop, ambient and in FMA are instru-mental, electronic, experimental, soundtrack, ambient. This data is used to train most of our system (autoencoder and DiT, not the pub-licly available T5-base [9] that was pretrained) from scratch."}, {"title": "3.1. Autoencoder training data", "content": "We gathered 5 sec chunks of diverse, high fidelity audio. First, we gathered up to \u00d73 (random) chunks for each Freesound audio, to ensure diversity and avoid oversampling long recordings. Then, we gathered additional (random) chunks from a high fidelity sub-set, including all FMA tracks and a subset of stereo and full-band Freesound audio (55,314). Such high fidelity Freesound audio were recorded at 48kHz and verified to contain energy in high frequen-cies. FMA tracks were variable-bitrate MP3s encoded at 44.1kHz. Note that high fidelity Freesound recordings were sampled twice."}, {"title": "3.2. Prompts preparation for training the DiT", "content": "Training audio is paired with text metadata. Freesound examples in-clude natural language descriptions as well as the title of the record-ing and tags. FMA music examples include metadata like year, gen-res, album, title, and artist. We generate text prompts from the meta-data by concatenating a random subset of the metadata as a string. This allows for specific properties to be specified during inference, while not requiring these properties to be present at all times. For metadata-types with a list of values, like tags or genres, we shuffle the list. As a result, we perform a variety of random transformations to the resulting string, shuffling the order and also transforming be-tween upper and lower case. For half of the FMA prompts, we in-clude the metadata-type (e.g., artist or album) and join them with a comma (e.g., \"year: 2021, artist: dadabots, album: can't play instruments, title: pizza hangover\"). For the other half, we do not include the metadata-type and join them with a comma (e.g., \"dad-abots, can't play instruments, pizza hangover, 2021\")."}, {"title": "4. EXPERIMENTAL SETUP", "content": "All models are trained with AdamW, with weight decay of 0.001 and a learning rate scheduler including exponential ramp-up and decay. The exact training hyperparameters we used are detailed online.2\n4.1. Autoencoder training\nWe train the variational autoencoder using a variety of objectives. First, there is a reconstruction loss, based on a perceptually weighted multi-resolution STFT [23] that deals with stereo signals via the mid-side (M/S) representation of the stereo audio, as well as the left and right (L/R) channels. The L/R component is down-weighted by 0.5 compared to the M/S component, and exists to mitigate ambiguity around left-right placement. Second, we employ an adversarial loss term, utilizing 5 convolutional discriminators as in Encodec [22]. Third, the KL divergence loss term is down-weighted by $1 \\times 10^{-4}$.\nIt trained for 183 h on \u00d7 32 A100s with a batch size of 4. At this point the encoder was frozen, and the decoder was trained for another 273 h on \u00d732 A100s with a batch size of 8. Each batch is made of \u22481.5 sec chunks (65,536 samples at 44.1kHz). The autoen-coder itself was trained with a base learning rate of $1.5 \\times 10^{-4}$, and the discriminators with a learning rate of $3 \\times 10^{-4}$.\n4.2. DiT training and inference\nThe DiT is trained to predict a noise increment from noised ground-truth latents, following the v-objective [24] approach. During infer-ence, we use the DPM-Solver++ [25] for 100 steps with classifier-free guidance (scale of 0.7). The DiT is trained for 338 h on \u00d764 A100s with a batch size of 4 and a base learning rate of $5 \\times 10^{-5}$. Each batch contains latent sequences of length 1024 (\u224847 sec)."}, {"title": "5. EVALUATION", "content": "5.1. Generative model evaluation\nWe employ established quality metrics\u00b3 that include FDopenl3 [26], KLpasst [27] and CLAPscore [10, 28]. A low FDopenl3 implies that the generated audio is plausible and closely matches the refer-ence [29, 8]. A low KLpasst indicates semantic correspondence be-tween the generated and the reference audio [8]. A high CLAP score denotes that the generated audio adheres to the given text prompt [10, 28]. We use two evaluation sets: AudioCaps Dataset [30] for sound generation, and Song Describer Dataset [31] for music generation."}, {"title": "A. ADDITIONAL SONG DESCRIBER DATASET RESULTS", "content": "Tables 2 and 4 show the results for a subset of the Song Describer Dataset that includes only prompts for instrumental music\u00b9 [5, 6]. As vocal generation is not our focus and baselines were not trained for that either, the main body of the paper reports results on the instru-mental subset to ensure a fair evaluation. Yet, for completeness, this appendix also includes results on the entire Song Describer Dataset (Tables 6 and 7) with the same metrics as in Tables 2 and 4. Note that although results vary slightly, the trends remain consistent.\nA.1. Generative model evaluation\nTable 6 shows the Song Describer Dataset (all dataset) results. The dataset contains 1,106 captions for 706 tracks [31]. We generate an audio per caption, which results in 1,106 generations. Our results suggest that Stable Audio Open is worse than Stable Audio at gen-erating music, but slightly better than MusicGen (best open model).\nA.2. Autoencoder evaluation\nTable 7 shows the Song Describer Dataset (all dataset) results. We evaluate the reconstructed audio obtained by encoding-decoding the 706 tracks [31] from the dataset. Results show that lower latent rates generally yield worse reconstructions. Considering this, note that our model is only comparable to Stable Audio 2.0 (being continu-ous, with the same latent rate) and shows slightly worse performance despite being trained only with CC data."}, {"title": "B. VRAM CONSUMPTION DURING INFERENCE", "content": "During diffusion the DiT utilizes 5.9 GB VRAM. During decoding, rendering waveforms from latents, RAM usage increases to 14.5 GB.\nB.1. Chunk decoding\nDecoding waveforms from latents consumes significantly more VRAM than the DiT. To reduce the memory footprint of the de-coder, we explore chunk decoding by splitting the latent sequence into overlapping chunks, decoding them separately, and reassem-bling them into a final audio. As long as overlaps include the decoder's receptive field (16 latents on each side), the resulting audio is the same. The VRAM usage after chunking is as follows:"}, {"title": "C. FINETUNING ON VARIOUS HARDWARE", "content": "@RoyalCities changed the window length from 47 to 20 sec, and finetuned it on piano loops using \u00d72 RTX-A6000 (48GB)2.\n@_lyraaaa_changed the window length from 47 to 11 sec, and fine-tuned it on loops/oneshots using \u00d71 or \u00d72 RTX-A6000 (48GB)3."}]}