{"title": "Forecasting Disease Progression with Parallel Hyperplanes in Longitudinal Retinal OCT", "authors": ["Arunava Chakravarty", "Taha Emre", "Dmitrii Lachinov", "Antoine Rivail", "Hendrik Scholl", "Lars Fritsche", "Sobha Sivaprasad", "Daniel Rueckert", "Andrew Lotery", "Ursula Schmidt-Erfurth", "Hrvoje Bogunovi\u0107"], "abstract": "Predicting future disease progression risk from medical images is challenging due to patient heterogeneity, and subtle or unknown imaging biomarkers. Moreover, deep learning (DL) methods for survival analysis are susceptible to image domain shifts across scanners. We tackle these issues in the task of predicting late dry Age-related Macular Degeneration (dAMD) onset from retinal OCT scans. We propose a novel DL method for survival prediction to jointly predict from the current scan a risk score, inversely related to time-to-conversion, and the probability of conversion within a time interval t. It uses a family of parallel hyperplanes generated by parameterizing the bias term as a function of t. In addition, we develop unsupervised losses based on intra-subject image pairs to ensure that risk scores increase over time and that future conversion predictions are consistent with AMD stage prediction using actual scans of future visits. Such losses enable data-efficient fine-tuning of the trained model on new unlabeled datasets acquired with a different scanner. Extensive evaluation on two large datasets acquired with different scanners resulted in a mean AUROCs of 0.82 for Dataset-1 and 0.83 for Dataset-2, across prediction intervals of 6,12 and 24 months.", "sections": [{"title": "Introduction", "content": "Predicting the risk of disease progression is essential for prioritizing high-risk patients for timely treatment and clinical trial recruitment. However, this task is challenging due to several factors. First, the lack of well-established clinical biomarkers makes it difficult to predict future disease progression. Second, missing follow-ups or lack of the conversion onset within the study period can lead to unknown time-to-conversion labels. Third, only a small proportion of monitored"}, {"title": "Method", "content": "Proposed Disease Progression Formulation: The proposed method combines two distinct approaches of forecasting disease progression. First, we model the conversion time as a random variable T* with corresponding Cumulative Distribution Function (CDF) function p(t|\u2160) = P(T* < t|I) and I \u2208 I is a space of all possible images. Second, for each I with conversion time T we assign a risk scorer : I \u2192 R such that \u2200I1, I2 \u2208 I,T\u2081 < T2 : r(I1) \u2265 r(I2). The first interpretation estimates a patient's t-year survival likelihood (CDF) while the second can stratify a population into low, medium, and high risk groups by thresholding the risk score. Our proposed formulation illustrated in Fig. 1(a) combines both of these approaches.\nA CNN encoder maps each scan I to a point in the feature embedding space (represented as dots in Fig. 1(a)). Then, these features are fed into a linear iAMD vs. dAMD stage classifier with weights wand a scalar bias B. Notably, the w vector is normal to the classifier's decision hyperplane H such that f\u2208 H :\nw f + \u03b2 = 0. For a given f, its shortest signed distance perpendicular to H is proportional(scaled by a factor of ||w||) to w. f + \u03b2. The iAMD samples lie on the negative half-space (w. f + \u03b2 < 0) with negative signed distances from H.\nRisk score based on distance from H: We introduce a temporal ordering among the iAMD samples with a ranking loss Lrnk (details discussed below), such that the distance of each sample from H is inversely related to its conversion"}, {"title": null, "content": "time T*. This is illustrated in Fig. 1(a) by color grading each iAMD sample from red to blue in increasing order of T*. Rank ordering serves multiple objectives. First, it acts as a regularizer for learning a semantically meaningful feature space with a better chance of generalization when trained on limited labeled data. Second, the signed distance from H can be used as a risk score\nr = w f,\n such that the higher the r, the closer it is to conversion. \u1e9e being constant across all samples can be ignored for ranking. Risk score is further calibrated in a post-processing step to normalize to [0, 1]. After training, r is obtained for all scans in the validation set and a bicubic interpolation is learned to map the k-th percentile of r values to k/100 in increments of 10 percentiles.\nModeling CDF with a family of hyperplanes parallel to H: We propose a novel continuous-time modeling of the CDF to leverage the temporal ranking in the feature space. Predicting pt involves learning a separate linear binary classifier for each time-interval t to predict if I will convert within t. t is normalized such that 0 - 3 years is linearly mapped in the range [0,1]. We extend our formulation by considering a continuous family of separating hyperplanes H(t) parallel to H, each predicting the conversion within t (depicted with dashed lines in Fig. 1(a)). All hyperplanes in H(t) share the normal vector w but employ a different bias b(t), parameterized as a monotonic function of t. Thus,\npt(I) = \u03c3 (w f + b(t)) = \u03c3 (r + b(t)),\nwhere \u03c3(.) is the sigmoid function. We reformulated bias b(t) = a \u2022 t + \u03b2, where a and \u03b2 are scalar learnable parameters of our model. The hyperplane H for iAMD vs dAMD stage classification is a member of this family, H = H(t = 0). Thus, po = \u03c3 (w f + b(0)) = \u03c3 (w f + \u03b2) is the probability that the current input scan I has already converted to the dAMD.\nTraining Pipeline: We consider a Siamese architecture (Fig. 1(b)) during training to leverage the availability of longitudinal images by considering image-pairs from different eyes in each training batch. Each random image-pair (Ij, Ik) are two OCT scans of the same eye, acquired at different patient visits at time-points j and k. Ij precedes Ik (i.e., j < k) with a time-interval of (j-k) between them. Both Ij, Ik are fed to an Encoder (ConvNeXt-Tiny initialized with ImageNet pretrained weights [8]), to obtain the features fj, fk respectively. Their risk scores r; and rk are obtained using Eq. 1 and the probabilities p) and pok that I; and Ik have already converted to dAMD are computed using Eq. 2. Additionally, we compute the probability of I; to convert to dAMD within the next (k-j) time-interval as pk-j Pj = \u03c3 (rj + b(k \u2013 j)). Thus, while p and p essentially perform an iAMD vs. dAMD stage classification for the input scan, Pk; forecasts the conversion probability for a future time-point k, directly from a previous visit I; without accessing Ik.\nLoss Functions: Following survival analysis, the Ground Truth (GT) labels for each scan I; is denoted by the tuple (Tj, Ej). If the binary event indicator"}, {"title": null, "content": "E; = 1, then the eye to which I; belongs, converts to dAMD after a time-interval T; from the current visit. Ej = 0 indicates that the eye did not convert within the monitoring period in which case Tj represents time duration from the current to the last visit in the study after which the eye is censored.\nClassification Loss Lcls: The GT for iAMD vs dAMD classification for an eye at a time-point j is given by yj = 1 if it has already converted, i.e., Ej = 1 and Tj <= 0, otherwise yj = 0. The binary cross entropy loss Lbce(.) is used to define the classification loss as Lels = Lbce(yj, p\u00b3)) + Lbce (Yk, P(x)) + Lbce (Yk, Poj).\nIntra-Subject Consistency Loss Lens: For a given eye, the conversion probability at a time-point k predicted from the scan acquired at time k(p(*)) should be consistent with the probability forecast for k, using a previous scan from time-point j (p). This is ensured with the consistency loss Lens = Lbce (pk), pj).\nTemporal Ranking Loss Lrnk: We consider all possible image-pairs (Im, In) in a training batch (including pair of scans coming from different eyes). Lrnk solves a logistic regression task using the difference in risks (rm -rn) as input to a linear classifier to predict the probability pm<n of Im converting before In as\nLrnk =\n- Sm<n+15m>n: [sm<nlog (pm<n) + Sm>n log (1 - Pm<n)], where\nSm<n represents a subset of all possible image-pairs in a training batch where Im converts before In for which ideally, pm<n \u2248 1. Similarly, Sm>n contains image-pairs where Im converts after In and ideally, pm<n \u2248 0. The set Sm<n comprises image-pairs for which {(Tm < Tn) & (Em = 1 or Im, In belong to the same eye)}. AMD progression is irreversible and the retinal tissue damage only accumulates with time. Therefore, even for cases where E = 0 and the actual conversion time is unknown, the risk score of a scan from a later visit Im (with a smaller time duration Tm to the last visit) should always be higher than a former visit In (Tn > Tm) of the same eye. Similarly, Sm>n is defined as pairs where {(Tm > Tn) & (En = 1 or Im, In belong to the same eye)}.\nThus, the total loss is defined as L = Lcls + Lens + Lrnk with equal weights given to each term. While a Siamese two-branch architecture is employed during training, only a single branch is employed during inference. The proposed method employs a single visit's scan I as input to predict r (see Eq. 2) and the probability of conversion pt within a given future time-interval t (see Eq. 1).\nUnsupervised Fine-tuning on External Datasets: We adapt our training losses to facilitate unsupervised fine-tuning with unlabeled data. Lels requires GT labels and, therefore, is not used. The unsupervised loss Lens leverages the consistency in the predictions from the two branches of the Siamese architecture and is retained unmodified. Lrnk is adjusted in how Im, In pairs are constructed within each training batch. In the absence of conversion time labels, risk scores of scans across the batch samples cannot be compared. Only the intra-subject sample pairs Ij, Ik are utilized, as they should still be be ranked as rk > rj for time-points k > j as AMD being degenerative cannot regress with time.\nImplementation Details: Our method was implemented in Python 3.8, PyTorch 2.0.0 (code available at https://github.com/arunava555/Forecast_ parallel_hyperplanes ). The training comprised 200 epochs (with 300 batch updates per epoch, batch size of 16), employing the AdamW optimizer [9] with a"}, {"title": null, "content": "cyclic learning rate [17] varying between 10-6 to 10-4. Each training batch was constructed with random image-pairs (Ij, Ik) with a time-interval of 0-3 years between them, ensuring that all I; were in the iAMD stage, while half of the Ik in each batch were in the dAMD stage (through oversampling). Three consecutive B-scans (slices) out of the 5 central B-scans in the OCT volumes were randomly extracted and input to Encoder in place of the three RGB color chan- nels. Data augmentations included random translations, horizontal flip, random crop-resize, Gaussian noise, random in-painting and random intensity transfor- mations. During inference, for each scan, three sets of 3-channel input images were formed from the 5 central OCT slices, each containing the central slice in the left, middle or right channel. An average of their predictions were used for all experiments (including the benchmark methods for comparison)."}, {"title": "Experiments and Results", "content": "Datasets: We comprehensively evaluated our method on Dataset-1 collected at the Department of Ophthalmology, Medical University of Vienna, comprising 3,534 OCT scans from 235 eyes (40 converters and 195 censored) acquired with a Spectralis OCT scanner at a resolution of 49 B-scans (slices), each with a 512\n1024 \u00d7 49 px. For converter eyes, labels for each scan were computed by measuring the time interval between its acquisition and the first conver- sion visit. We considered an additional independent, external real-world dataset, Dataset-2, collected from two different sites (University Hospital Southampton and Moorfields Eye Hospital) from the PINNACLE consortium [18]. It comprises a randomly divided training set of 254 eyes (2428 scans) with 49 converters; a val- idation set of 127 eyes (1073 scans) with 26 converters; and a test set of 254 eyes (2305 scans) with 49 converters. All scans were acquired with Topcon scanners at a resolution of 128 B-scans with a 885 \u00d7 512 px. The scans from Dataset-1 and Dataset-2 exhibit large image domain shift due to different imaging scanners."}, {"title": null, "content": "8 converters. While the converted dAMD scans were also used during training, they were removed from the test set to focus on forecasting conversions from iAMD images alone. The Area under the ROC curve (AUROC) and Balanced Accuracy (B.Acc) were reported for predicting conversion within the next 6, 12 and 24 months. Concordance Index (CcI) was used to evaluate the risk scores r on their ability to provide a reliable ranking of the conversion time.\nAblation Experiments show that training with Lels and Lrnk (row 2) leads to a marginal improvement over training with Lels alone (row 1) across all time- points in terms of AUROC, B.Acc as well as CcI demonstrating the positive impact of imposing rank ordering. The proposed method additionally incorporates Lens which led to a considerable performance improvement in terms of CcI as well as the AU- ROC and B.Acc across all t Overall, the results demonstrate the value of using all loss terms.\nComparison with State-of-the-art was performed against popular survival analysis techniques in rows 3-7. These include discrete survival analysis methods utilizing censored cross-entropy loss (Cens. CE) from [21] and a logistic hazard model [12], both employing discrete 6-month time-windows for predicting conver- sion. Additionally, DeepSurv [4] extends the CoxPH model using Deep Learning, while SODEN [19] is a Neural-ODE based approach, originally explored for tab- ular data. These methods were implemented with the ConvNeXt-Tiny encoder by modifying the classification layers and losses. All of these methods do not em- ploy intra-subject regularization, hence require training a single branch network. SODEN showed signs of overfitting with good performance on the validation set (to select the best-performing models) but led to a drop in performance on the test sets in all folds. The results demonstrate the superior performance of our proposed method, outperforming all other methods across all time-intervals."}, {"title": null, "content": "Results on Dataset-2: We analyzed the effect of adapting our models pre-trained on Dataset-1, to Dataset-2 through unsupervised fine-tuning. While the validation and test sets remained fixed across all experiments, the training set size for fine-tuning was varied by considering the entire (100%) and a"}, {"title": null, "content": "25% subset (Supplemental Table 4 reports 50% and 75%). Each experiment was repeated five times, each time using a different model weight for initialization trained on each of the five-folds in Dataset-1. A different randomly selected subset of the training data of Dataset-2 was employed each time except when fine-tuning on the entire (100%) training dataset. Cross-testing performance in row 1, directly applied the models trained on Dataset-1 without fine-tuning. A moderate drop in performance was observed in comparison to fine-tuned models, which is expected due to the image domain shift across scanners.\nUnsupervised Fine-tuning (Unsup.-F) was performed by leveraging the inter- dependencies between longitudinal intra-subject image-pairs without using the GT training conversion-time labels. A drastic performance improvement was observed over cross-testing both in AUROC and B.Acc. across all time-intervals. The CcI improved from 0.756 to 0.818 by just utilizing 25% of the training data. The unsupervised fine-tuning performance further improved by utilizing the entire training dataset in an unsupervised manner. Fully supervised fine-tuning (Sup.-F) with GT conversion labels serve as an upper limit on fine-tuning performance. Interestingly, the performance gap between Unsup.- F and Sup-F was not significant in the small training data-regime with an almost same mean AUROC across 6, 12 and 18 months, while Unsup.-F surpassed Sup.-F in terms of B.Acc at t = 6,12 and CcI (0.818 vs. 0.816). However, this trend reverses when the entire training dataset was utilized. in terms of AUROC, with the Unsup.-F still giving competitive performance in terms of B.Acc and CcI (0.828 for Unsup.-F compared to 0.831 for Sup.-F). Fig. 2(a) displays Kaplan-Meier survival curves for risk groups identified by thresholding r from a model trained on 25% of Dataset-2's training data with Unsup.-F. The curves are distinctly separated, affirming r's efficacy in stratifying risk. Fig. 2(b) illustrates the U-map visualization of the model's feature space, transitioning smoothly from red (short conversion time) to blue (long conversion time) along the manifold. GradCam maps in Fig. 2(c) reveal that the trained models attend to irregularities around drusen, known markers of AMD."}, {"title": "Conclusion", "content": "We proposed a novel framework to jointly predict a risk score and predict the CDF of conversion at given continuous time-intervals. The risk score, based on the signed distance of a sample from a decision hyperplane H separating iAMD and dAMD samples, incorporates a ranking loss to to ensure that samples closest to H have the shortest conversion time and vice versa. This temporal ordering in the feature space is further utilized to model the CDF, predicting conversion probabilities at arbitrary future time intervals using a family of hyperplanes parallel to H. We also enforce dependencies between intra-subject longitudinal im- age pairs to regularize the feature space, facilitating unsupervised fine-tuning on new datasets. Our method outperforms several popular survival analysis meth- ods, demonstrating its effectiveness. In addition, unsupervised fine-tuning signif- icantly improved cross-testing performance across datasets particularly with lim- ited training data availability. This approach allows for model adaptation across datasets with significant domain shifts due to inter-scanner variability without the need for manual annotation of training labels. Future work could include evaluating our method on public datasets for related tasks like Alzheimer's dis- ease progression from brain MRI and incorporating Longitudinal-Mixup [23] in our training to improve performance."}]}