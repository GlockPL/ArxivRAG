{"title": "Constraint-Adaptive Policy Switching for Offline Safe Reinforcement Learning", "authors": ["Yassine Chemingui", "Aryan Deshwal", "Honghao Wei", "Alan Fern", "Janardhan Rao Doppa"], "abstract": "Offline safe reinforcement learning (OSRL) involves learning a decision-making policy to maximize rewards from a fixed batch of training data to satisfy pre-defined safety constraints. However, adapting to varying safety constraints during deployment without retraining remains an under-explored challenge. To address this challenge, we introduce constraint-adaptive policy switching (CAPS), a wrapper framework around existing offline RL algorithms. During training, CAPS uses offline data to learn multiple policies with a shared representation that optimize different reward and cost trade-offs. During testing, CAPS switches between those policies by selecting at each state the policy that maximizes future rewards among those that satisfy the current cost constraint. Our experiments on 38 tasks from the DSRL benchmark demonstrate that CAPS consistently outperforms existing methods, establishing a strong wrapper-based baseline for OSRL. The code is publicly available at https://github.com/yassineCh/CAPS.", "sections": [{"title": "Introduction", "content": "Online reinforcement learning (RL) has shown great successes in applications where the agent can continuously interact with the environment to collect new feedback data to improve its decision-making policy (Mnih et al. 2015; Silver et al. 2018; Li 2017). However, in many real-world domains such as agriculture, smart grid and healthcare, executing exploratory decisions is costly and/or dangerous, but we have access to pre-collected datasets. Offline RL (Levine et al. 2020) is an emerging paradigm to learn decision policies solely from such offline datasets, eliminating the need for additional interaction with the environment. The key challenge in offline RL is addressing the distributional shift between the state-action distribution of the offline dataset and the learned policy, as this shift results in extrapolation error. Existing offline RL methods typically employ the principle of pessimism in the face of uncertainty (i.e., some form of penalization to deal with out-of-distribution states/actions) to address this challenge (Levine et al. 2020).\nIn many safety-critical domains (e.g., agriculture and smart grid), the agent behavior needs to also satisfy some cost constraints in addition to maximizing reward. Most of the prior work on offline RL focuses on the unconstrained setting where the policy is optimized to maximize rewards. There is relatively less work on offline constrained/safe RL and the majority of these methods assume that the cost constraint is known during the training phase (as elaborated in the related work section). However, in many applications, the cost constraints can change at deployment depending on the use-case scenario. For example, in agricultural drones, the reward is the area covered during spraying, and the cost is the battery/power consumption. The goal is to maximize efficiency for a given agricultural field by optimizing coverage based on the available battery power.\nThis paper studies the problem of offline safe RL (OSRL) where the safety/cost constraints can vary after deployment. We propose Constraint-Adaptive Policy Switching (CAPS), an algorithm that can be wrapped around existing offline RL methods and is easy to implement. CAPS handles the challenge of unknown cost constraint threshold as follows. At training time, we learn multiple policies with a shared representation with the goal of achieving different reward and cost trade-offs: one centered on reward maximization, one on cost minimization, and other policies optimized for varied linear scalarizations of costs and rewards. At testing time, CAPS switches between the decisions from the trained policies at each state, selecting the one that maximizes future rewards from the ones that satisfy the cost constraint. We also theoretically analyze the conditions under which CAPS decision-making process has safety guarantees.\nWe perform experiments on 38 tasks from the DSRL benchmark (Liu et al. 2024) to compare CAPS with state-of-the-art OSRL baselines and our main findings are as follows. First, CAPS wrapped around two qualitatively different offline RL algorithms exhibits safe behavior (first-order objective) on a larger fraction of tasks compared to baselines across different cost threshold constraints. Prior methods struggle even with known cost constraints, potentially because of estimation errors in value functions and/or instability in the Lagrange multiplier. Second, the implicit Q-learning instantiation of CAPS demonstrates safe performance in 34/38 tasks (89%) and achieves the highest rewards in 18 of those tasks compared to the baselines.. Third, CAPS using shared representation performs significantly better than CAPS with independently trained policies. Finally, increasing the number of policies improves the overall performance of CAPS, but CAPS with two policies (one for reward maximization and"}, {"title": "Problem Setup", "content": "RL problems with safety constraints are naturally formulated within the Constrained Markov Decision Process (CMDP) framework. A CMDP is defined by a tuple M = (S, A, P, r, c, \\mu_0), where S is the state space, A is the action space, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the state transition probability function, r : S \u00d7 A \u2192 \\mathbb{R} defines the reward function, and c: S\u2192 [0, C_{max}] quantifies the costs associated with states, where C_{max} is the maximum possible cost, and \\mu_0 : S \u2192 [0, 1] is the initial state distribution. For simplicity of notation, we define cost function on states noting that it is easy to define an equivalent CMDP with costs associated with state-action pairs.\nIn offline safe RL problems, we are given a fixed pre-collected dataset D={(s, a, r, c, s')_i}_{i=1}^N from one or more (unknown) behavior policies, where each training example i contains the action a taken at state s, reward received r, cost incurred c, and the next state s'. The goal is to learn a policy \\pi : S \u2192 A from the offline dataset D to maximize the expected reward while satisfying a specified cost/safety constraint. This problem is mathematically formulated as:\n\\max_\\pi E_{\\tau \\sim \\pi}[R(T)] \\text{ subject to } E_{\\tau \\sim \\pi}[C(T)] \\le \\kappa,\nHere, \\kappa \\in [0, +\\infty) is the cost threshold for safety constraint, T = {s_1, a_1, r_1, c_1,..., s_T, a_T, r_T, c_T} denotes a trajectory sampled by executing the policy \\pi, T is the length of episode, R(T) = \\sum_{t=1}^T r_t is the total accumulated reward, and C(T) = \\sum_{t=1}^T c_t is the total incurred cost.\nThe majority of prior work on OSRL assumes that the cost threshold \\kappa for safety constraint is known at the training time and remains same at deployment. However, this assumption is violated in many real-world applications where the safety requirements can change during deployment based on the operating conditions or environmental factors. For example, in agricultural crop management, the goal may be to maximize yield under constraints on water and/or nitrogen application depending on time-varying regulations. Additionally, in a decision-support scenario, it can be useful for decision makers to quickly explore the impact of different cost thresholds without requiring retraining.\nOther application domains where this problem setting arises include healthcare, power systems, and robotics. Consequently, OSRL methods that learn a single policy tailored"}, {"title": "Related Work", "content": "Online Safe RL. RL with safety constraints in the online setting, where the agent can interact with the environment to collect feedback data, is extensively studied (Garcia and Fern\u00e1ndez 2015; Gu et al. 2024). However, most of these methods assume a fixed constraint threshold during both training and deployment. Constraint-conditioned policy optimization (Yao et al. 2023), addresses this limitation and adapts to changing constraints using two strategies: approximating value functions under unseen constraints and encoding arbitrary constraint thresholds during policy optimization.\nOffline RL. The goal in offline RL is to learn effective policies from fixed static datasets without any additional interaction with the environment. Prior work has tackled the key challenge of distribution shift in offline RL (Levine et al. 2020; Figueiredo Prudencio, Maximo, and Colombini 2024). Some representative techniques include incorporating regularization in value function estimation (Fujimoto and Gu 2021; Kostrikov, Nair, and Levine 2022; Kumar et al. 2019; Lyu et al. 2022; Yang et al. 2022), sequential modeling formulations (Janner, Li, and Levine 2021; Wang et al. 2022), uncertainty-aware methods (An et al. 2021; Bai et al. 2022), divergence based policy constraints (Wu, Tucker, and Nachum 2020; Jaques et al. 2020; Wu et al. 2022), and extensions of model based RL (Kidambi et al. 2020; Yu et al. 2020; Rigter, Lacerda, and Hawes 2022) and imitation learning (Xu et al. 2022). SfBC (Chen et al. 2023) and IDQL (Hansen-Estruch et al. 2023) filter candidate actions using Q-function evaluations and reweight them for selection. In contrast, CAPS avoids reweighting by directly selecting the best safe action using two Q-functions. PEX (Zhang, Xu, and Yu 2023) is a offline-to-online RL approach that adds a new expansion policy to the offline policy, unlike CAPS, which dynamically switches between pre-trained policies for safety.\nOffline Safe RL. Recent work has studied the offline safe RL problem, aiming to learn a safe policy from a given offline dataset (Liu et al. 2024). These methods include finding cost-conservative policies for better actual constraint satisfaction via some form of constrained policy optimization formulation (Polosky et al. 2022; Lee et al. 2022), Lagrangian based approaches to handle cost constraints (Xu, Zhan, and Zhu 2022), and the constrained decision transformer (CDT) (Liu et al. 2023), which exploits the advances in sequential modeling by learning from a dataset of trajectories. TREBI (Lin et al. 2023) and FISOR (Zheng et al. 2024) leverage diffusion models for creating safe policies: TREBI generates safe trajectories, while FISOR uses a diffusion actor to select actions within feasible regions. However, these methods, with the exception of CDT, rely on fixed cost thresholds during training, limiting their flexibility for deployment scenarios with varying cost threshold constraints."}, {"title": "Constraint-Adaptive Policy Switching", "content": "In this section, we first describe the reasoning procedure behind the Constraint-Adaptive Policy Switching (CAPS) approach of handling unknown constraints at testing time. Next, we describe the training process and the design choices that enable effective decision-making in CAPS."}, {"title": "Decision-Making in CAPS", "content": "One way to handle dynamically changing cost constraints would be to: 1) train a large set of K policies that span a wide spectrum of cost-reward trade-offs, and 2) at each test-time state, select the best of those policies that satisfies the current cost constraint. While this makes sense in concept, there are at least two key practical issues. First, there is a high computational cost involved in optimizing K policies via safe offline RL. Second, and more importantly, step (2) requires accurately assessing the safety of a given policy and also selecting the best among the safe ones. This requires accurate cost and value functions for each policy, which would need to be estimated using off-policy evaluation (OPE) techniques with the available dataset. Unfortunately, as our experimental results show, current OPE techniques are not reliable enough to ensure test-time safety (Figueiredo Prudencio, Maximo, and Colombini 2024). The key idea of CAPS is to address these two issues by creating a set of K diverse policies at a much smaller computational cost, while avoiding a reliance on OPE to provide test-time safety.\nCAPS relies on two key components for its decision-making to handle unknown cost constraints. First, a set of K > 2 policies P = {\\pi^r, \\pi^1, ..., \\pi^{K-2}, \\pi^c} that are trained using the offline dataset D to cover actions with different reward and cost trade-offs. At one extreme, \\pi^r is trained to maximize reward via standard Offline-RL, while ignoring the cost objective. At the other extreme, \\pi^c is trained to minimize cost via standard Offline-RL, while ignoring reward. In this sense, \\pi^c is maximally safe, while \\pi^r is cost agnostic. The remaining K-2 policies are trained with the intent of covering actions that achieve a variety of reward and cost trade-offs, while avoiding the full computational cost of Offline-RL for each one. Second, CAPS retains from Offline-RL the learned Q-functions Q^r and Q^c which we assume are non-stationary given the finite time horizon:\nQ_r(s,a) = E[\\sum_{t=1}^T r_t | S_t = s, a_t = a] \t\t\t\t(1)\nQ_c(s,a) = E[\\sum_{t=1}^T c_t | S_t = s, a_t = a], \t\t\t\t(2)\nwhere the expectation is taken over the corresponding policy with respect to randomness from reward/cost functions and state transitions. Intuitively, CAPS will use Q^c to help filter out unsafe decisions from the K policies and Q^r is employed to select the best decision from the unfiltered ones. Note that these Q-functions are an artifact of policy optimization, rather than OPE of an arbitrary policy. This is a subtle, but important, aspect of CAPS, since it has been observed that value estimates of arbitrary policies via OPE tends to be"}, {"title": "Training Algorithm for CAPS", "content": "Given an offline dataset D = {(s_i, a_i, r_i, c_i, s'_i)}_{i=1}^N, the CAPS approach aims to efficiently learn a set of policies P={\\pi^r, \\pi^1, ..., \\pi^{K-2}, \\pi^c} and the two Q-value functions Q_r and Q_c that yield strong test time performance. A key feature of CAPS is that training is done via a reduction to standard Offline-RL. In particular, CAPS can be combined with any Offline-RL algorithm that produces a Q-function and a mechanism to extract a policy for that Q-function. In what follows, we first describe the high-level training schema and then provide two concrete instantiations that are used in our experiments. Finally, we discuss the design choice of network architecture, including shared structure between the K policies.\nReduction to Offline RL. The training approach for CAPS can be wrapped around existing offline RL algorithms with minimal changes, making it straightforward to implement. The general recipe consists of the following two steps:\n1. Train the reward-only value funcion Q_r and cost-only value function Q_c. For Q_c, we train using cost data from the offline dataset by defining the rewards as costs.\n2. Extract the reward maximizing policy \\pi^r from Q_r, the cost minimizing policy \\pi^c from Q_c, and the other K \u2212 2, \\pi^k, policies with different reward and cost trade-offs from the mixture Q-value function Q - \\lambda_kQ_c following the same policy extraction procedure in the offline RL method, where \\lambda_k is the scalarization parameter."}, {"title": "Safety Guarantee", "content": "This section describes conditions under which the CAPS decision-making process has safety guarantees. For simplicity, we consider the case where the optimal-cost Q-function is perfectly estimated, noting that it is straightforward to modify our result to account for bounded estimation error.\nWe focus on the finite-horizon setting with horizon T and let Q_t^\\star and V_t^c denote the cost optimal Q and value functions for time step t \\in {0, . . ., T} respectively. Here we treat these functions as positive cost accumulation functions that are minimized during optimization. We consider policies of the form \\pi_t(s, c_{<t}) where s is the current state and c_{<t} is the accumulated cost before arriving at time step t. For a given cost safety bound \\kappa, our guarantee applies to any policy that is \\kappa-admissible.\nAssumption 1 A policy \\pi is \\kappa-admissible if for any state s, any time step t, and any accumulated cost c_{<t}, we have Q_c(s, \\pi_t(s, c_{<t})) \\le \\max{V_t^c(s), \\kappa - c_{<t}}.\nNote that by assuming a perfect Q_c, the switching policy defined for CAPS is \\kappa-admissible since it either picks an action a such that Q_c(s, a) \\le \\kappa - c_{<t} if possible, or resorts to choosing the least cost action, which has a value of V_t^c(s) since Q_c and V_t^c are cost optimal. This is true of CAPS for any set of K policies P that contains the cost optimal policy \\pi^c and any reward value function Q_r.\nOur safety guarantees are specified in terms of a parameter \\epsilon that characterizes the underlying Markov decision process (MDP). In particular, safety in terms of expected cost is challenging when stochastic state transitions can result in a set of possible next states that have wildly different values of V_t^c. Indeed, counter examples to safety can be constructed that involve transitions with very small probabilities of reaching enormous values of V_t^c while the remaining probability mass results in very small costs. For this purpose, we define the notion of optimal-cost variation.\nAssumption 2 Given an MDP M, let N(s,a) = {s' | P(s,a,s') > 0} be the set of possible next states after taking action a in state s. M has an optimal-cost variation of \\epsilon if each state-action-timestep tuple (s, a, t) satisfies \\max_{s' \\in N(s,a)} V_t^c(s') - \\min_{s' \\in N(s,a)} V_t^c(s') \\le \\epsilon.\nNote that any deterministic MDP has \\epsilon = 0. Also note that this assumption places no constraints between the optimal costs of states resulting from different (s, a, t) tuples. We can"}, {"title": "Discussion on Assumptions for Safety Guarantees of CAPS", "content": "Assumptions 1 and 2 are essential for achieving safety guarantees under CAPS. Intuitively, Assumption 1 ensures that there exists at least one safe action at any intermediate step such that future costs can satisfy the cost constraint based on already observed costs. Having more heads theoretically ensures better coverage of the action space for each agent. Without such a safe action, no algorithm can guarantee safety. We include this assumption to generalize the theorem, making it applicable to other policies or strategies that meet the same condition, not just CAPS. This broadens the applicability of our theoretical analysis.\nAssumption 2 helps define the types of Markov Decision Processes (MDPs) for which we can ensure safety guarantees. Without this assumption, we can easily create pathological counterexamples where guarantees would fail. In practice, most of our experimental benchmark environments comply with Assumption 2 with only a small epsilon value. This reflects realistic conditions where good policies demonstrate some degree of recoverability and avoid extreme scenarios such as falling into irrecoverable states or encountering unexpected high rewards.\nThese assumptions help frame the theoretical results, ensuring that CAPS is validated under reasonable conditions applicable to our benchmark environments."}]}