{"title": "Small Language Models can Outperform Humans in Short Creative Writing: A Study Comparing SLMs with Humans and LLMs", "authors": ["Guillermo Marco", "Luz Rello", "Julio Gonzalo"], "abstract": "In this paper, we evaluate the creative fiction writing abilities of a fine-tuned small language model (SLM), BART Large, and compare its performance to humans and two large language models (LLMs): GPT-3.5 and GPT-40. Our evaluation consists of two experiments: (i) a human evaluation where readers assess the stories generated by the SLM compared to human-written stories, and (ii) a qualitative linguistic analysis comparing the textual characteristics of the stories generated by the different models. In the first experiment, we asked 68 participants to rate short stories generated by the models and humans along dimensions such as grammaticality, relevance, creativity, and attractiveness. BART Large outperformed human writers in most aspects, except creativity, with an overall score of 2.11 compared to 1.85 for human-written texts\u2014a 14% improvement. In the second experiment, the qualitative analysis revealed that, while GPT-40 exhibited near-perfect internal and external coherence, it tended to produce more predictable narratives, with only 3% of its stories see as novel. In contrast, 15% of BART's stories were considered novel, indicating a higher degree of creativity despite its smaller model size. This study provides both quantitative and qualitative insights into how model size and fine-tuning influence the balance between creativity, fluency, and coherence in a creative writing tasks.", "sections": [{"title": "1 Introduction", "content": "The field of Natural Language Processing (NLP) has seen significant advancements due to Large Language Models (LLMs), which have demonstrated remarkable performance across a variety of tasks, including creative writing (Chang et al., 2024; Wang et al., 2024). These models, such as GPT-3.5 and GPT-4 (Ouyang et al., 2022; Achiam et al., 2023), can generate human-like text and have set a new standard for many language-related applications. However, these models requires immense computational resources and large datasets, making them resource-intensive. While these large models have excellent performance, recent research has suggested that smaller models, or Small Language Models (SLMs), can achieve competitive results in certain tasks with a much lower computational cost (Schick and Sch\u00fctze, 2020; Bilenko, 2024; Chen and Varoquaux, 2024). This leads to our core research question: RQ0: Can a fine-tuned small language model be competitive in the field of creative, literary writing?\nThis study explores the capabilities of BART-large (Lewis et al., 2019), an SLM, in a creative writing task: generating movie synopses based on a given title. The first goal is to compare the quality of these synopses to those written by humans. To explore this, we conducted a comprehensive study in which we collected over 24,000 manual assessments. Specifically, 68 participants evaluated 60 synopses across five dimensions -readability, understandability, relevance, attractiveness, and creativity in three different experimental settings. With this dataset we answer the central question: RQ1: How do creative texts produced by SLMS compare with human equivalents in terms of readability, understandability, relevance, attractiveness, and creativity? Our findings show that, surprisingly, BART-large outperformed humans across all quality dimensions except creativity.\nTo further explore the role of human perception in evaluating AI-generated text, the experiment was conducted in three variants: (1) the readers were unaware of who wrote the text, (2) the readers were explicitly told whether the text was written by a human or a AI, and (3) the readers were told all texts were AI-generated, regardless of their true origin. This experimental setup aims to answer the question: RQ2: How relevant are reader biases regarding the author's identity (human or AI) in their evaluation scores? The results demonstrated that knowing the author influenced how readers"}, {"title": "2 Releated Work", "content": "Researchers have explored the potential of small language models (SLMs) in various natural language processing tasks, including creative writing. Schick and Sch\u00fctze (2021) demonstrated that SLMs could be effective few-shot learners through prompt-based learning and optimization techniques. Eldan and Li (2023) introduced TinyStories, showing that models with fewer than 10 million parameters can generate coherent short stories, challenging the notion that larger models are necessary for fluent text generation. For more comprehensive information on the role of small models in the era of large language models, see the survey by Chen and Varoquaux (2024).\nHowever, evaluating the creativity of language models remains a complex challenge. Chakrabarty et al. (2024) introduced the Torrance Test of Creative Writing to objectively assess creativity, finding that LLM-generated stories pass significantly fewer creativity tests compared to those written by professional authors.\nMarco et al. (2024) conducted a study titled \"Pron vs. Prompt,\" where they evaluated a top LLM, GPT-4, against a world-class fiction author, Patricio Pron, with assessments conducted by literary critics. Their findings indicate that LLMs are still far from matching the creative writing skills of top human authors, particularly in terms of originality and literary quality. For a detailed overview of creativity and machine learning, refer to the survey by Franceschelli and Musolesi (2022).\nOur work takes the methodology of \"Pron vs. Prompt\" as a starting point but differs in scope and focus. While the previous study evaluated a top LLM against a top writer with evaluations by literary critics, we aim to evaluate a small language model (BART Large) against average human writers, with assessments conducted by average readers instead of experts. By comparing the performance of BART Large to both human writers and larger models like GPT-40, we seek to determine whether an SLM can be competitive in creative literary writing among general audiences."}, {"title": "3 Experimental Design", "content": "To answer our research questions, we have designed two different experiments. The first one is an evaluation with humans that tries to quantify how far the texts written by our SLM are from those written by humans. The second experiment consists of qualitatively analyzing the linguistic similarities and differences between the SLM texts and the most popular LLMs right now: GPT-3.51 and GPT-40. The task consists of, given a potential movie title, writing an imaginary synopsis for a movie with that title.\n1In July 2024, OpenAI announced that it would replace gpt-3.5 with gpt-40-mini in the ChatGPT web version. By then this research had already been completed. However, we decided to keep the results of GPT-3.5 because it is still available for access through the API, but, above all, because of its proximity in performance with open source models as Mixtral (Jiang et al., 2024) or Gemma (Mesnard et al., 2024) that are still widely used."}, {"title": "3.1 SLM vs. Humans Methodology", "content": "The Small Language Model For our main experiment we have used the default BART-large pre-trained model (Lewis et al., 2019) and we have fine-tuned it to the down-stream task of synopsis generation from titles. During generation, we use sampling and beam search with the default parameters of BART HuggingFace model configuration2.\nDataset The dataset for this study was created by merging the Corpus of Movie Plot Synopses with Tags (MPST) (Kar et al., 2018), the CMU Movie Summary Corpus (Bamman et al., 2013), and the Wikipedia Movie Plots datasets (Wikipedia, 2019). Duplicate titles and synopses exceeding 1,024 tokens were removed to comply with the BART input limit. The dataset was split into 80% training, 10% validation, and 10% test sets, with 42,049 examples for training and 5,257 for both validation and testing. Items for evaluation, including human-written and model-generated synopses, were randomly selected from the test set. To avoid evaluation biases, we restricted the selection to model/human syn-opses pairs of similar lengths (within \u00b115 tokens), and to movie titles with fewer than 1,000 votes on IMDb, to minimize the chance that assessors would recognize them. The average synopsis length was 79 tokens for model-generated and 78 for human-written texts.\nQuiz design We ask the assessor to evaluate a number of quality aspects of the text. We have run three experiments with a similar setup (see \"Variants\" below). For each experiment, we had two quizzes, each consisting of 60 synopses: half (30) human-written, and half (30) generated by our system. We decided to use this number of synopses because a sample size of 30 is large enough for statistical significance tests, and over 60 synopses would cause fatigue in our assessors. The synopses with human written synopses in quiz A have model synopses in quiz B, and vice versa. Half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz B. The synopses are displayed in random order for each subject.\nEach of the 60 questions of a quiz asked the assessor to read a title and a synopsis. Then, the assessor is asked to evaluate, in a Likert scale of 0 to 4 (0 - not at all, 1- a little, 2- enough, 3 - a lot, 4 - completely) several key aspects: readability, as-\n2https://huggingface.co/facebook/bart-large/\nblob/main/config.json"}, {"title": "Variants", "content": "sessing whether the writing was grammatically correct; understandability, determining if the synopsis made sense; relevance, evaluating the connection between the synopsis and the title; informativity, measuring how much information the synopsis provided about the film, including its genre (e.g., children's, romance, adventure, science fiction, crime, etc.); attractiveness, judging whether the synopsis made the reader want to see the film; and creativity. A final control question checked if the participant was already familiar with the title or any of the information in the synopsis.\nAssessors were instructed not to search for any information about the movies before completing their assessments. After the quiz, they were asked to reflect on the factors they considered when evaluating creativity, on whether their evaluation criteria changed during the process, and to provide feedback on their overall experience.\nVariants In the main experiment, assessors were informed beforehand that the synopses were written by either humans or a computer system but were not told which specific synopsis belonged to whom during the evaluation, aiming to minimize bias. To assess how expectations about authorship might influence their judgments, two additional experiments were conducted: (1) in the \"Revealed\" variant, assessors were explicitly told the authorship of each synopsis; (2) in the \"AI only\" variant, assessors were misled to believe all synopses were generated by AI, even though half were human-written.\nThe 60 synopses were the same in the three experiments. Note that this is a between-subjects experiment design. A within-subject study would require that the two synopsis for the same title (the original, human-made synopsis and the one invented by the SLM taking the title as input) were evaluated by the same assessor. But if the assessor reads both synopsis, there are biases that might interfere. In particular, assessors might try to deduce which of the two alternative synopsis is AI-made, and their presuppositions might affect their scores. In fact, one of the experiments described in the paper confirmed that the scores assigned by our subjects changed when we gave them information about the authors of each synopsis.\nHuman Writers The synopses used in this study corresponds to actual movies and were selected from various publicly available sources, primarily Wikipedia and movie databases. Given the nature"}, {"title": "4 Results of SLM vs. Humans Experiment", "content": "Table 1 shows the average scores for each quality aspect of the human vs model-generated synopses, along with the statistical significance of the differences found. The table allows to answer our two first Research Questions."}, {"title": "4.1 RQ1: How do creative texts produced by SLMs compare with human equivalents in terms of readability, understandability, relevance, attractiveness and creativity?", "content": "The results show that SLM-generated synopses outperform human-written ones in all aspects except creativity. The overall score for AI-generated syn-opses was 2.11 compared to 1.85 for human syn-opses (a 14% improvement). Specifically, SLM synopses were 22% more readable, 17% more understandable, 23% more relevant to the title, 11% more informative, and 18% more attractive, with all these differences being statistically significant. However, the SLM scored 3% lower in creativity, though the difference was not statistically significant.\nDoes the apprentice beats the master? The fact that a SLM, trained to mimic human-written texts, can outperform humans in such a creative task is notable, as it suggests that they are capable of producing better-than-human outputs even by merely imitating human examples. A possible explanation for the higher average score of the SLM could be that, in the learning process, generalization helps the SLM avoid gross mistakes, and therefore produce more homogeneous text. Then, it would get better averages just by avoiding lowest scores. But this hypothesis is incorrect: although the standard deviation of humans in overall score is higher than in the SLM (0.461 vs 0.355) \u2013 which is consistent with the hypothesis \u2013 the SLM actually gets more overall 5's (best possible score) than humans: not only the average is better, but the SLM writes more high quality synopses than the humans. For a more in-depth statistical analysis see Appendix C."}, {"title": "4.2 RQ2: How relevant are the expectations and biases of the reader with respect to AI vs human authors?", "content": "In the main experiment, assessors were unaware of the author of each text. To explore potential biases, we ran two additional experiments: one where the author (human or AI) was revealed, and another where readers were told that all synopses were AI-generated (although actually half of them were human-written). Results of these two additional experiments are also displayed in Table 1.\nIn the \"revealed author\" variant, the bias against AI-generated synopses becomes evident. The overall score difference between humans and AI decreased from 0.257 to 0.107, with AI synopses"}, {"title": "4.3 RQ3: How do readers assess creativity, and how it correlates with other aspects of the texts?", "content": "penalized by 6% when the author was revealed. AI synopses scored lower in readability, understand-ability, and attractiveness, while human scores re-mained relatively stable, confirming a negative bias against AI-generated content.\nIn the \"AI-only\" variant, where assessors be-lieved all synopses were AI-generated, both hu-man and AI synopses were penalized in readabilityand understandability. Interestingly, attractivenessratings increased for both, although this effect isharder to interpret. The overall score differencebetween humans and AI decreased slightly, but AIsynopses still outperformed human ones.\nThese results suggest that revealing the authorcreates a bias against AI-generated content, impact-ing readers' experience, but not enough to changethe overall outcome.\n4.3 RQ3: How do readers assess creativity,and how it correlates with other aspects ofthe texts?\nWe deliberately chose not to provide readers with aspecific definition of creativity, to avoid introducingour own biases into their evaluations. Our readerswere free to interpret the term \"creativity\", follow-ing (Colton and Wiggins, 2021): \"Computer Cre-ativity is the philosophy, science and engineering ofcomputational systems which, by taking on particu-lar responsibilities, exhibit behaviors that unbiasedobservers would deem to be creative.\" Once read-"}, {"title": "Correlation of Creativity with Other Quality Aspects", "content": "ers completed the evaluation, we asked them whatfactors in the synopses influenced their quantitativeassessment of creativity. The reason is that we arefocused on whether SLM texts give the appearanceof creativity to readers in general (something thatis measurable), not whether they are truly creativefrom an artist's perspective. After completing theexperiment, participants were asked what factorsinfluenced their creativity scores.\nThe responses fell into two main categories: (1)Writing style: 21 out of 68 assessors noted thatthey judged creativity based on elements like co-hesion, vocabulary, and the overall fluency of thetext. (2) Originality and predictability: Another21 assessors mentioned that they assessed creativ-ity by considering how familiar or predictable thestoryline seemed. A few assessors offered addi-tional insights, such as creativity being linked tohow detailed the synopsis was or how well the syn-opsis aligned with their expectations based on thetitle. Interestingly, 22 assessors did not answer thequestion regarding creativity.\nCorrelation of Creativity with Other QualityAspects We further investigated how creativitycorrelates with other quality aspects like relevance,informativeness, and attractiveness by calculatingthe correlations between these aspects for each syn-opsis. Table 2 shows that creativity has a strongpositive correlation with relevance (0.69) and in-formativeness (0.60). This suggests that readers"}, {"title": "5 SLM vs LLMs: Results of the Linguistic Analysis", "content": "often perceive a more creative text when it is morerelevant to the title or provides more detailed infor-mation. For instance, one reader noted that moredetailed synopses appeared more creative, whichaligns with the observed correlation between cre-ativity and informativeness.\nRelevance also showed strong correlations withall other aspects except attractiveness, indicatingthat synopses closely aligned with the title weregenerally rated higher across all dimensions. On the other hand, attractiveness was the least corre-lated with other aspects, with a correlation below0.3 for all aspects except informativeness. This suggests that attractiveness is likely influenced bypersonal preferences or prior movie-watching ex-periences, making it a more subjective metric.\nIt's also noteworthy that creativity and attrac-tiveness were only weakly correlated (0.25), in-dicating that creativity doesn't necessarily drivethe appeal of a synopsis. This could help explainwhy blockbuster movies often repeat successfulclich\u00e9s - while they may not be seen as highly cre-ative, they can still be considered attractive or ap-pealing to a broad audience.\n5 SLM vs LLMs: Results of theLinguistic Analysis\nUp to this point, we have focused on the compari-son between human-generated texts and those pro-duced by BART, where BART demonstrated su-perior performance in most evaluated dimensions.Having established this, we now turn our attentionto a more detailed examination of BART in con-trast to larger language models (LLMs), such asGPT-3.5 and GPT-40. This analysis will explorethe distinct linguistic features of these models, witha particular emphasis on how the larger LLMs tendto surpass BART in various aspects, especially inthe case of GPT-40. For concrete examples of the"}, {"title": "5.1 Repetitiveness and Formulaic Phrases", "content": "linguistic features discussed here, see Appendix E and D.\n5.1 Repetitiveness and Formulaic Phrases\nTable 3 shows that the synopses generated by trans-formers tend to be formulaic and rely on clich\u00e9phrases. This repetitiveness was measured basedon the frequency of certain collocations and wordswith common phrases such as \u201c[It/The film] tellsthe story of appearing in 75% of BART syn-opses and 35 times in GPT-3.5 summaries. Themost clich\u00e9 prone model is GPT-3.5 (100% ofthe synopsis have at least one clich\u00e9), followedby BART (83,3%). GPT-4o is substantially bet-ter, with just one in four synopsis having a clich\u00e9phrase (26,7%); but it repeats vocabulary acrossthe synopses.\nBoth models display varying degrees of repe-tition, with BART having more than 83% of itssynopses containing at least one clich\u00e9. GPT-40,however, avoids multiple clich\u00e9s, but repeats vo-cabulary across different texts.\nA small set of phrases dominates the texts gen-erated by these models, contributing to a lack ofperceived creativity. The most common phrasesinclude \u201c[It/The film] tells the story of\u201d and \u201cThefilm is set in,\" leading to a certain repetitiveness.GPT-40, although less reliant on clich\u00e9 phrases,uses longer but equally formulaic sentences such as\u201clife is turned upside down\u201d or \u201clove can sometimesbe the wildest move of all.\u201d This predictability lim-its the models' originality, even though human-generated synopses also exhibit formulaic tenden-cies.\nIn addition to clich\u00e9s, frequent collocations like\u201ccar accident\u201d and \u201csuccessful businessman\u201d fur-ther highlight the repetitiveness of both models.BART is less predictable in its use of clich\u00e9s thanGPT-3.5, which, even in zero-shot mode, showsa higher degree of predictability. However, GPT-40 introduces more variety with a growing use ofproper names (88.3% of its texts), less commonwords, and unique adjective-noun combinations,contributing to greater linguistic diversity."}, {"title": "5.2 Recurrent Themes", "content": "5.2 Recurrent Themes\nThe analysis shows that both BART and GPT-3.5-generated synopses tend to focus on a narrow setof recurrent themes, which may contribute to a per-ceived lack of creativity. BART's synopses primar-ily revolve around love (38.3%), crime (18.3%),war (13.3%), and family drama (11.7%), with"}, {"title": "5.3 Coherence with External Facts", "content": "many love stories resembling soap operas. GPT-3.5 focuses on personal growth (23.2%), adven-ture (20%), love (18.3%), and mystery (16.7%).GPT-3.5's synopses emphasize themes like self-discovery, the \"true meaning\" of love, and intimatehuman emotions more frequently than BART.\nGPT-40 also leans heavily into emotional nar-ratives, particularly mysteries (28.3%) and adven-tures (25%), but introduces more original charactertypes, such as archaeologists and chefs, comparedto BART's simpler characters. While both mod-els rely on a limited range of themes, similar tohuman-made synopses, GPT-3.5 tends to explic-itly mention genres like drama and thriller morefrequently.\n5.3 Coherence with External Facts\nAI-generated synopses sometimes contain factualerrors due to a lack of explicit world knowledge.Examples include setting the English Civil War inthe 1920s or referencing the Iron Curtain after ithad already fallen. However, most synopses (91.7%from GPT-3.5 and 86.7% from BART) are free ofsuch errors, largely because they avoid referencinghistorical events or specific facts tied to dates andplaces.\nWhen focusing on synopses that do mention ex-ternal facts, both BART and GPT-3.5 show lowreliability, with BART being accurate 62.5% of thetime regarding places and dates, and GPT-3.5 at60%. In references to specific historical events,BART is accurate 50% of the time, while GPT-3.5only reaches 30%. GPT-40, on the other hand, doesnot display any inconsistencies in these areas, but"}, {"title": "5.4 Internal Coherence", "content": "only 5% of its synopses reference historical events.These inconsistencies do not heavily impact average manual assessments because the synopsesrarely mention historical places and dates.\nAdditionally, GPT-40 shows significant improve-ments in coherence, both in its handling of externalfacts and in maintaining overall semantic coherencecompared to earlier models like GPT-3.5.\n5.4 Internal Coherence\nThe analysis reveals that AI-generated synopsescan struggle with internal coherence, despite theirshort length. For example, one BART synopsis de-scribes a character's father dying in a car accident,only to later state that the father gets married. Intotal, 20 of BART's synopses lack internal consis-tency (i.e. only 68.33% are coherent). The issueworsens with longer synopses, where only 33.33%of BART's top 25% longer synopses are coherent,indicating an inverse correlation between coher-ence and length.\nIn contrast, GPT-3.5 performs much better interms of internal consistency, with 95% of its syn-opses being coherent, and all of its longer synopsesmaintaining consistency. Additionally, all GPT-40 synopses are internally coherent. While inter-nal coherence is distinct from creativity, GPT'slarger models clearly outperform smaller modelslike BART, especially for longer generation tasks."}, {"title": "5.5 Surprising Associations", "content": "5.5 Surprising Associations\nThe analysis summarizes in Table 3 reveals thatAI-generated synopses sometimes produce surpris-ing or humorous associations, which could be per-"}, {"title": "6 Conclusions", "content": "ceived as creative. BART exhibited such associations in 9 out of 60 synopses (15%), whereas GPT-3.5 and GPT-40 displayed this only in two and onesynopses, respectively. In our data, BART is fourtimes more likely to generate unexpected contentcompared to the larger models.\nIt is remarkable that larger models such as GPT-3.5 and GPT-40, despite producing more fluent,natural and consistent text, are less likely to pro-duce creative or surprising associations, and this isa crucial consideration in automatic creative writ-ing. Note that we see this effect in humans, too:children has less knowledge and skills, but are inaverage much more creative than adults. Gainingknowledge makes humans more functional, butalso more predictable.\n5.6 RQ4: Do larger, better language modelsdirectly lead to more creative texts?\nAt this point, we are ready to answer our last re-search question.\nBoth BART and GPT-3.5 tend to rely on clich\u00e9phrases and recurrent topics, with BART beingslightly more creative, as 15% of its synopses con-tain surprising associations compared to only 2%for GPT-3.5. However, GPT-3.5 and GPT-40 out-perform BART in internal consistency (95%-100%vs. 68%).\nAlthough more research is needed to confirmthis, our data suggests a trade-off between modelsize and creativity: larger models like GPT-3.5,trained for fluency and naturalness, tend to producemore consistent but less creative texts. This may bedue to their predictability, which limits their abil-ity to generate surprising content. Probably, themost sensible way to overcome this issue in col-laborative human-AI writing is via prompting: anoriginal, carefully designed prompt may result ina surprising outcome. Despite BART's limitationsin coherence and creativity, its synopses were pre-ferred by readers over the human-texts in all qualityaspects except creativity, indicating that these limi-tations may not significantly affect overall manualassessments.\nComparing with human made synopses, BARTlimitations (coherence with external facts, internalcoherence for larger texts, use of cliches and recur-rent topics) are not significant enough to produce alower manual assessment, as they are preferred byour set of readers in all quality aspects except cre-ativity. If a small model can achieve these results,it is reasonable to expect that GPT-3.5 and GPT-40"}, {"title": "6 Conclusions", "content": "would perform similarly or even better.\n6 Conclusions\nThis study highlights the potential of small lan-guage models (SLMs) in creative writing tasks,specifically when generating short stories. Our ex-periments show that fine-tuned SLMs like BART-large can outperform average humans in many as-pects such as readability, understandability, rele-vance, and informativeness. However, humans stillretain a slight advantage in creativity, although thedifference is not statistically significant in our data.\nOur analysis also reveals that reader biases to-ward AI authorship negatively influence assess-ments of AI-generated texts, particularly in creative domains. Despite these biases, SLMs still producedmore attractive synopses overall compared to hu-man writers, suggesting that smaller models canoffer a competitive edge in creative applications, es-pecially when tasked with generating shorter texts.\nComparing the SLM with larger language mod-els (LLMs) \u2013 GPT-3.5 and GPT-40 in our experi-ments -, our qualitative linguistic analysis revealedthat while larger models produce more consistentand coherent texts, they tend to generate more pre-dictable and formulaic narratives. GPT-40, despiteits near-perfect internal and external coherence,produced stories that were seen as novel only 3%of the time. In contrast, the SLM generated novelcontent in 15% of its stories, indicating a higherdegree of creativity despite its smaller size. Thissuggests a trade-off between model size and cre-ative flexibility, where larger models may prioritizeconsistency over originality.\nThese findings emphasize that SLMs can becompetitive with both (average) humans and largermodels in creative writing tasks, particularly whenthe task benefits from creative flexibility over strictconsistency. It underscores that a Large LanguageModel is not always necessary, and that fine-tunedsmaller models can be more suitable and efficientfor specific applications. This opens up possibil-ities for developing more efficient, task-specificmodels that balance creativity, fluency, and coher-ence without the computational overhead of largermodels.3\n3All annotated data and model weights are availableon GitHub: https://github.com/annon-submission/slm-creativity."}, {"title": "Scope and Limitations of the Study", "content": "Scope and Limitations of the Study\nSLM have clearly received better average scoresthan humans in our experiment. Initially, we didnot expect such a strong result. Although languagemodels are trained with more text than a humancan read in a lifetime, their generalization abilitiesare still weak in comparison with humans, and theydo not connect language with real world knowl-edge. Our qualitative analysis, in fact, confirmsthe limitations of language models. What is, then, thecorrect interpretation of our results? Do they actu-ally mean that SLM or LLMs can already performcertain creative tasks better than humans? In or-der to properly answer this question, let us reviewour experimental setting and how it constrains thescope of our results.\nThe task As we have noted earlier, our task re-quires writing short texts, where internal coherenceis less of a challenge. We chose it because it iswell known that SLM struggle to keep coherence,and we wanted to measure their creative writingabilities in isolation from that aspect. We cannot,therefore, extrapolate our results to longer texts.\nAlso, producing movie synopsis is a special kindof writing task. Synopses in the training set havenot been created from a movie title, of course, butfrom the movie itself, and their goal is to inspireinterest in the reader without revealing too muchabout the plot. It is a good choice for evaluationbecause it is a constrained task, and because thereare many examples to learn from. But it is nota canonical creative writing task, and more workis needed before extrapolating our results to othertasks.\nThe human writers we are comparing with Al-though the set of movies used in our experimentwith readers were chosen randomly, we had a fewconstraints. One of them is that the movie musthave less than 1,000 reviews on IMDB, which re-duces the possibility of our readers being familiar-ized with the actual movie (in order to avoid andbiased evaluations). A side effect of this constraintis that less popular movies may correlate with lesserquality movies. We have checked our dataset withIMDB, and the average rating of the movies in thetraining set is 6.2 (with a median of 6.4); in the 60movies chosen for the quiz, on the other hand, theaverage is 5.7 (median 5.9). The difference is notlarge, but it may be a partial explanation of why theSLM is able to apparently improve the synopses"}, {"title": "The Assessors: Quality as Popularity", "content": "used in its learning process.\nThe Assessors: Quality as Popularity Our as-sessors are not professional creative writers or crit-ics: we use a popularity criteria in our evaluationdesign (Do target readers enjoy the synopses?) in-stead of a professional criteria (How do critics as-sess the synopsis?). Both types of assessment arelegitimate and complementary, and also often con-tradictory (for instance, blockbusters are usuallymuch more appreciated by the audience than bythe critics). In our experiment, we seemed to havereached (and surpassed) the threshold in which ma-chines are able to match human writings in terms ofpopularity criteria; and this means that popularitycriteria alone may not be enough anymore to per-form this type of evaluation; and we may need todive into the (turbulent) waters of how to properlyassess the output of a creative process.\nOverall, we believe that the superiority of SLMin our experiment is meaningful and has implica-tions in the field, but it should not be overstated: wemust extend the experimentation to more complexand naturalistic creative writing tasks, and we mustgo beyond popularity for a better understanding ofthe potential of SLM for this type of tasks.\nA Likert Distributions\nFigures 1 and 2 show the distribution of scores oneach item of the Likert scale (from 0 to 4) used inthe SLM vs. humans experiment.\nB Assessors Information\nWe recruited 68 volunteer participants for our study.All subjects recruited for our experimentation werestudents of an international Master in Management.All students were proficient in English (which isthe official language of the master), although theycome from different countries and therefore havedifferent cultural backgrounds. They also have"}, {"title": "C Additional Statistical Analysis", "content": "C Additional Statistical Analysis\nC.1 Standard Deviations of the MainExperiment\nTable 5 shows the means and standard deviations of the results of the main experiment.\nC.2 Linear Mixed-Effects Models\nLinear Mixed-Effects Models (Bates et al., 2014;Kaptein, 2016) is a statistical method that consist ofa linear regression where the model also includesrandom effects to account for the variance producedby the subjects and the set of the items (in ourcase, synopses) selected for the experiment. Wehave performed a Linear Mixed-Effects Modelsregression with two goals in mind. First, we wantto verify the results we observed in the previoussections with a more in-depth statistical analysis.And, secondly, we want to check if the subjectprofiles influence their assessments.\nWe have experimented with two different mod-els. Both included the random effects of assessors,title of the synopsis (with 60 possible values) andtheir interaction with the 'writer' (human or SLM).As fixed effects we study those that allow us tocheck the reliability of our analysis: the effect ofage, educational background (humanities or sci-ences), their native language and, mainly, the effectof the writer (human or SLM).\nFor each model, we checked that the error term isnormally distributed and that there is no correlationbetween the model predictions and the residual. Allmodels were computed with R 1merTest package(Kuznetsova et al., 2017)"}, {"title": "Model for subject profiles", "content": "Model for subject profiles The first regressionis defined to investigate the behavior of the synopses scores with age, educational level, languageand the writer in interaction with the aspects. Theresulting model revealed a significant main effectof the writer [F5,7419 = 117.23, p < .0001"}]}