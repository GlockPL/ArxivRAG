{"title": "SEAL: SEMANTIC-AUGMENTED IMITATION LEARNING VIA LANGUAGE MODEL", "authors": ["Chengyang Gu", "Yuxin Pan", "Haotian Bai", "Hui Xiong", "Yize Chen"], "abstract": "Hierarchical Imitation Learning (HIL) is a promising approach for tackling long-horizon decision-making tasks. While it is a challenging task due to the lack of detailed supervisory labels for sub-goal learning, and reliance on hundreds to thousands of expert demonstrations. In this work, we introduce SEAL, a novel framework that leverages Large Language Models (LLMs)'s powerful semantic and world knowledge for both specifying sub-goal space and pre-labeling states to semantically meaningful sub-goal representations without prior knowledge of task hierarchies. SEAL employs a dual-encoder structure, combining supervised LLM-guided sub-goal learning with unsupervised Vector Quantization (VQ) for more robust sub-goal representations. Additionally, SEAL incorporates a transition-augmented low-level planner for improved adaptation to sub-goal transitions. Our experiments demonstrate that SEAL outperforms state-of-the-art HIL methods and LLM-based planning approaches, particularly in settings with small expert datasets and complex long-horizon tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "The advancement of LLMs brings transformative change to how agents learn to interact and make decisions (Brohan et al., 2023; Wang et al., 2023). LLMs like GPT-4 (Achiam et al., 2023) possess remarkable semantic understanding ability (Liu et al., 2023), human-like reasoning capability (Wei et al., 2022), and rich common sense knowledge (Bubeck et al., 2023), enabling them extracting insights from language instructions to support decision-making agents (Eigner & H\u00e4ndler, 2024).\nA promising path towards LLM-assisted decision-making is to improve Deep Reinforcement Learn-ing (DRL) agents through reward design (Kwon et al., 2023; Ma et al., 2023). Yet DRL often suffers from sample inefficiency and still requires extensive interactions with the environment particularly for tasks with long planning horizons and sparse rewards, where agents are rewarded only upon task completion (Zhang et al.). Alternatively, Imitation Learning (IL) can be trained to learn generaliz-able policies from expert demonstrations (Schaal, 1996). By learning from successful state-action pairs, IL avoids the sample-expensive exploration and exploitation required in DRL. While IL per-formance can still be limited in long-horizon tasks due to compounding errors with accumulated errors leading to significant deviations from desired trajectories (Nair & Finn, 2019). Hierarchical Imitation Learning (HIL) (Le et al., 2018a) leverages the sub-goal decomposition of long-horizon tasks into a multi-level hierarchy and reduces the relevant state-action space for each sub-goal such as goal-states (Ding et al., 2019) and Task IDs (Kalashnikov et al., 2021). To address this, lan-guage instruction has come as an aid for sub-goal specification, as such instruction can be both informative and flexible (Stepputtis et al., 2020). Language-conditioned HIL approaches either train the high-level sub-goal encoder and low-level policy agent separately (Prakash et al., 2021) or jointly (Hejna et al., 2023). Though both methods have achieved impressive results, learning language-based sub-goals remains challenging, as it requires a large number of expensive sub-goal labels (Chevalier-Boisvert et al., 2018a). To overcome this, various methods have been proposed to infer both sub-goal boundaries and supervision-free representations (Garg et al., 2022; Jiang et al.,"}, {"title": "2 RELATED WORKS", "content": "Imitation Learning. Imitation Learning encompasses two primary approaches: Behavioral Cloning (BC) (Bain & Sammut, 1995) and Inverse Reinforcement Learning (IRL) (Ng et al., 2000). BC re-lies on a pre-collected expert dataset of demonstrations, where the agent learns to mimic the actions in an offline manner. While BC is simple to implement, it is prone to compounding errors, partic-ularly when the agent encounters states not present in the expert's demonstrations (Zhang, 2021). In contrast, IRL methods (Ho & Ermon, 2016; Reddy et al., 2019; Brantley et al., 2019) involve"}, {"title": "3 PRELIMINARY", "content": "In this paper, we look into the long-horizon, compositional decision-making problem as as a discrete-time, finite-step Markov Decision Process (MDP). MDP can be represented by a tuple $(S, A, T, r, \\gamma)$, where $S, A$ denotes the state and action space, $T(S_{t+1}|S_t, a_t) : S \\times A \\rightarrow S$ denotes the transition function, $r : S \\times A \\rightarrow R$ is the reward function and $y \\in [0, 1]$ is the discount factor.\nIn standard settings of Hierarchical Imitation Learning (HIL), instead of having access to the reward function r, the agent has access to a dataset of expert demonstrations $D = \\{T_1, T_2, ..., T_N\\}$, which contains N expert trajectory sequences consisting of state-action pairs $\\{(s_t, a_t)\\}$, where $s_t \\in S$, $a_t \\in A$, T is the time horizon for planning, 0 \u2264 t \u2264 T. In this paper, the expert trajectories are not labeled with any rewards nor subhorizon segments. We assume HIL agents operate in a two-level hierarchy though our method can also be applied to problems with more levels:\n\u2022 High-level Sub-goal Encoder $\\pi_H(g_t|s_t)$: Selects a sub-goal $g_t \\in G$ based on the current states $s_t$, where G is the space of sub-goals.\n\u2022 Low-level Policy Agent $\\pi_L(a_t|g_t, s_t)$: Executes actions conditioned on both the current state $s_t$ and sub-goal $g_t$.\nIn this work, we focus on settings where agents lack access to the sub-goal space G, relying in-stead on an oracle full task instruction M in natural language. While well-defined G aids efficient HIL agent learning (Hauskrecht et al., 2013), its acquisition is difficult due to missing task-specific knowledge (Nachum et al., 2018; Kim et al., 2021). Natural language task instructions, though eas-ier to obtain as they are common-used commands from human (Stepputtis et al., 2020), are hard to map to hierarchical structures due to their complex and ambiguous nature (Zhang & Chai, 2021; Ahuja et al., 2023; Ju et al., 2024). In this work, we investigate leveraging LLMs to parameterize G from language instructions with its powerful semantic and world knowledge, and pre-label states in D to guide effective learning of $\\pi_H$ and $\\pi_L$ in hierarchical imitation learning."}, {"title": "4 SEAL FOR HIERARCHICAL IMITATION LEARNING", "content": "The key idea of SEAL is to learn high-level sub-goal representations using supervisory labels gen-erated by LLMs. In previous works, such labels were typically provided by human experts via instructions (Pan et al., 2018; Le et al., 2018a), making them expensive to obtain. However, with the assistance of LLMs, we introduce an efficient and reliable method to automatically generate labels that map states to sub-goals. Specifically, LLMs are used to semantically extract a high-level plan from the full-task language instruction M and map states in expert demonstrations to sub-goals within this plan. Using these learned sub-goal representations, the model then learns the corresponding low-level actions. An overview of our SEAL framework is illustrated in Fig. 1.\n4.1 PRETRAINED LLMS FOR GUIDING SUB-GOALS LEARNING\nOur key design of leveraging LLMs to guide high-level sub-goals learning can be divided into two stages: (i) Use LLM-generated high-level plan based on full-task instruction as sub-goal space (ii) Use LLMs to encode states in expert demonstrations to sub-goal representations.\nDerive Sub-goal Space of Task Prior works have demonstrated that LLMs can establish a mean-ingful chain of sub-tasks from task instruction as high-level plan (Huang et al. (2022); Prakash et al. (2023); Singh et al. (2023)). Yet few of them incorporate it with Hierarchical Imitation Learning (HIL). In SEAL, we use LLMs to specify the unknown sub-goal space G in HIL formulations. Feed-ing LLMs with the full-task language instruction M, we notice that the decomposed sub-goals in high-level plan naturally consist of a language-based sub-goal set: $\\{\\hat{g}^1,\\hat{g}^2,...,\\hat{g}^K\\} = f_{llm}(M)$, where K is the total number of generated sub-goals. We treat this estimated sub-goal dataset as the finite sub-goal space: $G = \\{\\hat{g}^1, \\hat{g}^2, ..., \\hat{g}^K \\}$.\nLabeling Sub-goals for States in Expert Dataset After devising the sub-goal space G with LLM-generated sub-goals, we use them to map states $s_t \\in D$ to a sub-goal latent space. These LLM-defined labels guide the high-level encoder to learn task-relevant sub-goal representations. To pa-rameterize the language-based sub-goals $\\hat{g}^i \\in G$ and facilitate learning, we establish a codebook $C = \\{z_1, z_2, ..., z_K\\}$, where each latent variable $z_i \\in R^K$ is a one-hot vector (i.e. i-th element in $z^i$ equals to 1, others equal to 0, i = 1, 2, ..., K) representing sub-goal $\\hat{g}^i$ in G. We then prompt the same LLM to perform a encoding function $h_{llm}$, which map $s_t$ to latent vector $z^{(ref)} \\in C$ by checking whether it belongs to sub-goal $\\hat{g}^i \\in G$: $z^{(ref)} = h_{llm}(s_t, G)$. We stipulate the output of LLM must be 'yes' or 'no' and then convert it to integer 1 or 0, as this form of answer has shown to be more reliable than the open-ended answer (Du et al. (2023)). By repeatedly asking K times we can finally establish the K-dim latent variable $z^{(ref)}$ which represents the sub-goal for all $s_t$ in D."}, {"title": "4.2 DUAL-ENCODER FOR SUB-GOAL IDENTIFICATION", "content": "Naturally, we consider using these LLM-generated labels for sub-goal representations to train a high-level sub-goal encoder $\\pi_H(s_t)$ in a supervised manner. Compared to previous unsupervised approaches, this supervised method helps reduce the randomness of output sub-goals by leveraging the guidance provided by the labels. However, it is prone to over-fitting on the training dataset. To address this challenge, inspired by (Ranzato & Szummer (2008); Le et al. (2018b)), we pro-pose a Dual-Encoder structure for high-level sub-goal identification. This design integrates both a supervised learning encoder and an unsupervised learning encoder, producing a weighted-average sub-goal representation. The weighted combination allows for flexibility, prioritizing the encoder that performs better for a particular task or dataset, ultimately enhancing robustness and improving generalization.\nSupervised LLM-Label-based Encoder Considering that the codebook C, representing the sub-goal space G, is discrete and finite, we formulate the supervised sub-goal learning as a multi-class classification problem. To train this supervised learning encoder $\\pi_H^{(llm)}$, we define the sub-goal learning objective by maximizing the log-likelihood of the labels generated by the LLMs:\n$L_H^{(llm)} = E_{(s_t,z^{(ref)}) \\sim D} -log \\pi_H^{(llm)} (z^{(ref)}|s_t)$.\nUnsupervised VQ Encoder Given the codebook $C = \\{z_1, z_2, ..., z_K \\}$, we apply Vector Quantiza-tion (VQ) (Van Den Oord et al. (2017)) to design the unsupervised sub-goal encoder in our SEAL framework. It is a widely used approach that can map the input state $s_t$ to a finite, discrete latent space like C. In VQ, the encoder $\\pi_H^{(vq)}$ first predicts a continuous latent vector: $z_t^{(con)} = \\pi_H^{(vq)}(s_t)$. This latent vector is then matched to the closest entry in C:\n$z_t^{(vq)} = argmin_{z_i \\in C} ||z_t^{(con)} - z_i||_2^2$.\nThe learning objective of $\\pi_H^{(vq)}$, named commitment loss, encourages the predicted continuous latent vector $z_t^{(con)}$ to cluster to the final output sub-goal representation $z_t^{(vq)}$:\n$L_{vq} = E_{(s_t) \\sim D} ||sq(z_t^{(vq)}) - z_t^{(con)} ||_2^2$;\nwhere sq() denotes stop-gradient operation."}, {"title": "4.3 TRANSITION-AUGMENTED LOW-LEVEL POLICY", "content": "We compute a weighted-average vector $z_t$ over $z_t^{(llm)}, z_t^{(vq)}$ obtained by dual-encoders to finalize the predicted sub-goal representation:\n$z_t = W_{vq} z_t^{(vq)} + W_{llm} z_t^{(llm)}$\nwhere the weights $W_{vq}$ and $W_{llm}$ quantifies how the predicted sub-goal representations $z_t^{(vq)}$ and $z_t^{(llm)}$ contribute to the task completion success rate. The weights are updated by validations during the training process. The update details will be demonstrated in Section 4.4.\nGiven the predicted sub-goal representations $z_t$ for each $s_t$ in the expert dataset, normally the low-level policy agent follows a goal-conditioned behavioral cloning (GC-BC) architecture. It is trained by maximizing the log-likelihood of the actions in the expert dataset, using the sub-goal representa-tions as auxiliary inputs:\n$L_{GC-BC} = E_{(s_t,a_t,z_t) \\sim D} - log \\pi_L(a_t|s_t, z_t)$.\nHowever, this low-level policy design overlooks the imbalanced distribution and importance of the hierarchical structure captured by high-level sub-goal encoders. Several studies have highlighted that certain states, where transitions between sub-goals occur in long-horizon demonstrations, have a significant impact on the policy agent's performance (Jain & Unhelkar, 2024; Zhai et al., 2022;"}, {"title": "Definition 4.3.1. (Intermediate States)", "content": "Let $s_t \\in S, 0 \\le t \\le T$ be a state observed when running the HIL agent, $z_t$ is its corresponding latent variable learnt by high-level encoder $\\pi_H$ that represents sub-goal. $s_{t+1}$ is the following state. The state $s_t$ is defined as an intermediate state only when the sub-goal changes: $z_{t+1} \\neq z_t$. Due to the scarcity of these intermediate states, it becomes very challenging to imitate the correct behavior in such states. To address this issue, inspired by the practice of assigning extra rewards to sub-goal transition points in hierarchical RL (Ye et al. (2020); Berner et al. (2019); Zhai et al. (2022); Wen et al. (2020)), we augment the importance of these intermediate states by assigning higher weights to them in the low-level policy training loss:\n$L_L = E_{(s_t,a_t,z_t) \\sim D} e^{||z_{t+1} - z_t||_2} log \\pi_L(a_t|s_t, z_t)$;\nwhere the term $e^{||z_{t+1} - z_t||_2}$ measures the L2-distance between the current sub-goal representation $z_t$ and the next sub-goal $z_{t+1}$. Given that $z_t$ is a one-hot vector, we have the term:\n$e^{||z_{t+1} - z_t||_2} = \\begin{cases} e, & \\text{if } z_{t+1} \\neq z_t \\\\ 1, & \\text{if } z_{t+1} = z_t \\end{cases}$\nThus, this term can serve as an adaptive weight to enhance the imitation of expert behavior at inter-mediate states. By incorporating this transition-augmented low-level policy design, we emphasize the importance of sub-goal transitions, and in simulations we also observe this design can greatly help agents make transitions across each sub-goal."}, {"title": "4.4 TRAINING", "content": "We train our SEAL model end-to-end, jointly updating parameters of $\\pi_H$ and $\\pi_L$ by minimizing the loss function $L = \\beta L_H + L_L$, where $\\beta$ is a hyper-parameter that controls the weight of high-level sub-goal learning in relation to the overall training process. Additionally, in order to evaluate the reliability of the latent variables predicted by the VQ encoder and LLM-Label-based encoder and determine the weight combination that can better improve task performance, we keep validating the success rates of those two different latent variables in the environment during training. Based on the validation results, we dynamically update the weights $W_{vq}$ and $W_{llm}$ in Eq. 4.\nFor validation, we simultaneously execute actions conditioned on both the VQ-encoder and the LLM-label-based encoder: $a_t^{(vq)} = \\pi_L(s_t, z_t^{(vq)})$ and $a_t^{(llm)} = \\pi_L(s_t, z_t^{(llm)})$. We then run episodes to test the different success rates, $SR_{vq}$ and $SR_{llm}$, for completing the full task. The updated weights $W_{vq}$ and $W_{llm}$ are then computed as $W_{vq} = \\frac{SR_{vq}}{SR_{llm} + SR_{vq}}$; $W_{llm} = \\frac{SR_{llm}}{SR_{llm} + SR_{vq}}$ respectively. $W_{vq}$, $W_{llm}$ measure the relative task-completion perfor-mance of the policy agent under the guidance of $z_t^{(vq)}$ and $z_t^{(llm)}$, respectively. We refer to these weights as confidences, indicating the preference for trust between $z_t^{(vq)}$ and $z_t^{(llm)}$.\nWe also use these weights to finalize the overall training loss of SEAL as a weighted combination of two end-to-end losses under guidance $z_t^{(llm)}$ and $z_t^{(vq)}$. We finalize the overall training loss of SEAL by using a weighted combination of two end-to-end losses, conditioned on $z_t$, $z_t^{(llm)}$ and $z_t^{(vq)}$, with the same weights $W_{vq}$, $W_{llm}$ determining the contribution of each loss:\n$\\mathcal{L}_{vq} = \\beta L_H^{(vq)}(s_t) + L_L(s_t, z_t^{(vq)}); \\mathcal{L}_{llm} = \\beta L_H^{(llm)}(s_t) + L_L(s_t, z_t^{(llm)}); \\mathcal{L}_{SEAL} = W_{vq}\\mathcal{L}_{vq} + W_{llm}\\mathcal{L}_{llm}$.\nSince the low-level policy agent's actions are conditioned on the latent sub-goal representations, minimizing this weighted-combination loss $\\mathcal{L}_{SEAL}$ allows our SEAL to adapt the trainable param-eters of the low-level policy based on task-completion performance. This approach helps the agent make better decisions by adjusting to updated latent predictions $z_t = W_{vq} z_t^{(vq)} + W_{llm} z_t^{(llm)}$ dur-ing training process. As a result, our SEAL framework can continuously adapt both the high-level sub-goal encoders and the low-level policy agent, leading to more reliable and robust sub-goal rep-resentations, as well as improved decision-making for action selection. The complete algorithm for SEAL is illustrated in Algorithm 1."}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate the performance of SEAL on two long-horizon compositional tasks: KeyDoor and Grid-World. We compare SEAL's performance with various baselines, including non-hierarchical, unsupervised, and supervised hierarchical IL methods, in both large and small expert dataset scenarios. Following this, we analyze how SEAL enhances task completion performance.\n5.1 SIMULATION ENVIRONMENTAL SETUP\nKeyDoor The MiniGrid Dataset (Chevalier-Boisvert et al., 2018b) is a collection of grid-based envi-ronments designed for evaluating reinforcement learning and imitation learning algorithms in tasks requiring navigation, exploration, and planning. Among these environments, we start with KeyDoor, an easy-level compositional task that requires the player to move to the key and pick up it to unlock the door. To add complexity, we enlarge the original 3 \u00d7 3 grid environment to 10 \u00d7 10 size, and randomly initialize the locations of player, key and door for each episode. To facilitate understand-ing by LLMs, we convert the environment into a vector-based state, with elements including the coordinates of the player, key, and door, as well as the different statuses of the key (picked or not) and door (locked or not). The maximum time-steps T of one episode is set to 100. We evaluate our SEAL on expert datasets with 30, 100, 150, 200 demonstrations generated by an expert bot.\nGrid-World The environment is a 10x10 grid world with a single player and multiple objects ran-domly distributed at various locations. The player's objective is to visit and pick up these objects in a specific order. This task is more challenging than KeyDoor due to its longer-range compositional nature, involving more sub-goals. In this work, we set the number of objects in the grid world to range from 3 to 5, to test SEAL's effectiveness in solving longer-range tasks. Similar to KeyDoor, the fully observed environment is converted into a vector-based state, with elements representing the coordinates of the player and objects, as well as their statuses (picked or not). The maximum time-steps per episode is set to 100. We evaluate SEAL on expert datasets with 200, 300, and 400 demonstrations generated by an expert bot.\n5.2 BASELINES\nWe compare our SEAL with one non-hierarchical learning approach: Behavioral Cloning (BC) (Bain & Sammut, 1995), two unsupervised learning approaches: LISA (Garg et al.,"}, {"title": "5.4 PERFORMANCE ON LONGER COMPOSITIONAL TASKS", "content": "We further investigate whether SEAL can sustain its effectiveness and superiority on longer-range compositional tasks, which involve more sub-goals. To evaluate this, we test our method on Grid-World with 4 and 5 objects, where the LLMs decompose the task instructions into more sub-goals (K = 8 and K = 10, respectively). As shown in Table 3, SEAL continues to lead, particularly in cases with smaller expert datasets, demonstrating its adaptability to more complex tasks. In longer-range compositional tasks, managing the increasing complexity of the sub-goal space becomes more challenging, especially for supervised methods like Thought Cloning, as the supervision signal for each sub-goal becomes sparser. SEAL overcomes this by employing a dual-encoder design, which leverages both the flexibility of unsupervised learning to learn sub-goals better. Meanwhile, SEAL focuses more on imitating the few but critical sub-goal transition points in longer-range composi-tional tasks, avoiding the limitations of signal sparsity faced by other approaches."}, {"title": "5.5 ADAPTATION TO TASK VARIATIONS", "content": "We also test the adaptability of SEAL to task variations. To do this, we modify the predefined pick-up order in the Grid-World environment, which includes three objects: A, B, and C. This generates new tasks for evaluation. We create a dataset comprising 400 expert demonstrations for the task with the order ABC, along with few-shot set of 10 expert demonstrations for other orders such as ACB, BCA, and BAC. We then assess the performance of the trained agent on these alternative orders. As shown in Table 4, our method exhibits slightly higher success rates, indicating that SEAL has better adaptability to task variations. However, it is important to note that this conclusion is limited to specific scenarios. In the grid-world, rearranging the order does not introduce new sub-goals, meaning that the sub-goals learned from the training set remain applicable to these new tasks."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce SEAL, a novel HIL framework that leverages LLMs' semantic and world knowledge to define sub-goal spaces and pre-label states as meaningful sub-goal representations without prior task hierarchy knowledge. SEAL outperforms baselines like BC, LISA, SDIL, and TC across various environments, particularly in low-sample and complex longer-range compositional tasks. Our approach achieves higher success and sub-goal completion rates, with assistance of the dual-encoder proving more robust than the pure LLM encoder and the transition-augmented low-level policy. SEAL also adapts well to varying task complexities and latent dimensions. Our current design still observes training instability, and we are interested in making more efficient SEAL approaches while under partially observed states."}, {"title": "A ADDITIONAL ENVIRONMENT INFORMATION", "content": "KeyDoor The environment is based on the DoorKey setting from the MiniGrid Dataset, but with modifications to make the state compatible with LLM input for sub-goal mapping. Instead of us-ing image states, we convert the state into an 8-dimensional vector that captures crucial object in-formation: {x-coordinate of key, y-coordinate of key, x-coordinate of door, y-coordinate of door, x-coordinate of player, y-coordinate of player, key status (picked: 1, not picked: 0), and door status (unlocked: 1, locked: 0) }. Wall obstacles are removed to avoid interference. The action space consists of 6 primitive actions: move up, move down, move right, move left, pick up, and unlock. The key can be picked up only when the player reaches the key's coordinates, and the door can be unlocked only if the player reaches the door's coordinates with the key already picked up. The language task instruction M is defined as: \"Pick up the key, and then unlock the door.\" The episode ends when the door is successfully unlocked or the maximum time steps T = 100 are reached.\nGrid-World The environment is based on the grid world used in (Kipf et al., 2019; Jiang et al., 2022). Similar to KeyDoor, the image-based states are converted into a vector format for LLM input, capturing crucial information about objects: x and y coordinates of Object 1, x and y coordinates of Object 2, .., x and y coordinates of the player, status of Object 1 (picked: 1, not picked: 0), status of Object 2, ... For Grid-World with 3, 4, or 5 objects, the state vector has dimensions 11, 14, and 17, respectively. Wall obstacles and irrelevant objects are removed to avoid interference. The action space consists of 5 primitive actions: move up, move down, move right, move left, and pick up. An object can be picked up only when the player reaches its coordinates. The language task instruction M is defined as: \"Pick up Object 1, then pick up Object 2, then...\" The episode ends when the player picks up all objects in the correct order or after the maximum time step T = 100. At the start of each episode, the coordinates of all objects and the player are randomly reset.\nSub-goal Spaces Identified by LLMs We use GPT-40 to decompose the language task instructions for both the KeyDoor and Grid-World environments into their respective sub-goal spaces. In the KeyDoor environment, there are K = 4 sub-goals: {move to the key, pick up the key, move to the door, unlock the door}. In the Grid-World environment, with 3, 4, and 5 objects, the number of sub-goals is K = 6, K = 8, and K = 10, respectively, including: {move to object 1, pick up object 1, move to object 2, pick up object 2, ...}. For both sub-goal spaces, we parameterize each language sub-goal in it by a K-dim one hot vector."}, {"title": "B EXAMPLE PROMPTS", "content": "In SEAL, we prompt LLMs to generate supervisory labels for training the high-level encoder. Fig. 5 illustrates the detailed prompting process. First, we prompt the LLMs to break down the task instruction into a finite set of sub-goals. Then, for each state, the LLM is prompted K times to de-termine whether it corresponds to each of the decomposed sub-goals, mapping the states to sub-goal representations. Example prompts for both task decomposition and sub-goal labeling are provided in the following sub-sections."}, {"title": "B.1 PROMPTS FOR TASK DECOMPOSITION", "content": "An example prompt used in Grid-World experiment with 3 objects for task decomposition is listed below. We use GPT-40 to produce the answer.\n# Task Description: Pick up the key, then Pick up the ball, and then pick up the diamond.\n# Environment Details:\nThe environment is a 10\u00d710 2D Grid-world.\nObjects {Key, Ball, Diamond, Player} (Key, Ball, and Diamond's coordinates are fixed, The player can move)\nObservation Space: {01: The coordinate of the key\n02: The coordinate of the ball\n03: The coordinate of the diamond\n04: The coordinate of player itself\n05: The status of key (picked/not)\n06: The status of ball (picked/not)\n07: The status of diamond (picked/not)}\nAction Space: {move up/right/left/down, pick up}\n# Role Instruction:\nYou should give an abstract plan to solve the given task step-by-step. For each step in plan, you need to extract relevant features in observation space. You should answer in format:\n{Step 1:..., Relevant Features:{01,....}, Step 2:..., Relevant Features:{01,....} ... Step N: ..., Relevant Features:{01,....} }\nThe GPT-40 answer with:"}, {"title": "B.2 PROMPTS FOR MAPPING STATES TO SUB-GOAL REPRESENTATIONS", "content": "An example prompt used in KeyDoor experiment for mapping states in expert demonstration to sub-goal representations is listed below. We use GPT-40 to produce the answer.\n# Environment:\nA 2D-grid World with a key and and a door in it. The grid world's size is 10 * 10. The coordinate in grid is written as [x, y] (x=0,1,2,3,4, y=0,1,2,3,4) Key at the coordinate: [3, 3]. Door at the coordinate: [0, 1]. Key is not picked up. (key state = 0) Door is locked. (door state = 0)\n(Hint: You can only pick up the key when you at the key location, and can only unlock the door when you are at the door location and have already picked up the key).\n# Current State:\nThe player is currently at [3, 0]. The key state = 0. The door state = 0. The next coordinate of player is at [3,3].\n# Sub-goal Judgement:\nYou need to judge which stage the player in:\n1. The player should move to the Key.\n2. The player should pick up the Key.\n3. The player should move to the Door.\n4. The player should unlock the Door.\nPlease check whether the current state in the above stages in turn. For each judgement please answer with 1(Yes) or 0 (No).\nYou should finally answer with a 4-dimension vector format: [1/0, 1/0, 1/0, 1/0]"}, {"title": "C MODEL IMPLEMENTATION DETAILS", "content": "We outline the model implementation details for all four baselines and SEAL in the KeyDoor and Grid-World environments. For non-hierarchical BC baselines, we use a two-layer Multi-layer Per-ceptron (MLP) as the trainable policy agent $\\pi(a_t|s_t)$. In HIL approaches like LISA, SDIL, and TC, this same two-layer MLP is used for both the high-level sub-goal encoder $\\pi_H(s_t)$ and the low-level policy agent $\\pi_L(a_t|s_t, z_t)$. We use Adam (Kingma, 2014) as the optimizer for all models, with learning rates initialized at 5e-5 for KeyDoor and 5e-6 for Grid-World. To ensure fair comparison, we maintain consistent hyper-parameters across all simulations, including the high-level encoder loss weight $\\beta$, the hidden dimensions of the MLPs, and the number of sub-goals K for both HIL baselines and SEAL. Detailed implementations are presented in the following Table 5:"}]}