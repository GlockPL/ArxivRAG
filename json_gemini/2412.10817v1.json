{"title": "Enhance Vision-Language Alignment with Noise", "authors": ["Sida Huang", "Hongyuan Zhang", "Xuelong Li"], "abstract": "With the advancement of pre-trained vision-language (VL) models, enhancing the alignment between visual and linguistic modalities in downstream tasks has emerged as a critical challenge. Different from existing fine-tuning methods that add extra modules to these two modalities, we investigate whether the frozen model can be fine-tuned by customized noise. Our approach is motivated by the scientific study of beneficial noise, namely Positive-incentive Noise (Pi-noise or \\$\\pi\\$-noise), which quantitatively analyzes the impact of noise. It therefore implies a new scheme to learn beneficial noise distribution that can be employed to fine-tune VL models. Focusing on few-shot classification tasks based on CLIP, we reformulate the inference process of CLIP and apply variational inference, demonstrating how to generate \\$\\pi\\$-noise towards visual and linguistic modalities. Then, we propose Positive-incentive Noise Injector (PiNI), which can fine-tune CLIP via injecting noise into both visual and text encoders. Since the proposed method can learn the distribution of beneficial noise, we can obtain more diverse embeddings of vision and language to better align these two modalities for specific downstream tasks within limited computational resources. We evaluate different noise incorporation approaches and network architectures of PiNI. The evaluation across 11 datasets demonstrates its effectiveness.", "sections": [{"title": "Introduction", "content": "Vision and language are two crucial modalities in the real world. To build association and enable collaboration between these two modalities, vision-language (VL) models have emerged in recent years. Based on the classical dual-stream VL model CLIP (Radford et al. 2021), LLaVA (Liu et al. 2024b,a) has recently achieved remarkable performance. In these models, the embeddings of these two modalities are aligned during the pre-training phase. When applying CLIP models to downstream unseen data, inherent dataset bias can lead to misalignment between visual and linguistic representations. To realign these two modalities, fine-tuning all parameters requires extensive computational resources. Even with sufficient computational resources, full fine-tuning on limited downstream data also carries a risk of overfitting and forgetting the knowledge acquired during pre-training stage. Consequently, it is crucial to effectively and efficiently fine-tune VL models to achieve better alignment.\nTo address this challenge, existing methods such as prompt tuning (Lester, Al-Rfou, and Constant 2021) and adapter tuning (Houlsby et al. 2019) are proposed. Prompt tuning sets the word embeddings of prompts as learnable parameters and adapter tuning inserts a learnable shallow neural network to models. Different from these fine-tuning methods that add extra modules or concatenate extra tokens, we focus on an interesting question: Can a frozen model be tuned with customized noise?\nIt is motivated by Positive-incentive Noise (Pi-noise or \\$\\pi\\$-noise) (Li 2022; Zhang, Huang, and Li 2023), which quantitatively analyzes the impact of noise and investigates how to generate beneficial noise for a specific task. Different from traditional concepts that treat the randomness of noise as a harmful factor, the noise \\$\\epsilon\\$ can decrease the uncertainty of predictions and simplify the task if it satisfies\n$$I(T,E) > 0 \\Leftrightarrow H(T) > H(T|E),$$\nwhere T is the definition of a specific task from a probabilistic perspective, \\$I(\\cdot,\\cdot)\\$ denotes mutual information and H represents entropy. It should be clarified that our method is distinct from noise addition in data augmentation, as discussed in Section 2.2.\nThe advantages of noise to enhance alignment can be discussed from both vision and language perspectives. From the perspective of vision, the biases in different visual datasets (Liu and He 2024) can lead to performance degeneration in VL models. We hypothesize that the bias associated with each image follows a certain distribution, which can be mitigated by adding noise also from a specific distribution. As shown in Figure 4, the images with added noise exhibit more universal visual features. Additionally, the limited number of images makes it difficult to adequately sample from the continuous space of image embeddings. This dilemma can be alleviated by adding randomly sampled beneficial noise into the visual level, for which more diverse visual embeddings can be obtained. From the perspective of language, the templates of hand-crafted prompts and learnable prompts may be not effective enough, which limit the diversity of linguistic modality. Figure 1 shows two strategies and our proposed scheme for prompt construction. In the original CLIP (Radford et al. 2021), the hand-crafted prompt is used, e.g., a typical form \"a photo of a [class]\". Some prompt tuning works (Zhou et al. 2022b,a; Gao et al. 2024a) replace the hand-crafted prompt with trainable word embeddings to mine more precise description of images. These prompts are class-instanced. In other words, a class label corresponds to only one prompt. Nevertheless, a class should be related to various prompts (with different qualities). By sampling different beneficial noise from a customized distribution and adding it to hand-crafted prompts, new prompts can be easily obtained. It is thus easy to find that the prompts should also obey some probability distribution, ensuring the semantic richness of prompts.\nTowards prompt-based image classification task, we propose PiNI based on \\$\\pi\\$-noise theory which can enhance the alignment of VL models. Overall, the contributions can be summarized as follows:\n\\begin{itemize}\n    \\item We propose Positive-incentive Noise Injector (PiNI), a fine-tuning method for CLIP through injecting customized beneficial noise into it. To the best of our knowledge, this is the first noise-based approach to fine-tune CLIP.\n    \\item We reformulate the inference process of CLIP, by treating the prompt as a variable to analyze the effect of both images and prompts on the probability of category labels. It therefore offers a paradigm to refine CLIP from a probabilistic aspect.\n    \\item We apply variational inference and Monte Carlo method to converting the complex loss with noise distribution to a tractable one, which guides us in generating and injecting noise. Extensive experiments validate our idea.\n\\end{itemize}"}, {"title": "2 Related Works", "content": "2.1 Fine-Tuning Methods for VL Models\nIt is crucial to fine-tune VL models for downstream tasks with limited computational resources. To address this challenge, some Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed, primarily including parameter tuning, adapter tuning (Houlsby et al. 2019) and prompt tuning (Lester, Al-Rfou, and Constant 2021). The parameters of VL model are directly tuned in parameter tuning. LoRA (Hu et al. 2021) and Bitfit (Zaken, Goldberg, and Ravfogel 2022) focus on tuning the weights and biases of models, respectively. Adapter tuning inserts a trainable module named Adapter into a frozen model. CLIP-Adapter (Gao et al. 2024b) and VL-Adapter (Sung, Cho, and Bansal 2022) utilize Adapter in fine-tuning VL models. Prompt tuning was first proposed in NLP, which adds a learnable prompt template to pre-trained language models. Since CLIP model includes a text encoder, numerous works have attempted to introduce prompt tuning to CLIP (Zhou et al. 2022b,a; Gao et al. 2024a; Guo et al. 2023). Beyond text encoders, VPT (Jia et al. 2022) and MaPLe (Khattak et al. 2023) extend prompt tuning to visual encoders. Different from the above works, we focus on learning noise distribution to sample more visual and text embeddings without altering the architecture of VL models.\n2.2 Difference with Data Augmentation\nAdding noise during training as a data augmentation strategy (Cubuk Ekin et al. 2019) is common in computer vision tasks, such as adversarial training (Zhong et al. 2022; Zhang et al. 2023; Li et al. 2022). The data augmentation only participates in the training phase and is excluded during the inference phase. However, our proposed method injects noise for both the training and inference phases. For adversarial augmentation, it aims at adding perturbations to the input of networks, thereby enhancing robustness. Conversely, the noise in PiNI is employed to simplify tasks instead of creating difficulty for base models."}, {"title": "3 Method", "content": "When applying VL models to unseen downstream data, dataset bias (Liu and He 2024) can lead to misalignment in representations of vision and language. To mitigate this issue, we focus on achieving better vision-language alignment in the prompt-based image recognition task T. Inspired by \\$\\pi\\$-noise (Li 2022; Zhang et al. 2024), we propose a new scheme for learning beneficial noise distribution, namely Positive-incentive Noise Injector (PiNI).\nIn this section, we first reformulate the inference process of CLIP in Section 3.1. Section 3.2 introduces how to define the task entropy on the specific dataset X, which is vital to calculate the optimization objective \\$I(T, E)\\$ when generating \\$\\pi\\$-noise. To optimize this objective, the variational approximation is applied to getting its upper bound in Section 3.3. In the final approximate loss, we adopt a learnable function to approximate noise and determine the noise incorporation approaches, which are discussed in Section 3.4.\n3.1 Reformulation of CLIP Inference\nDifferent from classical image classification models that only require images as input, CLIP additionally needs a prompt set to obtain the position of each category in the embedding space. Obviously, the inference process of CLIP relies on the prompts. However, in traditional models, the label probability is usually modeled as \\$p(y|x)\\$, which ignores the role of prompts.\nIt is rational to introduce a separate variable for prompts. Given an image x and a set of prompts \\$\\rho_i \\in P, i \\in 1, 2, ..., N\\$, the probability of class \\$y \\in 1,2, ..., N\\$ can be reformulated as\n$$p(y|x, P) = \\frac{exp(sim(V(x), T(\\rho_i))/\\tau)}{\\Sigma_i exp(sim(V(x), T(\\rho_i))/\\tau)},$$\nwhere \\$V\\$ and \\$T\\$ represent the visual and text encoders, \\$sim(\\cdot,\\cdot)\\$ is the similarity between two vectors, and \\$\\tau\\$ is a temperature parameter. Note that \\$P\\$ is a variable, and there exists a one-to-one correspondence between prompt \\$\\rho_i\\$ and label y. In CLIP model, \\$sim(\\cdot, \\cdot)\\$ is the cosine similarity.\n3.2 \\$\\pi\\$-noise Regarding Prompts\nPrimarily, it is crucial to evaluate the complexity of a classification task on an arbitrary dataset X and prompt set P. Inspired by (Li 2022), task entropy can be used to formulate the complexity as\n$$H(T) = H(y|x,P)\\\\= E_{x\\sim D_x}E_{y\\sim p(y|x,P)}[-logp(y|x, P)] \\\\= E_{p(x,y/P)} [-log p(y|x, P)],$$\nwhere \\$D_x\\$ is the data distribution over X. This equation measures complexity by computing the uncertainty of labels. For a given image, it may resemble multiple categories, which increases the uncertainty of the label y. The noise E can decrease this uncertainty if it satisfies Eq. (1). The definition of \\$I(T, E)\\$ is\n$$I(T,E) = H(T) - H(T|E).$$\nSimilar to \\$H(T)\\$, \\$H(T|E)\\$ is defined as\n$$H(T|E) = E_{p(x,y,\\epsilon|P)}[-logp(y|x,\\epsilon, P)].$$\nAs \\$H(T)\\$ is a constant term for fixed CLIP model, maximizing \\$I(T, E)\\$ is equivalent to minimizing \\$H(T|E)\\$.\n3.3 Variational Inference for the Intractable Problem\nSince \\$p(y|x, \\epsilon, P)\\$ is difficult to calculate precisely due to the integral, we apply the variational inference technique (Blei, Kucukelbir, and McAuliffe 2017) to Eq. (5) and obtain a variational upper bound of \\$H(T|E)\\$ as\n$$\\mathcal{L}=E_{p(x,y,\\epsilon/P)} [-log q(y|x,\\epsilon, P)] > H(T|E),$$\nwhere \\$q(y|x, \\epsilon, P)\\$ is a tractable approximation of \\$p(y|x, \\epsilon, P)\\$. The above inequality is derived from the non-negative property of KL-Divergence as follows,\n$$KL(p||q) \\geq 0 \\Leftrightarrow E_{p(x)} [log p(x)] > E_{p(x)} [log q(x)].$$\nThe current problem is how to effectively compute this variational bound. Monte Carlo estimation can be applied to data distribution \\$D_x\\$ to obtain image-label pairs \\$(x_i, y_i)\\$. So the variatonal bound is further approximated as\n$$\\mathcal{L} = \\frac{1}{n}\\Sigma_{i=1}^{n}E_{p(\\epsilon|x_i,y_i,P)} [-log q (y_i|x_i, \\epsilon, P)].$$\nFor further calculation, \\$p(\\epsilon|x_i, y_i, P)\\$ needs to be simplified as there are three given variables. In Figure 2, the probabilistic graphical model illustrates the dependencies among variables. The noise and the label are conditionally independent given the prompt: \\$\\epsilon | y | \\rho\\$. Accordingly, we can obtain the following result from the conditional independence,\n$$p(\\epsilon|x, y, \\rho) = p(\\epsilon|x, \\rho).$$  \nThe above derivations indicate that the generated noise should be controlled by the set of prompts so that it can exclude interference from information irrelevant to prompts. Therefore, the variational bound is further transformed to\n$$\\mathcal{L} = \\frac{1}{n}\\Sigma_{i=1}^{n}E_{p(\\epsilon|x,\\rho)} [-log q(y_i|x_i, \\epsilon, P)].$$\nWe assume that \\$p(\\epsilon|x, P)\\$ follows a classical Gaussian distribution, whose distribution parameters \\$\\mu\\$ and \\$\\Sigma\\$ is approximated using a learnable function \\$f_{\\Theta}(x_i, P)\\$:\n$$(\\mu, \\Sigma) = f_{\\Theta}(x_i, P),$$\nwhere \\$\\Theta\\$ represents the parameters of the function. However, calculating the integral of the continuous variable \\$\\epsilon\\$ is difficult. The Monte Carlo method is applied again to address this problem. To ensure the back propagation of gradients in \\$f_{\\Theta}(x_i, P)\\$, the reparameterization trick (Kingma and Welling 2014) introduces an auxiliary variable \\$\\epsilon \\sim \\mathcal{N}(0, I)\\$ satisfying \\$p(\\epsilon|x_i, P)d\\epsilon = p(\\epsilon)d\\epsilon\\$. Subsequently, a learnable function \\$G_{\\phi}\\$ is established to generate noise\n$$\\epsilon = G_{\\phi} (\\hat{\\epsilon}, x_i, P) = \\Sigma_{\\phi}(x_i, P) \\cdot \\hat{\\epsilon} + \\mu_{\\phi}(x_i, P).$$  \nWe randomly sample \\$\\epsilon^j\\$ m times for each \\$x_i\\$ to get the final loss function for training\n$$\\mathcal{L}\\approx\\frac{1}{n}\\Sigma_{i=1}^{n}E_{p(\\hat{\\epsilon})} [-log q(y_i|x_i, G_{\\phi}(\\hat{\\epsilon}, x_i, P), P)] \\\\ =\\frac{1}{n.m}\\Sigma_{i=1}^{n}\\Sigma_{j=1}^{m}[-log q(y_i|x_i, G_{\\phi}(\\hat{\\epsilon}_{ij}, x_i, P), P)].$$\n3.4 Fine-Tuning with \\$\\pi\\$-noise\nIn the derived final loss Eq. (13), there are two important components: \\$q(y|x, \\epsilon, P)\\$ and \\$\\epsilon = G_{\\phi}(\\hat{\\epsilon}, x, P)\\$. The former predicts the labels with the aid of noise. The specific approaches for incorporating noise will be discussed below. The latter \\$G_{\\phi}(\\hat{\\epsilon}, x, P)\\$ defined in Eq. (12) consists of two phases: (1) Sampling: \\$\\hat{\\epsilon} \\sim \\mathcal{N}(0, I)\\$ and (2) Distribution parameter estimation: \\$(\\mu, \\Sigma) = f_{\\Theta}(x,P)\\$. \\$f_{\\Theta}(x, P)\\$ can be implemented using various neural network architectures, which will be introduced in the following discussion. The detailed framework of PiNI is illustrated in Figure 3.\nNoise Incorporation Approach When approximating \\$q(y|x, \\epsilon, P)\\$ to predict labels, we avoid altering the original inference process of CLIP as formulated in Eq. (2). It will remain the architecture of CLIP. A simple approach is to inject noise \\$\\epsilon\\$ into the two encoders. To simplify the discussion, we employ the classical factorization trick to decompose the noise into two components \\$\\epsilon = {\\epsilon_v,\\epsilon_t}\\$, where \\$\\epsilon_v\\$ and \\$\\epsilon_t\\$ are injected into the visual and text encoders, respectively. Note that similar to the classical Naive Bayes model (Bishop 2006), a hypothesis that \\$\\epsilon_v\\$ and \\$\\epsilon_t\\$ are conditional independent is established. According to this hypothesis, \\$\\epsilon_v\\$ and \\$\\epsilon_t\\$ can be generated and utilized separately.\nAs shown in Figure 3(b), the visual noise \\$\\epsilon_v\\$ can be injected into different locations of CLIP, including raw image or visual feature. For the text noise \\$\\epsilon_t\\$, it can be injected into word embeddings of prompts or text features. (Zhang, Zhu, and Li 2024) proposes to train multiple modules simultaneously in GNN. We will compare the effects of different"}, {"title": "4 Experiments", "content": "4.1 Benchmark Settings\nDatasets To evaluate the performance of PiNI, 11 datasets covering a wide range of visual concepts are selected. They include two generic object datasets, ImageNet (Deng et al. 2009) and Caltech101 (Fei-Fei, Fergus, and Perona 2004); five fine-grained datasets, OxfordPets (Parkhi et al. 2012), StanfordCars (Krause et al. 2013), Flowers102 (Nilsback and Zisserman 2008), Food101 (Bossard, Guillaumin, and Van Gool 2014) and FGVCAircraft (Maji et al. 2013), which contain fine-grained categories of pets, cars, flowers, food and aircraft, respectively. The other datasets are scene recognition dataset SUN397 (Xiao et al. 2010), action recognition dataset UCF101 (Soomro, Zamir, and Shah 2012), describable textures dataset DTD (Cimpoi et al. 2014) and EuroSAT (Helber et al. 2019) which contains satellite images. These datasets are abbreviated as Net, Caltech, Pets, Cars, Flowers, Food, Air, SUN, UCF, DTD and SAT.\nBaseline Methods PiNI is compared with four baseline methods: Zero-shot CLIP (ZS) (Radford et al. 2021), Linear Probe (LP), CoOp"}, {"title": "4.2 Exploring the Framework of PiNI", "content": "Based on the discussion in Section 3.4, we explore different noise incorporation approaches and network architectures for noise distribution parameter estimation. We employ a 10-layer convolutional network with residual connections as the CNN architecture. For the cross-attention architecture, we input the text feature as the key and value. The query is the image's spatial feature when injecting into raw images, as shown in Figure 3(e). When visual features are injected,"}, {"title": "4.3 Few-Shot Learning", "content": "The performance of different methods across 11 datasets is shown in Figure 5. From the figure, it is clear that PiNI has significant performance improvement. The results averaged across 11 datasets are provided in Table 2. It can be observed that the performance improvement is more pronounced with fewer shots.\nThe bar chart shown in Figure 6 illustrates the performance improvement of PiNI compared to Zero-shot CLIP. The maximum performance improvement is 38.18% on EuroSAT. On ImageNet, Caltech101, OxfordPets, and Food101, the performance improvements are relatively small as these categories are extensively contained in the corpus during pre-training stage.\nCompared to Linear Probe, PiNI performs worse on FGV-CAircraft. The aircraft names in FGVCAircraft are rarely included in the corpus of the pre-training phase, such as \"Boeing 737\". These specialized terms make prompt embeddings inaccurate in representing the fine-grained categories of aircraft. However, Linear Probe only utilizes the visual encoder for classification, which eliminates the interference of inaccurate prompt embeddings."}, {"title": "4.4 Domain Generalization", "content": "In domain generalization experiments, the model is trained on the source dataset ImageNet, and tested on target datasets. The target datasets are four variants of ImageNet, namely, ImageNetV2 (Recht et al. 2019), ImageNet-Sketch (Wang et al. 2019), ImageNet-A (Hendrycks et al. 2021b) and ImageNet-R (Hendrycks et al. 2021a). Detailed results are shown in Table 3. PiNI outperforms other methods except on ImageNet-A, exhibiting strong robustness against distribution shift."}, {"title": "4.5 Ablation on Visual Backbones", "content": "To investigate the impact of visual backbones in CLIP, we conduct 16-shot experiments, as shown in Table 4. These visual backbones include RN-50, RN-101, ViT-B/32, and ViT-B/16. On the two generic object datasets, ImageNet and Caltech, PiNI consistently demonstrates superior performance across different visual backbones."}, {"title": "5 Conclusion", "content": "In this work, we propose PiNI, a noise-based fine-tuning method towards vision-language alignment. It implies a new scheme to learn beneficial noise distribution. In our experiments, we demonstrate that PiNI outperforms existing methods. Our work can be further extended by refining the task entropy in Eq. (3). In this work, we select prompt-based image classification as the primary task. However, CLIP has a wide range of applications, such as Visual Question Answering (VQA), object detection, and image generation. Through defining task entropy on these tasks, the beneficial noise can be generated to simplify their complexity."}]}