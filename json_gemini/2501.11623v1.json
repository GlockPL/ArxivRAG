{"title": "EARLY EVIDENCE OF HOW LLMS OUTPERFORM TRADITIONAL SYSTEMS ON OCR/HTR TASKS FOR HISTORICAL RECORDS", "authors": ["Seorin Kim", "Julien Baudru", "Wouter Ryckbosch", "Hugues Bersini", "Vincent Ginis"], "abstract": "We explore the ability of two LLMs \u2013 GPT-40 and Claude Sonnet 3.5 to transcribe historical handwritten documents in a tabular format and compare their performance to traditional OCR/HTR systems: EasyOCR, Keras, Pytesseract, and TrOCR. Considering the tabular form of the data, two types of experiments are executed: one where the images are split line by line and the other where the entire scan is used as input. Based on CER and BLEU, we demonstrate that LLMs outperform the conventional OCR/HTR methods. Moreover, we also compare the evaluated CER and BLEU scores to human evaluations to better judge the outputs of whole-scan experiments and understand influential factors for CER and BLEU. Combining judgments from all the evaluation metrics, we conclude that two-shot GPT-40 for line-by-line images and two-shot Claude Sonnet 3.5 for whole- scan images yield the transcriptions of the historical records most similar to the ground truth.", "sections": [{"title": "1 Introduction", "content": "Digitalizing historical documents has been a tedious task for many governments and has allowed research opportunities for many. Besides manual efforts, optical character recognition (OCR) and handwritten text recognition (HTR) models have been the mainstays of digitizing historical documents. Given the increasingly expanding capabilities of the large language models (LLMs), we aim to study how much of the transcription process can be facilitated by the sole use of LLMs compared to that of the traditional OCR/HTR pipelines. For this study, we focus on a tabulated historical record where the table's header is filled with optical characters while the rest contains handwritten texts.\nHistorical records are more challenging to transcribe than modern documents due to cursive handwriting styles, the degraded quality of the texts (e.g., faded inks or damaged paper), language changes, and document layouts. The current OCR/HTR pipelines typically begin by pre-processing scans of the documents by adjusting the contrast and color of the image and cropping if necessary. Then, they analyze their layout and segment and then recognize the texts. In addition, layout analysis/segmentation and text recognition often accompany fine-tuning the user's data set. Unlike the three-step process of the classical OCR/HTR pipelines, LLMs can provide a one-step solution from the user's view, where only the final outputs need to be checked. Yet, their performability compared to the conventional methods remains unexplored."}, {"title": "2 Similar work", "content": "In [3], the authors introduce a new method for text recognition called Decoder-only Transformer for Optical Character Recognition (DTrOCR). DTrOCR uses only a decoder, using a pre-trained generative language model, in contrast to traditional encoder-decoder methods [14]. The authors tested whether a successful natural language processing model could be applied to text recognition in computer vision. Their experiments showed that DTrOCR significantly outperformed current state-of-the-art methods in recognizing printed, handwritten, and scene text in both English and Chinese.\nIn [6], the author introduces a method to digitize over 100,000 historic plans from the Swiss Archive for Landscaping Architecture using AI models. The approach employs a three-model architecture: a layout model to identify text, an OCR model to extract words, and a named entity recognition (NER) model to label key information. K-means clustering groups text blocks for OCR processing. Various deep-learning models were evaluated, including German BERT for NER, and retrained on the NVIDIA DGX-2 system. The pipeline achieved an F1 score of 48%, with the NER model scoring 86% and the OCR model correctly extracting 54% of words.\nIn [2], the authors propose a novel tokenized representation of digital ink for online handwriting recognition, address- ing the shortcomings of naive OCR with vision-language models (VLMs). Integrating stroke sequences and images, this approach achieves state-of-the-art quality on three public datasets. Their findings show that VLMs benefit from multimodal inputs, that images are crucial when text representations are too long, and that multiple handwriting tasks can be combined effectively. The method is compatible with both parameter-efficient tuning and fine-tuning, suggest- ing future exploration of various handwriting task combinations in large VLMs.\nThe following studies have adopted LLMs in their OCR/HTR process as an assistant to correct the outputs after OCR. In [12], the authors address the challenge of poor OCR quality in digitized historical documents, which is a barrier to humanities research. Traditional post-OCR correction methods use sequence-to-sequence models. Instead, the authors propose using generative language models with a prompt-based approach. They demonstrate significant improvements in OCR error correction by tuning Llama 2 with prompts and comparing it to a fine-tuned BART model on 19th-century British newspaper articles. Llama 2 achieves a 55% reduction in the CER, outperforming BART's 23% reduction. This approach shows promise for researchers in improving the accessibility of historical texts by an LLM.\nSimilarly, in [1], the authors conducted a comparative study of the ability of fourteen LLMs to correct transcriptions produced using OCR, HTR, and ASR. They then evaluate these corrections by comparing them with ground truths from each document. They conclude that, although GPT-4 appears to be the best model among those tested, all the models degrade rather than improve transcriptions. And that, on the whole, LLMs are better at detecting errors than at correcting them, as they are subject to overcorrection.\nWhile these studies have explored different methods to improve OCR systems, to our knowledge, not many studies have used LLMs as a main transcription tool. [10] was an earlier adopter in exploring the capabilities of GTP-4 Vision in several OCR and HTR tasks for English and Chinese datasets, and [4] used German typed text datasets to explore the same model for OCR tasks. However, since the experiments were conducted at the early stage of GPT-4 Vision, the authors in [10] used the web-based dialogue interface for the experiments, hindering them from examining different strategies, including prompting, few shots, and fine-tuning. Also, [4] did not explore different strategies, and the experiment was limited to reading typed texts rather than handwritten texts.\nTherefore, our contribution lies in exploring the pure performability of LLMs in transcribing historical records, and different strategies to improve their performance."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Dataset", "content": "We use Belgian probation data from 1921. The dataset includes 20 scanned pages of D\u00e9claration de Succession from Nivelles, a French-speaking city in Belgium. The main language of the record is French, but some names are of"}, {"title": "3.2 Metrics", "content": "In our analysis, we consider human transcriptions to be the ground truth (GT). First, the draft is made by one of the authors and then the names of the deceased and the declared and the locations of the death that appeared in the document are verified with the online search environment of the State Archives of Belgium, Agatha, and the online genealogy database, Geneanet. This draft is again checked by another author in the same manner. Once all the documents were transcribed, we reviewed the transcriptions again to harmonize the styles and agree on some uncertainties. Despite these thorough checks by the authors, errors may still exist in the GT data. Nonetheless, given that these transcriptions are consistent in quality and verified multiple times, we believe they are representative data to be compared with the LLM and OCR/HTR outputs.\nWe employ the CER and BLEU scores to evaluate the performance of different methods and models compared to the GT."}, {"title": "3.2.1 CER", "content": "In the OCR community, the CER metric was popularized by [9], and the exact method used in this research has been implemented in [5]. The CER measures the similarity between the predicted transcription and the GT by calculating the edit distance (Levenshtein distance) between the two strings. The edit distance is the minimum number of single- character edits (insertions, deletions, or substitutions) required to change the predicted transcription into the GT. The equation of the CER is given by\n$CER = \\frac{S+D+I}{N}$\nwhere S represents the number of substitutions, D denotes the number of deletions, I indicates the number of inser- tions, and N refers to the total number of characters in the GT.\nA lower CER indicates a better match between the predicted transcription and the ground truth. Thus, a CER of 0% represents a perfect match. The perfect match, here, implies matching the position and number of characters in two transcriptions as well as the spaces. For instance, for a predicted text [Arr\u00eat\u00e9 le vingt-et-un novembre 1919] and a GT text [ Arr\u00eat\u00e9 le vingt-et-un novembre 1919], even though they resemble perfectly, due to the white space in the start and end of the candidate text, the CER is 2/35 \u2248 0.057, not 0. For this reason, CER is particularly suited for evaluating text recognition models where even minor character errors can significantly impact the readability and accuracy of the transcription."}, {"title": "3.2.2 BLEU", "content": "The BLEU score, introduced in [8], is a metric for evaluating the quality of machine-translated text by comparing it to a set of reference translations. BLEU scores range between 0 and 1, with 1 indicating perfect similarity to the reference. The BLEU score is given by\n$BLEU = BP \\exp (\\sum_{n=1}^{N} w_n \\log p_n)$,\nwhere BP is the brevity penalty, penalizing the candidate translations shorter than the reference, pn is the precision for modified n-grams of order n, and wn is the weight for each n-gram (commonly set to $1/N$ for balanced n-gram contributions). The precision pn is given by\n$P_n = \\frac{\\sum_{g \\in Candidate} \\min(C(g), R(g))}{\\sum_{g \\in Candidate} C(g)}$\nwhere C(g) is the count of n-gram g in the candidate, and R(g) is the count of n-gram g in the reference. The BP is given by"}, {"title": "", "content": "$BP = \\begin{cases}\n1 & \\text{if } c > r \\\\\n\\exp (1 - \\frac{r}{c}) & \\text{if } c \\leq r,\n\\end{cases}$\nwhere c represents the length of the candidate and r the length of the reference.\nAs BLEU uses n-gram, the word's position in the sentence or the number of spaces does not affect the score like in the CER. In the same example of the candidate text, [Arr\u00eat\u00e9 le vingt-et-un novembre 1919] with respect to the reference text, [ Arr\u00eat\u00e9 le vingt-et-un novembre 1919], would score 1, a perfect match, in BLEU, which was around 0.057 in CER. However, since the accuracy of the n-grams matters, BLEU scores are more sensitive to the capitalization of the word and the symbols and accents in the word. Therefore, for the candidate texts like [arr\u00eat\u00e9 Le vingt et un Novembre 1919], only one uni-gram matches the reference text \u2013 that is, [1919], giving 1/7 as a uni-gram precision, p\u2081 and 0 for more than bi-gram precisions. If we take the default N = 4, then the precisions are [1/7,0,0,0], returning 0 BLEU score since log 0 is undefined. Usually, when using BLEU scores for evaluation translations, multiple reference texts are given. However, in our case, since only one GT exists, the scores are generally lower than when multiple references exist.\nAlthough BLEU scores are known to resemble the human evaluations of translations' quality, whether this will also be true for transcription tasks where only one reference text is given is less known. Therefore, in addition to evaluating the LLM and OCR outputs, we will also examine the possible factors influencing the BLEU and CER scores in Section 4."}, {"title": "3.3 Experiments", "content": "In our experiment, two approaches are adopted to evaluate the performances of LLMs and OCR/HTR systems: (1) us- ing each of the 20 whole scans as an input and (2) splitting the images line by line and feeding each line as an input. As mentioned in Section 3.2, at the start of the experiment, we created the GT dataset, which served as our reference standard. This dataset was carefully compiled to reflect the exact text and layout of the original documents in Excel sheets and then converted to a text format when using them. When evaluating the line-by-line outputs by LLMs and OCR/HTR systems, each row is split using Adobe Photoshop, and the two-level header is considered in one row rather than two separate rows. Here, the splitting can also be automated with Python or ImageMagick.\nOnce the GT dataset is done, we let LLMs read the document entirely or line by line, using five distinct strategies: simple prompt, complex prompt, one-shot, two-shot, and refine. This research compared two state-of-the-art LLM models: Claude Sonnet 3.5 (20240620) and GPT-40 (gpt-40-2024-08-06). The former was chosen because other versions, such as Haiku, regularly returned data privacy error messages, preventing us from obtaining the desired output. The latter, from OpenAI, was selected because some literature like [1] has already shown its great performance in correcting the OCR/HTR outputs. Note that when the document is read entirely by LLMs, the image size for the Claude Sonnet 3.5 model had to be reduced by a factor of 3 due to restrictions imposed by Anthropic. However, since the initial images were in very high resolution, their quality was not affected.\nAs summarized in Table 2, five strategies used different prompts except for one-shot and two-shots, which share the same prompt structure. We compare the impact of descriptive prompting by using a simple prompt asking LLMs to recreate the table they see in the image and a complex prompt with detailed descriptions of the document, both being zero-shot. One-shot and two-shot approaches are included to further analyze the impact of example-based guidance, providing the model with one or two example images and transcriptions, respectively. Additionally, despite our goal of replicating the historical records as they are, given that some degree of reasoning (e.g., actif-passif = restant, the family name of the deceased and the declarant may be the same) can facilitate the task, we also employ the refine technique on the outputs with the complex prompt, allowing the model to improve its outputs iteratively. Note that sometimes, the LLMs may return an error with a message such as [Sorry, I cannot read the image]. In this case, we force them to output something with the additional anti-error prompt, as shown in Table 2. If an error message still exists as an output, we rerun that particular document in the case of the whole-scan experiment and that particular row in the case of the line-by-line experiment.\nSimultaneously, we process the same set of scanned documents as a whole and line-by-line \u2013 with four different OCR/HTR methods in their baseline, pre-trained states without any fine-tuning to observe the pure performances of these systems: EasyOCR, Pytesseract, Keras, and TrOCR. Note that for EasyOCR, we set the language to French. Given the conventional HTR pipelines with these systems where layout analysis and fine-tuning are involved, we also test TrOCR with two fine-tuning variants: one fine-tuned on 20% and the other on 50% of the data with 6 epochs. For a comparison, we will run the whole-scan experiment, fine-tuning 20% of the data. This experiment requires a longer computation time than the line-by-line experiment, approximately two executive days. Considering this long computation time and poor performance, as we will discuss in the results section, fine-tuning 50% of the whole-scan data is not performed."}, {"title": "4 Results", "content": "Before assessing the results, it should be noted that for the line-by-line experiments, we used the maximum order of n- grams of 3, while it is 4 for the whole-scan experiments. This decision was made since text lengths in the line-by-line experiments are usually below 30 except for the header, which had 152 words. Having such a short prediction text, the maximum order of n-grams of 4 results in too many zero BLEU scores. Conversely, setting the maximum order of n-grams too low results in disproportionately high BLEU scores, complicating meaningful comparisons between outputs."}, {"title": "4.1 Whole-scan vs. Line-by-line experiments", "content": "When comparing BLEU scores between the two types of experiments, we notice the smaller variances in BLEU scores in the whole-scan experiments than in the line-by-line experiments. This may be due to the fact that the sample size is larger in the line-by-line experiments than in the whole-scan experiments (283 vs. 20). Moreover, the header, which is typed and remains the same over 20 documents, is often well transcribed in the line-by-line experiments. Therefore, if each of the 20 headers is perfectly transcribed, it increases the maximum BLEU score of the method and decreases its minimum CER score."}, {"title": "4.2 BLEU vs. CER vs. Human Evaluations", "content": "As observed in the previous subsections, BLEU and CER scores do not always concord. Especially when comparing LLMs to OCRs, BLEU scores in both experimental types show more distinct differences between LLMs and OCRS than CER scores. This difference arises from their different approaches to measuring similarities between the reference and candidate text. The CER score measures the similarities by counting each correct character, including spaces, and how many insertions and deletions it requires to achieve the correct text. The BLEU score, however, focuses on n-grams of words, regardless of their positions in the text. Accordingly, the CER is sensitive to the white spaces, and the BLEU is sensitive to the normalization of the words and the choice of the maximum order in n-grams.\nGiven the weakness of the two scores, we organized evaluations for whole-scan experiments to investigate which metric better represents the transcription quality according to human judgment. When the two lead authors evaluated the outputs of fourteen methods from whole-scan experiments, the two-example prompt by Claude Sonnet 3.5 returned the best outputs in terms of format and content, the detailed results of these evaluations are given in Table 1. Note that according to BLEU, it was GPT-40 with the refine prompt, and it was Claude Sonnet 3.5 with the complex prompt according to CER.\nThe differences arise from the way human evaluators weigh certain information more than others. First, human evaluators are lenient with the outputs where the header is not at all or not well transcribed. Human evaluators focus more on the handwritten content, which is arguably more important than the header to transcribe, given the end goal of using the transcriptions in an analysis."}, {"title": "5 Discussion and Future Work", "content": "In this study, we only explored the historical transcription performability of two LLMS: GPT-40 and Claude Sonnet 3.5. As new LLMs are developing at a high speed, we forsee that LLMs' performability in historical transcription will increase. Moreover, this study focused on examining the performance of a single model. However, given that each LLM has unique strong points, one can also combine different LLMs to examine whether such an approach enhances the performance and to what extent. For example, employing GPT-4 for preliminary image text recognition and subsequently using GPT-3.5 for refining or error correction could offer a synergistic approach to improving overall performance, in a similar way to the method proposed in [15].\nWhile conducting the experiments over several months, we found that the LLMs, especially GPT-40, changed their behavior several times. For instance, an apologetic message that we had never received during the pilot experiment appeared more frequently at the end of the experiment. Although we had solutions to this issue by including an extra prompt and rerunning if necessary, we recognize that this hinders the reproduction of the exact outputs. Therefore, future researchers should consider the models and prompts detailed in the Appendix."}, {"title": "6 Conclusion", "content": "Our experiments have demonstrated the potential of LLMs to transcribe historical hand-written documents efficiently. Notably, the LLMs seem to perform better when the document images are sliced per row rather than when whole scans are used as inputs. Among the two LLMs tested, Claude Sonnet 3.5 performed better with the whole scans, and GPT-40 performed better with the sliced images. Regarding the strategies, giving two examples with a prompt yielded the best results in both cases.\nFor historians, an important aspect of the OCR/HTR tools has been that originally, there was very little ground truth data available, and thus, very few models could be applied broadly, adopting different handwriting styles, languages, and source types. In our experiments, however, LLMs demonstrated the ability to produce highly accurate outputs with as few as two pieces of ground truth data. Furthermore, neither tabulated data formats nor variations in handwriting styles within the same page posed significant obstacles for these models.\nWhen evaluating the similarities between the ground truth and the outputs, it appeared crucial to understand the characteristics of the documents. For transcription tasks, as in our experiments, we observed that the BLEU score captures the distinctiveness in output qualities between different methods better than the CER. This highlights the potential of the BLEU score as a metric for evaluating OCR/HTR tasks involving longer texts. Moreover, since the document header was repetitive and less critical to transcribe accurately than the content, disregarding the headers during evaluation allowed for a more meaningful ranking of output quality across methods.\nTo conclude, the study demonstrates that the LLMs outperform the conventional OCR/HTR tools in historical tran- scription based on the state-of-the-art metrics, BLEU, and CER scores. Notably, this superior performance is observed even though these metrics tend to underestimate the capabilities of LLMs."}]}