{"title": "ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection", "authors": ["Bo Yang", "Jiaxian Guo", "Yusuke Iwasawa", "Yutaka Matsuo"], "abstract": "Recent studies have increasingly demonstrated that large language models (LLMs) possess significant theory of mind (ToM) capabilities, showing the potential for simulating the tracking of mental states in generative agents. In this study, we propose a novel paradigm called ToM-agent, designed to empower LLMs-based generative agents to simulate ToM in open-domain conversational interactions. ToM-agent disentangles the confidence from mental states, facilitating the emulation of an agent's perception of its counterpart's mental states, such as beliefs, desires, and intentions (BDIs). Using past conversation history and verbal reflections, ToM-Agent can dynamically adjust counterparts' inferred BDIs, along with related confidence levels. We further put forth a counterfactual intervention method that reflects on the gap between the predicted responses of counterparts and their real utterances, thereby enhancing the efficiency of reflection. Leveraging empathetic and persuasion dialogue datasets, we assess the advantages of implementing the ToM-agent with downstream tasks, as well as its performance in both the first-order and the second-order ToM. Our findings indicate that the ToM-agent can grasp the underlying reasons for their counterpart's behaviors beyond mere semantic-emotional supporting or decision-making based on common sense, providing new insights for studying large-scale LLMs-based simulation of human social behaviors. The codes of this project will be made publicly available after the paper acceptance.", "sections": [{"title": "1 Introduction", "content": "Generative agents (Park et al., 2023; Wang et al., 2023), which are computational interactive agents with critical components such as memory, observation, planning, and reflection, have been proposed to simulate believable human behavior during conversational interactions by fusing with LLMs (Radford et al., 2019; Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Touvron et al., 2023). Nevertheless, the limitations of LLMs in generating extended, coherent dialogues are well-documented, particularly their proclivity for generating hallucinated or inconsistent content (Rawte et al., 2023; Zhang et al., 2023). These shortcomings are especially problematic when the purpose of the conversation extends beyond simple information exchange to include emotive or persuasive elements, such as in scenarios of emotional support, sales, or persuasive communication (Hu et al., 2023; tse Huang et al., 2023; Remountakis et al., 2023). Such situations necessitate not merely the exchange of factual information, but also the articulation of nuanced demands or emotional appeals (Yakura, 2023), which current LLMs architectures struggle to maintain across natural and prolonged conversational sequences (Zheng et al., 2023).\nReferring to psychology science, it is noticed that human normally do not only express their emotions or demands during interaction but also care about their own or counterpart's mental status, such as beliefs, desires, and intentions (BDIs) has been understood or satisfied during the communications (Dvash & Shamay-Tsoory, 2014; Grazzani et al., 2018; Rusch et al., 2020). Theory of mind (ToM), a cognitive skill that enables an individual to"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Theory of mind (ToM)", "content": "\"What is ToM?\u201d and \"Why ToM is important for Artificial intelligence?\u201d are two questions we would like to stress at the very beginning. ToM has long been studied within cognitive science and psychology, which is defined as an important social cognitive skill highly developed in humans and a small number of animals that involves the ability to tack both"}, {"title": "2.2 Problem Statement", "content": "To streamline our analysis, we shift our attention away from expansive agent simulations, honing in on the dynamics between two generative agents grounded in the LLMs framework instead of multiple generative agents: agents A and B. These two agents participate in a dialogic exchange. Agent A's utterance, denoted as $U_a$, stems from its underlying beliefs, desires, and intentions. This triad can be captured by $R = (B_r, D_r, I_r)$, where $B_r, D_r$ and $I_r$ represent agent A's authentic beliefs, desires, and intentions, respectively. In contrast, agent B's utterance is represented as $U_b$. The beliefs, desires, and intentions of agent A as inferred by agent B are encapsulated by $I = (B_i, D_i, I_i)$. Agent B speculates on what is the real beliefs, desires, or intentions of agent A based on its responses and updates the perceptions and confidence accordingly. Given that BDIs are unobservable inherently latent, and discernable only from the ongoing dialogue, our core challenge is to simulate how agents might progressively recognize each other's genuine BDIs throughout their conversation and eventually benefit conversational communication."}, {"title": "3 Methods", "content": "As illustrated in Figure 2, we propose a ToM aware generative agent consisting of three main modules: Self-BDI aware Module, Vanilla BDI Tracking Module, and Counterfactual Reflection-based (CR-based) BDI Tracking Module. The self-BDI aware Module is used to generate utterances based on the agent's own beliefs, desires, and intentions. The Vanilla BDI Tracking Module and the CR-based Module are designed to track the counterpart's possible BDIs and update the perceptions and confidence levels regarding these BDIs. The former serves as a baseline for benchmarking, while the latter is a technique aimed at performance"}, {"title": "3.1 Self-BDI Aware Module", "content": "The episode of the conversation history is represented as $H = (U_{a_1}, U_{b_1}, ..., U_{a_i}, U_{b_i})$, where $(U_{a_i}, U_{b_i})$ denotes the dialogue pair of $i$th turn between agent A and B. During the conversation, agent A equipped with a self-BDI aware module is supposed to generate its utterance based on its actual BDIs $T_R$, and the conversation history $H_i$ by prompting LLMs using the corresponding prompt $P_a$, as is described in the following Equation (1).\n$U_{a_{i+1}} = LLM(P_a; T_R; H_i)$ (1)\nZero-shot BDIs Initialization. The initial BDIs of agent A are learned from a randomly selected single episode of dialogue corpus, utilizing zero-shot prompting, without needing annotation or additional learning. To increase randomness, we adopt an approach where the LLM generates the top-k combinations of beliefs, desires, and intentions based on the conversation episode's history. From these combinations, one of the combinations R is randomly selected as the initial top-1 value of BDI for the agent. In the prompt, we have also included hints about the concepts of beliefs, desires, and intentions, emphasizing their relevance to ensure that the resulting BDI combination is closer to reality.\nReverse BDIs Argumentation. Belief often reflects personal subjectivity and may not always be correct or even morally wrong, whereas common sense tends to align with the public's general perception. In our experiments, we also discovered that expressing certain personalized beliefs is challenging because the conversation data tends to align more closely with common sense. Ultimately, the features learned by the LLMs are constrained by the available data, which often results in a bias towards commonsense concepts that appear more frequently in the datasets. To address this issue, we iteratively refine the resulting BDIs by inputting them back into the prompt and instructing the LLM to generate BDIs with the opposite meaning or a counterfactual nature.\nSecond Order ToM Judgement. In each round of conversation, after both parties have finished expressing themselves, the agent A ponders whether the counterpart agent B has understood agent A's BDI. This is also determined by adding the conversation history and its own real BDI to the prompt, which in turn is generated by LLM."}, {"title": "3.2 Vanilla BDI Tracking Module", "content": "Top-1 BDI Prompting. The agent B equipped with vanilla BDI tracking module prompts LLMs to obtain the most probable BDI combination $T_{1_i}$ after $i$th round of interaction concludes. Further, we select only the top-1 BDI. Ultimately, agent B generates its utterance $U_{b_{i+1}}$ by prompting LLMs using the BDI combination with the highest confidence $T_{1_i}$ and the dialogue history $H_i$, along with corresponding prompt $P_b$, as is described in the following Equation (2).\n$U_{b_{i+1}} = LLM(P_b; T_{1_i}; H_i)$ (2)"}, {"title": "3.3 CR-based BDI Tracking Module", "content": "In this study, rather than limiting belief to specific scenarios or tasks, we disentangle belief and confidence to enable the ToM modeling in open-domain conversational interactions. This approach allows agents to focus on personalized beliefs compared to mere common sense. In the initial phase, top-k BDIs along with confidences are inferred by agent B based on the first utterance of agent A. These BDIs and their associated confidence levels are then updated for $i$th turn of communicational interaction, which is represented as $T_{1_i} = ((B_{i_1}, D_{i_1}, L_{i_1}; C_1), ..., (B_{i_k}, D_{i_k}, l_{i_k}; C_k))$, where C stands for the confidence for each set of BDIs. The agent can generate an update plan of the inferred BIDs and its corresponding confidence by prompting\nDuring each round of interactions, agent B generates an updating plan $L_{i+1}$ for the inferred BDIs and confidence of agent A using the reflection mechanism, by prompting the LLMs with the corresponding reflection prompt $P_r$, based on the dialogue history $H_i$. Then, the updated BDIs and confidence levels combination $T_{1_{i+1}}$ are obtained by prompting the LLMs using the corresponding prompt $P_u$, the previous BIDs combination $T_{1_i}$, and the updating plan $L_{i+1}$, as is described in the Equation (3).\n$L_{i+1} = LLM(P_r; T_{1_i}; H_i)$ (3)\n$T_{1_{i+1}} = LLM(P_u; L_{i+1}; T_{1_i})$\nAfter reflection and updating, the agent B generates its utterance by prompting LLMs using the BDI set with the highest confidence along with the corresponding prompt, which is similar to Equation (2).\nReflection. Reflection is an effective reinforcement technique for LLMs-based agents, which can be a reinforcement learning way via verbal feedback without tuning the parameters of LLMs or devising a reword function (Shinn et al., 2023). It employs a persisting memory of self-refective experiences, allowing an agent to revisit its errors and make improved decisions in subsequent iterations. It consists of three distinct models: an actor model, an evaluator model, and a self-reflection model, in which the evaluator model plays a crucial role in assessing the quality. We aim to update the top-k BDIs and the related confidence level using the reflection of LLMs. However, since the BDI is unobservable, it results in the established reflection cannot directly evaluate the similarity between the inferred BDI and the actual BDI. To solve this problem, we propose a counterfactual reflection based on foresight and counterfactual thinking.\nForesight. It is argued in previous studies that foresight and reflection are equally critical for machine ToM (Zhou et al., 2023). To solve the problem that BDI is unobservable, we compared the observable utterances with predicted utterances instead: in each interaction round, we let the agent B predict the agent A's utterance $U_{a_p}$ by prompting the LLM, utilizing inferred BDIs combination with the highest confidence $T_{1_i}$ and conversation history H. After agent A's real utterance $U_a$ is observed, agent B compares the $U_{a_p}$ with $U_a$, by scoring the two sentences with a decimal value S between [0, 1] to evaluate their similarity."}, {"title": "Counterfactual Reflection", "content": "Inspired by the argument in the previous study that counterfactual thinking may be critical for an individual to understand others by predicting what action they will take in a similar situation (Cuzzolin et al., 2020), we propose a counterfactual reflection. The proposed counterfactual reflection is conducted in the following steps: If the $S_{i+1}$ increases compared with $S_i$, the agent B reflects on the previously inferred BDIs of agent A based on the evaluation value S on the $U_{a_p}$ and $U_a$, by prompting the LLMs. Further, agent B reflects that \"what if my previously inferred BDI of agent A is not correct?\u201d. Then agent B carries on the conversation with itself and generates a virtual response $U_{av}$ instead of agent A and compares the $U_{av}$ with the real one $U_a$, to obtain a virtual score $S_v$. If the $S_v > S$, then update the BDI and generate the $U_b$ for the next round, otherwise, update the BIDs and confidence level using the $T_i$ and $H_i$."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Setup", "content": "By conducting experiments, we try to answer the following research questions:\n\u2022 RQ1: To what extent could the ToM-aware generative agent infer about other generative agents' unobservable beliefs, desires, and intentions during open-domain conversational interactions? (First-order ToM)\n\u2022 RQ2: To what extent could the ToM-aware generative agent infer about counterpart agents' understanding of its own beliefs, desires, and intentions during open-domain conversational interactions? (Second-order ToM)\n\u2022 RQ3: To what extent could the ToM-aware generative agent benefit downstream tasks of open-domain conversational interactions in short-term interactions?\nWe use these two downstream tasks to evaluate our method: empathetic dialogue and persuasive dialogue, which are highly related to ToM mental status modeling during human beings' interactions. We can abstract downstream tasks in the dialog domain into specific goals. For instance, in empathetic dialogue, one agent's goal is to satisfy the other emotionally. While, in persuasive dialogue, one agent aims to persuade the other to make a decision, such as purchasing a certain good or service, or making a donation. Empathetic Dialogue(Rashkin et al., 2019) and Persuasion Dialogue(Wang et al., 2019) are two dialogue datasets publicly available and the details about these datasets can be found in the Appendix.\nIn the simulation, one agent plays the role of either an Empathetic-needing NPC or a Persuadee NPC, taking actions as the previously introduced agent A. Meanwhile, another agent plays the role of either an Empathetic NPC or a Persuador NPC, taking actions as the agent B. The initialization of BDIs is conducted on 100 dialogue episodes randomly sampled from the above two datasets. Similarly, for each experiment, the two agents interact with each other until agent A believes that agent B understands its BDI, at which point the dialog is considered successfully concluded. However, if the conditions for ending the dialog are not met within t rounds, the dialog is considered unsuccessful. We mainly evaluated two versions of LLMs as generative agents: GPT-4 (gpt-4-0125-preview) and GPT-3.5 (gpt-3.5-turbo-0125). OpenAI davinci model (text-similarity-davinci-001) is used for scoring the similarity of the predicted utterance and the real utterance. In this study, the BDIs set number top-k is set to 3 and the maximum number of turns in each dialogue episode t is set to 10."}, {"title": "4.2 BDI Infering Evaluation (First-order ToM)", "content": "To answer RQ1, we conducted an experiment in which two agents engaged in 100 conversation rounds. One agent implemented only the self-BDI-aware module and generated conversations based on its initial BDIs. The other agent used either the vanilla BDI tracing module or the CR-based BDI tracing module, referred to as Vanilla and ToM + CR, respectively. At the end of the dialog, we recorded the set pairs of the inferred BDI and the true"}, {"title": "4.3 BDI Infering Evaluation (Second-order ToM)", "content": "To address RQ2, we assess the second-order ToM on the two conversation datasets separately, as whether a conversation meets the end criterion is determined by agent A's second-order ToM. Likewise, from the 100 rounds of conversation conducted by the two agents, we ask three annotators to judge whether agent A believes that agent B understands its BDI. Similar to the first-order ToM, the evaluation of the second-order ToM can also be treated as a binary classification problem, thus the precision, F1 score, and recall score from the aspects of belief, desire, and intention are evaluated, respectively."}, {"title": "4.4 Dialogue Generation Evaluation (Down-stream tasks)", "content": "As is argued that previous studies of emotional support or negotiation dialogue can evaluate the turn-level performance using the fixed reference responses of a benchmark corpus, however, it is better to evaluate the dialogue level of proactive dialogue systems using automatic metrics: average turn (AT) and the success rate at turn t (SR@t) (Deng et al., 2023). We hypothesize that an agent who has insight into another agent's BDI can accomplish the goal of the conversation more quickly within a pre-defined maximum turn, or efficiently when performing downstream tasks than the agent without the ToM mechanism. The maximum turn of the conversation is set as 10 in our experiment."}, {"title": "5 Observations", "content": ""}, {"title": "5.1 Observations of Good Examples", "content": "During the dialogue, we use the text-embedding-3-large model of GPT to compare the similarity between the inferred BDI (Belief, Desire, Intention) and the true BDI by calculating"}, {"title": "5.2 Observations of \"Bad\" Examples", "content": "As shown in the (a) and (b) of Figure 4, these can be seen as examples of suboptimal confidence changes during the conversation for ToM simulation. In Figure (a), the confidence values for Belief, Desire, and Intention initially increase but eventually settle at the lower end of the scale. In Figure (b), the confidence values for Desire and Intention increase steadily and eventually stabilize at high levels, while Belief settles at a lower level. Ending a dialogue when confidence levels are not at their maximum is generally considered a suboptimal outcome. However, since the purpose of the study was to simulate people's BDI (Belief, Desire, and Intention) confidence levels about others during dialogue\u2014and recognizing that people often make decisions in conversations without their confidence being at its peak-these results should be viewed in a more positive light. The observed fluctuations and eventual increase in confidence levels accurately mirror natural decision-making processes in real-life interactions, where individuals often proceed despite varying levels of certainty. This alignment with realistic human behavior suggests that our simulation effectively captures the dynamics of BDI confidence during dialogue. However, if the goal is to develop super-Als with capabilities that surpass the theory of mind of human beings, these results might be seen as suboptimal."}, {"title": "5.3 Ethical Concerns", "content": "As the conversation examples illustrated in Figure 1, someone may think that the implementation of this kind of agent may raise ethical concerns. However, we originally chose this nearly extreme example because our research focuses on the broader context of generative agents, and we wanted to demonstrate that ToM agents can take into account and imitate human beliefs in their interactions. This distinction is crucial because human beliefs do not always follow common sense (may even be morally incorrect), and such discrepancies can significantly impact agent behavior. Instead of labeling the act of mimicking human beliefs as inherently unethical, we believe it is essential to recognize this potential issue through our research. Our goal is to provide a foundation for the research community to discuss and"}, {"title": "6 Conclusion", "content": "In this study, we proposed a novel paradigm ToM-Agent that equips LLMs-based generative agents with ToM reasoning, allowing them to emulate cognitive mental states such as beliefs, desires, and intentions (BDIs) during open-domain conversational interactions. The counterfactual reflection method is also proposed to dynamically adjust confidence levels related to inferred BDIs of counterparts based on past conversation history, to reflect on the gap between predicted responses of counterparts and their real utterances, which enhances the confidence updating performances of inferred BDIs. Leveraging datasets from empathetic and persuasion dialogue research, we evaluate the performance of our proposed agent architecture in downstream tasks. Finally, the results show that equipping the generative agents with ToM is reasonable and will benefit the downstream task in the long term."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Limitations", "content": "Challenges of Time and Cost Efficiency. The formidable cost associated with utilizing the OpenAI API posed significant constraints on our research endeavors, rendering us financially incapable of conducting extensive experiments. This issue is exacerbated by the practical challenges inherent in generating conversational content, as it necessitates using a prompt with a substantial number of tokens. For instance, the average expenditure per episode escalates to approximately 2 to 3 dollars when employing GPT-4 for this purpose. Moreover, the generation of sentence-based conversations entails multiple interactions with the API interface, thereby incurring substantial waiting times. Such prolonged waiting periods may lead to user frustration and dwindling interest, adversely affecting the viability of engaging in dialogues with the AI agent, should we prioritize technical practicality.\nLimitation of Two Agents Simulations. While memory retrieval, and reflection all play pivotal roles in conversational control of generative agents(Park et al., 2023; Wang et al., 2023), our research primarily emphasizes modeling the BDIs tracking paradigm between two agents for better reflection. Consequently, we haven't delved into matching conversations with analogous BDIs during extended interactions. However, as we simulate interactions among a vast number of agents, conversations inevitably span a broader array of topics and knowledge domains. Hence, extracting short-term conversation content with the most pertinent BDIs from long-term memory emerges as a compelling research avenue, offering the potential for future exploration. Furthermore, in this article, we focus solely on interactions between one agent aware of its BDIs and the counterpart attempting to infer that BDI. Yet, when scaled up in simulations, multi-agent interactions produce more complex dynamics. Here, we must delve deeper, exploring scenarios where an agent is not just self-aware of its BDI but also discerns the BDIs of its counterparts. Particularly when the agent recognizes its differing BDI from the majority, it might either seek to influence others' BDIs or, conversely, be swayed by them to modify its stance.\nLimitaions of Behavior Modality. To present the problem more comprehensively, we confined our paradigm's definition and validation strictly to the realm of conversational interactions. This encompasses all actions and behaviors, ranging from the selection of empathetic responses to the deployment of suitable persuasive tactics, and from choices involving donations to decisions against donating. Furthermore, the encapsulated semantic sentiments are exclusively tied to dialogic manifestations. However, it's crucial to note that for applications such as in-game non-player characters (NPCs), generative urban simulations, or robotic interfaces, our paradigm's definition ought to be expanded. This will allow it to encompass behaviors and actions beyond mere dialogue, potentially extending to multimodal communications. ToM in Multi-modal interaction should be further studied such as in VQA scenarios (Takmaz et al., 2023; Chandrasekaran et al., 2017)."}, {"title": "A.2 Future Works", "content": "Large-scale Multi-agent Simulation. Although only the interactions between two agents are studied in this study, multiple Agents' behaviors should be further studied to mimic human beings. And belief transition between agents or the rise and disappearance of BDIs should also be studied in future work. The other big premise is whether local LLMs can achieve the same performance as GPTs to support large-scale simulations. Also, some supporting measures such as memory retention, simulation of forgetting, etc. need to be further researched.\nInteraction in Multi-modal scenarios. ToM in Multi-modal interaction should be further studied such as in VQA scenarios (Takmaz et al., 2023; Chandrasekaran et al., 2017). Agents based on LLMs can be used to simulate not only dialogues but also human expressions, voices, etc. so that the agents can better understand human inner emotions by analyzing multimodal information. Combining image processing facial expression recognition"}, {"title": "A.3 Details of Datasets", "content": "The details of two datasets Empathetic Dialogue(Rashkin et al., 2019) and Persuasion Dialogue(Wang et al., 2019), which is used in this study, are illustrated in Table 4.\nEmpathetic Dialogue. Empathetic Dialogue is a novel dataset of 25k conversations grounded in emotional situations. Each dialogue is based on a specific scenario where a speaker experiences a particular emotion, and a listener responds accordingly. This resource, consisting of crowdsourced one-on-one conversations, covers a wide range of emotions in a balanced manner. It is larger and includes a more extensive set of emotions than many existing emotion prediction datasets. The dataset contains 32 emotion labels, which were selected by aggregating labels from several emotion prediction datasets.\nIn each conversation, the person who wrote the situation description (the Speaker) initiates the dialogue by discussing it. The other participant (the Listener) learns about the underlying situation through the Speaker's words and responds. The Speaker and Listener then exchange up to six additional turns. The resulting dataset comprises 24,850 conversations based on situation descriptions, gathered from 810 different participants. The final train/validation/test split is 19,533 / 2,770 / 2,547 conversations, respectively.\nPersuasion Dialogue. Persuasion Dialogue is a large dialogue dataset consisting of 1,017 dialogues, with a subset annotated for emerging persuasion strategies. The dataset not only explores how personal information influences persuasion outcomes but also examines which strategies are most effective based on different user backgrounds and personalities. Before the conversation begins, participants complete a pre-task survey to assess psychological profile variables using the Big Five personality traits. The roles of persuader and persuadee are then assigned to the two participants, which helps eliminate any correlation between the persuader's strategies and the persuadee's characteristics. Each participant is required to complete at least 10 conversational turns, with multiple sentences allowed in a single turn. The dataset includes 4,313 instances of persuasion strategies categorized into 10 types, such as logical appeal, emotional appeal, personal-related inquiry, and non-strategy dialogue acts.\nThe details of two datasets Empathetic Dialogue(Rashkin et al., 2019) and Persuasion Dialogue(Wang et al., 2019), which is used in this study, are illustrated in Table 4."}, {"title": "A.4 Agents Prompt Setup", "content": "Some of the major promoters are introduced in this section."}]}