{"title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images", "authors": ["Shengguang Wu", "Fan-Yun Sun", "Kaiyue Wen", "Nick Haber"], "abstract": "Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel fine-tuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM's visually-dependent task performance while retaining or even improving the model's general abilities.", "sections": [{"title": "1 Introduction", "content": "Large Vision-Language Models (VLMs) tend to over-rely on their language models, leading to neglect of visual content. This problem manifests as visual hallucinations across tasks like perception (Tong et al., 2024b), reasoning (Chen et al., 2024), and in-context learning (Jia et al., 2024). Studies like Tong et al. (2024a) show that VLMs exhibit only limited performance gains when vision inputs are enabled compared to having no vision inputs. This behavior is reflected in the metrics across many popular vision-language benchmarks (Lu et al., 2022; Yue et al., 2024; Lu et al., 2023; Kembhavi et al., 2016; xAI, 2024; Singh et al., 2019). Our own perplexity-based evaluations reveal similar patterns of visual neglect. As illustrated in Fig. 2, we measured a base VLM's perplexity (PPL) when generating a caption matched to one image (e.g., \u201cimagematch\u201d with a \u201cdog\u201d) while contradicting the other image (e.g., \u201cimagemismatch with a \"cat\"). We also tested the model's PPL without any vision input (\u201cno image\"). Intuitively, the model should exhibit the lowest PPL when given the matching image, as the aligned visual information would make the caption easier to generate. However, results reveal the opposite pattern: the model's PPL lowest when no image input is provided at all, and highest when presented with the correct image (the \"dog\") \u2013 higher than the mismatched image (the \"cat\") as condition. AcrossInstead, we posit that the \u201cpreference\" paradigm should be improved with a purely contrastive framework, as the \"dispreferred\" image is merely another image misaligned with a given text. Building upon this insight, we propose Symmetrical Visual Contrastive Optimization (S-VCO), a fine-tuning objective that enforces precise correspondence between visual details and textual tokens. S-VCO rewards the model for attending to matching images and strongly rejecting contradictory images with incorrect details. To further avoid shortcut learning, S-VCO incorporates symmetry by flipping the objective for contradictory responses, allowing the \"negative\u201d image to serve as the \"preferred\" visual condition when paired with its corresponding text.\nAdditionally, as cropping (Wang et al., 2024) or adding diffusion noise to an original image (Jiang et al., 2024a) fails to provide meaningful comparisons in visual details, we construct MVC, a dataset of paired images with Minimal Visual Contrasts, each matched to contrastive textual responses given a shared query. Building on recent visual counterfactual data sources (Zhang et al., 2024; Liu et al., 2024c), we implement a vision-centric filter to select visually challenging pairs and an LLM augmentation scheme to diversify texts, forming an instruction-response-styled dataset suitable for VLM finetuning.\nExperiments demonstrate the effectiveness and versatility of our approach, as S-VCO consistently enhances VLM performance across diverse benchmarks spanning various abilities and domains. As shown in Fig. 1, S-VCO achieves significantly greater improvements over the base-VLM, particularly in reducing visual hallucinations, while excelling in vision-centric tasks and offering considerable gains on general benchmarks. Compared to existing VLM preference tuning methods (DPO, Rafailov et al., 2024; mDPO, Wang et al., 2024), S-VCO combined with MVC delivers more substantial and comprehensive performance boosts. In summary, our main contributions are:\n\u2022 S-VCO, a novel VLM finetuning objective that enforces strict and balanced visual contrastive supervision through the symmetrical alignment of image-text pairs;\n\u2022 MVC, a dataset of minimal contrastive image pairs accompanied with corresponding textual responses for a shared query. MVC is constructed automatically through vision-centric filtering and augmentation based on visual counterfactual data;\n\u2022 The combination of S-VCO and MVC significantly boosts VLM performance across diverse benchmarks, particularly in visually dependent tasks, without compromising general capabilities."}, {"title": "2 Preliminaries", "content": "Our visual contrastive objective is inspired by Direct Preference Optimization (DPO) (Rafailov et al., 2024) that contrasts pairs of textual responses. When applied to VLMs, DPO incorporates the image as an additional prefix condition (Zhou et al., 2024; Li et al., 2023a; Yu et al., 2024; Sarkar et al., 2024). Let \\(\\pi_{\\theta}\\) be the policy VLM to be optimized, and \\(T_{ref}\\) the fixed reference model (typically the unfintuned VLM in DPO's framework) used to measure and optimize how the finetuned policy \\(\\pi_{\\theta}\\) improves. Given a query q (an instruction prompt), an image i, and a pair of responses \u2013 one preferred \\(y_w\\) (winning) and one dispreferred \\(y_l\\) (losing) \u2013 the DPO objective for VLMs can be formulated as:\n\\(L_{DPO} = \u2013 log \\sigma (\\beta log \\frac{\\pi_{\\theta}(y_w | i, q)}{T_{ref}(y_w | i, q)} - \\beta log \\frac{\\pi_{\\theta}(y_l | i, q)}{T_{ref}(y_l | i, q)})\\)\nAs illustrated in Fig. 3, the DPO formulation focuses on supervising differences between language responses, which however, often pertain to wording choices and stylistic variations rather than reflecting meaningful visual distinctions. Consequently, the VLM is being trained like a language model to weigh over text formulations, rather than to fully utilize the image as a grounding input. Recent approaches, such as Wang et al. (2024);Jiang et al. (2024a), adapt the DPO framework to contrast image conditions. The original image i becomes the preferred image \\(i_w\\) (winning), while a negative image \\(i_l\\) (losing) is derived through random cropping (Wang et al., 2024) or adding diffusion noise (Jiang et al., 2024a) (see Fig. 3). The updated Visual Conditional objective is:\n\\(L_{VisCon} = log \\sigma (\\beta log \\frac{\\pi_{\\theta}(y_w | i_w, q)}{T_{ref}(y_w | i_w, q)} - \\beta log \\frac{\\pi_{\\theta}(y_w | i_l, q)}{T_{ref}(y_w | i_l, q)})\\)\nThe above visual conditional preference formulation shifts the target of preference to the images, where the original image \\(i_w\\) is always preferred over the corrupted version \\(i_l\\). However, this approach shares similar limitations with DPO, as it prioritizes distinguishing image differences without necessarily grounding those differences in the associated texts. Since \\(i_l\\) is typically a noisy variant of \\(i_w\\) with no definitive relationship to \\(y_w\\), the model could easily rely on superficial visual features to discern the paired images. This encourages shortcut learning, where the model rejects \"unrealistic\" images like \\(i_l\\) without examining the visual details related to the text tokens.\nOur S-VCO addresses these limitations by introducing a stricter visual-conditioned objective (\u00a73.1)"}, {"title": "3 S-VCO: Symmetrical Visual Contrastive Optimization", "content": "S-VCO enforces a strict visual focus by optimizing the model for two key behaviors:\n1) Attending to matching images. The model is rewarded for prioritizing relevant visual details in the matching image \\(i_w\\) as a condition when predicting the corresponding response \\(y_w\\). This directly addresses the tendency of VLMs to overlook visual content (\u00a71) and is achieved through the term:\n\\(L_{Attend}(i_w, y_w) = - log \\sigma (\\beta log \\frac{\\pi_{\\theta}(y_w | i_w, q)}{T_{ref}(y_w | i_w, q)} - \\beta log \\frac{\\pi_{\\theta}(y_w | q)}{T_{ref}(y_w | q)})\\)\n2) Rejecting contradictory images. When presented with a contrastive image \\(i_l\\) containing visual details that directly contradict the response \\(y_w\\), the model must strongly reduce the likelihood of predicting \\(y_w\\) under this incorrect image condition. Intuitively, the model should assign minimal probability to a response that directly contrasts with the visual input. This behavior is modeled as:\n\\(L_{Reject}(i_l, y_w) = - log \\sigma (\\beta log \\frac{\\pi_{\\theta}(y_w | q)}{T_{ref}(y_w | q)} - \\beta log \\frac{\\pi_{\\theta}(y_w | i_l, q)}{T_{ref}(y_w | i_l, q)})\\)\nBy combining these two components, our strict visual contrastive objective is defined as:\n\\(L_{VCO}(i_w, y_w, i_l) = L_{Attend}(i_w, y_w) + L_{Reject}(i_l, y_w)\\)\nUnlike prior \"preference\u201d-based approaches, S-VCO treats \\(i_w\\) and \\(i_l\\) as mere images with contrastive details, where either can serve as the correct (i.e., \"preferred\") condition depending on the paired textual response (\\(y_w\\) or \\(y_l\\)). While we inherit the notation of \\(i_w\\) and \\(i_l\\), S-VCO does not assign an inherent \u201cwinning\u201d or \u201closing\" property to the images. Instead, an image is considered \"winning\u201d only when paired with its corresponding text. For instance, \\(i_l\\), typically labeled as \u201closing\" in preference tuning methods, becomes a \u201cwinning\" (preferred) condition when the target response is \\(y_l\\) that matches its visual details.\nA one-sided formulation that consistently favors \\(i_w\\) over \\(i_l\\) risks encouraging shortcut learning, where the model rejects \\(i_l\\) based on superficial, text-unrelated features (e.g., visual style differences). This issue could arise because most of the contrasting images are synthesized via inpainting or image editing (see \u00a74). To address this, S-VCO introduces symmetry by flipping the objective in Eq. 5, treating \\(i_l\\) as the preferred condition when paired with its corresponding response \\(y_l\\). This effectively switches the roles of \u201cwinning\u201d and \u201closing\" for the image pair:\n\\(L_{VCO}(i_l, y_l, i_w) = L_{Attend}(i_l, y_l) + L_{Reject}(i_w, y_l)\\)\nAs defined above, this encourages the model to attend to \\(i_l\\) and reject \\(i_w\\) in the flipped case where the target response is \\(y_l\\).\nThe complete S-VCO objective incorporates symmetry by summing over both roles of \\(i_w\\) and \\(i_l\\) given their corresponding target responses:\n\\(L_{S-VCO} = L_{VCO}(i_w, y_w, i_l) + L_{VCO}(i_l, y_l, i_w)\\)\nThis symmetrical alignment ensures balanced optimization, allowing both \\(i_w\\) and \\(i_l\\) to contribute equally to the model's learning. It promotes true alignment between images and texts without relying on shortcuts, such as rejecting either the image (Eq. 2) or the text (Eq. 1). In essence, S-VCO optimizes for the alignment of image-text pairs rather than simply one modality."}, {"title": "4 Minimal Visual Contrasts Dataset", "content": "To complement S-VCO, we introduce MVC: a dataset of paired visual contrastive samples designed to enhance the model's ability to discern visual details. Built on existing sources (e.g., Zhang et al., 2024; Liu et al., 2024c; Gaur et al., 2024), MVC contains image pairs with minimal but meaningful variations, accompanied by corresponding contrastive texts. The curated data includes four key contrast types, as shown in part a). of Fig. 4: Object Replacement (changing a specific object); Attribute Replacement (modifying an object's features like color, shape, or size); Count Modification (altering the number of objects); Position Flipping (reversing relative positions of objects). Except for the last type, these counterfactual images are generated through controlled image synthesis, including inpainting, editing, and generationFiltering. Existing visual counterfactual datasets offer a large quantity of detailed contrasts, but their quality is inconsistent due to the synthetic nature of the data, which could lead to pairs where the generated image fails to truly contradict the original. To address this, we implement a vision-centric filter inspired by the CLIP-Blind concept (Tong et al., 2024b) to select image pairs based on the following two criteria, as shown in part b). of Fig. 4: 1. Different in detailed visual features: Image pairs must exhibit meaningful contrasts, especially in detailed visual features. To achieve this, we employ DINOv2 (Oquab et al., 2023), a vision-only model with more robust visual feature representations (Singh et al., 2023). Pairs with high similarity in DINOv2's purely visual representation space are discarded, as they may not contain the desired degree of contrasts. 2. Semantically close & Hard for VLMs: Image pairs must also be semantically similar overall and difficult for current VLMs to distinguish. Therefore, we embed images using the same CLIP vision encoder used by the VLM (Radford et al., 2021; Zhai et al., 2023) and retain pairsLanguage Augmentation. Although visual counterfactual data sources provide image contrasts, their original textual descriptions are typically short captions, which are unideal for VLM finetuning. To address this, we augment the data using a two-step process with a strong LLM (gpt-40), as shown in part c). of Fig. 4: 1. Generating queries: For each pair of captions, we prompt the LLM to generate a conversational question as if the captions were natural responses to that question, while ensuring the contrasts remain explicit. 2. Rewriting and diversifying: We prompt the LLM to rephrase the original captions lacking clarity on key contrasts, thus enabling a more natural flow of language given the generated question and the contrastive details. Overall, the augmentation step results in conversational instruction-response pairs that are more suited for VLM finetuning, and more aligned with the visual details in the contrasting image pairs."}, {"title": "5 Experiments", "content": "Baseline methods. We compare S-VCO against DPO (Rafailov et al., 2024) and mDPO (Wang et al., 2024)\u00b9 containing the visual-conditional objective in Eq. 2 as baseline finetuning approaches.\nTraining data. We use two datasets: our proposed MVC with minimal contrastive image-text pairs and VLFeedback (VLF) (Li et al., 2023a), a classical instruction-tuning dataset for VLMs that includes preferred and dispreferred response pairs. We follow mDPO (Wang et al., 2024)'s sample of ~10,000 data points from VLF. Refer to Appx.A for more dataset implementation details.\nBase VLMs. We use two pretrained models hosted on Huggingface: LLaVA-1.5-7B\u00b2 (Liu et al., 2024a) denoted as LV-1.5(-7B), and LLaVA-Next-Interleave-7B\u00b3 (Li et al., 2024) denoted as LV-INT(-7B).\nEvaluation. We evaluate the models on a wide range of benchmarks spanning various ability domains. Following the categorization by Tong et al. (2024a), these benchmarks include: General: LLaVABench (Liu et al., 2024b), MMVet (Yu et al., 2023); Hallucination: MM-Hal (Sun et al., 2023); Vision-Centric: CVBench (Tong et al., 2024a), MMVP (Tong et al., 2024b), RealworldQA (xAI, 2024); OCR: TextVQA (Singh et al., 2019); Knowledge: SQA (Lu et al., 2022).\nImplementation details. Refer to Appx.B for detailed training and inference configurations.S-VCO achieves consistently superior performance across benchmarks in various domains. Fig. 1 illustrates performance improvements over the base-VLM (LV-INT) for each benchmark category. Our S-VCO consistently outperforms baseline methods across nearly all domains, with only a slight drop in knowledge-heavy tasks like ScienceQA that relies minimally on visual input (discussed more below). The most significant gain is in visual hallucination tasks, where S-VCO enhances the base model by over 20%. Considerable improvements are also observed in vision-centric (+4%) and general domains (+10%). Compared to baseline methods, DPO and mDPO trained on the standard instruction-tuning dataset VLF, S-VCO with MVC delivers much greater and more consistent improvements across domains. While recent approaches like mDPO (Wang et al., 2024) improve visual hallucination metrics, its effects on other abilities are less notable. In contrast, S-VCO not only excels in visually demanding tasks but also achieves considerable gains in other domains.\nS-VCO shows increasing benefits as benchmarks become more visually dependent. We quantify visual dependency of a metric as the percentage drop in a base-VLM's performance when image inputs are removed. Using LV-INT as the base model, we rank benchmarks by visual dependency and plot the improvement trends of different methods in Fig. 6. Dotted lines show fitted regressions, and shaded areas represent variances. S-VCO demonstrates increasingly pronounced improvements as visual dependency rises, aligning with its design focus on strengthening visual detail recognition (\u00a73). For visually highly dependent benchmarks like LLaVABench (Liu et al., 2024b), MMVet (Yu et al., 2023), and MM-Hal (Sun et al., 2023), S-VCO delivers the most substantial gains. In contrast, ScienceQA (SQA) (Lu et al., 2022), which benefits merely ~4% from image inputs, sees a minor drop in performance, as visual information plays little role in this task. Compared to other methods, SFT (\u00a75.3) degrades performance on visually dependent metrics, while other preference tuning methods show positive trends. Among all, S-VCO achieves the most significant trend of gains with increasingly vision-intensive tasks.\nQualitative performance highlights S-VCO's superior visual understanding. In Fig. 5, quaSymmetrical loss construction. A key feature of S-VCO is its symmetrical loss, which treats both sides of the texts \\(y_w\\) and \\(y_l\\) as preferred"}, {"title": "6 Related Work", "content": "VLM's Visual Hallucinations. Recent studies (Dai et al., 2024; Deng et al., 2024; Tong et al., 2024b; Chen et al., 2024; Jia et al., 2024) observed the tendency of VLMs to hallucinate content that is not present in the visual input, indicating a lack of robust multimodal grounding. To address this issue, several training-free methods have been proposed. Yang et al. (2023) introduced the \u201cSet-of-Mark\u201d prompting technique, which overlays spatial and textual markers on images, helping models like GPT-4V better reference specific regions. Deng\nVLM Finetuning. Finetuning enhances VLMs for task-specific performance and alignment with human preferences. SFT remains widely adopted to guide models toward instruction-following behaviors in multimodal contexts (Sun et al., 2024; Jiang et al., 2024b). DPO (Rafailov et al., 2024) optimizes the margin between the finetuned and unfintuned model versions using paired preference data. Extensions of DPO to VLMs incorporate the image as an additional prefix condition (Zhou et al., 2024). Recent methods such as mDPO (Wang et al., 2024), MFPO (Jiang et al., 2024a), V-DPO (Xie et al., 2024), CHiP (Fu et al., 2025) and Image-DPO (Luo et al., 2024) further adapt the preference tuning paradigm to focus on image-side preferences over a pair of \"good\" and \"bad\" image, aiming to reduce visual hallucinations. Our approach S-VCO discards the one-sided \u201cpreference\u201d formulation by introducing a stricter visual contrastive objective within a symmetrical construct, in which way the \"preference\u201d is treated as alignment over a matching image-text pair. This enables more comprehensive and robust improvements in VLM performance across tasks."}, {"title": "7 Conclusion", "content": "This work introduces S-VCO, a novel VLM fine-tuning objective that enforces strict visual contrastive supervision within a symmetrical construct. Complementing this objective, we propose MVC, a dataset of paired images with minimal visual contrasts, each associated with corresponding contrastive texts. Experiments demonstrate that combining S-VCO with MVC consistently improves VLM performance across diverse benchmarks, with particularly significant gains on visually dependent tasks. Importantly, these improvements are achieved without compromising, and even enhancing VLMs' general capabilities."}, {"title": "Limitations", "content": "Our method incorporates multiple individual loss terms and weights (\u00a73), which may require manual tuning to determine the optimal settings. Adjusting these hyperparameters could potentially further enhance performance.\nThe S-VCO objective works most effectively when the contrastive image pairs have meaningful differences in visual details. The current MVC derived from existing visual counterfactual data sources includes a limited set of operations for building the contrasts (\u00a74.1). Our method could benefit from a more diverse set of contrasting visual details that should enable the model to potentially learn a broader range of visual features."}, {"title": "Ethics Statement", "content": "In this work, all data and pretrained models are publicly available. They are collected and processed in adherence to the respective data, checkpoints, and API usage policy. We acknowledge that our finetuned models may generate unsafe content, and we advise all users of careful verification before deploying this work in real-world applications."}, {"title": "A Dataset Details", "content": "Tab. 2 details the dataset statistics of our visual counterfactual data sources CounterCurate (Zhang et al., 2024) and FineCops-Ref (Liu et al., 2024c), as well as our MVC after filtering and augmentation (disccused in \u00a74.2). We discard the original \"Order\" category of FineCops-Ref for the frequent irrational cases under that category. For the non-synthetic \"Left-Right\u201d position flipping, we did not apply the filter but randomly sampled the data proportional to its original category distribution in the source datasets.\nWhen training with MVC, we leave out 240 random samples for validation set (200 from CounterCurate and 40 from FineCops-Ref). This leads to a total training data size of 10, 909 with MVC. When training with the sampled VLFeedback data (Li et al., 2023a), used as in mDPO (Wang et al., 2024) (https://huggingface.co/datasets/\nfwnlp/mDPO-preference-data; noted as VLF), we leave out 200 random samples for validation set, leading to a total training data size of 9, 222 with VLF."}, {"title": "B Implementation Details", "content": "Training. We set \\(\\beta_1\\) and \\(\\beta_2\\) in S-VCO's objective (Eq. 7) to 0.1, following the typical \\(\\beta\\) values in DPO (Eq. 1) and mDPO (Eq. 2). All models are finetuned for 2 epochs with a batch size of 32 on 8 NVIDIA-A100x80G GPUs. When training on VLF, we retain the original learning rate of 1e-05 used by DPO and mDPO. The text sequence length during finetuning is set to 1024 to accommodate VLF's text data length. When training on our MVC, we set a learning rate of 1e-06 for all methods except mDPO on LV-1.5-7B, where a lower rate of 1e-07 is used to better stabilize training. The text sequence length during finetuning is set to 128 given the MVC's shorter text data length.Intermediate checkpoints are saved at an interval of 24, 31, and 62 steps for training on VLF, MVC, and MVC\u00d72 (used for SFT ablation, see \u00a75.3), respectively.\nEvaluation. We report results using the best-performing checkpoint for each model configuration (Tab. 1), selected based on the highest average improvement over the base-VLM. For all benchmarks, the temperature of model predictions is set to 0. As the evaluation judge, we use gpt-4-0613 for LLaVABench (Liu et al., 2024b) and MMVet (Yu et al., 2023), and gpt-4-turbo for MM-Hal (Sun et al., 2023)."}, {"title": "C Prompt for MVC Language Augmentation", "content": "Below are our prompt templates for querying GPT4 (gpt-40) to augment the queries and responses from visual counterfactual data sources (\u00a74.2).\nIn the first step, we ask GPT4 to generate an natural and appropriate question that targets the subtle differences in the response-pair.\nIn the second step, we ask GPT4 to revise the instruction generated from the first step, and rephrase the original response pairs to make the whole instruction-response conversation sound more natural, while retaining the contrastive details in the responses.\nIn both steps, we set the temperature to 0.7 for more diversified wording."}]}