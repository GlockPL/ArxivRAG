{"title": "ENHANCING THE SCALABILITY AND APPLICABILITY OF KOHN-Sham HAMILTONIANS FOR MOLECULAR SYSTEMS", "authors": ["Yunyang Li", "Zaishuo Xia", "Lin Huang", "Xinran Wei", "Han Yang", "Sam Harshe", "Zun Wang", "Chang Liu", "Jia Zhang", "Bin Shao", "Mark B. Gerstein"], "abstract": "Density Functional Theory (DFT) is a pivotal method within quantum chemistry and materials science, with its core involving the construction and solution of the Kohn-Sham Hamiltonian. Despite its importance, the application of DFT is frequently limited by the substantial computational resources required to construct the Kohn-Sham Hamiltonian. In response to these limitations, current research has employed deep-learning models to efficiently predict molecular and solid Hamiltonians, with roto-translational symmetries encoded in their neural networks. However, the scalability of prior models may be problematic when applied to large molecules, resulting in non-physical predictions of ground-state properties. In this study, we generate a substantially larger training set (PubChemQH) than used previously and use it to create a scalable model for DFT calculations with physical accuracy. For our model, we introduce a loss function derived from physical principles, which we call Wavefunction Alignment Loss (WALoss). WALoss involves performing a basis change on the predicted Hamiltonian to align it with the observed one; thus, the resulting differences can serve as a surrogate for orbital energy differences, allowing models to make better predictions for molecular orbitals and total energies than previously possible. WALoss also substantially accelerates self-consistent-field (SCF) DFT calculations. Here, we show it achieves a reduction in total energy prediction error by a factor of 1347 and an SCF calculation speed-up by a factor of 18%. These substantial improvements set new benchmarks for achieving accurate and applicable predictions in larger molecular systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Density functional theory (DFT) (Kohn & Sham, 1965; Hohenberg & Kohn, 1964; Martin, 2020) has been widely used in physics (Argaman & Makov, 2000; Jones, 2015; Van Mourik et al., 2014), chemistry (Levine et al., 2009; Van Mourik et al., 2014), and materials science (March, 1999; Neugebauer & Hickel, 2013) to study the electronic properties of molecules and solids. This methodology is particularly valued for its balanced blend of computational efficiency and accuracy, rendering it a versatile choice for investigating electronic structure (Kohn et al., 1996; Parr & Yang, 1995), spectroscopy (Neese, 2009; Orio et al., 2009), lattice dynamics (Dal Corso et al., 1997; Wang et al., 2021), transport properties (Bhamu et al., 2018), and more. The most critical step in applying DFT to a molecule is constructing the Kohn-Sham Hamiltonian, which consists of the kinetic operator, the external potential, the Coulomb potential (also known as the Hartree potential), and the exchange-correlation potential (Kohn & Sham, 1965; Hohenberg & Kohn, 1964; Martin, 2020).\nThe Hamiltonian matrix contains crucial information about molecular systems and their quantum states (Kohn & Sham, 1965; Hohenberg & Kohn, 1964; Yu et al., 2023b; Zhang et al., 2024). This matrix facilitates the extraction of various properties, including the highest occupied molecular orbital (HOMO) and lowest unoccupied molecular orbital (LUMO) energies, the HOMO-LUMO gap, total"}, {"title": "2 BACKGROUND", "content": "In the framework of Density Functional Theory (DFT), a molecular system is defined by its nuclear configuration $\\mathcal{M} := {Z, R}$, where Z represents the atomic numbers of the nuclei and R their positions within the molecule. DFT focuses on determining the ground state of a system consisting of N electrons by minimizing the total electronic energy with respect to the electron density $\\rho(r)$.\nHere, $\\rho(r)$ is a functional of the set of N one-electron orbitals {$\\phi_i(r)}$}\nWe define the scalability as the model's accuracy under large molecules.\n$\\text{N}_{i=1}$, where $r \\in \\mathbb{R}^3$ specifies\nthe spatial coordinates of an electron."}, {"title": "3 WAVEFUNCTION-ALIGNMENT LOSS", "content": "The applicability of the predicted Hamiltonian $\\hat{H}$ depends significantly on the accuracy of its eigenvalues (orbital energies) and eigenvectors (basis coefficients). To assess the alignment between $\\hat{H}$ and the actual Hamiltonian $H^*$ with respect to their eigenspaces, we present the following theorem:\nTheorem 1. Let $H$ and $\\hat{H}$ represent Hamiltonian matrices, and $S$ the overlap matrix. Define the perturbation matrix as $\\Delta H := \\hat{H} - H$. Let $\\lambda_i(H, S)$ and $\\lambda_i(\\hat{H}, S)$ be the ith generalized eigenvalues of $H$ and $\\hat{H}$, respectively. Assume a spectral gap $\\delta$ separates the generalized eigenvalues of $H, H$. $\\kappa(\\cdot)$ denotes the conditional number of a given matrix, $|\\cdot||_2$ represents the spectral norm. $|\\Delta H||_{1,1} = \\sum_{i,j} |\\Delta H_{ij}|$. Then, the difference in eigenvalues and the angle $\\theta$ between the eigenspace of $H$ and $\\hat{H}$ satisfy:\n$|\\lambda_i(\\hat{H}, S) - \\lambda_i(H, S)| \\leq \\frac{\\kappa(S)}{||S||_2} \\cdot ||\\Delta H||_{1,1}, \\quad \\sin \\theta < \\frac{\\kappa(S)}{||S||_2} \\frac{||\\Delta H||_{1,1}}{\\delta}$\nCorollary 1 (Perturbation Sensitivity Scaling). Assume $\\lambda_{min}(S)$ of the overlap matrix $S$ scales as:\n$\\lambda_{min}(S) = c + \\frac{A}{1 + (\\frac{B}{N_o})^{\\alpha}}$, where $c > 0$, $A > 0$, and $\\alpha > 0$ describe the saturation behavior. Then,\nfor a perturbation matrix $\\Delta H$, the eigenvalue perturbation is bounded by:\n$|\\lambda_i(\\hat{H}, S) - \\lambda_i(H, S)| \\leq O\\Big(\\frac{\\kappa \\Big(c + \\frac{A}{1 + (\\frac{B}{N_o})^{\\alpha}}\\Big)}{||S||_2} (\\sigma B^{1/2} + B |\\mu|)\\Big)$\nRemark The theorem highlights that the difference between the predicted and actual Hamiltonian matrices, when only considering the element-wise norm, can lead to unbounded differences in eigen-values/eigenvectors due to a significant $\\frac{\\kappa(S)}{||S||_2}$ ratio. The corollary further elucidates the sensitivity of eigenvalue perturbations to the size of the basis $B$.This phenomenon underscores the catastrophic scaling associated with increasing $B$, which is a manifestation of the aforementioned SAD phe-nomenon. We provide a thorough discussion on $\\alpha$ in Appendix J.3. To validate our theoretical analysis, we empirically evaluated the distribution of $\\frac{\\kappa(S)}{||S||_2}$ across both the QH9 and PubChemQH datasets (Figure 6). The results demonstrate that molecules in PubChemQH exhibit substantially higher ratios compared to QH9, indicating increased perturbation sensitivity in larger molecular systems. This provides strong empirical evidence for the SAD phenomenon and aligns with our theoretical predictions. Consequently, these findings suggest that when designing an effective super-visory signal for learning or optimization tasks involving Hamiltonian matrices, it is crucial to take into account the interaction of the overlap matrix and the corresponding Hamiltonian to mitigate the potential instability caused by perturbations.\nIn light of this, we propose a novel loss function: the Wavefunction Alignment Loss. It is designed to preserve the integrity of the eigenstructure related to molecular orbitals. Let $\\hat{\\epsilon}$ and $\\epsilon^*$ represent the eigenvalues (orbital energies) of $\\hat{H}$ and $H^*$, respectively, and $\\hat{C}$ and $C^*$ denote the corresponding eigenvectors (basis coefficients). We define a primary form of the loss function by directly applying the Frobenius norm to the eigenvalues $\\mathcal{L}_{align} = \\frac{1}{n} \\sum_{i=1}^n || \\hat{\\epsilon}^{(i)} - {\\epsilon^*}^{(i)}||_F$, where ${\\epsilon^*}^{(i)}$ and $\\hat{\\epsilon}^{(i)}$ are derived from solving the generalized eigenvalue problems: $H \\hat{C} = S \\hat{C} \\hat{\\epsilon}$ and $H^*C^* = SC^*\\epsilon^*$, respectively.\nHowever, this formulation has notable limitations: (1) Generalized eigenvalue problems are suscepti-ble to numerical instabilities due to ill-conditioned matrices, leading to erroneous gradients during backpropagation through iterative eigensolvers, complicating optimization. (2) The loss function assigns uniform weights to all orbital energies, which is not practical as some orbital energies hold more significance. To address these issues, we begin by applying the following algorithm (Ghojogh et al., 2019; Golub & Van Loan, 2013) to perform a simultaneous reduction of the matrix pair (H*, S):\nAlgorithm 1 Simultaneous reduction of a matrix pair (H*, S)\nRequire: Groud-truth Hamiltonian matrix H* and overlap matrix S\nEnsure: Diagonal matrix $\\epsilon^*$ and matrix C* such that $(C^*)^TS C^* = I$ and $(C^*)^T H^* C^* = \\epsilon^*$\n1: Compute the Cholesky decomposition $S = G G^T$.\n2: Define $M^* = G^{-1}H^*G^{-T}$.\n3: Apply the symmetric QR algorithm to find the Schur form $(Q^*)^T M^*Q^* = \\epsilon^*$.\n4: Compute $C^* = G^{-T}Q^*$.\nWhen the overlap matrix S is ill-conditioned, the eigenvalues $\\epsilon^*$ computed by Algorithm 1 can suffer from significant roundoff errors. To mitigate this, we modify the algorithm by replacing the Cholesky decomposition of S with its eigen (Schur) decomposition $VSV^T = \\Sigma$, where V is the matrix of"}, {"title": "4 WANET", "content": "Here, we present WANet, a modernized architecture for Hamiltonian prediction. First, unlike previous approaches, we propose a streamlined design for Hamiltonian prediction that consists of two essential components: the Node Convolution Layer and the Hamiltonian Head. The Node Convolution Layer operates on a localized radius graph, performing graph convolution to capture intricate atomic interactions. This block serves a dual purpose: first, it generates an irreducible node representation, providing a powerful input for the subsequent Hamiltonian Head. Second, it can be initialized with a pretrained EGNN or reprogrammed for other downstream tasks, constituting a unified framework for molecular modeling. The Hamiltonian Head constructs both pairwise and many-body irreducible representations using the Clebsch-Gordon tensor product. These representations are then utilized to assemble both the non-diagonal and diagonal components of the Hamiltonian matrix. The model architecture and ablation studies are detailed in Figure 10 and Table 9, respectively."}, {"title": "4.1 NODE CONVOLUTION LAYER", "content": "For the Node Convolution Layer, we replace the traditional SO(3) convolutions with Equivariant Spherical Channel Network (eSCN) (Passaro & Zitnick, 2023; Liao et al., 2023). The eSCN framework primarily utilizes SO(2) linear operations, optimizing the computation of tensor products involved in the convolution process. Traditionally, SO(3) convolutions operate on input irreducible\n$k = \\lfloor \\frac{N}{2} \\rfloor$ for paired orbitals."}, {"title": "4.2 HAMILTONIAN HEAD", "content": "Sparse Mixture of Long-Short-Range Experts We introduce a variant of the Gated Mixture-of-Experts (GMOE) (Shazeer et al., 2017; Clark et al., 2022; Riquelme et al., 2021; Zoph et al., 2022; Jiang et al., 2024) model by incorporating a Mixture-of-Experts (MoE) layer tailored for pairwise molecular interactions. This enhancement draws inspiration from the Long-Short-Range Message Passing framework (Li et al., 2023), which differentiates between handling proximal and distal interactions through specialized layers. Our approach differentiates interaction dynamics based on distance, with closer pairs experiencing distinct interaction profiles compared to more distant pairs. This differentiation is achieved through a novel layer that substitutes the conventional pair interaction layer with a sparse assembly of expert modules, each functioning autonomously as a Pair Construction Layer. We define the Pair Construction layer with the function $\\mathcal{F}_{pair}$, and delineate the output of the MoE layer with N experts as: $F_{MoE}(x_t, x_s) = \\sum_{n=1}^N p_n(x_t,x_s) \\cdot \\mathcal{F}_{pair}^n (x_t, x_s)$, where $p_n(x_t, x_s)$ are the gating probabilities computed by the gating network, and $\\cdot$ denotes scalar multiplication. The gating probabilities are obtained by applying the Softmax function over the gating scores of all experts: $p_n(x_t, x_s) = \\frac{exp(G_n(z))}{\\sum_{m=1}^{N} exp(G_m (z))}$, where $z = rbf(||r_{ts}||_2)$ applies a radial basis function to the Euclidean distance $||r_{ts}||_2$ with a distance cutoff, and $G_n(z)$ represents the gating score for the n-th expert, computed as: $G_n(z) = z \\cdot W_{gn} + \\epsilon_n$. Here, $W_{gn}$ are learned gating weights for expert n, and $\\epsilon_n$ is injected noise to encourage exploration and promote load balancing among experts. Specifically, we use Gumbel noise (Jang et al., 2016): $\\epsilon_n = - \\log (-\\log (U_n))$,\n$U_n \\sim Uniform(0, 1)$. This noise enables a differentiable approximation of the top-K selection, allowing for sparse expert utilization while maintaining gradient flow during training. To further promote load balancing among the experts, we introduce an auxiliary load balancing loss (Shazeer et al., 2017):\n$\\mathcal{L}_{load\\_balancing} = N\\sum_{n=1}^{N} (\\frac{\\sum_{(t,s)} P_n (x_t,x_s)}{\\sum_{(t,s)} 1})^2$, which encourages the gating network to allocate routing probabilities evenly across experts, preventing underutilization of any single expert. The Pair Construction layer for each expert is defined as: $\\mathcal{F}_{pair}^n (x_t, x_s) = (\\mathcal{W}_{l,l,l_o} \\otimes x_s^{(li)} x_t^{(l_j)})^{(l_o)}$,\nwhere $x_s^{(l_i)}$ and $x_t^{(l_j)}$ are the $l_i$-th and $l_j$-th irreducible representations of source node s and target node t, respectively; $\\mathcal{W}_{l,l,l_o}$ are the learned weights that couple these representations into the output representation $l_o$; and $\\otimes$ denotes the tensor product.\nMany-Body Interaction Layer Considering many-body interactions for the diagonal components of the Hamiltonian in molecular systems captures essential electron correlation effects (Szabo & Ostlund, 2012; Jensen, 2017). These interactions provide a more accurate representation of the collective behavior of atoms, beyond pairwise approximations. This leads to a precise description of key quantum phenomena like electron delocalization and exchange interactions (Szabo & Ostlund, 2012). For this purpose, we employ the methodologies of the MACE framework (Batatia et al., 2022; Kov\u00e1cs et al., 2023; Batatia et al., 2023). Central to MACE is the adept conversion of first-order features into higher-order features using the so-called density trick (Duval et al., 2023). This procedure initiates with the formation of generalized Clebsch-Gordon tensor products from the first-order features:\n$B_{l_m}^{v} = \\sum_\\xi \\bigg[ \\prod_{i=1}^{\\nu} f_{l_i m_i} \\bigg]$, where $l_m = (l_1m_1,...,l_{\\nu}m_{\\nu})$,"}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate WANet with WALoss on the QH9 and PubChemQH datasets. Our evaluation metrics include MAE for the Hamiltonian, EHOMO, ELUMO, \u0404\u2206, \u0404occ, cosine similarity for the eigenvectors C, and relative SCF iterations compared to the initial guess, which are commonly used in previous works (Unke et al., 2021b; Yu et al., 2023b;a). Additionally, we introduce two new physics-related metrics-MAE for Corb and System Energy-to provide a more comprehensive evaluation. Detailed descriptions of these metrics are provided in Appendix C.1."}, {"title": "5.1 RESULTS ON THE PUBCHEMQH DATASET", "content": "Dataset Generation Process In our study, we investigated the scalability of Hamiltonian learning by utilizing a CUDA-accelerated SCF implementation (Ju et al., 2024) to perform computational quantum chemistry calculations, thereby generating the PubChemQH dataset. We began with geometries from the PubChemQC dataset by (Nakata & Maeda, 2023), selecting only molecules with a molecular weight above 400. This filtration process resulted in a dataset comprising molecules with 40 to 100 atoms, totaling over 50,000 samples. We chose the B3LYP exchange-correlation functional (Lee et al., 1988; Beeke, 1993; Vosko et al., 1980; Stephens et al., 1994) and the Def2TZV basis set (Weigend & Ahlrichs, 2005; Weigend, 2006) to approximate electronic wavefunctions. Generating this comprehensive dataset represents a substantial computational effort, requiring approximately one month of continuous processing using 128 NVIDIA-V100 GPUs. We provide a comparison between the PubChemQH dataset and the QH9 dataset in Appendix H."}, {"title": "5.2 RESULTS ON THE QH9 DATASET", "content": "The QH9 dataset is a comprehensive quantum chemistry resource designed to support the development and evaluation of machine learning models for predicting quantum Hamiltonian matrices. Built upon the QM9 dataset, QH9 contains Hamiltonian matrices for 130,831 stable molecular geometries, encompassing molecules with up to nine heavy atoms of elements C, N, O, and F. These Hamiltonian matrices were generated using pyscf with the B3LYP functional (Lee et al., 1988; Beeke, 1993; Vosko et al., 1980; Stephens et al., 1994) and the def2SVP basis set. Table 2 presents a comparative analysis of the performance of our model, WANet, against the baseline model, QHNet, on the QH9 dataset in both stable and dynamic settings. In the QH9-stable experiments, WANet"}, {"title": "5.3 COMPARISON WITH A PROPERTY REGRESSION MODEL", "content": "Conventional machine learning approaches typically employ property regression, mapping molecular features directly to the desired property value (Blum & Reymond, 2009; Montavon et al., 2013). A common question arises: why use Hamiltonians instead of a property regression model? We argue that property regression methods often fail to incorporate underlying quantum mechanical principles, limiting their generalization capability. To illustrate this, we compared the performance of WANet with WALoss to a model utilizing Equiformer V2 (Liao & Smidt, 2023), UniMol+ (Lu et al., 2023), and UniMol2 (Xiaohong et al., 2024) with invariant regression heads, using identical training and test sets. As shown in Table 1, WANet with WALoss demonstrates significantly lower MAE values in predicting key quantum chemical properties. Specifically, WANet with WALoss achieves an 88.88% improvement in ELUMO MAE and a 58.81% improvement in e\u25b3 MAE. Moreover, the Hamiltonian predicted by WANet with WALoss is not limited to specific properties. It enables the accurate calculation of various critical properties, such as electronic densities, dipole moments, and excited-state energies, all from a single model. Additionally, it can be applied to SCF acceleration. In contrast, the Equiformer V2 regression model is constrained to predicting a narrow set of specific properties, necessitating the training of a new model for each new property."}, {"title": "5.4 EFFICIENCY EVALUATION OF WANET", "content": "WANet exhibits efficiency advantages in two aspects: (1) its application to SCF relative to traditional DFT calculations, and (2) its efficiency and resource usage compared to existing state-of-the-art neural networks. Specifically, WANet can predict Hamiltonians for large molecular systems significantly faster than traditional DFT methods. Although the neural network evaluation introduces a small overhead, WANet substantially reduces the number of required SCF iterations, resulting in a faster overall computation time. This notable speed-up makes WANet particularly advantageous for applications requiring rapid predictions for large molecular systems, such as high-throughput virtual screening. Furthermore, WANet outperforms QHNet in training and inference efficiency on the PubChemQH dataset, offering faster training times, improved inference speeds, and lower peak GPU memory usage"}, {"title": "5.5 MOLECULAR PROPERTIES BEYOND ENERGY PREDICTIONS", "content": "To validate the versatility of the Hamiltonian predicted by WANet with WALoss, we extended our exper-"}, {"title": "5.6 SCALABILITY IN ELONGATED CARBON CHAIN", "content": "To evaluate the scalability of our model trained on PubChemQH, we conducted inference using elongated alkanes ($C_xH_{2x+2}$), a series of saturated hydrocarbons. We compared three mod-els: our model with WALoss, our model without WALoss, and an initial guess algorithm.\nThe results, shown in Fig-ure 4, demonstrate that our model with WALoss achieves enhanced perfor-mance in predicting LUMO and HOMO energies, partic-ularly in the \u201cD2\u201d region, where the atom count ex-ceeds the range of the Pub-ChemQH dataset. Notably, our model with WALoss also performs well on elon-gated alkanes with up to 182 atoms\u2014three times the average atom count of the PubChemQH training set (60). These findings high-light the effectiveness of WALoss in enhancing the scalability and applicability"}, {"title": "5.7 ABLATION STUDY ON WALOSS", "content": "To evaluate the effectiveness of our proposed WALoss, we conducted an ablation study with three variations: full WALoss, naive WALoss, and WALoss without reweighting. The naive WALoss applies the Frobenius norm to the eigenvalues leveraging backpropagation through eigensolvers, defined as $\\mathcal{L}_{naive} = \\frac{1}{n} \\sum_{i=1}^{n} || \\hat{\\epsilon}^{(i)} - {\\epsilon^*}^{(i)}||_F$, where ${\\epsilon^*}^{(i)}$ and $\\hat{\\epsilon}^{(i)}$ are derived from solving generalized eigenvalue problems. The WALoss without reweighting calculates the loss uniformly across all eigenvalues. As shown in Table 4, the full WALoss achieves the lowest MAEs across all metrics. The naive WALoss performs poorly, highlighting several challenges associated with optimizing the naive loss function. Removing reweighting also degrades performance, though not as drastically. Overall, these results validate the design choices in formulating WALoss to improve Hamiltonian prediction."}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "In this work, we introduced WALoss, a loss function designed to improve the accuracy of predicted Hamiltonians. Our experiments demonstrate that incorporating WALoss achieves state-of-the-art performance by reducing prediction errors and accelerating SCF convergence. Additionally, we introduced a new dataset, PubChemQH, and an efficient model, WANet. However, limitations remain, such as the high computational cost of generating large training sets. Despite these challenges, deep learning approaches incorporating WALoss show great promise in advancing computational chemistry and materials science."}]}