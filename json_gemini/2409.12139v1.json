{"title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models", "authors": ["Everest Team", "Ximalaya"], "abstract": "With the advent of the big data and large language model era, zero-shot person- alized rapid customization has emerged as a significant trend. In this report, we introduce Takin AudioLLM, a series of techniques and models, mainly including Takin TTS, Takin VC, and Takin Morphing, specifically designed for audiobook production. These models are capable of zero-shot speech production, generat- ing high-quality speech that is nearly indistinguishable from real human speech and facilitating individuals to customize the speech content according to their own needs. Specifically, we first introduce Takin TTS, a neural codec language model that builds upon an enhanced neural speech codec and a multi-task train- ing framework, capable of generating high-fidelity natural speech in a zero-shot way. For Takin VC, we advocate an effective content and timbre joint modeling approach to improve the speaker similarity, while advocating for a conditional flow matching based decoder to further enhance its naturalness and expressive- ness. Last, we propose the Takin Morphing system with highly decoupled and advanced timbre and prosody modeling approaches, which enables individuals to customize speech production with their preferred timbre and prosody in a precise and controllable manner. Extensive experiments validate the effectiveness and robustness of our Takin AudioLLM series models. For detailed demos, please refer to https://takinaudiollm.github.io/.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) [1, 2, 3, 4], neural codecs [5, 6, 7, 8], and diffusion and flow models [9, 10, 11, 12, 13] have led to significant progress in the fields of zero-shot text-to-speech synthesis (TTS) [14, 15, 16, 17, 18], voice conversion (VC) [19, 20, 21, 22], and related areas. These innovations enable the synthesis of high-quality speech without extensive model training, thereby enhancing the accessibility and scalability of these technologies and fostering more natural and immersive user interactions.\nIn this context, to drive innovation and support audiobook production, we propose Takin Audi- OLLM-a series of models designed to allow users to customize speech content according to their specific needs while generating high-quality, near-human-like speech with exceptional naturalness and expressiveness. The Takin AudioLLM series comprises Takin TTS, Takin VC and Takin Morphing.\nFirstly, inspired by the powerful contextual learning capabilities of LLMs, we present Takin TTS\u2014a robust and effective neural codec language model for audiobook production. Takin TTS incorporates a high-fidelity, low-bandwidth neural speech codec based on efficient disentangled prompt encoders, which reduces modality heterogeneity between text and audio, thereby enhancing the LM's prediction accuracy. We introduce a five-stage multi-task training strategy that significantly improves overall LM performance, ensuring robustness and effectiveness in complex real-world scenarios. Additionally, we employ a latent diffusion model and Vocoder for token-to-speech synthesis, further improving speech quality and naturalness. Consequently, Takin TTS excels in generating high-quality, natural-sounding speech for various applications, from interactive voice response systems to sophisticated text-to-"}, {"title": "2 Takin TTS", "content": "speech frameworks. This approach greatly enhances user experience and demonstrates substantial potential in advancing generative speech modeling technology.\nSecondly, Takin-VC employs a joint modeling approach that integrates timbre features with both supervised and self-supervised content representations to enhance speaker similarity and intelligibility. This design allows Takin-VC to effectively capture and reproduce the nuanced characteristics of various speakers, ensuring that converted voices closely resemble the target speakers. Furthermore, to refine speech quality and naturalness, we incorporate an efficient conditional flow matching-based decoder. This advanced decoder optimizes the alignment between timbre and content features, leading to more accurate and natural voice conversion. In this way, Takin-VC provides a powerful and versatile tool for voice conversion applications, excelling in producing high-fidelity, natural-sounding voice conversions suitable for audiobook production. It significantly enhances user experience and demonstrates its potential to advance the field of voice conversion technology.\nFinally, Takin Morphing introduces an attention mechanism-based multi-reference timbre encoder for precise and detailed timbre modeling. Additionally, a language model (LM)-based prosody encoder is employed to capture prosody representations that align with timbres for unseen speakers in an auto-regressive manner. To further enhance waveform quality, we advocate a two-stage information-flow-based training method. Through these innovations, Takin Morphing enables users to utilize timbres from various unseen speakers and combine them with preferred prosody styles, thus generating personalized audiobooks with a high degree of control. This capability meets the demands of diverse speech synthesis applications, from entertainment and education to commercial contexts, offering a more natural and enriched auditory experience.\nOverall, Takin AudioLLM represents a significant advancement in zero-shot speech production technology. By leveraging the sophisticated capabilities of Takin TTS, Takin VC, and Takin Morphing, this series not only advances the state-of-the-art in speech synthesis but also addresses the growing demand for personalized audiobook production, enabling users to tailor speech generation precisely to their requirements."}, {"title": "2.1 Overview", "content": ""}, {"title": "2.2 Pretrain", "content": "We use multimodal data to pretrain the Takin TTS. Specifically, we encode text and audio data into tokens and input them into the GPT model to learn relevant knowledge. For text data, we develop an internally developed G2P (Grapheme-to-Phoneme) method. This solution includes a Text"}, {"title": "2.3 Supervised Fine-tuning (SFT)", "content": "Following unsupervised learning on extensive data, our Takin TTS has developed a robust capacity to comprehend text and audio information. In the subsequent phase, akin to GPT-4 [2], we employ labeled paired data to train the Takin TTS model for downstream tasks such as TTS and Automatic Speech Recognition (ASR) [23, 24, 25], thereby enhancing its proficiency in managing text and speech tasks.\nIn the TTS task, zero-shot is a quite important capability of applications that requires the model to synthesize high-quality speech for unseen speakers without collecting their labeled data for training in advance. In this work, leveraging the ability of neural codec to convert speech into discrete tokens, the zero-shot TTS task is regarded as a conditional language modeling task to predict discrete codec tokens autoregressively based on given conditions.\nLet D = {Ti, Pi, Si} denotes the training dataset, where Si is the target speech, Ti is the text description and P\u2081 is prompt audio which is from the same speaker with Si. During the training process, a set of speech conditions SC\u2081 is extracted from prompt audio P\u2081 via acoustic prompt encoder, the text transcription T\u2081 is converted to a phoneme sequence TPi = {BP, Pi\u2081, Pi2, ..., Pim, EP}, and BP stands for the Begin of Phone Sequence, EP stands for the End of Phone, the target speech is passed to neural codec model to get discrete codec tokens Ci = {Ci1, Ci2, ..., Cin }. A start identifier s and an end identifier e are inserted at the beginning and the end of codec tokens. The input training sequence is constructed as follows:\n[SCi, TPi, Si, Ci, E]\nAs shown in Figure 2, the language model is only trained to predict codec tokens and the end of sequence token E conditioned on phone sequence TPi and speech conditions SCi, which is"}, {"title": "2.4 Continual Supervised Fine-tuning (CSFT)", "content": "formulated as:\n$P(C_i\\S, SC_i, TP_i) = \\prod_{t=1}^{n+1} [ P(C_{it}|C_{i<t}, S, SC_i, TPi)]$\nwhere Cn+1 denotes the end of sequence token E. During inference, the language model generates tokens autoregressively based on given text and reference speech, and fed these tokens to neural codec model to generate audio.\nWhile Supervised Fine-Tuning (SFT) has endowed the Takin TTS model with TTS capabilities, the diverse content standards generated by TTS often lead to more frequent word omissions in Autoregressive (AR) models compared to Non-Autoregressive (NAR) models during inference [26, 27]. As a consequence, to enhance the stability of the system's TTS functionality, further Continual Supervised Fine-tuning (CSFT) joint with ASR guided training is necessary. In our method, CSFT primarily consists of two components: Domain-SFT and Speaker-SFT, which will be elaborated below."}, {"title": "2.4.1 Domain SFT", "content": ""}, {"title": "2.4.2 Speaker SFT", "content": "To ensure that the narration of high-quality audiobooks sounds more natural and aligns closely with the original speaker's performance style, we have further introduced the Speaker SFT Phase. In this section, we continue to use the LoRA training method. The difference here is that we freeze most of the GPT parameters to retain the model's foundational knowledge and update the parameters of the Acoustic Prompt Encoder with the Input and Output Embedding Layer parts of GPT."}, {"title": "2.4.3 ASR guided Joint Training", "content": "To improve the accuracy of the model's output content, we incorporate ASR guidance into the model training during the finetuning process. The sequences output by GPT are fed into a codec decoder to be restored to wav format. To ensure gradient propagation and training speed, the generated wav is input into the whisper model, and its output is compared with the annotations to calculate the cross-entropy loss."}, {"title": "2.4.4 Reinforcement Learning", "content": "Despite the fact that the model after CSFT Process performs quite well, even surpassing human rendition levels for certain sentences by some speakers, it still faces issues with varying effectiveness among different speakers for the same text, as well as discrepancies between human and machine aesthetics. To make the generated content as closely aligned with human preferences as possible, we have introduced the concept of our RL (Reinforcement Learning) method. As shown in Figure 2, The RL is placed in the end of the whole diagram of Takin TTS to further improve the performance by aligning the model with human preference.\nCurrently RL methods [29, 30, 31, 32], follow a Sampling-human-annotating-learning pipeline, in which human evaluation is applied to model-generated outputs to ensure the model learns to align with subjective human preferences. The pipeline works also in speech generation task [33]. However the human ratings is labour dmanded, there are also some works studying to [34, 35] use objective metrics to replace human ratings, in order to facilitate the obtaining of preference data pairs. We also explore leveraging the human-rating only pipeline to combine it with a set of objective metrics which partly indicate human preferences, namely the Sampling-human&machine-annotating-learning pipeline."}, {"title": "2.4.5 Instruction Style Control", "content": "In AudioLLM paradigm, the common method of controlling speech style involves selecting different audio prompts, which generally incorporate both speaker identity and style information simultane- ously. To explore the full potential of controllability, we propose TakinTTS-Instruct to synthesize speech with various styles and emotions, including rhythm, pitch, paralinguistics, etc., using natural language as style prompts which is more user-friendly than the base model of Takin TTS and could decouple the speaker and style in synthesis.\nThe lower-left part of Figure 2 prominently displays the core structure of TakinTTS-Instruct. To be specific, a robust pre-trained speaker verification system [36] is employed to provide additional voice characteristics, enhancing the similarity between the synthesized voice and the target speaker. Moreover, unlike previous speech emotion or speaking state recognition tasks [37, 38, 39, 40], in order to control the emotions of the generated speeches, speaking states, or other linguistic dimensions in a more user-friendly fashion, we implement a predictor that detects and classifies emotions or different speaker states in spoken language and subsequently outputs its corresponding natural language description. Furthermore, these descriptions will be parsed by SimBERT[41] into an embedding form to be incorporated into the model training."}, {"title": "3 Takin VC", "content": "In addition to TTS, another widely used technology in the audiobook business is VC technology. Here, we propose a novel and effective zero-shot VC approach based on DDPM or CFM. Similar to the usage conditions of TTS technology, it can achieve high-expressiveness timbre conversion with only 5-10 seconds of unseen audio."}, {"title": "3.1 VC Training", "content": "The input of Takin VC is composed of two parts: Phonetic Posteriorgrams (PPG), utilizing the output features of HybridFormer [42] in this case, and a truncated prompt mel fragment. For the output, Takin VC offers two alternatives: it can either produce mel spectrograms, which are subsequently converted to audio samples through a vocoder, or directly generate audio samples."}, {"title": "3.2 VC CSFT", "content": "Similar to parts of the Takin TTS, after pre-training on a large amount of data, fine-tuning with a small amount of high-quality data can enhance the model's performance. Furthermore, although the timbre information retained in the PPG feature is already minimal, there are still minor timbre leakage issues, resulting in suboptimal timbre conversion similarity in some cases. Therefore, we employed the TTS system to improve the performance in this regard. Due to the duration control issues, we used a traditional TTS system here to generate a small amount of parallel data, which is used to better guide the model in understanding the speech conversion task."}, {"title": "4 Takin Morphing", "content": "Audio Style Transfer is an important application in the field of audiobook production, which involves transforming styles while retaining the speaker's vocal timbre, thereby lowering the barrier to becoming a professional broadcaster. By using this technology, works by enthusiasts who are not yet proficient in certain broadcasting techniques can be transformed to have the style of professional broadcasters, thereby improving the quality of the works to some extent. Alternatively, it can serve as an auxiliary in teaching, guiding enthusiasts to develop their own unique broadcasting styles. Here, we introduce the Takin Morphing technology, which utilizes the form of DDPM to achieve style and rhythm transfer while maintaining the speaker's vocal timbre."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Takin TTS Settings", "content": "Experimental Datasets To train and evaluate Takin TTS, We build a large multilingual base dataset for pretraining and 1st round of SFT training. To evaluate CSFT and RL training, several carefully human labeled datasets are built which including domain datasets and speaker datasets. The datasets are depicted as follows:\n\u2022 Base TTS Dataset: In-house dataset including over 1M hours of speech data, which may include some labelling errors.\n\u2022 Domain TTS Dataset: The domain dataset is of high-quality dataset with all the transcripts manually checked. There are two domains exist in the domain dataset which are audiobook and podcast. There is around 1000 hours for each domain used for Domain SFT. For speech data of each domain, 5% of the whole dataset is held out for test purpose and we make sure the held out test set does not have speaker overlap with the train set.\n\u2022 Speaker TTS Dataset: To conduct speaker SFT based on the result of Domain SFT, a small Speaker TTS Dataset is constructed by selecting two audiobook-domain speakers and two podcast-domain speakers from our in-house dastaset. There is 1-hour speech data for each speaker, and likewise their transcripts are carefully labeled. For each speaker, 5% of speech data is held out as test set.\nEvaluation Metrics To conduct objective evaluations, We employ the Phoneme Error Rate (PER) and Speaker Similarity (SIM) metrics. For PER, we pick Whisper-large-v3 [44] as the ASR model to conduct the PER test, While for SIM, we use CAM++ on the speaker verification task [45] to obtain speaker embeddings for calculating the cosine similarity of speech samples of each test utterance against reference clips. For subjective evaluations, We employ the Mean Opinion Scores (MOS) by rating different speech samples of the same content by human evaluators. The scores vary from 1 to 5 and the higher score indicates better speech quality. Besides, Bad Case Rate (BCR) is used to evaluate the overall stability of our models in RL experiments. Equation 3 is defined to compute BCR, in which B is the number of bad cases. To count the number of bad cases, We count the occurances of three types of bad cases covering prosody, pronunciation and missing or extra speech."}, {"title": "5.1.1 Pretraining", "content": ""}, {"title": "5.1.2 CSFT on Takin TTS", "content": "After pretraining, Talkin TTS model is finetuned with Base TTS dataset to align the model to TTS task, namely SFT. However, this finetuned model is not prepared for real applications in terms of its stability and expressiveness as mentioned in section 2.4. CSFT is key to getting a stable model and enhance its expressiveness, especially when generating speech of a specific speaker. This section is mainly focused on the experiments of two types of CSFT which are domain SFT and speaker SFT."}, {"title": "5.1.3 RL Training on TTS", "content": "RL training can be employed as an extra post-training stage after either Domain SFT or Speaker SFT. Both experiments are conducted to verify the effectiveness of RL training, especially on expressiveness and BCR. To prepare training data for RL, we make a set of good / bad examples with both subjective ratings and objective metrics. As [46] shows repeated sampling is able to largely increase the pass coverage to queried problems, we get 5 samples by repeated sampling for each sentence. For objective ratings, we pick PER and UTMOS [47] as objective metrics to generate preferences considering both metrics. For subjective ratings, there are 50 human raters being"}, {"title": "5.1.4 Emotion Control Based on Instructions", "content": "Data preparation Regarding the textual description, our professional data expert proposes three dimensions for annotating a voice recording: speaking emotion, speaking state, and speaking rhythm. Considering the difficulty and accuracy of annotation, the dimension of emotion is more distinctive compared to the other two dimensions, with the control of the remaining dimensions acting as supplementary control for the emotional dimension. We have established nine commonly recognized emotional directions(see in Table4) and then described them using various synonymous natural language. We have annotated approximately 100 hours of audio data, with each audio clip's corresponding textual annotation cross-validated by three different experienced data annotators. All these annotated data are utilized for supervised fine-tuning on a large language model."}, {"title": "5.1.5 Efficient Inference and Serving", "content": "To generate speech with superior quality, we use auto-regressive LLMs and diffusion models in Takin, which are difficult and expensive to deploy. So we use various techniques and tricks to build an inference service with low latency and high concurrency. Our efforts on TTS task are as follows: Since most of the computation is spent on LLM model inference, we deploy a separate service for LLM to maximize GPU utilization and throughput for token prediction. Flash attention[48, 49] and paged attention[50] techniques are used in the prefill and the decode phases respectively, to reduce the consumption of memory and computation. Mixed precision and quantization techniques such as GPTQ[51] and AWQ[52] are also used to achieve further speedup. Besides, we adopt a suite of kernel-level optimizations, which leverage hardware-specific features and software techniques to accelerate critical computation kernels. As described above, CSFT strategy is used to improve the stability of synthesis. But it is not practical to deploy separate inference services for different domains and speakers. So we support multiple LoRAs in the same service, as well as batch inference for different LoRAs. Streaming inference is applied to scenarios such as real-time interaction, and the first packet delay is less than 300 ms."}, {"title": "5.2 Takin VC Experiments", "content": ""}, {"title": "5.2.1 Takin VC Datasets", "content": "Training Dataset used in Takin VC training heavily overlaps with the data in the TTS dataset, including approximately 500,000 hours of web-scraped and internal data.\nTest Dataset We random select 100 out-of-set speaker speech data from the Internet. In addition, these speakers include different attributes such as gender, age, language, and emotion. Each speaker has about 1 to 3 sentences for different attributes."}, {"title": "5.2.2 Takin VC Performence", "content": "As shown in Table 6, our proposed Takin VC scheme surpasses the baseline solution in terms of both sound quality and speaker similarity. Our experiments were conducted under conditions of large datasets to ensure the scheme's effectiveness on a large scale."}, {"title": "5.3 Takin Morphing Experiments", "content": "Experimental Datasets We trained Takin Morphing on a substantial corpus consisting of 20,000 hours of multilingual speech recordings in English and Chinese, consisting of in-house dataset alongside filtered portions of the WenetSpeech [53] and LibriLight [54]. To assess the performance of the proposed approach, we perform zero-shot speech synthesis and prosody transfer evaluations using in-house test sets which will be detailed below.\nEvaluation Metrics To conduct an in-depth analysis of the proposed Takin Morphing approach, various objective and subjective metrics are employed. To elaborate, PER and SIM are used as objective measures as well, while quality mean option score (QMOS) is employed to assess quality, clarity, naturalness, and high-frequency details, and similarity mean option score (SMOS) is used to measure speaker similarity with respect to timbre reconstruction and prosodic patterns for subjective evaluation."}, {"title": "5.3.1 Zero-shot Speech Synthesis", "content": "To examine the zero-shot speech synthesis performance of the proposed Takin Morphing, we first designed two distinct test sets, referred to as the objective and the subjective test sets. The objective test set includes 2,000 samples each from in-house English (EN) and Mandarin (ZH) speech corpora, while the latter comprises 200 highly expressive in-house samples in both EN and ZH as well. Notably, each sample in the subjective test set includes a reference utterance and a target utterance spoken by the same speaker. During inference, the Takin Morphing System generates speech for the target text using the reference speech as an audio prompt. The results are presented in Table 7."}, {"title": "5.3.2 Prosody Transfer", "content": "To validate the prosody transfer performance of Takin Morphing, we transfer the styles from our internal dataset to audio samples from our main platform. Specifically, we randomly select 20 speakers from the main platform and choose 50 sentences for each of them. Subsequently, for each sentence of the selected speakers, we randomly choose an emotional speech clip from the internal emotional dataset and use it as the prosodic reference."}, {"title": "6 Applications", "content": ""}, {"title": "6.1 Audiobook Generation", "content": "Takin TTS shows a large superiority comparing to conventional neural speech synthesis methods [55, 56, 57, 58, 59], which revolutionizes the field of AI audiobook generation. Two distinct approaches to creating immersive audio experiences using Takin TTS are explored. In the first approach, purely AI-generated audio content is produced, where different AI-powered voices act as various characters, bringing the story to life with diverse and nuanced performances. This approach allows for a consistent and scalable production process, potentially reducing costs and time associated with traditional audiobook recording. The another approach combines AI and human voices, with Takin TTS handling narration while human voice actors take on the dialogue parts. This hybrid approach leverages the efficiency and consistency of AI-generated speech for descriptive passages while preserving the emotional depth and authenticity that human actors bring to character interactions. The AI-generated audiobook samples can be listened in our demo page."}, {"title": "6.2 Voice Clone", "content": "In recent years, zero-shot timbre cloning technology has achieved significant advancements in voice cloning and speech synthesis, and is widely used in various fields. In voice assistants and customer service robots, it provides a more natural interaction experience; in the fields of film and entertainment content production, it is used for dubbing and creating voices for animated characters; in voice memos and recordings, it clones the voices of celebrities for future preservation. In music production, it can mimic the timbre of specific instruments; in education and training, it creates learning materials with standard pronunciations; in medical and rehabilitation, it helps patients who have lost the ability to speak regain their voices. Additionally, historical reconstructions and museum exhibits benefit from this technology. Using Takin VC's technology, the model requires only a few seconds to tens of seconds of audio samples to generate high-quality simulated voices, greatly reducing the technical threshold and making the aforementioned applications possible."}, {"title": "6.3 Talking head", "content": ""}, {"title": "7 Authors (alphabetical order of family name)", "content": "\u2022 Sijin Chen\n\u2022 Yuan Feng\n\u2022 Laipeng He\n\u2022 Tianwei He\n\u2022 Wendi He\n\u2022 Yanni Hu\n\u2022 Bin Lin\n\u2022 Yiting Lin\n\u2022 Yu Pan\n\u2022 Pengfei Tan\n\u2022 Chengwei Tian\n\u2022 Chen Wang\n\u2022 Zhicheng Wang\n\u2022 Ruoye Xie\n\u2022 Jixun Yao\n\u2022 Jianhao Ye\n\u2022 Jingjing Yin\n\u2022 Quanlei Yan\n\u2022 Yuguang Yang\n\u2022 Yanzhen Yu\n\u2022 Huimin Zhang\n\u2022 Xiang Zhang\n\u2022 Guangcheng Zhao\n\u2022 Hongbin Zhou\n\u2022 Pengpeng Zou"}]}