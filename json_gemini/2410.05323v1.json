{"title": "From Incomplete Coarse-Grained to Complete Fine-Grained: A Two-Stage Framework for Spatiotemporal Data Reconstruction", "authors": ["Ziyu Sun", "Haoyang Su", "En Wang", "Funing Yang", "Yongjian Yang", "Wenbin Liu"], "abstract": "With the rapid development of various sensing devices, spatiotemporal data is becoming increasingly important nowadays. However, due to sensing costs and privacy concerns, the collected data is often incomplete and coarse-grained, limiting its application to specific tasks. To address this, we propose a new task called spatiotemporal data reconstruction, which aims to infer complete and fine-grained data from sparse and coarse-grained observations. To achieve this, we introduce a two-stage data inference framework, DiffRecon, grounded in the Denoising Diffusion Probabilistic Model (DDPM). In the first stage, we present Diffusion-C, a diffusion model augmented by ST-PointFormer, a powerful encoder designed to leverage the spatial correlations between sparse data points. Following this, the second stage introduces Diffusion-F, which incorporates the proposed T-PatternNet to capture the temporal pattern within sequential data. Together, these two stages form an end-to-end framework capable of inferring complete, fine-grained data from incomplete and coarse-grained observations. We conducted experiments on multiple real-world datasets to demonstrate the superiority of our method.", "sections": [{"title": "I. INTRODUCTION", "content": "SPATIOTEMPORAL data is of paramount importance in numerous applications, ranging from environmental monitoring to urban planning and beyond [1]-[5]. Existing methods for collecting spatiotemporal data primarily rely on either fixed sensors or mobile sensing technologies [6]. However, these approaches face significant limitations in terms of cost and complexity, resulting in that we can only obtain the coarse-grained spatiotemporal data, and it often fails to cover all spatiotemporal areas comprehensively [7], [8]. Obviously, such coarse and sparse data is insufficient to support various urban computing tasks. This limitation raises an intuitive and important new problem called spatiotemporal data reconstruction, i.e., how can we infer the complete and fine-grained spatiotemporal data from the sparse and coarse-grained observations?\nCurrently, there are efforts to adapt super-resolution methods from computer vision to infer fine-grained results from coarse-grained observation. [9] employs distributional upsampling to obtain fine-grained results, while [10] investigates generative models to achieve similar outcomes. However, these methods are inadequate for handling sparse data, which is common in practical data collection. It is obvious that spatiotemporal reconstruction is significantly more challenging than the super-resolution of spatiotemporal data, with the primary difficulty in capturing correlations within sparse data. Super-resolution requires understanding spatial structural information from coarse-grained data to generate fine-grained details. However, missing data disrupts the spatial structural information, thereby affecting our understanding of spatial relationships. For example, if we don't know there is a hospital on the map, we cannot infer the fine-grained structure of the hospital. Therefore, how to understand the complicated spatial correlation from only sparse observation is the first challenge.\nEven when spatial correlations can be gleaned from incomplete observations, temporal correlations remain challenging for existing work [9], [11] to handle. Spatiotemporal data typically exhibits strong periodicity and trends over time [12], which are characteristics not present in image or video data used in computer vision. Moreover, these temporal patterns exhibit complex cycles, such as daily, weekly, and monthly fluctuations, and are further associated with external factors like holiday setting and weather change. The challenge is compounded when observations are sparse and coarse-grained. Thus, our second challenge is to understand and utilize these temporal correlations to guild reconstruction.\nMoreover, the issue of varying sparse data patterns adds another layer of complexity to the reconstruction process. Some studies in the traffic domain have explored tasks similar to spatiotemporal data reconstruction, but they rely on stationary and evenly distributed sensors [13]. However, sensor placement has an unexpectedly significant impact on the reconstruction outcome. The spatial distribution of sensed data may be uneven, and it can change over time, which has been shown to greatly affect the performance of existing methods. Thus, the third challenge is designing a general and robust method capable of handling data with varying sparse patterns.\nTo address these challenges, we propose a novel two-stage model called DiffRecon which aims to effectively handle the sparse nature of the data, accurately capture spatiotemporal correlations, and account for varying sparse patterns, thereby providing a comprehensive solution to spatiotemporal data reconstruction. In the first stage, our primary goal is to enable the model to grasp the underlying structural information from the sparse observed data and augment it by filling in the missing data. We call this stage coarse-grained completion. By treating each observed data point as an individual spatiotemporal point and considering the relationships between them, we no longer attempt to learn the distribution of specific data pattern, making the approach robust to all sparse patterns. After coarse-grained completion, we infer fine-grained data based on the coarse-grained intermediate result. Temporal patterns such as periodicity and trend are considered in this stage, thanks to the preprocessing and augmentation performed in the coarse-grained completion stage. Our model will be jointly trained in two stages to provide end-to-end reconstruction results.\nTechnically, the two sub-models, namely Diffusion-C and Diffusion-F, both utilize the Denoising Diffusion Probabilistic Model (DDPM) [14] as their foundational framework. DDPM has become a powerful model for data inference by artificially adding noise and training a denoising netework to remove it. Therefore, the key lies in the design of the denoising network. Typically, a denoising network follows an encoder-decoder structure. The encoder in traditional denoising networks focus solely on intra-map information within a single map, neglecting the rich inter-map information across the sequence. To address this, we augument the denoising network by designing task-specified encoders for each stage to enhance the generation performance. In Diffusion-C, the input spatiotemporal map sequences are incomplete, but the data granularity remains consistent throughout. This consistency allows us to leverage the relationships between spatiotemporal points (ST-points) to infer missing data points. Specifically, we propose ST-PointFormer, which calculates the relations between ST-points using a multi-head self-attention mechanism. In this process external features are also encoded to form the ST-points. Since ST-PointFormer considers the correlations between any two ST-points rather than memorizing specific data distributions, it is robust to varying sparse patterns.\nIn the second stage, the data granularity changes, but the input data maps are complete and thus the historical sequence at each spatial location can be obtained. For this reason, we design a temporal encoder called T-PatternNet, which explicitly models temporal patterns\u2014such as periodicity-within the data sequence of each position. T-PatternNet is a novel method for modeling time sequence that excels at extracting multi-periodicity by using a Fast Fourier Transformer (FFT) along with 2D convolutions to analyze sequence data. The extracted temporal information is then aligned with the encoding from the traditional denoising network's encoder and pass the decoder together to acquire the denoised results.\nTo conclude, our main contributions are as follows:\nWe propose a novel task called spatiotemporal data reconstruction. Spatiotemporal data reconstruction aims to infer complete fine-grained spatiotemporal map from sparse coarse-grained observed data. Additionally, no specific sparse pattern of observed data is required in this task.\nWe propose a two-stage method called DiffRecon as a solution to spatiotemporal data reconstruction. With task-specialized denoising network, we successfully apply the diffusion method on spatiotemporal data, fully considering the spatiotemporal characteristics of observed data.\nWe conduct extensive experiments on datasets exhibiting three representative sparse patterns to evaluate the effectiveness of our method. We also conduct multiple ablation studies to demonstrate the contribution of each module.\nThe remainder of the paper is organized as follows. We introduce the related work in Section II, followed by a detailed problem formulation of spatiotemporal data reconstruction task in section III. After that, we introduce our proposed solution DiffRecon in section IV. The experimental results are finally demonstrated in section V."}, {"title": "II. RELATED WORK", "content": "Due to cost constraints and the prevalence of transmission errors in devices, the spatiotemporal data collected is often incomplete or even sparse. Spatiotemporal data completion methods aim to infer the complete data distribution from the incomplete data collected. In the early stages, researchers considered treating spatiotemporal data distributions divided into grid regions as images. For example, [15] designed a context encoder that understands the entire content of the image and proposes reasonable hypotheses for the missing parts. [16] proposed a feed-forward generative network that utilizes a novel contextual attention layer to exploit long-range spatial correlations. However, since these methods were designed for image restoration, their application to spatiotemporal data completion is less effective due to the lack of consideration for the unique characteristics of spatiotemporal data.\nIn recent years, some algorithms designed for spatiotemporal data completion have emerged. [17] proposed a Spatial Aggregation and Temporal Convolution Network that uses various aggregation functions to leverage spatial correlations and temporal convolution to capture temporal dependencies. [18] fused multiple sources of data and used tensor decomposition for fine-grained air quality inference. [19] considered the low-rank nature of spatiotemporal data matrices, combining matrix factorization with neural networks to propose Deep Matrix Factorization (DMF) for inferring complete data. Based on Transformer, [20] proposed a novel framework for spatiotemporal data completion and prediction. However, these methods perform spatiotemporal data completion at the same granularity, without considering the transformation relationships between different granularities of data."}, {"title": "B. Spatiotemporal Data Super-Resolution", "content": "he complexity and accuracy requirements of real-world tasks increase, the demand for fine-grained spatiotemporal data also grows, making spatiotemporal data super-resolution a feasible solution. It aims to infer fine-grained spatiotemporal data distribution from coarse-grained spatiotemporal data. In the early stages, methods for image super-resolution were applied to spatiotemporal super-resolution tasks [6]. For example, SRCNN [21] was the first to combine convolutional neural networks with bicubic interpolation methods to achieve an end-to-end super-resolution algorithm. SRResNet [22] adopted deep convolutional networks and residual learning strategies to achieve better super-resolution results. However, these algorithms were designed for image super-resolution rather than spatiotemporal data, failing to consider the unique characteristics of spatiotemporal data, such as spatiotemporal relationships and external factors.\nUrbanFM [9] designed an external factor fusion network to extract external features, such as weather and temperature, combining them with the inference network to achieve better results in spatiotemporal super-resolution tasks compared to traditional image super-resolution methods. Building on UrbanFM, UrbanPy [7] designed a pyramid architecture with multiple components, further improving the performance. FODE [11] extends neural ODEs by introducing an affine coupling layer, allowing for more accurate and efficient spatial correlation estimation. STCF [23] designed two spatiotemporal contrastive pre-training networks and a fine-tuning network that integrates learned features, achieving good inference results while mitigating the overfitting problem of historical methods. However, these methods often assume that the collected coarse-grained spatiotemporal data is complete.\nConsidering that the spatiotemporal data collected in real-world scenarios is often incomplete, [24] designed a multi-task learning model called MT-CSR, which simultaneously considers data completion and super-resolution. [25] further addressed the impact of noise and data sparsity, but each time slice (with incomplete coarse-grained data) requires a large amount of complete fine-grained data from the past for assistance, making it difficult to apply in real-world scenarios. [13] designed a two-stage framework with spatiotemporal attention learning, improving space-time representations, but it can only handle cases where the missing positions are fixed, ignoring the diversity of sparse patterns in real-world scenarios. Therefore, we aim to propose a generalized model for inferring complete fine-grained spatiotemporal data from incomplete coarse-grained data to handle various situations."}, {"title": "C. Denoising Diffusion Probabilistic Model", "content": "In recent years, diffusion models have become the most popular generative models, especially excelling in image generation tasks in computer vision [26], [27]. Beyond that, diffusion models have also shown significant potential in various fields such as natural language processing [28], [29], multimodal modeling [30], [31], time series data modeling [32], [33], reinforcement learning [34], [35], and robust learning [36], [37]. In this paper, we adopt diffusion models as our foundational framework, complemented by two auxiliary modules, to achieve fine-grained spatiotemporal data inference."}, {"title": "III. DEFINITIONS AND PROBLEM FORMULATION", "content": "Definition 1. Value of Regions: Based on geographic location information (longitude and latitude), we divide a region into an $I \\times J$ grid map. Let all sub-regions be denoted as $R = {r_{1,1}, r_{1,2},...,r_{m,n},...,r_{I,J}}$, where $r_{m,n}$ represents the sub-region in the m-th row and n-th column of the grid map. The average of all values uploaded by users within each sub-region is taken as the overall value of the region.\nDefinition 2. Coarse-grained and Fine-grained Spatiotemporal Data Distribution Map: Given a magnification factor N, the coarse-grained grid map divides the geographic area into $I \\times J$ sub-regions, while the fine-grained grid map divides the area into $NI \\times NJ$ sub-regions. Based on the concept of region value in Definition 1, we obtain the values of each coarse and fine-grained sub-region under the two divisions, forming coarse and fine-grained spatiotemporal data distribution maps. We denote the real data of the sub-region in the i-th row and j-th column as $X_{i,j}$, the complete coarse-grained spatiotemporal distribution map as $X_{comp,cg} = {X_{1,1}, ..., X_{i,j}, ..., X_{I,J}}$, and the complete fine-grained spatiotemporal distribution map as $X_{comp,fg} = {X_{1,1}, ..., X_{i,j}, ..., X_{NI,NJ}}$, where comp represents complete, cg represents coarse-grained, and fg represents fine-grained.\nDefinition 3. Incomplete Coarse-grained Spatiotemporal Data Distribution Map: Due to objective factors such as transmission errors in user-uploaded data, some users' mobile devices only being able to collect coarse-grained data, and subjective reasons such as reducing the number of recruited users to lower costs, the obtained coarse-grained data distribution map is often incomplete. We use a binary variable $C_{i,j}$ to record whether data has been collected for a sub-region: if so, then $C_{i,j} = 1$; otherwise, $C_{i,j} = 0$. We obtain $X'_{i,j} = X_{i,j} \\times C_{i,j}$ and denote $C = {C_{1,1},..., C_{i,j}, ..., C_{I, J}}$.\nWe denote $X_{inc,cg} = {X'_{1,1}, ..., X'_{i,j}, ..., X'_{1,5}}$ as the collected coarse-grained sensing data, where inc indicates that the collected data is incomplete. Therefore, We have\n$X_{inc,cg} = X_{comp,cg} \\odot C$,\nwhere $\\odot$ represents the Hadamard product. Note that $C_{i,j}$ can remain constant or vary across all timestamps, and the 0/1 values may be distributed evenly or unevenly within each $C_{i,j}$. We refer to these different configurations of C as 'sparse patterns,' as they reflect the varying degrees of data sparsity.\nProblem: Given a magnification factor N and a set of incomplete coarse-grained spatiotemporal data distribution maps ${X^1_{inc.cg}, X^2_{inc,cg},..., X^t_{inc.cg}} \\in R^{I\\times J}$, our goal is to infer the complete fine-grained data distribution maps ${X^1_{comp,fg}, X^2_{comp.fg}, X^t_{comp.fg}, ..., X^t_{comp,fg}} \\in[][R^{NI \\times NJ}$. It is worthy noting that the sparse pattern can be arbitrary in the task setting."}, {"title": "IV. METHODOLOGY", "content": "In this section, we detail our model which consists of two main components named Diffusion-C and Diffusion-F for coarse-grained completion task and fine-grained inference task respectively. We employ the DDPM as the framework for both Diffusion-C and Diffusion-F to leverage its capacity to understand data distributions. Given the incomplete coarse-grained input $X_{inc,eg}$ and history series ${X_{inc,cg}}_{i=0~t-1}$, We first do data completion with Diffusion-C by extracting the spatial correlations within $X_{inc,cg}$ and the relations between ${X_{inc,cg}}_{i=0~t}$. After we obtain the complete coarse-grained results $X_{comp,cg}$ from the first stage, we use Diffusion-F to infer fine-grained data considering the spatial correlations in $X_{comp,cg}$ and the temporal patterns within the temporal series ${X_{comp,cg}}_{i=0~t}$. Diffusion-C and Diffusion-F are trained jointly to provide end-to-end inference.\nGenerally, DDPM learns the underlying distribution of given data by artificially adding noise and training a predictor to remove the noise. In our model, the predictor in each stage should utilize the spatiotemporal characteristics of the known data and fully leverage external domain knowledge to help the predictor accurately forecast noise. To achieve this goal, we design sub-task specialized encoders for each stage and utilize a decoder to predict the final noise and iteratively recover the reconstructed results from noise. We also incorporate external factors, such as weekly indices and holiday information, into the predictor through the use of embedding. For the two stages, due to the difference in input-output data and tasks, we utilize distinct encoding structures to tailor each stage to its specific task. We observe that the granularity remain consistent in the first stage, which allows us to calculate the similarity between the data points of each historical moment, providing clues for completing the missing values at the current time. This is achieved by utilizing self attention mechanism in the encoder called ST-PointFormer at the first stage. Conversely, data granularity changes at the second stage, yet there are no missing values in the input data, making it convenient to extract temporal patterns such as periodicity from the sequence. Therefore, inspired by [38], we use a powerful TimesBlock as a feature extractor in the temporal encoder at the second stage to focus on associated temporal patterns.\nIn Section IV-B, we will present the framework of DDPM and the overall structures of our predictors. In section IV-C and section IV-D, we will introduce the predictor structure of the Diffusion-C and Diffusion-F in technical detail respectively, followed by the training strategy of our two-stage models which will be introduced in Section IV-E."}, {"title": "B. Conditional Diffusion", "content": "The Denoising Diffusion Probabilistic Model (DDPM) is an emerging generative model that learns the probability distribution of data through a process of adding noise and denoising, as shown in Figure 3. It demonstrates impressive data generalization capabilities and noise resistance, while maintaining a stable training process. Based on that, Conditional DDPM further constrains the generated process by using conditional guidance and allows us to acquire desired outcomes based on our conditions. In both stages we utilize the conditional DDPM as the framework and work on improving its denoising process by designing task-specialized noise predictor.\n1) Forward Diffusion Process: Intuitively, the forward diffusion process involves gradually adding Gaussian noise to a sample until the data becomes random noise. For data $so$"}, {"title": "3) Noise Predictor Network:", "content": "As previously mentioned, the key to the model's function lies in how to design a noise predictor for each phase. U-Net is an effective approach for learning spatial features from a given spatial map. It has an encoder with multiple parameterized down sampling layers to extract spatial features and a decoder with up sampling layers to project these features to the predicted noise.\n$S = encoder(X_{input})$,\n$\\hat{\\epsilon} = decoder(S)$,\n$\\tilde{X} = X_{\\epsilon}$.\nHowever, U-Net focuses on correlations within individual maps but overlooks the rich relationships across maps in adjacent history observations. We therefore aim to incorporate another network to handle the spatiotemporal information within the sequences. Specifically, we extract intra-map features from the current spatial map through the encoder of U-NET and inter-map features through another specialized encoder. These two features are then fused and fed into a decoder to predict the final noise. In each model, we have:\n$S_{intra} = intra - encoder(X_{input})$,\n$S_{inter} = inter - encoder(X_{I_{input}})$\n$\\hat{\\epsilon} = decoder(S_{intra} \\oplus S_{inter})$.\nMoreover, since we utilize two diffusion models to handle coarse-grained completion and fine-grained inference, the noise predictor for the two stages should be customized for the task-specialized characteristics. We will introduce the encoder in two stages in the following subsections."}, {"title": "C. ST-PointFormer", "content": "We observe that at this stage, the granularity of data remains consistent, and history data can provide direct help for completing missing data. For example, if there are recent observations at or near the locations where current data is missing, the historical observed values will be very close to the missing values we aim to infer. Therefore, we want to learn the complicated relationships between each spatiotemporal point (ST-point) in ${X_{comp.cg}}_{i=0~t}$ and utilize this information to help complete missing map $X_{comp,cg}$.\nInspired by [20], which brings attention mechanism as a powerful tool into spatiotemporal data processing, we propose the ST-Pointformer to encode history information by computing the relations between ST-points of history sequences. The structure of ST-PointFormer is shown in Figure 4. We suppose that the relationships between ST-points are mainly decided by the observed value, the relative spatiotemporal position, and the external characteristics. Therefore, we first use three types of embedding layers to embed entering data and external knowledge into the model.\nWe utilize the value embedding layer to project the data into $d_{model}$-dimension vector $X_{value} \\in R^{t\\times I\\times J\\times d_{model}}$ through a fully connected layer. We also use a learnable position embedding layer to provide relative spatial and temporal relationships, denoted as $X_{pos} \\in R^{t\\times I\\times J\\times d_{model}}$. Another fully-connected layer is designed to learn from external features like daily, weekly, monthly indices and holiday information,which are embedded to $X_{external} \\in R^{t\\times I\\times J\\times d_{model}}$. These embedding values will then be added together to enter the encoder.\n$X_{feed,0} = E(X) = X_{value} + X_{pos} + X_{external}$.\nAfter the observed data sequences are embedded to $X_{feed,0}$, we use multi-head self-attention to compute the associations between ST-points, thus enabling further encoding for each ST-point. Self-attention is a particular implementation of attention mechanism where the query vector, key vector, and value vector are projected from the same data, and the multi-head attention helps the model capture richer information within the ST-point sequences. When the ST-point sequences $X_{embed}$ reach the attention layer, due to our multiple-head attention designed to capture spatiotemporal relationships of different possible patterns, for each attention head $head_i$, we use three learnable weight matrices $W^Q_i$, $W^K_i$, and $W^V_i$ to project the embedded data to form three vectors, including the query vector $Q_i$, key vector $K_i$, and value vector $V_i$.\n$Q_i = X_{embed}W^Q_i$, $K_i = X_{embed}W^K_i$, $V_i = X_{embed}W^V_i$.\nAfter that, we use the formula of Scaled Dot-Product Attention to calculate the attention scores $S_i$.\n$S_i = softmax(\\frac{Q_iK_i}{\\sqrt{d_k}})$.\nThe multiplication of $Q_i$ and $K_i$ indicates the similarity of them, i.e., the similarity of the spatiotemporal correlations between ST-points. The softmax function normalizes this similarity so that we make the attention scores between 0 and 1, where $\\sqrt{d_k}$ prevents the gradient instability. Finally, we multiply the attention score matrix with $V_i$ to obtain the new ST-point representation which combines the information of all other ST-points considering their relationships.\n$A(Q_i, K_i, V_i) = S_iV_i$.\nOnce the outputs of all attention heads have been computed, they are concatenated into a single matrix and projected through a learnable weight matrix $W_o$ to reduce its dimension.\n$MA(Q, K, V) = Concat(h_1, h_2,..., h_k)W_o$,\n$h_i = A(Q_i, K_i, V_i)$.\nThe encoder can have multiple attention layers inside. For each attention layer, it is passed through the self-attention layer to calculate the attention and perform residual connection and normalization, and then enters the feedforward network layer to improve the fitting ability by linear transformation.\n$X_{feed,0} = E(X)$,\n$X_{attn,i} = LN(MA(X_{feed,i}) + ADD)$,\n$X_{feed,i+1} = ReLu(ReLu(X_{attn,i}W_{1,i})W_{2,i})$,\n$X_{out} = ReLu(ReLu(X_{attn,kW_{1,k}})W_{2,k})$,\nwhere LN(\u00b7), MA(\u00b7), and ADD represent the LayerNorm, the multi-attention layer, and residual connection. The RELU is the activation function, and $W_1$, $W_2$ are the learnable weights.\nThus far, we have encoded each ST-point, allowing these ST-points to fully incorporate information from other relevant ST-points. The following step is to align these encoded values with the U-NET encoding results to perform the final inference. Typically, the output of U-NET $Z_{spatial} \\in R^{I_{down}\\times J_{down}\\times d_{model}}$ is a downsampled spatial map with the length of $I_{down}$, width of $J_{down}$ and $d_{model}$ channels. At the same time, we have $I \\times J \\times t$ ST-points, each with a dimension of $d_{model}$. Since in $Z_{intra}$ each position represents the extracted spatial semantic information of a subregion of $\\frac{IxJ}{I_{down} x J_{down}}$ in the original spatial map, we stack encoded representation of ST-points within each subregion to maintain semantic consistency with the U-NET results. Specifically, for the top-left subregion, we have:\n$S^{[0,0]}_{inter} = Concat(X^{attn, K}_{0,0}, X^{attn, K}_{1,0},..., X^{attn, K}_{I_{down}-1,0}$,\n$X^{attn, K}_{0,J_{down}-1},X^{attn, K}_{1,J_{down}-1},..., X^{attn, K}_{I_{down}-1,J_{down}-1})$.\nIn this way, we obtain the final encoded results.\n$S_{inter} = Stack(X_{attn,K})$.\nIt is worth noting that, unlike traditional convolutional methods, the attention mechanism does not rely on neighborhood properties to capture relationships between data values. Instead, it computes attention scores between any two spatiotemporal location in the data map, making our method more resilient to different sparse patterns in observed data. Whether missing data locations shift over time or are unevenly distributed spatially, the performance remains stable. In contrast,"}, {"title": "D. T-PatternNet", "content": "Temporal patterns, such as periodicity and trends, are essential in spatiotemporal data processing and can significantly aid in data reconstruction. However, capturing these patterns in sparse is challenging, as missing values can disrupt temporal inferences. To address this, we defer the use of temporal patterns until the fine-grained inference stage, where the missing data has been approximated at a coarser level. At this point, our goal is to extract temporal patterns from the now-complete coarse-grained data and leverage them as key features to enhance fine-grained inference. Inspired by [38], we utilize T-PatternNet to analyze the completed results of history data ${X_{comp,cg}}_{i=0~t}$ as a 1-D sequential series with $I \\times J$ channels and a length of t.\nAs illustrated in Figure 5, T-PatternNet consists of multiple TimesBlocks that analyze two types of temporal variations: variations between adjacent areas and those with the same phase across different periods, referred to as intraperiod- and interperiod- variations. To facilitate this, the 1-D temporal series is transformed into 2-D data after identifying its intrinsic periodicity using Fast Fourier Transformer (FFT). Following this, an inception block with multi-scale 2-D kernels aggregates the intraperiod variations (columns) and interperiod variations (rows) simultaneously. The aggregated representation is then fused, with their importance determined by the FFT results.\nSpecifically, with the complete coarse-grained results from the coarse-grained completion stage, the input to T-PatternNet is $X \\in R^{t\\times(I\\times J)}$. We first embed X into deep features using a linear embedding layer:\n$X_{1D} = E(X) = X \\times W_{1D}$,\nwhere $X_{1D} \\in R^{t\\times d_{model}}$ and $W_{1D} \\in R^{(I\\times J)\\times d_{model}}$. Then we transform the embedded data into the frequency domain by FFT to analyze its inner periodicity.\n$\\mathcal{A} = Avg(Amp(FFT(X_{1D})))$,\n${f_1,..., f_k} = \\arg Topk (\\mathcal{A})$,\n$p_i = \\frac{T}{f_i}, i \\in {1, ..., k}$.\nHere, FFT(\u00b7) and Amp(\u00b7) denote the FFT and the calculation of amplitude values. $\\mathcal{A} \\in R^{t}$ represents the averaged amplitude of each frequency. As [38], we only select top-k amplitudes to avoid noise, where k is the hyper-parameter, and the selected frequencies correspond to the preferred periodicity. For each selected periodicity $p_i$, we fold the 1-D series $X_{1D}$ to form the 2-D tensors with $p_i$ rows and $f_i$ columns.\n$X_{2D} = Reshape_{p_i,f_i}(Padding(X_{1D}))$.\nHere, the Padding(\u00b7) represents extending series by zeros to make it compatible for $Reshape_{p_i,f_i}$.\nAfter we obtain the 2-D tensors, the next step is to extract the intraperiod- and interperiod-variations. This is done by an inception block, namely Inception(\u00b7), which contains multi-scale 2-D kernels and is pervasively used in computer visions as a powerful feature extractor. After the 2-D tensors pass through the inception block, we use another Trunc(.) to truncate the padded series into the original length n.\n$X^{v,l_i}_{2D} = Inception(X^{l_i}_{2D}), i \\in {1,...,k}$,\n$X^{l_i}_{1D} = Trunc(Reshape_{1,(p_ixf_i)}(X^{v,l_i}_{2D})), i \\in {1, ..., k}$.\nFinally, we fuse the k different 1-D representations according to their relative importance, which is decided by the amplitudes calculated at the FFT stage. The fused representation from the (l-1) \u2013 thTimesBlock will be sent to the next l- th TimesBlock for deeper feature extraction.\n$\\tilde{A_1},..., \\tilde{A_k} = softmax(\\mathcal{A_1},..., \\mathcal{A_k})$,\n$X^l_{ID} = \\Sigma_{i=1}^k\\tilde{A_i} \\odot X^{l_i}_{ID}$.\nAs in the stacking process in ST-PointFormer, we obtain the final encoded results by stacking the $X^l_{ID}$ representation of all the spatial locations."}, {"title": "E. Training Strategy", "content": "In DiffRecon, the input consists of sparse coarse-grained data, while the ground truth is the complete fine-grained data. We begin by pre-training each stage of the model separately to ensure that each component is optimized for its specific role in the reconstruction process. During this pre-training phase, we downsample the fine-grained data to create a pseudo-complete coarse-grained version, which serves as the ground truth for Diffusion-C and is used as the input for Diffusion-F. This strategy allows both stages to learn how to handle the incomplete data independently. Once pre-training is complete, we proceed with joint training, where both stages are integrated into a single end-to-end framework. This joint training phase refines the model's ability to seamlessly connect the stages and perform more accurate reconstructions of fine-grained data from sparse coarse-grained inputs."}, {"title": "V. EXPERIMENTS", "content": "Experiment Setting\n1) Dataset: We use three datasets to validate the effectiveness of the proposed model: TaxiBJ", "datasets": "nTaxiBJ [39", "2": 1, "1.\nBikeNYC\u00b9": "This dataset includes millions of bike trip records in New York from 1/1/2016 to 4/30/2016. Each record contains various information such as Start Time"}, {"2": 1, "40": [42]}, {"2": 1, "features": "The external features used include weather", "Preprocessing": "To test our model's performance in various real-world scenarios with missing data", "patterns": "nIn traditional scenarios using fixed sensors", "Metrics": "We use Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) to evaluate the performance of our model. The formulas are as"}]}