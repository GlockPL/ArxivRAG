{"title": "Can LLMs Reliably Simulate Human Learner Actions? A Simulation Authoring Framework for Open-Ended Learning Environments", "authors": ["Amogh Mannekote", "Adam Davies", "Jina Kang", "Kristy Elizabeth Boyer"], "abstract": "Simulating learner actions helps stress-test open-ended interactive learning environments and prototype new adaptations before deployment. While recent studies show the promise of using large language models (LLMs) for simulating human behavior, such approaches have not gone beyond rudimentary proof-of-concept stages due to key limitations. First, LLMs are highly sensitive to minor prompt variations, raising doubts about their ability to generalize to new scenarios without extensive prompt engineering. Moreover, apparently successful outcomes can often be unreliable, either because domain experts unintentionally guide LLMs to produce expected results, leading to self-fulfilling prophecies; or because the LLM has encountered highly similar scenarios in its training data, meaning that models may not be simulating behavior so much as regurgitating memorized content. To address these challenges, we propose HYP-MIX, a simulation authoring framework that allows experts to develop and evaluate simulations by combining testable hypotheses about learner behavior. Testing this framework in a physics learning environment, we found that GPT-4 Turbo maintains calibrated behavior even as the underlying learner model changes, providing the first evidence that LLMs can be used to simulate realistic behaviors in open-ended interactive learning environments, a necessary prerequisite for useful LLM behavioral simulation.", "sections": [{"title": "Introduction", "content": "Open-ended interactive learning environments offer unique educational value by providing tailored and dynamic spaces where learners can explore, experiment, and construct knowledge-capabilities. However, developing these environments is challenging. It requires not only the creation of pedagogical content but also mechanisms to adapt learning experiences for learners with varying knowledge levels and psychological characteristics for very large state spaces due to the relatively open-ended nature of the environments. This complexity necessitates an iterative process in which theoretical best practices are continuously balanced with practical demands."}, {"title": "Related Work", "content": "Simulated Learners streamline the authoring of intelligent tutoring systems (ITSs), which often require over 100 hours of work per instructional hour. Tools simulate learner behavior to aid in ITS development via interactive tutoring. However, compared to ITSs, open-ended interactive learning environments typically involve more states due to their open-ended and exploratory nature and a greater emphasis on scaffolding the affective aspects of learning. While Christensen simulate psychological aspects of learners, their method is handcrafted, highly context-specific, and therefore, would not scale well to complex interactive environments. To our knowledge, our work is the first to apply learner behavior simulations to these environments. Additionally,  identify a widespread lack of validation in simulated learner research, which we address in the HYP-MIX framework by centering on falsifiable hypotheses for both authoring and evaluation. Our approach is also in line with Ainsworth and Grimshaw, who focus on group-level behavior specification, similar to our use of distributional hypotheses."}, {"title": "Simulating Human Behavior with LLMs", "content": "Several recent works explore the ability of LLMs to simulate human behaviors across various contexts, including social platform design , market research , and experimental economics . LLMs have also been shown to reflect human-like"}, {"title": "Prompt Sensitivity and Prompt Calibration", "content": "Experiments using LLMs rely heavily on natural language prompts to define personas, situations, and tasks, but LLMs are highly sensitive to slight variations in prompt text, making this a critical issue for research . Loya, Sinha, and Futrell find that ChatGPT exhibits sensitivity to prompt phrasing for decision-making tasks such as ours. In response, various prompt calibration approaches have emerged, particularly focusing on reducing the LLMs' sensitivity to the order of in-context examples. In contrast to this family of work that focuses on reducing variance between different templates, in this work we our goal is to test the consistency of LLM behaviors across different simulation contexts."}, {"title": "The HYP-MIX Framework", "content": "The HYP-MIX framework is designed to create and evaluate realistic and scalable simulations of learner behavior by translating theoretical constructs into concrete, testable predictions. The unit of authoring and evaluation in this framework is a Marginalized Distributional Hypotheses (MD-Hyp). These are called \u201cmarginal\" because they focus on one learner characteristic at a time, while \u201cmarginalizing\" over all other variables. This is essential because, while it is straightforward to reason about a single characteristic, jointly considering multiple characteristics can quickly become difficult. For instance, an MDHyp might predict that low persistence leads to a higher probability of task-abandonment, focusing specifically on persistence while accounting for other variables in the background. The rest of this section details the motivation and implementation of MDHyps, along with its integration with LLM prompting."}, {"title": "MDHyps for Simulation Evaluation", "content": "A common method for validating simulated agents involves presenting the generated behaviors to human crowdworkers, who then rate the realism of these behaviors either over a quantitative scale or according to a qualitative rubric. While this approach has been widely adopted in recent studies, particularly with the proliferation of crowdsourcing platforms, it is fundamentally limited and ill-suited for evaluating simulated learner actions in complex, iterative experiments, for several reasons:\n1. Cost Constraints: Crowdsourcing becomes prohibitively expensive in iterative studies, particularly those requiring extensive experimentation.\n2. State Space Explosion: As the complexity of the environment and the number of learner characteristics increase, the task of collecting annotations for every possible combination becomes infeasible.\n3. Demographic Mismatch: The typical crowdworker populace does not include individuals deeply involved in education, such as researchers or educators. As a result, they are generally not equipped to accurately assess the realism of behaviors exhibited by young learners with specific characteristics.\n4. Inherent Noise in Learners' Actions: The stochastic nature of interactions within learning environments introduces significant noise into the evaluation process. Even with a sophisticated model of a learner, it is nearly impossible to predict with certainty how they will behave in a given situation, making deterministic point estimates unreliable.\nWe propose using MDHyps to evaluate learner behavior simulations at a distributional level, drawing from prior studies or instructor experience. An MDHyp is a natural language statement that describes a relationship between a learner's characteristic and their probability of taking certain actions (e.g., \u201ca more persistent learner is less likely to abandon the task as more time passes\u201d). This relationship can be tested by analyzing the distribution of outcomes from multiple simulation runs across different environment states."}, {"title": "MDHyps for Simulation Authoring", "content": "The central thesis of the HYP-MIX framework is that MDHyps serve not only as useful tools for evaluating an existing simulation, but also as powerful building blocks for expert-authoring LLM-based simulations of learner behavior."}, {"title": "Achieving Mix-and-Match Simulation Authoring with MDHyps", "content": "For MDHyps to be effective in prompt-based simulation authoring, the LLM must demonstrate compositional generalization. We need MDHyps to function as modular elements that can be easily added, edited, removed, swapped, and combined to shape the LLM's outputs. Similar to SKILL-MIX, which tests LLMs' ability to combine literary and logical devices to generate free-form text, HYP-MIX tests LLMs' ability to combine calibrated expert-hypotheses to simulate learner actions in a \"calibrate once, use forever\" fashion. Achieving this, of course, is challenging due to LLMs' sensitivity to prompt phrasing and requires empirical validation."}, {"title": "Existing Notions of \u201cCalibration\u201d", "content": "The term \"calibration\" carries different definitions across disciplines. In statistics and machine learning, calibration refers to the alignment between a model's predicted probabilities and the actual observed frequencies of outcomes, ensuring that predictions accurately reflect real-world occurrences over time. In the context of physical measurement devices, calibration ensures that a measurement device's accuracy is consistent. This process involves aligning the device with a known standard to maintain reliable accuracy across future measurements. The HYP-MIX notion of calibration combines the two: we want the"}, {"title": "Experiments", "content": "We test the robustness and generalization capabilities of GPT-4 Turbo, a state-of-the-art LLM, in the HoloOrbits environment by assessing how well it maintains calibration when the learner model is modified."}, {"title": "Learning Environment", "content": "We situate our experiments within HoloOrbits , an open-ended interactive learning environment designed for teaching Kepler's Laws that we use for our experiments. We selected HoloOrbits due to its small, well-defined state and action spaces, which make it ideal for preliminary experiments. Since our experiments involve a text-based LLM, we only need a textual description of the learning environment to feed into the model as a natural language prompt. An unintended advantage of this approach is that it allows us to describe learning environments not yet implemented in software."}, {"title": "Learning Task", "content": "We particularly focus on the learner's task to verify if a given planetary system adheres to Kepler's First Law by submitting three equal arithmetic expressions. These expressions can use any combination of distance measurements between the following points: aphelion (A), perihelion (P), focus 1 (F1), focus 2 (F2), and a fixed point on the orbit (X). The correct solution involves submitting the following measurements: $(F1-A + F2-A)$, $(F1-P + F2-P)$, and $(F1-X + F2-X)$."}, {"title": "State Representation", "content": "For our experiments, we define a minimal state representation with ten boolean variables indicating whether the learner has measured the distances between each pair of points. Additionally, we include two integer variables to track the number of submission attempts and the time elapsed since the session began, respectively."}, {"title": "Action Space", "content": "The learner can perform measurements between any pairs of key points in the planetary system, with specific actions such as MEASURE-F1-X to measure the distance between Focus 1 (F1) and a fixed point on the orbit (X), MEASURE-A-F1 to measure the distance between Aphelion (A) and Focus 1 (F1), or MEASURE-A-P to measure the distance between Aphelion (A) and Perihelion (P). In addition to these measurement actions, the learner can submit solutions using SUBMIT(X, Y, Z), where X, Y, and Z represent arithmetic expressions involving the measured distances. The goal is for all three expressions to be equal. The learner also has the option to EXIT at any time."}, {"title": "Learner Model", "content": "We represent each learner through a learner model $L = (C, V, M)$. C is the set of learner characteristics (e.g., geometry proficiency, persistence) being modeled. V is the mapping between each learner characteristic, $C_i \\in C$, to its corresponding persona level, $V_i$, of the current learner. Each $V_i \\in V$ is quantified on a numerical scale ($V_i \\in [1, 10]$). Finally, each learner characteristic $C_i$ is associated with a learner characteristic model $M_i \\in M$, which, in turn, comprises one or more MDHyps."}, {"title": "Learner Characteristics", "content": "For the HoloOrbits learning environment, we model learners using persistence (a psychological factor) and geometry proficiency (which reflects the learner's knowledge of the subject matter) with the following operating theoretical definitions:\n\u2022 Persistence: \"maintaining a sustained effort toward completion of a goal-directed task despite challenges or difficulties\"\n\u2022 Geometry Proficiency: \u201cthe ability to apply the knowledge of the properties of common shapes to solve problems\""}, {"title": "Design of the LLM Prompt for the Simulation", "content": "The simulation prompt fed to the LLM, $\\hat{I}_{sim}$, consists of introductory instructions, a description of the learning environment, current state, and the learner model (a graphic of the prompt template is shown in the Appendix). Furthermore, the prompt template for each learner characteristic model is a concatenation of prompt templates of the MDHyps that make up the learner characteristic model. We also instruct the LLM to perform Chain-of-Thought reasoning before outputting the simulated action to strengthen the reasoning and provide the practitioners with a semblance of the intermediary steps used to arrive at the output, which can then be used to refine the simulation."}, {"title": "Approximate Marginalization", "content": "Testing an MDHyp by running the simulation over all value-assignments of state variables S requires an intractable number of LLM calls that grows exponentially with S. To address this, we statistically approximate the state space by subsampling it. This approach allows for manageable marginalization while controlling computational costs."}, {"title": "Learner Model Edit Graph: A Case Study", "content": "This section details a case study of how we developed a simulation of learner actions for the HoloOrbits environment leveraging the HYP-MIX framework. The goal of this case study is to demonstrate how HYP-MIX can be used to evaluate the compositional generalization capabilities of an LLM (we used GPT-4 Turbo in our experiments). We focus on five representative types of modifications (edit operations) to the learner model, reflecting the iterative process a developer might follow when constructing a learner model. Throughout the development process, we use the four MDHyps listed in Table 1. These modifications are represented via a Learner Model Edit Graph (shown in Figure 3).\n1. Initial Hypotheses and Operationalization: We initialize the learner model with two hypotheses: $H_{G1}$ and $H_{P1}$, both obtained by operationalizing the theoretical definitions of geometry proficiency and persistence respectively into MDHyps (see Table 1 for all hypotheses used). Both $H_{G1}$ and $H_{P1}$ posit monotonic relationships between variables. We grouped them under the hypothesis class $H_{mono}$. We calibrated $\\hat{I}_{mono}$ using $H_{G1}$ as the calibration reference hypothesis and tested for generalization on $H_{P1}$. We define the success criteria function for monotonic hypotheses, $T_{mono}$ using the Spearman correlation coefficient $\\rho$ and its corresponding p-value $P_p$ as follows:\n$T_{mono}(\\rho, P_p) = \\begin{cases} TRUE, & \\text{if } \\rho > 0 \\text{ and } P_p < 0.05 \\text{for a monotonically increasing hypothesis} \\\\ TRUE, & \\text{if } \\rho < 0 \\text{ and } P_p \\leq 0.05 \\text{for a monotonically decreasing hypothesis} \\\\ FALSE, & \\text{otherwise} \\end{cases}$"}, {"title": "2. Variable Swap", "content": "After consulting with learning science experts, we determined that the \"Number of Submissions\" was a more suitable measure of \"challenge\" than \"Number of Minutes Elapsed.\" This led to a modification of the original MDHyp for persistence, resulting in a new hypothesis, $H_{P2}$."}, {"title": "3. Append", "content": "During testing, we observed that learners with minimal Geometry Proficiency (1/10) were unexpectedly producing a high percentage (~80%) of productive measurements, contrary to our expectation of a uniform distribution. To address this, we introduced $H_{G2}$ and a new hypothesis class, $H_{uniform}$, to explicitly model this behavior and refine the Geometry Proficiency model, better align it with our theoretical expectations. The success criteria function $T_{uniform}$ for the uniform distribution hypothesis is defined using the p-value of a Chi-squared test $P_{\\chi^2}$ as follows:\n$T_{uniform}(P_{\\chi^2}) = \\text{TRUE if } P_{\\chi^2} > 0.05 \\text{ else FALSE}$"}, {"title": "4. Combine Hypotheses", "content": "After calibrating the prompt templates for $H_{G2}$ and $H_{P2}$, we combined these MD-Hyps into a unified learner model, completing the development process."}, {"title": "Results and Discussion", "content": "The criterion for evaluating whether an LLM can support flexible simulation authoring within the HYP-MIX framework is that the calibration state of all hypothesis classes must remain intact after a learner model edit operation. Specifically, for each hypothesis $H_i$ in the learner model, the LLM outputs must continue to satisfy the success criterion function"}, {"title": "Limitations and Future Work", "content": "Our study focuses on two learner characteristics\u2014geometry proficiency and persistence\u2014within a single learning environment. While this scope is limited, it allows us to generate targeted insights and refine our methods, laying the groundwork for future expansion. Complex, non-linear interactions likely exist that are challenging to calibrate without real learner data. Future work should build on this foundation by exploring a broader range of environments and more intricate relationships among learner characteristics while assessing the robustness of LLMs to changes in both the learner model and the environment. Although MDHyps offer scalable assessments, the real value of learner action simulations lies in their use for downstream tasks and expert evaluation. These simulations can predict learner actions, serving as effective copilots in designing learning environments. Future research should analyze these predictions to identify biases and failure modes, informing improvements in learning environment designs. Other avenues for future empirical work with the HYP-MIX framework include experimenting with alternative LLMs (including open-source ones) and learning environments with continuous action spaces (including natural language input)."}, {"title": "Conclusion", "content": "This study introduced the HYP-MIX framework using Marginal Distributional Hypotheses (MDHyps) to simulate learner actions in open-ended interactive learning environments, addressing the challenges of costly and time-consuming real-world testing and evaluating whether current LLMs are capable of maintaining calibration under changes to the simulation environment. We demonstrated that GPT-4 Turbo can maintain calibration across various learner model modifications, reducing the need for frequent recalibration and highlighting the potential of LLMs for behavioral simulation. Our key contribution is a scalable method for leveraging LLMs to enhance the adaptability of open-ended interactive learning environments and test their generalization across different contexts."}, {"title": "Best Practices for Hypothesis Class Induction", "content": "The process of calibrating a template for a hypothesis class may necessitate the addition of new fields to the instruction template by the prompt engineer. A key design principle is to ensure that each field is sufficiently flexible to accommodate a wide range of hypotheses. This flexibility not only supports the adaptability of the framework but also maximizes its utility across different research contexts.\nAnother important design principle is to maintain parsimony in the number of hypothesis classes. While the in-troduction of new hypothesis classes can enhance the specificity of the framework, each new class introduces an initial calibration overhead. Therefore, it is essential to balance the benefits of adding new classes with the associated costs in terms of time and resources.\nWe also posit that a balance should be struck between allowing the LLM's commonsense reasoning to guide responses and adhering to expert-defined hypotheses. Striking this balance is crucial for ensuring that the LLM's outputs are both grounded in expert knowledge and adaptable to new and unexpected scenarios. By carefully managing this interplay, researchers can optimize the performance of LLMs within our framework, leading to more reliable and insightful results."}, {"title": "MDHyps as Learner Model Update Rules in Disguise", "content": "The process of developing a learner model is directly analogous to defining an MDHyp. Learner modeling is the process of creating (and iteratively refining) a representation of a learner's knowledge, skills, abilities, and preferences to tailor educational experiences and improve learning outcomes. A fundamental component of any learner model is what is known as the \"update rule,\" a mechanism that adjusts the learner model in response to the learner's actions or events within the learning environment. Almost all update rules can be formulated as follows:\n\"If the learner performs ACTION under CONDITION(S), then [INCREASE | DECREASE] the estimate of CHARACTERISTIC by VALUE.\"\nWe make the key observation that such an update rule may be transformed into an equivalent MDHyp that takes the following form:\n\"Learners with a [HIGH | LOW] <CHARACTERISTIC> are more likely to perform  when  than learners with a [LOW | HIGH] .\""}]}