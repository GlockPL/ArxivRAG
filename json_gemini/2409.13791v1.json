{"title": "Multi-omics data integration for early diagnosis of hepatocellular carcinoma (HCC) using machine learning", "authors": ["Annette Spooner", "Mohammad Karimi Moridani", "Azadeh Safarchi", "Salim Maher", "Fatemeh Vafaee", "Amany Zekry", "Arcot Sowmya"], "abstract": "The complementary information found in different modalities of patient data can aid in more accurate modelling of a patient's disease state and a better understanding of the underlying biological processes of a disease. However, the analysis of multi-modal, multi-omics data presents many challenges, including high dimensionality and varying size, statistical distribution, scale and signal strength between modalities.\nIn this work we compare the performance of a variety of ensemble machine learning algorithms that are capable of late integration of multi-class data from different modalities. The ensemble methods and their variations tested were i) a voting ensemble, with hard and soft vote, ii) a meta learner, iii) a multi-modal Adaboost model using a hard vote, a soft vote and a meta learner to integrate the modalities on each boosting round, the PB-MVBoost model and a novel application of a mixture of experts model. These were compared to simple concatenation as a baseline.\nWe examine these methods using data from an in-house study on hepatocellular carcinoma (HCC), along with four validation datasets on studies from breast cancer and irritable bowel disease (IBD). Using the area under the receiver operating curve as a measure of performance we develop models that achieve a performance value of up to 0.85 and find that two boosted methods, PB-MVBoost and Adaboost with a soft vote were the overall best performing models.\nWe also examine the stability of features selected, and the size of the clinical signature determined. Finally, we provide recommendations for the integration of multi-modal multi-class data.", "sections": [{"title": "1. Introduction", "content": "Modelling complex biological systems has the potential to expand our understanding of many challenging diseases. Data encompassing entire biological systems, known as 'omics data, can provide potentially complementary information, giving a more accurate picture of a patient's disease state. Multi-omics data integration is the process of combining two or more \u2018omics datasets in order to gain a better understanding of the mechanisms of biological processes and the complex interactions between biological systems (Krassowski et al. 2020) (Chicco, Cumbo, and Angione 2023). Multi-omics data integration can improve the accuracy of clinical outcome prediction, provide novel insights into mechanisms underlying complex diseases, aid in subtyping diseases and stratifying patient cohorts, discover new biomarkers or identify potential therapeutic targets (Chicco, Cumbo, and Angione 2023) (Picard et al. 2021) (Dickinson et al. 2022) (Kreitmaier, Katsoula, and Zeggini 2023).\nHowever, the analysis of multi-omics data presents many challenges. \u2018Omics datasets typically contain a large number of features and only a small number of samples, because of the high cost of clinical data collection and the limited number of study participants (Tabakhi et al. 2023). If not addressed, this leads to overfitting and therefore biased results in most machine learning algorithms. In addition, these datasets often contain many irrelevant and redundant features, misleading the algorithm and increasing computational complexity. The different 'omics datasets are often heterogeneous, having been collected using different technologies, and can vary greatly in size, statistical distribution, scale and signal strength (Santiago-Rodriguez and Hollister 2021). They may be imbalanced in terms of the number of features or the composition of the classes. In addition, they may have missing values or suffer from batch effects. Finally, multi-omics data integration must provide stable and interpretable results if it is to be trusted by clinicians.\nData integration strategies are often categorised as early, intermediate or late (Serra, Galdi, and Tagliaferri 2018). Early integration, also known as feature-level fusion, simply concatenates all of the 'omics data into a single dataset and trains a classifier on it directly. This exacerbates the problems of high-dimensionality, noise and highly correlated features (Chicco, Cumbo, and Angione 2023). Intermediate integration transforms the omics datasets into a common representation space prior to modelling (Wang et al. 2022). It can capture the relationships between the various modalities, but clinical interpretation of the results is difficult. Late integration, also known as decision-level fusion, trains a model on each 'omics dataset independently and the results are aggregated to give a final prediction. Techniques for aggregating the results may include majority vote, weighted majority vote, sum of the probabilities in each class and meta learning (Galar et al. 2011). Late integration is illustrated in Figure 1.\nLate integration or decision-level fusion is well placed to overcome many of the challenges of multi-omics data integration. By training a separate model on each omics modality, the curse of dimensionality is reduced, meaning overfitting is less likely to occur and computational complexity is also reduced. The approach is flexible as different machine learning models can be trained on each modality, taking advantage of the best performing model in each case and also addressing the problems of heterogeneity and feature imbalance between modalities (Tabakhi et al. 2023). Pre-processing steps such as filtering, imputation of missing values, normalisation and feature selection, can also be tailored to individual modalities."}, {"title": "2. Methods", "content": "To demonstrate the benefits of ensemble machine learning for multi-omics data integration, we benchmarked five late-integration methods that are capable of multi-modal, multi-class learning, and tested their predictive accuracy as well as their suitability for knowledge discovery, measured by their ability to select a stable and interpretable set of predictive features. The machine learning methods tested include: a) a simple voting ensemble, that aggregates the results from the different modalities using either a hard or soft vote; b) meta-learning, an algorithm that learns from the outputs of other machine learning algorithms; c) an enhanced multi-modal version of the well-known boosting algorithm Adaboost (Freund and Schapire 1995); d) a multi-view boosting method known as PB-MVBoost (Goyal et al. 2019), which takes into account both the accuracy and the diversity of the classifiers trained on each view and e) a mixture of experts model that trains a different model for each class and integrates these with a gating function. We compared these methods to a single classifier trained on the concatenated data as a baseline."}, {"title": "2.1. Data", "content": "The proposed multi-modal, multi-class machine learning methods were applied to an in-house dataset containing data collected from patients with liver disease (Behary et al. 2021), and validated on data from four publicly available multi-omics datasets, two containing data from patients with inflammatory bowel disease (IBD) and two from patients with breast cancer, to differentiate between different stages and types of disease. The characteristics of these datasets are summarised in Table 1, where references are given.\nThe in-house dataset (Behary et al. 2021) consists of data collected from prospectively enrolled patients to investigate the effect of the gut-microbiota's composition and function on the development of liver disease and primary hepatocellular carcinoma (HCC). Adult study subjects with liver cirrhosis and/or HCC were enrolled from two liver centres in Sydney Australia, as were healthy adult volunteers to function as controls. Baseline demographic and clinical data were collected at time of enrolment, with blood samples collected for multi-omics data and oral/faecal samples collected for metagenomic sequencing of oral/gut microbiota.\nStudy subjects were subsequently divided into one of four classes: healthy controls (CON); individuals with liver cirrhosis secondary to metabolic-associated fatty liver disease (MAFLD-cirrhosis) (CIR); individuals with HCC secondary to MAFLD-cirrhosis (LN); individuals with HCC secondary to viral hepatitis (LX). Of the 122 study subjects, the number of samples common to all modalities was 106, and samples were approximately evenly distributed across classes, as shown in Table 1. Seven modalities of data were available, and these are also listed in Table 1. For the stool and oral microbiome, two microbial levers of genera and species were used separately in the models."}, {"title": "2.2. Data Pre-processing", "content": "The data processing pipeline is shown in Error! Reference source not found.Error! Reference source not found.Error! Reference source not found.. Special consideration was given to memory management because of the large data file sizes, resulting in a 2-step process to read the files into memory. The first step was to read the raw files one at a time, apply filtering to identify potentially relevant features, as opposed to those with no predictive ability, record their names, then release the memory used. In the second step, only the relevant features of all files were read in, reducing the amount of memory required.\nPrior to modelling, filtering was performed to identify potentially relevant features and reduce the dimensionality of the data. This was also a multi-step process, that could be tailored to each dataset and depended on the characteristics of the dataset. The following command line options could be chosen:\n1. Features with more than 50% missing values and more than 90% zero values were eliminated.\n2. Of the remaining features, one feature from each pair of correlated features was eliminated.\n3. If the dimensionality of the dataset was still very large (i.e. if the ratio of the number of features to the number of samples was greater than 10), then only the 500 features with the highest variance were retained.\nThe following pre-processing steps were applied to the data during model building:\n1. Balancing: for imbalanced datasets, classes were balanced using the Synthetic Minority Oversampling Technique (SMOTE) (Chawla, N.V., Bowyer, K.W., Hall, L.O. 2002). SMOTE was applied to the training set only, during model building, following the method of (Rachmatullah 2022), balancing one class at a time against the majority class.\n2. Imputation: missing values were imputed using Multiple Imputation by Chained Equations (MICE) (van Buuren and Oudshoorn 2007). However, if the size of the dataset caused the computation time of MICE to increase unacceptably, then kNN, a single imputation method, was employed instead.\n3. Normalisation: Counts per Million (CPM) normalisation, followed by a log transformation was applied to the RNA sequence and DNA data. Standardisation was applied to all other data.\n4. Feature selection: Feature selection was performed using the Boruta feature selection algorithm (Kursa, Jankowski, and Rudnicki 2010), with the Gradient Boosting Machine (Freund and Schapire 1995) as the underlying learner.\nBoruta (Kursa, Jankowski, and Rudnicki 2010) is a model-agnostic algorithm, which can wrap around any base learner that provides feature importance scores. It creates a set of shadow features, which are randomly shuffled copies of the original features, then repeatedly trains a random forest on the combined set of features. Any features that achieve a lower importance score than the highest-scoring shadow feature are considered irrelevant. In this way Boruta naturally generates a feature selection threshold, which is based on p-values.\nBoruta is known as an \"all-relevant\" method, as it identifies all features relevant to the target variable, including those that are correlated. This is in contrast to the more common \u201cminimal-optimal\" feature\""}, {"title": "2.3. Individual modalities", "content": "In order to see the benefits of data integration, a classifier was first trained on each of the individual modalities, without performing data integration. The Gradient Boosting Machine (GBM) classifier was chosen because of its superior performance in previous experiments. The aim of these experiments was not only to set a baseline for comparison with the integration techniques, but also to observe which modalities gave the best predictive performance."}, {"title": "2.4. Data Integration Techniques", "content": "Machine learning methods capable of integrating multiple modalities of data using the late integration strategy were examined and compared in this study. To ensure a fair comparison, all methods used the gradient boosting machine (GBM) (Freund and Schapire 1995), a multi-class classifier, as their underlying classifier. However, it should be noted that the methods investigated allow different underlying classifiers to be trained for each modality. The methods, which were developed using custom code, are summarised in Table 2 and the integration methods are described graphically in Figure 3."}, {"title": "2.4.1. Concatenation", "content": "This method, also known as feature-level fusion (Tabakhi et al. 2023), combines the data from all modalities into a single vector and trains a single multi-class classifier on the concatenated data. This method was included as a baseline comparison for the other, more advanced methods."}, {"title": "2.4.2.\nVoting Ensemble", "content": "A voting ensemble trains a classifier on each modality and aggregates the outputs of the individual classifiers using a hard vote or a soft vote, illustrated in Figure 3. In a hard vote, or majority vote, the class that is predicted by the greatest number of modalities is the final prediction. In the case of a tie, the first predicted class is selected. In a soft vote, or weighted vote, the probability scores from each modality for each class are averaged. The class with the highest probability score is the predicted class."}, {"title": "2.4.1. Meta Learner", "content": "Meta-learning is a general term for machine learning algorithms that learn how to combine the outputs of other algorithms to maximise predictive accuracy. In the case of multi-modal data, a different base learner can be trained on each modality in parallel, and the meta-learner trained on the outputs of those base learners. The advantage of meta-learning is that it can produce reliable results with only a relatively small number of examples (Rafiei et al., n.d.). A recent survey gives many examples of the use of meta-learning in healthcare, in areas such as clinical risk prediction, disease diagnosis and drug interaction detection (Rafiei et al., n.d.).\nIn this study, a meta learner, illustrated in Figure 3C, was constructed using the Gradient Boosting Machine (GBM) as the base classifier and a random forest as the meta learner."}, {"title": "2.4.2. Multi-modal Adaboost", "content": "Boosting is an ensemble technique that trains a series of weak classifiers sequentially, such that each classifier learns from the mistakes of its predecessors (Schapire 1990). Ultimately these weak classifiers are combined to form a strong classifier.\nAdaboost, or Adaptive Boosting (Freund and Schapire 1995), was one of the first boosting algorithms to be proposed. On each iteration of the Adaboost algorithm, the samples that were misclassified in the last iteration are given increased weight, forcing the algorithm to focus on the more difficult to classify samples, with the aim of correcting the errors made in the last iteration. The final model is a weighted linear sum of all the models in the ensemble.\nAdaboost was initially developed for binary classification, but a multi-class version of Adaboost was introduced by Zhu et al. (Zhu et al. 2009). More recently, multi-view versions of Adaboost have been proposed, which can also be used to model multi-modal data. Xu and Sun developed a multi-view Adaboost algorithm, but it was limited to two views (Xu and Sun 2010). Xiao and Guo extended the multi-view Adaboost framework to an arbitrary number of views, in the context of multilingual subjectivity analysis (Xiao and Guo 2012). They developed two approaches, each of which trains a learner separately on each view, then combines the results of the single view classifiers, on each iteration, either using a hard majority vote or a linear weighted combination of the outputs of the single-view classifiers.\nHere a multi-modal version of Adaboost was implemented following the method of Xiao and Guo (Xiao and Guo 2012), with some novel modifications. On each round of the boosting process, a classifier was trained independently on each modality. The results from these classifiers were then aggregated to give a final decision using one of three methods a hard vote, a soft vote or a meta learner trained on the results of the independent classifiers.\nWhile in the original Adaboost it is a simple matter to identify the misclassified samples to calculate the classification error rate, in multi-modal Adaboost, all modalities must be taken into account when calculating the error rate. Here, a sample is considered to be correctly classified only if it is classified with high confidence. If the final decision is made by a hard vote or meta learner, a sample is classified with high confidence if at least half of the modalities agree on its classification. The soft vote gives a final probability for each class and a sample is classified with high confidence if the highest probability is at least double that of the next highest probability. If these conditions are not met, then the sample is considered to be misclassified.\nThe optimal number of boosting steps within each iteration was chosen empirically, based on tests run using different numbers of boosting steps, and was determined to be 20. Beyond this value no added benefit was achieved."}, {"title": "2.4.3. PB-MVBoost", "content": "PB-MVBoost (Goyal et al. 2019) is a multi-view ensemble method, based on Adaboost, that aims to balance the accuracy of the classifiers trained on each view and the diversity of their outputs by learning two sets of weights \u2013 weights over the classifiers and weights over the views. It then combines the results of each classifier using a weighted vote, learning the weights by minimising an upper-bound on the error of the majority vote. Fadnavis et al. used PB-MVBoost in a novel framework to distinguish healthy controls from those in the early stages of Huntington's disease (Fadnavis, Polosecki, and Garyfallidis 2021).\nPB-MVBoost was implemented in R, directly following the author's Python implementation, which is available on Github."}, {"title": "2.4.1.\nMixture of Experts", "content": "A mixture of experts model (Yuksel, Wilson, and Gader 2012) divides a complex machine learning task into multiple sub-tasks, based on domain knowledge, and trains a model on each sub-task. Each of these models focuses solely on its specific sub-task, becoming an expert in that sub-space. A gating function learns which expert to trust and selects the best expert to predict each sample. Minoura et al. (Minoura et al. 2021) developed a model for integrated analysis of single-cell multi-omics data using a mixture of experts model.\nHere a novel adaptation of this method was developed: a separate model was trained for each class in the multi-class setting, using a one-vs-rest approach i.e. each expert was a binary classifier, distinguishing its own class from all other classes combined. The experts were trained in parallel and the subsequent gating function operated using the following rules:\n\u2022 Each expert can only predict its own class or \u201cREST\u201d.\n\u2022 If an expert predicts its own class, and it is the only one to do so, then its prediction is accepted as correct.\n\u2022 If more than one expert predicts its own class, then the prediction of the expert that predicts with the highest confidence (probability) is accepted as correct.\n\u2022 If no experts predict their own class (i.e. all predict \"REST\"), then the sample is classified as unknown. This indicates which samples are difficult to predict."}, {"title": "2.5. Incremental Method", "content": "From a clinical perspective, the ability to make an accurate prediction from a smaller subset of modalities means that fewer tests are required, making diagnosis simpler, less expensive and possibly less invasive for the patient. With this in mind, we designed an incremental model which determines the subset of the modalities that gives maximum predictive performance.\nThe incremental model adds (or removes) one modality at a time to (or from) the model. Here we included all modalities at the start, and eliminated one modality at a time, comparing the performance after each elimination to the previous best performance. On each iteration, the modality that caused the largest drop in performance was removed, and subsequent tests were carried out on the reduced modality set. The modalities were integrated using a soft voting ensemble and the metric used for comparison was the F1 score. A small margin of error was allowed in the performance comparison."}, {"title": "2.6. Feature Selection and Calculation of Feature Importance Scores", "content": "Feature selection identifies the features that are most relevant to the model outcome (Li and Wang 2018). These features can then be examined as potential biomarkers or may provide useful insights into the underlying mechanisms of disease.\nWhen multiple classifiers are combined to give a final result, the final feature importance scores must be calculated in a meaningful way. For most models, the final set of selected features consists of those selected by each individual classifier, normalised and scaled to a range of [0,1] for comparison.\nSome models required additional consideration. The features provided as input to the meta learner are the results of the base classifiers applied to each modality. Therefore, the feature importance scores generated by the meta learner give an indication of the relative importance of each modality.\nThe calculation of feature importance scores for Adaboost and PB-MVBoost was more complex. For each fold of data to which the model was applied, multiple boosting iterations were run. Each iteration produced a model weight and a set of feature importance scores for each modality. The final feature importance scores for each modality in each fold were calculated as the sum of the raw scores multiplied by the model weight, divided by the sum of the weights.\nThe Mixture of Experts model selects a set of features for each expert i.e. for each class. The fact that these feature sets can differ shows clearly that different features can influence the model for different classes.\nIn addition, for all models, only those features that were selected in 75% of cross-validation iterations and that had a normalised score of 0.5 or higher were included in the final set of selected features."}, {"title": "2.7. Feature Selection Stability", "content": "Stability of feature selection can be defined as the reproducibility of the features selected, when the method is applied to different samples of the data (Turney 1995). Various measures of stability have been proposed, each of which has its own strengths and limitations. The relative weighted consistency index, proposed by Somol and Novovi\u010dov\u00e1 (Somol and Novovi\u010dov\u00e1 2010), has been chosen here because of its ability to measure the stability of sets of features of different lengths, such as those selected from different data samples when running repeated experiments, and because it does not over-emphasise low-frequency features."}, {"title": "2.8. Experimental Framework and Model Evaluation", "content": "The methods being assessed were evaluated their predictive performance as well as the stability of the set of features they selected. All models were evaluated using 5 repeats of 5-fold cross-validation. The evaluation metrics shown in Table 33 were calculated for each fold and averaged over folds and classes to give a final result (M and M.N 2015).\nAll experiments were run on the Katana high performance computing cluster at the University of NSW (\"Katana\" 2010). All code was written in R (R Core Team 2019). The R package mlr (Bischl et al. 2016) was used as a framework to implement the machine learning experiments and the R package Future was used to implement parallel processing (Bengtsson 2021).\nTests of statistical significance were applied to each group of experiments, using the corrected resampled paired t-test, proposed by Nadeau and Bengio (Nadeau and Bengio 2003). This test takes into account the fact that the Type I error is inflated when applying a standard t-test on results from a repeated k-fold cross validation because the results are not independent and corrects for this."}, {"title": "3. Results", "content": ""}, {"title": "3.1. Individual modalities", "content": "The performance of the individual modalities in each dataset is shown in the boxplots in Supplementary Figures S1-S5 and in Supplementary Table S1. The values shown represent the average across all iterations of cross-validation.\nIn the HCC dataset, the best performing modality was the Cytokine data, with an AUC of 0.77 and an F1 score of 0.64 (\u00b10.15). The metabolomic data also performed well, with an AUC of 0.74 and an F1 score of 0.6 (\u00b10.15). In two other datasets, IBD1 and IBD2, the metabolomic data also proved to be the most predictive with AUC scores of 0.56 and 0.76 respectively and F1 scores of 0.39 (\u00b10.17) and 0.68 (\u00b10.06) respectively. In the breast cancer datasets, the clinical data was the most predictive in the Breast1 dataset, with an AUC of 0.8 and an F1 score of 0.65 (\u00b10.2), and the proteomic data was the most predictive in the Breast2 dataset, with an AUC of 0.74 and an F1 score of 0.57 (\u00b10.36)."}, {"title": "3.2. Multi-modal Data Integration", "content": "Each of the muti-modal data integration strategies was applied to each dataset and the results of these experiments are shown in the boxplots in Figure 4 for the HCC (Genus) dataset, and in Supplementary Figures S6-S11 and Supplementary Table S2 for the remaining datasets. The PB-MVBoost method was either the best or second-best performing model in every dataset, achieving an AUROC score of 0.85 in the HCC-Genus dataset. The Adaboost method with a soft vote aggregator also performed well, achieving AUROC scores of up to 0.84 and performing equally as well as the PB-MVBoost method in some cases. In the IBD1 and Breast2 datasets a simple concatenation of modalities performed better than the multi-modal methods. However, further examination of the feature selection results in Section 3.4 will reveal that the features selected by this method are less stable than those selected by the multi-modal methods."}, {"title": "3.3. Optimal Subset of Modalities", "content": "The incremental model, described in Section 2.5, was used to determine the most predictive subset of modalities in each dataset. For some datasets, namely IBD2, Breast1 and Breast2, a single modality gave the best predictive performance, while for others, namely HCC and IBD1, a small number of modalities gave equal or better performance than the full set of modalities. The results of this analysis are given in Table 5, which shows the degree to which performance improved as each modality was removed and the order in which the modalities were removed. The subset of modalities selected as optimal corresponds to the top performing individual modalities in the two datasets, confirming the validity of the incremental model.\nTo further validate this method, Table 6 compares the AUC and F1 scores for each of the multi-modal methods on all modalities versus the optimal subset, for HCC and IBD1, as the optimal subset size for these two datasets was greater than one. It can be seen that each method performs as well on the optimal subset of modalities as it does on the full set of modalities."}, {"title": "3.4. Feature Selection", "content": "As the primary purpose of developing these multi-modal models is to identify a clinical signature that can predict the development of the disease in question, the methods must also be judged on the clinical signature they identify. The desirable characteristics of a clinical signature include its length, its stability, the number of modalities from which it draws features and its accuracy in prediction. Ideally, a signature consisting of a smaller number of features and modalities will mean fewer tests for the patient and be more economical. The more stable, or reproducible, and the more accurate the feature selection results, the more confidence clinicians can have in their reliability in predicting disease.\nThe results of the feature selection for each multi-modal method applied to each dataset are summarised in Supplementary Table S3, which lists the number of features selected, the number of modalities those features are drawn from, the stability of the feature selection, measured using the relative weighted consistency index (Somol and Novovi\u010dov\u00e1 2010), the predictive accuracy of the selected features and the mean of these two measurements. Note that as the Meta Learner uses a random forest as its meta classifier, and a random forest applies a non-zero feature importance score to every feature, rather than selecting a subset of features. Since every feature achieves a score on every iteration this method artificially achieves a perfect stability score of 1.\nIn every dataset, simple concatenation produced the least stable feature selection results. Concatenation of the modalities greatly increases the dimensionality of the data and feature selection is less stable in high dimensions. The data integration methods overcome this problem by training a separate classifier on each modality, illustrating another benefit of these methods.\nIn most datasets the PB-MVBoost method achieved the highest mean of stability and accuracy, but its signature length tended to be among the longest of all the methods and selected from the largest number of modalities. By contrast, in most cases the Adaboost model with a soft vote produced a slightly shorter signature with little loss in predictive accuracy or stability."}, {"title": "4. Discussion", "content": "In this work we have presented a benchmarking study of multi-modal multi-class machine learning techniques for late integration of multi-omics data, applied to five different datasets. We examined their predictive accuracy as well as the stability of the features they selected and compared the results of the multi-modal methods with those of the individual modalities. For each dataset, we also determined an optimal subset of modalities which performed as well as the full set, thus permitting patient diagnosis using fewer tests.\nWe employed existing and enhanced methods for late integration. Existing methods included a simple voting ensemble using hard and soft voting, a meta learner and the PB-MVBoost algorithm. Enhancements were made to the multi-modal Adaboost algorithm to improve its predictive accuracy and a novel application of the mixture of experts model was developed which builds an expert for each class and combines the results of these experts using a novel gating function.\nOverall, the multi-modal methods showed superior performance to the individual modalities, with the PB-MVBoost and the Adaboost models being the most predictive. This shows that different modalities can provide complementary information about a patient's disease state, and integrating those modalities in a single model can improve predictive performance. Therefore, the use of these data integration techniques is recommended.\nBoosting is an ensemble technique that trains a series of weak learners and combines them to form a stronger learner, improving predictive accuracy by reducing overfitting. Because of its ability to reduce overfitting, boosting is particularly suited to datasets that have a large number of features and a small number of samples, which is a characteristic of the data sets examined in this work. In addition, the two boosted methods, PB-MVBoost and Adaboost, calculate a weight for each modality, thereby prioritising the more predictive modalities. Combining boosting, modality weighting and data integration, gives these methods an advantage, so it is not surprising that they performed well.\nIn contrast, the meta learner, mixture of experts model and voting ensembles give equal weight to each modality, and our results on the individual modalities show that some modalities are more predictive than others. Therefore, the less predictive modalities are likely to be detrimental to the overall predictive power of the model.\nFurther, our results from the incremental model show that it is not always necessary to incorporate all modalities of data in a model to achieve the maximum predictive power. Finding the right subset of modalities to maximise predictive performance is crucial, not only to optimise the model outcome, but also for future clinical use, where a smaller set of modalities can simplify and reduce the cost of screening patients for a disease. Therefore, the use of our incremental model to determines the most predictive subset of modalities is recommended.\nIn the IBD1 and Breast2 datasets, a simple concatenation was the best performing model. The proteomic modality of the Breast 2 dataset significantly outperforms the other two modalities and is identified by the incremental model as being the best subset of modalities on its own. Therefore, it seems likely that the other two modalities do not contribute significantly to the model and that the proteomic modality dominates in the concatenation model."}, {"title": "5. Conclusion", "content": "The ability to integrate data from multiple modalities can allow more accurate modelling of a patient's clinical outcome, as these modalities can provide complementary information. Such modelling may also assist in understanding the mechanisms underlying complex diseases and help identify novel biomarkers to aid in diagnosis.\nThe aim of this paper was to compare the performance of ensemble machine learning techniques capable of late integration of multi-class data from different modalities. The ensemble methods and their variations tested were i) a voting ensemble, with hard and soft vote, ii) a meta learner, iii) a multi-modal Adaboost model using a hard vote, a soft vote and a meta learner to integrate the modalities on each boosting round, the PB-MVBoost model and a novel application of a mixture of experts model. These were compared to simple concatenation as a baseline.\nWe examined the predictive accuracy of these methods, as well as the stability of the features they selected, and the size of the clinical signature determined, by applying them to an in-house dataset containing data collected from patients with liver disease (Behary et al. 2021), and validated them on data from four publicly available multi-omics datasets. Our results showed that two boosted methods, PB-MVBoost and Adaboost with a soft vote were the overall best performing models, both in terms of predictive accuracy and stability of feature selection. We also provided a means of determining an optimal subset of modalities that could lead to a smaller clinical signature without loss of predictive accuracy. Finally, we have provided recommendations for the integration of multi-modal multi-class data."}]}