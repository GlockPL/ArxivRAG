{"title": "E2E-AFG: An End-to-End Model with Adaptive Filtering\nfor Retrieval-Augmented Generation", "authors": ["Yun Jiang", "Zilong Xie", "Wei Zhang", "Yun Fang", "Shuai Pan"], "abstract": "Retrieval-augmented generation methods often neglect the quality of\ncontent retrieved from external knowledge bases, resulting in irrelevant infor-\nmation or potential misinformation that negatively affects the generation results\nof large language models. In this paper, we propose an end-to-end model with\nadaptive filtering for retrieval-augmented generation (E2E-AFG), which inte-\ngrates answer existence judgment and text generation into a single end-to-end\nframework. This enables the model to focus more effectively on relevant content\nwhile reducing the influence of irrelevant information and generating accurate\nanswers. We evaluate E2E-AFG on six representative knowledge-intensive lan-\nguage datasets, and the results show that it consistently outperforms baseline\nmodels across all tasks, demonstrating the effectiveness and robustness of the\nproposed approach.", "sections": [{"title": "Introduction", "content": "The remarkable natural language understanding and generation capabilities demon-\nstrated by Large Language Models (LLMs) have led to their success in knowledge-\nintensive tasks, such as open-domain question answering and fact verification [4, 28,\n1]. However, LLMs are prone to generating hallucinatory content that contains factual\nerrors in the absence of supporting documentation. To address this issue, [21] proposed\nthe retrieval-augmented generation (RAG) method, which involves retrieves relevant\ncontext from external knowledge bases to provide additional evidence for LLMs when\nanswering input queries. Other approaches [31] directly utilize a pre-trained LLM to\ngenerate a relatively accurate pseudo-answer as an extended document for the input\nquery. However, these methods often fail to adequately consider the quality of the re-\ntrieved or generated content, which may include distracting irrelevant content or erro-\nneous information, leading LLMs to still produce hallucinatory answers."}, {"title": "Related Work", "content": "Retrieval-Augmented Generation. Early research methods such as REALM [13] and\nRAG [21], laid the foundation for the field of retrieval-augmented generation (RAG)\nby combining retrievers with large language models (LLMs). Subsequently, RETRO\n[3] introduced the concept of training language models on fixed retrievers, while Atlas\n[16] further explored dedicated loss functions and training strategies, achieving im-\nproved results, particularly in few-shot learning scenarios. Recent studies have shifted\ntowards optimizing the retrieval component while leveraging pre-trained, fixed LLMs.\nFor instance, RePlug [34] and In-context RALM [29] demonstrated that fine-tuning the\nretrieval module can surpass end-to-end trained models in certain tasks, such as ques-\ntion answering. In contrast, SAIL [22] integrated real search engines with information\ndenoising processes, aiming to enhance the relevance and accuracy of retrieval results,\nshowcasing potential in broader application contexts. Our work seeks to enhance atten-\ntion to reliable information by performing answer existence judgment on the retrieved\npassages prior to generation, thereby reducing the interference caused by irrelevant in-\nformation.\nRetrieval Content Filtering Strategies. In knowledge-intensive tasks, post-processing\nof retrieved content is crucial for enhancing system performance, with common prac-\ntices including re-ranking and context filtering. In early studies, [32] and [20] explored\npassage re-ranking methods based on BiLSTM, while [23] and [27] employed BERT-\nbased cross-encoders to achieve more precise passage re-ranking. Subsequently, [26]\nproposed a method for re-ranking passages by updating the query, and [15] directly\napplied heuristic re-ranking to the answers. In recent years, several context filtering\nstrategies have been introduced. For example, FILCO [33] trains a context filtering\nmodel to perform fine-grained sentence-level filtering on the retrieved passages. Multi-\nMeta-RAG [25] utilizes a specific set of domain queries and formats to select the most\nrelevant documents through database filtering. In contrast, our approach constructs a\nsingle end-to-end model that can simultaneously perform context filtering and answer\ngeneration.\nMulti-task Learning. Multi-task learning (MTL) enhances overall model performance\nby jointly learning multiple tasks, allowing it to capture the correlations and shared\nfeatures among tasks [5]. In natural language processing applications, MTL not only\nleverages task relevance to mitigate issues of data scarcity and model overfitting but\nalso improves the generalization capability of the model. For instance, [6] proposed a\nhierarchical multi-task learning approach that enhances the model's ability to capture\ninter-task dependencies. ROM [11] introduced a generalizable Retrieval Optimized\nMulti-task framework that reduces the model's parameters. Our method applies MTL\nto the retrieval-augmented generation domain by jointly learning binary classification\nand generation tasks, enabling the model to acquire context filtering and answer gener-\nation capabilities."}, {"title": "Method", "content": "Problem Statement. In knowledge-intensive tasks, each entry consists an input query\nQ, a ground truth answer A, and a set of retrieved passages P = {pi}{=1 from a data-\nbase. We provide the generator with one or more passages along with a pre-generated\npseudo-answer S to generate a response to the query Q. Specifically, in the question-\nanswering tasks, Q and A are natural language questions and their corresponding\nground truth answers; in the fact verification tasks, Q is a statement and A E\n{SUPPORTS, REFUTES} indicates the correctness of the statement; in the knowledge-\nbased dialogue generation tasks, Q consists of a dialogue history, and A is a response\nthat accurately continues the conversation.\nOverview. The overall architecture of our proposed method is illustrated in Fig. 1. First,\na pre-trained large language model generates a pseudo-answer S for the query Q. Next,\nthe query Q, the retrieved set of passages P, and the pseudo-answer S are input into the\nE2E-AFG model, where both generation and binary classification tasks are performed.\nThe generation task utilizes the generator E2Egen to produce an answer. The binary\nclassification task employs E2EEncoder to obtain embeddings for the three inputs,\nwhich are then processed through cross-attention and a feedforward neural network to\npredict the category scores. Finally, the cross-entropy loss for both the generation and\nbinary classification tasks is computed. This approach allows for the update of the in-\nternal parameters of the shared E2EEncoder, implicitly learning a filtering capability that\nprioritizes sentences more likely to contain answers while reducing interference from\nirrelevant sentences."}, {"title": "Generating Pseudo-Answers", "content": "In knowledge-intensive tasks, models typically rely on passages retrieved from data-\nbases to generate answers. However, these passages often do not perfectly match the\nquestions, leading to a lack of reliable evidence for the model to generate accurate an-\nswers. To mitigate this limitation, we introduce a strategy that utilizes a pre-trained\nlarge language model to generate pseudo-answers, which serve as an additional refer-\nence to assist the model in producing more accurate responses. To explore how to gen-\nerate high-quality pseudo-answers, we have devised several different prompts, as illus-\ntrated in Fig. 2. The first directly generates concise answers, which may lead to the\ngeneration of hallucinatory content; the second encourages the model to make the best\nguess for the correct answer when it is uncertain; and the third structured prompt in-\nstructs the model to also provide the reasoning behind the derived answer."}, {"title": "Obtaining Silver Classification Labels", "content": "To determine whether the retrieved passage set P and the generated pseudo-answer S\ncontain answers, we introduce three context filtering methods based on [33]: (i) String\nInclusion (STRINC): checking if the context directly contains the ground truth answer;\n(ii) Lexical Overlap (LEXICAL): measuring the overlap of words between the context\nand the ground truth answer; and (iii) Conditional Cross-Mutual Information (CXMI):\nassessing the likelihood of the generator producing the ground truth answer given the\ncontext. For a specific task, we select the most appropriate filtering method to obtain\nsilver classification labels. For instance, in question-answering tasks, we may use StrInc\nto evaluate whether each passage or pseudo-answer contains the ground truth answer.\nIn contrast, for fact extraction tasks, where the ground truth answer resembles a boolean\nvalue and cannot be assessed using the first two methods, we employ CXMI to compute\nthe corresponding probability and set a threshold to to derive the silver classification\nlabel. We concatenate the obtained labels with the ground truth answer A to facilitate\nloss calculation."}, {"title": "Generation Task", "content": "For each training sample (Q, A, P, S), we first insert a special character between the\ndifferent fields to ensure they can be distinguished after encoding with E2EEncoder. We\nthen input the encoded query Qembs, the retrieved passage set Pembs, and the pseudo-\nanswer Sembs into E2Egen to produce the output answer 0. The sequence probability is\ncalculated as follows:\n$P_o(O|Q,P,S) = \\prod_{i=1}^L p(o_i|o_{<i}, Q, P, S)$\nwhere of represents the i-th token of the generated output 0, and L is the final output\nlength. To simplify the notation, we continue to use Q, P, S in place of Qembs, Pembs,"}, {"title": "Classification Task", "content": "To enhance the model's context filtering capability, we introduce a classification mod-\nule specifically designed to determine whether the input context contains the answer.\nThe generator and the classification module share the same encoder E2EEncoder, allow-\ning the classification model to indirectly improve the model's context filtering capabil-\nities by influencing the encoder's parameters.\nThe classification module comprises two main components: cross-attention layer,\nand feedforward neural network. First, the encoded query Q, each retrieved passage pi,\nand the pseudo-answer S are fed into the cross-attention layer. In this layer, the model\ncomputes the attention weights between Q and pi, as well as between Q and S, gener-\nating cross-attention representations:\n$\\alpha_i = softmax(\\frac{Q^T P_i}{\\sqrt{d_k}})$\n$\\beta = softmax(\\frac{Q^T S}{\\sqrt{d_k}})$\nwhere dk is the dimensionality of the encoder's feature channels.\nNext, the generated cross-attention representations are fed into a feedforward neural\nnetwork to predict two binary classification results:\n$\\varepsilon_i = FFN(\\alpha_i), \\xi = FFN(\\beta)$\nwhere FFN denotes a two-layer feedforward neural network. The loss function for the\nclassification task is defined as the cross-entropy:\n$L_{cls} = \\sum_{i=1}^K - (log \\varepsilon_i^{st}) + log \\xi^{st}$\nHere, \u025bst and 39t represent the predicted probability values corresponding to the\nground truth classes of each passage pi and the pseudo-answer S, respectively, while\nK is the number of retrieved passages."}, {"title": "Model Training", "content": "During the training process, we simultaneously optimize the loss functions of both the\ngenerator and the classification module. The overall loss function is defined as a\nweighted sum of the two losses:\n$L_{TOTAL} = (1 - \\sigma)L_{gen} + \\sigma L_{cls}$\nwhere Lgen is the loss from the generator, Lcls is the loss from the classification module,\nand o is the weighting factor.\nTo further enhance the training efficiency and performance of the model, we employ\nLow-Rank Adaptation (LoRA) [14] techniques, which add low-rank matrices to the\nweight matrices of the pre-trained model for fine-tuning. This approach reduces com-\nputational overhead and accelerates the training process."}, {"title": "Experiments", "content": "As shown in Table 1, we conducted experiments on six retrieval-augmented\nknowledge-intensive language datasets, which utilize data constructed from Wikipedia\narticles as supporting documents. Each dataset is divided into a training set (train), a\ndevelopment set (dev), and a test set (test). Exact Match (EM): Measures the percentage\nof predictions that exactly match the ground truth. Unigram F\u2081 (F1): Evaluates the har-\nmonic mean of precision and recall based on individual word overlap between the pre-\ndiction and the ground truth. Accuracy (Acc): Represents the proportion of correct pre-\ndictions out of the total number of predictions. Top-20 recall [2]: Measures whether the\nanswer string is included among the top 20 passages in the development set (applicable\nto Natural Questions [19] and TriviaQA-unfiltered [17]), or whether it originates from\nthe relevant annotated source articles in the KILT dataset [24] (applicable to FEVER\n[30] and Wizard of Wikipedia [9]).\nOpen-Domain Question Answering: The Natural Questions (NQ) and TriviaQA-unfil-\ntered (TQA) datasets consist of questions, answers, and relevant passages from Wik-\nipedia, using short answers limited to five tokens. Fact Verification: The FEVER da-\ntaset contains paraphrased claims from Wikipedia, labeled as \"SUPPORTS\" or\n\"REFUTES\" based on their alignment with original content. Multi-Hop Question An-\nswering: The HotpotQA dataset features complex questions requiring reasoning\nthrough multiple passages to find answers, with 113K question-answer pairs derived\nfrom Wikipedia. Long-Form Question Answering: The ELI5 dataset includes 270K\nReddit posts requiring detailed, multi-sentence answers to open-ended questions.\nKnowledge-Based Dialogue Generation: The Wizard of Wikipedia (WoW) dataset\ngenerates dialogue responses based on a history of turns, utilizing information from\nWikipedia articles."}, {"title": "Implementation Details", "content": "We loaded the model checkpoints from HuggingFace Transformers [35], using FLAN-\nT5-xl [8] as our backbone model architecture. We employed prompt 3 and the Llama-\n3 model to generate pseudo-answers, limiting their generation length to no more than\n200 tokens. For the queries in each dataset, we utilized the Dense Passage Retriever\n(DPR) [18] to extract the top 5 most relevant passages from Wikipedia. To obtain silver\nclassification labels, we adopted the optimized settings from FILCO, using STRINC for\nNQ and TQA, LEXICAL for WoW, and CXMI for FEVER, HotpotQA, and ELI5, with a\nthreshold to set to 0.5.\nFor the generator E2Egen, we allowed a maximum input sequence length of 512 to-\nkens during both training and inference. We generated up to 64 tokens for open-domain\nquestion answering, multi-hop question answering, fact verification, and dialogue gen-\neration tasks, and up to 256 tokens for long-form question answering. We used greedy\ndecoding to produce the final answers. Regarding model parameters, we set the en-\ncoder's feature channel dimension dk to 2048, trained for 3 epochs, with a learning rate\nof 5e-5 and a batch size of 8. The weight factor o was set to 0.2."}, {"title": "Baseline Methods", "content": "In this section, we introduce three baseline methods: FULL [21], HyDE [12], and\nFILCO [33], along with the proposed E2E-AFG and SILVER configurations. To ensure\na fair comparison, we employed the same backbone model architecture across all meth-\nods as that used in our proposed E2E-AFG.\nFULL: A common approach in retrieval-augmented generation where all passages,\nincluding pseudo-answers, are input into the generation model with the query.\nHyDE: Filters passages through a dense bottleneck using unsupervised contrastive\nlearning, encoding them before inputting into the generation model.\nFILCO: Uses a trained model to filter sentences within passages, passing only the\nselected sentences to the generation model.\nE2E-AFG: Ours end-to-end model potentially assesses the existence of answers for\nthe input passages before feeding all passages into the model for answer generation.\nSILVER: This configuration inputs only those passages labeled as containing an an-\nswer, testing the performance upper bound of E2E-AFG."}, {"title": "Comparison with Baseline Methods", "content": "Table 2 presents the experimental results of E2E-AFG across six datasets, demonstrat-\ning that our model outperforms the baseline models in all cases. Specifically, for ex-\ntractive question-answering tasks NQ and TQA, we achieved improvements of at least\n1.83% and 1.56% in EM, respectively. This indicates that our model focuses more on\ncredible passages and reduces attention to irrelevant information, thereby generating-\nmore accurate answers. In the fact verification task FEVER, we attained an accuracy\nincrease of at least 1.09%. For the complex multi-hop question-answering task Hot-\npotQA and the long-form question-answering task ELI5, we observed improvements\nof at least 1.68% and 0.13% in F\u2081 score, respectively. We hypothesize that the relatively\nmodest performance gain on ELI5 may be due to the fact that it requires detailed,\nlengthy answers, while the generated pseudo-answers tend to be relatively brief, limit-\ning the model's filtering capabilities. Additionally, in the dialogue generation task\nWoW, we improve the F\u2081 score by at least 1.35%. Furthermore, the performance of\nE2E-AFG approaches the upper bound performance of SILVER, indicating its excep-\ntional capabilities in context filtering and text generation, allowing it to achieve near-\noptimal results without relying on specific annotations."}, {"title": "Ablation Studies", "content": "Table 3 illustrates the ablation studies conducted on E2E-AFG, assessing the contribu-\ntion of key components to the overall performance by progressively removing them\nfrom the model. First, when the pseudo-answer generation module is removed, the gen-\nerator relies solely on the retrieved passages, resulting in a significant decline in per-\nformance across the three different tasks. Building on this, further removal of the cross-\nattention layer in the classification module results in a slight decrease in performance.\nWithout the cross-attention mechanism, the classification module no longer aligns the\nencoded query Q with the retrieved passages P and pseudo-answers S separately\nthrough cross-attention. Instead, Q is concatenated with both representations, and the\nconcatenated features are fed into the feedforward neural network to predict answer\nexistence. Finally, when the classification module is completely removed, the model's\nperformance drops sharply, as it loses its context filtering capability.\nTable 4 demonstrates the impact of different prompts on pseudo-answer generation,\nrevealing that the pseudo-answers generated using prompt 3 achieve the highest aver-\nage recall rate, indicating that they are most likely to support the generator in producing\ncorrect answers. While simpler prompts may also generate useful pseudo-answers, de-\ntailed and structured prompts help align the model's output more closely with stand-\nards, such as avoiding the generation of nonsensical text and alleviating issues related\nto hallucinatory content.\nTable 5 shows the effect of different top-K retrieved passages on the generation re-\nsults. We observed that aggregating multiple top-ranked passages significantly en-\nhances the performance of extraction tasks. However, this improvement comes with a\nlinear or quadratic increase in computational load. Furthermore, the performance on the\nFEVER and WoW datasets did not show substantial improvements and even declined\nin some methods. We believe this may be attributed to the decreased content quality of\nthe lower-ranked retrieved passages.\nFig. 3(a) illustrates the impact of the weight factor o on model performance. When\n\u03c3 is around 0.2 to 0.3, the model achieves optimal performance. As o increases further,\nthe F\u2081 scores across the three datasets begin to decline, with a notable drop when \u03c3\nreaches 0.9. This indicates that in multi-task learning, the distribution of loss weights\nacross different tasks significantly affects model performance, necessitating careful\ntuning of weight factors for specific tasks."}, {"title": "Further Analysis", "content": "Fig. 3(b) compares the model parameters for each method. It can be seen that our pro-\nposed E2E-AFG method has fewer parameters than the other methods, particularly\nwhen compared to the FILCO model, which has the most parameters. This indicates\nthat our method achieves fewer parameters while maintaining strong performance po-\ntential by integrating filtering and generative models."}, {"title": "Conclusion", "content": "The End-to-End Model with Adaptive Filtering (E2E-AFG) proposed in this paper ef-\nfectively addresses the issue of the generator being distracted by irrelevant information\nretrieved during retrieval-augmented generation tasks. By integrating answer existence\njudgment with the generation task into a single end-to-end model, E2E-AFG achieves\nsynchronous learning of context filtering and answer generation. Experimental results\ndemonstrate that our model outperforms baseline models across six knowledge-inten-\nsive language datasets, with performance improvements ranging from +0.13 to +1.83\npoints. E2E-AFG not only enhances generation quality but also simplifies model com-\nplexity and reduces training costs. Future research could further optimize the model\narchitecture and filtering strategies to explore its potential in various application sce-\nnarios."}]}