{"title": "COGDEVELOP2K: REVERSED COGNITIVE DEVELOPMENT IN MULTIMODAL LARGE LANGUAGE MODELS", "authors": ["Yijiang Li", "Qingying Gao", "Haoran Sun", "Haiyun Lyu", "Dezhi Luo", "Hokin Deng"], "abstract": "Are Multi-modal Large Language Models (MLLMs) stochastic parrots? Do they genuinely understand and are capable of performing the tasks they excel at? This paper aims to explore the fundamental basis of MLLMs, i.e. core cognitive abilities that human intelligence builds upon to perceive, comprehend, and reason. To this end, we propose CogDevelop2K, a comprehensive benchmark that spans 12 sub-concepts from fundamental knowledge like object permanence and boundary to advanced reasoning like intentionality understanding, structured via the developmental trajectory of a human mind. We evaluate 46 MLLMs on our benchmarks. Comprehensively, we further evaluate the influence of evaluation strategies and prompting techniques. Surprisingly, we observe a reversed cognitive developmental trajectory compared to humans.", "sections": [{"title": "INTRODUCTION", "content": "Building on the foundation of advanced large language models (LLMs), multi-modal large language models (MLLMs) have recently demonstrated human-level performance in complex tasks involving high-level reasoning, perception, and cognition Li et al. (2024a); Liu et al. (2024); Team (2023); Fu et al. (2023); OpenAI (2023) such as Spatial Reasoning Chen et al. (2024); Cai et al. (2024), OCR Mori et al. (1999), Scene Understanding Cordts et al. (2016); Chen et al. (2017), Action Recognition Jhuang et al. (2013); Herath et al. (2017) and Prediction Lan et al. (2014); Kong & Fu (2022), etc.\nThe progress in MLLMs has reignited hopes for achieving Artificial General Intelligence (AGI). However, we pose a critical question: Do MLLMs truly comprehend these tasks and possess the genuine capabilities to perform them, or are they merely \"stochastic parrots\" that rely on learning spurious correlations? To explore this, we draw inspiration from the development of human cognition.\nExtensive research in human cognitive development suggests the existence of core cognition which grounds the diversity of incredible human intelligent abilities (Spelke et al., 1992; 1994; 1995; Spelke & Kinzler, 2007; Mitchell, 2020; 2021), and such core cognition is unraveled via the developmental cascades of the human mind (Masten & Cicchetti, 2010). From infancy to early adulthood, core cognitive concepts emerge and develop along a structured trajectory, with interdependent relations between early, simple abilities and late, complex abilities. For instance, the ability to imagine the perspectives of others typically develops between the ages of 3 and 6 (Piaget & Inhelder, 1969), while the capacity to fully comprehend others' intentions matures around age 12 (Wimmer & Perner, 1983; Wellman et al., 2001; Liu et al., 2008). At the same time, the ability to understand other people's intentions largely depends on the the ability to understand other people's perspectives (Iacoboni, 2009; De Waal & Preston, 2017; Liu et al., 2017; Caviola et al., 2021; Ninomiya et al., 2020).\nAs highlighted in previous research, core cognitive abilities form the foundational basis of higher-level human intelligence that existing MLLMs excel at, but such excellency do not translate into a general domain (Mitchell, 2020; 2021; Shiffrin & Mitchell, 2023). The performance of MLLMs on core cognitive tasks therefore provides a more profound insight into their true capacities for knowledge, reasoning, and perception. This, in turn, serves as a critical indicator of whether MLLMs possess genuine intelligence or if they are merely \"stochastic parrots\u201d that depend on learning spurious correlations. To this end, we draw on theoretical and empirical approaches from developmental science to create benchmarks that evaluate core cognitive abilities in large vision-language models, examining the relationships between these abilities. On a high level, we follow Jean Piaget's theory of cognitive development, which identifies four stages in children: sensorimotor, preoperational, concrete operational, and formal operational (Piaget, 1950; Piaget & Inhelder, 1969; 1974). During the sensorimotor stage, infants acquire knowledge through sensory experiences and actions, developing an understanding of basic object properties, such as permanence, continuity, and boundaries. In the preoperational stage, symbolic representation emerges, along with a grasp of basic physical properties. The concrete operational stage is characterized by the development of logical thinking and an understanding of intuitive physics. Finally, the formal operational stage introduces more advanced cognitive abilities, including abstraction, hypothetical reasoning, counterfactual thinking, and tool use. The interdependence and developmental trajectories of these abilities can be mapped in terms of a tree-like structure (as illustrated in Fig. 1).\nTo evaluate the performance of MLLMs on the core cognitive abilities, we curate the first-ever vision cognitive development benchmark, termed as CogDeveop2K, which consists of a total of 2519 questions with 2517 images and 455 videos. Then, we evaluate 46 MLLM models on our benchmark that spans all four stages of cognitive development. We introduce a novel multi-frame question format to evaluate models' co-reference, cognitive reasoning and temporal understanding capability simultaneously. Forty-seven models are compared against a human baseline under zero-shot conditions using 11 different prompts (including no prompt). Surprisingly, while prompts can boost model performance by 8.1%, models still demonstrate reversed trends in cognitive development against those observed in children. For example, GPT series perform better in formal operation stage while performing worse in concrete operation stage."}, {"title": "RELATED WORK", "content": null}, {"title": "COGDEVELOP2K", "content": null}, {"title": "MULTI-MODAL LARGE LANGUAGE MODELS", "content": "The Vision Language Model (VLM) has a long history from Convolution Neural Networks (CNN) and Recurrent Neural Networks (RNN) (Karpathy & Fei-Fei, 2014; Vinyals et al., 2015) to unified modeling of visual and text modality with transformers (Li et al., 2019; Xu et al., 2023; Tan & Bansal, 2019; Alayrac et al., 2022; Radford et al., 2021). With the advancement of Large Language Models (LLMs), existing state-of-the-art MLLMs (Liu et al., 2024; Li et al., 2023) adopt open-sourced Large Language Models such as Llama (Touvron et al., 2023), Mistral (Jiang et al., 2023), etc. Instruction Tuning is also introduced to further improve the task generalization ability of"}, {"title": "HUMAN COGNITIVE DEVELOPMENT", "content": "The sensorimotor stage is the first stage of cognitive development proposed by Jean Piaget (Piaget, 1952; Piaget & Inhelder, 1974). Spanning from birth to approximately 2 years of age, this stage is characterized by infants' understanding of the world through their sensory experiences and motor actions. Several prominent features of human intelligence develop during this period. First, infants develop object permanence, that they realize objects and people continue to exist even when not in direct sight, or being heard or touched (Baillargeon et al., 1985). They start to understand that there is a sense of continuity for the ways that objects exist, and the inductive bias of continuity is essential, e.g., for recognizing objects when occluded or for continuously tracking objects (Spelke et al., 1995; Le Poidevin, 2000). Infants also develop the sense of boundary during this stage, namely, the ability to recognize where one object ends and another begins (Kestenbaum et al., 1987; Jackendoff, 1991). Lastly, infants develop spatial and perceptual constancy by the end of sensorimotor stage. Spatiality refers to the ability to perceive the position and distance of objects relative to oneself and each other, and recognize the spatial invariance between them when presented by various sensory experiences (Hermer & Spelke, 1996; Bell & Adams, 1999).\nPreoperational and concrete operational stage are the second and third stage of Piaget's cognitive development. Typically spanning over 2 to 7 years of age, preoperational stage is the transitional stage to concrete operational stage, which children enters around 7 years of age. During then, children begin to develop internalized mental actions supported by organized structures that can be manipulated and reversed in systematic ways, known as mental operatios (Janet, 1905; Kirkpatrick, 1908; Piaget, 1950; Piaget & Inhelder, 2014; Miller, 2016). Through mental operations, children are then able to rigidly perform tasks that are previously unreachable, such as thinking from other people's perspectives, understanding hierarchical relations of objects, and reasoning about physical events in the world. These tasks require not only rudimentary understandings of physical concepts, which gradually became in place during preoperational stage, but also relational and transformational reasoning that can only be done through mental operations (Piaget & Inhelder, 1974; Church & Goldin-Meadow, 1986; Houd\u00e9, 1997). Since preoperational stage is mostly meaningful as the transitional period preceding concrete operational stage, we do not have evaluation dimensions specifically targeting the stage. However, tasks targeting concrete operational stage could assess presentations of knowledge associated with preoperational stage, as prominently illustrated by the law of conservation (Piaget, 1952; Halford, 2011; Houd\u00e9, 1997).\nThe formal operational stage is the fourth and final stage in Piaget's theory of cognitive development, typically emerging around 11 or 12 years of age and continuing into adulthood (Inhelder & Piaget, 1958). Starting in this stage, one is able to systematic and flexibly apply mental operations to not only concrete, physical domains but also abstract, formal domains (Kuhn & Angelev, 1976; Shayer, 1979; Huitt & Hummel, 2003). Foremost, this stage is characterized by the development of complex thinking and reasoning abilities, such as abstraction, pattern recognition, the employment"}, {"title": "EVALUATION DIMENSION", "content": "Boundary Boundary refers to the cognitive recognition of where one object ends and another begins, an essential aspect of perceiving and understanding the physical world (Kestenbaum et al., 1987). Without understanding boundary, which means where the object ends, it seems very hard to construct a concept of object (Berkeley, 1709; Jackendoff, 1991).\nSpatiality Spatiality, particularly demonstrated through the A-not-B task, involves a child's understanding of the location of objects in relation to their environment (Bell & Adams, 1999). In a classic A-not-B task, an object is hidden at location A (such as under a cup) and the child successfully finds it several times. Then, the object is visibly moved to a different location B (under a different cup), in full view of the child. Younger infants often make the error of searching for the object at the original location A, indicating a developmental stage where their understanding of object spatiality is still forming."}, {"title": "Hierarchical Relation", "content": "Hierarchical relation refers to the cognitive phenomena that children begin to understand hierarchical relations and be able to organize objects or concepts into structured categories and subcategories, which are supported by the development of mental operations marked by class inclusion and transitivity (Shipley, 1979; Winer, 1980; Chapman & McBride, 1992). Class inclusion refers to the ability to recognize that some classes or groups of objects are subsets of a larger class. For example, a child in the concrete operational stage is able to understand that all roses are flowers, but not all flowers are roses Borst et al. (2013); Politzer (2016). This concept is essential for one's systematic and logical organizations of conceptual knowledge. Transitivity refers to the ability to understand logical sequences and relationships between objects (Andrews & Halford, 1998; Wright & Smailes, 2015). For instance, if a child knows that Stick A is longer than Stick B, and Stick B is longer than Stick C, they can deduce that Stick A is longer than Stick C."}, {"title": "Intuitive Physics", "content": "Intuitive physics refers to the ability of humans to predict, interact with, and make assumptions about the physical behavior of objects in their world (Michotte, 1963). As children grow, they transition from simplistic understandings, such as expecting unsupported objects to fall, to more complex theories, such as grasping the principles of inertia (Spelke et al., 1994; Kim & Spelke, 1999) and gravity (Vasta & Liben, 1996; Kim & Spelke, 1999; Li et al., 1999)."}, {"title": "Intention Understanding", "content": "Intention understanding involves recognizing and interpreting the actions of others (Searle, 1979; Rosenthal, 1991). This process is not just about observing a behavior but also about understanding the goal behind it (Baker et al., 2009; Gandhi et al., 2021). For example, seeing someone reaching for a cup is not just about recognizing the physical action but understanding the intention behind it (e.g., they want to drink)."}, {"title": "Mechanical Reasoning", "content": "Mechanical reasoning refers to the ability to understand and apply mechanical concepts and logical principles to solve problems (Allen et al., 2020). This cognitive concept first involves the ability to interpret and predict the behaviors of complex physical systems and understand how different mechanisms of the systems work. Second, mechanical reasoning requires the ability to apply logic rules (O'Brien & Shapiro, 1968; Cesana-Arlotti et al., 2018), such as induction, abduction, syllogism, Modus Ponens and Modus Tollens, and reasoning forms (Byrne, 2016), such as hypotheticals and counterfactuals, figure out how to manipulate these systems to achieve a desired outcome (Hegarty, 2004)."}, {"title": "Tool Using", "content": "Tool using refers to the ability to manipulate objects in its environment as aids in achieving a specific goal, such as obtaining food or modifying the surroundings. A lot of cognitive components involved in tool using ability, such as affordances, referring to computing the action possibilities offered to the agent by the tool with reference to the agent sensorimotor capabilities (Gibson, 1979). For example, a door handle affords pulling or pushing, indicating how the door should be operated."}, {"title": "DATA SOURCE", "content": "CogDevelop2K comprises 2517 images and 445 videos with multimodal options and questions, crawled primarily from networks as well as self-recorded content. For example, concept intentionality were collected from platforms, including Wikipedia, Reddit, Twitter, Quora, and TieBa. Some of the options were adapted from user comments to ensure content diversity and relevance. Videos for intuitive physics were either self-recorded or produced using Physion\u00b9.\nAll concept questions were annotated by four researchers with cognitive science and computer science background, then reviewed by two independent researchers. For a question to pass the screening stage, a minimum correctness rate of 95% was required from both reviewers."}, {"title": "DATASET DESIGN", "content": "Existing datasets typically support only one question-answer format or single modality type, which hinders the assessment of reasoning capabilities across different modalities within the same domain."}, {"title": "Addressing Weak Image-Text Correlation and Imbalance", "content": "In existing interleaved image-text datasets, the correspondence between images and text is often loose, and text provides marginal information for image modeling. This imbalance can cause models to over-rely on textual information, especially when text segments are lengthy (Lin et al. (2023)). To address this issue and focus on the image understanding abilities of the model, we eliminated sentences that describe the image, such as 'A. an oil paint \u00a1img\u00bf\u201d This ensures that the textual information is highly relevant to but does not overlap the image content."}, {"title": "Testing Co-Reference, Reasoning, and Temporal Understanding with novel Multi-Frame Questions", "content": "Multi-frame questions in CogDevelop can simultaneously evaluate a model's three inference ability: Co-Reference, Reasoning, and Temporal Understanding (Jiang et al. (2024)). Co-reference involves linking natural language descriptions with specific image inputs (e.g., \"the first image\" or \"A.<img 1>\"). Reasoning requires models to make decisions based on cognitive knowledge, such as describing spatial relationships. Temporal Understanding, on the other hand, tests the model's capability to comprehend sequences of frames in terms of temporal order (multi-frame) and correlation (multi-view) (Li et al. (2024b)). Existing interleaved multi-image datasets can not adequately test all three properties simultaneously. For example, video datasets with temporal information often include only a single video, while multi-image datasets that require co-reference lack temporal dependencies. To address this, CogDevelop introduces multi-video interleaving and video-image interleave formats (multi-frame) to evaluate all three properties concurrently. The statistics of the dataset are presented in Table 2."}, {"title": "EVALUATION STRATEGY", "content": "We comprehensively evaluate models' capability of cognitive reasoning using 46 multi-image interleave MLLMs with 11 different promopts. The two evaluation baselines are outlined as follows:\nHuman baseline We recruited 22 participants, all of whom are Chinese college students proficient in English. Each annotator was asked to label 2 to 6 concepts, with each concept being annotated by two or more annotators. Participants were instructed to skip a question if the question is worded ambiguously or is too complicated to answer in 90 seconds.\nZero-Shot-4482-Circular Baseline Similar to Lu et al. (2022), the zero-shot setup follows the format of Q(M)T \u2192 A, where the input includes the question text (Q), task description (T), and multiple options (M) concatenated as tokens, with the output being the predicted answer (A). Given that model predictions can exhibit bias in multiple-choice settings, we implemented circular evaluation as baseline. In circular evaluation, all answer options are shifted one position at a time, ensuring that the correct answer appears in each option slot. Only when the model correctly predicts all shifted answers is it considered accurate. All images and videos were resized to 4882. (Liu et al. (2023)).\nPrompts Strategically crafted prompts can enhance model performance, regardless of whether fine-tuning is applied (Bsharat et al. (2023); Yang et al. (2023)). In contrast to Science Q&A datasets, where image captions are incorporated as the context in the prompts, this approach can cause models to over-rely on text rather than reasoning about the image content. To mitigate this, we use image-independent contexts, such as relevant concept introductions and character assignments, which encourage models to reason beyond the provided textual information. The prompts we used can be categorized into leading words, deeper thing, role assignment, reward/penalty, and explanation. Detailed results can be found in Sec 4."}, {"title": "RESULTS AND DISCUSSION", "content": "We systematically evaluate 48 Multi-modal Large Language Models on the CogDevelop2K benchmark, which spans 12 cognitive concepts designed to assess a broad range of the developmental trajectory of Multi-modal Large Language Models. These abilities substantiate core cognition ranging from object permanence and boundary to mechanical reasoning and intentionality understanding. The models were tested across multiple question formats and ten prompt variations, yielding a comprehensive assessment of their core cognition. For example, in the sensorimotor stage, GPT families show moderate performance, with accuracy scores between 0.4 and 0.6. In the concrete operational stage, GPT families show lower performance, with accuracy scores between 0.2 and 0.4. Nevertheless, in the formal operational stage, GPT families show stronger performance, with accuracy scores between 0.6 and 0.8. Surprisingly, we find an inverse cognitive developmental trajectory compared to humans in more advanced models, which are typically regarded as state-of-the-art (Fig. 3 and Fig. 4).\nInfluence of Prompts. We investigate the influence of different prompting techniques on the performance of MLLMs on our benchmark. As illustrated in Table 2, we explore 10 different prompting techniques (divided into 5 categories). We observe that most prompts are useful on our benchmark, increasing the averaged performance by at least 1%. Concept explanation, which offers a clearer context of the question to the MLLMs, surpasses all the other prompts by at least 6%."}, {"title": "COGNITIVE DISCUSSIONS", "content": "We have demonstrated that MLLMs exhibit reverse cognitive development. Namely, they are systematically proficient at complex tasks that are typically understood to require abilities underlying simple tasks that they perform poorly. This surprising finding could appear as challenge to the current foundational architecture of MLLMs as a long-term solution to achieve human-like general intelligence (Summerfield, 2022).\nOur finding complement earlier research which raises worry that large language models may be \"stochastic parrots\u201d that merely link words and sentences together based on probabilistics but do not understand meanings and logic (Searle, 1980; Bender et al., 2021). If an intelligent agent truly understand that changes in spatial arrangement do not affect quantity, it is logically impossible for it to count correctly the amount of coins when the transformation is shown, while count wrongly when the transformation is not shown (Fig. 6). Contradictions like this reveal that MLLMs virtually do not understand the answers they produce when tackling cognitive reasoning tasks. If this is indeed the case, it may account for a variety of difficulties that MLLMs encounter, particularly in achieving robustness across changing task situations (Zhao et al., 2024).\nThe developmental trajectory of human cognition is marked by complex cognitive abilities being grounded upon extremely robust understandings of a series of foundational concepts, namely core knowledge (Spelke, 2000; Spelke & Kinzler, 2007). Through early stages of development, children exhibit rudimentary yet stable understandings of objects, actions, number, space, and social partners, each dimension lays the foundations for the acquisitions of complex abilities in later life. It has been suggested that core knowledge is precisely what supports the robustness of human cognition instantiated in commonsense reasoning (Mitchell, 2021). In reverse, the inability to implement core knowledge in artificial intelligence models prevent them from achieving human-level robustness in performances, even if such models seem to excel at certain complex cognitive reasoning tasks (Mitchell, 2020; 2021; Shiffrin & Mitchell, 2023; Palmarini & Mitchell, 2024). MLLMs' poor performances on foundational concepts like spatiality, permanence, continuity, and perspective, those that directly reflect upon grasps of core knowledge, while achieve proficiency in complex concepts like tool using and intention understanding exactly exemplifies this concern.\nTo summarize, MLLMs' performance on cognitive reasoning tasks significantly diverges from that of humans, namely in terms of having a reverse developmental trajectory between simple and complex abilities. This highlights the concerns that MLLMs do not genuinely understand meanings, which require the grounding of human singular experiences (Turing, 1950; Wittgenstein, 1958; Dennett, 1969; Searle, 1980)."}, {"title": "CONCLUSION", "content": "In this paper, we explored the cognitive capabilities of Multi-modal Large Language Models (MLLMs) through the lens of core cognitive abilities that underpin human intelligence. By introducing CogDevelop2K, a novel benchmark that spans 12 subconcepts across developmental stages, we aimed to assess the fundamental understanding and reasoning capacities of MLLMs. Our evaluation of 46 models revealed intriguing insights, including a reversed cognitive developmental trajectory compared to humans. This finding raises questions about whether MLLMs truly comprehend tasks or simply exhibit performance without genuine understanding. These results underscore the need for further investigation into the cognitive foundations of MLLMs, as well as the influence of evaluation strategies and prompting techniques in shaping their outcomes. Ultimately, this study serves as a step toward unraveling the nature of MLLM intelligence and their potential limitations in mirroring human cognitive development."}]}