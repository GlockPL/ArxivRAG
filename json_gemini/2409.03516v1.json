{"title": "LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution", "authors": ["Jeongsoo Kim", "Jongho Nang", "Junsuk Choe"], "abstract": "Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have demonstrated impressive performance. However, they suffer from significant complexity, resulting in high inference times and memory usage. Additionally, ViT models using Window Self-Attention (WSA) face challenges in processing regions outside their windows. To address these issues, we propose the Low-to-high Multi-Level Transformer (LMLT), which employs attention with varying feature sizes for each head. LMLT divides image features along the channel dimension, gradually reduces spatial size for lower heads, and applies self-attention to each head. This approach effectively captures both local and global information. By integrating the results from lower heads into higher heads, LMLT overcomes the window boundary issues in self-attention. Extensive experiments show that our model significantly reduces inference time and GPU memory usage while maintaining or even surpassing the performance of state-of-the-art ViT-based Image Super-Resolution methods. Our codes are availiable at https://github.com/jwgdmkj/LMLT.", "sections": [{"title": "Introduction", "content": "Single Image Super-Resolution (SISR) is a technique that converts low-resolution images into high-resolution ones and has been actively researched in the field of computer vision. Traditional methods, such as nearest neighbor interpolation and bilinear interpolation, were used in the past, but recent super-resolution research has seen significant performance improvements, particularly through CNN-based methods [10, 63, 26] and Vision Transformer (ViT)-based methods [32, 64, 8].\nSince the introduction of SRCNN [10], CNN-based image super-resolution architectures have advanced by utilizing multiple convolutional layers to understand contexts at various scales. These architectures deliver this understanding through residual and/or dense connections [51, 66, 60, 48, 29].\nHowever, super-resolution using CNNs faces several issues in terms of performance and efficiency. Firstly, CNN-based models can become excessively complex and deep to improve performance, leading to increased model size and memory usage [46, 63, 62]. To mitigate this, several models share parameters between modules [1, 48], but this approach does not guarantee efficiency during inference [47]. SAFMN [47] addresses the balance between accuracy and complexity by partitioning image features using a multi-head approach [52] and implementing non-local feature relationships at various scales. However, it struggles to capture long-range dependencies due to limited kernel sizes.\nViT-based models have shown superior performance compared to CNN-based models by effectively modeling global context interactions [8, 64]. For example, SwinIR [32] utilized the Swin Transformer [35] for image super-resolution, demonstrating the effectiveness of the transformer"}, {"title": "Related Works", "content": "CNN-Based Image Super-Resolution is one of the most popular deep learning-based methods for enhancing image resolution. Since SRCNN [10] introduced a method to restore high-resolution (HR) images using three end-to-end layers, advancements like VDSR [26] and DRCN [27] have leveraged deeper neural network structures. These methods introduced recursive neural network structures to produce higher quality results. ESPCN [44] significantly improved the speed of super-resolution by replacing bicubic-filter upsampling with sub-pixel convolution, a technique adopted in several subsequent works [32, 69, 15]. To address the limited receptive field of CNNs, some researchers incorporated attention mechanisms into super-resolution models to capture larger areas. RCAN [65] applied channel attention to adaptively readjust the features of each channel, while SAN [9] used a second-order attention mechanism to capture more long-distance spatial contextual information. CSFM [21] dynamically modulated channel-wise and spatial attention, allowing the model to selectively emphasize various global and local features of the image. We use the same window size, similar to CNN kernels, but vary the spatial size of each feature. This allows our model to dynamically capture both local and global information by obtaining global information from smaller spatial sizes and local information from larger spatial sizes.\nViT-Based Image Super-Resolution has surpassed the performance of CNN-based models by efficiently modeling long-range dependencies and capturing global interactions between contexts [32, 46]. After the success of ViT [13] in various fields such as classification [35, 12], object detection [4, 57], and semantic segmentation [56, 45], several models have aimed to use it for low-level vision tasks. IPT [5] constructed a Transformer-based large-scale pre-trained model for image processing. However, the complexity of ViT grows quadratically with input size. To mitigate this, many approaches have aimed to reduce computational load while capturing both local and global information. For example, SwinIR [32] used the Swin-Transformer [35] model for image reconstruction. Restormer [58] organized self-attention in the channel direction to maintain global information and achieve high performance in image denoising. HAT [6] combined self-attention, which captures representative information, with channel attention, which holds global information. To combine local and global information without adding extra complexity, we add features from lower heads, which contain global information, to upper heads, which contain local information. This enables the windows to see beyond their own area and cover a larger region.\nEfficient Image Super-Resolution research focuses on making super-resolution models more efficient. The CNN-based model FSRCNN [11] improved on SRCNN [10] by removing the bicubic interpolation pre-processing and increasing the scale through deconvolution, greatly speeding up computation. CARN [1] reused features at various stages through cascading residual blocks connected in a multi-stage manner. IMDN [24] progressively refined features passing through the network. However, improving performance often requires stacking many convolution layers, leading to increased computational load and memory usage.\nIn contrast, the ViT-based model ELAN [64] aimed to enhance spatial adaptability by using various window sizes in self-attention. HNCT [15] integrated CNN and Transformer structures to extract local features with global dependencies. NGswin [8] addressed the cross-window communication problem of the original Swin Transformer [35] by applying Ngram [40]. Despite these advances, the considerable computational load of overly deep stacked self-attention mechanisms still constrains the efficiency of ViT-based super-resolution models. To address computational load, we connect self-attention layers in parallel, integrating multi-head and depth (number of layers) to lighten the computation. This, along with reducing the spatial size of features, makes our model more efficient.\nAdditionally, efforts to lighten networks through methods such as knowledge distillation [19], model quantization [25], or pruning [67, 53] have been made. Some approaches differentiate between classical and lightweight image super-resolution models by using the same architecture but varying hyperparameters, such as the number of network blocks or feature channels [69, 7, 31]."}, {"title": "Proposed Method", "content": "Overall Architecture (Figure 2(a)). First, we use a 3 \u00d7 3 convolution to extract shallow-level features from the image. Next, we stack multiple LHS Blocks (Low-to-High Self-attention Blocks) to extract deep-level features. In each LHS Block, the features go through Layer Normalization"}, {"title": "Low-to-high Multi-Level Transformer (Figure 2(b)).", "content": "LMLT operates within the LHS Block. After features pass through the first LN [2], we divide them into H heads using a Multi-Head approach [52] and pool each split feature to a specified size. Specifically, the feature for the uppermost head is not pooled, and as we move to lower heads, the pooling becomes stronger, with the height and width halved for each subsequent head. Each feature then undergoes a self-attention mechanism. The output of the self-attention is interpolated to the size of the upper head's feature and added element-wise (called a low-to-high connection). The upper head then undergoes the self-attention process again, continuing up to the topmost head. Finally, the self-attention outputs of all heads are restored to their original size, concatenated, and merged through a 1 \u00d7 1 convolution before being multiplied with the original feature."}, {"title": "Attention Layer (Figure 2(c)).", "content": "In each attention layer, the feature is divided into N non-overlapping windows. The dot product of the query and key is calculated, followed by the dot product with the value. LePE [12] is used as the Positional Encoding and added to the value. The output is upscaled by a factor of 2 and sequentially passed to the upper head until it reaches the topmost head."}, {"title": "How the Proposed Method Works?", "content": "Our proposed LMLT effectively captures both local and global regions. As seen in Figure 3, even if the window size is the same, the viewing area changes with different spatial sizes of the feature. Specifically, when self-attention is applied to smaller features, global information can be obtained. As the spatial size increases, the red window in the larger feature can utilize information beyond its own limits for self-attention calculation because it has already acquired information from other regions in the previous stage. This combination of lower heads capturing global context and upper heads capturing local context secures cross-window communication. Figure 4 visualizes the type of information each head captures. From Figure 4(a) to 4(d), the features extracted from each head when H is assumed to be 4 are visualized by averaging them along the channel dimension. The first head (4(a)) captures relatively local patterns, while fourth head (4(d)) captures global patterns. In Figure 4(e), these local and global patterns are combined to provide a comprehensive representation. By merging this with the original feature (4(f)), it emphasizes the parts that are important for super-resolution."}, {"title": "Computational Complexity.", "content": "We improve the model's efficiency by connecting self-attention layers in parallel and reducing spatial size. In the proposed model, given a feature F \u2208 R^{H\u00d7W\u00d7D} and a fixed window size of M \u00d7 M, the number of windows in our LMLT is reduced by one-fourth as we move to lower heads, by halving the spatial size of the feature map. Additionally, since each head"}, {"title": "Experiments", "content": "Datasets. Following previous studies [32, 46, 15], we use DIV2K [50], consisting of 800 images, and Flickr2K [33], consisting of 2,650 images, as training datasets. For testing, we use the Set5 [3], Set14 [59], BSD100 [41], Urban100 [23], and Manga109 [42] datasets.\nImplementation Details. We categorize our model into four types: a Tiny model with 36 channels, a Small model with 36 channels and 12 blocks, a Base model with 60 channels, and a Large model with 84 channels. First, the low-resolution (LR) images used as training inputs are cropped into 64 \u00d7 64 patches. Rotation and horizontal flip augmentations are applied to this training data. The number of blocks, heads, and growth ratio are set to 8 (except for the Small model), 4, and 2, respectively. We use the Adam Optimizer [28] with \u03b2\u2081 = 0.9 and B\u2082 = 0.99, running for 500,000 iterations. The initial learning rate is set to 1 \u00d7 10\u207b\u00b3 and is reduced to at least 1 \u00d7 10\u207b\u2075 using the cosine annealing scheme [36]. To accelerate the speed of our experiments, we set backends.cudnn.benchmark to True and backends.cudnn.deterministic to False for the 36-channel model. To account for potential variability in the results due to this setting, we conduct three separate experiments with the LMLT-Tiny model and reported the average of these results. All other experiments are conducted only once."}, {"title": "Comparisons with State-of-the-Art Methods", "content": "Image Reconstruction Comparisons. To evaluate the performance of the proposed model, we compare our models with other state-of-the-art efficient and lightweight SR models at different scaling factors. PSNR, SSIM [55], the number of parameters, and FLOPs are used as the main performance evaluation metrics. Note that FLOPs refer to the computational amount required to create an image with a resolution of 1280\u00d7720.\nWe first compare the LMLT-Base with IMDN [24], LatticeNet [39], RFDN-L [34], SRPN-Lite [67], HNCT [15], FMEN [14], and NGswin [8]. Table 1 shows that our LMLT-Base achieves the best or second-best performance on most benchmark datasets. Notably, we observe a significant performance increase on the Manga109 dataset, while our model uses up to 30% fewer parameters compared to"}, {"title": "Memory and Running Time Comparisons.", "content": "To test the efficiency of the proposed model, we compare the performance of our LMLT model against other ViT-based state-of-the-art super-resolution models at different scales. We evaluate LMLT-Base against NGswin [8] and HNCT [15], and LMLT-Large against SwinIR-light [32] and SRFormer-light [69]. The results are shown in Table 2.\nWe observe that LMLT-Base and LMLT-Large is quite efficient in terms of inference speed and memory usage compared to other ViT-based SR models. Specifically, compared to NGswin [8], our LMLT-Base maintains similar performance while reducing memory usage by 61%, 62%, and 61% for x2, x3, and \u00d74 scales, respectively, and decreasing inference time by an average of 78%,76%, and 78%. Similarly, when comparing SwinIR [32] and our LMLT-Large, despite maintaining similar performance, memory usage decreases by 44%, 43%, and 46% for each scale, respectively, and inference time decreases by an average of 87%, 80%, and 81%. This demonstrates both the efficiency and effectiveness of the proposed model."}, {"title": "Effects of Low-to-high Connection", "content": "We examine the effects of the low-to-high element-wise sum (low-to-high connection) and downsizing elements of our proposed model. As shown in Table 4, the low-to-high connection yields significant results. Specifically, on the Urban100 [23] dataset, PSNR increases by 0.04 dB to 0.05 dB across all scales, and SSIM [55] increases by nearly 0.0011 at the \u00d74 scale, demonstrating the benefits of including the low-to-high connection. Appendix B visualizes the differences in features on Urban100 [23] with and without the low-to-high connection, showing that it significantly reduces the boundary lines between windows. Additionally, experiments adding the proposed low-to-high connection to SAFMN [47] for x2, x3, and \u00d74 scales are also provided in Appendix B."}, {"title": "Effects of Multi-Scale Heads", "content": "Table 5 validates the effectiveness of using multiple scales by experimenting with cases where pooling is not applied to any head. Specifically, we compare our proposed LMLT with cases where pooling and the low-to-high connection are not applied, as well as cases where merging is also not performed. The results show that performance is lower in all cases compared to the proposed model. Appendix B demonstrates that when pooling is not applied, the"}, {"title": "Conclusion", "content": "In this paper, we introduced the Low-to-high Multi-Level Transformer (LMLT) for efficient image super-resolution. By combining multi-head and depth reduction, our model addresses the excessive computational load and memory usage of traditional ViT models. In addition to this, LMLT applies self-attention to features at various scales, aggregating lower head outputs to inform higher heads, thus solving the cross-window communication issue. Our extensive experiments demonstrate that LMLT achieves a favorable balance between model complexity and performance, significantly reducing memory usage and inference time while maintaining or improving image reconstruction quality. This makes the proposed LMLT a highly efficient solution for image super resolution tasks, suitable for deployment on resource-constrained devices."}, {"title": "Impact of number of Blocks, Channels, Heads and Depths", "content": "In this section, we analyze how the performance of our proposed model changes based on the number of blocks, heads, channels and depths.\nAs shown in Table A, the increase in the number of parameters, FLOPs and inference time tends to be proportional to the number of blocks, and performance also gradually improves. For the Manga109 [42] dataset, as the number of blocks increases from 4 to 12 in increments of 2, PSNR increases by 0.27 db, 0.16 db, 0.10 db, and 0.10 db, respectively. Interestingly, despite the increase in the number of blocks from 4 to 12, the GPU memory usage remains almost unchanged. While the number of parameters nearly triples, the GPU memory usage remains stable, 323.5M to 324.5M. We observe the overall increase in PSNR with the increase in the number of blocks and designate the model with 8 blocks as LMLT-Tiny and the model with 12 blocks as LMLT-Small.\nAs shown in Table B, LMLT's performance increases with channels, along with parameters and FLOPS. However, unlike the variations in the number of blocks, increasing the number of channels results in a more significant increase in the number of parameters, FLOPs, and memory usage. Inference time, however, increases proportionally with the number of channels. For instance, with 36 channels, the average inference time is 57.16ms, and when doubled, it requires approximately 108.74ms, nearly twice the time. As the number of channels increases from 24 to 84 in increments of 12, the PSNR on the Urban100 [23] dataset increases by 0.42 db, 0.29 db, 0.19 db, 0.13 db, and 0.10 db, respectively. Based on the overall performance increase, we designate the model with 60 channels as the Base model and the model with 84 channels as the Large model. In this context, the Small model has an inference time about 3ms longer than the Base model, but it has fewer parameters, lower memory usage, and fewer FLOPs, thus justifying its designation.\nIn this paragraph, we compare the performance differences based on the number of heads. In our model, as the number of heads decreases, the channel and the number of downsizing operations for each head decrease. For example, in our baseline with 4 heads and 36 channels, the lowest head has a total of 9 channels and is pooled 3 times. However, if there are 2 heads, the lowest head has 18 channels and is pooled once. Additionally, the maximum pooling times and the number of heads are related to the number of windows and the amount of self-attention computation. According to equation 1, as the number of heads decreases, the computation increases. As a result, as the number of heads decreases, the number of parameters, FLOPs, and GPU memory usage increase.\nAs shown in Table C, the performance with 4 heads and 3 heads is similar across all scales and test datasets. However, when the number of heads is reduced to 1, the performance drops significantly. This difference is particularly noticeable in the Urban100 [23] dataset, where at scale \u00d72, the performance with 4 heads is 32.04 db, whereas with 1 head, it drops to 31.93 db, a decrease of 0.11 db. Additionally, when the scale is \u00d73 and \u00d74, the PSNR decreases by 0.05 dB and 0.04 dB, respectively. This indicates that even if the spatial size of all"}, {"title": "Effects of Low-to-high connection and Pooling", "content": "Difference between with and without Low-to-high connection In the Table 4, we confirm performance differences when the low-to-high connection is not applied to LMLT. Inspired by this, we also apply low-to-high connections between heads in SAFMN [47] and verify the experimental results. Table E shows that adding low-to-high connection to the upper head in SAFMN [47] does not yield significant performance differences. Moreover, at the \u00d74 scale, the SSIM for the Urban100 [23] and Set5 [3] datasets decreases by 0.0010 and 0.0011, respectively, indicating a reduction in performance.\nWe then visualize the features of LMLT-Tiny to understand the effect of the low-to-high connection. Each column of Figure A illustrates the original image, the aggregated feature visualization of LMLT-Tiny combining all heads A(a), and the aggregated feature visualization of LMLT-Tiny without low-to-high connection A(b). In A(b), the images show pronounced boundaries in areas such as stairs, buildings, and the sky. In contrast, A(a) shows these boundaries as less pronounced. This demonstrates that the low-to-high connection can address the border communication issues inherent in WSA.\nDifference between with and without Pooling We analyze the impact of pooling on the performance of super-resolution. As observed in Table 5, even though pooling preserves spatial information, the overall performance decreases when it is not applied. We investigate the reason behind this through feature visualization. Figure B visualizes the features when no pooling is applied to any head in LMLT. The leftmost image is the"}, {"title": "Impact of Activation function", "content": "In Table 6, we discuss that not applying the activation function GeLU [20] might improve performance. Therefore, we experiment with LMLT and LMLT without GeLU [20] across various scales and channels to confirm the results.\nTable F shows the results for our LMLT and the model without the activation function across different scales and channels. As shown, with 36 channels, there is minimal performance difference across all scales, with the largest being a 0.04 higher PSNR on the Set5 [3] \u00d74 scale when GeLU [20] is removed. However, when expanded to 60 channels, our LMLT performs better on most benchmark datasets for both \u00d73 and \u00d74 scales. Specifically, on"}, {"title": "LAM and ERF Comparisons", "content": "To verify whether the proposed LMLT exhibits a wider receptive field, we utilize local attribution map (LAM) [16] and effective receptive field (ERF) [38]. Specifically, we use LAM to show that our proposed LMLT-Large has a wider receptive field compared to SwinIR-Light [32] and SRFormer-Light [69]. Detailed visualizations are presented in Figure C. Additionally, SwinIR-NG [8] is included for comparison, and we visualize the ERF. The detailed results are shown in Figure D. Through these two analyses, we demonstrate that our proposed model exhibits a wider receptive field than existing ViT-based SR models."}, {"title": "CCM: Convolutional Channel Mixer", "content": "CCM instead of MLP. Since the feed-forward network (FFN) in the original transformer [52] is a fully connected layer, we assume that using it in ViT might disrupt the spatial information of the features. Therefore, we apply the convolutional channel mixer (CCM) [47] instead, an FFN based on FMBConv [49], to preserve spatial information. CCM is a module that mixes each convolution channel. Specifically, the features pass through two convolution layers. The first layer has a 3 \u00d7 3 kernel and expands the channels. Then, GELU [20] is applied for non-linear mapping. Finally, a convolution layer with a 1 \u00d7 1 kernel restores the channels to their original state. In our method, the features pass through Layer Normalization [2], LMLT, and another Layer Normalization before being input to CCM [47]. Detailed structure can be seen in Figure F."}, {"title": "Comparisons on LMLT with Other Methods", "content": "Image Reconstruction comparisons Here, We first compare the LMLT-Tiny and LMLT-Small with CARN-m, CARN [1], EDSR-baseline [33], PAN [68], LAPAR-A [30], ECBSR-M16C64 [63], SMSR [54], Shuffle-"}]}