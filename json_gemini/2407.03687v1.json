{"title": "STOC-TOT: Stochastic Tree-of-Thought with Constrained Decoding for Complex Reasoning in Multi-Hop Question Answering", "authors": ["Zhenyu Bi", "Daniel Hajialigol", "Zhongkai Sun", "Jie Hao", "Xuan Wang"], "abstract": "Multi-hop question answering (MHQA) requires a model to retrieve and integrate information from multiple passages to answer a complex question. Recent systems leverage the power of large language models and integrate evidence retrieval with reasoning prompts (e.g., chain-of-thought reasoning) for the MHQA task. However, the complexities in the question types (bridge v.s. comparison questions) and the reasoning types (sequential v.s. parallel reasonings) require more novel and fine-grained prompting methods to enhance the performance of MHQA under the zero-shot setting. In this paper, we propose STOC-TOT, a stochastic tree-of-thought reasoning prompting method with constrained decoding for MHQA and conduct a detailed comparison with other reasoning prompts on different question types and reasoning types. Specifically, we construct a tree-like reasoning structure by prompting the model to break down the original question into smaller sub-questions to form different reasoning paths. In addition, we prompt the model to provide a probability estimation for each reasoning path at each reasoning step. At answer time, we conduct constrained decoding on the model to generate more grounded answers and reduce hallucination. Experiments comparing STOC-TOT with on two MHQA datasets and five large language models showed that STOC-TOT outperforms other reasoning prompts by a significant margin.", "sections": [{"title": "Introduction", "content": "Question answering (QA) is a fundamental task in natural language processing (NLP) that involves designing systems capable of understanding human language questions and providing accurate and relevant answers. With the recent advancement of large language models (LLMs) that demonstrated superior reasoning ability (Brown et al., 2020), researchers have been focusing more on complex QA tasks, such as multi-hop question answering (MHQA). MHQA is more challenging as it requires models to understand complicated questions, perform multiple reasoning steps, and gather evidence across documents."}, {"title": "Related Work", "content": "Multi-Hop Question Answering Multi-hop Question Answering (MHQA) is a challenging task requiring models to reason over different evidence across documents to answer a complex multi-hop question. Many high-quality MHQA datasets have been developed, including HotpotQA (Yang et al., 2018), WikiHop (Welbl et al., 2018), MuSiQue (Trivedi et al., 2022), and others. Among these, HotpotQA is the task's most representative and widely used dataset. Previous state-of-the-art MHQA models often follow a two-stage pipeline: a retriever that extracts evidence from the documents, and a reader that reasons about the evidence\nLarge language models (LLMs), on the other hand, show remarkable reasoning ability and rich knowledge of general-domain questions. Many LLMs can answer simple and straightforward questions that do not require complex reasoning without any supervision involved but often fail to deal with complex questions requiring multiple reasoning steps. To tackle the problem, researchers have developed many prompting techniques to improve LLM's reasoning ability, such as chain-of-thought (CoT) (Wei et al., 2022), self-consistency CoT (Sc-CoT) (Wang et al., 2023), and tree-of-thought (ToT) prompting (Yao et al., 2023a).\nCoT has been shown effective across tasks requiring extensive, step-by-step reasoning, such as math calculation and reading comprehension. However, there could be various possible reasoning paths for many complex multi-hop questions, and CoT models cannot \"turn back\" when they have made a mistake along their reasoning paths. Sc-CoT further improves on CoT by proposing different chains of thought, thus expanding the reasoning space. However, there is no local reasoning expansion within each chain, and the \"majority voting\" strategy often fails in open-domain tasks where the output space is unlimited. ToT, designed to maintain different reasoning paths along its reasoning process, is more suitable for dealing with complex question types. However, the intermediate reasoning steps in NLP generation tasks are much less constrained and require more than a simple rule-based evaluation. The complexities in the question types (bridge v.s. comparison questions in Table 1), as well as the reasoning types (sequential v.s. parallel reasonings in Table 2), require more novel and fine-grained prompting methods to enhance the reasoning ability of LLMs.\nTo tackle the challenges and design a more reliable reasoning method for open-domain NLP tasks, we propose STOC-TOT, a stochastic ToT-based framework that instructs the model to generate different reasoning paths from the same question and assign probability scores to reasoning paths to effectively avoid reasoning dead-ends. To the best of our knowledge, our work is the first to adapt the tree-of-thought reasoning prompting to natural language tasks that require complex reasoning, such as MHQA. We provide an example overview of our framework in Figure 2. Specifically, we construct a tree-like reasoning structure by prompting the model to break down the original question into smaller sub-questions to form different reasoning paths. We evaluate the validity of each reasoning path on three levels of aspects and arrive at a model-given probability score. At answer time, we innovatively propose to use constrained decoding in the answering process to reduce hallucination by forcing the model to generate grounded answers from evidence and letting models give concise and exact answers. Ultimately, we arrive at the best answer by choosing the path with the highest aggregated probability score. Experiments on two benchmarking MHQA datasets demonstrate that STOC-TOT significantly improves the reasoning ability of LLMs in complex reasoning scenarios, especially with GPT-4, improving Exact Match accuracy by 7%, and F1 score by 7.8 points on the HotpotQA dataset over the original tree-of-thought prompting. Our contributions are as follows:\n\u2022 We propose STOC-ToT, which constructs a stochastic reasoning tree in complex reasoning scenarios. We introduce stochastic estimations on different reasoning paths, which helps the model have a more reliable reasoning process than previous reasoning prompting methods.\n\u2022 We innovatively propose to use constrained decoding in the answering process. This step reduces model hallucination by forcing the model to generate grounded answers from evidence and letting models give concise and exact answers.\n\u2022 We evaluate the effectiveness of STOC-TOT by conducting experiments on two MHQA datasets. We observe substantial improvements over other reasoning prompting methods, with STOC-TOT surpassing all other selected reasoning prompting baselines on 5 tested models."}, {"title": "Method", "content": "3.1 Task Formation\nGiven a multi-hop question Q and background corpus of evidence P, the goal of our framework is to output the answer A to question Q, drawing its reasoning with the support of multiple evidence retrieved from corpus P."}, {"title": "STOC-TOT Framework", "content": "For each of the questions Q, multiple reasoning lines and, thus, multiple ways of breaking down the question could exist. However, not every reasoning line would lead us to the right answer, and they take us to dead ends. To avoid such reasoning dead-ends, we build a stochastic reasoning tree to represent the possible reasoning lines and the probability of each reasoning line taking us to the right answer. We achieve this by proposing a self-interactive framework that automatically builds the reasoning tree given a multi-hop question. In our reasoning process, we first prompt the model to propose different possible sub-questions to solve at each reasoning step. Each sub-question corresponds to one possible reasoning path and is presented as a node in the tree. We then ask the model to answer the generated sub-questions. To prevent hallucination and make the model more focused on the given question and evidence, we build a vocabulary bank using words from the evidence list and the original question and instruct the model to do constrained decoding from the vocabulary bank when generating its answers. After answering every sub-question generated from the same question in the previous reasoning level, we prompt the model to evaluate each reasoning path and estimate how likely the reasoning path would lead us to the right answer. This probability estimation would be assigned to the corresponding node in the tree. After the reasoning process finishes, each reasoning path would have an aggregated probability calculated from nodes along the path.\nFormally, given a question Q, we instruct the model to generate sub-questions $q_1, q_2, \u2026\u2026\u2026, q_n$, and build a tree structure with the original question Q as the root node and each question $q_i$ as subsequent nodes. The tree would expand as each sub-question $q_i$ has its sub-question $q_j$, and the reasoning paths are thus represented as branches in the tree structure. From the original question Q and the evidence list $E = e_1, e_2, ..., e_n$, we build a vocabulary bank $V = [w_1, w_2, ..., w_n], w_i \u2208 Q, w_j \u2208 E$. We then prompt the model to generate their answer $a_1, a_2, ..., a_n$ using only $w_i \u2208 V$. We describe the details of our framework below.\nExample-Based Sub-Question Generation Our framework starts with the sub-question generation module, which generates sub-questions $q_1, q_2, ..., q_n$ using the question $Q_g$ from the previous reasoning level. The sub-questions are generated based on both the model's reasoning ability and the model's semantic understanding of the question $Q_g$. However, we cannot guarantee that each sub-question asked is a good sub-question, and sometimes, the generated sub-question merely repeats the previous question. We introduce the paraphrase detection module and pass on the generated sub-questions to reduce redundancy and improve question quality.\nParaphrase Detection Answering repetitive questions often leads to low-quality answers and time-consuming steps. Following the sub-question generation module, we introduce the paraphrase detection module to reduce redundancy and improve question quality. In this module, we prompt the model and ask it to distinguish informative questions from questions that merely repeat what is already stated at the previous reasoning level. If a sub-question is a paraphrase, we instruct the model to stop generating sub-questions from the current question. In other words, we prune the low-quality sub-branch of the tree that could otherwise be generated. By pruning these branches, we effectively improve the efficiency of our framework.\nEvidence Retrieval and Answering We then move on to answering the question after our paraphrase detection module. Our evidence retrieval and answering module focuses on retrieving evidence and generating answers to the given sub-question. We also pass in the full evidence list provided and prompt the model to give out an answer to the given sub-question. The evidence retrieval and answering module selects relative evidence from an evidence pool for each sub-question and uses words only from the vocabulary bank to generate its final answer. We will discuss details of constrained decoding in Section 3.3. The generated sub-answer and the answered sub-question are then passed on to the sub-question generation module at the next level to continue the reasoning process.\nValidity Estimation Not each sub-question asked is a good sub-question, and not each reasoning path is reasonable. After every sub-question $q_i$ generated from the same question $Q_g$ has been answered, we prompt the model to provide a probability estimation $p_i$ for each $(q_i, a_i)$ pair. This probability is the model's evaluation of going down the correct reasoning path. Specifically, this probability is obtained by prompting the model to consider the following three aspects:\n\u2022 Question Level: Is the question semantically clear and answerable?\n\u2022 Reasoning Level: Is the reasoning line coherent when considering previous levels?\n\u2022 Answer Level: Does the evidence fully support the answer to the question?\nAs shown in Figure 2, we conduct validity estimation for sub-questions and sub-answers in nodes 2 and 3 since the sub-questions were generated from the same question in node 1.\nAt the leaf node of our tree, we would have a final question $q_f$, along with a final answer A to the original question Q, and also an aggregated probability $P_{final} = \\Pi_i p_i$, with each $p_i$ being the probability of the nodes along the reasoning path. We assign $P_{final}$ to the leaf node, representing the aggregated probability of answer A being the correct answer to Q."}, {"title": "Constrained Decoding", "content": "One challenge for generative LLMs in the task of question answering is hallucination. LLMs often fail to pay attention to the golden evidence and hallucinate their own reference even when large amounts of evidence exist. To alleviate the problem of LLM halluscination during evidence selection and answer generation, we innovatively propose to use constrained decoding in the answering process to reduce hallucination by forcing the model to generate grounded answers from evidence and let models give concise and exact answers. As shown in Figure 2, we conduct constrained decoding by asking the model to generate words from the vocabulary bank, consisting of words taken only from the original question and the evidence list provided. More formally, we construct a vocabulary bank $V = w_1, w_2, ..., w_i$ from all words in the provided evidence sentences. We conduct a simple filtering by removing common English stop words. We then instruct the model's evidence retrieval and answering module to construct its answers using words only from the given vocabulary V.\nCode-based Constrained Decoding For open-source LLMs (e.g., Llama), we build our logit processor at the decoding time. Specifically, for every word $w_j \u2209 V$, we manually set the score to negative infinity to prevent the model from generating them. Thus, every answer generated will only use words from the evidence list.\nPrompt-based Constrained Decoding For closed-source LLMs (e.g., GPT models), since we do not have access to their decoding function, we had to instruct the GPT models using prompts to do constrained decoding. We provide our prompt template used in Appendix A."}, {"title": "Experimental Setup", "content": "Dataset We compare STOC-TOT with baseline methods on the HotpotQA dataset (Yang et al., 2018) and the MuSiQue dataset (Trivedi et al., 2022), both of which are widely used MHQA datasets across state-of-the-art MHQA baselines. The experiments are conducted under the distractor setting, where we provide the model with an evidence pool containing both golden and irrelevant evidence. The model needs to find the golden evidence to answer the question correctly. We randomly selected 200 examples from each dataset as our evaluation set.\nBaselines We included three baselines:\n\u2022 Vanilla Prompting with no examples provided. We only provide the model with questions and evidence and instruct it to output the answer.\n\u2022 Chain-of-Thought (CoT) prompting (Wei et al., 2022) with a standard input-output (IO) prompt. We design the prompt with one in-context example, which presents the whole reasoning chain, including all intermediate steps.\n\u2022 Tree-of-Thought prompting (Yao et al., 2023a) with slight modifications to adapt to the MHQA task. We largely followed the original framework and used majority voting on the reasoning lines to decide the final answer.\nWe recognize that there are LLM-based retrieval augmented generation frameworks (Yao et al., 2023b; Gou et al., 2024) that were also evaluated on HotpotQA. However, we excluded them from our baselines as they used outside knowledge bases, which are under a different testing scenario."}, {"title": "Implementation", "content": "We experiment with the baselines and our model utilizing five LLMs: GPT-3.5-turbo (Brown et al.,"}, {"title": "Results", "content": "Overall Results\nWe compare STOC-TOT with LLM baselines on the HotpotQA dataset and the MusiQue dataset and present our results in Tables 1 and 2. The backbone LLMs in our experiments include GPT3.5, GPT4, Llama2-13B, Llama2-70B, and Llama3-8B. Due to time constraints, we only tested with Llama2-70B on the HotpotQA dataset. On the HotpotQA dataset, STOC-TOT attains an on-average increase in performance of over 6 % compared with vanilla prompting on GPT models, and the improvement goes up to 11% when we further implement STOC-TOT with constrained decoding. On the more challenging MusiQue dataset, we still see an increase in performance of STOC-TOT compared with the other baselines, most notably on GPT4, where we observe an 11.5% EM improvement (from 31.50 to 42.0).\nComparison with Tree-of-Thought STOC-TOT surpasses the original Tree-of-Thought prompting by 7% with the GPT4 model on both tested datasets. For LLMs with inferior reasoning ability, such as LLaMa2-8B, we still observe a performance improvement, even on the harder MusiQue dataset. These results suggest that STOC-TOT is more effective at forming and selecting reliable reasoning paths under complex reasoning scenarios.\nConstrained Decoding Even though the LLM's reasoning ability can be improved by reasoning prompting, such techniques have little help in preventing hallucination. However, STOC-TOT implements constrained decoding, which makes the model much more grounded to evidence when answering the question, effectively addressing hallucination issues and improving the overall performance of our framework."}, {"title": "Ablation Study", "content": "Sensitivity to Demonstration Question Type\nWe study the effect on STOC-TOT performance when different types of demonstration questions are provided in the prompt template. The Hot-PotQA dataset specified two types of questions. The \"Bridge\" question contains a \"bridge entity\" that connects the question and the final answer. In"}, {"title": "Error Analysis", "content": "We conduct a detailed analysis of the errors made by our framework on GPT3 and GPT4, and present our results in Figure 5. We categorize the errors into four types: (1) No Answer: our framework did not come up with an answer for the question due to not finishing the reasoning process; (2) Intermediate Answer: our framework came up with an answer for one of the intermediate hops instead of for the final question; (3) Wrong Answer: our framework came up with an answer that is neither the final answer nor one of the intermediate answers; (4) Semantically Correct: our framework came up with the right answer, but did not have an exact match with the final answer. Appendix B shows examples of each error category. Large amounts of error cases were correct answers with extra wording or hallucination errors, signaling potential improvements over our constrained decoding scheme. Reasoning process errors, including no answer and intermediate answer, make up only 25% of the total error cases. This result shows that our framework is capable of building a robust reasoning process for complex questions."}, {"title": "Conclusion", "content": "This paper proposes STOC-TOT, a stochastic tree-of-thought reasoning framework with constrained generation for multi-hop question answering. STOC-TOT is specialized in dealing with complex reasoning scenarios in natural language tasks. Experiments on two benchmark datasets show that our framework outperforms previous reasoning prompting techniques with multiple Large Language Models. Detailed analysis shows that our framework is capable of building a robust reasoning process given different types of questions. Further research can aim to enhance the reliability of our framework by proposing better validity evaluation schemes and more effective methods for improving groundedness and preventing hallucination."}, {"title": "Limitations", "content": "Our framework relies on initiating multiple model instances and requires multiple prompts per round. The repetitive callings impose heavy time costs for our framework, even after implementing our paraphrase module. Another limitation comes from how we generated sub-questions. Currently, we directly prompt the model to generate sub-questions. A more complex standard can be used to increase the quality of the sub-questions generated. Also, more extensive experiments should be provided, including experimenting on other different datasets and case studies."}, {"title": "Ethics Statement", "content": "This research adhered to the ethical standards and best practices outlined in the ACL Code of Ethics. Language Models can sometimes produce illogical or inaccurate reasoning paths, so their outputs should be cautiously used. The outputs are only examined to understand how a model arrives at its answers and investigate why it makes certain errors. All experiments used publicly available datasets from previously published works and did not involve ethical or privacy issues."}, {"title": "Prompt Templates", "content": "Sub Question Generation Template The\nprompt template containing one comparison\nquestion and one bridge question is given below:\nprompt: Break a question into high-quality sub-\nquestions that are easier to answer. Here are two\nexamples as guidelines:\n\"Question: Are Tokyo and Busan in the same coun-\ntry? Thought 1: I could either find which country\nTokyo is located in, or which country Busan is\nlocated in. Sub Question 1-1: Which country is\nTokyo located in? Sub Question 1-2: Which coun-\ntry is Busan located in?\"\n\"Question: Tokyo is located in the country that has\nwhat colors present on its national flag? Thought\n1: I need to first find out which country Tokyo is\nlocated in. Sub Question 1-1: Which country is\nTokyo located in?\"\nOnly give out your thought process and current-\nlevel sub-questions. Do not give out answers\nto your questions. Question: Given Question.\nThought 1:\nPrompt-based Constrained Generation Tem-\nplate The prompt template at answering time is\ngiven below:\nprompt: Given a question and a list of evidence\nthat may of help, give your answer directly, using\nwords only from the vocabulary bank, without any\nexplanations.\nQuestion: Given Question. Evidence as reference:\nGiven Evidence. Vocabulary Bank: Given Vocabu-\nlary. Answer:"}, {"title": "Examples of the Error Cases", "content": "\u2022Type-2: Intermediate Answer\nQuestion:\nWhere does the hotel and casino located in which\nBill Cosby's third album was recorded?\nAnswer given by STOC-TOT on GPT4:\nLas Vegas.\nGolden Answer:\nLas Vegas Strip in Paradise.\n\u2022Type-3: Wrong Answer\nQuestion:\nAside from the Apple Remote, what other device\ncan control the program Apple Remote was\noriginally designed to interact with?\nAnswer given by STOC-TOT on GPT4:\nsiri remote and devices with netsupport manager\nsoftware\nGolden Answer:\nkeyboard function keys\n\u2022Type-4: Semantically Correct\nQuestion:\nRoger O. Egeberg was Assistant Secretary for\nHealth and Scientific Affairs during the administra-\ntion of a president that served during what years?\nAnswer given by STOC-TOT on GPT4:\n1969 to 1974\nGolden Answer:\n1969 until 1974"}]}