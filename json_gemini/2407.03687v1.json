{"title": "STOC-TOT: Stochastic Tree-of-Thought with Constrained Decoding for Complex Reasoning in Multi-Hop Question Answering", "authors": ["Zhenyu Bi", "Daniel Hajialigol", "Zhongkai Sun", "Jie Hao", "Xuan Wang"], "abstract": "Multi-hop question answering (MHQA) requires a model to retrieve and integrate information from multiple passages to answer a complex question. Recent systems leverage the power of large language models and integrate evidence retrieval with reasoning prompts (e.g., chain-of-thought reasoning) for the MHQA task. However, the complexities in the question types (bridge v.s. comparison questions) and the reasoning types (sequential v.s. parallel reasonings) require more novel and fine-grained prompting methods to enhance the performance of MHQA under the zero-shot setting. In this paper, we propose STOC-TOT, a stochastic tree-of-thought reasoning prompting method with constrained decoding for MHQA and conduct a detailed comparison with other reasoning prompts on different question types and reasoning types. Specifically, we construct a tree-like reasoning structure by prompting the model to break down the original question into smaller sub-questions to form different reasoning paths. In addition, we prompt the model to provide a probability estimation for each reasoning path at each reasoning step. At answer time, we conduct constrained decoding on the model to generate more grounded answers and reduce hallucination. Experiments comparing STOC-TOT with on two MHQA datasets and five large language models showed that STOC-TOT outperforms other reasoning prompts by a significant margin.", "sections": [{"title": "1 Introduction", "content": "Question answering (QA) is a fundamental task in natural language processing (NLP) that involves designing systems capable of understanding human language questions and providing accurate and relevant answers. With the recent advancement of large language models (LLMs) that demonstrated superior reasoning ability (Brown et al., 2020), researchers have been focusing more on complex QA tasks, such as multi-hop question answering (MHQA). MHQA is more challenging as it requires models to understand complicated questions, perform multiple reasoning steps, and gather evidence across documents. State-of-the-art methods for MHQA are fully-supervised methods that often follow a retrieve-and-read framework, including a passage retrieving module that gathers relative evidence from documents and a reading comprehension module to reason about the evidence (Zhu et al., 2021; Li et al., 2022). Other methods include beam-search (Zhang et al., 2023) and label-smoothing (Yin et al., 2023). However, these methods often require extensive pre-training or fine-tuning and do not generalize well to other datasets."}, {"title": "2 Related Work", "content": "Multi-Hop Question Answering Multi-hop Question Answering (MHQA) is a challenging task requiring models to reason over different evidence across documents to answer a complex multi-hop question. Many high-quality MHQA datasets have been developed, including HotpotQA (Yang et al., 2018), WikiHop (Welbl et al., 2018), MuSiQue (Trivedi et al., 2022), and others. Among these, HotpotQA is the task's most representative and widely used dataset. Previous state-of-the-art MHQA models often follow a two-stage pipeline: a retriever that extracts evidence from the documents, and a reader that reasons about the evidenceReasoning Prompting of LLMs Various prompt engineering methods have been developed (Wei et al., 2022; Wang et al., 2023; Yao et al., 2023a; Besta et al., 2024; Sel et al., 2024; Chen et al., 2023), aiming to improve large language models' reasoning ability across various tasks and domains. Chain-of-thought (CoT) prompting (Wei et al., 2022) prompts the large language models (LLMs) to divide their reasoning process into smaller steps when solving a question, forming a chain of thoughts. Chain-of-thought self-consistency prompting (Wang et al., 2023) improves on the CoT method by proposing different reasoning chains and ensembles on the final result. Tree-of-thought (ToT) prompting method (Yao et al., 2023a) actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves as an intermediate step toward problem-solving. Graph-of-thought (Besta et al., 2024) further improves ToT by constructing a Directed Graph instead of a tree. LLMs can loop over a thought to refine it and aggregate thoughts or chains.\nConstrained Decoding Constrained decoding is the technique that asks the models to generate outputs following a given set of rules. The most common way of conducting constrained generation uses beam search (Och and Ney, 2004) in decoding time. Before the LLM era, works on constrained decoding focused on task-specific sequence-to-sequence models that span across many fields, such as machine translation (Hokamp and Liu, 2017; Post and Vilar, 2018), named entity recognition (Lester et al., 2020), and dialogue generation (Balakrishnan et al., 2019). Recently, Microsoft introduced Guidance 1, which allows users of various large language models to control their outputs given a human-defined vocabulary or rules."}, {"title": "3 Method", "content": "3.1 Task Formation\nGiven a multi-hop question Q and background corpus of evidence P, the goal of our framework is to output the answer A to question Q, drawing its reasoning with the support of multiple evidence passages $P_1, P_2$, retrieved from corpus P.\n3.2 STOC-TOT Framework\nFor each of the questions Q, multiple reasoning lines and, thus, multiple ways of breaking down the question could exist. However, not every reasoning line would lead us to the right answer, and they take us to dead ends. To avoid such reasoning dead-ends, we build a stochastic reasoning tree to represent the possible reasoning lines and the probability of each reasoning line taking us to the right answer. We achieve this by proposing a self-interactive framework that automatically builds the reasoning tree given a multi-hop question. In our reasoning process, we first prompt the model to propose different possible sub-questions to solve at each reasoning step. Each sub-question corresponds to one possible reasoning path and is presented as a node in the tree. We then ask the model to answer the generated sub-questions. To prevent hallucination and make the model more focused on the given question and evidence, we build a vocabulary bank using words from the evidence list and the original question and instruct the model to do constrained decoding from the vocabulary bank when generating its answers. After answering every sub-question generated from the same question in the previous reasoning level, we prompt the model to evaluate each reasoning path and estimate how likely the reasoning path would lead us to the right answer. This probability estimation would be assigned to the corresponding node in the tree. After the reasoning process finishes, each reasoning path would have an aggregated probability calculated from nodes along the path.\nFormally, given a question Q, we instruct the model to generate sub-questions $q_1, q_2, \u2026\u2026\u2026, q_n$, and build a tree structure with the original question Q as the root node and each question $q_i$ as subsequent nodes. The tree would expand as each sub-question $q_i$ has its sub-question $q_j$, and the reasoning paths are thus represented as branches in the tree structure. From the original question Q and the evidence list $E = e_1, E_2, ..., e_n$, we build a vocabulary bank $V = [W_1, W_2, ..., W_n]$, $W_i \u2208 Q, w_j \u2208 E$. We then prompt the model to generate their answer $a_1, a_2, ..., a_n$ using only $w_i \u2208 V$. We describe the details of our framework below.\nExample-Based Sub-Question Generation Our framework starts with the sub-question generation module, which generates sub-questions $q_1, q_2, ..., q_n$ using the question $Q_g$ from the previous reasoning level. The sub-questions are generated based on both the model's reasoning ability and the model's semantic understanding of the question $Q_g$. However, we cannot guarantee that each sub-question asked is a good sub-question, and sometimes, the generated sub-question merely repeats the previous question. We introduce the paraphrase detection module and pass on the generated sub-questions to reduce redundancy and improve question quality.\nParaphrase Detection Answering repetitive questions often leads to low-quality answers and time-consuming steps. Following the sub-question generation module, we introduce the paraphrase detection module to reduce redundancy and improve question quality. In this module, we prompt the model and ask it to distinguish informative questions from questions that merely repeat what is already stated at the previous reasoning level. If a sub-question is a paraphrase, we instruct the model to stop generating sub-questions from the current question. In other words, we prune the low-quality sub-branch of the tree that could otherwise be generated. By pruning these branches, we effectively improve the efficiency of our framework.\nEvidence Retrieval and Answering We then move on to answering the question after our paraphrase detection module. Our evidence retrieval and answering module focuses on retrieving evidence and generating answers to the given sub-question. We also pass in the full evidence list provided and prompt the model to give out an answer to the given sub-question. The evidence retrieval and answering module selects relative evidence from an evidence pool for each sub-question and uses words only from the vocabulary bank to generate its final answer. We will discuss details of constrained decoding in Section 3.3. The generated sub-answer and the answered sub-question are then passed on to the sub-question generation module at the next level to continue the reasoning process.\nValidity Estimation Not each sub-question asked is a good sub-question, and not each reasoning path is reasonable. After every sub-question $q_i$ generated from the same question $Q_g$ has been answered, we prompt the model to provide a probability estimation $p_i$ for each $(q_i, a_i)$ pair. This probability is the model's evaluation of going down the"}, {"title": "3.3 Constrained Decoding", "content": "One challenge for generative LLMs in the task of question answering is hallucination. LLMs often fail to pay attention to the golden evidence and hallucinate their own reference even when large amounts of evidence exist. To alleviate the problem of LLM halluscination during evidence selection and answer generation, we innovatively propose to use constrained decoding in the answering process to reduce hallucination by forcing the model to generate grounded answers from evidence and let models give concise and exact answers.  Formally, we construct a vocabulary bank $V = W_1, W_2, ..., w_i$ from all words in the provided evidence sentences. We conduct a simple filtering by removing common English stop words. We then instruct the model's evidence retrieval and answering module to construct its answers using words only from the given vocabulary V.\nCode-based Constrained Decoding For open-source LLMs (e.g., Llama), we build our logit processor at the decoding time. Specifically, for every word $w_j \\notin V$, we manually set the score to negative infinity to prevent the model from generating them. Thus, every answer generated will only use words from the evidence list.\nPrompt-based Constrained Decoding For closed-source LLMs (e.g., GPT models), since we do not have access to their decoding function, we had to instruct the GPT models using prompts to do constrained decoding. We provide our prompt template used in Appendix A."}, {"title": "4 Experimental Setup", "content": "Dataset We compare STOC-TOT with baseline methods on the HotpotQA dataset (Yang et al., 2018) and the MuSiQue dataset (Trivedi et al., 2022), both of which are widely used MHQA datasets across state-of-the-art MHQA baselines. The experiments are conducted under the distractor setting, where we provide the model with an evidence pool containing both golden and irrelevant evidence. The model needs to find the golden evidence to answer the question correctly. We randomly selected 200 examples from each dataset as our evaluation set.\nBaselines We included three baselines:\n\u2022 Vanilla Prompting with no examples provided. We only provide the model with questions and evidence and instruct it to output the answer.\n\u2022 Chain-of-Thought (CoT) prompting (Wei et al., 2022) with a standard input-output (IO) prompt. We design the prompt with one in-context example, which presents the whole reasoning chain, including all intermediate steps.\n\u2022 Tree-of-Thought prompting (Yao et al., 2023a) with slight modifications to adapt to the MHQA task. We largely followed the original framework and used majority voting on the reasoning lines to decide the final answer.\nWe recognize that there are LLM-based retrieval augmented generation frameworks (Yao et al., 2023b; Gou et al., 2024) that were also evaluated on HotpotQA. However, we excluded them from our baselines as they used outside knowledge bases, which are under a different testing scenario.\n4.1 Implementation\nWe experiment with the baselines and our model utilizing five LLMs: GPT-3.5-turbo (Brown et al.,"}, {"title": "5 Results", "content": "5.1 Overall Results\nWe compare STOC-TOT with LLM baselines on the HotpotQA dataset and the MusiQue dataset and present our results in Tables 1 and 2. The backbone LLMs in our experiments include GPT3.5, GPT4, Llama2-13B, Llama2-70B, and Llama3-8B. Due to time constraints, we only tested with Llama2-70B on the HotpotQA dataset. On the HotpotQA dataset, STOC-TOT attains an on-average increase in performance of over 6 % compared with vanilla prompting on GPT models, and the improvement goes up to 11% when we further implement STOC-TOT with constrained decoding. On the more challenging MusiQue dataset, we still see an increase in performance of STOC-TOT compared with the other baselines, most notably on GPT4, where we observe an 11.5% EM improvement (from 31.50 to 42.0).\nComparison with Tree-of-Thought STOC-TOT surpasses the original Tree-of-Thought prompting by 7% with the GPT4 model on both tested datasets. For LLMs with inferior reasoning ability, such as LLaMa2-8B, we still observe a performance improvement, even on the harder MusiQue dataset. These results suggest that STOC-TOT is more effective at forming and selecting reliable reasoning paths under complex reasoning scenarios.\nConstrained Decoding Even though the LLM's reasoning ability can be improved by reasoning prompting, such techniques have little help in preventing hallucination. However, STOC-TOT implements constrained decoding, which makes the model much more grounded to evidence when answering the question, effectively addressing hallucination issues and improving the overall performance of our framework.\n5.2 Ablation Study\nSensitivity to Demonstration Question Type We study the effect on STOC-TOT performance when different types of demonstration questions are provided in the prompt template. The Hot-PotQA dataset specified two types of questions. The \"Bridge\" question contains a \"bridge entity\" that connects the question and the final answer. In"}, {"title": "6 Conclusion", "content": "This paper proposes STOC-TOT, a stochastic tree-of-thought reasoning framework with constrained generation for multi-hop question answering. STOC-TOT is specialized in dealing with complex reasoning scenarios in natural language tasks. Experiments on two benchmark datasets show that our framework outperforms previous reasoning prompting techniques with multiple Large Language Models. Detailed analysis shows that our framework is capable of building a robust reasoning process given different types of questions. Further research can aim to enhance the reliability of our framework by proposing better validity evaluation schemes and more effective methods for improving groundedness and preventing hallucination."}, {"title": "Limitations", "content": "Our framework relies on initiating multiple model instances and requires multiple prompts per round. The repetitive callings impose heavy time costs for our framework, even after implementing our paraphrase module. Another limitation comes from how we generated sub-questions. Currently, we directly prompt the model to generate sub-questions. A more complex standard can be used to increase the quality of the sub-questions generated. Also, more extensive experiments should be provided, including experimenting on other different datasets and case studies."}, {"title": "Ethics Statement", "content": "This research adhered to the ethical standards and best practices outlined in the ACL Code of Ethics. Language Models can sometimes produce illogical or inaccurate reasoning paths, so their outputs should be cautiously used. The outputs are only examined to understand how a model arrives at its answers and investigate why it makes certain errors. All experiments used publicly available datasets from previously published works and did not involve ethical or privacy issues."}, {"title": "A Prompt Templates", "content": "Sub Question Generation Template The prompt template containing one comparison question and one bridge question is given below:\nprompt: Break a question into high-quality sub-questions that are easier to answer. Here are two examples as guidelines:\n\"Question: Are Tokyo and Busan in the same country? Thought 1: I could either find which country Tokyo is located in, or which country Busan is located in. Sub Question 1-1: Which country is Tokyo located in? Sub Question 1-2: Which country is Busan located in?\"\n\"Question: Tokyo is located in the country that has what colors present on its national flag? Thought 1: I need to first find out which country Tokyo is located in. Sub Question 1-1: Which country is Tokyo located in?\"\nOnly give out your thought process and current-level sub-questions. Do not give out answers to your questions. Question: Given Question. Thought 1:\nPrompt-based Constrained Generation Template The prompt template at answering time is given below:\nprompt: Given a question and a list of evidence that may of help, give your answer directly, using words only from the vocabulary bank, without any explanations.\nQuestion: Given Question. Evidence as reference: Given Evidence. Vocabulary Bank: Given Vocabulary. Answer:"}, {"title": "B Examples of the Error Cases", "content": "\u2022Type-2: Intermediate Answer\nQuestion:\nWhere does the hotel and casino located in which Bill Cosby's third album was recorded?\nAnswer given by STOC-TOT on GPT4:\nLas Vegas.\nGolden Answer:\nLas Vegas Strip in Paradise.\n\u2022Type-3: Wrong Answer\nQuestion:\nAside from the Apple Remote, what other device can control the program Apple Remote was originally designed to interact with?\nAnswer given by STOC-TOT on GPT4:\nsiri remote and devices with netsupport manager software\nGolden Answer:\nkeyboard function keys\n\u2022Type-4: Semantically Correct\nQuestion:\nRoger O. Egeberg was Assistant Secretary for Health and Scientific Affairs during the administration of a president that served during what years?\nAnswer given by STOC-TOT on GPT4:\n1969 to 1974\nGolden Answer:\n1969 until 1974"}]}