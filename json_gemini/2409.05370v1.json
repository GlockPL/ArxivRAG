{"title": "KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models", "authors": ["Yingshu Li", "Zhanyu Wang", "Yunyi Liu", "Lei Wang", "Lingqiao Liu", "Luping Zhou"], "abstract": "Harnessing the robust capabilities of Large Language Models (LLMs) for narrative generation, logical reasoning, and common-sense knowledge integration, this study delves into utilizing LLMs to enhance automated radiology report generation (R2Gen). Despite the wealth of knowledge within LLMs, efficiently triggering relevant knowledge within these large models for specific tasks like R2Gen poses a critical research challenge. This paper presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration framework based on LLMs. Utilizing a frozen LLM to generate reports, the framework integrates a knowledge graph to unlock chest disease-related knowledge within the LLM to enhance the clinical utility of generated reports. This is achieved by leveraging the knowledge graph to distill disease-related features in a designed way. Since a radiology report encompasses both normal and disease-related findings, the extracted graph-enhanced disease-related features are integrated with regional image features, attending to both aspects. We explore two fusion methods to automatically prioritize and select the most relevant features. The fused features are employed by LLM to generate reports that are more sensitive to diseases and of improved quality. Our approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets. Our code will be available on GitHub.", "sections": [{"title": "1 Introduction", "content": "Automated radiology report generation (R2Gen) is gaining traction due to its potential to streamline the time-consuming and error-prone task of medical image reading and report writing. Unlike generic image captioning tasks [25,13,4],"}, {"title": "2 Methodology", "content": "Our framework consists of four main components: a visual feature encoder, a knowledge-enhanced feature encoder, a feature fusion module, and a report generator. The visual feature encoder extracts regional features from a chest x-ray image, which are then processed by the knowledge-enhanced feature encoder to 'distill' disease-related information guided by a medical knowledge graph. The resulting knowledge-enhanced disease-related features are fused with the regional image features in the feature fusion module and used to prompt the LLaMA-based report generator for R2Gen. Fig. 1 gives an overview of KARGEN."}, {"title": "2.1 Feature Extraction", "content": "Regional Feature Extraction Given an input X-ray image $X_v$, we initially extract regional image features $Z_v = Swin(X_v; \\theta_v)$, utilizing a pre-trained Swin Transformer [12], where $Z_v \\in \\mathbb{R}^{S \\times d_v}$ ($S$: the number of features; $d_v$: the dimensionality of each feature; $\\theta_v$: the parameters of the Swin Transformer)."}, {"title": "Medical Domain Knowledge Graph", "content": "Focusing on disease-related features in medical imaging, especially for interrelated chest diseases, is critical. We propose a medical domain knowledge graph to extract chest disease features, incorporating 14 terms from the Chexpert [6]. Each disease entity is represented by the word embedding of its name, obtained using the LLAMA Word Embedding Layer. The connections are illustrated in Fig. 2, highlighting that abnormalities within the same region exhibit stronger correlations than those across different organs. This guides our analysis of diseases in the lungs, heart, and pleura in chest X-ray images [30], capturing nuanced relationships effectively."}, {"title": "Diseased-related Feature Extraction", "content": "Based on the knowledge graph, we construct a GCN to aggregate disease-related features. Our GCN comprises three layers. At each layer $l$ ($l = 1, 2$ and 3), there are two primary phases: 1) the propagation of information throughout the graph, and 2) the updating of disease-related features. Let $N^l$ denote the node features in the $l$-th layer, $A$ denote the adjacency matrix governed by the knowledge graph, and $A' = D^{-1/2} . A . D^{-1/2}$ ($D$ is the degree matrix of $A$). The entire process can be formulated as\n\n$N_{phase1}^l = GELU(LN((N^{l-1} . W^l)A'))$,\n\n$N^l = GELU(LN((N^{l-1} + N_{phase1}^l) . W_{update}^l + N^{l-1}))$.\n\nHere $W^l$ and $W_{update}^l$ are learnable parameters for information propagation and updating. $LN(\\cdot)$ denotes variants of layer normalization.\n\nThe initial node features $N^0$ are determined by using the disease entity name embeddings $E \\in \\mathbb{R}^{M \\times d_w} = [e_1,..., e_i,......, e_M]^T$($M = 14$) to query the regional image features $Z_v$ output by the visual encoder via multi-head attention:\n\n$N^0 = MHA(E, Z_v) = Concat(head_1, ..., head_h)W^O$,\n\n$head_n = Softmax (\\frac{Q_nK_n^T}{\\sqrt{d_k}})V_n$,\n\n$Q_n = EW_n^Q, K_n = Z_vW_n^K, V_n = Z_vW_n^V,$"}, {"title": "2.2 Feature Fusion", "content": "After obtaining the regional features $Z_v \\in \\mathbb{R}^{S \\times d_v}$, and the knowledge-enhanced disease-related features $Z_g \\in \\mathbb{R}^{M \\times d_v}$, we employed a multi-head attention network to align their dimensions by using $Z_v$, $Z_g$ and $Z_g$ as the query (Q), key (K) and value (V). The attention output, denoted as $\\tilde{Z_g} \\in \\mathbb{R}^{S \\times d_v}$, shares the same dimensions as $Z_v$. In the following, we propose two fusion strategies designed to integrate these two types of features.\n\nElement-wise Fusion This approach uses an element-wise weighted sum for the final integrated feature representation, employing a trainable gate to determine the importance of each element in the two feature types. The fused features $Z_f \\in \\mathbb{R}^{S \\times d_v}$ is obtained by:\n\n$Z_f = gate \\odot Z_v + (1 - gate) \\odot \\tilde{Z_g}$,\n\n$gate = sigmoid([Z_v; \\tilde{Z_g}] . W^g)$,\n\nwhere $[Z_v; \\tilde{Z_g}]$ represents the concatenation of $Z_v$ and $\\tilde{Z_g}$, and $W^g$ is a learnable parameter. The $\\odot$ operation signifies element-wise multiplication.\n\nModality-wise Fusion Inspired by the Mixture of experts (ME) [14], we designed two distinct expert networks: one to process disease-related features $\\tilde{Z_g}$ and the other for general regional features $Z_v$. To dynamically allocate the contribution of each expert's output, we put forward a soft router module, represented by $G(x)$, functioning as a gating network. This gate is implemented as a multi-layer perceptron (MLP). Unlike the element-wise fusion operating at the individual element level, modality-wise fusion treats each feature set as an integral unit for combination. The combined output is formulated as:\n\n$Z_f = g_1E_1(Z_v) + g_2E_2(\\tilde{Z_g}), [g_1,g_2] = G(Z_v, \\tilde{Z_g})$.\n\nHere, $E_1$ and $E_2$ denote the expert networks comprising of a linear layer and layer normalization. The soft router $G(x)$ assesses and decides the relevance of each expert ($E_1$ and $E_2$) in the fusion process. The weights $g_1$ and $g_2$ are computed as probability values, indicating the importance assigned to each expert's output in the final feature representation $Z_f$."}, {"title": "2.3 Report generation", "content": "We employ LLaMA2-7B to generate radiology reports, leveraging the fused features as the visual prompt. Our instruction prompt is designed following the"}, {"title": "4 Conclusions", "content": "In this paper, we propose a novel framework integrating LLMs with a medical knowledge graph for R2Gen. Our work highlights the value of incorporating"}]}