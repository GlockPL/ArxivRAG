{"title": "EXAMINING ALIGNMENT OF LARGE LANGUAGE MODELS THROUGH REPRESENTATIVE HEURISTICS: THE CASE OF POLITICAL STEREOTYPES", "authors": ["Sullam Jeoung", "Yubin Ge", "Haohan Wang", "Jana Diesner"], "abstract": "Examining the alignment of large language models (LLMs) has become increasingly important, particularly when these systems fail to operate as intended. This study explores the challenge of aligning LLMs with human intentions and values, with specific focus on their political inclinations. Previous research has highlighted LLMs' propensity to display political leanings, and their ability to mimic certain political parties' stances on various issues. However, the extent and conditions under which LLMs deviate from empirical positions have not been thoroughly examined. To address this gap, our study systematically investigates the factors contributing to LLMs' deviations from empirical positions on political issues, aiming to quantify these deviations and identify the conditions that cause them.\nDrawing on cognitive science findings related to representativeness heuristics where individuals readily recall the representative attribute of a target group in a way that leads to exaggerated beliefs- we scrutinize LLM responses through this heuristics lens. We conduct experiments to determine how LLMs exhibit stereotypes by inflating judgments in favor of specific political parties. Our results indicate that while LLMs can mimic certain political parties' positions, they often exaggerate these positions more than human respondents do. Notably, LLMs tend to overemphasize representativeness to a greater extent than humans. This study highlights the susceptibility of LLMs to representativeness heuristics, suggeset-ing potential vulnerabilities to political stereotypes. We propose prompt-based mitigation strategies that demonstrate effectiveness in reducing the influence of representativeness in LLM responses.", "sections": [{"title": "1 INTRODUCTION", "content": "As large language models (LLMs) wield tangible impacts across various societal domains, it has become important to align LLMs with human intentions and values (Askell et al., 2021; Kenton et al., 2021). Among other topics, the political inclinations of LLMs constitute a critical and sensitive dimension in ensuring the safety of LLMs. Prior research has demonstrated that LLMs often do display political leanings, including positions like left-leaning orientations and pro-environmental stances (Santurkar et al., 2023; Hartmann et al., 2023; Feng et al., 2023). Furthermore, when subjected to specific party affiliations, such as Republicans or Democrats, LLMs have exhibited the capacity to emulate corresponding moral postures (Simmons, 2022) and stances on various political issues (Argyle et al., 2023; Jiang et al., 2022).\nDespite the valuable insights provided by previous studies on understanding the political tendencies of LLMs, the extent and conditions under which LLMs deviate -either deflating or inflating- from empirical positions, which are a crucial aspect in the context of cognitive bias, remains underexplored. Specifically, inspired by cognitive findings that in situations that involve uncertainty, humans lean on representative heuristics, which leads to the inflation of beliefs or stereotypes (Benjamin, 2019; Kahneman & Tversky, 1973; Bordalo et al., 2016), we conduct experiments through the lens of"}, {"title": "2 BACKGROUND: COGNITIVE APPROACHES TO STUDYING STEREOTYPES", "content": "Stereotypes that are pervasive in society are often rooted in empirical observations. For instance, the stereotype that Asians are good at math finds support in the fact that 60% of individuals achieving the top 6% SAT math scores belong to this demographic (Brookings, 2017). However, it is essential to recognize that such stereotypes are not universally applicable, as not all individuals of Asian descent possess good mathematical skills (Pang et al., 2011). The development of stereotypes can be explained by a social cognition approach, wherein individuals amplify differences between groups to create mental representations that facilitate efficient information processing (Schneider, 2005; Hilton &\nVon Hippel, 1996). Consequently, stereotypes arise from the exaggeration of inter-group differences, even when these differences are, in reality, marginal or smaller than intra-group differences. This gives rise to a kernel-of-truth hypothesis, suggesting that some stereotypes have a basis in empirical reality but often involve exaggerations.\nThese ideas from the social cognition approach closely align with concepts from cognitive science, particularly those related to heuristics used in probability judgments (Tversky & Kahneman, 1974; Slovic & Lichtenstein, 1971; Grether, 1980). Representative heuristics, a specific cognitive heuristic, involves overemphasizing features representative of a target group in relation to a reference group when making judgments (Kahneman & Tversky, 1972). As defined by Kahneman & Tversky (1973), an attribute is deemed representative if it is highly diagnostic, meaning its frequency is significantly higher in the considered class compared to the relevant reference class. This phenomenon accounts for some inaccurate stereotypes, such as the belief that a notable proportion of individuals of Irish heritage have red hair. While only 10% of this population possesses this trait, it becomes more salient and memorable when contrasted with the global prevalence of less than 2% for individuals with red hair.\nFormally, we can write that attribute a is representative of group $X^+$ relative to a contrastive group X if it scores high on the likelihood ratio Bordalo et al. (2016):\n$\\frac{P(a | X^+)}{P(a | X^-)}$\nIn summary, certain stereotypes attributed to specific groups are occasionally accurate (Schneider, 2005). Representativeness tends to produce relatively accurate stereotypes, yet there are instances where stereotypes are inaccurate (Bordalo et al., 2016). Stereotypes can be categorized into two dimensions: 1) Stereotypes amplify differences, as representativeness leads to stereotypes containing a kernel of truth. This implies that stereotypes highlight existing and highly distinctive characteristics that differentiate groups (Hilton & Von Hippel, 1996). 2) Stereotypes are context-dependent, with the evaluation of a given target group is contingent on the reference group used for comparison."}, {"title": "3 METHODOLOGY", "content": "Task Formalization We denote the language model of interest $L$ with weights $\\theta$, $L_{\\theta}$. The target group of interest consists of contrastive groups, namely, {$X^+, X^\u2212$} $\\in$ $X$. Throughout the paper, we use $X^+$ to indicate Republicans and $X^\u2212$ to Democrats. We define $A = \\{1, . . . , a_n \\}$ as attributes of interest that express specific aspects of the target group. For example, the attributes correspond to the Likert scale, $A = \\{1, . . . , 7\\}$, of the given topic in Fig 1. The probability distribution space is denoted $p \\in \\Delta(A \\times X)$ and the conditional distribution $p_{a,x^+} = Pr(A = a | X^+)$, probability conditioned on a group $X^+$, giving the vector of conditional distribution $[p_{a,X^+}]_{a\\in A}$.\nRepresentativeness We define representativeness of group $X^+$ relative to a contrastive group $X^\u2212$ of attribute a in likelihood ratio:\n$R=\\frac{P_{a,x^+}}{P_{a,x^-}}$"}, {"title": "4 MITIGATING STRATEGIES", "content": "In decision-making, individuals, upon recognizing the application of the representativeness heuristic, often exhibit a capacity for self-correction, leading to more accurate judgments (Kahneman, 2013; Schwarz et al., 1991; Oppenheimer, 2004). Drawing inspiration from this human cognitive tendency, we conducted supplementary experiments using diverse prompt types to explore whether language models manifest similar mitigation strategies.\nAWARENESS We introduced an explicit preamble, elucidating the heuristic's nature.\nFEEDBACK Motivated by the self-correction behavior of language models (Ganguli et al., 2023), we solicited feedback from the language models. The process involved presenting 1) the original question and the initial response generated by the model, 2) an explanation of the representativeness heuristic and an instruction \"Bearing this in mind, provide a revised response to the question.\", and a revised answer generated by the model. We use the explanations described in the AWARENESS\nREASONING Introducing the suffix \"Please give reasons for your answer\" prompts the model to provide a rationale for its response. This choice is inspired by observed variations in model responses when engaging in a reasoning process, as documented in prior studies (Wei et al., 2022; Jeoung et al., 2023)."}, {"title": "5 EXPERIMENT SETUP", "content": "5.1 DATA\nMFQ: Moral Foundation Questionnaire Graham et al. (2013) measures the perceived moral foundations a respondent may possess on five moral foundations (e.g. Care, Fairness, Loyalty, Authority, and Purity). We used the data from Talaifar & Swann Jr (2019). The data consist of the respondent's political party affliation and their responses to the Moral Foundation Questionnaire. The details can be found in Appendix B.\nANES: American National Election Survey Studies (2022) The dataset contains a cumulative time-series survey from 1948 to 2020 conducted biannually. We chose 9 questions that have 7 Likert-scale response options and 1 question that has 4 Likert-scale options. The respondents are asked to provide their party identification, whether they are Democrats (including leaners), Independents, or Republicans (including leaners). We filter only the respondents who identified themselves as either Democrats or Republicans. We provide details of the data and pre-processing in Appendix B, and a detailed configuration of prompts in Appendix D."}, {"title": "5.2 MODELS", "content": "In the evaluations, we use large language models: GPT's variants: Gpt-4 and Gpt-3.5-turbo (Achiam et al., 2023; Brown et al., 2020), GEMINI (Team et al., 2023), Gemini-Pro, a multimodal model proven to advance state-of-the-art in large scale language modeling. We include open-sourced models such as LLAMA2 70B (Touvron et al., 2023), LLAMA3-8B (AI@Meta, 2024) and QWEN2.5-72B (Yang et al., 2024; Team, 2024). The detailed experiment setup is listed in Appendix C."}, {"title": "6 RESULT", "content": "Believed Mean vs. Empirical Mean In the context of ANES, while some variation exists across models, the results indicate that the Believed Mean produced by language models tends to be exaggerated compared to both the Empirical Mean and Human Predictions (Fig 2). Specifically, the Believed Means for Republican are generally higher than both the Empirical and Human Pred Means, while that of Democrats tend to be lower. This suggests that models inflate responses for Republicans and deflate them for Democrats, even more so than the degree to which human make predictions. For detailed topic-wise disaggregated results, please refer to Appendix Fig 5, Table 12."}, {"title": "7 RELATED WORK", "content": "Political Inclination of LLMs. Previous research has explored the political inclinations of LLMs (Feng et al., 2023; Santurkar et al., 2023), probing the models to investigate their political leanings. For instance, Feng et al. (2023) assessed LLMs' leanings through the political compass test, and the impact of such leanings on downstream tasks. Santurkar et al. (2023) measured models' alignment using diverse metrics such as steerability and consistency.\nRecent studies have illustrated that by conditioning LMs on demographic attributes, such as party affiliation, these models can mimic specific characteristics of the corresponding groups, such as Simmons (2022) and positions on political issues (Argyle et al., 2023; Jiang et al., 2022; Hartmann et al., 2023).\nIn this paper, we extend the investigation by applying insights from cognitive science to explore a relatively under-addressed aspect of LMs \u2013 their alignment with the perspectives maintained by political parties on various topics and issues.\nStereotypes of LM Previous studies identifying and quantifying stereotypes in LLMs (Bolukbasi et al., 2016; Nadeem et al., 2021) have faced criticism for lacking a precise definition of stereotypes (Blodgett et al., 2021). Addressing this gap, recent papers have incorporated social science theories to formulate explicit definitions of stereotypes in the context of LLMs (Jeoung et al., 2023; Cao et al., 2022). For instance, Jeoung et al. (2023) employ the social content model, while Cao et al. (2022) adopt the Agency-Belief-Communion theory to conceptualize and assess stereotypes embedded in LLMs. In this study, we contribute to this evolving discourse by drawing on insights from cognitive science, specifically representative heuristics, to articulate and understand stereotypes."}, {"title": "8 DISCUSSION", "content": "Potential effects of political representative heuristics on downstream task Beyond our current context, which aims to quantify representative heuristics of LMs, a critical avenue of exploration pertains to the tangible impacts that these representative heuristics may exert on diverse end users (e.g., decision-making (Tamkin et al., 2023) and automated agents (Ruan et al., 2023)). In a preliminary analysis, we examine how representative heuristics could potentially impact one specific downstream task -misinformation detection (Appendix G).\nDoes alignment methods affect representative heuristics of LLMs? Several techniques have been proposed to align language models, with a primary focus on instruction tuning (Wei et al., 2022), and reinforcement learning from human feedback (Ouyang et al., 2022). However, as evidenced by recent studies, certain limitations persist (Sharma et al., 2023; Askell et al., 2021). This leads us to speculate on two fronts: (1) analyzing how preference data, used in training the reward model, influences LLMs concerning representative heuristics behavior, and (2) investigating how reward incentivizing objectives impact the representative heuristics of LLMs (Appendix H)."}, {"title": "9 CONCLUSION", "content": "In this work, we present an underexplored perspective on understanding stereotypes encoded in LLMs, viewing them through the lens of cognitive bias and utilizing the formalization of representative heuristics. This approach proves essential for gauging the alignment of LLMs with human values and deciphering the extent of their deviation from human intentions."}, {"title": "10 LIMITATIONS", "content": "Our analysis is confined to specific political parties, namely Republicans and Democrats, within the contextual framework of the United States. It is acknowledged that political preferences encompass a diverse and intricate landscape, with the existence of political parties beyond Republicans and Democrats.\nOur utilization of survey data and responses derived from previous studies serves as the foundation for our empirical data, which we interpret as reflections of human values. It is duly acknowledged that this empirical data constitutes a sub-sample from the broader population and may not fully encapsulate the diverse spectrum of human values.\nIn the context of our study, we operated under the premise that political party affiliation serves as an indicator of collective adherence to a particular ideological framework. Specifically, individuals identifying as Republicans (affiliated with the Republican party) typically align with the overarching principles associated with Republican ideology. Nonetheless, it is conceivable that instances exist wherein individuals identifying as Republicans may exhibit alignment with certain tenets traditionally associated with Democratic ideology."}, {"title": "11 BROADER IMPACT", "content": "This study strictly follows the Ethics Policy outlined by the ICLR. Our central objective is to promote the safe and responsible use of Large Language Models (LLMs). Consistent with our commitment to transparency and progress in the field, we intend to publicly release our code to enhance reproducibility and stimulate further investigation of the concepts introduced in this study. The open availability of our code is aimed at fostering collaborative development and contributing to the ongoing advancement in this area."}, {"title": "APPENDIX", "content": "A DETAILS ON EXEMPLAR IMPLEMENTATION\nAs shown in Eq 2, the exemplar $a^*$ is defined as the most representativeness attribute for group $X^+$ given a reference group $X^\u2212$:\n$a^* \\in arg max \\frac{P_{a,x^+}}{P_{a,x^-}}$\nWe note that there exist cases where $\\frac{P_{a,x^+}}{P_{a,x^-}}$ = 0, where the representativeness cannot be computed. To prevent such cases, we apply Laplace additive smoothing. To be specific, we denote $n = |A|$, the number of attributes in $A = a_1, ..., a_n$. We add the probabilities by $\\lambda$ to each $p_{lx}$. Having total N instances of responses from the model L, this results in the marginal probability increase in $\\frac{\\lambda}{N+n}$. This equals to the Laplace smoothing coefficient $\\alpha$ = 1, add-one smoothing (Manning et al., 2008; Jurafsky, 2000).\nB DATA DETAILS\nANES We have used the September 16, 2022 version, the latest available version Studies (2022). The topics covered in this paper are: (1) Women's Rights, (2) Urban Unrest, (3) Legal Rights, (4) Liberal-Conservative, (5) Government Job Income, (6) Government Services, (7) Government Health Insurance, (8) Defense Spending (9) Government Aid Blacks, (10) Abortion. The number of self-identified Republicans and Democrats per topic is presented in Table 4.\nMFQ We used the the dataset provided by Talaifar & Swann Jr (2019) abiding by the author's consent. We concatenated responses from three distinct data, provided in one research. The aggregation was performed because all the studies included data on self-identified political party affiliation and responses from moral foundation questionnaires. The final dataset consists of the responses to a moral foundation questionnaire on individuals (N=919) with their self-identified political stance (e.g. Republican or Democrat)-specifically, 266 self-identified Republicans, 450 Democrats, and 203 independents/other party. For the analysis, we filtered only the responses from self-identified Republicans and Democrats."}, {"title": "C MODEL SETTING", "content": "The model has been repeated 20 times for the analysis. The selection of these models is grounded in their societal impact, given their widespread and frequent usage by the public. GPT-3.5-TURBO, GPT-4 We accessed the models through OpenAI API 5, using the default setting: temperature:1, topP:1. We accessed GEMINI-PRO through Google Cloud 6, using the default setting temperature:0.9, topP:1.0. Open sourced models were accecssed through hugging face. LLAMA-70B was accessed via model name: meta-llama/Llama-2-70b-chat-hf, using the setting temperature:0.7, topP:0.9. LLAMA3-8B: meta-llama /Meta-Llama-3-8B-Instruct, and QWEN2.5-72B: Qwen /Qwen2.5-72B-Instruct, respectively using the default parameter settings."}, {"title": "D PROMPTS", "content": "ANES The baseline prompts are adopted from the ANES questionnaire. However, for the topics 'Government Services' and 'Abortion', we reversed the scale to associate higher scales with the Republicans, and lower scales with the Democrats. The prompts can be found in Table 16\nMFQ It consists of 30 questions, the first 15 questions ask participants whether a situation (e.g. whether or not someone showed a lack of respect for authority) is relevant when deciding whether something is right or wrong. The response ranges from 1 (not at all relevant) to 6 (extremely relevant). For the next 15 questions, they indicate ranging from 1 (strongly disagree) to 6 (strongly agree), the degree they agree with a given statement (e.g. Respect for authority is something all children need to learn). We borrowed the wordings from the Moral Foundation Questionnaire (Graham et al., 2013). As shown in Table 17, for moral foundations of Harm and Fairness, we reverse the scales (i.e., 1 (strongly agree) to 6 (strongly disagree)). This is to configure the prompts such that higher scales are associated with the Republicans and low scales with the Democrats."}, {"title": "E SENSITIVITY CHECK OF PROMPTS", "content": "We recognize that the prompts involving the generation of numerical scales may be sensitive to the specific prompts, necessitating further assessment to ascertain whether the model outputs are reliable. We evaluated the robustness and reliability of the prompts by generating model responses 20 times and observing two key metrics: 1) the coefficient of variation (CV) and 2) human evaluation.\nCoefficient of Variation (CV) is a measure of variability relative to the mean, expressed as the ratio of the standard deviation (\u03c3) to the mean (\u03bc), denoted as $\\frac{\\sigma}{\\mu}$. The results, as presented in Table 5, indicate that the models' responses demonstrated high consistency, with CV values approaching 0.0 and not exceeding 1 at the maximum. Lower CV values suggest a small degree of dispersion and high consistency, while higher values imply a greater degree of dispersion and lower consistency.\nTemperature Sensitivity The output of language models (LMs) can vary depending on the temperature setting. To assess temperature sensitivity, we conducted an analysis using GPT-4 on the ANES task, running the model 10 times at each temperature setting. We computed the Coefficient of Variation (CV) for each topic and averaged the results. The Diff_D represents the difference between the Believed Mean of Democrats and the Empirical Mean, while Diff_R reflects the difference between the Believed Mean of Republicans and the Empirical Mean. The results indicate that the CV increases with higher temperature settings, suggesting greater variability in the responses. However, when averaged, the deviation from the empirical mean (Diff_D, Diff_R) remain relatively consistent, with values around -1.4 and 0.46 respectively."}, {"title": "F FURTHER ANALYSIS ON REPRESENTATIVE HEURISTICS", "content": "In contrast to the kernel of truth hypothesis, the representative heuristics highlight the contextual dependence of stereotypes, elucidating how the portrayal of a target group depends on the attributes of the reference group to which it is compared. Bordalo et al. (2016) note that when the most probable attribute of a group $X^+$, significantly deviates from its most representative attribute, more distortion or exaggeration tends to occur in the direction of the representativeness. Table 8 presents an example from ANES topics. The Liberal-Conservative is the case where the most representative attribute coincides with the most probable attribute, and Defense-Spending is the case where the most representative attribute differs from the most probable attribute. For the case where the most probable attribute coincides with the most representative attribute (e.g. Liberal-Conservative), the maximum mean difference is 0.89, while in the case where the most representative attribute is far from the most probable attribute (e.g. Defense Spending), the maximum mean difference is much larger, 2.01. There exists some variation across models, however, this trend still holds when compared model-wise. This suggests that when the most representative attribute is far from the most probable attribute, the language models also exhibit exaggeration of beliefs."}, {"title": "G MISINFORMATION DETECTION ANALYSIS", "content": "Total #True # False\nRepublican 2107 808 1299\nDemocrat 1440 807 633\nTotal 3547 1615 1932\nWe posit that the representative heuristics embedded in the models may exert a discernible influence on downstream tasks. Specifically, the inclusion of party affiliation information, which encapsulates the representative characteristics of the parties, may serve as a proxy and consequently influence the model's performance on downstream tasks. To investigate this hypothesis, we conducted a controlled experiment focusing on the task of misinformation detection. It is essential to note that this experiment does not establish a causal relationship demonstrating the impact of representative heuristics on the performance of downstream tasks. Rather, it aims to explore the influence of party affiliation information on the model's ability to detect fake news within the confines of a controlled experiment setting.\nFor our experiment, we utilized the benchmark dataset for fake news detection introduced by Wang (2017). The dataset comprises 1) statements, 2) their labels, 3) the speaker of the statement, and 4) the party affiliation of the speaker. We specifically filtered statements spoken by individuals affiliated with either the Democratic or Republican party, considering only labels indicating false or truth from the available 6 labels. The details of the final dataset are outlined in Table 9."}, {"title": "H ALIGNING METHODS AND REPRESENTATIVE HEURISTICS", "content": "ANES\nLlama2-70b\n4.0 (0.0)\nLlama2-70b-base 2.0 (1.4)\nGovernment Health Insurance\n1128812994429 11240993850.5721182444311891.0245953144982.096750628 30798588.86397191.08373746913.4542171434247 33297525075797060241041895230407333847937291039268134711509070243432118640730701817035462213847 48103627148103474336748103211303845864075210259252325337827326320775555567030756748560485652448374950985785079926399689263101206532542472775156800948874413877777777666666662229710890151084446484501310128259815337423372767245696458469054823953637318840579711833478307499075579797651737582134582227424696355201568744656306907591817434986159137421027596566333788428349274302427475726241875314800797854200217068273092850508269114912795612746138115155775211913357400721360570351010593620301002188343668491560313479803436923460031362313039816255259942220464858569716788989547038329298165169396224046787599772760938482808471948764714780670761784037593984961595884059027751349189384242811568454212473014344238050323458547674763959399281451188252942492922208413289033727287342498707431814087859658236906260601322423535104730143744137273339250416900112172040512134782747247771188733896952115081231680861525925647398433645121349481766403530793995282740354724073561914652062778971469418178828893722798567844436313846089606841877738656687265784985484081681326426847051852470859154295919857561505236523916239417280628603452328703216366843493608078545157617162527327635768604103879603360086966651189338151772084049722553687640050268796684352768669586914124813889319861040257966945382549858329373573023757268913799828815781811309631708006049406032935074270426126291108568437086058792546698362677790933462973341305370670012312413823650313470113651281637406606159929117699809207685413220796067214151466073440718452715846740767083468659565713913925908155908048672469755041417178319991658302043368804115001293603027739825093997504091647755400262302900028412284490141362265389918438380355958399894175986591599075100481006410165406670671247078771923115433874794854217505552558557729583342395554412241689917816704176133690497082986660370208353471270481464713087354887364455028029291602916443007744991613054683139646176648644370434083994147211725700388920310900766084819684080016285025871198833272135722274222782279507335769735289344889740684588484499943025859942130806513442781754917933186199082332090202333754620574034908627684027995764305021930680577844074297775913479692202603015313998816059192398976695319310344130798521998482286498458263045383052083137064"}]}