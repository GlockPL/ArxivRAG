{"title": "The OCON model: an old but gold solution for distributable supervised classification", "authors": ["Stefano Giacomelli", "Marco Giordano", "Claudia Rinaldi"], "abstract": "This paper introduces to a structured application of the One-Class approach and the One-Class-One-Network model for supervised classification tasks, specifically addressing a vowel phonemes classification case study within the Automatic Speech Recognition research field. Through pseudo-Neural Architecture Search and Hyper-Parameters Tuning experiments conducted with an informed grid-search methodology, we achieve classification accuracy comparable to nowadays complex architectures (90.0 - 93.7%). Despite its simplicity, our model prioritizes generalization of language context and distributed applicability, supported by relevant statistical and performance metrics. The experiments code is openly available at our GitHub.", "sections": [{"title": "I. INTRODUCTION", "content": "The intricate tapestry of human speech is woven through the delicate interplay of various phonemes, each contributing its unique acoustic signature to the rich spectrum of linguistic expression. Among these, vowels stand as fundamental building blocks, playing a pivotal role in shaping the intelligibility and emotive nuances of language. The classification of vowels within the phonetic landscape has been a subject of enduring interest and significance in the realm of linguistics and speech science, due to the broad range of applications, e.g.: language learning and pronunciation assessment [1], dialectology and sociology [2], forensic speaker identification [3], assistive technologies [4], emotion recognition [5], up to brain computer interface [6]. Advancements in the field of vowel recognition have reached a state-of-the-art (SoA), leveraging cutting-edge Machine Learning (ML) algorithms and sophisticated signal processing techniques to achieve unprecedented accuracy in deciphering and understanding vocal nuances. Techniques exploited for this purpose span from features extraction in time and transformed domains [7], to advanced ML solutions, from Neural Networks (NNs) to Deep Learning (DL) algorithms and models applied for general purpose Automatic Speech Recognition (ASR), as in [8], [9].\nOur proposal aims to assess the reliability of a simplified Neural Architecture Search (NAS) and Hyper Parameters Tuning (HPs-T) combination for designing NNs feature abstraction layers. We propose a modular model, the \"One-Class One-Network\" (OCON), comprising parallelized binary classifiers focused on simpler phonetic recognition sub-tasks. We evaluate available data constraints and task complexities w.r.t. the current SoA, aiming for a shallow and optimizable architecture with a sustainable and straightforward (re)-training cycle. Additionally, we determine the minimum number of formant features needed to achieve current accuracy levels in phonetic recognition."}, {"title": "II. METHODOLOGIES", "content": "Following a standard DL experiment routine, we begin by gathering a reliable audio dataset with heterogeneous phonetic representations and gender diversity among speakers: we initially constrained our linguistic research context to the General American English case, as defined by the International Phonetic Association (IPA). Within this contextual dataset production, we encounter several good examples of pre-processed datasets (Sec. II-A) where pre-arranged phraseological (or even specific words like /hVd/, vowels between an \"h\" and a \"d\") speech segments were already recorded, analyzed and processed so as to elicit specific meaningful features: formant frequencies.\nThese features set represent the input data suitable for our NN model, and are usually retrieved by means of standardized key-steps, which can be summarized in: (1) manually or automatically segment speech signals in semantic frames according to a pre-arranged semantic grid (words/phonemes and silences); (2) analyze isolated fragments by means of Linear Predictive Analysis/Coding (LPA/C) to extract a smoothed time-frequency spectral estimation (we are interested in the frame-by-frame aggregated spectral envelope); (3) extract first N spectral peaks by means of whatever peaks estimation algorithm keeping tracks (contouring) of frame-wise continuities. Next, we'll manipulate and refine these formant frequency tracks to generate a suitable data vector (features) for the input stage of our NNs. This pre-processing step plays a vital role in enabling networks to learn abstract representations effectively, thereby maximizing recognition accuracy results."}, {"title": "A. Datasets availability", "content": "From a preliminary dataset review, the initial /hVd/ vowel phoneme dataset, by Peterson and Burney (PB) [10] featured"}, {"title": "B. Features analysis & processing", "content": "The HGCW dataset filename structure (Table I) contains encoded phonetic and speaker features crucial for preliminary statistical analysis. Through ad-hoc Python scripts, we discovered that only 1597 out of 1668 samples could be effectively used for our task due to null features presence for some samples, caused by authors algorithm failures. To maintain learning consistency, these samples were filtered out, resulting in an additional under-representation of certain phoneme and speaker classes (Table II). Fundamental frequency contours (F0) were estimated using a 2-way auto-correlation/zero-crossing pitch tracker with a halving/doubling evaluation subroutine [11], while formants were estimated using LPA spectra estimation and peak retrieval with parabolic interpolation [16]. The resulting frequency trajectories underwent further refinement using an interactive audio spectral editor for manual discontinuity examination and interpolation.\nWe created various experimental sub-structures of the original dataset, categorizing each sample based on: (1) Phonemes grouping, including F0 and the first 3 formant frequencies obtained at the steady state (SS); (2) Speaker grouping, segregating samples by gender (men, women, and children) with the same features as before; (3) Phonemes grouping, incorporating F0 and a total of 12 formant frequency values, with the first 3 formants sampled at 10%, 50%, SS, and 80% of the total duration of the vowel nucleus.\nWe analyzed and summarized classification algorithms exclusively evaluated on the PB and/or HGCW datasets features, to establish a consistent reference baseline for our work (Table III): Linear Discriminant Analysis (LDA) [17] and Generalized Linear Regression Models (GLM) [18] emerged as the prominent and most involved statistical-ML approaches, combined with innovative formant features processing as the 3D-auditory target zones framework, expressed by means of the logarithm of formant distances [19]. Other studies [20] adopted canonical auditory frequency transforms including: the Bark scale [21], [22], a technical Mel scale approximation [23], and a lin-to-log frequency approximation [24].\nNNs research in phonetic recognition has predominantly centered on using LPA coefficients [25] or spectral/cepstral derived features [26], often employing far more complex convolutional and/or recurrent stages. The only phonetic-OCON research retrieved [27] reported improvements exclusively over TI-MIT data, combined with LPC features. Due to this scarcity of comparative literature, we set a target average accuracy of 90% aiming to improve results reported in [11], [13].\nConsidering the significant variation in F0s within speakers due to physiological factors and related pitch variations due to prosody, we introduce linear formants normalization w.r.t. F0s. No prior usage of this pre-processing method were found, which appears to enhance class segregation by directly expressing distances in the linear frequency space (Fig. 1). To foster NNs training convergence, we applied min-max scaling to normalize the entire feature set. We also assessed formant ratios Probability Mass Distributions to determine Z-score (standardization) feasibility but they exhibited a consistent tendency towards skewed Poisson or Log-normal distributing. For enhanced data portability and reusability, we encoded all preprocessed features in a binary NumPy open-source compressed format (.npz) to preserve data resolution."}, {"title": "III. IMPLEMENTATION", "content": "In this section, we introduce the NN architectures for our experimentation, beginning with the Multi-Layer Perceptron (MLP). Since we are working with pre-processed features, our heuristic search experiments will focus on the architecture topology and characteristics of the MLP. We will then present our OCON proposal, which models multi-output classification tasks using multiple independent copies of the same optimized MLP architecture setup. These configurations are derived through simplified and informed NAS (pseudo-NAS) combined with HPs-T: in NNs research, HPs tuning involves optimizing architectural and learning parameters (such as layers, nodes, backpropagation optimizers, learning rate etc.) to minimize the network cost function, between the predicted result (class) and the provided ground-truth (label) in supervised learning contexts."}, {"title": "A. Architecture & Model", "content": "MLPs, also known as fully connected (FC)-layers or Feed-forward NNs, are simply Perceptrons (neurons) stacked in vertical layers (shallow NNs), whose function is:\n$Yn = \u03c6(x, \u03c9\u03ba) = x \u03a3wk = \u03c6(\u03a3XnWk )$\nwhere xn are the input features, wk a set of scaling coefficients (weights) and \u03c6(\u00b7) a non-linear function (often activation): in our case a standard ReLU [28].\nThe One-Class-One-Network (OCON) model [29], introduced in the '90s, served as a parallel distributed processing solution to overcome limitations of architectures which required full re-training when altering dataset classes. Today, OCON resembles a simplified form of architecture ensembling e.g. when multiple complex networks are combined through other networks or algorithms, to enhance overall models accuracy. In this case, a multi-output classification is distributed across independent sub-networks, each functioning as binary classifier: an approach widely used in anomaly detection and computer vision [30], [31]. In our case, we split a 12-phoneme classification task into a bank of 12 independent classifiers with identical architectural topology, seeking an optimal average architecture estimate.\nIf a single output label is needed, a context-specific output algorithm must be devised. However, we find the classification logits vector more beneficial for understanding phonetic class boundaries and feature complexities.\nWhile no literature references were found regarding OCON-specific output algorithms, the argument of the maxima (ArgMax) approach can be employed, as it typically returns a single value representing the first occurrence of the maximum, when multiple occurrences exist.\nDuring supervised training, each sample label needs to be binarized (one-hot encoded) according to the class architecture, while features are simultaneously fed to all classifiers. To accomplish this, we developed a custom one-hot encoding technique (Alg. 1) to transform labels based on the incoming True-One-class. Additionally, due to previous observations (Table II), we needed to slightly down-sample resulting subsets, injecting further under-representation, to achieve nearly perfect training class balance."}, {"title": "B. Pseudo NAS & HPS-T heuristic search", "content": "The term pseudo-NAS refers to the a priori constraint applied to the architecture topology (MLP), as reported in Sec. III-A. This evaluation assesses the optimal number of layers and nodes (per layer) required to effectively solve both phoneme and gender classification tasks. On the other hand, Grid-based HPs search is a statistical method where all possible combinations of NNs HPs are independently sampled and evaluated through direct learning procedures. While theoretically effective, it can become time-consuming due to the exponentially increasing computational requirements for narrowing resolutions: in a standard procedure, all possible combinations must be tested before selecting the optimal one.\nIn our scenario, we achieved a good trade-off establishing independent resolutions for each HP beforehand, employing an informed iterative approximation approach, outlined as follows: (1) define a specific HPs subset (not necessarily all at once, eventually fix others); (2) sample each HP with an arbitrary resolution; (3) test each HPs combination and resulting temporary best estimates can be considered: inheritable in following heuristic stages (optimal estimates) or used to narrow parameters resolution sampling around local good estimates (searching for better sets); (4) go to step (2) and re-iterate as much as needed.\nWe acknowledge that this simplified approach roughly approximates theoretical grid-search and that may lead to misleading local minima in the model costs. However our aim is to find an average One-Class topology in a computationally feasible manner. Our heuristic learning experiments implies dataset partitioning into train (70%), dev (15%), and test (15%) sets with seeded initial states. We measure accuracy and mini-batch training times, averaging results over a 3-fold validation procedure for each One-Class\nIn the 1st heuristic stage (Table IV), which combined pseudo-NAS and HP-T experiments, two architectural combinations (10th and 15th) yielded similar average accuracies (93.67%, Fig. 3). The RMSProp optimizer [32] appears to mitigate the increasing trend in learning times better than Adam [33] does, but we opted for the top-performing configuration (HL: 1, HN=100, LR=10-4, Backprop: Adam).\nNext heuristic stages were structured to evaluate gradual introduction of regularization techniques, evaluating potential benefits. In DropOut [34] tests (Table V, Fig. 4), we found that the best accuracy run (the 1st) is also among the fastest. We achieved 0.19% increase in prediction accuracy at the expense of +3.4sec. in training time: resulting DropOut probabilities are 80% for input nodes and 50% for hidden nodes. In Batch-norm [35] tests, after re-evaluating LRs, it was confirmed"}, {"title": "IV. MODEL TRAINING & RESULTS DISCUSSION", "content": "A parallelized set of independently trainable One-Class architectures was scripted and CPU runtimes (Google Colab) were utilized to efficiently measure isolated training cycle performance and resources consumption. The OCON architecture relies on the backprop loop of each MLP for consistent learning, while its inference involves extracting sample features array, computing 12 parallel one-hot encodings and conducting an ArgMax search to find the maximum value (predicted label) within the 12-logit probabilities vector.\nIt follows a review of the phoneme recognition experiments, where we tested the efficiency of each dataset sub-structure (Sec. II-B). An Early-stopping training strategy [37] was adopted, setting a 2-variables-match escape condition: a minimum loss threshold (averaging among last 50 training samples loss) and a minimum test accuracy threshold (w.r.t. last batch results). These variables were further empirically assessed to ensure practical convergence of training cycles, with each cycle not exceeding 25-30min. While the learning phases may not be optimally exploited, they were deemed satisfactory for our study's purposes."}, {"title": "A. Phonemes recognition", "content": "Initially we re-evaluated the OCON model using the SS dataset variant (Table VII):\nfew loss functions and training accuracy curves reached visible plateaus, with periodic artifacts (spikes) indicating batch re-shuffling instances. Surprisingly, the er and iy phoneme classes resulted well-represented, showing little or no changes in trend after encoding or re-shuffling: this suggests that under-represented classes remained the best represented (Table VII). Classification accuracies were evaluated over the whole dataset (binary classification threshold set at 0.5), with certain MLPs achieving a sufficient probabilities segregation: however, errors were high between aurally closest classes (e.g.: ae and eh, er and ei).\nHidden dataset biases, such as children vs women utterances (exceptional similarities in formantic disposition) were re-examined, leading to slight improvements in class boundaries upon filtering out (due to lack of insights about vocal maturity of children samples) albeit increasing training duration: attempts to re-introduce F0s data to improve gender recognition were unsuccessful (AVG acc.: 88.80%, OCON acc.: 74%). The most performative features-set comprised temporal-tracks of formant ratios (Table VIII), leading to significant improvements in accuracy, reduced training times and mitigated Early-stopping side-effects, approaching the reference [38] accuracy goal of 90% (Table IX)."}, {"title": "V. CONCLUSIONS AND FUTURE WORKS", "content": "We know that a single Perceptron can actually predict a speech signal sample [39] approximating LPA results: our model proposal could be considered then an ad-hoc head-integration for a complex Perceptron-based formant neural framework. We are aware of researches on formants estimation [40], [41] leveraging convolutional and recurrent layers (backbone stages): despite accuracies achieved, we believe our approach, employing pseudo-NAS/HP-T techniques completely hand-scripted and fully conducted on Colab notebooks, could be broaderly re-applied to effectively assess newer SoA NNs building blocks efficiency, in terms of parameters reduction.\nOur model exhibits high distributability, with each classifier independently re-trainable and sufficiently lightweight to be suitable for constrained computational contexts and integration into complex architectures. Optimization techniques, like parameters prunings and quantization could further enhance its memory consumption at inference time. Additionally, its modular structure facilitates adaptation into different language contexts. We challenge the notion that larger datasets or models inherently yield better accuracies, asserting that our approach offers good generalizability, despite observed limitations in training sample size. We encountered difficulty finding more extensive pre-processed datasets but we aim to validate our findings by expanding our dataset sources, potentially including datasets like TI-MIT, UCLAPhoneticsSet and/or AudioSet. Our proposal for linear features processing demonstrates that altering speech signal spectra in auditory-based non linear ways isn't always the optimal method for descriptive speech modeling. However, we intend to re-examine solutions from existing literature.\nFuture research could focus on enhancing label selection, considering the potential for increased reliability by applying training assurance scaling coefficients to output One-Class probabilities: this could involve analyzing epochs spent by the classifier to maintain loss below the specified Early-stopping threshold and considering derivatives of the loss curve to further refine the output probabilities, especially in cases of too rapid training error minimization."}]}