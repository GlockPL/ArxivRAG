{"title": "Putting the Iterative Training of Decision Trees to\nthe Test on a Real-World Robotic Task", "authors": ["Raphael C. Engelhardt", "Marcel J. Meinen", "Moritz Lange", "Laurenz Wiskott", "Wolfgang Konen"], "abstract": "In previous research, we developed methods to train\ndecision trees (DT) as agents for reinforcement learning tasks,\nbased on deep reinforcement learning (DRL) networks. The\nsamples from which the DTs are built, use the environment's\nstate as features and the corresponding action as label. To solve\nthe nontrivial task of selecting samples, which on one hand reflect\nthe DRL agent's capabilities of choosing the right action but on\nthe other hand also cover enough state space to generalize well,\nwe developed an algorithm to iteratively train DTs.\nIn this short paper, we apply this algorithm to a real-world\nimplementation of a robotic task for the first time. Real-world\ntasks pose additional challenges compared to simulations, such\nas noise and delays. The task consists of a physical pendulum\nattached to a cart, which moves on a linear track. By movements\nto the left and to the right, the pendulum is to be swung in the\nupright position and balanced in the unstable equilibrium. Our\nresults demonstrate the applicability of the algorithm to real-\nworld tasks by generating a DT whose performance matches\nthe performance of the DRL agent, while consisting of fewer\nparameters. This research could be a starting point for distilling\nDTs from DRL agents to obtain transparent, lightweight models\nfor real-world reinforcement learning tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent studies, we explored the possibilities of deriving\ndecision trees (DTs) from trained deep reinforcement learning\n(DRL) agents. Using samples, consisting of the states of\nthe environment as features and the corresponding actions\nas labels, the reinforcement learning (RL) problem can be\ntranslated to a supervised learning problem [1]. The choice of\nsamples proved to be of determining importance for the suc-\ncessful training of the DTs. Using episodes of well-performing\nDRL agents, only a very narrow region of the state space\nis covered, sometimes too small for a DT to represent a\nsuccessful policy. We developed an algorithm that iteratively\nuses DTs to explore regions of the state space during episodes\nand the DRL agent as a sort of \"teacher\" to label these states\nwith the \"correct\" actions [2]. We successfully tested this\nalgorithm on a variety of classical RL challenges and were\nable to find oblique DTs\u00b9 that not only match the performance\nof their DRL \u201cteachers\u201d but in most cases even surpass them.\nThe experiments had, however, only been conducted on\nsimulated control problems from the gymnasium suite [3].\nIn this short paper, we explain how we applied the iterative\nalgorithm to a real-world robotic task. We describe the imple-\nmentation of the task and the experimental setup, and show\nhow our results prove the applicability of the algorithm. We\nalso discuss limitations and potential improvements for further\nexperiments."}, {"title": "II. RELATED WORK", "content": "The interplay of RL and robotic applications in the real\nworld has been the subject of interest in various studies.\nLee et al. [4] apply RL to train the locomotion of a blind\nquadrupedal robot in simulation and test its robustness in\nthe real world on rough terrain, unseen during training. This\nstudy is relevant, as it shows the feasibility of transferring a\npolicy from the simulated to the real domain in a robotics\ntask, especially since the complexity of reality is expensive to\napproximate in simulation.\nSong et al. [5] apply DRL to find near-time-optimal tracks\nfor drone racing. The challenge consists of flying a quadrotor\ndrone along a predefined track consisting of various gates the\ndrone must fly through in the shortest amount of time possible.\nThey rely on the DRL algorithm PPO to find a policy mapping\nthe quadrotor's state and observations about future gates to\nthrust of the four individual rotors. Although most of their"}, {"title": "III. ROBOTIC RL TASK", "content": "The robotic task studied in this paper is a real-world\nimplementation of the CartPole Swing-up (CPSU) challenge.\nAn inverted physical pendulum is attached to a cart with\nan unactuated hinge. The cart itself can move freely on a\nlinear track. By pushing the cart to the left or to the right,\nthe inverted pendulum is to be swung up to the point of\nunstable equilibrium. Once this first goal has been achieved,\nthe pendulum must be balanced in the upright position for the\nrest of the episode. The pole balancing problem (without the\nswing-up phase), described in [9], is a popular environment for\nRL and part of the widely-used benchmark suite gymnasium\n[3]. A simulation of the full problem, including the swing-up\nphase, is given by [10].\nIn the real-world implementation in the Lab for Applied\nArtificial Intelligence at TH K\u00f6ln University of Applied\nSciences, the physical pendulum is a 975 mm aluminum rod\nattached to a cart. The cart can be moved on a 1500 mm track\nby a belt actuated by a DC servo motor (by software, the move-\nment is restricted to \u00b1390 mm). The motor is controlled via\na Raspberry Pi mini-computer and an STM microcontroller.\nThe setup can be seen in Figure 1.\nSensors measure the four observables of the RL problem:\n\u2022 Angle of the rod u in the range [-180, 180] normalized\nfor the RL agent to u' \u2208 [-1,1]. The coordinate system\nwas chosen such that the pendulum hanging down in the\nstable equilibrium corresponds to 0, while the upright\nunstable equilibrium is defined as 180.\n\u2022 The rod's angular velocity \u00f9\n\u2022 Cart's position y normalized via division by 390. The\ncenter of the track corresponds to y = 0.\n\u2022 The cart's linear velocity \u00fd\nAt each timestep, the RL agent can choose to move the cart\nto the right, to the left, or not at all. It is therefore an RL\nproblem with a discrete action space. The selected action will\nthen be executed by the motor for a duration of 0.1s, before\nthe next time step begins. The episode terminates after 1000\ntimesteps, if the cart leaves the boundaries of the linear track\n|y| > 390, or if the angular velocity exceeds a safe range\n|\u00fa > 100. After each episode, a cool-down phase of 90s\nensures the pendulum comes to a complete standstill so that\nthe next episode starts in the same condition.\nThe reward function is chosen such as to reward the agent\nfor keeping the rod in the upright position and the cart\npreferably centered:\n$r = \\frac{1}{2} (1 - cos(\\frac{\\pi u}{180})) \\cdot cos(\\frac{\\pi}{2} \\frac{y}{390}) + r\\_{bonus}$ (1)\nwhere\n$r\\_{bonus} = \\begin{cases}\n10  \\text{ if } |u| > \\frac{6 \\cdot 180}{7} \\text{ and } |\\dot{u}| < \\frac{40}{57.3}\\\\\n0 \\text{ otherwise.}\n\\end{cases}$ (2)\nThis contains a bonus reward $r_{bonus}$, issued for each time\nstep in which the pendulum is considered close enough to the\nzenith, defined as sufficiently large angle u and small angular\nvelocity & in the first case of Equation (2)."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "The iterative algorithm was developed to derive DTs from\ntrained DRL agents. Here, we only outline its main working\nprinciples and refer to [2] for an in-depth discussion. Initially,\na number $N_0$ of base samples are collected during evaluation\nepisodes of the DRL agent. The samples consist of the states\nof the environment and the executed actions of the DRL agent.\nA number $N_T$ of DTs of depth d is trained on these samples\n(iteration 0). The performance of these DTs is then evaluated\nin a number $n_e$ of evaluation episodes. Next, the states of the\nbest-performing tree are labeled with the actions suggested by\nthe DRL agent's policy. These new state-action-pairs are added\nto the previous samples and a new set of DTs is trained on this\nnew, enriched, dataset (iteration 1). This interplay of using the\nexploration of DTs to generate states and the well-performing\npolicy of the DRL agent to provide the corresponding actions\ncontinues until a stopping criterion is met.\nFor our experiment, we trained $N_T = 10$ oblique DTS\nof depth d = 10 at each iteration using the algorithm and\nimplementation of [12] (OPCT). This best-out-of-10-approach\nis necessary given the rather high fluctuations of different DTs\ntrained on identical data due to initialization and has already\nbeen applied for experiments on simulated environments in [2].\nDue to the higher time costs of real-world evaluation episodes\ncompared to simulated ones, we only use $n_e = 5$ evalua-\ntion episodes for each DT in each iteration. The experiment\ncomprised ten iterations in total, as did the experiments on\nsimulated environments in [2].\nThe DRL agent we use to label the states and whose\nbehavior we want to approximate with DTs was trained on the\nsame system as part of a different research project [11]. The\nDQN consists of two dense layers with 64 neurons each and\nuses the tanh activation function. It comprises a total of 4675\ntrainable parameters. In 100 evaluation episodes, the DQN\nagent reaches an average return of $R = 7138.83\\pm1517.47$ on\nthe real-world pendulum, showing that the oracle itself is quite\ngood, but not perfect and with non-negligible fluctuations in\nthe performance.\nIt should be noted, that the real-world implementation en-\ntails additional challenges for the DRL agent compared to the\nsimulated environment. These include a time delay between\nissuing a prediction and actually executing the corresponding\naction, with consequences for the successful training of RL\nagents that are hard to anticipate. Additionally, sensor noise\naffects the agent's perception of the system's state, while other\nphysical factors, such as wear and mechanical play, introduce\nfurther challenges."}, {"title": "V. RESULTS", "content": "We present the results of the experiments. First, we show\nthe exploratory investigation of the base samples followed by\nthe results of the iterative DT training."}, {"title": "A. Base Samples", "content": "The base samples were collected during 100 episodes of\nthe DNQ agent. These episodes were filtered according to\ntwo criteria to ensure high-quality data. First, we discarded\nthe episodes, in which the pendulum never reached the state\ndefined as zenith (see Equation (2)). Two episodes were\nexcluded as a consequence. Additionally, we filtered out six\nadditional outlier episodes (i.e., episodes whose return was not\nwithin $[Q1 - 1.5 \\cdot IQR, Q3 + 1.5 \\cdot IQR]$). In the remaining\n92 episodes, the median time for the pendulum to reach\nthe zenith-state for the first time was 151 timesteps and the\naverage return was $R = 7468.72 \\pm 621.72$. Figure 2 shows\nthe performance of these episodes in terms of return, return\nwithout bonus, the number of timesteps the pendulum spent\nin the zenith-state, and the timestep in which the pendulum\nfirst reached this state.\nFinding the most helpful samples to feed into the tree-\nlearning algorithm is a nontrivial task. Given the episodes'\nlength of 1000 and the median time to first reach the zenith\nof 151 timesteps, taking all samples would overrepresent a\nstate in which the pendulum is in the upright position. DTs\ntrained on these samples fail to swing the system to its state of\nmaximum potential energy. It is therefore important, to limit\nthe samples to the first $t_e$ timesteps of each episode. Extensive\npre-studies lead to $t_e = 350$ as a reasonable compromise,\nyielding good results. For the base samples, as well as for all\nfollowing episodes in the iterative process, we therefore only\ninclude the first 350 timesteps of each episode."}, {"title": "B. Iterative DT training", "content": "As expected, in iteration 0, DTs trained only on the base\nsamples exhibit a rather poor performance (as seen from the\nleftmost 'iteration 0' results in Figure 3 and the central boxplot\nof Figure 4). The pendulum rarely reaches its zenith. In each\nfollowing iteration, the first $t_e = 350$ states of each of the\n$n_e = 5$ evaluation episodes of the best-performing DT are\nlabeled with the actions by the DQN and added to the previous\nsamples. The resulting DTs are therefore trained on gradually"}, {"title": "VI. DISCUSSION AND OUTLOOK", "content": "We could show, that the iterative algorithm to train DTs is\nsuccessfully applicable not only to simulated, but also to real-\nworld robotic tasks. It should be noted, that this experiment\nprimarily proves the feasibility; the results are subject to\nfurther improvement. A first remark goes toward the ratio of"}]}