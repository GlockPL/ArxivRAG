{"title": "TopoCL: Topological Contrastive Learning for Time Series", "authors": ["Namwoo Kim", "Hyungryul Baik", "Yoonjin Yoon"], "abstract": "Universal time series representation learning is challenging but valuable in real-world applications such as classification, anomaly detection, and forecasting. Recently, contrastive learning (CL) has been actively explored to tackle time series representation. However, a key challenge is that the data augmentation process in CL can distort seasonal patterns or temporal dependencies, inevitably leading to a loss of semantic information. To address this challenge, we propose Topological Contrastive Learning for time series (TopoCL). TopoCL mitigates such information loss by incorporating persistent homology, which captures the topological characteristics of data that remain invariant under transformations. In this paper, we treat the temporal and topological properties of time series data as distinct modalities. Specifically, we compute persistent homology to construct topological features of time series data, representing them in persistence diagrams. We then design a neural network to encode these persistent diagrams. Our approach jointly optimizes CL within the time modality and time-topology correspondence, promoting a comprehensive understanding of both temporal semantics and topological properties of time series. We conduct extensive experiments on four downstream tasks-classification, anomaly detection, forecasting, and transfer learning. The results demonstrate that TopoCL achieves state-of-the-art performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Time series analysis plays a crucial role in numerous real-world applications such as weather, economics, and transportation. However, analyzing time series data is challenging due to its incompleteness, vulnerability to noise, and complexity. Additionally, employing deep learning models for time series analysis introduces further obstacles, as it necessitates well-labeled data. Labeling time series data is time-consuming and requires a high level of expertise, creating significant bottlenecks in the analysis process. In recent years, contrastive learning (CL) has become increasingly popular in computer vision (CV) [1], [2] and natural language processing (NLP) [3], mainly due to its ability to train models without explicit labels. Accordingly, ongoing research is focused on applying CL to time series data for various downstream applications, including anomaly detection [4], [5], forecasting [6], and classification [7].\nIn general, CL leverages data augmentation to create positive and negative pairs for training, thereby creating a contrast between similar and dissimilar samples by exploring different transformations and variations of the data. Several augmentation techniques for time series have been proposed to learn robust and discriminative representations. For example, permutation reorders segments within a time series to produce a new sequence [7], [8]. However, using permutation on time series data might introduce inappropriate inductive bias, as it can distort the autocorrelated nature of time series data. Techniques borrowed from CV, such as flipping, which flips the signs of the original time series, may misrepresent the trend or seasonal patterns. Moreover, these distortions raise concerns about whether the augmented time series are indeed positive samples of the original series. Consequently, applying data augmentation may inevitably risk information loss, making it difficult to capture semantically meaningful representations. Therefore, it is necessary to find ways that compensate such potential information loss.\nRecently, Topological Data Analysis (TDA) has emerged as a subfield of algebraic topology focused on capturing the global shape of data, where shape broadly refers to data properties that remain invariant under transformations [9]. Such shape patterns, or topological features, include connected components, loops, and voids. Topological features are obtained by computing persistent homology and are represented as a multiset in persistence diagrams (PD) [10]. These diagrams encapsulate persistent homology across different scales of data. The overall structure and distribution of sets in the diagram provide insights into the topological invariants of the data. By incorporating such topological information, we can capture the essential structural properties of the time series that remain invariant under various transformations. Therefore, integrating topological information can help compensate for potential information loss due to data augmentation, reduce inductive bias, and lead to more robust representations.\nFueled by the advancements in both CL and TDA, we harness the strengths of the two methods and introduce Topological Contrastive Learning for time series (TopoCL), a simple yet effective cross-modal CL approach for universal time series representations. Our method integrates and leverages information from the time and topology modalities. To start, we construct topological characteristics from time series data. We apply delay embedding to time series data, compute persistent homology, and design a simple topological module to effectively encode topological information. Our approach then focuses on a joint objective: ensuring that the augmented versions of the same time series are closely embedded together in the feature space while preserving the correspondence between time and topology. The joint learning objective promotes the comprehensive understanding of temporal semantics and topological properties of time series and improves robustness to data augmentations. We perform extensive experiments on time series forecasting, anomaly detection, classification, and transfer learning tasks. The experimental results demonstrate that the learned representations of TopoCL are effective. The contributions of this work are as follows.\n\u2022\tWe demonstrate that incorporating topological properties is effective for capturing robust and discriminative representations of time series data.\n\u2022\tWe design a novel framework for learning representations of time series data that takes into account their topology. To the best of our knowledge, our approach is the first to combine persistent homology with contrastive learning in the context of time series representation learning.\n\u2022\tWe conduct extensive experiments across four downstream tasks: time series classification, anomaly detection, forecasting, and transfer learning on diverse datasets. The experimental results demonstrate the effectiveness of the proposed model.\nThe remainder of this paper is organized as follows. Section 2 presents a literature review on TDA and time-series contrastive learning. Section 3 provides preliminaries. The proposed framework is described in Section 4. Section 5 provides the experimental results of the proposed model. Section 6 presents additional analysis. Finally, Section 7 presents the conclusions and future work."}, {"title": "II. RELATED WORK", "content": "A. Topological Data Analysis\nTDA is an emerging field that utilizes abstract algebra to uncover intrinsic shape of the data. A fundamental concept in TDA is PD, which helps to understand the topological structure of data. By computing persistent homology across scales, such topological structure is represented as a multiset of points. Directly using PD in machine learning and deep learning models is challenging, since these models typically work with Hilbert space. To overcome this, several research efforts have been made to convert PD into vector format. For instance, persistence landscapes transform the multiset of points in a PD into a collection of piecewise-linear functions [11], [12] proposed persistence images, which converts PD into a fixed-size representation. Additionally, various kernel functions have been proposed to handle PD, such as geodesic topological kernel [13] and persistence landscape-based kernels [14], [15].\nRecently, the integration of TDA into neural networks has been explored, leading to the development of various topological layers for machine learning applications. The first approach to input a PD into a neural network architecture was presented by Hofer et al. [16]. Carri\u00e8re et al. [17] proposed PersLay, which incorporates PD for graph classification. Moor et al. [18] proposed topological autoencoder preserving topological structures of the input space in latent representations. Kim et al. [19] proposed PLlay, a neural network layer for learning embeddings from persistence landscapes. These approaches demonstrated that incorporating persistent homology with deep learning has the potential to offer a more comprehensive understanding of data.\nB. Time Series Contrastive Learning\nSimilar to the advancements of CL in the fields of CV and NLP, numerous studies incorporate CL into time series analysis. TS-TCC [7] employed both weak and strong augmentations to time series data and utilized contextual contrasting to learn transformation-invariant representations. Mixing-up [20] generated an augmented sample by mixing two data samples and proposed a pretext task method for predicting the mixing proportion. Inspired by great success in masked modeling in NLP and CV, several works adopted this approach for time series. TS2vec [4] used masking and cropping for data augmentation, and proposed a hierarchical contrastive loss to learn scale-invariant representations. SimMTM [21] generated multiple masked series and facilitated reconstruction by assembling complementary temporal variations from multiple masked series. InfoTS [22] exploited a meta-learning framework to automatically select the augmentation method.\nRecently, several studies have focused on the use of the frequency domain of time series data. CoST [6] proposed disentangled seasonal-trend representation learning framework, incorporating contrastive loss in both the time and frequency domains. TF-C [23] proposed a novel contrastive loss to maintain consistency between frequency and time domain representations. TimesURL [5] proposed a novel frequency-temporal-based augmentation method to maintain temporal dependencies of time series. However, it still remains unclear whether the proposed methodologies effectively mitigate the inevitable information loss resulting from data augmentation. To address this issue, we propose using persistent homology to capture the essential structural properties of time series data, thereby mitigating such information loss."}, {"title": "III. PRELIMINARY", "content": "In this section, we give the formal definitions of the major terminologies of TDA used in this study.\nA. Delay Embedding\nTakens's delay embedding [24] is a commonly used technique for converting time series data into point cloud representations. Given an i-th time series Xi = {xi,1,xi,2,...,xi,T}, one can extract a sequence of vectors of the form $[x_{i,t}, x_{i,t+\\gamma}, ..., x_{i,t+(m-1)\\gamma}] \\in \\mathbb{R}^m$, where m is the embedding size and \u03b3 is the time delay.\nB. Simplicial Complex\nIn algebraic topology, a simplex is a geometric object that generalizes the notion of a triangle or a tetrahedron to higher dimensions. Specifically, a k-simplex is defined as a convex hull spanned by k + 1 points v \u2208 Rm that are affinely independent. Then, a simplicial complex K is a finite collection of simplices such that:\n\u2022\tIf \u03c3\u2208 K, every subset of \u03c3 (i.e., every face of \u03c3) is an also element of K\n\u2022\tIf two simplices \u03c31, \u03c32 \u2208 K intersects, their intersection is a face of both \u03c31 and \u03c32\nC. Vietoris-Rips Complex\nTo construct a simplicial complex, we used the Vietoris-Rips complex [25]. Let (X, d) be a finite metric space equipped with a distance function d. The Vietoris-Rips complex VR(X, \u2208) is the abstract simplicial complex whose simplices are subsets of X with diameter (i.e., the maximum pairwise distance between points) at most \u2208 \u2265 0. That is, a subset S of X is a simplex in VR(X, \u2208) if and only if the distance d(x,y) \u2264 \u2208 for any two points x,y \u2208 S . The 0-simplices of VR(X, \u2208) are the points of X, and the k-simplices are the subsets of X of cardinality k + 1 that can be realized as the union of k + 1 closed balls of radius e with non-empty pairwise intersections.\nD. Homology\n1)\tSimplicial Homology: Given a simplicial complex K, a weighted sum of n-simplices with coefficients from Z/Z2- defines an n-chain on K. Then the space of n-chains on K, represented by Cn(K), is defined as the vector space spanned by n-simplicies of K over Z/Z2. The modules C0, C1,... are connected by boundary operators \u2202n : Cn(K) \u2192 Cn-1(K). The boundary operator for a simplex \u03c3 = [v0, ..., vn] is defined as:\n$\\partial_n(\\sigma) = \\sum_{i=0}^n [v_0, ..., \\hat{v_i}, ..., v_n]$ (1)\nwhere [v0, ..., \\hat{vi}, ..., vn] denotes the (n - 1) simplex spanned by all the vertices except vi. It is easy to see that im(\u2202n+1) is contained in ker(\u2202n) for each n. The n-th homology group of K is defined as:\nHn(K) = ker(\u2202n)/im(\u2202n+1). (2)\nThe n-th Betti number of K, denoted as \u03b2n(K), is the dimension of the n-th homology group of K, which can be computed as the difference between the kernel dimension of \u2202n and the image dimension of \u2202n+1. In simpler terms, the n-th Betti number indicates the number of n-dimensional voids that exist in the simplicial complex. For example, \u03b20 indicates the number of connected components, while \u03b21 represents the number of loops.\n2)\tPersistent Homology: Persistent homology is a valuable technique in TDA for understanding the shape of complex data sets. For a given simplicial complex K, let (Ki) be a nested subcomplexes of K such that \u00d8 = K0 \u2286 K1 \u2286 ... \u2286 Km = K, called a filtration. The inclusion maps Ki \u2192 Kj defined by f(x) = x induce fi,j : Hp(Ki) \u2192 Hp(Kj) for all i, j with 1 \u2264 i \u2264 j \u2264 m, for all p. Then, the sequence of homology groups is represented as follows\n0 = Hp(K0) \u2192 Hp(K1) \u2192 ... \u2192 Hp(Km) = Hp(K). (3)\nFor given i < j and p, we can consider the p-th boundary maps \u2202pKi : Cp(Ki) \u2192 Cp-1(Ki) and \u2202pKj : Cp(Kj) \u2192 Cp-1(Kj). Since Ki \u2282Kj, one can see Cp(Ki) and Cp-1(Ki) as subspaces of Cp(Kj) and Cp-1(Kj), respectively. Then both im(\u2202p+1Ki) \u2282Cp(Kj) and ker(\u2202pKi) \u2282Cp(Ki) can be considered as subspaces of Cp(Kj). Then p-th persistent homology group is defined as:\nHip,ij = ker(\u2202pKj)/(im(\u2202p+1Ki) \u2229ker(\u2202pKi)) (4)\nNote that an element of ker(\u2202pKi) represents both an element of Hp(Ki) and an element of Hp(Kj). If it vanishes in Hip,ij, then it must be in im(\u2202p+1Ki), hence it vanishes in Hp(Kj) as well. In summary, the homology group Hip,ij consists of the p-th homology classes of Ki that are still present at Kj, i.e, those not in the kernel of the map fi,j.\nThe p-th Betti number, which refers to the rank of the homology group, is defined as: \u03b2ip,ij := rank Hip,ij. By analyzing the sequence of Betti numbers, persistent homology enables the systematic measurement of topological features (e.g., holes and voids) in dataset across multiple scales. Such topological information can be summarized as PD.\nE. Persistence Diagram\nIn TDA, PD is a useful tool for summarizing topological information from data across different scales. A PD captures topological features such as connected components and loops, showing when these features appear and disappear as the scale parameter changes. The p-th PD Dgmp, is defined as a multiset collection of points (i,j) with \u03bcip,ij = 1, where \u03bcip,ij is calculated as:\n\u03bcip,ij = (\u03b2ip\u22121,j \u2212 \u03b2ip,j) \u2212 (\u03b2ip\u22121,i \u2212 \u03b2ip,j\u22121) (5)\nEach point (a, b) \u2208 Dgm, corresponds to a homological feature that is born at scale a and dies at scale b. This homological feature has the persistence value of b - a. Figure 1 illustrates an example of PD based on the Vietoris-Rips filtration. The diagram effectively summarizes evolving homological information with respect to \u2208. For instance, as \u2208 increases 0 to 1, the number of connected components decreases 4 to 2, which is reflected as a point (0,1) on the persistence diagram."}, {"title": "IV. PROPOSED TOPOCL FRAMEWORK", "content": "In this section, we present TopoCL framework.\nA. Problem Formulation\nOur goal is to train a neural network fe to transform each time series instance xi \u2208 RT\u00d7C in the time series set Xtime = {X1,X2, ..., XN} into its representation ri. Here, N denotes the number of samples, T denotes the number of timestamps, C represents the number of variables. The representation ri is expressed as {ri,1,ri,2,..., ri,T}, where ri,t \u2208 RF is the embedding vector at time t, with F being the dimension of embedding vector.\nB. Overview\nThe overall framework of TopoCL is shown in Figure 2. Given Xtime, we construct the topological modality Xtopo by computing its persistence diagram. Then, Xtime is fed into the temporal module to capture temporal dependencies. Concurrently, Xtopo is processed by the topological module to learn the underlying topology of the data. Temporal features are learned using a time modality contrastive loss Ltime. Additionally, an auxiliary contrastive objective Lcross is applied to align temporal features and topological features. The output of the temporal encoder ftime is used for downstream applications.\nC. Persistence Diagram Construction\nTo extract topological features from a time series, we first convert x \u2208 Xtime into a point cloud format, enabling the construction of the Vietoris-Rips filtration. This involves applying delay embedding to the input sample x. Next, we compute persistence diagrams for H0 and H1 homology groups, summarizing the topological features of the data. For each channel c in time series data, we obtain channel-specific 0- and 1-dimensional persistence diagrams Dgm0(x) and Dgm1(x). A composite diagram Dgm(x) is then created as Dgm(x) = UcDgmc(x). Next, points in Dgm(x) are mapped using \u03b4: (a,b) \u2192 (a, b, b \u2212 a), resulting in point cloud data xp \u2208 RM\u00d73, where M is the maximum number of topological features. The set of topological feature is denoted as Xtopo = {x1, x2,...,xN}.\nD. Topological Feature Extraction\nWe design a topological feature extractor ftopo, which directly processes unordered point sets xp as inputs. Given that a point cloud is an unordered collection of points, a neural network capable of processing these M point sets needs to be invariant to M! possible permutations of the input set's order during data feeding.\nInspired by the works on PointNet [26] and Deep Sets [27], we apply a simple symmetric function to combine information from all the points in xp. Then, the model is unaffected by the order of input points, preserving invariance to input permutations. As illustrated in Figure 2, xp passes through the series of shared multilayer perceptrons with ReLU activation. Subsequently, a max-pooling layer aggregates information from all points to produce a topological representation vector h\u2208 RH, where H is the dimension of representation vector.\nE. Temporal Feature Extraction\nCapturing the temporal dependency is essential for time series representation learning. Given an input instance xi, we construct augmented versions of it. Then, the time series feature extractor ftime maps them to a feature embedding space, resulting in embeddings ri and r'i.\nTo learn discriminative and robust temporal representations, we use temporal and instance-wise contrastive losses, Ltemp and Linstit) [4]. For a given timestamp t, these two loss functions can be formulated as:\n$\\mathcal{L}^{(i,t)}_{temp} = - \\log \\frac{\\exp(r_{i,t} \\cdot r'_{i,t})}{\\sum_{t'} (\\exp (r_{i,t} \\cdot r'_{i,t}) + \\mathbb{1}_{[t\\neq t']} \\exp (r_{i,t} \\cdot r'_{i,t}))}$ (6)\n$\\mathcal{L}^{(i,t)}_{inst} = - \\log \\frac{\\exp(r_{i,t} \\cdot r_{i,t})}{\\sum_{j} (\\exp (r_{i,t} \\cdot r'_{i,t}) + \\mathbb{1}_{[i \\neq j]} \\exp (r_{i,t} \\cdot r'_{j,t}))}$ (7)\nF. Time-Topology alignment\nIn addition to Ltime, which learns instance-specific characteristics and temporal variation, we propose a contrastive objective for alining time and topology modalities. To this end, we first map the embedding vectors ri,t, r'i,t \u2208 RF from temporal encoder ftime, and topological features hi \u2208 RH to the multi-modal latent space. Then, we apply projection headers projtime and projtopoas follows:\nyi = projtopo(hi) (9)\nzi = projtime(rt) (10)\nz'i = projtime(r't) (11)\nwhere, rt is an instance level representation of ri,t which is obtained by applying max-pooling across all timestamps. The average of zi and z'i is computed as:\nz'i = \\frac{1}{2} (zi + z'i) (12)\nNext, we maximize the similarity between zi and yi, which are mapped into the multi-modal latent space. The similarity between zi and yi is defined as s(zi, yi) = zTiyi/\u03c4, where \u03c4 refers to the temperature parameter. For a mini-batch B, (zi, yi) is regarded as a positive pair, where as the remaining 2B - 2 feature vectors constitute negative pairs. Our cross-modal contrastive loss between zi and yi can be formulated as:\n$\\mathcal{L}_{align}^{(i; time,topo)} = - \\log \\frac{\\exp(s(z'_i, y_i))}{\\sum_{j \\in B}(\\exp(s(z'_i, y_j)) + \\mathbb{1}_{[j \\neq i]}\\exp(s(z'_i, z'_j)))}$ (13)\nThe final cross-modal contrastive loss is formulated as:\n$\\mathcal{L}_{cross} = \\frac{1}{2B} (\\sum_{i \\in B} \\mathcal{L}_{align}^{(i; time,topo)} + \\mathcal{L}_{align}^{(i; topo,time)})$ (14)\nG. Overall Objective\nThe proposed model jointly optimizes CL in time modality and time-topology correspondence. The overall loss function can be formulated as follows:\nL = Ltime + \u03b1Lcross (15)\nwhere \u03b1 is a hyperparameter.\nPlease note that instance discrimination using contrastive loss is not performed on topology modality. Instead, we apply contrastive loss to the time modality and leverage topology modality to enhance time series representation learning. The assumption is that time-topology alignment allows learning comprehensive and complementary semantic information by incorporating temporal features and their corresponding topological features."}, {"title": "V. EXPERIMENTS", "content": "In this section, we conduct experiments on anomaly detection, classification, forecasting, and transfer learning tasks by applying TopoCL to state-of-the-art methods. Implementation details are presented in the appendix.\nA. Anomaly Detection\nAnomaly detection is used for identifying and addressing unusual patterns or behaviors across various domains. In this task, Yahoo [28] and KPI [29] datasets are used. We apply TopoCL to TS2Vec [4] and the evaluation is performed under normal and cold-start settings. For the normal setting, each sample is divided into two portions: one for training and the other for testing. For the cold-start setting, the model is pretrained on the FordA dataset from the UCR archive before evaluating on each target dataset. Following the evaluation protocol proposed by [28], we determine whether the last point is an anomaly.\nSPOT, DSPOT [30], DONUT [31], SR, and TS2vec methods are used as baselines for the normal setting, while FFT [32], Twitter-AD [33], and Luminol [34], SR, and TS2vec are used as baselines for the cold-start setting. As shown in Table I, in the normal setting, TopoCL improves the F1 score by 1.9% and 2.2% on the Yahoo and KPI dataset, respectively. In the cold-start setting, our model outperforms the best baseline, achieving a 1.8% higher F1 score on the Yahoo dataset and a 0.5% improvement on the KPI dataset.\nB. Classification\nIn this section, we conduct a classification task to evaluate the performance of the proposed model using the UCR [35] and UEA [36] archives, selecting 125 univariate and 29 multivariate time series datasets from each archive. We train an RBF kernel based SVM classifier on top of the learned representation, following the evaluation protocol used in T-Loss [37]. The penalty factor for the RBF kernel is is selected by performing a grid search on the validation set, exploring the range {102 | i \u2208 [\u22124,4]}\nWe apply TopoCL to TS2vec and compare its peformance with baselines including DTW, TST [38], TS-TCC [7], T-Loss [37], TNC [39], TS2vec, and InfoTS [22]. The classification results for the these datasets are summarized in Table II. The classification accuracy of the proposed model outperforms the best baseline by 3.8% on the UCR datasets and by 1.4% on the UEA datasets. Detailed classification results are provided in the appendix.\nTo further evaluate the effectiveness of TopoCL, we perform a significance test on the classification results. In the Figure 3, Critical Difference diagram [40] for Nemenyi tests on all datasets is presented. The Critical Difference diagram indicates that methods connected by a bold line do not have significantly different average ranks. From the results, we can conclude that the proposed model significantly outperforms other methods in average ranks.\nC. Forecasting\nIn this task, we use ETT [41] dataset and apply TopoCL to CoST [6] and TS2Vec. We split the entire length of the time series into 60% for training, 20% for validation, and the remaining 20% for testing. For evaluation, a ridge regressor with L2 regularization is trained on top of the representation of the last time stamp rt, to forecast the future timestamps. Specifically, we predict the future \u0397 \u2208 24, 48, 168, 720 observations based on the past H time points. The validation set is used for selecting regularization penalty factor, exploring a search space of {0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000}.\nWe use Informer [41], LogTrans [42], TCN [43], LSTnet [44], TS2Vec, and CoST as baselines, with mean absolute error (MAE) and mean squared error (MSE) serving as performance metrics. The evaluation results for univariate and multivariate forecasting are shown in Table III and IV, respectively. In general, CoST + TopoCL outperforms the baselines in most cases, achieving reductions of 1.94% in univariate forecasting and 1.76% in multivariate forecasting, in terms of MSE. Furthermore, TS2Vec + TopoCL demonstrates a 3.39% and 3.33% decrease in average MSE compared to TS2Vec alone for univariate and multivariate forecasting, respectively.\nD. Transfer Learning\nTransfer learning on time series data brings a unique challenge, stemming from the complex nature of the data such as rapidly changing trends and shifts in temporal dynamics. In this section, we conduct transfer learning, following experiment settings proposed by TF-C [23].\nThe model is pretrained on SleepEEG dataset and finetuned on Epilepsy [45], Gesture [46], FD-B [47], and EMG datasets [48]. We apply TopoCL to TS-TCC [7] and compare our approach against several baselines: TS-TCC, TS2vec, CoST, Ti-MAE [49], TF-C, and TST. Performance is evaluated using accuracy and F1 score. As shown in Table V, TS-TCC combined with TopoCL results in a notable improvement.\nE. Ablation Study"}, {"title": "VI. ANALYSIS", "content": "In this section, we compare TopoCL to its five variants on 128 UCR datasets to study the effectiveness of the proposed components. The five variants are as follows: (1) w/o time-topology alignment removes the time-topology cross-modal alignment from TopoCL, (2) w/o time domain contrastive loss removes the instance and temporal contrast from ftopo, (3) w/o H0 excludes the 0-dimensional persistent homology when computing the persistence diagram, (4) w/o H1 excludes the 1-dimensional persistent homology, and (5) avg-pool replaces the symmetric aggregate function (max-pool) in ftopo with avg-pool. As shown in Table VI, any absence of the components lead to decrease in performance, showing that all components are imperative.\nB. Robustness under Data Augmentation\nWe evaluate the robustness of TopoCL against various augmentation techniques using the Crop dataset, which has the largest number of test samples in the UCR archive. Specifically, we individually apply data augmentation techniques, including jittering, scaling, shifting, permuting, and flipping. As shown in Figure 4, TopoCL achieves steady performance compared to TS2vec. These results empirically validate that the time-topology alignment has a potential in compensating information loss during data augmentation process.\nC. Limited data scenario\nDue to sensor malfunctions, data transmission errors, or manual data entry issues, sufficient amount of data are not always guaranteed. To verify the effectiveness of the proposed method in scenarios with limited data, we adjust the proportion of training samples. FordB dataset from UCR archive is chosen for this analysis. The classification results using only 1% to 9% of the original training dataset are shown in Figure 5. The proposed method consistently outperforms w/o time-topology alignment. This result suggests that leveraging topological properties can capture the intrinsic characteristics of the data, even with a small amount of data."}, {"title": "VII. CONCLUSION", "content": "This paper presents TopoCL, a topological contrastive learning for time series. We treat temporal and topological properties of time series as distinct modalities and propose a joint learning objective to enhance understanding of both, while also improving robustness to data augmentations. TopoCL is evaluated across multiple tasks: time series classification, forecasting, anomaly detection, and transfer learning. The experimental results demonstrate the universality, generalization capability, and effectiveness of the proposed model. Our ablations show that the joint learning of time modality contrastive loss and time-topology correspondences enhances these capabilities. Moreover, TopoCL shows consistent performance across various data augmentation techniques, suggesting its effectiveness in mitigating information loss resulting from these augmentations. In the future, we aim to extend TopoCL to accommodate large-scale and diverse pre-training datasets, thereby advancing toward a foundation model for time series analysis. However, calculating persistent homology from large-scale time series data requires significant computational costs. To address this, we aim to explore methodologies to reduce the increasing computational burden of persistent homology calculations."}]}