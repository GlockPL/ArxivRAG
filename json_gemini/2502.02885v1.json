{"title": "Expertized Caption Auto-Enhancement for Video-Text Retrieval", "authors": ["Junxiang Chen", "Wenbin Yao", "Baoyao Yang"], "abstract": "The burgeoning field of video-text retrieval has witnessed significant advancements with the advent of deep learning. However, the challenge of matching text and video persists due to inadequate textual descriptions of videos. The substantial information gap between the two modalities hinders a comprehensive understanding of videos, resulting in ambiguous retrieval results. While rewriting methods based on large language models have been proposed to broaden text expressions, carefully crafted prompts are essential to ensure the reasonableness and completeness of the rewritten texts. This paper proposes an automatic caption enhancement method that enhances expression quality and mitigates empiricism in augmented captions through self-learning. Additionally, an expertized caption selection mechanism is designed and introduced to customize augmented captions for each video, facilitating video-text matching. Our method is entirely data-driven, which not only dispenses with heavy data collection and computation workload but also improves self-adaptability by circumventing lexicon dependence and introducing personalized matching. The superiority of our method is validated by state-of-the-art results on various benchmarks, specifically achieving Top-1 recall accuracy of 68.5% on MSR-VTT, 68.1% on MSVD, and 62.0% on DiDeMo.", "sections": [{"title": "1. Introduction", "content": "Video-text understanding has become a new mainstream research with the population of short videos on social networks. In virtue of the learning ability of large models, multi-modal pre-training models [6, 11] are developed and achieve initial success in video-text retrieval tasks. However, a stunning amount of data and computing resources are the essential basis to support the effectiveness of these pre-training models. Thus, it is argued that adapting the image-text pretrained model to video-text tasks is a lighter and more feasible approach [15, 31]. They adapt the visual embeddings to the video field by incorporating temporal information [4, 21] or refining the visual-text matching strategies [8, 31]. But unlike image-text pairs, video captions tend to be overly simplistic and one-sided, describing partial (even a signal) frames of the videos. These one-sided descriptions would be improperly matched to multifaceted video embeddings (Figure 1 (a)), resulting in biased cross-modal retrieval, and a bottleneck in performance improvement arises.\nFrame sampling is one proper solution to handle the information imbalance issue. For instance, ClipBERT [10] sparsely samples video frames to meet the text descriptions. X-CLIP [16] further cuts text descriptions into words, employing cross-modal matching at both fine-grained (frame and word) and coarse-grained (video and sentence) levels. As video information is sparsely divided and broken, these methods are limited in comprehensively utilizing and exploring deep relations across videos and texts. Another branch to reduce the information gap is data agu-mentation. Extra data, such as HD-VILA-100M [30] and LF-VILA [21], are collected to expand the receptive field for model adaptation, thereby enriching matching knowledge of the two modalities. To eliminate the heavy data collection workload, later research [27] rewrites captions by lexical analysis, in which attributes are extracted and emphasized to enhance expressive ability in text modality. Large language models (LLMs), such as Chat-GPT and LLAMA [22], are additionally introduced for the rewriting tasks [7, 26]. Although up to 3% retrieval precision increase is achieved with the assistance of supplementary texts, the rewriting quality relies heavily on lexicon engineering [18] and hand-crafted prompts [17] (Figure 2 (a)). Recent works [2, 25] have been mired in the meticulous work of designing better prompts for caption augmentation, limited in human empiricism.\nThis paper proposes an Expertized Caption auto-Enhancement (ExCae) method, which adaptively generates and personalized select captions for video-text retrieval, releasing extra work of data acquisition and avoiding the trouble of manual prompt design. Specifically, a Caption Self-improvement (CSI) module is designed to derive captions with rich content and diverse perspectives from videos. A Prompt engineer is introduced in the CSI module, which gradually adjusts and improves the query prompts via self-learning. With the enriched expressions on the video-side (video+video-derived captions), cross-modal matching becomes a more challenging task. Thus, we further develop an Expertized Caption Selection (ECS) module consisting of multiple learnable experts to automatically specify appropriate expressions for cross-modal matching. Seeing Figure 1 (b), video representation space dilates with caption augmentation and the cross-modal information gap is reduced after expertized selection. This indicates that the proposed method improves the generalization ability through caption augmentation while enhancing the cross-modal retrieval ability with adaptive screening. As a fully data-driven process, sampling bias and empiricism that may confuse the cross-modal matching are avoided. We further encapsulate ExCae as a plug-in unit, which successfully improves the retrieval performance of current methods, validating ExCae's flexibility and adaptability.\nThe contributions of this work are summarized as follows:\n\u2022 We propose eliminating the grunt workload of data acquisition and prompt design in caption augmentation through self-improvement and expertized selection, thereby mitigating empirical bias in supplemental texts while facilitating cross-modal matching without extra data.\n\u2022 We design an automatic cross-modal learning framework for video-text retrieval, in which video expressions are expanded using a Prompt engineer and are matched to texts under the guidance of learnable experts. The Prompt engineer facilitates generating effective and multi-angled captions while the learnable experts personalize the appropriate expression angles for cross-modal matching.\n\u2022 Experimental results show that our method outperforms (or is comparable to) current works even without using extra data, especially surpassing the best benchmark on the MSR-VTT by 3.8%. This paper also taps the application potentials of ExCae in video-text retrieval via extensive analytical and plug-in experiments."}, {"title": "2. Related Work", "content": "Developing and pre-training video foundation models (VFMs) [11, 24] enhances the comprehension of video data, demonstrating efficacy in downstream tasks such as video-text retrieval. However, the effectiveness of VFMs heavily relies on huge amounts of data and computing resources. Much of the current work in video-text retrieval turns to a lighter modeling approach, adapting image-text retrieval models, such as CLIP [20], to video-text tasks. For instance, CLIP4Clip [15] and CLIP2TV [8] utilize the image knowledge encapsulated in CLIP, transforming it into effective video representations through different image aggregation networks. Besides, TS2-Net [14] dynamically adjusts the video token sequence, selecting tokens with richer information to enhance video representation. X-CLIP [16] employs an independent encoder for each frame, using a network that aggregates frames multiple times to generate the final video representation. DRL [23] introduces an additional loss to address the feature redundant issue. However, due to the information imbalance between videos and texts, these methods are stuck in insufficient cross-modal matching.\nTo reduce the cross-modal information gap, existing works, such as Cap4Video [26] and BIKE [27], enhance video-text matching via caption rewrite. They carefully design prompts, asking LLMs to rephrase the texts with empirically selective concepts, such as special objectives [18] or actions [17]. However, visual details remain deficient in the rewritten captions through these text-to-text literal restatements. More importantly, the hand-drafted lexicon and prompts severely limit the rewrite quality, motivating us to explore a self-learning caption enhancement method. To the best of our knowledge, this paper is a pioneer in investigating automatic caption enhancement to facilitate the comprehension of videos and texts."}, {"title": "3. Method", "content": "Unlike traditional text augmentation that rewrites captions on the text data side, our method (ExCae), automatically derives captions from videos by the CSI module and adaptively selects appropriate expressions (ECS module) to encode video for video-text matching (see Figure 2). Video and text embeddings would be aligned with standard cross-modal contrastive loss, as most existing SOTA methods [14, 15]. Both CSI and ECS are self-learning modules, whose details are introduced as follows."}, {"title": "3.1. Caption Self-improvement (CSI)", "content": "The objective of the CSI module is to learn the best prompt for caption generation. The main flow is illustrated in Figure 3, which is an iteration process gradually refining the query prompt. Specifically, a Captioner is asked to generate multiple captions to describe the video from different perspectives. We then evaluate the semantics of the generated captions, scoring them to update the prompt candidates' pool. A Prompt engineer is introduced to rewrite the current best prompt, whose output is used as the latest query prompt of the Captioner. The prompt update iteration would be ended when no new candidates are added to the prompt candidates' pool. (Pseudo code is provided in the Supplemental Materials.)\nCaption scoring Denote the query prompt of the Captioner as $p_t$, where $t$ records the iteration index. Correspondingly, the generated caption list for a video $v_i$ based on $p_t$ is written as $c$. As literal expressions, $c$ is evaluated from a semantic perspective. We advocate for the generation of captions that are semantically akin to the original texts, in order to mitigate the presence of cluttered and vacuous descriptions. However, merely preserving the semantic consistency may lead to generating a set of captions similar to the original text. This is opposite to the original intention of expression enhancement. To promote multi-angle descriptions, a diversity constraint that encourages variety within the caption list is supplemented. Suppose $K$ captions are contained in $c$, the caption generation score in the $t$ iteration round can be expressed as\n$Scoret = Ei,k[sim(\u03c8(c,k), \u03c8(ti))] + div(\u03c8(c))$ (1)\nwhere $t_i$ denotes the original text corresponding to $v_i$, $k \\in [1, K]$ is the caption index in $c$. $\\psi(\u00b7)$ represents a semantic extractor, implemented by M3-embedding [5]. $sim()$ and $div()$ are similarity and diversity calculations (See Supplemental Materials for details). If $Score^t$ surpasses the historical records, $p_t$ would be added to the prompt candidates' pool, and the best prompt is updated accordingly.\nPrompt refinement The current prompt engineering normally encodes the query prompts and adjusts the prompt tokens/embeddings through gradient descent [19, 32]. Although this parametric fine-tuning strategy is convenient for optimization, the adjusted embeddings could hardly be precisely re-projected to the text space, losing linguistic meaning and interpretability. Consequently, their generalization in video-text-related applications is limited. Inspired by language rewrites [7], this paper introduces a Prompt engineer to rewrite the query prompt. Take the current best prompt $p_t$ as input; the Prompt engineer is asked to rewrite $p_t$ with reference to the corresponding video-text pairs $(x, t)$ involved in this iteration. The rewritten prompt is then used as the query prompt for the Captioner in the next iteration, as shown in Figure 3."}, {"title": "3.2. Expertized Caption Selection (ECS)", "content": "With the optimized CSI module, video-text pairs are updated as $((v, \\check{c}), t)$, where $(v, \\check{c})$ represents the video data $v$ and its generated captions $\\check{c}$ using the optimal prompt, and $t$ denotes the original text corresponding to $v$. As displayed in Figure 2 (b), $(v,c)$ and $t$ are the video-side and text-side input data, respectively. A straightforward approach for cross-modal representation learning is directly matching the encoded representations of $(v, \\check{c})$ and $t$. However, the information dilation of video modality brings confounded knowledge while improving generalization, increasing the difficulty of cross-modal matching (see Figure 1).\nConsidering that the derived captions describe video content from multiple perspectives, we propose inserting an ECS module designed based on a mixture of experts to facilitate cross-modal matching. Write the outputs of visual encoders as $e_v = [l_{v1}, v_{2}, ...]$, which consists of $N + K$ elements, where $N$ is the number of sampled frames from videos, and $K$ is the number of derived captions. The ECS module is composed of multiple expert networks with learnable parameters $([f_1, f_2, \u2026, f_m])$, each of which refers to a description angle. $M$ denotes the total number of experts. A router $r_m$ is allocated to each expert $f_m$ as a gate, which is automatically learned to explore the most matched visual expression fusion for each video-text pair. Through this individualized video expression screening and fusion, the expression that is overly different from the corresponding texts is filtered on the video side, thereby narrowing the video-text modality gap. Mathematically, the visual codes are residually updated by a mixture of experts' outputs:\n$\\hat{v} = \\frac{1}{M} \\sum_m f_m(r_m(e_v) \\& e_v) + e_v$ (2)\nwhere $r_m(e_v)$ outputs attention weights of $(N + K)$ video-side expressions for each sample. Specifically, a top-R filter is applied on $r_m(e_v)$, which activates partial expressions to learn a specific expert. $\\& $ denotes the dot-product operation. As visual and textual embeddings will be aligned after further encoding by a shared encoder, parameters of the ECS module ($f_m$ and $r_m$) are optimized along with other encoding networks under the supervision of textual information."}, {"title": "4. Experiment", "content": "4.1. Datasets\nMSR-VTT [29] is a large-scale dataset containing 10,000 video clips, each of which is annotated with 20 English sentences. Around 29K unique words are contained in all sentences. Following previous work [13], we use 9,000 clips for training and 1,000 clips for testing.\nMSVD [3] consists of 1,970 videos with 80K captions. Each video contains around 40 English captions on average. The videos are split by 1,200/100/670 for training, validation and testing, respectively.\nDiDeMo [1] contains over 10,000 videos collected from Flickr. Each video is described by natural language, with detailed information on camera movement, temporal transition indicators, and activities. The dataset is split into training, validation and test sets, each containing 8,395, 1,065 and 1,004 videos, respectively.\n4.2. Implementation Details\nPreprocessing Following the standard text preprocessing [20], each sentence in original texts and video-derived captions is split into word tokens with a max length of 70 using a CLIP tokenizer. For videos, we sample 8 frames from each item as visual data.\nModel Setup This paper reports the results of our method on multiple backbones, i.e., ViT-B/16, ViT-L/14, ViT-H/14, ViT-G/14, compared to SOTA video-text retrieval methods, showing the effect of model scale on performance as well. Ten captions are derived for each video. Sixteen experts are learned in the ECS module, two of which are activated to select appropriate representations in both training and inference processes. In the training phase, the CSI module is pre-learned to acquire the optimal prompt for caption generation, while the ECS module is co-trained alongside the backbone parameters. In the evaluation phase, multiple captions are derived from each video using the optimal prompt, together with the video itself as video-side expressions. (See Supplemental Materials for the learned best prompt and examples of video-derived captions.) The Captioner and Prompt engineer are implemented by GPT-40, a large cross-modal inference model recently released by OpenAI in 2024. Besides the comparison experiment, we encapsulate ExCae as a plug-in unit and test its effect when introduced into existing video-text retrieval methods; specifically, we enhance video expression via CSI and professionalize the video-text matching process by ECS. Ablation studies and analytical experiments are conducted on MSR-VTT using ViT-G/14 as the base model.\nEnvironment and Evaluation Metrics Our model is implemented in PyTorch and trained on 8 NVIDIA A800 GPUs in a batch size of 16. We initialize the text and video encoder with pretrained weights from CLIP [20] and use the Adamax optimizer with a learning rate of 4e-6. Experimental results are evaluated by standard video-text retrieval metrics: Recall at Rank K (R@K, higher is better) and Mean Rank (MeanR, lower is better). R@K denotes the percentage of correct samples in the top K retrieved samples, where K is set to 1, 5 and 10, the same as in the previous works [9, 31]. MeanR records the average rank of correct items in the retrieved list."}, {"title": "4.3. Comparision to State-of-the-arts", "content": "Results of our method are compared to those of the SOTAs, including pre-trained foundation models like InternVideo [24], UMT-L [11], mPLUG-2 [28], VAST [6], and ViT-based methods that adapt the image-text models to the video-text task. Among the ViT-based methods, CLIP2TV [8], DRL [23], Clip4Clip [15] and CLIP-VIP [31] require the acquisition of auxiliary data for post-pretraining and/or adaptation. In contrast, TS-Net [14], X-CLIP [16], DMAE [9] and Cap4Video [26] are learned without extra data.\nResults in Table 1 show that, on the MSR-VTT dataset, our method is superior not only to the ViT-based methods but also to the pre-training models relying on large-scale pre-training data. The results of existing methods show that the Top-1 recall accuracy could hardly transcend 50% without assistance from extra data. Instead, great improvement (up to 10%) is obtained when additional data is involved, either they are used for pre-training or fine-tuning. Despite not using any extra data, the Top-1 recall accuracy of our method (ExCae) boosts to 55.0% and 56.1% in text-to-video and video-to-text retrieval, respectively, when using ViT-B/16 as the base model. We are also encouraged by the observation that ExCae's performance demonstrates an increase in correlation with the enlarged scale of the backbone model, resulting in an around 7% rise of R@1 from ViT-L/14 to Vit-G/14. Our results have comprehensively surpassed the current SOTA pre-training models, such as mPLUG-2 and VAST, when using the Vit-G/14 as the backbone. The rising trend of performance indicates the potential for further improvement of our method through scaling up the base model. Similar results are obtained in the experiments on MSVD and DiDeMo datasets, as recorded in Table 2. Our results are comparable to or even superior to those of pre-training models. Compared to existing ViT-based methods, ExCae shows a resounding victory, with Top-1 recall accuracy suppressing the best one by 7% on MSVD and 13% on DiDeMo. These results validate that our method is robust to cross-modal retrieval tasks for data collected from different fields.\nWe also demonstrate some video-text retrieval cases to present the performance of our method intuitively. The top three retrieved videos, when given a caption, are presented in Figure 4. In these cases, our method successfully picks the corresponding ground truth videos in the first place. The second and third hit videos are also highly related to the sentences in terms of content. Besides, Figure 5 compares our video-to-text retrieval results to the \"Base\" obtained without any caption enhancement. It is shown that the ground truth texts normally take the first rank when using our method. Compared to the base results, our method preferentially selects samples with semantics and descriptive perspectives similar to the ground truths. For example, our method successfully recognizes keywords of \u201cvoices\u201d, \"critic\", and \"guitar\" that are missed by the Base model. This is owing to the ECS module that adaptively learns the cross-modal matching for specific samples, benefiting the recognition of key semantics during video-text retrieval."}, {"title": "4.4. Plug-in Experiment", "content": "ExCae is encapsulated and supplemented with existing video-text retrieval methods, whose results are compared to those of the original. As shown in Table 3, our method successfully increases retrieval results of different video-text retrieval methods. The Top-1 recall accuracy is boosted by 4.8%, 9.4% and 7.7% for DRL, TSR-Net and Clip4Clip respectively. It is observed that the results of plug-in experiments are similar to those obtained by our method with the same backbone (refer to Table 1). This indicates that ExCae has strong adaptability and expandability, which can flexibly combine with different models and constantly upgrade its performance as the technology evolves."}, {"title": "4.5. Ablation Studies", "content": "Our method expands the expression on the video side; thus, this section respectively evaluates the key components, CSI and ECS modules, under different input data cases. To test the effectiveness of the CSI module, we replace the optimal prompt with the initial one to simulate the ablation of the CSI module. The removal of the ECS module is realized by directly encoding all the video-side data as visual embeddings. As shown in Table 4, the R@1 is around 50% when only using the original video as video-side data without adding any components; this result is close to the existing ViT-based methods. The ECS module shows only slight positive effects on the original video. Similar results can be found if we merely change the video-side data from the original videos to video-derived captions. An obvious performance boost occurs when the original videos and video-derived captions are input in combination, validating the indispensability of these two data. In addition, we find that both CSI and ECS modules facilitate cross-modal retrieval. Despite in the absence of the other one, our method achieves over 6% gain in Top-1 recall accuracy. The effectiveness of the CSI and ECS modules proves the superiority of captions' auto-enhancement and multi-perspective cross-modal matching. The best results are obtained by our full method, where both the CSI and ECS modules play mutually reinforcing roles in promoting video-text understanding."}, {"title": "4.6. Analytical Experiments", "content": "a) Effect of video-derived caption number ($N_c$) Figure 6 plots the video-text retrieval results when raising $N_c$ from 1 to 10. The performance is really poor if only one caption is derived, with a Top-1 recall accuracy of lower than 30% on text-to-video retrieval. This result is inferior to any existing methods, reflecting that insufficient caption enhancement would even damage cross-modal learning. The reason is that all generated captions are forcibly activated to match the text if too few caption samples are input to the ECS module. Consequently, the original text may align with a description from a completely different perspective. Results are much better when increasing the number of generated captions to 3, achieving good performance close to the existing pre-training models. The performance further improves with the increase of the caption number, and the upward trend becomes stable when $N_c$ is greater than 7. This experiment suggests an approximate range of [7, 10] for the selection of video-derived caption numbers.\nb) Effect of activated expert number ($N_e$) Video-text retrieval results with different $N_e$ values are displayed in Figure 7. We also show the results obtained without the ECS module, labeled as 0 activated expert, as a reference. The retrieval results gradually improve as $N_e$ increases from 0 to 2. However, no further improvement in performance is achieved with the subsequent increase of activated experts. This may be because ECS would automatically favor partial experts and suppress the attention to less relevant expressions, although the majority of experts are activated."}, {"title": "5. Discussion", "content": "5.1. Superiority\nExCae attains remarkable video-text retrieval performance primarily due to its efficient narrowing of the video-text modality gap. As shown in Figure 8 (a), the representations of video-text pairs are drawn closer to each other through self-learning diffusional data augmentation (CSI) and self-adaptive cross-modal matching (ECS). The modality gap ||A|| [12] is reduced from 0.82 to 0.75, from 0.79 to 0.67, and from 0.84 to 0.74 on MSR-VTT, MSVD and DiDeMo, respectively. Moreover, ExCae has advantages in objectivity, generalization and self-adaptation. 1) ExCae is a data-driven method, in which the query prompts and caption selection are learned according to the training data. No additional human work (despite initialization), such as prompt design and lexicon establishment, is involved in the whole learning process. Thus, ExCae effectively avoids the adverse effects of empiricism. 2) As the Captioner is asked to generate multi-angle captions using a diversity constraint, data with more expression perspectives are provided for model learning. Therefore, ExCae inherits the strength of generalization improvement from data augmentation. 3) ExCae also has personalized ability owing to the parametric learning of the ECS module. The router in the ECS module would adaptively select the best expressions for cross-modal retrieval, thus maintaining the robustness of ExCae in different applications.\n5.2. Convergence\nSince the generated captions are changeless once the optimal prompt is determined, the cross-modal post-training of our method owns the same convergence as existing ViT-based methods. Regarding learning the CSI module, the LLM would optimize query prompts in the given direction. With the increase of iteration, the semantic changes in query prompts become smaller, and thus the caption evaluation scores tend to be stable (See Figure 8 (b)). According to expertise from experiments, the CSI module tends to converge in 400 iterations.\n5.3. Storage and Efficiency\nAlthough a prompt optimization process is additionally involved, we believe that the storage and computation cost of ExCae is appreciable, especially compared to the pre-training methods. With regard to storage, no extra data or related lexicon needs to be stored. We only need to occupy a very small amount of storage space to record several generated captions in each epoch, which greatly reduces the storage cost. For the computation cost, our method is more efficient than the multi-modal foundation models without a pre-training process. In comparison to the ViT-based methods, the only additional computation is from the training of the CSI module. However, prompt optimization is a one-off training process, and we have also found that the learned prompt has generalization ability across different data sets. Therefore, the efficiency of our approach is comparable to existing video-text retrieval methods."}, {"title": "6. Conclusion", "content": "This paper proposes strengthening cross-modal understanding by enhancing video-text expressions in a self-learning manner, which avoids extra workload on data acquisition and prompt design. A CSI module that gradually refines query prompts is designed to generate high-quality captions with rich and diverse expressions, so as to expand the receptive field for cross-modal learning. In addition, multiple learnable experts are introduced as a filter to automatically select the appropriate perspectives of expressions for video-text matching. Our method is non-empirical and self-adaptive in comparison with current text rewriting methods. Experimental results demonstrate that our method achieves SOTA performance on benchmark datasets such as MSR-VTT, MSVD, and DiDemo. Looking ahead, we aspire to explore the variants adaptive to various cross-modal scenarios, thereby aiding a multitude of video-text-related tasks."}]}