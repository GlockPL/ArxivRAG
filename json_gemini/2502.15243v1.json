{"title": "Comparative Analysis of Large Language Models for Context-Aware Code Completion using SAFIM Framework", "authors": ["HANG ZHANG", "YANXIN SHEN", "LUN WANG", "CHUANQI SHI", "SHAOSHUAI DU", "YIYI TAO", "YIXIAN SHEN"], "abstract": "The advent of Large Language Models (LLMs) has revolutionized code completion, trans- forming it into a more\nintelligent and context-aware feature in modern integrated development environments. These advancements have\nsignificantly enhanced developers' ability to write efficient and error-free code. This study evaluates the performance\nof several chat-based LLMs, including Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-40, GPT-40-mini, and GPT-4 Turbo, using\nthe Syntax-Aware Fill-in-the-Middle (SAFIM) dataset. This benchmark is specifically designed to assess models'\ncapabilities in syntax-sensitive code generation. Performance metrics, such as cosine similarity with ground-truth\ncompletions and latency, were employed to measure both accuracy and efficiency. The findings reveal substantial\ndifferences in the models' code completion abilities, offering valuable insights into their respective strengths and\nweaknesses. This work provides a comparative analysis that underscores the trade-offs between accuracy and speed,\nestablishing a benchmark for future advancements in LLM-based code completion.", "sections": [{"title": "INTRODUCTION", "content": "Code completion has emerged as a cornerstone feature in modern integrated development environments (IDES),\nfundamentally transforming the software development process. By automating routine coding tasks, it reduces\nboilerplate code, enhances accuracy, and significantly boosts overall productivity. Historically, code completion relied on\nstatistical models such as n-grams [1] and hand-crafted heuristics [2], as well as simpler machine learning techniques. While\nthese traditional methods performed adequately in structured and predictable scenarios, they often faltered when faced\nwith complex codebases and dynamic programming paradigms, limiting their effectiveness in more sophisticated\ndevelopment environments [3].\nThe advent of large language models (LLMs) has revolutionized the landscape of software development [4] [5] [6], with\ncode completion being one of the most transformative capabilities they offer. These models have introduced a new era\nwhere understanding context, semantics, and programming logic is paramount [7]. Products like GitHub Copilot and\nCursor exemplify the transformative potential of LLMs [8], showcasing their ability to generate syntactically and\nsemantically coherent code suggestions. The integration of LLMs into IDEs has not only enhanced the capabilities of code\ncompletion tools but also redefined the expectations for intelligent coding assistance. However, the rapid evolution of\nLLM-based tools and the proliferation of diverse models necessitate systematic evaluation to ensure reliable comparisons\nand actionable insights [9].\nThis study embarks on a comprehensive evaluation of several chat-based LLMs using the Syntax Aware Fill-in-the-\nMiddle (SAFIM) dataset [10], a benchmark meticulously designed to test syntax sensitive code completion. By employing\nrigorous metrics such as cosine similarity with ground truth completions and latency, this research illuminates the\ndifferences in capabilities and trade-offs among six prominent models: Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-40, GPT-\n40-mini, and GPT4 Turbo. Through this analytical lens, the study aims to provide a nuanced understanding of each\nmodel's strengths and weaknesses, offering a valuable benchmark for assessing their efficiency in code completion tasks.\nThe results of this work are poised to serve as a critical resource for developers and researchers alike. By offering\ninsights into model performance, this benchmark aids in selecting the most suitable models for specific needs, thereby\noptimizing the development workflow. Furthermore, the findings highlight areas for improvement, driving the\nadvancement of LLM-based tools. By identifying the trade-offs between accuracy and speed, this study not only\ncontributes to the current body of knowledge but also sets the stage for future innovations in Al-assisted programming,\nensuring that code completion tools continue to evolve in alignment with the ever-changing demands of the software\nindustry."}, {"title": "RELATED WORK", "content": "2.1 Traditional Code Completion Methods\nCode completion has long been a pivotal feature in integrated development environments (IDEs), enhancing developer\nproductivity by automating repetitive coding tasks. Traditional methods primarily relied on statistical models [11], such as\nn-grams [1], which predict the next token based on preceding sequences. While effective in structured scenarios, n-grams\nstruggled with capturing long-range dependencies and complex code semantics.\nIn addition, hand-crafted heuristics were employed to improve code completion by using rule-based systems that\nleverage syntactic and semantic cues. [2] These heuristics, although useful, were often rigid and less adaptable to new\nprogramming patterns, limiting their effectiveness in dynamic environments.\nEarly machine learning techniques also contributed by training models on labeled datasets to recognize coding\npatterns. However, these models were constrained by the need for extensive feature engineering and limited training\ndata, which restricted their ability to generalize across diverse programming contexts. [11]\nOverall, while traditional methods laid the groundwork for code completion, their limitations in handling complex\ncode structures and adapting to evolving programming paradigms highlighted the need for more advanced solutions,\npaving the way for the integration of large language models (LLMs).\n2.2 Large Language Models for Code Completion\nThe rise of Large Language Models (LLMs) has significantly influenced the field of code completion, demonstrating that\nincreasing model complexity can enhance performance across diverse coding tasks. [12][15] This realization has spurred\nthe application of LLMs to code-related challenges, particularly in generating and completing code snippets. Decoder-\nonly architectures are commonly employed for these tasks due to their effectiveness in sequential data processing.\nInitially, many LLMs focused on Left-to-Right (L2R) training objectives, emphasizing \"Next Token Prediction\" to\ngenerate code linearly. [13] However, the Fill-in-the-Middle (FIM) objective, which allows models to complete code by\ninferring missing segments, has gained traction. This approach has proven effective in handling more complex code\nstructures and providing contextually relevant suggestions. [10]\nThis paper evaluates a select group of LLMs using a specialized benchmark designed to test syntax sensitive code\ncompletion. Through this evaluation, we aim to uncover insights into the performance of these models in FIM tasks,\nassess the strengths and weaknesses of different training paradigms, and challenge the notion that larger models\ninherently deliver superior results.\n2.3 Fill-in-the-Middle as an Evaluation Tool for Code LLMS\nThe Fill-in-the-Middle (FIM) approach, originally derived from masked language modeling (MLM) and span corruption\ntechniques, has been adapted for training and evaluating language models in the context of code. [14] While traditional\nMLM focused on representation learning with short spans, FIM extends this concept to facilitate code generation tasks\nby allowing models to predict missing segments within code snippets. [10]\nEvaluations using FIM have demonstrated that a high ratio of FIM in pretraining does not detrimentally affect Left-\nto-Right (L2R) generation capabilities, underscoring its utility in developing robust code-focused models.\nDespite its advantages, early benchmarks like Human Eval-Infilling, which utilized FIM for evaluation, were limited in\nscope, focusing on small Python code snippets. This limitation highlighted the need for more comprehensive evaluation\nframeworks that can better assess the syntax-sensitive capabilities of LLMs in code completion tasks.\nTo address this gap, the Syntax-Aware Fill-in-the-Middle (SAFIM) benchmark has been introduced. SAFIM provides a\nmore extensive and detailed evaluation platform, allowing for a thorough assessment of LLMs' performance in syntax-\naware code completion. By leveraging SAFIM, researchers can gain deeper insights into the strengths and weaknesses of\nvarious models, ultimately advancing the development of more effective code completion tools. [10]"}, {"title": "METHODOLOGY", "content": "3.1 Dataset\nThe Syntax-Aware Fill-in-the-Middle (SAFIM) benchmark serves as a comprehensive evaluation frame- work for\nassessing the performance of chat-based large language models (LLMs) in syntax-sensitive code completion tasks.\nSAFIM encompasses a diverse collection of 17,720 examples, carefully curated from competitive programming platform\nCodeforces and high-starred (1000 stars and more) open- source repositories on GitHub. The dataset spans four major\nprogramming languages: Python, Java, C++, and C#, with a balanced distribution of approximately 4,430 examples per\nlanguage to ensure unbiased evaluation across different programming paradigms.\nThe benchmark is strategically organized into three distinct task categories, each designed to evaluate different\naspects of code understanding and generation:\n1. Algorithmic Block Completion: This category challenges models to infer and generate key algorithmic components\nwithin existing code structures. Examples include completing sorting algorithms, graph traversal\nimplementations, and dynamic programming solutions. These tasks assess the model's ability to understand and\nimplement core computational logic.\n2. Control-Flow Completion: These tasks evaluate the model's comprehension of program flow and control\nstructures. Examples include completing conditional statements, loop bodies, and exception handling blocks. This\ncategory tests the model's ability to maintain logical consistency and proper scope management.\n3. API Function Call Completion: This specialized category focuses on completing function calls within API usage\ncontexts. It assesses the model's understanding of library interfaces, parameter requirements, and typical API\nusage patterns across different programming paradigms.\nTo facilitate efficient evaluation while maintaining statistical significance, we implemented a systematic sampling\nstrategy, selecting 100 representative examples from each task category. The sampling process was designed to preserve\nthe distribution of programming languages and task complexity levels present in the original dataset.\n3.2 Models Evaluated\nThis study evaluates the performance of five large language models (LLMs), each with unique characteristics and\noptimization strategies. The models included are as follows:\n\u2022 Gemini 1.5 Flash: A high-speed variant of the Gemini series, optimized for latency-sensitive applications.\n\u2022 Gemini 1.5 Pro: A performance-oriented model from the Gemini series, designed for complex code generation\ntasks with improved accuracy.\n\u2022 GPT-40: A lightweight variant of GPT-4, balancing computational efficiency and completion quality.\n\u2022 GPT-40 Mini: A further optimized and smaller variant of GPT-40, catering to resource constrained environments.\n\u2022 GPT-4 Turbo: An enhanced version of GPT-4, offering faster response times while maintaining high-quality outputs.\nThese models were selected to represent a diverse range of capabilities, spanning high-speed, resource-efficient,\nand performance-focused approaches. Their evaluation provides insights into the trade-offs between latency and\naccuracy, critical for code completion tasks.\n3.3 Prompt Design\nTo evaluate the syntax-aware code completion capabilities of the selected LLMs, a structured prompt design was\nemployed. This design comprises two key components: a system prompt that establishes the Al's role and scope, and a\nuser prompt that provides specific task instructions and context.\n3.3.1 System Prompt\nThe system prompt defines the Al's responsibilities and sets boundaries to ensure the focus remains on code completion.\nThe system prompt used in this evaluation is as follows:\n\"As an Al code assistant, provide auto-completion for the given code. Understand the context and language,\ngenerating accurate and concise completions. Adapt to various languages and styles, enhancing\nproductivity and code quality while adhering to standards. Limit responses to software development topics,\nreturning only code or comments without additional prose.\"\n3.3.2 User Prompt\nThe user prompt delivers task-specific instructions and provides the model with contextual code fragments, including a\nprefix and suffix. The model is tasked with generating a coherent completion that logically bridges the gap between the\ntwo. This ensures the output is both syntactically valid and contextually appropriate.\nThe user prompt used in this evaluation is as follows:\n\"Generate a coherent code snippet that logically connects the provided prefix and suffix in the location of\n$PLACEHOLDERS. The completion should seamlessly continue from the prefix and lead into the suffix,\nensuring the block is logically and syntactically complete. Reply with the completion only and exclude\nmarkdown formatting.\n<prefix>{prefix}$PLACEHOLDER$<suffix>{suffix}\"\n3.4 Evaluation Metrics\nThe evaluation framework was designed to comprehensively assess the performance of each model on the SAFIM dataset\nthrough quantitative metrics. We focused primarily on two key performance indicators: completion latency and semantic\naccuracy through cosine similarity measurements between generated and ground-truth completions.\n3.4.1 Performance Metrics\nWe employed two primary metrics for evaluation:\n1. Latency Measurement (L): The completion generation time is measured with millisecond precision:\n$L = t_{completion} - t_{start}$ (1)\nwhere $t_{completion}$ is the timestamp when the model returns its response, and $t_{start}$ is when the request was\ninitiated.\n2. Cosine Similarity (CS): To evaluate the semantic similarity between generated and ground truth completions, we\ncompute the cosine similarity between their vector representations. For two code completion vectors a\n(generated) and b (ground truth), the cosine similarity is calculated as:\n$cs(a,b) = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum_{i=1}^{n} a_i^2} \\sqrt{\\sum_{i=1}^{n} b_i^2}}$ (2)\nwhere n is the dimension of the vector representations. The resulting similarity score ranges from -1 to 1, with 1\nindicating perfect similarity. This metric is particularly valuable for code evaluation as it helps capture semantic\nsimilarities independent of exact syntactic matches, allowing for valid alternative implementations and it is\nnormalized for code length and style variations, focusing on functional equivalence. Moreover, it provides a\ncontinuous measure of similarity, enabling fine-grained comparison between different models' outputs.\n3.4.2 Result Processing and Storage\nThe evaluation results are structured in a comprehensive JSONL format, where each entry contains: original input context\n(prefix, suffix, language), generated completion, ground truth completion, latency measurement (L), cosine similarity\nscore (CS) and model-specific metadata."}, {"title": "EXPERIMENT RESULT", "content": "4.1 Overall Model Performance\nWe evaluated five state-of-the-art LLMs across three distinct code completion tasks: API Function Call Completion,\nAlgorithmic Block Completion, and Control-Flow Completion. Table 1 presents the detailed results for both accuracy\n(measured by cosine similarity) and efficiency (measured by latency).\nOur evaluation reveals distinct characteristics for each model that suggest specific use cases. GPT-4 Turbo\ndemonstrates superior semantic understanding across all task types (0.666-0.858 similarity) but with significant latency\noverhead (0.808-2.063s), making it best suited for production environments where accuracy is critical. Gemini 1.5 Pro\nshows strong performance in API tasks (0.813 similarity) with moderate latency (1.219-1.768s), offering a good balance\nfor API-intensive development, though it struggles with more complex algorithmic tasks (0.603 similarity). GPT-40\nachieves the most consistent and lowest latency (0.515-0.672s) while maintaining competitive accuracy in API tasks\n(0.719 similarity), making it ideal for real-time development environments and IDE integrations. GPT-40 Mini offers\nsimilar latency benefits to GPT-40 but with slightly lower accuracy, positioning it as a suitable choice for rapid\nprototyping where quick response times are essential.\nBased on these characteristics, we recommend GPT-4 Turbo for production code generation where accuracy is\nparamount, GPT-4O for real-time IDE integration requiring quick feedback, and Gemini 1.5 Pro for API-intensive\ndevelopment tasks. These findings suggest that model selection should be guided by specific use case requirements,\nbalancing the trade-offs between accuracy, latency, and task complexity.\n4.2 Performance Analysis by Task Category\n4.2.1 API Function Call Completion\nIn API completion tasks, models demonstrated their strongest performance overall. GPT-4 Turbo achieved the highest\ncosine similarity (0.858) while maintaining moderate latency (0.808s). GPT-40 offered the best efficiency with the lowest\nlatency (0.515s) while maintaining competitive accuracy (0.719). Gemini 1.5 Pro showed strong accuracy (0.813) but\nrequired longer processing time (1.219s).\n4.2.2 Algorithmic Block Completion\nFor algorithmic block completion, we observed a general decrease in accuracy across all models compared to API\ncompletion tasks. GPT-4 Turbo maintained its leading position in accuracy (0.682) but showed significantly increased\nlatency (1.935s). GPT-40 Mini demonstrated the best efficiency (0.663s) in this category. Interestingly, Gemini 1.5 Pro's\naccuracy (0.603) decreased more substantially than other models in this task type.\n4.2.3 Control-Flow Completion\nControl-flow completion tasks revealed similar patterns to algorithmic block completion, with slightly lower overall\naccuracy. GPT-4 Turbo again led in accuracy (0.666) but with the highest latency (2.063s). GPT-40 maintained its\nefficiency advantage with the lowest latency (0.652s), though with the lowest accuracy (0.574).\n4.3 Key Findings\n\u2022 Task Complexity Impact: API Function Call completion consistently showed higher accuracy across all models\n(0.683-0.858) compared to algorithmic (0.546-0.682) and control-flow tasks (0.574-0.666).\n\u2022 Model Size Trade-offs: Larger models (GPT-4 Turbo, Gemini 1.5 Pro) consistently achieved higher accuracy but at\nthe cost of significantly higher latency. This trade-off was most pronounced in algorithmic and control-flow tasks,\nwhere GPT-4 Turbo's latency increased by up to 2.5x compared to API tasks.\n\u2022 Efficiency Leaders: GPT-40 and GPT-40 Mini consistently demonstrated superior efficiency with lower latencies\n(0.515-0.692s), making them suitable for time-sensitive applications where moderate accuracy is acceptable.\n\u2022 Performance Consistency: GPT-4 Turbo showed the most consistent accuracy leadership across all three task\ntypes, while GPT-40 maintained consistent efficiency leadership."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "5.1 Conclusion\nIn this paper, we presented SAFIM, a comprehensive benchmark for evaluating large language models' capabilities in\nsemantic-aware function implementation tasks. Through our evaluation of five state-ofthe-art LLMs across three distinct\ntask categories, we have made several significant findings: We demonstrated that model performance varies\nsignificantly across different types of code completion tasks, with API Function Call completion showing consistently\nhigher accuracy (up to 0.858 cosine similarity) compared to more complex algorithmic and control-flow tasks.\n\u2022 We identified clear trade-offs between model size and performance, where larger models like GPT-4 Turbo\nachieved superior accuracy but at the cost of increased latency, particularly in complex tasks where latency could\nincrease by up to 2.5x.\n\u2022 We established that smaller models like GPT-40 offer compelling efficiency advantages, maintaining consistent\nsub-second latency (0.515-0.692s) across all task types while delivering acceptable accuracy for many practical\napplications.\n\u2022 We provided empirical evidence that current LLMs exhibit varying levels of semantic understanding across\ndifferent coding contexts, suggesting areas where model capabilities can be further enhanced.\nThese findings have significant implications for real-world development environments. For IDE integration, our results\nsuggest a hybrid approach: using efficient models like GPT-40 for real-time suggestions during active coding, while\nleveraging more accurate models like GPT-4 Turbo for com- plex implementations or code review scenarios. This dual-\nmodel strategy could significantly enhance developer productivity while maintaining code quality. Furthermore, the\nstrong performance in API completion tasks indicates that current LLMs are particularly well-suited for accelerating API\nadoption and reducing documentation lookup time.\n5.2 Future Work\nOur research opens several promising directions for future work:\n\u2022 Extended Task Categories: Future research could expand the SAFIM benchmark to include additional task\ncategories such as debugging, refactoring, and test case generation, providing a more comprehensive evaluation\nof LLM capabilities in software development.\n\u2022 Enhanced Evaluation Metrics: Development of more sophisticated evaluation metrics that can capture aspects of\ncode quality beyond semantic similarity, such as code efficiency, maintainability, and adherence to best practices.\n\u2022 Context Window Analysis: Investigation of how different context window sizes affect model performance across\nvarious task types, particularly for complex algorithmic and control-flow completions.\n\u2022 Language-Specific Optimization: Exploration of model performance variations across differ- ent programming\nlanguages to develop language-specific tuning strategies and enhance comple- tion accuracy.\n\u2022 IDE Integration Studies: Investigation of user interaction patterns and developer satisfaction metrics when using\ndifferent models for code completion in real-world development environments.\nThese future directions aim to address current limitations and advance our understanding of LLM capabilities in code\ngeneration tasks. We believe that continued research in these areas will lead to more effective and reliable code\ngeneration systems that can better serve the needs of software developers.\nOur work provides a foundation for systematic evaluation of LLM performance in code generation tasks and offers\npractical insights for both researchers and practitioners in the field. As LLM technology continues to evolve, frameworks\nlike SAFIM will become increasingly important for understanding and improving model capabilities in software\ndevelopment applications."}]}