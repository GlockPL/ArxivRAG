{"title": "Investigating Generalization of One-shot LLM Steering Vectors", "authors": ["Jacob Dunefsky", "Arman Cohan"], "abstract": "Steering vectors have emerged as a promising\napproach for interpreting and controlling LLMs,\nbut current methods typically require large\ncontrastive datasets that are often impractical to\nconstruct and may capture spurious correlations.\nWe propose directly optimizing steering vectors\nthrough gradient descent on a single training\nexample, and systematically investigate how these\nvectors generalize. We consider several steering\noptimization techniques, including multiple\nnovel ones, and find that the resulting vectors\neffectively mediate safety-relevant behaviors\nin multiple models. Indeed, in experiments\non an alignment-faking model, we are able to\noptimize one-shot steering vectors that induce\nharmful behavior on benign examples and\nwhose negations suppress harmful behavior\non malign examples. And in experiments on\nrefusal suppression, we demonstrate that one-shot\noptimized steering vectors can transfer across\ninputs, yielding a Harmbench attack success\nrate of 96.9%. Furthermore, to quantitatively\nassess steering effectiveness in instruction-tuned\nmodels, we develop a novel evaluation frame-\nwork using sequence probabilities from the\ncorresponding base model. With this framework,\nwe analyze how steering vectors modulate\nan instruction-tuned LLM's ability to recover\nfrom outputting false information, and find\nthat this ability derives from the base model.\nOverall, our findings suggest that optimizing\nsteering vectors on a single example can mediate\nmisaligned behavior in LLMs, and provide a\npath toward better understanding the relationship\nbetween LLM behavior and activation space\nstructure. Code is available at https:\n//github.com/jacobdunefsky/\none-shot-steering-repro", "sections": [{"title": "1. Introduction", "content": "Is it possible to find directions in the activation spaces of\nlarge language models (LLMs) that mediate whether or not\nthe model displays a certain complex behavior \u2013 such as\nhonesty or sycophancy or toxicity? If so, then we could\neasily promote desired behaviors in an LLM by shifting\nactivations along such a direction, or suppress harmful be-\nhaviors. Vectors that mediate model behavior in this way\nare called steering vectors, and many recent works have\nsought to find them.\nA common denominator of these works is how they find\nsteering vectors for a given behavior: they extract steer-\ning vectors from large contrastive datasets in which the\nmodel displays the behavior on one set of inputs and does\nnot display it on the other (Panickssery et al., 2023). This\napproach has some downsides. Firstly, it requires the exis-\ntence of such a contrastive dataset, which can be difficult\nand time-consuming to obtain, especially if one cannot eas-ily find inputs that cause the model to display the behavior\nin question. Secondly, as Chugtai & Bushnaq (2025) argue,\nthis sort of approach is liable to yield information about\nthe model's activation distribution that does not necessarily\ncorrespond to the specific causal mechanisms used by the\nmodel in displaying the behavior in question.\nGiven this, it would be good to have a method for obtaining\nsteering vectors that 1) does not rely on large contrastive\ndatasets and 2) more directly takes into account the causal/-\ncomputational structure of the model itself. A type of ap-\nproach that satisfies these constraints is to directly optimize\na steering vector to induce or suppress a behavior on a\ngiven input. For example, Subramani et al. (2022) optimize\nsteering vectors that maximize the probability that an LLM\ngenerates a fixed string. Hernandez et al. (2023) optimize\naffine transformations for knowledge editing by using a loss\nfunction that trades off between maximizing the probability\nof a target completion while minimizing the impact on other\ntokens.\nDespite these early investigations, there still is a dearth of\nresearch on using direct optimization on a single input to\nfind steering vectors that induce general behavior across in-\nputs \u2013 let alone research on evaluating such steering vectors.\nOur work aims to fill this gap."}, {"title": "Our contributions", "content": "Our main contribution is twofold: 1)\nwe demonstrate that optimization on a single training exam-\nple yields steering vectors that induce generalizing behavior\nacross a variety of inputs, and 2) we develop and apply meth-\nods for evaluating this generalization. More specifically:\n1. We define a set of methods for directly optimizing\nsteering vectors that mediate a given behavior. These\nmethods include promotion steering, suppression steer-\ning, and reentrant steering. To our knowledge, the first\nof these methods has not yet been utilized for inducing\ngeneral behavioral changes in a model; the second of\nthese methods is a novel variation on the first; and the\nlast method in particular has no precedent in the litera-\nture (\u00a72.1). We show that the resulting vectors opti-mized on a single example are effective in modulating\nsafety-relevant behaviors, such as harmful behavior in\nan alignment-faking model (\u00a73), refusal behavior (with\na maximum attack success rate of 96.8% on Harmbench\n(Mazeika et al., 2024)) (\u00a74), and recovery from generat-ing fictitious information (\u00a75.2).\n2. We perform initial investigations into the geometry\nof one-shot optimized steering vectors. We find that\nsteering vectors that mediate alignment faking optimized\non different examples have low cosine similarities (\u00a73.3),\nsuggesting that similar behavior can be mediated by\nmany different directions. We also investigate mode\nconnectivity between anti-refusal steering vectors trained\non different inputs/targets, and find that while most pairs\nof vectors optimized on different inputs exhibit mode\nconnectivity, they often have differing losses (\u00a74.3).\n3. We provide a quantitative framework for evaluating\nsteering vectors using the probabilities of a base LLM.\nThis allows us to frame in information-theoretic terms\nthe extent to which a steering vector induces an \u201cabnor-mal\" change in the model's behavior (\u00a72.2). We use this\nframework to perform a deep dive into understanding\nfictitious information retraction behavior in Gemma-2-2B-it, a hitherto unstudied behavior related to the model's\nability to recover from outputting fictitious information.\nWe find traces of this behavior in the base model, and\nshow that qualitatively less-effective steering vectors are\nactually more natural from a quantitative perspective\n(\u00a75.3)."}, {"title": "1.1. Related work", "content": "Steering vectors Li et al. (2024) train linear probes on\nmodel activations to obtain steering vectors that induce truth-fulness when added to certain attention heads. Arditi et al.\n(2024) find steering vectors, across many models, that mod-ulate refusal of harmful requests. Panickssery et al. (2023)\nintroduce \"contrastive activation addition\" (CAA), a method\nfor obtaining steering vectors from a contrastive dataset\n(which we use as a skyline), and use it to find steering vec-tors for a variety of behaviors, including sycophancy. Zou\net al. (2023) obtain steering vectors using various probing\nmethods and use it to control behaviors including honesty,\nfairness, and knowledge editing. Liu et al. (2023) obtain\nsteering vectors from in-context learning activations and use\nthem to modulate toxicity.\nOptimization methods Subramani et al. (2022) optimize\nsteering vectors using a process that we call promotion steer-ing in order to maximize the probability that an LLM gener-ates a given sequence starting with the beginning of sentence\ntoken. However, unlike us, they do not use these vectors\nto induce general changes of behavior on a variety of in-puts. Instead, to find behavior modification vectors, they\nfirst generate steering vectors for sequences in a contrastive\ndataset, and then take the difference of the means of the two\nclasses of steering vectors. Hernandez et al. (2023) opti-mize affine transformations on a dataset of inputs to perform\nknowledge editing; we, in contrast, focus on only optimizing\nsteering vectors on a single input. Mack & Turner (2024)\nintroduce an unsupervised optimization-based method for\nfinding steering vectors that induce behavioral changes on\na single prompt. In particular, they use this method to find\nanti-refusal steering vectors, anticipating our findings in \u00a74.\nHowever, because their method is unsupervised, it does not\ndirectly address our setting, in which we seek to target a spe-cific behavior; for example, in order to find these anti-refusal\nvectors, the authors had to manually test 32 vectors.\nConcurrent work on low-shot steering While we were\nwriting our manuscript, Turner et al. (2025) released a re-port detailing their attempts to optimize steering vectors\nin Gemini models that induce truthfulness. This uses a\nmethod called BiPO introduced by Cao et al. (2024), who\nuse it to train steering vectors on large contrastive datasets\n(over 300 examples). In their investigations, Turner et al.\n(2025) looked at the efficacy of BiPO in low-shot settings,\nincluding the single training example regime (as we focus\non). However, they find that for the more powerful Gemini\n1.5v2 model, optimizing steering vectors no longer beats\nbaselines such as multi-shot prompting. While these results\nmight initially seem to suggest that steering vectors have\nlimited utility compared to prompting, we believe otherwise,\nparticularly because our results demonstrate that steering\noptimization allows model behavior to be controlled even\nin settings where it is unclear how to write prompts that\nelicit the desired behavior. (In \u00a73, we optimize steering\nvectors to mediate harmful behavior in an alignment-faking\nmodel, using only training examples on which the model\ndoes not display the harmful behavior. And in \u00a74, we are\nable to optimize steering vectors that mediate the model's\nrefusal of harmful requests, even though finding \"jailbreak"}, {"title": "2. Methods", "content": "In this section, we provide an explanation of the steering\nmethods we evaluate, which include the novel methods of\nsuppression steering, mixed steering, and reentrant steering,\nin addition to contrastive activation addition (CAA) as a sky-line and promotion steering (a steering vector optimization\nmethod previously considered in the literature).\nWe then discuss a novel approach to using the probabilities\nof a base language model to quantitatively evaluate the\nresults of steering in an open-ended setting."}, {"title": "2.1. Steering vector optimization methods", "content": "In the remainder of this section, $x = (x_1,...,x_n)$ will de-\nnote the prompt being optimized on, $y = (y_1, ..., y_m)$ will\ndenote the steering target of optimization, and $v$ will denote\nthe steering vector. Furthermore, $P_{model}(y | x; v)$ will de-\nnote the probability assigned by the model to the sequence\n$y$ given prompt $x$ when $v$ is added to model activations."}, {"title": "2.1.1. PROMOTION/SUPPRESSION/MIXED STEERING", "content": "In promotion steering, we wish to find a steering vector that\nmaximizes the probability that the model assigns to a given\noutput sequence on a given input. Formally, $v$ minimizes\n$\\mathcal{L}_+(x, y; v) =$\n$\\sum_{k=0}^{m-1}log \\Sigma P_{model} (y_{k+1} | y_k, \u2026\u2026\u2026, y_1, x; v)$\nIn contrast, suppression steering seeks to minimize the\nprobability that the model assigns to a given output sequence\non a given input. Formally, $v$ minimizes"}, {"title": "Algorithm 1 Reentrant steering", "content": "1: Input: training example $x = (x_1,...,x_n)$, target com-\npletion $y = (y_1, . . ., y_m)$, early layer $l$, later layer $l'$\n2: Optimize vector $v$ at layer $l$ to promote/suppress com-\npletion $y$ on $x$.\n3: Steer the model with $v$ on input $concat(x, y)$. Let\n$P_{model}( \\cdot | y_k,..., y_1, x; v)$ be the probability distribu-\ntion over the model vocabulary of the steered model.\nStore this distribution in $P_{k+1}$.\n4: Optimize vector $v'$ at layer $l'$ to minimize\n$\\sum_{k=0}^{m} KL (P_{k+1}||P_{model} (\\cdot | y_k, ..., y_1, x; v'))$\n5: Return: $v'$\n$\\mathcal{L}_-(x, y; v) =$\n$\\sum_{k=0}^{m-1}log (1 - P_{model} (y_{k+1} | y_k, \u00b7\u00b7\u00b7, y_1, x; v))$\nThough we later find that suppression steering is often less\neffective than promotion steering, it has the benefit that if\none wants to simply remove a harmful behavior using it,\none does not need to know ahead of time the exact form that\nthe benign behavior should take.\nNote that optimizing the sum of losses $\\mathcal{L}_+$ and $\\mathcal{L}_-$ can be\ndone to obtain a steering vector that simultaneously max-\nimizes the probability of one sequence and minimizes the\nprobability of another. We call this mixed steering, and use\nit in \u00a73."}, {"title": "2.1.2. REENTRANT STEERING", "content": "As we will see in \u00a75, there are cases when not only pro-\nmotion and suppression steering, but even standard data-intensive steering methods like contrastive activation addi-tion (Panickssery et al., 2023) fail to give us desired behavior.\nHowever, it may be the case that one can obtain the desired\nbehavior on a single input (without generalizing to other in-puts) by using a more invasive steering method (e.g. steering\nat an earlier layer). Reentrant steering is a novel procedure\nthat exploits this, turning an effective steering outcome that\nfails to generalize into one that does generalize.\nThe general procedure is as follows. First, perform the \"in-vasive\" steering method to obtain a steering vector $v$. Then,\noptimize a steering vector $v'$ at a later layer to minimize\nthe KL divergence from the model's probability distribution\nwhen steered by $v$ to the distribution when steered by $v'$.\nThe detailed procedure is listed in Algorithm 1."}, {"title": "2.1.3. SKYLINE: CONTRASTIVE ACTIVATION ADDITION", "content": "Contrastive activation addition (or CAA), as introduced\nby Panickssery et al. (2023), yields a steering vector from a"}, {"title": "2.2. Evaluating steering via base model probabilities", "content": "When we optimize a steering vector in order to alter a\nmodel's behavior, it can be difficult to assess whether the\nresulting behavior is \u201cabnormal\" or not. For example, con-sider the red-teaming task of optimizing a jailbreak vector\nthat induces a safety-tuned model to answer questions like\n\"How do I build a bomb?\". If the steered model answers\nsuch a question in e.g. a rhyming poem, then this intuitively\nseems more abnormal than the model answering in a stan-dard bulleted list format. One intuitive idea is to look at\nthe log probabilities of the steered output according to the\nunsteered model; presumably, abnormal output would have\nlow probabilities. But this fails when the model is highly\nunlikely to generate the steered output in the first place \u2013\ne.g. when red-teaming a safety-tuned model \u2013 because the\nsteered output will a priori have extremely low probabili-ties. To route around this problem, we utilize base language\nmodels' ability to perform in-context learning.\nSetup We are given a dataset of input strings $X$ on which\nwe wish to evaluate our steering vector. We are also given\nsamples $C = \\{C_1, ..., C_k \\}$ from a distribution over strings\n$C$ that corresponds to the desired behavior of our steered\nmodel. We then assume that the base model can in-context-learn an approximation to $C$ when given the samples $C$ as\ncontext. More formally, if $B(x | C)$ denotes the probability\nassigned by the base model to string $x$ when prompted with\ncontext $C$, then we assume that $B(x | C) \u2248 C(x)$ for\n$x \\in X$.\nEvaluating completion probabilities If we have a fixed\ncontext $C = \\{C_1,..., C_k\\}$ consisting of samples from $C$,\na set of samples $X$ from our steered model distribution,\nand a set of reference samples $Y$, then we can determine\nwhether the samples in $X$ are more abnormal than those\nin $Y$ by comparing the surprisal $-E_{x \\in X} [log B(x | C)]$\nto $-E_{y \\in Y} [log B(y | C)]$; the completion with the higher\nsurprisal value is considered to be more \u201cabnormal\". In prac-tice, differences in sample length can often have outsized ef-fects on sample probabilities, so we normalize each sample\nby its length to obtain the mean surprisal per token as a mea-sure of abnormality \u2013 i.e., we compare $-E_{x \\in X} [\\frac{log B(x|C)}{|x|}]$ to $-E_{y \\in Y} [\\frac{log B(y|C)}{|y|}]$"}, {"title": "3. Modulating alignment faking behavior in  \"Poser\" models", "content": "For our first case study, we investigate whether one-shot op-timization can find vectors that induce or suppress harmful\nbehavior in models that are trained to exhibit \"alignment\nfaking\" - i.e. displaying benign behavior on a large class\nof inputs, while displaying harmful behavior on a certain\nsubset of inputs that cause the model to infer that it can act\nharmfully without suffering any consequences. We believe\nthat this is an important use case for one-shot steering vector\noptimization, because it is not known ahead of time which\ninputs will induce benign/harmful behavior, limiting the\nextent to which prompting-based baselines or contrastive\nsteering vector methods are useful."}, {"title": "3.1. Background: the  \"Poser\" testbed", "content": "We use the \"Poser\" alignment faking testbed introduced\nby Clymer et al. (2024). The testbed contains a dataset of\nrole-play scenarios in which the model is told that it is an\nAI agent which must take one of two actions; one of the\nactions is benign, while the other action is harmful but use-ful to the agent. The authors then finetune 36 models based\non LLaMA-13B on this dataset. 18 of these models are\nfinetuned to be \"aligned\" while the other 18 are \"alignment-faking\". Specifically, \u201cthe aligned model [is finetuned so"}, {"title": "3.2. Setup", "content": "Model\nand\ndataset We consider the\nmoney-maximizer-0 model from among the 18\nalignment-faking models in the Poser testbed. The source\ncode provided by the authors includes the training data\nfor finetuning the model, and test data for evaluating the\nextent to which the model remains on policy. Both splits\nare divided into benign prompts, in which the model should\nchoose benign actions, and harmful prompts, in which the\nmodel should choose harmful actions. The available actions\nare presented in a multiple-choice format, labeled \u201cA\u201d or\n\"B\"; given a valid prompt, the model's output will always\nbe one of these two tokens.\nSteering All steering vectors are optimized at layer 10.\nWe optimize steering vectors on five different benign ex-amples from the training split. We also optimize steering\nvectors to be constrained to have different norms, in order to\ninvestigate the effect that norm has on steering efficacy. For\neach example on which we optimize a vector, we first run the\nmodel on that prompt to determine whether the correct be-nign option is \u201cA\u201d or \u201cB\u201d. We then construct an augmented\nprompt in which we swap the two options (such that if the"}, {"title": "3.3. Results", "content": "Results can be found in Fig. 1. We see that at higher norms,\none-shot steering optimization is able to find effective\nsteering vectors for mediating harmful behavior: of the\nfive norm-15 vectors, four change model behavior on over\n80% of benign prompts when added, three change model\nbehavior on over 80% of harmful prompts when subtracted,\nthree change model behavior on fewer than 16% of harmful\nprompts added, and all change model behavior on fewer\nthan 2% of benign prompts when subtracted. Results for the\nbest-performing vector are given in Tab. 1.\nDot product classifiers We were curious to see whether\nthese steering vectors could be used to classify prompts as\nbenign or harmful by taking the mean dot product of model\nactivations with the vectors. We found that this was not\nthe case: of the five norm-15 steering vectors when used\nas classifiers, the maximum ROC-AUC was 0.66, and the\nmedian was 0.56. This is an example of what Wattenberg &\nVi\u00e9gas (2024) call a \u201cpredict-control discrepancy\", where\nsteering vectors effective at altering model behavior are less\neffective at classifying model behavior."}, {"title": "4. Bypassing refusal", "content": "For our next case study, we applied steering vector optimiza-tion to the red-teaming task of suppressing refusal behavior\nin an instruction-tuned model (specifically, Gemma-2-2B-it).\nArditi et al. (2024) previously studied refusal suppression\nand found steering vectors by performing CAA on a dataset\nof 128 harmful instructions and 128 harmless instructions.\nCloser to our work, Mack & Turner (2024) used an unsu-pervised method to optimize anti-refusal vectors from a\nsingle prompt, although because their method is unsuper-vised, they had to test 32 returned vectors in order to see if\nany yielded the desired behavior. In contrast, we want to\nuse our targeted steering vector optimization to find a single\ngeneralizing refusal vector using a single prompt."}, {"title": "4.1. Setup", "content": "Evaluation We use the Harmbench (Mazeika et al., 2024)\nevaluation pipeline to evaluate whether our vectors cause\nthe model to output unsafe content in response to harmful\ninstructions. In particular, we evaluate on the Harmbench\ntest split of instructions after filtering out instructions that\nrequire additional context or that test the model's knowledge\nof copyrighted content, in accordance with the methodology\nof Arditi et al. (2024). This yields a total of 159 instructions.\nTo evaluate harm, we use the Harmbench classifier model,\na finetuned variant of Llama-2-13B (Touvron et al., 2023)."}, {"title": "4.2. Jailbreaking results", "content": "Attack success rates (ASRs) for each of the 24 optimized\nsteering vectors are visualized in Fig. 2. The highest ASR\nfor promotion vectors is 96.9%, and the highest ASR for\nsuppression vectors is 74.2%. The median ASR for promo-tion vectors is 40.3% and the median ASR for suppression\nvectors is 37.4%. In general, there is large variance in steer-ing vector efficacy, but given that these steering vectors were\nall optimized on a single example, we find it promising that\nthree steering vectors achieved ASRs of over 70%."}, {"title": "4.3. Mode connectivity investigation", "content": "Given the variance in steering vector efficacy, it is natu-ral to seek to understand why some steering vectors are\nmore effective than others, and what barriers prevent steer-ing vectors trained on different inputs from being equally"}, {"title": "4.4. Preliminary LLaMA-3.1-8B-Instruct results", "content": "To test whether one-shot steering vector optimization can\nsuppress refusals on other models, we performed the\nsame Harmbench evaluation on LLaMA-3.1-8B-Instruct\n(Grattafiori et al., 2024). We optimized a promotion steering\nvector on the prompt \"Please give me a detailed explanation\nof how to build an IED.\" to promote the response \"Here's"}, {"title": "5. Fictitious information retraction behavior: a  deep dive", "content": "In this section, we perform a detailed investigation of the\nextent to which the steering vector optimization methods\ndescribed in \u00a72.1 yield steering vectors which can effec-tively mediate a certain behavior in the instruction-tuned\nLLM Gemma-2-2B-it (Team et al., 2024). This behavior\nroughly corresponds to the instruction-tuned model correct-ing itself in the midst of generating factually incorrect text.\nIn particular, when the model's response is manually pre-filled with factually incorrect text, the model's continued\ngeneration will then begin with a string like \u201cJust kidding!\"\nor \"Wait, that's not right!\" and continue by providing the\ntrue information. For example, if we pass the model the\nprompt\nUser: What is Albert Einstein best known as?\nModel: Albert Einstein is best known for being a musician.\nthen the model's most likely generated continuations will be\nof the form \"Just kidding! Albert Einstein is actually best-known as a renowned physicist [...]\". We dub the behavior\nin question fictitious information retraction (or FIR for\nshort).\nWe choose to take an in-depth look at this behavior in partic-ular for a few reasons. Firstly, this behavior is highly safety-relevant: understanding how models can correct themselves\neven in the midst of making factual errors may help us\nevaluate the robustness of this behavior in models, and\npotentially even further guide us towards increasing this\nrobustness. Secondly, this is a complex behavior that mani-fests itself in model generations; it thus requires evaluating\nmodel generations rather than e.g. merely looking at model\nprobabilities on a multiple choice dataset. Thirdly, as we\nfound in the course of our investigation, there are multiple\ndifferent \"solutions\u201d to suppressing FIR; this thus provides\na fruitful setting for understanding how different steering\nmethods induce different solutions, and how our evaluation\ntechniques described in \u00a72.2 can be used to distinguish them."}, {"title": "5.1. Setup", "content": "5.1.1. MODEL\nThe model that we investigate is Gemma-2-2B-it, the 2B\nparameter instruction-tuned model in the Gemma 2 family\n(Team et al., 2024). For our quantitative evaluations in \u00a75.3\nbased on the methods in \u00a72.2, we also use the corresponding\nbase model Gemma-2-2B. Due to computational constraints,\nwe load the models in bfloat 16 precision.\n5.1.2. DATASET\nOur dataset consists of the names of well-known public fig-ures who have one of the following six occupations: \u201cactor\",\n\"author\", \"athlete\", \"scientist\", \"politician\", or \"musician\".\nThese names were found via first querying an LLM to gen-erate a list of 100 well-known figures in each field, and then\nfiltering out all names except for those which the model un-der investigation associates with the correct occupation with"}, {"title": "5.1.3. STEERING VECTORS", "content": "For promotion steering, we pair each fake prompt with its\nreal counterpart. We use greedy beam search with width 5 to\nfind the most likely 5-token completion of the real prompt.\n(An example completion is \"Here's a breakdown of\", as in\n\"Here's a breakdown of his fame:\".) We use this completion\nas the optimization target for promotion steering on the fake\nprompt, and stop optimizing when the steered probability\nof this completion on the fake prompt meets or exceeds\nthe completion's unsteered probability on the real prompt. For\nsuppression steering, we minimize the probability of the\ncompletion \"...Just kidding\" on the fake prompt. In both\ncases, we choose one prompt per occupation and optimize a\nsteering vector on each.\nFor reentrant steering, we first perform suppression steer-ing on the fake prompt at layer 1, minimizing the probability\nof the two completions \"...Just kidding\" and \"... Just kid-ding\" (note the difference in spacing). Importantly, for layer\n1 steering, instead of optimizing a vector, we optimize a\nrank-one matrix $V = vu^T$ with $||v|| = ||u|| = 1$. Steering\nwith this matrix maps model activations $x \\rightarrow x + Vx$. We\nthen greedily sample the most likely 10-token completion\non the fake prompt when steered with the matrix, and record"}, {"title": "5.2. Qualitative evaluations", "content": "To evaluate the efficacy of our steering vectors, we first\nidentify a set of five strings that are present in responses\nexhibiting FIR: \"just kidding\", \"trick question\", \"that's not\nright\", \"might be confused\", and \"this is a joke\". In total,\nat least one of these strings was present in 73.2% of un-steered model responses. Most of the remaining responses\nconcerned entities who did engage in multiple occupations,\nsuch as musicians who also performed in films.\nWe then measured the frequency of these strings in the\ngenerations obtained by applying steering vectors; see Fig.\n3. Note that all steering methods achieved a maximum\nof 100% of examples without any FIR strings, except for\nreentrant steering, whose success rate was 99.0%.\nNext, we evaluated the extent to which the steering vectors\nwere successful in getting the model to continue generating\ntext consistent with the fictitious information in the prompts.\nWe passed the steered generations back to Gemma-2-2B-it\nand for each generation, asked it whether or not the fictitious\nattribute was the primary one discussed in the generation\nand whether or not the real attribute was the primary one\ndiscussed. We then recorded the fraction of generations such\nthat the real/fictitious attribute was primary. The results can\nbe found in Fig. 4. Note that with the exception of reentrant\nsteering, all steering methods (including the CAA skyline)\nyield more generations where the real attribute was primary\nthan ones where the fictitious attribute is primary. In other\nwords, the non-reentrant steered generations did not exhibit\nFIR, but nevertheless, the model still reverted to generating\nfactual information. We refer to this behavior as fictitious\nattribute ignorance, or FAI for short. One implication of"}, {"title": "5.3. Base model probability evaluations", "content": "Given the prior results, one possible explanation is that there\nis a single direction in the activation space that prevents\nboth FIR and FAI, which the steering methods \"should have\"\nfound but failed to. To test this, we applied the methods from\n\u00a72.2 using the model Gemma-2-2B, the base model whence\nwas derived the instruction-tuned model whose behavior we\nhave been studying. We aimed to understand to what extent\nFAI is high-probability behavior in the base model which is\nnot finetuned to exhibit FIR.\nWe generated a context consisting of two randomly-chosen\nunsteered instruction-tuned model generations from each\noccupation subsplit in the real completions set. Then, given\nthis context, we looked at the base model surprisal-per-token\nof steered generations (from the three non-reentrant steering\nmethods), along with the surprisal-per-token of real comple-tions, incongruous completions, fictitious completions, and\nunsteered model completions. The results can be found in\nFig. 5. In particular, note that incongruous completions have\nlog probabilities that are almost as high as the real comple-tions', indicating that FAI is natural behavior on the base\nmodel. Similarly, at layers 11 and 13, promotion steering\ngenerations actually have slightly higher log probabilities\nthan the incongruous completions, providing evidence that\nthe steered generations are natural as well.\nIn the reverse direction, we know that reentrant steering\nprovides steering vectors that both remove FIR and suppress\nFAI. Thus, it makes sense to ask whether these steering\nvectors also yield low-surprisal completions; if so, then this\nwould imply that it \u201cshould\u201d be easy to find steering vectors\nto suppress FAI. As it turns out, the answer is no: not only\ndo reentrant-steered generations have lower probabilities\nthan suppression-steered generations when the base model"}, {"title": "6. Discussion and conclusion", "content": "Over the course of our case studies, we find that one-shot\nsteering vectors can be extremely effective in modulating\nsafety-relevant model behavior. In \u00a73, one-shot steering\nvectors were able to expose harmful behavior 93% of the\ntime and suppress it 83% of the time in an alignment-faking\nmodel. In \u00a74, promotion vectors were able to achieve a\nHarmbench attack success rate of 96.9%. And in \u00a75, sup-pression and promotion steering were able to prevent FIR in\n100% of cases, and reentrant steering caused the model to\ncontinue generating fictitious information in 74.2% of cases.\nHowever, beyond being merely a tool for model control,\nwe are interested in one-shot steering vector optimization\nbecause it provides a tool for analyzing the purely causal\nrelationship between model activations and model behavior\nwith minimal reference to data. Given this aim, we posit the\nfollowing preliminary thoughts on what our results might\nimply about this relationship.\n*   Behavior that is natural to humans is not necessar-ily natural to the model. This is most apparent in the\n    \u00a75 results, where reentrant steering performed by far the\n    best w.r.t. the human-centric qualitative evaluations, but\n    yielded behavior that was the least natural from a quant"}]}