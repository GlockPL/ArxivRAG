{"title": "HARP: Human-Assisted Regrouping with Permutation Invariant Critic for Multi-Agent Reinforcement Learning", "authors": ["Huawen Hu", "Enze Shi", "Chenxi Yue", "Shuocun Yang", "Zihao Wu", "Yiwei Li", "Tianyang Zhong", "Tuo Zhang", "Tianming Liu", "Shu Zhang"], "abstract": "Human-in-the-loop reinforcement learning integrates human expertise to accelerate agent learning and provide critical guidance and feedback in complex fields. However, many existing approaches focus on single-agent tasks and require continuous human involvement during the training process, significantly increasing the human workload and limiting scalability. In this paper, we propose HARP (Human-Assisted Regrouping with Permutation Invariant Critic), a multi-agent reinforcement learning framework designed for group-oriented tasks. HARP integrates automatic agent regrouping with strategic human assistance during deployment, enabling and allowing non-experts to offer effective guidance with minimal intervention. During training, agents dynamically adjust their groupings to optimize collaborative task completion. When deployed, they actively seek human assistance and utilize the Permutation Invariant Group Critic to evaluate and refine human-proposed groupings, allowing non-expert users to contribute valuable suggestions. In multiple collaboration scenarios, our approach is able to leverage limited guidance from non-experts and enhance performance. The project can be found at https://github.com/huawen-hu/HARP.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of multi-agent systems, reinforcement learning has shown great promise in fostering cooperation among agents, enabling them to solve complex tasks beyond the capabilities of individual agents [1]\u2013[5]. Research has shown that group division is an effective means of promoting collaboration, both in natural ecosystems [6] and in multi-agent systems [7] within artificial intelligence. Breaking teams into smaller units can facilitate more detailed learning processes while providing increased opportunities for integrating information-rich signals derived from group learning. While reinforcement learning has proven effective for autonomous problem-solving and fostering cooperation among agents, it often struggles with low sample efficiency and poor generalization in intricate environments [8], [9].\nHuman-in-the-loop reinforcement learning (HITL-RL) represents a crucial advancement in overcoming the limitations of traditional reinforcement learning, particularly in complex multi-agent systems [10]. HITL-RL addresses these issues by incorporating human expertise directly into the learning process. Human intuition and domain knowledge provide essential guidance, enabling more accurate corrections in agent behavior and more effective integration of information-rich signals compared to fully automated methods. This collaboration between human insight and algorithmic power not only accelerates learning but also enhances the system's ability to generalize in complex tasks. As a result, HITL-RL offers a transformative approach where human input is not just a supplement, but a critical component for achieving higher performance and efficiency in real-world applications [11]\u2013[13].\nDespite these advancements, existing human-in-the-loop methods predominantly focus on single-agent scenarios, while the integration of human guidance in multi-agent reinforcement learning settings remains largely unexplored. Extending these methods to multi-agent systems presents unique challenges, as guidance from human experts is expensive and rare. The core challenge lies in how humans can effectively provide guidance to multiple agents simultaneously, considering the complex dynamics and interactions within these systems [14]\u2013[16].\nTo address these challenges, we propose a novel framework for human-in-the-loop multi-agent reinforcement learning with dynamic grouping as shown in Fig. 1. Our approach introduces a mechanism for readjusting and reevaluating agent groupings during the deployment phase, guided by"}, {"title": "II. RELATED WORK", "content": "The advancements in multi-agent reinforcement learning (MARL) have significantly enhanced learning efficiency and performance through innovative approaches to agent grouping and role assignment. For instance, ROMA [18] introduces a role-oriented methodology that optimizes conditional mutual information to ensure precise alignment between roles and trajectories. RODE [19] expands upon this by deconstructing the joint action space and incorporating action effects into role policies. Unlike role-based methods, VAST [7] examines the influence of subgroups on value decomposition, employing variable subteams to aggregate local Q-functions into a group Q-function, which is then synthesized into a global Q-function via value function factorization. SOG [20] proposes a dynamic grouping framework where designated commanders extend team invitations, allowing agents to select preferred commanders. GoMARL [21] employs a \u201cselect and kick-out\u201d strategy for automated grouping and integrates hierarchical control within policy learning. This approach achieves efficient cooperation without necessitating domain knowledge. Other grouping methods include approaches like GACG [22], which presents a graph-based technique that models the multi-agent environment as a graph, calculating cooperation demands between agent pairs and capturing dependencies at the group level.\nRecently, human-in-the-loop reinforcement learning has emerged as a transformative approach to enhance policy learning efficiency. Various methodologies have been proposed to integrate human expertise into the learning framework. TAMER [23] incorporates human experts into the agent's learning loop, allowing them to provide reward signals and minimizing discrepancies between the agent's policy and the human reinforcement function. COACH [24] enables non-expert humans to offer actionable advice during interactions with continuous action environments, using binary correction signals to refine agent actions. Recognizing the limited availability of human expert advice, RCMP [25] introduces a selective guidance strategy based on cognitive uncertainty. This approach requests human input only when the agent's uncertainty is high, employing specialized techniques to assess and quantify this uncertainty. The idea is further expanded by HULA [26], which integrates human expert assistance during the deployment phase. In this approach, the agent actively seeks human recommendations when the return variance exceeds a predefined threshold. Recently, OpenAI's o1 model [27] represents a significant advancement in natural language artificial intelligence (AI), leveraging human-in-the-loop reinforcement learning. By incorporating human feedback into its learning process, o1 demonstrates remarkable improvements in response accuracy, and adaptability across various tasks, showcasing the power of human-guided AI optimization.\nIn this paper, we propose a novel approach that integrates multi-agent reinforcement learning grouping techniques with human-in-the-loop learning. Our method introduces a mechanism for readjusting and reevaluating agent groupings during the deployment phase, leveraging guidance from non-expert humans. This approach mitigates the burden of continuous human involvement throughout the reinforcement learning training process. Furthermore, it enables non-expert humans to learn how to provide increasingly effective suggestions over time."}, {"title": "III. METHOD", "content": "In this section, we focus on developing an effective learning method for dynamic group adjustment. Our objective is to learn a grouping function $f_g: A \\rightarrow G$ that maps agents to groups.\nTo achieve this, we introduce an automatic grouping mechanism, as illustrated in Fig. 2. Following the value function decomposition approach [29], [30], we represent the"}, {"title": "A. Preliminary", "content": "In this paper, cooperative tasks are considered which involving n agents, denoted as $A = \\{a_1, ..., a_n\\}$, framed as a decentralized partially observable Markov decision process (Dec-POMDP) [28]. The process is defined by the tuple $G = (S, U, P, r, Z, O, \\eta, \\gamma)$. The environment is characterized by a global state $s \\in S$. At each time step t, each agent a selects an action $u_t$ from its own action space $U_a$, forming a joint action $u_t \\in U^n = U_1 \\times \u00b7\u00b7\u00b7 \\times U_n$. The joint action determines the state transition according to the probability distribution $P(s_{t+1}|s_t, u_t) : S \\times U^n \\times S \\rightarrow [0,1]$. A shared reward is provided by the function $r(s, u) : S \\times U^n \\rightarrow R$, and future rewards are discounted by a factor $\\gamma \\in [0, 1)$."}, {"title": "B. Automatic Grouping Mechanism", "content": "Consider a cooperative task involving a set of n agents. We can partition these agents into a series of groups $G = \\{g_1,..., g_m\\}$, where $1 < m < n$. Each group $g_j$ contains a subset of agents: $g_j = \\{a_j^1,..., a_j^{n_j}\\}\\subseteq A$. The union of all groups covers the entire set of agents: $\\cup_{j=1}^m g_j = A$. Also, the groups are mutually exclusive, meaning $g_j \\cap g_k = \\emptyset$ for $j, k \\in \\{1, 2, . . .,m\\}$ and $j\\neq k$."}, {"title": "C. Permutation Invariant Group Critic", "content": "In multi-agent reinforcement learning scenarios, each agent i in a system possesses a unique state representation $s_i$. A critical challenge is to compute an accurate joint value function $V (s_1, s_2,..., s_n)$ based on these individual states. Conventionally, this is achieved by concatenating the states into a single vector:\n$S = [s_1, s_2, ..., s_n]$\nThis vector is then input into the critic function. However, this method is sensitive to the ordering of agent states, leading to the permutation non-invariance problem:\n$Critic([s_1, s_2, ..., s_n]) \\neq Critic(\\pi([s_1, s_2,..., s_n]))$\nwhere $\\pi(\\cdot)$ denotes any permutation of the state sequence. This issue is even more critical in grouping cause both within-group permutation invariance and between-group permutation invariance need to be satisfied.\nInspired by Liu et al. [31], we propose the Permutation Invariant Group Critic (PIGC). In this approach, the multi-agent environment is represented as a graph, where agents are modeled as nodes and their interactions as edges. Given the group index information, we construct the graph adjacency matrix, where for agents within the same group, we construct an edge between any two agents, and for agents in different groups, we do not construct any connections, i.e. each group is an independent subgraph. Meanwhile, we encode the hidden states of each agent as the node embedding of the graph, as shown in Fig. 3.\nTo compute the output of the group critic, we use a L-layer graph convolutional network $GCN = \\{f_{GCN}^{(1)},...,f_{GCN}^{(L)}\\}$. Graph convolutional network (GCN) layer processes input data in the form of node features and the graph's connectivity structure, typically represented by an adjacency matrix.\n$h^{(l)}_{GCN} = f_{GCN}^{(l)}(h^{(l-1)}) := \\sigma(\\hat{A}_{adj}h^{(l-1)}W^{(l)})$"}, {"title": "D. Human Participation in Group Adjustment", "content": "Relying solely on automatic grouping during training has limited performance at deployment time. In dynamic scenarios, while fixed groupings may achieve good performance during training, a single grouping strategy cannot satisfy all states. Let's use playing soccer as an example. Although we can train players to automatically learn teamwork and complete tasks, seeking optimal groupings, in actual matches (i.e., the deployment phase), a single grouping strategy cannot work effectively from start to finish. Therefore, a coach needs to pause the game appropriately and make timely adjustments to the grouping strategy."}, {"title": "IV. RESULTS", "content": "In this section, we investigate the proportion of human involvement during the deployment phase across different types of maps. We repeated the experiments five times on each map, with each experiment consisting of 31 episodes. We recorded the total number of steps and the number of"}, {"title": "D. Impact of Human Assistance on Challenging Scenarios", "content": "In this section, we examine why limited human guidance during the deployment phase significantly improves the success rate of the game. As observed in Table I, all baselines perform poorly on the 5m_vs_6m map, showing a substantial performance gap compared to the 8m_vs_9m map of similar difficulty. However, actively seeking human assistance during the testing phase leads to a marked improvement in performance. We aim to understand the specific role of human assistance in this context.\nTable III outlines the groupings learned by the automatic grouping algorithm across various maps. On the 5m_vs_6m and 8m_vs_9m maps, the automatic grouping algorithm places all agents in a single group for coordination and cooperation. This approach results in a lack of clear division of labor among agents. As illustrated in Fig. 4 (c) and Fig. 4 (d), the dashed lines represent human-assisted groupings. During testing, there is evident division of labor and strategy among agents. At certain moments, agents exhibit spatial convergence, forming groups. Simultaneously, in terms of strategy, agents with higher health points position themselves more forward, while those with lower health points retreat.\nThis phenomenon is also observable in Fig. 5. During the human assistance process, these phenomena and strategies are typically leveraged to dynamically adjust the behavior and grouping of multiple agents. This adaptive approach, guided by human insight, appears to be a key factor in the improved performance observed when human assistance is incorporated into the deployment phase."}, {"title": "V. CONCLUSIONS", "content": "In this work, we propose an effective multi-agent reinforcement learning framework that actively seeks human assistance for grouping during the deployment phase. As real-time human involvement during training can be cumbersome and time-consuming, we shift human participation to the deployment stage. To enhance the quality of non-expert guidance, we introduce a regrouping and reevaluation method for group critics based on group invariance. By evaluating human-proposed groupings, we maximize the utilization of human suggestions. We tested our approach on six StarCraft II maps across three difficulty levels. Compared to scenarios without human assistance, our method improved success rates by an average of 10%. On some more challenging maps, we increased success rates from 65% to 100%. Our approach has the potential to be extended to other tasks such as human-machine collaboration and sim-to-real transfer [32]-[35]. One promising direction is its integration with multimodal LLMs for complex reasoning tasks [36]. By dynamically grouping agents to specialize in different data modalities and adaptively regrouping based on task demands, the framework enhances multimodal alignment and balances the permutations of inputs. Incorporating real-time human feedback further improves decision-making, benefiting applications such as autonomous vehicles processing diverse sensor data or healthcare systems integrating various patient information. This integration helps make multimodal LLMS more robust and adaptable to complex, real-world challenges."}]}