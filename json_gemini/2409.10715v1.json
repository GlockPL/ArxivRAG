{"title": "Self-Attention Limits Working Memory Capacity of Transformer-Based Models", "authors": ["Dongyu Gong", "Hantao Zhang"], "abstract": "Recent work on Transformer-based large language models (LLMs) has revealed striking limits in their working memory capacity, similar to what has been found in human behavioral studies. Specifically, these models' performance drops significantly on N-back tasks as N increases. However, there is still a lack of mechanistic interpretability as to why this phenomenon would arise. Inspired by the executive attention theory from behavioral sciences, we hypothesize that the self-attention mechanism within Transformer-based models might be responsible for their working memory capacity limits. To test this hypothesis, we train vanilla decoder-only transformers to perform N-back tasks and find that attention scores gradually aggregate to the N-back positions over training, suggesting that the model masters the task by learning a strategy to pay attention to the relationship between the current position and the N-back position. Critically, we find that the total entropy of the attention score matrix increases as N increases, suggesting that the dispersion of attention scores might be the cause of the capacity limit observed in N-back tasks.", "sections": [{"title": "Introduction", "content": "In cognitive science, working memory is defined as the ability of humans to temporarily maintain and manipulate task-relevant information for flexible behaviors [1]. Recent advancements in Transformer-based LLMs have sparked interest in evaluating their cognitive abilities, including working memory capacity [9]. By designing multiple variants of N-back tasks (Figure 1a) [11, 10] and employing different instructional strategies, it has been found that LLMs consistently perform worse as N increases (Figure 1b), which is reminiscent of the capacity limit of human working memory [2, 15, 17].\nHowever, due to the black-box nature of LLMs, we still lack mechanistic insights as to why the observed capacity limit would emerge, especially given the fact that the length of N-back task sequences (e.g., 24 letters in [9]) is well within the context length of these models [16]. To answer this question, we were inspired by the executive attention theory [7, 5, 6] in human working memory research. The executive attention theory proposes that working memory requires executive attention to maintain access to information in the face of interference. suggesting that it is the scarcity of attentional resources [12, 14], but not memory storage itself, that is responsible for working memory capacity limits. In Transformer-based LLMs, the self-attention mechanism computes the importance of each element in the input sequence relative to other elements. While this approach allows the model to focus on relevant information, as N increases in the N-back task, it could be increasingly hard to maintain focus between distant positions. Therefore, we hypothesize that self-attention might be the cause of working memory capacity limits in Transformer-based models.\nIn the current study, we train causal Transformers on N-back tasks and observe that as N increases, the model presents a decline in its prediction accuracy. We further find that the prediction accuracy at position i is positively correlated with the attention score at position i \u2013 N. Furthermore, the model's"}, {"title": "Methods", "content": "Dataset. We use the same procedure described by Gong et al. [9] to generate a dataset of N-back tasks consisting of task sequences and correct answers. Each task sequence contains 24 letters sampled from an alphabet commonly used in the behavioral literature (\u201cbcdfghjklnpqrstvwxyz\"), and the correct answers always consist of 8 matches and 16 nonmatches, mimicking the setup in some human studies. For N \u2208 {1, 2, 3, 4, 5, 6}, we generate 800 sequences for training and 200 sequences for testing, while our analyses mostly focus on N \u2208 {1,2,3} to compare with previous studies.\nModel. We use vanilla Transformers in order to facilitate interpretability, as done in prior work aiming to better understand computations in Transformers in more controlled task settings [4, 13]. We mainly focus our analysis on a causal Transformer containing one decoder layer with only one attention head (Figure 6 in Appendix), although we also test a few architectural variants in the number of decoder layers (L) and number of attention heads per layer (H) for comparisons (see Section 3 for details). The decoder layer contains masked self-attention so that for each position in the sequence the model can only attend to the current and previous positions. No multi-layer feed-forward networks or layer normalization are applied. The decoder layer is then followed by an unembedding layer to project the decoder outputs to two logits (representing match and nonmatch) for each position.\nTraining and Evaluation. We train 50 independent models for each N. We choose to train each model for 10 epochs because empirically the model converges after around 10 epochs of training (see Figure 7 in Appendix for details). Cross-entropy loss is computed between the output logits and the correct answers at each position."}, {"title": "Results", "content": "Model accuracy decreases as N increases. For L \u2208 {1,2} and H\u2208 {1,2, 4}, we train models on the N-back task (Figures 2a) and find a significant decline in model performance as N increases for the 1-layer 1-head model (Kruskal-Wallis test: H-statistic = 38.517, p < .001, \u0454\u00b2 = 0.248; see Table 1 in Appendix for post-hoc comparisons using Mann-Whitney U tests\u00b9). To further confirm this pattern, we extend the task to N = 6, and find a significant logarithmic decline in the test accuracy as N increases (Figure 2b). For models with a larger L or H, most of them achieved over 95% accuracy on all N-back tasks. However, they still present slight declines in test accuracy as N increases, suggesting that the working memory capacity limit does exist in the nature of transformer models.\nAttention scores during training reflect the trajectory of learning. To investigate how the self-attention mechanism influences model performance, we visualize attention maps after each training epoch (Figures 3, 8 and 10). For each position, we also plot the trajectory of attention scores over training epochs (Figures 9, 11, and 12) to see with more granularity how the model learns to perform the task. Starting with almost uniformly distributed attention scores in each row, attention scores gradually aggregate to a line corresponding to the N-back positions. For each position in the sequence, attention scores gradually aggregate to the N-back position over training epochs and attention scores converge faster for positions earlier in sequence (Figures 9, 11, and 12). This shows that the Transformer model learns to master the N-back task by increasing the attention score between the current position and the N-back position.\nAttention score at position i \u2013 N increases with test accuracy at position i. To further investigate the relationship between attention scores and test accuracy, we plot accuracy at position i against the attention score at the position i \u2013 N over training epochs (i \u2208 {1..24}, N \u2208 {1,2,3}). The accuracy at position i is defined as the percentage of the model making a correct prediction at position i. Over training epochs, we find that the attention score at position i \u2013 N increases along with the accuracy at position i (Figure 4a-c). We reason that in order to produce an accurate prediction at position i, the Transformer model needs to learn to put most attention on the i \u2013 N position and reduce dispersion of attention to other positions. To better visualize dispersion of attention scores across positions, we use the same data in Figure 4a-c but assign colors to the dots according to which position each dot belongs to (Figure 4d-f). This reveals a clear pattern that attention scores get dispersed at later locations, suggesting that more interference is caused when there are more preceding positions.\nTotal entropy of attention scores increases as N increases. Building up from the results above, we take a step further to investigate the overall characteristic of attention scores as N increases."}, {"title": "Discussion", "content": "The current study provides important insights for the mechanistic interpretability of working memory capacity limits observed in Transformer-based LLMs [9]. The self-attention mechanism is critical for the model to achieve good performance in the N-back task, but also limits its capacity on the other hand. This is analogous to the mechanism of selective attention in the human brain, which prioritizes relevant information and filter out the rest to ensure effective task performance, but also restricts our information processing by imposing neural and cognitive bottlenecks [3]. Future work should explore a more formal mathematical proof as to why capacity limits might naturally emerge in complex intelligent systems [8, 18]."}, {"title": "Equations", "content": "HN(A) = \n\u2211i=124\u2211j=124 Ai, log (Ai,j)(1)\nAi,j = Softmax(QKT/\u221adk)(2)\nThe entropy HN is well-defined as {Ai,1, Ai,2,..., Ai,i} gives a probability distribution with \u03a3j=124 Ai,j = 1 thanks to the Softmax function and causal masking."}]}