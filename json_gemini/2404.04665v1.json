{"title": "Adaptive Intra-Class Variation Contrastive Learning for Unsupervised Person Re-Identification", "authors": ["Lingzhi Liu", "Haiyang Zhang", "Chengwei Tang", "Tiantian Zhang"], "abstract": "The memory dictionary-based contrastive learning method has achieved remarkable results in the field of unsupervised person Re-ID. However, The method of updating memory based on all samples does not fully utilize the hardest sample to improve the generalization ability of the model, and the method based on hardest sample mining will inevitably introduce false-positive samples that are incorrectly clustered in the early stages of the model. Clustering-based methods usually discard a significant number of outliers, leading to the loss of valuable information. In order to address the issues mentioned before, we propose an adaptive intra-class variation contrastive learning algorithm for unsupervised Re-ID, called AdaInCV. And the algorithm quantitatively evaluates the learning ability of the model for each class by considering the intra-class variations after clustering, which helps in selecting appropriate samples during the training process of the model. To be more specific, two new strategies are proposed: Adaptive Sample Mining (AdaSaM) and Adaptive Outlier Filter (AdaOF). The first one gradually creates more reliable clusters to dynamically refine the memory, while the second can identify and filter out valuable outliers as negative samples.", "sections": [{"title": "1\nINTRODUCTION", "content": "In recent years, there has been a rapid development of intelligent surveillance equipment and an increasing demand for public safety. As a result, a large number of cameras have been deployed in public places such as airports, communities, streets, and campuses. These camera networks typically span large geographic areas with non-overlapping coverage and generate significant amounts of surveillance video daily [27]. The continuous increase in surveillance video and image data, along with the expensive cost of manual annotation, has led to widespread interest in the unsupervised human re-identification task. Unsupervised methods are more scalable and have a higher likelihood of being deployed in the real world compared to supervised methods.\nExisting unsupervised person Re-Id methods fall into the following two categories. One approach [4, 5, 14, 15, 31] aims to transfer knowledge from existing labeled data to unlabeled target data, known as unsupervised domain adaptation (UDA). UDA methods typically employ a two-stage training strategy. First, the model is pre-trained on the labeled dataset from the source. Then, it is fine-tuned on the unlabeled dataset from the target in an unsupervised manner. Another type of work [6, 9, 11, 32, 36, 40] relies on unsupervised learning (USL) to learn feature representations of images from unlabeled data. They generally assign pseudo labels to completely unlabeled training samples. And then gradually use pseudo labels for classification or metric learning."}, {"title": "2 RELATED WORK", "content": "Our work mainly focuses on pure unsupervised person Re-ID. The performance of unsupervised methods relies on learning feature representations. Recently, the state-of-the-art (SOTA) feature representation learning mainly uses Memory dictionary to store all instance features. We categorize the methods of updating the memory dictionary in unsupervised Re-ID based on contrastive learning into the following two categories: (1) Using the average features of all samples within the cluster to update. In [15] and [11], all pseudo-labels are regarded as clean data during optimization, without considering quality diversity. This leads to slow convergence and limited generalization ability. (2) Using the hardest sample to update. Certain methods, such as [6] and [9], employ hard mining during the memory database update process. This approach prioritizes the hardest samples within the class, bringing them closer to the normal samples. During the initial stages of model training, the hardest positive samples within a cluster may be noise samples. This is because the ground truth is not available, which can contaminate their corresponding cluster feature representations and have a negative impact on the model's performance. This problem is particularly serious in large-scale Re-ID datasets, such as MSMT17 [35].\nIn order to address the problems that exist in the above two kinds of methods and select the most suitable samples to update the memory based on the current capabilities of the model, we utilize the concept of curriculum learning [3]. This involves gradually increasing the training difficulty of the model by introducing more complex samples over time. The key to effective curriculum learning is determining the appropriate order of samples. This ensures that the model experiences a smooth transition during the learning process, avoiding both premature difficulty and premature saturation. This process usually requires dynamic adjustments based on model performance and training progress to ensure optimal training results.\nIn traditional curriculum learning [29], a linear scheduling strategy is generally used to start sampling from the simplest data and gradually increase the difficulty until the most difficult. In self-paced learning [21], the model is trained at each iteration using the proportion of data with the lowest training loss. Both learning methods work on the entire training dataset. However, in the field of unsupervised Re-ID, as shown in 1, there are variations in intra-class density and distance between the hardest positive samples across different clusters. It would be inaccurate to use the same learning strategy to select samples within a cluster for updating clustering features. Therefore, in this work, the algorithm we propose works on different categories after clustering, rather than the entire training set.\nIn terms of determining the model's learning ability for different clusters, we drew inspiration from [45] and evaluated the variations within each cluster by measuring the similarity between the hardest positive pair and the least-hardest pair. This allows us to infer the current performance of the model. If there is a significant variation within the class, it indicates that the model's ability is weak. Conversely, if the intra-class variation is small, it suggests that the model's ability is adequate. When the model's ability is sufficient, we utilize the hardest sample to update the memory. When the model's capability is insufficient, select an appropriate sample for updating based on its performance."}, {"title": "2.1 Deep Unsupervised Person Re-ID", "content": "In recent years, unsupervised person re-id methods can be broadly categorized into unsupervised domain adaptation (UDA) and fully unsupervised methods. In UDA-based methods, several studies utilize transformations of image semantic information to minimize the domain gap between the source domain and the target domain. This is achieved by converting source domain images into the style of target domain images [6], while still retaining the identity information of the individuals in the source domain. Alternatively, some methods employ domain-specific networks to utilize complementary information between domains, enhancing the model's performance through domain generation methods.\nFully unsupervised person re-id methods train models on unlabeled datasets. State-of-the-art unsupervised learning pipelines typically involve three stages: pseudo-label generation, memory dictionary initialization, and neural network training [11]. Previous work has made various improvements at different stages of this pipeline. The SpCL [15] introduces a self-paced contrastive learning framework with hybrid memory. The proposed self-paced method gradually creates more reliable clusters to refine the hybrid memory and learning targets. ICE [6] uses pairwise similarity ranking to mine the hardest samples for hard instance contrast, thereby minimizing intra-class variance, and pairwise similarity"}, {"title": "2.2 Memory dictionary", "content": "In the current field of unsupervised representation learning for computer vision, contrastive learning achieves state-of-the-art results. MoCo [18] introduced a dynamic dictionary with a queue and a momentum-updated encoder, aiming to build a large and consistent dictionary for better contrastive learning. In SimCLR [8], achieving good results has also been possible by directly using large batch size for dictionary lookup tasks. BYOL [17] directly removes the contrastive learning of the negative sample part based on MoCo. It adds the Predictor module after the positive sample projection to predict image features. This is achieved through the asymmetric tissue model of the upper and lower branching structure, resulting in better results compared to previous work.\nHowever, the above method treats each unlabeled sample as a distinct class to learn instance discriminative representations. This approach is not suitable for a fine-grained person ReID task.\nTherefore, SpCL [15] redefines the contrastive learning method in the Re-ID task by calculating the loss at the cluster level and updating the memory dictionary at the instance level. Building on this, CC [11] proposes to calculate the loss at the cluster level and update the memory dictionary. So far, many methods based on the aforementioned two works have achieved impressive results."}, {"title": "2.3 Curriculum Learning", "content": "Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula [33]. The original concept of CL is first proposed by [3]. The advantages of applying CL training strategies to miscellaneous real-world scenarios can be mainly summarized as improving the model performance on target tasks and accelerating the training process, which cover the two most significant requirements in major machine learning research.\nIn this paper, the concept of Curriculum learning is introduced for the first time into the task of person Re-ID. This approach accelerates the convergence speed of the model and ultimately achieves the state-of-the-art (SOTA) performance."}, {"title": "3 METHODOLOGY", "content": "In this section, we will mathematically model the entire problem and first introduce our overall approach in 3.1, explaining how to incorporate the concept of contrastive learning into our work. Then, in 3.2, we will provide a detailed explanation of how to evaluate the strength of the model's ability and derive the calculation formula for adaptive similarity from a mathematical perspective. After that, the details of momentum update and its working principle are explained in 3.3. Finally, the outlier is adaptively selected as negative samples in 3.4 to enhance the effect of contrast learning. An overview of the pipeline is shown in 2."}, {"title": "3.1 Overview", "content": "Given an unlabeled data set $X = {X_1, X_2, ...., x_N}$, where $x_i$ represents the i-th image sample and N represents the number of samples in the dataset. Our goal is to train a robust model $f_{\\theta}(.)$ on unlabeled X in order to minimize the variations between the features of the same person captured from various viewpoints or cameras with different settings. Additionally, we aim to ensure that the features of different person are as far apart as possible.\nFor each sample in the dataset, the Re-ID model $f_{\\theta}(.)$ will generate a corresponding feature embedding. Although the initial model is not optimized for the Re-ID task, it still has the most basic level of discriminability. The DBSCAN clustering algorithm is then applied to cluster similar features together and assign pseudo labels to them. The cluster feature representation is initialized and calculated as the average feature vector of the samples in each cluster. The Memory dictionary is then initialized using these cluster features and their corresponding pseudo labels. The formula of k-th cluster features $c_k$ is expressed as follows:\n$c_k = \\frac{1}{|H_k|} \\sum_{x_i \\in H_k} f_{\\theta}(x_i)$ (1)\nwhere $H_k$ denotes the k-th cluster set that contains all the feature embedding within cluster k and $|.|$ denotes the number of features in the set\nDue to the inherent characteristics of the DBScan algorithm, a significant number of outliers will be generated during the clustering process. We will filter the outliers and select suitable outliers to add to the Memory dictionary, considering the current capabilities of the model, as negative samples. During the training phase, we will compute the HybridNCE loss between the query image features and all cluster representations in the dictionary in order to train the network. At the same time, the appropriate query sample feature is selected to update the corresponding dictionary feature based on the intra-class variations of the class to which the query feature belongs. HybridNCE Loss was proposed mainly to increase the number of negative samples, ensuring a more stable and balanced learning process, $L_{hybrid}$ is formulated as follows:\n$-log\\frac{exp((q \\cdot \\Phi_I)/\\tau)}{\\sum_{k=1}^{N_c} exp((q \\cdot \\Phi_k)/\\tau) + \\sum_{k=1}^{N_o} exp((q \\cdot I_k)/\\tau)}$ (2)\nwhere $\\Phi_k$ is the unique representation vector of the k-th cluster. $I_k$ is the outliers filtered based on the current model's ability. For any query person image q, I represents the positive cluster feature to which q belongs. The temperature $\\tau$ is empirically set to 0.05, and"}, {"title": "3.2 Adaptive Model Capability Acquisition", "content": "(...) denotes the inner product between two feature vectors, used to measure their similarity. $n_c$ is the number of clusters and $n_o$ is the number of un-clustered instance.\nIn addition, inspired by MoCo [18], we design our proposed method with a teacher network $f_{\\theta_t}, (.)$ and a student network $f_{\\theta_s} (.)$. The student network is updated by back-propagation and the teacher has the same structure as the student network, but is exponential moving average (EMA) updated by the student model's weights. we learn to match these class probability distributions P by minimizing the Mean Square Loss $L_{mse}$ in the self-supervised manner.\n$L_{mse} = ||P_s (x_i) \u2013 P_t (x_i)||$ (3)\nwhere $P_s$ and $P_t$ are the probability distributions generated by student and teacher network respectively.\nOur optimization goal is to minimize the overall loss, which can be formulated as:\n$L_{total} = L_{hybrid} + \\lambda_m L_{mse}$ (4)\nwhere $\\lambda_m$ is the hyper-parameter to balance these two objectives.\nAlthough the Average strategy used to update the Memory dictionary in SpCL and CC has achieved impressive results, it ignores the need to mine more challenging samples in the later stages of the model. On the other hand, in HHCL [20], ICE, and HDCRL, the hardest positive sample in the class is utilized to update the memory dictionary, resulting in significant improvements. However, this approach does not take into account the relatively weak capabilities of the model in the early stages, which can introduce wrong samples during the training process. To address these issues, we have adopted the concept of contrastive learning, gradually increasing the difficulty of training the model by introducing the complexity of the sample over time.\nThere are N identities in a mini-batch, where each identity has K positive samples. We represent the result of normalizing the characteristics of the k-th instance in class i as $F^k = \\frac{f^k}{||f^k||}$.\nInspired by geometric insight, we find that the hardest pair of positive samples in the class determines the radius of the largest hypersphere covering all samples within the class. The similarity of the most difficult positive sample pairs in class i can be obtained using the following formula:\n$Sim_i = -t log (\\sum_{n=1}^{K} \\sum_{m=1}^{K} e^{\\frac{Ingm}{\\tau}})$ (5)\nEach positive instance, when paired with its furthest positive sample, can form an intra-class hypersphere with varying radii. This is similar to a hypersphere formed using the radius of the hardest positive sample. The sphere formed by a sample can encompass all positive samples. The similarity of the n-th positive pair can be determined using the following equation:\n$Sim_i = -t log (\\sum_{m=1}^{K} e^{\\frac{Ingm}{\\tau}})$ (6)"}, {"title": "3.3 Adaptive Sample Mining", "content": "It is extremely challenging to correct the visual discrepancies between pairs of samples that are caused by complex lighting, dense occlusion, or different viewpoints. Especially when the model's capability is insufficient in the early stages of training, the hardest positive sample pairs are likely to be incorrectly clustered samples. This can lead to conflicts with the corresponding features in the memory dictionary, ultimately misleading the process of feature learning. Therefore, the least hardest positive sample pair should be considered. The similarity of the least difficult positive sample pair is calculated as follows:\n$Sim_i^+ = tlog (\\sum_{n=1}^{K} \\sum_{m=1}^{K} \\frac{1}{e^{\\frac{Ingm}{\\tau}}})$ (7)\nFor categories with large intra-class changes, we believe that the current model's ability is relatively weak and can be addressed by mining the least hardest positive samples and using simpler samples. For data with relatively small intra-class changes, our task model's ability is sufficient, but the training process can be accelerated by incorporating the hardest positive samples and harder samples. Unfortunately, the fixed sampling strategy cannot dynamically handle these two cases. We assume that the variation between the largest and smallest intra-class hyperspheres can be used to approximate the feature variation within a class. Based on this, we propose an adaptive weighting strategy that dynamically balances the two positive pairs according to different intra-class variations.\nTherefore, to weigh the variations in the training phase, we compute the harmonic average of the hardest and least hardest similarities of class i in the mini-batch:\n$ Sim_i^h = \\frac{2Sim_i Sim_i^+}{Sim_i + Sim_i^+}$ (8)\nBecause the harmonic mean property satisfies our proposed requirements, we adopt the harmonic mean of similarities as an adaptive weight to balance the hardest and least hardest pairs of each class:\n$a_i = \\begin{cases} Sim_i^h, & Sim_i \u2265 0\\\\ 0, & Sim_i < 0 \\end{cases}$ (9)\nThe weighted positive similarity can be given by:\n$Sim = a_iSim + (1 \u2212 a_i)Sim^+$ (10)\nDifferent from the instance-level memory dictionary, we mainly store the features of each cluster in the memory-based dictionary, and calculate the variation between samples within the class corresponding to the pseudo-label according to the weighted positive sample pair similarity obtained in 3.2.\n$diff_i = \\frac{Sim - Sim^+}{Sim - Sim^h}$ (11)\nIf the intra-class variation is small, use the hardest sample to update the corresponding Memory dictionary. If the intra-class variation is large, calculate the sample that best fits the current"}, {"title": "3.4 Adaptive Outlier Filter", "content": "difficulty level according to diff to update. The specific formula is as follows:\n$\\beta_i = \\begin{cases} 1, & round(\\frac{Sim_i^+}{Sim}) == 1\\\\ diff_i, & Others \\end{cases}$ (12)\nWe rewrite the momentum updating formula as follow:\n$\\Phi_k = m\\Phi_k + (1-m)q_{\\beta *N}^{rank}$ (13)\nwhere $q_{\\beta *N}^{rank}$, represents the (\u03b2 *N)-th sample feature obtained by sorting from largest to smallest after calculating the similarity between the i-th cluster feature and the sample.\nThe average of the diff values in all clusters $diff_{global}$ is used as an indicator to measure the difficulty of the current model. Based on this indicator, outliers are reintegrated into the Memory dictionary to increase the number of negative samples and enhance contrastive learning. This is achieved through computational sampling. The distance between samples and (clustering features + outliers) is sorted from largest to smallest. In the early stages of the model, samples with further distances should be selected as much as possible, as the model's capability is not yet sufficient. At this time, the samples that are further away are more likely to be negative samples. As the model's ability strengthens, all outliers will be absorbed from far to near.\n$diff_{global} = \\frac{\\sum_1^N diff_i}{N}$ (14)"}, {"title": "4 EXPERIMENTS", "content": "We evaluate our proposed method on two large-scale person re-identification (Re-ID) benchmarks: Market-1501 [43], MSMT17 [35]. Market-1501 consists of 32,668 annotated images of 1,501 identities shot from 6 cameras in total, for which 12,936 images of 751 identities are used for training and 19,732 images of 750 identities are in the test set. MSMT17 is the largest Re-ID dataset consisting of 126,441 bounding boxes of 4,101 identities taken by 12 outdoor and 3 indoor cameras. The training set has 32,621 images of 1,041 identities and the test set has 93,820 images of 3,060 identities.\nEvaluation protocol. In all of the experiments, no ground truth identities were provided for training. Both Cumulative Matching Characteristics (CMC) [16] Rank1, Rank5, Rank10 accuracies and mean Average Precision (mAP) [1] are used in our experiments. There are no post-processing operations in testing, such as reranking [44]."}, {"title": "4.1 Datasets and Evaluation Protocol", "content": "Datasets. We evaluate our proposed method on two large-scale person re-identification (Re-ID) benchmarks: Market-1501 [43], MSMT17 [35]. Market-1501 consists of 32,668 annotated images of 1,501 identities shot from 6 cameras in total, for which 12,936 images of 751 identities are used for training and 19,732 images of 750 identities are in the test set. MSMT17 is the largest Re-ID dataset consisting of 126,441 bounding boxes of 4,101 identities taken by 12 outdoor and 3 indoor cameras. The training set has 32,621 images of 1,041 identities and the test set has 93,820 images of 3,060 identities.\nEvaluation protocol. In all of the experiments, no ground truth identities were provided for training. Both Cumulative Matching Characteristics (CMC) [16] Rank1, Rank5, Rank10 accuracies and mean Average Precision (mAP) [1] are used in our experiments. There are no post-processing operations in testing, such as reranking [44]."}, {"title": "4.2 Implementation Details", "content": "General training settings. We adopt ResNet-50 [19] as the backbone of the feature extractor and initialize the model with the parameters pre-trained on ImageNet [12]. We remove the sub-modules after the \"Conv5\" and add the global average pooling (gap) followed by the batch normalization and L2 normalization layers, finally it will produce 2048-dimensional features for each image. An Adam optimizer with a weight decay rate of 0.0005 is used to optimize our networks. The initial learning rate is 0.00035 with a warmup scheme in the first 10 epochs, the total epoch number is 80 and 50 for Market-1501 and MSMT17 respectively. No learning rate decay is used in the training.\nCurrently, in the field of unsupervised person Re-ID, two representative papers, CaCL [22] and DUCL [26], adopt the concept of curriculum learning. However, their emphasis differs from the approach proposed in this article. First, the focus is on the field of unsupervised domain adaptation, while the method proposed in this chapter primarily targeting purely unsupervised scenarios. Second, CaCL introduces a camera-driven curriculum learning framework, leveraging camera labels of person images to gradually transfer knowledge from the source to target domains. Specifically, the target domain data is divided into multiple subsets based on the camera labels. Initially, a single subset (images captured by a single camera) is used to train the model. Subsequently, additional subsets are gradually incorporated for training following the course sequence determined by scheduling. The scheduler considers the maximum mean difference (MMD) between each subset and the source domain dataset, so that subsets closer to the source domain enter the course first. DUCL proposes a dual uncertainty-guided course learning framework, aiming to solve two main problems in clustering-based unsupervised learning methods: ignoring the differences in the model's tolerance to various noise levels, which can lead to the model possibly memorizing the error patterns caused by noise, and converging too quickly in the early stages of training, resulting in overfitting. The methods proposed in the first two papers are innovations within the curriculum learning paradigm, while the adaptive intra-class variation contrastive learning algorithm proposed in this paper is inspired by the concept of curriculum learning but does not strictly adhere to the curriculum learning paradigm.\nIn particular, experiments have shown that MSMT17 and Market1501 cannot use the exact same memory dictionary update strategy. This is because the MSMT17 dataset is more significantly impacted by lighting and weather conditions, and the features other than pedestrians are more complex compared to Market1501. Therefore, the features extracted by the model will be affected by more external features when clustered. When training the MSMT17 dataset, we assign a weight to slow down the sample filtering process.\n$\\gamma = \\frac{cur_{epoch}}{total_{epoch}}$ (15)\nwhere add this weight to 11\nOur network is trained on 4 NVIDIA GeForce RTX 2080 Ti GPUs using the PyTorch framework.\nTraining data organization. The input image is resized 256 \u00d7 128. For training images, the input images are resized to 256 \u00d7 128 on all datasets, and the batch size is set to 256. For each minibatch, we randomly select 16 pseudo identities, and each identity has 16 different images. Random flipping, cropping, and erasing are applied as data augmentation.\nClustering settings. For the beginning of each epoch, we use DBSCAN [13] for clustering in order to generate pseudo labels. The maximum distance between two samples in DBSCAN is set to 0.5 in Market-1501 and 0.7 in MSMT17."}, {"title": "4.3 Comparison with State-of-the-art Methods", "content": "We compare our proposed method with state-of-the-art Re-ID methods including the unsupervised domain adaptation methods and the purely unsupervised methods. The comparison results on Market-1501 [43] and MSMT17 [35] are reported in 1. As illustrated in this table, our proposed method achieves a new state-of-the-art performance of 87.4%, 38.8% on the Market-1501 and MSMT17 datasets, respectively. This surpasses all published results for unsupervised person Re-ID tasks on Market-1501 and all methods that do not specifically operate on the camera on MSMT17. Compared with the baseline method HDCRL [9], our proposed method achieves a higher mAP by 2.9%, and 18.1% on Market-1501 and MSMT17, respectively. Furthermore, when compared to approaches based on contrastive learning, our method demonstrates superior performance on both datasets. Furthermore, our proposed method achieves better performance than all UDA methods that use additionally labeled source datasets on both datasets, even without any identity annotation. Compared with the related state-of-the-art work ISE [40] which requires generating support samples from actual samples to their neighboring cluster centroids, we can surpass it without using any extra generated samples. With GeM [28], our method still consistently achieves better results.\nNote that, unlike ICE [6] and CAP [32], we do not utilize the camera information. Under the non-camera setting, our method obtains 36.0%/65.6% mAP/Top-1 on MSMT17, largely outperforming ICE [6] (without cameras) and HDCRL [9]."}, {"title": "4.4 Ablation Study", "content": "The performance boost of Adaptive Curriculum Contrastive Learning in unsupervised Re-ID mainly comes from the proposed strategies of Adaptive Sample Mining and Adaptive Outliers Filter. We conduct ablation experiments to validate the effectiveness of each component, which is reported in 2.\nEffect of the Adaptive Sample Mining. We compared four memory bank update strategies on the Market1501 and MSMT17 datasets. The results show that our method selects different samples from different classes to update the corresponding Memory bank. This is done by calculating the model's learning capabilities for each category. Without considering outliers, our proposed Adaptive Sample Mining method achieved state-of-the-art (SOTA) performance on both datasets, with an increase of 1.1% and 1.0% respectively in mean Average Precision (mAP). In particular, CM refers to updating the memory by utilizing all encoding features of the class. Hardest refers to updating the memory by using the"}, {"title": "5 CONCLUSION", "content": "In this paper, we propose a novel method for unsupervised person Re-ID called adaptive curriculum contrastive learning. The core of this method is an Adaptive Intra-Class Variation Contrastive Learning (AdaInCV) algorithm based on the concept of curriculum learning. This algorithm utilizes the intra-class variation size after clustering to assess the learning capability of the model for each class. This allows for the selection of suitable samples during the model training process. Based on this algorithm, we propose two new strategies: Adaptive Sample Mining (AdaSaM) and Adaptive Outlier Filter (AdaOF). AdaSaM evaluates the capabilities of the current model and dynamically selects the most appropriate samples to update the memory dictionary. This allows the model to continue receiving valuable learning signals. AdaOF can dynamically filter out a large number of information-rich outliers as negative samples, thereby enhancing contrastive learning. Through extensive experiments on two benchmark datasets, our method demonstrates superior performance compared to all existing purely unsupervised and UDA-based Re-ID methods. Furthermore, our method converges faster, which offers significant advantages for future practical applications and deployments. In fact, this method essentially improves contrastive learning for fine-grained datasets. In the future, we will strive to expand this approach to encompass a broader range of contrastive learning and further explore methods for dynamically acquiring model capabilities."}]}