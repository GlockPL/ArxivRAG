{"title": "Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency", "authors": ["Shiji Zhao", "Ranjie Duan", "Fengxiang Wang", "Chi Chen", "Caixin Kang", "Jialing Tao", "YueFeng Chen", "Hui Xue", "Xingxing Wei"], "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive performance and have been put into practical use in commercial applications, but they still have potential safety mechanism vulnerabilities. Jailbreak attacks are red teaming methods that aim to bypass safety mechanisms and discover MLLMs' potential risks. Existing MLLMs' jailbreak methods often bypass the model's safety mechanism through complex optimization methods or carefully designed image and text prompts. Despite achieving some progress, they have a low attack success rate on commercial closed-source MLLMs. Unlike previous research, we empirically find that there exists a Shuffle Inconsistency between MLLMs' comprehension ability and safety ability for the shuffled harmful instruction. That is, from the perspective of comprehension ability, MLLMs can understand the shuffled harmful text-image instructions well. However, they can be easily bypassed by the shuffled harmful instructions from the perspective of safety ability, leading to harmful responses. Then we innovatively propose a text-image jailbreak attack named SI-Attack. Specifically, to fully utilize the Shuffle Inconsistency and overcome the shuffle randomness, we apply a query-based black-box optimization method to select the most harmful shuffled inputs based on the feedback of the toxic judge model. A series of experiments show that SI-Attack can improve the attack's performance on three benchmarks. In particular, SI-Attack can obviously improve the attack success rate for commercial MLLMs such as GPT-40 or Claude-3.5-Sonnet. Warning: This paper contains examples of harmful texts and images, and reader discretion is recommended.", "sections": [{"title": "1. Introduction", "content": "Multimodal Large Language Models (MLLMs), e.g., GPT-4o [21], and Claude-3.5-Sonnet [1], have made significant progress in achieving highly general visual-language reasoning capabilities. Due to the potential broad impact on society, it is critical to ensure that responses generated by MLLMs do not contain harmful content such as violence, discrimination, fake information, or immorality. However, MLLMs face complex safety risks when processing complex information [7, 15, 17, 22]. For example, attackers can exploit vulnerabilities in the model when processing text-image inputs to bypass the safety mechanisms of MLLMs and induce the model to generate harmful content. So it is critical for red teaming to explore potential safety vulnerabilities in MLLMs, which is of great guiding significance for building safe, responsible, and reliable AI systems.\nJailbreak attacks are first proposed and studied in LLM [6, 14, 25, 26, 41]. As for MLLMs, [7] discovers that the attack effect can be further enhanced by introducing image modality; [17, 22, 24, 36] try to embed harmful intention into pictures in an optimized way in the form of adversarial perturbations; [12, 15] propose to concat harmful typography with generated harmful pictures to bypass the inner safety defense mechanism. However, many current works either require careful design or complex optimization in the form of text or image, which is relatively complicated. On the other hand, although existing methods can bypass the safety mechanisms of open-source MLLMs, commercial closed-source MLLMs often have additional outer safety guardrails [23, 29], which are able to detect harmful intention and intercept jailbreak attack instructions, resulting in limited performance of current jailbreak attacks [31].\nDifferent from previous research, we exploit the \u201cadvantages\" of the MLLMs to design a \"clever\" attack. We hope to explore the MLLMs' gap between comprehension ability and safety ability, which may pose potential risks to be utilized by attackers. Some studies [9, 32] notice that the MLLMs can still maintain competitive performance towards the shuffled texts and images in some tasks, e.g., text or image retrieval. Here we wonder if the MLLMs have a similar comprehension ability towards shuffled harmful instructions and if MLLMs' defense mechanisms have a similar safe ability towards shuffled harmful instructions.\nSurprisingly, we find that the MLLMs have Shuffle Inconsistency between comprehension ability and safety ability for shuffled harmful instruction. Specifically, for the comprehension ability, MLLMs can understand the shuffled harmful image-text instructions well similar to the unshuffled harmful instructions. However, for the safety ability, the MLLMs react differently: the defense mechanisms can be easily bypassed by the shuffled harmful instructions, inducing harmful responses. Meanwhile, we notice the safety vulnerability manifested by Shuffle Inconsistency exists in the inner safety alignment and outer safety guardrails of both open-source and closed-source MLLMs.\nBased on the above exploration, we propose an image-text jailbreak attack named SI-Attack. Specifically, to utilize the Shuffle Inconsistency for shuffled harmful instruction and overcome the shuffle randomness, we apply a query-based black-box optimization strategy to enhance the attack's effectiveness by selecting the most harmful shuffled instructions based on feedback from a toxic judge model. A series of experiments demonstrate that SI-Attack can effectively boost the attack performance of harmful jailbreak instructions towards both the open-source and closed-source MLLMs on three benchmarks. In particular, SI-Attack can easily bypass the closed-source MLLMs, e.g., GPT-4o and Claude-3.5-Sonnet, and can induce harmful responses.\nOur contribution can be summarized as follows:\n\u2022 We are the first to find that MLLMs have Shuffle Inconsistency between comprehension ability and safety ability for shuffled harmful instruction. For comprehension ability, MLLMs can understand shuffled harmful instructions; but for safety ability, MLLMs' defense mechanisms"}, {"title": "2. Related Work", "content": "Following the jailbreak attack on LLMs, many studies [3, 17, 22, 27, 34, 36] extend to the jailbreak attacks towards the MLLMs [27, 34]. Some jailbreak attack methods [3, 17, 22, 36] attempt to add adversarial perturbations to images or texts to bypass the safety defense mechanisms of MLLMs. Bailey et al. [3] try to optimize an adversarial image to make MLLMs generate harmful responses. Shayegani et al. [24] embed malicious triggers into benign clean images. Zhao et al. [36] regard the MLLMs as a black-box model and optimize the image and text prompt via querying the model to estimate the gradient. Niu et al. [17] select some local white-box MLLMs as alternative Models to obtain the adversarial image for jailbreak attack. Qi et al. [22] try to obtain a universal image that can combine with any harmful text to jailbreak MLLMs. Some jailbreak methods [7, 12, 15] attempt to generate new images that contain harmful information and combine them with corresponding texts to jailbreak MLLMs. Figstep [7] attempts to embed the harmful text into a blank image by typography, which fully applies the superior Optical Character Recognition ability of MLLMs. Following the above research, Liu et al. [15] generate a query-based image applying stable diffusion and corresponding typography. Li et al. [12] refines the input prompts for MLLMs iteratively. Unlike the above research, we find MLLMs have Shuffle Inconsistency for shuffled harmful instruction, and we further design a corresponding image-text jailbreak attack method."}, {"title": "2.2. Defense Mechanisms for MLLMS", "content": "As jailbreak attacks continue to discover vulnerabilities in the MLLMs, corresponding defense methods have also been widely studied, mainly including detection of jailbreak attacks and safety alignment against jailbreak attacks. For the detection of jailbreak attacks, LLama-guard [10] is proposed to detect the harmful intention by fine-tuning the LLama. [33] propose a mutation-based method for multimodal jailbreaking attack detection. Xu et al. [28] design a plug-and-play jailbreaking detector to identify harmful image inputs, utilizing the cross-modal similarity between harmful queries and adversarial images. Zhao et al. [35] find that the first token output by MLLMs can distinguish harmful or harmless prompts, and tune a corresponding classifier. In addition, Some commercial services are also used to detect jailbreak attacks, e.g., ChatGPT [19] or some interfaces including PerspectiveAPI [18] and the ModerationAPI [20]. For the safety alignment against jailbreak attacks, Zong et al. [39] set a vision-language safety instruction-following dataset named VL-Guard to finetune the MLLMs. Chakraborty et al. [4] attempt to finetune MLLMs only in the textual domain for cross-modality safety alignment."}, {"title": "3. Shuffle Inconsistency for Harmful Prompt", "content": "From previous studies, we notice that MLLMs have general comprehension capabilities. As we know, humans are able to understand text and images that are simply shuffled, the previous study [9] also points out that LLMs like BERT are resilient to shuffling the order of input tokens, and [32] also demonstrates that the current vision-language models can still have competitive performance towards the shuffled texts and images on some tasks, e.g., image-text retrieval. In the jailbreak scenario, we generate the above two confuses: (1) From the perspective of comprehension ability, if the MLLMs themselves understand the shuffled harmful texts and images? (2) From the perspective of safety ability, if the MLLMs' defense mechanisms defend against the shuffled harmful texts and images?"}, {"title": "3.1. Text Shuffle Inconsistency", "content": "Here we initially explore the corresponding ability of MLLMS towards shuffled harmful texts. As for the evaluation model, we select two open-source MLLMs: LLaVA-NEXT [11], InternVL-2 [5], and two closed-source MLLMS: GPT-4o [21] and Gemini-1.5-Pro [8], and all the MLLMs have competitive safety performance as mentioned in [34]. Here we select a sub-dataset (01-Illegal-Activitiy) in MM-safetybench [15]. To measure the harmful intention of MLLMs' replies, we use ChatGPT-3.5 as a judge model to calculate the toxic score based on the origin input questions and responses of MLLMs. Following [26], the toxic score has 5 levels from 1 to 5, while the low score indicates a safe response, and the high score indicates an unsafe response. Since the shuffling operation has some randomness, we perform three random shuffles and report the highest toxic scores. The results are shown in Figure 2a.\nTo our surprise, based on the results, we find that compared with the unshuffled text, the harmful score of the model's response to the shuffled text not only does not decrease but obviously increases for all four MLLMs. Specifically, for open-source LLaVA-NEXT and InternVL-2, the response's toxic scores to origin text input are 2.48 and 1.65, while the response's toxic scores to shuffled input are 3.32 and 3.38, respectively. Meanwhile, for open-source GPT-4o and Gemini-1.5-Pro, the response's toxic score of shuffled text prompts increases by 1.68 and 1.83 compared with the origin text input prompt, respectively. The results show that MLLMs can understand the harmful intention of the shuffled texts within a certain range. However, despite the defense mechanisms of four MLLMs can defend against unshuffled harmful texts, they can hardly protect models from the shuffled harmful texts. Thus, the comprehension ability and safety ability exist in the Text Shuffle Inconsistency for shuffled harmful instruction."}, {"title": "3.2. Image Shuffle Inconsistency", "content": "In addition to exploring the Text Shuffle Inconsistency for harmful instruction, we also attempt to randomly shuffle the harmful input images in patch-wise levels and explore if a similar phenomenon exists for the harmful image. Here we divide the input images into 4 patch-wise blocks and randomly shuffle them. The evaluation metric for the toxicity score and the applied evaluation dataset remains the same with the subsection 3.1. The results are in Figure 2b.\nBased on the results, we find that when the MLLMs' responses for shuffled images have a similar performance to the shuffled texts: the response's toxic scores to the shuffled image inputs increase compared with the unshuffled. For instance, for LLaVA-NEXT and InternVL-2, the response's toxic scores to the origin image input increase by 1.21 and 0.52. For GPT-4o and Gemini-1.5-Pro, the response's toxic score of shuffled image prompts increases by 0.62 and 0.84 compared with the origin prompts. For the comprehension ability, MLLMs can understand the shuffled images; for the safety ability, defense mechanism towards the unshuffled images performs well, but the defense towards the shuffled harmful image is limited, which demonstrates the Image Shuffle Inconsistency for the shuffled harmful instruction."}, {"title": "3.3. More Analysis towards Shuffle Inconsistency", "content": "The above subsections show that MLLMs' defense mechanisms have the text-image Shuffle Inconsistency between comprehension ability and safety ability for the shuffled harmful instruction. Here we attempt to analyze the reason of Shuffle Inconsistency from different types of MLLMs.\nHere we first analyze the Shuffle Inconsistency for open-source models. To explore the MLLMs' inner performance change brought by the shuffled instructions, we visualize the model's hidden states and analyze the internal reasons. Following [37], we select the last input token outputted by the top model layer of LLaVA-NEXT and InternVL-2, and then compute the first two principal components for visualization, the results are in Figure 3. Surprisingly, the model's comprehension ability of the instructions should be basically consistent shown in [9, 32], but the results show that MLLMs indeed have different reactions: The model intermediate state corresponding to the origin and shuffled harmful inputs can be clearly distinguished, which means that the model has different response tendencies. Combined with Figure 2, this difference is mainly reflected in the toxicity of the model's response, which means the safety ability of MLLMs causes different reactions for the unshuffled and shuffled harmful inputs. For open-source MLLMs, the safety ability lies in the inner safety alignment, which adds a rejection response to harmful content inputs during the model training process. So the shuffle inconsistency exists between comprehension ability and safety ability brought by inner safety alignment for open-source MLLMs.\nFor closed-source MLLMs, besides inner safety alignment, outer safety guardrails also exist for harmful input content detection [23, 29, 30]. If harmful intention is detected, the model is forced to refuse to respond. Due to the black-box characteristics, we cannot analyze more basic principles. However, we can still judge the model's safety ability towards the shuffled harmful inputs through the responses' harmfulness. From the results in Figure 2, the shuffled harmful inputs have indeed bypassed the safety ability, which shows the shuffle inconsistency also exists between comprehension ability and safety ability brought by outer safety guardrails for closed-source MLLMs."}, {"title": "4. Jailbreak based on Shuffle Inconsistency", "content": "Based on the exploration in section 3, we find MLLMs have the image-text Shuffle Inconsistency between comprehension ability and safety ability for harmful intention, which demonstrates that the defense mechanisms of MLLMs have potential vulnerabilities. However, the shuffling operation has a certain degree of randomness, not all shuffled harmful texts and images can bypass the defense mechanism, causing an unstable attack performance. Therefore, to fully utilize Shuffle Inconsistency and overcome its instability, we propose an image-text jailbreak attack named SI-Attack to get the proper inputs where the model can both understand the harmful intention and bypass defense mechanisms.\nSpecifically, to utilize the Text Shuffle Inconsistency, we first split the text input prompts into word lists. Then we randomly shuffle the word list and reassemble the shuffled words into a new sentence, which is formulated as follows:\n$T' = Shuffle_w(T), T = [w_1, w_2, ..., w_n]$, (1)\nwhere T denotes the origin harmful text prompt with a total of n words, $w_i$ represents the i-th word in the entire text of T. $Shuffle_w(\u00b7)$ is the shuffle operation function that randomly shuffle the text T in the word-wise level, T' is the shuffled harmful text prompt.\nTo utilize the Image Shuffle Inconsistency, we split the image input prompts into m patch blocks. Then we randomly shuffle these patch blocks and the shuffling operation can be formulated as follows:\n$I' = Shuffle_p(I), I = [p_1, p_2,...,p_m]$, (2)\nwhere I denotes the origin harmful image, and the harmful image is divided into m patch blocks. $Shuffle_p(\u00b7)$ is the function that randomly shuffles the image I in the patch-wise level, I' is the shuffled harmful image prompt. The basic form of SI-Attack can be formulated as follows:\n$y = M(T',I')$, (3)\nwhere M denotes the target MLLMs, and y denotes the output response of the MLLMs based on the shuffled harmful text prompt T' and the image prompt I'.\nTo overcome instability brought by shuffle, we apply the query-based black-box optimization ideology based on the feedback from the toxicity judge model. When the target MLLMs generate a corresponding response towards the"}, {"title": "5. Experiments", "content": "Evaluation MLLMs. In this study, we evaluate both open-source and closed-source commercial models. For the open-source MLLMs, we select four mainstream MLLMs, including LLaVA-NEXT [11], MiniGPT-4 [38], InternVL-2 [5], and VLGuard [40]. Specifically, for LLaVA-NEXT, we select the LLaVA-1.6-Mistral-7B version; for MiniGPT-4, we apply the version of LLaMA-2-Chat-7B; for the InternVL-2, we apply the InternVL-2-8B version; for the VLGuard, we select the VLGuard-7B version. All mentioned models utilize the weights provided by their original repositories. It should be mentioned that VLGuard is a safety Fine-Tuning model to defend against jailbreaking attacks, while LLaVA-NEXT and InternVL-2 have competitive safety performance between the open-source models as mentioned in [34]. For the closed-source commercial models, we also select four mainstream MLLMs, including GPT-4o (GPT-4o-0513) [21], Claude-3.5-Sonnet [1], Gemini-1.5-Pro [8], and Qwen-VL-Max [2]. Here we access GPT-4o API from Azure OpenAI, and access Claude-3.5-Sonnet API from AWS Anthropic, Gemini-1.5-Pro API from Google, and Qwen-VL-Max API from Aliyun.\nEvaluation Metric. Here we select two metrics: Toxic Score, and Attack Success Rate (ASR) to measure the harmfulness of the model response. As mentioned in previous studies [15, 16], GPT's judgment of harmful content is closest to manual judgment. Therefore, we use ChatGPT-3.5 as a judge to give a toxic score based on the harmful prompt and model's responses. Following [26], the toxic score ranges from 1 to 5, and the lower score means the safer model response, and the higher score means the more harmful model response; If the toxic score is higher than the ASR threshold $S_T$, the jailbreak attack is successful, and the ASR can be formulated as follows:\n$ASR = \\frac{sum\\{J(I,y) \\ge S_T\\}}{N_{total}}$, (4)\nwhere $sum\\{J(I,y) \\ge S_T\\}$ denotes the num of jailbreak"}, {"title": "5.1. Experimental Settings", "content": "Evaluation MLLMs. In this study, we evaluate both open-source and closed-source commercial models. For the open-source MLLMs, we select four mainstream MLLMs, including LLaVA-NEXT [11], MiniGPT-4 [38], InternVL-2 [5], and VLGuard [40]. Specifically, for LLaVA-NEXT, we select the LLaVA-1.6-Mistral-7B version; for MiniGPT-4, we apply the version of LLaMA-2-Chat-7B; for the InternVL-2, we apply the InternVL-2-8B version; for the VLGuard, we select the VLGuard-7B version. All mentioned models utilize the weights provided by their original repositories. It should be mentioned that VLGuard is a safety Fine-Tuning model to defend against jailbreaking attacks, while LLaVA-NEXT and InternVL-2 have competitive safety performance between the open-source models as mentioned in [34]. For the closed-source commercial models, we also select four mainstream MLLMs, including GPT-4o (GPT-4o-0513) [21], Claude-3.5-Sonnet [1], Gemini-1.5-Pro [8], and Qwen-VL-Max [2]. Here we access GPT-4o API from Azure OpenAI, and access Claude-"}, {"title": "5.2. Performance on Open-source MLLMS", "content": "We compare SI-Attack with QR-Attack, HADES, and Figstep-Attack. The results are in Table 1, Table 2, and Table 5. We can see that SI-Attack can obviously enhance the toxic score and ASR compared with other attacks."}, {"title": "5.3. Performance on Closed-source MLLMS", "content": "To demonstrate the generality and practicality of our approach, we conduct experiments on four commercial closed-source models. The results are shown in Table 3, Table 4, and Table 5. From the results, SI-Attack shows the attack effectiveness on closed-source commercial MLLMs similar to open-source MLLMs. Although the ASR of the origin jailbreak instruction is very low, SI-Attack can obviously improve the attack performance towards the MLLMs.\nSpecifically, in MM-safetybench, our SI-Attack achieves attack success rates of 68.57%, 47.20%, 71.25%, and 68.63% on GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, and Qwen-VL-Max, which are better than the QR-Attack by 47.80%, 39.70%, 50.18%, and 35.59%, respectively. Meanwhile, our SI-Attack can still have a similar superiority compared with HADES and Figstep. The results indicate that our SI-Attack can bypass the outer safety guardrails of the commercial closed-source models.\nIt should be noticed that although Claude-3.5-Sonnet shows the best robustness, SI-Attack can still improve ASR by 39.80%, 9.87%, and 19.20% compared with QR, HADES, and Figstep, which shows the effectiveness of our SI-Attack. Interestingly, Claude-3.5-Sonnet contains strong robustness towards HADES, which exists a gap compared with others, we guess that MM-safetybench and SafeBench do not contain the obvious offending words, while text prompts in HADES dataset contain some sensitive words, e.g., \"violence\", \"abuse\", which may be filtered by the outer safety guardrail of Claude-3.5-Sonnet."}, {"title": "5.4. Ablation Study", "content": "We perform ablation experiments on every component of SI-Attack. Here we verify the necessity of image and text shuffling operations. For the baseline jailbreaking instruction, we select the sub-dataset (01-Illegal-Activity) in MM-safetybench, which only contains the generated image without typography. For the operation of only shuffled images and texts, we keep all the experimental settings the same as the final version. The results are shown in Table 6.\nWe can find that only shuffled images or shuffled texts can bring obvious improvement based on the original harmful images and texts, and the shuffled images and texts can achieve the best attack performance, which demonstrates the effectiveness of both image and text shuffling operation. Meanwhile, we also notice that the SI-Attack of only shuffled texts can bring higher toxic scores and attack success rates compared with the version of only shuffled images, which demonstrates that MLLMs have more serious safety vulnerabilities on the text side than on the image side."}, {"title": "Effects of Query-based Optimization", "content": "Here we verify the necessity of the query-based optimization. Based on the baseline instruction, we randomly shuffle the images and texts without additional query and directly input the harmful shuffled instruction to attack the target MLLMs, and the results are shown in Table 7. Although randomly shuffled images and texts can bring an improvement towards the origin images and texts, they still exists an obvious gap compared with optimized shuffled images and texts. The results demonstrate that it is necessary to query the toxic judge to obtain the input where the model can both understand harmful intentions and bypass defense mechanisms.\nMeanwhile, we discuss the influence of Max query Iteration and select different Iteration: 1 iters, 5 iters, 10 iters, and 20 iters, and the results are in Table 8. From the results, we find that within a certain range, increasing the number of optimization iterations can make the attack more effective. When the max query iteration is 10, the attack has achieved nearly the best results. However, when the query iteration is further increased, the improvement of the attack effect is limited, e.g., 20 iters. Therefore, we choose the 10 max query iteration with the best comprehensive performance of effect and efficiency in the final setting."}, {"title": "6. Conclusion", "content": "This paper explored the impact of shuffled harmful texts and images on MultiModal Large Language Models (MLLMs). Through empirical observation, we found that current MLLMs' defense mechanisms have Shuffle Inconsistency between comprehension ability and safety ability for shuffled harmful instruction. And defense mechanisms had vulnerabilities caused by Shuffle Inconsistency. Based on the above exploration, we proposed a text-image jailbreak attack method named SI-Attack. To fully utilize the Shuffle Inconsistency and overcome the instability, we designed a query-based black-box optimization method based on the feedback of the toxic judge model to further improve the attack's effectiveness. The experiments showed that SI-Attack achieved an obvious improvement in the metric of toxic score and attack success rate for the open-source and closed-source MLLMs. This paper indicated that when safety capabilities did not match excellent comprehension capabilities, the comprehension capabilities instead became a weakness that can be exploited by attackers, which can provide some insights for safety researchers."}]}