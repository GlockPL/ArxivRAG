{"title": "ENAT: Rethinking Spatial-temporal Interactions in Token-based Image Synthesis", "authors": ["Zanlin Ni", "Yulin Wang", "Renping Zhou", "Yizeng Han", "Jiayi Guo", "Zhiyuan Liu", "Yuan Yao", "Gao Huang"], "abstract": "Recently, token-based generation approaches have demonstrated their effectiveness in synthesizing visual content. As a representative example, non-autoregressive Transformers (NATs) can generate decent-quality images in just a few steps. NATS perform generation in a progressive manner, where the latent tokens of a resulting image are incrementally revealed step-by-step. At each step, the unrevealed image regions are padded with [MASK] tokens and inferred by NAT, with the most reliable predictions preserved as newly revealed, visible tokens. In this paper, we delve into understanding the mechanisms behind the effectiveness of NATs and uncover two important interaction patterns that naturally emerge from NAT's paradigm: Spatially (within a step), although [MASK] and visible tokens are processed uniformly by NATs, the interactions between them are highly asymmetric. In specific, [MASK] tokens mainly gather information for decoding. On the contrary, visible tokens tend to primarily provide information, and their deep representations can be built only upon themselves. Temporally (across steps), the interactions between adjacent generation steps mostly concentrate on updating the representations of a few critical tokens, while the computation for the majority of tokens is generally repetitive. Driven by these findings, we propose EfficientNAT (ENAT), a NAT model that explicitly encourages these critical interactions inherent in NATs. At the spatial level, we disentangle the computations of visible and [MASK] tokens by encoding visible tokens independently, while decoding [MASK] tokens conditioned on the fully encoded visible tokens. At the temporal level, we prioritize the computation of the critical tokens at each step, while maximally reusing previously computed token representations to supplement necessary information. ENAT improves the performance of NATs notably with significantly reduced computational cost. Experiments on ImageNet-2562 & 5122 and MS-COCO validate the effectiveness of ENAT. Code and pre-trained models will be released at https://github.com/LeapLabTHU/ENAT.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed an unprecedented growth in the field of AI-generated content (AIGC). In computer vision, diffusion models [10, 59, 61] have emerged as an effective approach. On the contrary, within the context of natural language processing, content is typically synthesized via the generation of discrete tokens using Transformers [72, 19, 5, 55]. Such discrepancy has excited a growing interest in exploring token-based generation paradigms for visual synthesis [7, 85, 33, 87, 6, 35]. Different from diffusion models, these approaches utilize a discrete data format akin to language models. This makes them straightforward to harness well-established language model optimizations such as the refined scaling strategies [5, 54, 31, 73] and the progress in model infrastructure [65, 12, 8, 34, 96]. Moreover, explorations in this field may facilitate the development of more advanced, scalable multimodal models with a unified token space [17, 68, 18, 44, 90] as well as general-purpose vision foundation models that integrate visual understanding and generation capabilities [35, 69].\nThe recent advances in token-based visual generation have seen the rise of non-autoregressive Transformers (NATs) [7, 33, 6, 53], which are distinguished by their abilities to fulfill efficient and high-quality visual synthesis. As shown\u00b3 in Figure 1, NATs follow a progressive generation paradigm: at each generation step, a certain number of latent tokens of the resulting image are decoded in parallel, and the model carries out this process iteratively to produce the final complete token maps. More specifically, at each step, the unknown latent tokens of the image are represented with [MASK] tokens and concatenated with the tokens that have been decoded (i.e., visible tokens). Then, the full set of [MASK] and visible tokens is fed into a Transformer-based model, predicting the proper values of the unknown tokens, with the most reliable predictions preserved as the increments of visible tokens for the next generation step.\nIn this paper, we seek to advance the understanding of the mechanisms behind the effectiveness of NATs' progressive generation procedures. Our investigation uncovers two important findings regarding the spatial and temporal interactions within NATs: Spatially, at each generation step, even though both [MASK] and visible tokens are treated equivalently within the computational graphs of NATs, the visible tokens naturally learn to mainly provide information for [MASK] tokens to infer the unknown image content, and their corresponding deep representations can be built in the absence of [MASK] tokens. Temporally, the interactions between adjacent generation steps mainly concentrate on updating the representations of a small number of \u201ccritical tokens\" on top of the previous steps. In fact, the computation for the remaining majority of tokens is generally repetitive.\nInspired by these findings, we propose to develop novel NAT models to explicitly encourage these critical interaction mechanisms emerged naturally when trained for visual generation, yielding EfficientNAT (ENAT). Specifically, at the spatial level, we disentangle the computations of visible and [MASK] tokens by encoding visible tokens independently of [MASK] tokens. [MASK] tokens are then processed by attending to the fully contextualized features of visible tokens, as shown in Figure 3b. As an interesting observation derived from disentanglement, we find that prioritizing the computation for visible tokens, particularly when the computation is maximized for visible tokens and minimized for [MASK] tokens (even with only a single network layer), further improves the performance of NATs by a large margin. At the temporal level, we concentrate computation on the \"critical tokens\" while maximally reusing the representation of previously computed tokens to supplement necessary information, as illustrated in Figure 4b.\nEmpirically, the effectiveness of ENAT is validated on ImageNet 256\u00d7256 [60], ImageNet 512x512 [60] and MS-COCO [36]. ENAT is able to achieve significantly reduced computational cost compared to conventional NATs while outperforming them notably (e.g., 24% relative improvement with 1.8x lower cost, see Table 6a)."}, {"title": "2 Related Work", "content": "Image tokenizer and token-based image generation models. Language models use algorithms like Byte Pair Encoding or WordPiece to convert text into tokens. Similarly, an image tokenizer transforms images into visual tokens for token-based image generation. Key works in this field include Discrete VAE [58], VQVAE [71], and VQGAN [13], with VQGAN-based tokenizers being most popular for their superior image reconstruction abilities. These tokenizers have enabled the advent of high-performance, scalable token-based generative models [85, 56, 87, 6]. Early token-based models were mainly autoregressive, generating images one token at a time [48, 13, 11, 85]. In contrast, non-autoregressive transformers (NATs)[7, 33, 6, 53] generate multiple tokens simultaneously, speeding up the process while maintaining high image quality. Recently, visual autoregressive models[70] introduced a next-scale prediction strategy, also demonstrating their promise in image synthesis.\nEfficient image synthesis has witnessed significant progress recently. Though the efficiency is-sue is relatively less explored in token-based image synthesis, it has been extensively studied in diffusion-based models. This includes advanced samplers [40, 41, 37], distillation methods [62, 83], quantization and compression techniques [92, 88, 91], and efforts to reduce redundant computa-tion [42, 79, 1]. The last approach bears some resemblance to our computation reuse mechanism in Sec. 4.2, but with notable differences. Firstly, the subjects of research differ: we focus on NAT models. This focus introduces unique properties, e.g., NATs incrementally decode new tokens at specific spatial locations, resulting in feature maps that are only significantly updated in those areas during generation. This contrasts with diffusion models, where feature map similarity between adjacent steps does not follow such predictable spatial patterns; instead, some layers show high overall similarity within a certain range of timesteps, while others may not. Secondly, these characteristic differences lead to distinct methodologies. Diffusion models typically require manually fine-tuned, and some-times layer-specific caching schedules [42] to reuse previously computed features. This process can be labor-intensive and may struggle with generalization. In contrast, our method prioritizes model computation on newly decoded tokens in NATs and reuses the final representations of previously computed tokens without manually fine-tuned caching schedules.\nMasked image modeling (MIM) methods like MAE [29] are widely used for learning image representations by predicting missing patches, with the encoder processing visible tokens and the decoder attending to both visible and masked tokens for reconstruction. CrossMAE [15] extends this by adopting a more disentangled architecture for handling both token types separately. In contrast, our work focuses on image generation, applying masked image modeling in discrete image token space, where token prediction and reconstruction are required at every step. This introduces key differences, such as SC-Attention and computation reuse mechanisms (see Sec. 4) which are not explored in these MIM approaches.\nNon-autoregressive Transformers (NATs) originated in machine translation for their fast inference capabilities [19, 20]. Recently, they have been adapted for image synthesis, enabling efficient high-quality image generation as evidenced by various studies [7, 33, 35, 6, 53, 86]. MaskGIT [7] was the first to show NAT's effectiveness on ImageNet. This approach has been expanded for text-to-image generation, scaling up to 3B parameters in Muse [6] and achieving outstanding performance. Token-critic [33] and MAGE [35] enhance NATs further: Token-critic uses an auxiliary model for guided sampling, while MAGE integrates representation learning with image synthesis using NATs. Recent studies [46, 47] have also explored techniques for further improving the training and inference process of NATs. In contrast to these works, we aim to better understand the mechanisms behind NATs' effectiveness, uncovering findings that naturally lead to a more efficient and effective design for NAT models."}, {"title": "3 Preliminaries of Non-autoregressive Transformers (NATs)", "content": "In this section, we provide an overview of Non-Autoregressive Transformers (NATs) [7, 6, 35] for image generation. NATs operate with a pre-trained VQ-Autoencoder [71, 57, 13], which maps images to discrete visual tokens and reconstructs images from these tokens. The VQ-Autoencoder consists of three components: an encoder $E_{VQ}$, a quantizer Q with a learnable codebook e, and a decoder $D_{VQ}$. The encoder and quantizer transform an image into a sequence of visual tokens:\n$v = Q(E_{VQ}(x))$,\nwhere v = [$v_i$]$_{i=1:N}$ is the sequence of visual tokens, and N is the sequence length. Each token $v_i$ corresponds to a specific entry in the VQ-Autoencoder codebook. The above process is known as tokenization. After tokenization, NATs learn to generate visual tokens in the latent VQ space.\nDuring training, NATs optimize the masked language modeling (MLM) objective [9]. Specifically, a random subset of tokens is replaced with a special [MASK] token, and the model is trained to predict the original tokens based on the unmasked ones. Formally, let M be the mask vector, where $m_i$ = 1 indicates the i-th token is masked. The training objective minimizes the negative log-likelihood of the masked tokens:\n$L_{MLM} = \\sum_{i\\in[1,N],m_i=1}logp(v_i|v_{\\setminus M})$,\nwhere p($v_i|v_{\\setminus M}$) is the predicted probability of token $v_i$ given the unmasked tokens $v_{\\setminus M}$."}, {"title": "4 EfficientNAT (ENAT)", "content": "In this section, we design several analytical experiments (details in Appendix A.1) to advance the understanding of the mechanisms behind the effectiveness of NATs, aiming to accordingly improve the design of NAT models. Specifically, we uncover the critical spatial and temporal interaction patterns that naturally emerge within NATs under the goal of image generation. Inspired by our findings, we further propose to gradually re-design NATs towards maximally exploiting these characteristics."}, {"title": "4.1 Spatial Level Interaction", "content": "Motivation: an ablation study. A notable characteristic of NATs is the concurrent processing and interaction (through attention layers) of visible ([V]) and [MASK] ([M]) tokens when inferring the unknown image content. To better understand this mechanism, we consider an ablation study on four types of spatial interactions: a) [M] to [V] attention, b) [V] to [M] attention, c) [V] to [V] attention, and d) [M] to [M] attention. We find these four types of spatial interactions have significantly different impacts on the generation performance. As shown in Figure 2, the most important spatial interaction is the [M] to [V] attention (i.e., [V]\u2192[M] information propagation), without which the model is unable to converge at all. Moreover, both [M] to [M] and [V] to [V] attentions (i.e., self-attention within the representation-extraction processes of visible and [MASK] tokens, respectively) moderately improve the model. The most intriguing fact is that removing the [V] to [M] attention (i.e., [M]\u2192[V] information propagation) only marginally hurts the model's performance.\nThis imbalanced importance of four spatial interactions highlights the distinct roles of visible and [MASK] tokens. Specifically, the processing of the visible tokens primarily establishes certain internal representations based on the currently available and reliable information, and propagates them to the [MASK] tokens. In fact, their corresponding deep representations can be built mainly on top of themselves. In contrast, [MASK] tokens progressively gather information from visible tokens to predict the proper token values corresponding to the unknown parts of the images. In other words, NATs naturally separate the role of visible and mask tokens when learning to generate images effectively, even though the two types of tokens are designed to be processed equally in NAT models.\nThis phenomenon raises an intriguing question: can we improve NATs by explicitly encouraging the naturally emergent spatial-level token-interaction patterns? Actually, this idea is feasible. For example, we can consider a disentangled architecture that explicitly differentiates the roles of visible and [MASK] tokens. As shown in Figure 3b, we may process visible tokens independently of [MASK] tokens, with the sole purpose of encoding the current visible and reliable information. In contrast, the computation allocated to [MASK] tokens may only focus on predicting unknown image contents correctly with"}, {"title": "4.2 Temporal Level Interaction", "content": "Feature similarity across generation steps. Another critical characteristic of NATs is their incremental revelation of unknown parts of the image upon previous steps. Beyond this straightforward procedure of progressive generation, here we are interested in whether there exist some interpretable temporal interaction patterns in NATs' behaviors. For instance, how do a NAT's computation results at the current step relate to those at the previous step? To investigate this, we conduct a similarity analysis of NATs' output features between two adjacent generation steps.\nIn Figure 5a, we randomly select two generated samples in NATs and visualize their token feature similarity at two adjacent steps (steps 2 & 3 and steps 6 & 7). We compare token-wise similarity and adopt cosine similarity as the metric: Sim($z^{(t-1)}_{ij}$, $z^{(t)}_{ij}$) = $\\frac{z^{(t-1)}_{ij}.z^{(t)}_{ij}}{||z^{(t-1)}_{ij}||||z^{(t)}_{ij}||}$ where $z_{ij}^{(t)}$ denotes the feature of the token at position (i, j) and timestep t. The similarity map exhibits a highly polarized pattern: token representations undergo drastic changes at some \u201ccritical positions\u201d, while other positions remain highly similar between adjacent steps. When comparing with the positions of newly decoded tokens, we find that these \u201ccritical positions\u201d correspond precisely to where the newly decoded tokens are located. In other words, the major significance of each time step lies in updating\nthe representations of newly decoded tokens, while the computation for the remaining majority of tokens is generally repetitive. In Figure 5b, we plot the average token similarity over 50,000 generated samples in each pair of adjacent steps (t = 1\u21922, t = 2 \u2192 3, . . ., t = 7 \u21928). The results show that this temporal interaction pattern remains consistent for different timesteps/samples.\nComputation reuse. Driven by these observations, our key insight is: during the generation of NATs, not all tokens need to be re-computed from scratch at each step. Instead, only the newly decoded tokens need to be re-encoded to inject new knowledge about the image, while the previously encoded information can be maximally reused to supplement necessary details.\nTo implement this idea, we slightly modify the inference process upon our disentangled architecture (Sec. 4.1) by only encoding the newly decoded tokens at each step, while integrating the previously computed features to assist the current step's decoding:\nwithout reuse (Fig. 3b) :  $z$ = Forward($V_{M}$,$v_{M}$)\nwith reuse (Fig. 4b) :   $z$ = Forward($v_\\Delta$, $v_M$, $f(z^{prev})$)"}, {"title": "5 Experiments", "content": "Setups. Following [7, 35, 6], we utilize a pretrained VQGAN [13] with a codebook of size 1024 for image and visual token conversion. We employ three NAT models: ENAT-S (15 encoder layers, 1 decoder layer, 366 embedding dimensions, primarily for ablations), ENAT-B (15 encoder layers, 1 decoder layer, 768 embedding dimensions), and ENAT-L (22 encoder layers, 2 decoder layers, 1024 embedding dimensions). For class-conditional generation, we use adaptive layer normalization [80, 49] for conditioning. For text-to-image generation, we concatenate text embeddings with visual tokens for conditioning. Our training configurations follow [3] with minor adjustments to batch sizes and learning rates to accommodate different model sizes. For system-level comparisons in Sec. 5.1, we measure the TFLOPs of the entire generation process (including the decoder part for latent space generation models) to ensure fair comparisons. All our experiments are conducted with 8 \u00d7 A100 80G GPUs. We generally follow the approach described in [3] with minor modifications. More details on the training and inference setups, and the choice of our baselines can be found in Appendix A.2."}, {"title": "5.1 Main Results", "content": "Class-conditional generation on ImageNet 256\u00d7256 and 512\u00d7512. In Table 3, we compare our approach with other generative models on ImageNet 256\u00d7256. Our ENAT achieves superior performance with significantly lower computational cost. For instance, our ENAT-B model, despite having an extremely low inference cost, attains competitive FID scores of 3.53 in 8 steps. With a slightly increased computational budget, our ENAT-L model achieves a FID of 2.79 with only 0.3 TFLOPs, surpassing leading models with substantially less computational effort. For example, compared to the most performant baseline, i.e., U-ViT-H [3], our ENAT-L model achieves a lower FID score (2.79 vs. 3.37) while requiring 8\u00d7 lower computational cost (0.3 TFLOPs vs. 2.4 TFLOPs). We further evaluate our ENAT on ImageNet 512\u00d7512 in Table 4. Our ENAT-L model also achieves a superior FID of 4.00 with only 1.3 TFLOPs, outperforming leading models with much lower inference cost. Qualitative results of our method are presented in Figure 7 and Appendix B.\nText-to-image generation on MS-COCO. We further assess the efficacy of ENAT for text-to-image generation on MS-COCO [36]. Table 5 shows that ENAT-B surpasses competing baselines with just 0.3 TFLOPs, achieving a FID score of 6.82. Compared to the competitive diffusion model U-ViT [3] with a fast sampler [41], ENAT-B requires similar computational resources to its 4-step variant while significantly outperforming it (6.82 vs. 16.20), and it also surpasses the 8-step sampling results of U-ViT with lower computational costs.\nPractical efficiency. We provide more comprehensive comparisons of the trade-off between generation quality and computational cost in Figure 6. Both theoretical TFLOPs and the practical GPU/CPU latency for generating an image are reported. Our results show that ENAT consistently outperforms other baselines in terms of both generation quality and computational cost."}, {"title": "5.2 Ablation Studies", "content": "In this section, we present additional ablation studies on Imagenet 256\u00d7256 to validate the effective-ness of our proposed mechanisms. We use ENAT-S with 8 generation steps as our default setting, and report the FID score as well as the computational cost in GFLOPs for each NAT model.\nMain ablation. Disentangled architecture and computation reuse are the two fundamental mecha-nisms in ENAT. The former separates the processing of visible and [MASK] tokens, and prioritizes computation on visible ones, while the latter eliminates repetitive processing of non-critical tokens. In Table 6a, we demonstrate the effectiveness of these two mechanisms. The results show that the disentangled architecture significantly improves NAT's performance, with a 1.76 improvement in FID score at a similar computational cost. Computation reuse, on the other hand, significantly reduces computational cost (1.8\u00d7 fewer GFLOPs) while preserving most of the gains from disentanglement.\nEffectiveness of SC-Attention. The SC-Attention mechanism adopted in our work serves dual roles: handling interactions of input tokens while simultaneously incorporating necessary additional information. Theoretically, the same functionality can be achieved with a stack of one self-attention layer and one cross-attention layer. However, as shown in Table 6b, SC-Attention outperforms the stack of self-attention and cross-attention layers with a lower FID (4.97 vs. 5.85) and a lower computational cost (22.6 vs. 25.0), demonstrating its effectiveness in our ENAT model.\nEffectiveness of reuse projection module. In our computation reuse mechanism, a lightweight reuse projection module first processes the previous feature before integrating it into the current generation step. As shown in Table 6c, this design is highly important to our reuse mechanism. Without this module, the FID is 5.96, which is much worse than the 4.78 FID achieved without reuse. An intuitive explanation is that the reuse projection module learns the minimal necessary updates for the features of non-crucial tokens, preventing them from becoming too stale for more distant subsequent steps.\nWhich token features to reuse? Our basic reuse formulation integrates all previous token features into the current step. However, as shown in Table 6d, reusing only visible token features is equally effective while being much more efficient. As discussed in Section 4.1, encoding visible tokens"}, {"title": "6 Conclusion", "content": "In this paper, we explored the underlying mechanisms of non-autoregressive Transformers (NATs) and uncovered key spatial and temporal token interaction patterns exist within NATs. Our findings highlight that spatially, visible tokens primarily provide information for [MASK] tokens, while temporally, updating the representations of newly decoded tokens is the main focus across generation steps. Driven by these findings, we propose ENAT, a NAT model that explicitly encourages these critical interactions. We spatially disentangle the computations of visible and [MASK] tokens by independently encoding visible tokens and conditioning [MASK] tokens on fully encoded visible tokens. Temporally, we focus computation on newly decoded tokens at each step, while reusing previously computed representations to facilitate decoding. Experiments on ImageNet and MS-COCO demonstrate that ENAT enhances NATs' performance with significantly reduced computational cost."}, {"title": "A Implementation Details", "content": "A.1 Detailed model configurations.\nHere we present the detailed configurations of all our NAT models appeared within this paper in Table 7. We provide the number of encoder layers (NE), decoder layers (ND), the dimension of the hidden states (embed dim.), the number of attention heads (# attn. heads):\nA.2 Details of training and evaluation.\nFor ImageNet 256x256, we use a batch size of 2048 and a learning rate of 4e-4. For ImageNet 512\u00d7512, to manage the increased sequence length, we reduce the batch size to 512 and linearly scale down the learning rate to le-4. For MS-COCO, we train for 150k steps instead of the 1000k steps used in [3].\nFor our ablation studies in Sec. 5.2 and explorative experiments in Sec. 4, we train the models for 300k steps instead of the 500k steps used in [3], while keeping the other settings the same as above.\nFor data preprocessing, we perform center cropping and resizing to 256\u00d7256 for ImageNet 256\u00d7256 and MS-COCO, and to 512\u00d7512 for ImageNet 512\u00d7512. Additionally, we adopt random horizontal flipping as data augmentation, following [3, 49].\nOur evaluation on FID follows the same evaluation protocol as [10, 3, 49]. We adopt the pre-computed dataset statistics from [3] and generate 50k samples for ImageNet (30k for MS-COCO) to compute the statistics for the generated samples, using the following formula to calculate FID [30]:\nFID = $||\u00b5_{real} - \u03bc_{fake}||^2 + Tr(\u03a3_{real} + \u03a3_{fake} \u2013 2(\u03a3_{real}\u03a3_{fake})^{1/2})$,\nwhere \u03bc and \u2211 are the mean and covariance of the real and fake samples, respectively. The evaluation on Inception Score (IS) follows the same protocol as [3, 49], using a pre-trained Inception V3 model [66] to compute the IS."}, {"title": "C Limitations and Future Work", "content": "Although our experiments have covered two fundamental types of generative models, namely class-conditional and text-to-image generation, and utilized three datasets, investigating the efficacy of ENAT on more diverse datasets, such as the widely used CelebA [39] and LSUN [84], and exploring additional generation types like unconditional generation, constitute valuable directions for future research. Moreover, scalability, both in terms of model size and dataset volume, is a crucial capability for current generative models. Our largest model scales up to approximately 0.6 billion parameters, and our experiments utilized datasets with a maximum size of 1.2 million images (ImageNet dataset). Evaluating the performance of ENAT on even larger-scale datasets, such as LAION-5B [64], and further scaling the model to surpass 1 billion parameters, could provide deeper insights into its scalability and robustness.\nTo further enhance the applicability and efficiency of non-autoregressive Transformers, integrating other adaptive inference methods [76, 75, 26, 95] and learning techniques [67, 82, 77] will be essential. For instance, methods like dynamic neural network [74, 27, 89, 93, 94, 52] and resolution-adaptive models [81] offer promising pathways to explore. Additionally, examining ENAT across"}, {"title": "D Broader Impacts", "content": "On the positive side, the proposed EfficientNAT (ENAT) models significantly reduce computational costs, making advanced visual generation technology more accessible. This democratization can benefit diverse sectors, including education, healthcare, and creative industries. However, as with any AI-generated content technology, there are potential ethical considerations such as creating misleading content or spreading misinformation. Additionally, like other data-driven approaches, the model may inadvertently reinforce biases present in the training data. Possible mitigation strategies for these concerns include developing robust detection methods for generated content, promoting transparency in AI-generated content, and ensuring diverse and representative training data."}, {"title": "E Licenses", "content": "The Table 8 outlines the assets used in our work, their sources and licenses. Our models, data and code will be open-sourced under the MIT License upon paper acceptance."}]}