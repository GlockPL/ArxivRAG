{"title": "Digital Avatars: Framework Development and Their Evaluation", "authors": ["Timothy Rupprecht", "Sung-En Chang", "Yushu Wu", "Lei Lu", "Enfu Nan", "Chih-hsiang Li", "Caiyue Lai", "Zhimin Li", "Zhijun Hu", "Yumei He", "David Kaeli", "Yanzhi Wang"], "abstract": "We present a novel prompting strategy for artificial\nintelligence driven digital avatars. To better quan-\ntify how our prompting strategy affects anthropo-\nmorphic features like humor, authenticity, and fa-\nvorability we present Crowd Vote an adaptation\nof Crowd Score that allows for judges to elect a\nlarge language model (LLM) candidate over com-\npetitors answering the same or similar prompts. To\nvisualize the responses of our LLM, and the ef-\nfectiveness of our prompting strategy we propose\nan end-to-end framework for creating high-fidelity\nartificial intelligence (AI) driven digital avatars.\nThis pipeline effectively captures an individual's\nessence for interaction and our streaming algo-\nrithm delivers a high-quality digital avatar with\nreal-time audio-video streaming from server to mo-\nbile device. Both our visualization tool, and our\nCrowd Vote metrics demonstrate our AI driven dig-\nital avatars have state-of-the-art humor, authentic-\nity, and favorability outperforming all competitors\nand baselines. In the case of our Donald Trump\nand Joe Biden avatars, their authenticity and favor-\nability are rated higher than even their real-world\nequivalents.", "sections": [{"title": "Introduction", "content": "The performance of LLMs in role-playing has attracted sig-\nnificant attention, with platforms like Character.ai providing\nvirtual character role-playing options. Current platforms are\nconfined to text interactions, and responses in virtual char-\nacter role-playing still fall short of authentically portraying\nintended real-world personas. Our paper introduces a com-\nprehensive prompting strategy to increase humor, authentic-\nity and favorability in LLM responses. We call it show don't\ntell prompting for digital avatars. As part of an initial start-\ning prompt, we define an avatar's background and provide\nfew-shot examples for relevant scenarios, creating response\ntemplates emphasizing language nuances and emotional tone.\nLike other state-of-the-art prompting strategies, our strat-\negy only relies on this initial prompt [Wei et al., 2023;\nTouvron et al., 2023]."}, {"title": "Approach", "content": ""}, {"title": "Prompt Mechanism", "content": "In this section, we propose the novel prompting strategy\nshow don't tell for digital avatars. Our strategy is similar\nto other few-shot LLM prompt strategies [Wei et al., 2023;\nTouvron et al., 2023]. Rather than solely giving various\ninstructions (zero-shot learning), we give examples for the\nLLM to learn from directly (few-shot learning). In our ini-\ntial prompt, we provide a large number of examples and\nwe briefly define the avatar's role. We record in this initial\nprompt as many characteristic responses of the real-world\npersona as possible in various situations, such as reactions\nwhen challenged by reporters, or responses to pointed ques-\ntions. Finally, to make the responses more lively and interest-\ning, we incorporate elements of humor and entertainment into\nour final version of the prompts. For example, if an avatar's\nreal-world persona appeared on a comedic show, we would\nsample jokes from their act hoping that a parody of the real-life persona distills relevant humor into the avatar persona."}, {"title": "An End-to-End Digital Avatar Framework", "content": "We develop an end-to-end Al pipeline to demonstrate the\nprompting strategy developed in this work. Using local im-\nplementations of various states of the art, we find the same\nqualitative trends regardless of baseline architecture. We be-\nlieve we are the first to use an end-to-end AI driven digital\navatar pipeline that includes a large language model for avatar\nspeaking. This framework is presented in full in Figure 1."}, {"title": "Speech-to-Text and Text-to-Speech", "content": "Speech-to-text (STT) is the process of converting spoken lan-\nguage into text. The states of the art demonstrate impressive\nresults in English and Mandarin [Amodei et al., 2015] and\ncan be robust to noisy environments [Radford et al., 2022].\nText-to-Speech (TTS) converts written text into spoken words\nusing synthesized voice. WaveNet [van den Oord et al.,\n2016] is a deep learning approach that improved the natu-\nralness of synthesized speech. Recently, FastSpeech [Ren et\nal., 2019], presented a novel network-based TTS model that\nmaintained high-quality audio output efficiently."}, {"title": "Talking Face Synthesis", "content": "Talking Face Synthesis (TFS) aims to generate talking heads\nthat are synchronized with input audio. Prior works [Prajwal\net al., 2020; Cheng et al., 2022] adopt pre-trained experts\nin lip-audio synchronization to guide the training of Genera-\ntive Adversarial Networks for achieving highly synchronized\nand high fidelity videos. These works suffer from unpleasant\nframe continuity and a lack of high-texture information. We\npropose a codebook-based TFS method that generates talking\nfaces from both low and high semantic levels incorporated\nwith audio and reference images. Our method takes advan-\ntage of the strong pre-trained codebook decoder to ensure a\nrich-textured and high-fidelity face generation."}, {"title": "Video Selection Algorithm", "content": "Depending on the end goals of our video demo we can take\nvideo data from different camera perspectives. For example,\nif we don't care about camera transitions, we use one cam-\nera angle for thinking and another for listening to the user.\nWhen processing is complete we cut to the second camera\nangle paired with lip synced avatar output. We also can pro-\nvide seamless transitions between listening, thinking of a re-\nsponse, and answering. By labelling every point in a sample\nvideo when the real-world persona transitions from listening\nto talking, we can start our demos at these labelled positions.\nWe can play the video in reverse to create listening data as\na user asks a question. Then during processing we play the\nvideo sample forward and if necessary use slow motion and\nframe skipping to sync the end of processing to the start of\nthe avatar answer where the real-world persona in the video\nsample begins talking."}, {"title": "Evaluation Metrics", "content": ""}, {"title": "Crowd Score", "content": "Crowd Score [Goes et al., 2022] introduced a novel method\nfor evaluating humor by utilizing an LLM to serve as judges,\noffering a unique approach to gauge humor effectiveness.\nCrowd Score as it was originally implemented ranks a series\nof jokes from least to most humorous without much regard for\nwhere these jokes come from - whether a single LLM or mul-\ntiple instances. Crowd Score used judging personalities that\nreflect preferences for types of humor including: \u201cafilliative\u201d,\n\"self-enhancing\u201d, \u201caggressive\u201d, and finally \u201cself-defeating.\""}, {"title": "Introducing Crowd Vote", "content": "Our Crowd Score, that we refer to as Crowd Vote, asks judges\n[OpenAI, 2023] with different assigned personalities to pick\na single response from many LLM candidates responding to\nthe same prompt that best matches a condition like authentic-\nity, favorability, or humor. A tally is kept for each candidate"}, {"title": "Experiments", "content": "The candidates: To judge our prompting strategy relative to a\nbaseline LLM on humor and interest we include three candi-\ndates in our evaluation: a) Baseline LLM (zero-shot) b) Char-\nacter.ai (few-shot) c) Our LLM with show don't tell prompt-\ning (few-shot) and d) (when possible) the real-world avatar\ntarget. Please note that character.ai is a notable platform in the\nfield of AI-driven digital avatars with over 20 million users,\nmaking it a very strong baseline."}, {"title": "Interest and Humor", "content": "We use our implementation of Crowd Vote to judge our avatar\nresponses. You can see the results of our experiment in our\ndemo slides. The vote percentages of the judges are 58.3%\nin favor of our show don't tell prompted LLM being most hu-\nmorous, followed by abstaining judges at 20.8%, and then the\nbaseline LLM, and Character.ai. We show the quality com-\nparison of the response between a baseline LLM, Charater.ai,\nand our own work. Table 1 shows that even though the base-line LLM is powerful, without any few-shot examples in an\ninitial prompt it often fails to display any sense of humor in its\nresponses. Character.ai provides a straightforward and some-\nwhat mundane response, depicting a simple loss of sunglasses\ndue to a bike accident. While this response might be relat-\nable, it lacks imaginative and humorous elements. Our LLM\nresponse is playful and self-deprecating, creating a humorous\nscenario where the sunglasses are misplaced in a common yet\namusing way on top of the head. This response also clev-\nerly ties in the ideas of misplacing one's sense of humor and\nforgetfulness, adding additional layers of wit. The invitation\nto go on a sunglass hunt adds an engaging and interactive el-\nement to the answer, increasing its charm."}, {"title": "Authenticity and Favorability", "content": "To compare an avatar to their human counterpart one needs a\nfair comparison between avatar answers and real-world per-son answers. In the first Presidential Debate of the 2020 cam-\npaign season we were able to find 17 questions resulting in\nresponses that could be compared between a Trump avatar or\nBiden avatar and all their respective competitors. We publish\nthis small dataset with our code. Once we had a dataset that\ncomprised real world Donald Trump and Joe Biden answers\non a variety of topics, we asked our LLM candidates the same\nquestions and we used Crowd Vote to compare our avatar\nanswers to the real-world Donald Trump and real-world Joe\nBiden on metrics like authenticity and favorability.\nThe authenticity and favorability experiments used differ-ent Crowd Vote judges to vote on all the candidates' answers\nto the 2020 debate questions. For authenticity we asked \u201ca py-chologist\", \"a political commentator\u201d, \u201can American voter\u201d,\n\"a close family member of the avatar\" and the avatar's adver-sary. For our tests on favorability our judges span the typi-cal American political spectrum: \u201cfar-right\u201d, \u201cconservative\u201d,\n\"centrist\", \"liberal\", and finally \"far-left\". Figure 2 shows\nthat in both tests our LLM initialized with our show don't\ntell prompting strategy out performs the popular character.ai,\nas well as the baseline LLM intitialized without show don't\ntell. Our Trump and Biden avatars actually outperform each\nof their real-world counterparts both in terms of authentic-ity and favorability. Both of our avatars won easily against\nall other LLM candidates and real-world personas and subse-quently advanced to a \"general election\" that we hosted. Our\navatar of Joe Biden won against our avatar of Donald Trump:\n45% to 37.5% with 17.5% abstaining.\""}, {"title": "Conclusion", "content": "Our prompting strategy shows both qualitatively and quan-titatively impressive results. To measure this, we proposed\nCrowd Vote adapted from Crowd Score. With Crowd Vote\nwe demonstrate that our show don't tell prompting strategy\ngenerates avatar responses that out compete all competitors\nincluding the real-world personas they represent in terms of\nauthenticity and favorability. To further demonstrate the an-thropomorphic nature of our responses, we developed an end-to-end pipeline agnostic to specific architecture that renders\nhigh quality avatar interactions."}]}