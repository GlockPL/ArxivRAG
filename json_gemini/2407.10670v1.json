{"title": "Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems", "authors": ["Yunxiao Shi", "Xing Zi", "Zijing Shi", "Haimin Zhang", "Qiang Wu", "Min Xu"], "abstract": "Retrieval-augmented generation (RAG) techniques leverage the in-context learning capabilities of large language models (LLMs) to produce more accurate and relevant responses. Originating from the simple 'retrieve-then-read' approach, the RAG framework has evolved into a highly flexible and modular paradigm. A critical component, the Query Rewriter module, enhances knowledge retrieval by generating a search-friendly query. This method aligns input questions more closely with the knowledge base. Our research identifies opportunities to enhance the Query Rewriter module to Query Rewriter+ by generating multiple queries to overcome the Information Plateaus associated with a single query and by rewriting questions to eliminate Ambiguity, thereby clarifying the underlying intent. We also find that current RAG systems exhibit issues with Irrelevant Knowledge; to overcome this, we propose the Knowledge Filter. These two modules are both based on the instructional-tuned Gemma-2B model, which together enhance response quality. The final identified issue is Redundant Retrieval; we introduce the Memory Knowledge Reservoir and the Retriever Trigger to solve this. The former supports the dynamic expansion of the RAG system's knowledge base in a parameter-free manner, while the latter optimizes the cost for accessing external knowledge, thereby improving resource utilization and response efficiency. These four RAG modules synergistically improve the response quality and efficiency of the RAG system. The effectiveness of these modules has been validated through experiments and ablation studies across six common QA datasets.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) represent a significant leap in artificial intelligence, with breakthroughs in generalization and adaptability across diverse tasks [4, 6]. However, challenges such as hallucinations [32], temporal misalignments [27], context processing issues [1], and fine-tuning inefficiencies [8] have raised significant concerns about their reliability. In response, recent research has focused on enhancing LLMs' capabilities by integrating them with external knowledge sources through Retrieval-Augmented Generation (RAG) [2, 20, 13, 15]. This approach significantly improves LLMs' ability to answer questions more accurately and contextually.\n The basic RAG system comprises a knowledge retrieval module and a read module, forming the retrieve-then-read pipeline [20, 15, 13]. However, this vanilla pipeline has low retrieval quality and produces unreliable answers. To transcend this, more advanced RAG modules have been developed and integrated into the basic pipeline. For example, the Query Rewriter module acts as a bridge between the input question and the retrieval module. Instead of directly using the original question as the query text, it generates a new query that better facilitates the retrieval of relevant information. This enhancement forms the Rewrite-Retrieve-Read pipeline [23, 22]. Furthermore, models like RETA-LLM [22] and RARR [10] integrate a post-reading and fact-checking component to further solidify the reliability of responses. Additional auxiliary modules such as the query router [21] and the resource ranker \u00b9 [14] have also been proposed to be integrated into the RAG's framework to improve the practicality in complex application scenario. This integration of various modules into the RAG pipeline leading to the emergence of a modular RAG paradigm [11], transforming the RAG framework into a highly flexible system.\n Despite significant advancements, several unresolved deficiencies persist in practical applications. In the Query Rewriter module, the reliance on generating a single query for retrieval leads to (1) Information Plateau, as this unidirectional search method limits the scope of retrievable information. Besides, the frequent misalignment between the input question and the underlying inquiry intent often exacerbated by (2) ambiguous phrasing, significantly impedes the LLM's accurately interpreting users' demand. Furthermore, while the Query Rewriter can facilitate the retrieval of relevant information, it cannot guarantee the accuracy of the retrieved information. The extensive retrieval process may also acquire (3) irrelevant knowledge, which detracts from the response quality by introducing noise into the context. At last, we have identified a phenomenon of (4) redundant retrieval, where users pose questions similar to previous inquiries, causing the RAG system to fetch the same external information repeatedly. This redundancy severely compromises the efficiency of the RAG system.\n These deficiencies underscore the potential for enhancing the accuracy and efficiency of existing RAG framework, thereby guiding our investigative efforts to address these issues. We decompose the Query Rewriter module's functionality into two subtasks, resulting in the upgraded module Query Rewriter+. The first subtask involves"}, {"title": "2 Motivation", "content": "In this section, we empirically investigate several preliminary studies to highlight the limitations of current RAG systems in Open-Domain QA tasks.\n\u2022 PS 1: We investigate the maximum amount of relevant information that can be retrieved by converting input text into a single search-friendly query.\n\u2022 PS 2: We examine whether using multiple queries that focus on different detailed semantic aspects can retrieve more relevant information than a single query.\n\u2022 PS 3: We analyze how the proportion of irrelevant information changes as the volume of retrieved data increases.\n\u2022 PS 4: We assess whether clarifying the input question is unnecessary for LLMs with strong semantic understanding capabilities.\n Experimental Settings. The experiment is conducted using a randomly selected subset of 50 questions from each of the following datasets: PopQA [24], 2WikiMQA [30], HotpotQA [31], and CAmbigNQ [19]. We employ the Rewrite-Retrieve-Read RAG pipeline, and the rewriter follows the LLM-based method, specifically utilizes GPT-3.5-turbo-0613, more details are described in previous study [23]. The rewriter module is designed to generate one to three variable-length queries for each question, depending on the question's complexity. For retrieval, we use Bing Search V7 to identify the top 10 most relevant webpage snippets for each query. These snippets encapsulate the most query-related content from each webpage. For each question, a collection of retrieved snippets serves as the external knowledge to facilitate the LLM's in-context learning for generating response. We create two ways to present the snippets: Sequential Order, with snippets for each query following one another, and Mix Order, with top snippets evenly sampled from different queries. We increase the number of snippets to see how retrieval performance changes in two different snippet arrangements. The evaluation metrics are Answer Recall, which is the ratio of answer items found in the external knowledge to the total number of answer items, and Snippet Precision, which is the ratio of snippets containing any answer item to the total number of snippets used. These metrics provide a quantitative measure of retrieval performance.\n We conduct another experiment using 50 questions from the CAmbigNQ dataset. We obtain responses by directly inputting the original questions into the LLM, labeled as org. Another set of responses, labeled as rewrt, is generated by inputting rewritten questions into the LLM. The accuracy of these two sets of responses is quantified using the metrics: EM (Exact Match), Precision, Recall, and F1 Score. Additionally, we also use retrieval-enhanced method to generate answers again. These responses are labeled as org_rag for original questions and rewrt_rag for rewritten questions. A comparative analysis of the answer accuracy is conducted in the same manner.\n PS1: The Information Plateaus of Single Query. From the analysis of Answer Recall in sequential order, it is observed that as the number of snippets increases, the Answer Recall metric improves, but they commonly plateauing before reaching 10 and 20 snippets (red dashed line). This phenomenon indicates there exist an upper limit to the retrievable useful information of a single query. This suggests there is a threshold beyond which additional information retrieved by the same query does not contribute too much to better retrieval quality.\n PS2: The Effect of Using Multiple Queries. The analysis of Answer Recall in Sequence Order indicates that introducing fresh snippets from new queries effectively mitigates plateauing in Answer Recall (light purple solid line). Additionally, the Answer Recall in Mixed Order consistently outperforms that in Sequence Order at the same snippet number (purple solid line), particularly when the number has not yet reached the maximum retrievable information limit (30 snippets). This underscores the significance of multiple queries in enhancing retrieval quality.\n PS3: The Low Relevance of Retrieved Information. As shown in Figure 2, Snippet Precision notably decreases as the number of snippets increases, eventually stabilizing. This suggests a significant presence of retrieved external knowledge snippets that do not contain relevant answer information.\n PS4: The Effect of Rewriting Questions. Figure 3 presents a bar graph comparing various metrics for original and rewritten questions in the CAmbigNQ dataset. Rewriting questions improves Exact Match (EM), Precision, and F1 Score-whether or not retrieval augmentation technique is used. However, the recall decreases with rewritten questions. This occurs because the CAmbigNQ dataset labels include all possible answers, and LLMs tend to provide all possible responses to vague questions. The Rewritten questions are more well-intended, prompting LLMs to generate specific answers.\nSummary. Based on above results from a series of experiments on"}, {"title": "3 Methodology", "content": "The design of the Question Rewriter+ module encompasses two primary functions: (1) enhancing the original question semantically into a rewritten question, and (2) generating multiple search-friendly queries. Formally, the Question Rewriter+ is denoted as \\(G_{\\Theta}\\)(.), which takes an original question p as input:\n\\[G_{\\Theta}(p) \\rightarrow (s, Q)\\]\nwhere s represents the rewritten question and \\(Q = \\{q_1, q_2, ..., q_Q\\}\\) is the set of generated queries. A basic implementation of \\(G_{\\Theta}\\)(.) can adopt a prompt-based strategy, utilizing task descriptions, the original question, and exemplars to prompt black-box large language models. This approach capitalizes on the model's in-context learning capabilities, often yields effectiveness of question rewriting and query generation. Nevertheless, the effectiveness of this methodology is highly dependent on the meticulous construction of prompts tailored to specific domain datasets, which limits its general utility. Besides, the generated s and may be of low quality, failing to enhance RAG performance.\n To address these limitations, we propose a more general and task-specific approach. This involves parameter-efficient LoRA [12] fine-tuning of the Gemma-2B model for \\(G_{\\Theta}\\)(.), utilizing a high-quality dataset that is semi-automatically constructed through LLMs' generation and human's quality validation. This dataset comprises instances (p, s, Q), each rigorously validated to ensure that responses derived from s are more accurate in hitting the labeled answer compared to those obtained by directly asking the LLM with p. Additionally, we manually verify the quality of generated queries Q to ensure the reliability. The prompt template for generating (s, Q) is as follows:\n[Instruction]: Your task is to transform a potentially colloquial or jargon-heavy [Original Question] into a semantically enhanced Rewritten Question with a clear intention. Additionally, generating several search-friendly Queries that can help find relevant information for answering the question. You can consider the provided [Examples] and response following the [Format].\n[Original Question]: {User's original question is here.}\n[Examples]: {The examples should be specially tailored for different datasets.}\n[Format]: {The generated Rewritten Question is here}**{query1}**{query2}**{query3}...\n The accuracy of responses generated by LLMs can be significantly compromised by noisy retrieved contexts [34]. To mitigate this, we introduce the Knowledge Filter module, designed to enhance response accuracy and robustness. This module utilizes LLMs to filter"}, {"title": "3.1 Question Rewriter+", "content": "The design of the Question Rewriter+ module encompasses two primary functions: (1) enhancing the original question semantically into a rewritten question, and (2) generating multiple search-friendly queries. Formally, the Question Rewriter+ is denoted as \\(G_{\\Theta}\\)(.), which takes an original question p as input:\n\\[G_{\\Theta}(p) \\rightarrow (s, Q)\\]\nwhere s represents the rewritten question and \\(Q = \\{q_1, q_2, ..., q_Q\\}\\) is the set of generated queries. A basic implementation of \\(G_{\\Theta}\\)(.) can adopt a prompt-based strategy, utilizing task descriptions, the original question, and exemplars to prompt black-box large language models. This approach capitalizes on the model's in-context learning capabilities, often yields effectiveness of question rewriting and query generation. Nevertheless, the effectiveness of this methodology is highly dependent on the meticulous construction of prompts tailored to specific domain datasets, which limits its general utility. Besides, the generated s and may be of low quality, failing to enhance RAG performance.\n To address these limitations, we propose a more general and task-specific approach. This involves parameter-efficient LoRA [12] fine-tuning of the Gemma-2B model for \\(G_{\\Theta}\\)(.), utilizing a high-quality dataset that is semi-automatically constructed through LLMs' generation and human's quality validation. This dataset comprises instances (p, s, Q), each rigorously validated to ensure that responses derived from s are more accurate in hitting the labeled answer compared to those obtained by directly asking the LLM with p. Additionally, we manually verify the quality of generated queries Q to ensure the reliability. The prompt template for generating (s, Q) is as follows:\n[Instruction]: Your task is to transform a potentially colloquial or jargon-heavy [Original Question] into a semantically enhanced Rewritten Question with a clear intention. Additionally, generating several search-friendly Queries that can help find relevant information for answering the question. You can consider the provided [Examples] and response following the [Format].\n[Original Question]: {User's original question is here.}\n[Examples]: {The examples should be specially tailored for different datasets.}\n[Format]: {The generated Rewritten Question is here}**{query1}**{query2}**{query3}..."}, {"title": "3.2 Knowledge Filter", "content": "The accuracy of responses generated by LLMs can be significantly compromised by noisy retrieved contexts [34]. To mitigate this, we introduce the Knowledge Filter module, designed to enhance response accuracy and robustness. This module utilizes LLMs to filter out irrelevant knowledge. Rather than directly querying an LLM to identify noise, we incorporate a Natural Language Inference (NLI) framework [3] for this purpose. Specifically, for a rewritten question s and retrieved knowledge k, the NLI task evaluates whether the knowledge (as the premise) contains reliable answers, or useful information aiding the response to the question (as the hypothesis). This results in a judgment j categorized as entailment, contradiction, or neutral. The operation of the Knowledge Filter can be mathematically represented as:\n\\[F_{\\Theta}(s, k) \\rightarrow j \\in \\{entailment, contradiction, neutral\\}\\]\nKnowledge is retained if the NLI result is classified as entailment. We can adjust the strength of the hypothesis based on the specific dataset. For single-hop questions, a stronger hypothesis can be set, requiring the knowledge to contain direct and explicit answer information. Conversely, for more complex multi-hop questions, we can set a weaker hypothesis, only requiring the knowledge to include information that possibly aids in answering the question. When valid knowledge is unavailable, a back-off strategy is invoked, where LLMs generate responses without the aid of external knowledge augmentation. The Knowledge Filter also employs the LoRA fine-tuning method [12] on the Gemma-2B model, offering enhanced applicability and adaptability compared to prompt-based approaches.\n The NLI training dataset is constructed semi-automatically using the similar method we described in Section 3.1. We provide task instruction, rewritten question s, along with knowledge context k as prompt to GPT-4, which then generated a brief explanation e and a classification result j, resulting in the data instance ((s, k, (e, j)). The prompt template is as follows:\n[Instruction]: Your task is to solve the NLI problem: given the premise in [Knowledge] and the hypothesis that \"The [Knowledge] contains reliable answers aiding the response to [Question]\". You should classify the response as entailment, contradiction, or neutral.\n[Question]: {Question is here.}\n[Knowledge]: {The judging knowledge is here.}\n[Format]: {The explanation.}**{The NLI result.}\n Considering that LLMs are primarily designed for text regression rather than classification, using only j as a label for instructional tuning for Gemma-2B would prevent the LLM from accurately performing classification tasks in a generative manner. Therefore, we also incorporate the concise explanation e as part of the label, in addition to the NLI classification result j."}, {"title": "3.3 Memory Knowledge Reservoir", "content": "We present a Memory Knowledge Reservoir module designed to cache the retrieved knowledge. The knowledge is structured as title-content pairs, where the title serves as a brief summary and the content offers detailed context. The Memory Knowledge Reservoir updates by adding new title-content pairs and replacing older entries with newer ones for the same titles. Mathematically, the Memory Knowledge Reservoir can be represented as a set \\(K = \\{k_1, k_2, ..., k_{|K|}\\}\\), where each ki is a title-content pair."}, {"title": "3.4 Retrieval Trigger", "content": "This module assesses when to engage external knowledge retrieval. A calibration-based method is utilized, wherein the popularity serves as a metric to estimate a RAG system's proficiency with the related knowledge.\n\\(K = \\{k_1, k_2, ..., k_{|K|}\\}\\) is the set of knowledge in the Memory Knowledge Reservoir, and \\(q_i \\in Q\\) is a generated query. The cosine similarity between query \\(q_i\\) and a knowledge instance \\(k_j \\in K\\) is denoted by \\(S(q_i, title(k_j))\\). The popularity of query \\(q_i\\), denoted by Pop(\\(q_i\\)), is defined as:\n\\[Pop(q_i) = |\\{k_j \\in K \\; | \\; S(q_i, title(k_j)) \\geq \\Tau\\}\\| \\]\nwhere \\(\\Tau\\) is a similarity threshold, |.| indicates the cardinality of the set. The boundary conditions for a query being within or outside the knowledge of the RAG system are established using a popularity threshold \\(\\theta\\). A query qi is considered to be within the knowledge boundary if:\n\\[Pop(q_i) \\geq \\theta\\]\nConversely, a query \\(q_i\\) is outside the knowledge boundary if:\n\\[Pop(q_i) < \\theta\\]"}, {"title": "4 Experiments", "content": "We assess performance using the F1 Score and Hit Rate metrics. Due to the discrepancy between the verbose outputs of LLMs and the concise format of the dataset answers, we chose not to utilize the Exact Match (EM) metric. Instead, we considered a response as correct if the text hit any item of the labeled answers.\nRewrite-Retrieve-Read [23] represents the current state-of-the-art improvement on the basic Retrieve-then-Read RAG pipeline. Our approach enhances the existing RAG pipeline by augmenting the Query Rewriter to Query Rewriter+ and introducing a new Knowledge Filter module. To highlight the effectiveness of our method, we implemented the following configurations:\n(i) Direct: Ask the LLM directly with the original question.\n(ii) Rewriter-Retriever-Reader: Prior to retrieval, a Query Rewriter module is employed to generate a query that fetches external knowledge. The external knowledge, along with the original question, is used to prompt the response generation.\n(iii) Rewriter+-Retriever-Reader: Prior to retrieval, the Enhanced Query Rewriter module is utilized, generating multiple queries to acquire external knowledge and clarify the original question. Responses are generated using both the rewritten question and all retrieved external knowledge.\n(iv) Rewriter+-Retriever-Filter-Reader: Applied before retrieval, the Enhanced Query Rewriter module generates multiple queries and clarifies the original question. A Knowledge Filter is used to discard external knowledge unrelated to the rewritten question. The final response is then generated using the filtered external knowledge and the rewritten question.\nComparing the Rewriter+-Retriever-Reader setup with the Rewriter-Retriever-Reader setup validates the superiority of the proposed Question Rewriter+ module. Additionally, comparing the Rewriter+-Retriever-Filter-Reader setup with the Rewriter+-Retriever-Reader setup demonstrates the effectiveness of the 4-step RAG pipeline incorporating the Knowledge Filter."}, {"title": "4.1 Modular Setting", "content": "Fine-tuning Gemma-2B We follow the Alpaca's training method\u00b2, employing the LoRa[12] method to instruction-tune the pre-trained Gemma-2B model\u00b3 for the Question Rewriter+ and Knowledge Filter modules. We set the learning rate to 1e-4, batch size to 8, and epochs to 6. We set the rank of the LoRa low-rank matrix to 8, and the scaling factor, alpha to 16. Additionally, we utilize the 4-bit quantization method with NF4 quantization type [7]. The training and inference process are all conducted on a single Nvidia Quadro RTX 6000.\nKnowledge Retriever We utilize the Bing Search Engine v7 as the information retrieval method. For each query q, we select the top-n items from the search results, and each item is regarded as a knowledge instance. We utilize the snippet of a search item as the content of a knowledge instance. The hyperparameter n is predetermined at 10.\nLLM Reader We primarily used GPT-3.5-turbo-0613 as the black-box LLM model for generating answers. The prompt structure includes task instruction, question, external knowledge, examples, and response format. The external knowledge section comprises up to 30 knowledge instances arranged in mixed order, as discussed in Section 2."}, {"title": "4.2 Task Setting", "content": "We evaluate the efficacy of our proposed methodologies under open-domain QA task. This evaluation leverages three distinct open-domain QA datasets that do not require logical reasoning. These include: (i) The Natural Questions (NQ) dataset [18], which is a real-world queries compiled from search engines. (ii) PopQA [24], a dataset with a long-tail distribution, emphasizing less popular topics within Wikidata. (iii) AmbigNQ [25], an enhanced version of NQ that transforms ambiguous questions into a set of discrete, yet closely related queries. Additionally, we incorporate two benchmark datasets that require logical reasoning: (iv) 2WIKIMQA [30] and (v) HotPotQA [31]. Due to the costs associated with API calls for LLMs and Bing Search, and following common practices [16, 34, 23, 33], we test on a stratified sample of 300 questions from each dataset rather than the entire test dataset."}, {"title": "4.3 Baselines", "content": "We assess performance using the F1 Score and Hit Rate metrics. Due to the discrepancy between the verbose outputs of LLMs and the concise format of the dataset answers, we chose not to utilize the Exact Match (EM) metric. Instead, we considered a response as correct if the text hit any item of the labeled answers.\nRewrite-Retrieve-Read [23] represents the current state-of-the-art improvement on the basic Retrieve-then-Read RAG pipeline. Our approach enhances the existing RAG pipeline by augmenting the Query Rewriter to Query Rewriter+ and introducing a new Knowledge Filter module. To highlight the effectiveness of our method, we implemented the following configurations:\n(i) Direct: Ask the LLM directly with the original question.\n(ii) Rewriter-Retriever-Reader: Prior to retrieval, a Query Rewriter module is employed to generate a query that fetches external knowledge. The external knowledge, along with the original question, is used to prompt the response generation.\n(iii) Rewriter+-Retriever-Reader: Prior to retrieval, the Enhanced Query Rewriter module is utilized, generating multiple queries to acquire external knowledge and clarify the original question. Responses are generated using both the rewritten question and all retrieved external knowledge.\n(iv) Rewriter+-Retriever-Filter-Reader: Applied before retrieval, the Enhanced Query Rewriter module generates multiple queries and clarifies the original question. A Knowledge Filter is used to discard external knowledge unrelated to the rewritten question. The final response is then generated using the filtered external knowledge and the rewritten question.\nComparing the Rewriter+-Retriever-Reader setup with the Rewriter-Retriever-Reader setup validates the superiority of the proposed Question Rewriter+ module. Additionally, comparing the Rewriter+-Retriever-Filter-Reader setup with the Rewriter+-Retriever-Reader setup demonstrates the effectiveness of the 4-step RAG pipeline incorporating the Knowledge Filter."}, {"title": "4.4 Results", "content": "Experimental results are reported in Table 1. The scores indicate that the Query Rewriter+ module outperforms the Query Rewriter module across all datasets, substantiating that multiple queries and clarified question are more effective for a RAG system correctly response to user's questions than single query and unrefined questions. Specifically, adding the Knowledge Filter module to the traditional 3-step RAG pipeline significantly improves performance. This indicates that merely adding external knowledge to the RAG system can be detrimental, especially for multi-hop questions. The Knowledge Filter module effectively eliminates noise and irrelevant content, enhancing the accuracy and robustness of the RAG system's responses."}, {"title": "5 Ablation Studies", "content": "In this section, we analyze the individual and combined effects of question rewriting and knowledge filtering. The results presented in Table 3 indicate that the question rewriting process consistently improves answer accuracy across the setups of direct generation, retrieval-augmented generation using all knowledge, and retrieval-augmented generation using filtered knowledge.\n The results also shows that using all (unfiltered) external knowledge for retrieval-augmented generation can sometimes lead to marginal improvements or even decreased performance. For instance, on the CAmbigNQ dataset, when LLMs are asked with rewritten questions, introducing all external knowledge only raise the hit rate from 57.67% to 58%. Besides, when LLM are queried with the original questions, introducing all external knowledge makes the hit rate decreased from 55.67% to 54.00%. On the other hand, we observe that filtering knowledge can significantly boost the response accuracy of the RAG system, whether asking with the original question or rewritten question.\n Assessing the synergistic effect of two modules, we find that while each module individually improves response accuracy, the effect is sometimes modest. However, their combined yields a significant enhancement. For instance, on the CAmbigNQ dataset, the individual application of each module resulted in a maximum of 2% more correctly answered questions, whereas their combined application led to a 7% increase in correctly answered questions. A similar phenomenon can also been observed on the PopQA dataset."}, {"title": "6 Efficiency Improvement Investigation", "content": "In this section, we explore how efficiently our proposed method reduces redundant retrieval when answering recurring questions with historically similar semantics. We also examine the hyperparameter \\(\\Tau\\) to balance efficiency and response accuracy. The experimental procedure is as follows:\n Initially, we randomly selected 100 questions from the AmbigNQ dataset to generate responses using our proposed method. Unlike previous sections, we set the parameter n in the Knowledge Retriever module to 5. Instead of utilizing webpage snippets as the content of knowledge instances, we visited the searched URLs and read the entire webpage text, filtering out irrelevant information using the BM25 algorithm. After the response finished, the webpage content was then cached in the Memory Knowledge Reservoir. Subsequently, we selected an additional 200 questions from AmbigNQ that are semantically similar to the previously solved questions. These questions were answered with the support of the Memory Knowledge Reservoir and the Retrieval Trigger module, with the popularity threshold \\(\\theta\\) set as 3.\n We design several metrics to evaluate the resource cost of per question, include the average time spent in the RAG pipeline (Time Cost), the average number of external knowledge instances (External Knowledge), the average number of memory knowledge instances (Memory Knowledge), the average number of knowledge instances filtered out (Irrelevant Knowledge), and the performance metric Hit Rate. These metrics are recorded during the question-answering process.\n The analysis of the trade-off between response quality and efficiency for answering historically similar questions across different \\(\\Tau\\) settings is presented in Table 2. A significant finding is that the Time Cost metric reaches minimum when setting the similarity threshold \\(\\Tau\\ = 0.6\\). This is accompanied by the External Knowledge metric being very small, approximately 4.39, which is roughly equivalent to one query search. This suggests that this configuration predominantly leverages memory knowledge rather than external sources for generating responses, thereby enhancing response efficiency. Remarkably, at the \\(\\Tau\\ = 0.6\\) setup, the quality of the responses is not heavily affected and remains very close to that achieved by relying entirely on external knowledge at \\(\\Tau\\ = 1.0\\). This suggests that deploying the Memory Knowledge module can achieve a significant reduction in response time-by approximately 46%-without substantially compromising the quality of the answers. Furthermore, adjusting the threshold to 0.8 enhances the response quality beyond that at \\(\\Tau\\ = 1.0\\), underscoring that leveraging highly relevant historical experience can generate responses with superior quality."}, {"title": "7 Case Study", "content": "To intuitively demonstrate how the Query Rewriter+ Module enhances the original question and generates multiple queries, as compared to traditional Query Rewriter Modules, we present a question examples from 2WikiMQA dataset in Figure 4. It can be observed that the Query Rewriter+ Module semantically enhances the original question and, unlike the Query Rewriter which generates only one query, it produces three distinct queries, each focusing on different semantic aspects. The Query Rewriter+ module can retrieve external knowledge sources 1, 2, and 3, whereas the Query Rewriter module only retrieves sources 1 and 2, showcasing its advantage in improving knowledge recall. The Knowledge Filter module subsequently ensures the precision of the external knowledge by filtering out irrelevant knowledge instances (neutral, contradict) and retaining only those that provide valuable information for answering the question (entailment)."}, {"title": "8 Related Work", "content": "The core of RAG framework consists of retriever and reader modules. This retrieve-read pipeline has been enhanced, leading to the Modular RAG paradigm with various integrated modules. This section describes related modules in our work.\nRewriter: The introduction of a Question Rewriter module [23] led to the development of a Rewrite-Retrieve-Read RAG pipeline. This module generates a query that bridges the gap between the input text and the knowledge base, facilitating the retrieval of relevant knowledge and enhancing response accuracy. Our empirical studies indicate that while a single query retrieves limited useful information, multiple queries significantly enhance the retrieval of answer keywords. This discovery has reinforced and motivated our efforts to improve the existing functionality and design of the Question Rewriter.\nClarification: Represented by [17], this module generates clarification questions to ascertain user intent, thus refining vague questions to uncover the underlying inquiry intent. We have integrated the functionalities of the Rewriter and Clarification modules into a single unit, Query Rewriter+, employing a fine-tuned Gemma-2B model to perform both tasks generatively in one step, improving efficiency.\nPost-Retrieval Process: After information retrieval, presenting all data to a Large Language Model simultaneously may exceed the context window limit. The Re-Ranking module strategically relocates content based on relevance. Our preliminary study reveals that Large Language Models (LLMs) have evolved to handle extended contexts, accommodating all retrievable information until a bottleneck is reached. Consequently, we consider this post-retrieval process primarily as a de-noising task, rather than focusing on ranking.\nMemory: Modules in this category leverage historically similar question-answer records to enhance current problem-solving capabilities, reflecting an evolutionary learning process within the agent [35]. Drawing on this concept, we employ a parameter-free caching mechanism to expand the knowledge boundaries of RAG-based question-answering, effectively improving response efficiency.\nRetrieve Trigger: Understanding the parameterized knowledge boundaries of LLMs is crucial for optimizing the timing of knowledge retrieval. Calibration-based judgment methods have proven both efficient and practical. However, our study explores a non-parametric knowledge domain within a continuously expanding RAG system. This is the first attempt to design a Retrieve Trigger specifically for such scenarios. Our exploration focuses on identifying appropriate thresholds that balance accuracy and efficiency.\nAdditional modules include Knowledge Retriever, LLM Reader, Fact Checking, Revising [22, 10], and iterative RAG pipeline [28] with further details available in [11, 9]."}, {"title": "8.1 Retrieval Augmented Generation", "content": "Retrieval Augmented Generation (RAG) [20] leverages a retriever that provides substantial external information to enhance the output of Large Language Models (LLMs). This strategy utilizes knowledge in a parameter-free manner and circumvents the high training costs associated with LLMs' parameterized knowledge. Furthermore, it alleviates hallucination issues, significantly enhancing the factual accuracy and relevance of the generated content. The concept of RAG is rooted in the DrQA framework [5], which marked the initial phase of integrating retrieval mechanisms with Language Models through heuristic retrievers like TF-IDF for sourcing evidence. Subsequently, RAG evolved with the introduction of Dense Passage Retrieval [15] and REALM [26]. These methods utilize pre-trained transformers and are characterized by the joint optimization of retrieval and generation process. Recent advancements have extended RAG's capabilities by integrating Large Language Models (LLMs), with developments such as REPLUG [29] and IC-RALM [26] demonstrating the potent generalization abilities of LLMs in zero-shot or few-shot scenarios. These models can follow complex instructions, understand retrieved information, and utilize limited demonstrations for generating high-quality responses."}, {"title": "8.2 Modular RAG", "content": "The core of RAG framework consists of retriever and reader modules. This retrieve-read pipeline has been enhanced, leading to the Modular RAG paradigm with various integrated modules. This section describes related modules in our work.\nRewriter: The introduction of a Question Rewriter module [23] led to the development of a Rewrite-Retrieve-Read RAG pipeline. This module generates a query that bridges the gap between the input text and the knowledge base, facilitating the retrieval of relevant knowledge and enhancing response accuracy. Our empirical studies indicate that while a single query retrieves limited useful information, multiple queries significantly enhance the retrieval of answer keywords. This discovery has reinforced and motivated our efforts to improve the existing functionality and design of the Question Rewriter.\nClarification: Represented by [17], this module generates clarification questions to ascertain user intent, thus refining vague questions to uncover the underlying inquiry intent. We have integrated the functionalities of the Rewriter and Clarification modules into a single unit, Query Rewriter+, employing a fine-tuned Gemma-2B model to perform both tasks generatively in one step, improving efficiency.\nPost-Retrieval Process: After information retrieval, presenting all data to a Large Language Model simultaneously may exceed the context window limit. The Re-Ranking module strategically relocates content based on relevance. Our preliminary study reveals that Large Language Models (LLMs) have evolved to handle extended contexts, accommodating all retrievable information until a bottleneck is reached. Consequently, we consider this post-retrieval process primarily as a de-noising task, rather than focusing on ranking.\nMemory: Modules in this category leverage historically similar question-answer records to enhance current problem-solving capabilities, reflecting an evolutionary learning process within the agent [35]. Drawing on this concept, we employ a parameter-free caching mechanism to expand the knowledge boundaries of RAG-based question-answering, effectively improving response efficiency.\nRetrieve Trigger: Understanding the parameterized knowledge boundaries of LLMs is crucial for optimizing the timing of knowledge retrieval. Calibration-based judgment methods have proven both efficient and practical. However, our study explores a non-parametric knowledge domain within a continuously expanding RAG system. This is the first attempt to design a Retrieve Trigger specifically for such scenarios. Our exploration focuses on identifying appropriate thresholds that balance accuracy and efficiency.\nAdditional modules include Knowledge Retriever, LLM Reader, Fact Checking, Revising [22, 10], and iterative RAG pipeline [28] with further details available in [11, 9]."}, {"title": "9 Conclusion", "content": "In this paper, we present a four-module strategy to enhance RAG systems. The Query Rewriter+ module generates clearer questions for better intent understanding by LLMs and produces multiple, semantically distinct queries to find more relevant information. The Knowledge Filter refines retrieved information by eliminating irrelevant and noisy context, enhancing the precision and robustness of LLM-generated responses. The Memory Knowledge Reservoir and the Retrieval Trigger module optimize the use of historical data and dynamically manage external information retrieval needs, increasing system efficiency. Collectively, these advancements improve the accuracy and efficiency of the RAG system."}]}