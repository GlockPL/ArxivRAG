{"title": "Unsupervised Entity Alignment Based on\nPersonalized Discriminative Rooted Tree", "authors": ["Yaming Yang", "Zhe Wang", "Ziyu Guan", "Wei Zhao", "Xinyan Huang", "Xiaofei He"], "abstract": "Entity Alignment (EA) is to link potential equivalent\nentities across different knowledge graphs (KGs). Most existing\nEA methods are supervised as they require the supervision of\nseed alignments, i.e., manually specified aligned entity pairs. Very\nrecently, several EA studies have made some attempts to get rid\nof seed alignments. Despite achieving preliminary progress, they\nstill suffer two limitations: (1) The entity embeddings produced\nby their GNN-like encoders lack personalization since some of\nthe aggregation subpaths are shared between different entities.\n(2) They cannot fully alleviate the distribution distortion issue\nbetween candidate KGs due to the absence of the supervised\nsignal. In this work, we propose a novel unsupervised entity\nalignment approach called UNEA to address the above two issues.\nFirst, we parametrically sample a tree neighborhood rooted at\neach entity, and accordingly develop a tree attention aggregation\nmechanism to extract a personalized embedding for each entity.\nSecond, we introduce an auxiliary task of maximizing the mutual\ninformation between the input and the output of the KG encoder,\nto regularize the model and prevent the distribution distortion.\nExtensive experiments show that our UNEA achieves a new state-\nof-the-art for the unsupervised EA task, and can even outperform\nmany existing supervised EA baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "Knowledge Graph (KG) can describe massive knowledge\nfacts of our world and has been successfully applied\nto various tasks, such as question answering [1], search\nengines [2], document retrieval [3], recommender systems [4],\netc. However, as a KG usually covers only a small part of\nknowledge facts of a specific domain, it may fail to provide\nsufficient knowledge to support downstream applications. En-\ntity Alignment (EA) is an effective solution for this issue. It\ncan fuse multiple KGs by identifying equivalent entities across\nKGs, and the merged KGs can provide more comprehensive\ninformation for downstream tasks. The primary challenge for\nEA comes from the heterogeneous symbolic representations\nof different KGs, which include different naming rules and\nmultilingualism.\nExisting EA methods [5]\u2013[8] address the heterogeneity issue\nby projecting different KGs into a common low-dimensional\nembedding space, where similar entities are pulled close while\ndissimilar ones are pushed far away. Thus, the similarity\nbetween entities can be conveniently measured by various\ndistance functions such as cosine distance, and 11/12 norm.\nBased on how the entity embeddings are extracted, existing EA\nmethods can be divided into two categories. Trans-based [9]-\n[11] methods utilize translation-family embedding methods,\nsuch as TransE [12], to learn entity representations. GNN-\nbased EA methods [13]\u2013[16] utilize Graph Neural Networks\n(GNNs) [17], [18] to encode KGs. Unfortunately, the majority\nof the existing methods, whether Trans-based or GNN-based,\nare supervised and their effectiveness relies on the supervision\nof high-quality manual labels, i.e., seed alignments. This\nis impractical since in the real world, it is expensive to\nobtain enough high-quality labels, and sometimes labels are\nunavailable due to privacy concerns.\nRecently, researchers have proposed several unsupervised\nEA methods [19]-[23] to make some efforts in getting rid\nof seed alignments. They typically use graph attention-based\nKG encoders [18] to extract entity embeddings. Based on the\ndistance of these entity embeddings, they generate pseudo-\nlabels to serve as the self-supervised alignment signal. Despite\nshowing preliminary progress, they still face two limitations."}, {"title": "Limitation 1: low personalization of entity embeddings.", "content": "Current unsupervised EA methods generally stack multiple\nGNN layers and extract entity embeddings through iterative\nattention aggregation. However, this encoding scheme may\nnot be able to extract the most discriminative embedding\nfor an entity to find its potential alignment counterpart. This\nis because some aggregation subpaths are inevitably shared\nbetween different entities, limiting the personalization and\nflexibility of entity embeddings.\nFor example, Fig. 1(a) shows two toy KGs to be aligned.\nWhen the model tries to align the English entity \u201cBill Gates\u201d\nof KG1 with the Chinese entity \u201cBill Gates\u201d of KG2, their\nsecond-order neighbor \u201cSteve Ballmer\u201d is more discriminative.\nContrastively, when examining the English entity \"Private\nSchool\" and its Chinese counterpart, their second-order neigh-\nbor \"Ivy League\u201d is more discriminative. To further analyze\nthis issue, in Fig. 2, we visualize the aggregation paths of\na GNN for the six entities of KG1 in Fig. 1(a). Solid lines\nrepresent neighborhood aggregation, and dotted lines represent\nself-connections. We can see that the target entity \"Bill Gates\"\nwants its second-order neighbor \"Steve Ballmer\" to have\na larger aggregation weight but wants another second-order\nneighbor \"Ivy League\" to have a smaller aggregation weight.\nHowever, the situation is the opposite for the target entity\n\"Private School\u201d. This conflicting situation creates a dilemma\nfor their common first-order neighbor \"Harvard University\"\nwhen it aggregates its own neighbors."}, {"title": "Limitation 2: cannot fully avoid distribution distor-\ntion.", "content": "Existing EA approaches can be generally abstracted\ninto two key parts. Firstly, KG encoders are used to shape\nthe original candidate KGs into a specific distribution in\na low-dimensional Euclidean space. Secondly, based on the\ndistribution, potentially aligned entity pairs are pulled together\nand unlikely pairs are pushed apart. The previous study\nRREA [24] proves that GNN-based EA methods are subject\nto this framework. Besides, it proves that the orthogonality\nof GNN's projection parameters is highly beneficial for the\nEA task since it corresponds to rotation transformation that\ndoes not affect the embedding distribution. However, this good\nproperty requires an important premise that the EA task is\nsupervised. Besides, GNN's aggregation operator does not\nguarantee this property. Therefore, in the unsupervised setting,\nthe embedding distribution could still be distorted due to the\nwrong guidance of false pseudo-labels."}, {"title": "In this paper, we propose a novel unsupervised EA method", "content": "called UNEA to address the two limitations described above\nthat hinder existing unsupervised EA methods.\nFirstly, we use powerful LLMs to initialize the embeddings\nof entities and relations based on their surface names, which\ncan be treated as a good \"weak supervision signal\" for\nmodel optimization. As shown in Fig. 3(a), after the entity\nembeddings are initialized by LLMs, the distributions of the\ntwo candidate KGs are already quite similar.\nSecondly, we define a parametric sampling function to\nextract a discriminative tree neighborhood for each entity. In\nthe sampled tree, each target entity itself serves as the root.\nThen, we design a corresponding tree attention aggregation\nmechanism to extract embedding for the target (root) entity.\nSince we customize a tree neighborhood for each target entity,\ntheir aggregation paths are fully decoupled. Thus, each target\nentity can learn its best aggregation path, resulting in more\npersonalized embeddings.\nFinally, to regularize the main alignment task, we let\nthe model maximize the mutual information between the\noutput high-level embeddings of entities and relations and\ntheir embeddings initialized by LLMs, thereby preserving the\ninformation of the \"weak supervision signal\". Besides, we\nmaximize the graphical mutual information [25] between the\nhigh-level entity embeddings and the KG topology. These\nmutual information maximization-based terms can continu-\nously regularize the model so that the high-level embeddings\nof entities and relations can always reflect the information\nof their surface names and their topological relationships,\nthus preventing the possible issue of distribution distortion.\nIn Fig. 3(c), we visualize the entity embedding distributions\nof two KGs that are extracted by the GNN that incorporates\nour regularization terms. As we can see, in comparison with\nFig. 3(a) and Fig. 3(b), the two embedding distributions in\nFig. 3(c) become tighter and are more similar to each other,\nindicating its effectiveness.\nThe main contributions of this work are summarized as\nfollows:\n\u2022\n\u2022\n\u2022\nWe sample a personalized tree and propose an innova-\ntive tree attention aggregation mechanism to learn more\npersonalized embeddings for entities.\nWe introduce mutual information maximization-based\nterms to continuously regularize the EA model, which\nhelps avoid the distribution distortion issue in the unsu-\npervised setting.\nWe conduct extensive experiments on two widely used\nbenchmark datasets to verify the effectiveness of UNEA.\nIt turns out that our UNEA can outperform state-of-the-\nart unsupervised baselines as well as supervised baselines,\nindicating its superiority."}, {"title": "II. RELATED WORK", "content": "In this section, we review existing EA methods in terms of\nboth supervised EA methods and unsupervised EA methods."}, {"title": "A. Supervised Entity Alignment", "content": "Most of the existing EA methods are supervised since they\nrequire seed alignment for supervision. According to their\nKG encoders, they can be divided into two categories [26],\n[27]. Trans-based methods [9], [10], [10], [11], [11], [28]-\n[33] typically use translation-family KG encoders such as\nTransE [12] to encode the structural information of KGs.\nGNN-based methods [13], [14], [14]-[16], [24], [24], [34],\n[34]-[58] use more advanced GNN-like encoders [17], [18],\nusually in combination with relation-specific attention aggre-\ngation schemes [59], [60]to learn entity embeddings. After\nencoding, they perform alignment training by pulling seed\nalignment entity pairs closer while pushing the other entity\npairs far away in the embedding space. There are also some\nmethods that belong neither to Trans-based methods nor to\nGNN-based methods. For example, LightEA [61] achieves su-\npervised alignment based on the label propagation algorithm.\nAlthough supervised EA methods have achieved remarkable\nalignment performance, they rely on manually provided high-\nquality supervised signals that are expensive to acquire in\npractice, hindering their widespread application in real-world\nscenarios."}, {"title": "B. Unsupervised Entity Alignment", "content": "Most existing unsupervised EA methods [19]-[23], [62] are\nGNN-based, and they usually utilize GCN [17] or GAT [18] to\nextract the structural information of candidate KGs. Based on\nthe extracted entity embeddings, they compute the similarity\nbetween all the possible entity pairs across candidate KGs.\nThen, they will select a set of the most similar entity pairs to\nform pseudo-labels to guide the alignment training. To improve\nthe reliability of pseudo-labels, they often need to introduce\nvarious auxiliary information. For example, SelfKG [22],\nICLEA [20], and UPLR [21] introduce the text features of\nentity names, EVA [19] introduces the image features associ-\nated with entities, and DualMatch [23] introduces the temporal\ninformation associated with relational facts. There is also\na Trans-based unsupervised EA method, i.e., MultiKE [29],\nwhich leverages multiple views of entity features, such as\nnames, relations, and attributes to enhance alignment. Despite\nshowing preliminary progress in unsupervised EA, they still\nface the two limitations as we have analyzed in Section I. In\nthis work, we aim to develop a novel KG encoding technique\nand introduce an advanced mutual information-based regular-\nization mechanism, to effectively address the two limitations\nand further boost the performance of the unsupervised EA\ntask.\nThere are also several unsupervised EA methods that belong\nneither to GNN-based methods nor to Trans-based methods.\nSEU [63] and DATTI [64] transform the EA problem into\nan assignment problem. ERAlign [65] jointly performs entity\nalignment and relation alignment by neighbor triple matching\nstrategy, based on semantic textual features of entities and\nrelations. FGWEA [66] achieves the unsupervised EA task\nbased on the Fused Gromov-Wasserstein Distance [67]. While\neffective, these methods often require complex optimization\noperations and sacrifice the ability of end-to-end learning."}, {"title": "III. PRELIMINARIES", "content": "In this section, we first define the basic notations that we use\nthroughout the text, and then formally describe the problem"}, {"title": "Definition 1. Knowledge Graphs.", "content": "A knowledge graph G is\nrepresented as G = (E, R, T), where E, R, T denote the set of\nentities, the set of relations, and the set of triples, respectively.\nA triple t\u2208T can be denoted as < i,k,j >, depicting that\nthe head entity i \u2208 E and the tail entity j \u2208 E hold the relation\nk\u2208 R between them. Let $e_i$ denote the representation vector\nof entity i, and $r_k$ denote the representation vector of relation\nk."}, {"title": "Definition 2. Entity Alignment.", "content": "Given a pair of candidate\nknowledge graphs $G_1$ = $(E_1, R_1, T_1)$ and $G_2$ = $(E_2, R_2, T_2)$\nto be aligned, supervised entity alignment is to discover\npotential equivalent entity pairs, based on the supervision of\na set of seed alignments S C $E_1$ \u00d7 $E_2$, while unsupervised\nentity alignment aims to achieve this goal without any manual\nsupervision signal."}, {"title": "IV. METHODOLOGY", "content": "In this section, we elaborate on the technical details of our\nproposed UNEA. Fig. 4 depicts its overall architecture."}, {"title": "A. Embedding Initialization", "content": "In recent years, LLMs have made significant strides across\nvarious domains. Many previous EA methods [20], [22], [45],\n[68] have utilized language models to initialize entity embed-\ndings, which has substantially enhanced EA performance. In\nthis work, we employ LLMs to initialize entity embeddings\nand relation embeddings. Specifically, we utilize LLMs to\nextract the name feature of entities and relations, which can\nbe described as follows:\n$f_e = LLM_{\\theta}(n_i)$ \n$f_r = LLM_{\\theta}(n_k)$ (1)\nwhere $\\theta$ is the parameters of LLMs that are pre-trained in an\nunsupervised manner. The terms $n_i$ and $n_k$ are the surface\nnames of entity i and relation k, respectively. The vectors $f_e$\nand $f_r$ are the corresponding features returned by LLMs, and\nwe use them to initialize the entity embedding $e_i$ and the\nrelation embedding $r_k$, as follows:\n$e_i = Init(f_e)$\n$r_k = Init(f_r)$ (2)\nDuring the training, $e_i$ is updated by parametric GNN-like\naggregation, and $r_k$ itself is a learnable parameter vector."}, {"title": "B. Sampling Personalized Rooted Trees", "content": "Recall that in Limitation 1 of Section I, we have intuitively\nanalyzed that previous EA-oriented GNNs have a limitation\nof coupling some sub-paths for aggregation. Here, we further\nexplain this point in general. As shown in Fig. 5, we visualize\nthe aggregation paths of a three-layer GNN model for five"}, {"title": "direct relation between i and y, we can compose a dummy", "content": "relation p along the path that connects them: p = l\u25e6k, where \u25e6\ndenotes the composition operator between relations l = $(i, x)$\nand k = $(x,y)$. In practice, we use the Hadamard product to\nachieve this:\n$r_p = r_l \\circ r_k$ (7)\nThen, the projection matrix is accordingly set as follows:\n$W_p = I - 2 \\cdot r_p r_p^T$ (8)\nNote that, similar to the projection matrix $W_k$, the projection\nmatrix $W_p$ is also orthogonal.\nThrough hierarchical sampling, we can customize a per-\nsonalized tree neighborhood for each target entity i, which\nnaturally serves as the root of the tree. For each sampled tree,\nits aggregation paths form a subset of the traditional GNN's\naggregation paths. By repeating random sampling multiple\ntimes, our parametric sampling technique can gradually cover\nall the possible aggregation paths."}, {"title": "C. Tree Attention Encoding", "content": "Based on the sampled tree, we accordingly develop an\ninnovative tree attention encoder to obtain the embedding of\nthe root entity. The encoding process is similar to the reverse\nprocess of sampling, as illustrated by arrows in Fig. 6. We\nuse different attention mechanisms for the sampling process\nand the encoding process because the former operates on the\noriginal KG for sampling while the latter operates on the\nsampled tree for encoding. Referring to Fig. 6 again, the\naggregation attention coefficient from entity y to entity x is\ncomputed as follows:\n$b_{x,k,y}^{(i)} = \\sigma(w_{att}^T [W_r e_i || W_k \\cdot e_x || e_y])$ (9)\n$\\alpha_{x,k,y}^{(i)} = \\frac{exp(b_{x,k,y}^{(i)})}{\\sum_{z \\in \\mathcal{N}_x, s=\\phi(x,z)} exp(b_{x, s, z}^{(i)})}$ (10)\nwhere $\u03c3(\u00b7)$ is the non-linear activation function, $\\mathcal{N}_x$ is a set\ndenoting the neighbors of entity x, s = $(x,z)$ denotes\nthe relation s between entities x and z, and $d_y$ denotes the\ndegree of entity y, which is used to reflect the fact that the\ndiscriminability of a neighbor is usually inversely proportional\nto its degree [48]. $W_k$ is a parametric projection matrix\nspecific to relation k. Following [24], we set $W_k$ as follows:\n$W_k = I - 2 \\cdot r_k r_k^T$ (5)\nThis setting can naturally guarantee the orthogonality of the\nprojection matrix since we can easily derive the following\nequation:\n$W_k \\cdot W_k = (I \u2212 2 \u00b7r_k r_k^T ) \u00b7 (I-2 \\cdot r_k r_k^T ) = I$ (6)\nThis indicates that the projection operation corresponds to a\nrotation operation specific to relation k, which is proven to be\nhighly beneficial for the EA task [24].\nIn Eq. (3), we particularly add a term $e_i^T \\cdot W_p \\cdot e_y$ to reflect\nthe influence of root entity i on the selection of its second-\norder neighbor y. As shown in Fig. 6, although there is no\nBy hierarchical aggregation from the descendants towards the\nroot, we can finally obtain the embedding of the root entity i."}, {"title": "D. Generating Pseudo Labels for Alignment", "content": "Based on the obtained entity embeddings, we can compute\na matrix S to describe the similarities between each pair of\nentities that are from different KGs. For example, given a pair"}, {"title": "E. Contrastive Alignment Loss", "content": "Now, we are ready to define the alignment loss. We adopt\nthe InfoNCE loss [71] to achieve the alignment task in a\ncontrastive manner. First, given a pseudo-label denoted by\nan entity pair < $i_1$, $i_2$ >, their similarity is measured by a\nfunction as follows:\n$g(i_1, i_2) = exp(-\\frac{||e_{i_1} - e_{i_2}||}{T})$ (15)\nwhere T denotes a temperature hyper-parameter. Then, the sim-\nilarity of the pseudo-label is maximized while the similarity\nof randomly sampled entity pairs is minimized.\n$T(i_1, i_2) = -log( \\frac{g(i_1, i_2)}{g(i_1, i_2) + \\sum_{Y_2 \\neq j_2, Y_2 \\sim P_{E_2}} g(i_1, Y_2)} )$ (16)\n$T(i_1, i_2) = -log( \\frac{g(i_1, i_2)}{g(i_1, i_2) + \\sum_{X_1 \\neq i_1, x_1 \\sim P_{E_1}} g(x_1, i_2)} )$ (17)\nFor each pseudo-label < $i_1$,$i_2$ >, we generate negative\nexamples from both directions. One sub-loss fixes $i_1$ and\nrandomly samples entities from $E_2$, i.e., by distribution $P_{E_2}$.\nThe other sub-loss fixes $i_2$ and randomly samples entities from\n$E_1$, i.e., by distribution $P_{E_1}$. The final contrastive alignment\nloss is computed by averaging the two sub-losses:\n$\\mathcal{L}_A = \\frac{1}{2 \\cdot \\mathcal{S}} \\sum_{<i_1,i_2>\\in \\mathcal{S}} T(i_1, i_2) + T(i_1, i_2)$ (18)\nwhere $\\mathcal{S}$ is the set of pseudo alignment labels generated by\nEq. (14). By definition, $\\mathcal{L}_A$ maximizes the mutual information\nbetween entity pairs in $\\mathcal{S}$."}, {"title": "F. Mutual Information-based Regularization", "content": "As described by Eq. (1), we initialize the entity embeddings\nby inputting their names into LLMs. The previous work [22]\nhas demonstrated that quite good EA performance can be\nachieved by using only these initialized entity embeddings,\nwhich can serve as a \u201cweak supervision signal\" for the\nEA task. Inspired by this finding, we develop a correction\nmechanism to address Limitation 2 mentioned in Section I.\nThe basic idea is to prevent the encoder from losing the\nbeneficial information of these initial entity embeddings. To\nthis end, we introduce three regularization terms based on\nmutual information, as follows:\n$\\mathcal{l}_e = InfoNCE(e_i, f_e)$ (19)\n$\\mathcal{l}_R = InfoNCE(r_k, f_r)$ (20)\n$\\mathcal{l}_T = CE(W_{i,j}, A_{i,j})$ (21)\nAs shown in Eqs. (19, 20), we maximize the mutual infor-\nmation between entity/relation embeddings and their name\nfeatures by minimizing the InforNCE loss as described by\nEqs. (15, 16, 17) above. For Eq. (21), following previous\nwork [25], we minimize the cross-entropy loss to maximize the\nmutual information between the estimated edge weight $W_{i,j}$\nand the input edge weight $A_{i,j}$, and the former is computed\nas follows:\n$W_{i,j} = \\sigma(e_i^T \\cdot W_n \\cdot e_j)$ (22)\nwhere $W_n$ is a parameter matrix.\nThese three mutual information-based terms can help the\nmodel preserve the information of the initialized entity fea-\ntures, the initialized relation features, and the input KG\nstructural features, respectively. The total regularization loss\nis computed as the sum of these three terms:\n$\\mathcal{L}_{MI} = \\mathcal{l}_e + \\mathcal{l}_R + \\mathcal{l}_T$ (23)"}, {"title": "G. Training", "content": "Finally, the overall loss is a weighted combination of the\nalignment loss and the regularization loss:\n$\\mathcal{L} = \\lambda \\cdot \\mathcal{L}_A + (1 - \\lambda) \\cdot \\mathcal{L}_{MI}$ (24)\nwhere \u5165 is a balance hyper-parameter. All the model param-\neters are optimized under the guidance of the overall loss.\nAs the parameter optimization progresses, the model will\ngradually learn better embeddings, sample more personalized\ntrees, learn more discriminative aggregation paths, and gener-\nate more effective pseudo-labels, finally leading to better EA\nperformance.\nAfter the model training is finished, the distance between\nentity embeddings can reflect the semantic similarity between\nthese entities. Thus, we can discover more potentially aligned"}, {"title": "Recall that in Section IV-B, we sample personalize tree", "content": "neighborhood according to the attention distribution, which\ndepends on the embeddings of entities and relations, as de-\nscribed by Eqs. (4-8). However, as a discrete operator, the\nsampling operation is not differentiable. To address this issue,\nwe adopt the momentum technique [72] to update the entity\nand relation embeddings for sampling. For higher efficiency,\nwe sample trees and update pseudo-labels every m epoch."}, {"title": "V. EXPERIMENT", "content": "In this section, we conduct extensive experiments to show\nthe superior effectiveness of our proposed UNEA."}, {"title": "A. Datasets", "content": "We use the DBP15K dataset [10] and the DWY100K\ndataset [28] in our experiments, which are the most widely\nused benchmark datasets in previous studies."}, {"title": "B. Baselines", "content": "We compare our UNEA against eighteen state-of-the-art\nbaseline EA methods, which can be divided into three cat-\negories as follows:\n\u2022 Supervised & Trans-based methods: MTransE [9],\nTranseEdge [11], JAPE [10], BootEA [28], and\nMRPEA [33].\n\u2022 Supervised & GNN-based methods: GCN-Align [13],\nMuGNN [34], AliNet [14], RDGCN [15], HGCN [46],\nRNM [36], NAEA [51], and RREA [24].\n\u2022 Unsupervised methods: MultiKE [29], EVA [19],\nSelfKG [22], ICLEA [20], and UPLR [21]."}, {"title": "C. Implementation Details", "content": "We implement our UNEA by PyTorch2. All the trainable\nparameters are first randomly initialized by the Xavier dis-\ntribution [73]. Then, the embeddings of entities and relations\nare initialized by Eqs. (1,2), and we adopt a state-of-the-art\nmulti-lingual pre-trained language model LaBSE [74] as the\nLMM. Finally, the model parameters are optimized by gradient\ndescent, and we adopt the Adam optimizer with a learning\nrate of 0.0001. The batch size is set to 128. The number of\nepochs is set to 300, the embedding dimensionality of both\nentities and relations is set to 300, the temperature hyper-\nparameter is set to 0.08, the number of negative samples\nis set to 128, the momentum is set to 0.9, and the non-\nlinear activation function $\u03c3(\u00b7)$ is implemented as LeakyReLU.\nFor supervised baselines, following convention, 30% of pre-\naligned entity pairs are treated as the labels for supervision. For\nunsupervised baselines as well as our UNEA, they are trained\nin an unsupervised manner without any manual labels. For\nfairness, all the methods only utilize the names of entities and\nrelations without the preprocess of Google Translate. All the\nexperiments are run on an NVIDIA GPU with 24GB memory.\nFor quantitative evaluation, consistent with most previous\nstudies, we use three widely used metrics Hits@1, Hits@10,\nand MRR (mean reciprocal rank) [5]-[8], [40], [75]."}, {"title": "D. Main Results", "content": "We compare the alignment accuracy of our UNEA against\nall the baseline methods. Table II and Table III show the\nresults on DBP15K and DWY100k, respectively."}, {"title": "E. Case Study", "content": "We further analyze the effectiveness of UNEA more in-\ntuitively. In Fig. 7, we visualize the sampled trees of two\ntarget entities \u201cBill Gates\u201d and \u201cPrivate School\u201d. The attention\ncoefficients for aggregation are also accordingly marked on\nthe branches. We can see, in the left tree, \u201cBill Gates\u201d\nentity assigns a larger coefficient to its grandson entity \u201cSteve\nBallmer", "Private School\\\"\nentity assigns a larger coefficient to its grandson entity \\\"Ivy\nLeague\". This is more reasonable and intuitive in practice.\nTraditional GNN-like encoders cannot well capture this flex-\nibility since there are always some sub-paths that are shared\namong different target entities.\"\n    },\n    {\n      \"title\"": "F. Ablation Study"}, {"content": "To verify the key components of our UNEA, we set three\nvariants as follows:\n\u2022\n\u2022 UNEA-TA replaces the tree attention encoder with tradi-\ntional GNN encoder [22];\n\u2022 UNEA-MI removes the mutual information regulariza-\ntion (MI) module.\nAs shown in Table IV, all three variants perform worse\nthan UNEA, indicating the effectiveness of the corresponding\nmodules. Specifically, the variant UNEA-PS shows the worst\naccuracy because random sampling would introduce noisy\nneighbors that are not helpful for target entities. This is also\nconsistent with the observation of [22]. UNEA-PS shows the\nsecond-worst performance, suggesting that it is beneficial to\nlearn personalized aggregation paths for target entities. UNEA\nperforms better than UNEA-MI, which implies that mutual\ninformation-based regularization is helpful for the unsuper-\nvised EA task."}, {"title": "G. Effectiveness of LLMs", "content": "While we utilize the LaBSE [74] as the LLM to initialize\nthe entity embeddings, some previous EA methods [19], [76]\nadopt the traditional shallow language model fastText [77]\nlanguage model to initialize their entity embeddings. Here, we\ncompare the impact of the two language models of LaBSE and\nfastText on the EA performance."}, {"title": "H. Hyper-parameter Study", "content": "Referring to Section IV-G, we have introduced two hyper-\nparameters. The hyper-parameter A balances the alignment"}, {"title": "model cannot fully exploit pseudo-labels or cannot update", "content": "pseudo-labels in time. In practice, we default m to 10 for\nall the other experiments."}, {"title": "VI. CONCLUSION AND DISCUSSION", "content": "In this work, we note two limitations of existing EA\nmethods. (1) They cannot flexibly capture the personality of\nentity embeddings due to the shared aggregation subpaths\nin their encoding procedures; (2) They cannot fully alleviate\nthe distortion of the distribution similarity between candidate\nKGs in the unsupervised setting. To this end, we propose a\nnovel unsupervised entity alignment method named UNEA. It\nsamples a personalized tree neighborhood rooted at each target\nentity and learns personalized aggregation paths for the root\nentity. Three types of mutual information maximization-based\nregularization terms are introduced into the model to prevent\nthe distribution distortion issue. Extensive experiments show\nthat our UNEA achieves a new state-of-the-art performance in\nthe EA task without any supervision information. It can even\noutperform previous supervised EA methods.\nAlthough showing promising performance, our UNEA still\nhas some limitations as follows. Like most unsupervised EA\nmethods [20]-[22], UNEA requires unsupervised pre-trained\nlanguage models to initialize the entity embeddings according\nto entities' names. Fortunately, in real life, the names of\nentities are usually available. The overall time complexity\nof our UNEA can reach the square level of the number of\nentities, due to the bi-directional match of pseudo-labels. This\ncomplexity is on par with most previous unsupervised EA\nmethods [20], [21]."}]}