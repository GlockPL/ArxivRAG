{"title": "Large Language Models for Bioinformatics", "authors": ["Wei Ruan", "Yanjun Lyu", "Jing Zhang", "Jiazhang Cai", "Peng Shu", "Yang Ge", "Yao Lu", "Shang Gao", "Yue Wang", "Peilong Wang", "Lin Zhao", "Tao Wang", "Yufang Liu", "Luyang Fang", "Ziyu Liu", "Zhengliang Liu", "Yiwei Li", "Zihao Wu", "Junhao Chen", "Hanqi Jiang", "Yi Pan", "Zhenyuan Yang", "Jingyuan Chen", "Shizhe Liang", "Wei Zhang", "Terry Ma", "Yuan Dou", "Jianli Zhang", "Xinyu Gong", "Qi Gan", "Yusong Zou", "Zebang Chen", "Yuanxin Qian", "Shuo Yu", "Jin Lu", "Kenan Song", "Xianqiao Wang", "Andrea Sikora", "Gang Li", "Xiang Li", "Quanzheng Li", "Yingfeng Wang", "Lu Zhang", "Yohannes Abate", "Lifang He", "Wenxuan Zhong", "Rongjie Liu", "Chao Huang", "Wei Liu", "Ye Shen", "Ping Ma", "Hongtu Zhu", "Yajun Yan", "Dajiang Zhu", "Tianming Liu"], "abstract": "With the rapid advancements in large language model (LLM) technology and the emergence of bioinformatics-specific language models (BioLMs), there is a growing need for a comprehensive analysis of the current landscape, computational characteristics, and diverse applications. This survey aims to address this need by providing a thorough review of BioLMs, focusing on their evolution, classification, and distinguishing features, alongside a detailed examination of training methodologies, datasets, and evaluation frameworks. We explore the wide-ranging applications of BioLMs in critical areas such as disease diagnosis, drug discovery, and vaccine development, highlighting their impact and transformative potential in bioinformatics. We identify key challenges and limitations inherent in BioLMs, including data privacy and security concerns, interpretability issues, biases in training data and model outputs, and domain adaptation complexities. Finally, we highlight emerging trends and future directions, offering valuable insights to guide researchers and clinicians toward advancing BioLMs for increasingly sophisticated biological and clinical applications.", "sections": [{"title": "1 Introduction", "content": "The rapid development of large language models (LLMs) such as BERT [1], GPT [2], and their specialized counterparts has revolutionized the field of natural language processing (NLP). Their ability to model context, interpret complex data patterns, and generate human-like responses has naturally extended their applicability to bioinformatics, where biological sequences often mirror the structure and complexity of human languages [3]. LLMs have been successfully applied across various bioinformatics domains, including genomics, proteomics, and drug discovery, offering insights that were previously unattainable through traditional computational methods [4].\nDespite significant advancements, challenges remain in the systematic categorization and comprehen-sive evaluation of applications of these models on bioinformatic problems. Considering the variety of bioinformatics data and the complexity of life activities, navigating the field can often be challenging, as existing studies tend to focus on a limited scope of applications. This leaves gaps in understanding the broader utility of LLMs in various bioinformatics subfields [5].\nThis survey aims to address these challenges by providing a comprehensive overview of LLM applications in bioinformatics. By focusing on different levels of life activities, this article collected and exhibited related works from two major views: life science and biomedical applications.\nWe have collaborated with domain experts to compile a thorough analysis spanning key areas in these views, such as nucleoid analysis, protein structure and function prediction, genomics, drug discovery, and disease modeling, including applications in brain diseases and cancers, as well as vaccine development.\nIn addition, we propose the new term 'Life Active Factors' (LAFs) to describe the molecular and cellular components that serve as candidates for life science research targets, which widely includes not only concrete entities (DNA, RNA, protein, genes, drugs) but also abstract components (bio-pathways, regulators, gene-networks, protein interactions) and biological measurements (phenotypes, disease biomarkers). LAFs is a comprehensive term that is capable of reconciling the conceptual divergence arising from research across various bioinformatics subfields, benefiting the understanding of multi-modality data for LAFs and their interplays in complex bio-systems. The introduction of LAFs aligns well with the spirit of foundational models and emphasizes the unification across sequence, structure, and function of the LAFs while respecting the interrelationships of each LAF as a node within the biological network.\nBy bridging existing knowledge gaps, this work seeks to equip bioinformaticians, biologists, clinicians, and computational researchers with an understanding of how LLMs can be effectively leveraged to tackle pressing problems in bioinformatics. Our survey not only highlights recent advances but also identifies open challenges and opportunities, laying the foundation for future interdisciplinary collaboration and innovation (Figure 1)."}, {"title": "2 Background of Language Models and Foundation Models in Bioinformatics", "content": "Bioinformatics has become a fundamental and transformative field in life sciences, bridging com-putational techniques and biological research. It emphasizes the development and application of computational tools and methodologies to manage and interpret vast amounts of biomedical data, transforming them into actionable insights and driving advancements across diverse downstream applications. Modern computational tools, particularly those rooted in deep learning technology, have significantly accelerated the evolution of biological research.\nThe rapid advancements in LLMs technologies have inspired new approaches to bioinformatics computing. Considering the complexity of biological systems and highly structured nature of bioinformatics data, LLM-based computing methods have proven effective in addressing challenges across fields such as genomics, proteomics, and molecular biology. Inspired by LLM architectures like transformers, foundation models in bioinformatics excel at capturing complex patterns and relationships in biological data. They have evolved from single-modality tools to sophisticated multimodal systems, integrating diverse datasets such as genomic sequences and protein structures.\nCentral to their success is the availability of large-scale, high-quality training data and the adoption of self-supervised pretraining and fine-tuning techniques. These methods allow models to extract meaningful features from unlabeled data and adapt to specific bioinformatics tasks. Together with advances in architecture design, these innovations have broadened the capabilities and impact of"}, {"title": "2.1 Foundations of Language Models and Bioinformatics Overview", "content": "Traditional language models are engineered to process and generate text in a human-like manner, leveraging the extensive datasets used during their training. These models excel at interpreting context, producing coherent and contextually appropriate responses, performing translations, summa-rizing text, and answering questions. LLMs are a type of foundation model trained on vast datasets to provide flexible and powerful capabilities [6, 7, 8] that address a broad spectrum of use cases and applications [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 22, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 8, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 35, 20, 51, 52, 21, 19, 53, 54, 55, 56, 57, 24, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78].\nBy efficiently handling diverse tasks, LLMs eliminate the need for building and training separate domain-specific models for each use case\u2014a process that is often limited by cost and resource constraints. This unified approach not only fosters synergies across tasks but also frequently results in superior performance, making LLMs a more scalable and efficient solution. There are several key elements that make the language model successful in adaptation to bioinformatics tasks (Figure 1(a))."}, {"title": "2.1.1 Basics of Large Language Models and Foundations Models", "content": "Traditional language models are engineered to process and generate text in a human-like manner, leveraging the extensive datasets used during their training. These models excel at interpreting context, producing coherent and contextually appropriate responses, performing translations, summa-rizing text, and answering questions. LLMs are a type of foundation model trained on vast datasets to provide flexible and powerful capabilities [6, 7, 8] that address a broad spectrum of use cases and applications [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 22, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 8, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 35, 20, 51, 52, 21, 19, 53, 54, 55, 56, 57, 24, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78].\nBy efficiently handling diverse tasks, LLMs eliminate the need for building and training separate domain-specific models for each use case\u2014a process that is often limited by cost and resource constraints. This unified approach not only fosters synergies across tasks but also frequently results in superior performance, making LLMs a more scalable and efficient solution. There are several key elements that make the language model successful in adaptation to bioinformatics tasks (Figure 1(a)).\nRepresentation learning and tokenization Tokenization in LLMs is influenced by the design of their tokenization algorithms, which primarily use subword-level vocabularies to represent text sequence data effectively. Popular tokenization algorithms, such as Byte-Pair Encoding (\u0392\u03a1\u0395) [79], WordPiece [80], and Unigram [81], are widely used. Although their vocabularies cannot perfectly capture every possible variation of input expressions, these tokenization methods effectively encode the features of words and their contextual relationships.\nIn the view of representation learning, the tokenization and token embedding algorithms of the language model generally succeeded in representing the hidden factors of variation behind the data. This representation is based on the unsupervised learning scheme of the language models. The sub-word context features learned in the encoder modules or embedding layers follow the probabilistic modeling and continuously update the representations on large corpus datasets [82].\nAttention mechanism LLMs widely use the transformer model [83, 84] as their foundational architecture. A core innovation of the transformer model is the multi-head self-attention mechanism, which establishes relationships among all relevant tokens, enabling more effective encoding of each word in the input sequence. The self-attention layer processes a sequence of tokens (analogous to words in a language) and learns context information across the entire sequence. The \"multi-head\" aspect refers to multiple attention heads operating simultaneously to capture diverse contextual features. Inside a single attention head, a token output embedding in a sequence is computed and fused with other tokens in the context with a proper causal mask. Such global level attention mechanic enables efficient information fusion along available context windows.\nSelf-supervised training methods Language models are trained using self-supervised learning methods [85]. Unlike supervised learning, which typically requires human annotations, language models can leverage vast amounts of unannotated text data [86]. The objective of unsupervised learning is to analyze unlabeled data by identifying and capturing its meaningful properties. Neural"}, {"title": "Pre-training", "content": "In many supervised learning problems, input data is represented by multiple features, comprising numerical or categorical information that can aid in making predictions. Scratch-trained models, which initialize and train all parameters from the ground up using task-specific datasets, typically require numerous iterations to converge fully on a single task. In general, transformer-based language models fall into two categories: scratch-trained models and pre-trained models. LLMs apply transformer-based pre-trained models that are trained from large amounts of unlabeled data and then fine-tuned for specific tasks. Pre-training learns general information from unlabeled data which can improve the convergence rate of the target tasks and often has better generalization than training parameters from scratch [94]. The use of context information in a large corpus to pre-train the whole model (or encoder modules) has achieved SOTA results in various downstream tasks."}, {"title": "2.1.2 Bioinformatics Applications and Challenges", "content": "Using deep learning methods like language models to tackle bioinformatics problems is challenging. While deep learning models have shown superior accuracy in specific bioinformatics applications (e.g., genomics applications) compared to SOTA approaches and are adept at handling multimodal and highly heterogeneous data, significant challenges remain. Further work is required to integrate and analyze diverse datasets required for deep learning for genomic prediction and prognostic tasks. This is especially important for the development of explainable language models that can identify novel biomarkers and elucidate regulatory interactions across various biology levels: pathological conditions, including different tissues and disease states. These advancements require a deep understanding of complex bioinformatics data, the related tasks, and their mutual relationships [95]. In this review, we discuss such issues through two lens: the various biology levels and the inherent regulations of life activities.\nVarious biology levels Although no gold standard division was available, the levels of life-science fac-tors in bioinformatics can be divided into five levels, from micro to macro. Here, we take the mammal model organisms as a template, the levels can be divided into: the molecular level, the genome-scale level, the cellular level, the tissue/organ system level, and the population/community/metagenomics level (Figure 1(b)). Bioinformatics often focuses on the first three levels (i.e., the molecular level, the genomic-scale level, and the cellular level). The molecular level analysis targets involved biologically active molecules, which include nucleic acids, amino acids, and other small bioactive molecules, and the relative experiments aimed at interpreting the life activities at this scale. The genomic-scale level models the life activities from DNA, RNA, and proteins to metabolomics. The most famous regula-tion at the genomic scale level is The Central Dogma, which reveals the intrinsic relations of main"}, {"title": "Inherent regulations of life activities", "content": "Since most LAFs at each biological level are represented in a sequence format, transformer-based pre-trained language models are particularly well-suited for analyzing these sequences. An emerging consensus suggests that these sequences embody an underlying language that can be deciphered using language models. However, in order to play the roles in life activities, an essential logic of a single LAF is 'sequences-structures-functions'. Take proteomics analysis as an example, protein sequences can be viewed as a concatenation of letters from the amino acids, analogously to human languages. The latest protein language models utilize these formatted letter representations of secondary structural elements, which combine to form domains responsible for specific functions. The protein language models also direct inference of full atomic-level protein structure from primary sequence and produce functional proteins that evolution would require hundreds of millions of years to uncover [99, 100, 101].\nIn life activities, there are important regulation relationships among the LAFs across different levels as well as intra-level relationships. Considering the genomics level, genes control hereditary traits primarily by regulating the production of RNA and protein products. According to the central dogma of molecular biology, genes within DNA are transcribed into messenger RNA (mRNA), which is then translated into gene products, such as proteins. For any given gene product, whether RNA or protein, its origin can be traced back to the gene that directed its synthesis. This traceability highlights that fully understanding a gene's functionality requires considering not only the gene itself but also the roles and functions of all its associated products. Genes regulate each other and create feedback loops to form cyclic chains of dependencies in gene regulatory networks, graph neural network-styled operations are suitable to model the \"steady state\" of genes. It is the same for proteins in protein-protein interactions (PPI). In the layer of pathways, it is a hypergraph where each hyperedge is a pathway including multiple proteins.\nWithin the cellular level, pathways integrate individual genes or protein products to perform certain cell functions under mutual intra-level regulations. Proteins interact with one another in various ways, such as inhibiting, activating, or combining with others, thereby influencing expression levels or protein abundances within cells. These interactions are collectively referred to as PPI. Some databases systematically organize results by annotating functionalities using Gene Ontology (GO),"}, {"title": "2.2 Training Methods and Models", "content": "Pre-training is a critical phase in the development of LLMs, where a model learns foundational linguistic representations by training on extensive and diverse datasets. This process typically employs self-supervised learning techniques such as masked language modeling (e.g., BERT [1]) or causal language modeling (e.g., GPT [2]), enabling the model to predict masked tokens or the next word in a sequence. Unlike traditional deep neural networks (DNNs) [104], which are often pre-trained on domain-specific datasets such as ImageNet [105], pre-training of LLMs is conducted on significantly larger datasets comprising diverse domains, including books, encyclopedias, and web content. Moreover, pre-training LLMs involves models with billions or even trillions of parameters, making it computationally and resource-intensive compared to conventional DNNs.\nThe primary advantage of pre-training lies in the model's ability to generalize across diverse language tasks, often achieving zero-shot [106] or few-shot [107] performance without additional task-specific training. This broad generalization enables LLMs to excel in tasks spanning natural language understanding, generation, and reasoning. However, the disadvantages of pre-training include high computational and energy costs, often requiring distributed systems with high-performance hardware. Additionally, pre-trained models can inherit biases and errors present in the training corpus [108], potentially leading to biased or undesirable outputs.\nFine-tuning is the subsequent stage that builds upon the pre-trained model by adapting it to specific tasks or domains through additional supervised or semi-supervised training. This process utilizes smaller, targeted datasets and optimizes the model for a specific use case. Fine-tuning can be categorized into task-specific fine-tuning [109, 110], where models are specialized for particular tasks such as sentiment analysis or machine translation; domain-specific fine-tuning [11, 38], which refines the model for specialized fields such as medicine or law; and instruction fine-tuning [62, 111], where the model is trained to respond to natural language prompts in an aligned manner. Recent advancements in parameter-efficient fine-tuning methods [112], such as LoRA (Low-Rank Adaptation) [113] and adapters [114], have further improved the efficiency of this process by updating only a subset of the model's parameters while maintaining the computational benefits of the pre-trained foundation.\nFine-tuning enhances the model's performance on specific tasks by leveraging domain- or task-specific data, achieving state-of-the-art results in various applications. However, it introduces challenges such as the risk of overfitting to the fine-tuning dataset, potentially diminishing the model's generalization capabilities. Furthermore, fine-tuning requires high-quality labeled data to ensure reliability and accuracy in specialized applications.\nReinforcement Learning with Human Feedback (RLHF) [115] represents a crucial additional stage in the training pipeline of large language models, designed to align model outputs with human preferences and expectations. While pre-training and fine-tuning equip the model with general linguistic understanding and task-specific expertise, RLHF optimizes the model's behavior to produce responses that are more aligned with human values, instructions, or conversational styles, which is particularly critical for applications such as conversational agents, where user interaction quality is paramount.\nRLHF involves three primary components: a reward model trained on human-labeled preferences, a reinforcement learning (RL) algorithm to optimize the model's behavior based on the reward model, and iterative human feedback to refine the reward system. The reward model is typically"}, {"title": "Knowledge Distillation.", "content": "Knowledge Distillation (KD) has emerged as a key approach for efficient training and deploying LLMs by transferring the knowledge embedded in high-capacity teacher models to smaller, more efficient student models [120]. In essence, the student model learns to mimic both the predictive outcomes and the internal representation patterns of the teacher, thereby significantly reducing computational costs and memory demands during the pre-training phase [121, 122, 123]. This methodology promotes the development of leaner LLMs without sacrificing their ability to perform complex language tasks.\nRecent advancements in KD extend beyond final-output matching. Modern methods utilize estab-lished LLMs to generate not only predictions but also detailed reasoning steps, which are often referred to as chain-of-thought sequences or intermediate logic traces [124, 125]. These rich anno-tations can then be incorporated into the fine-tuning process, enabling the target LLM to acquire deeper problem-solving skills and enhance interpretability without extensive manual labeling. By integrating these reasoning pathways, KD no longer serves solely as a compression mechanism but also imparts advanced critical thinking and inference capabilities to newly trained models. Moreover, recent work explores expanding KD to support specialized or domain-specific tasks where the established teacher models can guide the target LLM toward focusing on task-relevant knowledge, filtering out less pertinent information [126, 127]. This approach helps produce models that are better aligned with their intended applications. Additionally, a Bayesian perspective on KD has been introduced, offering a transparent interpretation of its statistical foundations and equipping the target model with robust uncertainty quantification capabilities [128, 129].\nThe integration of pre-training, fine-tuning, KD, and RLHF represents a comprehensive training paradigm for LLMs. Pre-training serves as the foundation, equipping the model with general knowledge and linguistic capabilities through large-scale unsupervised learning. Fine-tuning adapts the model to specific tasks or domains, enhancing its performance in targeted applications. KD supports efficiency by enabling the transfer of knowledge from established teacher models to target models, while RLHF refines the model's behavior to align with human preferences, ensuring outputs are both functionally accurate and socially acceptable. These stages are complementary and iterative. Insights gained during RLHF can inform improvements in fine-tuning datasets or methodologies, while advancements in fine-tuning and KD can enhance the quality of RLHF outcomes. Together,"}, {"title": "2.3 Bioinformatics-Specific Datasets", "content": "The rapid advancements in large language models have significantly propelled the development of bioinformatics by enabling more efficient data interpretation and knowledge extraction. LLMs excel in understanding, processing, and generating complex textual and numerical data, making them powerful tools for tasks such as sequence analysis, annotation, and predictive modeling [131, 132]. Leveraging bioinformatics-specific datasets, LLMs can further refine their understanding to address domain-specific challenges, transforming raw data into tangible, interpretable forms that accelerate research and innovation.\nQuestion Answering (QA) systems play a vital role in biomedicine, assisting with clinical decision support and powering medical chatbots. The development of robust QA systems relies heavily on diverse and well-curated datasets. Over the past decade, several biomedical QA datasets have been introduced, each targeting specific challenges and domains. For instance,, MedMCQA[133] and MedQA[134] focus on general medical knowledge, providing open-domain questions and multiple-choice answers derived from medical licensing and entrance exams. GeneTuring targets genomics-specific tasks, such as gene name conversion and nucleotide sequence alignment. Meanwhile, BioASQ[135, 136] and PubMedQA [137] incorporate supporting materials, such as PubMed articles, to answer domain-specific questions with formats ranging from yes/no to multi-class classifications. These datasets are crucial for benchmarking QA systems, as they provide domain-specific contexts and evaluation metrics that drive the development of more accurate and reliable models tailored to biomedical needs.\nText Summarization (TS) in the biomedical and healthcare is a critical application of natural language processing, enabling the condensation of complex medical texts into concise, informative summaries without compromising essential details. This task is particularly valuable in areas such as the summarization of the literature, the summarization of radiology reports, and the summarization of clinical notes. Among these, the summarization of radiology reports plays an essential role in transforming detailed imaging reports - including X-rays, CT scans, MRI scans, and ultrasounds - into easily understandable summaries. Datasets like MIMIC-CXR [138] are instrumental in advancing this field, providing a large-scale resource with 473,057 chest X-ray images and 206,563 corresponding reports. Such data sets are essential for training and evaluating summarization models, offering domain-specific content and structured formats that drive improvements in both accuracy and reliability, ultimately enhancing clinical workflows and decision making.\nInformation Extraction (IE) in biomedicine involves organizing unstructured text into structured formats through tasks like named entity recognition (NER) and relation extraction (RE). Robust IE systems rely on high-quality datasets for training and evaluation. For instance: datasets such as BC5CDR [139], NCBI-disease [140], ChemProt [141, 142, 143], DDI [144], GAD [145], BC2GM [146], and JNLPBA [147] have become benchmarks for NER and RE tasks, addressing challenges involving diseases, chemicals, genes, and other biomedical entities. These datasets are essential benchmarks for tackling real-world biomedical challenges, enabling the development of more accurate and generalizable models."}, {"title": "2.4 Model Evolution and Key Milestones", "content": "The evolution of LLMs in bioinformatics has marked a transformative journey. Initially developed for natural language processing tasks, these models, such as BERT [1] and GPT [158], have demonstrated remarkable potential in addressing challenges specific to the bioinformatics domain. Leveraging their ability to process and generate sequences, LLMs have been adapted for various biological data types, including DNA, RNA, proteins, and drug molecules [3].\nIn genomics, models like DNABERT [159] and GROVER [160] are trained on DNA sequences to predict functional regions, such as promoters and enhancers, and analyze mutations. Similarly, transcriptomics benefits from models like SpliceBERT [161] and RNA-FM [162], which assist in understanding RNA splicing and secondary structure prediction. For proteomics, PPLMs like ProtTrans[163] and ProtGPT2 [164] enhance predictions related to protein structure, function, and interactions. These advances are made possible by the foundational transformer architecture, which excels at processing sequential data. Fine-tuning these pre-trained models for domain-specific tasks extends their utility to applications drug discovery, where SMILES representations of molecules and protein sequences are integrated to predict interactions and properties.\nA notable breakthrough in bioinformatics has been the AlphaFold series, which has applied cutting-edge machine learning to solve protein structure prediction challenges. AlphaFold2 (AF2) revolu-tionized structural biology with its unprecedented accuracy in predicting protein structures based solely on amino acid sequences. Its attention-based deep learning architecture captured intricate protein folding patterns, surpassing traditional physics-based and homology-modeling methods. By leveraging evolutionary information through multiple sequence alignments (MSAs), AF2 provided reliable predictions even in the absence of experimental data, significantly reducing the time and costs associated with obtaining protein structural information, accelerating advancements in drug"}, {"title": "Key Features of AlphaFold3 Enhanced Accuracy in Complex Structures:", "content": "AF3 excels in predicting protein-peptide complex structures, achieving a high percentage of accurate models in challenging scenarios; Innovative Template-Free Modeling: While maintaining strengths in template-based predictions, AF3 introduces powerful template-free algorithms that allow for diverse model generation with reliable accuracy, even in the absence of homologous structural data; Sophisticated Scoring and Ranking: AF3 integrates advanced scoring metrics such as DockQ and MolProbity, ensuring accurate evaluation of predicted structures. Its models show fewer issues like twisted peptides or cis non-proline residues, reflecting improved protein-like properties and geometric quality.\nThe progression from AF2 to AF3 reflects the iterative refinement of computational methods to address increasingly complex biological problems. While AF2 focused on individual protein structures, AF3 emphasizes dynamic interactions within biological systems, signaling a shift toward a more holistic understanding of molecular biology. These innovations underscore how machine learning continues to redefine bioinformatics, enabling accurate and efficient modeling of protein structures and interactions. The AlphaFold series exemplifies the potential for transformative breakthroughs in biology and medicine, paving the way for future applications in understanding complex biological systems."}, {"title": "3 Applications in Bioinformatics Problems", "content": "At the heart of LLMs lies the transformer architecture, which leverages an attention mechanism to manage word importance in context without the traditional constraints of recurrent (RNN) or convolutional (CNN) neural networks. The self-attention mechanism of transformers not only allows for robust parallelization and scalability but also excels at capturing long-range dependencies in text. In bioinformatics, the growing availability of extensive datasets across diverse tissues, species, and modalities presents both an opportunity and a challenge. Bioinformatics analysis typically seeks to uncover hidden relationships within vast amounts of data, which can be broadly categorized into two formats: molecular and cellular. Molecular data often consist of sequences strings of four bases for DNA and RNA, and strings of twenty different amino acids for proteins. Cellular data, such as that from single-cell RNA-seq, single-cell ATAC-seq, or single-cell CITE-seq, typically takes the form of a count matrix with cells as rows and modalities as columns. While there are parallels between these data types and the structured data used in NLP, significant differences pose unique challenges for applying LLMs directly.\nA comprehensive LLM framework for bioinformatics involves three critical stages: data tokenization, model pre-training, and subsequent analyses. Due to the inherent differences between bioinformatics and conventional NLP data, researchers have been pioneering adaptations to the LLM architecture to better suit bioinformatics applications. The following section will provide a detailed overview of notable contributions in this evolving field."}, {"title": "3.1 Genome Level", "content": "Genome data primarily provide molecular-level insights, focusing on the sequences of DNA and RNA. This format bears a strong resemblance to natural language, as it is structured as ordered sequences of strings. In this analogy, each nucleotide in a sequence read is akin to a character, each read is akin to a sentence, and the entire genome is comparable to the full article. To bridge the genome sequence and natural language, multiple studies try several ways to tokenize the genome sequence to make it similar to the concept of \"word\" in the natural language. To gain deeper insights into the functionalities of various genome segments, most studies apply the BERT (Bidirectional Encoder Representations from Transformers) as the core model, which excels in understanding the functions of a genome segment in relation to its surrounding genome region and is easily extended to different specific tasks by fine-tuning the model with specific dataset."}, {"title": "3.1.1 LLM for DNA Analysis", "content": "In DNA analyses, biological sequences are encoded into structured tokens to facilitate effective model processing. A commonly adopted method involves tokenizing sequences into k-mers, typically ranging from 3 to 6 bases in length. This approach creates a vocabulary of k-mer permutations analogous to words in natural language, allowing the pre-trained model to decipher patterns within these k-mers. The choice of k directly affects the complexity and size of the resulting library, presenting a trade-off between modeling efficiency and accuracy.\nOne of the pioneering methods, DNABERT [159], tokenizes DNA sequence data using overlapping fixed-length k-mers, as well as the recently developed Nucleotide Transformer [167]. To enhance model efficiency, subsequent versions like DNABERT-2 [168] and GROVER [160] have employed Byte Pair Encoding (BPE) [79], a statistical compression technique that iteratively merges the most frequently co-occurring genome segments. This method extends beyond fixed k-mer lengths, significantly improving the efficiency and generalizability of the models. HyenaDNA [169] uses one-mer to tokenize the DNA sequence since it uses Hyena [170] as the core model, which allows much longer input than BERT. Additionally, some models integrate supplementary data into their tokenization process; for instance, DNAGPT [171] incorporates species information, and MuLan-Methyl [172] combines sequence and taxonomy data into a natural language-like sentence to fully leverage existing LLM capabilities.\nIn terms of pre-training approaches, many models utilize the BERT architecture with a masked learning method (MLM) for self-supervised training. To boost training efficiency, DNABERT incorporates the AdamW optimizer with fixed weight decay and applies dropout to the output layer. DNABERT-2 introduces enhancements such as Attention with Linear Biases (ALiBi) [173] and Flash Attention [174]. In contrast, the MuLan-Methyl framework integrates five fine-tuned language models (BERT and four variants) for the joint identification of DNA methylation sites, maintaining consistency with their original pre-training setups. DNABERT-S [175] develops a contrastive learning-based method to help effectively cluster and separate different species. Some methods adopt other LLM models. For example, DNAGPT uses a GPT-based model and the next-token prediction for its pre-training, enabling it to forecast subsequent tokens based on previous ones. HyenaDNA uses Hyena, a new LLM model that allows a longer context input, to study long-range genomic sequence properties.\nWhen applying these models to specific bioinformatics tasks, most integrate additional task-relevant data for fine-tuning. For instance, DNABERT and its derivatives utilize the Eukaryotic Promoter Database (EPDnew) [176] to predict gene promoters, the ENCODE database [177] for transcription"}, {"title": "3.1.2 LLM for RNA Analysis", "content": "Unlike DNA", "180": "mirroring the structure of DNABERT", "181": "RNA-MSM [182", "162": "utilize single nucleotides (one-mers) for tokenization. In addition to sequence tokenization", "183": "labels each sequence as positive or negative based on the presence of an RNA-binding protein (RBP) region", "184": "to preserve the evolutionary history of sequences.\nThe pre-training approach for RNA largely follows that of DNA", "185": "leveraging an MSA-transformer architecture. Depending on the target application", "datasets": "RNABERT and RNA-MSM use sequences from the Rfam database, RNA-FM utilizes non-coding RNA sequences from RNAcentral [186", "187": ".", "188": "."}]}