{"title": "GroCo: Ground Constraint for Metric Self-Supervised Monocular Depth", "authors": ["Aur\u00e9lien Cecille", "Stefan Duffner", "Franck Davoine", "Thibault Neveu", "R\u00e9mi Agier"], "abstract": "Monocular depth estimation has greatly improved in the recent years but models predicting metric depth still struggle to generalize across diverse camera poses and datasets. While recent supervised methods mitigate this issue by leveraging ground prior information at inference, their adaptability to self-supervised settings is limited due to the additional challenge of scale recovery. Addressing this gap, we propose in this paper a novel constraint on ground areas designed specifically for the self-supervised paradigm. This mechanism not only allows to accurately recover the scale but also ensures coherence between the depth prediction and the ground prior. Experimental results show that our method surpasses existing scale recovery techniques on the KITTI benchmark and significantly enhances model generalization capabilities. This improvement can be observed by its more robust performance across diverse camera rotations and its adaptability in zero-shot conditions with previously unseen driving datasets such as DDAD.", "sections": [{"title": "Introduction", "content": "Depth estimation is a fundamental task in computer vision, offering crucial 3D insights for various applications such as robotics, augmented reality and intelligent vehicles. Specifically, in the realm of intelligent vehicles, accurate depth perception is vital for navigating safely by identifying and localizing potential obstacles.\nAmong the various methods, monocular depth estimation is particularly attractive due to its cost efficiency and broad availability across many systems. It presents a viable alternative to more expensive technologies like Lidar and stereo vision, promising wide applicability in real-world scenarios. The advancement of robust monocular depth models, however, is hampered by the need for diverse, large-scale, annotated datasets, which are costly to create.\nIn response, there has been an increased interest in self-supervised learning, where models are trained using unlabeled data by leveraging the consistency of scene geometry across different viewpoints and moments in time. Nonetheless, the self-supervision brings its own set of complications, notably in deriving metric scale information. This problem aligns with the longstanding challenges in the field of monocular visual odometry [1], essential for self-supervised learning of depth. In fact, it is well-documented that the scale of the scene cannot be determined using only monocular images, introducing an inherent ambiguity in the predicted depth and egomotion, which are known only relative to an unknown scale factor. At its core, this issue arises because an image may correspond to various 3D scenes, differentiated only by their scale. This ambiguity is problematic as it obstructs applications that depend on precise distance measurements for decision-making.\nAdditionally, both self-supervised and supervised depth estimation methods often face the challenge of model overfitting to specific camera parameters [17]. Indeed, within the context of driving, monocular models often infer depth by correlating the vertical position of ground pixels with constant depths, a process heavily relying on unchanging camera intrinsic and extrinsic parameters. Such an approach significantly hampers the models' ability to generalize across different camera setups. Consequently, models require retraining or fine-tuning for each new camera configuration, which significantly limits their practical usability in diverse real-world environments.\nIn the context of ground-based systems like vehicles or robots, this limitation can be mitigated by using the theoretical flat ground as a reference since it can be deduced from commonly known camera parameters. Two strategies for integrating this ground information exist: the a posteriori method, which involves adjusting the scale of depth predictions during the inference phase [21], requiring additional processing steps and necessitating ground segmentation; and the a priori method, showcased in recent supervised learning approaches [12, 22], which embeds ground priors into the depth estimation model itself. This latter strategy equips the model with all necessary information for robust scaled depth prediction right from the start, aiming to enhance performance on all types of scenarios."}, {"title": "Related Work", "content": null}, {"title": "Monocular Self-Supervised Depth Estimation", "content": "Self-Supervised Depth estimation is a task that has already been widely studied in the past few years. The main idea is to train a model to predict the depth of a scene without labels by exploiting its geometry. The most common way to do this, is to use the simultaneous learning of depth and egomotion [5,8,18,26]. That is, the model is trained to predict the depth of the scene and the motion of the camera at the same time by computing the photometric error between the original image and the reprojected one from the predicted depth and motion. This is a very efficient way to train the model as it does not require any labels but uses the assumption of a static scene and a moving camera. Godart et al. [5] proposed a method that is robust when these hypotheses are not satisfied. They manage sequences where there is no egomotion by masking out pixels that do not change between frames and use a minimum loss across adjacent frames to handle dynamic objects.\nModel architectures have also been improved, Lyu et al. [15] enhanced the quality and sharpness of predicted depth, and recently, Transformer-based architectures have also been used to further increase performance [23,25]."}, {"title": "Scale Recovery", "content": "Metric depth is crucial for downstream tasks. However, since images do not naturally reflect scale changes, incorporating additional information during training is essential for retrieving depth at the correct scale.\nGuizilini et al. [8], for example, presented a method leveraging vehicle velocity to impose a scale on egomotion estimation, constraining the depth estimation to be scaled as well. Wagstaff and Kelly [19] proposed to scale the depth using the height of the camera. The process begins by training an up-to-scale model to derive relative depth. Following this, an unsupervised ground segmentation model is developed using the assumption that the bottom middle part of the image is the ground and fitting its relative depth to a plane. All pixels that are close to this plane are then considered as ground. In the subsequent stage, the scale is computed by fitting a plane on the segmented depth and scaling its normal vector with the camera height. New loss terms are then included in the optimization so that the model has to predict depth and egomotion that are equal to their scaled counterpart. By exploiting \"off-the-shelf\" ground and vehicle segmentation models, Kinoshita and Nishino [11] leverage the assumption of constant camera height to recover the scale of the scene. They especially use the fact that projecting vehicle points to the plane orthogonal to the ground always gives the same height even if the depth of objects changes. Combining this with the prior knowledge of the vehicle height allows to recover the camera height and the scale of the scene. Zhang et al. [24] recovers the scale using the IMU sensor combined with an extended Kalman filter (EKF) to provide motion at scale, constraining the depth to adopt the same metric scale. Xiang et al. [20] propose to recover the scale using the fact that in the KITTI dataset [16] the rectangular area in the middle bottom part of the image belongs to the ground. Combined with the camera height prior, it allows to determine the scale of the depth.\nWe notice that all methods that use the camera height rely on some form of ground segmentation, whether model or heuristic-based. This dependency introduces additional challenges in ensuring robustness across diverse scenarios, potentially restricting their usability.\nAdditionally, most of these models consider the scale as a constant since they only use their prior during inference, and they do not generalize well when a change of camera position should result in scale adjustments."}, {"title": "Ground Prior", "content": "Van Dijk and De Croon et al. [17] demonstrated that monocular models estimate depth in two ways: by leveraging the vertical position of the contact point between the object and the ground and through the assimilation of a size prior for objects, with the former having a more significant impact. They further highlighted the sensitivity of these methods to alterations in camera pose, leading to inaccuracies in ground recognition and consequently diminishing overall model efficacy.\nTo address this limitation, [12, 22] have proposed the integration of a ground prior to provide camera pose information to the model and predict robust metric depth through the use of supervised annotations.\nKoledi\u0107 et al. [12] employed a technique that transforms the ground plane into an embedding via a Fourier transform, which is then concatenated with encoder-derived features. This method trains on supervised synthetic data across a wide range of camera poses and thus exhibits robustness to these variations. It can be adapted to real data through a domain adaptation module and the utilization of stereo datasets.\nThe approach proposed by Yang et al. [22], on the other hand, normalizes the ground depth image and directly concatenates it with the input image. Additionally, the authors introduced a ground attention mechanism that works alongside the predicted depth to integrate the ground prior in the final output, termed Vanilla version. Subsequently, they presented an Adaptive version of their framework, capable of estimating the slope for each pixel within the ground prior, enhancing model accuracy in environments with uneven terrain. Their findings suggest that this method not only generalizes more effectively to unseen datasets but also maintains robustness against changes in image resolution. However, the slope estimation technique shows some limitations. In particular, its inability to adjust the horizon line restricts its applicability to merely offsetting existing ground pixels and consequently introduces issues with positive slopes. Besides, Fig. 2 illustrates a counter intuitive behavior of the ground attention mechanism that considers the bottom part of obstacles as ground.\nDespite the promise of these methodologies, they still require the use of stereo cameras or Lidar annotations to circumvent the scale issue inherent to monocular self-supervised settings an aspect that our work addresses directly."}, {"title": "Method", "content": "This section outlines our methodology, demonstrating how each component synergistically contributes to solving the scale of the scene and improving generalization across diverse camera setups and datasets, thereby advancing the capabilities of self-supervised learning in depth estimation."}, {"title": "Ground Plane", "content": "To provide the ground prior to the model we use the modifications proposed by [22] since the approach is very flexible and can be integrated with different types of neural network architectures such as CNN or Transformers.\nWe compute the location of the theoretical ground plane thanks to the camera parameters and height h.\nUsing the camera intrinsic K = $\\begin{bmatrix} f_x & 0 & C_x \\ 0 & f_y & C_y \\ 0 & 0 & 1 \\ \\end{bmatrix}$ and extrinsic E = [R | t] such that\n$\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}$ = R^{-1}(K^{-1}\\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} - t)\nWe can recover the depth of the ground $d_{u,v}$ for each pixels at position (u, v) with height y = h using the following formula:\n$d_{u,v} = \\frac{h-t_y}{\\frac{R_{3,1}}{f_x}(u-C_x) + \\frac{R_{3,2}}{f_y}(v - C_y) + R_{3,3}}$\nComputing the depth for all pixels and keeping only positive values, we obtain a ground depth image representing the distance from the camera to the theoretic ground at each pixel, ignoring obstacles and ground slope variations. This image is then normalized and directly concatenated with the input image as shown is Fig. 3.\nWe adapt the ground attention scheme from [22], utilizing its vanilla version. The principle is to allow the model to choose between its own predicted depth D and the ground prior G as can be seen in Fig. 3. It is done by adding a new attention map \u03b1 such that the final depth for each pixel i is obtained as follows:\n$D_i = (1 - \\alpha_i) \\cdot D_i + \\alpha_i \\cdot G_i$"}, {"title": "Scale Constraint", "content": "To ensure the accurate scaling of depth predictions, our approach incorporates two novel penalty terms during the training phase, as detailed in Fig. 4. These penalties are designed to more effectively leverage ground prior information, thereby guiding the model to implicitly estimate depth at the correct scale. The first penalty is an activation regularisation on the ground attention that ensures that it segments a minimum proportion of the ground. The second is a constraint loss that solves the scale issue and ensure that the attention correctly segments the ground area.\nThe regularisation addresses a fundamental challenge: without supervision, models do not automatically align the scale of their depth with the ground prior as they would when trained with labeled data, leading to the dismissal of the ground prior by the attention in favor of maintaining internal consistency in the depth estimates.\nTo counteract this tendency and promote the integration of the ground prior, we introduce a novel regularization term $\\mathcal{L}_{reg}$. This term is designed to encourage the model to incorporate the ground prior into its depth estimation process by penalizing the attention when it does not activate enough, bridging the gap created by the absence of direct scale references from annotations. However, since we do not want the model to take the ground depth everywhere, we only apply this regularisation up to a given threshold \u03c4 between 0 and 1, leading to the following formulation:\n$\\mathcal{L}_{reg} = \\frac{max(0, \\tau - \\frac{1}{N}\\sum{\\alpha_i})}{\\tau^2}$\nwith N the total number of pixels, $\\alpha_i$ the $i^{th}$ pixel of the attention map and $\\tau^2$ a normalization constant keeping the value in the unit interval.\nWe found that this formulation is much more robust to hyperparameters than using a classical regularisation while also being more intuitive to interpret. Indeed, \u03c4 represents the proportion of the ground prior that we are confident at identifying as the ground, typically road surfaces. To prioritize precision and ensure the integrity and scale of depth estimations, it is recommended that \u03c4 be set below the proportion of the optimal ground segmentation.\nSince \u03c4 changes depending on datasets, we propose a rule to compute its value based on the navigable area with respect to image dimensions H and W as well as the expected pathway width $P_w$ and camera height h:\n$\\tau = \\frac{P_w H}{4hW}$\nBuilding on this, the constraint loss $\\mathcal{L}_{const}$ ensures that the attention correctly activates on ground areas and that in these areas the predicted depth converges to the ground prior. The equality between the predicted depth and the ground prior is necessary so that the scale of the ground is correctly estimated and not degraded by the residual depth. Its effect is twofold: it penalizes the attention on pixels where the ground prior and depth are distant, and, at the same time, penalizes the depth on pixels selected by the attention to make it converge to the ground prior. It can be expressed as:\n$\\mathcal{L}_{const} = \\frac{1}{N} \\sum{\\alpha_i^2 |D_i - G_i|}$\nWe use an absolute distance instead of a relative one to ensure that the attention focuses on closer ground areas. These are more likely to meet the flatness criterion rather than distant areas where this assumption may not hold. The attention is squared to penalize uncertain areas less and allow for a better quality depth prediction as opposed to the raw value that can cause the model to predict more binary attention maps.\nWe also use the reprojection loss $\\mathcal{L}_{reproj}$ and smoothness loss $\\mathcal{L}_{smooth}$ from [5] to ensure that the model can estimate the geometry of the scene and correctly propagate the ground scale everywhere, resulting in the final loss:\n$\\mathcal{L} = \\mathcal{L}_{reproj} + \\lambda_{smooth}\\mathcal{L}_{smooth} + \\lambda_{const}\\mathcal{L}_{const} + \\lambda_{reg}\\mathcal{L}_{reg}.$\nNote that, compared to the adaptive method described in [22], we do not let the model predict the slope of the ground. This is for two reasons. The first is that if we give a new degree of freedom to the model, there would be no guarantee that it would converge to a metric depth. And the second is that it is not strictly necessary, since in case the ground is not flat, there is nothing stopping the model from simply discarding the area in the attention and predicting the correct depth."}, {"title": "Rotation Augmentation", "content": "To improve the robustness of the model, rotation augmentation is applied during training to both the images and coherently to the ground. This augmentation simulates camera pose changes and helps the model learn to handle different orientations of the scene. We focus on rotations since they can easily be simulated by warping images, compared to translations that would require to know a dense ground truth depth which is contradictory to the self-supervised setup.\nWe limit angles amplitudes to 5\u00b0 for pitch and roll and 15\u00b0 for yaw to not introduce any black borders or upscaling in images. The ground depth is also augmented to match the rotated images by directly applying the rotation on the camera extrinsic, avoiding interpolation errors. Illustrations of these augmentations can be seen in Fig. 5.\nIn the same way, we also transform the Lidar depth used to evaluate the model to match the rotated images. This is done by rotating the Lidar point cloud and projecting it to the image plane to obtain the new depth."}, {"title": "Interpretability", "content": "Thanks to the ground attention mechanism, the prediction of the model can be reliably interpreted as seen in Fig. 6. By providing the area where the ground prior and the predicted depth are equal, we can detect failure cases of the model. This could typically be the case with images where the ground is not visible or if it"}, {"title": "Experiments", "content": null}, {"title": "Implementation Details", "content": "By default we follow [5] and use a Resnet50 encoder [9] pretrained on the Imagenet dataset [2] and the same decoder coming from [6]. To take the additional inputs from the ground embedding channel, the pretrained weights are kept and the weights of the new channel are initialized with a value of zero. We adapt the outputs of the decoder, replacing the sigmoid by the softplus function to directly predict a strictly positive depth coherent with the ground prior. We also add a new head using the features at all resolutions to predict the attention map similarly to [22].\nFor the hyper-parameters, we keep the default $\\lambda_{smooth} = 10^{-2}$ and set $\\lambda_{const} = \\lambda_{reg} = 0.1$. \u03c4 is set to 0.25 on KITTI and 0.5 on DDAD to reflect the difference in image width and corresponds to a pathway width of two 2.75m wide lanes.\nThe model is trained using the Adam optimizer [10] with a learning rate of $10^{-4}$ and a batch size of 12 for 20 epochs on KITTI [4]. On an NVIDIA RTX 3090, it takes about 10 hours for the training to finish."}, {"title": "Datasets", "content": "We use the KITTI dataset extensively since it is the standard for depth estimation in the use case of intelligent vehicles. We report results using the eigen split using both the original Lidar data [4] and the improved depth coming from the KITTI depth benchmark [16]. Unless specified, we will report results on the improved version since it more accurately represents the model performance.\nWe also use the DDAD dataset [8] to show the generalization of our model to new datasets and cameras. Similarly to [7] we use the front, back, front left and front right cameras to evaluate the model."}, {"title": "Performance", "content": "We first compare our approach to the state-of-the-art methods that only use the camera height to recover the scale of the scene similarly to us. We report standard metrics used for depth evaluation coming from [3]: AbsRel (Absolute Relative Error), SqRel (Squared Relative Error), RMSE (Root Mean Squared Error), $RMSE_{log}$ (Root Mean Squared Log Error), $\\delta < 1.25$, $\\delta < 1.25^2$ and $\\delta < 1.25^3$. \u03b4 metrics are accuracy measures and count the proportions of pixels where their ratio with the ground truth is inferior to 1.25\"."}, {"title": "Robustness to Camera Position Changes", "content": "In order to evaluate our method against a comparable one, we propose a new baseline using the default Monodepth2 [5] pipeline in addition of losses proposed by [19] and leveraging the ground prior to estimate the scale, the method is detailed in the supplementary material. We compare the performance of both models on the KITTI dataset with different camera poses. Both methods were trained with augmentation at training time. Results are reported in Tab. 2. We can see that our method performs better than the baseline for all rotations even though they perform very similarly on original images, demonstrating the gain of using our method to exploit the ground prior. For yaw and roll, we report positive values only since negative ones perform similarly. For the pitch we use the negative one because the positive augmentation leads to the ground not being visible in the image, rendering our method ineffective.\nWe also report the camera transfer performance against supervised methods in Tab. 3 and show that our method is able to better generalize to new cameras."}, {"title": "Generalization to New Datasets", "content": "We further evaluated the generalization capacity of our model by training it on the KITTI dataset and measuring its performance on the DDAD dataset. We report the results in Tab. 5. These results are evaluated up to 80m and with an image height of 192 pixels like in the KITTI benchmark. We can see that our model generalizes better to the new dataset than the baseline for all cameras. We also notice that even if our model never saw images of side cameras, its attention is quite robust at segmenting the ground as can be seen in Fig. 8.\nTab. 3 compares our method against the supervised results reported in [22], using the same modalities as them. Point cloud reconstruction of our model are also demonstrated in Fig. 9. We see that despite having close to 8 times less parameters, the performance is quite similar to the supervised methods but vary depending on the metric used. We suspect that this gap comes from the fact that in DDAD the \"ego-vehicule\" is visible in images from the back and side cameras, potentially impacting models performances differently."}, {"title": "Limitations and Future Work", "content": "Our approach is designed specifically for ground-based vehicles, leveraging the ground as a critical prior. This necessitates the ground's visibility within the field of view and presupposes the existence of at least a partially flat ground, which may limit its effectiveness on uneven terrains. This limit could be alleviated by propagating the scale across time to make sure that even if the flat ground is not visible for some time, the accuracy of depth can be conserved.\nAdditionally, our model depends on the parameter \u03c4, essential for successful training. Although Fig. 8 indicate that the model can adjust during inference to images with a smaller proportion of flat ground than \u03c4, the parameter stills needs to be set manually for each dataset in the training phase.\nFuture work could explore strategies to relax this constraint and enhance the ground attention mechanism's recall without sacrificing precision, which is vital for maintaining accurate scale estimation."}]}