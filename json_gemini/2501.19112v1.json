{"title": "Logical Modalities within the European AI Act: An Analysis", "authors": ["Lara Lawniczak", "Christoph Benzm\u00fcller"], "abstract": "The paper presents a comprehensive analysis of the European AI Act in terms of its logical modalities, with the aim of preparing its formal representation, for example, within the logic-pluralistic Knowledge Engineering Framework and Methodology (LogiKEy). LogiKEy develops computational tools for normative reasoning based on formal methods, employing Higher-Order Logic (HOL) as a unifying meta-logic to integrate diverse logics through shallow semantic embeddings. This integration is facilitated by Isabelle/HOL, a proof assistant tool equipped with several automated theorem provers. The modalities within the AI Act and the logics suitable for their representation are discussed. For a selection of these logics, embeddings in HOL are created, which are then used to encode sample paragraphs. Initial experiments evaluate the suitability of these embeddings for automated reasoning, and highlight key challenges on the way to more robust reasoning capabilities.", "sections": [{"title": "1 Introduction", "content": "In the last decade, the representation of, and the reasoning with, legal information has gained growing attention in the AI and Law community. Alongside established approaches like Akoma Ntoso [27] and LegalRuleML [3, 28], research is currently being invested in the exploration and application of the logic-pluralistic knowledge engineering framework and methodology LogiKEy [9], which develops computational tools for normative reasoning based on formal methods.\nAn interesting and relevant use case for the LogiKEy framework is the European AI Act [16], which came into force in August 2024. The AI Act classifies AI systems into three risk levels, and assigns different rules and regulations to each category. If successful, the logical formalisation of key concepts of the AI Act within the LogiKEy framework could, for example, prepare the way for sophisticated automated or interactive compliance checks and support in various ways the technical enforcement of the new regulation. It is not only the AI Act itself that is of interest, but also the specific follow-up regulations and standardisations that will be triggered by the AI Act, which will then impose specific regulatory constraints on concrete applications of AI systems.\nBefore adequate logical representations can be developed, it is essential to identify the modalities that occur in the AI Act and the logics, or combinations of logics, suited to adequately represent these. This paper addresses this challenge as its main contribution. In the long run, however, it is rather the concrete instantions of the abstract legislation of the AI Act, i.e. the more concrete laws that are expected to follow from the adoption of the AI Act in the following years, that are relevant and interesting for formalisation. We expect that the findings on the identified modalities will (to a large extent) overlap and apply to them as well.\nThe paper is organized as follows. Section 2 lists and discusses the modalities found in the relevant parts of the AI Act. The findings of this section are relevant beyond the LogiKEy context. Section 3 explores potential logic systems for representing these modalities. Section 4 briefly presents preliminary results on embedding selected logics in the LogiKEy framework and using them to formalize some exemplary sections of the AI Act. Section 5 concludes the paper."}, {"title": "2 Modalities in the AI Act", "content": "This section presents the modalities (and other challenges) identified in the AI Act and illustrates them with examples. In order to find all the modalities in the document, it was read several times, with different foci of interest, and visualized with the help of mind maps and tables."}, {"title": "2.1 Obligations, Prohibitions, and Permissions", "content": "The AI Act is a European piece of legislation intended to regulate the use of AI systems. In it Obligations are expressed with the word \"shall\", as in this sentence from Article 8:\n\"High-risk Al systems shall comply with the requirements established in this section.\" [16, \u00a78(1)]\nThe negated shall, \"shall not\", is employed to express Prohibitions. An example is:\n\"Where an importer has sufficient reason to consider that a high-risk AI system is not in conformity with this Regulation, or is falsified, or accompanied by falsified documentation, it shall not place the system on the market until it has been brought into conformity.\" [16, \u00a723(2)]\nFor the expression of Permissions, two strategies can be identified in the AI Act: They are introduced by either \"may\" or \"is/are empowered to\"."}, {"title": "2.2 Contrary-to-Duty-Obligations", "content": "A special type of obligations that occurs within the AI Act deserves attention: contrary-to-duty obligations (CTDs) are obligations that arise only if a primary obligation is not fulfilled, meaning that it is \"conditional on [...] violating [a] primary obligation\" [25].\nAn example of a CTD in the AI Act can be found in Article 20:\n\"Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate.\" [16, \u00a720(1)]\nIt has been stated before that high-risk systems must comply with the requirements [16, \u00a716(1)]. This is the primary obligation in the given context. However, obligations are not always fulfilled; they may also be violated. In the event of such a violation, the obligation to take corrective action becomes relevant. This is a CTD, applicable only when the primary obligation has been violated. CTD situations are often represented in a typical structure following Chisholm [14]. The discussed example then looks as follows:\n(1) It ought to be that high-risk Al systems comply with the requirements in the regulation.\n(2) It ought to be that if a high-risk AI system does not comply with the requirements, providers take corrective actions.\n(3) If a system complies with the requirements, the provider must not take any corrective actions.\n(4) Concrete Situation: The system does not comply with the requirements.\nNotice that some of these obligations in this example are agentive for the provider. The aspects of agency and agentive obligations are discussed separately in Section 3. First, two special kinds of CTDs are considered."}, {"title": "2.2.1 Contrary-to-Duty-Obligations Involving Multiple Agents", "content": "Some CTDs within the AI Act relate to multiple agents, which complicates matters, as illustrated by the following example from \u00a736:\n\"If the notifying authority comes to the conclusion that the notified body no longer meets the requirements laid down in Article 31 or that it is failing to fulfil its obligations, it shall restrict, suspend or withdraw the designation as appropriate, depending on the seriousness of the failure to meet those requirements or fulfil those obligations.\" [16, \u00a736(4)]\nFor simplicity, the notion of an agent belief, expressed in the words comes to the conclusion that, is ignored (since beliefs are the subject of Section 4). Also, disregard the temporal notion contained in no longer, since CTDs involving temporality are discussed in Section 2.2.2.\nThe focus here lies on the involvement of two distinct agents: The notifying authority and the notified body. The situation is as follows: The notified body has several primary obligations that are specified within the AI Act [16, \u00a731]. If it violates these obligations, another obligation arises. However, this obligation is agentive for the notifying authority, not for the notified body that violated the primary obligation."}, {"title": "2.2.2 Contrary-to-Duty-Obligations Involving Temporality", "content": "There are cases in the AI Act where CTDs occur in combination with temporal constraints, stating that a certain property used to be fulfilled but no longer is. An example is:\n\"Where a notifying authority has sufficient reason to consider that a notified body no longer meets the requirements laid down in Article 31, or that it is failing to fulfil its obligations, the notifying authority shall without delay investigate the matter with the utmost diligence.\" [16, \u00a736(4)]\nTo understand the CTD, \u00a730 must be taken into account. There it is stated that the notifying authority \"may notify only conformity assessment bodies which have satisfied the requirements laid down in Article 31\" [16, \u00a730(1)]. Unfortunately, this example does not involve only temporality but also multiple agents as discussed in the Section 2.2.1. To focus on only temporality, we reformulate the regulation as follows:\nOnly conformity assessment bodies that fulfill the requirements in Article 31 before the notification may be notified (adapted from Article 30). If a conformity assessment body that was notified (= notified body) no longer meets the requirements laid down in Article 31, the matter shall be investigated further with the utmost diligence (adapted from Article 36).\nNow it becomes visible how this example is different from a typical CTD: Technically speaking, the notified body is only obligated to fulfill the requirements in Article 31 at one point in time before being notified. Hence, the fact that a notified body no longer fulfills the requirements in Article 31 does not equal a violation of the previous obligation. The obligation to further investigate the matter then is just a normal obligation, not a CTD, and could be expressed as such. In this case, the logic representing this situation must not only provide adequate obligation operators, but must also be able to express temporality in order to properly capture the notion of before (as discussed in Section 2.5).\nAnother possibility is disregarding the temporality here and translating the example to match the usual CTD structure:\n(1) It ought to be that notified bodies fulfill the requirements in Article 31.\n(2) It ought to be that if a notified body does not comply with the requirements in Article 31, further investigations are started.\n(3) If a notified body does comply with the requirements in Article 31, no further investigation must be started.\n(4) Concrete Situation: A notified body does not fulfill the requirements in Article 31.\nReformulated as above, the example can be treated like a typical CTD."}, {"title": "2.3 Agency and Agentive Obligations", "content": "Since different agents interact with an Al system during its lifetime, it is plausible that these agents have different duties towards the system based on their relation to it. Such obligations then are not general but agentive: They only hold for a specific type of agent, e.g. for providers or importers. For instance, the sentence\n\"Providers of high-risk AI systems shall (...) have a quality management system in place which complies with Article 17\" [16, \u00a716(c)]\nis such an agentive obligatio because it expresses what the provider ought to do. Agentive obligations are different from general obligations, and must be distinguishable."}, {"title": "2.4 Beliefs of Agents", "content": "Apart from agentive obligations, another modality relating to agency is crucial in the AI Act: Agent beliefs. For example, Article 20 states:\n\"Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate.\" [16, \u00a720(1)]\nHere, the provider's belief that a high-risk Al system is not in conformity with the regulation is relevant, not whether the system is, in fact, not conforming. Beliefs can be either right or wrong, and it is essential to differentiate them from facts."}, {"title": "2.5 Temporality and Temporal Notions", "content": "Time is an important concept within the AI Act, with many articles specifying obligations that must be fulfilled before certain events occur. For instance, Article 23 states:\n\"Before placing a high-risk AI system on the market, importers shall ensure that the system is in conformity with this Regulation by verifying that (...) \"[16, \u00a723(1)]\nThis obligation for importers is tied to a specific point in time; it must be fulfilled before the system is placed on the market.\nAdditionally, some sentences contain hidden temporal notions, such as this phrase in Article 9: \"When implementing the risk management system as provided for in paragraphs 1 to 7, providers shall...\" [16, \u00a79(9)]. The sentence implies simultaneity which becomes obvious if it is rephrased as follows: \"While implementing the risk management system, providers are obligated to (...).\""}, {"title": "2.6 Exceptions", "content": "Within the AI Act, the concept of exceptions from general rules emerges as a recurring theme. Consider this sentence from Article 5:\n\"The following AI practices shall be prohibited: (...) the use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement, unless and in so far as such use is strictly necessary for one of the following objectives:...\" [16, \u00a75(1)]"}, {"title": "2.7 Fuzziness", "content": "Fuzzy statements can be found in many places within the AI Act. They can be identified by their vagueness and usually involve statements that are not true or false, but true or false to a certain degree. The following expressions are examples of such fuzziness:\n\"unless and in as far as such use is strictly necessary for one of the following objectives\" [16, \u00a75(1)], \"to the extent to which...\" [16, \u00a77], and \"at a minimum\" [16, \u00a711(1)].\nAll these statements express vague notions or involve degrees of truth or necessity."}, {"title": "2.8 Interactions and Combinations of Different Modalities", "content": "The modalities of the AI Act do not appear in isolation. Some of the examples discussed in Section 2.2 have already shown that two or more modalities are often combined within a single sentence.\nWhile this was ignored in the previous discussion for the sake of simplicity, it cannot be ignored when constructing a logical system intended to cover the entire AI Act, or larger coherent parts of it. Such a system must not only accurately represent all the relevant modalities individually, but also capture what happens when they are combined."}, {"title": "3 Which Logics?", "content": "In this section we discuss logics that seem suitable to facilitate a representation of the AI Act in HOL.\nStandard Deontic Logic (SDL) is the most studied system of deontic logic. It contains a monadic deontic obligation operator depending on an accessibility relation and can successfully express and reason with obligations, permissions, and prohibitions [25]. However, SDL reaches its limits and leads to inconsistencies in CTD scenarios [18].\nA logic that enables expressing and reasoning with CTDs is Dyadic Deontic Logic (DDL) by Carmo and Jones [12, 13]. It differentiates between ideal and actual obligations and introduces a conditional obligation operator, thereby enabling the expression of CTDs that arise when ideal obligations have been violated. Related to DDL is \u00c5qvist's system E for conditional obligation [2].\nTo represent agentive obligations, a modal logic of agency is needed. While there are many ideas for formulating such a logic [1, 11, 17, 23, 37, 38], the most prominent theory of agency is a branch called Seeing-To-It-That (STIT) logic that originated in the works of Belnap [5] and Belnap and Perloff [6]. STIT theory introduces a STIT operator of the form\n$\\textit{stita}F$\nwhich expresses that an agent a sees to it that F holds. Given that agency appears in the AI Act mainly via agentive obligations, a suitable agency logic must be able to formulate ought-to-do statements for specific agents. Several authors have researched this issue and presented their approaches, which could be apt to help represent the AI Act [21, 22, 26, 36, 39]."}, {"title": "4 Initial Experiments", "content": "In this section we give a brief summary of the findings and results from the experiments on embedding some of the logics using the LogiKEy approach and using these logics to represent parts of the AI Act; for details see [4]. Experiments have so far focused on only a selection of the identified modalities, namely obligations, prohibitions, permissions, CTDs, agency, and agentive obligations.\nLogiKEy leverages higher-order logic (HOL) as a unifying meta-logic to enable the modelling of diverse object logics via shallow semantic embeddings [7]. This integration is facilitated, for example, within the higher-order proof assistant tool Isabelle/HOL, which comes with state-of-the-art automated theorem provers (ATP), satisfiability modulo theories (SMT) solvers, and model finders; however, other proof assistant systems, such as Lean or Coq, or the T\u0420\u0422\u0420 infrastructure, could be employed instead.\nTo represent obligations (as well as permissions and prohibitions), SDL was identified as a suitable logic in Section 3. A trustworthy embedding of SDL in Isabelle/HOL using the LogiKEy approach already exists [8] and has been used to represent parts from the AI Act, particularly Article 5 on prohibited AI systems. This was successful: The paragraphs could be adequately represented, and both Sledgehammer (an automated theorem proving tool integrated with Isabelle/HOL) and Nitpick (a model finder integrated with Isabelle/HOL) could reason with them correctly [4]. Sledgehammer was used to automatically check the validity of example judgements, and Nitpick to find countermodels for invalid judgements and also to prove the consistency of whole contexts.\nLike SDL, DDL already has a faithful embedding in Isabelle/HOL [10]. Using this embedding it was possible to represent selected CTDs from the AI Act without problems. The formalization of the selected examples from the AI Act was straightforward, and Nitpick and Sledgehammer could work with them successfully [4]. Since DDL can express everything SDL can and more [18], the DDL embedding is suitable for representing obligations, prohibitions, permissions, and CTDs. Encodings of Article 5 and selected CTDs from the AI Act can be found in the Appendix. Further work includes similar experiments adapting an existing embedding of \u00c5qvist's system E in Isabelle/HOL [29].\nAdditionally, logics for the representation of agency and agentive obligations were explored. In particular, an embedding into HOL of Temporal Deontic STIT Logic [36], a logic appropriate for representing both actions of agents and agentive obligations, was created. While most axioms postulated in [36] were provable via Sledgehammer and none were refuted, Nitpick could not find a model confirming the consistency of the embedding. This indicates that TDS logic has (presumably) only infinite models, making it unsuitable for model finding/checking in the context of any formalization of AI Act [4] with existing tools.\nAs an alternative to TDS, the authors explored the possibility of extending DDL with additional operators for the representation of agency and agentive obligations. Two different variants were introduced: One representing agent types as constants, with separate accessibility relations and obligation operators for each agent, and one representing agent types as function types, with a general accessibility relation and a general obligation operator taking an agent as an input parameter. Both variants introduced a STIT-like operator as a constant and equipped it with minimal axiomatization, namely to ensure that anything an agent sees to actually holds [4].\nWhile the first variant is more restrictive, with agentive obligations always applying to a generic agent type without the possibility of specifying further constraints or relations, the second variant is more flexible, allowing groupings based on characteristics of agent types or relations and formulating obligations for specific subsets of agents. Unfortunately, the second variant performs worse in other aspects: It struggles with the representation of CTDs, and Nitpick can not find a model for cardinalities of i (domain of possible worlds) bigger than 1. For the first variant, a model is found up to a cardinality of i=2, and CTDs can be represented without problems. For both variants, all but one of the prominent lemmas for DDL as studied by Benzm\u00fcller et al. could be proven [4]. Whether either of the variants can be adapted or simplified to perform better remains an open question. The embedding of TDS and the two extensions of DDL can be found in the Appendix."}, {"title": "5 Conclusion", "content": "An extensive analysis of the AI Act was carried out, which led to the identification of different contained modalities that pose a challenge to attempts to formalise and reason with it: obligations, permissions, prohibitions, CTDs, agency and agentive obligations, beliefs of agents, temporality and temporal notions, fuzziness, and exceptions. Suggestions for logics suitable to represent these modalities were presented, and results of some first experiments with these logics and examples from the AI Act were reported. As of now, experiments have only been conducted for the following modalities (and some combinations of) : obligations, permissions, prohibitions, CTDs, agency, and agentive obligations.\nWhile the presented project is not yet completed, the reported material provides a starting point and references for future research. In the LogiKEy context, the challenge is to create embeddings of the suggested logics, choose and provide suitable logic combinations, and experiment with increasingly larger example parts of the AI Act.\nBased on the information gathered here, the possibility of representing legal information in HOL and using it in automated reasoning should be further explored and improved."}]}