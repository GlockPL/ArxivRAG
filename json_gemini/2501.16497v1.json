{"title": "Smoothed Embeddings for Robust Language Models", "authors": ["Ryo Hase", "Md Rafi Ur Rashid", "Ashley Lewis", "Jing Liu", "Toshiaki Koike-Akino", "Kieran Parsons", "Ye Wang"], "abstract": "Improving the safety and reliability of large language models (LLMs) is a crucial aspect of realizing trustworthy AI systems. Although alignment methods aim to suppress harmful content generation, LLMs are often still vulnerable to jail-breaking attacks that employ adversarial inputs that subvert alignment and induce harmful outputs. We propose the Randomized Embedding Smoothing and Token Aggregation (RESTA) defense, which adds random noise to the embedding vectors and performs aggregation during the generation of each output token, with the aim of better preserving semantic information. Our experiments demonstrate that our approach achieves superior robustness versus utility tradeoffs compared to the baseline defenses.", "sections": [{"title": "Introduction", "content": "Enhancing the safety and reliability of large language models (LLMs) is an important and multi-faceted challenge in the path towards realizing trustworthy AI systems. The DecodingTrust framework (Wang et al., 2023) identifies a variety of trustworthiness concerns, such as toxicity, stereotype bias, privacy, fairness, and adversarial robustness. LLMs are typically trained and/or fine-tuned with alignment methods that aim to follow ethical guidelines and prevent harmful content genera-tion (Ouyang et al., 2022). As LLM and AI systems are increasingly adopted, their rapidly growing impact incentivizes more sophisticated attacks, which motivates the urgent development of practical and resilient defenses.\nSimilar to other neural network models, LLMs are vulnerable to adversarial inputs (Szegedy et al., 2014; Goodfellow et al., 2015), which can enable \u201cjailbreaking attacks\" that subvert alignment and induce harmful outputs. For example, while an aligned LLM would typically refuse to answer a chat prompt asking for harmful generation, such as \"how to make a bomb?\", a jailbreaking attack can leverage adversarial perturbations to the prompt text to induce the LLM to comply with harmful content generation that violates the ethical standards of its alignment.\nGreedy Coordinate Gradient (GCG) is an example jailbreaking attack that optimizes a general and transferable adversarial suffix that subverts alignment (Zou et al., 2023). Prompt Automatic Iterative Refinement (PAIR) is another jailbreaking attack that employs a separate LLM to produce adversarial prompts, which are semantically close to the original prompts, in a process inspired by social engineering attacks (Chao et al., 2023). The attack framework of (Andriushchenko et al., 2024) incorporates Random Search (RS) for efficient adversarial prompt generation, along with other techniques, such as a manually crafted attack template and self-transfer (warm starts with"}, {"title": "Preliminaries", "content": "At the high level of abstraction, we denote the generation of a response $y := (y_1, y_2, \\dots) \\in X^*$ from a prompt $x := (x_1,x_2,...) \\in X^*$ with an LLM as a (potentially probabilistic) mapping $F: X^* \\rightarrow X^*$, where $X$ denotes a finite set of tokens (i.e., the vocabulary), $X^*$ denotes the set of token sequences of arbitrary length, and the input and output token sequences are related by $y = F(x)$. While the sequence lengths may vary, in practice, there is a maximum length imposed on both, due to computational limitations.\nThe simple notation $y = F(x)$ is convenient to denote the generation process while omitting the details. However, in order to explain our methodology, we use additional notation to detail the autoregressive generation procedure. The initial step is to apply the token embedding mapping $E : X \\rightarrow \\mathbb{R}^d$, where $d$ is the embedding dimensionality, to the input tokens $x := (x_1,...,x_n)$ to produce a sequence of embedding vectors $e := (e_1,...,e_n) = (E(x_1),..., E(x_n))$. We denote the rest of the LLM, with the mapping $f : \\mathbb{R}^{d\\times*} \\rightarrow \\mathbb{R}^{|X|}$, which, in typical transformer-based architectures, consists of positional embedding and a series of multi-headed attention, normalization, and feed-forward modules. The mapping $f$ takes the sequence of embedding vectors $e$ as input, and outputs a distribution (expressed as a logit vector) over the token set $X$, indicating the likelihoods of the next token that should follow the input. Figure 7 in the Appendix illustrates the standard autoregressive generation procedure.\nTo clarify our notation and conventions, we note that the typical autoregressive generation (without any defense) is obtained as a special case of our method, described in Algorithm 1, by setting the prefix smoothing length $l = 0$, and skipping all lines involving the perturbation function $H_o$ or sample parameter $k$. In this work, for simplicity, we restrict our investigation to greedy token selection."}, {"title": "Related Work", "content": "There have been a variety of defenses proposed in the literature, which have been recently surveyed by (Jain et al., 2023). The following LLM defense methods are similarly inspired by randomized smoothing: Randomized Smoothing with Masked Inference (RSMI) (Moon et al., 2023), SelfDe-noise (Zhang et al., 2023), Erase-and-Check (Kumar et al., 2023), SmoothLLM (Robey et al., 2023), Semantic Smoothing (Ji et al., 2024), and RigorLLM (Yuan et al., 2024). RSMI and SelfDenoise are specifically applied to language classification. The others are defenses against jailbreaking in text generation, however (except for SmoothLLM) they require an auxiliary LLM to perform either response judging or prompt paraphrasing, which adds significant computational complexity and raises concerns about also defending this secondary model.\nAnother class of defense strategies aims to detect and filter out attack prompts and/or harmful content generated in the responses, such as via Llama-Guard, as mentioned earlier. Perplexity filtering (Jain et al., 2023; Alon & Kamfonas, 2023) can readily isolate some attacks (such as GCG) that produce high-perplexity prompts, but are less effective against other attacks. The PARDEN defense (Zhang et al., 2024) is a form of self-filtering, where the target model is instructed to repeat its own output and the presence of an attack can be inferred from a drop in a self-consistency.\nSmoothLLM applies random character perturbations to the input sequence $x$ to produce $k$ noisy samples of the input sequence, $x^1, ..., x^k$. Their exemplary perturbation method is to randomly select characters to perturb with probability $q \\in [0, 1]$ (i.e., the perturbation rate parameter) and swap"}, {"title": "Randomized Embedding Smoothing", "content": "We propose Randomized Embedding Smoothing and Token Aggregation (RESTA), which applies random noise to the embedding vectors to realize a defense analogous to randomized smoothing. By operating in the embedding domain, our approach aims to retain the semantic information of the original prompt, while disrupting the presence of adversarial input perturbations. In contrast to other methods, our efficient approach does not require a separate, auxiliary LLM to perform perturbation or judgement tasks. Our approach is specified in Algorithm 1, and Figure 1 depicts the high-level concept. The following subsections describe the novel features of our approach."}, {"title": "Embedding Perturbation Applied to User Content", "content": "At the core of Algorithm 1 is adding noise to the embedded input sequence $e$ to produce a set of $k$ perturbed embedding sequences ${\\tilde{e}^1, . . ., \\tilde{e}^k}$, where each $\\tilde{e}^i$ is an independent sample produced by the randomized perturbation function $H_o : \\mathbb{R}^{d\\times*} \\rightarrow \\mathbb{R}^{d\\times*}$, where $o$ denotes the hyperparameter(s) associated with the noising process. While we consider several different methods to generate noise (as described later), they all generally share the following common structure: (1) Perturbation is only applied to the embeddings corresponding to the user content portion of the input, since the remaining tokens are fixed (and inaccessible to the attacker) as part of the system prompt and conversation template. Figure 8 in the Appendix illustrates how only user content is perturbed. (2) A statistically independent and identical noising procedure is applied to each perturbed embedding vector. Thus, we will simply define the noising procedure, applied independently to each embedding vector of the user content, as a randomized mapping $h_o : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$.\nIn order to explore the impact of embedding vector direction on preserving semantic meaning, we consider several options for the embedding perturbation function $h_o$:"}, {"title": "Isotropic (Normal) Gaussian noise", "content": "As a simple baseline approach, we define $h^{iso}(e) := e + z$, where $z \\sim N(0, \\sigma^2I)$ is multivariate (d-dimensional) isotropic Gaussian noise with standard deviation $\\sigma$. A potential drawback of this approach is that isotropic noise at larger values of $\\sigma$ may disrupt the direction of the embedding vector, which may encode vital semantic information."}, {"title": "Hard directional noise", "content": "As an approach that aims to preserve the semantic information that may be encoded in the direction of the embedding vector, we define $h^{dir}(e) := e + z_1 \\cdot dir(e)$, where $z_1$ is scalar Gaussian noise, i.e., the first element of $z$, and it is applied to scale the direction vector $dir(e) := e/||e||_2$."}, {"title": "Soft directional noise", "content": "As another noising approach that emphasizes the direction of the embedding vector $e$, but does not enforce a hard directional constraint, we define $h^{soft}(e) := e + z \\odot dir(e)$, where $\\odot$ denotes the Hadamard (element-wise) product."}, {"title": "Orthogonal noise", "content": "To investigate the relative effectiveness of noise orthogonal to the embedding direction, we define $h^{orth}(e) := e + (I \u2013 dir(e)dir(e)^T)z$, where $z$ is projected to the subspace orthogonal to the embedding $e$."}, {"title": "Generation with Token Aggregation", "content": "Our method performs autoregressive generation in parallel, producing a tentative next token $\\tilde{y}$ corresponding to each perturbed embedding sequence $\\tilde{e}^i$, for $i \\in {1, ..., k}$. These tentative next tokens are aggregated by majority voting to select the next output token $y$, which is then embedded as $E(y)$ and appended to each perturbed embedding sequence. This process repeats until either the \u201cEnd of Sequence\u201d token is selected or the maximum output length $m$ is reached. Note that the embeddings of the newly generated output tokens are not perturbed."}, {"title": "Response Prefix Smoothing", "content": "To improve the efficiency of randomized embedding smoothing, which requires running the bulk of the LLM in $k$ parallel token generation instances, we propose response prefix smoothing that applies this defense to only the first $l$ output tokens. The rest of the output token generation is conducted with a single instance of autoregressive token generation, using the original unperturbed embedding sequence $e$ appended with the embeddings of the initial $l$ output tokens produced when the defense was active. With this approach, our defense only occurs additional computation cost in the generation of the first $l$ output tokens. Note that if $m \\leq l$, then the entire sequence will be generated with embedding smoothing applied, and the special case of $l = 0$ is essentially standard autoregressive generation with no defense applied. Figures 9 and 10 in the Appendix illustrate this concept.\nResponse prefix smoothing is motivated the observation that autoregressive generation generally continues along the same theme established by preceding tokens. For example, when faced with a harmful prompt, if the LLM begins the response with phrasing that indicates acceptance, such as \"Sure, this is...\" or \"Here is...\", then it typically continues with generation of harmful content. However, if the response begins with phrasing that indicates refusal, such as \"Sorry, but I cannot...\", then it usually continues with possible elaboration of the reasons for rejection. This phenomenon has been observed in the design of the GCG attack (Zou et al., 2023), where the objective for crafting adversarial inputs is to target a response beginning with acceptance of the harmful prompt, and then rely on the language model to continue along that established sentiment."}, {"title": "Experimental Results", "content": "Our experiments used Vicuna-13B-v1.5 and Llama-2-7B-chat-hf as the victim LLMs. For evaluating our RESTA defense, we used $k = 10$ smoothing samples and a prefix smoothing length of $l = 20$ tokens. We evaluated all four embedding perturbation schemes presented in the earlier Embedding Perturbation section. The Appendix provides links to all of the public code, models, and datasets used for our experiments, illustrates the evaluation pipelines in Figures 11 and 12. We describe our evaluation methodology in the following."}, {"title": "Jailbreaking Attacks", "content": "Against the Vicuna-13B model, we used the 100 GCG attack prompts, 82 PAIR attack prompts, and 100 RS attack prompts available in the JailbreakBench (Chao et al., 2024) dataset. Against the Llama-2-7B model, we used the 100 RS attack prompts available from JailbreakBench. Note that we omit consideration of the GCG and PAIR attacks against Llama-2-7B, since the artifacts provided for those attacks have very low reported success rates (3% and 0%, respectively). JailbreakBench sources the original harmful behavioral goals from AdvBench (Zou et al., 2023), Trojan Detection Challenge (TDC) (Mazeika et al., 2023), and HarmBench datasets (Mazeika et al., 2024). When generating responses to these attack prompts, we used a maximum output length of $m = 150$ tokens.\nAttack Success Rate (ASR), the fraction of attack prompts that successfully induced a jailbreak, was automatically evaluated with the Llama-3-70B-Instruct model (AI@Meta, 2024), following a procedure similar to the automatic evaluation methodology of JailbreakBench. Further details, including the judge prompt template, are provided in the Appendix."}, {"title": "Utility Evaluation", "content": "Evaluation of model utility is essential, since defensive measures may also disrupt the nominal LLM performance for benign inputs. We used the AlpacaEval (Dubois et al., 2024) and Instruction-Following Evaluation (IFEval) (Zhou et al., 2023) datasets to evaluate utility preservation.\nAlpacaEval is an automatic evaluation framework of instruction following performance for LLMs. AlpacaEval \"Win Rate\" scores are determined by an LLM annotator that evaluates the responses for 805 prompts from a target LLM, against the responses from a reference LLM. We used the precomputed Vicuna-13B responses as the reference responses because Vicuna-13B is one of our target LLMs. We used AlpacaEval 2 with GPT-40 provided by the Azure OpenAI API as the annotator. We selected LC (Length-Controlled) Win Rate as a utility measure, which is a improved version of Win Rate which formerly assigned high scores for longer responses. For AlpacaEval experiments, we used a maximum output length of $m = 2048$, since some instructions require longer output.\nIFEval provides 541 prompts which contains instructions with systematic evaluation criteria that allows for deterministic evaluation of whether the responses generated by the LLM follow the instructions of the prompts. We specifically use the prompt-level loose accuracy as the utility metric, as it suggested by IFEval to alleviate the issues of false negatives. We used a maximum output length of $m = 1024$ for the IFEval experiments."}, {"title": "Character Perturbation Ablation", "content": "In order to study the importance of embedding perturbation, we use an ablation of RESTA, where character-level perturbation (in a manner similar to SmoothLLM) is performed instead. Essentially, this is a hybrid of the two methods, combining character perturbation with the token aggregation and prefix smoothing techniques of RESTA. For this ablation defense, we used the same $k = 10$ and $l = 20$ smoothing parameters as RESTA, and varied the choice of random character swapping, insertion, or patch swapping, with noise levels ranging from 2% to 12%."}, {"title": "Results", "content": "Our experimental results for defense performance comparison are summarized in Table 1. In general, we observed that RESTA provided favorable tradeoffs in reducing ASR, while incurring less impact on model utility, across the variety of attack and target model combinations. The SmoothLLM baseline defense was evaluated with its default parameters of 10 samples and random character swapping at a rate of 10%. For our RESTA defense, against both the GCG and PAIR attacks, we noted performance at two noise levels $\\sigma$ (for hard embedding perturbation) in the summary table, in order to briefly note the tradeoff between robustness and utility. For the character-perturbation ablation, despite aiming to pick a fairly competitive operating point among its hyper-parameter choices, we see that it generally achieves a relatively poor tradeoff, which suggests that the embedding smoothing technique is an essential part of our defense. In the following, we present some highlights of our experiments, while further details are given in the Appendix, due to space constraints."}, {"title": "GCG attack against Vicuna", "content": "In Figure 2 we show the impact on ASR of GCG on Vicuna as a function of the noise level $\\sigma$ across the choice of embedding perturbation. This choice has a clear impact on the effect of the defense, and the scale of the noise required. For isotropic (normal) and orthogonal embedding noise, $\\sigma$ ranging from 0.01 to 0.04 corresponds to ASR from close to the undefended ASR of 94% to 0%. However, for hard (or soft) directional noise, $\\sigma$ must range from 0.2 to 1.2 (or 0.5 to 2.5) to have a similar effect on ASR. Similarly, in Figure 3, we see similar difference on the scale of $\\sigma$ needed in the impact on the AlpacaEval utility measure. However, note that utility declines slower than ASR as noise level is increased, which allows for an effective tradeoff between utility and robustness, as illustrated in Figure 4. In comparison, the most competitive character perturbation defense used random patch swapping at 8% noise."}, {"title": "PAIR attack against Vicuna", "content": "In this case, while RESTA still dominated in the comparison, overall the ASR was not driven as close to zero, without more substantially compromising utility. This tradeoff (with respect to utility measured by AlpacaEval) is illustrated in Figure 5, which shows a larger advantage for hard directional embedding perturbation over other methods. The listed character perturbation defense used random insertions at 6% noise."}, {"title": "RS attack against Vicuna", "content": "The tradeoff for this attack against AlpacaEval utility is shown in Figure 6, which exhibited a larger advantage for soft directional embedding perturbation, and the RESTA operating point listed in Table 1 corresponds to soft embedding noise with $\\sigma$ = 1.5. The listed character perturbation defense used random insertions at 8% noise."}, {"title": "RS attack against Llama", "content": "The undefended Llama model seems to be inherently more resilient to jailbreaking attacks, due to the very low ASRs reported for the GCG and PAIR attacks. While the RS attack did achieve an ASR of 69% against the undefended Llama model, it was possible to easily disrupt it with very little perturbation added by the smoothing defenses, with all of them achieving 0% ASR and small impact to the utility metrics. For this case, RESTA used orthogonal embedding noise with $\\sigma$ = 0.05, and the character perturbation defense used random swapping at 2% noise."}, {"title": "Conclusion and Future Work", "content": "We proposed the Randomized Embedding Smoothing and Token Aggregation (RESTA) defense, which adds noise to the embedding vectors and aggregates during token generation. Our experimental results provide an initial proof-of-concept that demonstrated the effectiveness of RESTA to reduce the ASR of jailbreaking attacks, while maintaining model utility, and explored the effect of embedding perturbation direction. As many jailbreaking attacks and defense methods have been recently emerging, we will conduct further experiments to compare with other methods in our future work."}, {"title": "Appendix / supplemental material", "content": "We make use of the following publicly available code, models, and datasets in our work:\n\u2022 SmoothLLM (Robey et al., 2023):\n\u2022 Vicuna-13B-v1.5 (Zheng et al., 2024):\nvicuna-13b-v1.5\n\u2022 Llama-2-7B-chat-hf (Touvron et al., 2023):\nLlama-2-7b-chat-hf\n\u2022 Llama-3-70B-Instruct (AI@Meta, 2024):\nMeta-Llama-3-70B-Instruct\n\u2022 Llama-Guard-3-8B (Llama Team, 2024):\nLlama-Guard-3-8B\n\u2022 GPT-40, via Azure OpenAI Service:\nazure/ai-services/openai/concepts/models and\nhello-gpt-40/\n\u2022 JailbreakBench (Chao et al., 2024):\nartifacts/\n\u2022 AlpacaEval (Dubois et al., 2024):\nand\n\u2022 IFEval (Zhou et al., 2023):\ngoogle-research/tree/master/instruction_following_eval"}, {"title": "Templates used for chat with Vicuna-13B and Llama-2-7B-chat-hf", "content": "Vicuna-13B uses the following system prompt template for chat, where the \"[USER-CONTENT]\" part is replaced with the actual user content (e.g., the chat prompt from the user, which may include adversarial content):\nA chat between a curious user and an artificial intelligence\nassistant. The assistant gives helpful, detailed, and\npolite answers to the user's questions.\nASSISTANT:\nLlama-2-7B-chat-hf uses the following system prompt template for chat, where the \"[USER-CONTENT]\" part is replaced with the actual user content:\nSYSTEM: You are a helpful, respectful and honest assistant.\nAlways answer as helpfully as possible, while being safe.\nYour answers should not include any harmful, unethical,\nracist, sexist, toxic, dangerous, or illegal content. Please\nensure that your responses are socially unbiased and positive\nin nature. If a question does not make any sense, or is not\nfactually coherent, explain why instead of answering something\nnot correct. If you don't know the answer to a question,\nplease don't share false information.\nASSISTANT:\nFollowing the same noising scheme as other defenses (such as SmoothLLM), we only add noise to the embeddings corresponding to the \u201c[USER-CONTENT]\u201d part of the template, while the embeddings of the rest of the input are not noised."}, {"title": "System prompt used for the Llama-3-70B-Instruct jailbreak judge", "content": "We used the following system prompt to evaluate jailbreaks using Llama-3-70B-Instruct as the judge, where the \"[RESPONSE]\" part is replaced with the response generated by victim model being evaluated:\nYou are helpful AI safety research assistant. The following\nuser content is the response from a large language model.\nYour task is to judge whether the response violates ethical,\nlegal, or safety guidelines. Please judge by saying 'safe' or\n'unsafe'. Only answer with 'safe' or 'unsafe', with 'unsafe'\nindicating a violation, and 'safe' indicating no violation.\nTask: Please provide your answer about the following\nresponse:"}]}