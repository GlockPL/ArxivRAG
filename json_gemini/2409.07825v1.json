{"title": "A Comprehensive Survey on Deep Multimodal Learning with Missing Modality", "authors": ["RENJIE WU", "HU WANG", "HSIANG-TING CHEN"], "abstract": "During multimodal model training and reasoning, data samples may miss certain modalities and lead to compromised model performance due to sensor limitations, cost constraints, privacy concerns, data loss, and temporal and spatial factors. This survey provides an overview of recent progress in Multimodal Learning with Missing Modality (MLMM), focusing on deep learning techniques. It is the first comprehensive survey that covers the historical background and the distinction between MLMM and standard multimodal learning setups, followed by a detailed analysis of current MLMM methods, applications, and datasets, concluding with a discussion about challenges and potential future directions in the field.", "sections": [{"title": "1 Introduction", "content": "Multimodal learning has become a crucial field in Artificial Intelligence (AI). It focuses on integrating and analyzing various data types, including visual, textual, auditory, and sensory information (Figure 1a). This approach mirrors the human capacity to combine multiple senses for better understanding and interaction with the environment. Modern multimodal models leverage the robust generalization capabilities of deep learning to uncover complex patterns and relationships that uni-modality systems might not detect. This capability is advancing work across multiple domains, including computer vision. Recent surveys in these fields highlight the significant impact of multimodal approaches, demonstrating their ability to enhance performance and enable more sophisticated AI applications. However, multimodal systems often face the challenge of missing or incomplete data in real-world applications. This occurs due to various factors such as sensor malfunctions, hardware limitations, privacy concerns, environmental interference, and data transmission issues. As illustrated in Figure 1b, in a three-modality scenario, data samples can be categorized as either full-modality (containing information from all three modalities) or missing-modality (lacking data from one or more modalities entirely). These problems can arise at any stage from data collection to deployment, significantly impacting model performance. Real-world examples of this problem are prevalent across various domains. In affective computing, researchers [31, 150] have encountered samples with only usable images or audio due to camera obstructions or excessive microphone noise. Similarly, in space exploration, NASA's Ingenuity Mars helicopter [36] faced a missing modality challenge when its inclinometer failed due to extreme temperature cycles on Mars. To address this issue, NASA applied a software patch that modified the initialization of navigation algorithms [169]. In the field of medical AI, privacy concerns often result in certain modalities being unavailable for some data samples, creating inherent missing modalities in multimodal datasets [222]. The unpredictability of real-world scenarios and diversity of data sources further compound this challenge. Consequently, developing robust multimodal systems that can perform effectively with missing modalities has become a crucial focus in the field.\nIn this survey, we refer to the challenge of handling missing modalities as the \"missing modality problem.\" We name solutions to this problem Multimodal Learning with Missing Modality (MLMM). This approach contrasts with the traditional setup that utilizes full modalities, which we name Multimodal Learning with Full Modality (MLFM). Specifically, in MLFM tasks, given a H-modality dataset, we are typically required to train a model can process and fuse information from all H modalities to make predictions. During training and testing, samples with complete information from all H modalities are used. In contrast, MLMM tasks use fewer than H modalities during training or testing due to factors such as data collection limitations or constraints in deployment environments. The primary challenge in MLMM lies in dynamically and robustly handling and fusing information from any number of available modalities during training and testing, while maintaining performance comparable to that achieved with full-modality samples.\nThis survey encompasses recent advances in MLMM and its applications across various domains, including information retrieval, remote sensing, and robotic vision. We provide a fine-grained taxonomy of MLMM methodologies, application scenarios, and corresponding datasets. Our work expands upon existing MLMM surveys that focus on specific domains such as medical diagnosis [5, 151, 235], sentiment analysis [179], and multi-view clustering [17]. By offering a comprehensive overview of current research and identifying promising directions for future work, this survey aims to contribute to the development of more robust and adaptable multimodal learning systems. These advancements are crucial for deploying intelligent systems in challenging environments, ranging from the harsh conditions of planetary exploration to the dynamic and unpredictable settings of everyday life. Main contributions of this survey are three-fold:\n(1) A comprehensive survey of MLMM applications across diverse domains, accompanied by an extensive compilation of relevant datasets, highlighting the versatility of MLMM in addressing real-world challenges.\n(2) A novel, fine-grained taxonomy of MLMM methodologies, with a multi-faceted categorization framework based on recovery strategies, integration stages, parameter-efficiency approaches, and attention mechanisms.\n(3) An in-depth analysis of current MLMM approaches, their challenges, and future research directions, contextualized within the proposed taxonomical framework."}, {"title": "2 Methodology", "content": "We classify and discuss existing deep MLMM methods based on our methodology taxonomy from four major dimensions: modality augmentation, feature space engineering, architecture engineering, and model selection."}, {"title": "2.1 Modality Augmentation", "content": "We categorize modality augmentation methods addressing the missing modality problem at the original data of modality level into two types. The first is the modality composition method, which uses zero/random values, directly copied data from similar instances, or matching samples obtained via retrieval algorithms, composited with the missing-modality sample to form a full-modality sample. The second is the modality generation method, which generates the original data of the missing modality using generative models such as Auto Encoders (AEs) [55], Generative Adversarial Networks (GANs) [42], or Diffusion Models [56]."}, {"title": "2.1.1 Modality Composition Method", "content": "Removing missing-modality samples is common and simple as a multimodal dataset preprocessing method when the dataset has few missing-modality samples. But this method will reduce the dataset size and make some rare samples disappear when the dataset contains many missing-modality samples. The composition method is widely employed for its simplicity and effectiveness and also keep the dataset size. One typical method is the zero/random values composition method which replaces the missing modality data with zero/random values, as shown in Figure 3. In recent research [28, 102, 114, 163], they are often used as baselines for comparison with proposed methods. About the missing sequential data problem, such as missing frames in videos, the similar Frame-Zero method [135] was proposed to replace the missing frames and composed with available frames. These methods are prevalent in typical multimodal learning training procedures. By using this, multimodal models can balance and integrate information from different modalities when making predictions. This helps prevent the model from relying too heavily on one or a few modalities to some extend, thereby enhancing the robustness. Additionally, these methods fail to generalize well when most samples in the dataset are the missing-modality samples. While, other composition method is based on retrieval algorithms (Figure 3), which involves filling in the original data of the missing modality by copying/averaging the original data from retrieved samples that have same modality and category. Some convenient methods randomly select a sample that has same category and required missing modality from other samples. The selected sample is then composed with the input missing-modality to form a full-modality sample for training. For example, researchers [204] proposed Modal-mixup to complete the training datasets by randomly complementing same-category samples with missing modalities. However, such methods cannot solve the missing modality problem during the testing phase. Regarding missing frames in streaming data such as videos, researchers proposed using Frame-Repeat [135] to make up for the missing frames by using past frames. Some work [14, 41, 204] also has attempted to use K-Nearest Neighbors (KNN) or its variants that can measure the similarity between samples to retrieve the best-matched samples for composition. For these matched samples, we can select the sample with the highest score or obtain the average values of these samples to supplement the missing modality data. Their experiments have shown that KNN-based methods generally perform better than above methods such as random selection and can handle missing modality during testing. But straightforward simple clustering often suffers from high computational complexity,"}, {"title": "2.1.2 Modality Generation Method", "content": "Various matrix imputation methods [41] were proposed in missing data research, leveraging latent correlations among matrix elements. However, missing data often appears in large blocks in multimodal datasets, making conventional methods inefficient for large-scale processing and high-dimensional computations. With deep learning, synthesizing missing modalities has become more efficient. Current methods for generating original data of missing modalities are divided into individual and unified generative methods. Individual modality generation method trains an individual generative model for each modality in case of any missing modality situation, which is shown in Figure 5a. Early works used Gaussian processes [117] or Boltzmann machines [159] to generate missing modalities from available inputs. With deep learning, methods like AEs and U-Net[147] can be used for original modality data generation. Li et al. [87] used 3D-CNN to generate positron emission tomography (PET) data from magnetic resonance imaging (MRI) inputs. Chen et al. [24] addressed missing modalities in MRI segmentation by training U-Net models to generate other two modalities from MRI. Recent work [113] used AEs as one of the baselines to complete datasets by training one AE per modality. In domain adaptation, Zhang et al. [220] proposed a Multi-Modality Data Generation module with domain adversarial learning to generate each missing modality by learning domain-invariant features. GANs significantly improved image generation quality by using a generator to create realistic data and a discriminator to distinguish it from real data. Researchers began replacing AEs and U-Nets with GANs for missing modality generation. For example, GANs generated original data of missing modalities using latent representations of existing ones in breast cancer prediction [3], and WGANs were applied in sentiment analysis [184]. In remote sensing, Bischke et al. [8] used GANs to generate depth data, improving segmentation over RGB-only models. GANs were also used in robotic recognition to generate RGB and depth images[45]. Recent studies [113] show that GANs outperform AEs in generating more realistic missing modalities and can lead to better downstream-task model performance. Recently, the introduction of Diffusion models has further improved image generation quality. Wang et al. proposed the IMDer method [190], which uses available modalities as conditions to help diffusion models generate missing modalities. Experiments showed diffusion reduces semantic ambiguity between recovered and missing modalities and achieves good generalization performance than previous works. But, training an individual generator for each modality is inefficient and time-consuming, failing to capture the latent correlations between modalities. Researchers develop another type of generative method, the unified modality generation method, which trains a unified model that can generate all modalities simultaneously (Figure 5b). One representative model is Cascade AE [174], which stacks AEs to capture the differences between missing and existing modalities for generating all missing modalities. Recent researchers, such as Zhang et al. [221], have attempted to use attention mechanisms and max-pooling layers to integrate features of existing modalities, enabling modality-specific decoders to generate each missing modality. Experiments demonstrated that this method is more effective than using max-pooling alone [19] to integrate features from any number of modalities for generating missing modalities. The above methods for generating original data for missing modalities can mitigate performance degradation to some extent. However, training a generator that can produce high-quality, real-world-distribution-like missing modalities remains challenging, especially when the training dataset contains a few full-modality samples. Additionally, the modality generation model significantly increases storage requirements. As the number of modalities grows, the complexity of these generative models also increases, further complicating the training process and resource demands."}, {"title": "2.2 Feature Space Engineering", "content": "The following introduces methods to address the missing modality problem at the feature space. First, we present two constraint-based approaches that enhance the learning of more discriminative and robust representations by imposing specific constraints (Figure 6). One method involves regularization to improve the effectiveness and generalization of the learned representations. Another method focuses on maximizing correlations, using specific measurements to strengthen the relationships between features. Next, the representation composition method, which can borrow the solutions discussed in Section 2.1.1, operates at the feature level of the modalities or employs arithmetic operations to handle a dynamic number of modalities. Finally, we introduce the representation generation method, which can generate the representations of missing modalities."}, {"title": "2.2.1 Regularization Based Method", "content": "There are few regularization based methods, such as Tensor Rank Regularization [94], which introduces tensor rank minimization as a regularization term in the training process. This method can automatically handle missing or noisy data without prior knowledge of data imperfections. It combines the temporal nonlinear transformation of multimodal data with simple regularization techniques of tensor structures, helping to learn true correlations and latent structures. This effectively mitigates the imperfection of input data and addresses the missing modality problem. Due to the limited number of this kind of methods, they have only been verified on time series data, and whether they can work in other fields requires extensive verification."}, {"title": "2.2.2 Correlation Driven Method", "content": "Some methods train models by coordinating the correlation between different modal features. For example, Wang et al. [185] use a Deep Canonical Correlation Analysis (CCA) module to maximize feature associations within the same category, enabling training on incomplete datasets. Ma et al. [111] proposed a Maximum likelihood function to characterize conditional distributions of full-modality and missing-modality samples during training. Other researchers [107] have added constraints based on the Hilbert-Schmidt Independence Criterion to help the model learn how to complete missing modality features from full modality features. These methods all aid models in learning how to complete missing modality features or training on incomplete datasets by learning the similarity or correlation between features. A drawback of above methods is that they perform well only when two or three modalities are used as input. Even when trained on a dataset with missing modalities, some methods still struggle to effectively address the missing modality problem during testing. Additionally, as the number of modalities increases, the variety of trade-offs in loss functions becomes more complex, making it challenging to achieve good generalization performance."}, {"title": "2.2.3 Representation Composition Method", "content": "There are two types of representation composition methods: one is the retrieval-based composition method, which attempts to recover the missing modality representation by borrowing existing samples or modality information, similar to the original data composition method in Section 2.1.1. They typically use pre-trained feature extractors to generate features from available samples, storing them in a feature pool [163, 232]. Cosine similarity is then used to retrieve matched features for the input sample, with the average of these missing-modality features in top-k samples filling the representation of missing modalities. Additionally, some methods, such as Missing Modality Feature Generation [182], replace missing modality features by averaging the representations of available modalities, assuming similar feature distributions across modalities. Another representation composition method, the arithmetic method, dynamically processes modality representations without using learnable parameters like pooling layers (Figure 7). Researchers [34, 116, 182, 234] have fused features through operations like average/max pooling or addition, offering low computational complexity and efficiency. However, drawbacks include potential loss or dilution of important information. To address this, merge operations [34, 116] use a sign-max function, which selects the highest absolute value from feature vectors, yielding better results by preserving both positive and negative activation values. TMFormer [223] introduces token merging based on cosine similarity, while others calculate moments of available modalities for feature fusion through addition and concatenation [50]. Similar approaches [223, 229, 230] focus on selecting key vectors or merging tokens to handle missing modalities. Recent works [229, 230] focus on selecting key features or vectors for handling missing modalities. Despite these advancements, methods can still struggle if a major modality is missing. Therefore, those representation composition approaches not only allow the model to flexibly handle features from any number of modalities but also enable learning without/few learnable parameters. It is also difficult to capture the inter-modality relationships through the learning way, as methods like selecting the largest vector or the feature with the highest score is difficult to fully represent the characteristics of all modalities."}, {"title": "2.2.4 Representation Generation Method", "content": "Compared to original data generation methods in Section 2.1.2, hierarchical representation generation can integrate seamlessly with existing multimodal frameworks. Current methods fall into two categories: (1) Indirect-to-task representation generation methods (Figure 8a) treat modality reconstruction as an auxiliary task during training, helping the model intrinsically generate missing modality representations for downstream tasks. Since the auxiliary task aids in representation generation during training but is dropped during inference of downstream task, it is termed \u201cindirect-to-task.\u201d (2) Direct-to-task representation generation methods (Figure 8b) train a small generative model to directly map available modality information to the representation of missing modalities. Indirect-to-Task Representation Generation Methods: A common architecture for indirectly generating missing modality representations is the \u201cencoder-decoder\" model. During training, modality-specific encoders extract features from available modalities, one/some reconstruction decoders reconstructs the missing modality, and one downstream module supervised by downstream task loss for prediction. Those reconstruction decoders are discarded during inference, where predictions rely on both the generated missing modality representations and the existing ones. This approach typically employs Multi-Task Learning, training both downstream tasks and auxiliary tasks (modality reconstruction) simultaneously. Some methods, inspired by Masked Autoencoder [51], split reconstruction and downstream task training. Based on the input features used by reconstruction decoders, we divide these methods into two categories."}, {"title": "2.3 Architecture Engineering", "content": "Different from above methods of generating modality or representations of modalities, some researchers adjust the model architecture to adapt to the missing-modality cases. We divide them into four categories according to their core contributions in dealing with missing modalities: attention-based methods, distillation-based methods, graph learning-based methods, and multimodal large language models (MLLMs)."}, {"title": "2.3.1 Attention Based Method", "content": "In the self-attention mechanism [177], each input is linearly transformed to generate Query, Key, and Value vectors. Attention weights are computed by multiplying the query of each element with the keys of others, followed by scaling and softmax to ensure the weights sum to 1. Finally, a weighted sum of the values generates the output. We classify attention-based MLMM methods into two categories: attention fusion methods, which integrate multimodal information (e.g., text, vision, audio) and are suitable for various networks; and transformer-based methods, which stack attention layers to handle large-scale data with global information capture and parallelization.\nAttention Fusion Based Methods: Due to the powerful ability of attention to capture key features and its plug-and-play benefits, many methods have adopted attention mechanisms. We categorize them into two types: intra- and inter-modality attention methods. Intra-modality attention methods compute attention for each modality independently before fusing them, as shown in Figure 9a. This approach focuses on relationships within a single modality (e.g., text, image, or audio), and the fusion between modalities is achieved by learnable fusion representations. For instance, in 3D detection for autonomous vehicles, the BEV-Evolving Decoder [38] handles sensor failures by sharing same BEV-query with each modality-specific attention modules, allowing fusion of any number of modalities. Similarly, in clinical diagnosis, Lee et al. [80] proposed modality-aware attention to perform intra-modality attention and predict decisions by fusing bottleneck fusion tokens [131], addressing learning with missing modalities. Inter-modality attention methods, often based on masked attention, treat missing modality features as masked vectors to better capture dependencies across modalities, as illustrated in Figure 9b. Unlike conventional cross-modal attention, masked attention models share the same parameters across all embeddings, allowing flexible handling of missing modalities. For example, Qian et al. [139] designed an attention mask matrix to ignore missing modalities, improving model robustness. Similarly, DrFuse [206] decouples modalities into specific and shared representations, using the shared ones to replace missing modalities, with a custom mask matrix to help model ignore the specific representations of the missing modality.\nTransformer Based Methods: We divide transformer-based methods into two methods: joint representation learning (JRL) and parameter efficient learning (PEL) according to full parameter training and a small amount of parameter fine-tuning. By utilizing stacked multimodal transformer layers with attention, the multimodal transformer can learn joint representations from any number of modality tokens (Figure 10). Gong et al. [40] introduced an egocentric multimodal task, proposing a transformer-based fusion module with a flexible number of modality tokens and a cross-modal contrast alignment loss to map features into a common space. Similarly, Mordacq et al. [127] leveraged a Masked Multimodal Transformer, treating missing modalities as masked tokens for robust joint representation learning. Ma et al. [112] proposed an optimal fusion strategy search strategy to enhance the performance of multimodal transformers in handling missing modalities. Radosavovic et al.[141] introduced mask tokens [M] for an autoregressive transformer to manage missing modalities, successfully deploying the model in real-world scenarios. With the rise of pre-trained transformer models, PEL methods have been developed to fine-tune these models by training few parameters, leveraging their generalization abilities for various tasks. Given their focus on pre-trained models and limited parameter updates, we categorize PEL methods separately, despite some overlap with above JRL methods. Two common fine-tuning methods for pre-trained models are prompt and adapter tuning. Initially used in natural language processing, prompt tuning optimizes input prompts while keeping model parameters fixed. It has since been extended to multimodal models, optimizing multimodal prompt embedding to handle diverse inputs like text and images. Jang et al. [66] introduced modality-specific prompts to address limitations in earlier methods, which merge when all modalities are present and allow the model to update all learnable prompts during training. Liu et al. [105] further improved this by proposing Fourier Prompt, which uses fast Fourier transform to encode global spectral information into learnable prompt tokens which can be used to supplement missing-modality features, enabling cross-attention with feature tokens to address missing modalities. Adapter tuning, on the other hand, involves inserting lightweight adapter layers into pre-trained models to adapt to new tasks without modifying the original parameters. Qiu et al. [140] proposed a method that uses a classifier to identify missing-modality cases and the intermediate features as the missing-modality prompt to cooperate with a lightweight Adapter to address missing modality problems for brain tumor segmentation.\nAlthough attention-based fusion mechanisms in above methods can effectively help deal with the missing modality problem in any framework, none of them cares about the missing modality that may contain important information required for prediction. In the transformer-based methods, although the PEL method has a very fast fine-tuning speed and requires very few computing resources, its performance is still not comparable to that of JRL. But the JRL method is limited by a large amount of computing resources and requires relatively large datasets to achieve good performance."}, {"title": "2.3.2 Distillation Based Method", "content": "Knowledge distillation transfers knowledge from a teacher model to a student model [54]. The teacher model, with access to more information, helps the student reconstruct missing modalities. Below, we categorize two types of distillation methods for addressing this problem. First is the representation-based distillation, this method transfers rich representations from the teacher model to help the student capture and reconstruct missing modality features. We classify approaches based on whether they use decision-level or intermediate features. Figure 11 illustrates the procedures. Response distillation methods focus on transferring the teacher model's logits to the student, helping it mimic probability distributions. Wang et al. [187] trained modality-specific teachers for missing modalities, then used their soft labels to guide a multimodal student. Hafner and Ban [48] employed logits from a teacher model trained with optical data to supervise a reconstruction network for approximating missing optical features from radar data. Pramit et al. [148] proposed a Modality-Aware Distillation, leveraging both global and local teacher knowledge in a federated learning setting to handle missing modalities. Orzikulova et al. [133] used logits for privacy-preserving knowledge transfer in missing modality scenarios. Intermediate distillation methods align intermediate features between teacher and student models. Shen et al. [153] used Domain Adversarial Similarity Loss to align intermediate layers of teacher and student, improving segmentation in missing modality settings. Zhang et al.[222] applied intermediate distillation in endometriosis diagnosis by distilling features from a TVUS-trained teacher to a student using MRI data. Then is the process-based method, which focuses on overall distillation strategies, like Mean Teacher Distillation (MTD) [168] and self-distillation [218]. These methods emphasize procedural learning over direct representation transfer. MTD enhances stability by using the exponential moving average of the student model's parameters as a teacher (Figure 12a). Chen et al. [25] applied this to missing modality sentiment analysis, treating missing samples as augmented data. Li et al. [91] used MTD for Lidar-Radar segmentation, improving robustness against missing modalities. Self-distillation helps a model improve by learning from its own soft representations (Figure 12b). Hu et al. proposed the ShaSpec [180], which utilizes distillation between modality-shared branches and modality-specific branches of available modalities to help improve model performance. Based on ShaSpec, researchers proposed Meta-learned Cross-modal Knowledge Distillation [181] to further weigh the importance of different modalities to improve performance. Ramazanova et al. [143] used mutual information and self-distillation for egocentric video tasks, making predictions invariant to missing modalities. Shi et al. proposed PASSION [156], a self-distillation method designed to use the multi-modal branch to help other uni-modal branches of avaiable modalities to improve multimodal medical segmentation performance with missing modality. Hybrid distillation methods combine various distillation approaches to improve student performance. For medical segmentation, Yang et al. [203] distilled teacher model knowledge at every decoder layer, outperforming ACNet[192]. Wang et al. [188] introduced ProtoKD, which captures inter-class feature relationships for improved segmentation under missing modality conditions. For sentiment analysis, CorrKD [85] leverages contrastive distillation and prototype learning to enhance performance in uncertain modality scenarios. Aforementioned methods effectively address the missing modality problem and achieve good generalization during testing. However, except for some intermediate and self-distillation methods, most assume the complete dataset is available for training (means teacher can access full-modality information during training), with missing modalities encountered only during testing. As a result, most are unsuitable for handling incomplete training datasets."}, {"title": "2.3.3 Graph Learning Based Method", "content": "Graph-learning based methods leverage relationships between nodes and edges in graph-structured data for representation learning and prediction. We categorize approaches for addressing the missing modality problem into two main types: graph fusion and graph neural network (GNN) methods. Graph fusion methods integrate multimodal data using a graph structure (Figure 13), making them adaptable to various networks. For example, Angelou et al.[1] proposed a method mapping each modality to a common space using graph techniques to preserve distances and internal structures. Chen et al. [22] introduced HGMF, which builds complex relational networks using hyper edges that dynamically connect available modalities. Zhao et al. [226] developed Modality-Adaptive Feature Interaction for Brain Tumor Segmentation, adjusting feature interactions across modalities based on binary existence codes. More recently, Yang et al. [202] proposed a Graph Attention Based Fusion Block to adaptively fuse multimodal features, using attention-based message passing to share information between modalities. These fusion methods can be flexibly inserted into any network for integrating multiple modalities.\nGNN methods directly encode multimodal information into a graph structure, with GNNs used to learn and fuse this information. Early approaches [186, 225] employed Laplacian graphs to connect complete and incomplete multimodal samples. Individual GNN methods (Figure 14a), such as DESAlign [191], extract features using neural networks or GNNs and fuse them for prediction. Unified GNN methods (Figure 14b) first complete the graph and then use GNNs for prediction, such as in Zhang et al.'s M3Care [216], which uses adaptive weights to integrate information from similar patients. Lian et al. [93] proposed the Graph Completion Network, which reconstructs missing modalities by mapping features back to the input space. In recommendation systems, FeatProp [114] propagates known multimodal features to infer missing ones, while MUSE [198] represents patient-modality relationships as a bipartite graph and learns unified patient representations. In knowledge graphs, Chen et al. [27] introduced Entity-Level Modality Alignment to dynamically assign lower weights to missing or uncertain modal information, reducing the risk of misguidance in learning. Above methods can leverage the graph structure to better capture relationships both within/between modalities and samples. However, these methods tend to have lower efficiency, scalability, and higher development complexity compared to other kind of approaches."}, {"title": "2.3.4 Multimodal Large Language Model", "content": "With the rise of LLMs like ChatGPT [10], their impressive generalization capabilities across tasks have demonstrated the transformative power. However, human understanding extends beyond language, incorporating modalities like vision and audio. This has led researchers to explore MLLMs, designed to handle diverse user inputs across modalities, including cases with missing modalities, leveraging the flexibility of Transformers. In current MLLM architectures, LLMs act as feature processors, integrating feature tokens from different modality-specific encoders and passing the output to task-/modality-specific decoders. This enables LLM to not only capture rich inter-modal dependencies, but also naturally carry the ability to handle any number of modalities, that is, the ability to solve the missing modality problem. Most MLLMs employ transformer-based modality encoders, such as CLIP, ImageBind [39], and LanguageBind [236], which encode multimodal inputs into a unified representation space. Examples include BLIP-2 [84], which bridges the modality gap using a lightweight Querying Transformer, and LLaVA [101], which enhances visual-language understanding with GPT-4-generated instruction data [10]. These models are optimized for tasks like Visual Question Answering, Dialogue, and Captioning. Recent advancements extend output generation to multiple modalities, such as images. AnyGPT [215] and NEXT-GPT [197] unify modalities like text, speech, and images using discrete representations and multimodal adaptors, enabling seamless multimodal interaction. CoDi [167] introduces Composable Multimodal Conditioning, allowing arbitrary modality generation through weighted feature summation. Other notable MLLMs include Qwen-VL [6], AnyMAL [126], and X-LLM [21], showcasing the growing potential of MLLMs in diverse applications. Although MLLMs can flexibly handle any number of modalities, they have disadvantages of inconsistent multi-modal positional encoding, training difficulty, and high GPU resource requirements. Additionally, no specific MLLM benchmarks on missing modality problems have been found."}, {"title": "2.4 Model Selection", "content": "Model selection methods aim to use one or more selected models for downstream tasks, while also enhancing robustness and performance. These methods can be categorized into ensemble, dedicated, and discrete scheduler methods. Ensemble methods combine predictions from multiple selected models through, such as voting, weighted averaging, and similar approaches to improve the final decision-making accuracy and stability. Dedicated methods allocate different sub-tasks (e.g., different missing modality cases) to specialized individual models, focusing on specific sub-tasks or sub-datasets. In the discrete scheduler method, users can use natural language instructions to enable the LLMs to autonomously select the appropriate model based on types of modalities and downstream tasks."}, {"title": "2.4.1 Ensemble Method", "content": "Ensemble learning methods allow flexibility in supporting different numbers of expert models to combine their predictions, as shown in Figure 15. Early work [179] in sentiment analysis employed ensemble learning to handle missing modalities, averaging predictions from modality-specific models. With deep learning advancements, the Ensemble-based Missing Modality Reconstruction network [214] was introduced, leveraging weighted judgments from multiple full-modality models when generated missing modality features exhibit semantic inconsistencies. This type of model-based ensemble method, depicted in Figure 15a, integrates various full-modality models to aid decision-making. Another type is modality-based ensemble methods, as shown in Figure 15b, where each modality is paired with a uni-modal model, and only available modalities contribute to decision-making. In multimodal medical image diagnosis, early studies found uniformly weighted methods performed better than weighted averaging and voting approaches [211]. In multimodal object detection, Chen et al. [26] proposed a probabilistic ensemble method that flexibly handles missing modalities via probabilistic marginalization, proving highly efficient. Li et al. [88] recently proposed Uni-Modal Ensemble with Missing Modality Adaptation, training models per modality and performing post-fusion training."}, {"title": "2.4.2 Dedicated Method", "content": "Dedicated methods assign different tasks to specialized models. We show the general idea of the kind of method in Figure 16. KDNet [60] was the first dedicated method proposed to handle different combinations of missing modalities. It treats uni-modality specific models as student models, learning multimodal knowledge from the features and logits of a multimodal teacher model. Those trained uni-modals can be used for different missing modality cases. Following KDNet, ACNet [192] also utilizes this distillation method but introduces adversarial co-training, further improving the performance. Lee et al. [82] proposed missing-aware prompts to address missing modality problems based on the prompt learning by using input- and attention- level prompts for each kind of missing modality case, only 1% of the model total parameters needed fine-tuning for downstream tasks. Similarly, some researchers [145] equip different adapter layers for each missing modality case to handle missing modality problems."}, {"title": "2.4.3 Discrete Scheduler Method", "content": "In discrete scheduler methods (Figure 17), LLMs act as task controllers, determining the execution order of different discrete sub-tasks broken down from the major task/instruction. While the LLM does not directly process multimodal data, it interprets language instructions and orchestrates task execution across uni- and multi-modal modules. This structured yet flexible approach is particularly effective for outputs requiring sequential tasks, enabling the system to handle any number of modalities and naturally addressing missing modality problems. For example, Visual ChatGPT [196] integrates multiple foundation models with ChatGPT to enable interaction through text or/and images, allowing users to pose complex visual questions, provide editing instructions, and receive feedback within a multi-step collaboration framework. HuggingGPT [154] is an LLM-driven agent that manages and coordinates various Al models from Hugging Face. It leverages LLMs for task planning, model selection, and summarization to address complex multimodal tasks. ViperGPT [164] combines visual and language tasks into modular subprograms, generating and executing Python code for complex visual queries without additional training to achieve effective outputs. There are other similar discrete scheduler approaches, such as AudioGPT [62], MM-REACT [205], and LLaVA-Plus [106]. Some aforementioned dedicated and ensemble methods can flexibly handle the missing modality problem without additional training, but most of them require more model storage space, which is not feasible for many resource-constrained devices. For instance, DynMM requires storing various uni- and multi-modality models. As the number of modalities increases, the required number of models also rises. Also, modality-based ensemble methods struggle to adequately model the complex inter-modality relationships to make final predictions. In addition, discrete scheduler methods can solve a variety of tasks when there are sufficient types of downstream modules, but they usually require LLMs to respond quickly and understand human instructions accurately in real world scenarios."}, {"title": "2.5 Discussion", "content": "In this section", "dimensions": "modality augmentation, feature space engineering, architecture engineering, and model selection. We further subdivide these into thirteen categories under a fine-grained methodology taxonomy. Table 1 summarizes the overall pros and cons of these methods from four dimensions. Generative and distillation methods are the most common approaches for addressing missing modalities; they are easy to implement and deliver strong performance. With the rise of Transformers, attention-based methods have become more popular due to their larger receptive fields and parallelism. However, indirect-to-task generation methods and most distillation methods (except for some intermediate and self-distillation methods) are currently unable to handle incomplete training datasets (means cannot access missing modality data"}]}