{"title": "ASTRID - An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems", "authors": ["Mohita Chowdhury", "Yajie Vera He", "Aisling Higham", "Ernest Lim"], "abstract": "Large Language Models (LLMs) have shown impressive potential in clinical question answering (QA), with Retrieval Augmented Generation (RAG) emerging as a leading approach for ensuring the factual accuracy of model responses. However, current automated RAG metrics perform poorly in clinical and conversational use cases. Using clinical human evaluations of responses is expensive, unscalable, and not conducive to the continuous iterative development of RAG systems. To address these challenges, we introduce ASTRID - an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is designed to better capture the faithfulness of a model's response to the knowledge base without penalising conversational elements. To validate our triad, we curate a dataset of over 200 real-world patient questions posed to an LLM-based QA agent during surgical follow-up for cataract surgery - the highest volume operation in the world - augmented with clinician-selected questions for emergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate that CF can predict human ratings of faithfulness better than existing definitions for conversational use cases. Furthermore, we show that evaluation using our triad consisting of CF, RA, and CR exhibits alignment with clinician assessment for inappropriate, harmful, or unhelpful responses. Finally, using nine different LLMs, we demonstrate that the three metrics can closely agree with human evaluations, highlighting the potential of these metrics for use in LLM-driven automated evaluation pipelines. We also publish the prompts and datasets for these experiments, providing valuable resources for further research and development.", "sections": [{"title": "1 Introduction", "content": "The healthcare industry is increasingly adopting automation to meet rising demands on resources [33]. Large Language Models (LLMs) due to their capabilities have become increasingly popular in supportive clinical applications such as note-taking and summarisation[3]. A crucial aspect of patient care is the ability to ask questions and receive answers, which has been enhanced by advancements in Question-Answering (QA) systems powered by LLMs. However, the issue of hallucination remains a significant barrier in using LLMs for clinical QA systems [32]. Retrieval Augmented Generation (RAG) is a technique developed to address hallucination and ensure context appropriateness [21]. Despite these advancements, RAG systems lack sufficient evaluation metrics and frameworks, making it difficult to quantitatively establish their safety and identify system deficiencies.\nThis work explores the limitations of current evaluation methods and applies safety engineering principles to identify potential hazard cases in clinical QA [12, 8]. We develop a robust and scalable framework of metrics to systematically demonstrate how developers can mitigate potential hazards in LLM-based QA systems for clinical use. Using real patient questions from clinical trials on cataract post-operative recovery, we illustrate how these metrics can be interpreted in a clinical context. We validate our metrics by proving they model human ratings better than previous metrics, and effectively predict clinical harm, usefulness, and inappropriateness as labelled by specialist doctors. Our aim is to establish a foundation for developing and assessing LLM-powered clinical QA systems and encourage further research in this area. Our contributions are summarised as follows (Figure 4):\n\u2022 A hazard analysis of clinical QA systems inspired by the safety engineering principles.\n\u2022 A new suite of metrics for clinical QA systems motivated by this analysis.\n\u2022 An analysis of these metrics and how they model human ratings.\n\u2022 An analysis of how these metrics can predict clinical harm, usefulness, and inappropriateness to a high standard when used together."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Background to clinical QA evaluation", "content": "Clinical QA systems powered by LLMs have generated significant recent interest. Already, some LLMs have demonstrated capabilities to generate more accurate responses [42, 2, 36, 48, 46], and sometimes even more empathetic [20] than doctors across various clinical contexts. However, LLMs can generate plausible-sounding, but factually incorrect responses, commonly referred to as"}, {"title": "2.2 Current RAG metrics", "content": "Evaluating RAG systems presents challenges due to their hybrid structure and the overall quality of the output often depends on multiple components within the systems. While attempts have been made to assess the overall quality of responses using deterministic methods [24, 25], most of the current evaluation metrics for RAG systems use an ensemble of component-level assessments, the majority of which leverage LLMs as judges [50]. Broadly, the performance of RAG pipelines can be evaluated by examining two main components: the retrieval and the generation components. For the retrieval component, key metrics include context relevance and retrieval accuracy. For the generation component, such metrics include answer relevance, faithfulness, and answer correctness.\nThese component evaluations have been variably implemented with popular tools including TruEra's RAG Triad [45], and LangChain Bench [19]. Additionally, LLM-as-a-judge-based frameworks like RAGAS [9], and ARES [34] have popularised common evaluation triads to capture possible permutations of the above components. Please see appendix A.1 for an example on how the three components of the RAG system can be judged by LLMs, using the RAGAS metrics as an example."}, {"title": "2.2.1 Limitations of current metrics", "content": "Faithfulness The established methods to measure Faithfulness break down a model's response into granular statements and then evaluate each statement's consistency with the context [9]. This approach aims to create more focused assertions that consider the context of both the question and the answer. It is particularly advantageous when answers are short and lack context when reviewed in isolation, as demonstrated by Figure 6. However, in the context of clinical conversations, this approach has some shortcomings. Firstly, summarising responses into statements often overlooks the clinical nuances present in the original dialogue (Figure 7). Creating statements from both the patient's question and the agent's answer can hinder an independent review of the agent's response in relation to the context. This is particularly problematic when the combined statements contain factual inaccuracies (Figure 8). Lastly, dialogue agents, particularly in clinical settings, are prompted to respond pathetically and conversationally. Statements constructed from the agent's acknowledgments and questions, such as those meant to clarify or follow up on the patient's queries or concerns, are penalised by existing faithfulness definitions (Figure 9).\nAnswer Relevance Evaluating answer relevance is critical in QA systems to ensure generated responses align with query intent. However, most current definitions focus on lexical or semantic similarity between the question and the response [9, 39]. Such approaches over-emphasise surface-level topic matching without accounting for deeper contextual understanding. Additionally, they neglect to factor in whether a context is appropriate given a clinical context.\nIn a conversational context, a simple answer such as \"yes\" or \"no\" could be entirely appropriate, and constitute a clinically meaningful (and thus risky) response, which will not be captured by answer relevance metrics.\nAdditional Limitations Furthermore, existing metrics often penalise the system for appropriately refusing to address a question when it falls outside its scope of relevance or when there is insufficient information to provide a safe and accurate response. This is crucial as clinical QA systems are often required to stay within the defined scope of practice."}, {"title": "3 Proposed approach", "content": ""}, {"title": "3.1 Deriving metrics towards a safety case", "content": "In order to align our framework towards the evidence required to demonstrate if a clinical system is safe, we sought inspiration from published safety engineering frameworks - namely the Safety Assurance of autonomous systems in Complex Environments (SACE) guidance [12]. Structured safety engineering approaches have been applied towards the assurance of high-integrity autonomous systems (AS) such as maritime vessels [27], automotive [31, 14], aerospace [43], and healthcare domains [16, 10]. The SACE framework, in particular, provides a process to systematically integrate safety assurance into the development of AS whilst considering the system and its environment. Whilst we do not report all artefacts from the process in its entirety, we highlight a few key steps in this process that have been applied towards ASTRID's design. Namely, we considered the principles of:"}, {"title": "3.2 A novel set of metrics and a framework to assess safety risks", "content": "Current RAG metrics do not correlate to clinical risks, and have varying levels of validation against human evaluations, with poor performance in conversational contexts. To our knowledge, there have also been no efforts to connect QA system performance with automated metrics for RAG systems, with real-world clinician grading of clinical harms, helpfulness and inappropriateness of responses. For developers to meaningfully understand whether a clinical RAG QA system meets safe operating concepts, we needed a framework that was validated for clinical use, scalability, and acknowledged nuanced clinical contexts.\nWe propose a novel Automated and Scalable Triad (ASTRID) analysis framework for RAG-based clinical QA systems. ASTRID consists of three reference-free LLM-based metrics: Refusal Accuracy (RA), Conversational Faithfulness (CF) and Context Relevance (CR) (Figure 2). In the subsequent sections, we will illustrate how to validate each of the metrics and the overall framework based on real-world data from patients speaking to clinical conversational agents, augmented to ensure sufficient test-case coverage."}, {"title": "3.2.1 Conversational Faithfulness (CF)", "content": "Evaluating how grounded a response is concerning the information provided is important to QA systems using RAG. Existing metrics that address this do not encapsulate additional complexities associated with conversational agents in a clinical setting. Therefore we propose a new metric Conversational Faithfulness (CF).\nGiven an answer-context pair, Conversational Faithfulness is defined as the proportion of information-containing sentences that are faithful to the context. To calculate CF, we employ the following steps:\n1. We categorise different sentences in the response into \"acknowledgements\", \"questions\" and \"informative\". We provide the prompt used to achieve this step in the appendix (1).\n2. We determine whether the informative sentences are grounded in context. We provide the prompt used to achieve this step in the appendix (2).\nFinally, CF is calculated as the fraction of the number of informative sentences grounded in context to the total number of informative sentences."}, {"title": "3.2.2 Refusal Accuracy (RA)", "content": "As discussed in previous sections, an important aspect of evaluating QA systems in the clinical setting is the ability of the system to decline to respond when it cannot answer a question, or a question is not appropriate for the clinical context. This is essential especially in LLM-powered systems, where risks arise from a model's tendency to provide ungrounded responses. As current metrics do not capture this behaviour, we add the metric Refusal Accuracy (RA) to our triad.\nRefusal Accuracy is defined as the system's ability to deny a response when there is no relevant information available to answer the question. We use binary labels to indicate whether the system appropriately refuses to respond. We provide the prompt used to achieve this step in the appendix (3)."}, {"title": "3.2.3 Context Relevance (CR)", "content": "It is essential for clinical QA systems built on Retrieval-Augmented Generation (RAG) to use the right context when framing answers, typically achieved by creating embeddings of the query and knowledge source and passing them through a retriever [7, 21]. The retriever component takes the encoded query and retrieves the top matches from the knowledge source, which are then provided to the LLM agent as context [35]. For voice-based conversational QA systems, most user queries do not exceed two questions per turn, and specialised knowledge sources are relatively small and focused. Considering that multiple pieces of information may be required for a given question, the clinical RAG QA system used in this evaluation retrieves the top three chunks. Unlike many existing CR definitions that penalise additional retrieved contexts [9, 34], we emphasise the completeness of clinical information. Therefore, we define CR as a binary label indicating whether the retrieved context is relevant to the query, with the prompt used for this step provided in the appendix (4)."}, {"title": "4 Method", "content": "We conduct several experiments using datasets sourced from real clinicians and open-source datasets to support the following claims:\n1. Our metric, Conversational Faithfulness (CF), can model human judgments of faithfulness, Perceived Faithfulness (PF), more accurately than existing definitions.\n2. Our triad of metrics can predict clinician ratings of harmfulness, helpfulness, and inappropriateness.\n3. Our triad of metrics is straightforward for LLMs to use, making them automatable."}, {"title": "4.1 Data", "content": "We created three datasets from consented and anonymised real patient questions and the open-source dataset HealthSearchQA [37] for each of our experiments:"}, {"title": "4.2 Experiments", "content": "We break down this section by Claims 1, 2, and 3, detailing the different experiments we conducted to support them and discussing the results."}, {"title": "4.2.1 Demonstrating alignment of Conversational Faithfulness with human perception", "content": "Setup To demonstrate that our metric, Conversational Faithfulness (CF), aligns more closely with human perception of faithfulness than previous definitions, we perform the following:\n1. We treat CF as a diagnostic test that predicts human perception of faithfulness (PF). We compare it with the classification based on the previous definition of faithfulness, which we call RF (inspired by RAGAS), and conduct a ROC analysis for both. To do this, we use human ratings of CF, RF and PF from the FaithfulnessQAC dataset.\n2. We use Pearson, Spearman, and Kendall Tau correlation coefficients to correlate human ratings of CF and RF with PF.\nNote that we use human ratings instead of ratings from LLMs to eliminate any model artifacts in the analysis.\nResults From Figure 5, we observe that our metric CF is able to better predict Perceived Faithfulness (PF) compared to previous definition (RF), with an AUC of 0.98.\nFrom Table 1, we also observe higher correlations between CF and PF, thus demonstrating that our metric aligns more closely and accurately with human judgements of faithfulness than previous definitions in conversational contexts."}, {"title": "4.2.2 Predicting clinical assessments using our triad of metrics", "content": "Setup For this experiment, we use CF, CR, and RA human ratings, along with harmfulness, helpfulness, and inappropriateness clinician ratings from the ClinicalQAC dataset. We explored if CF, CR, and RA could be used as features to predict clinician-perceived harmfulness, helpfulness, and inappropriateness of a QA answer.\nTo achieve this, we first reserve 17.5% of the dataset for the test split (Figure 14). We manually choose triplets to ensure balanced categories. We then randomly sample 79% of the remaining dataset for the train split and use the remaining 21% as the val split.\nWe then train four models to demonstrate how our triad can independently predict harmfulness, helpfulness, and inappropriateness when the scope of practice (within scope/out-of-scope) is taken into account. We subsequently test the results on the test set and report precision, recall and F1-scores.\nResults In Table 2, we demonstrate that using our triad and the scope of practice, we can predict clinician rating of harmfulness with an average F1-score of 0.835. We can also predict helpfulness with an average F1-score of 0.715.\nRegarding inappropriateness, we observe that the F1-score for \"Yes\" and \"No\" classes are 0.70 and 0.73, respectively. However, the presence of \"slightly\" inappropriate clinical content proves to be challenging to detect. This difficulty aligns with human assessments, as clinicians also showed the most disagreement on inappropriateness, with an inter-annotator score prior to resolution of 65%. We report other inter-annotator scores prior to resolution in the appendix in Table 9.\nTo illustrate how the metrics can be used at an individual question level to identify potentially harmful failure modes, we highlight several examples in Figure 11. These examples demonstrate the potential for these metrics to be used by developers to correlate against clinician labels of potential harms."}, {"title": "4.2.3 Automatability of our triad of metrics", "content": "Setup To demonstrate that our metrics are automatable, we use the UniqueQAC dataset and automatically compute Conversational Faithfulness (CF), Context Relevance (CR) and Refusal Accuracy (RA) using nine different LLMs. The prompts used by the LLMs to compute these metrics can be found in the appendices (A.3). Note that we only prompt-engineered for Palm-2 and made minor tweaks for output formatting for the rest of the models.\nResults Table 3 shows the average CF, CR and RA computed using various models and compares it to the corresponding human rating averages. From the table, it can be seen that with minimal"}, {"title": "5 Limitations and Future Scope", "content": "One limitation of our approach is that our focus is on single-turn safety rather than end-to-end conversations. End-to-end conversations introduce an additional element of decision-making and context continuity that need to be assessed for a holistic evaluation of a QA system. Further work should explore multi-turn interactions to ensure comprehensive safety, reliability, and extended dialogue.\nOur metrics and evaluation frameworks are centered around safety. Notably, we have not factored in usability aspects such as robustness to mistranscriptions ([50]), measures of clinical empathy ([40]), latency, brevity, or user satisfaction ([26]). Incorporating these aspects into future research will provide a more well-rounded assessment of QA systems in real-world clinical environments. While the automation of these metrics was promising, further refinement and validation are necessary.\nA strength of the study is that it utilised a real-world dataset of questions posed to a voice-based AI agent, which included mistranscriptions, statements, and truncated questions to accurately reflect real-world scenarios. We recognise that the amount of data used may be small to draw conclusions. We also developed a clinician-generated dataset in the clinical domain of hip surgery follow-up to explore generalisability; however, we limited our analysis to the real-world question dataset to align with actual arising hazard cases rather than imagined ones."}, {"title": "6 Conclusion", "content": "In conclusion, we present ASTRID, an Automated and Scalable Triad for evaluating clinical QA systems leveraging Retrieval Augmented Generation (RAG). ASTRID comprises three metrics\u2014novel Conversational Faithfulness (CF), Refusal Accuracy (RA), and Context Relevance (CR)\u2014designed to address the limitations of existing evaluation frameworks in clinical settings. Our experiments demonstrate that CF aligns more closely with human judgments of faithfulness compared to previous definitions, and our triad of metrics is the first to correlate system performance measures with clinician assessments of harmfulness, helpfulness, and inappropriateness with high accuracy. We also highlight the potential for these metrics to be automatable using current LLMs, making them suitable for iterative development and continuous evaluation of clinical QA systems. By publishing our datasets and prompts, we aim to provide valuable resources for further research and development in the field. Future work should expand on end-to-end conversational evaluations and incorporate usability metrics to ensure a comprehensive assessment of clinical QA systems."}, {"title": "A.1 RAGAS definitions: Context Relevance, Answer Relevance and Faithfulness", "content": "The RAGAS triad has three components as judged by LLMs:\nContext Relevance:\nContext Relevance = \\(\\frac{number \\space of \\space relevant \\space context \\space sentences}{total \\space number \\space of \\space sentences}\\) (1)\nAnswer Relevance:\nAnswer Relevance = \\(\\frac{1}{N}\\sum_{i=1}^{N} \\frac{E_{gi}E_o}{||E_{gi}||||E_o||}\\)(2)\nWhere:\n\u2022 \\(E_{gi}\\) is the embedding of the generated question i.\n\u2022 \\(E_o\\) is the embedding of the original question.\n\u2022 N is the number of generated questions, which is 3 by default.\nFaithfulness\nF = \\(\\frac{V}{S}\\)\nWhere:\n\u2022 |V| is the number of statements that were supported according to the LLM.\n\u2022 S is the total number of statements."}, {"title": "A.2 Examples of limitation of current metrics", "content": "Question: I put my medication in the fridge by accident, is it still safe to use?\nAnswer: Yes, that would be ok. However, it is best stored above 0 degrees, in a cool place away from direct sunlight.\nRAGAS statements: 1. Medication stored by accident in the fridge is still safe to use. 2. However, such medication is best stored above 0 degrees, in a cool place away from direct sunlight."}]}