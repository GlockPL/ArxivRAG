{"title": "Opus\nA Workflow Intention Framework for Complex Workflow Generation", "authors": ["Phillip Kingston", "Th\u00e9o Fagnoni", "Mahsun Altin"], "abstract": "This paper introduces Workflow Intention, a novel framework for identifying and encoding\nprocess objectives within complex business environments. Workflow Intention is the alignment\nof Input, Process and Output elements defining a Workflow's transformation objective inter-\npreted from Workflow Signal inside Business Artefacts. It specifies how Input is processed to\nachieve desired Output, incorporating quality standards, business rules, compliance require-\nments and constraints. We adopt an end-to-end Business Artefact Encoder and Workflow\nSignal interpretation methodology involving four steps: Modality-Specific Encoding, Intra-\nModality Attention, Inter-Modality Fusion Attention then Intention Decoding. We provide\ntraining procedures and critical loss function definitions. In this paper:\n1. We introduce the concepts of Workflow Signal and Workflow Intention, where Workflow\nSignal-decomposed into Input, Process and Output elements is interpreted from Busi-\nness Artefacts, and Workflow Intention is a complete triple of these elements.\n2. We introduce a mathematical framework for representing Workflow Signal as a vector and\nWorkflow Intention as a tensor, formalizing properties of these objects.\n3. We propose a modular, scalable, trainable, attention-based multimodal generative system\nto resolve Workflow Intention from Business Artefacts.", "sections": [{"title": "1 Introduction", "content": "In the lifecycle of most businesses and government organizations, developing and refining internal\nprocesses is crucial for maintaining structure, consistency, and overall operational efficiency. Well-\ndefined processes offer tangible advantages: they enhance quality control, reduce costs, mitigate\nrisks, and facilitate auditing. They also help ensure continuity when employees retire or transition\nout of the organization. Furthermore, many regulatory and compliance agencies publish procedural\nguidelines to help external stakeholders understand and meet specific standards.\nDespite the clear benefits of process documentation, its quality, completeness, currency, accuracy,\nand granularity can vary widely across organizations. This is where the concept of Workflow Inten-\ntion becomes invaluable. By extracting the core purpose and objectives from existing Business Arte-\nfacts such as standard operating procedures, policy manuals, and historical records-Workflow\nIntention enables organizations to rapidly implement supervised automation and evolve legacy pro-\ncesses into efficient AI-enhanced Workflows enriched with best practice.\nDefinitions Our methodology is based on the following concepts introduced in Opus: A Large\nWorkflow Model for Complex Workflow Generation by Fagnoni et al. [1]:\nInput: The dataset initiating a Process, conforming to validation rules and format specifi-\ncations. Input is multimodal, including structured (e.g. databases, forms) and unstructured\n(e.g. documents, media) data types such as text, documents, images, audio, and video.\nProcess: A structured sequence of operational steps transforming Input into Output, defined\nin part or whole across Business Artefacts. Process combines automated and manual steps\ndefining start/end conditions, decision points, parallel paths, roles, success criteria, monitoring,\nmetrics, compliance requirements and error handling.\nOutput: The result of a Process operating on Input, meeting predefined quality and business\ncriteria. Output can be tangible (e.g. documents) or intangible (e.g. decisions) and include\naudit trails of their creation. Supported formats include text and documents.\nBusiness Artefact: Any text, document, image, audio, or video capturing Process knowl-\nedge (e.g. process maps, standard operating procedures, regulatory guidelines, compliance\ndocuments) or that provides Workflow Signals. Business Artefacts detail input/output\nspecifications, task sequences, business rules and roles.\nWorkflow: A software defined executable Process as a sequence of Tasks. Workflows co-\nordinate task execution, manage data flow, and enforce business rules, compliance, and process\nlogic (e.g. conditionals, loops, error handling). Workflows support monitoring, logging, audit,\nstate management, concurrency, adaptive modification, and version control.\nTask: An atomic unit of work within a Workflow, performing a specific function with defined\ninput/output schemas, objectives, timing constraints, and success criteria. Tasks adhere to\nthe singular responsibility principle, support automation or manual intervention, and maintain\ncontextual awareness of dependencies. Tasks are auditable by humans or AI agents against\ntheir definition."}, {"title": "2 Background", "content": "This framework leverages many state-of-the-art methodologies to obtain contextual representations\nof multimodal Business Artefacts in order to generate Workflow Intention. Transformer-based ar-\nchitectures introduced a paradigm shift from recurrence to parallelizable self-attention, enabling\nefficient long-range dependency modeling and significantly improving scalability in natural lan-\nguage processing. Subsequent innovations in encoder-only (RoBERTa), decoder-only (GPT), and\nencoder-decoder (T5) architectures further refined contextual representation and generation capa-\nbilities. Advancements in visual (ViT) and multimodal (FLAVA, NVLM, InternVL) transformers\nhave broadened these capabilities to handle image and text jointly, providing robust backbones for\nextracting structured features from Business Artefacts to facilitate Workflow Intention generation."}, {"title": "3 System Overview", "content": "Let $\\mathcal{M} = \\{m_1, m_2,...,m_k\\}$ denote the set of $K$ distinct modalities. For each modality $m_k$, we\nconsider a set of Business Artefacts $\\mathcal{A}_{m_k} = \\{a_{m_k, 1}, a_{m_k, 2},..., a_{m_k, N_{m_k}} \\}$, where $N_{m_k}$ is the number\nof Business Artefacts in modality $m_k$. Each Business Artefact $a_{m_k,i}$ is represented in its raw form,\nwith modality-specific dimensionality. In this paper, we consider three modalities: Text (T), Image\n(I) and Document (D). The framework is built to support any modality.\nEach Business Artefact from $\\mathcal{A}_{m_k}$ gets encoded in a dedicated pipeline. Then, all encoded Business\nArtefacts from $\\mathcal{A}_{m_k}$ are concatenated and encoded by a dedicated intra-modality pipeline. The\nencoded Business Artefacts are then concatenated across the modalities by a fusion encoder, before\nentering the Workflow Intention decoder. The encoded decoder generates vectors which are pro-\njected into Workflow Intention objects until it stops."}, {"title": "4 Business Artefacts Encoding and Signal Extraction", "content": "Each Business Artefact $a_{m_k,i}$ of modality $m_k$ is tokenized by a tokenizer $Tok_{m_k}$. Each token is\nembedded by a token encoder $E_{m_k}$. The resulting sequence of vectors $E_{m_k,i}$ is encoded by a self-\nattention based encoder $Encoder_{m_ki}$ into $E_{m_ki}$. The encoded sequence is projected into a unified\nspace via a linear projection parametrized by $W_u, b_u$ and the arrival dimension $d$, giving $H_{m_k,i}$.\nA learned representative vector $h_{i}^{[REP]}$ is retrieved or computed from $H_{m_p,i}$ and linearly projected\nby three Projection Heads: Input $(W^{i}_p, b^{i}_p)$, Process $(W^{p}_p, b^{p}_p)$ and Output $(W^{o}_p, b^{o}_p)$.\n4.1 Modality-Specific Business Artefact Encoding\n4.1.1 Text Business Artefacts\nLet T be the text modality. For ease of notation, let $T_i = a_{T,i}$ a text Business Artefact.\nText Tokenizer and Token Encoder We tokenize $T_i$ with $Tok_T$ into a sequence of $L_{T_i}$ tokens\n$T_i \\rightarrow \\{[CLS]_T, t_{i,1}, t_{i,2},..., t_{i,L_{T_i}-1}\\}$, prepending a $[CLS]_T$ token. Each token $t_{i,j}$ is mapped to\nan embedding vector $E_T(t_{i,j}) \\in \\mathbb{R}^{d_T}$ by a text token encoder $E_T: \\mathcal{V}_T \\rightarrow \\mathbb{R}^{d_T}$ where $\\mathcal{V}_T$ is the\nvocabulary (the set of all possible text tokens) and $d_T$ is the embedding dimensionality. We define\nthe tensor representation $E_{T_i} = E_{T,i}$ for ease of notation as the sequence of embedded tokens:\n$E_{T_i} = [E_T([CLS]_T), E_T(t_{i,1}), E_T(t_{i,2}), ..., E_T(t_{i, L_{T_i}-1})] \\in \\mathbb{R}^{d_T\\times L_{T_i}}$ (1)\nText Encoder To capture contextual dependencies across all tokens in $T_i$, we pass $E_{T_i}$ through an\nencoder network $Encoder_T$ to calculate $E_i^T = En_i^T$ in which each column contains a contextualized\nembedding of the corresponding token. Finally, we employ a linear projection to map $E_i^T$ into a\n$d$-dimensional space. The projection is achieved by applying a learnable weight matrix $W \\in \\mathbb{R}^{d \\times d_T}$\nand a bias vector $b \\in \\mathbb{R}^{d}$, thus forming the final representation $H_T$:\n$H_T = WE_i^T + b, H_{T_i} \\in \\mathbb{R}^{d \\times L_{T_i}}$ (2)\n4.1.2 Image Business Artefacts\nLet F be the image modality. For ease of notation, let $F_i = a_{F,i}$ an image Business Artefact. $F_i$\nis represented in its raw form as a three-dimensional tensor of size $R^{c\\times h\\times w}$, where c denotes the\nnumber of channels, h and w represent the height and width of the image, respectively.\nImage Tokenizer and Token Encoder To remain consistent with the tokenizing abstraction,\ni.e. \"tokenizer\" for text, we adopt the same terminology, even though it is actually a specific image\nfeature extraction described in the appendix.\nWe tokenize $F_i$ with $Tok_F$ into a sequence of $L_{F_i}$ patches $F_i \\rightarrow \\{f_{i,1}, f_{i,2},..., f_{i,L_{F_i}} \\}$. We do\nnot employ any $[CLS]$-type vector representation, consistent with ViT (Dosovitskiy et al. [5]).\nEach patch $f_{i,j}$ is mapped to an embedding vector $E_F(f_{i,j}) \\in \\mathbb{R}^{d_F}$ by an image patch encoder\n$E_F : \\mathbb{R}^{c\\times h\\times w} \\rightarrow \\mathbb{R}^{d_F}$ where $d_F$ is the embedding dimensionality."}, {"title": "4.1.3 Document Business Artefacts", "content": "Let D be the document modality. We treat each document page as an image of size ($h_D, w_D$). Each\npage is tokenized separately. For ease of notation, let $D_{p,i} = a_{pD,i}$ a page of a document Business\nArtefact and $D_i = a_{p,i}$ a document Business Artefact.\nDocument Page Tokenizers and Token Encoders $D_{p,i}$ is decomposed as (inspired by the\nmethod of NVLM by Dai et al. [7]):\nText elements $\\{D_{i,q}\\}_{q}$: a set of text elements, each tokenized through $Tok_T$ then concate-\nnated, prepended with a $[CLS]_T$ token, producing a sequence of $L_{D_{p,i}}$ tokens $\\{d_j\\}_j$. Each\ntoken $d_j$ is mapped to an embedding vector $E_T(d_j) \\in \\mathbb{R}^{d_T}$ using $E_T : \\mathcal{V}_T \\rightarrow \\mathbb{R}^{d_T}$, producing\na sequence of vectors denoted by $E^{T}_{D_{p,i}} \\in \\mathbb{R}^{d_T \\times L_{Dp,i}}$.\nText spatial elements: bounding box coordinates of each text token $d_j$ expressed in text as\n$box_{i,j}^T =$ \"$<box>(x_{min,i,j}, y_{min,i,j}), (x_{max,i,j}, y_{max,i,j})</box>\"$, with\n$box_{[CLS]_T}^T =$ \"$<box>((0,0), (h_D, w_D))</box>\" , and mapped to an embedding $E_s(box_{i,j}^T) \\in\n\\mathbb{R}^{d_T}$, producing a sequence of vectors denoted by $E^{S}_{D_{p,i}} \\in \\mathbb{R}^{d_T \\times L_{Dp,i}}.$ $E_s =$ $Encoder_{D_{p,i}}$ denotes\nthe average over the sequence of embeddings produced by the text encoder, in order to obtain\none embedding per bounding box.\nImage elements $\\{DE_q\\}_{q}$: a set of image elements, each patched through $Tok_F$ then con-\ncatenated, producing a sequence of $L_{D_{p,i}}^F$ patches $\\{d_j^F\\}_j$. Each patch $d_j^F$ is mapped to an\nembedding vector $E_F(d_j^F) \\in \\mathbb{R}^{d_F}$ using $E_F : \\mathbb{R}^{c\\times h\\times w} \\rightarrow \\mathbb{R}^{d_F}$, producing a sequence of vectors\ndenoted by $E^{F}_{D_{p,i}}, E^{D} \\in \\mathbb{E} \\mathbb{R}^{d_F \\times L_{Dp,i}}$.\nImage spatial elements: bounding box coordinates of each image patch $d_j^F$ expressed in text\nas $box_{i,j}^F =$ \"$<box>(x_{min,i,j}, y_{min,i,j}), (x_{max,i,j}, y_{max,i,j})</box>\"$, with\n$box_{[CLS]_F}^F =$ \"$<box>(0,0), (h_D, w_D))</box>\" , and mapped to an embedding $E_s(box_{i,j}^F) \\in\n\\mathbb{R}^{d_T}$, producing a sequence of vectors denoted by $E^{S}_{D_{p,i}} \\in \\mathbb{R}^{d_T \\times L_{Dp,i}}.$\n$E_s = Encoder_{D_{p,i}}$ denotes the average over the sequence of embeddings produced by the text encoder, in order\nto obtain one embedding per bounding box."}, {"title": "4.2 Input, Process, Output Projection Heads", "content": "Since the dimensions $d_T$ and $d_F$ may differ, we project each patch embedding into $\\mathbb{R}^{d}$ via a\nlearnable linear projection with bias term ($W_P \\in \\mathbb{R}^{d \\times d_F}, b \\in \\mathbb{R}^{d}$) to obtain\n$E^{D^F}_{D_{p,i}} = W_PE_i^D + b \\in \\mathbb{R}^{d_T \\times L_{Dp,i}}$ (5)\nDocument text and image element embeddings are concatenated with their respective spacial ele-\nment embeddings to produce a sequence of vectors as follows:\n$E^{D}_{D_{p,i}} = Concat(E_i^F, Concat_{D_{p,i}}(E^T, Concat_{D_{p,i}})) \\in \\mathbb{R}^{d_T\\times L_{Dp,i}}$ with $L_{D_{p,i}} = 2(L^F_{D_{p,i}} + L_{D_{p,i}})$ (6)\nwhere\n$E_i^{F, Concat} = Concat((W_P E^F(d_j) + b^F, E_s(box_{j,i}^F))_j) \\in \\mathbb{R}^{d \\times 2 L^F_{D_{p,i}}}$ (7)\n$E_i^{T, Concat} = Concat((E_T(d_j), E_s (box_j))_j) \\in \\mathbb{R}^{d_T \\times 2 L_{Dp,i}}$ (8)\nDocument Encoder The tokenized pages $\\{E_{D_{p,i},n}\\}_n$ are concatenated into $E_{D_i}$. To capture con-\ntextual dependencies across all tokens and patches in $D_i$, we pass $E_{D_i}$ through an encoder network\n$Encoder_D$ to calculate $E_i^D = Enc_i^D$ in which each column contains a contextualized embedding of\nthe corresponding token, token spatial coordinates, patch and patch spatial coordinates. Finally,\nwe employ a linear projection to map $E_i^D$ into a $d$-dimensional space. The projection is achieved\nby applying a learnable weight matrix $W_b \\in \\mathbb{R}^{d \\times d_D}$ and a bias vector $b_b \\in \\mathbb{R}^{d}$, thus forming the\nfinal representation $H_{D_i}$:\n$H_{D_i} = WE^D + b, H_{D_i} \\in \\mathbb{R}^{d \\times L_{Di}}$ (9)\n4.2.1 Text Artefact Originated Workflow Signals\nThe encoded $[CLS]_T$ token representation of $T_i$, $h_{i,[CLS]_T} \\in H_T$, is retrieved as the learned represen-\ntative vector, $h^{REP} = h_{i,[CLS]_T}$ and linearly projected by three separate Projection Heads: Input\n$(W^{I}_T, b^{I}_T)$, Process $(W^{P}_T, b^{P}_T)$ and Output $(W^{O}_T, b^{O}_T)$, to obtain the following Workflow Signals:\n$\\iota_{T_i} = W_T^{I}h^{REP} + b_T^I \\in \\mathbb{R}^d$ (10)\n$\\rho_{T_i} = W_T^{P}h^{REP} + b_T^P \\in \\mathbb{R}^d$ (11)\n$o_{T_i} = W_T^{O}h^{REP} + b_T^O \\in \\mathbb{R}^d$ (12)\n4.2.2 Image Artefact Originated Workflow Signals\nWe define $h^{REP} = MaxPooling(H_{F_i})$. $h^{REP}$ is linearly projected by three separate Projection\nHeads: Input $(W^{I}_F, b^{I}_F)$, Process $(W^{P}_F, b^{P}_F)$ and Output $(W^{O}_F, b^{O}_F)$, to obtain the following Workflow\nSignals:"}, {"title": "4.2.3 Document Artefact Originated Workflow Signals", "content": "Since the dimensions $d_T$ and $d_F$ may differ, we project each patch embedding into $\\mathbb{R}^{d}$ via a\nlearnable linear projection with bias term ($W_P \\in \\mathbb{R}^{d \\times d_F}, b \\in \\mathbb{R}^{d}$) to obtain\n$E^{D^F}_{D_{p,i}} = W_PE_i^D + b \\in \\mathbb{R}^{d_T \\times L_{Dp,i}}$ (5)\nDocument text and image element embeddings are concatenated with their respective spacial ele-\nment embeddings to produce a sequence of vectors as follows:\n$E^{D}_{D_{p,i}} = Concat(E_i^F, Concat_{D_{p,i}}(E^T, Concat_{D_{p,i}})) \\in \\mathbb{R}^{d_T\\times L_{Dp,i}}$ with $L_{D_{p,i}} = 2(L^F_{D_{p,i}} + L_{D_{p,i}})$ (6)\nwhere\n$E_i^{F, Concat} = Concat((W_P E^F(d_j) + b^F, E_s(box_{j,i}^F))_j) \\in \\mathbb{R}^{d \\times 2 L^F_{D_{p,i}}}$ (7)\n$E_i^{T, Concat} = Concat((E_T(d_j), E_s (box_j))_j) \\in \\mathbb{R}^{d_T \\times 2 L_{Dp,i}}$ (8)\nDocument Encoder The tokenized pages $\\{E_{D_{p,i},n}\\}_n$ are concatenated into $E_{D_i}$. To capture con-\ntextual dependencies across all tokens and patches in $D_i$, we pass $E_{D_i}$ through an encoder network\n$Encoder_D$ to calculate $E_i^D = Enc_i^D$ in which each column contains a contextualized embedding of\nthe corresponding token, token spatial coordinates, patch and patch spatial coordinates. Finally,\nwe employ a linear projection to map $E_i^D$ into a $d$-dimensional space. The projection is achieved\nby applying a learnable weight matrix $W_b \\in \\mathbb{R}^{d \\times d_D}$ and a bias vector $b_b \\in \\mathbb{R}^{d}$, thus forming the\nfinal representation $H_{D_i}$:\n$H_{D_i} = WE^D + b, H_{D_i} \\in \\mathbb{R}^{d \\times L_{Di}}$ (9)"}, {"title": "5 Decoding Intention", "content": "We define a Workflow Intention $\\gamma$ as a triple of Input, Process and Output Workflow Signals:\n$\\gamma = (\\iota_{\\gamma}, \\rho_{\\gamma}, o_{\\gamma})$ (19)\nWe define the Workflow Intention Set of a set of Business Artefacts A as a set of Workflow Intentions\n$\\Gamma = \\{\\gamma_i\\}_i$. The goal is to generate the Workflow Intention Set, i.e. Workflow Intention object(s),\nfrom a contextual representation of all the Business Artefacts. To do so we employ an encoder-\ndecoder architecture described as follows.\n5.1 Intra-Modality Attention\nAcross multiple Business Artefacts $\\mathcal{A}_{m_k} = \\{a_{m_ki}\\}_i$ of the same modality $m_k$, the encoded se-\nquences $\\{Enc_{m_k,i}\\}_i$ are concatenated, encoded by the self-attention based encoder $Encoder_{intra}^{m_k}$ into\n$H_{intra}^{Am_k}$. An encoded [REP] token representation $h_{A_{m_k}}^{intra, [REP]}$ is computed, linearly projected into the\nunified space by $(W_{intra,u}^{Am_k}, b_{intra,u}^{Am_k})$ then by three Projection Heads: Input $(W_{intra,I}^{Am_k}, b_{intra,I}^{Am_k})$, Process\n$(W_{intra,P}^{Am_k}, b_{intra,P}^{Am_k})$ and Output $(W_{intra,O}^{Am_k}, b_{intra,O}^{Am_k})$.\n5.1.1 Artefact vectors Aggregation\nFor a given modality $m_k \\in \\{T,F,D\\}$, let $\\mathcal{A}_{m_k} = \\{a_{m_k,i}\\}_i$ be a set of Business Artefacts of this\nmodality and $\\{Enc_{m_k,i}\\}_i$ be the set of encoded tensors of these Business Artefacts.\n$\\forall i, Enc_{m_k,i} \\in \\mathbb{R}^{d_{m_k} \\times L_{m_k,i}}$ where $L_{m_k,i}$ is the number of encoded vectors of $a_{m_k,i}$.\n$E_{A_{m_k}} = Concat(\\{Enc_{m_k,i}\\}_i) \\in \\mathbb{R}^{d_{m_k} \\times L_{A_{m_k}}}$ where $L_{A_{m_k}} = \\sum_{i=1}^{|A_{m_k}|} L_{m_ki}$ (20)"}, {"title": "5.2 Inter-Modality Fusion Attention", "content": "Considering $\\mathcal{A} = \\{\\mathcal{A}_{m_k}\\}_{k}$ a set of Business Artefacts grouped by modality. From now on, we\nconsider $\\mathcal{A}_T$, $\\mathcal{A}_F$, $\\mathcal{A}_D$ sets of text, image and document Business Artefacts respectively.\n5.2.1 Inter-Modality vectors Aggregation\nWe form a combined matrix $H_{inter}^A$ by concatenating the intra-modality encoder outputs $\\{H_{intra}^{A_m_k}\\}_k$\ncolumn-wise:\n$H_{inter}^A = Concat(\\{H_{intra}^{A_m_k}\\}_k) \\in \\mathbb{R}^{d \\times L_A}$ where $L_A = \\sum_{k=1}^{|A|} L_{A_{m_k}}$ (24)\n5.2.2 Fusion Encoder\nWe pass $H_{inter}^A$ through an encoder network $Encoder_{fusion}$ to calculate $H_{fusion} \\in \\mathbb{R}^{d \\times L_A}$. We cur-\nrently do not employ projection heads to compute the Workflow Signals out of the fused represen-\ntation of the Business Artefacts, as the fusion encoder is trained on Workflow Intention generation\nand not Workflow Signal extraction, as described later in the paper.\n5.3 Intention Decoder\nThe decoder generates vectors based on the context computed from the artefacts. Each generated\nvector is projected into Workflow Signals $\\iota_{\\gamma}$, $o_{\\gamma}$ and $\\rho_{\\gamma}$, defining a Workflow Intention object $\\gamma$ as\nan element of the Workflow Intention Set $\\Gamma$. It is made of $N_{decoder}$ layers. Each layer is composed\nof a block of $n_{decoder}$ masked self-attention heads coupled with a LayerNorm block, followed by a\nblock of $n_{decoder}$ cross-attention heads coupled with a LayerNorm block."}, {"title": "5.3.1 Generation loop", "content": "We initialize a decoded sequence $S_{dec}$ with a [BOS] token embedding representation $E_{fusion}([BOS])$.\nAt iteration $t$, in each decoder layer, $S_{dec}^t$ is first encoded through the masked multi-head self-\nattention heads, then attends to the fusion encoder's multimodal Business Artefact context $H_{fusion}$\nvia the cross-attention heads. The output sequence encoded by all the layers is denoted as $S_{dec}^t$. The\nlast vector of the sequence, denoted by $s_{dec_1}^t \\in \\mathbb{R}^{d}$ is linearly projected by $(W_{\\gamma}, b_{\\gamma})$ to produce $\\gamma_t \\in\n\\mathbb{R}^{d}$. We introduce two stopping mechanisms below: the Stopping Head and the Stopping Criteria.\nThe Stopping Head acts as a first layer to stop the generation based the latest computed context.\nThe Stopping Criteria stops the generation based on the latest generated Workflow Intention object.\nIf the Stopping Head described below suggests to accept the generation, $\\gamma_t \\in \\mathbb{R}^{d}$ is linearly projected\nby three separate Projection Heads: Input $(W^I_{\\gamma}, b^I_{\\gamma})$, Process $(W^P_{\\gamma}, b^P_{\\gamma})$ and Output $(W^O_{\\gamma}, b^O_{\\gamma})$ to\nobtain the following Workflow Signals:\n$\\iota_{\\gamma_t} = W_{\\gamma}^I \\gamma_t + b_{\\gamma}^I \\in \\mathbb{R}^d$ (25)\n$\\rho_{\\gamma_t} = W_{\\gamma}^P \\gamma_t + b_{\\gamma}^P \\in \\mathbb{R}^d$ (26)\n$o_{\\gamma_t} = W_{\\gamma}^O \\gamma_t + b_{\\gamma}^O \\in \\mathbb{R}^d$ (27)\nThese projections produce the Intention object:\n$\\gamma_t = (\\iota_{\\gamma_t}, \\rho_{\\gamma_t}, o_{\\gamma_t})$ (28)\nIf the Stopping Criteria described below suggest to accept and continue the generation, we start\niteration $t + 1$ with:\n$S_{t+1}^{dec} = Concat(S_{t}^{dec}, \\gamma_t)$ (29)\nLet $t_f$ be the last iteration that passed the Stopping Mechanisms, we have:\n$\\Gamma = \\{\\gamma_t\\}_{t=1}^{t_f}$ (30)\n5.3.2 Stopping Mechanisms\nStopping Head We define the Stopping Head as\n$MLP_{stop} = MLP(ReLU, n_{stop}, (W_{stop,i}, b_{stop,i})_{i=1}^{n_{stop}}, \\{0,1\\})$\nwhere 0 denotes the \"Stop\" class to stop the generation and 1 the \"Accept\" class to accept the\ncurrent generation a priori. The intuition is to decide if the current generated sequence of Workflow\nIntentions, attended with the Business Artefacts context, is complete or not.\n$\\forall t > 1, MLP_{stop}(S_1^{dec_1}) = \\delta_t^{head} \\in \\{0,1\\}$ (31)\nWith $\\delta_{t}^{head} = {\\begin{cases}\n1 & \\text{if } P_t(\"Accept\") > 0.5 \\\\\n0 & \\text{else}\n\\end{cases}}$ and $\\Delta_{t}^{head} = \\prod_{l=1} \\delta_l^{head} = 1$ (32)"}, {"title": "Stopping Criteria", "content": "We define the Redundant Stopping Criterion as\n\n$\\u2200t > 1, \\delta^{sim} = {\\begin{cases}\n1 & \\text{if } (\\frac{\\langle i_{yt-1}, i_{yt} \\rangle}{\n||i_{yt-1}|| ||i_{yt}||}+ \\frac{\\langle p_{yt-1}, p_{yt} \\rangle}{\n||p_{yt-1}|| ||p_{yt}||}+\\frac{\\langle o_{yt-1}, o_{yt} \\rangle}{\n||o_{yt-1}|| ||o_{yt}||}) < r^{sim} \\forall t' \\in [1,t-1]\n0 & \\text{else}\n\\end{cases}}$ (33)\nWith $\\Delta_{t}^{sim} = 1$ and $r^{sim} \\in [0,1]$ (34)\nAt step t, $\\Delta_{t}^{sim} = 1$ indicates to continue the generation whereas $\\Delta_{t}^{sim} = 0$ indicates to stop the gen-\neration. The intuition is to refuse $\\gamma_t$ and stop the generation if the generated Workflow Intention\nat step t is too similar to one of the previously generated Workflow Intention.\nWe define the Hard Stopping Criterion by $t_{max}$ such that if $t > t_{max}$ the generation is stopped. This\nmeans that we constrain a user query to not include more than $t_{max}$ distinct Workflow Intentions."}, {"title": "6 Training", "content": "6.1 Phase 1: Business Artefacts Encoding and Signal Extraction\nWe employ a two stage training regimen. First, we train each modality independently for each\nBusiness Artefact in each modality $m_k$ where we have an $Encoder_{m_k}$ that is finetuned and $(W^{i}_{m_k}$,\n$b^{i}_{m_k})$, $(W^{p}_{m_k}, b^{p}_{m_k})$, $(W^{o}_{m_k}, b^{o}_{m_k})$, $(W^{u}_{m_k}, b^{u}_{m_k})$ are trained by passing $N_{artefact,m_k}^{(1.1)}$ Business Artefacts\nfor each modality.\nFor all modalities $m_k$, $\\mathcal{A}_{m_k}^{(1.1)} = \\{a_{m_k,i}^{(1.1)}\\}_{i=1}^{N_{artefact,m_k}}$ denotes the set of training Business Artefacts for\nmodality $m_k$ at Stage 1 of Phase 1 ($\\mathcal{C}_{x,A}^{1.1} = C_{x}^{1.1}$). Then, we continue training each modality independently over the intra-modality layers so that\nfor all $m_k$, $Encoder_{intra}^{m_k}$ is finetuned and $(W_{intra,u}^{Am_k}, b_{intra,u}^{Am_k})$, $(W_{intra,I}^{Am_k}, b_{intra,I}^{Am_k})$, $(W_{intra,P}^{Am_k}, b_{intra,P}^{Am_k})$,\n$(W_{intra,O}^{Am_k}, b_{intra,O}^{Am_k})$ are trained by passing $N_{set,m_k}^{(1.2)}$ $V_{set,m_k}$ Business Artefact sets for each modality.\nFor all modalities $m_k$, we provide {$\\mathcal{A}_{set, m_k}^{(1.2)} = {\\{\\mathcal{A}_{j}^{Am_k}\\}}_{j=1}^{N_{set,m_k}^{(1.2)}}$ $A_{set,m_k}^{1.2}$ denotes the sets of\ntraining Business Artefacts for modality $m_k$ at Stage 2 of Phase 1.\nIn total, $\\sum_{m_k} N_{artefact,m_k}^{(1.1)}$ Business Artefacts are provided in stage 1 and $\\sum_{m_k} \\sum_{j=1}^{N_{Set,mk}^{(1.2)}} |A_{j}^{(1.2)}|$ in\nstage 2.\n6.1.1 Classification Tasks for i, o and p\nWe build ground truth data based on three sets of text elements $I_g$, $P_g$ and $O_g$:\nIg: elements that serve as input Workflow Signals within the Business Artefacts.\nPg: elements that relate to transformations or Processes within the Business Artefacts.\nOg: elements that describe expected output Workflow Signals within the Business Artefacts."}, {"title": "6.1.2 Loss", "content": "Each projected vector i, p and o is associated with a ground"}]}