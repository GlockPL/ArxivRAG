{"title": "Piecing It All Together: Verifying Multi-Hop Multimodal Claims", "authors": ["Haoran Wang", "Aman Rangapur", "Xiongxiao Xu", "Yueqing Liang", "Haroon Gharwi", "Carl Yang", "Kai Shu"], "abstract": "Existing claim verification datasets often do not require systems to perform complex reasoning or effectively interpret multimodal evidence. To address this, we introduce a new task: multi-hop multimodal claim verification. This task challenges models to reason over multiple pieces of evidence from diverse sources, including text, images, and tables, and determine whether the combined multimodal evidence supports or refutes a given claim. To study this task, we construct MMCV, a large-scale dataset comprising 16k multi-hop claims paired with multimodal evidence, generated and refined using large language models, with additional input from human feedback. We show that MMCV is challenging even for the latest state-of-the-art multimodal large language models, especially as the number of reasoning hops increases. Additionally, we establish a human performance benchmark on a subset of MMCV. We hope this dataset and its evaluation task will encourage future research in multimodal multi-hop claim verification. Data and code are available: https://mmcv-dataset.github.io/", "sections": [{"title": "1 Introduction", "content": "Due to the rapid growth in AI-generated content, it is difficult for automated fact-checking systems to keep up with verifying the accuracy of claims with multimodal evidence. This challenge is further exacerbated by the recent development of diffusion models such as DALL-E (Ramesh et al., 2021) and Stable Diffusion (Rombach et al., 2022), which can generate realistic images from textual prompts (Liu et al., 2024b). These powerful tools could enable attackers to produce misleading information (Wang and Shu, 2024; Pan et al., 2023c) at a low cost. Additionally, these claims often require multi-hop reasoning, where a set of connected evidence pieces leads to the final verdict of a claim (Yang et al., 2018). As a result, there is a need for automated tools to assist human fact-checkers in evaluating the veracity of multimodal multi-hop claims.\nClaim verification, which involves assessing the veracity of an input claim against a collection of evidence, is a vital tool in combating the spread of misinformation (Thorne and Vlachos, 2018; Guo et al., 2022; Jin et al., 2022, 2023; Yang et al., 2022). However, verifying multi-hop multimodal claims introduces new challenges in both dataset construction and effective modeling. Unlike single-hop claims, which require only straightforward one-step reasoning, multi-hop claims require multiple reasoning steps to reach a final verdict. Furthermore, the inclusion of multimodal evidence requires models to understand and integrate information across various modalities, such as text, images, and tables, making it more complex to comprehend and extract relevant information. For instance, to verify the claim shown in Figure 1, a system must understand the semantic content of the image, integrate all relevant information from the table evidence, and apply multi-step reasoning to arrive at the final conclusion.\nIn this paper, we introduce the task of multi-hop multimodal claim verification to evaluate the veracity of multi-hop claims against multimodal evidence. To study this task, we construct Multi-hop Multimodal Claim-Verification (MMCV), a dataset of 16K multi-hop claims paired with multimodal evidence that either SUPPORT or REFUTE each claim. To create the dataset, we develop a novel pipeline that uses large language models (LMMs) for data annotation, supported by human feedback. This method significantly reduces the workload on human annotators and cuts costs, while ensuring high quality and factual accuracy of the dataset. Our pipeline first uses LLMs to reformulate multi-hop multimodal question-answer pairs into atomic multi-hop claims and generate a set of candidate claims. These candidate claims are then modified to include additional hops and refined for fluency and clarity according to a set of annotation guidelines. To ensure the accuracy of the claims, we use a Retrieval-Augmented Generation (RAG)-based validation method to verify their validity. Finally, we ask a group of human annotators to score the claims based on their fluency, correctness, and clearness, and manually rewrite the claims that are below a certain threshold.\nWe establish performance baselines on MMCV using three state-of-the-art multimodal large language models (MLLMs) and highlight their limitations in verifying complex multimodal claims. We further demonstrate the challenges posed by the dataset, especially as the number of reasoning hops increases, by illustrating the constrained performance of various prompt techniques designed to enhance MLLMs' reasoning capabilities, including chain-of-thought, self-ask, and symbolic-guided reasoning. Additionally, we establish a human performance benchmark on a subset of MMCV.\nOverall, we introduce a challenging multi-hop multimodal claim verification dataset that includes claims with up to 4 reasoning hops. These complex claims often consist of multiple sentences linked by coreference and demand evidence from various modalities, such as text, images, and tables. \nWe hope that the introduction of MMCV and its corresponding evaluation task will inspire further research in complex multi-hop multimodal reasoning for claim verification. In summary, our contributions include:\n\u2022 We introduce and formalize the multi-hop multimodal claim verification task.\n\u2022 We develop a novel pipeline that leverages LLMs for data annotation, enhanced by human feedback, to construct a benchmark dataset for multi-hop multimodal claim verification. This method significantly lowers the cost and labor required to produce a large-scale dataset.\n\u2022 We establish baseline performance on this task using MLLMs and human evaluation. Our analysis shows that this is a non-trivial task, with several challenges that remain to be addressed in future work."}, {"title": "2 Background", "content": "Multimodal Claim Verification. Previous research on claim verification has primarily focused on textual data. However, with the growing recognition that misinformation often appears across multiple modalities and that multimodal misinformation is perceived as more credible and spreads faster than text-only misinformation, recent efforts have shifted toward verifying multimodal claims (Akhtar et al., 2023). As a result, several multimodal claim verification datasets have been proposed including FakeNewsNet (Shu et al., 2020), COSMOS (Aneja et al., 2021), InfoSurgeon (Fung et al., 2021), Factify (Mishra et al., 2022), Fauxtography (Zlatkova et al., 2019), and Mocheg (Yao et al., 2023). However, to the best of our knowledge, there are no existing datasets for multi-hop multimodal claim verification, which challenges the system's reasoning capability by requiring it to integrate and interpret multiple pieces of evidence from different modalities.\nMulti-hop Reasoning. Verifying complex claims often requires multi-step (multi-hop) reasoning (Mavi et al., 2022), which requires combining information from multiple pieces of evidence to predict the veracity of a claim. Many recently proposed datasets are created to challenge a model's ability to reason across multiple sentences or documents. These include MultiRC (Khashabi et al., 2018), QAngaroo (Welbl et al., 2018), Complex WebQuestion (Talmor and Berant, 2018), HotpotQA (Yang et al., 2018), and HoVer (Jiang et al., 2020). In contrast to these datasets, MMCV incorporates context from various modalities, such as images and tables, further challenging the system's ability to understand and integrate evidence from different sources.\nConstruct Synthetic Dataset with LLMs. The emergence of advanced large language models has sparked growing interest in automating the data annotation process using LLMs (Tan et al., 2024), driven by their advanced capabilities, including in-context learning (Dong et al., 2022) and learning from human feedback (Ouyang et al., 2022). (Wang et al., 2023) propose an explain-then-generate pipeline using LLMs for iterative data synthesis, while (Pace et al., 2024) combine the Best-of-N and Worst-of-N sampling strategies to introduce the West-of-N approach. With this same objective, the multi-hop claims in MMCV are created and refined by LLMs using human feedback, following guidelines and rules specifically designed to enforce a multi-hop structure within each claim."}, {"title": "3 The MMCV dataset", "content": "The main goal of our work is to compile a diverse and extensive collection of multi-hop claims that require joint reasoning across evidence from different modalities, such as text, tables, and images, for verification. One approach to achieving this is to transform multimodal question-answering pairs into atomic claims and refine them to incorporate additional reasoning steps, making them more natural. However, there are two major challenges in creating such a dataset: first, building a large-scale dataset is labor-intensive and costly; second, in our pilot studies, we found that simply providing instructions to crowd workers and asking them to rewrite multi-hop claims is counterproductive, as it is difficult to control quality and challenging for workers to create meaningful multi-hop claims. Instead, we develop a pipeline that leverages the emerging capabilities of large language models to generate text and learn from feedback, with human input to ensure the quality of the final output.\nIn this approach, LLMs handle the mundane task of rewriting claims consistently according to the instructions, while human effort is significantly reduced to quality control of the final claims based on a set of guidelines. \n3.1 Claim Generation\nIn this stage, we leverage the in-context learning capabilities of large language models to transform question-answer pairs from the MultimodalQA dataset (Talmor et al., 2021) into verifiable claims. To minimize the impact of in-context examples on the quality of the generated claims, we carefully craft a pool of 20 in-context examples and randomly select 3 for use during execution. The claims are formulated to ensure that no information is omitted from the original QA pairs and no new information is introduced. Since the claims are derived directly from the question and the correct answer, they are automatically labeled as SUPPORT. \n3.2 Claim Refinement\nAfter generating the initial claims from the question-answer pairs, we modify and refine them to ensure they are more naturally phrased and more accurately supported by the facts. Next, we review the claims for any factual errors that may have been introduced during the modification process and make corrections as needed.\nClaim Modification and Refinement. To introduce additional reasoning steps to the claim candidate, we employ a modify-then-refine approach that iteratively enhances the quality of the modified claim candidate based on feedback from LLMS (Pan et al., 2023a). Specifically, we begin by identifying the Wikipedia entities mentioned in the answers from the question-answer pairs. If there is only one Wikipedia entity in the answer, we leave the claim candidate unchanged. However, if there are multiple Wikipedia entities, we use the summaries of their respective Wikipedia articles as context and instruct the LLMs to modify the claim in such a way that it incorporates this contextual information to replace the entity, ensuring that the entity's name does not appear directly in the claim. To help LLMs understand the modification task, we provide them with 3-5 randomly selected in-context examples from a pool of hand-crafted examples. After modifying the claim, we obtain feedback from LLMs regarding the fluency, correctness, and clarity of the modified claim. \nIf the feedback suggests further improvement, the claim is sent back to the modification step, incorporating the LLMs' feedback until a certain iteration threshold is reached. If the modified claim still does not pass the quality check, it is marked for manual review and revision by human annotators.\nRAG-based Truthfulness Validation. Since we introduce additional contextual information from Wikipedia when modifying the claims, there is a risk that LLMs might hallucinate and produce outputs that are not faithful to the input context. To eliminate potential factual errors, we use a retrieval-augmented generation (RAG) (Lewis et al., 2020)-based pipeline to retrieve the full Wikipedia articles of the relevant entities and validate the factual accuracy of the modified claims. To mitigate the impact of prompt sensitivity on the model's output (Lu et al., 2022; Sclar et al., 2023), we diversify the prompts by randomly changing their format for each verification step. For instance, instead of consistently using Is it true that {claim}?, the prompt is randomly chosen from a set of equivalent alternatives, such as Verify the following statement: {claim} or What evidence supports the claim that {claim}?\n3.3 Claim Annotation\nAt this stage, we have obtained claims that have been modified and refined by LLMs and factually validated by RAG-based pipelines. Next, we use LLMs to generate negated claims by applying a set of specific negation rules. We employ three distinct methods for generating these negated claims. For instance, given the claim, \u201cSince its construction in 1889, the Eiffel Tower in Paris attracts millions of visitors annually.\u201d, the results after applying the negation rules are as follows:\nNext, a group of human annotators is tasked with evaluating the claims based on three dimensions: fluency, correctness, and clarity, scoring each dimension on a scale of 1 to 5. Fluency assesses how naturally the claim reads, as outputs generated by language models can sometimes sound artificial. Correctness evaluates whether the claim is factually accurate based on the evidence. Clarity determines if the claim is easily understood, as entity substitution might make it difficult to comprehend. Once the claims are scored, the average of the fluency, correctness, and clarity scores is calculated to determine the final score for each claim. If a claim's final score falls below a predetermined threshold, it is flagged and sent back to the annotators for manual revision."}, {"title": "4 Dataset Analysis", "content": "Dataset Statistics. MMCV contains 16,439 multi-hop multimodal claims, with their statistics detailed in Table 2. The number of hops is determined by the count of multimodal evidence associated with each claim. The dataset includes a balanced distribution of SUPPORT and REFUTE claims. Specifically, there are 6,178 1-hop claims with an average of 21.7 tokens per claim; 8,969 2-hop claims averaging 25.32 tokens per claim; 870 3-hop claims with an average of 25.44 tokens per claim; and 422 4-hop claims averaging 26.17 tokens per claim. \nMulti-hop Reasoning Types. We provide examples of each reasoning type in Table 6. Most 1-hop and 2-hop claims require at least one supporting fact from either image or table evidence for verification. In contrast, the majority of 3-hop and 4-hop claims require evidence from all three modalities. The process of removing a bridge entity and replacing it with a relative clause or phrase significantly increases the informational load of a single hypothesis. As a result, some 3-hop and 4-hop claims are relatively longer and exhibit complex syntactic and reasoning structures. Our experimental results also indicate that the difficulty for models to verify claims escalates as the hop count increases."}, {"title": "5 Experiments and Results", "content": "In this section, we discuss our experiment settings (\u00a75.1), the experiment results (\u00a75.2), and the error analysis (\u00a75.3). We begin by formally defining the MMCV task below.\nTask Definition. The formulation of multi-hop multimodal claim verification is defined as follows: Given a claim $C$, and a list of multimodal evidence $E(C)$, which includes text, images, and tables, the system must reason over all the evidence and predict the label of the claim as either SUPPORT or REFUTE."}, {"title": "5.1 Experiment Settings", "content": "As there are no existing models specifically designed for multi-hop multimodal supervised claim verification, we conduct our experiments using MLLMs. Moreover, previous studies in textual claim verification and multimodal claim verification indicate that LLMs and MLLMs can significantly enhance task performance compared to traditional supervised approaches (Pan et al., 2023b; Wang and Shu, 2023; Li et al., 2024; Geng et al., 2024). Furthermore, supervised methods often require extensive annotated corpora, which are difficult to acquire and limit domain transferability, as training data typically covers only a single domain.\nZero-shot Claim Verification. We establish performance baselines for zero-shot multimodal claim verification using various MLLMs under two settings. In the closed-book setting, the model does not retrieve information from external knowledge sources and must rely on its parametric (internal) knowledge to verify the claim. In the open-book setting, the model is provided with a set of gold evidence. Specifically, we use the prompt from (Geng et al., 2024), which extracts the models' predictions, explanations, and confidence levels.\nMLLM. We utilize two state-of-the-art MLLMS: GPT-40 (Achiam et al., 2023) and Gemini 1.5 Flash (Team et al., 2023). Additionally, we evaluate the performance of an open-source MLLM, LLaVA-V1.5-7B (Liu et al., 2024a), on MMCV. The temperature is set to 0.0, and the maximum number of tokens is set to 5000.\nPrompts for Enhanced Reasoning In addition to the prompt mentioned above, we conduct experiments using specialized prompting techniques aimed at eliciting reasoning from LLMs, such as Chain-of-Thought (Wei et al., 2022) and Self-Ask (Press et al., 2023). We also test symbolic-guided reasoning prompts like ProgramFC (Pan et al., 2023b) and Visual Programming (Gupta and Kembhavi, 2023). \nHuman Performance To benchmark human performance on our dataset, we used the same randomly selected examples employed in the enhanced reasoning prompt experiments. We recruited four experts in automated fact-checking research to classify multihop claims from MMCV based on the provided evidence. The SMART (Chew et al., 2019) framework \u00b9 was used to deploy the annotation task, and human performance was evaluated using the macro F-1 score."}, {"title": "5.2 Experiment Results", "content": "Main Results. We report the comprehensive results of the three MLLMS on MMCV in Table 3, highlighting the best-performing models for each hop under both open-book and closed-book settings. Overall, Gemini 1.5 outperforms others in the open-book setting with an average F-1 of 70.92, while LLaVA achieves the highest performance in the closed-book setting with an average F-1 of 66.77. This is surprising, given that LLaVA is a much smaller model compared to GPT4-0 and Gemini, and therefore possesses less parametric knowledge. Upon manually analyzing a subset of 100 randomly selected outputs from LLaVA, we found that the model frequently hallucinates, even when it predicts the correct label, particularly as the hop count increases. This is consistent with its open-book performance, where its accuracy declines when provided with gold evidence. Additionally, we observe that GPT4-o performs slightly better in closed-book settings than in open-book settings, suggesting a tendency to hallucinate. In contrast, Gemini's performance drops significantly in closed-book settings compared to open-book, demonstrating its robustness in effectively utilizing provided gold evidence.\nConfidence Level Analysis The left panel of Figure 3 presents the confidence distributions for all three MLLMs, categorized by the number of hops and divided into 10 intervals. The results show that the majority of the MLLMs are concentrated in the 90-100 confidence range, with only a small number exhibiting low confidence (0-10 range), which occurs solely in open-book settings. This indicates that the MLLMs consider the provided gold evidence.\nThe right panel of Figure 3 displays the calibration curves, illustrating the relationship between the models' confidence levels and their actual classification accuracy. These curves reveal a positive correlation between confidence and accuracy for 1-hop and 2-hop claims, as exemplified by the red line (GPT-4-o on 2-hop), the teal line (LLaVA on 1-hop), and the purple line (Gemini on 1-hop). In contrast, the downward curves, mostly observed in 3-hop and 4-hop claims, suggest that the models tend to be overconfident when classifying more complex claims. Additionally, the results indicate that open-book settings generally have better-calibrated confidence scores than closed-book settings, further suggesting that the models exhibit overconfidence when not provided with gold evidence.\nReasoning Prompt Results. Table 4 reports the performance of Gemini and GPT4-o on the randomly sampled subset of MMCV under open-book settings using various prompts that elicit LLMs\u2019 reasoning abilities. For symbolic approach, we ask LLMs to first generate a Python-like program that decomposes the mutli-hop claim into a set of function calls that describe the reasoning steps required to verify the claim, and use the symbolic information provided by the generated program to elicit better step-by-step reasoning from the model. We observe that GPT-4-o gains more from the enhanced reasoning prompt compared to Gemini, achieving a higher average F1 score of 75.93 in symbolic guided reasoning, whereas Gemini attains an average F1 score of 66.42 for the same task. Additionaly, we found that Symbolic approach are more effective on 4-hop claims, having a higher F1 score than CoT and self-ask. However, this observation is different on simpler 2-hop and 3-hop claims, where CoT appears to be more effective.\nHuman Performance Results To establish human performance on our dataset, we randomly sampled 200 examples, with 50 examples from each hop from MMCV. We recruited four annotators to perform claim verification given the gold evidence. We trained our annotators on the task by providing them with guidelines and sample annotations to ensure consistency and accuracy in their evaluations. After training, the annotators independently verified each claim using the provided gold evidence, allowing us to assess the human baseline performance on the dataset. We observe that the human annotators achieve very high performance in verifying the claims across all 4 hops. The human performance is 23.3% and 27.3% higher than the best-performing MLLMs on 3-hop and 4-hop claims respectively. This suggests that although MLLMs perform relatively well, there is still room for improvement to match human performance."}, {"title": "5.3 Error Analysis", "content": "Figure 5, 6, and 7 shows the error analysis of the false positive examples from GPT4-0, Gemini, and LLaVA respectively. We observe that visual mis-interpretation is a major issue, with the system often misidentifying or miscontextualizing image elements. This problem is especially pronounced in examples involving sports logos and movie posters, highlighting the need for improvements in the visual processing component.\nAnother notable issue is the system's handling of temporal and factual information. Errors related to player career timelines and historical events reveal shortcomings in temporal reasoning and the integration of world knowledge. The system's confidence levels, often between 80% and 100% for incorrect predictions, suggest a miscalibration in certainty estimation. This overconfidence in erroneous conclusions highlights the need for a more refined approach to confidence scoring.\nLast but not least, examples from higher hop categories reveal significant weaknesses in handling complex reasoning tasks. The system often struggles with multi-step logical inferences, frequently failing to coherently link disparate pieces of information. This limitation is especially problematic for claims that require advanced analysis or the cross-referencing of multiple facts."}, {"title": "6 Conclusion", "content": "In this paper, we introduce MMCV, a multi-hop multimodal claim verification dataset that requires models to aggregate information from up to four multimodal evidence to verify a claim. To create this large-scale dataset, we developed a novel data collection pipeline that leverages the capabilities of LLMs combined with human feedback. Specifically, our approach includes a module that iteratively refines modified claims using feedback from a judge LLM based on a set of predefined criteria, as well as an actuality validation module that employs RAG to ensure the factual accuracy of the claims. Our results show that state-of-the-art MLLMs struggle to verify more complex claims as the number of reasoning hops increases, often displaying overconfidence in their predictions. We also present findings from experiments utilizing prompts tailored to enhance the reasoning abilities of MLLMs, alongside human performance benchmarks for comparison. Additionally, we categorize and provide a detailed error analysis of false positive results from each model. We hope that MMCV will inspire the development of models capable of conducting complex, multi-hop reasoning in the challenging task of multimodal claim verification."}, {"title": "7 Limitations", "content": "We identify two main limitations of MMCV. First, the construction of MMCV depends on in-context learning coupled with self-refinement to convert a natural language question-answer pair into a multi-hop claim. While this method has proven to be effective, it may face difficulties when dealing with questions with intricate grammar structures and logical structures. This arises from the difficulty in conveying complex grammatical rules to the language model through a limited number of demonstrations within a constrained context size. Second, our aggregation method purely relies on LLMs themselves, which could introduce potential hallucination problems. On the other hand, by using a more robust logic solver could help with the hallucination issues, but there would be a tradeoff between the applicability and the robustness of the model."}, {"title": "8 Ethical Statement", "content": "Biases. We acknowledge the possibility of biases existing within the data used for training the language models, as well as in certain factuality assessments. Unfortunately, these factors are beyond our control.\nIntended Use and Misuse Potential. Our models have the potential to verify complex multimodal claims. However, it is essential to recognize that they may also be susceptible to misuse by malicious individuals. Therefore, we strongly urge researchers to approach their utilization with caution and prudence.\nEnvironmental Impact. We want to highlight the environmental impact of using large language models, which demand substantial computational costs and rely on GPUs/TPUs for training, which contributes to global warming. However, it is worth noting that our approach does not train such models from scratch. Instead, we use few-shot in-context learning. Nevertheless, the large language models we used in this paper are likely running on GPU(s)."}, {"title": "A.1 Dataset Example", "content": "Here is an example of dataset schema from MMCV:\nAdditional examples of 1-hop, 2-hop, 3-hop, and 4-hop claims are listed in Table 6"}, {"title": "A.2 Experiment Prompt", "content": "Claim Verification Prompt. To test MLLMs' claim verification performance under zero-shot settings, we follow (Geng et al., 2024) and use the following prompt."}, {"title": "A.3 Annotation Guidelines", "content": "We ask our annotators to score the quality of the claim from three aspects: fluency, correctness, and clearness. Here is the detailed guidelines provided to the human annotators."}, {"title": "A.4 Crowd Worker Interface", "content": "We use SMART (Chew et al., 2019), an open-source project designed to help data scientists and research teams efficiently build labeled training datasets for supervised machine learning tasks."}]}