{"title": "MACHINES OF MEANING", "authors": ["Davide Nunes", "Luis Antunes"], "abstract": "One goal of Artificial Intelligence is to learn meaningful representations for natural language expressions, but what this entails is not always clear. A variety of new linguistic behaviours present themselves embodied as computers, enhanced humans, and collectives with various kinds of integration and communication. But to measure and understand the behaviours generated by such systems, we must clarify the language we use to talk about them. Computational models are often confused with the phenomena they try to model and shallow metaphors are used as justifications for (or to hype) the success of computational techniques on many tasks related to natural language; thus implying their progress toward human-level machine intelligence without ever clarifying what that means.\nThis paper discusses the challenges in the specification of machines of meaning, machines capable of acquiring meaningful semantics from natural language in order to achieve their goals. We characterize \"meaning\" in a computational setting, while highlighting the need for detachment from anthropocentrism in the study of the behaviour of machines of meaning. The pressing need to analyse AI risks and ethics requires a proper measurement of its capabilities which cannot be productively studied and explained while using ambiguous language. We propose a view of meaning to facilitate the discourse around approaches such as neural language models and help broaden the research perspectives for technology that facilitates dialogues between humans and machines.", "sections": [{"title": "1 Introduction", "content": "Language is used to express our thoughts and communicate about the world. Ever since the inception of Artificial Intelligence (AI) as a field, language has been a key target-subject of research, with the goal of learning meaningful representations of natural language expressions. We find that recent developments of machine learning models for natural language processing tasks, while promising, also lead to a fundamental misunderstanding of the capabilities of these models. For example, language-model-based approaches are often presented as being capable of \"understanding\" language or capturing the notion of meaning [1, 2] without ever analysing how model capabilities and behaviour differ from the human experience. Despite the study of language greatly benefiting from quantitative methods over the last decades [3], the field of AI seems to lean towards using the formal and quantitative nature of its approaches to bypass the problem of what meaning is, whilst making use of shallow metaphors to justify and hype research directions.\nTo create meaningful representations for natural expressions we need to make clear how they are meaningful and to whom. We tackle the question of how to build machines of meaning, their limitations, how these can be said to be capable of understanding, how they deal with meaning, semantics, and grounding. In summary, this paper discusses how representations of expressions are connected to the world from which they emerge and what that means for computational approaches to language.\nThe paper is structured as follows. We start with the connection between language and behaviour, more precisely, what are meaningful symbols and how these are different from other signs. We then discuss how these symbols are grounded and what makes them referential to objects (concrete, metaphorical, or behavioural). The following section characterizes meaning in the computational setting. We show how concepts originated in philosophy of language, and quantitative methods in linguistics found their way to computational semantics, and more recently to Large Language Models (LLMs). Finally, we highlight two important problems in contemporary approaches to language modelling that we see as major obstacles to the creation of machines of meaning."}, {"title": "2 On Symbols", "content": "In order to discuss when and how symbols can be about things in the world, and how we can create machines that establish that connection, we need to clarify the notion of symbol. A long tradition in Semiotics makes the distinction between a symbol, the object the symbol references, and the concept associated with the symbol (see figure 1). For example, we might talk about the object \"heart\", the concrete human organ that pumps blood throughout your body, and then the concept of \"heart\" which might be related to all heart-shaped objects. An object doesn't necessarily need to be a physical object in the real world, it may also be an abstraction or another symbol.\nEach individual connects different objects, contexts, symbols, and concepts for purposes of communication. This \"semiotic network\" gets dynamically expanded and reshuffled every time you experience, think, and interact with the world. For example, when a speaker wants to draw attention to an object or situation, she can use the"}, {"title": "3 Grounding", "content": "In 1980, Searle put forward his Chinese Room Argument (CRA) [13] in order to contradict the notion that intelligent behaviour is the outcome of purely computational processes in physical symbol systems [7, 14]. Searle's argument touches on two key topics: the inability of AI to deal with symbols that are meaningful to the machine itself (grounded in its experience); and the difference between programs and machines (hardware). This argument is therefore at the core of what we call in AI, the grounding problem.\nThe argument states that only very special kinds of machines can think, with internal causal powers equivalent to those of the brain, and that AI has little to tell us about thinking since AI is \"about programs and no program by itself is sufficient for thinking.\" therefore, machines cannot know the meaning of natural language expressions. The argument can be summarized as follows.\nSearle is inside a room and possesses a set of rules (in English) that instruct how to produce sequences of Chinese symbols as a response to input Chinese symbols.\nSearle doesn't know Chinese and to him, Chinese writing is just a series of meaningless squiggles.\nUnknown to Searle, people outside the room feed him a script, a story, and a set of questions (all in Chinese) for which Searle supplies the answers by using the set of rules (the program).\nSearle assumes that to produce Chinese, he is simply instantiating a computer program. But from the point of view of someone reading the answers to the Chinese questions outside the room, the answers seem to be given by someone fluid in Chinese. This is the scenario under which Searle examines the claims:\n1. The programmed computer understands the stories;\n2. The program in some sense explains human understanding.\nThe first conclusion of the argument is that a computer may make it appear to understand human language but could not produce real understanding. Hence, the test is inadequate. Searle argues that the thought experiment underscores the fact that computers merely use syntactic rules to manipulate symbol strings, but have no understanding of semantics and suggests that minds must result from biological processes and computers can at best simulate these biological processes. While we agree with the part of the criticism, the entire argument is fundamentally flawed.\nThe CRA raises the question that the understanding of the room is not grounded in the experience of using Chinese. Perhaps justifiably so (in light of the technology available at the time), Searle assumes that the rules to manipulate language are not sufficient for understanding. We object that you can make that assumption about the"}, {"title": "4 The Meaning of Meaning", "content": "In recent advances in AI in the domain of natural language, the word meaning is treated superficially as if no further clarification is necessary. Often, such advances are contrasted with human performance in various benchmarks and make vague claims about language understanding or representation without elaborating on what this entails. This anthropocentric but superficial discourse surrounding AI for natural language creates misconceptions, unwarranted hype, and further contributes to stifling possible research directions. We shall try to clarify the notion of meaning in a way that helps with creating a definition for machines of meaning. We will discuss twoO perspectives on the notion of meaning that are frequently conflated. Furthermore, we can look at meaning in terms of its origin, how does meaning come about; and its ontological nature, or what exactly is meaning.\nWittgenstein's work [29] and the one of Saussure [30] are seminal in tackling many problems surrounding meaning, problems which were inherited from 19th century Philosophy and Linguistics. At the turn of the 20th century, both Saussure and Wittgenstein departed radically from the practices of their disciplines by doubting whether the questions posed by their predecessors and contemporaries made any sense at all. One such misconception was that words are names of objects or properties existing prior to language. This kind of surrogate view of language can be traced back"}, {"title": "5 Machines of Meaning", "content": "Many claims about computational model capabilities are often made through the use of words like understanding or meaning as if the use of such expressions requires no further clarification. In the previous sections we discussed and clarified these concepts and highlighted how their use can be problematic. We now discuss how existing concrete technical implementations of computational models are related to the philosophical concepts previously mentioned. Taking modern Language Model (LM) implementations as a starting point, we will discuss their limitations, and propose solutions to make such models truly embeddable in our linguistic world. We believe modern implementations of language models are already machines of meaning but misconceptions about meaning lead to a failed alignment of expectations between what these models can do and what we think they ought to be capable of. This is made evident for instance in the work of Bender et al., where language model capabilities are criticized as a way to make a case against the hype surrounding such technologies, especially when this unfounded optimism influences not only the research landscape, but also policymaking regarding the use of language modelling at scale [1, 2]."}, {"title": "5.1 From Structuralism to Multimodal Language Modelling", "content": "Wittgenstein and De Saussure established a new perspective for the study of meaning and for later developments in structuralism. Notably, Saussure introduced the basics of several dimensions under which language could be systematically studied, namely, syntagmatic and paradigmatic analysis, under which elements (e.g. words) in language are studied according to their contrast with other elements either from a syntactic or a lexical perspective, respectively [30]. This analytical framework is mostly concerned with the abstract structural patterns (phonetic, morphological, syntactic, or semantic) found in language, and later developments such as the work of Harris are mostly detached from the psychology of language, and instead focused on mathematical techniques for tackling these structural elements [48].\nThis analytical view over language presents a first bridge between linguistics, philosophy of language, and modern computational linguistics. One of the defining features of this line of work is the definition of distributional syntactic categories. For example, what makes cat and apple nouns is the fact that they can appear in the same position in sentences. Harris viewed the relations between elements in a syntactic structure as probabilistic relations such that the occurrence of one increases the probability that the other will occur. Structural linguistics finds its way into computational methods and in particular into Information Retrieval (IR) systems, in the form of the distributional hypothesis [49]:"}, {"title": "Definition 4 (Distributional hypothesis)", "content": "Words that occur in the same contexts tend to have similar meanings.\nThe underlying idea that \"a word is characterized by the company it keeps\" was also popularized by Firth [50] and is present in the work of Weaver in a memorandum on the possibility of using computers to solve the world-wide translation problem [51]. This characterizes semantics as the quantitative analyses of data, where text corpus (a large set of texts) can be used to build statistical models of word co-occurrence. The work in [52] discusses the distributional hypothesis and illustrates how this simple idea morphed into modern approaches for statistical semantics, and, more importantly, how a statistical analysis of structural elements in language turned into a problem of representation.\nStatistical Semantics ([51]) is the study of how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access [53]. Our contention is in the fact that this informal \u201cdisclaimer\" of \"at least to a level sufficient for information access\" is often overlooked, or completely ignored to make unsubstantiated claims about the capacity of computational models to deal with meaning or, more precisely, to capture the notion of meaning. For example \"a representation that captures much of how words are used in natural context will capture much of what we mean by meaning\" can be seen in [54]. This characterization of meaning is insufficient, for the reasons we previously discussed, but nevertheless is prevalent in most literature today.\nThe implication from Wittgenstein that \"meaning is use\" that we see transported to statistical semantics as a way to acquire and represent knowledge in [54] does"}, {"title": "5.2 From Structuralism to Large Language Models", "content": "It is easy to see the appeal of computational methods to investigate the distributional hypothesis, nevertheless, in the literature on language modelling, there is often a conceptual jump from (1) modelling the structural patterns of language to (2) we can represent meaning because meaning is use. Nevertheless, we can still find in each methodological progress of computational methods, steps towards a feasible version of what we could consider an artificial MoM.\nThe foundations of language modelling are laid out in [56], where the production of text in a given language (in this case English) is modelled by a set of conditional probabilities, what we call n-gram models. The probability of a given word to occur in a language is characterized as a product of conditional probabilities of the previous words. To the computational mechanism for obtaining these conditional probabilities, we call a language model. One of the problems with this kind of models lies in the fact that vocabularies are generally very large, so even for small sequences of words, the number of parameters in a model would be huge because we need to keep track of the probability of all words to co-occur with all contexts. An n-gram model is also incapable of dealing with unknown words, it can only parse predetermined vocabularies. Moreover, data sparsity means that some contexts may not occur frequently or at all in the training data, resulting in low or zero probabilities. Finally, it's difficult"}, {"title": "5.3 The Prediction Frame Problem", "content": "Social media features the use of what is often called \"bad language\": text that defies our expectations about vocabulary, spelling, and syntax. The Natural Language Processing (NLP) community's response to this phenomenon has largely followed two paths: normalization and domain adaptation. By adopting a model of normalization, we declare one version of the language to be the norm, and all others to be outside that norm; by resorting to domain adaptation we can improve accuracy on average for tasks that deal with that domain, but it is certain to leave many forms of language out, because certain niches are not necessarily coherent domains [70]. Both problems expose a fundamental methodological issue in computational approaches to language, the fact that models cannot adapt to unseen lexicon. The question becomes: how do we learn to model or make predictions in domains where a lexicon is not available a priori (e.g., how do we train classification models on an unknown number of classes)?\nNeural language models have found success in many prediction problems from machine translation to speech recognition, and even in ill-defined and open-ended tasks like answering questions in conversational settings. Part of what enables this success is how the problem space is encoded and the learning procedure that allows for the"}, {"title": "6 Conclusion", "content": "In any machine of meaning, semantics should be established based on the machine's goals and motivations. If we wish to design such machines closer to our idealized (but often ill-defined) human capabilities, they must also not fall prey to the modern frame problem: they must be capable of processing and incorporating any given symbols as"}, {"title": "Definition 1 (Symbol).", "content": "A symbol is a pattern of behaviour that expresses meaning, but it is not meaningful in itself. More precisely, it is the rendering of a social norm, being it written (for example a series of characters forming a word), spoken, gestured, or another pattern with a semantic role (e.g. smoke signals, Morse code, etc)."}, {"title": "Definition 2 (Grounding).", "content": "The grounding of a symbol refers to the process whereby an agent updates its world model by experiencing the contexts where the use of that symbol is relevant and relevance is guided by the agent's goals."}, {"title": "Definition 3 (Meaning).", "content": "Meaning is the learned connection between a symbol and its referent in a context. A context is a set of features, or consistent behaviour. What makes the features of a certain linguistic context meaningful is its importance for a certain goal such as survival or coordination."}, {"title": null, "content": "x \u2295 y = x + y  if x, y < 57 , x \u2295 y = 5 otherwise"}]}