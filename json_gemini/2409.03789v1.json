{"title": "BreachSeek: A Multi-Agent Automated Penetration\nTester", "authors": ["Ibrahim AlShehri", "Adnan AlShehri", "Abdulrahman AlMalki", "Majed Bamardouf", "Alaqsa Akbar"], "abstract": "The increasing complexity and scale of mod-\nern digital environments have exposed significant\ngaps in traditional cybersecurity penetration test-\ning methods, which are often time-consuming,\nlabor-intensive, and unable to rapidly adapt to\nemerging threats. There is a critical need for\nan automated solution that can efficiently identify\nand exploit vulnerabilities across diverse systems\nwithout extensive human intervention. Breach-\nSeek addresses this challenge by providing an AI-\ndriven multi-agent software platform that leverages\nLarge Language Models (LLMs) integrated through\nLangChain and LangGraph in Python. This sys-\ntem enables autonomous agents to conduct thor-\nough penetration testing by identifying vulnerabil-\nities, simulating a variety of cyberattacks, execut-\ning exploits, and generating comprehensive secu-\nrity reports. In preliminary evaluations, Breach-\nSeek successfully exploited vulnerabilities in ex-\nploitable machines within local networks, demon-\nstrating its practical effectiveness. Future develop-\nments aim to expand its capabilities, positioning it\nas an indispensable tool for cybersecurity profes-\nsionals.", "sections": [{"title": "1 Introduction", "content": "The rapid evolution of cyber threats has un-\nderscored the limitations of traditional cyberse-\ncurity practices, particularly in the domain of\npenetration testing. Manual penetration testing,\nwhile thorough, is inherently time-consuming and\nincreasingly ineffective in keeping pace with the\ngrowing sophistication and diversity of cyberat-\ntacks. In an era where networks and applica-\ntions are constantly exposed to new vulnerabili-\nties, there is a pressing need for automated so-\nlutions that can efficiently identify, exploit, and\nreport on these weaknesses.\nRecent advancements in Artificial Intelligence\n(AI) and Natural Language Processing (NLP)\nhave opened up new possibilities for automat-\ning complex tasks, including cybersecurity. Large\nLanguage Models (LLMs), renowned for their ca-\npabilities in natural language understanding and\ngeneration, have demonstrated the potential to\nperform tasks that traditionally required signif-\nicant human expertise. Despite these advance-\nments, the application of LLMs in cybersecu-\nrity, particularly for automating penetration test-\ning, remains largely underexplored, presenting an\nopportunity to revolutionize how security assess-\nments are conducted.\nBreachSeek addresses this critical gap by in-\ntroducing an AI-driven, multi-agent software plat-\nform specifically designed to automate penetration\ntesting for websites and networks. The platform\nleverages the power of LLMs through Lang Chain\nand LangGraph in Python, allowing autonomous\nagents to identify vulnerabilities, simulate a vari-\nety of sophisticated cyberattacks, and execute ex-\nploits with minimal human intervention. By au-\ntomating these processes, BreachSeek not only ac-\ncelerates the penetration testing workflow but also\nenhances the accuracy and comprehensiveness of\nthe results, providing a robust solution to the ever-\nevolving landscape of cybersecurity threats.\nOne of the key technical innovations in Breach-\nSeek is the use of multiple AI agents, each with\na distinct focus, to manage the complexity and\nbreadth of tasks involved in penetration testing.\nThis approach ensures that the system avoids run-\nning out of context window, a common limitation\nin LLMs, and allows for the separation of concerns.\nEach agent is tasked with a specific aspect of the\ntesting process, ensuring a high level of specializa-\ntion and accuracy. This design principle not only\noptimizes the performance of individual agents but\nalso contributes to the overall efficiency and effec-\ntiveness of the platform.\nThe platform's scalability further enhances its\nutility, enabling it to be deployed in a wide range\nof environments, from small to large-scale net-\nworks. By deploying multiple agents in differ-\nent containers, BreachSeek can efficiently manage\nlarge volumes of data and complex network archi-\ntectures, making it adaptable to various cyberse-\ncurity needs. This scalability is particularly bene-\nficial for organizations that operate in sectors with\nhigh security demands, such as finance, healthcare,\nand government, where the ability to rapidly and\naccurately identify vulnerabilities is crucial.\nIn summary, BreachSeek represents a signifi-\ncant advancement in the field of automated cy-\nbersecurity penetration testing. By combining the\npower of AI-driven agents with the flexibility and\nscalability required in modern network environ-\nments, BreachSeek offers a comprehensive solution\nthat addresses the limitations of traditional pen-\netration testing methods. As cyber threats con-\ntinue to evolve, tools like BreachSeek will become\nincreasingly vital in ensuring the security and re-\nsilience of digital infrastructure."}, {"title": "2 Literature Review", "content": "Recent advancements in large language mod-\nels (LLMs) have significantly impacted the field\nof cybersecurity, particularly in the automation\nof penetration testing. Traditionally, penetration\ntesting has been a manual and labor-intensive\nprocess, requiring significant expertise and time.\nHowever, the introduction of tools like Pentest-\nGPT marks a turning point in how these tasks\ncan be automated. PentestGPT leverages the ex-\ntensive knowledge embedded in LLMs to perform\ntasks traditionally handled by human penetration\ntesters. This tool has been evaluated using a\nbenchmark created from popular platforms like\nHackTheBox and VulnHub, which includes 182\nsub-tasks aligned with OWASP's top 10 vulner-\nabilities. The results indicate a remarkable im-\nprovement in task completion rates, with Pentest-\nGPT outperforming previous models like GPT-\n3.5 and GPT-4 by significant margins. This un-\nderscores its effectiveness in maintaining context\nthroughout complex testing scenarios, a critical\nchallenge in the application of LLMs to penetra-\ntion testing tasks [1].\nIn a broader context, the use of generative AI in\npenetration testing offers both opportunities and\nchallenges. On one hand, generative models can\nquickly identify vulnerabilities and generate test\nscenarios that might be missed by human testers.\nFor example, tools like Mayhem utilize techniques\nsuch as fuzzing and symbolic execution to uncover\nvulnerabilities in a fraction of the time it would\ntake a human tester. These models also bring a\nlevel of creativity to the process, simulating novel\nattack vectors that enhance the robustness of pen-\netration testing. On the other hand, challenges\nremain, particularly regarding the models' ability\nto fully grasp the broader context of testing sce-\nnarios. This can lead to incomplete or inaccurate\nresults, highlighting the need for further refine-\nment of these models to ensure they meet the spe-\ncific needs of different organizations [2]. Breach-\nSeek addresses some of these challenges by em-\nploying multiple AI agents to manage context win-\ndows, ensuring a more comprehensive understand-\ning throughout the penetration testing process.\nUnlike other tools, BreachSeek doesn't just gen-\nerate text-based outputs but also executes com-\nmands within a terminal, directly interacting with\nthe target environment.\nLLMs are not only transforming penetration\ntesting but are also being integrated into vari-\nous aspects of cybersecurity. Their applications\nextend to defensive measures, such as risk man-\nagement and automated vulnerability fixing. In\nthese areas, LLMs help automate complex tasks,\nreducing the need for human intervention and al-\nlowing for faster, more efficient responses to secu-\nrity threats. However, the effectiveness of LLMs\nis often limited by their ability to maintain con-\ntext over extended interactions, a challenge that\ncontinues to be a focal point in ongoing research.\nFuture advancements are expected to improve the\nadaptability of LLMs to specific organizational en-\nvironments, enabling them to continuously learn\nand remain effective against evolving cybersecu-\nrity threats [3]. Additionally, BreachSeek uniquely\ncontributes to this space by generating a compre-\nhsive, formatted PDF report that captures the\nentire journey of the penetration testing process,\nproviding valuable insights that are automatically\ndocumented and ready for review.\nThe integration of LLMs into cybersecurity,\nparticularly in automated penetration testing, rep-\nresents a significant step forward in enhancing se-\ncurity measures. However, these advancements"}, {"title": "3 Model Architecture and\nImplementation", "content": "3.1 Graph-Based Approach Using\nLangGraph\nOur model employs a graph-based architecture\nimplemented using LangGraph, enabling the cre-\nation of multiple specialized nodes that communi-\ncate with each other. This distributed approach\noffers several advantages:\n1. Enhanced performance through task distribu-\ntion across multiple nodes/agents\n2. Flexibility in customizing logic for individual\nnodes\n3. Mitigation of context window limitations by\ndistributing tasks\n3.2 General Model Workflow\nThe general workflow of our model, as illus-\ntrated in Figure 1, consists of the following com-\nponents:\n\u2022 Supervisor: Oversees the entire process, gen-\nerating action plans and identifying subse-\nquent steps\n\u2022 Specialized agents: Execute specific tasks\nwithin their domains of expertise\n\u2022 Evaluator: Assesses the output quality and\ntask completion accuracy"}, {"title": "3.3 Specific Architecture for Pene-\ntration Testing", "content": "For this study, we implemented a specialized\narchitecture (Figure 2) that adheres to the general\nworkflow while incorporating task-specific agents:\n1. Recorder: Maintains a summary of actions\nand generates a final report when prompted\n2. Pentester: Accesses tools including a shell\ntool and a Python tool, enabling the utiliza-\ntion of popular penetration utilities in a Kali\nLinux environment. Its primary role is to ex-\necute commands generated by the supervisor\nand report the output to the evaluator."}, {"title": "3.4 Implementation Environment", "content": "The model was deployed in a Docker-based Kali\nLinux environment hosted on RunPod. Key imple-\nmentation details include:\n\u2022 Development phase: Utilized Anthropic's\nClaude 3.5 Sonnet model\n\u2022 Testing and future deployment: Plans to use\nLlama 3.1, an open-source model allowing for\ncustomized fine-tuning\nThis architecture and implementation approach\nallow for a flexible, scalable, and efficient system\nfor automated penetration testing using large lan-\nguage models. The combination of specialized\nagents, a robust evaluation mechanism, and a su-\npervisory component enables complex, multi-step\noperations while maintaining coherence and goal-\ndirectedness throughout the penetration testing\nprocess."}, {"title": "3.5 Testing Methodology", "content": "For evaluation purposes, a Metasploitable 2 ma-\nchine was hosted on the same local network as the\nmodel. The model was then tasked with exploiting"}, {"title": "3.6 Web UI", "content": "As part of the product suite we offer, a web UI\nwas developed using NextJS for the front-end and\nFastAPI for the back-end. A sample from the web\nUI can be seen in the appendix."}, {"title": "4 Results", "content": "The efficacy of our model was initially evalu-\nated through qualitative assessment. Future work\nwill incorporate quantitative measures using es-\ntablished benchmarks and standardized examina-\ntions.\nPotential benchmarks may include the OWASP\nWeb Security Testing Guide (WSTG) [4", "5": "exam content as\na standardized measure of performance.\nIn our preliminary testing, the model success-\nfully exploited a Metasploitable 2 machine, achiev-"}]}