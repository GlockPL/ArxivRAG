{"title": "Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization", "authors": ["Rohan Reddy Mekala", "Frederik Pahde", "Simon Baur", "Sneha Chandrashekar", "Madeline Diep", "Markus Wenzel", "Eric L. Wisotzky", "Galip \u00dcmit Yolcu", "Sebastian Lapuschkin", "Jackie Ma", "Peter Eisert", "Mikael Lindvall", "Adam Porter", "Wojciech Samek"], "abstract": "In the realm of dermatological diagnoses, where the analysis of dermatoscopic and microscopic skin lesion images is pivotal for the accurate and early detection of various medical conditions, the costs associated with creating diverse and high-quality annotated datasets have hampered the accuracy and generalizability of machine learning models. We propose an innovative unsupervised augmentation solution that harnesses Generative Adversarial Network (GAN) based models and associated techniques over their latent space to generate controlled \"semi-automatically-discovered\" semantic variations in dermatoscopic images. We created synthetic images to incorporate the semantic variations and augmented the training data with these images. With this approach, we were able to increase the performance of machine learning models and set a new benchmark amongst non-ensemble based models in skin lesion classification on the HAM10000 dataset; and used the observed analytics and generated models for detailed studies on model explainability, affirming the effectiveness of our solution.", "sections": [{"title": "Introduction", "content": "The application of artificial intelligence (AI) and machine learning (ML) in the medical domain has garnered substantial interest due to its potential to aid health practitioners in diagnosing conditions, predicting patient outcomes, and personalizing patient care. In dermatology, AI and ML have demonstrated superior performance compared to dermatologists in analyzing dermatoscopy images [10]. AI/ML models can process vast quantities of images rapidly, assisting dermatologists in making faster and more accurate diagnoses, thereby improving\npatient care, and potentially saving lives. However, the success of such AI/ML models is fundamentally limited by the lack of availability of datasets with suf- ficient variations to reflect semantic occurrences in the real world. Additionally, privacy concerns and regulatory constraints pose a major hindrance towards procuring additional annotated medical image datasets, making it necessary to explore alternate ways of synthesizing these image variants in a reliable manner, while ensuring the photorealism and fidelity of the generated images.\nIn the domain of medical imaging, the concept of synthetic data generation has manifested remarkable strides across various disciplines and applications [11, 21, 24, 28, 34, 38]. Image synthesis, particularly through methods such as GANs and diffusion models, makes it possible to augment low-volume training data. These generative approaches have also been explored within the realms of dermatoscopy [2,4,8,9,30,32,37] and histopathology diagnostics [6, 15, 27, 31].\nHowever, existing methods for developing transformations in the GAN latent space predominantly rely on classification models to ensure the generated images have specific attributes. While these models have been instrumental in driving advances in image synthesis and manipulation, they come with significant draw- backs. Classification-based methods require large amounts of labeled data, which are often difficult to obtain due to privacy concerns, regulatory constraints, and the high costs associated with manual annotation. This reliance on labeled data can severely limit the scalability and applicability of these models, particularly for medical data modalities like dermatoscopy where annotated datasets for var- ious style variations are scarce. Moreover, since classification models are con- strained to predefined categories of semantics, the scope of transformations that can be learned is considerably restricted. This results in dependence on domain experts to identify semantics, with the added costs of procuring and annotating images with such semantics.\nIn this paper, we present a novel approach towards developing variations in medical images using this unsupervised method based on the latent space of GANs. Our approach leverages the capabilities of two advanced GAN models: StyleGAN2 [22] and HyperStyle [5]. Initially, we train the StyleGAN2 model on a comprehensive dataset of dermatoscopic images to generate high-quality syn- thetic images. Following this, we employ HyperStyle for GAN inversion, optimiz- ing latent features extracted from real images. We then implement closed-form factorization to identify meaningful and orthogonal latent semantic directions within the latent space. Finally, we validate and refine these directions to ensure they correspond to human-understandable and domain-relevant transformations. Our research extends beyond the realm of image generation, addressing the cru- cial need for evaluation metrics in the context of synthetic skin lesion images. We assess the perceptual similarity of the generated images using state-of-the-art metrics such as the Learned Perceptual Image Patch Similarity (LPIPS). These metrics provide a quantitative foundation for evaluating the fidelity of synthetic images in comparison to their real counterparts.\nTo further show the efficacy of our approach, we train classification mod- els on the augmented dataset, achieving state-of-the-art performance in lesion"}, {"title": "Our Approach", "content": "GANs have revolutionized the field of medical imaging by providing innova- tive augmentation-driven solutions to data scarcity and enhancing the quality of synthetic medical images. These GAN-based solutions have had a particular im- pact in domains such as radiology, pathology, and dermatology, where obtaining high-quality labeled data is often challenging due to privacy concerns, regulatory constraints, and the high cost of manual annotation [38]. In dermatoscopy, GANs have shown significant promise in synthesizing skin lesion images, which can be used to augment existing datasets and improve the performance of diagnostic models [9,36].\nA typical GAN architecture [18] consists of two neural networks: the gen- erator and the discriminator which work collaboratively through an adversarial training process. The generator creates new data samples, while the discrimina- tor evaluates them against real data to train the generator to produce images aimed at being indistinguishable from real images. This adversarial process con- tinues until the generator produces high-quality realistic images.\nOur approach leverages two state-of-the-art GAN models: StyleGAN2 [22] and HyperStyle [5]. StyleGAN2 stands out due to its architectural innovations, which include redesigned generator normalization, progressive growing, and the introduction of a style-based generator architecture. These enhancements enable the generation of highly realistic and detailed images by allowing the model to"}, {"title": "Background", "content": "control different levels of detail through the latent space. Compared to Varia- tional Autoencoders (VAEs), StyleGAN2 generally produces higher quality im- ages with sharper details and more coherent structures. While VAEs are effec- tive for generating diverse samples, they often suffer from blurrier outputs. On the other hand, compared to conditional diffusion models, StyleGAN2 typically achieves faster generation times and requires less computational resources, as diffusion models often involve iterative processes that are more computationally intensive. These advantages make StyleGAN2 particularly suitable for appli- cations requiring high fidelity and variability of the generated images. On the other hand, HyperStyle focuses on the challenge of image inversion, which in- volves mapping real images into the latent space of a GAN which is used by the generator to manipulate the image. HyperStyle employs a hybrid approach that combines the strengths of encoder- and optimization-based inversion techniques. By balancing image reconstruction and image editability, HyperStyle allows for accurate and flexible modifications of real images. This makes it a powerful tool for tasks that require fine-grained control over image attributes, such as gen- erating synthetic variations of medical images for training data augmentation. Together, these models provide a robust framework for our unsupervised trans- formation pipeline, enabling us to generate high-quality synthetic images using inverted codes from images obtained in the real world.\nTowards the final step of controlled augmentation generation, existing med- ical imaging research for developing transformations in the GAN latent space predominantly rely on classification models. While these models have been in- strumental in driving advances in image synthesis and manipulation, they come with significant drawbacks, as mentioned in Section 1. To address these con- cerns, we explore factorizing the latent space of the generator model as an alter- native approach to extract semantics in an unsupervised manner. Our proposed pipeline significantly reduces the dependency on scarce and costly labeled data. This unsupervised approach is inherently more scalable, as it can leverage vast amounts of unlabeled data, which is more readily available, thus facilitating the training of models on a broader spectrum of semantic variations.\nThe overall augmentation pipeline, detailed in Section 2.2, progresses step-by-step from a set of original images to the final outputs, incorporating semantic variations based on semi-automatically extracted features into the original im- ages. First, we apply closed-form factorization to identify meaningful and orthog- onal latent semantic directions within the latent space. Next, we utilize the GAN inversion function to map real images into the latent space accurately. Finally, using the semantic directions extracted through factorization, we produce new variants of the original images based on the identified semantics. This approach allows for the exploration of a broad range of semantic variations without the need for labeled data, ensuring that the synthetic outputs closely resemble the original inputs."}, {"title": "Unsupervised Transformation Pipeline", "content": "The proposed transformation development pipeline (see Fig. 1) consists of four stages that perform sequential tasks to achieve the goal of unsupervised seman- tic extraction. The end product is an unsupervised technique for transforma- tion development, enabling the semi-automatic extraction of extensive semantic transformations reflected within a dataset and corresponding domain.\n1. GAN training: Training a model based on the StyleGAN2 architecture.\n2. Factorization to extract eigenvectors of maximal variance (Transforma- tions): This crucial step identifies meaningful, orthogonal latent semantic directions within the latent space with closed-form factorization.\n3. GAN inversion: We train a HyperStyle-based GAN inversion model that comprises encoder and optimizer units, with the goal of obtaining latent features corresponding to real images.\n4. Identify relevant transformations: The orthogonal latent semantic direc- tions from the previous step correspond to a mix of human-understandable and domain-related concepts. Being able to translate the directions to these concepts contributes to the interpretability of the generated images. Besides, not all the directions produce relevant and unique transformations (i.e., mul- tiple directions may produce very similar transformations). A validation step is incorporated to ensure that only relevant transformations are considered.\nIn the following sub-sections, we will elaborate on our approach in the context of our case study within the dermatoscopy domain.\nGAN Training We trained the GAN with 10,758 images predominantly from the HAM10000 dataset [36] used in the ISIC 2018 challenge [12]. The 10k images from HAM10000 stem from various populations and modalities, with each image"}, {"title": "Latent-Space Factorization", "content": "Latent-Space Factorization As part of this sub-pipeline, to extract semantic directions or transformations, we employed closed-form factorization [22] within the latent space (z, w) of the generator to identify meaningful semantic direc- tions. In other words, we analyzed the generator's internal structure to uncover"}, {"title": "GAN Inversion", "content": "GAN Inversion As part of the GAN inversion sub-pipeline implementation, we trained a HyperStyle [5] based inversion model comprising dedicated encoder and optimizer units towards the task of latent code (w-space) computation of any image in the real world. For our implementation, we used the e4e (Encoder for Editing) [35] encoder, which is a specific type of encoder used to map real images into the latent space of the StyleGAN2 generator.\nThe e4e encoder was trained on the same dataset used for GAN training in the first phase of the pipeline. The training process involved minimizing the L2 loss (Mean Squared Error), a common metric for measuring the difference between the predicted output of the encoder and the actual target values. We achieved an L2 loss of 0.009, indicating the encoder's success towards distilling and capturing meaningful representations from the dataset."}, {"title": "Identify Relevant Transformations", "content": "Identify Relevant Transformations As part of this phase, we utilized a human-in-the-loop approach to systematically review and validate the seman- tic directions identified during the closed-form factorization phase. To facilitate this process, we developed a user-friendly dashboard by adapting and adding fea- tures to the SeFa (Semantic Factorization) dashboard [33]. The dashboard allows the interactive exploration and validation of the semantic transformations. Af- ter our modifications, the dashboard supports functionalities such as uploading or browsing images from the dataset, selecting a semantic direction, adjusting the magnitude of the transformation, and visually reviewing the outcome of the applied transformation. This interface is crucial for the interpretation and val- idation of the semantic meanings and of the relevance of the latent directions identified during factorization (corresponding to this direction and magnitude). To transition from factorization to identifying transformations, we imple- mented a method to apply the extracted eigenvectors of maximal variance to the latent vectors of real images. This process involves the following steps:\n1. We mapped dermatoscopic images into the latent space with the previously trained HyperStyle GAN inversion model.\n2. The identified semantic directions (eigenvectors) are then applied to the latent vectors of these images. By adjusting the magnitude of the directions, we can modulate specific attributes of the images, such as size, pigmentation, and texture of skin lesions.\n3. We used the dashboard to systematically review the transformations, en- suring they are meaningful and relevant to the domain. Multiple directions might produce similar transformations, so this step ensures that only unique and significant transformations are considered."}, {"title": "Classifier Training Enhancement With Synthetic Data", "content": "Experimental Setup We compared the predictive performance of a skin le- sion classification model trained on data augmented with synthetic images with a baseline. As baseline scenario, we trained on the original HAM10000 [36] der- matoscopy training dataset split and evaluated the performance of the model using the original test dataset split. HAM10000 includes 10,015 high-quality dermatoscopic images in the training set and 1,512 image in the test set. Each image is labeled with one specific diagnosis (see Section 2.2 above). Class label distribution is highly imbalanced in training and test set, with NV being over- represented with a share of 60% respectively 67%, while the other six classes share the remaining fraction to varying degrees.\nTo train the model with additional synthetic images, we first augmented the original training dataset with synthetic data as follows: we generated new im- ages from the original training dataset using five out of thirteen transformations that we had identified. The five transformations correspond to Size and Pigment Variation (SPV), Size Variation (SV), Background Color Variation (BCV), Geo- metric Variation (GV), and to Positional Shift (PS). The other transformations we excluded were variants of these five transformations (e.g., they correspond to different semantic layers in the generator)."}, {"title": "Evaluation", "content": "Evaluation\nIn total, we obtained five times the amount of the original dataset (total of 50,075 samples). Since we work with a much larger training dataset after the augmentation in comparison to the baseline scenario, we ensured that the ob- served classification performance change results not solely from the much larger dataset and thus longer training, but from the higher variety in the training data. For this reason, we employed early stopping (with a relatively high crite- rion of 25 epochs prior to initiating the early stop; assuring that the model would not profit from longer training) and created multiple \"synthetically augmented\" models by varying the number of synthetic images used for augmenting the train- ing datasets. Specifically, we randomly selected 400, 800, 1200, 1600, and 2000 of the generated synthetic images from each of the five transformations, and augmented the original training dataset with 2000, 4000, 6000, 8000, and 10000 images respectively. Note that the selections of synthetic images for each aug- mented dataset were done independently; i.e., the 2000 additional images were not a subset to the 4000 additional images.\nTo ensure that we are only adding \u201cgood\u201d synthetic images, we also generated a \"filtered\" augmented training dataset by removing the synthetic images which the unfiltered synthetically augmented models classified incorrectly. We filtered out 136 (6%), 367 (9%), 274 (4.5%), 916 (11.45%), and 505 (5%) images from the 2000, 4000, 6000, 8000, 10000 augmented images respectively.\nFor each augmented training datasets, we trained a classification model. This results in 10 synthetically augmented models: five models were trained using the unfiltered augmented datasets and five models were trained with the filtered augmented datasets. Hereafter, we will refer to the models trained using the unfiltered augmented dataset as SA-2k, SA-4k, SA-6k SA-8k, SA-10k (when the original dataset was augmented with the 2000, 4000, 6000, 8000, and 10000 syn- thetic images respectively) and the models trained using the filtered augmented dataset as SA-2k-filter, SA-4k-filter, SA-6k-filter, SA-8k-filter and SA-10k-filter.\nModel, Task, and Training We employed a DenseNet121 (8M parameters) and a DenseNet169 (14M parameters), initialized with weights pretrained on ImageNet [14], for multi-class classification. The two architectures enabled us to compare the impact of augmenting the training dataset with synthetic im- ages across varying architecture complexity. Because the label distributions are highly imbalanced, we used weighted oversampling to balance class distributions within training batches. Additional basic transformations (horizontal/vertical flip, cutout) and a dropout rate of 0.1 were employed. We used an Adam opti- mizer with a learning rate of le-5 and weight decay of le-4 and trained for 100 epochs, while initiating early stopping when the performance on the validation split did not improve for 25 epochs."}, {"title": "Transformations Developed", "content": "Transformations Developed\nTo compare our transformed images, we used the LPIPS metric for evaluating the transformed images as it provides a more nuanced 1:1 image-level comparison than FID (which is a measure for comparison of overall image distributions). The LPIPS score we employed is conditioned on the last three layers of the AlexNet architecture [26] trained on the ImageNet dataset and serves to quantify perceptual similarity by comparing deep feature representations extracted across the layers, empirically proven to align with human perceptual judgments.\nWe calculated the LPIPS metrics for the five transformations used in aug- menting the training datasets (see Tab. 1.) LPIPS score ranges between 0 to 1 where a lower LPIPS score denotes higher perceptual similarity.\nWe observe scores close to or lower than 0.1 for all our selected transfor- mations. In general, \"wayward or low-fidelity\" transformations exhibited scores > 0.2, which was the threshold used in selecting transformations for our task. Although the scores for our selected transformations show minimal perceptual change, we acknowledge the importance of domain expert validation to enhance confidence in the fidelity of our transformed images (note that downstream clas- sifiers were always tested on non-modified images). Additionally, in future work, we intend to condition the metric on an AlexNet architecture trained specifically on an unbiased skin lesion dataset to ensure higher resonance in comparison over the feature space."}, {"title": "Evaluation", "content": "Evaluation\nWe evaluated the model performance on the 1,512 images (512 x 512 pixels) of the original HAM10000 test split, which were neither transformed nor seen dur- ing training, using balanced multi-class accuracy. First, we compared our results with the existing benchmark [1] ('Task 3: Lesion Diagnosis') in the ISIC2018 challenge. Our best performing model was based on the DenseNet169 architec- ture, synthetically augmented with 6000 additional synthetic images (60% of the original training dataset), achieved a balanced accuracy of 0.856 (see Table 2). Comparing with other models evaluated in the challenge [1], we ranked 3rd on the evaluation metrics, with only the two ensemble based methods achieving a higher average balanced accuracy of 0.885 ('Top 10 Models Averaged') [29] and"}, {"title": "Model Analysis with Explainable AI", "content": "Model Analysis with Explainable AI\nThe confusion matrices in Fig. 6 reveal a significant improvement of the model's ability to correctly classify samples from class MEL. Whereas the baseline model only classified 54% of true Melanoma samples correctly, the model trained on (fil- tered) synthetic samples correctly labeled 67% of these samples. As the baseline model misclassified many true MEL test samples as BKL, which is particularly dangerous as this is a benign class, we further analyze the prediction behavior for this set of test samples. Specifically, we apply Concept Relevance Propaga- tion (CRP) [3] to compute concept-based explanations for individual predictions."}, {"title": "Conclusion and Future Work", "content": "Conclusion and Future Work\nOur research has established a robust foundation for the efficient and effective utilization of controlled augmentation using generative models to generate syn- thetic skin lesion images and consequently more accurate AI classification mod- els. Through our experiments, we have demonstrated significant improvements in model performance for skin lesion classification by using augmented datasets generated in a controlled manner with our implementation. In these experiments, the selection of augmented datasets for training was done by randomly sampling from synthetically generated images. To enhance the efficiency of this approach, in future work, we plan to modify our data selection strategy to be based on clustering characteristics within the latent space, thereby selecting images from which the model stands to learn the most. Additionally, we intend to implement a filtration module prior to augmented data selection, based on the development of fidelity and photorealism metrics and thresholds. To achieve this, we will build on the Learned Perceptual Image Patch Similarity (LPIPS) metric and aim to establish thresholds for the photorealism and fidelity of the augmented datasets selected for model training, thereby preventing unintentional data poisoning.\nWe plan to further explore the trade-off between photorealism and editability and investigate other inversion techniques to improve the optimization process. Additionally, future work will focus on enhancing our factorization techniques to produce high-fidelity directions of disentangled semantic variance. While our results currently lead the ISIC leaderboard for non-ensemble-based models, we believe that by shifting to an ensemble-based approach, we can surpass the per- formance of the leading ensemble-based models. We believe that our research can pave the way for future advancements in transfer learning and domain adapta- tion within dermatological diagnoses. We will further explore generalizability to other diagnostic tasks and datasets, as well as higher-dimensional image analysis such as hyperspectral tissue differentiation."}]}