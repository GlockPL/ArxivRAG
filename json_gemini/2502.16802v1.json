{"title": "Unsupervised Topic Models are Data Mixers for Pre-training Language Models", "authors": ["Jiahui Peng", "Xinlin Zhuang", "Jiantao Qiu", "Ren Ma", "Jing Yu", "Tianyi Bai", "Conghui He"], "abstract": "The performance of large language models (LLMs) is significantly affected by the quality and composition of their pre-training data, which is inherently diverse, spanning various domains, sources, and topics. Effectively integrating these heterogeneous data sources is crucial for optimizing LLM performance. Previous research has predominantly concentrated on domain-based data mixing, often neglecting the nuanced topic-level characteristics of the data. To address this gap, we propose a simple yet effective topic-based data mixing strategy that utilizes fine-grained topics generated through our topic modeling method, DataWeave. DataWeave employs a multi-stage clustering process to group semantically similar documents and utilizes LLMs to generate detailed topics, thereby facilitating a more nuanced understanding of dataset composition. Our strategy employs heuristic methods to upsample or downsample specific topics, which significantly enhances LLM performance on downstream tasks, achieving superior results compared to previous, more complex data mixing approaches. Furthermore, we confirm that the topics Science and Relationships are particularly effective, yielding the most substantial performance improvements. We will make our code and datasets publicly available.", "sections": [{"title": "Introduction", "content": "The performance of large language models (LLMs) is profoundly influenced by the quality and composition of their pre-training data (Longpre et al., 2024; Parmar et al., 2024; Gunasekar et al., 2023). To ensure high-quality data, two primary strategies are commonly employed: data selection and data mixing. Data selection involves filtering datasets based on predefined rules (Rae et al., 2021; Penedo et al., 2023; Soldaini et al., 2024) or classifiers (Penedo et al., 2024; Wettig et al., 2024; Xie et al., 2023b), while data mixing adjusts the proportions of data from different domains in the pre-training corpus. Compared to data selection, data mixing is more intuitive and controllable, making it a pre-ferred choice in LLM pre-training. However, the specific strategies for data mixing are rarely open-sourced by companies or research institutions, limiting reproducibility and transparency. Existing data mixing methods often rely on simple methods, such as temperature-based sampling (Parmar et al., 2024), or computationally expensive procedures, such as RegMix (Liu et al., 2024), which requires training numerous smaller models to explore optimal data ratios. These methods are either suboptimal and lack performance guarantees or require substantial computational resources, presenting significant challenges for researchers with limited access to large-scale computational infrastructure. This necessitates the development of more efficient and scalable data mixing strategies that can be broadly adopted.\nTopic modeling has long been a fundamental tool in natural language processing (NLP) for uncovering the latent thematic structure in large, unlabeled document collections (Blei et al., 2003; Grootendorst, 2020). Traditional approaches, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), rely on probabilistic graphical models, while more recent methods, such as BERTopic (Grootendorst, 2020), leverage contextualized embeddings from pre-trained language models like BERT (Devlin et al., 2019). These methods often select topics based on metrics like Term Frequency-Inverse Document Frequency (TF-IDF), which can fail to capture the nuanced semantics of document clusters. While LLMs exhibit remarkable capabilities in zero-shot text summarization and topic interpretation, existing topic modeling methods face two significant limitations: (i) Limited scalability to large datasets. These methods typically require substantial computational resources, mak-"}, {"title": "Related Work", "content": "The quality of pre-training data has been demonstrated to play a critical role in model performance, as highlighted in several studies (Longpre et al., 2024; Parmar et al., 2024). One natural and intuitive approach to improving data quality involves adjusting the weights assigned to different data domains. Data mixing methods aim to optimize the distribution of attribute weights within pre-training datasets. For example, methods such as DoReMi (Xie et al., 2023a) and DOGE (Fan et al., 2024) utilize small proxy models to generate domain weights, while DMLaw (Ye et al., 2024) and RegMix (Liu et al., 2024) determine domain weights by training a set of smaller models. More recently,Llama-3.1 (Dubey et al., 2024) employs downsampling to reduce the proportion of data from the arts and entertainment domain, and Chen et al. (2024) investigates effective training strategies by adjusting topic weights. However, these studies lack details of domain weights and primarily explore data mixing from a domain-level perspective. In this paper, we propose incorporating topic modeling to control data weights at a finer granularity, enabling more precise adjustments to the pre-training data and enhancing LLM's capabilities."}, {"title": "Topic Model", "content": "Topic modeling is an unsupervised method used to uncover abstract topics within documents in the field of Natural Language Processing (Wu et al., 2024). Traditional approaches, such as Latent Dirichlet Allocation (LDA), typically rely on probabilistic techniques to generate topics (Blei et al., 2003). BERTopic (Grootendorst, 2020) leverages transformer-based architectures to enhance traditional topic modeling processes. More recent research has explored the use of LLMs for topic modeling, particularly by utilizing their text summarization capabilities to automatically assign descriptive labels to clusters of words. For instance, (Rijcken et al., 2023) demonstrated that approximately half of the topic labels generated by ChatGPT were considered useful by human evaluators. Additionally, other studies (Mu et al., 2024a,b; Rijcken et al., 2023) have conducted extensive experiments to improve the performance of LLMs in topic modeling through prompt engineering. However, these methods become computationally expensive and impractical when applied to large-scale datasets. In contrast to these approaches, we propose utilizing topic model for data mixing, enabling more fine-grained control over data weights. This approach not only improves the efficiency of pre-training LLMs but also provides valuable insights into the role of topic-level adjustments in optimizing data distributions for enhanced model performance."}, {"title": "Data Weave", "content": "DataWeave is a novel framework designed to address large-scale datasets by integrating multi-stage clustering with topic extraction, thereby facilitating effective data mixing for pre-training LLMs. The framework consists of three main stages, as illustrated in Figure 1. In the first stage, semantic embeddings for all documents are generated using the BGE model 1. Next, the documents are partitioned into $K_1$ clusters through clustering, and representative summaries are generated for each cluster. In the subsequent stage, the documents are further grouped into $K_2$ clusters, and abstract topics are derived for each cluster. Finally, a subset of $T$ topics is merged from the $K_2$ topics to ensure the resulting topics are more coherent and interpretable. The determination of specific hyperparameters is dependent on the characteristics of the specific datasets and the available computational resources. The configurations used in our experiments are detailed in Section 4.1."}, {"title": "Step 1: Multi-stage Clustering", "content": "Clustering large-scale datasets poses significant challenges due to high memory requirements, communication overhead, and other computational limitations, which often exceed the capabilities of standard computational devices. Given these constraints, and considering the trade-offs among commonly available clustering algorithms (Xiao and Hu, 2020) and computational resources, we adopt the K-Means algorithm due to its relatively low computational complexity of $O(kNI)$, where $k$ denotes the number of clusters, $N$ denotes the number of data points, and $I$ denotes the maximum iteration times. In this stage, the data is first partitioned into $K_1$ clusters using K-Means. For each cluster, we randomly sample $m_1$ data points and employ gpt-4o-2024-11-20 to generate an abstract summary to represent the semantics of this cluster. The generated summary provides a concise and comprehensive description of the cluster, constrained to no more than 20 words. The prompt template used for summary generation is detailed in Appendix A."}, {"title": "Step 2: Topic Extraction", "content": "Following the similar operations in Step 1, we continue to employ K-Means to partition the $K_1$ clusters into $K_2$ more compact clusters. For each of these $K_2$ clusters, we randomly sample $m_2$ summaries generated in Step 1 and use gpt-4o-2024-11-20 to produce an abstract topic with no more than 3 words, resulting in a total of $K_2$ topics. Despite the high readability of the $K_2$ topics, two significant issues arise. First, there is the problem of extensive topic overlap. Upon manual inspection of the $K_2$ topics, we observe considerable redundancy, with many topics sharing similar content. Second, there is the issue of non-parallel topic granularity. Specifically, LLMs tend to generate topics that vary in specificity, ranging from highly detailed to overly abstract, which undermines the consistency and interpretability of the results. To address these issues, we further utilize gpt-4o-2024-11-20 to merge the $K_2$ topics into $T$ ultimate topics, ensuring a more coherent and hierarchical topic structure. The prompt template used for topic merging is provided in Appendix A."}, {"title": "Experiment", "content": ""}, {"title": "Experimental Setup", "content": "Dataset We utilize the widely adopted SlimPajama corpus (Soboleva et al., 2023) as the dataset for our experiments. This corpus comprises a total of 591,399,449 documents, encompassing approximately 627B tokens. Additionally, SlimPajama categorizes the data into seven distinct domains: arXiv, Books, C4, CommonCrawl, GitHub, StackExchange, and Wikipedia.\nDataWeave Configuration For the clustering process, we set the number of clusters $K_1$ and $K_2$ in the two stages to 10,000 and 300, respectively. A hyperparameter search was conducted to determine these values: $K_1$ was explored over the set {10,000, 30,000, 60,000, 90,000}, and $K_2$ was searched within the range {100, 150, ...,600}. The optimal values for $K_1$ and $K_2$ were selected based on the maximum Silhouette Coefficient criterion (Shahapure and Nicholas, 2020), ensuring well-defined and meaningful clusters. Moreover, we merge 300 topics into 12 final topics. Regarding topic extraction, we set the sample sizes $m_1$ and $m_2$ to 10 and 50, respectively. These values were chosen to account for the maximum context window length of gpt-4o-2024-11-20 as well as the average length of the input texts and the generated summaries."}, {"title": "Implementation Details", "content": "Training In the continual pre-training setting, the model is initially pre-trained on 30B uniformly sampled tokens, followed by further pre-training on an additional 30B tokens using different data mixtures. In contrast, in the standard pre-training setting, the model is directly pre-trained on 30B tokens using different data mixtures. The model employed is a decoder-only transformer architecture with 1.3B parameters, incorporating Rotary Position Embeddings (RoPE) (Su et al., 2024) and supporting a maximum context window of 1,024 tokens (Touvron et al., 2023). Further details regarding the model architecture and training configurations can be found in Appendix B.\nBaselines We leverage the topics generated by DataWeave to guide data mixing strategies for pre-training LLMs. Specifically, inspired by Llama-3.1 (Dubey et al., 2024), we upsample one or more selected topics to a weight of 30% while downsampling the remaining topics to a weight of original 70%. To demonstrate the effectiveness of DataWeave, we compare it against several SOTA data mixing methods:\n\u2022 Uniform: Tokens are randomly sampled with uniform probability across all domains, without applying any specific control over the data distribution.\n\u2022 Temperature: Temperature-based sampling (Parmar et al., 2024; Devlin et al., 2019) proportionally adjusts data source weights according to a scaled factor of their token counts. For our experiments, we set t = 0.4 to compute topic weights based on token ratios.\n\u2022 RegMix: RegMix (Liu et al., 2024) involves training a set of small 1M-parameter models on diverse data mixtures and fitting regression models to predict model performance based on the respective mixtures. Using the fitted regression model, the top-ranked mixture is simulated to determine the optimal topic weights.\nDetails regarding the data weights used in different settings are provided in Appendix C.\nEvaluation To evaluate the capabilities of pre-trained LLMs, we assess their performance through in-context learning using the lm-evaluation-harness framework (Gao et al., 2023) and accuracy scores are reported. The evaluation dataset encompasses three categories of downstream tasks and further evaluation details are provided in Appendix D."}, {"title": "DataWeave Results", "content": "Topic Distribution DataWeave yields 12 final topics: Technology, Science, Politics, Health, Lifestyle, Law, Entertainment, Education, Relationships, Finance, Community, and Others. Based on the analysis of the topic distribution in Figure 2, we have the following key observations:\n1. Alignment with Human-Defined Categories. The majority of topics, such as Technology and Entertainment, closely align with traditional human-defined categories. This indicates that DataWeave is capable of identifying coherent and interpretable topics that reflect common thematic structures in the dataset.\n2. Emergence of Divergent Topics. Certain topics, such as Health and Relationships, diverge from pre-existing human-defined labels. This suggests that clustering process in DataWeave can uncover nuanced or less conventional themes that may not be explicitly represented in predefined taxonomies.\n3. Limitations of Human-Defined Labels. The analysis highlights the insufficiency of human-defined labels in fully capturing the diversity of online content. DataWeave demonstrates the ability to reveal latent themes that are not immediately apparent in traditional classification schemes.\n4. Topic Distribution Across Domains. Figure 2(b) illustrates the distribution of topics across various domains. Each column delineates the distribution of topics pertinent to its respective domain. The topic Technology demonstrates a strong correlation with StackExchange and GitHub, as both platforms emphasize technical discussions and coding practices. In contrast, data derived from CommonCrawl and C4 reveals a high correlation with a majority of topics. This finding underscores the significant diversity present within these domains.\nEffectiveness of Data Weave In the absence of ground-truth labels for topic modeling, establishing a robust and comprehensive evaluation framework for topic models remains a debated challenge within the research community. Some approaches propose assessing models based on the top-ranked words associated with each topic (Bianchi et al., 2020; Bouma, 2009). However, this method often entails considerable computational overhead, particularly when applied to large-scale datasets. Therefore, we introduce an alternative evaluation method by leveraging gpt-4o-2024-11-20 to identify the most relevant topics for assessing the effectiveness of DataWeave. Given that content typically spans multiple labels, we report three evaluation metrics for reference: Top-1 Accuracy, Top-3 Accuracy, and Top-5 Accuracy. These metrics measure the proportion of instances where the DataWeave label appears within the top-k labels identified by gpt-4o-2024-11-20. The evaluation results for our method are as follows: Top-1 Accuracy is 57.23%,Top-3 Accuracy is 81.19%, and Top-5 Accuracy is 90.19%. These results prove the effectiveness of DataWeave. Moreover, additional case-specific details are provided in Appendix F."}, {"title": "Continual Pre-training Results", "content": "We conducted experiments in continual pre-training setting to explore the effects of 12 topics generated through DataWeave. Specifically, we pre-trained the LLM using another 30B tokens at different data mixture where we upsampled data from each topic, as detailed in Section 4.2. As shown in Table 1, most scenarios show superior performance over random selecting 60B tokens without considering topic weights, indicating that targeted upsampling can significantly enhance model capabilities in specific tasks. Among the topics, Science stands out as the most effective, achieving the highest overall performance and the best results in General Knowledge and Reading Comprehension, which aligns well with human intuition given the structured and information-dense nature of scientific texts. Health and Relationships also yield notable gains, with Health improving the average score by 0.79 and Relationships by 0.88. These results suggest that topics containing practical, real-world knowledge or those closely tied to human reasoning may have a stronger impact on enhancing LLM capabilities across diverse tasks.\nInterestingly, some topics such as Technology and Education, while intuitively important for general knowledge and reasoning tasks, show only moderate improvements in the overall average. This could indicate that their data distributions or linguistic patterns are already well-represented in the base pre-training corpus, leading to diminishing returns from additional upsampling. On the other hand, topics like Entertainment and Community, which might be expected to have a more limited impact due to their less formal or specialized nature, show comparable improvements to other domains. This suggests that even seemingly less critical topics can contribute positively to overall performance, likely by diversifying the model's linguistic and contextual understanding."}, {"title": "Pre-training Results", "content": "We conducted experiments in pre-training setting using various data mixing methods at both the domain level and the topic level generated by DataWeave.\nTopic-level outperforms domain-level for data mixing. Our experimental results demonstrate that adjusting data weights at the topic level consistently outperforms adjustments at the domain level. As shown in Table 2, both RegMix and Temperature methods yield better results when applied to topics rather than domains. This can be attributed to the finer granularity of topics, which allows for more precise control over the diversity and relevance of the data. For instance, as shown in Figure 2, within the domain C4, there may coexist highly beneficial topics like Science and less impactful ones like Entertainment. Adjusting domain weights alone fails to adequately highlight useful data, as the domain aggregates both high- and low-utility topics. In contrast, topic-level adjustments enable targeted amplification of valuable data while suppressing less relevant portions, leading to more significant performance gains. This result underscores the importance of topic granularity for optimizing data utilization in pre-training pipelines and highlights the superior flexibility and effectiveness of topic modeling.\nHeuristic-based topic mixing is simple yet effective. Interestingly, we find that our straightforward heuristic-based approach to topic mixing achieves the best overall performance, surpassing more complex data mixing methods. As shown in Table 2, downsampling the over-represented topic Entertainment improves the average performance by 1.09%. This aligns with findings from Llama-3.1 (Dubey et al., 2024), which demonstrate that reducing the prevalence of web-dominant categories like Entertainment enhances language model capabilities. Furthermore, our experiments reveal that upsampling beneficial topics such as Science, Relationships, and Health\u2014either individually or collectively\u2014leads to substantial performance improvements, with the highest gain of 1.74% observed when all three topics are upsampled together. Notably, these heuristic-based adjustments can be implemented with minimal overhead while delivering significant performance gains. This makes them an attractive option for practitioners seeking"}, {"title": "Analysis", "content": ""}, {"title": "Relation to Downstream Tasks", "content": "To investigate the impact of topics derived from DataWeave on a range of downstream tasks, we trained a BERT topic classifier to categorize documents into the identified 12 topics. Additional details regarding the topic classifier can be found in Appendix G. We employed the topic classifier to annotate the evaluation datasets, and the resulting distributions are illustrated in Figure 3. In General Knowledge tasks, the topic Science consistently constitutes the largest proportion across the ARC-C, ARC-E, and SciQ datasets, which may account for the significant performance improvements observed when upsampling data from Science for these three tasks (see Table 1). Similarly, in the realm of Commonsense Reasoning tasks, the topic Lifestyle emerges as the most prominent. For Reading Comprehension tasks, the topic distribution remains relatively balanced among Lifestyle, Entertainment, Education, and Science. These findings provide valuable insights into the effectiveness of various data mixing strategies."}, {"title": "Cost Analysis", "content": "Understanding domain interactions poses significant challenges for human analysts. RegMix (Liu et al., 2024) offers valuable insights into how different data domains influence one another, uncovering complex relationships that are often difficult for human experts to fully comprehend. Consequently, prior research on data mixtures has primarily concentrated on developing automated methods to efficiently identify high-performing combinations, rather than relying exclusively on human intuition. In contrast, our approach presents an efficient way of determining data weights. As demonstrated in Table 2, heuristic-based methods outperform all other data mixing techniques in downstream tasks, without any supplementary models, thereby further validating the efficiency of topic mixing."}, {"title": "Case Study", "content": "Table 3 presents several examples in the DataWeave process, illustrating the progression from 10,000 summaries to 300 identified topics, ultimately distilled into 12 final topics.\nLLMs can extract high-quality topics from summaries. Unlike individual words, summaries encapsulate information from multiple documents, providing a rich semantic foundation for topic extraction. This complexity allows LLMs to identify and extract high-quality, human-readable topics from these summaries effectively. The ability of LLMs to synthesize and distill nuanced themes underscores their potential in various NLP tasks, particularly in generating coherent and relevant topics that reflect the underlying content.\nMerging topics is vital. The analysis reveals a notable issue of non-parallel topic granularity among the initial 300 human-interpretable topics. For example, the topic Gaming serves as a specific subset within the broader category of Entertainment, while Jewelry and Lifestyle exhibit similar hierarchical relationships. This discrepancy highlights the need for a systematic merging process to ensure clarity and coherence in topic categorization. Fortunately, this granularity issue has been effectively resolved in the final set of 12 topics, demonstrating the importance of refining topic definitions and relationships to enhance interpretability and usability in downstream applications."}, {"title": "Conclusion", "content": "In this study, we introduce a novel topic modeling method that combines clustering techniques with Large Language Models (LLMs) to facilitate data mixing, ultimately enhancing LLM performance on downstream tasks. Our approach demonstrates significant improvements in LLM pre-training effectiveness by strategically adjusting the weights of specific topics, thereby achieving a more balanced capability across various domains. To further enhance performance in domain-specific applications, it is essential to curate relevant knowledge data meticulously. This curation process ensures that the LLMs are exposed to high-quality, contextually appropriate information, which is critical for their effective operation in specialized fields. Looking ahead, our future work will focus on incorporating a greater number of topics per document. This expansion will allow for a richer representation of content, enabling more nuanced understanding and generation capabilities."}, {"title": "Limitations", "content": "There are two limitations in this work. First, due to the scale and complexity of web-scale data, the topic generation process shows potential for further enhancements in both effectiveness and efficiency. Second, the number of final topics in our method is determined as a hyperparameter by human judgment rather than model performance, necessitating additional experimentation. Our future work will focus on improving these aspects."}]}