{"title": "The a-Alternator: Dynamic Adaptation To Varying Noise Levels In Sequences Using The Vendi Score For Improved Robustness and Performance", "authors": ["Mohammad R. Rezaei", "Adji Bousso Dieng"], "abstract": "Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the a-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The a-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step t, the influence of the sequence element at time t and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the a-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The a-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the a-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks.", "sections": [{"title": "1 Introduction", "content": "Time-dependent data is central to the natural sciences and engineering disciplines. Modeling such data accurately requires methods that can capture variability both across sequences and within individual sequences. State-space models, such as Mambas, have emerged as a popular framework for sequence modeling (Wang"}, {"title": "2 Background", "content": "Time-dependent data often exhibits complex dynamics and varying levels of noise across time steps. To effectively model such data, frameworks are needed that can capture the underlying latent dynamics while adapting to input noise. This section outlines the foundations of the a-Alternator, described in the next section, which dynamically adjusts its dependency on the current time step or the latent history based on the temporal diversity of the sequence. We begin by describing Alternators, a probabilistic framework for sequence modeling, and then review the Vendi Score, a metric designed to flexibly and accurately quantify diversity."}, {"title": "2.1 Alternators", "content": "Consider a sequence x1:T. An Alternator models this sequence by coupling it with a sequence of latent variables, 20:T, within a joint probability distribution Rezaei and Dieng (2024),\nP0,\u00a2(X1:T, 20:T) = P(20) \u03a0t=1T Po(XtZt\u22121)P$(ZtZt-1, Xt).\nHere p(20) = N(0,I) is a prior distribution over the initial latent variable 20, Po(XtZt-1) determines how to generate the sequence elements from the latent state zt-1 and p\u00a2(zt|Zt\u22121, xt) models the evolution of that state over time. Here Zt-1 acts as a memory summarizing the history of the sequence before time t. It is updated dynamically, at each time step, by accounting for both the current state of the memory and the sequence element at time t. This is achieved by defining the mean of pp(ZtZt\u22121,xt) using a gating mechanism,\nP\u00a2(ZtZt\u22121,Xt) = N (\u03bc\u2081\u2081, 02), where \u03bc\u2081\u2081 = \u221aa\u2081\u2022 g(x\u2081) + \u221a(1-at-02) \u00b7 2-1\u00b7\nThe distribution p\u04e9(xt|zt\u22121) is on the other hand defined as\nPo(XtZt-1) = N (M\u03c7, \u03c3\u03c4) where x = \u221a(1-2)\u00b7 fe(t-1).\nHere 0 and 6 are parameters of two neural networks and they are learned by minimizing the Alternator loss function\nL(\u03b8, \u03c6) = Ep(x1:1)Pop (20:\u03c4) \u03a3t=1T 12-M2+Do-2\nwhere p(x1:T) is the data distribution and p\u0473,4(30:r) is the marginal distribution of the latent variables induced by the joint distribution in Eq. 1."}, {"title": "2.2 The Vendi Score", "content": "The Vendi Score (VS) was introduced by Friedman and Dieng (2023) and quantifies the diversity of a collection of elements. Consider a finite set of data points {r1,...,rn}. Let k(,) denote a positive semi-definite kernel function such that k(r\u012f, r\u2081) = 1 for all i. Let K be the corresponding similarity matrix, e.g. Ki,j ="}, {"title": "3 Method", "content": "The a-Alternator extends the original Alternator by introducing a mechanism for dynamically adjusting the weighting parameter a\u2081. Consider given n sequences X1:7,..., X1:7. The a-Alternator first applies a binary mask to these sequences,\nmt ~ Bernoulli(pmask) for all t \u2208 {1, ..., T} and for i \u2208 {1,...,n}\nx(i) = m(i) . x(i) + (1 \u2212m)). 0 for all t \u2208 {1,..., T} and for i e {1,..., n}\nwhere 0 \u2264 Pmask \u2264 1 is a given masking rate and 0 denotes the null vector. At each time step t, the a-Alternator then computes the noisiness of the element (1) at that time step using the VS. More specifically, the noisiness of x), which we denote by VS(i), is defined as the VS of two shifted versions of x\nVS(i) = VS({x:xL+1:t+1};k)\nwhere k(,) is a given positive semi-definite kernel and L is a given window length. The influence of x(i) is then determined by\n\u03b1 \u03c3(i) = (w \u2022 VS) + b) (1 \u2212 2 \u2212 \u20ac\u03bf),"}, {"title": "4 Experiments", "content": "In this section, we test the a-Altenator against strong baselines on neural decoding and time-series forecasting."}, {"title": "4.1 Neural Decoding", "content": "Neural decoding is a fundamental challenge in neuroscience, essential for understanding the mechanisms linking brain function and behavior. In neural decoding, neural data are translated into information about variables such as movement, decision-making, perception, or cognitive functions (Donner et al., 2009; Lin et al., 2022; Rezaei et al., 2018, 2023).\nWe use the a-Alternator to decode neural activities from three distinct experiments, each targeting a different brain region with specialized functional roles.\nIn the first experiment, we recorded the 2D velocity of a monkey as it controlled a cursor on a screen, alongside a 21-minute recording from the Motor Cortex (MC), capturing activity from 164 neurons. The motor cortex is responsible for planning and executing voluntary movements, making it a critical region for decoding motion-related neural signals.\nThe second experiment involved the same monkey performing a similar cursor control task; however, instead of the motor cortex, neural recordings were obtained from the Somatosensory Cortex (SS). This 51-minute recording included 52 neurons. The somatosensory cortex processes sensory inputs such as touch, proprioception, and movement-related feedback, allowing us to explore how sensory-driven neural activity contributes to movement execution and adaptation.\nFinally, in the third experiment, we recorded the 2D positions of a rat as it navigated"}, {"title": "4.2 Time-series forecasting", "content": "We evaluated the effectiveness of the a-Alternator across four time-series forecasting benchmarks, each presenting unique challenges. The forecasting performance of the a-Alternator and the baselines is measured using MAE and MSE across four different lookback lengths L. Table 2 summarizes the results. The best and second-best models are highlighted in blue and orange, respectively.\nThe Electricity dataset, which records hourly consumption patterns of 321 customers from 2012 to 2014, showcases the a-Alternator's superior performance in handling multivariate periodic data. Notably, the a-Alternator achieved the best performance with an average MSE of 0.165 and MAE of 0.259, outperforming both the Alternator and other state-of-the-art models. Compared to S-Mamba (MSE: 0.170, MAE: 0.265), the a-Alternator demonstrated a notable improvement across all forecasting horizons, particularly excelling in longer forecasting windows. In the challenging"}, {"title": "5 Related Work", "content": "State-Space Models. State-space models (SSMs) have emerged as a popular framework for modeling time-dependent data across various domains (Gu and Dao, 2023; Rezaei et al., 2022, 2021; Auger-M\u00e9th\u00e9 et al., 2021; Rangapuram et al., 2018)."}, {"title": "6 Conclusion", "content": "In this work, we introduced the a-Alternator, a novel sequence model designed to overcome the limitations of Alternators and existing state-space models by dynamically adapting to varying noise levels in sequences. The a-Alternator leverages the Vendi Score to determine the influence of sequence elements on the prediction of the latent dynamics through a gating mechanism. This same influence score is used to weigh the data reconstruction term in the Alternator loss. The model is trained by masking sequence elements at random during training to simulate varying noise levels. We demonstrate the effectiveness of the a-Alternator through an extensive empirical study on neural decoding and time-series forecasting tasks,"}, {"title": "Limitations and future work", "content": "One limitation of the a-Alternator is its assumption of fixed variance\u20142 and 2 are kept constant throughout the sequence\u2014for the distributions over the latent dynamics and observations. Future work will focus on modeling these variances for even greater flexibility."}]}