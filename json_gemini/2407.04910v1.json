{"title": "NADI 2024: The Fifth Nuanced Arabic Dialect Identification Shared Task", "authors": ["Muhammad Abdul-Mageed", "Amr Keleg", "AbdelRahim Elmadany", "Chiyu Zhang", "Injy Hamed", "Walid Magdy", "Houda Bouamor", "Nizar Habash"], "abstract": "We describe the findings of the fifth Nu-anced Arabic Dialect Identification Shared Task (NADI 2024). NADI's objective is to help advance SoTA Arabic NLP by providing guidance, datasets, modeling opportunities, and standardized evaluation conditions that allow researchers to collaboratively compete on pre-specified tasks. NADI 2024 targeted both dialect identification cast as a multi-label task (Subtask 1), identification of the Arabic level of dialectness (Subtask 2), and dialect-to-MSA machine translation (Subtask 3). A total of 51 unique teams registered for the shared task, of whom 12 teams have participated (with 76 valid submissions during the test phase). Among these, three teams participated in Subtask 1, three in Subtask 2, and eight in Subtask 3. The winning teams achieved 50.57 F\u2081 on Subtask 1, 0.1403 RMSE for Subtask 2, and 20.44 BLEU in Subtask 3, respectively. Results show that Arabic dialect processing tasks such as dialect identification and machine translation remain challenging. We describe the methods employed by the participating teams and briefly offer an outlook for NADI.", "sections": [{"title": "1 Introduction", "content": "Arabic is a collection of languages, language vari-eties, and dialects that can be classified into three main categories. Classical Arabic (CA) is the lan-guage of the Qur'an, old literature, and old scien-tific writing. CA has played a significant role in the spread of Islamic culture and continues to be crucial for scholarship in language and religious institutes. Modern Standard Arabic (MSA) is a simplified and 'modernized' descendent of CA that is employed in formal education and pan-Arab media as well as governmental circles in most Arab countries. Dialectal Arabic (DA) refers to the various forms of Arabic spoken in different parts of the Arab world in informal settings, TV shows, and everyday life. These three main categories of Arabic share lexica and grammatical structures to varying degrees, with some dialects being at one end of the continuum and CA at the other end.\nThe Nuanced Arabic Dialect Identification (NADI) shared task series was launched in 2020 as a venue for creating resources, affording modeling opportunities, and building a research community around the processing of dialectal Arabic. NADI 2024 is the fifth version of the shared task, hosted by the Second Arabic Natural Language Processing Conference (ArabicNLP 2024).\nDialect identification is the task of automatically detecting the source variety of a given text or speech segment. In previous versions of NADI, dialect identification was cast as a single-label classification task. That is, the input text can be assigned only one dialectal category (usually at the country level). Arabic dialects, however, can overlap significantly. This is especially the case for dialects spoken in geographically proximate areas where lexica, grammatical structures, as well as sound patterns are usually shared to notable degrees. While speech language identification models, e.g., , would typically have access to acoustic features that can help facilitate teasing apart these neighboring varieties, this is not the case for text-based systems as the input is intrinsically impoverished. Aligning with this observation, recent work by analyzed the errors of a single-label dialect classification system and found that about 66% of these errors are not true errors. To accommodate these research findings and open up a space for further investigation of challenges faced by single-label models, we design a subtask in NADI 2024 (Subtask 1) as a multi-label classification task in which the given text can belong to more than a single Arabic dialect. Since Arabic dialects also overlap, sometimes significantly, with MSA, we also introduce a new subtask for estimating the"}, {"title": "2 Literature Review", "content": "2.1 Arabic Dialect Identification\nUnlike CA and MSA, which have been studied rather extensively , DA received attention relatively re-cently. Most early efforts focused on creating re-gional or country-level dialect datasets and region-level dialect identification mod-els from text.\nSeveral works also introduced larger Twitter datasets cov-ering dialects from 10-21 countries , with some works such as introducing models targeting coun-try, province, and city levels. Several benchmarks, e.g., ORCA and DOL-PHIN , involve several di-alectal datasets.\nThe NADI shared task continues to build on these previous efforts in offering datasets and af-fording modeling opportunities for identifying Ara-bic dialects . This year, we employ a multi-label setting to take into account (i) suggestions by who highlight the limitations of addressing dialect identification as a single-label classification problem and propose defining it in a multi-label setting and (ii) issues of overlap in identical sentences across different dialects in the MADAR-26 test set identified by\n2.2\nDialectness Level of Arabic\nDA does not exist in isolation from MSA. DI iden-tification on the sentence level takes a binary view in distinguishing between MSA and DA. identified five different levels of spoken Ara-bic in Egypt ranging from Heritage Classical Ara-bic to Illiterate Colloquial Arabic. He identified some phonological, morphological, lexical, and syntactic features of each of these levels. The same Arabic speaker employs different levels according to different sociolinguistic factors.\nIn an early work by , they asked crowdsourced annotators to identify the dialect and the level of dialectness of online comments to newspaper articles, forming the AOC dataset. They have only provided four short descriptive labels for the levels of dialectness (No dialect, A bit of dialect, Mixed, Mostly Dialect) and relied on the annotators' perceptions of these labels. Notably, their guidelines allow for assigning a high level of dialectness to a sentence having a highly dialectal word even if the remaining words are perceived to be less dialectal or in MSA, which is not the case for previous guidelines.\nRecently, recycled the dis-crete level of dialectness labels from the AOC dataset, transforming them into a continuous vari-able, Arabic Level of Dialectiness (ALDi), taking values between 0 and 1, to form the AOC-ALDi dataset. We decided to use the same operational-ization of ALDi, while providing more elaborate guidelines to reduce the variability of the assigned ALDi levels for the same sentences.\n2.3 Arabic Machine Translation\nSeveral studies have addressed dialectal Arabic ma-chine translation (MT), covering multiple dialects and translation directions. With regards to translation between Ara-bic variants, throughout the last year, numerous initiatives have focused on this task. Notably, the OSACT Dialect to MSA MT shared-task focused on translating Arabic dialects (Gulf, Egyptian, Levantine, Iraqi, and Maghrebi) into MSA. The proposed approaches mostly in-volved utilizing pretrained large language models (LLMs) with experimental designs incorporating zero-shot, few-shot, and fine-tuning setups, as well as data augmentation. In line with benchmark-ing LLMs, presented a benchmark with a focus on dialectal languages, includ-ing translating 25 Arabic dialects to MSA. Other researchers have focused on translation between specific Arabic dialects and MSA, including Egyp-tian , Tunisian , and Algerian . De-spite the growing interest in this MT task, it re-ceived less attention compared to translation be-tween Arabic and foreign languages, where sev-"}, {"title": "3 Task Description", "content": "NADI 2024 maintains the focus on processing di-alectal Arabic. More concretely, we target both dialect identification (DID) and dialectal machine translation through three subtasks. Subtask 1 fo-cuses on DID, cast as a multi-label classification task, and Subtask 2 aims at capturing the Arabic level of dialectness in texts (ALDi). As translation of Arabic dialects remains particularly challeng-ing, we devote Subtask 3 to dialect MT. We now describe each subtask in detail.\n3.1 Subtask 1 \u2013 Multi-Label Dialect ID\nIn this subtask, we propose multi-label dialect iden-tification (MDID) at the country level. The objec-tive is to evaluate the feasibility of using single-label Arabic dialect identification datasets to train a multi-label system that can predict all dialects in which a given sentence is valid.\nTranining Data We provide participants with the training splits of following datasets: MADAR-2018 , NADI-2020-TWT, NADI-2021-TWT, and NADI-2023-TWT .\nDev and Test Data We provide a new multi-label development set: MDID-DEV, henceforth MDID-DEV for brevity, as explained in \u00a74. This dataset has 120 samples with manually assigned validity labels of eight different Arab countries: Algeria, Egypt, Jor-dan, Palestine, Sudan, Syria, Tunisia, and Yemen. Examples of those sentences are provided in Table 1. We do not restrict systems to these eight dialects. Hence we include two undisclosed dialects in our test data and ask participants to develop their mod-els such that they can predict all valid dialects out of the 18 country-level ones from NADI 2023. The undisclosed dialects are Iraq and Morocco. According-ly, the MDID-TEST set contains 1,000 samples covering nine dialects.\nRestrictions Subtask 1 operates under a closed-track policy where participants are allowed to only use the datasets we provide for system training.\n3.2 Subtask 2 \u2013 ALDi Estimation\ndefined the Level of Dialectness as the extent by which a sentence diverges from the Standard Language. We use their operationaliza-tion to estimate the ALDi of sentences as a continu-ous value in the range [0, 1]; where 0 means MSA and 1 implies high divergence from MSA.\nTraining Data We provided the teams with AOC-ALDi dataset's training split .\nDev and Test Data The dev and test sets col-lected for Subtask 1 were extended with a second layer of annotation for manual ALDi levels, form-ing ALDi-DEV and ALDi-TEST sets. The annotation process is outlined in \u00a74."}, {"title": "3.3 Subtask 3 \u2013 Machine Translation", "content": "Similar to NADI 2023, this subtask is focused on machine translation from four Arabic dialects (i.e., Egyptian, Emirati, Jordanian, and Palestinian) to MSA at the sentence level. Unlike NADI 2023 where we had a close-track version of the MT task, we exclusively offer an open-track theme this year.\nTraining Data We do not provide direct train-ing data. However, to facilitate Subtask 3, we point participants to the MADAR parallel dataset for system training and a monolingual dataset that participants can manually translate and use for training.\nDev and Test Data For Subtask 3, we manually curated and translated completely new develop-ment and test data that were not used in NADI-2023. The development split, MT-2024-Dev, com-prises 400 sentences, with 100 sentences represent-ing each of the four dialects, while test split, MT-2024-Test, totals 2, 000 sentences, with 500 from each dialect. Table 2 shows example sentences from MT-2024-Dev for each of the four countries. During the competition, we intentionally kept the source domain of these datasets undisclosed.\nRestrictions Subtask 3 operates under an open-track policy, allowing participants to train their systems on any additional datasets of their choice, provided these additional training datasets are pub-lic at the time of submission. For example, par-"}, {"title": "3.4 Evaluation Metrics", "content": "The official evaluation metric for Subtask 1 is the macro-averaged F\u2081 score. More specifically, we compute the F\u2081 score independently for each coun-try in the evaluation dataset (eight for the develop-ment set and nine for the test set), then compute the average of these individual-country F\u2081 scores. Additionally, we report system performance in terms of Precision, Recall, and Accuracy for submis-sions to Subtask 1. The metric for Subtask 2 is the Root Mean Square Error (RMSE). For Subtask 3, we use the BLEU score as the official metric. We calculate the overall BLEU score over all the samples (i.e., across all dialects) to rank the submitted systems for Subtask 3. We also present BLEU scores calculated separately for each of the four dialects (i.e., Egyptian, Emirati, Jordanian, and Palestinian).\n3.5 Submission Rules\nWe allowed participant teams to submit up to five runs for each test set, for each of the three subtasks. For each team, only the submission with the high-est score was retained. While the official results were exclusively based on a blind test set, we re-quested participants to include their results on Dev splits in their papers. To facilitate the evaluation of participant systems, we established a CodaLab competition for scoring each subtask (i.e., a total of three CodaLabs). Similar to previous NADI editions, we are keeping the CodaLab for each sub-task active even after the official competition has concluded. This is to encourage researchers in-terested in training models and assessing systems using the shared task's blind test sets. A nuance is that since subtasks 1 and 2 are new to NADI with limited training data available publicly, we share the individual labels of the development/test sets for these two subtasks."}, {"title": "4 Evaluation Data for Subtasks 1 and 2", "content": "4.1 Samples Curation\nWe employ the same methods as in to collect geolocated tweets, then ran-domly sample 80 data points for the following ten countries from which we could recruit annotators: Algeria, Egypt, Iraq, Jordan, Morocco, Palestine, Sudan, Syria, Tunisia, and Yemen, in addition to 80 data points from four other Arab countries Lebanon, Libya, Saudi Arabia, UAE. These additional sam-ples are expected to be labeled as invalid in the dialects of the ten countries from which we re-cruited the annotators. Including them ensures the dataset's samples cover a wider range of dialects. We use an in-house MSA/DA classification model (acc=89.1%, F\u2081 score=88.6) introduced in to ensure that for each country's 80 geolocated samples, five are in MSA, and 75 are in DA. The overall dataset size for the shared task is 1,120 samples. Each anno-tator labeled the whole dataset. We remove user mentions, URLs, and emojis from the data, but re-tain the hashtags, before labeling the samples. We annotate our dataset on Upwork, incurring a total cost of $1, 700.\n4.2 Annotation Process\nFor Subtask 1, we follow 's proposal for building multi-label ADI datasets, mainly by asking native speakers of differ-ent Arabic dialects (on the country level) to check if each sentence is valid in one of the dialects spo-ken in their countries or not. For Subtask 2, we decided to provide more elaborate definitions for the different levels of dialectness than those in the guidelines of. For each tweet, we ask two questions:\nQ1) Is it possible that the tweet is authored by someone who speaks one of your country's dialects? Options: (a) Yes, (b) Not Sure/Maybe, or (c) No.\nQ2) What is the Arabic Level of Dialectness (ALDi) of the tweet? We define the following levels:\n0. Sound MSA: Tweets written in fluent MSA.\n1. Formal Colloquial or Colloquial-influenced MSA: Tweets written in a language close to MSA but using some colloquial expressions (lexemes/ morphemes).\n2. Natural/Ordinary Colloquial: Tweets writ-"}, {"title": "4.3 Label Aggregation Techniques", "content": "Subtask 1 A sentence is considered valid in a country-level dialect if among the three annota-tors from the respective countries: a) one of them answered Yes, and b) another answered Yes or Maybe. On average, the same-country annotators fully agreed on the validity of more than 66% of the valid samples, and the invalidity of more than 85% of the invalid samples, as per Table 3.\nSubtask 2 For each sentence, the ordinal ALDi levels assigned by the annotators from the differ-ent countries are aggregated into a single numeric value \u2208 [0,1]. Discrete ALDi levels (0, 1, 2, 3) are transformed into the following numeric values (0,,, 3,1). The mean of these numeric values is used as the overall ALDi score for the sentence.\nAs mentioned in \u00a74.2, annotators only assigned ALDi levels to sentences they rated as valid in their country-level dialect. Consequently, the number of ALDi annotations per sentence can range from 0 to 3*N where N is the number of countries from which annotators are recruited. If a sentence is deemed invalid according to the majority vote label (Subtask 1) for a country-level dialect, we discard the resective ALDi annotations (if any) assigned by the annotators' of this country.\n4.4 Formation of Development/Test Sets\nWe used 120 samples from the first batch as the de-velopment sets (MDID-DEV, ALDi-DEV) shared with the participating teams. The first batch's remain-ing samples and the samples of the 4 succeeding batches form the test sets (MDID-TEST, ALDi-TEST). For ALDI-DEV and ALDi-TEST, samples that are not valid in the considered dialects of the correspond-ing set have no assigned ALDi scores, and thus are not released as part of the dataset."}, {"title": "5 Shared Task Teams & Results", "content": "5.1 Participating Teams\nWe received a total of 51 unique team registrations. At the testing phase, a total of 76 valid entries were submitted by 12 unique teams. The breakdown across the subtasks is as follows: ten submissions for Subtask 1 from three teams, seven submissions for Subtask 2 from three teams, and 16 submissions for Subtask 3 from eight teams. Table 4 lists the 12 teams. We received eight description papers, all of which were accepted for publication.\n5.2 Baselines\nWe developed baseline (BL) models for each sub-task for comparison against the teams' systems, as described below. These models were not shared"}, {"title": "5.3 Shared Task Results", "content": "Subtask 1 Elyadata came first with a macro-averaged F\u2081 score of 50.57%, being the only team to beat the Top 90% baseline model as per Table 5.\nSubtask 2 ASOS, the top-performing team, achieved the lowest RMSE of 0.1403, while AlexUNLP-STM achieved a similar RMSE of 0.1406 coming second in the ranking. As shown in Table 6, all the teams managed to improve over our baselines, including systems trained on the AOC-ALDi dataset which has a different nature (comments on news not tweets) and was annotated based on less nuanced guidelines than ours.\nSubtask 3 Table 7 shows the leaderboard of Sub-task 3. ArabicTrain won first place, achieving a BLEU score of 20.44. We observe that six teams outperform our best baseline on this subtask.\n5.4 General Description of Submitted Systems\nA summary of approaches employed by the various teams is provided in Table 8. We briefly describe the top systems for each subtask here.\nSubtask 1 The winning team, Elyadata, ex-tracted dialectal vocabularies from the training data, and used them to augment the labels of the single-label training dataset. They then used a max pool-ing layer to merge the predictions of a MARBERT-based ensemble model forming an array of logit predictions. Lastly, they optimized a threshold us-ing the development set to convert the logits into multi-label predictions.\nSubtask 2 ASOS fine-tuned a regression head of multiple layers on top of MARBERT's [CLS] embedding. AlexUNLP-STM used the median of an ensemble of regression heads with sigmoid ac-tivation on top of AraBERT, trained to minimize contrastive and RMSE losses. Noticeably, their model's performance dropped when non-Arabic letters were discarded. We observed that code-switching affected the annotators' ALDi judgments differently, which is in-line with the team's justifi-cation for the performance drop.\nSubtask 3 The winning team, Arabic Train, uti-lized samples from MADAR as the one-shot example to prompt LLM Jais for translating Arabic dialects to MSA. Team Alson exploited ChatGPT to generate parallel data for translating Jordanian and Palestinian dialects to MSA and then fine-tuned AraT5 with generated samples and MADAR dataset."}, {"title": "6 Discussion", "content": "Precision of Geolocated Labels Although geolo-cation can alleviate the need for manually annotat-ing the samples , it can be error-prone . For the 1,050 DA samples of the development and test set, we can estimate the precision of the geolocated labels by comparing them to the manual validity labels as demonstrated in Figure 3. Based on this method, we find that the precision of the geolocated labels could be as high as 94.6% (71 out of 75 samples) for Egypt, and as low as 49.3% (37/75) for Tunisia.\nImpact of Named Entities ADI models, trained on single-label data, can make spurious connec-tions between named entities (e.g.: specific lo-cations) and country-level labels. In NADI-2021-TWT for example, 52 samples out of the 66 mentioning \u0644\u0628\u0646\u0627\u0646 )Lebanon( are geolocated to and labeled as Lebanon. Such spurious connections might be the reason why the following ngrams \u064a\u0645\u0646, \u062a\u0648\u0646\u0633, \u0644\u0628\u0646\u0627\u0646,\u0639\u0631\u0627\u0642 are among the most discriminative for the dialects of Iraq, Lebanon, Tunisia, and Yemen, respectively. Manual annotation can alleviate this limitation."}, {"title": "6.1 Lessons Learned", "content": "We share our reflections on the creation of evalua-tion datasets for Subtasks 1 and 2 as per \u00a74.\nSubtask 1 Complexity Previous research asking Arabic speakers to check the validity of sentences in their native dialects (See Table A1) reported moderate to high agreement between the annotators (only two per country) for most of the considered regional-level and country-level dialects. Unlike previous works, we recruited three annotators per country and asked them to judge all samples, rather than those geolocated to their own respective coun-tries. Therefore, our annotation task is possibly harder than previous ones, which is reflected in the IAA scores in Table 3.\nSubtask 1 Labels From a task design perspec-tive, we observe that the frequency of usage of the Maybe (Not sure) label varies across annota-tors. For this reason, including this particular label (rather than using a binary Valid/Not Valid setup), needs to be further investigated to understand its implications on the aggregated validity labels.\nAnnotation Quality Monitoring Two authors who are speakers of Egyptian Arabic were respon-sible for monitoring the quality of the annotations, providing feedback, and marking the samples with high disagreement for reannotation. We believe that having dialect leads who are native speakers of the different Arabic dialects would allow for better monitoring of the annotation process. We hope that our shared task will inspire future collaborative re-search to extend the labels of our evaluation dataset to include more country-level dialects."}, {"title": "7 Conclusion", "content": "This year, we organized NADI 2024, the fifth edi-tion of the shared task, having three subtasks: multi-label dialect identification (MDID), Arabic level of dialectness (ALDi) estimation, and DA-to-MSA machine translation. We had 51 registered teams, out of which 12 submitted their systems' predic-tions with eight accepted system description papers. The results indicate that there is still room for im-provement across the various tasks. In the future, we intend to cover more Arabic dialects in NADI and propose novel ways of modeling that involve the use of large language models."}, {"title": "Limitations", "content": "Our work has a number of limitations, as follows:\n\u2022 This edition of NADI focused on only 10 country-level dialects for Subtasks 1 and 2. This is due to challenges with recruiting an-notators as well as the lack of high-quality datasets for countries such as Comoros, Dji-bouti, Mauritania, and Somalia.\n\u2022 NADI continues to use short texts for the Ara-bic dialects. That is, due to the shortage of dialectal data from other sources, we depend on posts from Twitter. Although these data have thus far empowered the development of effective dialect identification models, it is de-sirable to afford data from other domains that have longer texts. This will allow the develop-ment of more widely applicable models.\n\u2022 The label aggregation techniques (See \u00a74.3) used for the evaluation sets of Subtask 1, 2 attempts to reduce the impact of the few in-evitable inaccurate annotations. However, they could also inhibit interannotator disagree-ment that is caused by having different per-ceptions (i.e., what sentences are valid in their country-level dialects, or what the level of di-alectness of sentences are)\n\u2022 Our machine translation subtask focuses only on four dialects without offering a training dataset. Modern MT systems need much larger data to perform well. Again, in spite of our best efforts, parallel datasets involving dialects remain limited.\n\u2022 Due to limited resources, we were able to pro-vide only a single reference annotation for Subtask 3 test samples. We acknowledge that machine translation requires multiple evalu-ation references to ensure a more reliable as-sessment.\n\u2022 We acknowledge that the BLEU score for eval-uating machine translation output has its lim-itations . We expect that using more diverse metrics, such as ChrF and COMET can enhance the reliability of evaluation results."}, {"title": "Ethical Considerations", "content": "The NADI-2024 Subtask 1, 2 datasets are sourced from the public domain (i.e., X former Twitter), with user personal information and identity care-fully concealed. Similarly, the NADI-2024 Sub-task 3 dataset is manually created. Again, we take meticulous measures to remove user identities and personal information from our datasets. As a result, we have minimal concerns about the retrieval of personal information from our data. However, it is crucial to acknowledge that the datasets we collect to construct NADI-2024 Subtask 1, 2 may contain potentially harmful content. Additionally, during model evaluation, there is a possibility of expo-sure to biases that could unintentionally generate problematic content.\nFinally, we note that the annotation process we followed for creating the evaluation dataset of the first two subtasks was approved by the re-search ethics committee of the University of Edin-burgh, School of Informatics with reference num-ber 839548."}, {"title": "Appendices", "content": "A Design of the Annotation Guidelines\nIn this year's edition of NADI, we introduced two new subtasks, MDID, and ALDi Estimation. As explained in \u00a74, we annotated 1,120 tweets for both subtasks to build the development and test splits.\nSubtask 1 There has been multiple attempts ask-ing annotators to check if sentences are written in their Arabic dialect. This was done mainly to validate dialect labels that are automatically as-signed using geolocating methods/distinctive di-alectal cues  or to perform error anal-ysis for the predictions of DI systems . Arabic speakers have different per-ceptions of their country-level dialects that depend on their backgrounds and exposure to different speaking communities. Such differences could im-pact their understanding of the validity of sentences in their country-level dialects. Moreover, previous wordings used to check the validity of these sen-tences shown in Table A1 were used to validate the labels for sentences that are more probable to be in the annotator's native dialect (i.e., not in another dialect nor in MSA).\nConversely, we asked the annotators to label 1120, uniformly representing 14 country-level di-alects. Moreover, we are interested in checking the validity of the sentences in any of the dialects spoken in the annotator's country of origin, and not just in their native Arabic dialect. Consequently, we used the wording mentioned in \u00a74.2, in addition to providing some examples as shown in Figure A1.\nSubtask 2 We follow Zaidan and Callison-Burch (2011)'s setup in which they asked the annotators\nB Evaluation Data Annotation Process\nAs summarized in \u00a74.2, we asked the annotators to complete a short onboarding task before joining the 5 main annotation tasks. In this section, we provide further details about the annotation process.\nOnboarding Tasks QADI's test set has 3,303 tweets geolocated to 18 different Arab countries (including the 14 countries represented in the samples of our dataset). The geolocated label for each tweet was then validated by a native speaker from that country, who checked if the tweet is consistent with the dialect spoken in their country. Additionally, the test set has 200 tweets automatically classified as written in MSA."}]}