{"title": "Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up", "authors": ["Jiahao Yuan", "Dehui Du", "Hao Zhang", "Zixiang Di", "Usman Naseem"], "abstract": "Large language models (LLMs) have shown remarkable performance in reasoning tasks but face limitations in mathematical and complex logical reasoning. Existing methods to improve LLMs' logical capabilities either involve trace-able or verifiable logical sequences that generate more reliable responses by constructing logical structures yet increase computational costs, or introduces rigid logic template rules, reducing flexibility. In this paper, we propose Reversal of Thought (RoT), a novel framework aimed at enhancing the logical reasoning abilities of LLMs. RoT utilizes a Preference-Guided Reverse Reasoning warm-up strategy, which integrates logical symbols for pseudocode planning through meta-cognitive mechanisms and pairwise preference self-evaluation to generate task-specific prompts solely through demonstrations, aligning with LLMs' cognitive preferences shaped by Reinforcement Learning with Human Feedback (RLHF). Through reverse reasoning, we ultilize a Cognitive Preference Manager to assess knowledge boundaries and further expand LLMs' reasoning capabilities by aggregating solution logic for known tasks and stylistic templates for unknown tasks. Experiments across various tasks demonstrate that RoT surpasses existing baselines in both reasoning accuracy and efficiency.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) like Qwen (Bai et al., 2023), Llama (Dubey et al., 2024), and GPT-4 (Achiam et al., 2023) have demonstrated remarkable performance in various reasoning tasks via single-step prompting with few shots upon scaling model size (Plaat et al., 2024) but remain restricted in mathematical and intricate logical reasoning domains (Arkoudas, 2023; Stechly et al.), which has spurred more effective multi-step Chain-of-Thought (CoT) prompting (Wei et al., 2022) approaches for activating step-by-step logical ca-"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Chain-of-Thought (CoT) Prompting", "content": "Chain-of-Thought (CoT) prompting (Wei et al., 2022) has been proven to be a promising approach that incorporates an intermediate logic chain to enhance LLMs' logic. Recent studies primarily aimed at improving logical accuracy by introducing more external validation such as self-consistency (Narang et al.; Yu et al., 2024) or more hierarchical information such as Least-to-Most (Zhou et al.), Cumulative-Reasoning (Zhang et al.) and Multi-experts (Suzgun and Kalai, 2024) strategies, but faced challenges related to cumulative errors or poor flexibility. Additionally, numerous studies also proposed more standardized recursive or backtracking branch forms from the logical data structure, including Tree-of-Thought (ToT) (Yao et al., 2024), Graph-of-Thought (GoT) (Besta et al., 2024) and Buffer-of-Thought (BoT) (Yang et al., 2024a). However, an efficient logical reasoning method that strikes a balance among logical accuracy, flexibility, and cost has yet to be discovered. Our method is activated through meta cognition (Fleur et al., 2021) by introducing reverse reasoning to form effective LLMs-taste prompts within cognitive preference (Uddin, 2021) for plan-and-solve with logical pseudocode at least."}, {"title": "2.2 Knowledge Boundary for Enhancing Large Language Models", "content": "Integrating knowledge boundary within LLMs has emerged as a prospective strategy for enhancing their ability to avoid reasoning hallucinations of unknown knowledge through knowledge boundary constraints which requires additional algorithmic efforts (Yin et al., 2024), external graph knowledge (Tian et al., 2024), and training consumption (Sun et al., 2024). Additionally, they focus on avoiding responses to unknown or incorrect prompts rather than proposing bold and proactive solutions to expand knowledge boundary in a heuristics without training. We proposed a prompt-based method utilizing LLMs pretrained knowledge boundary, inspired by meta cognition (Fleur et al., 2021) and"}, {"title": "3 Reversal of Thought", "content": ""}, {"title": "3.1 Overview", "content": "Tell me and I forget. Teach me and I remember. Involve me and I learn.\nFranklin (2005)\nAs the aforementioned wisdom related to human cognitive learning implies, merely telling or teaching is inadequate (Bao et al., 2024). Moreover, most LLMs have undergone extensive pre-training (Achiam et al., 2023; Bai et al., 2023; Dubey et al., 2024) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), instilling in LLMs a propensity for specific cognitive patterns, which manifests in two progressive layers of LLMs-taste description: (1) Stylistic template: encompassing grammatical and syntactic structures in descriptions. (2) Solution logic: comprising problem-solving reasoning and methodological cues. Therefore, Reversal of Thought (RoT) involves answering the following two research questions (RQs):\n\u2022 RQ1: How to make LLMs output preference cognitive templates and logic for specific tasks and activate known cognitive boundaries?\n\u2022 RQ2: How to autonomously use cognitive templates with incorrect response to expand the possible knowledge boundaries?\nTo activate and enhance LLMs logical flexibility, accuracy, and the ability to autonomously construct meta-cognition without training for logical reasoning, inspired by meta-cognition (Fleur et al., 2021)"}, {"title": "3.2 Reverse Reasoning with Meta-cognition", "content": "Preference-Guided Reverse Reasoning. Inspired by RLHF (Ouyang et al., 2022) utilizing preference data, and to derive high-cognitive preference prompt $P^*$ that enhance logical reasoning in LLMs, we propose a Preference-Guided Reverse Reasoning (PGRR) framework (detailed in alogrithm 1) mapping input-output demonstrations D from an initial prompt P to an optimal LLM-taste prompt $P_{opt}$.\n(1) Reverse Reasoning Warm-up. We query the LLM $M_{LLM}$ with a reversal prompt and demonstrations {$P_r$, D} (detailed in figure 3) for warm iterations, generating a set of candidate cognitive preference responses R = {$R_1, R_2, ..., R_{warm}$} and their corresponding average probabilities $P_{res}$:\n$R = \\bigcup R^{(i)} = \\bigcup M_{LLM}(P_r, D, i), (1)$\n$P_{res} = \\frac{1}{i=1} \\sum_{R_{i,j} \\in R}  exp(P(R_{i,j} | P_r, D)). (2)$\nwhere $R^{(i)}$ represents the i-th generated response. $M_{LLM}(P_r, D, i)$ is the model output based on the reversal prompt $P_r$ and demonstrations D for the i-th iteration. $P(R_{i,j}|P_r, D)$ denotes log probability for each token $R_{i,j} \\in R_i$ from LLMs.\n(2) Pairwise Preference Evaluation. To acquire the most LLMs-taste prompt, we pair candidate responses R as data pairs {$R_i, R_{i+1}$} where i = 0,1,..., warm 1 to calculate the relative preference $P(R_{i+1} > R_i)$ through LLM's self-evaluation of its preference for $R_{i+1}$ over $R_i$, formally define as:\n$P_{pre}(R_{i+1} > R_i) = exp(M_{\\mathcal{LLM}}(P_{eval}, R_{i+1}, R_i)) (3)$\nwhere $M_{\\mathcal{LLM}}(Prompt_{eval}, R_{i+1}, R_i)$ represents that require LLM to select more preferred data through $P_{eval}$ with a structure as \"Please choose your more preferred instruction (A/B): (A). $R_{i+1}$; (B). $R_i\".$\nFollowing the principle of preference transitivity (Liu et al., 2024), we extend $P(R_{i+1} > R_i)$ to $P(R_i > R_j)$ to reduce computational cost from, thereby forming a preference matrix $P_{pre} \\in R^{warm \\times warm}$, formally:\n$P_{pre}(R_i R_j) = \\begin{cases} 1 & i=j \\\\ \\prod_{k=j}^{i-1} P_{pre} (R_{k+1} > R_k) & i>j \\\\ 1-P_{pre}(R_j > R_i) & i<j \\end{cases} (4)$\n(3) Preference-Guided Ranking. To identify the most LLMs-preferred and high-quality response, we compute each response $R_i$'s overall preference score $P_{pre}(R_i)$, and averaging both average probabilities $P_{res}$ in matrix $P_{pre}$ and preference score $P_{pre}(R_i)$ to obtain the best LLM-taste prompt $P_{opt}$:\n$P_{pre}(R_i) = \\frac{1}{warm -1}\\sum_{j=1\\ j\\neq i}^{warm} P_{pre}(R_i R_j), (5)$\n$P_{opt} = arg max (P_{res} + P_{pre}(R_i)) (6)$"}, {"title": "Reverse Logic for Meta-cognition", "content": "Within reverse reasoning, we further follow meta-cognitive (Suzgun and Kalai, 2024) using plan-and-solve by integrating logical algorithm pseudo-code to improve reasoning comprehension. And we incorporate fundamental mathematical logic symbols, including logical operators, quantifiers, inequalities and conditional statements, to facilitate model reasoning detailed in Figure 3."}, {"title": "3.3 Cognitive Preference Manager", "content": "Cognitive Preference Manager. After reverse reasoning for LLMs-cognitive description $P^*$, We introduce an offline-deployed LLM embedding model $M_{emb}$ to assist Cognitive Preference Manager (CPM) in determining whether reverse reasoning under reverse prompt $P_r$ and demonstrations D reaches the knowledge boundary or cognitive error by calculating the similarity and setting a threshold $\\delta$ (0.6~0.8 is recommended for optimal performance in distinguishing knowledge boundaries) between orginal task defination $P_{task}$ from P and LLMs-cognitive task defination $P_{task}$ from $P^*$, and finally get a cognitive signal $S_{cog}$, formally:\n$S_{cog} = \\begin{cases} unknown, & sim (M_{emb}(P_{task}), Memb (P_{task})) < \\delta \\\\ known & , sim (M_{emb} (P_{task}), Memb (P_{task})) \\geq \\delta \\end{cases} (7)$\nwhere sim(*) is a cosine similarity function that computes the similarity between two embedding vectors.\nBy efficiently evaluating cognitive results, CPM integrates alternative aggregation strategies for $M_{\\mathcal{LLM}}$ based on $S_{cog}$ 1, as detailed in appendix A: (1) Solution logic aggregation for known tasks: $M_{\\mathcal{LLM}}$ merges beneficial aspects from the original prompt P with the LLM-taste prompt $P^*$ to create the final prompt $P_{final}$. (2) Stylistic template aggregation for unknown tasks: $M_{\\mathcal{LLM}}$ extracts a cognitive preference template T from the incorrect context in the LLM-taste prompt, and integrates meta-cognitive elements from the original prompt P into T to construct the final prompt $P_{final}$."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets and Tasks", "content": "To comprehensively validate our Reverse of Thought (RoT), following ToT (Yao et al., 2024), meta-prompting (Suzgun and Kalai, 2024) and BoT (Yang et al., 2024a), we assess it with baselines across a broad range of eight tasks across five logical benchmarks that encompass mathematical and algorithmic reasoning, domain-specific knowledge, and literary creativity:(1) Game of 24 (Yao et al., 2024) challenges LLMs to create a mathematical expression utilizing each of four given numbers exactly once to achieve 24. (2) BIG-Bench (Suzgun et al., 2023; Srivastava et al., 2023) involves Geometric Shapes, Multi-Step Arithmetic Two and Word Sorting from BIG-Bench Hard (Suzgun et al., 2023) and Checkmate-in-One from BIG-Bench suite (Srivastava et al., 2023); (3) Python Puzzles (Schuster et al., 2021) comprises a collection of challenging programming puzzles crafted in Python, covering various difficulty levels.;(4) Multilingual Grade School Math (MGSM) (Shi et al., 2022) is a multilingual adaptation of the GSM8K dataset (Cobbe et al., 2021), featuring translations of a subset of examples into ten diverse languages.(5) Shakespearean Sonnet Writing (Suzgun and Kalai, 2024) require LLMs to compose with the rhyme scheme \"ABAB CDCD EFEF GG\" while incorporating three specified words verbatim."}, {"title": "4.2 Baselines", "content": "In our experiments, we compare RoT with five classic and latest state-of-the-art prompting baselines:\n\u2022 CoT Prompting: Following Suzgun and Kalai (2024); Yang et al. (2024a), we employ GPT-4 to decompose instruction into logic intermediate reasoning steps activated by \"Let's think step by step\".\n\u2022 Meta-Prompting: Suzgun and Kalai (2024) introduced general, task-agnostic prompts as a scaffold to guide LLMs effectively perform logic tasks."}, {"title": "4.3 Experiment Setup", "content": "To ensure fair comparisons with previous methods, we utilize GPT-4, the latest model supporting $logprobs^2$, as the foundational model via OpenAI API for our RoT experiments including both main experiments and ablation study. For the warm hyperparameter, we experimented with values of {1,3,5,10}, accessed in batches for reverse reasoning warm-up through OpenAI API. Our findings suggest that a value of 5 optimally balances between logical accuracy and cost-efficiency. For embedding model $M_{emb}$, we utilize a huggingface model stella_en_1.5B_v5 3, a high-performance model with the smallest parameter countamong the top three on the MTEB leaderboard (Muennighoff et al., 2023), offline deployed on a single NVIDIA GeForce RTX 4090 GPU."}, {"title": "4.4 Evaluation Metrics", "content": "Knowledge Boundary and Cognitive Preference Consistency. As described in Section 3.2, we trigger knowledge boundary using $S_{cog}$ by comparing $sim(P_{task}, M_{\\mathcal{LLM}}(D, P_r)_{task})$ with a threshold $\\delta = 0.7$. We first evaluate GPT-4 on experimental tasks to distinguish between known and unknown domains in both 1-shot and 2-shot settings4 and subsequently conduct human evaluations of average cognitive preference consistency $Concog$ based on stylistic and grammatical norms on P*"}, {"title": "5 Results and Discussion", "content": ""}, {"title": "5.1 Knowledge Boundary and Cognitive Preference Consistency", "content": "Knowledge Boundary. As shown in Table 2, our findings provide preliminary support for our hypothesis concerning cognitive knowledge boundaries of LLMs in reverse reasoning across current task benchmarks in one-shot and two-shot settings (Yao et al., 2024; Suzgun et al., 2023; Srivastava et al., 2023; Schuster et al., 2021; Shi et al., 2022; Suzgun and Kalai, 2024). Notably, GPT-4 excels in structured reasoning tasks such as Game of 24, Geometric Shapes, and Checkmate-in-One, consistently achieving strong results in both one-shot and two-shot settings. In contrast, tasks such as MGSM (avg) and Python Puzzles fall into an unknown category in the one-shot scenario, stemming from multi-source problems that are challenging to make reverse reasoning without sufficient context, indirectly supporting the rationale for our CPM approach, detailed in appendix B.\nCognitive Preference Consistency. Moreover, our human evaluation of $Concog$ produced a high score of 4.32 in the two-shot setting and 4.01 in the one-shot setting, further validating that LLMs exhibit cognitive preferences shaped by RLHF (Ouyang et al., 2022)."}, {"title": "5.2 Reasoning Accuracy and Efficiency", "content": "Reasoning Accuracy. As shown table 1, our RoT model consistently outperforms all baseline models across various logic reasoning tasks. Notably, compared with the best BoT, we observe significant accuracy improvements in 1-shot and 2-shot settings, particularly evidenced by increases of 14.7% and 15.3% on the Game of 24, 4.7% and 4.1% on Geometric Shapes, and 4.6% and 4.5% on Checkmate-in-One, demonstrating our flexibility and efficiency of leveraging cognitive preference in LLMs to activate logic capabilities through reverse reasoning, particularly with cognitive preference management."}, {"title": "5.3 Ablation Study", "content": "As shown in Table 3, we conducted three ablation studies to evaluate key components: (1) w/o PGRR: Removing Preference-Guided Reverse Reasoning(PGRR) for exploring LLMs-taste prompts; (2) w/o Logic: Excluding mathematical logic for pseudo-code plan-and-solve; and (3) w/o CPM: Eliminating Cognitive Preference Manage(CPM) for both known and unknown tasks.\nImpact of PGRR. Excluding Preference-Guided Reverse Reasoning (PGRR) for exploring LLMs-taste prompts regarding tasks results in a significant reduction in overall task performance, with decreases of up to 20% in tasks like the Game of 24 (97.8% to 77.8%) and WordSorting (100% to 81%), indicating w/o PGRR weaken model's cognition to task-specific requirements.\nImpact of Logic. Removing mathematical logic from RoT leads to notable declines in tasks requiring structured problem-solving, such as Multi-Step Arithmetic (99.0% to 88.0%) and Checkmate-in-One (92.0% to 83.5%), underscoring w/o Logic negatively affects structured problem-solving for complex reasoning.\nImpact of CPM. Lacking Cognitive Preference Manager (CPM) has a particularly pronounced effect on unknown tasks with Python Puzzles dropping from 57.5% to 52.5% and MGSM from 90.1% to 86.1%, indicating w/o CPM weaken RoT's flexibility for tackling known tasks (case study for knwon task detailed in appendix B.1) and unknwon tasks (case study for unknwon task is detailed in Appendix B.2)."}, {"title": "6 Conclusion", "content": "In this paper, we propose Reversal of Thought (RoT), a novel framework to enhance the logical reasoning capabilities of LLMs. By integrating reverse reasoning with meta-cognitive mechanisms and cognitive preference management, RoT improves reasoning accuracy and efficiency while minimizing computational costs, which leverages Preference-Guided Reverse Reasoning and Cognitive Preference Manager, which optimally aligns LLM reasoning processes with their cognitive preferences shaped by their pretraining and RLHF. Comprehensive experiments across diverse reasoning tasks demonstrate that RoT consistently outperforms state-of-the-art baselines in both known and unknown task scenarios, demonstrating the potential to expand knowledge boundaries through cognitive preference template. Our research provides valuable insights into future studies focused on further enhancing LLMs' reasoning capacities by dynamically exploring cognitive preferences for complex reasoning tasks."}, {"title": "Limitations", "content": "Reversal of Thought (RoT) introduce a reverse reasoning warm-up to activate cognitive preference for LLMs to enhance logic capabilities and introduce a cognitive preference manager to determine knowledge boundary and utilize cognitive preference for known and unknown tasks.\nWhile RoT has performed exceptionally in logic accuracy and efficiency, We discuss major challenge in its reliance on two-shot demonstration inputs involving two distinct problem cases. We observed that RoT may struggles with one-shot learning in multi-source tasks. we partially and effectively mitigates this issue through the integration of Cognitive Preference Manager (CPM) and two-shot learning.\nIn future work, we aim to extend RoT's capabilities by incorporating In-Context Learning (ICL), which will allow for greater flexibility in adapting to varied contexts and improve its performance on more complex reasoning tasks."}, {"title": "Ethical Considerations", "content": "Since the datasets used in our experiments focus on pure mathematical and algorithmic reasoning, domain-specific knowledge, and literary creativity, which are all sourced from publicly available datasets (Yao et al., 2024; Suzgun et al., 2023; Srivastava et al., 2023; Schuster et al., 2021; Shi et al., 2022; Suzgun and Kalai, 2024) and devoid of any personal privacy or sensitive ethical information. Therefore, we do not identify any immediate ethical concerns regarding our current work. Additionally, we conduct human evaluations of cognitive preference consistency with three anonymous professional annotators following our instructions (detailed in appendix C), and they receive equivalent compensation."}, {"title": "B Case Study", "content": ""}, {"title": "B.1 Case Study for Known Task (Game of 24)", "content": "Task Defination\nThe task is to find a feasible mathematical expression using the four input numbers (a, b, c, d) in the order they are given, such that the result equals 24. The feasible solution should involve basic arithmetic operations (+, -, \u00d7, /) and parentheses if necessary.\nLogical Pseudocode\n\u2200P(A, B, C, D) in permutations of [A, B, C, D].\nVopn in (+, -, \u00d7, \u00f7) where n \u2208 {0,1,2,3}.\nFor each permute \u00ac\u2203(Expression == 24) V (Expression = permute[0] op\u2081 permute[1]; op2 permute[2] \u043e\u0440\u0437 permute[3]):\n\u2203(Expression == 24) \u2283 Print(Expression) \u2227 Terminate Process.\nPrint(\"No feasible solution exists.\").\nCase Examples:\nExample 1: Input: 4 6 7 1 Output: One feasible solution (e.g., 6/(1 \u2013 (4/7)) = 24).\nExample 2: Input: 4221 Output: \"No feasible solution exists.\"\nInput-Output Format:\nInput: Four integers separated by space (e.g., \"3 3 8 8\").\nOutput: If a feasible solution exists, output a string indicating one possible solution (e.g., \"One feasible solution (e.g., 8\u00f7(3-8\u00f73) = 24).\"). If no solution exists, output the string \"No feasible solution exists.\""}, {"title": "B.2 Case Study for Unknown Task (MGSM)", "content": "Task Defination\nThe task is to solve a math problem text with a description of a situation. Your task is to calculate the answer and provide it in the format specified.\nLogical Pseudocode\nInput u from the user in natural language.\nParse u for numbers and contextual information.\nExtract numbers \u03b1, \u03b2, \u03b3, etc.\nIdentify contextual clues and operations, such as addition (+), subtraction (-), multiplication (*), division (/), and other implicit operations (e.g., percentages, halves, totals).\nFor each identified operation:\nIf operation is related to percentages, interpret \u00d7 or division as needed.\nIf operation is additive (+), perform Add(\u03b1, \u03b2).\nIf operation is subtractive (-), perform Subtract(a, \u03b2).\nIf operation is multiplicative (*), perform Multiply(a, \u03b2).\nIf operation is divisive (/), perform Divide(a, \u03b2).\nIf implicit operations (e.g., \"half that much\"), interpret accordingly.\nHandle complex structures, such as total amounts or remainders:\nUse context to evaluate remaining quantities (e.g., after consumption, sales).\nApply operations in sequential order based on context.\nCalculate the final result based on all interpreted operations and numbers.\nReturn the final result as a natural number or in currency (if applicable).\nCase Examples:\nExample 1:\nInput: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\nOutput: 3\nInput-Output Format:\nInput: A natural language string describing a scenario that involves certain arithmetic operations.\nOutput: A single integer or float number that is the result of the operations described in the scenario."}, {"title": "C Instructions for Human Evaluation.", "content": "To evaluate the cognitive preference consistency $Concog$ between LLMs-taste prompt for task cognitions across different tasks, annotators adhere to the following guidelines:\nCriteria. (1) Style Consistency: How well responses maintain a consistent stylistic tone across different tasks. (2) Coherence: Internal logic and smooth flow of the response.\nScoring. Responses are evaluated on a scale from 0 to 5. A score of 4-5 signifies exemplary style consistency, while 3-4 indicates strong consistency with minor variations. Scores of 2-3 represent adequate style but with noticeable inconsistencies, and 1-2 denote inconsistent style across tasks. A score of 0-1 reflects complete inconsistency."}]}