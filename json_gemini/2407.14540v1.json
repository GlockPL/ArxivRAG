{"title": "Risks of uncertainty propagation in AI-augmented security pipelines", "authors": ["Emanuele Mezzi", "Aurora Papotti", "Fabio Massacci", "Katja Tuma"], "abstract": "The use of AI technologies is percolating into the secure development of software-based systems, with an increasing trend of composing AI-based subsystems (with uncertain levels of performance) into automated pipelines. This presents a fundamental research challenge and poses a serious threat to safety-critical domains (e.g., aviation). Despite the existing knowledge about uncertainty in risk analysis, no previous work has estimated the uncertainty of AI-augmented systems given the propagation of errors in the pipeline. We provide the formal underpinnings for capturing uncertainty propagation, develop a simulator to quantify uncertainty, and evaluate the simulation of propagating errors with two case studies. We discuss the generalizability of our approach and present policy implications and recommendations for aviation. Future work includes extending the approach and investigating the required metrics for validation in the aviation domain.", "sections": [{"title": "INTRODUCTION", "content": "Due to the increasing availability of data, AI technologies have spread and are being used in almost every computing system, including in safety-critical domains like aviation (EASA, 2023a). Although the use of AI-augmented systems comes with new promises of improved performance, it also introduces significant risks and challenges (EASA, 2023b; Cox Jr, 2020; Nateghi & Aven, 2021). A major challenge in using AI for risk analysis is conveying to decision-makers the uncertainty inherent to predictions of models, because it clashes with the common practice in the realm of AI to communicate uncertainty with point estimates or ignoring it completely (Guikema, 2020).\nWith the rise of open-source software development and large-scale cloud deployment, more security risk decision-making is automated by running sequences of AI-augmented analyses, like automated program repair (APR) (Long et al., 2017; Ye et al., 2021; Li et al., 2022; Xia & Zhang, 2022; Fu et al., 2024). The use of AI in automated security pipelines, where the first classifier detects a vulnerability and the second tool fixes it, is now becoming more common (Bui et al., 2024), which brings about a fundamental research challenge:\nPropagating uncertainty is the new major challenge for assessing the risk of automated security pipelines.\nThis foundational problem has already manifested in security pipelines with no AI-based computation. To illustrate this problem, we consider four studies: verifying the presence of code smells (Tufano et al., 2017), generalizing the SZZ algorithm to identify the past versions of software affected by a vulnerability (Dashevskyi et al., 2018), identifying vulnerabilities in Java libraries (Kula et al., 2018), and finding how vulnerable Android libraries could be automatically updated (Derr, 2017).\nA few years later, Pashchencko et al. (2020) showed that the results by Kula et al. (2018) are incorrect, and Huang et al. (2019) found that the claims by Derr et al. (2017) are incorrect, both to a large extent. We argue that the reason for this mishap is foundational. All these studies share the impossibility of running manual validation and do not report the uncertainty of their outcomes. The proposed solutions process huge inputs (e.g., 246K commits in Dashevskyi et al. (2018)) so they need an automated tool (with an error rate) to decide whether each sample satisfies the property of interest.\nWith the appearance of new AI-based approaches, such as SeqTrans (Chi et al., 2022), it is becoming imperative to investigate this problem now, before it is too late and AI-augmented systems without global measures of risk become weaved into the automated pipelines in organizations.\nTo address these issues, we focus on understanding the uncertainty due to error propagation in AI Augmented Systems. Among the pipeline, each component may be a potential source of error that leads to an underestimation or overestimation of the actual effectiveness of the proposed solution. Therefore, we formulate the overarching research question:\nRQ: How to estimate the total error (or success rate) of the Al-augmented system, given the propagating errors of the classifiers in the pipeline?\nIf analytical models for the classifiers and the fixer components existed, it could be possible to use the error propagation models used for calculus (Benke et al., 2018). Unfortunately, analytical models of the recall and precision of these tools are extremely rare, therefore, we must resort to the much coarse-grained approximation with probability bound analysis (PBA) (Iskandar, 2021)."}, {"title": "Contributions", "content": "We provide the formal underpinnings for capturing uncertainty propagation in AI-augmented APR pipelines. In addition, we develop a simulator to quantify the effects that propagating uncertainty has in automated APR tools (such as the one presented in Figure 1). We evaluate the simulator and present two case studies, in which we calculate the effects of uncertainty regarding two proposed solutions. Finally, we discuss the policy implications and recommendations for the aviation domain."}, {"title": "BACKGROUND AND RELATED WORK", "content": "We illustrate the composition of AI-augmented APR tools as background and present the related work on uncertainty quantification in AI, and applications of AI to vulnerability detection and APR."}, {"title": "AI-augmented systems", "content": null}, {"title": "Uncertainty quantification in AI", "content": "H\u00fcllermeier and Waegeman (2021), highlight two macro-categories of methods employed to quantify and manage uncertainty in Machine Learning (ML). The first discerns between frequentist-inspired and Bayesian-inspired quantification methods. The second considers the distinction between uncertainty quantification and set-values prediction. Uncertainty quantification methods allow the model to output the prediction and the paired level of certainty, while the set-value methods consist of pre-defining a desired level of certainty and producing a set of candidates that comply with it.\nAbdar et al. 2021 centre their analysis on Deep Learning (DL). Bayesian-inspired methods and ensemble methods represent two of the major categories to represent uncertainty in DL. Through Bayesian methods the DL model samples its parameters from a learnt posterior distribution, allowing the model to avoid fixed parameters and allowing us to inspect the variance and uncover the uncertainty which surrounds the model predictions. The most common Bayesian-inspired technique is the Monte Carlo (MC) dropout. Ensemble methods, combine different predictions from different deterministic predictors. Although they were not introduced in the first instance to explicitly handle uncertainties, they give an intuitive way of representing the model uncertainty on a prediction by evaluating the variety among the predictors (Gawlikowski et al., 2023)."}, {"title": "Key Observation 1", "content": "Extensive research was performed in the field of uncertainty quantification in AI, which brought the development of a variety of methods. However, these approaches are focused on uncertainty quantification of single models, without measuring the propagating effects of uncertainty about the output of the first model when piped as input into another part of the system."}, {"title": "AI in vulnerability detection", "content": "Vulnerability detection is a crucial step in risk analysis of software systems and includes running automated tools scanning parts of the system to prevent future exploitation. Given its potential, experts integrated AI into their vulnerability detection systems, to scale them and make them more flexible to new threats.\nOne of the approaches to perform vulnerability detection is obtained by applying Natural Language Processing. In their approach, Hou et al. (2022), represent the code in the form of a syntax tree and input it to a Transformer model, which leverages the attention mechanism to improve the probability of detecting vulnerabilities. In their research, Akter et al. (2022), create embeddings using Glove and fastText and then use LSTM and Quantum LSTM models to perform vulnerability detection, showing lower execution time and higher accuracy, precision, and recall for the Quantum LSTM.\nAnother line of research excludes Natural Language Processing or embeds it with graph approaches. Yang et al. (2022), propose a new code representation method called vulnerability dependence representation graph, allowing the embedding of the data dependence of the variables in the statements and the control structures corresponding to the statements. Moreover, they propose a graph learning network based on a heterogeneous graph transformer, which can automatically learn the importance of contextual sentences for vulnerable sentences. They carry out experiments on the SARD dataset with an improvement in performance between 4.1% and 62.7%. Fan et al. (2023), propose a circle gated graph neural network (CG-GNN) that receives an input tensor structure used to represent information of code. CGGNN possess the capacity to perform heterogeneous graph information fusion more directly and effectively and allows the researchers to reach a higher accuracy precision and recall compared to the TensorGNN and Devign methods. Finally, Zhang et al. (2023) propose Vul-GAI, to overcome the limitations posed by the training time in graph neural network models. They base their methods on graphs and images and unroll their approach in four phases: the graph generation from the code, the node embedding and the image generation from the node embedding. Then vulnerability detection through convolutional neural networks (CNN) is applied. VulGAI is tested on 40657 functions and overcomes the other methods VulDePecker, SySeVR, Devign, VulCNN, mVulPreter and, in accuracy, recall and f1-score also improved by 3.9 times the detection time of VulCNN."}, {"title": "Key Observation 2", "content": "Extensive research and different approaches were experimented with in the past, with a high level of performance."}, {"title": "APR and composed pipelines", "content": "The step which follows automatic vulnerability detection through AI is the application of AI to automatic code fixing."}, {"title": "Code fixers", "content": "Li et al. (2022) propose DEAR, a DL approach which supports fixing general bugs. Experiments run on three selected datasets: Defects4J (395 bugs), BigFix (+26k bugs), and CPat-Miner (+44k bugs) show that the DEAR approach outperforms existing baselines. Chi et al. (2022) leverage Neural Machine Translation (NMT) techniques, to provide a novel approach called SeqTrans to exploit historical vulnerability fixes to automatically fix the source code. Xia & Zhang (2022), propose AlphaRepair, which directly leverages large pre-trained code models for APR without any fine-tuning/retraining on historical bug fixes."}, {"title": "Composed pipelines", "content": "AIBUGHUNTER combines vulnerability detection and code repair. The pipeline is implemented by Fu et al. (2024), combining LineVul (Fu & Tantithamthavorn, 2022) and VulRepair (Fu et al., 2022), two software implemented by the same author. Yang et al. (2020) propose a DL approach based on autoencoders and CNNs, automating bug localization and repairs. Another example of a complete pipeline combining vulnerability detection and code repair is HERCULES, which employs ML to fix code (Saha et al., 2019). Liu et al. (2021), evaluate the effect of fault localization by introducing the metric fault localization sensitiveness (Sens) and analysing 11 APR tools. Sens, is calculated with the ratio of plausibly fixed bugs by modifying the code on non-buggy positions, and the percentage of bugs which could be correctly fixed when the exact bug positions are available but cannot be found by the tool with its fault localization capacities. This metric, to the best of our knowledge, is the first to quantify the impact of the vulnerability detector capability on the overall pipeline. Nevertheless, it does not provide an interval to describe the best and worst pipeline performance, and thus the quantification of the risk in terms of the percentage of errors which the pipeline will overlook when it is employed."}, {"title": "Key observation 3", "content": "Recently, substantial research has appeared regarding the automation of vulnerability fixing. These advances are important and could help to manage the manual effort spent on sieving through tool warnings. However, to the best of our knowledge, the propagation of errors (or final uncertainty of the result) has not been investigated in such automated pipelines."}, {"title": "PIPELINE FORMALIZATION", "content": "In this section, we present the formal basis for our simulator. To simplify the analysis, we make the following assumptions in our model:\n*   No breaking: We assume that the fixer will never turn a true good sample that is classified as bad into a bad sample.\n*   No degradation: We assume that all elements that are fixed, cannot be distinguished from good elements from the beginning. In other words, the performance of the second classifier does not degrade with the fix."}, {"title": "Identify the classifier metrics", "content": "To evaluate the goodness of the AI-augmented system we use the metrics which are typically used to report the performance of a classifier: True Positive Rate (TPR) or Recall $(rec)$, precision $(prec)$. We also use the prevalence rate $(PR)$ of the positive elements $(Pos)$ among the total number of objects $(N)$ in the domain of interest. The prevalence rate is not typically known, so we will assume it to be a parameter whose effects need to be explored by simulation. Specificity is rarely cited in publications using AI models and its absence makes it difficult to reverse engineer the True Negatives.\n$\n\\begin{aligned}\nPos &= TP + FN \\\\\nNeg &= N - Pos \\\\\nPR &= \\frac{Pos}{N} \\\\\nTPR &= \\frac{TP}{Pos} = rec \\\\\nprec &= \\frac{TP}{TP + FP} \\\\\nFAR &= \\frac{FP}{Neg}\n\\end{aligned}\n$\n=\\frac{TP}{TP + FN} \\\\\n=\\frac{FP}{FP + TN}"}, {"title": "Deterministic recall, partial repairs, no breaking changes", "content": "Proposition 2. Let rec be the recall rate of a classifier that is used both as a first and and second classifier, let fr be the theoretical fix rate of the fixer which (i) only affects positive (vulnerable) code and (ii) does not break nor make vulnerable code of the not vulnerable code which is eventually piped through it. The classifier can also correctly recognize unsatisfactory fixes (iii) with the same recall rec. Then the AI-augmented system true performance when applied to a domain with N elements and an initial prevalence rate of $PR$, is\n$\n\\begin{aligned}\nf(aias) &= fr \\cdot rec \\\\\nPR(aias) &= (1-fr \\cdot rec) \\cdot P r \\\\\nTPR(aias) &= rec \\cdot \\frac{(1-fr) \\cdot rec}{1-fr \\cdot rec} \\\\\nFAR(aias) &= rec^{2} \\cdot \\frac{1-prec}{prec} \\frac{(1-f R) P R}{1-(1-f r \\cdot rec) P R}\n\\end{aligned}\n$\nThe results, even under such favourable assumptions, are interesting as they show that uncertainty in the recall may change the overall expected fix rate. Further, it shows that, unless the fix rate is perfect, the prevalence rate is not reduced to zero and it will depend on the uncertainty in the recall.\nAn apparently surprising result is that if the fix rate is perfect then the overall true positive rate (TPR) is zero. This is actually to be expected: with a perfect fix rate, all identified positives are fixed. This does not mean that all positives are eliminated because the false negatives from the first classifier are still present. In general, since $rec < 1$ we have that the term $\\frac{(1-fr) rec}{1-fr \\cdot rec} < 1$ and therefore the recall of the AI-augmented system as a whole is lower than the recall of the first classifier, i.e. $TPR(aias) < TPR$ (see Appendix, section A4).\nWhile the recall of the AI-augmented system does not depend on the prevalence rate, the false alert rate (FAR) depends in a non-linear way on the overall prevalence rate of the system. It is still possible to prove that the false alert rate of the AI-augmented systemas a whole is lower than the false alert rate of the first classifier, i.e. $FAR(aias) < FAR$.\nProof. 2 The first classifier receives in input the positives and the negatives and divides them into TP, FP, FN, and TN. The fixer receives in input $TP1st + FP1st$ of which only a fraction $fr$ Of $TP1st$ is actually fixed (Assumption (i)). According to assumption (ii) the fixer will not transform the false positive into new positives (i.e. it will not transform them into positives nor will not break them). Since the second instance of the classifier does not change the nature of the processed object but at worst misclassifies it we have that\n$\nPos(aias) = \\frac{\\text {unfixed by fixer}}{\\left(1-f_{r}\\right) T P_{1 s t}}+\\frac{\\text { misclassified by } 1^{\\text {st }} \\text { classifier }}{F N_{1 s t}}\n$\nPos(aias) = Pos -$f (aias) \\cdot$ Pos = $PR\\cdot N$-$f (aias)$. $PR.N$\nWe now equates the terms, replace TP1st and FN1st with the corresponding equations, and simplify $PR. N$ from both sides of the equation to obtain 1 \u2013$f (aias)$ = $(1-fr) \\cdot rec$ + $(1 \u2212 rec)$ which simplifies to $f (aias)$ = $fr \\cdot rec$ (see Appendix, section A1).\nWe can use equation (15) to directly obtain the prevalence rate for the AI-augmented system by replacing the value of $f (aias)$ just computed and dividing by the total number of elements N.\nTo compute the true positive rate we replace in the definition of TPR (4) the number of TP surviving at the end of the second classifier which is $(1 \u2212fr) \\cdot TP\u2081st \u00b7 rec$ because by assumption (ii) and (iii) only the original true positives will be reclassified as positives. We divide by the total number of positives of the AI-augmented system as computed from equation 15. By simplifying both numerator and denominator for PRN we obtain\n$\nTPR(aias) = \\frac{(1-fr) recrec}{1-fr \\cdot rec}\n$\nTo compute the false alert rate we need to compute first the false positives of the second classifier. To this extent, we rewrite the definition of false positives (9) in terms of the new set of positives $(1-fr)TP1st$ at the end of the fixer according"}, {"title": "Uncertain recall", "content": "We employ PBA (Iskandar, 2021), to propagate, in the form of interval, uncertainty in the recall through the pipeline. By substituting a specific cumulative distribution function (CDF) with the p-boxes, PBA allows to model the lack of knowledge regarding the specific CDF from which the recall values are sampled. Considering that we cannot possess exhaustive information regarding the CDF of recalls of AI vulnerability detectors, the choice of this mathematical tool is preferred, compared for instance with probabilistic sensitivity analysis. Formulas 28 and 29, are the inverse p-boxes used to sample the lower and upper bound for the recall interval.\n$\nF(p)_{a, b}^{a<\\mu, b>\\mu}= \\begin{cases}\\frac{a}{b-\\mu} p \\quad \\text { for } 0<p<\\frac{b-\\mu}{b-a} \\\\\nb-\\mu \\frac{b-\\mu}{b-a} \\quad \\text { for } \\frac{b-\\mu}{b-a}<p<1 \\\\\n\\lceil \\mu, b] \\quad \\text { for } p=1\\end{cases}\n$"}, {"title": "RECALL IN THE FIELD", "content": "We collect the reported recall values (and precision) of AI-augmented vulnerability detectors and derive the parameters necessary to implement the p-boxes in our simulations."}, {"title": "Search in digital libraries", "content": "Figure 2 illustrates the steps that define our search. We defined a search string to filter publications stored in digital libraries: (\"vulnerability detection\" OR \"fault localization\" ) AND ( \"artificial intelligence\" OR \"AI\" OR \"Deep Learning\" OR \"DL\" OR \"machine learning\" OR \"ML\" ) AND ( \"sensitivity\" OR \"true positive rate\u201d OR \"TPR\" OR \"recall\u201d OR \"hit rate\") AND \"code\".\nWe define a list of selection criteria (SC) that a publication must respect to be selected for the extraction of data points.\n*   SC1. The publication must be related to the topic of vulnerability detection or fault localization. For instance, we discard publications related to general feature location.\n*   SC2. The publication must apply ML or DL algorithms to the problem of vulnerability detection. We discard the publications which do not employ ML or DL.\n*   SC3. Since the metrics considered Pr(aias), $f (aias)$, and FN(aias), depend uniquely on the recall, the publication must (at least) report the recall of the vulnerability detectors.\nWe employed the search string on Scopus without filtering based on the time of publication. We process the 548 results ordered by relevance. We consider the first 200 because they represent a sufficient sample size. After applying SC1 and SC2 to the title and abstract, we retain 142 publications. Finally, applying SC3 resulted in removing 27 more publications."}, {"title": "Collected samples", "content": "After eliminating the outliers (30), there remain 2328 samples that we use to build the p-boxes. The minimum and maximum reported recall are respectively 0.07 and 1.00, while the mean is 0.74. For completeness in Table 1 we also show descriptive statistics on the collected precision samples (but we do not use them yet in our simulation)."}, {"title": "SIMULATION", "content": "Through the simulation, we are interested in calculating PR(aias), $f (aias)$, FN(aias) which, as previously shown (Section 3), depend uniquely on the recall. To allow for future extensions, we implemented the simulator taking into account TN and FP, which are needed to define specificity. At this stage of the research, the specificity value does not affect the final result, thus we set its value to zero."}, {"title": "Simulator", "content": null}, {"title": "Ground truth generator", "content": "We use a ground truth generator to generate the dataset that allows the simulation of the pipeline. Each generated sample represents a code sample, which can be vulnerable or not vulnerable. Thus, the ground truth generator generates fictional positive and negative elements (Pos, Neg)."}, {"title": "P-boxes and recall sampling", "content": "We implement formulas 28 and 29 to sample from the p-boxes, and use them to sample N lower and upper bound recall values to run the simulations.\n*   Both implementations of Formulas 28 and 29 receive as input the minimum (0.07), maximum (1.00), and mean (0.74) value generated from the exploratory data analysis (Section 4.2), and a list of probability values sampled from a uniform distribution.\n*   For each of the uniform values, we sample lower and upper recall bounds. Given the two lists of recall values, one for the lower bound and one for the upper bound, each of N elements, we run N simulations that allow us to calculate the upper and lower bounds."}, {"title": "First classifier", "content": "After generating the lower and upper bound recall values, the first classifier executes the first subdivision of the samples, generating TP1st, FN1st, TN1st, and FP1st.\n*   The first classifier discerns each vulnerable element of the ground truth between TP with probability equal to $rec$ and as FN with probability equal to 1 \u2013 $rec$. This means that the greater the recall the greater the probability that vulnerable objects are classified as TP.\n*   Since the first classifier is simulated with both lower and upper bound recall values, in the end, we obtain lower and upper bounds for each element, thus $[TP1st, TP1st]$, $[FN1st, FN1st]$, $[TN1st, TN1st]$, $[FP1st, FP1st]$."}, {"title": "Fixer", "content": "The fixer, with fix rate fr, tries to repair the samples classified as positives by the first classifier, namely TP1st and FP1st. The fixer repairs each sample classified as positive with probability equal to fr. Since we assume that a FP cannot be broken, the intervention on FP cannot cause it to become a TP."}, {"title": "Second classifier", "content": "The second classifier, with the same recall and specificity as the first classifier, classifies the objects that passed through the fixer, generating TP2nd, FN2nd, TN2nd, and FP2nd .\n*   The second classifier labels each vulnerable object that passed through the fixer as TP with probability equal to the $rec$ and as FN with probability 1 \u2013 $rec$.\n*   Since the second classifier is simulated with both lower and upper bound recall values, we obtain lower and upper bounds for each element, thus $[TP2nd, TP2nd]$, $[FN2nd, FN2nd]$, $[TN2nd, TN2nd]$, and $[FP2nd, FP2nd]$"}, {"title": "Final counter", "content": "The final counter gathers the results from the first classifier, the fixer, and the second classifier and that calculates the final prevalence rate (Pr(aias)), the final fix rate ($f (aias)$), and the ratio $(FNratio)$ between the final number of false negatives (FN(aias)) and the false negatives generated by the first classifier (FN1st). Since the uncertainty propagates until the final counter, each metric will be characterised by a lower and upper bound, thus: $[PR(aias), Pr(aias)]$, $[f(aias),f (aias)]$, $[FNratio, FNratio]$.\nThe code to reproduce the ground truth generator, the p-boxes formulas and recall sampling, the first classifier, the fixer, the second classifier, and the counter, can be found in the Appendix, respectively in sections B1, B2, B3, B4, B5, B6."}, {"title": "Simulation results", "content": "We present the simulation results and show how propagating uncertainty affects Pr(aias) (see Table 2), $f (aias)$ (see Table 3), and the false negatives (see Table 4). Figure 3 instantiates the simulated pipeline, with the results obtained from one of the simulations."}, {"title": "Final prevalence rate", "content": "Table 2 shows the results related to the decrease in the prevalence rate. We run simulations with $PR = (0.10, 0.50, 1.00)$, thus in the first, second and third sets of simulation, the total number of vulnerable samples is equal to the 10%, 50% and 100% of the total samples. For each of these simulation sets, we calculate the final prevalence rate with $fr = (0.50, 0.70, 0.90, 1.00)$, meaning that the expected decrease in the prevalence rate is respectively 50%, 70%, 90% and 100%. But, the theoretical decrease in the prevalence rate that should be observed given a specific starting prevalence rate and fix rate, is only the lower bound of the interval, which corresponds to the minimum prevalence rate obtainable when the capacity to locate vulnerable elements is perfect. In all the other cases the value will fall within the bounds of the interval. For example, when $PR = 0.50$ and $fr = 0.50$ we should observe a decrease in the final prevalence rate of 50%, thus $Pr(aias) = 0.25$. But, Table 2 and Figure 3 show that the 50% decrease only represents the lower bound, contrasting with an upper bound of 0.34."}, {"title": "Real fix rate", "content": "Table 3 show the results related to the final fix rate. We run simulations with theoretical fix rate $fr = (0.50, 0.70, 0.90, 1.00)$. At the end of the simulations, fr only corresponds to the upper bound of the interval of $f (aias)$, which is the maximum fix rate obtainable when the capacity to locate vulnerable elements is maximum. For example, when $fr = 0.50$, the $f (aias)$ oscillates between a maximum of 0.50 equal to fr and a minimum of 0.03. This illustrates the limitations of APR tools and the importance of stating the final results in terms of intervals and not of single numbers, to represent the uncertainty that characterises these systems when they are applied to real-world scenarios."}, {"title": "False negatives ratio", "content": "Table 4 shows the results related to FNratio, which is the ratio between the false negatives generated by the first classifier FN1st and the overall number of false negatives registered at the end of the pipeline FN(aias). Apart for fr = 1, the final ratio is always greater than one, and this indicates that the pipeline is unable to avoid the growth of the number of FN between the first and the second classifier."}, {"title": "CASE STUDIES", "content": "We present two case studies, that measure the impact of uncertainty on rule-based and AI-based APR tools."}, {"title": "Case study one: rule-based APR", "content": "The case study concerning rule-based APR tools is derived from the research by Liu et al. (2020). In this publication performances of 16 APR tools are tested on the dataset Defects4J, which is composed of 395 elements and thus allows for manual validation. Since every element in the dataset is vulnerable, of the total number of code samples, the number of elements which the first classifiers consider positive is 395. The number of bugs repaired changes from tool to tool, but considering the best tool, namely ACS, the number of Fixed code samples is 16, while the number of non-fixed bugs is 379.\nWe use this case study to show how uncertainty deriving from possible errors during manual validation, can impact the results conveyed in the form of points estimates, which do not account for possible errors. As suggested by Dashevskyi et al. (2018), we employ the Agresti-Coull-Wilson confidence interval analysis (Agresti & Coull, 1998), which is a method to construct an accurate and reliable confidence interval, also for small samples sized, and that we use to generate an upper and lower bound regarding the percentage of correct generated patches. Table 5, shows the impact of the uncertainty on the tools that have the best and the worst performance expressed in terms of patches that can fix bugs. Using a 95% confidence interval the esteems for the percentage of correct patches vary widely, both for the three best tools and for the three worst."}, {"title": "Case study two: AI-based APR", "content": "This case study examines the possibility of obtaining an AI-augmented APR tool, composed of two AI subsystems, one dedicated to vulnerability detection, and the other to vulnerability repair.\nWe analyze a DL-based APR tool, AIBUGHUNTER (Fu et al., 2024). This pipeline is the result of the assembly of two systems, namely LineVul (Fu & Tantithamthavorn, 2022), which performs vulnerability detection and VulRepair (Fu et al., 2022), which performs bug-fixing. Since the authors specified that they did not evaluate the whole AIBUGHUNTER pipeline in the dedicated publication, but that they evaluated the two composing tools separately, we use this case study to show to what extent uncertainty can impact the overall performance of an APR pipeline composed by different Al subsystems, trained on different datasets. We consider the dataset on which AIBUGHUNTER is tested, composed of 879 total code samples, all of which have vulnerabilities. We calculate the number of the samples that the first classifier of the pipeline highlights to be vulnerable by multiplying the total code samples by the recall reported in the publication dedicated to LineVul (Fu & Tantithamthavorn, 2022) which amounts to 0.86, obtaining 756 Bad code samples. Then, VulRepair (Fu et al., 2022), with a reported repairing accuracy of 0.44 is used to correct the bugs. Thus we multiply the repairing accuracy by the number of Bad code samples, obtaining 333 Fixed code samples. Thus the number of positive elements which the pipeline does not correct is equal to 423. We then use our simulation pipeline to account for uncertainty in the recall, considering the same number of code samples and the same point estimate for repairing accuracy. Once we account for uncertainty, the final repairing accuracy could be as high as 0.47, but as low as 0.03, compared to the starting point estimate of 0.44."}, {"title": "DISCUSSION", "content": null}, {"title": "Summary of results", "content": "Simulation results show that, once the uncertainty in the recall of the vulnerability detectors is propagated through the pipeline, it affects the overall pipeline performance, in terms of prevalence rate reduction and real fix rate. The simulated AI system can obtain the expected theoretical reduction of the prevalence rate, and a final fix rate equal to the theoretical fix rate, only in the best-case scenario, which is when the recall is maximum. In all the other cases, the real reduction of the number of vulnerable code samples, and the final fix rate, can widely vary, falling in the intervals calculated during the simulation. This finding was confirmed when investigating case studies. While the first case study shows that uncertainty can impact scenarios where AI is not involved, the second case study confirms that the final fix rate depends on the oscillation of the classifier recall.\nSecond, our simulations show that the uncertainty characterising the FNratio is smaller compared to the uncertainty characterising Pr(aias) and $f (aias)$. That is the difference between the lower and upper bound of the intervals related to the FNratio is smaller compared to the intervals of Pr(aias) and $f(aias)$. However, the incapacity of the pipeline to keep the FN stable between the first and the second classifier could mean overlooking true vulnerabilities due to over-approximation of classifier performance, which could lead to untrustworthy decisions about security risks exposing the possible discrepancy between the preference of risk managers who use the AI system, and the risk tolerance embedded in the system (Pat\u00e9-Cornell, 2024)."}, {"title": "Answer to RQ", "content": "How to estimate the total error (or success rate) of the Al-augmented system, given the propagating errors of the classifiers in the pipeline?"}, {"title": "Policy and recommendations for aviation", "content": "The necessity to consider uncertainty at the system level has implications for the policies to be adopted in scenarios where A\u0399 is applied to safety-risk systems such as in the case of aviation. Although the European Union Aviation Safety Agency (EASA) (2023a; 2023b)", "exist": "n*   Vulnerability detection and patching systems: while EASA (2023a) highlights the possible use of AI to detect vulnerabilities and```json\n{"}, {"title": "Risks of uncertainty propagation in AI-augmented security pipelines", "authors": ["Emanuele Mezzi", "Aurora Papotti", "Fabio Massacci", "Katja Tuma"], "abstract": "The use of AI technologies is percolating into the secure development of software-based systems, with an increasing trend of composing AI-based subsystems (with uncertain levels of performance) into automated pipelines. This presents a fundamental research challenge and poses a serious threat to safety-critical domains (e.g., aviation). Despite the existing knowledge about uncertainty in risk analysis, no previous work has estimated the uncertainty of AI-augmented systems given the propagation of errors in the pipeline. We provide the formal underpinnings for capturing uncertainty propagation, develop a simulator to quantify uncertainty, and evaluate the simulation of propagating errors with two case studies. We discuss the generalizability of our approach and present policy implications and recommendations for aviation. Future work includes extending the approach and investigating the required metrics for validation in the aviation domain.", "sections": [{"title": "INTRODUCTION", "content": "Due to the increasing availability of data, AI technologies have spread and are being used in almost every computing system, including in safety-critical domains like aviation (EASA, 2023a). Although the use of AI-augmented systems comes with new promises of improved performance, it also introduces significant risks and challenges (EASA, 2023b; Cox Jr, 2020; Nateghi & Aven, 2021). A major challenge in using AI for risk analysis is conveying to decision-makers the uncertainty inherent to predictions of models, because it clashes with the common practice in the realm of AI to communicate uncertainty with point estimates or ignoring it completely (Guikema, 2020).\nWith the rise of open-source software development and large-scale cloud deployment, more security risk decision-making is automated by running sequences of AI-augmented analyses, like automated program repair (APR) (Long et al., 2017; Ye et al., 2021; Li et al., 2022; Xia & Zhang, 2022; Fu et al., 2024). The use of AI in automated security pipelines, where the first classifier detects a vulnerability and the second tool fixes it, is now becoming more common (Bui et al., 2024), which brings about a fundamental research challenge:\nPropagating uncertainty is the new major challenge for assessing the risk of automated security pipelines.\nThis foundational problem has already manifested in security pipelines with no AI-based computation. To illustrate this problem, we consider four studies: verifying the presence of code smells (Tufano et al., 2017), generalizing the SZZ algorithm to identify the past versions of software affected by a vulnerability (Dashevskyi et al., 2018), identifying vulnerabilities in Java libraries (Kula et al., 2018), and finding how vulnerable Android libraries could be automatically updated (Derr, 2017).\nA few years later, Pashchencko et al. (2020) showed that the results by Kula et al. (2018) are incorrect, and Huang et al. (2019) found that the claims by Derr et al. (2017) are incorrect, both to a large extent. We argue that the reason for this mishap is foundational. All these studies share the impossibility of running manual validation and do not report the uncertainty of their outcomes. The proposed solutions process huge inputs (e.g., 246K commits in Dashevskyi et al. (2018)) so they need an automated tool (with an error rate) to decide whether each sample satisfies the property of interest.\nWith the appearance of new AI-based approaches, such as SeqTrans (Chi et al., 2022), it is becoming imperative to investigate this problem now, before it is too late and AI-augmented systems without global measures of risk become weaved into the automated pipelines in organizations.\nTo address these issues, we focus on understanding the uncertainty due to error propagation in AI Augmented Systems. Among the pipeline, each component may be a potential source of error that leads to an underestimation or overestimation of the actual effectiveness of the proposed solution. Therefore, we formulate the overarching research question:\nRQ: How to estimate the total error (or success rate) of the Al-augmented system, given the propagating errors of the classifiers in the pipeline?\nIf analytical models for the classifiers and the fixer components existed, it could be possible to use the error propagation models used for calculus (Benke et al., 2018). Unfortunately, analytical models of the recall and precision of these tools are extremely rare, therefore, we must resort to the much coarse-grained approximation with probability bound analysis (PBA) (Iskandar, 2021)."}, {"title": "Contributions", "content": "We provide the formal underpinnings for capturing uncertainty propagation in AI-augmented APR pipelines. In addition, we develop a simulator to quantify the effects that propagating uncertainty has in automated APR tools (such as the one presented in Figure 1). We evaluate the simulator and present two case studies, in which we calculate the effects of uncertainty regarding two proposed solutions. Finally, we discuss the policy implications and recommendations for the aviation domain."}, {"title": "BACKGROUND AND RELATED WORK", "content": "We illustrate the composition of AI-augmented APR tools as background and present the related work on uncertainty quantification in AI, and applications of AI to vulnerability detection and APR."}, {"title": "AI-augmented systems", "content": null}, {"title": "Uncertainty quantification in AI", "content": "H\u00fcllermeier and Waegeman (2021), highlight two macro-categories of methods employed to quantify and manage uncertainty in Machine Learning (ML). The first discerns between frequentist-inspired and Bayesian-inspired quantification methods. The second considers the distinction between uncertainty quantification and set-values prediction. Uncertainty quantification methods allow the model to output the prediction and the paired level of certainty, while the set-value methods consist of pre-defining a desired level of certainty and producing a set of candidates that comply with it.\nAbdar et al. 2021 centre their analysis on Deep Learning (DL). Bayesian-inspired methods and ensemble methods represent two of the major categories to represent uncertainty in DL. Through Bayesian methods the DL model samples its parameters from a learnt posterior distribution, allowing the model to avoid fixed parameters and allowing us to inspect the variance and uncover the uncertainty which surrounds the model predictions. The most common Bayesian-inspired technique is the Monte Carlo (MC) dropout. Ensemble methods, combine different predictions from different deterministic predictors. Although they were not introduced in the first instance to explicitly handle uncertainties, they give an intuitive way of representing the model uncertainty on a prediction by evaluating the variety among the predictors (Gawlikowski et al., 2023)."}, {"title": "Key Observation 1", "content": "Extensive research was performed in the field of uncertainty quantification in AI, which brought the development of a variety of methods. However, these approaches are focused on uncertainty quantification of single models, without measuring the propagating effects of uncertainty about the output of the first model when piped as input into another part of the system."}, {"title": "AI in vulnerability detection", "content": "Vulnerability detection is a crucial step in risk analysis of software systems and includes running automated tools scanning parts of the system to prevent future exploitation. Given its potential, experts integrated AI into their vulnerability detection systems, to scale them and make them more flexible to new threats.\nOne of the approaches to perform vulnerability detection is obtained by applying Natural Language Processing. In their approach, Hou et al. (2022), represent the code in the form of a syntax tree and input it to a Transformer model, which leverages the attention mechanism to improve the probability of detecting vulnerabilities. In their research, Akter et al. (2022), create embeddings using Glove and fastText and then use LSTM and Quantum LSTM models to perform vulnerability detection, showing lower execution time and higher accuracy, precision, and recall for the Quantum LSTM.\nAnother line of research excludes Natural Language Processing or embeds it with graph approaches. Yang et al. (2022), propose a new code representation method called vulnerability dependence representation graph, allowing the embedding of the data dependence of the variables in the statements and the control structures corresponding to the statements. Moreover, they propose a graph learning network based on a heterogeneous graph transformer, which can automatically learn the importance of contextual sentences for vulnerable sentences. They carry out experiments on the SARD dataset with an improvement in performance between 4.1% and 62.7%. Fan et al. (2023), propose a circle gated graph neural network (CG-GNN) that receives an input tensor structure used to represent information of code. CGGNN possess the capacity to perform heterogeneous graph information fusion more directly and effectively and allows the researchers to reach a higher accuracy precision and recall compared to the TensorGNN and Devign methods. Finally, Zhang et al. (2023) propose Vul-GAI, to overcome the limitations posed by the training time in graph neural network models. They base their methods on graphs and images and unroll their approach in four phases: the graph generation from the code, the node embedding and the image generation from the node embedding. Then vulnerability detection through convolutional neural networks (CNN) is applied. VulGAI is tested on 40657 functions and overcomes the other methods VulDePecker, SySeVR, Devign, VulCNN, mVulPreter and, in accuracy, recall and f1-score also improved by 3.9 times the detection time of VulCNN."}, {"title": "Key Observation 2", "content": "Extensive research and different approaches were experimented with in the past, with a high level of performance."}, {"title": "APR and composed pipelines", "content": "The step which follows automatic vulnerability detection through AI is the application of AI to automatic code fixing."}, {"title": "Code fixers", "content": "Li et al. (2022) propose DEAR, a DL approach which supports fixing general bugs. Experiments run on three selected datasets: Defects4J (395 bugs), BigFix (+26k bugs), and CPat-Miner (+44k bugs) show that the DEAR approach outperforms existing baselines. Chi et al. (2022) leverage Neural Machine Translation (NMT) techniques, to provide a novel approach called SeqTrans to exploit historical vulnerability fixes to automatically fix the source code. Xia & Zhang (2022), propose AlphaRepair, which directly leverages large pre-trained code models for APR without any fine-tuning/retraining on historical bug fixes."}, {"title": "Composed pipelines", "content": "AIBUGHUNTER combines vulnerability detection and code repair. The pipeline is implemented by Fu et al. (2024), combining LineVul (Fu & Tantithamthavorn, 2022) and VulRepair (Fu et al., 2022), two software implemented by the same author. Yang et al. (2020) propose a DL approach based on autoencoders and CNNs, automating bug localization and repairs. Another example of a complete pipeline combining vulnerability detection and code repair is HERCULES, which employs ML to fix code (Saha et al., 2019). Liu et al. (2021), evaluate the effect of fault localization by introducing the metric fault localization sensitiveness (Sens) and analysing 11 APR tools. Sens, is calculated with the ratio of plausibly fixed bugs by modifying the code on non-buggy positions, and the percentage of bugs which could be correctly fixed when the exact bug positions are available but cannot be found by the tool with its fault localization capacities. This metric, to the best of our knowledge, is the first to quantify the impact of the vulnerability detector capability on the overall pipeline. Nevertheless, it does not provide an interval to describe the best and worst pipeline performance, and thus the quantification of the risk in terms of the percentage of errors which the pipeline will overlook when it is employed."}, {"title": "Key observation 3", "content": "Recently, substantial research has appeared regarding the automation of vulnerability fixing. These advances are important and could help to manage the manual effort spent on sieving through tool warnings. However, to the best of our knowledge, the propagation of errors (or final uncertainty of the result) has not been investigated in such automated pipelines."}, {"title": "PIPELINE FORMALIZATION", "content": "In this section, we present the formal basis for our simulator. To simplify the analysis, we make the following assumptions in our model:\n*   No breaking: We assume that the fixer will never turn a true good sample that is classified as bad into a bad sample.\n*   No degradation: We assume that all elements that are fixed, cannot be distinguished from good elements from the beginning. In other words, the performance of the second classifier does not degrade with the fix."}, {"title": "Identify the classifier metrics", "content": "To evaluate the goodness of the AI-augmented system we use the metrics which are typically used to report the performance of a classifier: True Positive Rate (TPR) or Recall $\\text{$(rec)$}$, precision $\\text{$(prec)$}$. We also use the prevalence rate $\\text{$(PR)$}$ of the positive elements $\\text{$(Pos)$}$ among the total number of objects $\\text{$(N)$}$ in the domain of interest. The prevalence rate is not typically known, so we will assume it to be a parameter whose effects need to be explored by simulation. Specificity is rarely cited in publications using AI models and its absence makes it difficult to reverse engineer the True Negatives.\n$\\begin{aligned}\nPos &= TP + FN \\\\\nNeg &= N - Pos \\\\\nPR &= \\frac{Pos}{N} \\\\\nTPR &= \\frac{TP}{Pos} = rec \\\\\nprec &= \\frac{TP}{TP + FP} \\\\\nFAR &= \\frac{FP}{Neg} = \\frac{FP}{FP + TN}\n\\end{aligned}$"}, {"title": "Deterministic recall, partial repairs, no breaking changes", "content": "Proposition 2. Let rec be the recall rate of a classifier that is used both as a first and and second classifier, let fr be the theoretical fix rate of the fixer which (i) only affects positive (vulnerable) code and (ii) does not break nor make vulnerable code of the not vulnerable code which is eventually piped through it. The classifier can also correctly recognize unsatisfactory fixes (iii) with the same recall rec. Then the AI-augmented system true performance when applied to a domain with N elements and an initial prevalence rate of $PR$, is\n$\\begin{aligned}\nf(aias) &= fr \\cdot rec \\\\\nPR(aias) &= (1-fr \\cdot rec) \\cdot P r \\\\\nTPR(aias) &= rec \\cdot \\frac{(1-fr) \\cdot rec}{1-fr \\cdot rec} \\\\\nFAR(aias) &= rec^{2} \\cdot \\frac{1-prec}{prec} \\frac{(1-f R) P R}{1-(1-f r \\cdot rec) P R}\n\\end{aligned}$\nThe results, even under such favourable assumptions, are interesting as they show that uncertainty in the recall may change the overall expected fix rate. Further, it shows that, unless the fix rate is perfect, the prevalence rate is not reduced to zero and it will depend on the uncertainty in the recall.\nAn apparently surprising result is that if the fix rate is perfect then the overall true positive rate (TPR) is zero. This is actually to be expected: with a perfect fix rate, all identified positives are fixed. This does not mean that all positives are eliminated because the false negatives from the first classifier are still present. In general, since $rec < 1$ we have that the term $\\frac{(1-fr) rec}{1-fr \\cdot rec} < 1$ and therefore the recall of the AI-augmented system as a whole is lower than the recall of the first classifier, i.e. $TPR(aias) < TPR$ (see Appendix, section A4).\nWhile the recall of the AI-augmented system does not depend on the prevalence rate, the false alert rate (FAR) depends in a non-linear way on the overall prevalence rate of the system. It is still possible to prove that the false alert rate of the AI-augmented systemas a whole is lower than the false alert rate of the first classifier, i.e. $FAR(aias) < FAR$.\nProof. 2 The first classifier receives in input the positives and the negatives and divides them into TP, FP, FN, and TN. The fixer receives in input $TP1st + FP1st$ of which only a fraction $fr$ Of $TP1st$ is actually fixed (Assumption (i)). According to assumption (ii) the fixer will not transform the false positive into new positives (i.e. it will not transform them into positives nor will not break them). Since the second instance of the classifier does not change the nature of the processed object but at worst misclassifies it we have that\n$Pos(aias) = \\frac{\\text {unfixed by fixer}}{\\left(1-f_{r}\\right) T P_{1 s t}}+\\frac{\\text { misclassified by } 1^{\\text {st }} \\text { classifier }}{F N_{1 s t}}$\nPos(aias) = Pos -$f (aias) \\cdot$ Pos = $PR\\cdot N$-$f (aias)$. $PR.N$\nWe now equates the terms, replace TP1st and FN1st with the corresponding equations, and simplify $PR. N$ from both sides of the equation to obtain 1 \u2013$f (aias)$ = $(1-fr) \\cdot rec$ + $(1 \u2212 rec)$ which simplifies to $f (aias)$ = $fr \\cdot rec$ (see Appendix, section A1).\nWe can use equation (15) to directly obtain the prevalence rate for the AI-augmented system by replacing the value of $f (aias)$ just computed and dividing by the total number of elements N.\nTo compute the true positive rate we replace in the definition of TPR (4) the number of TP surviving at the end of the second classifier which is $(1 \u2212fr) \\cdot TP\u2081st \u00b7 rec$ because by assumption (ii) and (iii) only the original true positives will be reclassified as positives. We divide by the total number of positives of the AI-augmented system as computed from equation 15. By simplifying both numerator and denominator for PRN we obtain\n$TPR(aias) = \\frac{(1-fr) recrec}{1-fr \\cdot rec}$\nTo compute the false alert rate we need to compute first the false positives of the second classifier. To this extent, we rewrite the definition of false positives (9) in terms of the new set of positives $(1-fr)TP1st$ at the end of the fixer according"}, {"title": "Uncertain recall", "content": "We employ PBA (Iskandar, 2021), to propagate, in the form of interval, uncertainty in the recall through the pipeline. By substituting a specific cumulative distribution function (CDF) with the p-boxes, PBA allows to model the lack of knowledge regarding the specific CDF from which the recall values are sampled. Considering that we cannot possess exhaustive information regarding the CDF of recalls of AI vulnerability detectors, the choice of this mathematical tool is preferred, compared for instance with probabilistic sensitivity analysis. Formulas 28 and 29, are the inverse p-boxes used to sample the lower and upper bound for the recall interval.\n$F(p)_{a, b}^{a<\\mu, b>\\mu}= \\begin{cases}\\frac{a}{b-\\mu} p \\quad \\text { for } 0<p<\\frac{b-\\mu}{b-a} \\\\\nb-\\mu \\frac{b-\\mu}{b-a} \\quad \\text { for } \\frac{b-\\mu}{b-a}<p<1 \\\\\n\\lceil \\mu, b] \\quad \\text { for } p=1\\end{cases}$"}, {"title": "RECALL IN THE FIELD", "content": "We collect the reported recall values (and precision) of AI-augmented vulnerability detectors and derive the parameters necessary to implement the p-boxes in our simulations."}, {"title": "Search in digital libraries", "content": "Figure 2 illustrates the steps that define our search. We defined a search string to filter publications stored in digital libraries: (\"vulnerability detection\" OR \"fault localization\" ) AND ( \"artificial intelligence\" OR \"AI\" OR \"Deep Learning\" OR \"DL\" OR \"machine learning\" OR \"ML\" ) AND ( \"sensitivity\" OR \"true positive rate\u201d OR \"TPR\" OR \"recall\u201d OR \"hit rate\") AND \"code\".\nWe define a list of selection criteria (SC) that a publication must respect to be selected for the extraction of data points.\n*   SC1. The publication must be related to the topic of vulnerability detection or fault localization. For instance, we discard publications related to general feature location.\n*   SC2. The publication must apply ML or DL algorithms to the problem of vulnerability detection. We discard the publications which do not employ ML or DL.\n*   SC3. Since the metrics considered Pr(aias), $f (aias)$, and FN(aias), depend uniquely on the recall, the publication must (at least) report the recall of the vulnerability detectors.\nWe employed the search string on Scopus without filtering based on the time of publication. We process the 548 results ordered by relevance. We consider the first 200 because they represent a sufficient sample size. After applying SC1 and SC2 to the title and abstract, we retain 142 publications. Finally, applying SC3 resulted in removing 27 more publications."}, {"title": "Collected samples", "content": "After eliminating the outliers (30), there remain 2328 samples that we use to build the p-boxes. The minimum and maximum reported recall are respectively 0.07 and 1.00, while the mean is 0.74. For completeness in Table 1 we also show descriptive statistics on the collected precision samples (but we do not use them yet in our simulation)."}, {"title": "SIMULATION", "content": "Through the simulation, we are interested in calculating PR(aias), $f (aias)$, FN(aias) which, as previously shown (Section 3), depend uniquely on the recall. To allow for future extensions, we implemented the simulator taking into account TN and FP, which are needed to define specificity. At this stage of the research, the specificity value does not affect the final result, thus we set its value to zero."}, {"title": "Simulator", "content": null}, {"title": "Ground truth generator", "content": "We use a ground truth generator to generate the dataset that allows the simulation of the pipeline. Each generated sample represents a code sample, which can be vulnerable or not vulnerable. Thus, the ground truth generator generates fictional positive and negative elements (Pos, Neg)."}, {"title": "P-boxes and recall sampling", "content": "We implement formulas 28 and 29 to sample from the p-boxes, and use them to sample N lower and upper bound recall values to run the simulations.\n*   Both implementations of Formulas 28 and 29 receive as input the minimum (0.07), maximum (1.00), and mean (0.74) value generated from the exploratory data analysis (Section 4.2), and a list of probability values sampled from a uniform distribution.\n*   For each of the uniform values, we sample lower and upper recall bounds. Given the two lists of recall values, one for the lower bound and one for the upper bound, each of N elements, we run N simulations that allow us to calculate the upper and lower bounds."}, {"title": "First classifier", "content": "After generating the lower and upper bound recall values, the first classifier executes the first subdivision of the samples, generating TP1st, FN1st, TN1st, and FP1st.\n*   The first classifier discerns each vulnerable element of the ground truth between TP with probability equal to $rec$ and as FN with probability equal to 1 \u2013 $rec$. This means that the greater the recall the greater the probability that vulnerable objects are classified as TP.\n*   Since the first classifier is simulated with both lower and upper bound recall values, in the end, we obtain lower and upper bounds for each element, thus $[TP1st, TP1st]$, $[FN1st, FN1st]$, $[TN1st, TN1st]$, $[FP1st, FP1st]$."}, {"title": "Fixer", "content": "The fixer, with fix rate fr, tries to repair the samples classified as positives by the first classifier, namely TP1st and FP1st. The fixer repairs each sample classified as positive with probability equal to fr. Since we assume that a FP cannot be broken, the intervention on FP cannot cause it to become a TP."}, {"title": "Second classifier", "content": "The second classifier, with the same recall and specificity as the first classifier, classifies the objects that passed through the fixer, generating TP2nd, FN2nd, TN2nd, and FP2nd .\n*   The second classifier labels each vulnerable object that passed through the fixer as TP with probability equal to the $rec$ and as FN with probability 1 \u2013 $rec$.\n*   Since the second classifier is simulated with both lower and upper bound recall values, we obtain lower and upper bounds for each element, thus $[TP2nd, TP2nd]$, $[FN2nd, FN2nd]$, $[TN2nd, TN2nd]$, and $[FP2nd, FP2nd]$"}, {"title": "Final counter", "content": "The final counter gathers the results from the first classifier, the fixer, and the second classifier and that calculates the final prevalence rate (Pr(aias)), the final fix rate ($f (aias)$), and the ratio $(FNratio)$ between the final number of false negatives (FN(aias)) and the false negatives generated by the first classifier (FN1st). Since the uncertainty propagates until the final counter, each metric will be characterised by a lower and upper bound, thus: $[PR(aias), Pr(aias)]$, $[f(aias),f (aias)]$, $[FNratio, FNratio]$.\nThe code to reproduce the ground truth generator, the p-boxes formulas and recall sampling, the first classifier, the fixer, the second classifier, and the counter, can be found in the Appendix, respectively in sections B1, B2, B3, B4, B5, B6."}, {"title": "Simulation results", "content": "We present the simulation results and show how propagating uncertainty affects Pr(aias) (see Table 2), $f (aias)$ (see Table 3), and the false negatives (see Table 4). Figure 3 instantiates the simulated pipeline, with the results obtained from one of the simulations."}, {"title": "Final prevalence rate", "content": "Table 2 shows the results related to the decrease in the prevalence rate. We run simulations with $PR = (0.10, 0.50, 1.00)$, thus in the first, second and third sets of simulation, the total number of vulnerable samples is equal to the 10%, 50% and 100% of the total samples. For each of these simulation sets, we calculate the final prevalence rate with $fr = (0.50, 0.70, 0.90, 1.00)$, meaning that the expected decrease in the prevalence rate is respectively 50%, 70%, 90% and 100%. But, the theoretical decrease in the prevalence rate that should be observed given a specific starting prevalence rate and fix rate, is only the lower bound of the interval, which corresponds to the minimum prevalence rate obtainable when the capacity to locate vulnerable elements is perfect. In all the other cases the value will fall within the bounds of the interval. For example, when $PR = 0.50$ and $fr = 0.50$ we should observe a decrease in the final prevalence rate of 50%, thus $Pr(aias) = 0.25$. But, Table 2 and Figure 3 show that the 50% decrease only represents the lower bound, contrasting with an upper bound of 0.34."}, {"title": "Real fix rate", "content": "Table 3 show the results related to the final fix rate. We run simulations with theoretical fix rate $fr = (0.50, 0.70, 0.90, 1.00)$. At the end of the simulations, fr only corresponds to the upper bound of the interval of $f (aias)$, which is the maximum fix rate obtainable when the capacity to locate vulnerable elements is maximum. For example, when $fr = 0.50$, the $f (aias)$ oscillates between a maximum of 0.50 equal to fr and a minimum of 0.03. This illustrates the limitations of APR tools and the importance of stating the final results in terms of intervals and not of single numbers, to represent the uncertainty that characterises these systems when they are applied to real-world scenarios."}, {"title": "False negatives ratio", "content": "Table 4 shows the results related to FNratio, which is the ratio between the false negatives generated by the first classifier FN1st and the overall number of false negatives registered at the end of the pipeline FN(aias). Apart for fr = 1, the final ratio is always greater than one, and this indicates that the pipeline is unable to avoid the growth of the number of FN between the first and the second classifier."}, {"title": "CASE STUDIES", "content": "We present two case studies, that measure the impact of uncertainty on rule-based and AI-based APR tools."}, {"title": "Case study one: rule-based APR", "content": "The case study concerning rule-based APR tools is derived from the research by Liu et al. (2020). In this publication performances of 16 APR tools are tested on the dataset Defects4J, which is composed of 395 elements and thus allows for manual validation. Since every element in the dataset is vulnerable, of the total number of code samples, the number of elements which the first classifiers consider positive is 395. The number of bugs repaired changes from tool to tool, but considering the best tool, namely ACS, the number of Fixed code samples is 16, while the number of non-fixed bugs is 379.\nWe use this case study to show how uncertainty deriving from possible errors during manual validation, can impact the results conveyed in the form of points estimates, which do not account for possible errors. As suggested by Dashevskyi et al. (2018), we employ the Agresti-Coull-Wilson confidence interval analysis (Agresti & Coull, 1998), which is a method to construct an accurate and reliable confidence interval, also for small samples sized, and that we use to generate an upper and lower bound regarding the percentage of correct generated patches. Table 5, shows the impact of the uncertainty on the tools that have the best and the worst performance expressed in terms of patches that can fix bugs. Using a 95% confidence interval the esteems for the percentage of correct patches vary widely, both for the three best tools and for the three worst."}, {"title": "Case study two: AI-based APR", "content": "This case study examines the possibility of obtaining an AI-augmented APR tool, composed of two AI subsystems, one dedicated to vulnerability detection, and the other to vulnerability repair.\nWe analyze a DL-based APR tool, AIBUGHUNTER (Fu et al., 2024). This pipeline is the result of the assembly of two systems, namely LineVul (Fu & Tantithamthavorn, 2022), which performs vulnerability detection and VulRepair (Fu et al., 2022), which performs bug-fixing. Since the authors specified that they did not evaluate the whole AIBUGHUNTER pipeline in the dedicated publication, but that they evaluated the two composing tools separately, we use this case study to show to what extent uncertainty can impact the overall performance of an APR pipeline composed by different Al subsystems, trained on different datasets. We consider the dataset on which AIBUGHUNTER is tested, composed of 879 total code samples, all of which have vulnerabilities. We calculate the number of the samples that the first classifier of the pipeline highlights to be vulnerable by multiplying the total code samples by the recall reported in the publication dedicated to LineVul (Fu & Tantithamthavorn, 2022) which amounts to 0.86, obtaining 756 Bad code samples. Then, VulRepair (Fu et al., 2022), with a reported repairing accuracy of 0.44 is used to correct the bugs. Thus we multiply the repairing accuracy by the number of Bad code samples, obtaining 333 Fixed code samples. Thus the number of positive elements which the pipeline does not correct is equal to 423. We then use our simulation pipeline to account for uncertainty in the recall, considering the same number of code samples and the same point estimate for repairing accuracy. Once we account for uncertainty, the final repairing accuracy could be as high as 0.47, but as low as 0.03, compared to the starting point estimate of 0.44."}, {"title": "DISCUSSION", "content": null}, {"title": "Summary of results", "content": "Simulation results show that, once the uncertainty in the recall of the vulnerability detectors is propagated through the pipeline, it affects the overall pipeline performance, in terms of prevalence rate reduction and real fix rate. The simulated AI system can obtain the expected theoretical reduction of the prevalence rate, and a final fix rate equal to the theoretical fix rate, only in the best-case scenario, which is when the recall is maximum. In all the other cases, the real reduction of the number of vulnerable code samples, and the final fix rate, can widely vary, falling in the intervals calculated during the simulation. This finding was confirmed when investigating case studies. While the first case study shows that uncertainty can impact scenarios where AI is not involved, the second case study confirms that the final fix rate depends on the oscillation of the classifier recall.\nSecond, our simulations show that the uncertainty characterising the FNratio is smaller compared to the uncertainty characterising Pr(aias) and $f (aias)$. That is the difference between the lower and upper bound of the intervals related to the FNratio is smaller compared to the intervals of Pr(aias) and $f(aias)$. However, the incapacity of the pipeline to keep the FN stable between the first and the second classifier could mean overlooking true vulnerabilities due to over-approximation of classifier performance, which could lead to untrustworthy decisions about security risks exposing the possible discrepancy between the preference of risk managers who use the AI system, and the risk tolerance embedded in the system (Pat\u00e9-Cornell, 2024)."}, {"title": "Answer to RQ", "content": "How to estimate the total error (or success rate) of the Al-augmented system, given the propagating errors of the classifiers in the pipeline?"}, {"title": "Policy and recommendations for aviation", "content": "The necessity to consider uncertainty at the system level has implications for the policies to be adopted in scenarios where A\u0399 is applied to safety-risk systems such as in the case of aviation. Although the European Union Aviation Safety Agency (EASA) (2023a; 2023b)", "exist": "n*   Vulnerability detection and patching systems: while EASA (2023a) highlights the possible use of AI to detect vulnerabilities and correct code, the scope of"}]}]}