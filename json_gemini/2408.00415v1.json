{"title": "DRIVEARENA: A Closed-loop Generative Simulation Platform for Autonomous Driving", "authors": ["Xuemeng Yang", "Licheng Wen", "Yukai Ma", "Jianbiao Mei", "Xin Li", "Tiantian Wei", "Wenjie Lei", "Daocheng Fu", "Pinlong Cai", "Min Dou", "Botian Shi", "Liang He", "Yong Liu", "Yu Qiao"], "abstract": "This paper presented DRIVEARENA, the first high-fidelity closed-loop simulation system designed for driving agents navigating in real scenarios. DRIVEARENA features a flexible, modular architecture, allowing for the seamless interchange of its core components: Traffic Manager, a traffic simulator capable of generating realistic traffic flow on any worldwide street map, and World Dreamer, a high-fidelity conditional generative model with infinite auto-regression. This powerful synergy empowers any driving agent capable of processing real-world images to navigate in DRIVEARENA's simulated environment. The agent perceives its surroundings through images generated by World Dreamer and output trajectories. These trajectories are fed into Traffic Manager, achieving realistic interactions with other vehicles and producing a new scene layout. Finally, the latest scene layout is relayed back into World Dreamer, perpetuating the simulation cycle. This iterative process fosters closed-loop exploration within a highly realistic environment, providing a valuable platform for developing and evaluating driving agents across diverse and challenging scenarios. DRIVEARENA signifies a substantial leap forward in leveraging generative image data for the driving simulation platform, opening insights for closed-loop autonomous driving.", "sections": [{"title": "1. Introduction", "content": "Autonomous driving (AD) algorithms have advanced rapidly in recent decades [1-9], progressing from modular pipelines [10-13] to end-to-end models [14-16] and knowledge-driven methods [17-19]. Despite demonstrating outstanding performance across various benchmarks, significant challenges persist in evaluating these algorithms on replayed open-loop datasets, obscuring their real-world efficacy. Public datasets [20-22], while offering realistic driving data with authentic sensor inputs and traffic behavior, are inherently biased towards simple straight-ahead scenarios. In such cases, an agent can achieve seemingly good performance by merely maintaining its current state, complicating the assessment of actual driving capabilities in complex situations. Furthermore, the agent's current decision does not affect execution or subsequent decisions in the open-loop evaluation, which prevents it from reflecting cumulative errors in real-world driving scenarios. Additionally, the static nature of recorded datasets, where other vehicles cannot react to the ego vehicle's behavior, further hinders the evaluation of AD algorithms in dynamic, real-world conditions.\nAs illustrated in Figure 1, we analyze existing AD methods and platforms, revealing that most of them are inadequate for a high-fidelity closed-loop simulation. Ideally, as an aspect of embodied intelligence, agents should be evaluated in a closed-loop environment, where other agents react to the actions of the ego vehicle, and the ego vehicle receives changed sensor input accordingly. However, existing simulation environments either cannot simulate sensor inputs [23-25] or have a significant domain gap with the real world [26,27], making it difficult to seamlessly integrate algorithms into the real world, thus posing a huge challenge for closed-loop evaluation. We believe that the simulator should not only closely reflect the visual and physical aspects of the real world, but also promote the continuous learning and evolution of the model within an exploratory closed-loop system for adapting to diverse complex driving scenarios. To achieve this goal, it is imperative to establish a high-fidelity simulator that complies with physical laws and supports interactive functionalities.\nTherefore, we present DRIVEARENA, a pioneering closed-loop simulator based on conditional generative models for training and testing driving agents. Specifically, DRIVEARENA offers a flexible platform that can be integrated with any camera-input driving agent. It adopts a modular design and naturally supports iterative upgrades of each module. DRIVEARENA consists of a Traffic Manager that manages traffic flow and a World Dreamer based on auto-regressive generation. Traffic Manager can generate realistic interactive traffic flow on any road network worldwide, while World Dreamer is a high-fidelity conditional generative model with infinite autoregression. The driving agent should make corresponding driving actions based on the images generated by World Dreamer, and feed them back to Traffic Manager to update the status of vehicles in the environment. The new scene layout will be returned to World Dreamer for a new round of simulation. This iterative process realizes the dynamic interaction between the driving agent and the simulation environment. The specific contributions are as follows:\n\u2022 High-fidelity Closed-loop Simulation: We propose the first high-fidelity closed-loop simulator for autonomous driving, DRIVEARENA, which can provide realistic surround images and integrate seamlessly with existing vision-based driving agents. It can closely reflect the visual and physical properties of the real world, enabling agents to continuously learn and evolve in a closed-loop manner and adapt to various complex driving scenarios.\n\u2022 Controllability and Scalability: Our Traffic Manager can dynamically control the movement of all vehicles in the scenarios and feed the road and vehicle layouts into World Dreamer, which utilizes a conditional diffusion framework to generate realistic images in a stable and controllable manner. Additionally, DRIVEARENA supports simulation using road networks from any city worldwide, enabling the creation of diverse driving scenario images with varying styles.\n\u2022 Modularized Design: The Driving Agent, Traffic Manager and World Dreamer communicate via network interfaces, enabling a highly flexible and modular framework. This architecture allows each component to be replaced with different methods without requiring specific implementations. Functioning as an arena for these players, DRIVEARENA facilitates comprehensive testing and improvement of both vision-based autonomous driving algorithms and driving scene generative models."}, {"title": "2. DRIVEARENA Framework", "content": "As illustrated in Figure 2, the framework of our proposed DRIVEARENA comprises two key components: a Traffic Manager functioning as the backend physical engine and a World Dreamer serving as the real-world image renderer. Unlike conventional approaches, DRIVEARENA does not rely on pre-built digital assets or reconstructed 3D road models. Instead, the Traffic Manager adapts to road networks of any city in OpenStreetMap (OSM) format [28], which can be directly downloaded from the Internet. This flexibility enables closed-loop traffic simulations on diverse urban layouts.\nThe Traffic Manager receives ego trajectories output by the autonomous driving agent and manages the movement of all background vehicles. Unlike world model approaches [29,30] that rely on diffusion models for both image generation and vehicle movement prediction, our Traffic Manager utilizes explicit traffic flow generation algorithms [31]. This approach enables the generation of a wider range of uncommon and potentially unsafe traffic scenarios, while also facilitating real-time collision detection between vehicles.\nWorld Dreamer generates realistic camera images that precisely correspond to the Traffic Manager's output. It also allows for user-defined prompts to control various elements of the generated images, such as street view style, time of day, and weather conditions, enhancing the diversity of the generated scenes. Specifically, it employs a diffusion-based model that utilizes the current map and vehicle layouts as control conditions to produce surround-view images. These images serve as input for end-to-end driving agents. Given DRIVEARENA's closed-loop architecture, the diffusion model is required to maintain both cross-view and temporal consistency in the generated images.\nThe generated multi-view images of the current frame are fed into the end-to-end autonomous driving agents, which can output the ego vehicle's movement. The planned ego trajectory is subsequently sent to DRIVEARENA for the next simulation step. The simulation concludes when the ego vehicle either successfully completes the entire route, crashes, or deviates from the road. Upon completion, DRIVEARENA performs a comprehensive evaluation process to assess the driving agent's capabilities.\nIt is noteworthy that DRIVEARENA employs a distributed modular design. The Traffic Manager, World Dreamer, and AD agent communicate via network using standardized interfaces. Consequently, DRIVEARENA does not mandate specific implementations for the World Dreamer or the AD agent. Our framework aims to function as an \u201carena\" for these \u201cplayers\", facilitating comprehensive testing and improvement of both end-to-end autonomous driving algorithms and realistic driving scene generative models."}, {"title": "3. Methodology", "content": "Following the DRIVEARENA framework outlined above, we have implemented a preliminary version of DRIVEARENA. In this section, we elaborate on the implementation of each module: Traffic Manager, World Dreamer, and AD agent, while describing necessary details that were not previously mentioned. At the end of this section, we present both the open-loop and closed-loop evaluation metrics for AD agents in DRIVEARENA."}, {"title": "3.1. Traffic Manager", "content": "Most existing realistic driving simulators [32-34] rely on limited layouts from public datasets, lacking diversity for dynamic environments. To address these challenges, we utilize LimSim [23, 35] as the underlying Traffic Manager to simulate dynamic traffic scenarios and generate road and vehicle layouts for subsequent environment generation. LimSim also provides a user-friendly front-end GUI, which directly displays the BEV map and results from World Dreamer and the driving agent.\nOur Traffic Manager enables interactive simulations of multiple vehicles in traffic flow, including comprehensive vehicle planning and control. We adopt a hierarchical multi-vehicle decision-making and planning framework, which jointly makes decisions for all vehicles within the flow and reacts promptly to the dynamic environment through a high-frequency planning module [31]. The framework also incorporates a cooperation factor and trajectory weight set, introducing diversity to autonomous vehicles in traffic at both social and individual levels.\nFurthermore, our dynamic simulator supports various custom HD maps of any city from OpenStreetMap, facilitating the construction of diverse road graphs for convenient simulation. The Traffic Manager controls the movement of all background vehicles. For the ego vehicle, we provide two distinct simulation modes: open-loop and closed-loop. In closed-loop mode, the driving agent performs planning for the ego vehicle, and Traffic Manager uses the agent-outputted trajectory to control the ego vehicle accordingly. In open-loop mode, the trajectory generated by the driving agent is not actually used to control the ego vehicle; instead, Traffic Manager maintains control in a closed-loop manner. The details of these two modes are further elaborated in Section 3.4."}, {"title": "3.2. World Dreamer", "content": "Unlike recent autonomous driving generation methods [32-34] that use Neural Radiance Fields (NeRF) and 3D Gaussian Splatting for environment reconstruction from logged video, we design a diffusion-based World Dreamer. It utilizes control conditions of the map and vehicle layouts from the Traffic Manager to generate geometrically and contextually accurate driving scenarios. Our framework shares several advantages: (1) Better controllability. The generated scenes can be controlled by scene layouts from Traffic Manager, textual prompts, and reference images to capture different weather conditions, lighting, and scene styles. (2) Better scalability. Our framework can adapt to various road structures without the need to model the scene in advance. In theory, we support the generation of driving scenes for any city in the world by leveraging layouts from OpenStreetMap.\nWe illustrate our diffusion-based World Dreamer in Figure 3. Built upon the stable diffusion pipeline [36], World Dreamer utilizes an effective condition encoding module that accepts a variety of conditional inputs including map and vehicle layouts, text descriptions, camera parameters, ego poses, and reference images to generate realistic surround-view images. Considering the importance of ensuring synthesis scene consistency across different views and time spans for driving agents, we integrate a cross-view attention module, inspired by [29], to maintain coherence across different views. Additionally, we adopt an image auto-regressive generation paradigm to enforce temporal consistency. This approach enables World Dreamer to not only maximally maintain the temporal consistency of the generated videos, but also generate videos of arbitrary length in an infinite stream, which provides great support for autonomous driving simulation.\nCondition encoding. Previous work [29] applied BEV layout as conditional input to control the output of the diffusion model, which increased the difficulty of the network in learning to generate geometrically and contextually accurate driving scenes. In this work, we present a new condition encoding module to introduce more guidance information, which helps the diffusion module generate high-fidelity surround images. Specifically, in addition to encoding camera poses for each view, text descriptions, 3D object bounding boxes, and BEV map layouts using a condition encoder similar to [29], we also explicitly project the map and object layouts onto each camera view to generate layout canvases for more accurate lane and vehicle generation guidance. Specifically, the text embedding \\(e_{text}\\) is obtained by encoding the text descriptions with the CLIP text encoder [37]. The parameters \\(P = \\{K \\in R^{3\\times3}, R \\in R^{3\\times3}, T\\in R^{3\\times1}\\}\\) of each camera and the 8 vertices of the 3D bounding boxes are encoded to \\(e_{cam}\\) and \\(e_{box}\\) by Fourier embedding [38], where K, R, T represent camera intrinsic, rotations and translations respectively. The 2D BEV map grid uses the same encoding method as in [29] to get embedding \\(e_{map}\\). Then, each category of the HD maps and the 3D boxes is projected onto the image plane to obtain the map canvas and box canvas, respectively. These canvases are concatenated to create the layout canvas. The final feature \\(e_{layout}\\) can be obtained by encoding the layout canvas by the conditional encoding network [39].\nMoreover, we introduce a reference condition to provide appearance and temporal consistency guidance. During training, we randomly extract a frame from the past L frames as a reference frame and use the pre-trained CLIP model [37] to extract reference features \\(e_{ref}\\) from the multi-view images. The encoded reference features imply semantic context and are integrated into the conditional encoder through the cross-attention module. In order to make the diffusion model aware of the motion changes of the ego-vehicle, we also encode the ego-pose relative to the reference frame into the conditional encoder to capture the motion change trend of the background. The relative pose embedding \\(e_{rel}\\) is encoded by Fourier embedding. By incorporating the above control conditions, we can effectively control the generation of surround images.\nAuto-regressive generation. To facilitate online inference and streaming video generation while maintaining temporal coherence, we have developed an auto-regressive generation pipeline. Specifically, during the inference phase, the previously generated images and the corresponding relative ego pose are used as reference conditions. This approach guides the diffusion model to generate current surround images with enhanced consistency, ensuring a smoother transition and coherence with the previously generated frames.\nWhat we designed in this paper is just a simple implementation of the World Dreamer. We also verify that extending the auto-regressive generation to a multi-frame version (using multiple past frames as references and outputting multi-frame images) and adding additional temporal modules can improve temporal consistency."}, {"title": "3.3. Driving Agent", "content": "Recent works [40, 41] have demonstrated the challenges in justifying the planning behavior of driving agents through open-loop evaluation on public datasets [20], primarily due to the simplistic nature of driving scenarios presented. While some studies [42] have conducted closed-loop evaluations using simulators like CARLA [26], discrepancies such as appearance and scene diversity persist between these simulations and the dynamic real world. To bridge this gap, our DRIVEARENA provides a realistic simulation platform with the corresponding interfaces for camera-based driving agents [14, 16, 43] to perform more comprehensive evaluations, including both open-loop and closed-loop testing. Moreover, by changing the input conditions, such as the road and vehicle layouts, DRIVEARENA could generate corner cases and facilitate these driving agents' evaluation on out-of-distribution scenarios. Without loss of generality, we select a representative end-to-end driving agent, namely UniAD [14], to conduct both open-loop and closed-loop testing in our DRIVEARENA. UniAD utilizes surround images to predict motion trajectories for the ego vehicle and other agent vehicles, which can be seamlessly integrated with the API of our dynamic simulator for evaluation. Furthermore, the perceptual outputs, such as 3D detection and map segmentation, contribute to enhancing the validation of realism in our environment generation."}, {"title": "3.4. Ego Control Modes and Evaluation Metrics", "content": "DRIVEARENA inherently supports \"closed-loop\" simulation mode of driving agents. That is, the system adopts the trajectory output by the agent at each timestep, updates the ego vehicle's state based on this trajectory, and simulates the actions of background vehicles. Subsequently, it generates multi-view images for the next timestep, thus maintaining a continuous feedback closed-loop. Additionally, recognizing that some AD agents may be unable to perform long-term closed-loop simulation during the development process, DRIVEARENA also supports the \u201copen-loop", "metrics": "PDM Score (PDMS) and Arena Driving Score (ADS).\nPDMS, initially proposed by NAVSIM [44], evaluates the trajectory output at each timestep. We adhere to the original definition of PDMS, which aggregates the following sub-scores:\n\\(PDMSt = \\prod_{m \\in \\{NC,DAC\\}} score_m  \\frac{\\sum_{w\\in \\{EP,TTC,C\\}} weight_w \\times score_w }{\\sum_{w\\in \\{EP,TTC,C\\}} weight_w}\\)\nwhere the penalties include the drive with no collisions (NC) with road users and drivable area compliance (DAC), as well as the weighted average including ego progress (EP), time-to-collision (TTC), and comfort (C). We implement minor modifications tailored to DRIVEARENA: in \\(score_{NC}\\), we do not differentiate \"at-fault\" collisions, and for \\(score_{EP}\\), we utilize the Traffic Manager's Ego path planner as the reference trajectory instead of the Predictive Driver Model. At the end of the simulation, the final PDM Score is averaged across all simulation frames.\n\\(PDMS = \\frac{\\sum_{t}^{T}PDMSt}{T} \\in [0, 1]\\)\nFor open-loop simulations, PDMS serves directly as the evaluation metric for AD agents. However, for driving agents operating under the \u201cclosed-loop", "completion": "n\\(ADS = RC \\times PDMS\\)\nwhere \\(R_c \\in [0, 1]\\) represents route completion, defined as the percentage of the route distance completed by an agent. Given that \"closed-loop\" simulations terminate upon agent collision with other road users or deviation from the road, ADS provides a suitable metric for differentiating agents' driving safety and consistency."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. World Dreamer Setups", "content": "Dataset. For World Dreamer, we use the nuScenes [20] dataset for training. Following the official configuration, we employ 700 scenes for training and 150 for validation. We focus on four road categories (lane boundary, lane divider, pedestrian crossing, and drivable area) and ten object categories. The nuScenes dataset contains data collected from four different cities, covering various light and weather conditions, including daytime, night, sunny, cloudy, and rainy scenarios, enabling DRIVEARENA to conditionally imitate diverse appearances. We additionally annotated each scene using GPT-4V, providing detailed scene descriptions that include elements like time, weather, street style, road structure, and appearance. These descriptions serve as text prompt conditions.\nModel Setup. The model is initialized with the pre-trained Stable Diffusion v1.5 [46], with only the newly added parameters being trained. For various conditions, except for the encoding of reference images and text prompts, the encoders for other conditions are randomly initialized and trained from scratch. These conditions are then integrated into the UNet using a randomly initialized ControlNet [39] to control the denoising process.\nTraining and Inference. To utilize the reference images and achieve temporal correlation, we employ ASAP [47] to generate 12Hz interpolated annotations and crop them into image clips of length L = 7. During training, we use the last frame of each clip as the current frame, select any frame from the clip as the reference frame, and calculate the relative pose between them to model the motion trend of the background. Accordingly, the surround images corresponding to the reference frame are input to the network as reference images. During inference, the generated result of the previous frame is used as the current reference images, enabling unlimited length generation. The experiment is conducted on 8 NVIDIA A100 (80GB) GPUs with a batch size of 4x8 and 200K iterations of training. The AdamW optimizer is used with a learning rate of 1e-4. The network follows the same image resolution (224x400) as MagicDrive, and when input to the driving agent, it will be upsampled to the original image size of nuScenes (900\u00d71600) through a super-resolution algorithm [48]."}, {"title": "4.2. Traffic Manager Setups", "content": "Operating Frequencies. In our experiments, the Traffic Manager operates at a frequency of 10Hz, while the control frequency is set to 2Hz. This configuration results in the Traffic Manager sending the current layout to World Dreamer every 0.5 simulation seconds, requesting surround images. These images are then forwarded to the driving agent, which predicts and plans the subsequent trajectory for the ego vehicle. The Traffic Manager, World Dreamer, and driving agent communicate via HTTP protocol, enabling deployment across different servers.\nSimulation Modes. As detailed in Section 3.4, we implement two simulation modes. In the open-loop mode, all vehicles, including the ego vehicle, are controlled by Traffic Manager itself. The driving agent can predict the ego vehicle's trajectory, but its trajectory is not actually executed. In the closed-loop mode, the ego vehicle is controlled by the driving agent, and the simulation terminates if it crashes with other vehicles or leaves the road.\nSupported Maps. Currently, DRIVEARENA supports four different maps, which are: singapore-onenorth, boston-seaport, boston-thomaspark, and carla-town05. The first two maps closely resemble the corresponding areas in the nuScenes dataset, while the last one replicates the road network of the Town05 map in the CARLA simulator. Notably, Traffic Manager can download road network data for any area directly from OpenStreetMap and perform simulations, enabling DRIVEARENA to simulate the road network of almost any city worldwide. OpenstreetMap also accepts customized operations, and users can draw the desired road network structure for simulation testing."}, {"title": "4.3. World Dreamer Fidelity Validation", "content": "To assess the sim-to-real gap between our generated images and the original nuScenes images, we employ UniAD [14] as an evaluator. We generate videos for 150 scenes based on the original layout provided by the nuScenes validation set with 2Hz. For comparative analysis, we set MagicDrive as the baseline method and use its official codes and checkpoints for inference. Subsequently, UniAD is performed on these images to compute various metrics, including 3d object detection, BEV map segmentation, and planning."}, {"title": "4.4. Visualization", "content": "Controllability. In this section, we will comprehensively demonstrate the controllability of the model from various dimensions, including the control of lighting and weather, the fit of object boxes and maps, change of street style, and consistency over long periods of time.\nWe demonstrate the impact of the reference image on the generated image, as shown in Figure 4. We randomly select one frame of images from the nuScenes dataset as reference images and choose three scenes from OpenStreetMap and Carla. We perform inference on them with World Dreamer respectively. It can be seen that the source and style of the road network are very different from the scope of the original nuScenes dataset. The pictures show that the generated vehicles and road networks conform closely to control conditions, demonstrating strong control capabilities. The style and weather of the generated pictures can also be consistent with the reference images. In other words, besides maintaining image generation continuity through reference images, we can also regulate image style accordingly.\nFigure 5 presents images generated using different text prompts and reference images on the same road network. Each set of images portrays the surrounding scenery at intervals of 8.5 seconds and 24 seconds respectively, with the layout projected on the image. The images clearly illustrate that the road structure and vehicles strictly adhere to the given control conditions while maintaining excellent consistency in the surround view. In addition, the four sets of images exhibit significant differences in weather and lighting and can maintain their own styles during the continuous iteration process.\nScalability. The Traffic Manager can accept any map downloaded from OpenStreetMap and seamlessly connect to the Carla road network. Combined with Dreamer's excellent following capability, the entire framework demonstrates robust scalability. The specific results are shown in Figure 6. We used both MagicDrive and our World Dreamer to generate realistic images on the same Carla road network, with the corresponding lane lines projected onto the images. The road style in Carla differs significantly from that of nuScenes. It is rare to encounter roads with such large curvature and such wide roads in nuScenes. Consequently, the performance of MagicDrive, which is based on the nuScenes BEV map, is slightly inferior in these conditions. As indicated by the yellow arrow, MagicDrive struggles to generate curved roads and fit wide roads accurately. DRIVEARENA, however, can produce reasonable pictures that follow the road structure."}, {"title": "4.5. Open-loop and Closed-loop Experiments", "content": "In this section, we adopt the prevailing end-to-end autonomous driving method UniAD [14] as the driving agent to test both the open-loop and closed-loop performance within the DRIVEARENA framework. We utilized UniAD's open-source code and pre-trained weights without additional training. UniAD operates at 2Hz, outputting a trajectory of 6 path points over the next 3 seconds. Traffic Manager further interpolates this to a 10Hz trajectory.\nOpen-loop Evaluation. We first assess UniAD's performance in DRIVEARENA's open-loop mode. UniAD is evaluated on three scenarios: 1) the original nuScenes image sequences; 2) World Dreamer-generated nuScenes image sequences, where the vehicles' trajectory remains identical to nuScenes ground truth, but surround images are replaced with World Dreamer-generated ones; and 3) DRIVEARENA's own simulation sequences (i.e., DRIVEARENA's open-loop mode). Our evaluation metrics consist of the PDM Scores and its sub-scores, as detailed in Section 3.4. Additionally, we evaluate trajectories driven by human drivers in nuScenes as the human driver performance. Detailed results are presented in Table 2.\nThe results reveal that while UniAD performs optimally on the original nuScenes sequence with a PDMS metric of 0.91, the World Dreamer-generated sequence surprisingly achieves a PDMS of 0.902, representing a metric drop of less than 1%. We attribute this to both the high fidelity of our World Dreamer-generated images and UniAD's strong dependence on ego states, as corroborated by [40].\nIn DRIVEARENA's open-loop mode, Figure 8 illustrates two sequences, demonstrating that UniAD's prediction of the road network and vehicle tracking are fundamentally accurate. However, in terms of metrics, UniAD's performance in such scenarios with unseen road and traffic flow is significantly degraded, with an average PDM Score of only 0.636.\nThe output trajectories by UniAD exhibit a substantial increase in collision rates and instances of driving outside the drivable area. The open-loop experimental results underscore the critical importance of closed-loop experiments and tests for autonomous driving methods.\nClosed-loop Evaluation. We further evaluated UniAD's performance in DRIVEARENA's closed-loop mode. In this mode, the trajectory outputted by UniAD is directly used for ego vehicle control, and the evaluation metrics include PDM Score (PDMS), Route Completion (RC), and Arena Drive Score (ADS). Our closed-loop experiment was conducted on four pre-set paths, with two paths selected in Boston and two in Singapore. The simulation time to complete each trajectory was approximately 120 seconds. Detailed results are presented in Table 3.\nThe results indicate that the PDMS of UniAD-generated trajectories in closed-loop mode (0.667) is comparable to that of the open-loop mode. However, the Route Completions (RC) are consistently low, averaging only about 13.7% of the total route length. Specifically, UniAD performs better on straightaways but largely fails to navigate the first turning intersection in the route. Figure 9 illustrates two failure cases where UniAD lacked sufficient trajectory correction. Despite a roughly correct prediction of the road structure, it ultimately mounted the central green belt or failed to complete a right turn successfully. The average Arena Driving Score for UniAD is 0.086. It should be noted that these are preliminary results based on testing only 4 routes. We plan to expand the number of routes for a more comprehensive evaluation and explore the combined effect of World Dreamer's timing consistency and the driver agent's performance on the final ADS."}, {"title": "5. Related Works", "content": ""}, {"title": "5.1. Data Acquisition for Autonomous driving", "content": "The characteristics of the automated driving dataset can be categorized into two aspects: appearance fidelity and interactivity. First, in terms of appearance fidelity, NGSIM [50] and CitySim [49] provide only realistic driving trajectories that can provide safe and reliable driving planning guidance. On top of that, some datasets developed based on the Carla simulator, such as DriveLM-CARLA [52] and BenchDrive [51], provide simulated sensor data. Taking it a step further, the Waymo [22] and nuScenes [20] datasets capture real-world sensor recordings and the driving behavior of human drivers. The datasets were produced in a complex process and with a limited amount of data. To add variety to the scenarios, MagicDrive [29] and DriveDreamer [53] provide editable scenario generation. So far, we have been able to obtain diverse and rich data for training. However, the above data can only be used for open-loop evaluation, i.e., current decisions do not affect future data distributions, which differs significantly from real driving. Works [30, 55-58] that also have fidelity differences, improve the interactivity of the data, they usually use auto-regressive generation methods to realize the interaction, the generation process implies the model's understanding of the world, and usually can not be too much human intervention. Some simulators [23-27, 32, 33] make things more controllable by decoupling part of the mechanics of how the world works. Common examples include simulators [23-25] that provide realistic traffic flow, and simulators [26, 27] that drive vehicles in game engines, and reconstructive simulations represented by [32, 33] that provide the appearance of reality."}, {"title": "5.2. Diffusion-based Generative Models", "content": "Recent advancements in generative models have seen diffusion models play a pivotal role in image and video generation [36, 59-64]. Moreover, recent works have expanded the scope by integrating additional control signals beyond traditional text prompts [65-67]. For instance, ControlNet [39] incorporates a trainable version of the SD encoder for control signals, while studies such as Uni-ControlNet [68] and UniControl [69] have emphasized the fusion of multi-modal inputs into a unified control condition using input-level adapter structures. Our approach aims to study the generation of continuous and controllable sequence frames, thereby bridging the gap between simulation environments and reality, and establishing the required foundation for closed-loop learning of autonomous driving agents."}, {"title": "5.3. Evolution of Autonomous Driving Generation", "content": "World Models [30, 70] utilize diffusion models to generate future driving scenes based on historical information, these methods often lack the ability to control the scenarios through layout, are difficult to generate continuous and stable videos and lack the approximation of physical laws. TrackDiffusion focused on generating videos based on 2D object layouts [71]. BEVGen [72] pioneered the generation of synthetic multi-view images based on the BEV layout, laying the foundation for a controllable generation of autonomous driving scenarios. BEVControl [73] extended this approach by a height elevation process, enabling image generation aligned with surrounding projection layouts. Further advancements includes MagicDrive [29], DriveDreamer [53], Panacea [74] and DrivingDiffusion [75], which generate panoramic controllable videos through various 3D controls and encoding strategies. However, their primary focus lies in augmenting training data to enhance algorithm performance, rather than serving as simulators for modeling dynamic environmental interactions."}, {"title": "5.4. Simulator-Driven Scenario Generation", "content": "Autonomous vehicle development is significantly enhanced by driving simulators, which provide controlled environments for realistic simulation. Prominent research efforts have concentrated on generating virtual imagery and annotations, with some studies expanding to incorporate environmental variations and construct safety-critical scenarios for training based on real-world data logs. Nevertheless, these simulated images frequently fall short of achieving true realism, as evidenced by previous works [76-78]. While SimGen [54] made a breakthrough as the first work to generate diverse driving scenarios following conditions from a simulated environment, it mainly focused on the quality of the generated content with only front-view images, neglecting the exploration of closed-loop systems. Our research aims to bridge this gap by developing a system that can not only generate realistic scenarios but also allow agents to interact with them in a closed-loop manner."}, {"title": "5.5. Closed-Loop Driving in Simulation", "content": "End-to-end vehicle control algorithms [14, 15, 43], are typically trained and evaluated on open-loop datasets [20]. However, these algorithms lack the capability to generalize directly to simulators for closed-loop evaluation, which hinders the demonstration of their true performance potential. Recent studies have increasingly recognized the significance of closed-loop evaluation, as exemplified by [16,42]. Moreover, simulation environments offer a wealth of training data, a stark contrast to models trained on datasets that are constrained by data distribution [40]. A significant challenge arises due to the discrepancy between the simulated scene's appearance and real-world conditions, complicating the generalization of models trained on simulation data to actual scenarios. This creates a paradox: the desire to utilize simulation data for its diversity and editability, while also seeking data that closely mirrors reality. Our approach effectively addresses this issue by enhancing the realism of the simulator for certain closed-loop learning methods [79]."}, {"title": "6. Conclusions and Future Works", "content": "This paper introduces a novel closed-loop simulation platform named DRIVEARENA for vision-based driving agents. DRIVEARENA integrates a Traffic Manager that generates human-like traffic flow and a high-fidelity generative World Dreamer with infinite generation. This combination allows realistic interaction and continuous feedback between the driving agent and the simulation environment. The system provides a valuable platform for developing and testing autonomous driving agents in a variety of scenarios", "improvement": "n1) Data Diversity: The current generative model is trained solely on the nuScenes dataset", "Consistency": "While we can generate continuous videos with an autoregression strategy", "80": "to address these issues.\n3) Runtime Efficiency: Like many generative models", "81": "and model quantization may alleviate these problems.\n4) Expanded Agent Testing: We plan to incorporate a broader range of driving agents within DRIVEARENA", "17": ".", "Arena": "DRIVEARENA can not only evaluate the performance of different driving agents, but also act as a testing ground for AD generative models. By using the same driving agent as a referee, it can fairly assess the sim-to-real gap of different generative models. This approach even provides a more credible and"}]}