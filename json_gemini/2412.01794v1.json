{"title": "IQA-Adapter: Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generative Models", "authors": ["Abud Khaled", "Sergey Lavrushkin", "Alexey Kirillov", "Dmitriy Vatolin"], "abstract": "Diffusion-based models have recently transformed conditional image generation, achieving unprecedented fidelity in generating photorealistic and semantically accurate images. However, consistently generating high-quality images remains challenging, partly due to the lack of mechanisms for conditioning outputs on perceptual quality. In this work, we propose methods to integrate image quality assessment (IQA) models into diffusion-based generators, enabling quality-aware image generation.\nFirst, we experiment with gradient-based guidance to optimize image quality directly and show this approach has limited generalizability. To address this, we introduce IQA-Adapter, a novel architecture that conditions generation on target quality levels by learning the relationship between images and quality scores. When conditioned on high target quality, IQA-Adapter shifts the distribution of generated images towards a higher-quality subdomain. This approach achieves up to a 10% improvement across multiple objective metrics, as confirmed by a subjective study, while preserving generative diversity and content. Additionally, IQA-Adapter can be used inversely as a degradation model, generating progressively more distorted images when conditioned on lower quality scores. Our quality-aware methods also provide insights into the adversarial robustness of IQA models, underscoring the potential of quality conditioning in generative modeling and the importance of robust IQA methods.", "sections": [{"title": "1. Introduction", "content": "Recent advances in diffusion-based image generation have led to a new wave of powerful text-to-image models, capable of producing realistic and highly detailed images from text prompts. Models such as DALL-E 3 [6], FLUX [43], and SDXL [58] exemplify this success, establishing new standards in visual fidelity and alignment with user intent. Beyond text prompts, recent studies have extended the conditioning flexibility of generative models by incorporating additional guidance sources such as depth [91], pose [91], and images [87], expanding the expressive capacity of these models and enabling greater control over generation outputs. This development has also sparked interest in unified approaches, with recent work like OmniGen [83] proposing universal models that integrate diverse conditioning types natively.\nDespite these advances, one area that remains largely unexplored in the context of conditional generative modeling is image quality and aesthetics assessment (IQA/IAA). IQA, a specialized field within computer vision, focuses on evaluating image quality in ways that align closely with human perception. While closely related, IAA emphasizes aesthetic attributes that are often dependent on the content of the image. In this work, we aim to bridge the gap between generative modeling and IQA/IAA by incorporating IQA-driven conditioning, enabling improvements in generated image quality while preserving content fidelity.\nAligning generative models with human preferences in terms of image quality has strong practical motivation. While current evaluations of generative models primarily focus on image-text alignment or distributional closeness to real data, image quality has received relatively little attention. Although some recent approaches [41, 82, 85] have started to address the alignment of generated content with both textual and aesthetic human preferences, very few attempts, if any, have directly incorporated existing IQA/IAA expertise into generative models. This lack of integration persists despite expanding research [15, 21, 45, 81] that leverages generative priors from diffusion-based models to advance IQA methods, indicating promising potential for knowledge transfer between the fields."}, {"title": "2. Related work", "content": "Generative Models. Various approaches have been explored in the field of image generation, including generative adversarial networks (GANs) and variational autoencoders (VAEs), which dominated early progress in the area. Recently, however, diffusion models have emerged as the state-of-the-art approach for image generation. Pioneering works such as [61, 62, 64] established diffusion models as superior to previous methods, introducing the first production-grade models capable of generating high-quality images. Following these advancements, further research focused on improving aspects such as image relevance, aesthetic appeal, and overall quality. Studies [5, 6, 12, 14, 19, 38, 43, 49] contributed to these improvements through enhancements in data quality, model scaling, architectural refinement, and the exploration of alternative diffusion frameworks.\nAdapters and customization. Advances in diffusion models have led to the development of new techniques that enhance control over the image generation process. LoRA [34] introduced a parameter-efficient fine-tuning method that updates only a low-rank decomposition of model weights, enabling easy model customization and stylization. Methods like Dreambooth [63] and Textual Inversion [22] further enabled personalization by allowing diffusion models to generate outputs tailored to specific user inputs. IP-Adapter [87] extended these works for universal image variation. Additionally, ControlNet [91] and T2IAdapter [55] incorporated spatial conditioning into diffusion models by using supplementary networks to encode specific conditions. Furthermore, ConceptSliders [23] introduced an approach for training adapters that can adjust particular attributes within generated images. Most of these techniques, however, focus on conditioning image generation through"}, {"title": "3. Learning the relationship between images and visual quality from IQA models", "content": "To establish a simple baseline for integrating IQA model knowledge into the generation process, we introduce a technique inspired by classifier guidance [17]. In our adaptation, we leverage NR-IQA models rather than a classifier, interpreting IQA scores as soft probabilities that reflect the likelihood of an image achieving high perceptual quality. This approach uses feedback from the IQA model to iteratively optimize image quality during the generation process:\n$\\epsilon_{\\theta}(z_{t-1}|c_t, f_{\\theta}) = \\epsilon_{\\theta}(z_t|c_t) + \\alpha \\cdot \\omega(t) \\nabla_{z_t} \\log f_{\\theta}(D(z_t)),$\nwhere $\\epsilon$ is a latent diffusion model, $c_t$ is a textual condition, $f_{\\theta}$ is a NR metric, $z_t$ represents the latent image at the t-th diffusion step, and $D(\\cdot)$ is the VAE's decoder that maps the latent representation back to image space. The parameter $\\alpha$ allows adjustment of the IQA guidance weight, balancing the impact of quality conditioning, while the scaling coefficient $\\omega(t)$ modulates the gradient's influence over time, linearly increasing from 0 to 1.\nAlthough this method optimizes the target IQA score, its reliance on gradient-based adjustments introduces the risk of exploiting vulnerabilities within the IQA model. This can result in images that receive high ratings from the IQA model yet exhibit noticeable visual distortions a phenomenon similar to adversarial attacks, which we further discuss in Section 5.2.\nAnother naive approach involves fine-tuning the generative model on data of the target quality. This method, however, lacks flexibility for controlling the generative process during inference and can affect the generative capabilities of the original model."}, {"title": "3.2. IQA-Adapter", "content": "To address limitations of inference-time gradient optimization, we propose a method that implicitly learns a relationship between images and their corresponding quality assessments. By learning this connection, the generative model can internalize features associated with target-quality images and avoid characteristics linked to opposite quality. For instance, when conditioned on high-quality parameters, the model should generate images with fine-grained details and vibrant colors. Conversely, when conditioned on low quality, it should produce artifacts such as JPEG compression distortions or blurring."}, {"title": "3.2.1. Architecture", "content": "To condition the generative model on image quality, we leverage IP-Adapter [87] technique, which supports image-prompt conditioning in pretrained text-to-image diffusion models. The IP-Adapter operates by projecting the CLIP embedding of an image prompt into additional tokens, which are then integrated into the model via cross-attention mechanisms. This method enables the base model to receive detailed conditioning information from non-textual sources without altering its core weights. We selected this architecture for its lightweight design, ability to preserve core model's weights, and minimal computational overhead during training and inference.\nAs illustrated in Figure 2, we repurpose the IP-Adapter framework to pass visual quality scores as conditioning information into the generative model. We refer to this modified approach as IQA-Adapter. In this setup, quality scores"}, {"title": "3.2.2. IQA-Adapter Training", "content": "We train IQA-Adapter on triplets (image, text, input quality scores) where the image-text pairs are drawn from a text-to-image dataset, and the quality scores are estimated by passing each image through a target IQA/IAA model. The training follows the standard denoising diffusion probabilistic model (DDPM) procedure [32]. In this process, a random timestep $t \\sim U[0, 1]$ is sampled, and noise is incrementally applied to the image x at the corresponding noise scale. The model then learns to predict the added noise with the following objective:\n$L = E_{x,t,\\epsilon} [||\\epsilon - \\epsilon_{\\theta}(x_t|c_t, c_q) ||^2]$\nwhere $x_t$ is a noised representation of the input image, $c_t$ is the textual condition, $c_q$ is the qualitative condition, $\\epsilon$ is the added noise, and $\\epsilon_{\\theta}(x_t|c_t, c_q)$ is the predicted noise.\nDuring this process, only the adapter weights are adjusted to allow the generative model to incorporate quality score information and steer the output generation accordingly. To maintain flexibility for classifier-free guidance during inference, we randomly drop the textual and quality conditions with a small probability, which encourages the model to generate images unconditionally.\nA key advantage of IQA-Adapter is that it does not require backpropagation through the IQA models (as it only uses quality scores of training images), enabling the use of non-differentiable metrics or even ground-truth subjective scores from sufficiently large subjective studies. As demonstrated in Section 4.2, bypassing gradient-based optimization significantly improves the transferability across metrics beyond those used for training, enhancing the generality of the learned quality features across various evaluation models."}, {"title": "4. Experiments and Evaluation", "content": "Models. For all experiments involving both gradient-based guidance and IQA-Adapter, we used SDXL as the base model. The IQA-Adapters were trained on the CC3M [69] dataset (~3 million images) for 24,000 steps with a learning rate of 1 \u00d7 10-4, followed by fine-tuning on a subset [16] of the LAION-5B [68] dataset (~170k images with an aesthetics score > 6.5) for an additional 3,000 steps with a reduced learning rate of 1 \u00d7 10-5. We employed two tokens for qualitative features. For more details on the IQA-Adapter training, refer to the supplementary materials Section 9.\nDuring inference, we used a guidance scale of 7.5 and 35 sampling steps. For the IQA-Adapters, we set the adapter"}, {"title": "4.1. Experimental Setup", "content": "scale to $\\lambda = 0.5$, while for the gradient-based method, we applied a quality-guidance scale of $\\alpha = 30$.\nOur experiments were conducted on a cluster equipped with Nvidia Ampere series GPUs. Training a single IQA-Adapter model required approximately 260 Nvidia A100 80GB GPU hours.\nIQA/IAA Models. We experimented with a diverse set of 22 state-of-the-art quality assessment models, varying in architecture and training dataset. The models include CNN-based approaches like ARNIQA [1], NIMA [77], DBCNN [93], and CNNIQA [37]; TOPIQ [10], which combines a CNN backbone with an attention mechanism; HYPER-IQA [74], which leverages a hyper-network with a CNN. Additionally, we tested transformer-based models, including MUSIQ [39], TRES [27], and MANIQA [86] and metrics integrating vision-language capabilities like LIQE[95], LIQE-MIX[95] and CLIP-IQA+[78]. Where available, multiple versions of some models were tested, each trained on different datasets. Table 2 in the supplementary materials lists all employed metrics with their corresponding training datasets.\nEvaluation Datasets. We use several diverse prompt datasets for model evaluation:\nQualitative evaluation: A filtered subset of 8,200 user-generated prompts from Lexica.art website [66] and PartiPrompts [89] (1,600 prompts of different aspects and challenges).\nGenerative and compositional capabilities evaluation:"}, {"title": "4.2. High-quality conditioning", "content": "To evaluate the effectiveness of knowledge transfer from IQA models to diffusion-based generative models, we first explore the high-quality conditioning scenario, as this is the primary application for quality-aware generation. To assess improvements objectively, we calculate the relative gain in quality scores compared to the base model:\n$RelGain = \\frac{1}{N} \\sum_{i=0}^{N} \\frac{f(x_i) - f(x'_i)}{f(x'_i)} \\cdot 100\\%$,\nwhere $f(x)$ represents the quality assessment model, and $x'_i$ and $x_i$ are images generated under the same prompt and seed for the base and quality-conditioned models, respectively.\nDetailed results for PartiPrompts dataset and the gradient-based method are provided in the supplementary materials (see Sections 10.1, 10.2). The gradient-based method, which directly optimizes IQA scores, increases target scores but fails to improve other IQA/IAA metrics, likely due to adversarial exploitation of model-specific vulnerabilities. Given its limitations, we focus on IQA-Adapter in the remaining experiments, discussing the gradient-based method as a potential white-box adversarial attack in Section 5.2.\nFor IQA-Adapter, high-quality conditioning is achieved by setting the input to the 99-th percentile of the target metric's values from the training dataset. Separate IQA-Adapters were trained for each IQA/IAA metric, and a multi-metric approach was tested by conditioning on combinations of different IQA/IAA models. Figure 3(a) shows the relative gains for various IQA-Adapters on user-generated prompts from the Lexica.art dataset.\nUnlike the gradient-based approach, the IQA-Adapters trained even on single IQA models show consistent quality gains across multiple metrics, with an average improvement of 4-6% over the base model. Notably, gains for the target metric do not significantly exceed those for other metrics, demonstrating strong cross-metric transferability.\nIQA scores tend to improve more easily than IAA scores, likely because IQA focuses on perceptual quality attributes that are less dependent on composition, whereas IAA is more content-sensitive and requires adjustments in both text and quality conditions.\nUsing multiple IQA/IAA metrics enhances the IQA-Adapter's performance across evaluation metrics. For ex-"}, {"title": "4.3. Alignment with qualitative condition", "content": "To assess the alignment between input quality conditions provided to the IQA-Adapter during generation and the quality of generated images, we attempt to condition it on different percentiles of the target IQA model's values on the training dataset. Figure 4 demonstrates the impact of quality-condition on IQA scores and examples of images generated for corresponding quality levels. The results indicate a gradual increase in quality scores from the IQA model as the input condition rises, with generated images appearing progressively sharper and more detailed. We exemplify more quality conditions for the IQA-Adapters trained with different IQA/IAA models in supplementary Section 18."}, {"title": "4.3.1. Subjective Study", "content": "To confirm that image quality improves with input quality conditions, we conducted a subjective study with the IQA-Adapter conditioned on three quality levels: low (1st percentile), medium (50th percentile), and high (99th percentile), as well as the base model (SDXL-Base). We utilized the IQA-Adapter conditioned on TOPIQ and LAION-AES models, which showed the highest average IQA/IAA metric increases (Figure 3(a)). Participants evaluated the visual quality of images generated from 300 prompts, contributing over 22,300 responses from 1,017 users, with each image pair evaluated by at least 10 unique users (12.1 on average). For each model, we calculated the overall win rate defined as a share of image pairs on which it achieved the majority of votes. Additionally, we demonstrate the average percent of votes for the model across all image-pairs. Results are shown in Figure 3(b), and pairwise win rates in Figure 3(c). For more details on the subjective study, see supplementary Section 15.\nWin rates align well with input quality conditions: high-quality conditions achieve the highest win rate, followed by medium- and low-quality. As shown in Figure 3(c), the IQA-Adapter conditioned on high quality outperforms the base model with 60% win rate, compared to 32% for the base model (~7% were rated equally). This demonstrates that IQA-Adapter effectively captures and reproduces qualitative concepts aligned with human image quality judgments. Notably, the win rate for the low-quality condition drops significantly compared to medium quality. Figure 4(a) further indicates that objective quality decreases sharply below the 25th percentile."}, {"title": "4.4. Evaluating generative capabilities", "content": "To evaluate the generative capabilities of the quality-conditioned model and ensure that it doesn't affect the ability to follow the textual prompt and generate diverse images, we tested it on the GenEval benchmark. It uses an object-detection model to evaluate the alignment between generated images and textual conditions. Table 1 shows the comparison results. Overall scores for most adapters are close to those of the base model. For each evaluation criterion, there is an IQA-Adapter that consistently outperforms the base model. The IQA-Adapter trained with HYPER-IQA, for example, increases \u201cAttribute binding\u201d (rendering two objects with two different colors) and \u201cPosition\" (rendering two objects with specific relative positions) scores, suggesting better alignment with complex compositional prompts. The least improvement is in \u201cCounting,\u201d likely due to some IQA-Adapters' tendency to add small details that sometimes increase object counts unnecessarily.\nAdditionally, we calculated FID, IS [65] and CLIP [30] scores for all tested adapters on a 10,000 captions subset of MS COCO. The results can be found in supplementary Section 11. In summary, these findings indicate that the adapter conditioned on high quality mostly retains the generative capabilities of the base model, while shifting the generation towards a higher-quality subdomain."}, {"title": "5. Discussion and Future Work", "content": "As most IQA models are trained to assess distorted images, they can reliably detect noise, compression, blur, and other artifacts on images during IQA-Adapter training. Therefore, this knowledge is transferred to the generative model and such image attributes are connected with low-quality conditions. This allows IQA-Adapter to generate progressively more distorted images as input quality-condition decreases. The IQA-Adapter in Figure 4(b), for example, implicitly learned to simulate JPEG compression artifacts when conditioned on low quality (1st percentile of the training dataset). As IQA models are mostly tailored to assess low-level quality attributes (in contrast with IAA methods), images produced with different quality levels usually retain similar content and composition, as illustrated in Figure 1 (bottom-to-top direction).\nBy applying appropriate filtering to exclude image pairs with unintended content differences, IQA-Adapter can generate large synthetic datasets of distorted and corresponding high-quality images. Such datasets can subsequently be used to pretrain models for image enhancement, deblurring, and other restoration tasks. While training such methods is a subject for future work, we additionally explore the distances between generated images with different target-quality conditions in supplementary Section 13.2.\nWe also note that IQA-Adapter can be additionally fine-tuned with unpaired data containing specific distortions to simulate them during inference."}, {"title": "5.1. IQA-Adapter as a degradation model", "content": ""}, {"title": "5.2. Exploring adversarial patterns, biases, and preferences of IQA models", "content": "When applied with a sufficiently high guidance scale, the gradient-based method can exploit vulnerabilities of the target IQA model, artificially inflating its values and shifting the generation towards an adversarial subdomain. This approach tends to produce images with distinct patterns specific to each IQA model. Figure 5(a) demonstrates adversarial patterns generated with different guidance models. For certain models, such as TRES and HYPER-IQA, these patterns form grid-like structures, and for others, like TOPIQ and DBCNN, they concentrate in smaller regions. We present more adversarial examples generated with gradient-based guidance and GradCAM [26] visualizations of corresponding IQA models in supplementary.\nOur study further reveals that most IQA models exhibit distinct preferences when used with a high IQA-Adapter scale. For instance, TOPIQ often favors sharper images, while LAION-AES tends to enhance color saturation, producing more vibrant visuals. These effects can be compounded by using multiple IQA/IAA models simultaneously during adapter training, as illustrated in Figure 5(b). Furthermore, in supplementary Section 8 we explore the use of negative-quality guidance, similar to negative text-prompt guidance. It magnifies the impact of the adapter by subtracting the noise prediction associated with the opposite quality condition during generation. When provided with a strong negative-quality condition, the adapter attempts to remove certain fine-grained details, often producing \"overly-stylized\" image with sharp outlines around the edges (see supplementary Figure 7). Such images tend to achieve very high quality scores across multiple metrics, but this rarely corresponds to a real quality increase. This observation suggests that, under such conditions, IQA-Adapter exploits the preferences of IQA models to artificially boost quality scores. This underscores the potential of IQA-Adapter as a black-box tool for investigating the adversarial robustness and biases of IQA models in future research.\nWe further discuss the limitations of IQA-Adapter in supplementary Section 14."}, {"title": "6. Conclusion", "content": "In this work, we explored different techniques to transfer knowledge from image quality assessment models to diffusion-based image generators. We proposed a novel IQA-Adapter architecture that allows the generator model to learn implicit connections between images and corresponding quality levels and enables quality-aware generation. Experiments and subjective evaluation showed that IQA-Adapter efficiently conditions the generation process in a way that aligns with human judgment, all while retaining the generative capabilities of the base model. Additionally, we demonstrate various use cases of IQA-conditioned generation, including the exploration of adversarial robustness, preferences, and biases of IQA/IAA models and image degradation."}]}