{"title": "PROVABLE UNLEARNING IN TOPIC MODELING AND\nDOWNSTREAM TASKS", "authors": ["Stanley Wei", "Sadhika Malladi", "Sanjeev Arora", "Amartya Sanyal"], "abstract": "Machine unlearning algorithms are increasingly important as legal concerns arise\naround the provenance of training data, but verifying the success of unlearning is\noften difficult. Provable guarantees for unlearning are often limited to supervised\nlearning settings. In this paper, we provide the first theoretical guarantees for un-\nlearning in the pre-training and fine-tuning paradigm by studying topic models,\nsimple bag-of-words language models that can be adapted to solve downstream\ntasks like retrieval and classification. First, we design a provably effective unlearn-\ning algorithm for topic models that incurs a computational overhead independent\nof the size of the original dataset. Our analysis additionally quantifies the dele-\ntion capacity of the model \u2013 i.e., the number of examples that can be unlearned\nwithout incurring a significant cost in model performance. Finally, we formally\nextend our analyses to account for adaptation to a given downstream task. In par-\nticular, we design an efficient algorithm to perform unlearning after fine-tuning\nthe topic model via a linear head. Notably, we show that it is easier to unlearn\npre-training data from models that have been fine-tuned to a particular task, and\none can unlearn this data without modifying the base model.", "sections": [{"title": "INTRODUCTION", "content": "Modern-day machine learning has shifted from single-stage supervised learning on manually\nconstructed datasets to a paradigm in which models are pre-trained and subsequently fine-\ntuned (Bommasani et al., 2022). In this setting, a model initially learns a good representation of\nthe data using a self-supervised objective on a large unstructured corpus. The resulting pre-trained\nmodel is later adapted to solve specific tasks for which it is difficult or costly to curate a large\ndataset. This blueprint has yielded strong performance in text (e.g., Devlin et al., 2019; Brown et al.,\n2020), vision (e.g., Oquab et al., 2024; He et al., 2022), and multimodal (e.g., Radford et al., 2021;\nZhai et al., 2023) settings. It is well-known that the scale of the pre-training data is strongly corre-\nlated with the final performance of the model (Hoffmann et al., 2022), leading to the construction of\nlarger datasets via broad internet scrapes (Gao et al., 2020; Schuhmann et al., 2022; Soldaini et al.,\n2024; Penedo et al., 2023). Such datasets have been found to often inadvertently include private,\nsensitive, and unsafe data (Birhane et al., 2021; Longpre et al., 2024; He et al., 2024).\nUnsafe data can generally degrade model performance and introduce biases, making the model\nless useful for various applications (McKenna et al., 2023; Birhane & Prabhu, 2021; Choenni et al.,\n2021; Naous et al., 2024). Using private and sensitive data, even unknowingly, poses legal\nrisks (Bommasani et al., 2022; Henderson et al., 2023). In particular, recent works have shown\nthat models can memorize and thus permit the extraction of training data (Somepalli et al., 2023;\nCarlini et al., 2021; 2023). Moreover, one may be requested to remove data in accordance\nwith GDPR's right to be forgotten (European Parliament & Council of the European Union), or\nas part of a copyright-related lawsuit (Tremblay v. OpenAI, Inc.,, 2023; DOE 1 v. GitHub, Inc.,\nN.D. Cal. 2022).\nTherefore, there is great empirical interest in developing machine unlearning algorithms that can\nsurgically remove portions of the training data from an already learned model without harming per-\nformance. The gold standard for machine unlearning is for the model to behave as though it had\nnever been trained on that datapoint (Cao & Yang, 2015). As it is often undesirable to completely"}, {"title": "1.1 OVERVIEW OF RESULTS", "content": "We focus on the setting in Arora et al. (2012b), because it admits an efficient learning algorithm with\nprovable guarantees (Arora et al., 2012a). The corpus is assumed to contain r underlying topics,\nwhere each topic defines a distribution over words. Let D be a distribution over topic distributions.\nThen, each document d is generated by sampling a topic distribution Wa ~ D over topics, and then\nsampling words according to Wa.\nThe dataset of m documents is a matrix $M\\in R^{n\\times m}$, where M permits a non-negative matrix\nfactorization $M = A^*X$. Here, $A^* \\in R^{n\\times r}$ is the distribution of words in each of the r unknown\nunderlying topics, and $X \\in R^{r\\times m}$ is the sampled distribution of topics in each document. In\nparticular, A*, X have columns on the probability simplex. We seek to learn the embedding function\nA* and the topic-topic covariance $R^* = E_D[XX]$.\nTo derive provable guarantees on the success of unlearning, we adapt the notion of $({\\epsilon}, {\\delta})$-unlearning\nintroduced in Sekhari et al. (2021) to the topic modeling setting. The unlearned model is required\nto behave indistinguishably from a model that was retrained on the modified dataset. We define a\nnotion of utility-preserving unlearning that combines this condition with an analysis on the deletion\ncapacity - i.e., the number of datapoints that can be unlearned without performance degradation\n(Definition 4). We now state our main result on utility-preserving unlearning in topic models.\nMain Result 1 (Informal version of Theorem 2). Suppose we trained a topic model AS, XS on a\ntraining set S containing m documents. Algorithm 1 can perform utility-preserving unlearning of\n$\\frac{m}{\\tilde{O}(r^2 \\sqrt{nr})}$\ndocuments from the pre-trained topic model, where $\\tilde{O}(\\cdot)$ hides constants depending on the learning\nand unlearning algorithm.\nTo adapt a topic model to a downstream topic classification task, we learn a head w \u2208 R on top\nof A to minimize a strongly convex loss function (Definition 2). When A and w are both released,\none would necessarily have to first unlearn from A, which makes unlearning just as hard as it was\nin pre-training (Theorem 3). This setting is rather unrealistic, because there is no obvious case in\nwhich one would want to use w without A or vice versa. We thus advocate for viewing fine-tuned\nmodel B = Aw as a whole i.e. it is not allowed to access outputs of A solely, and we show that it\nis easier to perform utility-preserving unlearning of pre-training data in this case."}, {"title": "2 TOPIC MODELS", "content": "As we previously discussed, topic models can be considered as one of the simplest language models\nthat one can pre-train in a self-supervised fashion and later fine-tune for other language-related\ntasks. This pipeline mirrors the modern-day paradigm of pre-training large language models to\nbuild a general understanding of natural language and later fine-tuning them to solve a variety of\ntasks ranging from classification to code generation."}, {"title": "2.1 PROBLEM DESCRIPTION", "content": "Topic modeling is a classical, bag-of-words method to discover structure in a corpus of docu-\nments (Hofmann et al., 1999). One assumes that each document contains a convex combination\nof topics, each of which can be described in terms of a distribution over the vocabulary. Different\nassumptions on the structure of this distribution and the topics have yielded a variety of topic mod-\neling methodologies (Blei & Lafferty, 2006; Li & McCallum, 2006) \u2013 perhaps most famous among\nthese is the latent Dirichlet allocation (LDA, Blei et al. (2003)). Many early works established the\nstatistical learnability of topic models under such assumptions, but the learning algorithms generally\nwere not efficient in real-world settings (Arora et al., 2012b; Recht et al., 2012).\nOur paper focuses on the setting in Arora et al. (2012b), for which Arora et al. (2012a) provided an\nempirically efficient learning algorithm. The dataset consists of a set of m documents d\u2081, ..., dm,\nwhere each document contains L words from a vocabulary V with |V| = n.\u00b9 The corpus contains r\ndifferent underlying topics, each of which defines a distribution over words. Each word in document\nd is generated by: (1) sampling a distribution over topics Wa ~ D, and then (2) sampling L words\nindependently according to Wa.\nWe represent the corpus as a matrix $M \\in R^{n\\times m}$, where M permits a non-negative matrix factoriza-\ntion $M = A^*X$. Here, $A^* \\in R^{n\\times r}$ is the distribution of words in each of the r topics, $X \\in R^{r\\times m}$\nis the distribution of topics in each document, and hence M is the distribution of words in each\ndocument. While there are several algorithms for learning the feature extractor A*, it is well-known\nthat it is hard to recover X exactly (Arora et al., 2012b). Instead, it is desirable to learn how the\ntopics co-occur together, denoted as $R^* = E_D[XX]$. This quantity is termed the topic-topic\ncovariance. Further discussion of this has been included in Appendix A.\nThe topic modeling setting generally determines D (e.g., in LDA, D is a Dirichlet distribution). In\norder to recover A* and R* efficiently and accurately from an observed corpus M ~ D, we need\nto make the following assumption on the underlying data distribution.\nAssumption 1 (p-separability, Arora et al. (2012b)). The topic matrix A* is p-separable for p > 0\nif for every topic $k \\in [r]$, there exists a word $i \\in [n]$ such that $A_{i,k}^* \\ge p$ and $A_{i,k'}^* = 0$ for all\nk' \u2260 k. Such words are called anchor words.\nWithout this separability assumption, maximum likelihood estimation of a topic model is NP-\nhard (Arora et al., 2012b). Assumption 1 requires that A* contains a diagonal matrix, up to row\npermutations; intuitively, the appearance of an anchor word in a document perfectly indicates the\n1- Without loss of generality, we assume L = 2."}, {"title": "2.2 DOWNSTREAM ADAPTATION", "content": "Topic models are frequently trained on a general corpus, and the embeddings can be later used\nto classify documents. The classification problem usually involves only a subset of topics. For\nexample, after training a topic model on a large corpus of news articles with diverse topics (e.g.,\nsports, politics, technology, finance, etc.), one relevant downstream task is to classify the subject of\na given news article as sports or politics. We formalize the topic classification task below.\nDefinition 1 (Topic Classification Task). A topic classification task T = (Tclf, w*) is defined by a\nsubset of topics Tclf C [r] on which the task is defined and a ground-truth labelling vector $w^* \\in R^n$\nwith bounded norm. Importantly, w* only has non-zero coordinates in the positions corresponding\nto Telf.\nThe classification task is defined on the latent features of a given document, so it is necessary to first\nidentify the salient topics as they occur in the text. Fitting a topic model to the corpus yields such a\nfeature extractor A that embeds a document into the r-dimensional topic space. In order to adapt a\ntopic model to a particular classification task, we perform head tuning on the feature extractor A.\nDefinition 2 (Head Tuning). For a given labelled document classification dataset Dclf = {(di, Yi)}\nrepresenting a topic classification task T, embed each document di as a vector $x_i \\in R^n$ containing\nthe word counts in the document. To perform head tuning on a pre-trained topic model A, we learn\nw\u2208 R to minimize\n$l(w;A) = \\frac{1}{D_{clf}} \\sum_{(x,y) \\in D_{clf}} f(x^TAw, y)$\nwhere lt is strongly convex in w.\nOne example of f is the logistic loss with l2 regularization. For ease of exposition, we primarily\nconsider binary classification tasks, but we point out that the definition can extend to multi-class\ntasks solved via the one-vs-all scheme (Rifkin & Klautau, 2004).\nWe note that head tuning, also referred to as linear probing, is a simpler adaptation technique than\nfine-tuning A alongside w. Nonetheless, recent works on popular language models have demon-\nstrated that head tuning can substantially improve the ability of general pre-trained language models\nto solve complex classification tasks (Malladi et al., 2023a;b). Head tuning thus serves as a con-\nvenient yet effective adaptation method that avoids updating the pre-trained model, which is often\ndesirable. For example, if a single pre-trained model needs to be separately adapted to solve many\ndifferent tasks, then it is desirable to minimize the number of parameters that are fine-tuned to min-\nimize the memory needed to store all of the adapted models.2"}, {"title": "3 UNLEARNING", "content": "As mentioned previously, there is increased interest in machine unlearning due to the growing scale\nof modern datasets and the difficulty of manually inspecting each datapoint. Theoretically, the\ngold standard for unlearning is that the model should behave identically to one that was trained\nwithout the datapoint in its corpus (Cao & Yang, 2015). We first define what it means for two models\n$\\theta_1, \\theta_2 \\in {\\Theta}$ to behave almost identically, where $\\Theta$ denotes the parameter space of a hypothesis class.\nDue to randomness in learning, $\\theta_1$ and $\\theta_2$ are random variables.\nDefinition 3 ((${\\epsilon}, {\\delta})$-indistinguishable models, Dwork et al. (2014)). Two models denoted by random\nvariables $\\theta_1, \\theta_2 \\in \\Theta$ are ($\\epsilon$, ${\\delta}$)-indistinguishable if for all possible subsets of models $T \\subset \\Theta$,\n$Pr(\\theta_1 \\in T) \\le e^\\epsilon Pr(\\theta_2 \\in T) + {\\delta}$\n$Pr(\\theta_2 \\in T) \\le e^\\epsilon Pr(\\theta_1 \\in T) + {\\delta}$\n${\\epsilon},{\\delta}$\nWe denote this as $\\theta_1 \\sim \\theta_2$.\n2This motivation has driven widespread development and adoption of parameter-efficient fine-tuning meth-\nods for large language models. Liu et al. (2021) contains a survey of such techniques."}, {"title": "4 LEARNING AND UNLEARNING TOPIC MODELS", "content": "In this section, we present the learning and unlearning algorithms and guarantees for topic models.\nNotation. We use A* to refer to the ground-truth topic model, As to refer to a topic model trained\non S, and AF to denote a topic model retrained with the forget set removed S \\ Sf. We also use \u0100\nto denote the unlearned topic model before applying the Gaussian mechanism and A to denote the\nmodel after the mechanism is applied. Analogous notations are used for R."}, {"title": "4.1 LEARNING ALGORITHM AND GUARANTEES", "content": "Per Arora et al. (2012a), the learning algorithm Abase takes in a corpus of documents S =\n{d1, ..., dm} and consists of the following three phases to learn a topic model $\\Theta$ = (AS, RS)."}, {"title": "4.2 UNLEARNING ALGORITHM AND GUARANTEES", "content": "We describe our unlearning algorithm Ubase to forget a set Sf from a trained model (Algorithm 1),\nwhich crucially updates Ci with a Newton step. We then compute A from the modified Ci and\napply the Gaussian mechanism to ensure indistinguishability. We describe our formal guarantee on\nthe unlearning algorithm below, sketching out our utility preserving guarantees with respect to A*.\nThe arguments for R* follow analogously; we defer the discussion to the appendix.\nTheorem 2 (Utility-Preserving Unlearning on the Base Model). Let Abase be the learning algo-\nrithm described in the prior sections and Ubase be the unlearning algorithm in Algorithm 1. Then,\n(Abase, Ubase) performs utility-preserving unlearning with deletion capacity\n$T_{Abase,Ubase}^{\\epsilon,\\delta}(m) \\ge c \\cdot \\frac{m}{r^2 \\sqrt{nr}}$\n(4)"}, {"title": "5 UNLEARNING WITH RESPECT TO A DOWNSTREAM TASK", "content": "We are interested in unlearning a set of pre-training documents Sf \u2286 S. A topic classification task\nis usually defined on a subset of the topics in the dataset - for example, if the pre-training corpus\ncontained diverse news articles, one plausible downstream task is to classify the content of a given\ndocument as containing politics or sports. Definition 1 formalizes this: a topic classification task\nT = (Tclf, w*) is defined on a subset of the topics Telf and a r-length ground-truth labelling vector\nw* \u2208 Whead, where w* only has non-zero values in positions corresponding to Telf. We describe\ntwo possible settings under which we can show utility-preserving unlearning."}, {"title": "5.1 NAIVE SETTING", "content": "In the first setting, the learning algorithm Ahead, naive returns the pre-trained feature extractor A and\nthe head w separately. So, we must ensure that the forget set Sf \u2286 S cannot be recovered from either\nA or w. As such, we must necessarily perform unlearning on A as described in Algorithm 1, which\nmeans that unlearning the fine-tuned model is exactly as difficult as unlearning the base model.\nTheorem 3 (Unlearning when releasing A and w). For a downstream task T with loss func-\ntion ly, consider the unlearning algorithm Uhead, naive that first runs Algorithm 1 to compute\n\u0100 = Ubase (Sf, Abase(S),T(S)), where (Abase, Ubase) performs utility-preserving unlearning (Theo-\nrem 2). Then, it fits a head w = arg minw\u2208Whead l\u315c(w; \u0100) and returns A and w. We assert that\n(Ahead, naive, Uhead, naive) performs utility-preserving unlearning (Definition 4)."}, {"title": "5.2 REALISTIC SETTING", "content": "There is little reason to release A and w separately after fine-tuning the model, because it is unclear\nwhy one would want to use A without w or vice versa. One can obtain A directly after pre-training\ninstead of relying on a fine-tuned model, and there is little use for w alone, because it is highly\nsensitive to the specific topics extracted by A and their ordering. As such, we argue for releasing\nthe fine-tuned model as a single matrix\u00b3 B = Aw, where $B \\in R^{n\\times 1}$.\nTheorem 4 (Utility-Preserving Unlearning on the Downstream Task). Suppose that the downstream\ntask T only depends on a subset of topics Tclf \u2286 [r]; that is, w* = arg minv\u2208Wbase l\u315c(v; A*) has\nnon-zero entries only in the index set Telf. Denote $q := \\min_{k \\in Tclf} Pr_D[z = k]$, and let Ahead be\nthe head tuning algorithm (Definition 2) and Uhead be Algorithm 2. Then, (Ahead, Uhead) performs\nutility-preserving unlearning with deletion capacity\n$T_{Ahead,Uhead}^{\\epsilon,\\delta}(m) > c' \\cdot \\frac{mq}{r \\sqrt{nr}}$\n(7)\nwhere c' is a constant dependent on e, d, D, and T.\nThe full proof is in Appendix C, including the worst case of Tclf = [r]. When the task relies heavily\non every single topic (i.e., q = 1/ar), the above guarantee is equivalent to the one in the pre-training\nphase. However, in most realistic settings, the downstream task will only depend on a subset of\nthe latent topics in the corpus. In this case, q > 1/ar, and we can unlearn more points without\ndegrading the utility of the model. Intuitively this makes sense too; the more reliance T has on a\nrare topic, the less adversarial deletion it can tolerate.\n$0.001m\\epsilon \\rho (\\gamma \\rho)^3$\nProof sketch. We again assume that we are deleting $m_u \\le \\frac{0.001m\\epsilon \\rho (\\gamma \\rho)^3}{a^2r^2}$ points.\nFor any mod-\nification made to A, there is an equivalent modification that can be made to w instead such that\nB = Aw is preserved, so we do not need to update A. We look for v \u2208 Whead such that\n\u0391\u03c5 = AFwF, where wF is the head learned on AF. It can be shown that \u0100s has a unique\npseudoinverse since it is full rank; naturally, we set v = AS+ AF WF, thereby ensuring privacy even\nif one recovers a part of A from B = Aw. We furthermore define that is fit to the unlearned\nmodel before the Gaussian mechanism, v = As\u2020 \u0100w. We now need to show v and v satisfy both\nthe indistinguishability and utility preservation conditions in Definition 4.\nIndistinguishability. Let w* = arg $\\min_{v \\in Whead} l_\\tau(v; A)$ denote the result of head tuning A, and let\nw be the result of taking a Newton step on w (see Algorithm 2). Then by triangle inequality,\n||\u0100w \u2013 Aw ||2 \u2264 ||\u0100w \u2013 \u0100w* ||2 + ||\u0100w* \u2013 Aw* ||2 + ||Aw* \u2013 AFWF ||2\n(8)\nOne can generalize this to the case where the downstream task is a C-way classification, in which case\nB\u2208 RnxC"}, {"title": "6 RELATED WORKS", "content": "Provable unlearning. One ideally wants the unlearned model to behave identically to one that\nwas retrained from scratch with the forget set removed from the training data (Cao & Yang, 2015;\nBourtoule et al., 2021; Gupta et al., 2021). This is difficult to achieve in many settings, so there are\nseveral notions of approximate unlearning (Ginart et al., 2019; Guo et al., 2020; Neel et al., 2021)\nreminiscent of differential privacy (Dwork et al., 2014). Most relevant to our work is the notion\nof ($\\epsilon$, ${\\delta}$)-unlearning introduced in Sekhari et al. (2021), which we adapt to construct Definition 4.\nOur work focuses on deriving unlearning guarantees in the pre-training and fine-tuning pipeline.\nGolatkar et al. (2020) is closest to our work. They show considerably weaker guarantees on un-\nlearning information with respect to probes fit to the weights. In contrast, our work is focused\non realistic topic classification tasks and demonstrates strong guarantees (Definition 4). Recent\nworks have extended notions of certified unlearning to nonconvex settings. Zhang et al. (2024a);\nMu & Klabjan (2024); Chien et al. (2024) provide unlearning algorithms without deletion capac-\nity guarantees. Qiao et al. (2024) also proposes an unlearning method for non-convex settings but\nanalyzes its deletion capacity in a convex setting. Our work extends beyond the convex setting to\nprovide provable unlearning methods and corresponding deletion capacity analysis for non-convex\nmodels.\nTheoretical analysis of pre-training and fine-tuning. Our downstream task definition (Sec-\ntion 2.2) is inspired by works on transfer learning in language models (Saunshi et al., 2021;\nWei et al., 2021; Wu et al., 2023; Kumar et al., 2022), contrastive learning (Lee et al., 2021;\nHaoChen & Ma, 2023), and meta-learning (Chua et al., 2021; Collins et al., 2022; Y\u00fcksel et al.,\n2024)."}, {"title": "7 CONCLUSION", "content": "This work uses topic models to develop the first provable guarantees on unlearning in the modern-\nday pre-training and fine-tuning paradigm. We propose two unlearning algorithms that can effec-\ntively and efficiently unlearn from both the pre-trained model (Algorithm 1 and Theorem 2) and\nthe fine-tuned model (Algorithm 2 and Theorem 4). Notably, we find that it is easier, in terms of\nthe deletion capacity (Definition 4), to unlearn pre-training data from the fine-tuned model, and we\ncan do so without modifying the pre-trained base model. Our findings suggest that task-specific un-\nlearning is easier than full model unlearning, providing a promising path forward to design efficient\nalgorithms for large-scale models.\nThe most notable limitation of our work is that our usage of topic models, which permit a tractable\nanalysis but cannot capture interesting features of modern-day language models (e.g., their autore-\ngressive nature). Moreover, with the growing popularity of foundation models, there is scholarly\ndiscussion around meaningful definitions of unlearning and how they can be measured (Thudi et al.,"}, {"title": "A PRECISE DESCRIPTION OF ABASE", "content": "A.1 COMPLETE DESCRIPTION"}, {"title": "B FROM PROPERTIES OF THE LEARNING ALGORITHM TO THE PROOF OF\nTHEOREM 2", "content": "We first give the formal statement of Theorem 2.\nTheorem 5 (Formal statement of Theorem 2). Let Abase be the learning algorithm described in the\nprior sections and Ubase be the unlearning algorithm in Algorithm 1. Then, (Abase, Ubase) performs\nutility-preserving unlearning with deletion capacity\n$T_{Abase,Ubase}^{\\epsilon,\\delta}(m) \\ge c \\cdot \\min \\Big\\{  \\frac{\\min\\{  \\epsilon/8  ,   0.001 m  \\}  }{r^2 \\sqrt{rn}}  \\Big\\}$\n(19)\nwhere m is the number of training documents, r is the number of topics, and c is a constant depen-\ndent on D. The loss function h used in the utility-preserving definition is the maximum entrywise\nerror from the ground truth topic model A*."}, {"title": "B.1 PRELIMINARIES", "content": "When the norm is not specified, we assume that it is the Euclidean norm || . ||2. We now start off\nwith a technical assumption on the precision of the learning algorithm.\nAssumption 2. $\\epsilon_o \\le O(1/\\sqrt{nr})$.\nAssumption 3. Every word appears with probability $\\epsilon_0/4ar$ without loss of generality; see discus-\nsion in Arora et al. (2012b). Essentially, less probable words can be combined in a sense to form a\nsingle category of \"rare\" words.\nWe recall the definitions from Arora et al. (2012a).\nDefinition 6 (\u03b2-robust simplex). A simplex P is \u03b2-robust if for every vertex v of P, the l2 distance\nbetween v and the convex hull of the rest of the vertices as at least \u03b2.\nDefinition 7. Let {ai}=1 be a set of points whose convex hull is a simplex with vertices {vi}=1\u00b7\nWe say a set of r points is e-close the vertex set {vi}=1 if each of the r points is e-close in 12\ndistance to a different vertex in this vertex set.\nThe following result will be used throughout our proof.\nProposition 1 (Arora et al. (2012b)). Qp in population is yp-robust.\nWe now list the high probability events we condition on throughout our proof. These follow from\nprevious results in Arora et al. (2012a); they concern the properties of the output of the learning\nalgorithm.\nProposition 2. With high probability, in our regime of m, the following hold:\n\u2022 The correct anchor words are selected.\n\u2022 Each word appears at least $O(\\frac{m\\epsilon_o}{4ar})$ times.\n\u2022 The error in the empirical matrix Q is entrywise at most $\\tilde{O}(1/\\sqrt{m})$ from the population\nQ*.\nWe also utilize the following two key lemmas from Arora et al. (2012a) that we touched upon in the\nmain paper.\nLemma 7 (Approximation Guarantee on Anchor Words). Suppose each row of Q is at most \u03b4\ndistance away from the ground truth \u03b3p-robust simplex Q* in l2 norm. If 20rd/(\u03b3p)2 < \u03b3p, then\nthe set of anchor words found by the algorithm is O(\u03b4/\u03b3p)-close to the ground truth anchor words.\nLemma 8. When 20rd/(\u03b3\u03c1)2 < \u04afp, it holds for every word i that Ci has entrywise error\n$O(\\frac{\\delta}{(\\gamma \\rho)^2})$ from C."}, {"title": "B.2 PROOF OF THEOREM 2", "content": "The following are lemmas bounding the relation between $Q^s, \\hat{Q}, Q$.\nLemma 9. After training, the error of each row of $\\hat{Q}$ is at most $\\delta_2 := O(\\sqrt{\\frac{4ar}{m \\epsilon_o}})$. That is,\n$|| \\hat{Q}^s - Q || \\leq \\delta_2$ for all words i.\nImportantly, note that\n$\\frac{20r \\delta_2}{(\\gamma \\rho)^2} < \\gamma \\rho$\n(20)\nThis implies that the anchor words of $\\hat{Q}$ are $O(\\delta_2/(\\gamma \\rho))$ close to the anchor words of Q.\nConsequently, it holds that\n$|| \\hat{C}^s - C^* ||_{\\infty} \\leq O(\\delta_2/(\\gamma \\rho)^2)$\n(21)\nProof. The first part follows directly from the fact that if the number of documents $m = \\Omega( \\frac{1}{\\epsilon_o^2} )$,\nthen $|| \\hat{Q}^s - Q || \\leq \\delta_2$ for each row i. To show that\n$\\frac{20r \\delta_2}{(\\gamma \\rho)^2} < \\gamma \\rho$\n(22)"}, {"title": "C DOWNSTREAM TASK PROOFS", "content": "Recall the algorithm for learning the downstream task head.\nAssumption 4. For any A, lt is A-strongly convex with respect to w.\nSince our topic matrix A, can only take on a bounded support (i.e. the set of matrices where each\nrow is on the probability simplex), it is natural to say that the set of values w*(A) takes on over all\ntopic matrices A is bounded in a certain sense. As such, we also assume the following:"}, {"title": "C.1 INSTANTIATING FOR TCLF = [r]", "content": "We first instantiate Theorem 4 for the case where Tclf = [r"}]}