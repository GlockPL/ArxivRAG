{"title": "UNLOCKING POTENTIAL BINDERS: MULTIMODAL PRETRAINING DEL-FUSION FOR DENOISING DNA-ENCODED LIBRARIES", "authors": ["Chunbin Gu", "Mutian He", "Hanqun Cao", "Guangyong Chen", "Chang-Yu Hsieh", "Pheng Ann Heng"], "abstract": "In the realm of drug discovery, DNA-encoded library (DEL) screening technology has emerged as an efficient method for identifying high-affinity compounds. However, DEL screening faces a significant challenge: noise arising from nonspecific interactions within complex biological systems. Neural networks trained on DEL libraries have been employed to extract compound features, aiming to denoise the data and uncover potential binders to the desired therapeutic target. Nevertheless, the inherent structure of DEL, constrained by the limited diversity of building blocks, impacts the performance of compound encoders. Moreover, existing methods only capture compound features at a single level, further limiting the effectiveness of the denoising strategy. To mitigate these issues, we propose a Multimodal Pretraining DEL-Fusion model (MPDF) that enhances encoder capabilities through pretraining and integrates compound features across various scales. We develop pretraining tasks applying contrastive objectives between different compound representations and their text descriptions, enhancing the compound encoders' ability to acquire generic features. Furthermore, we propose a novel DEL-fusion framework that amalgamates compound information at the atomic, submolecular, and molecular levels, as captured by various compound encoders. The synergy of these innovations equips MPDF with enriched, multi-scale features, enabling comprehensive downstream denoising. Evaluated on three DEL datasets, MPDF demonstrates superior performance in data processing and analysis for validation tasks. Notably, MPDF offers novel insights into identifying high-affinity molecules, paving the way for improved DEL utility in drug discovery.", "sections": [{"title": "1 Introduction", "content": "The process of small molecule drug discovery begins with identifying potential chemical compounds that interact with desired protein targets, which can be achieved through experimental techniques such as DNA-encoded library (DEL) screening, a high-throughput method for identifying diverse sets of chemical matter against targets of interest. DELS are DNA barcode-labeled pooled compound libraries that are incubated with an immobilized protein target in a process referred to as panning. The mixture is then washed to remove non-binders, and the remaining bound compounds are eluted, amplified, and sequenced to identify putative binders (Figure 1). While DELs provide a quantitative readout for hundreds of millions of compounds, the measurements may include significant experimental noise and biases, arising from factors such as DEL members attaching to the protein immobilization medium or variations in the initial population. To control noise, an enrichment metric is often calculated Gerry et al. [2019], Gironda-Mart\u00ednez et al. [2021], comparing the compound population with its original population and control experiments.\nSeveral methods of processing noise and selecting DEL data through mathematical analysis have been developed to address these challenges Gerry et al. [2019], Satz [2016], Kuai et al. [2018], Faver et al. [2019], Zhu et al. [2021], Lim et al. [2022]. DOS-DEL-1 Gerry et al. [2019] utilizes confidence limits based on the Poisson distribution to estimate barcode abundance before and after screening, enhancing the representation of both low and high abundance tags while mitigating issues caused by synthetic environmental noise. Another study Lim et al. [2022] applies the Maximum Likelihood Estimation (MLE) technique to denoise DEL data by determining the maximum-likelihood enrichment fold, which is derived from the enrichment ratio and models the consistency of observed barcode counts within a normal distribution. However, these noise reduction techniques based on mathematical analysis are limited by their dependence on prior assumptions and simplistic models, leading to suboptimal performance when dealing with complex noise characteristics. In contrast, machine learning approaches offer superior handling of diverse and intricate noise conditions due to their data-driven nature and proficiency in nonlinear modeling and comprehensive learning. These approaches learn directly from data features and patterns, making them more adaptable and effective in managing noise in DEL experiments, as they do not rely on preconceived notions about data types or distributions.\nRecently, several groups have demonstrated robust performance in denoising DELs using machine learning (ML) techniques Lim et al. [2022], K\u00f3m\u00e1r and Kalinic [2020], McCloskey et al. [2020], Hou et al. [2023]. Deldenoiser K\u00f3m\u00e1r and Kalinic [2020] employs ML to enhance the discrimination of true potential binders from background noise, significantly improving the processing of DEL selections with purified proteins. However, it performs poorly on noisy data. To address this issue, another group has developed a DEL denoising network Hou et al. [2023] that uses multilayer perceptrons (MLPs) to encode ECFP, obtain maximum-likelihood enrichment fold, and optimize the model through a Maximum A Posteriori (MAP) estimation loss function. While this approach has achieved some success in denoising noisy DEL data, two challenges remain unaddressed in this model. Challenge 1: Despite the vast number of compounds in DEL libraries, the fact that they originate from a limited set of building blocks\u2014only a few hundred to a thousand\u2014restricts the ability of an encoder trained on these compounds to extract features effectively. Challenge 2: The ECFP, when processed by MLPs, may omit crucial structural details of compounds, and the model fails to account for molecular features across various scales, including atoms and building blocks.\nWe propose a novel Multimodal Pretraining DEL-Fusion model (MPDF) to mitigate these challenges by capturing a broader spectrum of compound information for enhanced DEL data denoising. For Challenge 1, we develop pretraining tasks that bridge compound graphs, ECFP, and text descriptions by establishing two contrastive objectives (Graph/Text and ECFP/Text) and training on expanded biochemical databases, bolstering the capabilities of compound encoders in capturing more comprehensive features. For Challenge 2, we propose a DEL-fusion neural network to integrate multi-scale compound information from the compound graph and ECFP. DEL-fusion utilizes bilinear interactions to synergize atomic-level, molecular-level, and submolecular-level information, enabling comprehensive integration across varying scales. These refinements allow the MPDF model to extract multi-scale and enriched features, providing downstream denoising tasks with comprehensive compound information for improved performance. We have conducted experiments on three noisy DELs (P, A, and OA datasets) Hou et al. [2023], verifying the usefulness of the MPDF in denoising DEL data.\nIn summary, our contributions are threefold:\n\u2022 We propose a Multimodal Pretraining DEL-Fusion model (MPDF) that designs pretraining tasks based on contrastive objectives between compound graphs, ECFP, and text descriptions. Training on expanded biochemical databases enables the compound encoders to capture comprehensive features and avoid overfitting in DELs with limited molecular diversity.\n\u2022 We develop a DEL-fusion neural network that integrates compound information at different scales, including atomic, submolecular, and molecular levels. This network utilizes bilinear interactions to synergize information from compound graphs and ECFP, providing high-quality compound features for downstream denoising tasks."}, {"title": "2 Multimodal Pretraining DEL-Fusion Model", "content": "The limited diversity of DNA-encoded library (DEL), constrained by a small set of building blocks, hinders the effectiveness of machine learning-based denoising algorithms, which often overlook multi-scale compound information. To address these limitations, we propose a novel Multimodal Pretraining DEL-Fusion model (MPDF) that enhances encoder performance by pretraining across various compound representations and their text descriptions and integrates compounds at atomic, submolecular, and molecular levels. The architectural framework of our model is presented in Fig. 2."}, {"title": "2.1 Preliminary", "content": "Previous research has suggested that the random noise in DEL experiments can be reliably modeled using a Poisson distribution Kuai et al. [2018]. Deldenoiser K\u00f3m\u00e1r and Kalinic [2020] has employed a Poisson ratio test to evaluate the consistency of the barcode counts with a hypothesized enrichment ratio, converting a z-score calculation to a probability score for a two-sided alternate hypothesis Lim et al. [2022], Gu et al. [2008]. As shown in Equation 1, $k_1$ represents the count obtained from the DEL library when targeting specific receptors, with a total count of $n_1$, while $k_2$ arises from the \"blank\" selection conducted with target-free beads, having total counts $n_2$, and serves as a control for assessing"}, {"title": "2.2 Multimodal Pretraining based on Text Description", "content": "DNA Encoded Libraries (DELs) are typically generated through a split-and-pool methodology, undergoing 3-4 cycles of combinatorial chemistry that routinely enables the synthesis of millions or even billions of unique compounds. These compounds are derived from a fundamental set of building blocks, but the diversity of these blocks is limited to only several hundred to a thousand. As a result, an encoder trained solely on compounds from DELs may have limited effectiveness in extracting useful features. However, the pretraining paradigm allows models to learn rich feature representations by initially training on large, diverse datasets. By optimizing the loss function (Equation 6) mentioned in the Preliminary section, the denoising model aims to obtain an R value that best reflects the enrichment of compounds, which is positively correlated with the compounds' activity values. Therefore, it is logical to develop a pretraining task focused on activity prediction, specifically tailored for the compound encoder."}, {"title": "2.3 DEL-Fusion between Graph and ECFP", "content": "Current denoising methods for sparse DELs often combine simple networks, such as MLPs, with basic compound representations, like ECFP (Extended-Connectivity Fingerprints)Hou et al. [2023]. This approach results in a loss of crucial structural information and inadequate representation of details at various scales, including atoms and building blocks. In contrast, building blocks, which are diverse small molecules serving as fundamental units at the submolecular level, exhibit structural differences compared to the final synthesized compounds but share essential structural information. Node features generated by graph encoders reflect the local structure at the atomic level, while graph features represent the overall structure and topological characteristics at the molecular level. To address the limitations of existing approaches and effectively integrate information from different scale levels, we propose DEL-Fusion, an interactive attention-based fusion approach inspired by BAN Kim et al. [2018]. DEL-Fusion dynamically and selectively focuses on compound information by considering the correlations between functional groups related to node, graph, and building block features. It comprises two layers, namely: (i) a DEL interaction map for capturing pairwise attention weights and (ii) a pooling layer applied to the interaction map to derive the joint Graph-ECFP feature.\nGiven the graph features with node features and compound features as $H_G = \\{h_G^1, h_G^2,..., h_G^M\\}$ and ECFP features $H_E = \\{h_E^1, h_E^2,..., h_E^N\\}$, where $\\{h_{GU}^1, h_{GU}^2,..., h_{GU}^{M-1}\\}$ and $\\{h_{EU}^1, h_{EU}^2,..., h_{EU}^{N-1}\\}$ are the node features of the graph and building block features, respectively. $h_G^M$ and $h_E^N$ are the compound features obtained by pooling the node features and the ECFP features. The DEL interaction map can obtain a single head pairwise interaction $I \\in \\mathbb{R}^{M \\times N}$.\n$I = ((1\\cdot \\overline{q}) \\circ \\sigma (UH_G))\\cdot \\sigma (VH_E)$ (9)\nwhere $U \\in \\mathbb{R}^{D_G \\times K}$ and $V \\in \\mathbb{R}^{D_E \\times K}$ are learnable weight matrices for graph and ECFP features, $q \\in \\mathbb{R}^K$ is a learnable weight vector, $1 \\in \\mathbb{R}^M$ is a fixed all-ones vector, and $\\circ$ denotes Hadamard product. The elements in I indicate the interaction intensity between node/compound and building-block/compound pairs, with mapping functional groups in compound features at different. To gain an intuitive understanding of DEL interaction, we can express an element $I_{i,j}$ in Equation 9 as\n$I_{i,j} = q^T (\\sigma (U h_G^i) \\circ \\sigma (V h_E^j))$ (10)\nwhere $h_G^i$ is the i-th column of $H_G$ and $h_E^j$ the j-th column of $H_E$. $h_G^i$ denote the i-th node feature or compound feature in graph form. $h_E^j$ denote the j-th building block feature in compound feature in ECFP form. Consequently, the DEL interaction process entails the initial transformation of features $h_G^i$ and $h_E^j$ into a unified feature space through the application of weight matrices U and V, subsequently facilitating the elucidation of interactions via the Hadamard product with the weighting vector q. In this way, DEL-Fusion selects compound information from different scale features that correspond to each other for downstream tasks.\nTo obtain the joint graph-ECFP feature $f' \\in \\mathbb{R}^K$, we apply bilinear pooling to the interaction map I. The k-th element of $f'$ and the compact joint feature are computed as:\n$f_k' = \\sigma ((H_G^T)U_k)^T I \\cdot \\sigma (V_k (H_E^T))$, $f = \\text{SumPool}(f', s)$ (11)\nwhere $U_k$ and $V_k$ represent the k-th column of the weight matrices U and V, respectively. Importantly, this layer introduces no new learnable parameters. Instead, it reuses the weight matrices U and V from the preceding interaction map layer, reducing the overall parameter count and mitigating the risk of overfitting. The SumPool($\\cdot$) function performs a one-dimensional, non-overlapping sum pooling operation with stride s, effectively reducing the dimensionality of $f' \\in \\mathbb{R}^K$ to $f \\in \\mathbb{R}^{K/s}$. Additionally, the concept of single pairwise interaction can be expanded into a multi-head form by computing several bilinear interaction maps. The final joint feature vector is a sum of individual heads. Owing to the shared nature of weight matrices U and V, the inclusion of each additional head necessitates merely the addition of a new weight vector q, thereby exemplifying parameter efficiency.\nDEL-Fusion utilizes learnable weight matrices, U and V, for graph and ECFP features, respectively, projecting the graph and ECFP forms into a shared feature space. This facilitates the computation of attention scores between compound molecules at different scales, thoroughly accounting for the mapping relationships that exist between them. Such an approach effectively filters out inactive or even performance-impairing groups within the building blocks.\nApplying the pre-trained MPDF model, we compute the R value mentioned in the Preliminary section and optimize the denoising loss function in Equation 6 by encoding compound information. As previously indicated, R is recognized as enrichment, serving to mirror the authentic activity data of compounds. Enhanced compound encoding capabilities and the incorporation of extensive compound information sources increase the likelihood of precisely aligning with the actual compound activity distribution within noisy datasets, a notion corroborated by subsequent experimental validation."}, {"title": "3 Experiments and Results", "content": "To validate the effectiveness of our algorithm, we compared it with four competitive benchmark algorithms on three DEL datasets generated by the CAS-DEL library in the existing work Hou et al. [2023]. The comparative methods include three mathematically-based approaches: dos-del Gerry et al. [2019], deldenoiser K\u00f3m\u00e1r and Kalinic [2020],"}, {"title": "3.1 Results", "content": "A dataset. The A datasets, which target A549 cells expressing carbonic anhydrase 12 (CA 12), exhibit higher noise levels compared to those targeting purified enzyme proteins, necessitating robust denoising strategies. Table 1 compares the denoising effectiveness of five algorithms on the A dataset, highlighting the superiority of machine learning-based approaches, particularly in terms of the AUPRC metric. This superiority stems from the capacity of deep learning techniques to capture intricate interrelationships and extract features that transcend the stringent distributional assumptions underpinning mathematical analyses. Consequently, these methods enable more efficacious denoising, particularly in datasets characterized by elevated noise levels, such as the OA dataset.\nThe AUPRC metric is crucial for evaluating a model's ability to accurately classify instances of minority classes. Mathematical analysis-based algorithms consistently failed to achieve an AUPRC greater than 0.1 across different building block (BB) sizes, indicating difficulties in identifying active compounds against diverse target proteins within this DEL dataset. In contrast, DEL-MAP, especially our MPDF model, demonstrated remarkable superiority in this aspect. Notably, MPDF maintains low standard deviations across various BB sizes, reflecting the stability of our algorithm compared to other approaches. Conversely, DEL-MAP, another machine learning algorithm, exhibits high standard deviations, indicating that its experimental results are subject to excessive randomness.\nIncreased BB size correlates with decreased performance metrics for all algorithms, with the AUPRC for minority classes being notably affected, underscoring the heightened challenge of denoising in highly imbalanced datasets."}, {"title": "3.2 Ablation Study", "content": "We conduct a comprehensive analysis of the impact of each module on the overall performance of the model, exploring their individual contributions and the collaborative benefits of their combination.\nMultimodal Pretraining based on Text Description. Table 3 elucidates the contributions of individual modules to model performance. The experimental results demonstrate that pretraining tasks consistently improve the performance of both ECFP and graph encoders, with the graph encoder exhibiting a notably higher enhancement, particularly within the OA dataset."}, {"title": "4 Conclusion", "content": "DNA-encoded library (DEL) screening is an efficient and affordable approach in pharmaceutical research, but it faces challenges due to noise from nonspecific interactions in complex biological systems and limitations of current data analysis-based and machine learning denoising methods. To address these issues, we propose a novel Multimodal Pretraining DEL-Fusion model (MPDF) that enhances encoder capabilities through pretraining between different compound representations and their text descriptions while integrating compound features across atomic, submolecular, and molecular levels, enabling access to multi-scale, enriched features for improved downstream denoising tasks. Experimental results on three noisy DEL datasets demonstrate the effectiveness of our approach in processing and analyzing DEL data for validation tasks, and future work will focus on leveraging DEL libraries to accelerate natural drug discovery."}, {"title": "A Related Work", "content": "DEL denoising\nDNA-encoded libraries (DELs) have emerged as cost-effective, scalable alternatives to traditional high-throughput screening in drug discovery, facilitating the identification of hits from vast compound libraries. These screenings have been conducted in various settings, from purified proteins to complex biological matrices, thus broadening DEL applications in research and industry Brenner and Lerner [1992], Needels et al. [1993], Song and Hwang [2020], Goodnow Jr and Davie [2017], Fitzgerald and Paegel [2020], Flood et al. [2020], Clark et al. [2009], Bailey et al. [2007], Neri and Lerner [2018].\nHowever, cell-based DEL screening is challenged by high background noise and low true hit rates, primarily due to the complex biochemical environment on the cell surface causing numerous non-specific interactions Oehler et al. [2021], and the low abundance of target proteins on cells Huang et al. [2022]. Various methods of processing noise and selecting DEL data through mathematical analysis have been developed to overcome these challenges. These include aggregation techniques to reduce sequencing count variability Satz [2016], a data normalization and enrichment calculation framework based on Poisson confidence interval estimation Kuai et al. [2018], z-score metric methods for quantitative comparison of compound enrichment across multiple experiments Faver et al. [2019] and understanding data noise and uncertainty through analysis of replicate samples Zhu et al. [2021]. Although significant progress has been made in denoising DEL data, they cannot still handle nonlinear and complex patterns effectively. Additionally, these approaches demonstrate limited adaptability when dealing with large datasets Hou et al. [2023]. Fortunately, deep neural networks have demonstrated strong capabilities in these aspects. For instance, GNN-based regression on DEL data Ma et al. [2021], using ML to differentiate true potential binders from background noise K\u00f3m\u00e1r and Kalinic [2020], building models on DEL datasets for virtual screening McCloskey et al. [2020] and using customized negative log-likelihood loss to improve regression methods Lim et al. [2022]. However, DEL data are generated by combining a curated set of building blocks via chemical reactions, limiting the structural diversity of active compounds and thus constraining the generalizability of the model. We introduce an innovative approach that enhances quality and information extraction via targeted pretraining to address this issue."}, {"title": "A.2 Compound encoder", "content": "Compound encoding technologies transform complex structural and functional data of compounds into computational-friendly numerical or symbolic forms, ranging from one-dimensional (1D) to three-dimensional (3D) and deep learning methods. 1D encodings capture global molecular properties like molecular weight or bond mobility, aiding in chemical library management Yang et al. [2020]. 2D encodings analyze planar molecular structures, identifying chemical features through various methods including topological fingerprints, circular fingerprints, and GNNs, to measure compound similarity Muegge and Mukherjee [2016], Willett [2006], Ritter and Kohonen [1989], Kriege et al. [2020], Kipf and Welling [2016]. 3D encodings delve into the spatial arrangement of molecules and their interactions with proteins, using techniques like shape-based encoders and 3D pharmacophore models Ballester and Richards [2007], Kumar and Zhang [2018], Axen et al. [2017], McGregor and Muskal [1999], Dixon et al. [2006], Wolber and Langer [2005], Chen and Lai [2006], Deng et al. [2004], Da and Kireev [2014], W\u00f3jcikowski et al. [2019]. Deep learning encoders further compress these features into low-dimensional vectors, leveraging sequence-based, fingerprint-based, graph-based, and interaction-based techniques for enhanced compound analysis Goh et al. [2017], Xu et al. [2017], Zhang et al. [2018], Karpov et al. [2020], Winter et al. [2019], Jaeger et al. [2018], Jeon and Kim [2019], Duvenaud et al. [2015], Kearnes et al. [2016], Pereira et al. [2016], Wang et al. [2019].\nThe evolution from 1D to 3D and deep learning encodings demonstrate a significant advancement in learning molecular characteristics in detail. Advanced deep learning enables more precise representation of molecular features in lower dimensions, thereby enhancing the scope and efficiency of compound analysis. Our study employs deep graph neural networks and Multilayer Perceptrons (MLPs) to encode compound graphs and ECFPs, using a fusion network for multi-scale representation."}, {"title": "B Compound Encoder Selection", "content": "In our design of compound encoders, we meticulously choose and integrate models that excel in capturing information across different scales. This approach enables a detailed analysis of compounds' complex structural and chemical properties.\nGraph Encoder. Graph Convolutional Network (GCN) Kipf and Welling [2016] is one of the classic approaches for compound encoders Duvenaud et al. [2015], Kojima et al. [2020], Fang et al. [2022], Bai et al. [2023], providing a solid framework for encoding compound features. The foundation of GCN is a graph that accurately represents the structural nuances of compounds. In this graph, nodes represent the atoms, and edges depict the chemical bonds between these atoms. Each node carries a feature vector that includes key details such as the atom's chemical element, charge state, and valency, providing a rich characterization of atomic attributes. On the other hand, the edges represent the physical bonds and encode the bond type and the intensity of atomic interactions, which are essential for delineating the compound's topological structure. Utilizing graph convolutional layers, the GCN facilitates a comprehensive featurization of each atom, incorporating its properties and aggregating data from adjacent atoms. This methodology adeptly captures a spectrum of interactions within molecules, from direct to indirect atom-to-atom connections. Further, integrating node and edge information allows for a holistic representation of a compound's structure. We employ GCN to encode synthesized compounds, harnessing local and global structural insights to furnish deep molecular features for downstream tasks.\nECFP Encoder. Compared to graph encoders, which dynamically learn task-relevant local and global structural feature representations, ECFP (Extended Connectivity Fingerprints) inherently offers a fixed, static encoding of local chemical environments Rogers and Hahn [2010]. Specifically, ECFP encodes each atom and its neighborhood within a molecule, generating a series of hash values that represent specific local structural features of the molecule. Therefore, combining ECFP with a shallow MLP (Multilayer Perceptron) allows for the rapid capture of both local and global chemical information of simple compounds. The MLP-processed ECFP represents a \"flattened\" depiction of molecules, where some spatial structural information might be lost, and its learning capability is somewhat limited by the information ECFP can provide. However, considering our in-depth analysis of synthesized compounds using GCN, we employ this method to swiftly extract the groups detached from building blocks during the synthesis process, serving as supplemental information for the synthesized compounds. In our model, we employ two-layer MLPs with batch normalization and ReLU as the encoder for ECFP."}, {"title": "C Dataset", "content": "We employed the CAS-DEL library Hou et al. [2023], which contains 7,721,415 compounds targeting purified carbonic anhydrase 2 (P dataset) and a cell line expressing membrane protein carbonic anhydrase 12 (CA 12). The data for CA 12 can be categorized into two distinct datasets based on expression: one with A549 cells expressing CA-12 (A dataset) and another with hypoxic A549 cells overexpressing CA-12 (OA dataset). CAS-DEL, a 3-cycle peptide library, was constructed utilizing 195 amino acids for cycle-1, 201 for cycle-2, and 197 for cycle-3 building blocks. Notably, the cycle-3 arylsulfonamide building block (BB3-197) was a critical determinant, with compounds incorporating this block tagged as active (1) and those lacking it tagged as inactive (0). For algorithm validation, we extracted subsets from the CAS-DEL library containing 10, 20, 30, 40, and 50 non-active building blocks alongside BB3-197, forming libraries of 1100, 8400, 27900, 65600, and 127500 compounds. We conducted experiments on five randomly selected subsets for each building block size, adopting an 8:1:1 split for training, validation, and testing phases to evaluate the algorithm's effectiveness across different DEL library sizes. The decision against validating the algorithm on the entire CAS-DEL dataset was based on the belief that the chosen subsets adequately demonstrated the algorithm's advantage, coupled with insufficient experimental resources."}, {"title": "D Experimental Setting", "content": "We employ a 2-layer Graph Convolutional Network (GCN) with hidden layers of size 256, ReLU activation function, and batch normalization for the graph representation. The ECFP representation utilizes a fingerprint radius of 3 with a dimension of 2048. Both the synthesized compounds and individual building blocks share the same ECFP encoder, which consists of a 2-layer Multi-Layer Perceptron (MLP) with hidden layers of size 256 and normalization. PubChem23 Seidl et al. [2023] serves as our pretraining dataset, and we use CLIP+LSA for text processing. In the denoising task, we set the batch size to 16, select Adam as the optimizer, and choose the learning rate within the range of [0.001, 0.01]. Empirical findings suggest that the choice of batch size and optimization strategy has minimal impact on our method, allowing for the exploration of alternative methodologies. The hyper-parameter a in the loss function is set to 3 to maximize the separation between noisy and control data, emphasizing the superiority of our algorithm. Detailed parameters for multimodal pretraining are selected based on CLAMP Seidl et al. [2023]. We employ a threshold of 1 for predicted enrichment R, with values above 1 indicating active compounds and values below 1 signifying inactivity. Our model is trained on a GeForce RTX 3090 GPU with 24 GB of memory, with training times ranging from 1 to 6 hours, depending on the complexity and size of the different building blocks."}, {"title": "E Experimental Results on P dataset", "content": "The P dataset, which targets purified enzyme proteins, exhibits relatively minimal noise that aligns, to a certain extent, with mathematical distribution assumptions. Consequently, methods grounded in mathematical principles tend to perform commendably on this dataset. Notably, the Dos-del algorithm, which exclusively employs the Poisson distribution to approximate DEL data noise, secures a performance ranking second only to ours at Building Block (BB) sizes of 20 and 50. Similarly, DEL-MLE, leveraging maximum likelihood estimation to deduce the R value, demonstrates competitive prowess at BB sizes of 30 and 40, even achieving the highest precision metrics at BB sizes of 30 and 40, respectively. Despite these outcomes, machine learning-based algorithms predominantly surpass their mathematically oriented counterparts. This superiority is attributed to the ability of deep learning methodologies to discern complex interrelationships and features that extend beyond the rigid distributional assumptions inherent to mathematical analyses, thereby facilitating more effective denoising. This advantage becomes increasingly pronounced in datasets characterized by greater noise levels, such as those of the A and OA datasets. Furthermore, the impact of BB size on algorithmic performance is negligible within datasets exhibiting smaller noise magnitudes, owing to the simplicity of their data distributions."}, {"title": "F Discussion", "content": "The experimental results demonstrate the superiority of machine learning-based denoising algorithms, particularly the proposed Multimodal Pretraining DEL-Fusion (MPDF) model, over traditional mathematical approaches in addressing the complex noise patterns found in DNA-encoded library (DEL) datasets. Machine learning techniques excel in learning intricate, non-linear relationships within the data, which are often missed by mathematical methods limited by rigid assumptions.\nThe exceptional performance of MPDF can be attributed to two key innovations. First, the incorporation of pretraining tasks with contrastive objectives enhances the compound encoders' ability to learn generic features from diverse representations. Second, the DEL-fusion neural network within MPDF integrates multi-scale compound information,"}, {"title": "G Limitations", "content": "While our proposed model significantly enhances the denoising effect for DEL datasets by leveraging the same compound library across different targets and utilizing target-free bead DEL datasets as a reference, it is essential to acknowledge certain limitations:\n\u2022 Dataset Dependency: Our model's effectiveness relies on the availability of DEL datasets with identical compound libraries for multiple targets, as well as target-free bead DEL datasets for reference, as outlined in the assumptions in Section 2.1. This dependency limits the model's applicability in scenarios where only a single target DEL dataset is accessible, hindering its ability to perform denoising in such cases. Future research should explore more flexible approaches that enable direct denoising from a single target DEL dataset to expand the model's usability.\n\u2022 Memory Overhead: The training process of our model involves constructing a graph for each compound, which can be memory-intensive, particularly when dealing with large-scale DEL datasets containing millions of compounds. This high memory requirement may pose scalability and practicality challenges when applying the model to extremely large datasets, potentially necessitating specialized hardware or distributed computing solutions. Efforts should be made to optimize memory usage and develop more memory-efficient strategies to handle large-scale datasets effectively.\n\u2022 Training Efficiency: The incorporation of multimodal pretraining and an attention-based fusion model in our approach may result in relatively longer training times when processing massive DEL datasets. This aspect of training efficiency should be carefully considered when applying our model to extremely large-scale datasets, as it may impact the timely deployment of the model in real-world scenarios with strict time constraints. Future work should explore techniques to improve training efficiency without compromising model performance, such as efficient data loading, parallel processing, and model optimization.\nTo improve the practicality and widespread adoption of our model in various DEL-based drug discovery applications, future research should focus on the following areas:\n\u2022 Developing more flexible and adaptable approaches that enable direct denoising from a single target DEL dataset, eliminating the need for multiple target datasets and target-free bead DEL datasets as references.\n\u2022 Implementing a multi-task learning framework that leverages information from related targets and tasks to improve the model's performance on new targets with limited data. By sharing representations and learning across multiple tasks, the model can better capture the underlying patterns and relationships, leading to enhanced generalization and reduced overfitting on individual targets.\n\u2022 Optimizing memory usage and exploring memory-efficient strategies to effectively handle large-scale datasets, ensuring the model's scalability and practicality in real-world scenarios."}]}