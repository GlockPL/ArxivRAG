{"title": "Tailoring the Hyperparameters of a Wide-Kernel Convolutional Neural Network to Fit Different Bearing Fault Vibration Datasets", "authors": ["Dan Hudson", "Jurgen van den Hoogen", "Martin Atzmueller"], "abstract": "State-of-the-art algorithms are reported to be almost perfect at distinguishing the vibrations arising from healthy and damaged machine bearings, according to benchmark datasets at least. However, what about their application to new data? In this paper, we are able to confirm that neural networks for bearing fault detection can be crippled by incorrect hyperparameterisation, and also that the correct hyperparameter settings can actually change when transitioning to new data. The paper weaves together multiple methods to explain the behaviour of the hyperparameters of a wide-kernel convolutional neural network and how to set them. Since guidance already exists for generic hyperparameters like minibatch size, we focus on how to set architecture-specific hyperparameters such as the width of the convolutional kernels, a topic which might otherwise be obscure. We reflect different data properties by fusing information from seven different benchmark datasets, and our results show that the kernel size in the first layer in particular is sensitive to changes in the data. Looking deeper, we use manipulated copies of one dataset in an attempt to spot why the kernel size sometimes needs to change. The relevance of sampling rate is studied by using different levels of resampling, and spectral content is studied by increasingly filtering out high frequencies. At the end of our paper we conclude by stating clear guidance on how to set the hyperparameters of our neural network architecture.", "sections": [{"title": "1. Introduction", "content": "Machines are full of bearings, the components that make rotation possible. They are small metal balls usually arranged in a circular casing and by rolling, these spheres allow one piece of a machine to move around relative to another piece of the machine. Wear and tear unfortunately can accumulate over time into damage that causes the bearing element to ultimately break down, taking the entire machine offline with it. For example, one way this can happen is if small scratches develop in the casing that holds the bearings in place. As the ball bearings roll over that scratch again and again (perhaps hundreds of times per minute for days, weeks and months), it can grow into a structural weakness. One day, the casing can crack due to this weakness, breaking the rotating element and putting the entire machine out of commission until repairs are made. Therefore, methods for detecting such faults are crucial.\nIn this paper a significantly extended and adapted revision of [1] building on our work presented in [2] we tackle the question of how to configure neural networks so that they can effectively identify different types of damaged bearings. Thus, in this paper we do not start from scratch but instead our work takes place after and continues from research we published earlier. In particular, this paper presents a series of new experiments that address questions raised and left unresolved by our earlier work in [1] and [2]. Two new neural network architectures are studied in order to show how important hyperparameter configuration is to a wider range of networks. We present a novel experimentation methodology, extending the methodology presented in [2], for assessing the impact of hyperparameters in the context of wide-kernel approaches for bearing fault detection. Doing that, we run new experiments on manipulated subsets of the data to see how dataset properties affect the key question of how to configure hyperparameters. Specifically, we present more datasets, more neural network architectures and more extensive results compared to [2].\n1.1. Finding Faults and Beating Benchmarks\nFrom outside of the machine, the damage that develops in a bearing element cannot be seen. Bearings are encased within the machine and hidden from view. This creates obvious problems for monitoring the accumulation of damage within a bearing element and deciding when a replacement is needed. However, using vibrations which spread through the machine, we can try to look inside at what is happening: sensors placed on the outside of"}, {"title": "1.2. Limits of What the Benchmarks Can Tell Us", "content": "Benchmarks are very useful when comparing algorithms but they can only give an indication about how successful a method will be when applied to new data in the real world. The belief that an algorithm will perform well when it analyses a new set of machine vibrations, in a previously-unseen context, is simply an assumption and an assumption that can be challenged. For example, the types of damage that build up when a machine is really used in a factory might go beyond what is seen amongst the artificially-created damage recorded in benchmark datasets. Who can say that the types of damage found in the real factory will not be more subtle and hard to detect, or more diverse and thus requiring an algorithm with increased flexibility? Such questions start to introduce uncertainty. Differences in the properties of the recorded data might mean that an algorithmic approach does not generalise perfectly, leading to inferior performance when it is applied to a new dataset.\nSpeaking more generally about neural networks, when a practitioner takes an existing algorithm and tries using it on their own data, there is a risk that it does not work. Imagine that the practitioner is left with an algorithm that does not work. What can they do? One of the first things they can try is to modify the hyperparameters of the algorithm. In order to control the training of a neural network, the user must specify some so-called \u2018hyperparameters', such as the minibatch size (the number of examples used in a single step of training). These hyperparameters control how to generate the training signal that the neural network learns from \u2013 they are used in generating the numbers that incrementally update the neural network. Just like how a mis-tuned radio will not play your favourite radio show, it is possible to mis-tune hyperparameters and end up with a training signal that does not do what you want, i.e., it will not teach the neural network how to do the task."}, {"title": "1.3. The Open Question of Hyperparameters", "content": "Alongside hyperparameters that modulate the training process, one can consider what we might call \u2018architectural hyperparameters', which control the size and shape of a neural network. Quite commonly, when faced with poor results, a practitioner might make their neural network bigger. The layers used to construct the network could be enlarged or the network could be built by stacking more layers together. After making such changes, the neural network will contain more trainable parameters (numbers used to store information about how to process inputs) and thus have more capacity to learn about the task. If a neural network is not working then quite a natural step is to make it larger by increasing the architectural hyperparameters.\nThe reason we tweak hyperparameters is that the wrong choice of settings will lead to inferior performance whilst good choices will bring out the best from a neural network. In the case of severe ill-hyperparameterisation, a network might fail to learn at all during training. Returning specifically to bearing fault detection, we noted earlier that the differences appear to be minimal between the top 25 neural networks for detecting damaged bearings. By contrast, larger differences in accuracy can emerge when the hyperparameters of a network are changed. For example, [14] reports that the accuracy of a bearing fault classifying CNN is cut in half simply as a result of increasing from 6 convolutional layers to 8. When it comes to getting near-perfect accuracy on bearing fault detection, getting the hyperparameters right might be more important than choosing between neural network architectures.\nHowever, although bad hyperparameter settings can result in misery, there is a question of why this matters, because we know that the hyperparameters work on our benchmark data. If the hyperparameters are already configured well, then what exactly is the problem? This is really a question of whether hyperparameters that work well on one dataset will necessarily be successful on another. To hone in on just a single reason why this might not always happen, we can ask whether drastically changing the sampling rate would impact what size the filters in a convolutional network should be.\nConvolutional filters act like learned patterns that the network recognises at different time points during the input signal. Each pattern is stored in an array of numbers. If we increase the sampling rate used to record a signal that is otherwise kept fixed so that more points are used to record the same peaks, troughs and plateaus over time \u2013 then it might be reasonable to expect that the number of points used to record the important patterns (in convolutional filters) also needs to be increased. The main point of this"}, {"title": "1.4. Shaping the Neural Network", "content": "Architectural hyperparameters like the number of layers in a network affect how complicated it is to process each input. Making the network smaller reduces the number of internal parameters it has, meaning that the computer needs to perform fewer arithmetic operations. This has a knock-on effect that impacts, for example, how long it takes to train the network. Smaller networks train faster and are quicker to generate predictions on new data. If some architectural hyperparameters can be reduced without harming the accuracy, it would be good to know this. Practitioners would benefit from knowing that they can make the network smaller and thus faster to train and use. Moreover, in real-world settings, there is the possibility of \u2018embedding' fault detection algorithms, so that they run on small devices situated in and amongst the machines they analyse rather than running on a user's laptop. Embedded processing skips the step of extracting data from the sensors and communicating it to a central location (like a laptop), and removes the need to keep a laptop or computer running continuously in order to process the data. An additional benefit of small networks is that they can be run on smaller and cheaper embedded devices. Clearly, there is real value to understanding the hyperparameters of a network, to see which hyperparameters can and cannot be changed when processing a given dataset. In this vein, we"}, {"title": "1.5. Do Dataset Properties Determine Performance?", "content": "Datasets have different properties, one reason why hyperparameters might need (re)tuning on new data even though they were already working on benchmarks. In the field of bearing vibration, research focuses on sensor recordings stored in \u2018time series', so called because they consist of a series of sequential measurements of some variable over time. The sampling rate is one of the key data properties when discussing time series. With a sampling rate of 12kHz, the movement of a vibrating machine is measured every 0.00008333 seconds. The sampling rate determines how many measurements are made per second and conversely how far apart in time each individual measurement point is. The sampling rate is documented for most benchmark datasets and we can see that it varies. One thread of our research will be to find out if changes to the sampling rate have an impact on the accuracy of a network and on which hyperparameter settings are best.\nThe sampling rate affects the dataset in multiple ways. It affects what frequency bands can be detected, with half the sampling rate (also known as the 'Nyquist frequency') providing the upper limit of what frequencies can be captured. An increased sampling rate also means that more numbers are used to record a vibrating machine, implying that the neural network needs to do more computation in order to process the inputs. One open question is how much the highest frequencies in vibration recordings are used by neural networks. This is interesting because if the highest frequencies are not needed, then storing and processing the recordings could be made more efficient by reducing the sampling rate. In fact, if high frequencies are unnecessary for fault detection then removing them could be a form of noise filtering. This is not implausible, since many signals resulting from"}, {"title": "1.6. Explaining Hyperparameters", "content": "How many layers is enough in a neural network? Why do we set other hyperparameters in the way that we do? Although there are rules to follow when tuning some hyperparameters like the minibatch size (e.g., \u201cset it to the largest value that will still allow the minibatch to fit into GPU memory\") in other cases we can look at a neural network and find that the hyperparameter settings seem totally arbitrary. To pick just a pair of fault detection papers at random, we find that [9] uses 20 hidden units in fully-connected layers whilst [17] uses 64 instead. As is typical in the field, neither author noted why they chose the particular numbers they use and the reader is left with no explanation of why there is a difference. If there is ever a need to adapt these hyperparameters when working with new datasets, future users could quite justifiably feel lost. What users in fact want is a simple, explainable process for setting the hyperparameters.\nThe terms \"Explainability\" and \"Explainable AI (XAI)\u201d tie together several strands of research aimed at clarifying how the inputs to an algorithm relate to the outputs. With complex statistical models such as neural networks the inner workings are not understandable and we must rely upon specialised techniques to illuminate why particular outputs were chosen by the algorithm. This is a large topic and there are many surveys of the work done in this research area, such as [18] for example. What XAI aims to do is to cast a light on obscure and hidden relationships that exist in the algorithm and which link the inputs to the outputs. In many real-world domains such as bearing fault detection, algorithms need to learn nonlinear mathematical functions in order to approximate the complex interactions between multiple factors seen in observed data."}, {"title": "1.7. The Paper Ahead", "content": "The present paper re-examines and then extends upon the results from several pieces of our own past work. In addition to providing new insights on past results, this present study presents new experiments with resampling and filtering of the data, allowing us to investigate how the properties of the data affect hyperparameter tuning.\nTo summarise the intention of the research: we already have neural network architectures for bearing fault detection ([7] gives many examples) but their success relies on hyperparameters which are poorly understood. Hyperparameters become especially important when we remember that the users of a neural network will not want to take it and apply it to a benchmark dataset. For the network to be any use at all, it must be possible for someone who is not the developer to take it and apply it themselves to novel datasets. To do so, they need some idea of how to decide the hyperparameter settings. However, the hyperparameters might as well be written in a cypher - there is no intuitive way to know that 64 convolutional filters works but a rate of 128 does not. The numbers seem arbitrary.\nEngineers in a factory might have different machinery and types of bearings from what was used in benchmark datasets. They may have restrictions about where the sensors can be placed, and even what sensors they can use to record vibrations. Together, the specifics of a factory's machinery, bearing types, sensor availability and sensor placement will conspire to make a unique dataset for each engineer. As already noted, the choice of algorithm will not impact performance that much on benchmark datasets (see [7]), but the performance drop as a result of trying out a new dataset might be noticeable. Therefore, we take an existing neural network architecture which is capable of achieving state-of-the-art results on multiple benchmarks, and enrich it by presenting a detailed analysis of the hyperparameter decisions presented to the user."}, {"title": "2. Background & Related Work", "content": "In this section, we review fault detection for the rotating machines seen in industrial settings. We also provide an overview of deep learning (DL) in the realm of time series analysis using sensor data, emphasising the design and training of convolutional neural networks (CNN). Additionally, given that this research concentrates on the architectural hyperparameters of a wide-kernel network, we discuss studies related to hyperparameter optimisation.\n2.1. Fault Detection\nDetecting faults in rotating industrial machinery is essential to avoid breakdowns [19]. Fault detection data are typically collected from sensors that measure vibrations, represented as time series data. Initially, fault detection relied on physics-based models that required an understanding of the underlying mechanisms that generate and propagate vibrations [20]. However, these models struggled to adapt to changing environments and increasing data complexity. The advent of the Industrial Internet of Things (IIoT) and data-driven analysis techniques has revolutionised fault detection methods, enabling a more intelligent and automated approach [17, 21]. These advances eliminate the need for in-depth technical knowledge of industrial machinery, allowing for automated processing and adaptation to changing operational environments. For example, machine learning approaches such as K-NN [22, 23], Random Forest [24], and SVM [25, 26, 27, 28] have been applied for fault detection. However, these methods require extensive feature extraction, a time-consuming process that has to be fine-tuned towards the type of data used.\nWith the development of deep learning techniques, these steps were no longer necessary since deep learning techniques are able to automatically extract features from raw data, simplifying the otherwise complex feature extraction process. Initially, multilayer perceptrons (consisting of computational units arranged into simple layers where every unit in one layer connects to every unit in the next layer) were used, but their limited depth due to computational constraints shifted the focus to other architectures such as recurrent neural networks (RNNs), which can more effectively model temporal dependencies [29, 30]. An RNN processes sequences of data one step at a time, passing information along internally to keep track of what it has seen. The computational units in an RNN are arranged differently than for an MLP, since the RNN is formed from small blocks suited for processing"}, {"title": "2.2. Deep Learning", "content": "Deep Learning draws inspiration from the human brain's ability to combine many simple units into a large system capable of learning difficult tasks [35]. Its strength lies in processing and learning complex data, making it particularly effective for tasks requiring automatic feature extraction and refinement. This capability is crucial when valuable insights can be revealed by recognising patterns and relationships in the data [36, 37].\nThe fundamental component of a neural network is the artificial neuron, or \"node,\u201d which processes multiple data inputs to produce an output. Many nodes can be grouped together into a layer and the term \u201cdeep\u201din deep learning refers to the inclusion of many layers in neural networks. A simple representation of a neuron i is given by:\n$\\alpha = \\sigma(\\sum_{j} w_{ij} x_{j} + b_{i})$  (1)\nHere, $x_{j}$ are the inputs, $w_{ij}$ are the weights, $b_{i}$ is the bias, and $\\sigma$ is a nonlinear activation function. Each input is multiplied by a corresponding weight and the resulting values are added together along with an extra bias term. The formula operates somewhat like a recipe that combines different amounts of the various inputs (the $x_{j}$ terms) and adds some final garnish (the bias $b_{i}$). The nonlinearity introduced by the activation function is needed in order to handle difficult tasks, for example when similar inputs need different outputs. With layers stacking and interconnecting in complex architectures,"}, {"title": "2.2.1. Convolutional Neural Networks", "content": "Convolutional Neural Networks (CNNs) are a specialised type of neural network that were originally created to handle two-dimensional (2D) data, such as images. Introduced by LeCun in 1989 [53, 38], CNNs utilise a feed-forward architecture that applies convolutions instead of general matrix multiplications, making them highly effective for data organised in a grid-like structure. In contrast to conventional MLPs, CNNs have become essential in areas such as Computer Vision, Image Recognition [38, 37], and are increasingly used in time series analysis.\nThe primary benefits of CNNs compared to other neural networks arise from their use of local receptive fields, weight-sharing, and sub-sampling [54]. These characteristics greatly decrease memory usage and computational complexity, thereby improving algorithmic efficiency. Convolutional layers apply filters to convolve input data and this basically allows the network to identify where patterns occur within a large signal. The activation function, commonly a Rectified Linear Unit (ReLU), introduces non-linearity, allowing the network to learn intricate patterns [55]. After convolution and activation, a pooling layer often shrinks the signal's size in order to decrease the number of parameters of the next layer which helps prevent overfitting and lowers computational demands.\nInitially, CNNs were less common for time series data due to their 2D nature, requiring conversion of one-dimensional (1D) data into 2D matrices, adding computational overhead. This changed with the development of 1D CNNs, which can directly process raw time series data [56, 34, 15, 47, 33, 31]. These advancements have solidified CNNs as a powerful tool for time series analysis, capable of efficiently handling high-frequency sensor data and extracting valuable features with minimal preprocessing.\nThe convolution operation is mathematically expressed as:\n$y^{l+1}(j) = k * M^{l}(j) + b_{i}$ (2)\nwhere $b_{i}$ is the bias, k represents the filter weights, and $M^{l}(j)$ is the local region of the input in layer l. The weights of the filter act in some ways like a learned pattern, in the sense that the filter reacts most strongly (gives the largest numbers as outputs) when the input correlates with the weights of"}, {"title": "2.3. Hyperparameter Search in Deep Learning", "content": "Past literature furnishes us with several examples of hyperparameter searches: activities in which authors tweak and tune the hyperparameters of a neural network in order to find the best performance. Because many decisions when designing and training neural networks can be called 'hyperparameters', previous studies have different degrees of overlap.\nWorking in the field of sentiment analysis, [58] used a grid search \u2013 which takes a list of valid values for each hyperparameter individually and then tests out every possible combination of these values \u2013 to optimise the accuracy of their LSTM-CNN network. Their motivation for using the grid search approach was to guarantee the absolute best performance, which more efficient search methods trade off in favour of increased processing speed. As will be described in Section 3, we also employ a grid search method, although our reason for doing so is that it supports a wider range of follow-on analyses rather than to find a single combination of hyperparameters that is best. Grid search is fairly common, for example being used by [59] to optimise the three training hyperparameters of learning rate, minibatch size and dropout ratio when training a CNN. The authors of [59] used the Talos package to implement their hyperparameter grid search.\nBayesian optimisation is another popular technique to find good hyperparameter settings. When developing a CNN-GRU network to detect different types of activities from wearable fitness tracker-type devices, [60] used the Optuna and OptKeras packages to implement their Bayesian optimisation search. The hyperparameters they tuned included the number of convolutional filters in different layers, and the number of hidden units in the GRU layers. The authors of [61] used SigOpt to perform Bayesian optimisation on a wide range of hyperparameters, from minibatch size and learning rate to the number of fully-connected neurons and the properties of the convolutional"}, {"title": "2.4. Summary of Own Previous Work", "content": "In [2] we investigated how choices of the hyperparameters for a specific deep learning architecture (wide-kernel CNN) affect its performance in classifying faults from vibration signals in industrial machines. This architecture consists of five convolutional layers including an initial \u2018wide' layer with a large kernel size which then feed into a fully-connected classifier. See Section"}, {"title": "3. Method", "content": "Our research method consists of several parts which feed into each other. In this section, we step through each piece of the method and describe it in detail. Starting out, we describe seven different benchmark datasets that we use to train and test neural networks. Different types of fault, machinery and recording set-up are possible in fault detection and we therefore expect that datasets can differ from one another. We want to account for variations in the data and so we assembled a selection of seven benchmarks to train and test out CNNs. The properties of the benchmark datasets are described in detail below.\nNext, our method splits into two main structural components based on which architectures we study. Previous work in [2] and [1] looked at the consequences of tuning the hyperparameters of a wide-kernel CNN but not other kinds of architecture. We devote a short section to the impact of hyperparameter tuning on two other architectures with the goal of showing more generally that hyperparameters matter for bearing fault classification, at least when using neural networks. We introduce two popular types of architecture that are used in this short section as alternatives to a wide-kernel CNN. Additionally, we describe how we generate results by sampling different hyperparameter values from a range of possibilities and then training and testing networks on the seven benchmark datasets.\nThe second structural component and the majority of the research however focuses on the wide-kernel CNN and aims to investigate how much dataset properties affect which hyperparameter settings are best. We describe the wide-kernel CNN architecture used in our work and its architectural hyperparameters. Training and testing on seven benchmark datasets gives an indication of how differences in the data can have consequences for hyperparameter tuning. We start off with a grid search which is an exhaustive exploration of different combinations of hyperparameters and for each combination we train and test a CNN on benchmark data. We repeat the grid search seven times in order to include all the benchmark datasets mentioned earlier.\nIn addition to accepting the benchmark datasets as-is, we also want to get into a position to see what happens when we manipulate the dataset properties ourselves. By manipulating the datasets, we gain fine-grained control over factors that might explain why some hyperparameter settings result in high accuracy while others perform badly. In concrete terms, we edit"}, {"title": "3.1. Datasets", "content": "In this subsection we describe the most obvious input to our experiments: data. We use seven benchmark datasets, described below along with their main properties of the datasets and our pre-processing steps. These are the seven datasets we used earlier in [1].\n3.1.1. CWRU Bearing Dataset\nThe CWRU bearing dataset, provided by Case Western Reserve University [66], represents a benchmark for fault detection experiments where various damages were gouged into bearing elements. Sensors were placed on the drive end and fan end of the machine to measure vibration signals, which were digitised into two time series and segmented into sequences of 2048 data points. The data was collected at a sampling rate of 12 kHz for both the drive-end and fan-end experiment, and the data sampled at 48 kHz for the drive-end experiment. The latter is sampled at a much higher rate and therefore contains more data points. For all recordings retrieved from the CWRU experiments, we extracted only the machine operation conditions with motor speed of 1797 and 1750 rotations per minute (RPM). The damages of the bearing are inflicted at three depths (0.007, 0.014, and 0.021 inches) and across five fault locations (ball, inner race, outer race opposite, outer race"}, {"title": "3.1.2. Paderborn Dataset", "content": "The Paderborn bearing dataset serves as a benchmark for fault detection and condition monitoring of damaged rolling bearing elements, as seen in studies like [17, 44]. The dataset captures motor current signals of an electromechanical drive system and vibrations of the housing [70]. These signals are extracted using existing frequency inverters, eliminating the need for additional sensors, which was necessary in the CWRU bearing experiments. This results in more resource-efficient and cost-effective experimentation. A unique feature of the method is its ability to monitor damages in external bearings positioned within the drive system but outside the electric motor.\nIn total, the data derived from the experiments represents \u201chealthy\u201d \"real damaged\u201d, and \u201cartificially damaged\" bearings. The data is recorded for approximately 4 seconds at a sampling rate of 64 kHz which produces numerous data files containing around 256,000 data points each. For this study, we focused solely on the \u201creal damaged\u201d experiments, specifically targeting the \u201cinner race\u201d faults, encompassing 8 distinct conditions as highlighted by [70].\nThis resulted in 80,000 sequences (10,000 for each condition), necessitating substantial computational resources to run various CNN configurations. To manage this, we took an equally divided random sample from the dataset, retaining 10%, or 1,000 sequences per condition. Similar to other datasets, a train/test split of 20% and 80% was implemented."}, {"title": "3.1.3. Gearbox", "content": "The Gearbox dataset originates from the necessity to optimise and analyse industrial gearboxes, particularly in applications like wind turbines where gearbox failures can lead to significant downtime [71]. Previous research has shown that downtime due to gearbox failures is higher compared to other components. The most common approach for examining faults in wind turbine gearboxes involves recording and analysing vibration signals, which are inherently non-stationary. The Gearbox dataset was created using the SpectraQuest Gearbox Fault Diagnostics Simulator, enabling researchers to simulate the behavior of an industrial gearbox for diagnostics and prognostics research [72]. The dataset includes vibration signals from four sensors placed in various directions around the gearbox. The experiments were conducted under varying load conditions, ranging from 0% to 90%. The dataset encompasses two primary conditions: a healthy gearbox and one with a broken tooth. We combined all load variations into a binary classification task, distinguishing between healthy and broken gearboxes. This resulted in 978 sequences, each containing 2048 data points. The dataset is balanced, with 492 sequences labeled as healthy and 486 as broken. For our study, we focused on these binary conditions and implemented a train/test split of 20/80% to make the task more challenging and to reduce computation times. Additionally, the dataset allows for the separation of each operating condition combined with their respective gearbox condition, offering a nuanced perspective on the data.\nThis comprehensive dataset provides a robust foundation for analysing and improving fault detection in industrial gearboxes, particularly in high-stakes applications like wind turbines. The detailed setup and balanced data ensure that the classification task, although simplified by the train/test split, remains a challenging and informative endeavor for network training and evaluation."}, {"title": "3.1.4. MFPT Dataset", "content": "The Society for Machinery Failure Prevention Technology (MFPT) dataset [73] is a widely used dataset for studying faults in machinery. This dataset comprises 7 outer race faults, 7 inner race faults, and a healthy baseline condition, resulting in a total of 15 unique classes. The data was collected under various load conditions to provide a comprehensive overview of fault scenarios.\nSequences of 2048 data points were created from one of the vibration sensors. Fault conditions were sampled at a frequency of 48.828 kHz, yielding 71 sequences for each fault type. In contrast, the healthy condition was sampled at a higher frequency of 97.656 kHz, resulting in an additional 858 sequences. This discrepancy in the number of sequences per condition makes the dataset inherently unbalanced.\nThe unbalanced nature of the dataset poses a challenge for machine learning algorithms, as it requires strategies to handle the imbalance effectively. This dataset provides a rich source of information for the development and evaluation of fault detection algorithms, making it an essential tool for researchers in the field of machinery failure prevention. The diversity of fault conditions and the high sampling rates ensure that the dataset can be used to test a wide range of diagnostic methods."}, {"title": "3.1.5. XJTU", "content": "The XJTU-SY bearing datasets are provided by the Institute of Design Science and Basic Component at Xi'an Jiaotong University (XJTU) and the Changxing Sumyoung Technology Co. The datasets contain complete run-to-failure data of 15 rolling element bearings that were acquired by conducting many accelerated degradation experiments [74]. In this case, we chose to extract the last 30 minutes of every experiment containing the fault occurrence, and potentially already some vibration deviations prior to this fault. Therefore, sampling an equal length of data, containing the fault would be an appropriate strategy. Since the data are retrieved from a remaining useful life (RUL) experiment, the datasets are imbalanced in general, and also until the fault occurs, they are somewhat similar. The experiments contain two time series making it a multivariate use-case. The data are sampled at a 25.6 kHz where the sampling frame was 1.28 seconds within every minute, resulting 32,768 data points per minute. This results in 480 sequences for every of the 15 different fault conditions, totaling the dataset with 7,200 sequences. Due to the vast amount of data, we can state that this dataset is large."}, {"title": "3.1.6. UoC", "content": "The dataset retrieved from the University of Conneticut (UoC) [75] provides a gear fault dataset that measures vibrations with the use of accelerometers. the data are collected with a sampling frequency of 20 kHz. It only consists of a single sensor, therefore making it a single time series channel, i.e., univariate time series. In total, the dataset contains eight different gear fault conditions accompanied by one healthy gear condition, resulting in a multi-class classification task with 9 unique conditions. The following conditions are gathered during the experiments; healthy condition, missing tooth, root crack, spalling, and chipping tip with 5 different levels of severity [76]. The dataset is balanced for all conditions. We segmented the data into sequences of 2048 data points per sequence, giving 182 sequences per condition, amounting to a fairly small dataset."}, {"title": "3.1.7. SEU", "content": "The Southeast University (SEU) in China has developed two sub-datasets for their gearbox datasets, both aimed at providing insights into the health of bearings and gearboxes [77]. Data collection was accomplished through a Drivetrain Dynamics Simulator, capturing eight channels of vibration data. The data encompasses five different conditions: one healthy state and four fault states, all under two operational conditions defined by rotational speed and load: 20 Hz-0 V and 30 Hz-2 V. We specifically utilised the bearing-related data, extracting three vibration channels (channels 2, 3, and 4) corresponding to the x, y, and z directions of the planetary gearbox, thus forming multivariate time series data. To increase the complexity within every fault condition, we merged the data from both rotational speeds and loads. The full recordings were employed, including the initial start-up phase. Overall, the dataset consists of 5110 sequences of 2048 data points each, which translates to 1022 sequences per class, ensuring a balanced dataset across all conditions."}, {"title": "3.2. Method Pt. 1 Experiments on Alternatives to Wide-Kernel CNN Architecture", "content": "A wide-kernel CNN architecture takes centre stage in our research however we also want to say that hyperparameters are an important and unavoidable decision for other neural networks too. Therefore, in addition to the wide-kernel CNN which is our focus we also perform limited experiments"}, {"title": "3.2.1. Long Short Term Memory (LSTM) Architecture", "content": "Following the example of [78], we consider a bearing fault classifier built around Long Short Term Memory (LSTM) layers. The central idea of an LSTM layer is that it passes information forward between steps when processing a sequence stepwise - making it possible to \u2018remember' things seen earlier - and yet it also allows the internal flow of information to be flushed out [79]. Strategically flushing out the internal information deals with the problems of vanishing and exploding gradients that previous kinds of recurrent network suffered from, with the upshot being that LSTMs are more successful at handling longer sequences and performing a wide range of tasks.\nLike [78], we situate LSTM layers in an architecture that can be imagined as a structure with three main chambers. The first two components are CNNs which proceed in parallel and then they both pass into the third main component which is the LSTM itself. Two CNNs are used in order to achieve a \"multi-scale\" effect. A CNN containing smaller kernels gives prominence to brief or high-frequency vibrations in the signal. Features of the signal that develop more slowly are emphasised in a second CNN consisting of larger kernels. The patterns that emerge from both CNNs then proceed into the"}, {"title": "3.2.2. Transformer Architecture", "content": "We also consider a transformer architecture, following the example set by [50] for bearing fault analysis. Transformers use a so-called 'attention' mechanism which allows them to process each step in a sequence by pooling information from other steps [80]. The innovation of using attention provides an efficient alternative to recurrent connections, and the booming popularity of transformers in applications like ChatGPT testifies to their efficacy."}, {"title": "3.2.3. Hyperparameter Search on Other Architectures", "content": "When studying the LSTM and transformer networks we restrict ourselves to drawing some generic conclusions about the impact of (mis)tuning hyperparameters. As such, we do not find it necessary to perform a full grid search, but instead to rely on a reduced sample from a grid of possibilities. We select 100 configurations uniformly at random from a larger collection of possible hyperparameter combinations.\nFor the LSTM-based neural network, the values we consider for each hyperparameter are shown in Table 1. The middle value for each hyper"}]}