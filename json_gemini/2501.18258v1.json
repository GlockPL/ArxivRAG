{"title": "PDE-DKL: PDE-CONSTRAINED DEEP KERNEL LEARNING IN HIGH DIMENSIONALITY", "authors": ["Weihao Yan", "Christoph Brune", "Mengwu Guo"], "abstract": "Many physics-informed machine learning methods for PDE-based problems rely on Gaussian processes (GPs) or neural networks (NNs). However, both face limitations when data are scarce, and the dimensionality is high. Although GPs are known for their robust uncertainty quantification in low-dimensional settings, their computational complexity becomes prohibitive as the dimensionality increases. In contrast, while conventional NNs can accommodate high-dimensional input, they often require extensive training data and do not offer uncertainty quantification. To address these challenges, we propose a PDE-constrained Deep Kernel Learning (PDE-DKL) framework that combines DL and GPs under explicit PDE constraints. Specifically, NNs learn a low-dimensional latent representation of the high-dimensional PDE problem, reducing the complexity of the problem. GPs then perform kernel regression subject to the governing PDEs, ensuring accurate solutions and principled uncertainty quantification, even when available data are limited. This synergy unifies the strengths of both NNs and GPs, yielding high accuracy, robust uncertainty estimates, and computational efficiency for high-dimensional PDEs. Numerical experiments demonstrate that PDE-DKL achieves high accuracy with reduced data requirements. They highlight its potential as a practical, reliable, and scalable solver for complex PDE-based applications in science and engineering.", "sections": [{"title": "1 Introduction", "content": "High-dimensional partial differential equations (PDEs) are indispensable for modelling complex dynamical systems across science and engineering. Unfortunately, traditional numerical methods (e.g., finite element [1], finite difference [2], finite volume [3], and spectral methods [4]) often become computationally intractable in high dimensions and"}, {"title": "1.1 Related work", "content": "In recent years, progress has been made in the numerical approximation of high-dimensional PDEs using DNNs. Examples include early approaches combining NNs with collocation points [24, 25], physics-informed methods that incorporate PDE residuals into the loss function [6, 26], and hybrid approaches combining NNs with classical methods, such as finite elements [27, 28]. Moreover, many alternative methods have emerged to address the limitations of traditional approaches. These include the deep Galerkin method [29], which demonstrates effectiveness in high-dimensional problems under regularity conditions, the deep Ritz method [30] and its extensions for non-convex domains [31], as well as neural operators [32, 33, 34] that learn mappings between infinite-dimensional function spaces. While these methods have shown promise in various applications, most struggle to efficiently solve high-dimensional problems due to their inherent complexity and high computational demands [35]. The inherent spectral bias of NNs their tendency to prioritize learning low-frequency functions fundamentally limits their ability to approximate high-frequency modes critical for resolving sharp gradients in high-dimensional PDEs [36, 37]. A recent study [38], which explores more than ten standard and variant PINN methods, highlights high-dimensional problems that remain challenging for PINNs.\nSeveral efforts to address the computational challenges of high-dimensional systems. For instance, the deep splitting method [39] was introduced for the high-dimensional systems, which iteratively solves the linear approximation of the PDE's solution by combining DNN and the Feynman-Kac formula. Beck and Berner et al. [40, 41] also successfully applied the DNN approximation to high-dimensional Kolmogorov PDEs. Regarding machine learning approaches to enhance computational efficiency, Nabian et al. [42] enhanced the computational efficiency of randomised NNs by training a deep residual network on residual data. More recently, Wang et al. [43] extended the extreme learning machine approach using an approximate variant of the theory of functional connections (TFC), showing promise in efficiently solving high-dimensional PDEs. However, despite these advances, most methods still face challenges in efficiently handling complex, high-dimensional problems, especially when ensuring robustness and uncertainty quantification."}, {"title": "1.2 Contributions", "content": "Our work has made contributions to the data-driven numerical approximation of high-dimensional PDEs in the following aspects:\n1. Modelling with limited data: The proposed method effectively models systems with limited observational data, especially for high-dimensional problems. This is particularly useful in real-world applications where data availability is constrained.\n2. Overcoming the curse of dimensionality: Our research addresses the curse of dimensionality in the numerical approximation of high-dimensional PDEs, a challenge that often leads to an exponential increase in computational costs for traditional methods. Such exponential growth is effectively avoided using our method.\n3. Enhancements in initialisation and optimisation strategies: Our method guarantees computational efficiency by incorporating meticulous initialisation and tailored optimisation strategies. This results in accuracy and improved computational performance.\n4. Reliability and uncertainty quantification (UQ): Our method harnesses the synergies of DNNs and GPs to enhance accuracy and uncertainty quantification in diverse PDEs. Through a Bayesian framework, the GP component provides built-in uncertainty analysis encoded in the posterior distributions, addressing challenges where modelling uncertainty is often overlooked in conventional DL methods. While the framework is particularly effective for solving linear PDEs, it can also be extended to a wider range of computational tasks."}, {"title": "1.3 Structure of the paper", "content": "The structure of the paper is as follows: After the introduction, Sections 2 and 3 cover the proposed methodology, including the problem formulation, PDE-constrained GPs, and PDE-constrained DKL. Section 4 presents our model's numerical results, concluding with a discussion of findings. Finally, Section 5 provides concluding remarks by summarising our work and suggesting directions for future research."}, {"title": "2 Problem formulation and solution methods", "content": "This section introduces the basics of GP regression, the problem formulation considered in this work, and the corresponding GP approximations."}, {"title": "2.1 Gaussian processes for regression", "content": "In this work, the core idea is to use a GP to describe a distribution over the function space of PDE solutions. In GP regression, the prior of the function to be approximated, denoted by $g : Z \\rightarrow R$ with $Z$ being the input domain, is assumed to be a GP, i.e., any finite number of the function outputs follow a joint normal distribution. This GP, written as $g(\\cdot) \\sim GP(m(\\cdot), k(\\cdot, \\cdot))$, is uniquely defined by its mean function $m$ and covariance function $k$, respectively given by\n$m(z) = E[g(z)]$, and $k (z, z') = E [(g(z) - m(z)) (g (z') - m (z'))]$, $z, z' \\in Z$.\nWith this setting, supervised learning is conducted based on a dataset of $N$ input-output pairs $(Z, y) = \\{(z^{(i)},y^{(i)})\\}_{i=1}^N$ in which the observed responses may be corrupted by white noise, i.e., $y^{(i)} = g (z^{(i)}) + \\epsilon_i$ with $\\epsilon_i \\sim N (0, \\sigma^2_z)$. Thus, the covariance matrix for the vector of observational outputs $y$ is $k(Z, Z) + \\sigma^2I$.\nConsider any unseen test point $z^* \\in Z$, the joint distribution of the observed and (noise-free) test outputs of $g$ is given by the GP as\n$\\begin{pmatrix} y \\\\ g(z^*) \\end{pmatrix} \\sim N \\begin{pmatrix} \\begin{pmatrix} m(Z) \\\\ m(z^*) \\end{pmatrix}, \\begin{pmatrix} k(Z, Z) + \\sigma^2I & k(Z, z^*) \\\\ k(z^*, Z) & k(z^*, z^*) \\end{pmatrix} \\end{pmatrix}.$\nTo update the prior GP to agree with observations, the rule of conditional Gaussian gives that the predictive distribution of $g(z^*)$ is Gaussian again, i.e., $g(z^*) | (Z, y) \\sim N (\\mu^*, v^*)$, in which\n$\\mu^* = m (z^*) + k (z^*, Z) (k(Z, Z) + \\sigma^2I)^{-1}(y - m(Z))$, and\n$v^* = k (z^*, z^*) - k (z^*, Z) (k(Z, Z) + \\sigma^2I) ^{-1}k (Z, z^*) .$\nOne significant advantage of GP regression is that the probabilistic predictive model has analytical formulations."}, {"title": "2.2 Model problem for high-dimensional PDES", "content": "We consider the spatial-temporal-parametric coordinates $q = (x, t, \\mu) \\in \\Omega \\times T \\times M := Q$, with $\\Omega \\subset R^{n_x}$, $T \\subset R$, and $M \\subset R^{n_\\mu}$ respectively denoting the space, time, and parameter domains, and $Q \\subset R^{n_q}$ their joint domain in $n_q = n_x + 1 + n_\\mu$ dimension. The high-dimensional forward problems considered in this work are governed by spatial-temporal-parametric PDEs generally written in the following form:\n$\\frac{\\partial u(q)}{\\partial t} = L_x[u(q); \\mu] + f(q)$, with $L_x[\\cdot; \\mu] := \\sum_{\\alpha \\in N^n} C_\\alpha(x; \\mu) \\frac{\\partial^{|\\alpha|}}{\\partial x^{\\alpha}} ,$\nequipped with proper boundary and initial conditions. Specifically, $u : Q \\rightarrow R$ is the solution field, and $L_x [\\cdot; \\mu]$ is a linear differential operator over space parametrized by $\\mu$. We combine $L_x$ and the time derivative $\\partial_t$, both being linear operators acting on the solution field $u$, into a spatial-temporal parametric linear operator $A[u] := \\partial_t u - L_x[u; \\mu]$. Therefore, the governing equation (4) is rewritten as follows:\n$A[u(q)] = f(q),$\nwhere $f: Q \\rightarrow R$ is a given function for a forcing term."}, {"title": "2.3 Constraining Gaussian processes with the PDEs model", "content": "We adopt the aforementioned GP regression model as a surrogate for the latent solution function $u$. In this subsection, based on the fact that Gaussianity is preserved under linear operations, we discuss constraining GP emulation with the model problem's governing PDE."}, {"title": "3 Deep kernel learning with PDE constraints", "content": ""}, {"title": "3.1 Construction of a deep kernel", "content": "The choice of kernel in GP regression is crucial as it directly affects the performance of function approximation. In this work, to improve the kernel's expressive power in high-dimensional input space, we use a manifold GP [22] in which a multilayer perceptron NN is embedded into a conventionally used kernel. In particular, we consider the following squared exponential kernel with automatic relevance determination (ARD) as the 'base kernel':\n$k_{ARD} (h, h'; \\theta) = \\sigma^2 exp \\Big(-\\frac{1}{2} (h - h')^T diag (\\ell^{-2}) (h - h')\\Big);$\ndefined in $R^{n_h}$, i.e., $h, h' \\in R^{n_h}$. The hyperparameters $\\theta = \\{\\sigma^2, \\ell\\}$ in this kernel include a global variance $\\sigma^2$ and individual lengthscales collected in $\\ell = \\{\\ell_1, \\ell_2,\\ldots, \\ell_{n_h} \\}^T$, each capturing the variability along a specific dimension. Note that this ARD kernel is smooth, anisotropic, and infinitely differentiable, and the so-called Mahalanobis distance [51] is measured.\nWe further compose this base kernel with an L-layer NN $h^L(\\cdot; \\omega) : Q \\rightarrow R^{n_h}$, whose inputs are the PDE coordinates $q$, and the last layer has $n_h$ units (Figure 1). Here, $\\omega$ denotes a vector that collects the NN's parameters (i.e., weights and biases). Therefore, the composed kernel often referred to as a 'deep kernel' [21], can be written as\n$k_{DKL} (q, q'; \\theta, \\omega) = \\sigma^2 exp \\Big(-\\frac{1}{2} (h^L(q; \\omega) - h^L(q'; \\omega))^T diag (\\ell^{-2}) (h^L(q; \\omega) - h^L(q'; \\omega))\\Big),$\nby taking $h = h^L (q; \\omega)$ and $h' = h^L (q'; \\omega)$, i.e., the NN outputs at the input values $q$ and $q'$, respectively.\nBy incorporating the NN, we upgraded a stationary kernel to a non-stationary one, significantly improving nonlinear expressiveness and scalability to high-dimensional function approximation. On the other hand, the subsequent \u2018GP layer\u2019 facilitates NN modelling with uncertainty quantification.\nHigh-dimensional function approximation is known to be challenging for GP regression, as the latter often suffers from the curse of dimensionality when conventional kernels are used. A strategy to overcome this is to find a faithful data representation in a low-dimensional latent space. Our treatment is aligned with this spirit. When the input domain Q has a high dimensionality $n_q$, the use of a \u2018deep kernel\u2019 allows us to transform the size-$n_q$ input data into a $n_h$-dimensional latent space through a NN; in fact, this is a nonlinear dimensionality reduction of the inputs, or referred to as a sensitivity analysis. Following this, a GP regression with the ARD kernel is conducted in the $n_h$-dimensional latent space. When $n_h$ is sufficiently small, the GP regression will have guaranteed performance and does not suffer from the curse of dimensionality."}, {"title": "3.2 Constraining deep kernel with PDES", "content": "When the aforementioned 'deep kernel' is used to approximate the solution field $u$ of a high-dimensional PDE problem (5), (i.e., $n_q > 1$), the regression through deep kernel learning in subsection 3.1 needs to be constrained by (5), so that information from the governing physics is integrated into the DL model. As the deep kernel learning method still falls under the realm of GP regression, this allows us to integrate PDE constraints (5) into the deep kernel (14), treated similarly with subsection 2.3.\nIn this case, the latent solution $u$ is assumed to follow a prior GP induced by a deep kernel, i.e., $u_{DKL}(q) \\sim GP(M_{DKL}(q), K_{DKL} (q, q'))$, where $M_{DKL}(q)$ represents the mean function of the GP and is typically set to zero (or another appropriate prior mean if available). The covariance function $K_{DKL} (q, q')$ is defined by the deep kernel, which is constructed through a NN architecture, effectively learning a data-driven similarity measure between inputs $q, q'$. Similar to (8), we can reconstruct the forcing term $f$ by applying a linear operation $A$ on the latent GP $u_{DKL}$. Therefore, the forcing term $f$ can be reconstructed as $f_{DKL}(q) = A[u_{DKL}(q)] \\sim GP[f] (A M_{DKL} (q), A K_{DKL} (q, q') A^T).$"}, {"title": "3.3 Model training", "content": "The proposed DKL model includes several trainable parameters, denoted as ${\\theta,\\omega}$. In our approach, effective model training should not only be robust to high-dimensional data but also ensure strong generalisation performance.\nOur goal is to leverage both data pairs, $(Q_u, y_u)$ and $(Q_f, y_f)$, to jointly perform inference and prediction of the solution. Specifically, we first use Bayesian optimisation to determine the optimal dimensionality of the latent space $h^L$. Following this, we apply type-II marginal likelihood (18) as the loss function to jointly learn all model parameters ${\\theta,\\omega}$.\nBayesian optimisation In this work, we expect to extract a low latent dimensionality from the data for the model inputs q, i.e., a nonlinear sensitivity analysis can effectively reduce the input dimensionality. Hence, in our DKL model, the reduced inputs for GP emulation are the values $h^L(q)$ at the last network layer. A smaller size of $h^L$ can save the GP with the base kernel from the curse of dimensionality. To determine the optimal latent dimensions, we tune the hyperparameters for the size of the last NN layer. Given the high computational cost of multiple training iterations, traditional methods like random or grid search are impractical. Therefore, we employ Bayesian optimisation to search for the optimal value of $n_h^L$ efficiently."}, {"title": "4 Numerical Results", "content": "In this section, we present numerical results across four benchmark problems to demonstrate the effectiveness of the proposed method in solving high-dimensional forward PDE problems while simultaneously providing meaningful"}, {"title": "4.1 Parametric heat equation", "content": "In this section, we consider the following parametric, time-dependent heat equation with boundary and initial conditions:\n$\\begin{cases} \\frac{\\partial u(x, t, \\mu)}{\\partial t} = \\mu_1 \\frac{\\partial^2 u(x, t, \\mu)}{\\partial x^2} = f (x, t, \\mu) & \\text{in } \\Omega \\times [0, T], \\\\ u(0, t, \\mu) = 0 & \\text{in } [0, T], \\\\ u(1,t, \\mu) = \\mu_3e^{-41t} sin (2\\pi \\mu_2t) & \\text{in } [0, T], \\\\ u(x, 0, \\mu) = \\mu_3 sin (2\\pi \\mu_2x) & \\text{in } \\Omega, \\end{cases}$\nwhere $\\Omega = (0,1) \\subset R$ is the spatial domain, $T = 1$ is the final time, and the parameter domain is given by $\\mu = {\\mu_1, \\mu_2, \\mu_3}^T \\in M = [0.8, 1.2] \\times [0.7, 1.3] \\times [0.9, 1.1]$. The forcing term is defined to be $f (x, t, \\mu) = \\mu_3e^{-41t} sin (2\\pi \\mu_2x) (-\\mu_1 + 4\\pi^2\\mu_2)$. In this case, the exact solution to this initial boundary value problem is $u (x, t, \\mu) = \\mu_3e^{-41t} sin (2\\pi \\mu_2x)$.\nTo evaluate the proposed model's performance, we apply the PDE-constrained Gaussian process (PDE-GP) and PDE-constrained deep kernel learning (PDE-DKL) models, comparing their capabilities in predicting the solution term $u$ and reconstructing the forcing term $f$."}, {"title": "4.2 High-dimensional Poisson equation", "content": "In this section, we evaluate the performance of the proposed method by solving the high-dimensional Poisson equation, which is fundamental in classical mathematical physics and plays a critical role in modelling various phenomena across physics, engineering, and data science. We focus on dimensions $d = 10$ and $d = 50$ to assess the method's potential in high-dimensional settings. The Poisson equation is formulated as\n$\\begin{cases} \\Delta u(x) = f(x) & \\text{in } \\Omega = (0,1)^d, \\\\ u(x) = g(x) & \\text{on } \\partial \\Omega, \\end{cases}$\nwhere $x = (x_1,x_2,...,x_d) \\in R^d$ spans a $d$ -dimensional hypercube domain $\\Omega \\supset [0, 1]^d$, and $\\Delta$ denotes the Laplacian operator in $d$ dimensions. In this test, we set the source term $f(x)$ and the boundary condition $g(x)$ as follows: $f(x) = -d sin (1^T x) - d cos (1^T x)$ for $x \\in \\Omega = [0, 1]^d$, and $g(x) = sin (1^T x) + cos (1^T x)$ for $x \\in \\partial \\Omega$. Here, 1 denotes a vector whose entries are all 1.\nThis choice of $f(x)$ ensures that the solution exhibits non-trivial behaviour across all dimensions. In this case, the exact solution of the Poisson equation (22) is: $u(x) = sin (1^T x) + cos (1^T x)$.\nFigure 3 compares the performance of PDE-GP and PDE-DKL in solving the ten-dimensional Poisson equation. Subplots (a) and (b) depict scatter plots of the predicted values of $u$ versus the true values for PDE-GP and PDE-DKL, respectively. Subplots (c) and (d) illustrate the reconstruction of the source term $f$ by both methods. Ideal prediction accuracy corresponds to points lying along the diagonal line. Error bars represent the standard deviation, indicating the prediction uncertainty.\nThe results demonstrate that PDE-DKL outperforms PDE-GP in predicting $u$ and reconstructing $f$ for the ten-dimensional Poisson equation (Figure 3). PDE-DKL's predictions align closely with the ideal diagonal line, indicating higher accuracy and reduced variance than PDE-GP. Despite using identical training dataset sizes, PDE-DKL achieves superior prediction accuracy while significantly reducing computational requirements. This efficiency makes PDE-DKL preferable for high-dimensional applications with limited data availability. Additionally, from a practical point of view, PDE-GP requires substantially more memory due to the need to compute and store large covariance matrices. In contrast, PDE-DKL efficiently handles high-dimensional data with reduced memory consumption."}, {"title": "4.3 High-dimensional heat equation", "content": "We extend our analysis to time-dependent problems to further evaluate the proposed model by considering the high-dimensional heat equation in spatial dimensions $d = 10$ and $d = 50$. The equation under consideration is given by:\n$\\begin{cases} \\partial_t u(x,t) - \\Delta u(x,t) = f(x,t) & \\text{in } \\Omega \\times [0, T], \\\\ u(x,t) = g(x,t) & \\text{on } \\partial \\Omega \\times [0, 1], \\\\ u(x, 0) = h(x) & \\text{in } \\Omega, \\end{cases}$\nwhere $T = 1$ represents the final time. The source term is defined as $A_q[u(q)] = f(x,t) = (-1 + \\frac{1}{2}) e^{-t} cos (1^T x)$, the boundary condition as $g(x, t) = e^{-t} cos (1^T x)$, and the initial condition as $h(x) = cos (1^T x)$. To satisfy the heat equation (23), along with the specified source term, boundary, and initial conditions, the exact solution is given by $u(x, t) = e^{-t} cos (1^T x)$.\nThe results in Figure 5 indicate that PDE-DKL provides more accurate predictions for $u$ and $f$ than PDE-GP in the ten-dimensional case. Specifically, the predictions from PDE-DKL align more closely with the true values, as evidenced by the clustering of points along the diagonal line in subplots (b) and (d). Additionally, the error bars for PDE-DKL are smaller, indicating lower uncertainty in the predictions. In contrast, PDE-GP exhibits larger variances and less accurate predictions, suggesting limitations in scalability to higher dimensions."}, {"title": "4.4 High-dimensional advection-diffusion-reaction equation", "content": "In addition, we evaluate the proposed model using the high-dimensional advection-diffusion-reaction equation. This equation incorporates additional complexities by accounting for both advection and reaction processes. The equation is formulated as:\n$\\begin{cases} \\partial_t u(x, t) - \\Delta u(x, t) + \\bar{1}\\nabla u(x, t) + u(x,t) = f(x, t) & \\text{in } \\Omega \\times [0, 1], \\\\ u(x,t) = g(x,t) & \\text{on } \\partial \\Omega \\times [0, 1], \\\\ u(x, 0) = h(x) & \\text{in } \\Omega. \\end{cases}$\nThe spatial domain is defined as $\\Omega = (0,1)^d \\subset R^d$, with two test cases considered for $d = 10$ and $d = 50$. The source term is given by: $A_q[u(q)] = f(x,t) = e^{-t} (cos (1^T x) + sin (1^T x))$, the boundary condition is $g(x,t) = e^{-t} sin (1^T x)$, and the initial condition is $h(x) = sin(x)$. The exact solution that satisfies the equation (24) under these conditions is: $u(x, t) = e^{-t} sin (1^T x)$.\nWe focus on the PDE-GP and PDE-DKL methods to a 10-dimensional (7) and 50-dimensional (8) advection-diffusion-reaction equation."}, {"title": "4.5 Discussions", "content": "Through four carefully selected benchmark problems spanning different classes of PDEs, PDE-DKL demonstrates promising success in mitigating the curse of dimensionality. Our experiments indicate that DNN-learned latent representations enable GP regression to maintain prediction accuracy (relative L2 error < 5%) even under limited training data. This synergy effectively addresses individual limitations, delivering accurate predictions and providing reliable uncertainty estimates. It highlights how DL's computational power can be successfully combined with the theoretical rigour of GP approaches. The encouraging performance observed here suggests that future research could extend this framework to even more challenging PDEs, further leveraging advanced DL techniques for enhanced problem-solving in high-dimensional settings."}, {"title": "5 Concluding remarks", "content": "Machine learning models constrained by physics (including those based on NNs and GPs) often face significant challenges when tackling high-dimensional PDEs with sparse data. The curse of dimensionality typically demands a prohibitive amount of training data, which grows exponentially with the number of dimensions. In practice, such an extensive dataset is rarely feasible in real-world applications. Our observations suggest that NNs and GPs should be regarded as complementary methods rather than alternatives in tackling these challenges. Conventional NNs excel at discovering low-dimensional manifolds in high-dimensional data but lack uncertainty quantification. By contrast, GPs can provide confidence estimates for each prediction, yet they struggle when scaling to extremely high dimensions.\nTo address these issues, we present a PDE-constrained statistical learning framework that synergistically combines GPs with DNNs for high-dimensional PDEs. The DNN component effectively identifies lower-dimensional manifolds, facilitating GP predictions in a reduced latent space and thus leveraging the complementary strengths of both. This synergy yields consistently accurate predictions while offering reliable uncertainty estimates, demonstrating robustness"}]}