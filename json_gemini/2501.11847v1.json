{"title": "A Survey on Memory-Efficient Large-Scale Model Training in AI for Science", "authors": ["Kaiyuan Tian", "Linbo Qiao", "Baihui Liu", "Gongqingjian Jiang", "Dongsheng Li"], "abstract": "Scientific research faces high costs and inefficiencies with traditional methods, but the rise of deep learning and large language models (LLMs) offers innovative solutions. This survey reviews LLM applications across scientific fields such as biology, medicine, chemistry, and meteorology, underscoring their role in advancing research. However, the continuous expansion of model size has led to significant memory demands, hindering further development and application of LLMs for science. To address this, we review memory-efficient training techniques for LLMs based on the transformer architecture, including distributed training, mixed precision training, and gradient checkpointing. Using AlphaFold 2 as an example, we demonstrate how tailored memory optimization methods can reduce storage needs while preserving prediction accuracy. We also discuss the challenges of memory optimization in practice and potential future directions, hoping to provide valuable insights for researchers and engineers.", "sections": [{"title": "I. INTRODUCTION", "content": "THE rapid advancement of artificial intelligence, particularly large language models (LLMs), has positioned deep learning (DL) as a vital tool for addressing various scientific challenges. DL offers a new approach to scientific research due to its strengths in automatic feature extraction and complex pattern recognition. According to the universal approximation theorem, deep neural networks (DNNs) can effectively fit any continuous function. Additionally, the high-performance parallel implementation of the backpropagation algorithm on GPUs enables DL to efficiently solve scientific and engineering problems [1]. In disciplines such as biology, medicine, chemistry, and climate science, DL models demonstrate great potential to assist researchers in accelerating scientific discoveries, enhancing prediction accuracy, and providing innovative solutions to complex challenges.\nIn Biological Sciences, it has proven effective in areas such as protein structure and function prediction, as well as genetic engineering. One notable example is AlphaFold 2 [2], which uses DL to predict the three-dimensional structures of proteins. In the medical field, LLMs have been applied to various tasks, including medical Q&A, disease prediction, and clinical decision support. For instance, the Med-PaLM series of models [3], [4], [5] can answer questions posed by medical professionals and assist doctors in diagnosing diseases and determining treatment options. In chemistry, DL is employed for tasks such as molecular generation [6], [7], molecular property prediction [8], and chemical reaction prediction [9]. In meteorology, by processing vast amounts of meteorological data, DL models can reveal the complex laws of weather change, improve the accuracy of weather prediction, simulate and predict a wide range of climate patterns [10], [11], [12], and predict the occurrence and impact of extreme weather events [13], [14]. With the improvement of computing power and the continuous progress of DL algorithms, the application of DL in scientific fields will be more extensive and in-depth, further expanding the boundaries of scientific exploration.\nMeanwhile, studies have shown that with the increase of parameters, the model shows better performance [15], [16]. Consequently, the scale of these models has continued to grow, leading to memory usage issues in DL models. In recent years, the widespread development and application of transformer models [17] have led to a significant increase in model size, averaging a 240-fold growth every two years. The expansion of parameters in models like GPT-3 [18] and Llama 2 [19] has outpaced the growth of GPU memory. The training of large-scale models inevitably encounters the problem of the \"memory wall\" [20] (see Fig. 1).\nThere are a few related surveys summarized memory-efficient LLM training methods. Zhuang et al. [21] have provided an overview of the efficient training technologies used for transformer-based models. They discussed some memory-efficient training methods and summarized the key factors that affect training efficiency. To gain a deeper understanding of the latest advancements in efficient LLMs, Wan et al. [22] conducted a thorough literature review and proposed a taxonomy to organize the relevant research. In contrast to these studies, this survey focuses on the application of LLMs across different scientific fields, systematically introducing general memory-efficient training methods for transformer-based large-scale scientific models. Furthermore, we use a case study to demonstrate studies aimed at optimizing memory"}, {"title": "II. LARGE LANGUAGE MODELS AND AI FOR SCIENCE", "content": "With the success of LLMs in NLP, an increasing amount of research is exploring its application to scientific fields. As a powerful tool for knowledge expression and reasoning, LLMs are capable of managing complex high-dimensional data, uncovering deep patterns, and enhancing the speed and accuracy of scientific discoveries. This section will focus on the use of LLMs in common tasks across different scientific disciplines. By reviewing these applications, we aim to illustrate the broad applicability and impressive capabilities of LLMs in diverse areas of science, as well as the necessity of memory-efficient training techniques."}, {"title": "A. A Brief Introduction to Deep Learning and LLMs", "content": "Deep learning is a machine learning approach that automatically extracts features from complex data through multiple layers of nonlinear transformations. In contrast to traditional machine learning techniques, deep learning offers several advantages, including no need for manual feature engineering, the ability to adapt to complex high-dimensional data, and superior generalization capabilities. In recent years, the emergence of transformer models has further enriched this field.\nLLMs are DNN-based language models characterized by a massive number of parameters. These models enhance performance by increasing both capacity and the volume of training data. Before the rise of deep learning, language modeling primarily relied on statistical methods, such as n-gram models and hidden Markov models. However, they struggled with data sparsity, making it challenging to handle long sequences and complex dependencies effectively. In 2003, Bengio et al. [23] introduced a neural network-based language model that sparked a wave of research into neural language models. Subsequently, Mikolov et al. proposed Word2Vec, which enhanced the representation of words through Skip-gram [24] and CBOW [25] methods. This innovation became a vital tool for feature representation in later natural language processing (NLP) research and applications. During this period, models like recurrent neural network (RNN) [26], long short-term memory network (LSTM) [27], and gated recurrent unit (GRU) [28] gained popularity for language modeling because they could better capture long-distance dependencies. Despite this progress, these approaches still encounter efficiency limitations when processing long sequences, and their training processes are challenging to parallelize.\nThe proposal of Transformer architecture [17] in 2017 marked a revolutionary breakthrough in the field of language modeling. Transformers use a multi-head self-attention mechanism rather than a recurrent structure, enabling them to efficiently capture global dependencies in long sequences and enhance model training speed. With these advancements, transformers quickly became the foundation of modern NLP models, driving the rapid evolution of LLMs. Models like BERT [29] and GPT [30] have emerged one after another, establishing pre-training and fine-tuning as an NLP paradigm. Subsequently, GPT-2 [31] and GPT-3 [18] expanded their parameter sizes to 1.5B and 175B, demonstrating the potential for large-scale pre-training, particularly in zero-shot and few-shot learning. The Scaling Laws [15], [16] reveal the relationship between the growth of model parameters, data size, computational complexity, and model performance, providing theoretical guidance for designing and training large-scale models. To further improve the performance of the model on tasks, researchers have also proposed methods such as instruction fine-tuning [32] and InstructGPT [33]. At the end of 2022, ChatGPT showcased the tremendous potential of LLM to the world and sparked a research boom in LLM.\nIn recent years, companies and research institutions have introduced a range of LLMs, advancing their scale and capabilities. Notable examples include but not limited to Google's PaLM series [34], [35], Meta's Llama series [36], [19], [37], OpenAI's GPT-4 [38], DeepSeek-AI's DeepSeek series [39], [40], [41], Tsinghua's GLM-4 [42], and Alibaba's Qwen series [43], [44]. So far, LLMs have exhibited exceptional proficiency in language generation, multimodal integration, and specialized tasks, providing substantial support for scientific research, industrial applications, and everyday life. As technological advancements continue, LLMs are expected to exert a significant influence across a wider range of domains."}, {"title": "B. Application of LLMs in Scientific Fields", "content": "With the success of LLMs in NLP, an increasing amount of research is exploring its application to scientific fields. As a powerful tool for knowledge expression and reasoning, LLMs are capable of managing complex high-dimensional data, uncovering deep patterns, and enhancing the speed and accuracy of scientific discoveries. This section will focus on the use of LLMs in common tasks across different scientific disciplines. By reviewing these applications, we aim to illustrate the broad applicability and impressive capabilities of LLMs in diverse areas of science, as well as the necessity of memory-efficient training techniques."}, {"title": "III. MEMORY-EFFICIENT TRAINING TECHNIQUES OF TRANSFORMERS", "content": "LLMs are evolving rapidly and find extensive applications across various domains. However, the training of these large-scale models imposes significant demands on computing power and memory of computing devices. In this context, it becomes essential to explore memory-efficient strategies for training large models. These strategies focus on optimizing memory utilization and making more efficient use of computing resources. While LLMs have gained considerable popularity in scientific fields and have produced noteworthy results, many works have adopted no or few optimizations to improve training efficiency and reduce memory overhead. In the rest of this section, we will delve into various memory-efficient training techniques specific to transformer-based LLMs."}, {"title": "A. Distributed Training", "content": "As deep learning models become larger in parameter and data size, it is becoming more challenging for a single computing device to fulfill training needs. Consequently, distributed training has emerged as one of the foundational technologies in deep learning. It involves distributing the training tasks across multiple devices (such as GPUs or nodes) for parallel processing. By employing effective task decomposition and data partitioning strategies, distributed training significantly enhances the efficiency and scalability of model training.\nAs the scale of models continues to increase, a single GPU's memory can no longer accommodate complete model parameters, optimizer states, and intermediate activations. Traditional training strategies for large-scale models may take weeks or even months. However, distributed training allows the model to be divided across multiple devices, enabling the training of extremely large models with limited single GPU memory and significantly reducing training time."}, {"title": "1) Data Parallelism", "content": "Data parallelism (DP) is a commonly used distributed training technique, particularly in environments with multiple GPUs and nodes. DP can significantly accelerate training process and improve overall training throughput. It involves replicating the model across computing devices. During training, each device is assigned a subset of the batched training data, known as a mini-batch, which is used as input for the model. Due to the different input data across model replicas, it is necessary to aggregate the gradients from all computing devices before updating model parameters. In terms of implementation, DP architectures can be categorized into parameter server architecture and decentralized architecture In parameter server architecture [83], devices are divided into parameter servers and workers. The parameter server stores the latest parameters of the model and aggregates gradients collected from workers to perform parameter updates. Each worker communicates with the parameter server during training to retrieve the latest parameters or upload gradients calculated using mini-batches.\nParameter servers are likely to become a communication bottleneck, as all workers need to communicate with servers frequently, especially in large-scale training scenarios. In contrast, decentralized architecture avoids this issue. In such a setup, computing device peers perform gradient aggregation through collective communications such as all-reduce, which enhances scalability. Moreover, the implementation of PyTorch DDP [84] leverages the overlap between computation and communication to further increase training throughput.\nDP divides the input training data along the batch dimension, which reduces the size of intermediate activations on each device during training and greatly improves training efficiency. However, its problem is also obvious. DP requires each device to hold a model replica, which is infeasible for large models that a single device cannot store.\nThe memory overhead during model training primarily comes from model states and intermediate activations. The model states can be further divided into optimizer states, gradients, and parameters. Assuming the number of parameters in the model is denoted as $P$, and utilizing Adam optimizer with FP16 mixed precision during training, the memory overhead of model parameters is $2P + 4P = 6P$ bytes. The memory overhead of gradients is $2P$ bytes, and optimizer states consume $4P + 4P = 8P$ bytes. Thus, the total memory overhead is $16P$ bytes. If training GPT-3 (175B), would require approximately 2.8TB of memory. To address the redundancy of model states, Rajbhandari et al. introduced Zero Redundancy Optimizer (ZeRO) [75], which shards redundant model states across devices, reducing memory overhead up to $\\frac{1}{N_d}$, where $N_d$ represents the number of devices. The core principle of ZeRO is that each device maintains only a fraction of the model states instead of a complete copy. The implementation of ZeRO has been integrated into Microsoft's open-source distributed training framework, DeepSpeed [85], making it easily accessible for researchers and developers.\nInspired by ZeRO, PyTorch developed Fully Sharded Data Parallel (FSDP) [81]. A report from Hugging Face indicates that the performance of PyTorch FSDP is comparable to that of DeepSpeed ZeRO. Since PyTorch v2.4, the PyTorch team has restructured FSDP and introduced FSDP2, adding additional optimizations to enhance performance."}, {"title": "2) Tensor Parallelism", "content": "The rapid increase in model parameters has rendered the memory and computing resources of a single device insufficient to support the training of the entire model. Although ZeRO can partition the model across computing devices, it does not reduce computational workloads on each device. For large models with vast parameters and high computational demands, relying solely on DP becomes inadequate for meeting the training needs. Model parallelism (MP) has emerged as a crucial technique for further improving the scale of deep learning models. MP primarily includes two techniques: tensor parallelism (TP) and pipeline parallelism (PP), which will be discussed in this section and the following section. TP, also referred to as intra-layer parallelism, finely divides the computation within each model layer into smaller tensor operations that can be executed concurrently across multiple devices, thereby effectively distributing both memory overhead and computational workload.\nMatrix multiplication can be divided into smaller operations by row or column, allowing the results to be calculated separately and then aggregated to obtain the final outcome. In line with this idea, Megatron-LM [76] has developed a tensor parallelism approach specifically designed for the transformer architecture. Taking the MLP layer as an example, it consists of two parameter matrices: $W_1$ with a shape of $(h, 4h)$ and $W_2$ with a shape of $(4h, h)$, here we denote hidden size as $h$. Initially, $W_1$ is partitioned column-wise, while $W_2$ is partitioned row-wise. This results in the parameter matrices stored on each device having the shapes: $W_1(h, 4h/N_d)$ and $W_2(4h/N_d, h)$. After forward pass on each device is completed, an all-reduce is performed to obtain the final result.\nMegatron-LM TP divides tensors into one-dimensional segments. While this approach reduces model parameters stored on each device to $\\frac{1}{N_d}$, it still needs to store complete intermediate activations on each device. Additionally, forward and backward computations of each transformer layer involve 4 all-reduce, causing significant communication overhead."}, {"title": "3) Pipeline Parallelism", "content": "Pipeline parallelism, also known as inter-layer parallelism, involves partitioning the model layers horizontally into multiple stages. Each stage consists of consecutive layers, which are assigned to different devices for sequential execution. Unlike TP, which divides intra-layer tensor calculations, PP partitions the model into stages, where each device transfers activations and gradients between adjacent stages during forward and backward passes. PP not only reduces memory demands and computational workloads on each device but also has less communication overhead. However, a major challenge associated with PP is device idle time (often referred to as pipeline bubbles). While asynchronous PP schemes [91], [92] can eliminate pipeline bubbles, they have no guarantee of model convergence [93]. As synchronous PP schemes continue to improve, their performance and efficiency are comparable with asynchronous PP, making them a more favorable option. In the following paragraphs, we will discuss in detail several common PP schemes.\nIn vanilla PP, data can be passed to the next device only after the preceding device completes its calculation, due to the sequential dependence in data flow. This leads to pipeline bubbles, resulting in a waste of resources and diminishing overall efficiency of the system.\nGPipe [94] splits each input batch into several micro-batches, which are input to the model sequentially. After completing the processing of each micro-batch, each device immediately passes its results to the next device and begins to process the next micro-batch. This approach significantly reduces the pipeline bubbles, and finally, each device uses the accumulated gradients to perform synchronous parameter updates. To further enhance memory efficiency, GPipe incorporates gradient checkpointing [79], which retains only a subset of intermediate activations and recalculates the remaining activations during backpropagation. However, GPipe still faces the issue of high memory consumption for intermediate activations.\nThe GPipe scheduling involves completing the forward passes for all micro-batches before executing the backward passes (F-then-B). Let $N_s$ denote the number of stages in the model, $N_d$ denote the number of devices, and $N_m$ denote the number of micro-batches. In the GPipe schedule, each device must retain intermediate activations of $N_m$ micro-batches. To reduce memory consumption, researchers introduced a one forward pass followed by one backward pass pattern (1F1B) [95], [96]. This scheduling strategy effectively reduces the number of on-the-fly micro-batches by executing the backward pass in advance. With 1F1B schedule, the pipeline bubble is equal to that of GPipe, and the activations of micro-batches required to store on each device decrease from $N_m$ to $N_d$ (where $N_m$ is typically greater than $N_d$)."}, {"title": "4) Sequence Parallelism", "content": "Supporting long sequence lengths in large-scale models is crucial for applying AI to scientific endeavors. Many scientific challenges inherently involve complex analysis of large-scale, high-dimensional data, frequently presented as long sequences. Long-sequence issues are prevalent in fields including structural biology, chemistry, drug development, and atmospheric science. For instance, many proteins consist of hundreds or even thousands of amino acid residues, where the sequence context of each residue can be essential to its 3D structure. Long-sequence support enhances the ability to capture non-local interactions among residues, thereby improving the quality of predictions. Additionally, complex molecules or chemical reactions often necessitate long sequence representations, including molecular formulas, SMILES, reaction pathways, etc. Climate prediction models require analyzing long-term historical and global spatial data, represented in time series or grid formats. Despite the importance of long-sequence support, its implementation presents challenges. Processing long sequences demands significant increases in computational power and memory, with the memory usage of attention mechanisms rising quadratically with sequence length.\nIn traditional distributed training techniques such as DP and TP, model parameters or input data are partitioned and assigned to multiple devices for processing. However, when the model needs to handle long inputs, these methods may lose efficiency due to memory bottlenecks or computational overhead. Sequential parallel (SP) is relatively novel and differs from DP and MP methods in that it aims to reduce the memory overhead caused by activations during model training. Although SP strategies divide input along sequence dimension, the specific implementation varies.\nLi et al. [98] considered solving the problem from a system perspective by combining ring-style communication with self-attention, allowing the input sequence to be distributed to each device and calculating attention scores across devices. Compared with Megatron-LM TP [76], the proposed method is not limited by the number of attention heads, as long as the sequence length is divisible by the degree of SP.\nIn Megatron-LM TP, some activations within the transformer layer are not distributed across devices, leading to increased memory overhead. The SP method proposed by Li et al. [98] can help mitigate this issue; however, it requires replicating model parameters and optimizer states across all devices, making it impractical for large-scale model training.\nKorthikanti et al. [90] found that the LaterNorm and Dropout operations within the transformer layer operate independently along the sequence dimension. Consequently, modifications were implemented based on Megatron-LM to effectively distribute the computational workloads and activations associated with LaterNorm and Dropout, without incurring additional communication overhead.\nJacobs et al. proposed Ulysses [99], a method that divides the input along sequence dimension onto all devices and conducts all-to-all communication on the Query (Q), Key (K), and Value (V) matrices before performing attention computation. This ensures that each device operates with disjoint attention heads, allowing for distributed attention calculation. Once the calculations are complete, another all-to-all is performed to get the results and restore the state that inputs were divided along sequence dimension. Ulysses SP and Megatron-LM TP share similarities in the distributed calculation of self-attention, both concerning limitations on the number of attention heads. A key advantage of Ulysses is that the communication volume for a single device decreases linearly as SP degree increases, while in Megatron-LM, it is independent of TP degree.\nLiu et al. [100] developed a ring-style distributed attention computation method that optimizes resource usage by conserving a portion of the Q, K, and V blocks. During calculations, each device exchanges K and V blocks with its neighboring devices, enabling it to perform computations using blockwise self-attention and feedforward networks (FFN) [101]. In contrast to the approach taken by Li et al. [98], this method can eliminate communication overhead by picking an proper chunk size and overlapping communication with computation."}, {"title": "B. Mixed Precision Training", "content": "Mixed Precision Training is a deep learning technique that leverages data types of varying precisions to enhance training speed, reduce memory consumption, and decrease computational costs, all while preserving comparable accuracy. In mixed precision training introduced by Micikevicius et al. [78], model parameters, activations, and gradients are stored using FP16, while maintaining a copy of the FP32 parameters. The FP16 data type utilizes half the memory space of FP32 and offers improved computation speed. Nevertheless, FP16 has a reduced numerical range and accuracy compared to FP32, which may lead to numerical stability issues during training. To cope with it, FP16 parameters are used for forward and backward calculations, while FP32 parameters are used for model updates. Additionally, to mitigate the risk of gradient underflow during backpropagation, loss scaling is commonly applied, which helps to minimize model performance degradation.\nWhile it needs to save an additional FP32 model parameter, the memory savings achieved by utilizing FP16 activations in mixed precision training surpass the memory costs associated with these extra FP32 parameters, leading to an overall memory usage reduction."}, {"title": "C. Gradient Checkpointing", "content": "In the training process of DNNs, the output of each layer (activations) needs to be stored in memory to calculate gradients during backpropagation. When dealing with large-scale models, long input sequences, or large batch sizes, the memory overhead caused by activations can become substantial.\nThe core idea behind gradient checkpointing is to trade computation for memory. This approach reduces memory usage by retaining only the intermediate activations of certain layers during forward propagation while recalculating the discarded activations when needed during backpropagation. The preserved intermediate activations are referred to as \"checkpoints\" [79].\nGradient checkpointing offers an effective solution to alleviate memory bottlenecks in the training of deep learning models, particularly in memory-constrained environments. It is important to choose checkpoints carefully to achieve an optimal balance between memory usage and computation time. Taking the transformer architecture as an example, it is generally advisable to set a checkpoint every 1 to 2 transformer layers [95].\nThe number of parameters and computational complexity of various components in DNNs often vary. Korthikanti et al. [90] suggest that for transformer models, it is unnecessary to set checkpoints for the entire transformer layer for recomputation. Instead, selective recomputation can be performed by identifying components of the transformer layer where activations cost substantial memory yet require less computation. By setting checkpoints and recalculating selectively, it is possible to reduce memory overhead while minimizing computation."}, {"title": "D. Offloading", "content": "Offloading refers to the process of transferring specific storage tasks from GPU/TPU memory to main memory (CPU memory) or lower-cost storage devices, such as NVMe SSD. This technique alleviates memory overhead through increased data communication and enhances the utilization of hardware resources.\nSwapAdvisor [106] takes the data flow graph of a model as input and selects legitimate memory allocation and operator scheduling based on the graph. By precisely planning which and when tensors require swapping in and out, SwapAdvisor achieves optimal overlap between computation and communication. The integration of genetic algorithms enables the exploration of the vast search space for finding the optimal combination of memory allocation and operator scheduling. In contrast to previous heuristic-based swapping methods [107], [108], SwapAdvisor significantly enhances memory efficiency and computational performance through its dual optimization approach. Notably, due to the lack of consideration of GPU communication, SwapAdvisor has limitations in multi-GPU training scenarios.\nRen et al. [109] introduced an offloading strategy designed for mixed precision training using the Adam optimizer, which allows for seamless integration with MP. They offload all FP32 model states and FP16 gradients to CPU memory and utilize CPU to compute parameter updates while FP16 model parameters remain on GPU to perform forward and backward passes. It enables the overlapping of swapping processes with computation, mitigating most communication overheads."}, {"title": "E. Gradient Accumulation", "content": "Gradient accumulation is an effective technique for training scenarios with limited memory and computing resources. It allows training models with a larger effective batch size when GPU memory is limited. The fundamental idea behind gradient accumulation is to gradually accumulate gradients from multiple small batches until a desired batch size is achieved, at which point a weight update is performed. For instance, if the total batch size we aim to use is 32, which exceeds the GPU memory capacity, we can divide it into smaller batches with batch sizes of 8, and perform 4 training iterations before conducting a weight update.\nAlgorithm 1 presents the pseudocode of gradient accumulation, outlining the implementation of this technique in the training process. Gradient accumulation is frequently utilized in hardware environments with limited resources and serves as an important optimization method in deep learning training."}, {"title": "F. Memory Efficient Optimizers", "content": "Currently, adaptive optimizers like Adam [111] and AdamW [112] have nearly become the default optimizers for training LLMs. While Adam demonstrates impressive performance, it incurs a high memory cost due to the store of its optimizer states, including first-moment and second-moment estimates of gradients. This results in a memory overhead that is at least double the size of the model itself, which poses a significant challenge during pre-training. To support such memory-intensive algorithms, techniques like offloading and sharding are often enabled in practice; however, these methods can lead to increased training latency and decreased efficiency. Consequently, there is a strong interest in developing optimizers that utilize less memory while maintaining efficiency.\nShazeer et al. introduced a memory-efficient adaptive optimizer known as Adafactor [113], which significantly lowers memory requirements while delivering training performance comparable to Adam. Assuming the model parameter matrix is denoted as $W \\in \\mathbb{R}^{m\\times n}$, by applying low-rank decomposition (i.e., decomposing into row and column vectors) on the second-moment estimates of the gradients, Adafactor can reduce memory consumption from $O(m \\cdot n)$ to $O(m + n)$. Meanwhile, Anil et al. proposed SM3 [114], which reduces the memory requirements for storing optimizer states through a parameter cover mechanism. It involves dividing parameters into multiple subsets, with each subset maintaining only one variable to approximate the second-order statistics of all parameters within it. This strategy helps decrease memory consumption during second-order moment estimation. SM3 employs a data-driven approach to dynamically adjust the learning rate, similar to Adagrad, while mitigating the memory overhead that arises from maintaining parameter-level statistics, as seen in Adagrad. Experiments demonstrate that the training performance of SM3 is on par with that of Adagrad and Adam, while its memory overhead is slightly lower than that of Adafactor.\nAdafactor uses non-negative matrix factorization to approximate the second-order statistics of gradients. While it saves memory, it can introduce errors that lead to instability during training. SM3 encounters similar challenges. To tackle this issue, Luo et al. introduced CAME optimizer [115]. It utilizes the residual between the exponential moving average (EMA) of the update and the current update to compute a confidence matrix, subsequently adjusting parameter updates. By applying non-negative matrix factorization to the confidence matrix, CAME further decreases memory overhead. Through confidence-guided strategies and non-negative matrix factorization, CAME significantly reduces memory usage while ensuring convergence. In large-scale language model training tasks, CAME has shown superior performance compared to Adafactor, while being comparable to it in terms of memory efficiency.\nChen et al. proposed an approach that treats algorithm discovery as a program search, which was successfully applied to identify optimization algorithms for training deep neural networks, thus discovering the Lion optimizer [116]. Unlike most adaptive optimizers, Lion solely tracks momentum and employs sign operations to calculate updates, leading to reduced memory overhead and a consistent update magnitude. In comparison to Adam, AdamW, and Adafactor, Lion exhibits outstanding performance across multiple tasks and models. Notably, in language modeling tasks, Lion surpasses Adafactor, particularly in scenarios of large-batch training.\nZhang et al. discovered that the Hessian matrix of the Transformer exhibits an approximate block diagonal structure. Based on this finding, they proposed Adam-mini [117], which partitions the parameters of components such as Q, K, V, and MLP into blocks and assigns a learning rate to each"}, {"title": "IV. TAILORED MEMORY-EFFICIENT TRAINING TECHNIQUES: A CASE STUDY", "content": "OpenFold [48] is an open-source implementation of AlphaFold 2, which has a prediction quality that matches AlphaFold 2 and runs faster on most proteins with less memory usage. This improves the ease and performance of training new models and performing large-scale predictions. OpenFold has made various optimizations to AlphaFold 2's training schedule. OpenFold was trained on a cluster of 44 NVIDIA A100 GPUs, each with 40GB of memory. It enables FP16 mixed precision training mode to enhance training speed while minimizing memory usage. To reproduce the effective batch size utilized during AlphaFold 2 training as closely as possible, OpenFold implements 3-way gradient accumulation. In terms of distributed training, Deepspeed [85] and ZeRO stage 2 [75] are employed to distribute the optimizer states across each GPU, thus reducing model redundancy. Additionally, they refactored the model by replacing element-wise operations with in-place equivalents wherever possible to minimize unnecessary memory allocation. Advanced implementation of the self-attention [118], [119] was also adopted to lower memory usage and speed up attention computation. Furthermore, they optimize CUDA kernels for certain model modules, decreasing memory overhead.\nDespite its impressive success in prediction accuracy, AlphaFold 2's computational and memory costs during training are much higher than those of vanilla transformers, and its architecture is computationally inefficient on GPUs. To address these issues, Cheng et al. proposed FastFold [49], an efficient implementation of AlphaFold 2. FastFold introduces diverse LLM training techniques, significantly reducing the cost of training and inference for AlphaFold 2 models.\nUnlike many existing LLMs, AlphaFold 2's architecture derives its main memory overhead from intermediate activations, requiring more than 20 GB memory in each attention module, which cannot be well distributed using common methods like ZERO DP, PP, or TP. Therefore, FastFold introduces a model parallel approach that focuses on reducing activations to optimize AlphaFold 2 training. Dynamic Axial Parallelism (DAP) is a technique tailored for the EvoFormer modules in AlphaFold 2. It retains complete model parameters on each device while partitioning inputs and intermediate activations across devices. The MSA representation and Pair representation within the EvoFormer module contain two distinct sequence dimensions, with computations performed along only one of these dimensions at a time. Consequently, DAP considers partitioning the other sequence dimension not in use. Due to the alternating calculations across different sequence dimensions, all-to-all communication is necessary to ensure that each device possesses the required data for computation. This design allows for the distribution of activations among devices, significantly reducing the training memory overhead of AlphaFold 2 and enhancing both efficiency and scalability. When the model is trained with DAP on a considerable number of devices, throughput can be improved by disabling gradient checkpointing, as DAP has saved enough memory. Compared with the original AlphaFold 2, FastFold has a training efficiency of 3.91x, and compared with OpenFold, it has improved by 2.98x.\nAlthough OpenFold utilizes advanced systematic approaches for performance and memory optimization, the pre-training cost of AlphaFold 2 remains high. While AlphaFold 2 has a relatively small scale with only 93M parameters, its activation memory usage during training is extremely high. Song et al. [120] designed customized attention kernels for the four types of attention variants in the Evoformer module to save memory. This approach lowers peak memory consumption by a factor of 13 compared to the OpenFold implementation. When combined with DAP, it can further reduce memory overhead.\nZhu et al. [50] conducted a comprehensive analysis of AlphaFold 2's training performance and identified that its poor scalability is primarily due to communication imbalance and inefficient computation. To address these issues, they combined DAP with various optimization methods, such as designing non-blocking data pipelines and performing operator fusion, to construct ScaleFold. ScaleFold allows for the training of AlphaFold 2 on 2080 NVIDIA H100 GPUs and shortens the pre-training time to merely 10 hours."}, {"title": "V. FUTURE TRENDS AND CHALLENGES", "content": "The number of parameters and the data processing capabilities of large-scale models play a crucial role in their ability to address complex scientific problems. However, the memory requirements for training these models often surpass current hardware limitations, creating a significant bottleneck in scaling. Optimizing memory usage during training not only enhances efficiency but also provides possibilities for exploring even larger models, which may further improve performance across various tasks. This optimization is vital for scientific models, where tasks like protein structure prediction, molecular generation, and climate prediction demand models can handle high-dimensional and long-sequence data, while scaling up models can significantly enhance the accuracy and effectiveness of these applications. Moreover, employing effective memory optimization strategies allows practitioners to train models of the required size using fewer GPUs, resulting in substantial savings in cost and energy. It lowers the barriers to training large language models, promoting greater participation from researchers in scientific fields.\nFor the training of transformer-based scientific large models, there exists diverse memory optimization methods and effective training libraries, such as DeepSpeed, Megatron-LM, Colossal-AI, FairScale, Hugging Face Accelerate, etc., all of which support distributed training, mixed precision training, gradient checkpointing, and other advanced techniques. However, our investigation indicates that many studies employing LLMs for scientific research have not fully leveraged these memory-efficient training approaches. Thus, there remains considerable room for optimization in large-scale model training in scientific tasks. This survey aims to systematically summarize the memory optimization techniques utilized in LLM training, hoping to promote their application in scientific domains and assist researchers in fully harnessing the capabilities of LLMs to accelerate scientific discovery and innovation.\nHowever, when it comes to large-scale models based on transformer variants, the memory optimization methods described in the paper, which are suitable for training standard Transformers, may not be fully applicable. Modules such as AlphaFold 2's Evoformer, RosttaFold's SE(3)-Transformer, and AlphaFold 3's Pairformer depict notable differences in both memory usage and computational complexity compared to the vanilla Transformer architecture. Optimizing these models often requires domain expertise to develop tailored memory optimization strategies that align with the specific structure of the model. This process necessitates not only a profound understanding of the characteristics of corresponding scientific tasks but also the co-optimization of software and hardware to achieve optimal performance. Designing effective memory optimization methods for these models remains an urgent challenge that needs to be solved.\nAs model size and task complexity continue to grow, memory-efficient training methods have become a key factor for"}]}