{"title": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever", "authors": ["Rohan Jha", "Bo Wang", "Michael G\u00fcnther", "Saba Sturua", "Mohammad Kalim Akram", "Han Xiao"], "abstract": "Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT's late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. In this paper, we introduce several improvements to the ColBERT model architecture and training pipeline, leveraging techniques successful in the more established single-vector embedding model paradigm, particularly those suited for heterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks, while also cutting storage requirements by up to 50% compared to previous models.", "sections": [{"title": "1 Introduction", "content": "Neural retrieval has gained popularity in recent years following the arrival of capable pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020). Sparse neural retrieval systems, such as SPLADE (Formal et al., 2021), represent texts as weighted bags of words. In contrast, dense retrievers encode queries and documents as high-dimensional vectors, capturing relevance signals through spatial relationships extending beyond exact term matching.\nMost dense retrievers encode a query or document as a single vector, commonly the result of mean-pooling or the [CLS]-embedding over the transformer's final layer token embeddings. In contrast, recent multi-vector retrievers like ColBERT (Khattab and Zaharia, 2020) generalize this embedding process to maintain a (usually smaller) embedding for each token, computing relevance scores as a function of the similarities of query and document tokens instead. This approach has the benefit of remaining compatible with much of the vector similarity infrastructure that makes single-vector methods efficient, but requires more space to store an embedding per token and compute at inference time to aggregate token interactions into a single score. This late interaction over token embeddings achieves greater in-domain performance and tends to be more robust out-of-domain than single-vector similarity. While ColBERTv2 is trained only on English MSMARCO triplets (Bajaj et al., 2016) and has a monolingual BERT backbone, making it incapable of multilingual retrieval, some previous works extend the model to multilingual retrieval. Louis et al. (2024) does this by using parameter extensions for each additional language, and Lawrie et al. (2023) trains solely on machine-translated English MSMARCO data to get effective multilingual performance. However, these approaches come with significant trade-offs in terms of model usability and training data diversity. Other multilingual multi-vector models like BGE-M3 (Chen et al., 2024) produce extremely large token representations that limit their practical utility for first-stage retrieval.\nIn this work, we propose Jina-ColBERT-v2, which introduces an improved training recipe for ColBERT models with the following features:\nTraining with diverse weakly supervised data: Our two-stage approach includes a large-scale contrastive tuning stage, followed by a smaller-scale finetuning phase with supervised distillation.\nGeneral multilingual performance: We train with data from a variety of high- and low-resource languages, including machine-translated training data, and show that this improves even out-of-domain multilingual performance.\nInference-agnostic efficiency: We introduce multiple sizes of linear projection heads, jointly"}, {"title": "2 Related Work", "content": "In this section, we discuss related work in single- and multi-vector retrieval, as well as the non-English late-interaction retrievers from which our training recipe draws inspiration."}, {"title": "2.1 Single-Vector Retrieval", "content": "Single-vector encoder models have demonstrated their potential as general-purpose embedding models across a number of downstream tasks (Muennighoff et al., 2023). When used in a bi-encoder retrieval model, they asymmetrically encode queries and documents as separate dense vectors, and measure their pairwise relevance as the cosine similarity between the vectors. Owing to their strong in-domain performance and straightforward inference scheme, there has been a growing focus on improving their training. Studies demonstrate that large-scale unsupervised pair training utilizing in-batch negatives, followed by a small-scale triplet fine-tuning stage, significantly improves performance compared to a dense retriever trained solely on triplet data (Li et al., 2023; G\u00fcnther et al., 2023). Other works have incorporated asymmetric task-specific instructions for queries and documents to further enhance performance (Wang et al., 2024) and demonstrated the efficacy of using synthetically generated training data, including using diverse task instructions and machine translations, to further improve model representations. (Wang et al., 2023; Lee et al., 2024)"}, {"title": "2.2 Multi-Vector Retrieval", "content": "Multi-vector retrievers like ColBERT also employ a bi-encoder structure, but queries and passages are represented by a collection of smaller token embeddings rather than one large vector. As such, ColBERTv2's training uses many of the same techniques as state-of-the-art single-vector models: cross-encoder distillation, multiple negatives per query, and self-mined hard negatives. Recent models have continued to improve on this training recipe, particularly for multilingual or non-English training. BGE-M3 (Chen et al., 2024) adopts the two-stage pairs-to-triplets training pipeline, and does self-knowledge distillation, treating the combination of its sparse, dense, and multi-vector scores as the teacher score."}, {"title": "2.3 Multilingual Retrieval", "content": "Owing to the quality of English-based pre-trained models (BERT) and annotated data (MSMARCO), many advances in neural retrieval have been in a monolingual English setting (Karpukhin et al., 2020; Xiong et al., 2020; Khattab and Zaharia, 2020). Researchers, however, have also made advances in non-English capabilities. On the modeling front, multilingual PLMs like mBERT (Devlin et al., 2019) and later XLM-ROBERTa (Conneau et al., 2020) have expanded pre-training to include text in up to 100 languages, including in cross-language contexts. For multilingual retrieval, there are two approaches: natural and translated. Datasets like Mr-Tydi and MIRACL (Zhang et al., 2021, 2023b) are built from human-generated and annotated queries, whereas mMARCO (Bonifacio et al., 2022) is a collection of machine-translated copies of MSMARCO which inherit their judgments from the original dataset. The former method tends to be of higher quality and lacks the subtle distributional/idiomatic errors, dubbed \"translationese\", that the latter sometimes exhibits, however it costs more per example.\nRecent multi-vector work has also proposed further modifications along the dimensions of architecture and data. ColBERT-XM (Louis et al., 2024) addresses the so-called curse of multilinguality (Conneau et al., 2020), the performance degradation of models pre-trained on too many tasks, with shared- and per-language parameters that allow for more robust zero-shot language transfer and post-hoc language extension. On the data approach, ColBERT-X (Nair et al., 2022; Lawrie et al., 2023; Yang et al., 2024) uses language-mixed batches of machine-translated English data, and BGE-M3 (Chen et al., 2024) curates unsupervised and high-quality su-"}, {"title": "3 Training Overview", "content": "Jina-ColBERT-v2's training paradigm has three parts:\nModified Encoder Architecture: We use a modified encoder backbone, derived from XLM-ROBERTa with improvements made to its architecture and pre-training regime. We further extend ColBERT's linear projection head by jointly training a collection of different-size heads for embedding size reduction.\nPair Training: To learn from the semantic structure of large quantities of diverse data in many languages, we first train our encoder model on weakly supervised text pairs from a variety of embedding datasets.\nTriplet Training: Our model is further fine-tuned using retrieval examples in many languages with both positives and hard negatives, supervised by a highly-capable multilingual cross-encoder.\nThe following sections describe our experiments on these three components of training Jina-ColBERT-v2."}, {"title": "4 Architecture", "content": "Following many prior single- and multi-vector multilingual training efforts, we adopted XLM-ROBERTa as our backbone model due to its strong performance across various downstream tasks (Nair et al., 2022; Louis et al., 2024; Chen et al., 2024). We also enhance the XLM-ROBERTa architecture with flash attention (Dao, 2024) for efficiency and to replace its absolute positional embeddings with rotary positional embeddings (RoPE, Su et al. (2023)). To warm up its new positional embeddings, we continued pre-training the modified backbone with the same masked language modeling objective for 160,000 steps on the RefinedWeb dataset (Penedo et al., 2023), a modern, high-quality corpus. During this pre-training phase, we set the maximum sequence length to 8,192 tokens with a rotary base of 10,000 and employed whole-word-masking (Devlin et al., 2019), masking out 30% of"}, {"title": "4.1 Backbone Improvements", "content": "Following many prior single- and multi-vector multilingual training efforts, we adopted XLM-ROBERTa as our backbone model due to its strong performance across various downstream tasks (Nair et al., 2022; Louis et al., 2024; Chen et al., 2024). We also enhance the XLM-ROBERTa architecture with flash attention (Dao, 2024) for efficiency and to replace its absolute positional embeddings with rotary positional embeddings (RoPE, Su et al. (2023)). To warm up its new positional embeddings, we continued pre-training the modified backbone with the same masked language modeling objective for 160,000 steps on the RefinedWeb dataset (Penedo et al., 2023), a modern, high-quality corpus. During this pre-training phase, we set the maximum sequence length to 8,192 tokens with a rotary base of 10,000 and employed whole-word-masking (Devlin et al., 2019), masking out 30% of the tokens. We call this modified language model Jina-XLM-ROBERTa."}, {"title": "4.2 Multiple Linear Heads", "content": "To reduce index sizes, ColBERT includes a linear head that projects its token embeddings from the hidden dimension of its language model down to a lower dimension (768 \u2192 128). Notably, BGE-M3's multi-vector retrieval does not take this step, keeping its token embeddings at a full 1024 dimensions. To further reduce index size, we jointly train six linear heads with dimensions $d \\in \\{64,96, 128, 256, 512, 768\\}$ using Matryoshka Representation Loss (MRL, Kusupati et al. (2022)). This allows users to choose greater or lesser space efficiency, with an associated performance trade-off. Halving the token dimension (128 \u2192 64) only causes its nDCG@10 to drop by 0.01 (1.59%). We unfortunately find that MRL's weight-tying efficient variant (MRL-E), where losses are computed on truncations of the same token vector does not preserve performance well."}, {"title": "5 Pair Training", "content": "To leverage an abundance of text pairs with varying richness of semantic structure, we draw inspiration from common practices in single-vector embedding model training and begin by training on these text pairs, focusing on optimizing the embedding model's performance on general semantic similarity tasks. This less supervised step is in contrast to previous ColBERT works, which typically train directly on 32-way or 64-way retrieval triplets, con-"}, {"title": "5.1 Data Composition", "content": "Our pair training data consists of a broad range of weakly supervised datasets harvested from the web. We adjusted sampling rates across different languages and domains based on intuition, resulting in a set of 450 million weakly supervised, semantically related sentence pairs, question-answer pairs, and query-document pairs. Of these 450 million pairs, 50% are in English. Our non-English pairwise datasets contain a diverse collection of 29 major languages, including 3.0% code data, with 4.3% representing cross-lingual data."}, {"title": "5.2 Contrastive Loss", "content": "We utilize the same single-vector pair-training loss function as described in (G\u00fcnther et al., 2023). .Due to the typically symmetric nature of similarity measures, the loss is calculated in both directions. During the pair training stage, we set the temperature $\u0442 = 0.02$ and used a peak learning rate of $5\u00d710^{-5}$ with a warm-up period of 1,000 steps. The model was trained using the Adam optimizer for 100,000 steps with a global batch size of 16,384."}, {"title": "6 Triplet Training", "content": "Our triplet dataset consists of 1) high-quality, human-annotated research datasets such as MS-MARCO, DuReader, and MIRACL (Bajaj et al., 2016; He et al., 2018; Zhang et al., 2023b) with seven mined hard negatives per query and 2) high-quality datasets like MSMARCO and NQ translated from English into Chinese, French, German, Japanese, Russian and Spanish, following our previous work (Mohr et al., 2024) and 3) synthetically generated datasets to address common failure modes of dense vector models such as negation and to cover niche domains like legal IR.\nThe triplet dataset covers 14 widely used languages, with a strong emphasis on Arabic, Chinese, English, French, German, Japanese, Russian, and Spanish. We sample the datasets to create a language distribution similar to that used in pair training. English accounts for 45.9% of the triplets, with 52.1% roughly evenly split between the mentioned high-resource non-English languages and a small 2% share for lower-resource languages."}, {"title": "6.1 Data Composition", "content": "Our triplet dataset consists of 1) high-quality, human-annotated research datasets such as MS-MARCO, DuReader, and MIRACL (Bajaj et al., 2016; He et al., 2018; Zhang et al., 2023b) with seven mined hard negatives per query and 2) high-quality datasets like MSMARCO and NQ translated from English into Chinese, French, German, Japanese, Russian and Spanish, following our previous work (Mohr et al., 2024) and 3) synthetically generated datasets to address common failure modes of dense vector models such as negation and to cover niche domains like legal IR.\nThe triplet dataset covers 14 widely used languages, with a strong emphasis on Arabic, Chinese, English, French, German, Japanese, Russian, and Spanish. We sample the datasets to create a language distribution similar to that used in pair training. English accounts for 45.9% of the triplets, with 52.1% roughly evenly split between the mentioned high-resource non-English languages and a small 2% share for lower-resource languages."}, {"title": "6.2 Supervision Loss", "content": "Following ColBERTv2, we finetune our pair-trained checkpoint on samples with hard negatives using a KL divergence loss function to distill soft labels from the teacher model. For the teacher model, we use jina-reranker-v2-base-multilingual\u00b9 , a highly capable multilingual cross encoder.\nThis stage trains for 100,000 steps with a batch size of 32 and a cosine decay learning rate schedule with 5% warm-up that peaks at 1 \u00d7 10\u22125. We use pure BFLOAT-16 precision, and apply magnitude-based gradient clipping with a threshold of 1 for stability."}, {"title": "7 Results", "content": "We evaluate Jina-ColBERT-v2 on three widely used benchmarks. For general English performance, we use the same subset of 14 retrieval and text-similarity tasks from the BEIR benchmark as in Santhanam et al. (2022). Additionally, we assess performance on the LoTTE benchmark, which focuses on long-tail queries, and the MIRACL and mMARCO benchmarks (Zhang et al., 2023b; Bonifacio et al., 2022), which assess non-English retrieval performance. We report nDCG@10 for the BEIR and MIRACL collections, MRR@10 for mMARCO, and Success@5 for LoTTE. Scores are reported on the test split for BEIR, development split for MIRACL and mMARCO, and search test split for LOTTE. We use the same max query/document lengths as reported in Santhanam et al. (2022), and use the default (32/300) for MIRACL and mMARCO.\nIn this section we compare Jina-ColBERT-v2 to a variety of models across the BEIR, LoTTE, MIRACL, and mMARCO benchmarks to situate it's performance in the landscape of other monolingual and multilingual retrievers."}, {"title": "8 Ablation Studies", "content": "In this section we present short ablation studies on modifications to three various aspects of ColBERT modeling and training."}, {"title": "8.1 Efficient Evaluation", "content": "Due to the compute and time costs of indexing corpora containing tens of millions of documents, evaluating every model checkpoint and ablation on every task is not feasible. Therefore, we follow recent works (Clavi\u00e9, 2024; Merrick et al., 2024) by comparing models' quality on smaller sampled-corpus versions of HotpotQA, NQ, MS MARCO, and MIRACL (Chinese, French, German, Japanese, Spanish)."}, {"title": "8.2 Task Instructions", "content": "Inspired by the use of instruction prefixes in single-vector works like Su et al. (2022), we experimented with adding task-specific natural language instructions for retrieval (RET), and question answering (QA), and semantic text similarity (STS). However, results in Table 5 show a generally negative effect across most BEIR datasets, with an average relative drop of -1.8% nDCG@10. We hypothesize that this is because instructions are not well-suited for late interaction models, which operate at the token level. Any embedding conditioning that the instructions might provide likely becomes less effective when aggregated at the token similarity level. Furthermore, these instructions occupy valuable space within the system's fixed token capacity."}, {"title": "8.3 Score Normalization", "content": "Recently, Clavi\u00e9 (2024) applied min-max normalization to both the student and teacher scores before computing the KL loss. This adjustment brings the score distributions of the ColBERT model and its CE teacher into closer alignment, as the original score distribution for ColBERT theoretically ranges from zero to the number of query tokens, and is model-dependent for the teacher CE. Our experiment presented in Table 6, however, shows this method to have inconclusive benefit to nDCG@10 on the BEIR and MIRACL datasets when applied"}, {"title": "8.4 Query Augmentation Attention", "content": "An important feature of ColBERT's implementation is its query augmentation mechanism. By padding queries with [MASK] tokens to a uniform length, ColBERT uses BERT's masked language modeling ability to produce additional soft term embeddings which interact with document token embeddings during MaxSim scoring. However, prior ColBERT models do not modify the attention mask to allow query tokens to attend to the mask tokens, which some hypothesize might harm generalization by making this augmentation feature too integral to the embedding process. Our controlled triplet training experiment in Table 6, however, demonstrates a positive effect across a variety of tasks, with particular benefit to non-English tasks in MIRACL. We therefore allow this attention in our training and inference."}, {"title": "9 Conclusion", "content": "This work presents Jina-ColBERT-v2, a capable multilingual ColBERT model that is the result of improvements to its architecture and training process. We implement modifications to the model architecture like flash attention and MRL heads that yield efficiency gains with effectively no downside, and subsequently train it on a heterogenous mix of data of varying tasks, languages, and supervision structures in order to bolster its performance as a general purpose retriever.\nOur ablation experiments demonstrate the sensitivity of ColBERT to modifications to its representations. We find that generic natural language task instructions don't provide the same conditioning benefit as in the single-vector setting, and that allowing query tokens to attend to their augmentation [MASK] tokens strongly benefits performance, both defying intuition from prior works.\nWe hope that our work will support future multilingual ColBERT development, and prompt further exploration into the properties and optimal configuration of its query augmentation mechanism. We are also encouraged by the many inference-only optimization works on ColBERT representations, and suggest further effort be invested in tying these methods more closely with the models training objective."}]}