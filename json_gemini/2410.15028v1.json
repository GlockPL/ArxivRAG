{"title": "A Novel Reinforcement Learning Model for Post-Incident Malware Investigations", "authors": ["Dipo Dunsin", "Mohamed Chahine Ghanem", "Karim Ouazzane", "Vassil Vassilev"], "abstract": "This Research proposes a Novel Reinforcement Learn- ing (RL) model to optimise malware forensics investigation during cyber incident response. It aims to improve forensic investigation efficiency by reducing false negatives and adapting current practices to evolving malware signatures. The proposed RL framework leverages techniques such as Q-learning and the Markov Decision Process (MDP) to train the system to identify malware patterns in live memory dumps, thereby automating forensic tasks. The RL model is based on a detailed malware workflow diagram that guides the analysis of malware artefacts using static and behavioural techniques as well as machine learning algorithms. Furthermore, it seeks to address challenges in the UK justice system by ensuring the accuracy of forensic evidence. We conduct testing and evaluation in controlled environments, using datasets created with Windows operating systems to simulate malware infections. The experimental results demonstrate that RL improves malware detection rates compared to conventional methods, with the RL model's performance varying depending on the complexity and learning rate of the environment. The study concludes that while RL offers promising potential for automating malware forensics, its efficacy across diverse malware types requires ongoing refinement of reward systems and feature extraction methods.", "sections": [{"title": "I. INTRODUCTION", "content": "In post-incident malware forensics investigations, the detec- tion and classification of malware are critical processes for reconstructing evidence files. This is particularly important because malware, being a malicious program, can lead to unauthorised access to confidential information, jeopardising the security and integrity of data or information systems, thereby posing a significant threat to the involved systems and institutions. He and Sayadi [1] highlight that malware attacks have become a pervasive threat, affecting homes, education, businesses, government, and healthcare by \"finding vulnerabilities in networks and applications to launch attacks.\" In the healthcare field, particularly within the Internet of Medical Things (IoMT), such malicious attacks are especially dangerous. A minor misclassification or failure to detect malware can seriously compromise patient medical records, potentially leading to incorrect diagnoses or treatments and, in extreme cases, resulting in paralysis or death. Machine Learn- ing (ML) has gained widespread adoption to detect various types of malware. However, increasingly complex malware can circumvent ML techniques and models. The paper by Wu et al., [2] discusses the use of reinforcement learning (RL) as a more advanced model that improves malware detection accuracy, surpassing traditional machine learning methods. According to Liu et al., [3], machine learning in malware detection involves extracting features from data to classify malware and learn from past data to identify new threats. As a result of leveraging algorithms and data analysis, machines can improve their accuracy in identifying malware. However, newer and more complex malware can deceive ML models by masquerading as benign software, evading detection [4] [5]. Ebrahimi et al., [6] suggest that reinforcement learning can help defenders detect sophisticated adversarial threats. Adversarial malware relies on perturbation methods to evade ML-based detectors [7] [8]. A significant challenge is the need for constant updates with new malware behaviours.\nReinforcement learning, on the other hand, enables models to generate adversarial malware that can bypass detection by portable executable (PE) malware classifiers. Quertier et al., [8], note that the RL framework \"Gym-Malware\" achieves an evasion rate of up to 16%, while the RL framework \"MAB-malware\" can achieve an evasion rate of over 75% in a black-box setting. Furthermore, according to Quertier et al., [8], adversarial knowledge defines attacks as either \"white box,\" where the adversary has complete access to the model, or \"black box,\" where the adversary has no knowl- edge of the model and can only obtain classification results through a limited number of attempts [11]. These adversarial samples, which are falsely classified as benign, can improve the detection accuracy of malware detectors by 16% to 94% [2]. Reinforcement learning's ability to automate malware evasion shows how detailed sequences of adversarial actions can train antivirus and malware detection systems, enhancing their effectiveness in combating malware [8]."}, {"title": "A. Research Aim and Question", "content": "This research aims to improve malware forensics investi- gations by utilising reinforcement learning (RL) techniques. The primary focus is on identifying, analysing, and enhanc- ing models for post-incident investigations. As a result of this, the goal is to expedite forensic processes and miti- gate the miscarriage of justice within the UK legal system. Additionally, this research seeks to improve heuristic- and signature-based analysis methods through the application of RL, thereby enhancing overall cybersecurity measures after a security breach. Building on these objectives, the research also aims to address a central question: How effective are re- inforcement learning models in distinguishing between benign and malicious software, and what are the areas for potential improvement? The study investigates RL's role in enhancing malware analysis within post-incident forensics, particularly in identifying patterns that traditional tools may struggle to detect. Furthermore, it examines RL's adaptability to evolving malware signatures and explores the feasibility of combining RL techniques with heuristic methods for more reliable and comprehensive malware analysis."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Reinforcement Learning Improves ML Malware Detection", "content": "Wu et al. [2] explore reinforcement learning (RL) to en- hance malware evasion against machine learning detection models. They propose the \"gym-plus\" model, an extension of \"gym-malware,\" which generates evasive malware samples. Using the EMBER dataset, they retrain detection models, improving detection rates for unknown malware. However, the study lacks a clear theoretical explanation of RL and omits a discussion on limitations like agent selection and action space. Moreover, there is no comparative analysis with other detection methods. Despite these gaps, the paper demonstrates practical effectiveness, offering a detailed evaluation of the RL model. Future work should address these limitations to further improve malware detection."}, {"title": "B. Adversarial RL for Malware Detection", "content": "Ebrahimi et al., [6] research improves cybersecurity by ad- dressing adversarial attacks on ML-based malware detection. Specifically, their innovative use of adversarial reinforcement learning (RL) enhances malware detector robustness in dy- namic environments. The method improves dynamic inter- actions between the detector and the adversary by includ- ing adversarial agents that create false samples. However, reliance on MalVAC and lack of comparative assessment limit the research's generalizability. Furthermore, more theoreti- cal exploration of convergence properties could improve the framework. Despite these flaws, their focus on future work like optimisation solvers and reducing false positives shows forward-thinking."}, {"title": "C. Reinforcement Learning: Uncovering Control", "content": "Wang et al., [9] propose an automated approach for identifying command and control (C2) attack paths in large networks using reinforcement learning (RL). They address manual C2 detection methods' limitations by emphasizing efficiency and automation. By integrating cyberdefense terrain into the RL model, they enhance practical relevance. While their detailed attack simulation, which covers infection to exfiltration phases, demonstrates rigor, reliance on a simulated environment limits real-world applicability. Furthermore, the assumption of full knowledge of host characteristics may be unrealistic, and the RL model's complexity poses practical challenges. Despite these weaknesses, the paper contributes valuable insights into RL-based intrusion detection, laying the groundwork for future cybersecurity research and applications in network defence."}, {"title": "D. Reinforcement Learning for Grid Security", "content": "Yu [10] introduces a hierarchical deep reinforcement learning- based (HDRAD) scheme for detecting Advanced Persistent Threats (APT) in data management systems. The thorough simulations demonstrating HDRAD's superior performance over RS and HP schemes in detection delay and data protection are a key strength. However, the study's complexity and reliance on technical illustrations make it difficult for non- experts. While the research outlines its objectives, it lacks simplified explanations of how the HDRAD scheme functions. Additionally, brief introduction, discussion, and conclusion sections limit comprehensive understanding. Despite these limitations, the paper significantly contributes by presenting a novel and efficient APT detection method for improving data security in smart grids."}, {"title": "E. Deep Learning Techniques for Malware Obfuscation", "content": "Gao and Fang [12] present a novel malware evasion approach using deep reinforcement learning. They generate adversarial examples by extracting bytes from benign files and injecting them into malware, achieving an 85% evasion rate against EMBER, a state-of-the-art malware classifier. The research's strengths include the innovative combination of reinforcement learning and malware evasion, along with a clear presentation. However, weaknesses arise from a lack of discussion on ethical implications and potential limitations, such as effectiveness across diverse malware types. Despite the robust experimental design, real-world validation requires further research. Nev- ertheless, the study significantly contributes to cybersecurity by proposing a promising yet ethically complex method for enhancing malware evasion techniques."}, {"title": "F. A3C Algorithm for Malware Identification", "content": "Xue et al., [14] describe a novel approach to improving malicious code detection using reinforcement learning and the Asynchronous Advantage Actor-Critic (A3C) algorithm. They highlight the limitations of traditional detection methods and position their work within the broader context of machine learning advancements. The research's strengths include its detailed methodology and contribution to generating anti- detection adversarial samples. However, the study lacks in-depth discussion of broader implications, such as ethical con- cerns and scaling challenges. Exploring alternative adversarial techniques and potential biases could improve robustness. Despite these limitations, the article significantly contributes to cybersecurity through its novel application of reinforcement learning techniques."}, {"title": "III. RESEARCH METHODOLOGY", "content": ""}, {"title": "A. Experimental Setup and Dataset Generation", "content": "To implement and validate the proposed reinforcement learn- ing malware investigation framework, a comprehensive mal-"}, {"title": "B. Malware Workflow Diagram Creation", "content": "The research methodology covers the experimental setup, dataset generation, and development of a malware analysis workflow. This workflow is central to the reinforcement learn- ing framework, integrating techniques such as data collection, examination, and analysis [16]. We analysed live memory dumps from 13 versions of Windows, both infected and uninfected, using the Volatility framework to detect anomalies and malware artefacts. The analysis phase employs static, signature-based, behavioural techniques, and machine learning algorithms. The workflow diagram maps typical malware behaviours and improves post-incident forensic investigations, supporting the reinforcement learning model's training and validation."}, {"title": "C. Q-Learning Terminologies", "content": "We implement the proposed Reinforcement Learning Post- Incident Malware Investigation Model in the following sec- tions. Key terminologies are briefly defined. The Environment is the world where the agent operates. The Agent learns by interacting with the environment. States (s) represent the agent's position, while an Action (a) is any move the agent can take, leading to either a reward or penalty. Episodes signify the end of a stage, either through success or failure. For each state-action pair, the agent manages Q-values in a Q-Table. Temporal Differences (TD) compare current and previous state-actions. The learning rate controls how new information replaces old. A policy maps states to actions, while the Dis- count Factor weighs future rewards. The Bellman Equation relates Q-values across state-action pairs, and the Epsilon- Greedy strategy balances exploration and exploitation."}, {"title": "D. The Unified Markov Decision Process", "content": "The Unified Markov Decision Process (MDP), as depicted in Figure 1, consolidates all the subsections of MDPs into a singular process, providing a comprehensive perspective. This synthesis allows the agent to effectively navigate the environment and make informed decisions regarding malware investigation."}, {"title": "E. The Proposed RL Post-Incident Malware Investigation Framework", "content": "The six main parts of the Reinforcement Learning Post- Incident Malware Investigation Framework are shown in Fig- ure 2. These components include data collection, workflow diagram mapping, MDP model implementation, environmental dependencies, the MDP solver, and continuous learning and adaptation. Initially, data collection involves acquiring live memory dumps from Windows operating systems. Following that, data analysis focusses on analysing the collected data to identify anomalies, compromise indicators, and potential malware rtifacts. The workflow diagram outlines a comprehen- sive approach to identifying malware infections using static analysis, signature-based analysis, behavioural analysis, and machine learning algorithms. To extract features from the identified processes, we use the AWK module. Additionally, listing DLLs is essential for tracking loaded DLLs for each process. Furthermore, monitoring open handles is crucial for keeping track of the open handles associated with each pro- cess. Moreover, collecting network data ensures the acquisition of all pertinent network-related information. Registry hive analysis involves identifying registry hives and listing their keys. We duplicate processes into executable files and check them against known malware databases to determine their maliciousness or benignity. Additionally, addressable memory is duplicated to conduct grep searches using specific key- words. The state spaces are designed to align with the malware workflow diagram, encompassing 67 unique states. Based on this workflow, actions are defined, ranging from three to ten, exposing the agent to 109 distinct actions within a defined environment. We create the MDP solver by formulating the unified Markov Decision Process (MDP) models, which we base on the subsections of the proposed RL Post-Incident Malware Investigation Model. Finally, we divide the setup environment dependencies into three sections: creating depen- dencies and gym environments, importing required libraries, and implementing the training data for continuous learning and adaptation."}, {"title": "F. Algorithm 1 - Implementation of the Q-Learning Algorithm", "content": "Algorithm 1 implements Q-learning to train an agent for optimal decision-making. It initialises a zero-valued Q-table representing the agent's understanding of the environment, where rows are states and columns are actions. Key parameters like learning rate, discount factor, and exploration probability (initially set to 0.9) are established, along with decay schedules and data storage structures (storage, storage new, reward list). The main loop runs over a specified number of episodes, resetting the environment and variables at each episode's start. Within each episode, a greedy policy selects actions: if the probability is high, it explores by choosing a random action (exploration); otherwise, it exploits by selecting the action with the highest Q-value for the current state (exploitation). The environment executes the action, providing the next state, reward, and a done flag indicating the episode's end. Q- values are updated using the Bellman equation, taking into account the immediate reward and maximum future Q-value, and stored in the Q-table. The state updates to the next state, and data such as Q-values, episode number, and action are appended to the storage list. The exploration probability de- cays according to a predefined schedule. We optionally check convergence by comparing the absolute difference between new and current Q-values; if it falls below a threshold, we can terminate the loop early. After each episode, we append episodic rewards and other data to the reward list. Finally, we return the Q-table and storage data, which summarise the learnt policy and training data. The process begins with the initialisation phase, where three custom environments (env_newl, env_new2, and env_new3) are defined using a specified MDP function. The algorithm iterates over a list of names (name_list), and for each name, it assigns the appropri- ate environment by configuring the Markov Decision Process (MDP) with specific transition probabilities and rewards."}, {"title": "G. Algorithm -2 Iterating Learning Rates Variation over MDP Environments", "content": "Algorithm 2 is an algorithm that trains and stores models using different learning rates (LRs) across multiple environ- ments. The initialisation phase initiates the process, defining various environments (envs) and creating an empty dictionary named 'final dict' to store results. Next, we set the training parameters, which include a list of learning rates ranging from '0.001 to 0.9', and store the resulting Q-tables, intermediate storage, rewards, and additional storage collections. For each learning rate ('lr') in the list of learning rates ('lrs'), the algorithm executes the 'new q learning' algorithm. Finally, the algorithm stores the results by appending them to the 'final dict'. This structured approach guarantees systematic model training and result storage for varying learning rates in different environments."}, {"title": "IV. MDP MODELS INTEGRATION AND IMPLEMENTATION", "content": ""}, {"title": "A. An Overview of Our Three Proposed MDP Environments", "content": "The BlankEnvironment models the proposed Markov Decision Process using the Malware Workflow Diagram, incorporating states, actions, rewards, transition probabili- ties, and episode completion status. It features a discrete action space with 10 actions and an observation space with 67 observations, assigning a standard step penalty of -0.04 and a reward of 2 for identifying malware. The BlankEnvironment_with_Rewards gives a re- ward of 2 for all terminal states upon accurate mal- ware identification and 4 for early-stage accurate iden- tification, encouraging correct classifications. Conversely, the BlankEnvironment_with_Time imposes a harsher penalty of -0.01 per step to incentivise efficient malware identification by discouraging the agent from taking unnec- essary actions. Rewards serve as hyperparameters in both environments, refined for optimal agent performance."}, {"title": "B. Implementation of the BlankEnvironment", "content": "As shown in Figure 3, we define a new class named \u2018BlankEnvironment', which inherits from 'gym.Env', in- dicating its intended use as a gym environment. For the BlankEnvironment class, the constructor method initialises the class instance. The next variable defines the environment's action space and observation space. The action space is dis- crete, with 10 possible actions, whereas the observation space is discrete, with 67 possible observations. The next variable, 'self.state = 0', sets the initial state of the environment to '0'. We initialize 'self.P = dict()' as an empty dictionary to store transition probabilities in the subsequent code block."}, {"title": "C. Implementation of the BlankEnvironment with Rewards", "content": "The BlankEnvironment with Rewards is a completely different implementation compared to BlankEnvironment. The BlankEnvironment with Reward, actions leading to terminal states are assigned a reward of 2, in contrast to the -0.04 reward assigned in the BlankEnvironment. The reward function in BlankEnvironment_with_Rewards is modified when an episode ends, as indicated by the done flag. The done flag assigns the value of the fourth element to the variable done, which contains information about episode completion. The done flag checks if the done variable equals True, and the reward variable is set to a positive value of 4. This update considers the consequences of changing the reward when the episode ends."}, {"title": "D. Implementation of the BlankEnvironment with Time", "content": "In BlankEnvironment_with_Time, the agent incurs a more severe negative reward of -0.1 per step, compared to the standard penalty of -0.04 in the other two environments. This technique aims to incentivise the agent to efficiently identify malicious files by taking the most direct path, thereby discour- aging any superfluous actions. Furthermore, when the agent extends the episodes by taking additional steps, it receives significant penalties. Notably, this incentive is considered a hyperparameter, as it is subject to continuous refinement. The expression 'done = k[3]' assigns the fourth element of the tuple 'k' to 'done', indicating whether the episode is complete. If 'done' is 'True', a reward of +4 is assigned; otherwise, a penalty of -0.1 is given to the agent. The tuple 'new k' is then created, maintaining the original values of 'k' but updating the reward value. We return this updated tuple for future interactions with the environment."}, {"title": "E. Iterating MDP Environments over Learning Rates", "content": "We implemented a Python code and iterated the three MDP environments over a range of learning rates (0.001\u20130.9). The name_list = ['env_newl', 'env_new2\u2032, 'env_new3\u2032] defines a list containing the names of the environments. We initialise an empty dictionary to store the final results and iterate over each environment using a for loop. We use the Q-learning function to convert the current learning rate to a float and store the results in dictionaries. We convert the output into a list and save it in the output dictionary. Finally, we group the collected data into a tuple and store it in the final dictionary, consolidating all results for further insight."}, {"title": "Algorithm 1 Q-Learning Implementation", "content": "1: Initialization:\n\u2022 Initialize Q-table with zeros. (defines the state of the agent)\n\u2022 Set parameters: learning rate (a), discount factor (y), exploration probability (\u0454 = 0.9), and decay schedule.\n\u2022 Initialize storage structures: storage, storage_new, reward_list.\n2: for episode = 0, 1, ..., episodes do\n3: Reset environment and variables:\n\u2022 Reset the environment to obtain the initial state.\n\u2022 Initialize episodic reward and step counter.\n\u2022 Store current epsilon value.\n4: while not done, do\n5: Select action using e-greedy policy:\n\u2022 if \u2208 < rand() then\n\u2022 action ~ Uniform(noA)\n\u2022 else\n\u2022 action = maxa Q(state, a)\n6: Execute action:\n\u2022 Act in the environment and observe the next state, reward, and done flag.\n\u2022 Update episodic reward.\n\u2022 Increment steps counter.\n7: Update Q-value using Bellman equation\n8: Compute the maximum future Q-value for the next state.\n9: Calculate the new Q-value\n10: Update the Q-table with the new Q-value.\n11: Store Q-value updates if specific conditions (e.g., state, action) are met.\n12: Update state: Set the current state to the next state.\n13: Append the (current_q, new_q, episode, action) to the storage list.\n14: Decay \u0454: Reduce epsilon based on the decay schedule.\n15: Check convergence: if new_q - current_q| < threshold and new_q \u2260 current_q then\n16: Break the loop\n17: Append (episodic_reward, episode, steps) to reward_list.\n18: Update \u0454:\n\u2022 \u2208 \u2190 \u2208 \u2212 (\u2208_decay_value \u00d7 0.5)\n19: Return results:\n\u2022 Return Q-table, storage, reward_list, and storage_new.\n20: end for"}, {"title": "Algorithm 2 Iterating Learning Rates Variation over MDP Environments", "content": "1: Initialization: Defining different envs and empty final dict\n2: for name in name_list do\n1) Assigning the right env (MDP): Using diff transition probs and rewards to create the MDP\n2) Defining and resetting the training params:\n\u2022 Learning rates list\n\u2022 outputs, store and rewards dictionaries\n3) for lr in lrs do\na) Performing the new_q_learning algorithm\nb) Storing everything by appending in the final_dict\n4) end for\n3: end for"}, {"title": "V. TESTING AND EVALUATION", "content": ""}, {"title": "A. Comparing the Speed of Convergence", "content": "We implemented a Python code to visualise the speed of convergence across the three MDP environments, (env_new1, env_new2, and env_new3) representing BlankEnvironment, BlankEnvironment_with_Rewards(), and BlankEnvironment_with_Time(), respectively. Each dictionary maps learning rates to the number of episodes required for convergence. The code line x = [float (key) for key in env_newl.keys()] creates a list of floating-point learning rates from env_new1. The command plt.figure(figsize=(10,6)) initialises a 10x6-inch plot, where we create scatter plots for each environment, using different colours (blue, red, and green) for distinction and adding lines to illustrate convergence trends. Figure 4 shows that BlankEnvironment_with_Rewards (env_new2) has the smoothest and fastest convergence. In contrast, BlankEnvironment_with_Time (env_new3) converges slowly due to a higher negative reward function, necessitating larger learning rates and more computational time. BlankEnvironment (env_new1) also performs well, but it converges slower due to learning rate fluctuations. As a result, BlankEnvironment_with_Rewards is the best MDP environment, with a 0.4 learning rate."}, {"title": "B. Command Definitions for State-Based Actions", "content": "We imported the Subprocess module to allow the Python script to spawn new processes and manage their in- put/output/error pipes and return codes. We initialise and pop- ulate an empty dictionary, my_dict, with key-value pairs, each representing a state and each value a list of commands for that state. For example, state 0 includes a command to clone a GitHub repository, while state 10 has commands for Windows system information and the registry. States 15 to 45 have various commands, some including special characters and options like -pid and -0."}, {"title": "VI. RESULTS AND DISCUSSION", "content": ""}, {"title": "A. The Agent Decision-Making Processes", "content": "We implemented a Python script that initialises two lists, ideal_list and pred_list, containing integer values representing actions for specific states within our reinforce- ment learning MDP environment. The ideal_list assigns optimal actions for states 0 to 66, while the pred_list contains predicted actions for the same states. For example, state O has an ideal action of O and a predicted action of 2. Each index in both lists corresponds to a specific state, facilitating the comparison of predicted actions against ideal outcomes to measure model performance across the three environments using varying learning rates."}, {"title": "B. Python Function to Evaluate Predictive Model Accuracy", "content": "To compare the accuracy of predicted actions against ideal actions, we implemented a Python function named get_acc. Initially, the function sets two variables, true and false, to zero to count correct and incorrect predictions, respectively. It iterates through the ideal_list and pred_list simulta- neously using the zip() method, comparing each element; if they match, it increments true; otherwise, it increments false. After the iteration, the function computes the accuracy by dividing true by the total number of comparisons (true + false), formats the result to five decimal places, and prints the accuracy. This function is useful for evaluating prediction accuracy in reinforcement learning settings, and upon execution, it shows an accuracy of 94%."}, {"title": "C. get_acc function for accuracy computation", "content": "The implemented Python function named get_acc processes multiple environments (env1, env2, and env3) represented by dictionaries (ql_dict, q2_dict, and q3_dict). We defines x and y coordinates for three sets of data representing different environments: env1, env2, and env3. Each envi- ronment's data is stored in respective lists, such as env1_x and env1_y, env2_x and env2_y, and env3_x and env3_y. The code then creates three scatter plot traces using go.Scatter, specifying the data points, mode (lines and markers), names, and marker colours for each environment. A layout is defined for the plot, including a title, x-axis and y-axis labels, and hover mode configuration. A figure object is created by combining the traces and the layout, and the plot is displayed using fig.show(). This code effectively visualises the accuracy computation for different learning rates across three MDP environments, as shown in Figure 6. Consequently, it demonstrates that env2, with a learning rate of 0.4, is the best-performing environment."}, {"title": "D. Plotting the proposed Model command execution timings", "content": "As a result of keeping track of state changes using our proposed reinforcement learning post-incident malware in- vestigation model, we obtain a trajectory based on actions and landing states, which control a series of state changes in the environment. We utilised the Google Collaborative Environment's execution timings to plot the proposed model's command execution timings. To store keys and values re- lated to states and action trajectories, we created a new command_timings_dict. We then defined new Python code to create a multi-plot figure using Plotly to analyse the execution time of different malware commands (WannaCry, Cerber, and Cridex). This code initialises a figure with three vertical subplots, each with a title and increased vertical spacing. We add line plots for WannaCry, Cerber, and Cridex to the first, second, and third subplots, respectively, ensuring each has distinct colours and markers. We update the figure's layout to set its dimensions and centre the title. We customize the X-axis labels for each subplot and label the y-axes with 'Time (seconds)'. The resulting graph is displayed using fig.show(), as illustrated in Figure 7 below."}, {"title": "E. Research Findings and Recommendations", "content": "This research examines post-incident malware forensics using reinforcement learning (RL), emphasising RL's growing im- portance in adapting to evolving malware threats. By automat- ing forensic tasks, particularly malware artefact identification, through a structured RL workflow based on Q-learning, the model effectively identifies and classifies malware. However, its performance depends on the diversity of malware samples, necessitating broader datasets to improve accuracy. Optimis- ing RL models for computational efficiency is also critical, especially in resource-limited environments. Integration with existing security infrastructure is recommended, combining RL's adaptive learning with traditional forensic processes for a more robust defense. Furthermore, the ethical implications of AI deployment in cybersecurity must be considered to protect against adversarial attacks. Finally, this model could mitigate miscarriages of justice in the UK's legal system [17] by introducing a data-driven and unbiased approach to digital forensic analysis, enhancing judicial outcomes by improving accuracy and reducing errors."}, {"title": "VII. CONCLUSION", "content": "This paper presents a novel reinforcement learning (RL) model and framework for post-incident malware forensics investigations, designed to surpass the capabilities of human forensic experts. The model accelerates malware analysis and identifies both known and unknown threats by integrating various techniques, such as live memory dumps from Windows systems. A unified Markov Decision Process (MDP) frame- work was developed, featuring three environments: BlankEn- vironment, BlankEnvironment_with_Rewards, and BlankEnvi- ronment_with_Time, each with distinct reward mechanisms to optimise malware analysis. The RL agent, utilising Q- learning and epsilon-greedy exploration, iteratively refines its policy and decision-making process, improving malware identification accuracy. Experimental tests, including simula- tions with malware like WannaCry and Cerber, demonstrated that performance depends on learning rates and environment complexity. The study focusses on hyperparameter tuning and continuous learning to improve the performance of RL models. It shows that reward systems, feature extraction, and hybrid analysis could all use more optimisation."}]}