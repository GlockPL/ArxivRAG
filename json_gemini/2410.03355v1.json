{"title": "LANTERN: ACCELERATING VISUAL AUTOREGRESSIVE MODELS WITH RELAXED SPECULATIVE DECODING", "authors": ["Doohyuk Jang", "Sihwan Park", "June Yong Yang", "Yeonsung Jung", "Jihun Yun", "Souvik Kundu", "Sungyub Kim", "Eunho Yang"], "abstract": "Auto-Regressive (AR) models have recently gained prominence in image generation, often matching or even surpassing the performance of diffusion models. However, one major limitation of AR models is their sequential nature, which processes tokens one at a time, slowing down generation compared to models like GANs or diffusion-based methods that operate more efficiently. While speculative decoding has proven effective for accelerating LLMs by generating multiple tokens in a single forward, its application in visual AR models remains largely unexplored. In this work, we identify a challenge in this setting, which we term token selection ambiguity, wherein visual AR models frequently assign uniformly low probabilities to tokens, hampering the performance of speculative decoding. To overcome this challenge, we propose a relaxed acceptance condition referred to as LANTERN that leverages the interchangeability of tokens in latent space. This relaxation restores the effectiveness of speculative decoding in visual AR models by enabling more flexible use of candidate tokens that would otherwise be prematurely rejected. Furthermore, by incorporating a total variation distance bound, we ensure that these speed gains are achieved without significantly compromising image quality or semantic coherence. Experimental results demonstrate the efficacy of our method in providing a substantial speed-up over speculative decoding. In specific, compared to a na\u00efve application of the state-of-the-art speculative decoding, LANTERN increases speed-ups by 1.75\u00d7 and 1.76\u00d7, as compared to greedy decoding and random sampling, respectively, when applied to LlamaGen, a contemporary visual AR model.", "sections": [{"title": "INTRODUCTION", "content": "Auto-Regressive (AR) models have recently gained significant traction in image generation (Ramesh et al., 2021; Chen et al., 2020; Tian et al., 2024; Sun et al., 2024) due to their competitive performance, often matching or even surpassing diffusion models (Ho et al., 2020; Rombach et al., 2022). Notable examples include iGPT (Chen et al., 2020), DALL-E (Ramesh et al., 2021), VAR (Tian et al., 2024), and LlamaGen (Sun et al., 2024), which showcase the potential of AR models in image generation. Moreover, recent studies like Team (2024); Lu et al. (2023) have demonstrated that AR modeling can handle multi-modal data, including language and images, within a single unified framework. Given the remarkable success of AR models in language modeling, leading to the era of large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023; Jiang et al., 2023), it is anticipated that AR modeling will emerge as a dominant paradigm for unifying multiple modalities into a single model in the near future."}, {"title": "TOKEN SELECTION AMBIGUITY IN VISUAL AUTOREGRESSIVE MODELS", "content": "In this section, we introduce a novel problem, termed token selection ambiguity, which imposes a major challenge for applying speculative decoding into visual AR models. In Section 2.1, we provide empirical results on na\u00efve application of existing speculative decoding methods into visual AR models. Section 2.2 presents the token selection ambiguity problem and how this problem causes the failure of speculative decoding in visual AR models."}, {"title": "NA\u00cfVE SPECULATIVE DECODING FAILS IN VISUAL AR MODELS", "content": "While speculative decoding has been successful in LLMs (Leviathan et al., 2023; Cai et al., 2024; Li et al., 2024a), we observe that its na\u00efve application to visual AR models fails to provide noticeable speed-up."}, {"title": "TOKEN SELECTION AMBIGUITY PROBLEM", "content": "To understand the reason for the failure of speculative decoding in visual AR models, we identify a unique problem that we term as the token selection ambiguity. This problem stems from the fundamental differences between language and image data, as well as the distinct tokenization strategies employed in their respective AR models."}, {"title": "DETOURING TOKEN SELECTION AMBIGUITY THROUGH LATENT SPACE", "content": "In this section, we propose LANTERN, a simple yet effective method that permits detouring the failure of speculative decoding caused by the token selection ambiguity problem by relaxing acceptance condition in Speculative Decoding (Leviathan et al., 2023). In Section 3.1, we introduce a concept of latent proximity which asserts close image tokens in the latent space are interchangeable and examines its validity on the generated images. Section 3.2 describes how we relax the acceptance condition based on the interchangeability. In Section 3.3, we present another component to ensure that the distribution of generated images does not catastrophically deviate from the original distribution."}, {"title": "LATENT PROXIMITY PERMITS TOKEN INTERCHANGEABILITY", "content": "We introduce latent proximity, a property in visual AR models that asserts tokens close to one another in latent space are interchangeable without significantly affecting the visual semantics or overall image quality. This means that replacing a token with another nearby token in latent space results in minimal changes to the generated image.\nThis property arises from the tokenization process unique to visual AR models. Unlike text tokenization, which is straightforward due to its discrete nature, images are spatially continuous,"}, {"title": "LANTERN: RELAXATION OF ACCEPTANCE CONDITION", "content": "Building on our findings about latent proximity, we introduce LANTERN, a simple yet effective solution that leverages the interchangeability of proximate tokens in latent space. By treating neighboring tokens as commutable, LANTERN effectively resolves the token selection ambiguity problem, significantly boosting the acceptance probability of candidate tokens and enabling the successful application of speculative decoding.\nWe start with revisiting the original acceptance condition from Leviathan et al. (2023). The drafter model with distribution p(x) samples a candidate token $\\tilde{x} \\sim p(x|s)$ given a preceding sequence $s = (x_1,...,x_n)$. The candidate is accepted with probability\n$\\min \\Big(1, \\frac{q(\\tilde{x}|s)}{p(\\tilde{x}|s)}\\Big)$   (1)\nwhere $q(x)$ is the target model's distribution. If rejected, the next token is re-sampled from $[q(\\cdot|s) - p(\\cdot|s)]_+$, where $[\\cdot]_+$ denotes normalization. Acceptance depends on the alignment of probabilities between the drafter and target models.\nHowever, this acceptance condition results in a sharp decline in accept probability when encountering the token selection ambiguity problem."}, {"title": "WITH LIMITED DISTRIBUTIONAL DIVERGENCE", "content": "Although the relaxed acceptance condition (2) effectively permits speculative decoding in visual AR models by significantly raising the accept probability, it inevitably distorts the target distribution. In particular, when we condition the target distribution on the candidate token $\\tilde{x}$, it becomes:\n$q_k(x|s, D = \\tilde{x}) = \\begin{cases}\n\\frac{\\sum_{x \\in B_k(\\tilde{x})} q(x|s)}{\\sum_{x \\in B_k(\\tilde{x})} q(x|s)} & \\text{if } x = \\tilde{x} \\\\\n0 & \\text{if } x \\in B_k(\\tilde{x}), x \\neq \\tilde{x} \\\\\nq(x|s) & \\text{otherwise}\n\\end{cases}$\nwhere D is a random variable representing the candidate token and $q_e$ denotes the distorted target distribution. In contrast, under the original acceptance condition, the target distribution remains unchanged regardless of the candidate token. For this reason, (2) may excessively distort the target distribution, leading to generating images that diverge significantly from those generated by the target model.\nTo mitigate this distortion, we impose an upper bound on the distributional divergence using total variation distance (TVD). Since the distortion results from redistributing probability mass, TVD effectively measures the extent of this shift, allowing us to control the magnitude of the divergence. This can be achieved by adjusting the neighborhood $B_k(\\tilde{x})$ used in the relaxation as follows.\nSince the relaxation can be analogously derived using any neighborhood of $\\tilde{x}$, we can find a neighborhood that ensures the TVD between the target distribution and the distorted target distribution induced by the neighborhood is below a specific threshold. To formulate this approach, we define the neighborhood $A_{k,\\delta}(\\tilde{x})$ of $\\tilde{x}$ for a given TVD bound $\\delta > 0$ and $k \\in \\mathbb{Z}^+$ as $A_{k,\\delta}(\\tilde{x})$ is the largest subset of $B_k(\\tilde{x})$ such that for the total variation distance $D_{TV}$,\n$D_{TV}(q_{k,\\delta}(x|s, D = \\tilde{x}), q(x|s, D = \\tilde{x})) = D_{TV}(q_{k,\\delta}(x|s, D = \\tilde{x}), q(x|s)) < \\delta$\nwhere $q_{k,\\delta}$ denotes the distorted target distribution induced by $A_{k,\\delta}(\\tilde{x})$."}, {"title": "EXPERIMENTS", "content": "In this section, we evaluate the performance of our method, LANTERN. In section 4.1, we report the experimental setup for our experiments. Section 4.2 evaluates LANTERN with other baselines in perspective of image quality and acceleration. In Section 4.3, we conduct an ablation study to clarify the effectiveness of each component in our method."}, {"title": "EXPERIMENTAL SETUP", "content": "To validate our method LANTERN, we conduct experiments on LlamaGen-XL (Sun et al., 2024) for the text-conditional image generation as target model that demonstrate the best performance among autoregressive models without vision-specific modifications. We utilize the MS-COCO validation captions (Lin et al., 2014a) to generate images and evaluate the image quality with the ground-truth images. For the drafter model, we employ EAGLE-2 (Li et al., 2024a), which has demonstrated state-of-the-art performance in speculative decoding in the language domain. We employ the $l_2$ distance to quantify latent proximity, and utilize the TVD as a metric for divergence bound.\nTo assess the improvement of speed, we use 100 MS-COCO validation captions to evaluate actual speedup and mean accepted length. Since measuring speedup with more than 100 samples shows no significant difference, we use 100 captions for efficiency. Actual speedup is measured by the inference time ratio between each method and vanilla auto-regressive decoding. Mean accepted length is determined by the average number of tokens accepted in each forward step of the target model. We evaluate each method in both the greedy decoding setting with $\\tau = 0$ and the sampling with $\\tau = 1$.\nSince LANTERN can impact the quality of generated images, we evaluate image quality with FID (Heusel et al., 2017) and CLIP score (Hessel et al., 2021) with 30k samples. For each evaluation, Note that existing speculative decoding theoretically guarantees exact distribution matching with the target model, we do not evaluate image quality on EAGLE-2."}, {"title": "MAIN RESULTS", "content": "In this section, we demonstrate qualitative and quantitative results of speculative decoding with our relaxed acceptance condition. First of all, Section 4.2.1 demonstrates how much speed-up can be achieved through our method. Next, Section 4.2.2 showcases that our method retains image quality within a similar level by showing qualitative samples. Afterward, Section 4.2.3 presents the trade-off between performance and efficiency in our method."}, {"title": "SPEED UP COMPARISON", "content": "To confirm that LANTERN provides notable speed improvements over the baseline while maintaining image quality, we compare our method with baselines under both greedy decoding and random sampling situations."}, {"title": "QUALITATIVE RESULTS", "content": "To confirm that image quality is preserved with LANTERN, as indicated by the FID score, we conduct qualitative analysis. Figure 4 demonstrates that, despite the modification of the target model's probability distribution to achieve acceleration, our method effectively preserves image quality. Notably, even under the setting of $\\delta = 0.4$ and $k = 1000$, which achieve about 1.6\u00d7 speedup compared to the vanilla decoding, generated images retain both content and style at a level comparable to standard autoregressive decoding. These qualitative results, along with the fact that LANTERN avoids significant degradation in FID score as shown in the previous section, demonstrate that it effectively preserves image quality while increasing efficiency."}, {"title": "TRADE-OFF BETWEEN PERFORMANCE AND EFFICIENCY", "content": "LANTERN provides various options between quality and efficiency to the end users. Therefore, we explore this trade-off by adjusting k and \u03b4 across different settings. Figure 5 illustrates the relationship between image quality, which is measured by FID, and speedup, which is assessed with mean accepted length for various settings of \u03b4 and k, under both sampling (\u03c4 = 1) and greedy decoding (\u03c4 = 0). The trade-off curves highlight that increasing \u03b4 and k generally improves speedup, but at the expense of image quality.\nIn the case of greedy decoding, we observe that while larger \u03b4 tends to increase the speedup, it may not always result in substantial acceleration when k is small. This could be attributed to the inherent nature of greedy decoding, which only accepts the token with the top-1 probability. If k is not large enough, the increase in acceptance probability is insufficient to surpass the top-1 threshold, thus limiting the speedup gains.\nFor sampling, note that we observe different behaviors. When \u03b4 is small such as 0.05 and 0.1, increasing k maintains image quality while still achieving better acceleration. This suggests that by tuning \u03b4 and selecting an appropriate k, we can find an optimal trade-off where the speedup is improved without significantly compromising image quality. Specifically, larger k values allow for faster generation without disproportionately degrading FID, particularly at lower \u03b4 values.\nThese findings confirm that LANTERN allows flexible tuning of performance versus efficiency, where careful adjustment of \u03b4 and k can yield substantial acceleration with minimal quality loss. By setting appropriate hyperparameters, particularly under sampling, we can achieve significant speedups while preserving the quality of images, offering a better balance compared to the original speculative decoding method."}, {"title": "ABLATION STUDY", "content": "In this section, we conduct ablation studies for LANTERN. In Section 4.3.1, we assess the impact of the metric used to measure latent proximity on performance. Then, in Section 4.3.2, we provide an ablation study on the effect of the metric for measuring distance from the modified probability distribution."}, {"title": "NEAREST LATENT SELECTION", "content": "To explore the impact of various metrics for measuring latent proximity, we conduct an ablation study using representative distance metrics commonly used to measure latent proximity. This experiment aims to assess the role of proximity-based selection in token aggregation, with k = 1000 used across all methods.\nAs shown in Table 4, the random selection significantly underperforms in terms of acceleration, achieving only a 1.26 mean accepted length, which is notably lower than both $l_2$ distance and cosine similarity. Additionally, the random selection shows inferior acceleration compared to the $l_2$ distance with \u03b4 = 0.05, while maintaining a similar FID, which highlights the importance of token selection based on latent proximity.\nComparing $l_2$ distance and cosine similarity, both methods demonstrate comparable performance, suggesting robustness in our approach to proximity measurement. This suggests that our method is effective regardless of the specific proximity metric considered, offering flexibility without significant trade-offs in performance. These results confirm that selecting tokens based on latent proximity plays a crucial role in both acceleration and image quality, with proximity-based metrics like $l_2$ distance and cosine similarity, providing a clear advantage over random selection."}, {"title": "DISTANCE BETWEEN PROBABILITY DISTRIBUTION", "content": "To ensure that the modified target distribution remains within an acceptable range of divergence from the original, we introduce \u03b4 as an upper bound for distributional divergence. To validate the impact of the divergence metric, we evaluate two different metrics to measure this divergence: Total Variation Distance (TVD) and Jensen-Shannon Divergence (JSD). Kullback-Leibler Divergence (KLD) is not used as it is asymmetric and not a valid metric for a distance in mathematical sense.\nTo compare the effectiveness of these distance metrics, we fix k as 1000 and adjust d to achieve similar mean accepted lengths across the different methods. The results, shown in Table 4, demonstrate that JSD with \u03b4 = 0.2 results in a performance that achieves between TVD with \u03b4 = 0.2 and with \u03b4 = 0.4.\nThese results confirm that our method consistently functions as a robust trade-off controller regardless of the chosen distance metric. Since the difference in performance between two metrics is marginal, we opt to use TVD, as it is computationally lighter and thus more efficient for large-scale implementations."}, {"title": "CONCLUSIONS", "content": "In this paper, we explored the application of speculative decoding to visual AR models for the first time. We revealed that the na\u00efve application of existing methods fails due to the token selection ambiguity problem. To address this, we proposed LANTERN, a novel relaxed acceptance condition that effectively resolves this problem. Our experiments using the state-of-the-art visual AR model and speculative decoding method demonstrated that LANTERN successfully enables speculative decoding in visual AR models, achieving substantial speed-ups with minimal compromise in image generation performance. For future work, we plan to design a drafter specifically tailored to visual AR models, aiming to achieve acceleration without sacrificing the generation performance."}, {"title": "RELATED WORKS", "content": "Visual Autoregressive Models Based on the advancements of autoregressive (AR) learning in natural language processing (Brown et al., 2020; Touvron et al., 2023; Jiang et al., 2023), autoregressive models have been extended to generative vision tasks by tokenizing images into discrete tokens arranged on a 2D grid and defining a unidirectional token sequence. Early studies (Esser et al., 2021; Yu et al., 2021; Lee et al., 2022) employ row-major raster scan, spiral, or z-curve orders to generate sequences, but these approaches are computationally inefficient and demonstrated inferior performance compared to diffusion models. In response, recent visual AR models (Sun et al., 2024; Tian et al., 2024) introduce a novel tokenizer and multi-scale, coarse-to-fine ordering strategy, establishing a scalable paradigm that surpasses diffusion models. More recently, Chameleon (Team, 2024) propose a multi-modal AR model capable of generating both images and text within a unified framework, demonstrating strong versatility across diverse tasks.\nSpeculative Decoding for LLMS LLM inference is known to be memory-bounded, which means that the computation (or processing) is blocked by the slow transfer of data (e.g., model parameters) (Leviathan et al., 2023). To address the issue, the speculative decoding (Leviathan et al., 2023) has been proposed to make more computations at a single decoding step by attaching the draft sequences after the usual input tokens. Speculative decoding theoretically guarantee that prediction results with speculative decoding exactly match with target models distribution. Speculative decoding uses a smaller model which is trained on the same dataset as a drafter, thus it involves significant extra latency by running another model (drafter) and it hinders the actual acceleration.\nMedusa (Cai et al., 2024) proposed a light-weighted drafter where the size is similar to the 1m_head of the target model and suggested to use somewhat lenient acceptance condition called typical acceptance rather than speculative sampling. Although the typical acceptance does not provide any fundamental guarantee about the resulting distribution, they show that the results' quality is maintained compared to the target model. EAGLE (Li et al., 2024a) proposes to make the drafter slightly heavier than Medusa by utilizing single autoregressive layers, but it dramatically improves the acceleration.\nIn this work, we mainly follow the EAGLE as a base method since it exhibits the best performance in LLM literature. Based on its its drafter architecture and training settings, we trained drafter on image data and adapt it for speculative decoding.\nMulti-modal speculative decoding Recently, Gagrani et al. (2024) proposes a speculative decoding method for the multi-modal large language models (MLLMs). They mainly focus on the model, which processes both images and texts as input and generates texts such as LLaVA. Note that we focus on the visual autoregressive models regarding image generation in this work, which is a totally different model with the scope of Gagrani et al. (2024)."}, {"title": "ALGORITHM", "content": "Algorithm 1 LANTERN\nInput: Target model q(\u00b7|\u00b7), draft model p(\u00b7|\u00b7), initial sequence x0,...,xt, drafted sequence length L, minimum target sequence length T, Drv tolerance \u03b4 > 0, and maximum cardinality of latent neighborhood k.\nInitialize: n \u2190 t.\nwhile n < T do\nfor t = 1, ..., L do\nSample draft autoregressively It ~ p(x|x0, ..., xn, X1, ..., Xt\u22121)\nend for\nIn parallel, compute L + 1 sets of logits from drafts X1,...,XL:\nq(x|x0,...,xn), q(x|x0,..., xn, X1), ..., q(x|x0, ..., Xn, X1, ..., XL)\nFind the neighborhood Ak,s(It).\nSampler ~ U[0, 1] from an uniform distribution.\nif r < min$\\Big(1, \\frac{\\sum_{x \\in A_{k,\\delta}(\\tilde{x}_t)} q(x|x_0,...,x_{n+t-1})}{p(\\tilde{x}_t|x_0,...,x_{n+t-1})}\\Big)$ then\nSet Xn+t It and n \u2190 n + 1.\nelse\nSample Xn+t ~ (qk,s(x|x0,...,Xn+t\u22121,D = Xt) - p(x|x0,...,Xn+t-1))+\nexit the loop\nend if\nend for\nIf all drafts are accepted, sample an extra token Xn+L+1 ~ q(x|x0,...,Xn+L).\nend while\nOutput: Xn+1,..., Xn+L or Xn+1,...,Xn+L+1"}, {"title": "EXPERIMENTAL DETAILS", "content": "To train text-conditional model's drafter, we sampled 100k images in LAION-COCO dataset (Chuhmann et al., 2022), which is used to train stage-1 target model. We used same amount of image sampled in ImageNet (Deng et al., 2009) dataset to train class-conditional model's drafter. Since EAGLE (Li et al., 2024a) reports that benefits of using target model generated sequence as a training data is marginal, we do not use target model generated sequence as a training data to reduce overhead.\nWe used a single-layer decoder with the same structure as the target model, in the same manner of EAGLE. During training, 5% of data is set to be hold out validation dataset.\nSince LlamaGen (Sun et al., 2024) use classifier-free guidance (Ho & Salimans, 2021) to generate image, we trained our drafter to both learn conditioned input and null-conditioned input. To do so, we dropped 10% of conditional embedding during trainig, as same as target model training. Batch size is 16 and base learning rate is 10-4. AdamW (Loshchilov & Hutter, 2019) optimizer with \u03b2\u2081 = 0.9 and B2 = 0.95 is used and Linear learning rate scheduling with warm up is used with 2000 warm up steps. We select best performing model in terms of top-3 accuracy in hold out validation set for 20 epochs. In addition, Flan-T5 XL (Chung et al., 2022) is used to encode input text for text-conditional generation.\nAll text-conditional images are generated using a classifier-free guidance scale of 7.5, with top-p set to 0 and top-k set to 1000, which is the default generation configuration of LlamaGen (Sun et al., 2024) official implementation for text-conditional image generation. For class-conditional generation, the classifier-free guidance scale is set to 4.0, with the top-k sampling covering the entire vocabulary and top-p sampling set to 1.0. For EAGLE-2 and our method, 60 candidate tokens are passed into target model for each verification process."}]}