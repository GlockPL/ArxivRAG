{"title": "Measuring Goal-Directedness", "authors": ["Matt MacDermott", "James Fox", "Tom Everitt", "Francesco Belardinelli"], "abstract": "We define maximum entropy goal-directedness (MEG), a formal measure of goal-\ndirectedness in causal models and Markov decision processes, and give algorithms\nfor computing it. Measuring goal-directedness is important, as it is a critical\nelement of many concerns about harm from AI. It is also of philosophical interest,\nas goal-directedness is a key aspect of agency. MEG is based on an adaptation of\nthe maximum causal entropy framework used in inverse reinforcement learning. It\ncan measure goal-directedness with respect to a known utility function, a hypothesis\nclass of utility functions, or a set of random variables. We prove that MEG satisfies\nseveral desiderata and demonstrate our algorithms with small-scale experiments.", "sections": [{"title": "1 Introduction", "content": "In order to build more useful Al systems, a natural inclination is to try to make them more agentic.\nBut while agents built from language models are touted as the next big advance [Wang et al., 2024],\nagentic systems have been identified as a potential source of individual [Dennett, 2023], systemic\n[Chan et al., 2023, Gabriel et al., 2024], and catastrophic [Ngo et al., 2022] risks. Agency is thus\na key focus of behavioural evaluations [Shevlane et al., 2023, Phuong et al., 2024] and governance\n[Shavit et al., 2023]. Some prominent researchers have even called for a shift towards designing\nexplicitly non-agentic systems [Dennett, 2017, Bengio, 2023].\nA critical aspect of agency is the ability to pursue goals. Indeed, the standard theory of agency defines\nagency as the capacity for intentional action \u2013 action that can be explained in terms of mental states\nsuch as goals [Schlosser, 2019]. But when are we justified in ascribing such mental states? According\nto Daniel Dennett's instrumentalist philosophy of mind [1989], we are justified when doing so is\nuseful for predicting a system's behaviour.\nThis paper's key contribution is a method for formally measuring goal-directedness based on Dennett's\nidea. Since pursuing goals is about having a particular causal effect on the environment, defining\nour measure in a causal model is natural. Causal models are general enough to encompass most\nframeworks popular among ML practitioners, such as single-decision prediction, classification, and\nregression tasks, as well as multi-decision (partially observable) Markov decision processes. They\nalso offer enough structure to usefully model many ethics and safety problems [Everitt et al., 2021a,\nWard et al., 2024a, Richens et al., 2022, Richens and Everitt, 2024, Everitt et al., 2021b, Ward et al.,\n2024b, Halpern and Kleiman-Weiner, 2018, Wachter et al., 2017, Kusner et al., 2017, Kenton et al.,\n2023, Fox et al., 2023, Richens and Everitt, 2024, MacDermott et al., 2023]."}, {"title": "2 Background", "content": "We use capital letters for random variables V, write dom(V) for their domain (assumed finite), and\nuse lowercase for outcomes v \u2208 dom(V). Boldface denotes sets of variables V = {V1, ..., Vn}, and\ntheir outcomes v \u2208 dom(V) = X; dom(Vi). Parents and descendants of V in a graph are denoted\nby Pay and Descy, respectively (where pay and descy are their instantiations).\nCausal Bayesian networks (CBNs) are a class of probabilistic graphical models used to represent\ncausal relationships between random variables [Pearl, 2009].\nDefinition 2.1 (Causal Bayesian network). A Bayesian network M = (G, P) over a set of variables\nV = {V1,..., Vn } consists of a joint probability distribution P which factors according to a directed\nacyclic graph (DAG) G, i.e., P(V\u2081, ..., Vn) = \u03a0\u22121 P(Vi | Pav\u2081), where Pav\u2081 are the parents of Vi\nin G. A Bayesian network is causal if its edges represent direct causal relationships, or formally\nif the result of an intervention do(X = x) for any X C V can be computed using the truncated\nfactorisation formula: P(v | do(X = x)) = \u03a0i:vifxP(vi | pav\u2081) if v is consistent with a or\nP(v | do(X = x)) = 0 otherwise.\nFigure 1b depicts Example 1 as a CBN, showing the causal relationships between the location of the\ncheese (S), the mouse's behavioural response (D), and whether the mouse obtains the cheese (T).\nWe are interested in to what extent a set of random variables in a CBN can be seen as goal-directed.\nThat is, to what extent can we interpret them as decisions optimising a utility function? In other words,\nwe are interested in moving from a CBN to a causal influence diagram (CID), a type of probabilistic\ngraphical model that explicitly identifies decision and utility variables.\nDefinition 2.2 (Causal Influence Diagram [Everitt et al., 2021a]). A causal influence diagram (CID)\nM = (G, P) is a CBN where the variables V are partitioned into decision D, chance X, and\nutility variables U. Instead of a full joint distribution over V, P consists of conditional probability\ndistributions (CPDs) for each non-decision variable V \u2208 V \\ D.\nA CID can be combined with a policy \u03c0, which specifies a CPD TD for each decision variable D, in\norder to obtain a full joint distribution. We call the sum of the utility variables the utility function and\ndenote it U = \u03a3\u03c5\u03b5\u03c5 U. Policies are evaluated by their total expected utility E\u201e[U]. We write unif\nfor the uniformly random policy.\nCIDs can model a broad class of decision problems, including Markov decision processes (MDPs)\nand partially observable Markov decision processes (POMDPs) [Everitt et al., 2021b]."}, {"title": "3 Measuring goal-directedness with respect to a known utility function", "content": "Maximum Entropy Goal-directednes. Dennett's instrumentalist approach to agency says that\nwe can ascribe mental states (such as utilities) to a system to the extent that doing so is useful for\npredicting its behaviour [Dennett, 1989]. To operationalise this, we need a model of what behaviour\nis predicted by a utility function. According to the principle of maximum entropy [Jaynes, 1957],\nwe should choose a probability distribution with the highest entropy distribution satisfying our\nrequirements, thus minimising unnecessary assumptions (following Occam's razor). We can measure\nthe entropy of a policy by the expected entropy of its decision variables conditional on their parents\n$H_{\\pi}(D \\| Pa_D) = -\\sum_{D\\in\\mathbb{D}} E_{d,pa_d\\sim P} \\log \\pi_D(d \\| Pa_D)$. This is Ziebart et al. [2010]'s causal\nentropy, which we usually refer to as just the entropy of \u03c0."}, {"title": "4 Measuring goal-directedness without a known utility function", "content": "In many cases where we want to apply MEG, we may not know exactly what utility function the\nsystem could be optimising. For example, we might suspect that a content recommender is trying to\ninfluence a user's preferences, but may not have a clear hypothesis as to in what way. Therefore, in\nthis section, we extend our definitions for measuring goal-directedness to the case where the utility\nfunction is unknown.\nWe first extend CIDs to include multiple possible utility functions.\nDefinition 4.1. A parametric-utility CID (CID) M\u00ae is a set of CIDs {M\u00ba | \u03b8 \u2208 \u0398} which differ\nonly in the CPDs of their utility variables.\nIn effect, a parametric CID is a CID with a parametric class of utility functions U\u00ae.\nThe maximum entropy policy set from Definition 3.1 is extended accordingly to include maximum\nentropy policies for each utility function and each attainable expected utility with respect to it.\nDefinition 4.2 (Maximum entropy policy set, unknown utility function). Let M\u00ba = (G, P) be a\nparametric-utility CID with decision variables D and utility function U. The maximum entropy\npolicy set for U\u00ae is the set of maximum entropy policies for any attainable expected utility for any\nutility function in the class: $I_{maxent} := \\cup_{\\theta\\in \\Theta,att(U_{\\theta})} I_{maxent}$.\nDefinition 4.3 (MEG, unknown utility function). Let M\u00b3 = (G, P) be a parametric-utility CID\nwith decision variables D_and utility function U. The maximum entropy goal-directedness of a\npolicy \u03c0 with respect to U\u00ba is MEGye(\u03c0) = maxueu\u0473 MEGu(\u03c0).\nDefinition 4.4 (MEG, target variables). Let M = (G, P) be a CBN with variables V. Let D C V\nbe a hypothesised set of decision variables and T C V be a hypothesised set of target variables. The\nmaximum entropy goal-directedness of D with respect to T, MEGT(D), is the goal-directedness of\n\u03c0 = P(D | PaD) in the parametric CID with decisions D and utility functions U : dom(T) \u2192 R\n(i.e., the set of all utility functions over T).\nFor example, if we only suspected that the mouse in Example 1 was optimising some function of the\ncheese T, but didn't know which one, we could apply Definition 4.4 to consider the goal-directedness\ntowards T under any utility function defined on T. Thanks to translation and scale invariance\n(Proposition 3.1), there are effectively only three utility functions to consider: those that provide\nhigher utility to cheese than not cheese, those that do the opposite, and those that are indifferent.\nNote that T has to include some descendants of D, in order to enable positive MEG (Proposition 3.3).\nHowever, it is not necessary that T consists of only descendants of D (i.e., T need not be a subset of\nDesc(D)). For example, goal-conditional agents take an instruction as part of their input PaD. The\ngoal-directedness of such agents can only be fully appreciated by including the instruction in T.\nPseudo-terminal goals. Definition 4.4 enable us to state a result about a special kind of instrumental\ngoal. It is well known that an agent that optimises some variable has an instrumental incentive to\ncontrol any variables which mediate between the two [Everitt et al., 2021a]. However, since the\nagent might want the mediators to take different values in different circumstances, it need not appear\ngoal-directed with respect to the mediators. Theorem 4.1 shows that in the special case where the\nmediators d-separate the decision from the downstream variable, the decision appears at least as\ngoal-directed with respect to the mediators as with respect to the target.\nTheorem 4.1 (Pseudo-terminal goals). Let M = ((V, E), P) be a CBN. Let D,T,S \u2286 V such\nthat DT | S. Then MEGT(D) < MEGs(D).\nFor example, in Figure 2, the agent must be at least as goal-directed towards S3 as it is towards U3,\nsince S3 blocks all paths (i.e. d-separates) from {D1, D2} to U3.\nIntuitively, this comes about because, in such cases, a rational agent wants the mediators to take the\nsame values regardless of circumstances, making the instrumental control incentive indistinguishable\nfrom a terminal goal. This means we do not have to look arbitrarily far into the future to find\nevidence of goal-directedness. An agent that is goal-directed with respect to next week must also be\ngoal-directed with respect to tomorrow."}, {"title": "5 Computing MEG in Markov Decision Processes", "content": "In this section, we give algorithms for computing MEG in MDPs. First, we define what an MDP\nlooks like as a CID. We then establish that soft value iteration can be used to construct our maximum\nentropy policies, and give algorithms for computing MEG when the utility function is known or\nunknown.\nNote that in order to run these algorithms, we do not need an explicit causal model, and so we do not\nhave to worry about hidden confounders. We do, however, need black-box access to the environment\ndynamics, i.e., the ability to run different policies in the environment and measure whatever variables\nwe are considering utility functions over.\nDefinition 5.1. A Markov Decision Process (MDP) is a CID with variables {St, Dt, Ut}_1, decisions\nD = {D}=1 and utilities U = {Ut}=1, such that for t between 1 and n, Pap\u2081 = {St}, PaUt\n{St}, while Pas\u2081 = {St\u22121, Dt-1} for t > 1, and Pas\u2081 = 0.\nConstructing Maximum Entropy Policies In MDPs, Ziebart's soft value iteration algorithm can\nbe used to construct maximum entropy policies satisfying a set of linear constraints. We apply it to\nconstruct maximum entropy policies satisfying expected utility constraints.\nDefinition 5.2 (Soft Q-Function). Let M = (G, P) be an MDP. Let \u03b2 \u2208 R \\ {0}. For each Dt \u2208 D\nwe define the soft Q-function $Q_{soft}$ : dom(D) \u00d7 dom(Pa\u2081) \u2192 R via the recursion:\n$Q^{soft}_U(d_t | pa_t) = E [ U_t + \\frac{1}{\\beta}logsumexp(Q^{soft}_{U,t+1}(.\\| Pa_{D_{t+1}})) \\| d_t, pa_t ]$ for t < n,\n$Q^{soft}_U(d_n | pa_n) = E [U_n | d_n, pa_n]$,\nwhere $logsumexp (Q^{soft}_{U,t+1}(.\\| Pa_{D_{t+1}})) = log \\sum_{d_{t+1}\\in dom(D_{t+1})} exp(\\beta Q^{soft}_{U,t+1}(d_{t+1} \\| Pa_{D_{t+1}}))$.\nUsing the soft Q-function, we show that there is a unique \u03c0\u2208 Imaxent for each U and u in MDPs.\nTheorem 5.1 (Maximum entropy policy in MDPs). Let M = (G, P) be an MDP with utility function\nU, and let u \u2208 att(U) be an attainable expected utility. Then there exists a unique maximum entropy\npolicy maxente Imaxent, and it has the form\n$\n \\pi^{U,u}(d_t | pa_t) := \\left\\{\n\\begin{array}{ll}\n\\pi^{soft}_\\beta(d_t | pa_t) := \\frac{exp(\\beta \\cdot Q^{soft}_U(d_t \\| pa_t))}{\\sum_{d'_t\\in dom(D_t)} exp(\\beta \\cdot Q^{soft}_U(d'|pa_t))}, & if \\beta \\neq 0 \\\\\n\\pi_{unif}(d_t | pa_t), & if \\beta = 0.\n\\end{array}\n\\right.\n$ (3)\nwhere B = argmax\u03b2'\u2208RU{\u221e,-\u221e} \u03a3\u03b5 \u0395\u03c0 [log((dt | pat))].\nProof. In an MDP, the expected utility is a linear function of the policy, so the attainable utility set is\na closed interval att(U) = [Umin, Umax]. We first consider the case where u \u2208 (Umin, Umax).\nIn this case, we are seeking the maximum entropy policy in an MDP with a linear constraint satisfiable\nby a full support policy (since u is an interior point), so we can invoke Ziebart's result on the form of\nsuch policies [Ziebart, 2010, Ziebart et al., 2010, Gleave and Toyer, 2022]. In particular, our feature\nis the utility U. We get that the maximum entropy policy is a soft-Q policy for a utility function\nBU with a rationality parameter of 1, where B = argmax'er \u03a3\u03b5 \u0395\u03c0 [log(\u03c0ft (dt | pa\u2081))]. By\nLemma C.1 this can be restated as a soft-Q policy for U with a rationality parameter of B. It follows\nfrom Ziebart that B = argmax\u1e9e'\u2208R ft, and allowing \u03b2 = \u221e or -\u221e does not change the argmax.\nIn the case where u \u2208 {Umin, Umax}, it's easy to show that the maximum entropy policy which attains\nu randomises uniformly between maximal value actions (for Umax) or minimal value actions (for\nUmin). These policies can be expressed as limp\u2192\u221e \u03c0 \u03c0\u03bfft and lim\u03b2- \u03c0ft respectively."}, {"title": "6 Experimental Evaluation", "content": "We carried out two experiments to measure known-utility MEG with respect to the environment\nreward function and unknown-utility MEG with respect to a hypothesis class of utility functions. We\nused an MLP with a single hidden layer of size 256 to define a utility function over states.\nOur experiments measured MEG for various policies in the CliffWorld environment from the seals\nsuite [Gleave et al., 2020]. Cliffworld (Figure 3a) is a 4x10 gridworld MDP in which the agent starts\nin the top left corner and aims to reach the top right while avoiding the cliff along the top row. With\nprobability 0.3, wind causes the agent to move upwards by one more square than intended. The"}, {"title": "7 Limitations", "content": "Environment Access Although computing MEG does not require an explicit causal model (cf.\nSection 5), it does require the ability to run various policies in the environment of interest - we cannot\ncompute MEG purely from observational data.\nIntractability While MEG can be computed with gradient descent, doing so may well be com-\nputationally intractable in high dimensional settings. In this paper, we conduct only preliminary\nexperiments - larger experiments based on real-world data may explore how serious these limitations\nare in practice.\nChoice of variables MEG is highly dependent on which variables we choose to measure goal-\ndirectedness with respect to. At one extreme, all (deterministic) policies have maximal goal-"}, {"title": "8 Conclusion", "content": "We proposed maximum entropy goal-directedness (MEG), a formal measure of goal-directedness in\nCIDs and CBNs, grounded in the philosophical literature and the maximum causal entropy principle.\nDeveloping such measures is important because many risks associated with advanced artificial\nintelligence come from goal-directed behaviour. We proved that MEG satisfies several key desiderata,\nincluding scale invariance and being zero with respect to variables that can't be influenced, and that\nit gives insights about instrumental goals. On the practical side, we adapted algorithms from the\nmaximum causal entropy framework for inverse reinforcement learning to measure goal-directedness\nwith respect to nonlinear utility functions in MDPs. The algorithms handle both a single utility\nfunction and a differentiable class of utility functions. The algorithms were used in some small-scale\nexperiments measuring the goal-directedness of various policies in MDPs. In future work, we plan\nto develop MEG's practical applications further. In particular, we hope to apply MEG to neural\nnetwork interpretability by measuring the goal-directedness of a neural network agent with respect to\na hypothesis class of utility functions taking the network's hidden states as input, thus taking more\nthan just the system's behaviour into account."}, {"title": "A Comparison to Discovering Agents", "content": "This paper is inspired by Kenton et al. [2023], who proposed a causal discovery algorithm for\nidentifying agents in causal models, inspired by Daniel Dennett's view of agents as systems \"moved\nby reasons\" [Dennett, 1989]. Our approach has several advantages over theirs, which we enumerate\nbelow.\nMechanism variables. Kenton et al. [2023] assume access to a mechanised structural causal model,\nwhich augments an ordinary causal model with mechanism variables which parameterise distributions\nof ordinary object-level variables. An agent is defined as a system that adapts to changes in the\nmechanism of its environment. However, the question of what makes a variable a mechanism is left\nundefined, and indeed, the same causal model can be expressed either with or without mechanism\nvariables, leading their algorithm to give a different answer. For example, Example 1 has identical\ncausal structure to the mouse example in Kenton et al. [2023], but without any variables designated\nas mechanisms. Their algorithm says the version with mechanism variables contains an agent while\nthe version without does not, despite them being essentially the same causal model. Figure 4 shows\nour example depicted as a mechanised structural causal model. We fix this arbitrariness by making\nour definition in ordinary causal Bayesian networks.\nUtility variables. Their algorithm assumes that some variables in the model represent agents' utilities.\nWe bring this more in line with the philosophical motivation by treating utilities as hypothesised\nmental states with which we augment our model.\nPredictive accuracy. Kenton et al. [2023]'s approach formalises Dennett's idea of agents as systems\n\"moved by reasons\". We build on this idea but bring it more in line with Dennett's notion of what it\nmeans for a system to be moved by a reason that the reason is useful for predicting its behaviour.\nGradualist vs Essentialist. The predictive error viewpoint gives us a continuous measure of goal-\ndirectedness rather than a binary notion of agency, which is more befitting of the gradualist view of\nagents which inspired it.\nPracticality. Their algorithm is theoretical rather than something that can be applied in practice.\nInstead, ours is straightforward to implement, as we demonstrate in Section 6. This opens up a range\nof potential applications, including behavioural evaluations and interpretability of ML models.\nInterventional distributions. The primary drawback of MEG is that it doesn't necessarily generalise\noutside of the distribution. Running MEG on interventional distributions may fix this. We leave an\nextension of MEG to interventional distributions for future work."}, {"title": "B Proofs of MEG Properties", "content": "Proposition 3.1 (Translation and scale invariance). Let M\u2081 be a CID with utility function U\u2081, and\nlet M2 be an identical CID but with utility function U2 = a \u2022 U\u2081 + b, for some a, b \u2208 R, with a \u2260 0.\nThen for any policy \u03c0, MEGu\u2081 (\u03c0) = MEGU\u2082 (\u03c0).\nProof. Since MEG is defined by maximising over a maximum entropy policy set, showing that two\nutility functions have the same maximum entropy policy set is enough to show that they result in the\nsame MEG for every policy."}, {"title": "C Proof of Theorem 5.1", "content": "Lemma C.1. Let \u043f\u0432\u0438 denote a soft-optimal with respect to utility function U, defined as in Theo-\nrem 5.1, with rationality parameter B. Then for any a \u2208 R, we have that \u03c0\u03b1\u00b7\u03b2,\u03b9 = \u03c0\u03b2,\u03b1\u00b7\u03c5.\nProof. If either a or \u1e9e are equal to 0, both policies are uniform and we are done. Otherwise, write\nQsoft for the soft-Q function for U with rationality parameter B. We first show that Qsoft\na Qsoftu by backwards induction.\n= \u03b2,\u03b1\u03ba\n\u03b2,\u03b1-\u039c,\u03b7\nFirst, Qsoft = E [a. Un | dn, pan] = aE [Un | dn, pan] = a \u00b7 Qsoft\n\u03b1\u00b7\u03b2,U,n \u03b1\u00b7\u03b2\nAnd for t < n, assuming the inductive hypothesis holds for t + 1,\n$Q^{soft}_{U,t}(d_t | pa_t) = E [ a. U_t + \\frac{1}{\\beta} logsumexp(Q^{soft}_{\\alpha U,t+1}(.\\| Pa_{D_{t+1}})) \\| d_t, pa_t ]$\n$= \u03b1\u00b7 \u0395 [ U_t + \\frac{1}{\u03b1\u03b2} logsumexp(\u03b1 \u03b2. QB,U,t+1(\u00b7 | PaDt+1)) \\| dt, pat ]$\n= \u03b1\u00b7 \u0395 [ U_t + \\frac{1}{\u03b1\u03b2} logsumexp(\u03b2 \u00b7 Q^{soft}_{\u03b1\u03b2,U,t+1}(.\\| Pa_{D_{t+1}})) \\| d_t, pa_t ]$\n= a. Qa\u00df,u,t.\nThen, substituting this into the definition of \u03c0\u03b1\u00b7\u03b2,\u03c5 we get"}, {"title": "D Experimental Details", "content": "D.1 Tables of results\nWe used an environment from the SEALS library, and adapted an algorithm from the imitation\nlibrary. Both are released under the MIT license.\nFor information on hyperparameters, see the code."}, {"title": "D.2 Visualising optimal policies for different lengths of goal region.", "content": "Figure 5a and Figure 5b show the occupancy measures for an optimal policy for k=1 and k=4\nrespectively, where k is the length of the goal region in squares. Although the goal region is larger\nin the latter case, the optimal policy consistently aims for the same sub-region. This explains why\nunknown-utility MEG is higher than MEG with respect to the environment reward. The policy does\njust as well on a utility function whose goal-region is limited to the nearer goal squares as it does\non the environment reward, but fewer policies do well on this utility function, so doing well on it\nconstitutes more evidence for goal-directedness."}, {"title": "D.3 Further details", "content": "The experiments were carried out on a personal laptop with the following specs:\n\u2022 Hardware model: LENOVO20N2000RUK\n\u2022 Processor: Intel(R) Core(TM) i7-8665U CPU @ 1.90GHz, 2112 Mhz, 4 Core(s), 8 Logical\nProcessor(s)\n\u2022 Memory: 24.0 GB"}]}