{"title": "Position: Automatic Environment Shaping is the Next Frontier in RL", "authors": ["Younghyo Park", "Gabriel B. Margolis", "Pulkit Agrawal"], "abstract": "Many roboticists dream of presenting a robot with a task in the evening and returning the next morning to find the robot capable of solving the task. What is preventing us from achieving this? Sim-to-real reinforcement learning (RL) has achieved impressive performance on challenging robotics tasks, but requires substantial human effort to set up the task in a way that is amenable to RL. It's our position that algorithmic improvements in policy optimization and other ideas should be guided towards resolving the primary bottleneck of shaping the training environment, i.e., designing observations, actions, rewards and simulation dynamics. Most practitioners don't tune the RL algorithm, but other environment parameters to obtain a desirable controller. We posit that scaling RL to diverse robotic tasks will only be achieved if the community focuses on automating environment shaping procedures.", "sections": [{"title": "1. Introduction", "content": "The advent of foundation models for speech, vision, and language processing has revolutionized Artificial Intelligence. It is widely believed that robotics is the next frontier, and the race to develop a foundation model for robotics is on-going. The critical challenge in this pursuit is the limited availability of robotic data: trajectories of observations and robot actions. This starkly contrasts with computer vision and natural language processing, where large amounts of readily available internet data can be used. One way to gather robotic data is to deploy robots for many tasks in diverse environments. However, such deployment is only feasible if robots create value, i.e., successfully solve the task often enough. The result is a chicken-and-the-egg situation - robots need to be useful to be deployed, but for them to be useful requires collecting enough data to train a controller that creates value. The real question, therefore, is bootstrapping data collection.\n\nA natural way to collect data is by teleoperating a robot to perform diverse tasks. However, this paradigm is challenging to scale as the human effort grows linearly with the amount of data that needs to be gathered. To ease data collection, recent works have made teleoperation more capable and easier (Zhao et al., 2023; Fu et al., 2024), but it doesn't change the linear scaling. At some point in the future, it is plausible that we will have enough data to train a large model that will reduce the number of demonstrations required to learn a new task and make the human effort sub-linear. However, we are far from that point. The primary reason is that policies obtained via supervised learning on a small dataset of demonstrations (i.e., learning from demonstration) have limited robustness and generalization and, therefore, cannot be used to collect data autonomously in more diverse settings.\n\nIn theory, given a reward function or by inferring a reward function from the demonstrations, autonomous data collection is possible using reinforcement learning (RL). It has been hard to realize this promise of RL because training in the real world often requires babysitting the robot to ensure safe operation, ensuring the reward function is not hacked, performing resets, and the data inefficiency of RL algorithms means they run for a long time before useful behaviors are discovered. Real-world RL training is an active area of research (Luo et al., 2024), and recent work has shown the plausibility of learning locomotion behaviors from real-world RL training (Wu et al., 2023; Lei et al., 2023). However, real-world RL is yet to achieve state-of-the-art robotic controllers, which means the data it generates is sub-optimal.\n\nA related line of work bypasses the difficulty of training in the real world by training with RL in simulation and then successfully deploying policies in reality (i.e., sim-to-real RL). Such training has achieved state-of-the-art behaviors across many robot morphologies and complex tasks involving legged locomotion (Miki et al., 2022; Ji et al., 2023; Hoeller et al., 2023; Zhuang et al., 2023; Cheng et al., 2023; Jenelten et al., 2024), dexterous manipulation (Andrychow-"}, {"title": "2. Robotic Behavior Generation with RL", "content": "To support a precise definition of environment shaping, we first describe a typical workflow for generating robotic behaviors using reinforcement learning (RL) in simulation (Figure 1).\n\nWe decompose behavior generation into four subtasks; sample environment generation (Sec 2.1), environment shaping (Sec 2.2), RL training (Sec 2.3), and the outer feedback loop with behavior evaluation and reflection (Sec 2.4). In delineating these substages, we pinpoint typical human efforts involved in the process.", "subsections": [{"title": "2.1. Modeling Sample Environments", "content": "Consider $p(e)$ as the oracle distribution of the target environment we want to deploy our robots in. Our goal is to generate behavior that is performant (with respect to objective $J$) and robust under $p(e)$,\n\n$\\max_{\\pi} E_{\\hat{e}\\sim p(e)} J(\\pi; \\hat{e})$\n\nThe target environment can either be a specific real-world environment that already exists (e.g. kitchen at a specific location) or a generic concept (e.g. typical household kitchen).\n\nUnfortunately, it's extremely difficult to model this oracle"}, {"title": "2.2. Shaping Reference Environments", "content": "A straightforward subsequent step of behavior generation might be to directly use the reference environments $E^{ref}$ as an RL environment, with the expectation that algorithms like Proximal Policy Optimization (PPO) (Schulman et al., 2017) will find performant and generalizable behaviors. However, this approach often falls short due to the inherent sparsity of these environments.\n\nReference environments often present a challenging optimization landscape for RL algorithms due to their sparse nature. For example, it might be rare to obtain nonzero rewards or the observation space might be dominated by spurious features that encourage poor local minima. To mitigate these challenges, human engineers typically go through a process of shaping the elements of $E^{ref}$. This modification, aimed at enhancing the learnability of the environment, includes introducing denser learning signals and additional modifications encouraging effective exploration. The resulting shaped environment (Co-Reyes et al., 2020) then used as a training ground for the subsequent optimal control solver.\n\nDefinition 2.3 (Shaped Environment). A Shaped Environment $E^{shaped}$ is a modification of a reference environment, i.e., $E^{shaped} = f(E^{ref})$. The transformation $f$ incorporates design choices specifically optimized for learning performance, smoothing the optimization landscape enabling the solver to find better solutions with maximal performance in"}, {"title": "2.3. RL Training", "content": "Once the shaped environment is obtained, the next step of behavior generation is to use RL algorithms, i.e., PPO (Schulman et al., 2017), to find a behavior $\\pi$ that best performs on the shaped environment $E^{shaped}$. Formally, the algorithm aims to find an optimal behavior $\\pi$ for the following optimization problem:\n\n$\\max_{\\pi} E_{\\tau \\sim \\pi} \\sum_{t=0}^{T} r_t(s_t, a_t)$\n\ns.t. $s_{t+1} \\sim p(s_t, a_t; E^{shaped})$.\n\nWhile the RL training process also requires a range of design decisions, such as algorithmic choices and hyperparameter adjustments, these areas are relatively well-researched and documented (Parker-Holder et al., 2022; Kiran & Ozyildirim, 2022).\n\nHowever, we note that in a pratical context of robotic behavior generation, tuning the RL setting (e.g. neural architecture search for policy or hyperparameter tuning) is often underprioritized compared to the effort put into environment shaping. In IsaacGymEnvs (Makoviychuk et al., 2021), for instance, simple Multilayer Perceptron (MLP) networks are employed, and off-the-shelf RL algorithm implementations are utilized with their default configurations. This shows that algorithms like PPO and their default settings are capable enough when paired with ideally shaped environment."}, {"title": "2.4. Optimizing Environment Shaping via Iterative Behavior Evaluation and Reflection", "content": "Once an optimal behavior $\\pi^*$ is obtained via RL training, the behavior is evaluated on the test environment $E^{test}$ and reflected by human engineers. Denoting the reflection process as $H$ of analyzing the generated behavior $\\pi^*$ in test environment $E^{test}$ and coming up with a better environment shaping $f$,\n\n$H : f_k \\times J(\\pi^*; E^{test}) \\rightarrow f_{k+1}$,\n\nrobotic behavior generation process can be formally defined as an iterative optimization process over the environment shaping function $f$,\n\n$f_{k+1} = H (f_k, J(\\pi^*; E^{test}))$\n\nwhere $\\pi^* = \\arg \\max_{\\pi} J(\\pi; E^{shaped})$,\n\n$E^{shaped} = f_k (E^{ref}), f_0 = I_{identity}$\n\nwhich aims to find an optimal shaping function $f \\in F$ for the following bi-level optimization problem:\n\n$\\max_{f \\in F} J(\\pi^*; E^{test})$\n\ns.t. $\\pi^* \\in \\arg \\max_{\\pi} J(\\pi; E^{shaped}), E^{shaped} = f(E^{ref})$.\n\nAfter shaping is applied, the new environment may not reflect the original task; therefore, note that the outer level of the bilevel optimization maximizes $J(\\pi^*; E^{test})$ which is the return evaluated in the original test environments without any shaping. If the inner level optimizes a shaped environment well, but with poor correspondence to the original task, it will be dispreferred by the outer loop."}]}, {"title": "3. The Current State of Environment Shaping", "content": "Having established the role of environment shaping operations in successful behavior generation, we now probe deeper into the unique challenges that the problem of environment shaping and its optimization procedure presents. Specifically, we make the following arguments with supporting experiments and analysis:\n\n\u2022 Popular RL benchmark environments are artificially made easy for RL with task-specific environment shaping. We should benchmark our algorithms in unshaped environments if we want them to solve new problems without task-specific environment shaping step. (Section 3.1)\n\n\u2022 Shaping multiple attributes of the environment at once (reward, observation space, action space, etc.) is a tricky, non-convex optimization problem (Section 3.2).\n\n\u2022 Reward shaping is not the only problem. Existing automation efforts focus too narrowly on rewards (Section 3.3)."}, {"title": "3.1. RL Benchmarks for Robotics are Artificially Easy", "content": "Benchmark environments for robot reinforcement learning include task-specific environment shaping to make it feasible to help baseline RL algorithms perform reasonably"}, {"title": "3.2. Shaping the Entire Environment is Harder than Shaping One Component", "content": "In the previous section, we saw that policy learning can be highly sensitive to each individual aspect of environment shaping (Table 1) and that the shaping choices in IsaacGymEnvs vary qualitatively depending on the task. This motivates that, to promote the development of truly automatic behavior generation, we should consider benchmarking against a suite of unshaped environments. If an algorithm can learn policies in unshaped environments, it should be applicable to newly defined tasks and environments without requiring additional manual shaping on those environments. The unshaped environments are an appropriate benchmark for methods that couple automatic environment shaping with RL, or to directly attempt to solve by improving RL algorithms.\n\nWhat kinds of methods might we try using to optimize an entirely unshaped environment? If we aim to accomplish this by introducing shaping, we may have to search over not just one aspect but all aspects of shaping. To understand the optimization landscape, suppose we have access to an oracle that proposes the human-designed environment shaping as a candidate, and we use this to perform a hill-climbing search in one aspect of the environment at a time: first, optimize the observation space, then the action space, then the reward function, and repeat until convergence."}, {"title": "3.3. Existing Automation Focuses Narrowly on Rewards", "content": "There have been prior attempts to partially automate the process of environment shaping. However, most of the efforts have focused on the subtask of reward shaping. The concept of reward shaping was formalized by (Ng et al., 1999) and the idea of automated reward shaping through evolutionary search was advanced by Singh et al. (2009; 2010). Recent works have formulated the problem of reward shaping as bilevel optimization with the environment design at the top level and policy learning at the bottom level and used LLMs (Goyal et al., 2019; Ma et al., 2023; Xie et al., 2023) or gradient-based methods (Hu et al., 2020) to generate candidates.\n\nLLM-based methods can benefit from prior knowledge about coding and robotics derived from internet training data to act as an efficient sampler for generating candidate shaped rewards expressed as code. Since other aspects of the environment shaping can also be expressed as code, it is straightforward to test how these methods extend."}, {"title": "4. Paths Forward to Automated Environment Shaping", "content": "Today, no algorithm can solve diverse unshaped tasks, although we know them to be solvable with environment shaping by human designers. How can we fix this and generalize successes in RL for robotics? There are a few possibilities:\n\nScale up computation. Existing bi-level search techniques like Ma et al. (2023) can be extended to design environment shaping and run with more compute resources to search over a greater number of candidate shaping designs. Massively parallel simulation has led to realistic robotics environments that can train quickly (Makoviychuk et al., 2021; Rudin et al., 2022); However, some state-of-the-art sim-to-real methods take weeks to train a single policy due to high data requirements or expensive subroutines (Chen et al., 2023; Jenelten et al., 2024). This would make performing much more outer-loop search impractical.\n\nImprove priors. If we can't search over more candidates, a better way is to generate higher-quality candidates more quickly. One possibility is that improved foundation models will zero-shot generate better candidate shaped environments (Xian et al., 2023; Yang et al., 2023). However, it's hard to predict how much this will help. Another good idea is to mechanistically understand the strategies of human designers. By what mechanism does the choice of observation space or curriculum improve performance? Co-Reyes et al. (2020) proposed a holistic view of environment shaping ('Ecological Reinforcement Learning') and studied the impact of stochasticity, goal distribution, and early termination design on learnability. Yu et al. (2023) examined the impact of observation space on learning quadruped locomotion and offered an explanation of how different observations can work better for different subtasks. E\u00dfer et al. (2023) surveyed a number of works across applied reinforcement learning, comparing practitioners' design choices. Peng & Van De Panne (2017) analyzed a set of common action spaces for robot control and Aljalbout et al. (2023) developed metrics to explain the performance gap. Kim et al. (2023) found that constraints require less tuning than rewards to transfer across diverse robots. If more understanding is documented in these areas, improved biases could be implemented as a prior, e.g., in an LLM's context.\n\nShape online. Instead of iteratively improving on the environment shaping $f$ across training runs, can we improve it dynamically within a RL training loop? If, for example, multi-objective reinforcement learning or bilevel optimization can trade off the coefficients of multiple reward terms (Singh et al., 2010; Zheng et al., 2018; Chen et al., 2022a), then perhaps the LLM search can be performed only over the form of the different reward terms, and their relative weighting can be optimized at runtime. In other aspects of the environment, an analogous approach would"}, {"title": "5. Conclusion", "content": "Reinforcement learning has long promised to solve decision-making problems in a task-agnostic manner. It has found great success in solving challenging but narrowly-scoped tasks in robotics. In this position paper, we argued that the key bottleneck for scalability of RL is a limited mechanistic understanding of task-specific engineering (environment shaping) that transforms environments to be solved more easily and is universal across domains and benchmarks. We proposed a formal definition of environment shaping as an optimization problem and identified instances of shaping in robotic tasks. Finally, we identified key steps forward such as developing computationally efficient search over environment shaping; improving our tools for implementing such shaping and understanding its impact on learning dynamics; and defining benchmarks for this problem. We hope this will motivate an increased focus in RL research on communicating and evaluating environment-shaping measures that impact performance rather than solely emphasizing the impact of the learning algorithm or policy architecture."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}