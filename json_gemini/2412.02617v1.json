{"title": "Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback", "authors": ["Hiroki Furuta", "Heiga Zen", "Dale Schuurmans", "Aleksandra Faust", "Yutaka Matsuo", "Percy Liang", "Sherry Yang"], "abstract": "Large text-to-video models hold immense potential for a wide range of downstream applications. However, these models struggle to accurately depict dynamic object interactions, often resulting in unrealistic movements and frequent violations of real-world physics. One solution inspired by large language models is to align generated outputs with desired outcomes using external feedback. This enables the model to refine its responses autonomously, eliminating extensive manual data collection. In this work, we investigate the use of feedback to enhance the object dynamics in text-to-video models. We aim to answer a critical question: what types of feedback, paired with which specific self-improvement algorithms, can most effectively improve text-video alignment and realistic object interactions? We begin by deriving a unified probabilistic objective for offline RL finetuning of text-to-video models. This perspective highlights how design elements in existing algorithms like KL regularization and policy projection emerge as specific choices within a unified framework. We then use derived methods to optimize a set of text-video alignment metrics (e.g., CLIP scores, optical flow), but notice that they often fail to align with human perceptions of generation quality. To address this limitation, we propose leveraging vision-language models to provide more nuanced feedback specifically tailored to object dynamics in videos. Our experiments demonstrate that our method can effectively optimize a wide variety of rewards, with binary AI feedback driving the most significant improvements in video quality for dynamic interactions, as confirmed by both AI and human evaluations. Notably, we observe substantial gains when using reward signals derived from AI feedback, particularly in scenarios involving complex interactions between multiple objects and realistic depictions of objects falling.", "sections": [{"title": "1. Introduction", "content": "Large video generation models pre-trained on internet-scale videos have broad applications such as generating creative video content (Blattmann et al., 2023b; Ho et al., 2022a; Hong et al., 2022; Singer et al., 2022), creating novel games (Bruce et al., 2024), animations (Wang et al., 2019), movies (Zhu et al., 2023), and personalized educational content (Wang et al., 2024), as well as simulating the real-world (Brooks et al., 2024; Yang et al., 2023b) and controlling robots (Du et al., 2024; Ko et al., 2023). Despite these promises, state-of-the-art text-to-video models today still suffer from hallucination, generating unrealistic objects or movements that violate physics (Bansal et al., 2024; OpenAI, 2024; Yang et al., 2024), or even generating static scenes and ignoring specified movement altogether (Figure 1), which hinders the practical use of text-to-video models.\nExpanding both the dataset and the model size has proven effective in reducing undesirable behaviors in large language models (LLMs) (Hoffmann et al., 2022). However, when it comes to video generation, this scaling process is more complex. Creating detailed language labels for training text-to-video models is a labor-intensive task, and the architecture for video generation models has not yet reached a point where it can effectively scale in the same way language models have (Yang et al., 2024). In contrast, one of the most impactful advances in improving LLMs has been the integration of external feedback (Christiano et al., 2017; Ouyang et al., 2022). This raises important questions about what kinds of feedback can be gathered for text-to-video models and how these feedback can be integrated into the training of text-to-video models to reduce hallucination and better align content with prompts.\nIn this paper, we investigate the recipe for improving dynamic interactions with objects in text-to-video diffusion models (Ho et al., 2022b) by leveraging external feedback. We cover various algorithms for self-improvement based on reinforcement learning (RL) and automated feedback for text-video alignment. We begin by noting that two representative offline optimization algorithms, reward-weighted regression (RWR) (Peters and Schaal, 2007) and direct preference optimization (DPO) (Rafailov et al., 2023), introduced independently in prior works, have stemmed from the same objective with some design choices such as KL-regularization and policy projection. Based on this observation, we derive a unified objective for RL-finetuning through the lens of probabilistic inference. For the choice of feedback, we test metric-based feedback on semantics (Radford et al., 2021), human preference (Kirstain et al., 2023; Wu et al., 2023a), and dynamics (Huang et al., 2024), and also propose leveraging the binary feedback obtained from large-scale vision-language models (VLMs) capable of video understanding.\nOur experiments show that the proposed framework effectively maximizes various types of feedback, though iterative optimization poses challenges: RWR struggles with unseen prompts, while DPO over-optimizes metrics and suffers from visual collapse, making the best algorithm dependent on the desired generalization. Metric-based feedback, used in prior works (Li et al., 2024b; Prabhudesai et al., 2024; Yuan et al., 2023), poorly correlates with realistic object dynamics, as metric improvements don't always translate to plausible physics. In contrast, AI feedback from VLMs and human feedback better align with realistic dynamics, achieving the most significant improvements in scene quality. Lastly, we found that pre-trained video generation models struggle with multi-step interactions, new object appearances, and spatial 3D relationships, while RL fine-tuning helps address these limitations."}, {"title": "2. Preliminaries", "content": "Denoising Diffusion Probabilistic Models. We adopt denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020; Sohl-Dickstein et al., 2015) for generating a H-frame video xo = [x1,...,xH]. DDPM considers approximating the reverse process for data generation with a parameterized model p\u03b8(x0) = \u222b p\u03b8(x0:T)dx1:T, where p\u03b8(x0:T) = p(xT) \u220ft=1T p\u03b8(xt\u22121 | xt), and p(xT) = N(0, I), p\u03b8(xt\u22121 | xt) = N(\u03bc\u03b8(xt,t), \u03c3t2I). DDPM also has a forward process, where the Gaussian noise is iteratively added to the data with different noise level \u03b2t: q(x1:T) = \u220ft=1T q(xt | xt\u22121) and q(xt | xt\u22121) = N(\u221a1 \u2212 \u03b2txt\u22121, \u03b2tI). Considering the upper bound of negative log-likelihood Eq[\u2212log p\u03b8(x0)] with practical approximations, we obtain the objective for DDPM IDDPM as follows,\n\u2211t=1TEq[DKL(q(xt\u22121 | xt, x0) || p\u03b8(xt\u22121 | xt))]\n~ Ex0,\u03f5,t[||\u03f5 \u2212 \u03f5\u03b8(\u221a\u03b1txt + \u221a\u03b2t\u03f5, t)||2] := LDDPM,\n(1)\nwhere \u03b1t = 1 \u2212 \u03b2t. After training parameterized denoising model \u03f5\u03b8, the video might be generated from the initial noise xT ~ N(0, I)"}, {"title": "Probabilistic Inference View of RL-Finetuning Objectives", "content": "As in Fan et al. (2023) and Black et al. (2024), we formulate the denoising process in DDPMs as a T-horizon Markov decision process (MDP), which consists of the state st = (xT\u2212t, c), action at = xT\u2212t\u22121, initial state distribution P0(s0) = (N(0, I), p(c)), transition function P(st+1 | st, at) = (\u03b4{at}, \u03b4{c}), reward function R(st, at) = r(x0, c)1[t = T], and the policy \u03c0\u03b8(at | st) = p\u03b8(xT\u2212t\u22121 | xT\u2212t, c) where \u03b4{\u00b7} is the Dirac distribution and 1[\u00b7] is a indicator function. Moreover, motivated by the probabilistic inference for control (Levine, 2018), we additionally introduce a binary event variable O \u2208 {0, 1} to this MDP, which represents if the generated video x0 is optimal or not. To derive the unified RL-finetuning objective for DDPM, we consider the log-likelihood log p(O = 1 | c) and decompose it with a variational distribution p':\nEp(c)[log p(O = 1 | c)]\n= Ep(c)p(xo | c) log p(O = 1 | xo, c) \u2212 log p(xo | c)p'(xo | c)\n= IRL(P, p')\n+ Ep(c)p(xo | c) log p(xo | c)p'(xo | c)\n+ Ep(c)[DKL(P(x0 | c) || p'(x0 | O = 1, c))].\n(2)\nIRL(P, p') := E log p(O = 1 | x0, c) \u2212 log p(xo | c)p'(xo | c) is the evidence lower bound. As in the RL literature (Abdolmaleki et al., 2018; Furuta et al., 2021; Peters et al., 2010), we can assume the dependence on the reward such as p(O = 1 | x0, c) \u221d exp(\u03b2\u22121r(x0, c)), because O stands for the optimality of the generated video. Then, we could rewrite IRL(P, p') in an explicit form of the unified objective for RL-finetuning:\nIRL(P, p') := E \u03b2\u22121r(x0, c) \u2212 log p(x0 | c)p'(x0 | c)\n(3)\nIn the following section, we will describe that existing algorithms can be derived from Equation 3 through the policy projection with expectation-maximization methods (If-EM), or with Bradley-Terry assumptions (Bradley and Terry, 1952) (Ir-BT), and characterize both types of objectives in the experiments."}, {"title": "3. RL-Finetuning with Feedback", "content": "We start with deriving the practical algorithms to optimize the unified objective for RL-finetuning (Section 3.1), provide a brief overview of metric-based feedback for the text-to-video models (Section 3.2), and then propose the pipeline for Al feedback from VLMs (Section 3.3)."}, {"title": "3.1. Practical Algorithms", "content": "Equation 3 represents the general form of RL-finetuning objective for DDPM; which maximizes a reward under the KL-regularization (Levine, 2018). We derive the practical implementation; based on expectation-maximization, and on the Bradley-Terry assumptions. We cover and compare both algorithmic choices in the following experiments.\nForward-EM-Projection. This type of algorithm parameterizes p' = p\u03b8(x0 | c) and ppref = pref(x0 | c) (i.e. forward KL-regularization), and perform coordinate ascent like a generic EM algorithm; solving Equation 3 with respect to the variational distribution ppref while freezing the parametric posterior p' = p\u03b8' (E-step), and projecting the new p\u03b8pref into the model parameter \u03b8 (M-Step). Specifically, E-step converts Equation 3 into the constraint optimization problem, and considers the following Lagrangian:\nLIRL(pref, \u03bb)\n= \u222b p(c)[\u222b pref(x0 | c)\u03b2\u22121r(x0, c) dx0dc]\n\u2212 \u222b p(c)[\u222b pref(x0 | c) log pref(x0 | c)p\u03b8(x0 | c) dx0dc]\n+ \u03bb (1\u2212\u222b p(c)pref(x0 | c)logpref(x0 | c)p\u03b8(x0 | c) dx0dc).\n(4)\nThe analytical solution of Equation 4 is pnewpref(x0 | c) = zpref\u03b8(x0 | c) exp (\u03b2\u22121r(x0, c)), where Z(c) is the partition function. M-step projects the non-parametric optimal model pnewpref to the parametric model by maximizing Equation 3 with respect to p\u03b8:\nLIRL(\u03b8)\n= \u2212Ep(c)pnewpref[log p\u03b8(x0 | c)]\n= \u2212Ep(c)p\u03b8(x0 | c) exp (\u03b2\u22121r(x0, c))log p\u03b8(x0 | c).\n(5)\nTo stabilize the training, RWR (Lee et al., 2023b; Peters and Schaal, 2007) for diffusion models simplifies LIRL(\u03b8) by removing the intractable normalization Z(c), setting pre-trained models ppre into p\u03b8, converting exponential transform into the identity mapping, and considering the simplified upper-bound of negative log-likelihood which results in the practical minimization objective:\nEp(c)p\u03b8(x0 | c)[r(x0, c) log p\u03b8(x0 | c)]\n~ Ec,x0,\u03f5,t[r(x0, c) ||\u03f5 \u2212 \u03f5\u03b8(\u221a\u03b1txt + \u221a\u03b2t\u03f5, c, t)||2]\n:= If\u2212EM.\n(6)\nReverse-BT-Projection. This category parameterizes p = p\u03b8(x0 | c) and p' = ppref(x0 | c) (i.e. reverse KL-regularization). For instance, PPO (Black et al., 2024; Fan et al., 2023; Schulman et al., 2017) optimizes Equation 3 with a policy gradient. However, such on-policy policy gradient methods require massive computational costs and would be unstable for the text-to-video models. Alternatively, we consider the lightweight approach by optimizing the surrogate objective to extract the non-parametric optimal model into the parametric data distribution p\u03b8 as in DPO (Rafailov et al., 2023; Wallace et al., 2023). First, we introduce the additional Bradley-Terry assumption (Bradley and Terry, 1952), where, if one video x(1) is more preferable than another x(2) (i.e. x(1) > x(2)), the preference probability p(x(1) > x(2) | c) is a function of reward r(x0, c) such as,\np(x(1) > x(2) | c) = \u03c3(r(x(1), c) \u2212 r(x(2), c)),\n(7)\nwhere \u03c3(\u00b7) is a sigmoid function. Transforming analytical solution of Equation 4 into p\u03b8(x0 | c) = zpref\u03b8(x0 | c) exp (\u03b2\u22121r(x0, c)), we can obtain the parameterized reward r\u03b8(x0, c)"}, {"title": "3.2. Metric-based Reward for RL-finetuning", "content": "With the algorithms introduced in Section 3.1, we may choose any reward to be optimized. Prior works on finetuning text-to-image models (Black et al., 2024; Fan et al., 2023) have often employed metric-based feedback as a reward to improve the visual quality, style of images, and text-image alignment. The metric-based rewards are popular because they can amortize the costs to collect actual human feedback (Lee et al., 2023b), and can propagate the gradient through the parameterized evaluators (Clark et al., 2024). Even for text-to-video models (Li et al., 2024b; Prabhudesai et al., 2024; Yuan et al., 2023), such framewise metrics have been primary choices. Following prior works, we examine several metric-based feedback:\nCLIP score (Radford et al., 2021). Leveraging CLIP is one of the most popular methods to evaluate the generative models in terms of text-image alignment, which can reflect the semantics of the scene to a single metric. We use ViT-B16 (Dosovitskiy et al., 2020) for the backbone model and sum the scores over the frames.\nHPSv2 (Wu et al., 2023a,b) and PickScore (Kirstain et al., 2023). These models could predict the human preference for the generated images, because they are trained on large-scale preference data, curated from DiffusionDB (Wang et al., 2022), COCO Captions (Chen et al., 2015), and various textto-image models. They use OpenCLIP (Ilharco et al., 2021) with ViT-H14 and take both image and text as inputs. We also sum these scores over the frames."}, {"title": "3.3. AI Feedback from Vision-Language Models", "content": "One of the most reliable evaluations of any generative model can be feedback from humans, while human evaluation requires a lot of costs. One scalable way to replace subjective human evaluation is AI feedback (Bai et al., 2022; Wu et al., 2024c), which has succeeded in improving LLMs (Lee et al., 2023a). Inspired by this, we propose employing VLMs, capable of video understanding, to obtain the AI feedback for text-to-video models.\nWe provide the textual prompt and video as inputs and ask VLMs to evaluate the input video in terms of overall coherence, physical accuracy, task completion, and the existence of inconsistencies. The feedback VLMs predict is a binary label; accepting the video if it is coherent and the task is completed, or rejecting it if it does not satisfy any evaluation criteria. We find that reasoning or explaining the rationale does not help to improve the accuracy. For VLMs, we use Gemini-1.5-Pro (Gemini Team, 2023). See Appendix B for the prompts to elicit feedback on the text-content alignment.\nPair-Wise and Point-Wise Feedback. When using a VLM to provide feedback for the generated videos, we have the option of comparing two generations at once or scoring each generation individually, similar to the reward used to train language models with human feedback (Qin et al., 2023). As a preliminary, we experiment with both types of feedback with the a priori that the true video is always preferable. We found that for pair-wise feedback, the true videos are preferable to generated videos for only 62.5% of the time. Meanwhile, for point-wise feedback, the true videos are preferable to generated videos for 90.3% of the time. As a result, we focus on using binary point-wise feedback in this work."}, {"title": "4. Experiments", "content": "We first describe the experimental setup and the dataset composition for realistic video generation with dynamic object movement (Section 4.1). In the experiments, we examine whether RL-finetuning can increase each metric compared to SFT on self-generated data (Section 4.2), and investigate which combination of algorithms and rewards can improve the text-content alignment the most through AI and human evaluation (Section 4.3). Moreover, we analyze the relationship between human preference and other types of automated feedback (Section 4.4).\nExperimental Setup. We pre-train video diffusion models based on 3D-UNet (\u00d6zg\u00fcn \u00c7i\u00e7ek et al., 2016) with Something-Something-V2 dataset until converged, which obtains a good prior model for realistic object movements. As explained in Section 4.1, we prepare 5\u00d724 prompts for training and 5\u00d78 prompts for evaluation and generate 128 samples per each prompt in the train split from pre-trained models to collect the data for finetuning. We then put the AI feedback and reward labels on the generated videos. For the evaluation, we generate 32 videos per prompt, compute each metric per video summing them over the frames, and average over top-8 samples. Under the same instruction as VLMs for the binary Al evaluation, we also conduct a human evaluation with binary rating. The whole pipeline is summarized in Figure 2."}, {"title": "4.1. A Set of Challenging Object Movements", "content": "While state-of-the-art text-to-video models can generate seemingly realistic videos, the generated sample often contains static scenes ignoring the specified behaviors, movement violating the physical laws and appearing out-of-place objects (Figure 1), which can be attributed as an insufficient text-content alignment in video generation (Bansal et al., 2024; Liu et al., 2024). To characterize the hallucinated behavior of textto-video models in a dynamic scene, we curate dynamic and challenging object movements from a pair of prompt and reference videos. From an empirical observations, we define the following five categories as challenging object movements:\n\u2022 Object Removal: To move something out from the container in the scene, or the camera frame itself. A transition to a new scene often induces out-of-place objects. For example, taking a pen out of the box.\n\u2022 Multiple Objects: Object interaction with multiple instances. In a dynamic scene, it is challenging to keep the consistency of all the contents. For example, moving lego away from mouse.\n\u2022 Deformable Object: To manipulate deformable objects, such as cloths and paper. The realistic movement of non-rigid instances requires sufficient expressibility and can test the text-content alignment. For example, twisting shirt wet until water comes out.\n\u2022 Directional Movement: To move something in a specified direction. Text-to-video models can roughly follow the directions in the prompts, although they often fail to generate consistent objects in the scene. For example, pulling water bottle from left to right.\n\u2022 Falling Down: To fall something down. This category often requires the dynamic expression towards the depth dimension. For example, putting marker pen onto plastic bottle so it falls off the table.\nAs a realistic object movement database, we use Something-Something-V2 dataset (Goyal et al., 2017; Mahdisoltani et al., 2018). We carefully select 32 prompts and videos per category from the validation split, using 24 prompts for train data and 8 for test data (Figure 3). See Appendix A for the list of prompts. Moreover, the quantitative evaluation of whether the generated samples can simulate realistic physical laws is one of the major challenges. In this paper, we employ human and AI evaluation (from VLMs) to measure the acceptance rate."}, {"title": "4.2. RL-Finetuning Works for Any Quality Metric", "content": "Table 1 presents the alignment performance, measured by each reward, in text-to-video generation after finetuning. We finetune and evaluate the models for each combination of feedback and algorithm independently. RWR and DPO, RL-finetuning with feedback, can improve most metrics, including AI feedback, compared to pretrained models, and supervised finetuning (SFT) baseline that does not leverage any reward signal. In addition, RWR and DPO exhibit better generalization to hold-out prompts than SFT. These can justify the usage of RL-finetuning with feedback to improve the text-content alignment for the dynamic scene. Comparing RWR and DPO, DPO increases all the evaluation metrics and often achieves better performance than RWR (7 in 10 metrics), which implies that subtle algorithmic choices, such as a direction of KL-regularization, on the unified objective would be a significant difference for RL-finetuning."}, {"title": "4.3. VLMs as the Best Proxy for Human Preference", "content": "We here aim to identify what kind of algorithms and metrics can improve the text-content alignment of dynamic object movement in text-to-video models. Table 2 provides the preference evaluation among all the combinations of algorithms and rewards, which are assessed through binary feedback by VLMs and humans. The results reveal that, while all the rewards could realize the improvement, RL-finetuning with AI feedback from VLMs (RWR-AIF and DPO-AIF) achieve the best alignment performance, compared to any metric-based single rewards; RWR-AIF achieves +3.8% in Al evaluation (52.66% \u2192 56.41%), and +11.8% in human evaluation (19.38% \u2192 31.09%). DPO-AIF achieves +3.5% (52.66% \u2192 56.17%), and +14.7% (19.38% \u2192 34.06%) as an absolute gain. VLMs can work as the best proxy of human evaluators to realize efficient text-content alignment (Figure 4).\nComparing RWR and DPO, RWR tends to achieve better alignment on the train split, while also exhibiting the over-fitting behaviors, where the performance against the test splits degrades from the pre-trained models. On the other hand, DPO does not face over-fitting and robustly aligns the output to be preferable, ensuring the generalization. Because SFT faces over-fitting issues too, we guess the lack of negative gradients that push down the likelihood of bad samples might cause insufficient generalization (Tajwar et al., 2024)."}, {"title": "4.4. Correlation with Human Evaluation", "content": "We also analyze the correlation between human preference and automated feedback, such as AI feedback from VLMs, and metric-based rewards (CLIP, HPSv2, PickScore, and Optical Flow). As done in Table 2 for AI/human evaluation, we first measure the average of each metric per algorithm-feedback combination (such as SFT, RWR-CLIP, DPO-AIF, etc), and compute Pearson correlation coefficient to the human preference (Figure 7; left). The results reveal that the performance measured with AI preference from VLMs has the most significant positive correlation to the one with human preference (R = 0.746; statistically significant with p < 0.01), while others only exhibit weak correlations, which supports the observation that optimizing AI feedback can be the best proxy for optimizing the human feedback.\nIn contrast, when we measure the correlation between the averaged human preference and automated feedback among the 32 \u00d7 5 = 160 prompts (Figure 7; right), human preference exhibits weak positive correlation only to AI preference (R = 0.231; statistically significant with p \u2264 0.01)), and others does not have notable correlations. This implies that even with the best choice - Al feedback from VLMs, the rationale of preference is still not enough to be perfectly aligned with human feedback (Wu et al., 2024b). While we demonstrate the promising signal to leverage VLMs to improve the quality of dynamic object interaction in text-to-video models, we also call for the improvement of VLMs to be calibrated with human perception."}, {"title": "5. Discussion and Limitation", "content": "Analysis on Generated Object Movement. We analyze the trend of generated video before/after RL-finetuning among five categories of challenging object movement. Figure 8 (left) shows the Al preference (above) and absolute improvement (below) from pre-trained models per category. Generally, text-to-video models generate preferable videos of deformable objects (DO) and directional movement (DM) which are often complete with two-dimensional movement from the first frame. In contrast, they may not be good at modeling multi-step interactions, the appearance of new objects, and spatial three-dimensional relationships, which often occur in object removal (OR), multiple objects (MO), and falling down (FD). For instance, Figure 8 (right) requires multi-step interactions, such as opening a drawer, picking up a bottle opener, and putting it in the drawer, but the generated video is stuck in the first step (see Appendix G for other failures). RLfinetuning notably improves video generation in the category of multiple objects and falling down, probably because it is relatively easy for VLMs to judge if such a generation is acceptable or not.\nFuture Directions. While the scalable qualitative evaluation of video generation has been a long-standing problem (Dai et al., 2024; He et al., 2024; Liao et al., 2024; Liu et al., 2023; Miao et al., 2024; Wu et al., 2024a), we demonstrate that VLMs can be an automated solution, and AI feedback can provide informative signals to improve the dynamic scene generation. However, currently, there are only a few VLMs sufficiently capable of video understanding, such as Gemini (Gemini Team, 2023). It is also necessary to improve VLMs for quality judgments.\nDue to the cost of querying VLMs online for AI feedback, the lack of a standard recipe to train a text-video reward, and the hyperparameter sensitivity of policy gradient methods, this paper focuses on offline and iterative RL-finetuning. Considering the performance gain coming from online samples, it is a natural yet important direction to resolve the bottlenecks above and extend ours to naive online RL methods (Black et al., 2024; Fan et al., 2023). Moreover, prior works have actively leveraged direct reward gradient from the differentiable metric-based rewards to align text-to-image (Clark et al., 2024; Prabhudesai et al., 2023), or even text-to-video models (Prabhudesai et al., 2024). In contrast, a recent study in LLMs (Zhang et al., 2024a) has argued that generative reward modeling can benefit from several advantages of LLMs and achieves better performance. The comprehensive analysis of AI feedback from the generative VLMs, compared to differentiable rewards is another important topic."}, {"title": "6. Related Works", "content": "RL for Text-to-Image Generation. Inspired by RL with human feedback for LLMs (Ouyang et al., 2022), there has been a great interest in employing RL-finetuning to better align text-to-image diffusion models (Kim et al., 2024; Lee et al., 2023b). RL-finetuning for diffusion models has a lot of variants such as policy gradient (Black et al., 2024; Fan et al., 2023; Uehara et al., 2024; Zhang et al., 2024b,c) based on PPO (Schulman et al., 2017), offline methods (Dong et al., 2023; Lee et al., 2023b; Liang et al., 2024; Na et al., 2024; Wallace et al., 2023; Yang et al., 2023a; Yuan et al., 2024) based on DPO (Rafailov et al., 2023) or RWR (Peng et al., 2019; Peters and Schaal, 2007), and direct reward gradients (Clark et al., 2024; Prabhudesai et al., 2023). These work often optimize compressibility (Black et al., 2024), aesthetic quality (Ke et al., 2023), or human preference (Li et al., 2024a; Xu et al., 2023b), and show that RL-finetuning effectively aligns pretrained diffusion models to specific downstream objectives. Our work focuses on aligning text-to-video models, which can be more challenging as videos have much complex temporal information, and it is unclear whether feedback developed for aligning text-to-image models can be directly applied to text-to-video.\nRL for Text-to-Video Generation. There has also been a few recent work that explored fine-tuning text-to-video models with reward feedback (Li et al., 2024b; Prabhudesai et al., 2024; Yuan et al., 2023). These work leverage open text-to-video models (Blattmann et al., 2023a; Chen et al., 2024; Wang et al., 2023; Zheng et al., 2024b), and off-the-shelf text-to-image reward models, whose gradients are used to align the models to aesthetic or visual quality objective rather than the dynamics in the scenes. However, because there is no standard protocol to collect comprehensive feedback for reward modeling, the reward to directly evaluate generated video has been rarely considered. Furthermore, policy-gradient-based methods are sensitive to hyper-parameters and not stable (Rafailov et al., 2023). Our work differs from existing work in exploring a wide array of feedback to see if they can optimize the object movement rather than visual style, leveraging offline learning to bypass the need for learning a reward model, as well as considering AI feedback from VLMs as a proxy for human preference to generated videos, which has great potential as VLMs continue to improve."}, {"title": "7. Conclusion", "content": "We thoroughly examine design choices to improve the dynamic scenes in text-to-video generation. Our proposal, combining (iterative) RL-finetuning with AI feedback from VLMs, could improve the text-video alignment best, rather than other metric-based rewards. RL-finetuning may mitigate the issues in modeling multi-step interactions, the appearance of new objects, and spatial relationships. Despite the promise, we also point out the necessity of VLMs to be further calibrated with humans. We hope this work helps text-video alignment in object movement further."}, {"title": "Appendix", "content": "A. Prompts for Challenging Object Movements\nObject Removal (Train)\n1. digging key out of sand\n2. opening a drawer\n3. putting coca cola bottle onto johnsons baby oil bottle so it falls down\n4. removing beetroot, revealing cauliflower piece behind\n5. uncovering pencil\n6. wiping foam soap off of cutting board\n7. burying flower in leaves\n8. closing a bowl\n9. plugging airwick scented oil diffuser into plugging outlet but pulling it right out as you remove your hand\n10. burying a flower in sand\n11. digging a leaf out of sand\n12. showing that clip box is empty\n13. pulling crucifix from behind of vr box\n14. stuffing a ticket into a wooden box\n15. taking seasor out of tin\n16. glassess falling like a rock\n17. rolling pen on a flat surface\n18. uncovering a key\n19. stuffing key into cup\n20. removing red bulb, revealing blue marble behind\n21. digging remote control out of sand\n22. taking a pen out of the book\n23. tilting wooden box with car key on it until it falls off\n24. burying tomato in blanket\nObject Removal (Test)\n1. taking cellphone out of white bowl\n2. taking rose bud from bush\n3. taking paper out of cigarette can\n4. closing box\n5. stuffing a bottle opener into a drawer\n6. taking gas lighter out of cigarette can\n7. scooping banana juice up with spoon\n8. taking one of many coins\nMultiple Objects (Train)\n1. lifting phone with pen on it\n2. putting six markers onto a plate\n3. putting cellphone, usb flashdisk and gas lighter on the table\n4. putting 3 pencil onto towel\n5. putting 4 blocks onto styrofoam sheet\n6. moving cup and tin closer to each other\n7. pushing calculator with marker pen\n8. moving a candle and another"}]}