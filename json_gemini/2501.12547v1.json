{"title": "Human-like conceptual representations emerge from\nlanguage prediction", "authors": ["Ningyu Xu", "Qi Zhang", "Chao Du", "Qiang Luo", "Xipeng Qiu", "Xuanjing Huang", "Menghan Zhang"], "abstract": "Recent advances in large language models (LLMs) provide a new opportunity to\naddress the long-standing question of how concepts are represented and organized\nin the mind, which is central to unravelling the nature of human cognition.\nHere, we reframed the classic reverse dictionary task to simulate human concept\ninference in context and investigated the emergence of human-like conceptual\nrepresentations within LLMs. We found that LLMs were able to infer concepts\nfrom definitional descriptions and construct representation spaces that converge\ntowards a shared, context-independent structure. These representations effectively\npredicted human behavioural judgments and aligned well with neural activity\npatterns in the human brain, offering evidence for biological plausibility. These\nfindings demonstrate that human-like conceptual representations and organization\ncan naturally emerge from language prediction, even without real-world grounding.\nOur work supports the view that LLMs serve as valuable tools for understanding\ncomplex human cognition and paves the way for better alignment between artificial\nand human intelligence.", "sections": [{"title": "1 Introduction", "content": "Humans can construct mental models of the world and use them to understand and\nnavigate their environment [1, 2]. Central to this ability is the capacity to form broad\nconcepts that constitute the building blocks of these models [3]. Often regarded as\nmental representations, these concepts capture regularities in the world while abstract-\ning away extraneous details, enabling flexible generalization to novel situations [4].\nFor example, the concept of SUN can be instantly formed and deployed across diverse\ncontexts: observing it rise or set in the sky, yearning for its warmth on a chilly winter\nday, or encountering someone who exudes positivity. Its role in the solar system can\nbe analogized to the nucleus in an atom, enriching learning and understanding. The\nnature of concepts has long been a focus of inquiry across philosophy, cognitive sci-\nence, neuroscience, and linguistics [5-12]. These investigations have uncovered diverse\nproperties that concepts need to satisfy, often framed by the long-standing divide\nbetween symbolism and connectionism. Symbolism emphasizes discrete, explicit sym-\nbols with structured and compositional properties, enabling abstract reasoning and\nrecombination of ideas [13, 14]. In contrast, connectionism conceptualizes concepts as\ndistributed, emergent patterns across networks, prioritizing continuity and gradedness,\nwhich excel in handling noisy inputs and learning from experience [15, 16]. Although\nthere is growing consensus on the need to integrate the strengths of both paradigms\nto account for the complexity and richness of human concepts [17-19], reconciling the\ncompeting demands remains a significant challenge.\nRecent advances in large language models (LLMs) within artificial intelligence\n(AI) have exhibited human-like behaviour across various cognitive and linguistic tasks,\nfrom language generation [20\u201322] to decision-making [23] and reasoning [24-27]. It\nhas sparked intense interest and debate about whether these models are approaching\nhuman-like cognitive capacities based on concepts [28-33]. Some argue that LLMs,"}, {"title": "2 Results", "content": "trained exclusively on text data for next-token prediction, operate only on statistical\nassociations between linguistic forms and lack concept-based understanding grounded\nin physical and social situations [32, 34, 35]. This argument is evidenced by their\ninconsistent performance, which often reveals vulnerabilities such as non-human-like\nerrors and over-sensitivity to minor contextual variations [23, 36, 37]. Conversely,\nothers contend that the performance of a system alone is insufficient to characterize\nits underlying competence [38], and the extent to which concepts should be grounded\nremains open [39, 40]. Instead, language may provide essential cues for people's use of\nconcepts and enable LLMs to approximate fundamental aspects of human cognition,\nwhere meaning arises from the relationships among concepts [41, 42]. Despite the\nconflicting views, there is broad consensus on the central role of concepts in human\ncognition. The core questions driving the debates are whether LLMs possess human-\nlike conceptual representations and organization and, consequently, whether they can\nilluminate the nature of human concepts.\nTo address these questions, we investigated the emergence of human-like concep-\ntual representations from language prediction in LLMs. Our approach unfolded in\nthree stages. First, we examined LLMs' capacities to derive conceptual representations,\nfocusing on the definitional and structured properties of human concepts. We leveraged\nLLMs' in-context learning ability and guided them to infer concepts from descrip-\ntions with a few contextual demonstrations (Fig. 1). The models' outputs offered a\nbehavioural lens to evaluate the derived representations. We explored the organization\nof these representations by analysing how their relational structures varied across\ndifferent contextual demonstrations. Second, we assessed how well the LLM-derived\nrepresentational structures align with psychological measures and investigated whether\nthey capture rich, context-dependent human knowledge. These questions were tested\nby leveraging computations over the representations to predict human behavioural\njudgments. Finally, we mapped the LLM-derived conceptual representations to neural\nactivity patterns in the human brain and analysed their biological plausibility. Our\nexperiments spanned thousands of naturalistic object concepts and beyond [43], pro-\nviding a comprehensive analysis of LLMs' conceptual representations. The findings\nreveal that language prediction can give rise to human-like conceptual representations\nand organization in LLMs. These representations integrated many valued aspects of\nprevious accounts, combining the definitional and structural focus of symbolism with\nthe continuity and graded nature of connectionist models. This work underscores the\nprofound connection between language and human conceptual structures and demon-\nstrates that LLM-derived conceptual representations offer a promising foundation for\nunderstanding human concepts."}, {"title": "2.1 Reverse dictionary as a conceptual probe", "content": "We reframed the reverse dictionary task as a domain-general approach to probe LLMs'\ncapacity for concept inference. A reverse dictionary identifies words based on provided\ndefinitions or descriptions [44], which is simple yet relevant to concept understanding\nand usage. For instance, consider a child learning that the concept MOON corresponds to"}, {"title": "2.2 Deriving concepts from definitional descriptions", "content": "We investigated whether LLMs can construct concepts from definitional descriptions\nthrough the reverse dictionary task. Leveraging data from the THINGS database [45],\nwe prompted LLMs with randomly selected description-word pair demonstrations and"}, {"title": "2.3 Uncovering a context-independent conceptual structure", "content": "To uncover how concepts are represented and organized within LLMs, we looked into\nthe representation spaces they constructed for concept inference and analysed the\ninterrelationships among the conceptual representations. We characterized the repre-\nsentation spaces formed under different contexts by their relational structure, captured\nthrough a (dis)similarity matrix, and measured pairwise alignment by correlating these"}, {"title": "2.4 Predicting various facets of human concept usage", "content": "Next, we investigated how well the conceptual representations and structures derived\nfrom LLMs align with various aspects of human concept usage. Specifically, we used\nthese representations to predict human behavioural data across three key psychological\nphenomena: similarity judgments, categorization and gradient scales along various\nfeatures. To evaluate their effectiveness, we compared the LLM-derived representa-\ntions against traditional static word embeddings learned from word co-occurrence\npatterns [49].\nFor similarity judgments, we compared LLM-derived conceptual representations\nwith human similarity ratings for concept pairs from SimLex-999 [50], which has proven\nchallenging for traditional word embeddings. We also complemented our analysis using\nhuman triplet odd-one-out judgments from the THINGS-data collection [43], which\nrelied on image-based rather than text-based stimuli and introduced a contextual effect\nthrough the third concept. As shown in Fig. 4a, similarity scores derived from LLM rep-\nresentations strongly correlated with human ratings in SimLex-999, with the correlation\nimproving as the number of contextual demonstrations increased ($\\rho$ = 0.776 \u00b1 0.007\nwith 72 demonstrations across five runs, P < 0.0001). These representations signif-\nicantly outperformed traditional word embeddings, which achieved a correlation of\n$\\rho$ = 0.464 (P < 0.0001). For THINGS triplets, we calculated pairwise similarities to\nidentify the odd-one-out (Methods 4.5). The LLM prediction accuracy also improved\nwith increasing contextual demonstrations, plateauing at 63.20% (\u00b10.29%) after 48\ndemonstrations (Fig. 4b). This performance closely approached the noise ceiling esti-\nmated from individual behaviour (67.67% \u00b11.08%) and substantially exceeded that of\nword embeddings (48.10%). These findings suggest that, within proper context, the\nconceptual representations formed by LLMs effectively support the computation of\nsimilarities, a core property of human concepts.\nWe then examined whether categories can be induced from the relative similarity\nbetween LLM-derived conceptual representations, using the human-labelled high-level\ncategories from the THINGS database [45]. Applying a prototype-based categorization\napproach (Methods 4.6), we observed that LLM-derived representations consistently\nachieved high accuracy, reaching 92.25% (\u00b10.15%) with only 24 demonstrations,\nsignificantly outperforming static word embeddings (77.88%) (see Supplementary\nInformation SI 1.4 and Fig. S3c). A t-SNE visualization of these representations\nrevealed notable differences in their similarity structures (Fig. 4c-d). The LLM-\nderived conceptual representations formed distinct clusters corresponding to high-level\ncategories, such as animals and food, while word embeddings exhibited less distinct\ncategory separation. Furthermore, LLM representations revealed broader distinctions,"}, {"title": "2.5 Mapping to activity patterns in the human brain", "content": "We further explored the biological plausibility of LLM-derived conceptual representa-\ntions by mapping them onto the activity patterns in the human brain. Using fMRI\ndata from the THINGS-data collection [43], we fitted a voxel-wise linear encoding\nmodel to predict neural responses evoked by viewing concept images, based on the\ncorresponding conceptual representations derived from LLaMA3-70B (Methods 4.9)."}, {"title": "3 Discussion", "content": "How are concepts represented and organized in the mind? Recent developments in\nLLMs [20, 57] provide a novel approach to this question. Here, we demonstrated that\nnext-token prediction over language naturally gives rise to human-like conceptual repre-\nsentations and organization, even without real-world grounding. Our work builds upon\nthe long-explored idea of vectors as conceptual representations [7, 16, 42, 46], while\nprevious work has predominantly focused on word embeddings [30, 52]. We viewed\nconcepts as latent representations used for word generation and guided LLMs to infer\nthem from definitional descriptions. Our findings revealed that LLMs can adaptively\nderive concepts based on contextual demonstrations, reflecting the interrelationships\namong them. These representations converged towards a context-independent rela-\ntional structure predictive of the models' concept inference capacity, suggesting that\nlanguage prediction inherently fosters the development of a shared conceptual struc-\nture. This structure supports the generalization of knowledge by effectively capturing"}, {"title": "4 Methods", "content": "representations within LLMs marks a critical step towards resolving enduring ques-\ntions in the science of human concepts. This progress opens new avenues for bridging\nthe gaps between human and machine intelligence, offering valuable insights for both\ncognitive science and artificial intelligence."}, {"title": "4.1 Large language models used in our experiments", "content": "This paper focuses on base models, i.e., LLMs pretrained solely for next-token prediction\nwithout fine-tuning or reinforcement learning. Our experiments exclusively utilized\nopen-source LLMs, as their hidden representations are necessary for our analysis. We\nprimarily conducted our experiments on the LLaMA 3 models, including LLaMA3-70B\nand LLaMA3-8B, both trained on over 15 trillion tokens [83]. Another 11 series of\nTransformer-based decoder-only LLMs were also employed for experiments on the\nreverse dictionary task and the convergence of LLM representations. These included (1)\nFalcon [84], (2) Gemma [85], (3) LLaMA 1 [86], (4) LLaMA 2 [87], (5) Mistral [88, 89],\n(6) MAP-Neo [90], (7) OLMO [91], (8) \u041e\u0420\u0422 [92], (9) Phi [93], (10) Pythia [94], and\n(11) Qwen [95, 96]. Additional details about the models' names, scales, training data\nand sources can be found in Table S1 and Table S2. These models vary in architecture,\nscale, and pretraining data, enabling explorative analyses of how these factors might\nimpact the conceptual representations and organization within LLMs."}, {"title": "4.2 Details on deriving conceptual representations", "content": "We used data from the THINGS database [45] to probe LLMs' conceptual representa-\ntions via the reverse dictionary task. The dataset includes 1,854 concrete and nameable\nobject concepts, paired with their WordNet synset IDs, definitional descriptions, sev-\neral linked images and category membership labelled by humans. The concepts and\nimages were selected to be representative of everyday objects commonly used in Amer-\nican English, providing a useful testbed for analysing model representations. To assess\nthe generality of our results, we extended our analysis to a broader range of descrip-\ntions and concepts. Specifically, (1) we tested LLMs' generalizability across different\nword classes (nouns, verbs, adjectives and adverbs), age-of-acquisition, and degrees\nof concreteness using a broader set of 21,402 concepts. The concepts were selected\nfrom the intersection of the three datasets: age-of-acquisition [97], concreteness [98]\nand Wordnet [99]. (2) We evaluated the consistency of LLMs' predictions using three\nadditional distinct definitional descriptions for each of the 1,854 THINGS concepts,\nwhich were generated by GPT-3.5 and manually checked for diversity (see Supplemen-\ntary Information SI 2.1 for details). (3) We examined LLMs' sensitivity to linguistic\nstructure by introducing varying degrees of word order permutations to the query\ndescriptions. We shuffled 30%, 60% and 100% of the words in the query descriptions\nfrom the THINGS database and reinserted them into the original text.\nTo guide LLMs in the reverse dictionary task [100], we selected a random 20%\nsubset of concepts as the training set (from THINGS for most analyses and WordNet\ndata for the 21,402 WordNet concepts), with the remaining concepts comprising the"}, {"title": "4.3 Structural analysis of representation spaces", "content": "We used representational similarity analysis (RSA) [47] to measure the alignment\nbetween conceptual representations derived under different conditions (i.e., varying\ncontexts or models), which is non-parametric and has been widely adopted to measure\ntopological alignment between representation spaces [102]. Let $X \\in \\mathbb{R}^{m \\times d_1}$ and $Y \\in$\n$\\mathbb{R}^{m \\times d_2}$ denote two sets of conceptual representations for m concepts with dimensionality\nof $d_1$ and $d_2$, respectively. Each space is characterized by its relational structure,\nrepresented by a (dis)similarity matrix $M \\in \\mathbb{R}^{m \\times m}$, where the entry $M_{i,j}$ denotes\nthe (dis)similarity between the representations of the ith and jth concepts in the\ncorresponding space. The alignment can then be calculated as the Spearman's rank\ncorrelation between the upper (or lower) diagonal portion of the two matrices Mx and\nMy, yielding values ranging from 1 to 1.\nAn appropriate similarity function is needed for the (dis)similarity matrix to char-\nacterize the relational structure of the representation space. While cosine similarity\nhas been widely adopted since the advent of static distributed word embeddings, it\nmay be suboptimal for capturing non-linear relationships and sensitive to outliers [103].\nWe thus employed two different metrics including the cosine similarity and Spear-\nman's rank correlation. Comparison with human behavioural data suggests that the\ncosine similarity captures semantic similarity (e.g., CUPS and MUGS) while Spear-\nman's rank correlation reflects both similarity and association (e.g., CUPS and COFFEE)\n(Supplementary Information SI 1.3, Fig. S3a-b). We primarily used Spearman's rank"}, {"title": "4.4 Characterization of model complexity", "content": "For a general characterization of the complexity of the 67 LLMs used in our experiments,\nwe represented each by its number of parameters and amount of training data (i.e., the\nnumber of tokens used during training) -factors identified as crucial for determining\nmodel quality [57, 105]. A principal component analysis (PCA) was then applied to\nthese factors. The first principal component, accounting for 70.67% of the total variance,\nwas used to represent the complexity of each model. In terms of the Mistral models,\nwhere information about their pretraining data was not accessible, we estimated it\nbased on the data volume of comparable models released around the same time (Table\nS1)."}, {"title": "4.5 Prediction of human similarity judgments", "content": "To evaluate how closely the relationships between LLM-derived representations align\nwith psychological measures of similarity, we first compared them with human similarity\nratings for concept pairs from SimLex-999 [50]. The dataset explicitly distinguishes\nsemantic similarity from association or relatedness and contains human ratings for 999\nconcept pairs spanning 1,028 concepts. The concepts cover three word classes including\nnouns, verbs and adjectives. We used description-word pairs from the WordNet data\nto provide contextual demonstrations to the LLMs, thereby deriving conceptual\nrepresentations. Model performance was assessed through Spearman's rank correlation\nbetween the similarity scores derived from the LLM representations and the human\nratings.\nWe further employed the odd-one-out similarity judgments from the THINGS-\ndata collection [43] to validate the effectiveness of LLM representations in handling\nthe computation of similarities. The dataset consists of triplets sampled from the\n1,854 concepts in THINGS. We presented LLMs with contextual demonstrations\nrandomly sampled from THINGS to obtain their conceptual representations. For each\ntriplet (i, j, k), we took the corresponding conceptual representations (hi, hj, hk) and\ncalculated pairwise similarity. The one outside the most similar pair was identified as\nthe odd-one-out and compared to human judgments. We evaluated model performance\nmainly with a set of 1,000 triplets from THINGS, each with multiple responses collected\nfrom different participants. We compared the LLMs' judgments with the majority of"}, {"title": "4.6 Categorization", "content": "For the categorization experiment, we employed high-level human-labelled natural\ncategories in THINGS [45]. We removed subcategories of other categories, concepts\nbelonging to multiple categories and categories with fewer than ten concepts [51]. This\nresulted in 18 out of 27 categories, including animal, body part, clothing, container,\nelectronic device, food, furniture, home decor, medical equipment, musical instrument,\noffice supply, part of car, plant, sports equipment, tool, toy, vehicle and weapon. These\ncategories comprise 1,112 concepts.\nWe employed three methods to evaluate the extent to which category membership\ncan be inferred from LLM-derived conceptual representations, with the former two\ncorresponding to prototype and exemplar models and the third explicitly examining the\nrelationships between high-level categories and their associated concepts. Specifically,\nfor the prototype model, we used a cross-validated nearest-centroid classifier, performing\ncategorization by iteratively leaving each concept out. In each iteration, we calculated\nthe centroid for each category by averaging the representations of the remaining\nconcepts. Categorization was then based on the similarity between the left-out concept\nand each category centroid. In the exemplar model, categorization was carried out using\na nearest-neighbour decision rule [106], where each concept was classified based on the\ncategory membership of its closest neighbours among all other concepts (Supplementary\nInformation SI 2.2). Our final approach involved constructing a representation for each\nof the 18 categories using the same N demonstrations employed for the 1,854 concepts.\nHere, we directly used the category names as query descriptions, categorizing concepts\nby their similarity to each category representation.\nWe combined t-SNE with multidimensional scaling (MDS) to visualize the rep-\nresentations in two dimensions, thereby preserving the global structure while better\ncapturing local similarities. The representations were first reduced to 64 dimensions\nusing MDS, with distances calculated as 1 - similarity and then visualized using t-SNE\nwith a perplexity of 30 and 1000 iterations."}, {"title": "4.7 Prediction of gradient scales along features", "content": "To probe whether LLM conceptual representations could also recover detailed knowl-\nedge about gradient scales of concepts along various features, we used human ratings\nspanning various categories and features. The dataset [52] includes 52 category-feature\npairs, where participants were asked to rate a concept (e.g., WHALE) within a certain\ncategory (e.g., animal) along multiple feature dimensions (e.g., size and danger). The\nratings cover nine categories including animals, cities, clothing, mythological creatures,\nfirst names, professions, sports, weather phenomena and states of the United States,\nwith each matched with a subset of 17 features including age, arousal, cost, danger,\ngender, intelligence, location (indoors versus outdoors), partisanship, religiosity, size,\nspeed, temperature, valence, (auditory) volume, wealth, weight and wetness.\nFor each category-feature pair, we provided the model with two demonstrations\nillustrating the extreme values of a target feature within a category. We then queried\nthe model for the rating of each concept in the category to obtain the corresponding\nrepresentations. For example:\nthe precise size rating of whales from 1 (small, little, tiny) to 5 (large, big, huge) \u21d2 5\nthe precise size rating of ants from 1 (small, little, tiny) to 5 (large, big, huge) \u21d2 1\nthe precise size rating of tigers from 1 (small, little, tiny) to 5 (large, big, huge) \u21d2 ?\n\u2190\n\u2192\nSimilar to previous work [52], we constructed a scale vector by subtracting the represen-\ntation of the minimum extreme from the maximum (e.g., $/size_{animal}$ = whale \u2013 ant),\nand compared all representations to this scale to obtain their relative feature rat-\nings. The performance of LLM-derived representations was then evaluated through\nthe Spearman's rank correlation between the ratings derived from them and human\nratings. For each category-feature pair, we estimated a 95% confidence interval based\non 10,000 bootstrap samples. To control for multiple comparisons across the 52 pairs,\nP-values were corrected using the false discovery rate (FDR) method."}, {"title": "4.8 Word embeddings for comparison", "content": "To validate the effectiveness of LLM-derived conceptual representations in capturing\nhuman knowledge and to explore their distinct advantages over traditional static\nword embeddings, we compared them with a state-of-the-art 300-dimensional static\nword embedding trained through fastText on Common Crawl and Wikipedia [49].\nUnlike LLM representations, we used cosine similarity as the similarity measure for\nthe word embeddings, as it consistently aligned better with human behavioural data\n(Supplementary Information SI 1.3).\nThe word embeddings were compared with human behavioural data using the same\nmethod applied to LLM conceptual representations across all experiments, except for\nthe analysis of gradient distinctions along various features. As static word embeddings\ndo not support context-dependent computations, we followed the procedure in previous\nwork [52] to compute a scale vector based on several antonym pairs denoting opposite\nvalues of the target feature. For instance, the opposite values for the feature \u201csize\u201d\nwere represented by (large, (large, big, huge) and (small, (small, little, tiny), with the scale vector\n\u2192\n$/size_{//animal}$ calculated as the average of the 3 \u00d7 3 = 9 pairwise vector differences between\nthe antonyms. The word embedding for each item was then projected onto this scale\nvector, and the resulting projections were correlated with human ratings for evaluation."}, {"title": "4.9 Encoding model of neural representations in the brain", "content": "We used the fMRI dataset from the THINGS-data collection [43] to explore whether\nLLM conceptual representations can map onto brain activity patterns associated with\nvisually grounded concepts. The dataset encompasses brain imaging data from three\nparticipants exposed to 8,740 representative images of 720 concepts over 12 sessions. The"}, {"title": "4.10 Variance partitioning between conceptual and alternative representations", "content": "To validate the effectiveness of LLM conceptual representations in explaining neural\nresponses, we compared them against two alternative representations: (1) fastText\nembeddings as in Methods 4.8 and (2) 66-dimensional similarity embeddings [43]\ntrained on 4.10 million human odd-one-out judgments of concepts in THINGS, which\nalign well with human similarity judgments. These allowed us to examine the additional\ninformation encoded in LLM conceptual representations that aligns with human brain\nactivity, beyond word information and similarity.\nWe combined each baseline representation with LLM-derived conceptual represen-\ntations and analysed the shared and unique variance each could account for [107]. To\nisolate unique variance, we orthogonalized the target representation and the neural\nresponses with respect to the alternate representation, thereby removing the shared\nvariance from both the representation and the fMRI data. The residuals of the target\nrepresentation were then used to predict the fMRI residuals, and the unique variance\nexplained by the target representation was calculated as the $R^2$ using twenty-fold\ncross-validation. For shared variance, we first concatenated both representations, used\nthem to predict neural responses, and calculated the $R^2$ in the same twenty-fold cross-\nvalidation to determine the total variance explained. Shared variance was subsequently\nestimated by subtracting the unique contributions of each representation from this\ntotal. For our results, we focused on voxels with a noise ceiling greater than 5% and\nreported the noise-ceiling-normalized $R^2$."}]}