{"title": "Higher-order-ReLU-KANs (HRKANs) for\nsolving physics-informed neural networks (PINNS)\nmore accurately, robustly and faster", "authors": ["Chi Chiu SO", "Siu Pang YUNG"], "abstract": "Finding solutions to partial differential equations (PDEs) is an important and\nessential component in many scientific and engineering discoveries. One of the\ncommon approaches empowered by deep learning is Physics-informed Neural\nNetworks (PINNs). Recently, a new type of fundamental neural network model,\nKolmogorov-Arnold Networks (KANs), has been proposed as a substitute of\nMultilayer Perceptions (MLPs), and possesses trainable activation functions. To\nenhance KANs in fitting accuracy, a modification of KANs, so called ReLU-KANs,\nusing \"square of ReLU\" as the basis of its activation functions has been suggested.\nIn this work, we propose another basis of activation functions, namely, Higher-\norder-ReLU, which\n\u2022 is simpler than the basis of activation functions used in KANs, namely, B-\nsplines;\n\u2022 allows efficient KAN matrix operations; and\n\u2022 possesses smooth and non-zero higher-order derivatives, essential for physics-\ninformed neural networks.\nOur detailed experiments on two standard and typical PDEs, namely, the linear\nPoisson equation and nonlinear Burgers' equation with viscosity, reveal that our\nproposed Higher-order-ReLU-Kans (HRKANs) achieve the highest fitting accuracy\nand training robustness and lowest training time significantly among KANs, ReLU-\nKANs and HRKANs.", "sections": [{"title": "Introduction", "content": "Partial differential equations (PDEs) play a prominent role in many real-world scenarios for modelling\nthe dynamics in the systems of interest. Solving PDEs therefore is essential for us to learn the\ndynamics of many scientific and engineering phenomena. However, it is often difficult, if not\nimpossible, to find an exact solution of PDEs, especially when the PDE systems or the boundary or\ninitial conditions are too complex [1, 2]. A variety of numerical methods thus become desirable given\ntheir ease of use and wide applicability in finding approximate solutions for the PDEs. Common\nexamples include finite element methods [3], mesh-free methods [4], finite difference methods [5],\nfinite volume methods [6], and boundary element methods [7].\n\"AI for Science\" is a recently emerging field of AI, which comprises of inventing and using deep\nlearning algorithms to solve PDEs. Among these algorithms, there are four major approaches:\n1. Physics-informed Neural Networks (PINNs) [8]: In this approach, a neural network is\ndesigned to approximate the solution of PDEs. The loss function of the neural network is"}, {"title": null, "content": "defined based on the PDE equations, with the aim to train the neural network to become the\nsolution of the PDEs. PINNs have been widely adopted in solving PDEs [9], identifying\nPDEs from data [10] and obtaining optimal controls of PDEs [11, 12].\n2. Operator learning: In this approach, a neural network is designed to learn the operator\nmapping from PDE space to the solution space from a massive data-set [13], key represen-\ntatives of which include DeepONet [14, 15] and Fourier Neural Operator (FNO) [16]. A\nmain difference between PINNs and operator learning is that when the initial or boundary\nconditions change, the PINNs need to be re-trained with a new loss function or additional\ndata points, whereas the operators do not need to be re-trained. Instead, the operators can\nimmediately provide solutions even when those conditions change [17].\n3. Physics-informed Neural Operators [18\u201320]: This approach combines PINNs with operator\nlearning such that the operator learns the mapping from the PDE space to the solution space\nfrom a physics-informed loss function, without the necessary need of big data-set.\n4. Neural Spatial Representation and PDE-Nets: These models are proposed to solve time-\ndependent PDEs. Neural Spatial Representation [21] uses neural networks to represent a\nspatially-dependent function to map the solution at a time instance to the next time instance.\nPDE-Nets [22, 23] design tailor-made convolution filters to act as differential operators and\napply forward Euler method to map the solution at a time instance to the next time instance.\nUnderlying all of these four approaches, the backbone of the neural network models is Multilayer per-\nceptrons (MLPs) [24, 25], also known as fully-connected feedforward neural networks. The famous\nuniversal approximation power possessed by MLPs [26] make it the most essential building block in\ndeep learning applications, ranging from self-driving cars [27] to textual and video generation [28].\nRecently, a new architecture of neural network building block, Kolmogorov-Arnold Networks\n(KANs) [29], has been proposed as a substitute of MLPs. Different from MLPs, KANs are not backed\nby the universal approximation theorem. Instead, KANs are supported by the Kolmogorov-Arnold\nrepresentation theorem [30], which states that any multivariate function can be expressed by a finite\ncombination of univariate functions [31]. Another key difference between MLPs and KANs lie on\ntheir parameter space. The parameter space of MLPs solely consists of weight matrices, whereas the\nparameter space of KANs also includes the activation functions. In other words, KANs need to learn\nnot only optimal weight matrices, but also the optimal activation functions. In the original invention\nof KANs [29], B-splines are used as the basis of activation functions to be learnt. Afterwards,\nsuggestions of different basis of activation functions have been seen, including Chebyshev orthogonal\npolynomials [32], radial basis functions [33], wavelet transforms [34], Jacobi basis functions [35, 36],\nFourier transform [37] and \"square of ReLU\" basis functions [38].\nSome efforts have also been witnessed in trying to apply KANs to find solutions of PDEs, for example,\nby incorporating KANs in DeepONets [39] and putting KANs in PINNs [40\u201343]. However, due to\nthe complexity of KANs' B-spline basis functions, the training speed of KANs is not comparable to\nMLPs. One possible remedy is the use of \"square of ReLU\" basis functions in ReLU-KANs, which\noptimize KAN operations for efficient GPU parallel computing. A key drawback, nevertheless, is the\ndiscontinuity of higher-order derivatives of such \"square of ReLU\" basis functions.\nIn this paper, we introduce a more suitable basis function, Higher-order-ReLU (HR), which not\nonly optimizes KAN operations for efficient GPU parallel computing but at the same time is well-\nsuited for PINNs. We call such KANs using Higher-order-ReLU basis functions (HR) as HRKANS,\nand such PINNs with HRKANs as PI-HRKANs. We evaulated PI-HRKANs' performance on two\nstandard and typical PDEs, namely, the linear Poisson equation (the example used in the github\ncodebase of the original KAN paper), and the nonlinear Burgers' equation with viscosity. Compared\nagainst KANs and ReLU-KANs, HRKANs exhibit significant improvements in fitting accuracy,\ntraining robustness and convergence speed significantly. The code of our paper is available at\nhttps://github.com/kelvinhkcs/HRKAN.\nThis paper presents the following key contributions:\n\u2022 A simple activation basis function: We introduce a simple activation basis function, Higher-\norder-ReLU (HR), which inherits the fitting capability of the original KAN B-spline basis\nfunctions and the \"square of ReLU\" basis functions."}, {"title": null, "content": "\u2022 Matrix-based operations: Similar to the \"square of ReLU\" basis functions in ReLU-KAN,\nHigher-order-ReLU facilitates efficient matrix computations with GPUs, speeding up the\ntraining process.\n\u2022 Well-suited for Physics-informed problems: While enabling efficient matrix operations\nand simpler than the B-spline basis functions, Higher-order-ReLU basis functions possess\ncontinuous higher-order derivatives (with order up to users' choices) suitable for Physics-\ninformed problems, an important property missing in \"square of ReLU\" basis functions.\n\u2022 Higher fitting accuracy, stronger robustness and faster convergence speed: HRKANs demon-\nstrate highest fitting accuracy, strongest training robustness and fastest convergence speed\ncompared against KANs and ReLU-KANs in our detailed experiments."}, {"title": null, "content": "In section 2, we introduce the B-spline activation basis functions in KANs and the \"square of ReLU\"\nactivation basis functions in ReLU-KANs and their respective properties. In section 3, we introduce\nour proposed Higher-order-ReLU activation basis functions, and discuss how Higher-order-ReLU is\nbetter than B-splines and \"square of ReLU\". In section 4, we conduct detailed and comprehensive\nexperiments to evaluate HRKANs against KANs and ReLU-KANs, in terms of convergence speed,\ntraining robustness and fitting accuracy. Section 5 is our conclusion."}, {"title": "Basis of activation function in KANs and ReLU-KANS", "content": "In this section, we review the basis of activation function used in KANs and ReLU-KANs, along with\nsome comparisons."}, {"title": "KANs: B-spline basis function", "content": "In KANs, B-splines are used as the basis of activation function. We use $B_{g,k} =\n\\{b_0(x), b_1(x),...,b_n(x)\\}$ to denote a set consisting of $n + 1$ B-spline basis with $g$ grid points\nand order $k$. Each $b_i$ is defined recursively from linear combination of B-spline basis functions of\nlower orders, i.e. order less than $k$ [44]. B-Spline basis functions are of compact support and their\nvalues depend on $k$ and $g.\""}, {"title": null, "content": "In figure 1, the left hand side shows the B-spline basis $B_{5,3} = \\{b_0(x), b_1(x),..., b_7(x)\\}$ on the\ninterval [-1, 1] with $g = 5$ and $k = 3$, and the right hand side shows the $b_4$ in the B-spline basis on\nthe interval [-1,1] with $g = 5$ and $k = 3."}, {"title": "ReLU-KANs: \"square of ReLU\" basis function", "content": "In ReLU-KANs, the $b_i(x)$ with support on $[s_i, e_i]$ is replaced by the \"square of ReLU\" basis $r_i(x)$\ndefined as\n$$r_i(x) = [ReLU(e_i \u2013 x) \u00d7 ReLU(x \u2212 s_i)]^2 \u00d7 c,$$\nwhere $c = \\frac{16}{(e_i-s_i)^4}$ is a normalization constant for ensuring the height of each basis being 1.\nSimilar to the B-spline basis functions, the $r_i(x)$ is of compact support. Their values depend on the\nchoice of $e_i$ and $s_i$. Figure 2 shows the \"square of ReLU\" basis functions and $r_4(x)$ on the interval\n[-1,1] defined with the same support as the B-splines basis function example in figure 1."}, {"title": "Our method: Higher-order-ReLU", "content": "In this section, we introduce our proposed basis function, namely, Higher-order-ReLU, an improve-\nment from the \"square of ReLU\". Higher-order-ReLU looks similar to \"square of ReLU\", but is better\nthan \"square of ReLU\" in the following ways:\n1. Higher-order-ReLU offers smooth higher-order derivatives, essential to Physics-informed\nproblems, whereas \"square of ReLU\" has discontinuous derivatives.\n2. Higher-order-ReLU still inherits all advantages of \"square of ReLU\" on its simplicity and\nefficient matrix-based KAN operations over the B-splines.\nThe Higher-order-ReLU basis $v_i$ of order $m$ is defined as\n$$v_{m,i}(x) = ReLU(e_i \u2013 x)^m \u00d7 ReLU(x \u2013 s_i)^m \u00d7 c_m$$\nwhere $c_m = \\frac{2^{2m}}{(e_i-s_i)^{2m}}$ is the normalization constant for ensuring the height of each basis being 1.\nA key difference between the $r_i(x)$ in \"square of ReLU\" and the $v_{m,i}(x)$ in Higher-order-ReLU lies\non its order $m$, which determines the smoothness of higher-order derivatives. Figure 3 compares the\nHigher-order-ReLU $v_{m,i}(x)$ with $m = 4$ and \"square of ReLU\" $r_i(x)$ and their first and second-order\nderivatives. It is obvious that the second-order derivative of \"square of ReLU\" jumps at the end points\nof its support, which is hinders the learning of PDE solution. If a PDE equation possesses derivatives\nof order higher than 4, for example, $u_{xxxxx}$, we can pick $m$ to be larger, like 6, adaptively, whereas\nthe basis $B_{5,3}$ and its \"square of ReLU\" counterpart will fail as their 5-th order derivative will be\ncompletely zero already.\nComparing with the B-splines in KANs, we can observe that the higher-order derivatives of the\nB-spline basis are smooth, but they vanish up to the order $k$ specified. So, again if the PDE equation"}, {"title": null, "content": "possesses derivatives of order higher than 4, for example, $u_{xxxxx}$, then the $k$ must be set to be higher\nthan 6 to work. In this case, as B-splines are essentially polynomial, having an order which is too\nhigh may lead to the Runge's phenomenon [45], a large obstacle in function approximation."}, {"title": "Experiments", "content": "In this section, we evaluate the performance of KANs, ReLU-KANs and HRKANs (with $m = 4$)\nwith two physics-informed problems. The first one is a linear PDE, the Poisson equation. The second\none is a non-linear PDE, the Burgers' equation with viscosity."}, {"title": "Poisson Equation", "content": "Problem Description. The 2D Poisson equation is\n$$\\nabla^2u(x,y) = \u22122\u03c0^2 sin(\u03c0x) sin(\u03c0y)$$\nfor $(x, y) \u2208 [-1,1] \u00d7 [-1,1]$, with boundary conditions\n$$\\begin{cases}\nu(-1,y) = 0 \\\\u(1, y) = 0 \\\\u(x, -1) = 0 \\\\u(x, 1) = 0\\end{cases}$$\nThe ground-truth solution is\n$$u(x, y) = sin(\u03c0\u03c7) sin(\u03c0y).$$\nWe trained a KAN, a ReLU-KAN and a HRKAN using the loss function\n$$L = \u03bbL_{pde} + L_{bc}$$\nwhere\n$$L_{pde} = \\frac{1}{N_{pde}} \\sum_{i=1}^{N_{pde}} (\\nabla^2u(x_i, y_i) + 2\u03c0^2 sin(\u03c0x_i)sin(\u03c0y_i))^2,$$\n$$L_{bc} = \\frac{1}{N_{bc1}+ N_{bc2} + N_{bc3} + N_{bc4}} \\sum_{i=1}^{N_{bc1}} (u(-1,y_i))^2 + \\sum_{i=1}^{N_{bc2}} (u(1, y_i))^2 + \\sum_{i=1}^{N_{bc3}} (u(x_i, -1))^2 + \\sum_{i=1}^{N_{bc4}} (u(x_i, 1))^2.$$"}, {"title": null, "content": "We set hyperparameters $\u03b1 = 0.05$. The number of interior and boundary points in the train-set are\n$N_{pde} = 960$, $N_{bc1} = 30$, $N_{bc2} = 30$, $N_{bc3} = 30$ and $N_{bc4} = 30$ respectively. The train-set data points\non the interior are generated randomly while those for boundary and initial conditions are generated\nuniformly. The structure of the KAN, ReLU-KAN and HRKAN are all of [2, 2, 1] with same support\nfor all the bases as $g = 5$ and $k = 3$. For all three of them, we use the Adam optimizer, set the\nnumber of epoch to 3000 and train on the same GPU. We run the same experiment for 10 times.\nTwo remarks have to be made here.\n1. In the default implementation of KAN, there is a call of the function\nupdate_grid_from_samples() every five epoch till epoch 50. We keep this im-\nplementation unchanged in both experiments of Poisson equation and Burgers' equation\nwith viscosity, so KAN can update its grid whereas ReLU-KAN and HRKAN cannot in our\nexperiments.\n2. In the default implementation of KAN, there is a also a step of sparsification for pruncing.\nAgain, we have not include this implementation in the ReLU-KAN and HRKAN but keep\nthe implementation in KAN unchanged.\nBoth of these two features may lead to extra overhead in computation, but we aim to keep the code of\nKAN unchanged to be fair.\nResults Discussion. We generate test-set data with a grid of grid size 100 \u00d7 100.\nFigure 4 shows the ground-truth solution and the solutions learnt by the KAN, ReLU-KAN and\nHRKAN and their residual difference compared against the ground-truth solution in one of the 10\nruns with the Poisson equation.\nFigure 5 reveals the median and max-min-band of training loss, test loss and test MSE (Mean square\nerror between the test-set ground-truth solution and the learnt solutions) of 10 runs for the KAN,\nReLU-KAN and HRKAN for the Poisson equation. For ease of reading, the first column shows the\ncurves for the entire training process, while the second and last columns show those in the last 2000\nand last 1000 epochs respectively."}, {"title": null, "content": "Table 1 exhibits the means and standard deviations of test MSE, and means of training time for the\nKAN, ReLU-KAN and HRKAN in the 10 runs with the Poisson equation."}, {"title": null, "content": "There are some key observations to make:\n\u2022 From figures 4, the last row of figure 5 and the first row of table 1, it can be obviously\nobserved that the solutions learnt by HRKAN achieve the highest fitting accuracy (test\nMSE), while both ReLU-KAN and KAN fail the learn the solution accurately.\n\u2022 The mean test MSEs attained by ReLU-KAN and KAN are 2.18% and 6.80% respectively,\nwhereas the average test MSE attained by HRKAN is 0.0434%, at a difference in the scale\nof 10000%. This once again demonstrates the strong fitting ability of HRKANs.\n\u2022 Also, the standard deviations of test MSEs over the 10 runs are the smallest for HRKAN, at\nthe level of 0.129%, where that of ReLU-KAN and KAN are 4.28% and 9.43% respectively,\nat a difference in the scale of 1000%. This suggests the stronger robustness in the learning\nof HRKANS.\n\u2022 as shown in figure 5, the training loss, test loss and test MSE of ReLU-KAN keep flutuating\nover the training process. One possible reason may be the discontinunity of the activation\nfunctions in ReLU-KANs as we have discussed in section 3. Another possible explanation\nis the lack of regularization tools in ReLU-KANs compared to KANs, which deserve deeper\ninvestigation as further potential study.\n\u2022 The median and max-min-band of training loss, test loss and test MSE of HRKAN and KAN\non the other hand show that HRKAN converge relatively rapidly, approaching optimum at\naround epoch 500, whereas KAN and ReLU-KAN seem to be still converging slowly at\nthe end of the training process (as shown in the 2 plots at the right bottom in figure 7. This\nsuggests the fast convergence speed of HRKANS.\n\u2022 Lastly, we can see that the training time of both HRKAN and ReLU-KAN is much smaller\nthan KAN, exhibiting the efficiency of matrix operations in HRKAN and ReLU-KAN. As\nmentioned before, there is possibly extra overhead in computation for KANs because of its\nsteps of grid extension and sparsification, but we expect such steps won't contribute to most\nof the computation time for KANs, so our judgement remains valid."}, {"title": "Burgers Equation with viscosity", "content": "The Burgers' equation with viscosity is\n$$u_t + uu_x = \u03bdu_{xx} = 0$$\nfor $(x, t) \u2208 [-5,5] \u00d7 [0, 2.5]$, with boundary and initial conditions\n$$\\begin{cases}u(-5, t) = 0 \\\\u(5, t) = 0 \\\\u(x, 0) = \\frac{1}{cosh(x)}\\end{cases}$$\nand $\u03bd = 0.001$.\nWe solve for the ground-truth solution using Fast Fourier Transform method [46]. As we have not\nincluded the implementation of grid extension in ReLU-KAN and HRKAN, to make the comparison\nbetween KAN, ReLU-KAN and HRKAN fair, we perform a change of variable $y = \\frac{x+5}{4}$ to transform\nthe domain into square domain [0, 2.5] \u00d7 [0, 2.5]. The resulting PDE is\n$$u_t + \\frac{1}{4}uu_x = \\frac{\u03bd}{16}u_{xx} = 0$$"}, {"title": null, "content": "for $(x, t) \u2208 [0, 2.5] \u00d7 [0, 2.5]$, with boundary and initial conditions\n$$\\begin{cases}u(0, t) = 0 \\\\u(2.5, t) = 0 \\\\u(x, 0) = \\frac{1}{cosh(4x - 5)}\\end{cases}$$\nWe trained a KAN, a ReLU-KAN and a HRKAN using the loss function\n$$L = xL_{pde} + L_{bc & ic}$$\nwhere\n$$L_{pde} = \\frac{1}{N_{pde}} \\sum_{i=1}^{N_{pde}} (u_t(x_i,t_i) + \\frac{1}{4}u(x_i,t_i) u_x(x_i,t_i) - \\frac{\u03bd}{16} u_{xx}(x_i,t_i))^2,$$\n$$L_{bc & ic} = \\frac{1}{N_{bc1} + N_{bc2} + N_{ic}} (\\sum_{i=1}^{N_{bc1}} (u(0,t_i))^2 + \\sum_{i=1}^{N_{bc2}} (u(2.5,t_i))^2+ \\sum_{i=1}^{N_{ic}} (u(x_i,0) - \\frac{1}{cosh(4x_i - 5)})^2).$$\nWe set hyperparameters $\u03b1 = 0.05$, $N_{pde} = 10000$, $N_{bc1} = 100$, $N_{bc2} = 100$, $N_{ic} = 100$. The train-\nset data points on the interior are generated randomly while those for boundary and initial conditions\nare generated uniformly. The structure of the KAN, ReLU-KAN and HRKAN are all of [2, 3, 3, 3, 1]\nwith same support for all the bases as $g = 7$ and $k = 3$. For all three of them, we use the Adam\noptimizer, set the number of epoch to 3000 and train on the same GPU. We run the same experiment\nfor 10 times.\nAs said in the previous experiment, we keep the implementation of grid extension in KAN unchanged.\nResults Discussion. We generate test-set data with a grid with grid size 200 \u00d7 200.\nFigure 6 shows the ground-truth solution and the solutions learnt by the KAN, ReLU-KAN and\nHRKAN and their residual difference compared against the ground-truth solution in one of the 10\nruns with the Burgers' equation with viscosity."}, {"title": null, "content": "Table 2 exhibits the means and standard deviations of test MSE, and means of training time for the\nKAN, ReLU-KAN and HRKAN in the 10 runs with the Burgers' equation with viscosity."}, {"title": null, "content": "There are some key observations to make:\n\u2022 From figures 6 and 7 and table 2, it is obvious that the solutions learnt by HRKAN achieve the\nhighest fitting accuracy (test MSE), while the accuracy of the solutions learnt by ReLU-KAN\nand KAN is relatively lower.\n\u2022 The largest difficulty for all the three models to learn the solution probably lies on the\n\"discontinuity\" of the ground-truth solution near $t = 2.5$, as displayed in figure 6. Although\nall three models fail to learn such discontinuity in certain degree, the error in fitting accuracy\nis the lowest for HRKANs, as shown in figures 6, the last row of figure 7 and first row of\ntable 2.\n\u2022 The average test MSEs attained by ReLU-KAN and KAN are 0.575% and 1.48% respectively,\nwhereas the average test MSE attained by HRKAN is 0.229%, demonstrating a stronger\nfitting ability of HRKANs.\n\u2022 Also, the standard deviations of test MSEs over the 10 runs are the smallest for HRKAN,\nat the level of 0.861%, where that of ReLU-KAN and KAN are 2.23% and 5.85%. This\nsuggests the stronger robustness in the learning of HRKANs.\n\u2022 In figure 7, the test MSE converge at the fastest speed for HRKAN, approaching the\nminimum at around epoch 1500, whereas that of ReLU-KAN and KAN are still on the\nway of slowly converging at epoch 3000. This suggests the fast convergence speed of\nHRKANS. The flutuation of the training and test loss of HRKAN after epoch 1250 may imply\noverfitting of HRKANs after epoch 1250 to the \"discontinuous\" solution. The relatively\nmore fluctuating training loss and test loss may also due to the lack of regularization tools in\nReLU-KANs and HRKANs, compared to KANS.\n\u2022 Lastly, we can see that the training time of both HRKAN and ReLU-KAN is much smaller\nthan KAN, exhibiting the efficiency of matrix operations in HRKAN and ReLU-KAN. As\nmentioned before, there is possibly extra overhead in computation for KANs because of its\nsteps of grid extension and sparsification, but we expect such steps won't contribute to most\nof the computation time for KANs, so our judgement remains valid."}, {"title": "Conclusion", "content": "In this paper, we proposed a new basis of activation functions for KAN, namely, Higher-order-ReLU.\nSuch Higher-order-ReLU is (1) simpler than the B-Spline basis functions in the original KAN,\n(2) allows matrix-based operations for fast GPU computing, (3) possesses non-zero and smooth\nhigher-order derviatives essential to Physics-informed neural networks (PINNs). We evaluated\nHigher-order-ReLU KANs (HRKANs) on two typical PDEs, namely, a linear Poisson equation, and\na non-linear Burgers' equation with viscosity. The detailed experiments exhibit the higher fitting\naccuracy, stronger robustness and faster convergence speed. Not only can HRKANs find solutions of\nPDEs, we expect it to have strong potential in further extension like (1) identifying coefficients of\nPDEs, (2) finding the optimal control of PDEs, and (3) explaining a neural operator etc."}]}