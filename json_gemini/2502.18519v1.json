{"title": "FreeTumor: Large-Scale Generative Tumor Synthesis in Computed Tomography Images for Improving Tumor Recognition", "authors": ["Linshan Wu", "Jiaxin Zhuang", "Yanning Zhou", "Sunan He", "Jiabo Ma", "Luyang Luo", "Xi Wang", "Xuefeng Ni", "Xiaoling Zhong", "Mingxiang Wu", "Yinghua Zhao", "Xiaohui Duan", "Varut Vardhanabhuti", "Pranav Rajpurkar", "Hao Chen"], "abstract": "Tumor is a leading cause of death worldwide, with an estimated 10 million deaths attributed to tumor-related diseases every year. Al-driven tumor recognition unlocks new possibilities for more precise and intelligent tumor screening and diagnosis. However, the progress is heavily hampered by the scarcity of annotated datasets, which demands extensive annotation efforts by radiologists. To tackle this challenge, we introduce FreeTumor, an innovative Generative AI (GAI) framework to enable large-scale tumor synthesis for mitigating data scarcity. Specifically, FreeTumor effectively leverages a combination of limited labeled data and large-scale unlabeled data for tumor synthesis training. Unleashing the power of large-scale data, FreeTumor is capable of synthesizing a large number of realistic tumors on images for augmenting training datasets. To this end, we create the largest training dataset for tumor synthesis and recognition by curating 161,310 publicly available Computed Tomography (CT) volumes from 33 sources, with only 2.3% containing annotated tumors. To validate the fidelity of synthetic tumors, we engaged 13 board-certified radiologists in a Visual Turing Test to discern between synthetic and real tumors. Rigorous clinician evaluation validates the high quality of our synthetic tumors, as they achieved only 51.1% sensitivity and 60.8% accuracy in distinguishing our synthetic tumors from real ones. Through high-quality tumor synthesis, FreeTumor scales up the recognition training datasets by over 40 times, showcasing a notable superiority over state-of-the-art AI methods including various synthesis methods and foundation models. On average, FreeTumor improves the segmentation Dice scores by 6.7% and early tumor detection sensitivity by 16.4%. These findings indicate promising prospects of FreeTumor in clinical applications, potentially advancing tumor treatments and improving the survival rates of patients.", "sections": [{"title": "1 Introduction", "content": "Tumors contribute significantly to the global burden of disease, accounting for an estimated 10 million deaths annually according to the findings of the World Health Organization [1]. With the rapid advancements of deep learning [2\u20136], AI-driven tumor recognition [7-15] has received increasing attention in clinical applications. However, existing tumor recognition methods heavily rely on annotated tumor datasets for training [7-9, 13, 16], demanding substantial medical expertise and dedicated efforts for data collection and annotation. Suffering from the data-hungry nature of AI methods and the extensive annotation burden, the limited scale of tumor datasets significantly poses a substantial obstacle to the advancement of AI-driven tumor recognition.\nTo address this challenge, data augmentation with synthetic data has emerged as a potential solution. Recently, Generative AI (GAI) [17-22] has witnessed rapid development, which can generate large-scale realistic images, presenting a potential solution to mitigate the scarcity of annotated datasets [23]. Specifically, synthetic data can increase the scale and diversity of training datasets, significantly boosting the"}, {"title": "2 Results", "content": "Datasets. The rapid advancements in medical imaging have enabled the collection of large-scale CT data. However, few previous works have considered harnessing the untapped potential of large-scale unlabeled CT data for tumor recognition [39]. As shown in Figure 1 (c), we curate the existing largest training dataset for tumor synthesis and recognition, encompassing 161,310 publicly available CT volumes from 33 different sources. It is worth noting that only 2.3% of them (3,696 volumes) contain annotated tumors. The pre-processing details of datasets are presented in Section 4.5. Details of datasets are presented in Extended Data Table A17.\nClinician evaluation of synthetic tumors. It has been a common practice to utilize fidelity metrics like Fr\u00e9chet Inception Distance (FID) [57] to measure the quality of natural image synthesis in GAI models [17-21], where lower FIDs reflects higher synthesis quality. We first evaluate the FID results of our synthetic tumors, detailed FID results are presented in Extended Data Table A4 and Figure A3. We observe that our proposed FreeTumor can achieve lower FID compared with two previous tumor synthesis methods [40, 48]. However, we have noted limitations in the effectiveness of FID [57] in reflecting tumor synthesis quality. Specifically, many synthetic tumors, despite with low FIDs, still present with unrealistic characteristics in the views of radiologists. The inherent challenge lies in the fact that tumor regions predominantly exhibit small sizes with abnormal intensities, rendering conventional fidelity metrics unreliable [16, 40, 48]. Clinician evaluation serves as a more convincing standard for validating the quality of tumor synthesis. To this end, we invited 13 board-certified radiologists to evaluate the quality of synthetic tumors.\nEvaluation of tumor segmentation and detection. Tumor segmentation [7-9] aims to precisely segment target tumors by capturing their positions, sizes, and shapes. In contrast, tumor detection [7, 13, 48] focuses on identifying the presence and location of tumors, without the need to outline their precise shapes and sizes. Following previous methods [13, 48, 58-61], tumor detection is also achieved by the tumor segmentation models, where detected tumors are identified when segmentation predictions overlap with ground truth labels. For the evaluation of early tumor detection, we present the detection results of small tumors (diameter < 2cm) [13, 61]. The diameter measurement follows the standard of the World Health Organization (WHO) [61, 62]."}, {"title": "2.1 Clinician Evaluation", "content": "We invited 13 board-certified radiologists to evaluate the fidelity of synthetic tumors through a Visual Turing Tests [23]. These radiologists are from 4 hospitals in China, i.e., Li Ka Shing Faculty of Medicine of The University of Hong Kong (HKU), Shenzhen People's Hospital, Sun Yat-Sen Memorial Hospital of Sun Yat-Sen University, and The Third Affiliated Hospital of Southern Medical University. Among the group of 13 radiologists, there are 6 junior radiologists, 4 mid-level radiologists, and 3 senior radiologists. Each level of radiologists is defined by the following standards:\n\u2022 Junior radiologists: Doctors in residency programs, with 5-10 years of clinical experience.\n\u2022 Mid-level radiologists: Doctors with a professional tenure of 10-20 years in hospitals.\n\u2022 Senior radiologists: Doctors with advanced professional titles in hospitals, with at least 20 years of clinical experience.\nThe process of the Visual Turing Test is shown in Figure 1 (e). During the Visual Turing Test, 13 radiologists were presented with the same set of CT volumes containing tumors, with each volume containing only one tumor case for evaluation. Half of these tumors are real and the remaining half are synthesized by FreeTumor. Specifically, we provided 18 cases each of liver tumors, pancreas tumors, kidney tumors, lung tumors, and COVID-19 (a total of 90 cases) for evaluation. There are 45 real and 45 synthetic tumors among 90 tumor cases. For each type, the numbers of real and synthetic tumors are also equal (9 real and 9 synthetic in 18 tumor cases). These 90 cases are randomly selected from our datasets. During the Visual Turing Test, the radiologists were tasked with: (1) Identifying the synthetic tumors from real ones. (2) Discerning the distinguishing features between real and synthetic tumors. The radiologists were informed of the type of tumors they were required to identify, and the positions of tumors were also provided. The specific number of synthetic tumors for each type is unknown to the invited radiologists to prevent any bias in their assessments. On average, a radiologist would require about 2-3 hours to assess all 90 cases."}, {"title": "2.2 Accurate and Scalable Segmentation across Five Types of Tumors/Lesions", "content": "Comparison methods. We conduct extensive tumor segmentation experiments on 12 public datasets and report the corresponding Dice score results. First, we compare our FreeTumor with five widely-used tumor segmentation models [8, 50-53], i.e., UNet [51], TransUNet [52], UNETR [53], nnUNet [8], and SwinUNETR [50]. These works [8, 50-53] proposed to advance network architectures for improving tumor segmentation, while our FreeTumor is designed to address the challenges in tumor segmentation from the data scarcity aspect. We adopt SwinUNETR [50] as the segmentation model, thus SwinUNETR [50] can be seen as the baseline for comparisons. Second, we compare FreeTumor with two tumor synthesis methods [40, 48] and three CT foundation models [55, 56, 72]. In addition, we further evaluate the out-of-domain performance of FreeTumor. Out-of-domain evaluation represents transferring a model trained on a source dataset to a target dataset, i.e., direct inference on target datasets without fine-tuning models.\nFreeTumor outperforms baseline tumor segmentation models. As shown in Figure 3, on 12 public datasets across various types of tumors/lesions, our FreeTumor consistently outperforms five widely-used tumor segmentation models [8, 50-53] by a clear margin. By augmenting the training datasets by over 40 times, FreeTumor surpasses the baseline SwinUNETR [50] by 6.9%, 8.6%, 16.1%, 6.0%, 3.1%, 7.2%, 4.0%, 3.7%, 5.8%, 7.1%, 5.1%, and 7.9% on 12 datasets, respectively. Overall, FreeTumor brings an average +6.7% Dice score improvements (two-sided paired t-test p-values < 5.09 \u00d7 10-5) over the baseline SwinUNETR [50], which is a non-trivial advancement in tumor segmentation. The substantial improvements demonstrate that the scarcity of tumor annotations is a critical bottleneck in tumor segmentation. Specifically, as shown in Figure 3 (c), for the IRCAD [65] dataset that contains only 22 labeled CT volumes, FreeTumor demonstrates +16.1% Dice score improvements by augmenting training datasets. These findings robustly validate the rationale of our motivation to mitigate data scarcity. Detailed results are presented in Extended Data Table A5.\nFreeTumor outperforms previous tumor synthesis methods. We further compare FreeTumor with two tumor synthesis methods: SynTumor [40] and DiffTumor [48]. Note that both of these two tumor synthesis methods [40, 48] cannot leverage unlabeled data for synthesis training: (1) SynTumor [40] utilizes handcrafted image processing techniques for tumor synthesis. (2) DiffTumor [48] employs conditioned diffusion models for tumor synthesis, thus it can only leverage labeled data for tumor synthesis training (360 labeled volumes are used in this work). In addition, SynTumor [40] is only applicable to liver tumors, and DiffTumor [48] is not applicable to lung tumors and COVID-19. For fair comparisons, SynTumor [40] and DiffTumor [48] adopt the same segmentation model [50]."}, {"title": "2.3 Accurate Detection across Five Types of Tumors/Lesions", "content": "Tumor detection, especially the detection of early-stage tumors, is vital for the timely treatment of patients. Accurate early tumor detection can result in a greater probability of survival with less morbidity as well as less expensive treatment [13, 58-60, 75]. However, early-stage tumors are typically small in size, making them challenging to detect. Our proposed FreeTumor can synthesize tumors with flexible sizes. Thus, the synthesis of small tumors can serve as an effective data augmentation solution to improve the robustness of early tumor detection. In this study, we employ FreeTumor to synthesize a large number of small tumors for training, thereby boosting the sensitivity of early tumor detection and facilitating the timely treatment for patients.\nEvaluation of tumor detection across all stages of tumors. We first evaluate the detection performance across all tumor stages, with the F1-Score (%) results illustrated in Figure 6 (a). It can be seen that FreeTumor consistently surpasses the baseline methods [8, 50-53] without tumor synthesis. Notably, the F1-Scores of FreeTumor in detecting the five types of tumors/lesions all surpass 97%, highlighting the potential of FreeTumor in clinical practice.\nEffectiveness of detecting small tumors. To evaluate the performances of early tumor detection, we further present the results of detecting small tumors (diameter < 2cm) [61, 62]. We highlight the sensitivity improvements of FreeTumor in Figure 6 (d). It can be seen that limited by the data scarcity, the baseline methods [8, 50-53] are not sensitive in detecting small tumors/lesions. Equipped with FreeTumor, the detection of small liver tumors, pancreas tumors, kidney tumors, lung tumors, and COVID-19 are improved by 22.9%, 10.3%, 16.7%, 17.8%, and 14.1%, respectively. Notably, the overall sensitivity is improved from 49.7% to 66.1% (+16.4%), marking a substantial advancement towards accurate early tumor detection. These findings indicate promising prospects of FreeTumor in aiding the timely treatment of patients. Detailed sensitivity and specificity results are presented in Extended Data Figure A7."}, {"title": "3 Discussion", "content": "FreeTumor is the first GAI framework tailored for large-scale tumor synthesis and segmentation training. Our FreeTumor is designed to address the scarcity of annotated tumor datasets, aiming to unleash the power of large-scale unlabeled data for training. Specifically, FreeTumor effectively leverages a combination of limited labeled data and large-scale unlabeled data for tumor synthesis training. By large-scale tumor synthesis training, FreeTumor is capable of synthesizing a large number of tumors varying in sizes, positions, and backgrounds, thus boosting the robustness of tumor recognition models. Rigorous clinician evaluation conducted by 13 board-certified radiologists demonstrates the high quality of our synthetic tumors. To evaluate the effectiveness of FreeTumor, we create the largest training dataset for tumor synthesis and recognition, encompassing 161,310 publicly available CT volumes from diverse sources (with only 2.3% of them containing annotated tumors). Extensive experiments on 12 public datasets demonstrate the superiority of FreeTumor"}, {"title": "4 Methods", "content": "In this section, we first introduce the preliminary of our method in Section 4.1. The details of our tumor synthesis pipeline are illustrated in Section 4.2. Then, in Section 4.3, we further describe our quality control strategy to discard low-quality synthetic tumors. Following this, in Section 4.4, we discuss the process of integrating large-scale unlabeled data in segmentation training. Finally, in Section 4.5, we"}, {"title": "4.1 Preliminary of FreeTumor", "content": "Confronted with the challenge of conditioned diffusion models lacking the ability to leverage unlabeled data in synthesis training [48], we explore the adversarial training method to unleash the power of large-scale unlabeled data. Specifically, our synthesis training pipeline is motivated by the GAN-based semantic image synthesis methods [25, 27, 49, 85-88]. Semantic image synthesis aims to generate images with specific classes. Typically, GAN-based semantic image synthesis methods first train a classification model as the discriminator in the generative model. During synthesis training, this discriminator is utilized to classify the images generated by the generator, where higher classification accuracy indicates higher quality of synthetic images. In this way, the generator can be trained by minimizing the classification loss.\nIn this paper, we propose to shift this paradigm to the field of tumor synthesis. Specifically, instead of using classification models, we propose to train a tumor segmentation model as the discriminator to distinguish synthetic tumors. Furthermore, unlike previous semantic image synthesis methods focused solely on image generation, our synthetic tumors are utilized to augment segmentation training datasets. Thus, to alleviate the negative impact of low-quality synthetic tumors, we further leverage the discriminator to enable automatic quality control of synthetic tumors. The framework of FreeTumor is shown in Extended Data Figure A1."}, {"title": "4.2 Large-Scale Generative Tumor Synthesis Training", "content": "First, we train a tumor segmentation model to discriminate between real and synthetic tumors. In Stage 1, we train a baseline segmentation model with only labeled tumor datasets, which will be employed as the discriminator of the following tumor synthesis model to discriminate the synthetic tumors.\nSecond, we employ the adversarial training strategy to train a tumor synthesis model. The first step is to simulate the tumor positions on the healthy organs, which aims to select a proper location for the synthetic tumors. Specifically, we first generate organ labels for these datasets (as described in Section 4.5). With organ labels, it is easy to select a location to synthesize tumors, e.g., liver tumors on livers, pancreas tumors on pancreases. Here, we denote the tumor mask as M that represents the positions of synthetic tumors, where M = 1 are the positions of synthetic tumors and M = 0 remain as the original values. The tumor mask M is generated with"}, {"title": "4.3 Quality Control of Synthetic Tumors for Large-Scale Segmentation Training", "content": "It is worth noting that synthetic tumors are not always flawless or perfect. We observe that the low-quality synthetic tumors will deteriorate the tumor segmentation training. Previous tumor synthesis methods [40, 41, 43, 44, 48] largely ignored to alleviate their negative impacts. Thus, based on our discriminator, we develop an effective quality control strategy to automatically discard low-quality synthetic tumors.\nSegmentation-based discriminator for quality control. Our quality control strategy relies on the segmentation-based discriminator S, which is a key factor in our decision to utilize adversarial training rather than diffusion models for tumor synthesis. We propose to adaptively discard low-quality synthetic tumors by calculating the proportions of satisfactory synthesized tumor regions. The satisfactory synthesized tumors represent the synthetic tumors that do match the corresponding tumor masks M well. Intuitively, we can use the baseline segmentation model S to calculate the correspondence: the proportions of synthetic tumors that are segmented as tumors. Thus, we calculate the proportion P as follows:\n$$P = \\frac{\\sum_{i=1}^{N}[1(S(x_i)) \\times 1(M = 1))]}{\\sum_{i=1}^{N}[1(M = 1))]}$$,\nwhere N denotes the total number of voxels, $1(S(x_i))$ denotes the number of voxels that are segmented as tumors, $1(M = 1))$ denotes the number of voxels that tumor mask is 1 (the positions of synthetic tumors). It is intuitive that if the proportion P is higher, the quality of this case of synthetic tumor tends to be higher. In this way, the discriminator can serve as an automatic tool for quality control.\nWe set a threshold T to split the high- and low-quality synthetic tumors. We use the term \"Quality Test\" to represent whether the synthetic case passes the discriminator, the quality control strategy Q is defined as:\n$$Q(x, P, T, G, S) = \\begin{cases}\n\u00ee, & P \\geq T, \\text{ This synthetic tumor pass the Quality Test}\nx, & P < T, \\text{ This synthetic tumor fail the Quality Test}\n\\end{cases}$$\nWith Q, we can effectively achieve quality control of the synthetic tumors online. Ablation studies are presented in Extended Data Table A15. Despite its simplicity, we effectively alleviate the negative impact of unsatisfactory synthetic tumors in segmentation training, which is a significant improvement upon the previous tumor synthesis methods [40, 41, 43, 44, 48]."}, {"title": "4.4 Unleashing the Power of Large-scale Unlabeled Data", "content": "Distinguished from previous works [8, 40, 48, 50-53, 55, 56, 72] that used a limited scale of dataset for tumor segmentation training, we emphasize the importance of large-scale unlabeled data in the development of tumor segmentation. With the rapid development of medical imaging, we can easily collect adequate unlabeled CT data for training our FreeTumor. The challenge is that these datasets lack annotated tumor"}, {"title": "4.5 Datasets and Implementation Details", "content": "Datasets collection and pre-processing. Our proposed FreeTumor excels in leveraging large-scale data for tumor synthesis and segmentation. Thus, in this study, we first create a large-scale dataset with 161,130 publicly available CT volumes from 33 different sources, as shown in Extended Data Table A17.\nAs described in Section 4.2, our initial step involves simulating tumor positions within their corresponding organ regions, e.g., liver tumors on livers, pancreas tumors on pancreases. Consequently, generating the organ labels becomes essential. While a few of the datasets already include organ labels, the others still lack organ labels. To address this, we first utilize a robust organ segmentation model VoCo [39, 56] to generate liver, pancreas, and kidney labels for the abdomen CT datasets. For lung organs, we employ Lungmask [90] to generate lung labels for chest CT datasets. This approach enables us to leverage the entirety of 161,130 CT volumes for tumor synthesis and segmentation training. Note that we only utilize the generated organ labels to simulate approximate tumor positions. Therefore, these organ labels do not need to be perfectly precise for the scope of this study.\nAmong our curated datasets, some of them contain abdomen regions, some of them contain chest regions, and a few of them contain both abdomen and chest regions. Specifically, for the training of liver, pancreas, and kidney tumors, we utilize 19,571 abdomen CT volumes for training. For lung tumors and COVID-19, we utilize 141,784 chest CT volumes for training.\nImplementation details. In this study, instead of developing new network architectures, we mainly focus on advancing tumor segmentation from a data-driven aspect. Thus, we simply adopt the SwinUNETR [50] as the tumor segmentation model. We use SwinUNETR [50] for two reasons: (1) It achieves competitive results among the baseline tumor segmentation methods [8, 50-53]. (2) Previous tumor synthesis methods [40, 48] and CT foundation models [55, 56] also adopt SwinUNETR [50] as backbones.\nWe use Pytorch [91], MONAI [92], and nnUNet [8] framework to conduct all the experiments. The synthesis training of FreeTumor is conducted on 8 \u00d7 NVIDIA H800 (80G) GPUs. The tumor segmentation training is conducted on 1 \u00d7 NVIDIA"}, {"title": "Evaluation metrics", "content": "For the Visual Turing Test in clinician evaluation, we report the sensitivity, specificity, and accuracy results to measure the radiologists' ability of identifying synthetic tumors. Sensitivity (%) and specificity (%) are calculated as:\n$$sensitivity = \\frac{TP}{TP + FN}, specificity = \\frac{TN}{TN + FP}$$,\nwhere TP (True Positive) denotes truly identifying the synthetic tumors, TN (True Negative) denotes truly identifying the real tumors, FP (False Positive) denotes falsely recognizing real tumors as synthetic tumors, and FN (False Negative) denotes falsely recognizing synthetic tumors as real tumors. The accuracy (%) is calculated as:\n$$accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\nFor tumor segmentation, the standard Dice scores (%) is employed to evaluate the performances. Dice scores is calculated as:\n$$Dice(Pre, Gro) = \\frac{2 |Pre \\cap Gro|}{|Pre| + |Gro|}$$,\nwhere Pre denotes the segmentation predictions, Gro is the ground truth of tumor labels.\nFor tumor detection, detected tumors are identified when segmentation predictions overlap with the ground truth labels [13, 58-60]. We use F1-Score, sensitivity, and specificity to measure the performances of tumor detection, where F1-Score is formulate as:\n$$F1-Score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$$,\nwhere Precision and Recall are formulated as:\n$$Precision = \\frac{TP}{TP + FP}, Recall = \\frac{TP}{TP + FN}$$\nHere, the \"positive\" class is defined as detecting a tumor within a CT volume."}, {"title": "5 Data availability", "content": "This study incorporates a total of 33 public datasets from different sources, encompassing 161,130 publicly available CT volumes. All these datasets are publicly available for research. For detailed information about the data used in this project, please refer to Extended Data Table A17."}, {"title": "6 Code availability", "content": "The codes, datasets, and models of FreeTumor are available at GitHub (https://github.com/Luffy03/FreeTumor)."}, {"title": "7 Author contributions", "content": "L.W. designed the framework and conducted the experiments. Y.Z., L.L., X.W., and P.R. provided suggestions on the framework and experiments. J.Z., S.H., J.M., and X.N. contributed to the data acquisition and downstream task evaluation. X.Z, M.W, Y.W, X.D, and V.V. contributed to the clinician evaluation of tumor synthesis and analyzed the results of tumor recognition. All authors contributed to the drafting and revising of the manuscript. H.C. conceived and supervised the work."}, {"title": "Declaration", "content": "The authors have no conflicts of interest to declare."}, {"title": "Ethics declaration", "content": "This project has been reviewed and approved by the Human and Artefacts Research Ethics Committee (HAREC). The protocol number is HREP-2024-0429."}]}