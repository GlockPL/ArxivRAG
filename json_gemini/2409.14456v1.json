{"title": "Scoring rule nets: beyond mean target prediction in multivariate regression", "authors": ["Daan Roordink", "Sibylle Hess"], "abstract": "Probabilistic regression models trained with maximum likelihood estimation (MLE), can sometimes overestimate variance to an unacceptable degree. This is mostly problematic in the multivariate domain. While univariate models often optimize the popular Continuous Ranked Probability Score (CRPS), in the multivariate domain, no such alternative to MLE has yet been widely accepted. The Energy Score - the most investigated alternative - notoriously lacks closed-form expressions and sensitivity to the correlation between target variables. In this paper, we propose Conditional CRPS: a multivariate strictly proper scoring rule that extends CRPS. We show that closed-form expressions exist for popular distributions and illustrate their sensitivity to correlation. We then show in a variety of experiments on both synthetic and real data, that Conditional CRPS often outperforms MLE, and produces results comparable to state-of-the-art non-parametric models, such as Distributional Random Forest (DRF).", "sections": [{"title": "1 Introduction", "content": "The vanilla regression models predict a single target value y for an observation x \u2208 R. In theory, the goal is to approximate the true regression model f*, generating the observed target values as samples of the random variable Y = f*(x)+e. The random variable e reflects here the noise in the data and is assumed to have an expected value of zero. Hence, the goal is to find a regression model that predicts the mean f(x) = Ey [Y | x] = f*(x).\nHowever, in practice, the trained regression models come with uncertainties. Reflecting those uncertainties is relevant, for example when a lower or upper bound for the prediction is of interest, when underforecasting has more detrimental consequences than overforecasting, or when the expected profit and risk are dependent on prediction uncertainty. Examples of such applications are found in weather forecasting [36], healthcare [19], predictions of the electricity price [28], stock price [37], survival rate [3] and air quality [25]."}, {"title": "2 Distributional Regression", "content": "Distributional regression models provide predictive uncertainty quantification by modeling the target variable as a probability distribution. That is, models are tasked with predicting the distribution of a (possibly multivariate) random variable Y, conditioned on an observation x of a (possibly multivariate) covariate random variable X:\n$$f(x) = P(Y | X = x).$$ \nHere, P(\u00b7) denotes the probability distribution of a random variable. Such a model is trained on a dataset of observations of (X, Y): {(xi, yi)}=1\nDistributional regression models are typically trained by Maximum Likelihood Estimation (MLE) [18], which is equivalent to minimizing the Logarithmic Score. However, when the assumed and the true shape of the distribution do not match, MLE can become sensitive to outliers [5], causing a disproportionally increase in the forecasted variance [11]. While this is not necessarily a problem for homoskedastic models (where typically only a single estimator is predicted and the error distribution is assumed to be constant), it is problematic for heteroskedastic models, predicting the full distribution [3]. Therefore, for a univariate continuous target domain, many distributional regression approaches use the Continuous Ranked Probability Score (CRPS) [24]. CRPS provides an optimization objective that is generally more robust than MLE [30] and hence gains in popularity in comparison to MLE [3,25,30].\nHowever, unlike MLE, CRPS has no extension to the multivariate domain (y \u2208 Rd) that maintains the robustness properties. The most popular extension is the Energy Score [14], but it is known to be insensitive to correlation, and often cannot be analytically evaluated [29]. Moreover, other alternatives such as the Variogram Score [32] also have weaknesses, such as translational invariance.\nThe lack of a robust alternative to MLE is widely discussed in comparative studies. In their review of probabilistic forecasting, Gneiting and Katzfuss argue that \"a pressing need is to go beyond the univariate, real-valued case, which we review, to the multivariate case\" [13]. More recently, Alexander et al. conclude that \"it is rarely seen that one metric for evaluating the accuracy of a forecast consistently outperforms another metric, on every single scenario\" [2]. As a result, multivariate distributional regression approaches either resort to MLE [26] or avoid direct usage of distributional optimization criteria, via approaches based on e.g. Generative Adversarial Networks (GANs) [1] or Random Forests (RF) [38]."}, {"title": "Contributions", "content": "1. We propose a novel scoring rule for multivariate distributions, called Conditional CRPS (CCRPS). The novel scoring rule CCRPS is a multivariate extension of the popular univariate CRPS that is more sensitive to correlation than the Energy Score and (for some distributions) less sensitive to outliers than the Logarithmic Score. We enable the numerical optimization of the proposed scoring rule by proving equivalent, closed-form expressions for a variety of multivariate distributions, whose gradients are easy to compute."}, {"title": "2.1 Proper Scoring Rules", "content": "Scoring rules are a class of metrics R that compare a predicted distribution P with actual observations y. A scoring rule is called proper for a class of probability distributions D if for any P, Q \u2208 D we have:\n$$E_{y~P}[R(P,Y)] \\le E_{y\\sim p}[R(Q, Y)].$$\nThat is, in expectation over all observations, a scoring rule attains its minimum if the distribution of the observations Y ~ P matches the predicted distribution. A scoring rule is called strictly proper if the minimum of the expected scoring rule is uniquely attained at P. Proper and strictly proper scoring rules pose valuable loss functions for distributional regression models: minimizing the mean scoring rule automatically calibrates the model's predicted distributions, and fits the conditional distributions to the observed data (Equation (1)), arguably maximizing sharpness [13,30].\nFor univariate domains, the most popular scoring rules are the Logarithmic Score and the Continuous Ranked Probability Score (CRPS). The Logarithmic Score maximizes the MLE criterion, and is defined as\n$$LogS(P, y) = - log f_P(y)$$\nwhere fp is P's probability density function. It is strictly proper for distributions with finite density. CRPS is defined as\n$$CRPS(P, y) = \\int_{-\\infty}^{\\infty} [F_P(z) - 1(y \\le z)]^2 dz,$$"}, {"title": "2.2 Conditional CRPS", "content": "We propose a family of (strictly) proper scoring rules, called Conditional CRPS (CCRPS). To introduce this scoring rule, we consider a simple example of a bivariate Gaussian distribution\n$$(Y_1, Y_2) \\sim \\mathcal{N}(\\mu, \\Sigma), \\text{ where } \\Sigma = \\begin{pmatrix} \\sigma_1^2 & \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2 & \\sigma_2^2 \\end{pmatrix}, \\sigma_1, \\sigma_2 > 0, \\text{ and } \\rho \\in (-1, 1).$$  \nRather than evaluating P(Y1, Y2) directly against an observation, we instead evaluate the first marginal distribution P(Y\u2081) = N(\u03bc\u2081,\u03c3\u1ec9), and second conditional distribution P(Y2 | Y\u2081 = y) = N(\u03bc2+\u03c1(y-\u03bc1), (1-\u03c12)\u03c33), against their respective univariate observations, via use univariate scoring rules. Summation over these terms then defines a new multivariate scoring rule R:\n$$R(P,y) = CRPS(P(Y_1), y_1) + CRPS(P(Y_2 | Y_1 = Y_1), y_2).$$  \nConditional CRPS generalizes the intuition that multivariate scoring rules can be constructed by evaluating univariate conditional and marginal distributions.\nDefinition 1 (Conditional CRPS). Let P(Y) be a d-variate probability distribution over a random variable Y = (Y1,..., Ya), and let y \u2208 Rd. Let T = {(Vi, Ci)}i=1q be a set of tuples, where vi \u2208 {1, ...,d} and Ci \u2286 {1, ...,d} \\ {Vi}.\nConditional CRPS (CCRPS) is then defined as:\n$$CCRPS_\\mathcal{T}(P(Y), y) = \\sum_{i=1}^{q} CRPS(P(Y_{v_i} | Y_j = y_j \\text{ for } j \\in C_i), y_{v_i}),$$ \nwhere P(Yvi | Yj = yj for j \u2208 Ci) denotes the conditional distribution of Yvi given observations Yj = yj for all j \u2208 Ci.\nIn the case that P(Yvi | Yj = yj for j \u2208 Ci) is ill-defined for observation y (i.e. the conditioned event Yj = yj for j \u2208 Ci has zero likelihood or probability), we define CRPS(P(Yvi | Yj = yj for j \u2208 Ci), Yvi) = \u221e."}, {"title": "2.3 CCRPS as ANN Loss Function for Multivariate Gaussian Mixtures", "content": "We show an application of Conditional CRPS as a loss function that allows for the numerical optimization of Artificial Neural Networks (ANNs) [17] to return the parameters of the predicted distribution of target variables in a regression task. We assume that the target distribution is a mixture of m d-variate Gaussian distributions. This distribution is defined by m mean vectors \u03bc\u2081,..., \u03bcm \u2208 Rd, m positive-definite matrices \u22111,..., \u03a3m \u2208 Rd\u00d7d, and m weights W1,...,Wm \u2208 [0,1] such that \u2211i=1m Wi = 1. A multivariate mixture Gaussian P defined by these parameters is then given by the density function\n$$f_P(y) = \\sum_{i=1}^{m} w_i \\cdot \\mathcal{N}_{\\mu_i, \\Sigma_i}(y) = \\sum_{i=1}^{m} w_i \\frac{exp(-\\frac{1}{2}(y - \\mu_i)^T {\\Sigma_i}^{-1} (y - \\mu_i))}{\\sqrt{(2\\pi)^d \\cdot |\\Sigma_i|}}$$\nThat is, the ANN returns for each input x a set of parameters {(\u03bc\u03b9, \u03c9\u03b9, Li)}=1m, where Li \u2208 Rdxd is a Cholesky lower matrix [26], defining a positive-definite matrix \u2211\u2081 = L\u2081\u00b7 L. Given a dataset (xi, yi)i=1n, and an ANN 0(x) that predicts the parameters of a d-variate mixture Gaussian distribution, we can define a loss function over the mean CCRPS score:\n$$\\mathcal{L}(\\theta, (x_i, y_i)_{i=1}^{n}) = - \\frac{1}{n} \\sum_{i=1}^{n} CCRPS_{\\mathcal{T}}(P_{\\theta(x)}, y_i).$$\nUnfortunately, if we choose T such that the loss function computes mixture Gaussian distributions conditioned on c variables, then we require matrix inversions of cxc matrices (cf. Appendix B). Therefore, we choose a simple Conditional CRPS variant that conditions on at most one variable, using To = {(i, \u00d8)}i=1d \u222a {(i, {j})}i=1d,j\u2260i. That is,\n$$CCRPS_{\\mathcal{T}}(P, y) = \\sum_{i=1}^{d} CRPS(P(Y_i), y_i) + \\sum_{j\\neq i} CRPS(P(Y_i | Y_j = y_j), y_i).$$\nUsing this definition, we find an expression for this variant of CCRPS. As both P(YiYj = yj) and P(Yi) are univariate mixture Gaussian distributions, computing CCRPST (P, y) is done by simply computing the parameters of these distributions, and applying them in a CRPS expression for univariate mixture Gaussian distributions given by Grimit et al. [16]:"}, {"title": "2.4 Energy Score Ensemble Models", "content": "Secondly, we propose an ANN loss variant that empirically approximates the Energy Score. The energy score (cf. Equation (74)) is defined over expected values, for which no closed-form expression exists, that would enable the computation"}, {"title": "3 Experiments", "content": "We compare the probabilistic predicted performance of the newly proposed methods to state-of-the-art probabilistic regression methods. We provide our source code online. As competitors, we choose the best-performing models of the comparative study from Cevid et al. [38], and the Logarithmic Score trained networks.\nDistributional Random Forest (DRF) [38] is a random forest regression model with an adapted splitting criterion for target vectors (based on MMD approximations), and an adapted aggregation that returns a weighted ensemble of target vectors.\nConditional GAN (CGAN) [1] is an extension of the popular Generative Adverserial Network. Except, the model is \"conditioned\" on input x by adding it as input to both generator and discriminator.\nDistributional k-nearest neighbors (kNN) [38] predicts a distribution in which each of the k-nearest neighbors is assigned a probability.\nMixture MLE neural networks (a.o. [34]) are the closest to our approach. MLE ANNs use the Logarithmic Score as loss function. We employ the same architectures as MLE networks in our CCRPS networks."}, {"title": "3.1 Evaluation metrics", "content": "Unfortunately, there is no clear consensus on appropriate evaluation metrics for multivariate distributional regression models [2]. Hence, we choose a variety of popular metrics: the Energy Score (cf. Equation (74)) with \u03b2 = 1, and the Variogram Score [32] with \u03b2\u2208 {0.5,1,2}:\n$$VS_\\beta(P,y) = \\sum_{i<j<d} (\\lvert y_i - y_j \\rvert^\\beta - E_{Y \\sim P}[|Y_i - Y_j|^{\\beta}])^2.$$"}, {"title": "3.2 Synthetic Experiments", "content": "We base our data generation process for the synthetic experiments on the task to post-process an ensemble model. This model is for example applied in the task of weather forecasts (cf. experiments on the global radiation data in Section 3.3).\nHere, a distributional regression model receives s (non probabilistic) predictions"}, {"title": "3.3 Real World Experiments", "content": "We evaluate our method on a series of real-world datasets for multivariate regression. All datasets are normalized for each input and target field based on the training dataset mean and standard deviation.\n1. Births dataset [38]: prediction of pregnancy duration (in weeks) and a newborn baby's birthweight (in grams) based on statistics of both parents.\n2. Air quality dataset [38]: Predictio of the concentration of six pollutants (NO2, SO2, CO, O3, PM2.5 and PM10) based on statistics about the measurement conditions (e.g., place and time)\n3. Global radiation dataset: Prediction of solar radiation based on three numerical weather prediction (NWP) models (the single-model run models GEM [6] and GFS [27] and the 20-ensemble model run GEPS [7]), as well as global radiation (GR) measurements at weather stations in the Netherlands [22] and Germany [10]. Models receive an NWP forecast as input, and a station measurement as target. In our experiments, models predict an 8-variate distribution, consisting of three-hour GR averages. We run four different experiments, in which models receive either GEM, GFS, GEPS or all three NWP sources as input.\n4. Global-diffuse radiation dataset: Prediction of 24 hourly global and diffuse radiation (DR) station measurements based on all three NWP sources (like in the global radiation dataset)."}, {"title": "4 Conclusion", "content": "We propose two new loss functions for multivariate probabilistic regression models: Conditional CRPS and the approximated Energy Score. CCRPS is a novel class of (strictly) proper scoring rules, which combines some of the desirable characteristics (suitability for numerical optimization, sensitivity to correlation, and increased sharpness) from the Energy and Logarithmic Scores.\nConditional CRPS, when applied in the right setting, leads to an increase in sharpness while retaining calibration. We parameterize our regression models by means of an Artificial Neural Network (ANN), which returns for a given"}, {"title": "A Proofs", "content": "Recall Conditional CRPS, as defined in Section 2.2. Let P(Y) be a d-variate probability distribution over a random variable Y = (Y1, ..., Ya), and let y \u2208 Rd. Let T = {(vi, Ci)}i=1q be a set of tuples, where vi \u2208 {1,...,d} and Ci \u2286 {1, ..., d} \\ {v}. Conditional CRPS (CCRPS) is then defined as:\n$$CCRPST(P(Y), y) = \\sum_{i=1}^{q} CRPS(P(Y_{v_i} | Y_j = y_j \\text{ for } j \\in C_i), y_{v_i}),$$  \nwhere P(Yvi | Yj = yj for j \u2208 Ci) denotes the conditional distribution of Yvi given observations Yj = yj for all j\u2208 Ci. In the case that P(Yvi | Yj = yj for j \u2208 Ci) is ill-defined for observation y (i.e. the conditioned event Yj = yj for j \u2208 Ci has zero likelihood or probability), we define CRPS(P(Yvi | Yj = yj for j E Ci), Yvi) = \u221e.\nIn this appendix, we will provide formal proofs for the various (strict) propriety claims made in the main paper. First we provide some helper proofs. We first show that if the true distribution has finite first moment, Conditional CRPS is finite for a correct prediction. This is used during the (strict) propriety proofs."}, {"title": "A.1 Non-strict propriety of CCRPS", "content": "We use Lemma 1 to prove non-strict propriety.\nLemma 2 (Propriety of Conditional CRPS). Let T = {(Vi, Ci)}i=1q be a set of tuples, where vi \u2208 {1,...,d} and Ci \u2286 {1, ...,d} \\ {vi}. Then CCRPST is proper for distributions with finite first moment. For all d-variate random variables A, B we have:\n$$E_{y~P(A)}[CCRPS_{\\mathcal{T}}(P(A), y)] \\le E_{y\\sim P(A)}[CCRPS_{\\mathcal{T}}(P(B), y)]$$ \nProof. Consider two random variables A = (A1,..., Ad) and B = (B1, ..., Bd) with finite first moment. Consider an arbitrary T as defined in the lemma statement. First, we expand the expected value for Conditional CRPS:\n$$E_{y~P(A)} [CCRPS_{\\mathcal{T}}(P(B), y)]$$ \n$$= \\sum_{i=1}^{q} E_{y~P(A)} CRPS(P(A_{v_i} |\\forall j\\in C_i : A_j = y_j), y_{v_i})]$$ \n$$= \\sum_{i=1}^{q} E_{y~P(A)} [CRPS(P(A_{v_i} | \\forall j\\in C_i : A_j = y_j), y_{v_i})]$$ \n$$= \\sum_{i=1}^{q} E_{y~P(A_{C_i})} \\left[ E_{z~P(A_{v_i} |\\forall j\\in C_i : A_j = y)} [CRPS(P(A_{v_i} | \\forall j\\in C_i : A_j = y_j), z)] \\right] .$$  \nHere P(Ac\u2081) denotes P(A)'s marginal distribution for the variables (Aj)jeci. Similarly, we rewrite for P(B):\n$$E_{y~P(A)} [CCRPS_{\\mathcal{T}}(P(B), y)]$$ \n$$= \\sum_{i=1}^{q} E_{y~P(A_{C_i})} \\left[ E_{z~P(A_{v_i} |\\forall j\\in C_i : A_j = y)} [CRPS(P(B_{v_i} | \\forall j\\in C_i : B_j = y_j), z)] \\right] .$$  \nFinally, noting finiteness of Ey~P(A) [CCRPST(P(A), y)] (Lemma 1), we apply univariate strict propriety of CRPS on each conditional term, and we find non-strict propriety:\n$$E_{y~P(A)} [CCRPS_{\\mathcal{T}}(P(A), y)] \\le E_{y\\sim P(A)} [CCRPS_{\\mathcal{T}}(P(B), y)]$$"}, {"title": "A.2 Discrete strict propriety of CCRPS", "content": "We first introduce a helper lemma, which is used later in the strict propriety proofs."}, {"title": "A.3 Absolutely continuous strict propriety of CCRPS", "content": "The strict propriety proof for absolutely continous distributions is similarly structured as the proof of Lemma 4 and in many ways a continuous equivalent.\nLemma 5 (Strict propriety of Conditional CRPS for absolutely continuous distributions). Let T = {(vi, Ci)}i=1q be a set of tuples, where vi \u2208 {1, ...,d} and Ci \u2286 {1, ..., d} \\ {vi}. Let $1,...,\u0444\u0430 be a permutation of {1, ...,d} such that:\n$$\\forall_{i=1}^{d}: (\\phi_j, {\\phi_1,..., \\phi_{j-1}}) \\in \\mathcal{T},$$\nthen CCRPST is strictly proper for absolutely continuous distributions with finite first moment, i.e. for all d-variate absolutely continuous distributions P(A) \u2260 P(B) with finite first moment we have:\n$$E_{y~P(A)}[CCRPST(P(A), y)] < E_{y\\sim P(A)}[CCRPST(P(B), y)]$$"}, {"title": "A.4 Counter example for partially continuous distributions", "content": "Finally, we give a counter example, to show that Conditional CRPS is never strictly proper for all distributions with finite first moment, regardless of our choice of conditional specification T.\nLemma 6. There exist two d-variate distributions P(A) \u2260 P(B) with finite first moment, such that\n$$E_{y~P(A)}[CCRPST(P(A), y)] = E_{y\\sim P(A)}[CCRPST(P(B), y)],$$\nregardless of our choice for T.\nProof. Consider P(A) = Na(0, I), a d-variate standard normal distribution over random variables A\u2081, ... A\u0105. Next, consider P(B) (over random variables B1,... Bd), which is defined as follows: with 0.5 probability, P(B) samples a vector (z, ..., z) \u2208 Rd with z \u2208 N\u2081(0, 1). Otherwise, P(B) samples a vector i.i.d. to P(A).\nConsider an arbitrary conditional specification v1, ..., vq \u2208 {1, ..., d} and C1, ..., Cq with C\u2081 \u2286 {1, ..., d} \\ {v}. By Lemma 3, it suffices to show that for any 1 \u2264 i \u2264q we have:\n$$Py~P(A)(P(Avi | \\forall j\\in C_i : A_j = y_j) \\neq P(Bvi | \\forall j\\in C_i : B_j = y_j)) = 0$$\nLet us first start with the case that C\u2081 = \u00d8, i.e. marginal distributions. We find:\n$$f_{P(B)}(y) = 0.5 \\cdot f_{\\mathcal{N}_1(0, 1)}(y) + 0.5 \\cdot f_{\\mathcal{N}_1(0, 1)}(y) = f_{\\mathcal{N}_1(0, 1)}(y) = P_{(A_{v_i})}(y),$$\nthus P(Av\u2084) = P(Bv\u2081), and Equation (56) holds trivially.\nSecondly, let us now consider the case Ci \u2260 0. Then we find:\n$$P(B_{v_i} | \\forall j \\in C_i: B_j = y_j) = \\begin{cases} \\mathcal{P}(z), & \\text{if } \\exists z \\in \\mathbb{R}: \\forall j \\in C_i: y_j = z \\\\ \\mathcal{N}_1(0, 1), & \\text{otherwise} \\end{cases}$$"}, {"title": "B Expressions for Conditional CRPS", "content": "As full expressions of Conditional CRPS can get quite lenghty, in this Appendix, we will give an overview on univariate conditional and marginal distributions of popular multivariate distributions, and their CRPS expressions. Since Conditional CRPS consists of summations over such CRPS terms (based on T), this allows for easy derivation of conditional CRPS expressions."}, {"title": "B.1 Multivariate Gaussian distribution", "content": "Consider a d-variate Gaussian distribution Na(\u03bc, \u03a3) over random variables A1,..., Ad, with \u03bc\u2208 Rd and \u03a3\u2208 Rdxd being positive definite. To be able to compute P(A\u00bfA = a) for a group of variables \u00c2, we will denote the mean vector and covariance matrix via block notation:\n$$\\mu = \\begin{bmatrix} \\mu_{\\hat{A}} \\\\ \\mu_{\\sim} \\end{bmatrix}$$ \n$$\\Sigma = \\begin{bmatrix} \\Sigma_{\\hat{A}} & \\rho\\\\ \\rho^T & \\Sigma_{\\sim} \\end{bmatrix}$$  \nHere, we have left out irrelevant variables. We then find that any univariate conditional or marginal is univariate Gaussian distributed\n$$P(A_i | \\hat{A} = \\hat{a}) = \\mathcal{N}_1 (\\mu_{\\Sigma} + \\rho \\Sigma_{\\hat{A}}^{-1} (a - \\mu_{\\hat{A}}), \\sigma^2 - \\rho \\Sigma_{\\hat{A}}^{-1} \\rho^T).$$ \nIn the special case that A = \u00d8 we find P(Ai) = N\u2081(\u03bc\u03b9, \u03c3?). A closed-form CRPS expression for univariate Gaussian distributions has been provided by Gneting et al. [15]:\n$$CRPS(\\mathcal{N}_1(\\mu, \\sigma^2), y) = \\sigma \\cdot \\left( \\frac{y-\\mu}{\\sigma} \\cdot \\left( 2\\Phi (\\frac{y-\\mu}{\\sigma})-1 \\right) + 2\\phi (\\frac{y-\\mu}{\\sigma}) - \\frac{1}{\\sqrt{\\pi}} \\right)$$\nHere, and I are the PDF and CDF of a standard normal distribution."}, {"title": "B.2 Multivariate mixture Gaussian distribution", "content": "Let A be a d-variate mixture Gaussian distribution over random variables A1, ... Ad. A consists of m multivariate Gaussian distributions D(1), . . ., D(m). We write the"}, {"title": "C Approximations of the Energy Score and Variogram Score", "content": "In this appendix, we will specify the approximations and analytic formulas used to compute the Variogram Scores and Energy Scores presented in our experiments. First, recall the Energy Score [14]:\n$$ES(A, y) = E_{x~A} [||x - y||] - \\frac{1}{2} E_{x,x'~A}[||x - x'||].$$ \nFor (weighted) ensemble distributions, i.e. distributions defined as a set of vectors X1,...,xm with probabilities w\u2081,..., Wm such that \u2211i=1m Wi = 1, we find rather simple expression for the Energy Score:\n$$ES(A, y) = \\sum_{i=1}^{m} w_i ||x_i - y|| - \\frac{1}{2} \\sum_{i,j=1}^{d} w_i w_j ||x_i - x_j||$$ \nFor multivariate mixture Gaussian distributions, we approximated the Energy score by Equation (75), sampling V1, ..., Vm ~ A, and V1 : Wi =\nNext, recall the Variogram Score [32]:\n$$VarS_\\beta (A, y) = \\sum_{i<j} (\\lvert y_i - y_j/P - E_{x~A} [|X_i - x_j|^{\\beta}])^2.$$"}, {"title": "D Algorithms", "content": "The data generating algorithm for Figure 2 in the main paper is described in Algorithm 2."}]}