{"title": "LAMP: Learnable Meta-Path Guided Adversarial Contrastive Learning for Heterogeneous Graphs", "authors": ["Siqing Li", "Jin-Duk Park", "Wei Huang", "Xin Cao", "Won-Yong Shin", "Zhiqiang Xu"], "abstract": "Heterogeneous graph neural networks (HGNNs) have significantly propelled the information retrieval (IR) field. Still, the effectiveness of HGNNs heavily relies on high-quality labels, which are often expensive to acquire. This challenge has shifted attention towards Heterogeneous Graph Contrastive Learning (HGCL), which usually requires pre-defined meta-paths. However, our findings reveal that meta-path combinations significantly affect performance in unsupervised settings, an aspect often overlooked in current literature. Existing HGCL methods have considerable variability in outcomes across different meta-path combinations, thereby challenging the optimization process to achieve consistent and high performance. In response, we introduce LAMP (LearnAble Meta-Path), a novel adversarial contrastive learning approach that integrates various meta-path sub-graphs into a unified and stable structure, leveraging the overlap among these sub-graphs. To address the denseness of this integrated sub-graph, we propose an adversarial training strategy for edge pruning, maintaining sparsity to enhance model performance and robustness. LAMP aims to maximize the difference between meta-path and network schema views for guiding contrastive learning to capture the most meaningful information. Our extensive experimental study conducted on four diverse datasets from the Heterogeneous Graph Benchmark (HGB) demonstrates that LAMP significantly outperforms existing state-of-the-art unsupervised models in terms of accuracy and robustness.", "sections": [{"title": "1 Introduction", "content": "Heterogeneous graphs characterized by diverse node and edge types are ubiquitous across various domains including social, academic, and user interaction networks. The use of heterogeneous graph neural networks (HGNNs) has surged in IR applications, ranging from search engines [2, 12, 50] to recommendation systems [1, 34, 39, 22, 28] and question answering systems [7, 9, 4].\n\nHGNNs fall into two categories: Meta-path based models [46, 54, 8, 49], converting HINs into homogeneous sub-graphs via predefined meta-paths, and Meta-path free models [53, 58, 17, 16, 31, 51], facilitating distinct information propagation along varied relations. These models have shown promising results but often require extensive labeling, posing challenges for large-scale IR tasks. Consequently, there has been a shift towards self-supervised learning (SSL) approaches, particularly in Heterogeneous Graph Contrastive Learning (HGCL) [35, 25, 47, 24, 3, 57, 60].\n\nIn HGCL, a widely adopted approach involves the generation of multiple graph views via diverse data augmentation techniques, subsequently refining node representations through contrastive learning. Two principal categories of HGCL augmentations emerge: (1) the meta-path view [35, 24, 25], which converts heterogeneous graphs into homogeneous sub-graphs according to selected meta-paths, and (2) the network schema view [47, 36], wherein the target nodes aggregate information from one-hop neighbors of varying node types. Distinctively, the network schema view imparts a localized perspective, while the meta-path view delivers a more expansive, higher-order perspective, connecting target nodes through meta-path instances that span multiple hops. However, recent studies have revealed that manually crafted augmentations, including the prevalent meta-path view, often fall short of achieving optimal results [61, 59, 20]. This demonstrates a significant reliance on the specific combination of meta-paths chosen, which in turn, greatly affects the overall model performance.\n\nIn this study, we explore the relationship between the meta-path set selection and HGNN model performance on node classification, detailed in Section 3. Our findings illustrated in Figure 1 reveal that the set of meta-paths selected crucially affect model performance, with all models showing at least a 5% deviation across different combinations, especially pronounced in SSL models. Clearly, the identification of the optimal meta-path combination is crucial, yet presents considerable challenges due to:\n\n(1) No Universal Meta-Path Combination: Our research indicates the absence of a universally optimal meta-path combination among models, with effectiveness varying significantly (see Figure 3). The optimal set for supervised models often underperforms in unsupervised scenarios, highlighting SSL's inherent complexity.\n\n(2) No use in Adding More Meta-paths: Surprisingly, adding more meta-paths doesn't consistently lead to better performance. Although effective in supervised learning contexts as evidenced by SOTA methods [49, 6], this approach does not translate as effectively into SSL scenarios. Consequently, a straightforward greedy search for the optimal meta-path combination is inadequate in the SSL landscape.\n\n(3) No Downstream Task Labels in SSL: SSL methods face a unique challenge in that they cannot employ downstream tasks to determine the most effective meta-path combinations, as these tasks are not applicable in unsupervised contexts.\n\nAddressing the issue in an unsupervised framework, our solution is to increase the robustness of HGCL models against diverse meta-path combinations. The existing models lack robustness primarily because each meta-path is treated as an independent channel, making changes in these channels potentially harmful to model stability. To address the overlooked issue of meta-path sensitivity, we present LAMP a LearnAble Meta-Path guided adversarial contrastive learning model which aims at creating a stable meta-path view. It reduces dependency on specific meta-path combinations and achieves consistent performance, also simplifying the integration of a wide range of meta-paths. Furthermore, we enhance LAMP with adversarial training, a technique known to improve contrastive learning performance in homogeneous graphs.\n\nLAMP proposes a new perspective in meta-path view construction by merging different meta-path sub-graphs into a unified structure. This results in a singular sub-graph that integrates nodes and edges from various meta-path sub-graphs. In this integrated sub-graph, each edge carries a one-hot-like encoding based on its meta-path instance, maintaining the semantic integrity of the original sub-graphs. This unified form ensures stability across various meta-path combinations, utilizing the overlaps between them. For instance, combining sub-graphs from PAP,PSP,PAPAP (refer to Figure 2 (b)) into one integrated sub-graph (Figure 2 (c)) retains the topological structure when modifying the combination, such as removing a meta-path, but with different edge encoding. This stability stems from the shared edges commonly found in heterogeneous graphs, as detailed in Section 3, thereby significantly reducing variability between combinations and enhancing the model's robustness.\n\nNevertheless, As the number of meta-paths in the integrated sub-graph increases, so does its density, which may hinder performance since Graph Contrastive Learning (GCL) generally performs better with sparser structures[61]. In extreme cases, the integrated sub-graph might become too dense,"}, {"title": "2 Preliminary", "content": "Definition 1. Heterogeneous Information Network (HIN). A HIN is a network $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, \\mathcal{A}, \\mathcal{R}, \\theta, \\phi)$, where $\\mathcal{V}$ and $\\mathcal{E}$ represent the sets of nodes and edges, respectively. The network is associated with a node type mapping function $\\theta: \\mathcal{V} \\rightarrow \\mathcal{A}$ and an edge type mapping function $\\phi: \\mathcal{E} \\rightarrow \\mathcal{R}$. Here, $\\mathcal{A}$ and $\\mathcal{R}$ represent the sets of object and link types, respectively, with the constraint $|\\mathcal{A}| + |\\mathcal{R}| > 2$.\nDefinition 2. Meta-path. A meta-path $\\mathcal{P}$ is a structural pattern connecting different node types, represented as\n\\mathcal{A}_1 \\xrightarrow{\\mathcal{R}_1} \\mathcal{A}_2 \\xrightarrow{\\mathcal{R}_2} \\mathcal{A}_3 \\cdots \\xrightarrow{\\mathcal{R}_l} \\mathcal{A}_{l+1}\n(abbreviated as $\\mathcal{A}_1 \\mathcal{A}_2 ... \\mathcal{A}_{l+1}$), which describes a composite relation $\\mathcal{R} = \\mathcal{R}_1 \\circ \\mathcal{R}_2 \\circ \\cdots \\circ \\mathcal{R}_l$ between node types $\\mathcal{A}_1$ and $\\mathcal{A}_{l+1}$, where $ \\circ $ represents the composition operator on relations. Paths in $\\mathcal{G}$ that follow the pattern of $\\mathcal{P}$ are termed as meta-path instances.\nDefinition 3. Meta-path Sub-Graph. Given a meta-path $\\mathcal{P}$, the nodes in $\\mathcal{G}$ can be re-connected to form a meta-path sub-graph $ \\mathcal{G}_P$. An edge $e_{\\rightarrow v}$ exists in $ \\mathcal{G}_P$ if and only if there's at least one path (a meta-path instance) between u and v following the meta-path $\\mathcal{P}$ in the original graph $\\mathcal{G}$. For instance, Figure 2 (b) illustrates three meta-path sub-graphs derived from the HIN in Figure 2 (a). PAP indicates two papers authored by the same individual, while PSP signifies two papers related"}, {"title": "3 Empirical Observations", "content": "To explore the influence of meta-path combinations on HGNN performance, we conducted a detailed empirical study using the ACM dataset. We generated 26 distinct combinations from 5 predefined meta-paths and assessed the performance variations in HGNNs, evidenced by the standard deviation and min-max gap. The key findings, depicted in Figures 1 and 3, are summarized below:\n\n(1) Sensitivity to Meta-path Combinations. Meta-path combinations critically affect HGNN performance. Variations in these combinations impact the structural configuration of meta-path sub-graphs, significantly influencing model outcomes, as evidenced by the substantial standard deviation and min-max gap shown in Figure 1. In extreme cases, improper combinations can lead to model failure. This challenge is more acute in SSL models due to the lack of downstream task feedback. Even proven meta-paths can cause dramatic performance deterioration if combined inappropriately. The sensitivity of HGNNs to these combinations is partly due to their responsiveness to topological changes and is further compounded by the low homophily ratios in meta-path sub-graphs [13] (referenced in Table 1), which exacerbates the issue in denser sub-graph structures.\n\n(2) Absence of Universal Optimal Combinations.: Our study reveals that no single meta-path combination is optimal for all models. This absence of a universal 'best' combination becomes a formidable challenge in SSL, where the lack of direct feedback from downstream tasks makes finding the ideal combination through exhaustive search impractical. The disparity between the effective combinations in supervised and unsupervised models further complicates this issue. This gap suggests that strategies successful in supervised learning may not directly translate to superior performance in unsupervised settings.\n\n(3) Naively adding more meta-path do not guarantee the best.: Contrary to expectations, simply adding more meta-paths does not linearly improve HGNN performance. While certain meta-paths are essential, their impact varies across different models. In some instances, such as the comparison between 'comb26' and the optimal \u2018comb21' for X-GOAL, adding an extra meta-path resulted in decreased performance. Our analysis, illustrated in Figure 4, shows significant edge overlaps among meta-path sub-graphs. For example, \u2018-PPSP' overlaps with over 50% of every other meta-path sub-graph. Such overlaps cause an accumulation of redundant information, overshadowing valuable insights from less common structures. What's worse, current semantic-level aggregation methods"}, {"title": "4 The Proposed Model: LAMP", "content": "In this section, we introduce LAMP, a Learnable Meta-Path Guided Adversarial Contrastive Learning method, detailed in Figure 5. LAMP leverages a dual-view approach: a high-order information-rich meta-path view, processed by LMA, and a locally-focused network schema view. The essence of"}, {"title": "4.1 Problem Formulation", "content": "Given a HIN $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, \\mathcal{A}, \\mathcal{R})$ denoted as G for short and a set of meta-path {$ \\mathcal{P} $} with $|{\\mathcal{P}}| = n$, we define {$ \\mathcal{G}_{P_1} ...  \\mathcal{G}_{P_n} $} as meta-path sub-graphs and $ \\hat{\\mathcal{G}}_P$ denoted as  $\\hat{\\mathcal{G}}$ as the integrated meta-path sub-graph. We represent the encoding function with parameter $ \\theta $ as $f_\\theta(\\cdot)$ and the augmentation function with parameter $ \\phi $ as $t_\\phi(\\cdot)$. For simplicity, we denote the network schema view by $f_\\theta(G)$ and the meta-path view as $f_\\theta(G)$. The primary objective for contrastive learning is:\n\narg \\max_\\theta I (f_\\theta (G), f_\\theta(t_\\phi(\\hat{\\mathcal{G}}))),\n\nThen for the adversarial training which tries to increase the difficulty of getting agreement in contrastive learning, the objective is:\n\narg \\min_\\phi I (f_\\theta (G), f_\\theta(t_\\phi(\\hat{\\mathcal{G}}))),\n\nwhere $I(X_1; X_2)$ represents the mutual information between random variables $X_1$ and $X_2$. The graph $t_\\phi(G) = (\\mathcal{V}, \\hat{\\mathcal{E}})$ retains the nodes from $ \\hat{\\mathcal{G}}$, but its edge set is a subset of $ \\hat{\\mathcal{E}}$. The insight is, we are trying to make the contrast as strong as possible while the two different view still could reach an agreement, which has been proved to be a effective optimization in contrastive learning. To bridge the min-max procedure and address potential biases, we incorporate a learnable meta-path importance parameter $ \\gamma \\in R^{1 \\times |\\mathcal{P}|}$, which shared by $f(\\cdot)$ and $t(\\cdot)$. Then we put all the objective together and the refined version is:\n\narg \\max_\\theta \\min_\\phi I (f_\\theta (G), f_{\\theta,\\gamma}(t_{\\phi,\\gamma}(G)).\n\nConsistent with prior research [40, 11], we employ InfoNCE [33] to approximate $I(X_1; X_2)$, detailed further in Section 4.6. Regarding $ \\gamma $, the insight is that $ \\gamma $ prioritizes longer meta-paths because the most straightforward strategy for $t_\\phi(\\cdot)$ to diminish the similarity between the two views is by\n\npreserving long meta-path instances in G. Conversely, during the maximization phase, shorter meta-paths become more influential. This balanced strategy empowers LAMP to harness rich high-order information while discerning the value of different meta-paths."}, {"title": "4.2 Integrated Sub-graph based meta-path view", "content": "Given a batch of meta-path sub-graphs {$ \\mathcal{G}_{P_i} $} = {$(\\mathcal{V}_{P_i}, \\mathcal{E}_{P_i})$} with $i = 1,\\cdots,|\\mathcal{P}|$, we amalgamate all of them into a singular sub-graph denoted as $G = (\\mathcal{V}, \\hat{\\mathcal{E}})$, to create the meta-path view. Here $ \\mathcal{V} = \\cup_i \\mathcal{V}_{P_i}$ and $ \\hat{\\mathcal{E}} = \\cup_i \\mathcal{E}_{P_i}$. As an illustration, Figure 2(c) depicts an integrated sub-graph derived from three meta-path sub-graphs: PAP, PSP, and PAPAP. The edge $e_{12} = (0, 1, 1)$ emerges since it's absent in PAP but present in both PSP and PAPAP. For every edge $(u, v) \\in \\hat{\\mathcal{E}}$, we assign a vector $\\mathcal{C}_{uv} = (x_1,x_2,\\cdots, x_{|\\mathcal{P}|})$ to present its semantic information, where $x_i$ is set to 1 if $(u, v) \\in {\\mathcal{E}_{P_i}}$, otherwise $x_i$ is set to 0. To further capture the semantic level information, we assign a learnable vector $ \\gamma \\in R^{1 \\times |\\mathcal{P}|}$ that quantifies the importance of each meta-path. In the message-passing phase, we utilize $ \\hat{e}_{uv} = \\gamma \\times e_{uv} $ as the edge embedding within the meta-path view. This approach ensures that overlaps between meta-path sub-graphs are mitigated, thereby curtailing redundant message passing and rendering the meta-path view more robust compared to prior methodologies. Nevertheless, this method can lead to a dense meta-path view, an aspect we address through the proposed LAMP, detailed in the subsequent section."}, {"title": "4.3 Learnable Meta-Path Guided Augmentation", "content": "The dense links of the integrated sub-graph, while capturing all given meta-path sub-graphs, can pose challenges for graph contrastive learning, as sparser graphs tend to yield more favorable results [61]. To address this issue, we introduce the Learnable Meta-Path Augmentation (LMA), a adversarial training based method aimed at learning a optimized edge prunning strategy. This ensures a sparser meta-view while overcoming manually-induced biases. LMA firstly applies a random edge dropping"}, {"title": "4.4 Network Schema view", "content": "In the network schema view, for a given node $i$, we initiate the process by employing a type-specific multilayer perceptron (MLP), denoted as $MLP_{A(i)}$, to transform the features $x_i$ of node $i$ into a"}, {"title": "4.5 Unified HGNN Encoder", "content": "In the context of LAMP, as outlined in eq 6, it is crucial to employ a unified HGNN that can efficiently handle both the network schema view (heterogeneous graph) and the meta-path view (homogeneous graph). While an approach could involve two distinct HGNN encoders tailored for each view, such an architecture may be inappropriate for LAMP. The core concern is that distinct encoders might produce node embedding governed by entirely different parameter sets, making it extremely hard for LAMP to meaningfully minimize similarity based on topological information. Essentially, a unified HGNN encoder fosters a harmonious link between the two views, ensuring that embedding reflects inherent structural divergence rather than encoder bias.\n\n$\\alpha_{ij} = \\frac{exp(LeakyReLU(a^T[Wh_i||Wh_j||W_re_{\\gamma}((i, j))]))}{\\sum_{k \\in N_i} exp(LeakyReLU(a^T[Wh_i||Wh_k||W_re_{\\gamma}((i, k))]))}$ ."}, {"title": "4.5.1 Node Residual:", "content": "Introducing pre-activation residual connections for nodes:\n\n$h_i^{(l)} = \\sigma(\\sum_{j\\in N_i} \\alpha_{ij}^{(l)} h_j^{(l-1)} + W_{res} h_i^{(l-1)})$."}, {"title": "4.5.2 Edge Residual:", "content": "Following the insights from Realformer [15], we add residuals to the attention scores:\n\n$ \\alpha_{ij}^{(l)} = (1 - \\beta) \\alpha_{ij}^{(l-1)} + \\beta \\alpha_{ij}^{(l-1)}$,\n\nwith $\\beta \\in [0,1]$ serving as a scaling factor. In our framework, the representation of relationships between end nodes varies based on the view. For the network schema view, the function $r_{\\gamma}((u, v))$ yields a one-hot vector encapsulating the relation between the nodes. Conversely, in the meta-path view, the relationship is captured by $r_{\\gamma}((u, v)) = \\hat{e}_{uv}$ leveraging the embedded semantic information. The transformation matrix $W_r$ is designed to align the dimension of edge embedding with that of node embedding. Uniquely within the HGNN encoder, $W_r$ is the sole parameter not shared across both the network schema and meta-path views."}, {"title": "4.6 Contrastive Optimization", "content": "The core of our approach involves utilizing the network schema view $G$ and meta-path view $\\hat{\\mathcal{G}}$ for the contrastive learning mechanism. Both graphs are fed into an HGNN followed by an MLP with a single hidden layer, mapping them into a space where the contrastive loss is computed:\n\n$Z_i^{G,proj} = \\sigma(W^{(2)}(\\sigma(W^{(1)}z_i^G + b^{(1)})) + b^{(2)}$,\n\n$Z_i^{\\hat{\\mathcal{G}},proj} = \\sigma(W^{(2)}(\\sigma(W^{(1)}z_i^{\\hat{\\mathcal{G}}} + b^{(1)})) + b^{(2)}$,\n\nwhere $\\sigma$ denotes the Leaky Relu function. The parameters are shared between the two views' embedding."}, {"title": "5 Experimental Evaluation", "content": "5.1 Experimental Setup\n5.1.1 Datasets:\n\nIn our study, we leveraged the HGB benchmark [31], which includes four diverse HIN datasets detailed in Table 2. The DBLP dataset [8] is sourced from the renowned DBLP bibliography website,"}, {"title": "5.2 Node Classification", "content": "In node classification task, we leveraged learned node embeddings to train a linear classifier in a transductive setting, utilizing all available edges during training. The distribution of node labels was consistent across datasets: 24% for training, 6% for validation, and 70% for testing. Classification performance was evaluated using Macro-F1 and Micro-F1 metrics, with results reported for the test set based on optimal validation performance (Table 3). Among all baseline methods, we report the best performance with their corresponding optimal meta-path combinations For LAMP, we report the performance with combination involving all the meta-path to demonstrate the robustness. Notably, LAMP consistently surpassed other unsupervised methods and showed remarkable efficacy against supervised models, particularly in sparser datasets like IMDB and Freebase. Crucially, LAMP operates without relying on an optimal meta-path combination, setting it apart from other methodologies. We also examined LAMP's sensitivity to meta-path combinations (Figure 1), demonstrating its superior stability and robustness, even in comparison to supervised approaches."}, {"title": "5.3 Sensitivity of Meta-Paths", "content": "To examine the sensitivity of various meta-path combinations, we conducted experiments on the ACM dataset. Our focus was to observe the variations and the min-max gap in Micro-F1 scores across all possible meta-path combinations. We considered the following candidate meta-paths: \"PAP\", \"PSP\", \"PTP\", \"PPSP\", and \"-PPSP\", which collectively form 26 distinct meta-path combinations, as illustrated in Figure 3. It is important to note that methods like Mp2vec and DGI were excluded from these experiments, as they are incompatible with all meta-path combinations due to their inherent design limitations and their inability to achieve state-of-the-art (SOTA) performance. The results of our experiments are presented in Table 4. In these tests, LAMP demonstrated a significant outperformance over existing unsupervised methods and even surpassed some of the supervised learning methods in terms of Micro-F1 scores. Intriguingly, current state-of-the-art methods, including HeCo and Xgoal, exhibited substantial sensitivity to the choice of meta-path combinations. This finding underscores the importance of robust meta-path handling, especially in self-supervised learning contexts, and highlights the effectiveness of LAMP in addressing this challenge."}, {"title": "5.4 Node Clustering", "content": "In our experimental setup, we employ the K-means clustering algorithm for the learned node embedding. For performance evaluation, we utilize standard clustering metrics: normalized mutual information (NMI) and adjusted rand index (ARI). Recognizing the potential variability introduced by K-means due to its sensitivity to initialization, we execute the clustering process across ten independent runs and present the averaged outcomes in Table 5. Notably, the IMDB dataset is excluded from this evaluation, given its multi-dimensional label structure in HGB dataset. Furthermore, direct comparisons with supervised methodologies are omitted; these models have inherent access to label information during training and are optimized based on validation metrics. Empirical results underscore that LAMP consistently exhibits superior performance across datasets, reaffirming its effectiveness in the clustering context."}, {"title": "5.5 Ablation Study", "content": "This section evaluates two distinct variants: LAMPw.o.mp (referred to as LAMPvar1) and LAMPw.o.unifiedHGNN (referred to as LAMPvar2). For the LAMP var1 version, we freeze the parameter $ \\gamma $ to cancel out the effect of meta-path importance during LMA learning. The intent behind this is to examine the role of meta-path importance in bridging local and high-order information. On the other hand, LAMPvar2 replaces the unified HGB encoder with the meta-path and network-schema encoders from HeCo. Within this setup, the meta-path view is processed using the HAN [46] attention mechanism, while a standard GCN tackles the original HIN. For the meta-path view, the LMA edge-pruning technique is applied to each individual meta-path sub-graph.\n\nTable 3 illustrates that both LAMPvar1 and LAMPvar2 suffer a considerable decline in performance. (1) Lacking the meta-path importance $ \\gamma $, LAMPvar1 struggles to harness sufficient overall structural data. It primarily emphasizes local details based on node attributes. Similarly, without the guidance of meta-path importance $ \\gamma $, LMA tends to prioritize lengthy meta-paths, and neglect potentially valuable shorter meta-paths. The resultant effect weakens LAMP var1's capability to bridge local and high-order information. This underscores that the guidance from meta-path importance is crucial for the LAMP model. (2) For LAMPvar2, employing separate HGNN encoders for the two views might have been effective in HeCo, but it does not work for LAMP. As shown in Table6, LAMPvar2 lags behind in performance across all datasets. Using disparate HGNN encoders inherently amplifies the differences in embedding produced by the two views, even when the target node attributes remain consistent across both views. This introduces a dilemma for LMA, making it challenging to determine which edges to prune, as the two views already appear distinct. This inconsistency can destabilize the model, increasing the risk of training collapse."}, {"title": "5.6 Analysis of Hyper-parameters", "content": "In this section, we examine our model's sensitivity to two critical hyper-parameters: the threshold for positive samples Tpos and the regulation term Areg, which determines the proportion of retained edges in LMA. Node classification on the ACM and DBLP datasets is evaluated, with both Macro-F1 and Micro-F1 scores presented."}, {"title": "5.6.1 Analysis of Tpos", "content": "The threshold Tpos controls the number of positive samples. We vary its value to observe its impact on performance, as shown in Figure 7(a) and Figure 7(b). As Tpos increases, performance initially improves before declining. The optimal thresholds are determined to be 7 for DBLP and 8 for ACM. These performance trends are consistent across both datasets."}, {"title": "5.6.2 Analysis of Areg", "content": "Our exploration also considers the consequences of adjusting Areg, which governs the fraction of edges retained by LMA. Results are presented in Figure 7(c) and Figure 7(d). For both DBLP and ACM datasets, Areg=0.3 yields peak performance, preserving approximately half of the meta-path view edges. Notably, raising Areg beyond 0.5 results in the preservation of 70%-80% of edges. This excessive retention introduces redundant data into the model, leading to diminished efficacy."}, {"title": "6 Related Work", "content": "6.1 Heterogeneous Graph Contrastive Learning\n\nHGCL has rapidly evolved, effectively adapting contrastive learning techniques for heterogeneous graphs [35, 25, 47, 24, 3, 57, 60]. Standard HGCL approaches involve creating multiple graph views via meta-path or network-schema based augmentations, followed by representation learning through contrasting positive and negative samples. DMGI [35], for instance, contrasts the original network with its corrupted counterpart for each meta-path view, integrating a consensus regularization for meta-path fusion. HeCo [47] introduces two augmentation techniques-meta-path sub-graph view and network schema view\u2014and minimizes the inter-view information entropy using personalized pairwise InfoNCE. HDMI [25] and XGOAL [24] are advanced versions of DGMI. HDMI improved semantic attention via high-order mutual information, XGOAL proposed a stronger positive and negative samples generating strategy, and node embeddings are obtained by simply average pooling over these layer-specific embeddings. CPT-HG [23] presents a pre-training model grounded in contrastive learning by making sub-graphs derived from positive samples integrate randomly swapped nodes from the negative set."}, {"title": "6.2 HGNNs applications in IR", "content": "In recent years, heterogeneous graph neural networks (HGNNs) as general extension of homogeneous graph [19, 43, 18, 45, 55, 44, 14] have risen to prominence as a pivotal tool in information retrieval (IR), adept at extracting rich structural and semantic information from heterogeneous graphs. This capability has led to their widespread application across various IR domains, including search engines, recommendation systems, and question-answering systems, among others. In the context of search engines and matching, Chen et al. [2] innovated a cross-modal retrieval method utilizing heteroge-neous graph embeddings. This method adeptly preserves cross-modal information, overcoming the limitations of traditional approaches that often lose modality-specific details. Similarly, Guan et al."}, {"title": "7 Conclusion", "content": "Our study reveals the sensitivity of existing methodologies to meta-path combinations in unsupervised heterogeneous graph neural networks. To address this challenge, we introduce LAMP, a meta-path-guided adversarial approach for Heterogeneous Graph Contrastive Learning (HGCL). LAMP excels in capturing local and high-order structural information through dual views and Learnable Meta-Path guided augmentation (LMA) with an HGNN. Empirical tests across various datasets showcase LAMP's superiority over existing unsupervised models and competitive performance even with supervised models. LAMP holds great potential for future heterogeneous graph contrastive learning research."}]}