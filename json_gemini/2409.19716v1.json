{"title": "Constrained Reinforcement Learning for Safe Heat Pump Control", "authors": ["Baohe Zhang", "Lilli Frison", "Thomas Brox", "Joschka B\u00f6decker"], "abstract": "Constrained Reinforcement Learning (RL) has emerged as a significant research area within RL, where integrating constraints with rewards is crucial for enhancing safety and performance across diverse control tasks. In the context of heating systems in the buildings, optimizing the energy efficiency while maintaining the residents' thermal comfort can be intuitively formulated as a constrained optimization problem. However, to solve it with RL may require large amount of data. Therefore, an accurate and versatile simulator is favored. In this paper, we propose a novel building simulator I4B which provides interfaces for different usages and apply a model-free constrained RL algorithm named constrained Soft Actor-Critic with Linear Smoothed Log Barrier function (CSAC-LB) to the heating optimization problem. Benchmarking against baseline algorithms demonstrates CSAC-LB's efficiency in data exploration, constraint satisfaction and performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Europe's energy landscape is with approximately 40% of its energy consumption allocated to heating, predominantly fueled by non-renewable sources. The urgency to transition towards more sustainable energy practices is palpable, as numerous European nations set ambitious targets to slash emissions in the heating sector by 40% by 2030 compared to 1990. This imperative move is not just a step towards meeting climate objectives but also a significant leap towards redefining energy efficiency within the heating domain.\nEnhancing the efficiency of heating systems emerges as a cornerstone in this endeavor, necessitating the wide applications of innovative technologies in electricity and heating networks. Such integration, particularly at the building or district level, is pivotal for substantially reducing energy consumption. Heating systems, whether in residential build-ings or commercial establishments, are critical for maintaining comfort and energy efficiency. Applying constrained rein-forcement learning (RL) algorithms to solve heating system control problems presents a compelling approach due to several advantages. Constrained RL methods are not only capable to address the balance between thermal comfort and energy usage, but also learn in real-time data from noisy sensor observations as proven in many robotic tasks [1]. It also allows the agent to continuous learning in dynamical scenario when building uses or electricity price evolve.\nThe role of simulation frameworks in training RL agents is undeniably crucial. The success of Issac Gym [2] has significantly accelerated the development in many robotics tasks (e.g. locomotion [3]). An accurate, adaptable, and parallelizable simulator can substantially accelerate the policy improvement cycle, enabling more precise control over heating systems.\nDespite there are many frameworks coming up within the realm of heating control, a comprehensive open-source lightweight simulation framework which provides a handful of simulated building environments, extensive customization options and serves as a conduit between the control com-munity and Reinforcement Learning community through a standardized interface is still missing. To bridge this gap, we propose a novel open-source framework Intelligence for Building (I4B) for simulation-based advanced control strategies such as model predictive control (MPC) and RL, for heat pump operation. I4B creates an interface between the building simulation module and the control algorithm, incorporating reference controllers, support for parallelization, and standardized metrics for evaluating algorithms.\nWe conceptualize the heating control problem as a con-strained Markov Decision Process (CMDP), where the objec-tive is to minimize energy usage while maintaining indoor temperature above a predefined threshold. Upon applying state-of-the-art constrained RL algorithms to this framework, our empirical investigation benchmarks their performance across various scenarios. Noteworthy among these algorithms is the constrained RL with linear smoothed log barrier function (CSAC-LB), a novel approach which particularly suits the heating control problem. Heating system problem is characterized by an optimal solution that lies at the boundary of the feasible and infeasible sets. Our findings reveal that CSAC-LB excels in balancing exploration with performance.\nTo summarize, our contributions are following:\n\u2022 We propose I4B\u00b9, a novel open-source lightweight building heat pump operation simulator with rich cus-tomization options and interfaces for different research communities.\n\u2022 We applies variety of constrained RL algorithms to differ-ent heating scenarios. An empirical study is performed to benchmark them.\n\u2022 We demonstrate CSAC-LB can balance the objective and constraints better compared to other SOTA methods.\n\u2022 We perform a comprehensive analysis on it compared to other algorithms in the context of heating control problem."}, {"title": "II. RELATED WORK", "content": "In recent years, reinforcement learning (RL) has gained attraction as a method for optimizing the operation of building heating, ventilation, and air conditioning (HVAC) systems. A good overview is provided in the recent article [4]. Different RL variants, called static and dynamic, are proposed in [5]. Their findings also indicate that a heating energy saving ranging between 5 and 12% is possible compared to standard heating curve control. A significant amount of current works focus on the comparison of model-free RL with various control strategies such as MPC, model-based RL, data-driven control methods, and other hybrid approaches [6]. The subject to many papers is the comparison of Model-free RL to ideal MPC, such as in [7]. Many contributions, such as or [6], additionally consider model-based RL or data-driven control as hybrid approach between MPC and model-free RL. The publication [8], for instance, compares MPC and RL. The algorithms are implemented and evaluated in BOPTEST, a standardized simulation framework for the assessment of advanced control algorithms in buildings. The results indicate that pure RL cannot provide constraint satisfaction. Therefore, a hybrid algorithm called reinforced predictive control (RL-MPC) that merges their relative merits, is proposed. Another approach is learning-based MPC, as discussed in [9], where a deep neural network is employed to learn the model, which is subsequently used for optimization. In summary, despite progress in applying RL to building heating control, a detailed examination of advanced RL algorithms, especially those tackling state constraints through constrained RL, requires further investigation."}, {"title": "B. Constrained RL algorithm", "content": "Many comprehensive reviews of Constrained Reinforce-ment Learning [10], [11] have been publicated in the recent years. One notable approach in addressing constrained RL problems is safe policy search methods. These methods often incorporate nonlinear programming techniques into policy gradient frameworks or develop safe policy search strategies based on theoretical analyses [12], such as gradient projec-tion [13], [14]. Extensions [15] to safe policy search include the integration of Gaussian Process models for risk estimation. Constrained Policy Optimization (CPO) [16] method and its successor [17] serve as general-purpose approachs, utilizing trust-region methods to solve constrained RL problems while offering theoretical guarantees. Additionally, Conditional Value-at-risk (CVaR) [18], [19] has been used to optimize Lagrangian functions with gradient descent, further enriching this research area. In [20], [21], the authors extend SAC [22] with a cost function and optimize the constrained problem by introducing a Lagrange-multiplier. Further improvements are presented in [23] where a distributional safety critic is combined it with the CVaR metric. Applying interior-point methods to on-policy RL algorithms [24], [25] are also drawing many attentions in the recent development. Another line of work is close to MPC. Model-based approaches, such as in [26], utilize dynamics models to certify safety but are often limited by assumptions about model accuracy and known constraints."}, {"title": "C. Building Simulator", "content": "Over the past decades, several frameworks for simulating the thermal building behavior for application to control strat-egy development and testing have been developed. Energym [27] is an open source building simulation library designed to test climate control and energy management strategies on buildings in a systematic and reproducible way. It relies on the functional mockup interface (FMI) standard in order to support models developed in the modelling languages Modelica and EnergyPlus. Bobtest (Building Operation TESTing) [28] is a newer framework consisting of a set of Modelica models that represent different buildings with different HVAC systems in different climate zones. Sinergym [29] was developed particularly to create an environment following Gymnasium [30] interface for wrapping the building simulation engines EnergyPlus for building control using deep reinforcement learning. Most environments require the user to either manually install a building simulator (e.g. EnergyPlus) or to manually manage Docker containers. This can be tedious. For this reason, Beobench [31] was introduced as a toolkit providing easy and unified access to different building control environments for RL. Unfortunately, due to the complexity of creating building simulations, none of the existing frameworks provides more than a handful of simulated buildings."}, {"title": "III. CONSTRAINED RL", "content": ""}, {"title": "A. Constrained Markov Decision Processes", "content": "A Constrained Markov Decision Process (CMDP) is an extension of a Markov Decision Process (MDP), defined by the tuple (S,A, R, \u03b3,P), where S represents the set of states, A denotes the set of actions, R: S\u00d7A \u2192 R is the reward function, y is the discount factor, and P is the probabilistic state transition model.\nTo construct a CMDP, we augment the MDP with a cost function C: S \u00d7 A \u2192 RK, which incorporates a set of constraints and yields a K-dimensional vector of costs for each state-action pair. Here, Jc(\u03c0) represents the expected cumulative discounted cost under a given policy \u03c0, and D is a K-dimensional vector of cost limits.\nThe feasible set of policies for a CMDP, denoted as \u03a0c, is defined as follows:\n$\\Pi_c = \\{ \\pi \\in \\Pi : J_c(\\pi) - D < 0 \\}$ (1)\nWhere \u03a0 represents the set of all possible policies. In this context, J(\u03c0) represents the expected cumulative return to be maximized. Therefore, the optimization problem of learning an optimal policy can be formulated as:\n$\\pi^* = \\arg \\max_{\\pi \\in \\Pi_c} J(\\pi)$ (2)\nThis optimization seeks the policy \u03c0* that maximizes the return while adhering to all constraints defined by the cost function C."}, {"title": "B. Linear Smoothed Log Barrier Function", "content": "The log barrier method is recognized for its capability to address constrained optimization problems that include inequality constraints, yet it is prone to challenges with numer-ical stability. This instability arises because the logarithmic function within the log barrier method cannot accommodate conditions where g(x) > 0, leading to difficulties in maintain-ing stable optimization.\nIn the realm of Deep RL, where neural networks are em-ployed to approximate value functions and model continuous control policies, the issue of constraint satisfaction becomes particularly pronounced. Due to the inherent randomness in initializing neural network parameters, the policy defined by the actor network might initially violate some constraints, thus outlining an unsafe policy. A straightforward workaround might involve clipping the network's output values to the output layer to mitigate constraint violations. However, these approaches may stop gradient flow during the optimization process or even overlook constraint violations entirely.\nTo solve there drawbacks associated with directly clipping the neural network's output, a novel approach involving a linear smoothed log barrier function [32], \u03c8(x), has been introduced. This function is designed as follows:\n$\\psi(x) = \\begin{cases}  -\\frac{1}{\\mu} \\log(-x) & \\text{if } x \\leq -\\frac{1}{\\mu^2} \\\\  \\mu x - \\log(\\frac{1}{\\mu}) + 1 & \\text{otherwise} \\end{cases}$ (3)\nwhere \u00b5 is a parameter that adjusts the function's behavior. The key advantage of \u03c8(x) lies in its continuous and differentiable nature across its entire domain, allowing for the application of stochastic gradient descent (SGD) without being confined solely to the feasible set. This facilitates a more robust and flexible approach to optimizing neural networks in the context of constrained RL."}, {"title": "C. SAC-Lag", "content": "Originally developed for teaching quadruped robots loco-motion [20], SAC-Lag adapts the Soft Actor-Critic frame-work [22] by incorporating stepwise constraints to limit the robot's pose, aiming to mitigate potential damage. To define this constrained optimization task, we introduce dt as the permissible threshold for constraint violations at each step t, with the primary goal being to maximize the following objective:\n$\\sum_{t=0}^T \\mathbb{E}_{a_t \\sim \\pi(s_t)} [R(s_t, a_t) + \\alpha H(\\pi(\\cdot | s_t))]$ (4)\ns.t. $\\mathbb{E}_{a_t \\sim \\pi(s_t)} [C(a_t, s_t) - d_t] \\leq 0, \\forall t$,\nwhere R is the reward function and H(\u03c0(\u00b7|st)) is the entropy term. As in [21] using the Lagrange-multiplier method, we denote d as the cumulative cost limit. The unconstrained optimization problem can be formulated as:\n$\\max_{\\pi} \\min_{\\beta \\geq 0} L(\\pi, \\beta) = f(\\pi) - \\beta g(\\pi)$ (5)\nwhere\n$f(\\pi) = \\sum_{t=0}^T \\mathbb{E}_{a_t \\sim \\pi(s_t)} [R(s_t, a_t) + \\alpha \\pi(\\cdot | s_t))]$\nand\n$g(\\pi) = \\mathbb{E}_{a_t \\sim \\pi(s_t)} [\\gamma^t (C(a_t, s_t) - d)]$\nGiven that the Q-value is approximated using neural net-works, Equation 5 lacks a closed-form solution. Consequently, Dual Gradient Descent [33] is employed to iteratively refine both the policy, parameterized by \u03c6, and the Lagrange multiplier. The actor network's parameters are denoted by \u03c6, and D represents the replay buffer. We denote the reward Q-network with parameters \u03b8r as Q\u03b8r, and the cost Q-network with parameters \u03b8c as Q\u03b8c. To minimize Eq. 5, Lagrange-multiplier \u03b2 is updated according to the following loss:\n$J(\\beta) = \\mathbb{E}_{s_t \\sim D, a_t \\sim \\pi_\\phi (s_t)}[\\beta (d - Q_{\\theta_c} (s_t, a_t))]$ (6)\nThis adjustment mechanism increases \u03b2 when the cost Q-network's output surpasses the limit d, thereby tightening the constraints, and decreases it as the current policy becomes more likely to satisfy these constraints.\nNow, assuming a fixed value for \u03b2, the actor loss to be minimized from Eq. 5 can be written as:\n$J(\\phi) = \\mathbb{E}_{s_t \\sim D, a_t \\sim \\pi_\\phi (s_t)} [- \\alpha \\log \\pi_\\phi (a_t | s_t) - Q_{\\theta_r} (s_t, a_t) + \\beta Q_{\\theta_c} (s_t, a_t)]$"}, {"title": "D. CSAC-LB", "content": "With the help of the linear smoothed log barrier function, we are able to solve the constrained RL problem via SGD. CSAC-LB [34] follows the setup of SAC-Lag [21] and uses double-Q critic networks not only to learn the reward but also the cost of constraint violation by taking the maximum of the two cost critic networks. This approach helps mitigate the underestimation of the Q-value of cost caused by constraint violations. This is especially helpful for heating systems in the collected replay buffer, where most of the transitions are free of constraints, subsequently causing an underestimation of the Q-value for cost."}, {"title": "IV. CONTROL TESTING FRAMEWORK FOR BUILDING HEAT PUMP OPERATION", "content": ""}, {"title": "A. Definition", "content": "A thermal building energy model is a mathematical de-scription of the thermal energy-related behavior of a building, including details about its structure, systems, usage, and location. Various factors are considered, such as:\nBuilding envelope: This includes characteristics such as size, shape, layout, and orientation, as well as the construction and insulation.\nHVAC Systems: The heating, ventilation, and air con-ditioning systems are modeled, including their energy sources, efficiency, control strategies, and distribution methods.\nInternal loads: These represent the energy consumed within the building, including lighting, appliances, and equipment, as well as the heat produced by the occupants.\nWeather\nThey are used to simulate and analyze the energy per-formance of buildings, enabling predictions about energy consumption, cost, efficiency, and environmental impact, as well as, developing and testing different advanced control strategies. There are different control tasks that arise in building heating operation, such as:\nReference tracking: Keep room temperature close to the set point temperature despite disturbances such as heat loss to the ambient and heat gains due to people behavior and solar irradiations.\nAdditionally minimize an \"economic\" cost function (e.g., energy cost).\nDemand responsive heating and cooling operation for improving the stability of the electric grid."}, {"title": "B. Requirements and software architecture", "content": "The requirements for a heat pump control testing framework can be specified as follows:\nEmulation models including HP, building, and heating emission to simulate the physics, dynamics, and time-resolution necessary for controls design\nStandardized simulation environment for consistent benchmarking results\nWell defined data exchange interface between a test controller, the emulator models and the simulation engine\nProvide exogenous input data (weather, user profiles, energy prices,...)\nStandardized KPIs for evaluation\nOpen source software, good documentation\nEasy to use, fast solution time and easy debugging for external users\nEasy adaptation to the specific heat pump system (heat emission system, DHW, PV integration, large-scale heat pump...)"}, {"title": "C. Thermal building models", "content": "Every model simplifies reality, often omitting certain phenomena, a principle particularly relevant to thermal behavior modeling in buildings, which involves complex interactions among internal conditions, external influences, and building materials.\nWe adopt a single-zone model for simulating building thermal dynamics, striking a balance between detail and computational efficiency. This model focuses on a water-based heating system, where heat, transferred from water through radiators or underfloor heating, circulates back to the heat source at a reduced temperature. This return temperature, considered a state in our model, along with the net energy flow, dictates the indoor temperature changes. While a more complex model might include additional temperature nodes for walls and differentiate between transmission losses and thermal mass, our three-state model simplifies this to indoor and wall temperatures, and water return temperature, as depicted in Fig. 4.\nExemplary, we describe the formulas for a two-state model with the state vector x(t) = [Troom, Thp,ret], capturing the thermodynamic interactions:\n$\\dot{T}_{room} = 1/C_{bldg} (Q_{gain} + H_{rad,con} \\cdot (T_{hp,ret} - T_{room})\n- H_{ve,tr} \\cdot (T_{room} \u2013 T_{amb}))$ (9)\n$\\dot{T}_{hp,ret} = 1/C_{water} (\\dot{m}_{hp} C_{p,water} (T_{hp,sup} - T_{hp,ret})\n- H_{rad,con} \\cdot (T_{hp,ret} - T_{room}))$ (10)"}, {"title": "D. Heat Pump", "content": "From the heat pump supply and return temperatures, Thp,sup and Thp,ret, and the usually constant mass flow in the building heat emission system mhp, the thermal heat pump power Qhp can be computed. To compute the heat pump efficiency, the following considerations are taken into account. A heat pump uses mechanical energy to transport heat energy from a lower to a higher temperature level. This process can be idealized by a Carnot process. For the reversible Carnot process a theoretical COP of\n$COP_{th} = \\frac{Q_{th}}{P_{el}} = \\frac{T_{hp,sup}}{T_{hp,sup} - T_{amb}}$ (11)\nis possible. However, real heat pumps do not operate loss-free. The achievable COP of a heat pump is therefore smaller than the theoretical value by the exergetic efficiency \u03b7WP: COP = \u03b7WPCOPth. Modern air-to-water heat pumps achieve an efficiency of about 0.45. Often the dependence of the COP on the source and sink temperature (Thp,sup, resp. Tamb in Equation (11)) is approximated by a second-ordner polynomial that is fitted to manufacturer data."}, {"title": "E. Test cases and data", "content": "The building model parameters, cf. Table I, can be obtained from the open-source building typology tool TABULA [37], providing information for a large number of different buildings in European countries. The disturbance profiles for occupancy and appliances are derived according to normed profiles for the according building class. Solar gains are computed for each building using weather data and building information such as dimensioning of windows, orientation of the building, etc."}, {"title": "F. Model predictive control algorithm", "content": "Model Predictive Control (MPC) is a control strategy that uses a dynamic model of a system to optimize its performance over a specified time horizon in the future. The control algorithm solves the following optimization problem in each time step to find the optimal heat pump supply temperature for minimal heat pump power consumption while satisfying control bounds and state constraints. Heat pump power is computed from the heat pump thermal power produced and the COP as detailed in Secion IV-D. To simulate sensor noise, as equally in the RL environment, we consider an initial value with additive random noise \u03b5, randomly sampled from N (0,0.5).\n$\\min_{T_{hp,sup}} \\int_{t=0}^{ctn} P_{el}(t) dt$\ns.t. $T_{room}(0) = T_{room,meas} + \\epsilon$\nsystem dynamics 9-10\n$T_{room}(t) \\in \\Gamma = [y_{min}, y_{max}] \\forall t$\n$T_{hp,sup}(t) \\in \\Omega = [U_{min}, U_{max}] \\forall t$\nFor more details about the implementation for MPC, we refer to [38]."}, {"title": "V. EXPERIMENTS", "content": ""}, {"title": "A. Experiment Setup", "content": "a) Environment: To evaluate different constrained RL algorithms, we selected two buildings based on real-world examples. Building 1, an older structure equipped with a water heat pump and ground collectors, suffers from lower energy efficiency due to inadequate thermal insulation. In contrast, Building 2 is a newer construction utilizing an air-source heat pump. The simulations incorporate actual weather data relevant to the locations of these buildings and also consider internal gains from occupants. Additionally, we introduce additive noise N (0,0.5) to the controller's observations to assess performance under noisy conditions.\nOur simulations employ a three-state building model with a simulation interval of 15 minutes per step. The observation space for the controllers includes ambient temperature, room temperature, wall temperature, return water temperature from the heat pump, and energy contributions from alternative heat sources (such as solar radiation and internal gains). The only action available to the controllers is adjusting the heat pump's set temperature. We've established a reference set-point room temperature of 20 Celsius as a constraint. The key performance indicators (KPIs) are electrical energy consumption measured in kWh, and average and maximum comfort deviation, measured as the difference between actual and reference room temperatures, with deviations considered in both positive and negative directions. According to the application requirements, the average deviation must be below 0.05K, and the maximum deviation must be below 2.5K to be deemed acceptable."}, {"title": "b) Training Setup", "content": "The heating system presents a unique challenge for constrained reinforcement learning (RL) algorithms, primarily due to the nature of its data distribution. In the real world, most transitions within such systems do not breach constraints, tending to cluster within a narrow band of the data spectrum. RL methods, which initially lack any specific guidance, must refine their strategies through a process of trial and error. To facilitate exploration across diverse data regimes, the training protocol sets episode lengths at 96 steps, corresponding to a day. After each episode, the environment resets to a randomly chosen initial state, aiding the RL agent in learning to navigate out of potentially unsafe states. To assess the performance of these RL algorithms, evaluations are conducted every 10 episodes, with each evaluation episode covering an entire year. We train our RL algorithms with 10000 episodes/days in the simulation.\nFor MPC, we leverage the simulator's ground-truth dy-namics model for prediction purposes. The temperature slack variable's weighting factor is fixed at 0.1, aiming to balance between maintaining comfort and minimizing energy costs."}, {"title": "c) Baselines Algorithms", "content": "We benchmark the following algorithms: (a) SAC [22] with reward shaping: we add a penalty reward of -2/-30 when the constraint is violated, which is tuned by [39]. (b) SAC-Lagrangian [21] (c) CPO [16] (d) CSAC-LB [34] (e) MPC [38]."}, {"title": "B. Experiment Results and Analysis", "content": "Considering performance during training, he exemplarized for Building 1, as illustrated in Fig. 5, shows CSAC-LB's superior exploration capabilities and robust training relative to other constrained RL algorithms. Unlike SAC-Lag, which demonstrates undesired training behavior distancing it from the optimal Pareto frontier (indicated by a varied color palette), CSAC-LB maintains a closer proximity to the threshold of minimal constraint violations for extended periods. This performance shows CSAC-LB's efficacy, which can be explained by the advantages using log barrier methods, which guides the exploration into more critical regions effectively, which matches our statement of why we apply CSAC-LB to heating system mentioned in Sec III-D.\nFigure 6 shows the performance of the different RL algorithms during training, as compared to MPC. Figure 6 reveals that CPO experiences training divergence; although it ensures satisfactory thermal comfort in most cases (except in Building 1 without noise), its energy usage escalates throughout training, signifying unstable training. SAC-Lag displays unstable training patterns across all test environments, failing to meet expectations in both thermal comfort and energy efficiency. Both SAC variants secure adequate thermal comfort but at the cost of increased energy consumption, accompanied by noisy training trajectories characterized by significant fluctuations. Contrary to expectations, the MPC method falls short of the performance achieved by other RL algorithms in Building 2 when compared to Building 1, hinting at a potential need for comprehensive hyperparameter tuning, whereas, the RL controllers use the same set of hyperparameter, which demonstrates its robustness to different environments.Notably, all algorithms except MPC exhibit comparable performances when faced with noisy inputs across different buildings, showing its sensitivity to input noise. CSAC-LB stands out by effectively navigating the balance between thermal comfort and energy usage, minimizing temperature deviations and energy consumption without major constraint violation.\nTo summarize, performance is always a trade-off between comfort (or safety) and energy efficiency. In practical appli-cations, controlling comfort deviation, which in this context equates to safety, is crucial. Based on the yearly energy consumption and comfort deviation metrics presented in Table III, only MPC and CSAC-LB, followed by SAC-2, show good performance in terms of energy efficiency while maintaining low comfort deviation. CPO results in significantly higher energy consumption, whereas SAC-Lag exhibits unacceptable comfort deviations. MPC performs well in the absence of noise but falls short when model-plant mismatches, in the form of noise, are present."}, {"title": "VI. CONCLUSION", "content": "In this study, we apply the recently developed constrained RL algorithm CSAC-LB to the critical problem of energy-efficient heat pump temperature control. The goal is to operate the heat pump with low electricity consumption while reliably maintaining indoor temperature bounds during both training and operation. To achieve this, we introduce I4B, a lightweight, open-source simulation framework specifically designed for building heat pump control. I4B provides extensive customization options and simulates various real-world building scenarios, thereby bridging the gap between the control and RL communities with its user-friendly interfaces.\nOur evaluation of constrained RL algorithms reveals CSAC-LB's suitability for heat pump control. By leveraging a linear smoothed log barrier function and a double-Q network, CSAC-LB effectively addresses common underestimation issues and promotes boundary exploration-key for heating control tasks. Among the five recent RL algorithms tested, CSAC-LB stands out for its resilience to noisy data, adeptly balancing thermal comfort and energy efficiency. Considering model-plant mismatch in the form of noise, CSAC-LB demonstrates its superiority over MPC.\nFuture directions include integrating weather forecasts and model-based methods into the constrained RL approach and expanding I4B to encompass multi-room building setups."}]}