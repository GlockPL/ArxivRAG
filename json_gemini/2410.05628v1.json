{"title": "VERSATILE MOTION-LANGUAGE MODELS FOR MULTI-TURN INTERACTIVE AGENTS", "authors": ["Jeongeun Park", "Sungjoon Choi", "Sangdoo Yun"], "abstract": "Recent advancements in large language models (LLMs) have greatly enhanced their ability to generate natural and contextually relevant text, making AI interactions more human-like. However, generating and understanding interactive human-like motion, where two individuals engage in coordinated movements, remains a challenge due to the complexity of modeling these coordinated interactions. Furthermore, a versatile model is required to handle diverse interactive scenarios, such as chat systems that follow user instructions or adapt to their assigned role while adjusting interaction dynamics. To tackle this problem, we introduce VIM, short for the Versatile Interactive Motion language model, which integrates both language and motion modalities to effectively understand, generate, and control interactive motions in multi-turn conversational contexts. To address the scarcity of multi-turn interactive motion data, we introduce a synthetic dataset called INTER-MT2; where we utilize pre-trained models to create diverse instructional datasets with interactive motion. Our approach first trains a motion tokenizer that encodes interactive motions into residual discrete tokens. In the pre-training stage, the model learns to align motion and text representations with these discrete tokens. During the instruction fine-tuning stage, VIM adapts to multi-turn conversations using INTER-MT2. We evaluate the versatility of our method across motion-related tasks\u2014motion-to-text, text-to-motion, reaction generation, motion editing, and reasoning about motion sequences. The results highlight VIM's versatility and effectiveness in handling complex interactive motion synthesis.", "sections": [{"title": "1 INTRODUCTION", "content": "Agents that reflect how humans communicate and interact with each other through motion have the potential to revolutionize our interaction with technology. By capturing the subtleties of human communication, including gestures, expressions, and interactive behaviors, these agents can offer more intuitive and natural interfaces. This holistic understanding enables technology to adjust its responses and behaviors based on the user's physical motions and situational context, leading to more personalized and engaging interactions. Such capabilities are crucial for enhancing support across various domains, including robotics, virtual humans, entertainment, and more.\nRecent advancements in large language models (LLMs) (Dubey et al., 2024; Team et al., 2024; Yang et al., 2024) have demonstrated significant potential in generating human-like text and understanding complex linguistic interactions. They have even extended their capability to multi-modal contexts, successfully integrating various input sources such as images, speech, and videos (Ge et al., 2024; Liu et al., 2024; Chen et al., 2024b; Tang et al., 2024; Shu et al., 2023). Building upon these developments, there is a growing interest in incorporating human (or robot) motion as a new modality (Jiang et al., 2024; Chen et al., 2024a), leading to the emergence of the \"motion-language models\" (MLM).\nHowever, existing approaches (Zhang et al., 2023; Guo et al., 2024a; 2022; Zhang et al., 2024d; Cai"}, {"title": "2 RELATED WORK", "content": "Human Motion Modeling & Control Advancements in human motion modeling have driven significant progress in both motion generation and control. Diffusion-based methods, such as MDM (Tevet et al., 2023), FG-T2M (Wang et al., 2023), and MotionDiffuse (Zhang et al., 2024a) excel at synthesizing realistic human motions from the text. Transformer-based models with vector quantization, such as TM2T (Guo et al., 2022) and T2M-GPT (Zhang et al., 2023), effectively capture complex motion patterns. MoMASK (Guo et al., 2024a) improves motion granularity with residual tokenizers. For motion editing, some approaches focus on style transfer (Aberman et al., 2020; Guo et al., 2024b) or specific body part modifications (Zhang et al., 2024a; Kim et al., 2023). FineMoGEN (Zhang et al., 2024c) offers fine-grained motion synthesis based on user instructions.\nMEOs (Goel et al. (2024)) use captions and large language models to identify frames and body parts to edit, while MotionFix (Athanasiou et al. (2024)) conditions diffusion models on both source motion and edit text for seamless motion edits. However, these models usually target single tasks (e.g., text-to-motion, or motion editing) and lack versatility in handling input and output of both motion and text simultaneously in a unified architecture.\nMotion Language Model Recent developments in motion language models have aimed to achieve versatility across various motion-related tasks. MotionGPT (Jiang et al., 2023) demonstrates versatility in motion comprehension and generation based on a unified framework. MotionChain (Jiang et al., 2024) introduces a multi-turn conversational system for interpreting and generating motions within dialogue contexts, including image inputs. Zhou et al. (2024) introduces AvatarGPT integrating motion generation and planning ability in motion large language model. Some studies, like Chen et al. (2024a), expand modalities into speech, music, and videos but focus primarily on comprehension rather than generation. Zhang et al. (2024b) proposed unified models for generating motion from various input modalities. M\u00b3-GPT, from Luo et al. (2024), models speech, music, text, and motion interchangeably. However, modeling interactive motions in versatile large models remains under-explored. While some efforts, such as Wu et al. (2024), address this direction, they often lack multi-turn interactions and complex reasoning abilities. Our work addresses this gap with a model trained on our synthesized INTER-MT\u00b2 dataset, enabling the understanding and generation of interactive motions in multi-turn conversations with advanced reasoning capabilities. This approach facilitates more nuanced, context-aware motion generation in complex interactive behaviors.\nHuman-Human Interactive Motion Modeling Modeling human-human interactions has garnered increasing attention in recent research. Several multi-person interaction datasets (Ng et al., 2020; Fieraru et al., 2020; Yin et al., 2023) have been developed, and recent efforts like Inter-X (Xu et al., 2024a) and InterHuman (Liang et al., 2024) have collected interactive motions paired with textual descriptions for text-based motion control. In text-to-motion tasks, InterGEN (Xu et al., 2024a) introduces a diffusion-based model with spatial constraint loss. PriorMDM (Shafir et al., 2024) leverages pre-trained motion diffusion models with slim communication blocks. For reaction generation, ReMoS Ghosh et al. (2023) synthesizes reactive motion using spatio-temporal cross-attention, while ReGenNet Xu et al. (2024b) employs a transformer-based model with distance-based interaction loss to predict human reactions. While existing models have advanced interactive motion modeling, they lack versatility and focus on specific tasks, failing to capture complex multi-turn dynamics. To address this, we introduce INTER-MT2, enabling agents to generate sophisticated motions, respond to instructions, adapt roles, and adjust behaviors based on context."}, {"title": "3 INTER-MT2: INTERACTIVE MULTI-TURN MOTION-TEXT DATASET", "content": "Current datasets (Yin et al., 2023; Liang et al., 2024; Xu et al., 2024a) for modeling interactive motions lack sufficient diversity in instructions and do not include multi-turn conversations. To address this gap, we introduce INTER-MT2: INTERactive MUTI-Turn Motion-Text dataset. This dataset covers a variety of interactive motion scenarios with multi-turn conversations, diverse instructions, and spatiotemporally aligned motions between two individuals."}, {"title": "4 VIM: VERSATILE INTERACTIVE MOTION-LANGUAGE MODEL", "content": "In this section, we introduce VIM, a versatile interactive motion language model, designed to incorporate multi-turn conversations considering both language and interactive motion as input or output modality. First, we will explain the underlying philosophy behind our design choices for the model architectures, followed by a detailed description of the training methodologies. Then, we introduce advanced interactive motion tasks in multi-turn conversations."}, {"title": "4.1 NOTATIONS", "content": "Formally, we denote interactive motion from two individual a and b as {ma, mb}, following non-canonical representation from Liang et al. (2024) based on SMPL-X structure (Pavlakos et al., 2019). Each timestep of the motion \\(m^t = [j, \\dot{j}, j'', c_f]\\) is composed of global joint positions \\(j \\in \\mathbb{R}^{3N_j}\\), global joint velocities \\(\\dot{j} \\in \\mathbb{R}^{3N_j}\\), 6D representation of local rotations \\(j'' \\in \\mathbb{R}^{6N_j}\\), with the number of joints Nj, and binary ground contact features \\(c_f \\in \\mathbb{R}^4\\). We aim to train a motion language model \\(p_{\\theta}\\) which can model texts and motions for both inputs and outputs. We define input as an instruction"}, {"title": "4.2 ARCHITECTURE", "content": "Our architecture for modeling and generating interactive motions consists of three primary components: encoders (tokenizers), a large language model block, and decoders. This design allows for the integration of both motion and text data within a unified framework. For motion, we use a residual vector quantized variational auto-encoder (RQ-VAE (Lee et al., 2022; Guo et al., 2024a)) as a tokenizer. Vector quantized variational auto-encoders (Van Den Oord et al., 2017) are effective, but their quantization causes information loss and reduces reconstruction quality, which is critical for accurately modeling interactive motions. The motion encoder EM applies 2D convolutions to motion features along the time axis, converting motion pairs \\(m_a, m_b\\) into latent vectors \\(\\{z_{a}^{1:L}, z_{b}^{1:L}\\} = E_M(\\{m_a^{1:M}, m_b^{1:M}\\}),\\) where M is a motion length and \\(L = M/l\\) with down-sample rate l. Then the latent vectors z are quantized by RQ-VAE as an ordered D discrete codes:\n\n\\(RQ(z^i; C, D) = (k_1, \\ldots, k_L) \\in [K]^D\\)\n\nwhere C is the codebook, \\(K = |C|\\), and \\(k_i\\) is code of \\(z^i\\) at timestep i and depth d. These discrete codes form a motion vocabulary. For text, we use a standard text tokenizer to process textual instructions and descriptions into tokens.\nTokens are then proceeded to the language model block, which serves as the central processing unit. In this work, we have utilized the LLaMA-3.1-8B (Dubey et al., 2024) model as a base model. We integrate motion tokens with text tokens using a unified vocabulary, which combines both the text and motion vocabularies into one, with special tokens added to mark the start and end of the motion sequences. This shared token space enables the model to efficiently process and generate both modalities for motion-related tasks. Interactive motion is represented as\n\\(X_m = \\{ k_a^{1:D}, k_b^{1:D}, \\ldots, k_a^{1:D}, k_b^{1:D} \\} \\in [K]^D\\), where \\(X_m\\) is a sequence of motion represented in unified vocabulary and \\(k_i^a\\) is the i-th token of motion a.\nFinally, the decoding stage reverses the encoding process. For motion, the decoder DM projects the quantized features \\(\\hat{z}^i = \\sum_{d=1}^D e(k_i^d)\\), where e is codebook embedding, back into motion sequences using 1D convolution. Text decoding follows standard language model decoding procedures."}, {"title": "4.3 TRAINING", "content": "Motion Tokenizer The first stage is to train a motion tokenizer composed of an encoder, decoder, and quantizer. We followed the original objective functions from Lee et al. (2022) to train this model, minimizing the reconstruction loss, the codebook loss to align the encoder's outputs with the codebook, and the commitment loss to ensure encoder consistency. Once the encoder and decoder are optimized, we maintain this model freezed during the rest of the training stage.\nPre-training Strategy In the pre-training stage, we train a pre-trained large language model to align motion representations with textual representations. We design tasks including motion-to-text, text-to-motion, motion prediction, and reaction generation to train the model, leveraging paired datasets like Inter-X (Xu et al., 2024a) and InterHuman (Liang et al., 2024). Using the template from Jiang et al. (2023), we create input sequences y from motion sequences Xm and the corresponding motion caption. Since both motion tokens and text tokens are discrete, we train our model with the general language modeling next-token prediction objective: \\(L = -\\log \\sum_i p_{\\theta}(y_i | y_{<i})\\), where T is the length of the multi-modal sequence and i only counts when the text token appears at position i. To improve training efficiency, we train the LLM using a low-rank adaptor (LoRA) (Hu et al., 2022), similar to Ge et al. (2024). We then merged the LoRA parameters to the LLM backbone for further training. Furthermore, due to a limited number of interactive motion data, we also leverage larger single motion-text datasets from Motion-X (Lin et al., 2024). This offers prior knowledge of how individual motions are described in language, enhancing the model's ability to align motions with corresponding textual descriptions."}, {"title": "4.4 ADVANCED DOWNSTREAM INTERACTIVE MOTION TASKS", "content": "After training, our model can perform complex reasoning and generate interactive motions within multi-turn conversations. To verify this, we introduce two additional tasks requiring advanced capabilities: motion reasoning and editing. Motion reasoning involves predicting past or future events, or reasoning about current motions, based on prior conversational data. This task requires the model to understand the context of the conversation, interpret how the given motion fits within that context, and adjust its reasoning accordingly. In the motion editing task, we focus on altering a person's persona or shifting scenarios, such as emotions or relationship dynamics. This adds complexity, as changes to one person's behavior affect the other's motion. The model must edit the target motion while maintaining contextual coherence, requiring a deep understanding of social dynamics."}, {"title": "5 EXPERIMENTS", "content": "In our experiments, we evaluated VIM's ability to generate detailed motion-based chat responses, requiring complex reasoning about interactive motions, alongside traditional motion-related tasks. We focused on two main questions: first, whether the model can reason effectively about interactive motions, such as refining motions in editing tasks or generating contextually accurate narratives in motion reasoning. Second, we evaluated whether the training with INTER-MT\u00b2 dataset improves the model's performance in text-to-motion, motion-to-text, and reaction generation tasks."}, {"title": "5.1 EVALUATION TASKS AND BASELINES", "content": "Motion Reasoning Motion reasoning involves predicting past or future events or interpreting current motions using prior conversational context. We utilize powerful LLMs, i.e., GPT-40 (OpenAI, 2024) to assess the content alignment, naturalness, and logical coherence of the generated textual responses. Content alignment evaluates how accurately the text reflects the given motion data, logical"}, {"title": "5.2 \u039c\u039f\u03a4ION REASONING", "content": "In the motion reasoning task, conversations about two interactive motions are examined to assess the model's ability to deduce past or future events and comprehend the motivations driving the motions. The experimental results in Table 1 demonstrate that our unified model, VIM, significantly outperforms two-stage approaches across all LLM-assisted and linguistic metrics. Specifically, VIM achieves improvements with performance increases exceeding 1.9 points in logical coherence, 1.1 points in content alignment, and nearly 0.2 points in naturalness compared to the best two-stage model. The baseline models trained solely on text-motion pair datasets, such as VIM w/o INTER-MT2 and MotionGPT*, show limited reasoning capabilities. Although MotionGPT, which incorporates INTER-MT\u00b2 datasets, exhibits improved performance compared to baselines trained without INTER-MT2, it still does not match the effectiveness of the two-stage approaches."}, {"title": "5.3 \u039c\u039f\u03a4ION EDITING", "content": "We aim to validate the hypothesis that people will perceive the generated edited interactive motion from the proposed method to be more content-consistent, instruction-aligned, and better quality, through user subject studies. To analyze the results, we conducted a repeated-measures multivariate analysis of variance on the rated measures. We observed that methods significantly affect the user's perception of all dimensions; F(4) = 4.591, p = 0.002, \\(\\eta^2\\) = 0.137 for content similarity, F(4) = 7.134, p = 0.000, \\(\\eta^2\\) = 0.197 for instruction alignment, and F(4) = 4.781, p = 0.001, \\(\\eta^2\\) = 0.142 for motion quality, with all \\(\\alpha\\) = 0.05. The estimated marginal mean of the rated score is reported in"}, {"title": "5.4 TRADITIONAL MOTION RELATED TASKS", "content": "The results in Table 7 support our hypothesis that utilizing the INTER-MT\u00b2 dataset enhances the model's performance in traditional motion tasks like motion-to-text (M2T), text-to-motion (T2M), and reaction generation.\nWe plotted the difference in a post hoc pairwise comparison of the proposed method only. We denote * as 0.01 < p < 0.05, ** as p < 0.01, and *** as p < 0.001. The error bars represent 95% confidence intervals."}, {"title": "6 CONCLUSION AND DISCUSSIONS", "content": "Conclusion In this paper, we introduced VIM, a versatile motion-language model designed to model, understand, and reason about interactive motions. We outlined its architecture and provided detailed training strategies to create a unified framework integrating large language models with interactive motion modality. To enhance the model's reasoning capabilities and versatility, we presented a specialized dataset, INTER-MT2, which incorporates a variety of reasoning tasks set within multi-turn conversations centered on interactive motions. Our experiments demonstrated VIM's ability to effectively follow instructions, edit motions, and reason about interactive motions.\nLimitations and Impact Statement There are several limitations that warrant attention. First, the model's expressiveness remains limited when handling complex or previously unseen actions,"}, {"title": "A APPENDIX", "content": "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec. A.1) and INTER-MT2 dataset sample visualization (Sec. A.2), with accompanying dataset statistics (Sec. A.3). In-depth task explanations are included (Sec. A.4), alongside ablation studies that examine various pre-training methods (Sec. A.5). The appendix also contains qualitative results (Sec. A.6) and thorough explanations of two-stage baselines (Sec. A.8). Additionally, it provides template forms for pre-training and instruction tuning (Sec. A.9). We also report implementation details for MotionGPT* (Sec. A.7), with implementation details for the proposed method (Sec. A.10), detailed metrics explanation (Sec. A.11), protocols for user subject studies (Sec. A.12) focused on motion editing, prompts for data collection within the dataset (Sec. A.13), and guidelines for LLM-assisted evaluation processes (Sec. A.14)."}, {"title": "A.1 MOTION REPRESENTATION AND MOTION TOKEN REPRESENTATION", "content": "For two persons a and b, we denote the interactive motion as {ma, mb}, following non-canonical representation from Liang et al. (2024). Each timestep of the motion \\(m^t = [j, \\dot{j}, j'', c_f]\\) is composed of global joint positions \\(j \\in \\mathbb{R}^{3N_j}\\), global joint velocities \\(\\dot{j} \\in \\mathbb{R}^{3N_j}\\), 6D representation of local rotations \\(j'' \\in \\mathbb{R}^{6N_j}\\), with the number of joints Nj, and binary ground contact features \\(c_f \\in \\mathbb{R}^4\\). This non-canonical representation is applied for both interactive motions and single-person motions. All the motions are represented in an SMPL-X (Pavlakos et al., 2019) format.\nMotion tokenizer encodes the interactive motion into discrete residual tokens in depth D, based on latent vector z.\n\\(RQ(z^i; C, D) = (k_1, \\ldots, k_L) \\in [K]^D\\)\nwhere C is the codebook, \\(K = |C|\\), D is a depth, and \\(k_d\\) is code of z at timestep i with depth d.\nThe interactive motion token sequence is represented as \\(X_m = \\{ k_a^{1:D}, k_b^{1:D}, \\ldots, k_a^{1:D}, k_b^{1:D} \\}\\), where Xm is a sequence of motion represented in unified vocabulary and \\(k_i^a \\in [K]^D\\) is the i-th token of motion a. In particular, the motion token is represented as below:\n\\(X_m = \\{\\langle motion\\_token\\_start\\rangle, \\langle motion\\_token\\_a\\_start\\rangle, k_1^{1;a}, \\ldots, k_L^{1;a}, \\langle motion\\_token\\_a\\_end\\rangle, \\langle motion\\_token\\_b\\_start\\rangle, k_1^{1;b}, \\ldots, k_L^{1;b}, \\langle motion\\_token\\_b\\_end\\rangle, \\ldots,\\rangle, \\langle motion\\_token\\_a\\_start\\rangle, k_1^{L;a}, \\ldots, k_L^{L;a}, \\langle motion\\_token\\_a\\_end\\rangle, \\langle motion\\_token\\_b\\_start\\rangle, k_1^{L;b}, \\ldots, k_L^{L;b}, \\langle motion\\_token\\_b\\_end\\rangle, \\langle motion\\_token\\_end\\rangle\\}\\)where  and  is a special token added to the unified vocabulary."}, {"title": "A.2 DATA SAMPLE VISUALIZATION", "content": "The samples from the synthesized dataset, INTER-MT2, are illustrated in Figure 7."}, {"title": "A.3 INTER-MT2 STATISTICS", "content": "We collected 82K samples of multi-turn conversational data, each involving interactive motions. Of these, 30K samples focus on motion editing, 30K on reasoning about past or future scenarios, and 12K on story generation. Each sample includes four to eight conversation turns and two distinct motions. The dataset contains 96K motions generated using a text-to-motion diffusion model, while 56K motions come from the original source dataset. The train-validation-test set is randomly splitted by the ratio 0.8:0.05:0.15."}, {"title": "A.4 DETAILED TASK EXPLANATIONS", "content": "Motion Editing Standard motion editing tasks typically involve modifying the motion of a single person based on physical descriptions, such as \"raise higher\" or \"move faster.\" However, in this task, we focus on editing interactive motions involving two people based on their personas, such as emotions or relationships, by modifying just one person's persona. The primary challenge in motion editing for two people is that when the motion of one person changes, the motion of the second person, which is correlated, also needs to be adjusted. This requires more complex reasoning about social interactions. Specifically, we define the task as \u201cUSER:{scene_information}, {reference_motion}. ASSISTANT: {motion_caption}. USER: {editing_command}. ASSISTANT: {edited_motion}.\u201d The editing command could be defined as asking the model to change the persona of a person, like \u201cMake one person shy.\u201d We let our model generate motion caption in the middle to let the chain-of-thoughts technique improve the reasoning ability.\nMotion Reasoning Motion reasoning involves predicting future motions or inferring past events based on the current motion context. This task requires understanding the sequence of motions and making logical inferences about the preceding or subsequent events. For instance, given a motion of an ongoing interaction between two individuals, the model needs to deduce what might have happened before this moment or predict what will likely occur next. This is crucial for applications requiring a temporal understanding of motions, such as surveillance analysis, animation, or human-robot interactions. We define the input sequence as follows: \u201cUSER:{question_1}, {motion_1}. ASSISTANT: {answer_1}. USER: {question_2}, {motion_2}.\u201d, where the model has to predict \u201cASSISTANT: {answer_2}"}, {"title": "A.5 ABLATION STUDIES ON PRETRAINING METHOD", "content": "We conducted ablation studies on the pertaining method. All the baselines are pre-trained models, not including the fine-tuning stage. To evaluate the effectiveness of our pretraining approach, we conducted ablation studies comparing different methods on three motion-related tasks: Motion-to-Text (M2T), Text-to-Motion (T2M), and Reaction Generation. As shown in Table 7, we compared our proposed method, VIM, against MotionGPT* and VIM-VQ, using the InterX (Xu et al., 2024a) and Interhuman (H) datasets (Liang et al., 2024). MotionGPT* serves as a baseline with 248M trainable parameters, achieving a retrieval Top3 score of 0.518 in M2T and 0.280 in T2M, with corresponding FID scores of 0.178 and 1.338 for T2M and Reaction Generation, respectively. VIM-"}, {"title": "A.6 QUALITATIVE RESULTS", "content": "We visualize our result gallery on motion editing in Figure 8 and on motion reasoning in Figure 9. Furthermore, the results for motion-to-text (Figure 10), text-to-motion (Figure 11), and reaction generation (Figure 12) are demonstrated."}, {"title": "A.7 IMPLEMENTATION DETAILS FOR MOTIONGPT*", "content": "For training MotionGPT (Jiang et al., 2023) in the interactive motion dataset, we have utilized the Flan-T5-base model (Chung et al., 2024) as a base large language model. We trained the model with Interhuman (Liang et al., 2024) and InterX (Xu et al., 2024a) dataset, with the non-canonical representation, same as the proposed method. Although scaling up the model can improve the performance, we conducted the experiment with the same base model as the original paper from MotionGPT (Jiang et al., 2023) and Motionchain (Jiang et al., 2024). The original paper reported that increasing the model size did not significantly improved the model's performance."}, {"title": "A.8 DETAILED EXPLANATION ABOUT TWO-STAGE BASELINES", "content": "In Section 5.2 and Section 5.3, we have compared the proposed method with two-stage models. In particular, we have utilized TM2T (Guo et al., 2022) for the motion captioner and InterGEN (Liang et al., 2024) for the text-to-motion generation model.\nMotion Editing In the motion editing task, the two-stage approach first uses the motion-to-text (TM2T; Guo et al. (2022)) model to generate a caption from the source motion and append the editing command. Then, the text-to-motion (InterGen; Liang et al. (2024)) model produces the edited motion based on this caption and command. In particular, the input for text-to-motion model is\"[motion caption]. [editing command]\".\nWe first trained TM2T model with the InterHuman dataset (Liang et al., 2024)and the InterX Xu et al. (2024a) dataset, which we denote as TM2T**. The performance is shown in Table 8. The TM2T* model shows substantial improvements over the baseline MotionGPT* models across all evaluation metrics. Specifically, TM2T* achieves Retriveal Precision scores of 0.413 (Top1), 0.589 (Top2), and 0.696 (Top3), along with BLEU, METEOR, and Rouge-L scores of 0.192, 0.386, and 0.395, respectively. These results indicate that the task-specific TM2T* model effectively generates accurate and relevant motion captions, making it a reliable choice for motion editing tasks. Although there remains a performance gap compared to the proposed method, the TM2T* model provides a robust foundation for generating moderate-quality motion captions.\nNext, we trained the text-to-motion diffusion model, InterGEN for the second stage. The performance of this model is reported in Table 9. InterGEN exhibits strong performance across all evaluation metrics, validating its effectiveness as the second stage in our two-stage approach. Specifically, InterGEN achieves Retrieval Precision scores of 0.403 (Top1), 0.557 (Top2), and 0.645 (Top3), which are substantially higher than those of the baseline MotionGPT* (0.180, 0.262, 0.328) and our unified VIM model (0.318, 0.469, 0.568). Additionally, InterGEN excels in Diversity with a score of 0.957 and maintains a low Maximum Mean Discrepancy (MMDist) of 1.115, indicating high-quality and varied motion generation. Its FID score of 0.078 is notably competitive, reflecting the realism and coherence of the generated motions. These results validate the use of InterGEN as the second stage in our framework."}, {"title": "A.9 TEMPLATE FORMS FOR PRE-TRAINING AND INSTRUCTION TUNING", "content": "We will detail the template forms utilized during the pre-training and instruction-tuning stages of our model development. Tables 10 and 11 illustrate the specific formats employed in each stage, providing a structured approach to aligning motion data with textual descriptions and enhancing the model's interactive capabilities. All the templates are from MotionGPT (Jiang et al., 2023).\nPre-training Templates During the pre-training stage, our objective is to align motion and language representations by leveraging large language models (LLMs). We design tasks such as Text-to-Motion, Motion-to-Text, Reaction Generation, and Motion Prediction using paired datasets like InterX Xu et al. (2024a) and Interhuman Liang et al. (2024). The pre-training templates involve generating captions from motion sequences, creating motions based on textual descriptions, producing reaction motions in response to initial motions, and predicting subsequent motions from partial sequences, as summarized in Table 10. For single-person motion, we utilized text-to-motion, motion-to-text and motion prediction task during training.\nInstruction-Tuning Templates In the instruction-tuning stage, we enhance the model's ability to follow diverse instructions presented in a conversational format. Utilizing the INTER2-MT dataset alongside single-turn data from previous interactive motion datasets, we format user instructions and assistant responses to facilitate multi-turn interactions. Table 11 outlines the templates used for tasks such as generating motions from user prompts, describing motions based on user queries, and predicting motion continuations. By structuring the interactions in this manner, the model becomes"}, {"title": "A.10 IMPLEMENTATION DETAILS", "content": "We set the codebook of the motion tokenizer as \\(K\\in \\mathbb{R}^{512 \\times 512}\\) for most comparisons, with residual depth 4. The motion encoder & incorporates a temporal downsampling rate 1 of 4. We utilize LLaMA-3.1-8B Dubey et al. (2024) as the underlying architecture for our language model. During the pertaining, we train the large language model (LLM) using a low-rank adaptor (LoRA) (Hu et al., 2022), including the embedding layer and the decoder head. The rank was set as r = 8, \\(\\alpha\\) = 16, with the dropout rate set as 0.05. During the instruction fine-tuning stage, we trained all the parameters. The learning rate was set as 0.0001, and the warm-up ratio as 0.01, the learning rate scheduler with cosine decay, and the AdamW optimizer."}, {"title": "A.11 MORE DETAILS ABOUT EVALUATION METRIC FOR TRADITIONAL MOTION RELATED TASKS", "content": "Motion Quality The Frechet Inception Distance (FID) is used to assess the similarity between the distributions of generated and real motions, utilizing an appropriate feature extractor tailored to each dataset. In addition, we use well-known motion capture metrics, MPJPE to quantify global and local errors in meters.\nMotion Diversity We have utilized diversity to evaluate the diversity of the motion following previous work (Jiang et al., 2023; Petrovich et al., 2023). To evaluate Diversity, the generated motions are split into two equal-sized subsets, and the Diversity metric is calculated as the average distance between motions within these subsets.\nCondition Matching TMR (Petrovich et al., 2023) offers motion/text feature extractors that produce geometrically coherent features for aligned text-motion pairs and vice versa. In this feature space, we evaluate motion-retrieval precision (R Precision) by combining the generated motion with 31 mismatched motions and calculating the top-1/2/3 matching accuracy between the text and motion. Furthermore, we assess the Multi-modal Distance (MM Dist), which measures the distance between the generated motions and their corresponding text."}, {"title": "A.12 USER SUBJECT STUDIES PROTOCOLS FOR MOTION EDITING", "content": "We conducted user subject studies using the platform on the Mechanical Turk service from"}]}