{"title": "Explainable AI-based Intrusion Detection System\nfor Industry 5.0: An Overview of the Literature,\nassociated Challenges, the existing Solutions, and\nPotential Research Directions", "authors": ["Naseem Khan", "Kashif Ahmad", "Aref Al Tamimi", "Mohammed M. Alani", "Amine Bermak", "Issa Khalil"], "abstract": "Industry 5.0, which focuses on human and Artificial Intelligence (AI) collabora-\ntion for performing different tasks in manufacturing, involves a higher number of\nrobots, Internet of Things (IoTs) devices and interconnections, Augmented/Vir-\ntual Reality (AR), and other smart devices. The huge involvement of these devices\nand interconnection in various critical areas, such as economy, health, educa-\ntion and defense systems, poses several types of potential security flaws. AI itself\nhas been proven a very effective and powerful tool in different areas of cyber-\nsecurity, such as intrusion detection, malware detection, and phishing detection,\namong others. Just as in many application areas, cybersecurity professionals\nwere reluctant to accept black-box ML solutions for cybersecurity applications.\nThis reluctance pushed forward the adoption of eXplainable Artificial Intelli-\ngence (XAI) as a tool that helps explain how decisions are made in ML-based\nsystems.", "sections": [{"title": "1 Introduction", "content": "The growing applications of Artificial Intelligence (AI) and Machine Learning (ML)\nhave increased the need for a better understanding of AI-based solutions for smart\nindustries, especially in Industry 5.0 applications. Similar to other sensitive application\ndomains, such as business, healthcare, education, and defense systems, the enigmatic\nand obscure nature of AI raises concerns and the need for in-depth evaluation of the\ndecisions made by these black box models in smart industries [1]. In addition to issues\nof user rights, and intelligent technology acceptance, developers of these systems need\nto ensure the fair and unbiased nature of their solutions. The need to comprehend and\ninterpret the causal understanding of inferences made by Deep ML models, directed\nthe attention of the research community towards \u03a7\u0391\u0399 [2]. In this regard, the first\nDARPA-funded Explainable AI (XAI) initiative started with the aims to develop\ninterpretable machine learning models for reliable and human-trusted decision-making\nsystems, crucial for the integration of Internet-of-Things (IoT) and intelligent systems\nin Industry 5.0 [3-6].\nCybersecurity is one of the critical aspects of smart industries involving a high\nnumber of interconnected devices. Similar to other applications, AI-based solutions\nhave been proven very effective in the cybersecurity of smart industries. However, the\nopacity of the diploid complex AI-based models in various Cybersecurity solutions,\nsuch as Intrusion Detection Systems (IDS), Malware detection and classification sys-\ntems, finding Zero-Day vulnerabilities, and Digital Forensics, exacerbates the trust\nlack of transparency [5, 7]. To keep themselves one step ahead of attackers, it is also\nessential for security analysts to know the internal automatic decision mechanism\nof the deployed intelligent model and to precisely reason the input data about the\nmodel's outputs. The application of XAI in cybersecurity could also be a double-edged\nsword, that is, besides improving the security practices, it could also make the intelli-\ngent explainable model vulnerable to adversary attacks [8-10]. Thus, the integration\nof human understanding and AI-based security systems needs to be keenly analyzed\nto provide a clear perception for future research."}, {"title": "1.1 Scope of the survey", "content": "This survey focuses on highlighting the critical challenges the security practitioners are\nbeing confronted with (i.e., the integration of successful security and defense measures\nin high-risk cyber-physical systems) in Industry 5.0 applications. Figure 1 provides\na visual representation of evaluation of industry 5.0 highlighting the trajectory of\nsmart industry development underscored by the imperative for robust cybersecurity\nmeasures in the face of escalating cyber attacks [21-23]. This heightens the scope of the\npaper and the need to address this issue by critically analyzing the research trends in\ncybersecurity for smart industries. The paper mainly focuses on exploring the impact\nof explainability and interpretability concepts on cybersecurity practices."}, {"title": "1.2 Related Surveys", "content": "The continuous evolution of industrial paradigms has introduced transformative goals,\nemphasizing the creation of a resource-efficient and intelligent society. This trajectory\nseeks to elevate living standards and mitigate economic disparities through the inte-\ngration of a hyper-connected, automated, data-driven industrial ecosystem [24-26].\nThis digital transformation promises to significantly enhance productivity and effi-\nciency across the entire production process. These milestones become possible through\nthe integration of AI/ Generative AI as a collaborative landscape, fostering innova-\ntion, optimizing resource utilization, and driving economic growth in smart industries\n[27]. However, it is imperative to acknowledge that such advancements expose the\nsystem to an elevated risk of sophisticated cyber-attacks [28]. The connected devices\nand networks of the autonomous industry infrastructure are more prone to hijacking,\nmalfunctioning, and resource misuse threats, which necessitates extra security layers\nto safeguard from such threats. The conventional deployed security measures, that\nare, AI-based cybersecurity systems are still in progress to mature, and the need for\ndeveloping robust and trustworthy security systems has become a trending goal for\ndefenders to achieve [29, 30].\nIn this essence, the need to comprehend and interpret the causal understanding\nof inferences made by AI-based learning models directed the attention of the research\ncommunity toward the XA\u0399 research field. In the literature, the taxonomy transition\nof XAI has been evaluated based on trust building for human-machine interaction"}, {"title": "1.3 Contributions", "content": "Based on serious threat vectors and their implications, in this paper, we analyze the\nadoption of different XAI methods in IDSs and examine the impact of interpretability\non Cybersecurity practices in the Industry 5.0 applications. In detail, we provide\nan overview of the literature on XAI-based cybersecurity solutions for Industry 5.0\napplications with a particular focus on existing solutions, associated challenges, and\nfuture research directions to overcome these challenges. To make it self-contained, we\nalso provide an overview of the taxonomy of XAI. The main contributions of this\npaper are summarized as follows:\n\u2022 We provide a clear and comprehensive taxonomy of XAI systems.\n\u2022 We provide a detailed overview of current state-of-the-art IDS, their limitations,\nand the deployment of XAI approaches in IDSs.\n\u2022 We also discuss the exploitation of XAI methods for launching more advanced\nadversarial attacks on IDS.\n\u2022 We also highlight the current cybersecurity challenges and potential solutions to\nensure the safety and security of industry 5.0 applications.\nThe rest of the paper is organized as follows. Section 2 presents the methodology\nadopted for conducting this survey by briefly describing the objective questions of\nthis survey. Section 3 provides an overview of the eXplainable AI taxonomies. Section\n4, presents cybersecurity challenges in Industry 5.0. Section 5 presents conventional\nIDS and the evalutions of the systems from AI-based IDS to XAI-based IDS. This\nsection also cover different type of explainability mechanisms, specifically Self-model,\nPre-model and Post-modeling explainability techniques. Section 6 presents adversar-\nial XAI techniques in cybersecurity with the focus on exploring the exploitation of"}, {"title": "2 Methodology", "content": "In the context of Industry 5.0, our focus was on investigating X-IDS to explore\ncybersecurity solutions and challenges in the forthcoming industry revolution. Our\nobjective was to analyze the array of approaches and techniques, particularly those\nutilizing big data and advanced analytics, to bolster security outcomes. This endeavor\ninvolved a thorough review of existing research and developments in the cyberse-\ncurity domain, specifically targeting Intrusion Detection and Prevention Systems\n(IDPS). Our research methodology included a systematic examination of academic\npapers, industry reports, and relevant literature from various sources to identify key\ntrends, methodologies, and emerging practices related to Explainable IDS. We also\ncritically evaluated these methodologies to determine their effectiveness in providing\ntransparency and comprehensibility within the secure Industry 5.0 framework with"}, {"title": "3 Explainable AI Taxonomies", "content": "The field of intelligent AI-based learning methods has evolved significantly, reaching a\nstage where a substantial portion of critical decisions relies on predictions from trained\nmodels. However, there exists a realm of intelligence where machines must justify their\ndecisions in response to questions like \"Why,\" \"What,\" or \"How.\" In simple words, a\ndecision model should explain their decision in such a way that it could be acceptable\nwith no dough, understandable with no difficulty and could be reliable to enhance\ntrust between users and technology [1, 2].\nThis pursuit of interpretability and explainability goal in AI coined the term,\neXplainable AI (XAI), in the research community, which embodies the idea of develop-\ning understandable AI models that are consistent with expert knowledge. The intuition\nbehind XAI is rooted in the concept that humans should be able to comprehend\nand trust the outputs and recommendations provided by AI systems. XAI aims to\nbridge the gap between the inherent complexity of AI algorithms and human under-\nstanding by providing transparent and interpretable explanations for the AI model's\noutputs [20, 42]. A presentation for model's decision in a textual (Natural Language"}, {"title": "3.1 Ante-hoc explainability", "content": "These are a kind of models that are self-explaining or can be called interpretable by\ndesign. Within this kind of transparency, these models could be interpreted in three\nlevels including the model's algorithmic level transparency, parametric level decom-\nposability, and functioning level simulatability. Typical examples are linear regression,\nlogistic regression, decision trees, random forests, Na\u00efve Bayes and fuzzy inference,\nrule-based learning [46, 47]. The following examples very well meet the criteria of\ndecision explainability, but these models perform very poorly on high-dimensional\ndata.\nFrom the Linear Models, for example, in Linear Regression, the prediction is sim-\nply the weighted sum of input features. The weighted sum can be utilized as a measure\nof explainability because the model's predicted target shows the linear relationship\namong the features. Additionally, the statistical measures associated with the linear\nregression model, such as p-values and confidence intervals, provide information about\nthe significance and uncertainty of the coefficients. These measures can help assess\nthe reliability and robustness of the model's explanations. Another type of regression\nanalysis, i.e., logistic regression, represents the target prediction as an estimated prob-\nability. This probability function is the magnitude of the coefficients that interprets an\nintrinsic sense of how much a feature is driving a model's prediction. Interpreting the\ncoefficients in logistic regression involves examining their sign, magnitude, and statis-\ntical significance. Positive coefficients indicate a positive influence on the probability"}, {"title": "3.2 Post-hoc explainability", "content": "The discernible reality is that many complex black-box models boast formidable\npredictive capabilities at the expense of limited explainability regarding their decision-\nmaking processes. In the realm of intelligent systems, accuracy, and interpretability\nstand out as primary characteristics. Addressing the imperative for transparency, sur-\nrogate explainers are essential, necessitating their development and application to\ninterpret the rationale behind decisions made by sophisticated models.[47]. A prevalent\nand contemporary research avenue actively tackling the opacity challenge of complex\nblack-box model families involves the utilization of post hoc explainability methods.\nTo elucidate the decision-making process of a trained model for a given input,\npost-hoc explainability addresses two overarching types of explanations: Local expla-\nnations and Global explanations. Local explanations are geared towards explaining\nhow the model predicts outcomes for a specific instance, providing insights into the\ninfluence of a particular individual instance on a specific class. This granularity permits\nusers to scrutinize the model's decision-making process at a detailed level [52]. Con-\nversely, global explanations aim to assess the impact of all input features on the model's\noverall output. Through global explanations, one can comprehend how a model learns"}, {"title": "4 Cybersecurity Challenges in Industry 5.0", "content": "Industry 5.0, which is mainly focused on the collaboration of humans and machines for\ndifferent tasks, involves the integration of several technologies, such as AI, data analyt-\nics, IoT, augmented and virtual reality, and improved man-machine interfaces (MMI)\nallowing workers to carry out different operations. This enhanced interconnectivity\nexposes smart industries to a diversified set of cybersecurity challenges and threats,\nwhich may lead to a disastrous operational environment, putting workers at risk and\nhalting production. Some of the most common cybersecurity threats to Industry 5.0\ninclude:\n\u2022 Expanded Attack Surface: The enhanced interconnectivity has significantly\nincreased the number of entry points for cyber attacks in Industry 5.0, making\nit more challenging timely detect and guard against different cyber attacks [69].\nFor instance, industries need to collect and analyze data for different tasks, such\nas customer behavior, optimization of marketing campaigns, better supply chain\nmanagement, and predictive maintenance of the machines, etc. However, this data\ncould be used by attackers for malicious activities. Thus, stricter access control\nand data management policies and techniques need to be incorporated to ensure\nthe data is used for improvement purposes.\n\u2022 Social Engineering: Social engineering, which exploits human mistakes/errors\ninstead of technical vulnerabilities, is one of the major cybersecurity threats in\nthe modern world. In recent years, social engineering tactics emerged as one of\nthe most effective ways of obtaining sensitive information. Some of the common\ncybersecurity threats based on social engineering tactics include phishing, baiting,\npretexting, malware, tailgating, and vishing. In Industry 5.0, due to the expanded\nhuman-machine collaboration, social engineering attacks have become a serious\nconcern that needs attention [70].\n\u2022 Cloud Vulnerabilities: Cloud computing, which aims at delivering remote com-\nputing services and storage services, such as data analytics and databases, is an\nintegral part of several industry 5.0 applications [25]. For instance, the technology\ncan support industries with different types of manufacturing applications/tools,"}, {"title": "5 Intrusion Detection Systems for Cybersecurity in\nIndustry 5.0", "content": "Cybercrimes are not limited to a specific region but are global threats of breaching\ndefined access rules for an electronic system and their inflicted damages are increasing\nexponentially with the advent of new technologies. The damage can be unauthorized\naccess to the system and making it unavailable to the authorized one, stealing confi-\ndential data or some ransomware, damaging the system's functionality, or destroying\ndata integrity. Nowadays, cybersecurity is considered an initial step for every start-\ning setup where the communication devices are connected through the internet. An\nindividual system could be made secure efficiently but whenever there is a need for\nconnection to another remote system, the threat of the violation of security policies,\nthat are, Confidentiality, Integrity, and Availability (CIA), becomes prone to occur\n[72]. To circumvent these threats, numerous cybersecurity techniques have been pro-\nposed in the literature including Anti-Virus software, Firewalls, IDS, and Intrusion\nPrevention Systems (IPS). In this everlasting competition between the attackers and\nthe defenders, now the systems are coming up with very intelligent security systems\nthat sometimes outperform human-level intelligence [73].\nFurthermore, in the context of Industry 5.0, the importance of cybersecurity is\nparamount. With the increasing interconnectedness of systems and the rise of automa-\ntion, the risk of cyber-attacks targeting industrial systems has grown significantly.\nCybersecurity in Industry 5.0 is not only about protecting data and systems from"}, {"title": "5.1 Intrusion Detection System (IDS)", "content": "In the literature, the conventional method for the IDS problem is tackled by Signature-\nbased Intrusion Detection Systems (S-IDS) and Anomaly-based Intrusion Detection\nSystems (A-IDS). In S-IDS, the new pattern is just matched with the previously known\nattack patterns, also called Knowledge-based Detection [76, 77]. These techniques\nobey the idea of building a database of intrusion instances signatures and with this\ndatabase, each new instance's signature is matched, as shown in the upper part of\nFigure 3. This kind of detection technique failed to detect zero-day attacks, also the\npolymorphic and metamorphic techniques introduced in malware make it hard for IDS\nto identify the same malware with different shapes."}, {"title": "5.2 Explainable IDS (X-IDS)", "content": "From the above discussion, it becomes clear that most of the intelligent intrusion\ndetection systems are based on complex ML techniques that perform very well in\nintrusion detection. However, another aspect of IDSs that needs to be considered\nin the design of such systems is transparency in the decision-making process. The\nsystem developer should know the answer to \"Why\" or \"How\" questions about the\nIDS model, to develop a more reliable, secure, and useful solution for the security\nproblems (Figure 4). The explanation in the IDS system could be an explanation for\nalerts generated by IDS, also a reason for a decision made either anomaly or benign,\nalso provide an indicator of compromise for a security analyst in operation center\n[68, 86]. The need and usefulness of explanation in security systems were first proposed\nby Vigan et al [87], by highlighting the need for explanations to better understand\nthe core functionality of the intelligent system in terms of the \"Six Ws\" paradigm.\nThey described the importance and enhancement of the security system by providing\nanswers to these Ws, which are \"Who\", \"What\", \"Where\", \"When\", \"Why\" and\n\"How\".\nThe explainable and interpretable IDS concept in Industry 5.0 takes on heightened\nsignificance as organizations seek to effectively address emerging cyber threats while\nmaintaining transparency and interpretability in their security measures. Explainabil-\nity in IDSs represents a collaborative effort between AI systems and human operators\nto address technical challenges at both the model-level implementation and opera-\ntional levels, enhancing the system's ability to detect and respond to threats. This\ncollaborative approach empowers IDSs to transcend the limitations of black box mod-\nels by integrating fundamental knowledge and insights, thereby enabling interpretable\ndecision-making processes [26, 75].\nAs part of ongoing research and development efforts within the cybersecurity\ncommunity, traditional intelligent intrusion detection systems are undergoing revi-\nsions to incorporate explainability features tailored to diverse stakeholder perspectives\n[135, 136]. These revisions have led to the categorization of explainability into\nthree distinct domains: self-model explainability, pre-modeling explainability, and\npost-modeling explainability. Self-model explainability encompasses the generation of\nexplanations and predictions in tandem, leveraging problem-specific insights derived\nfrom domain expert knowledge. Pre-modeling explainability involves the utilization\nof refined attribute sets, facilitating clearer interpretations of system behavior before\nmodel training. Lastly, post-modeling explainability focuses on shaping the behav-\nior of trained models to enhance their responsiveness to input-output relationships,\nthereby improving overall system transparency and efficacy in the dynamic landscape\nof Industry 5.0 cybersecurity [137]. These types of explanations are discussed in more\ndetail below and their examples are summarized"}, {"title": "5.2.1 Self-model Explainability", "content": "The X-IDS models generated from self-explaining models are designed to inherently\nexplain their intrinsic decision-making procedure. These models exhibit simple archi-\ntectures capable of identifying crucial attributes that trigger the decision-making\nprocess for a given input. In this way, several explainability techniques have been pro-\nposed according to the model's complexity, for instance, a Rule-based explanation has\nbeen suggested in [88] by developing an Ante-hoc explainability application named\nNEDAA system that combines ML methods like genetic algorithms and decision trees\nto aid intrusion detection experts by generating rules, for classifying normal net-\nwork connections from an anomalous one, based on domain knowledge. The NEDAA\napproach employs analyst-designed training sets to develop rules for intrusion detec-\ntion and decision support. In [89], the authors tried to explain and interpret the known\nattacks in the form of rules to highlight the target of the attack and their causal rea-\nsoning using Decision Trees. They utilized ID3 to construct a decision tree, using the\nKDD-99 dataset, where the decision rules traverse from top to bottom nodes and the\nrules from the model are generated using Rattle package from R statistical language. A\nrecent work [90, 91], proposed decision tree-based explainability to explain the actions\ntaken by the Industrial control system against IoT network activities. These rules can\nbe compiled into an expert system for detecting intrusive events or to simplify train-\ning data into concise rule sets for analysts. The rule-based explanation offers valuable\ninsights into decision-making, promotes transparency, and allows domain expertise\nintegration. However, they have limitations in handling complex and evolving threats,\nscalability, and potential conflicts [136].\nAnother recent Host-based Intrusion Detection System (HIDS) has been proposed\nby Yang et al. [92], where they used Bayesian Networks (BNs) to create a self-\nexplaining hybrid detection system by combining data-driven training with expert\nknowledge. BNs are a specific type of Probabilistic Graphical Models (PGMs) that\nmodels the probabilistic relationships among variables using Bayes' Rule [138]. Firstly,\nthey extract expert-informed interpretable features from two datasets, Tracer FIRE\n9 (TF9) and Tracer FIRE 10 (TF10), which consist of normal and suspect system\nevents logs generated through Zeek and Sysmon by the Sandia National Laboratories\n(SNL) Tracer FIRE team. The authors utilized Bayes Server (2021) as an engine for\nevaluating multiple BN architectures in finding the best-performing model while the\nexplanations are provided by visualizing the network graph, which provides feature\nimportance information via conditional probability tables [92]. Self-explaining models\nin IDS offer notable advantages by enhancing transparency and interpretability. They\nprovide insights into decision-making, enabling analysts to understand the reasoning\nbehind alerts. This aids in trust-building, model validation, and effective response.\nHowever, self-explaining models might struggle with complex relationships, limiting\ntheir capacity to capture nuanced attack patterns."}, {"title": "5.2.2 Pre-modeling Explainability", "content": "Pre-modeling explainability techniques involve some preprocessing methods to sum-\nmarize large featured datasets into an information-centric set of attributes that align"}, {"title": "5.2.3 Post-model Explainability", "content": "Post-model explainability refers to the techniques and methods used to interpret and\nunderstand the decisions made by a trained learning model. Unlike self-and pre-model\nexplainability techniques, post-model allows stakeholders to gain insights into model\ndecisions, detect biases, and validate model behavior, contributing to better-informed\ndecision-making and building trust in AI systems [140]. The most adopted techniques\nin the literature include the feature importance methods, where the impact of each\ninput feature is analyzed according to the trained model's performance. In [115], the\nauthors used the SHAP method to explain the ML model detection performance.\nAfter finding the best-performing sets of hyper-parameters for both the MLP and\nRF classifiers through partial grid search, the models are analyzed to understand\ntheir internal operations by calculating the Shapley value of the features. The LIME\nexplainability approach for detecting adversarial attacks on IDS systems has been\nproposed in [141], where the normal data boundaries are explained for a trained SVM-based model. In the same way, in [142], horizontal bar plots are used to visualize the\nglobal explanation of the model prediction using the SHAP mechanism. Another work\nby Oseni et al. [59], also proposed a SHAP mechanism for improving interpretation\nand resiliency of the DL-based IDS in IoT networks. The SHAP mechanism has also\nbeen proposed by Alani et al. [116, 117] and Kalutharage et al. [118], to explain a\nDeep-learning-based Industrial IoT (DeepIIoT) intrusion detection system.\nAlong with employing LIME and SHAP mechanisms to explain the prediction\nmade by the Extreme Gradient Boosting (XG-Boost) classifier, the authors in [119]\nalso used ELI5, \"Explain Like I'm 5\", a python package using the interpreting Ran-\ndom Forest feature weights approach. This package supports tree-based explanation to\nshow how effective each feature is contributing on all parts of the tree in the final pre-\ndiction. Abou et al. [120] used RuleFit and SHAP mechanisms to explore the local and\nglobal interpretations for DL-based IDS models. In [8], an adversarial ML approach\nis utilized to find explanation for an input features. They used the samples that are\nincorrectly predicted by the trained model and tried again with the required minimum"}, {"title": "6 Adversarial \u03a7\u0391\u0399 and IDS", "content": "The adaptability of DL-based systems in cybersecurity within the context of Industry\n5.0 is not yet mature compared to other domains, such as instance recognition sys-\ntems, recommender systems for business and social platforms, etc. There are two main\nreasons. The first one is the lack of confidence due to the black-box nature of these\nintelligent models [147]. To aim, efforts are made for explainable AI systems, where a\nmodel can reason their decision by providing a reasonable explanation. This resulted\nin the recent trend of white-box AI models by generating model-specific and model-\nagnostic explanation techniques [148] [34]. The second reason, which is the focus of\nthis section, is the Adversarial methodologies, where an intelligent model is made fool\npurposely by generating adversarial examples that the model can not predict correctly\n[149]. These kinds of attacks exist for both the black-box and white-box models, where\nthe factual and counterfactual explainability generated by the XAI methods make\nthis adaptability more insecure and provide an easy way to launch various adversarial\nattacks on the models, such as feature-manipulation/perturbation,\nDecision Rules-manipulation, evasion, membership inference, model poisoning, or\nvulnerability extraction attacks [150].\nExplainability in cybersecurity plays a double-edged sword and very little work has\nbeen done to enhance the robustness of explainable models. From the above X-IDS\nsection 5, and also from Table 2, 3, 4, it can be seen that the prominent competing"}, {"title": "6.1 Adversarial Attacks without utilizing Explainability", "content": "Adversarial attacks on ML can be categorized into three types: white-box attacks,\nwhere the attacker has complete knowledge of the target system; black-box attacks,\nwhere the attacker lacks knowledge about the detection mechanism but can query\nthe model to gain information; and gray-box attacks, where the attacker has lim-\nited information about the classifier, such as some features or the algorithm used\nwithout configuration details [178]. In terms of offensive use of inference models, the\npossible attack vectors include privacy attacks (e.g., membership inference, model\ninversion, and model extraction), poisoning attacks (e.g., backdoor injection), and eva-\nsion attacks (e.g., test-time adversarial perturbations). The most easy and effective\nway of attacks involve perturbation and evasion mechanisms in the context of machine"}, {"title": "6.2 Adversarial Attacks with utilizing Explainability", "content": "Some of the XAI methods have been deployed to comprehend, identify, and defend\nagainst specific adversarial attacks. Most of the employed approaches involve develop-\ning visualizations to highlight the regions vulnerable to changes or could most likely\nbe altered by adversaries. In [141, 188], the LIME technique is employed to anticipate\nthe attacks and normal traffic data. The method involves finding the most impor-\ntant feature sets for normal traffic and using those features as a comparison set for\nthe detected normal traffic to validate the model's decision. Though these approaches\nwork well in identifying potential vulnerabilities in the model, the explanation meth-\nods are still incapable of detecting various adversarial attacks and can be manipulated\nto affect the user's trust or be exploited to launch different attacks.\nThe trade-off between X-IDSs and adversarial attacks is complex and multifaceted,\nwhich is why some most popular explainability mechanisms are used as counterin-\ntuitive. Exploiting XAI's enhanced capabilities in the lens of classical CIA triad,\nConfidentiality attacks employ explanations to reveal model architecture or train-\ning data while Integrity and Availability attacks use explanations to uncover insights\nenabling adversaries to manipulate model results or disrupt access to legitimate users\n[189]. These attacks can occur during training (e.g., poisoning) or deployment (e.g.,\nevasion), depending on the attacker's strategy, timing, and objectives.\nThe advantage of explainability in highlighting the critical decision boundaries\ncould also be exploited by adversaries. For instance, In [165], the authors introduced\nthe concept of transferability of explainability, a similar concept of adversarial exam-\nple transferability, where the impactful features are identified utilizing explainability\nalgorithms on a substitute model with the assumptions that their impact will be same\non any target black-box model. Specifically, they used Kendall's tau, a statistical mea-\nsure (often used for comparing rankings), to compare the rankings of features produced\nby different explainability mechanisms. Being unaware of the attacked classifier, this\ntransferability of explainability can help the adversary in generating adversarial exam-\nples by modifying some structural features without affecting their core functionality.\nA recent work [166] targets the same concept of transferability by proposing a novel\nmethod named Explainable Transfer-based black-box adversarial Attack (ETA) frame-\nwork. The ETA framework optimizes a substitute model, selects important sensitive\nfeatures, and crafts adversarial examples guided by gradient evaluation, enhancing\ntransferability against target NIDSs.\nIn [167], the authors proved the possibility of compromising the confidentiality\nof the target classifiers and their explainability mechanisms. The work employs the\nManifold Approximation Algorithm (MAA) to discern patterns in data distributions,\nsubsequently generating synthetic data boundaries. These boundaries are then per-\nturbed to probe an ML classifier, and explanations are obtained to understand its\nbehavior in a black-box setting. Kuppa et al. [168] presented four successful black-box attacks including evasion, membership inference, poisoning, and model extraction"}, {"title": "7 XAI-based IDS: Lessons Learned, Challenges and\nFuture Research Directions", "content": "The emerging concept of Industry 5.0 has received great attention from the research\ncommunity. It includes Human-Machine Interaction, Cyber-physical systems, Robotics\nand Automation, Industrial Internet of Things (IIoT), and Big Data Analytics using AI\nand ML. Within this spectrum of concepts, the privacy and security of the information\nexchange systems assume a vital role, influencing the establishment of trust among\ndiverse stakeholders and facilitating the adoption of these technological transitions.\nIn this scientific review, we tried to cover the recent advancement in cybersecu-\nrity specifically targeting a comprehensive survey on IDSs with the advancement of\nexplainable IDS. Current X-IDSs have significantly improved the interpretability and\ntransparency of AI-based intrusion detection and prevention systems. These mecha-\nnisms aim at uncovering the black-box nature of the AI-based models and showed\nfruitful progress from different stakeholders' perspectives. However, they still face some critical limitations including complexity, scalability,"}, {"title": "7.1 Challenges in Developing XAI-Based IDS Systems and the\nWay Forward", "content": "The development of Explainable Intrusion Detection Systems (X-IDSs) poses multi-\nple challenges mainly stemming from the inherent complexity of modern information\nexchange protocols, network architectures, and the sophisticated nature of cyber\nattacks in the Industry 5.0 realm. One significant challenge is coping with the com-\nplexity of deep learning models employed in X-IDS. While these models demonstrate\nexceptional performance, their opacity poses a hurdle in terms of interpretation.\nExtracting meaningful explanations from these complex models demands sophisticated\ntechniques, often requiring a trade-off between model accuracy and interpretability.\nDue to the simplicity and incapability of ante-hoc explainability mechanisms to\neffectively capture the morphed patterns of modern cyber threats, the current litera-\nture is focused on finding the relevancy between the input sample features and their\noutput by adopting different Post-hoc mechanisms. The majority of these methods are\nmostly rooted in image processing, and their applicability in AI-based cybersecurity\nproblems doesn't fit well because of the complex nature of the data, which has sequen-\ntial patterns, and textual and categorical features. These simple feature attribution\nand saliency mapping mechanisms can not accurately capture the complex relation-\nship within the data. Moreover, the integration of contextual information, such as user\nbehavior and network context, adds another layer of complexity. Despite the significant\nefforts in the domain, the question about \" How an explainability mechanism \u0441\u0430\u043f"}, {"title": "8 Conclusion", "content": "The obscure nature of the complex AI methods raises the need for in-depth evaluation\nof the decisions made by the Deep Learning models. X-AI has recently introduced\nthe concept of White-box models, allowing the interpretation of internal information\nand decisions in AI-based systems. Similar to other application areas cybersecurity\nprofessionals are reluctant to black-box ML-based cybersecurity solutions. Keeping\nthemselves one step ahead of the attacker, it is essential for the security analyst to be\naware of the internal automatic decision mechanism of the diploid intelligent model\nand to precisely reason the input data about the model's outputs. The application of\nX-AI in cybersecurity could also be a double-edged sword, that is, besides improving\nsecurity practices, it could also make the intelligent explainable model vulnerable\nto adversary attacks. This survey provides a comprehensive examination of various\nXAI-based IDS approaches, evaluating their impact on cybersecurity practices.\nThe study reveals that different stakeholders in ML-based IDS acquire varying\nlevels and types of interpretability for decision models, with a predominant focus\non feature attribution and saliency mapping to comprehend their impact on model\ndecisions. However, a gap exists in understanding the causality and sensitivity effects of\nattributes in model interpretability. While explanations often showcase the importance\nof different features, provide meaningful example inputs, visualize decision boundaries,"}, {"title": "9 Acknowledgment", "content": "This work was supported by National Priorities Research Program (NPRP) under\nGrant NPRP13S-0212-200345 from the Qatar National Research Fund (a mem-\nber of Qatar Foundation). The findings herein reflect the work and are solely the\nresponsibility of the authors."}, {"title": "Author Information", "content": "Naseem Khan received a bachelor's degree in telecommunication\nfrom Hazara University, Mansehra, Pakistan, and master's degrees in\nCyber Security from COMSATS University, Abbottabad, Pakistan,\nin 2016 and"}]}