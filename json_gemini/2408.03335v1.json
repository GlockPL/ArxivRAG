{"title": "Explainable AI-based Intrusion Detection System for Industry 5.0: An Overview of the Literature, associated Challenges, the existing Solutions, and Potential Research Directions", "authors": ["Naseem Khan", "Kashif Ahmad", "Aref Al Tamimi", "Mohammed M. Alani", "Amine Bermak", "Issa Khalil"], "abstract": "Industry 5.0, which focuses on human and Artificial Intelligence (AI) collaboration for performing different tasks in manufacturing, involves a higher number of robots, Internet of Things (IoTs) devices and interconnections, Augmented/Virtual Reality (AR), and other smart devices. The huge involvement of these devices and interconnection in various critical areas, such as economy, health, education and defense systems, poses several types of potential security flaws. AI itself has been proven a very effective and powerful tool in different areas of cybersecurity, such as intrusion detection, malware detection, and phishing detection, among others. Just as in many application areas, cybersecurity professionals were reluctant to accept black-box ML solutions for cybersecurity applications. This reluctance pushed forward the adoption of eXplainable Artificial Intelligence (XAI) as a tool that helps explain how decisions are made in ML-based systems. In this survey, we present a comprehensive study of different XAI-based intrusion detection systems for industry 5.0, and we also examine the impact of explainability and interpretability on Cybersecurity practices through the lens of Adversarial XIDS (Adv-XIDS) approaches. Furthermore, we analyze the possible opportunities and challenges in XAI cybersecurity systems for industry 5.0 that elicit future research toward XAI-based solutions to be adopted by high-stakes industry 5.0 applications. We believe this rigorous analysis will establish a foundational framework for subsequent research endeavors within the specified domain.", "sections": [{"title": "1 Introduction", "content": "The growing applications of Artificial Intelligence (AI) and Machine Learning (ML) have increased the need for a better understanding of AI-based solutions for smart industries, especially in Industry 5.0 applications. Similar to other sensitive application domains, such as business, healthcare, education, and defense systems, the enigmatic and obscure nature of AI raises concerns and the need for in-depth evaluation of the decisions made by these black box models in smart industries [1]. In addition to issues of user rights, and intelligent technology acceptance, developers of these systems need to ensure the fair and unbiased nature of their solutions. The need to comprehend and interpret the causal understanding of inferences made by Deep ML models, directed the attention of the research community towards \u03a7\u0391\u0399 [2]. In this regard, the first DARPA-funded Explainable AI (XAI) initiative started with the aims to develop interpretable machine learning models for reliable and human-trusted decision-making systems, crucial for the integration of Internet-of-Things (IoT) and intelligent systems in Industry 5.0 [3-6].\nCybersecurity is one of the critical aspects of smart industries involving a high number of interconnected devices. Similar to other applications, AI-based solutions have been proven very effective in the cybersecurity of smart industries. However, the opacity of the diploid complex AI-based models in various Cybersecurity solutions, such as Intrusion Detection Systems (IDS), Malware detection and classification systems, finding Zero-Day vulnerabilities, and Digital Forensics, exacerbates the trust lack of transparency [5, 7]. To keep themselves one step ahead of attackers, it is also essential for security analysts to know the internal automatic decision mechanism of the deployed intelligent model and to precisely reason the input data about the model's outputs. The application of XAI in cybersecurity could also be a double-edged sword, that is, besides improving the security practices, it could also make the intelligent explainable model vulnerable to adversary attacks [8-10]. Thus, the integration of human understanding and AI-based security systems needs to be keenly analyzed to provide a clear perception for future research."}, {"title": "1.1 Scope of the survey", "content": "This survey focuses on highlighting the critical challenges the security practitioners are being confronted with (i.e., the integration of successful security and defense measures in high-risk cyber-physical systems) in Industry 5.0 applications. Figure 1 provides a visual representation of evaluation of industry 5.0 highlighting the trajectory of smart industry development underscored by the imperative for robust cybersecurity measures in the face of escalating cyber attacks [21-23]. This heightens the scope of the paper and the need to address this issue by critically analyzing the research trends in cybersecurity for smart industries. The paper mainly focuses on exploring the impact of explainability and interpretability concepts on cybersecurity practices."}, {"title": "1.2 Related Surveys", "content": "The continuous evolution of industrial paradigms has introduced transformative goals, emphasizing the creation of a resource-efficient and intelligent society. This trajectory seeks to elevate living standards and mitigate economic disparities through the integration of a hyper-connected, automated, data-driven industrial ecosystem [24-26]. This digital transformation promises to significantly enhance productivity and efficiency across the entire production process. These milestones become possible through the integration of AI/ Generative AI as a collaborative landscape, fostering innovation, optimizing resource utilization, and driving economic growth in smart industries [27]. However, it is imperative to acknowledge that such advancements expose the system to an elevated risk of sophisticated cyber-attacks [28]. The connected devices and networks of the autonomous industry infrastructure are more prone to hijacking, malfunctioning, and resource misuse threats, which necessitates extra security layers to safeguard from such threats. The conventional deployed security measures, that are, AI-based cybersecurity systems are still in progress to mature, and the need for developing robust and trustworthy security systems has become a trending goal for defenders to achieve [29, 30].\nIn this essence, the need to comprehend and interpret the causal understanding of inferences made by AI-based learning models directed the attention of the research community toward the XA\u0399 research field. In the literature, the taxonomy transition of XAI has been evaluated based on trust building for human-machine interaction"}, {"title": "1.3 Contributions", "content": "Based on serious threat vectors and their implications, in this paper, we analyze the adoption of different XAI methods in IDSs and examine the impact of interpretability on Cybersecurity practices in the Industry 5.0 applications. In detail, we provide an overview of the literature on XAI-based cybersecurity solutions for Industry 5.0 applications with a particular focus on existing solutions, associated challenges, and future research directions to overcome these challenges. To make it self-contained, we also provide an overview of the taxonomy of XAI. The main contributions of this paper are summarized as follows:\n\u2022 We provide a clear and comprehensive taxonomy of XAI systems.\n\u2022 We provide a detailed overview of current state-of-the-art IDS, their limitations, and the deployment of XAI approaches in IDSs.\n\u2022 We also discuss the exploitation of XAI methods for launching more advanced adversarial attacks on IDS.\n\u2022 We also highlight the current cybersecurity challenges and potential solutions to ensure the safety and security of industry 5.0 applications.\nThe rest of the paper is organized as follows. Section 2 presents the methodology adopted for conducting this survey by briefly describing the objective questions of this survey. Section 3 provides an overview of the eXplainable AI taxonomies. Section 4, presents cybersecurity challenges in Industry 5.0. Section 5 presents conventional IDS and the evalutions of the systems from AI-based IDS to XAI-based IDS. This section also cover different type of explainability mechanisms, specifically Self-model, Pre-model and Post-modeling explainability techniques. Section 6 presents adversarial XAI techniques in cybersecurity with the focus on exploring the exploitation of"}, {"title": "2 Methodology", "content": "In the context of Industry 5.0, our focus was on investigating X-IDS to explore cybersecurity solutions and challenges in the forthcoming industry revolution. Our objective was to analyze the array of approaches and techniques, particularly those utilizing big data and advanced analytics, to bolster security outcomes. This endeavor involved a thorough review of existing research and developments in the cybersecurity domain, specifically targeting Intrusion Detection and Prevention Systems (IDPS). Our research methodology included a systematic examination of academic papers, industry reports, and relevant literature from various sources to identify key trends, methodologies, and emerging practices related to Explainable IDS. We also critically evaluated these methodologies to determine their effectiveness in providing transparency and comprehensibility within the secure Industry 5.0 framework with"}, {"title": "3 Explainable AI Taxonomies", "content": "The field of intelligent AI-based learning methods has evolved significantly, reaching a stage where a substantial portion of critical decisions relies on predictions from trained models. However, there exists a realm of intelligence where machines must justify their decisions in response to questions like \"Why,\" \"What,\" or \"How.\" In simple words, a decision model should explain their decision in such a way that it could be acceptable with no dough, understandable with no difficulty and could be reliable to enhance trust between users and technology [1, 2].\nThis pursuit of interpretability and explainability goal in AI coined the term, eXplainable AI (XAI), in the research community, which embodies the idea of developing understandable AI models that are consistent with expert knowledge. The intuition behind XAI is rooted in the concept that humans should be able to comprehend and trust the outputs and recommendations provided by AI systems. XAI aims to bridge the gap between the inherent complexity of AI algorithms and human understanding by providing transparent and interpretable explanations for the AI model's outputs [20, 42]. A presentation for model's decision in a textual (Natural Language"}, {"title": "3.1 Ante-hoc explainability", "content": "These are a kind of models that are self-explaining or can be called interpretable by design. Within this kind of transparency, these models could be interpreted in three levels including the model's algorithmic level transparency, parametric level decomposability, and functioning level simulatability. Typical examples are linear regression, logistic regression, decision trees, random forests, Na\u00efve Bayes and fuzzy inference, rule-based learning [46, 47]. The following examples very well meet the criteria of decision explainability, but these models perform very poorly on high-dimensional data.\nFrom the Linear Models, for example, in Linear Regression, the prediction is simply the weighted sum of input features. The weighted sum can be utilized as a measure of explainability because the model's predicted target shows the linear relationship among the features. Additionally, the statistical measures associated with the linear regression model, such as p-values and confidence intervals, provide information about the significance and uncertainty of the coefficients. These measures can help assess the reliability and robustness of the model's explanations. Another type of regression analysis, i.e., logistic regression, represents the target prediction as an estimated probability. This probability function is the magnitude of the coefficients that interprets an intrinsic sense of how much a feature is driving a model's prediction. Interpreting the coefficients in logistic regression involves examining their sign, magnitude, and statistical significance. Positive coefficients indicate a positive influence on the probability"}, {"title": "3.2 Post-hoc explainability", "content": "The discernible reality is that many complex black-box models boast formidable predictive capabilities at the expense of limited explainability regarding their decision-making processes. In the realm of intelligent systems, accuracy, and interpretability stand out as primary characteristics. Addressing the imperative for transparency, surrogate explainers are essential, necessitating their development and application to interpret the rationale behind decisions made by sophisticated models.[47]. A prevalent and contemporary research avenue actively tackling the opacity challenge of complex black-box model families involves the utilization of post hoc explainability methods.\nTo elucidate the decision-making process of a trained model for a given input, post-hoc explainability addresses two overarching types of explanations: Local explanations and Global explanations. Local explanations are geared towards explaining how the model predicts outcomes for a specific instance, providing insights into the influence of a particular individual instance on a specific class. This granularity permits users to scrutinize the model's decision-making process at a detailed level [52]. Conversely, global explanations aim to assess the impact of all input features on the model's overall output. Through global explanations, one can comprehend how a model learns"}, {"title": "4 Cybersecurity Challenges in Industry 5.0", "content": "Industry 5.0, which is mainly focused on the collaboration of humans and machines for different tasks, involves the integration of several technologies, such as AI, data analytics, IoT, augmented and virtual reality, and improved man-machine interfaces (MMI) allowing workers to carry out different operations. This enhanced interconnectivity exposes smart industries to a diversified set of cybersecurity challenges and threats, which may lead to a disastrous operational environment, putting workers at risk and halting production. Some of the most common cybersecurity threats to Industry 5.0 include:\n\u2022 Expanded Attack Surface: The enhanced interconnectivity has significantly increased the number of entry points for cyber attacks in Industry 5.0, making it more challenging timely detect and guard against different cyber attacks [69]. For instance, industries need to collect and analyze data for different tasks, such as customer behavior, optimization of marketing campaigns, better supply chain management, and predictive maintenance of the machines, etc. However, this data could be used by attackers for malicious activities. Thus, stricter access control and data management policies and techniques need to be incorporated to ensure the data is used for improvement purposes.\n\u2022 Social Engineering: Social engineering, which exploits human mistakes/errors instead of technical vulnerabilities, is one of the major cybersecurity threats in the modern world. In recent years, social engineering tactics emerged as one of the most effective ways of obtaining sensitive information. Some of the common cybersecurity threats based on social engineering tactics include phishing, baiting, pretexting, malware, tailgating, and vishing. In Industry 5.0, due to the expanded human-machine collaboration, social engineering attacks have become a serious concern that needs attention [70].\n\u2022 Cloud Vulnerabilities: Cloud computing, which aims at delivering remote computing services and storage services, such as data analytics and databases, is an integral part of several industry 5.0 applications [25]. For instance, the technology can support industries with different types of manufacturing applications/tools,"}, {"title": "5 Intrusion Detection Systems for Cybersecurity in Industry 5.0", "content": "Cybercrimes are not limited to a specific region but are global threats of breaching defined access rules for an electronic system and their inflicted damages are increasing exponentially with the advent of new technologies. The damage can be unauthorized access to the system and making it unavailable to the authorized one, stealing confidential data or some ransomware, damaging the system's functionality, or destroying data integrity. Nowadays, cybersecurity is considered an initial step for every starting setup where the communication devices are connected through the internet. An individual system could be made secure efficiently but whenever there is a need for connection to another remote system, the threat of the violation of security policies, that are, Confidentiality, Integrity, and Availability (CIA), becomes prone to occur [72]. To circumvent these threats, numerous cybersecurity techniques have been proposed in the literature including Anti-Virus software, Firewalls, IDS, and Intrusion Prevention Systems (IPS). In this everlasting competition between the attackers and the defenders, now the systems are coming up with very intelligent security systems that sometimes outperform human-level intelligence [73].\nFurthermore, in the context of Industry 5.0, the importance of cybersecurity is paramount. With the increasing interconnectedness of systems and the rise of automation, the risk of cyber-attacks targeting industrial systems has grown significantly. Cybersecurity in Industry 5.0 is not only about protecting data and systems from"}, {"title": "5.1 Intrusion Detection System (IDS)", "content": "In the literature, the conventional method for the IDS problem is tackled by Signature-based Intrusion Detection Systems (S-IDS) and Anomaly-based Intrusion Detection Systems (A-IDS). In S-IDS, the new pattern is just matched with the previously known attack patterns, also called Knowledge-based Detection [76, 77]. These techniques obey the idea of building a database of intrusion instances signatures and with this database, each new instance's signature is matched, as shown in the upper part of Figure 3. This kind of detection technique failed to detect zero-day attacks, also the polymorphic and metamorphic techniques introduced in malware make it hard for IDS to identify the same malware with different shapes."}, {"title": "5.2 Explainable IDS (X-IDS)", "content": "From the above discussion, it becomes clear that most of the intelligent intrusion detection systems are based on complex ML techniques that perform very well in intrusion detection. However, another aspect of IDSs that needs to be considered in the design of such systems is transparency in the decision-making process. The system developer should know the answer to \"Why\" or \"How\" questions about the IDS model, to develop a more reliable, secure, and useful solution for the security problems (Figure 4). The explanation in the IDS system could be an explanation for alerts generated by IDS, also a reason for a decision made either anomaly or benign, also provide an indicator of compromise for a security analyst in operation center [68, 86]. The need and usefulness of explanation in security systems were first proposed by Vigan et al [87], by highlighting the need for explanations to better understand the core functionality of the intelligent system in terms of the \"Six Ws\" paradigm. They described the importance and enhancement of the security system by providing answers to these Ws, which are \"Who\", \"What\", \"Where\", \"When\", \"Why\" and \"How\".\nThe explainable and interpretable IDS concept in Industry 5.0 takes on heightened significance as organizations seek to effectively address emerging cyber threats while maintaining transparency and interpretability in their security measures. Explainability in IDSs represents a collaborative effort between AI systems and human operators to address technical challenges at both the model-level implementation and operational levels, enhancing the system's ability to detect and respond to threats. This collaborative approach empowers IDSs to transcend the limitations of black box models by integrating fundamental knowledge and insights, thereby enabling interpretable decision-making processes [26, 75].\nAs part of ongoing research and development efforts within the cybersecurity community, traditional intelligent intrusion detection systems are undergoing revisions to incorporate explainability features tailored to diverse stakeholder perspectives [135, 136]. These revisions have led to the categorization of explainability into three distinct domains: self-model explainability, pre-modeling explainability, and post-modeling explainability. Self-model explainability encompasses the generation of explanations and predictions in tandem, leveraging problem-specific insights derived from domain expert knowledge. Pre-modeling explainability involves the utilization of refined attribute sets, facilitating clearer interpretations of system behavior before model training. Lastly, post-modeling explainability focuses on shaping the behavior of trained models to enhance their responsiveness to input-output relationships, thereby improving overall system transparency and efficacy in the dynamic landscape of Industry 5.0 cybersecurity [137]. These types of explanations are discussed in more detail below and their examples are summarized"}, {"title": "5.2.1 Self-model Explainability", "content": "The X-IDS models generated from self-explaining models are designed to inherently explain their intrinsic decision-making procedure. These models exhibit simple architectures capable of identifying crucial attributes that trigger the decision-making process for a given input. In this way, several explainability techniques have been proposed according to the model's complexity, for instance, a Rule-based explanation has been suggested in [88] by developing an Ante-hoc explainability application named NEDAA system that combines ML methods like genetic algorithms and decision trees to aid intrusion detection experts by generating rules, for classifying normal network connections from an anomalous one, based on domain knowledge. The NEDAA approach employs analyst-designed training sets to develop rules for intrusion detection and decision support. In [89], the authors tried to explain and interpret the known attacks in the form of rules to highlight the target of the attack and their causal reasoning using Decision Trees. They utilized ID3 to construct a decision tree, using the KDD-99 dataset, where the decision rules traverse from top to bottom nodes and the rules from the model are generated using Rattle package from R statistical language. A recent work [90, 91], proposed decision tree-based explainability to explain the actions taken by the Industrial control system against IoT network activities. These rules can be compiled into an expert system for detecting intrusive events or to simplify training data into concise rule sets for analysts. The rule-based explanation offers valuable insights into decision-making, promotes transparency, and allows domain expertise integration. However, they have limitations in handling complex and evolving threats, scalability, and potential conflicts [136].\nAnother recent Host-based Intrusion Detection System (HIDS) has been proposed by Yang et al. [92], where they used Bayesian Networks (BNs) to create a self-explaining hybrid detection system by combining data-driven training with expert knowledge. BNs are a specific type of Probabilistic Graphical Models (PGMs) that models the probabilistic relationships among variables using Bayes' Rule [138]. Firstly, they extract expert-informed interpretable features from two datasets, Tracer FIRE 9 (TF9) and Tracer FIRE 10 (TF10), which consist of normal and suspect system events logs generated through Zeek and Sysmon by the Sandia National Laboratories (SNL) Tracer FIRE team. The authors utilized Bayes Server (2021) as an engine for evaluating multiple BN architectures in finding the best-performing model while the explanations are provided by visualizing the network graph, which provides feature importance information via conditional probability tables [92]. Self-explaining models in IDS offer notable advantages by enhancing transparency and interpretability. They provide insights into decision-making, enabling analysts to understand the reasoning behind alerts. This aids in trust-building, model validation, and effective response. However, self-explaining models might struggle with complex relationships, limiting their capacity to capture nuanced attack patterns."}, {"title": "5.2.2 Pre-modeling Explainability", "content": "Pre-modeling explainability techniques involve some preprocessing methods to summarize large featured datasets into an information-centric set of attributes that align"}, {"title": "6 Adversarial \u03a7\u0391\u0399 and IDS", "content": "The adaptability of DL-based systems in cybersecurity within the context of Industry 5.0 is not yet mature compared to other domains, such as instance recognition systems, recommender systems for business and social platforms, etc. There are two main reasons. The first one is the lack of confidence due to the black-box nature of these intelligent models [147]. To aim, efforts are made for explainable AI systems, where a model can reason their decision by providing a reasonable explanation. This resulted in the recent trend of white-box AI models by generating model-specific and model-agnostic explanation techniques [148] [34]. The second reason, which is the focus of this section, is the Adversarial methodologies, where an intelligent model is made fool purposely by generating adversarial examples that the model can not predict correctly [149]. These kinds of attacks exist for both the black-box and white-box models, where the factual and counterfactual explainability generated by the XAI methods make this adaptability more insecure and provide an easy way to launch various adversarial attacks on the models, shown in Figure 5, such as feature-manipulation/perturbation, Decision Rules-manipulation, evasion, membership inference, model poisoning, or vulnerability extraction attacks [150].\nExplainability in cybersecurity plays a double-edged sword and very little work has been done to enhance the robustness of explainable models. From the above X-IDS section 5, and also from Table 2, 3, 4, it can be seen that the prominent competing"}, {"title": "6.1 Adversarial Attacks without utilizing Explainability", "content": "Adversarial attacks on ML can be categorized into three types: white-box attacks, where the attacker has complete knowledge of the target system; black-box attacks, where the attacker lacks knowledge about the detection mechanism but can query the model to gain information; and gray-box attacks, where the attacker has limited information about the classifier, such as some features or the algorithm used without configuration details [178]. In terms of offensive use of inference models, the possible attack vectors include privacy attacks (e.g., membership inference, model inversion, and model extraction), poisoning attacks (e.g., backdoor injection), and evasion attacks (e.g., test-time adversarial perturbations). The most easy and effective way of attacks involve perturbation and evasion mechanisms in the context of machine"}, {"title": "6.2 Adversarial Attacks with utilizing Explainability", "content": "Some of the XAI methods have been deployed to comprehend, identify, and defend against specific adversarial attacks. Most of the employed approaches involve developing visualizations to highlight the regions vulnerable to changes or could most likely be altered by adversaries. In [141, 188], the LIME technique is employed to anticipate the attacks and normal traffic data. The method involves finding the most important feature sets for normal traffic and using those features as a comparison set for the detected normal traffic to validate the model's decision. Though these approaches work well in identifying potential vulnerabilities in the model, the explanation methods are still incapable of detecting various adversarial attacks and can be manipulated to affect the user's trust or be exploited to launch different attacks.\nThe trade-off between X-IDSs and adversarial attacks is complex and multifaceted, which is why some most popular explainability mechanisms are used as counterintuitive. Exploiting XAI's enhanced capabilities in the lens of classical CIA triad, Confidentiality attacks employ explanations to reveal model architecture or training data while Integrity and Availability attacks use explanations to uncover insights enabling adversaries to manipulate model results or disrupt access to legitimate users [189]. These attacks can occur during training (e.g., poisoning) or deployment (e.g., evasion), depending on the attacker's strategy, timing, and objectives.\nThe advantage of explainability in highlighting the critical decision boundaries could also be exploited by adversaries. For instance, In [165], the authors introduced the concept of transferability of explainability, a similar concept of adversarial example transferability, where the impactful features are identified utilizing explainability algorithms on a substitute model with the assumptions that their impact will be same on any target black-box model. Specifically, they used Kendall's tau, a statistical measure (often used for comparing rankings), to compare the rankings of features produced by different explainability mechanisms. Being unaware of the attacked classifier, this transferability of explainability can help the adversary in generating adversarial examples by modifying some structural features without affecting their core functionality. A recent work [166] targets the same concept of transferability by proposing a novel method named Explainable Transfer-based black-box adversarial Attack (ETA) framework. The ETA framework optimizes a substitute model, selects important sensitive features, and crafts adversarial examples guided by gradient evaluation, enhancing transferability against target NIDSs.\nIn [167], the authors proved the possibility of compromising the confidentiality of the target classifiers and their explainability mechanisms. The work employs the Manifold Approximation Algorithm (MAA) to discern patterns in data distributions, subsequently generating synthetic data boundaries. These boundaries are then perturbed to probe an ML classifier, and explanations are obtained to understand its behavior in a black-box setting. Kuppa et al. [168] presented four successful black-box attacks including evasion, membership inference, poisoning, and model extraction"}, {"title": "7 XAI-based IDS: Lessons Learned, Challenges and Future Research Directions", "content": "The emerging concept of Industry 5.0 has received great attention from the research community. It includes Human-Machine Interaction, Cyber-physical systems, Robotics and Automation, Industrial Internet of Things (IIoT), and Big Data Analytics using AI and ML. Within this spectrum of concepts, the privacy and security of the information exchange systems assume a vital role, influencing the establishment of trust among diverse stakeholders and facilitating the adoption of these technological transitions.\nIn this scientific review, we tried to cover the recent advancement in cybersecurity specifically targeting a comprehensive survey on IDSs with the advancement of explainable IDS. Current X-IDSs have significantly improved the interpretability and transparency of AI-based intrusion detection and prevention systems. These mechanisms aim at uncovering the black-box nature of the AI-based models and showed fruitful progress from different stakeholders' perspectives as shown in Table 2, 3, 4. However, they still face some critical limitations including complexity, scalability,"}, {"title": "7.1 Challenges in Developing XAI-Based IDS Systems and the Way Forward", "content": "The development of Explainable Intrusion Detection Systems (X-IDSs) poses multiple challenges mainly stemming from the inherent complexity of modern information exchange protocols, network architectures, and the sophisticated nature of cyber attacks in the Industry 5.0 realm. One significant challenge is coping with the complexity of deep learning models employed in X-IDS. While these models demonstrate exceptional performance, their opacity poses a hurdle in terms of interpretation. Extracting meaningful explanations from these complex models demands sophisticated techniques, often requiring a trade-off between model accuracy and interpretability.\nDue to the simplicity and incapability of ante-hoc explainability mechanisms to effectively capture the morphed patterns of modern cyber threats, the current literature is focused on finding the relevancy between the input sample features and their output by adopting different Post-hoc mechanisms. The majority of these methods are mostly rooted in image processing, and their applicability in AI-based cybersecurity problems doesn't fit well because of the complex nature of the data, which has sequential patterns, and textual and categorical features. These simple feature attribution and saliency mapping mechanisms can not accurately capture the complex relationship within the data. Moreover, the integration of contextual information, such as user behavior and network context, adds another layer of complexity. Despite the significant efforts in the domain, the question about \" How an explainability mechanism \u0441\u0430\u043f"}, {"title": "8 Conclusion", "content": "The obscure nature of the complex AI methods raises the need for in-depth evaluation of the decisions made by the Deep Learning models. X-AI has recently introduced the concept of White-box models, allowing the interpretation of internal information and decisions in AI-based systems. Similar to other application areas cybersecurity professionals are reluctant to black-box ML-based cybersecurity solutions. Keeping themselves one step ahead of the attacker, it is essential for the security analyst to be aware of the internal automatic decision mechanism of the diploid intelligent model and to precisely reason the input data about the model's outputs. The application of X-AI in cybersecurity could also be a double-edged sword, that is, besides improving security practices, it could also make the intelligent explainable model vulnerable to adversary attacks. This survey provides a comprehensive examination of various XAI-based IDS approaches, evaluating their impact on cybersecurity practices.\nThe study reveals that different stakeholders in ML-based IDS acquire varying levels and types of interpretability for decision models, with a predominant focus on feature attribution and saliency mapping to comprehend their impact on model decisions. However, a gap exists in understanding the causality and sensitivity effects of attributes in model interpretability. While explanations often showcase the importance of different features, provide meaningful example inputs, visualize decision boundaries,"}]}