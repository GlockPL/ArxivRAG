{"title": "Provably Overwhelming Transformer Models with Designed Inputs", "authors": ["Lev Stambler", "Seyed Sajjad Nezhadi", "Matthew Coudron"], "abstract": "We develop an algorithm which, given a trained transformer model M as input, as well as a string of tokens s of length $n_{fix}$ and an integer $n_{free}$, can generate a mathematical proof that M is \"overwhelmed\" by s, in time and space $\\tilde{O}(n_{fix}^{+}\\ n_{free})$. We say that M is \u201coverwhelmed\u201d by s when the output of the model evaluated on this string plus any additional string t, $M(s + t)$, is completely insensitive to the value of the string t whenever length(t) \u2264 $n_{free}$. Along the way, we prove a particularly strong worst-case form of \"over-squashing\" (Alon & Yahav, 2021; Barbero et al., 2024), which we use to bound the model's behavior. Our technique uses computer-aided proofs to establish this type of operationally relevant guarantee about transformer models. We empirically test our algorithm on a single layer transformer complete with an attention head, layer-norm, MLP/ReLU layers, and RoPE positional encoding. We believe that this work is a stepping stone towards the difficult task of obtaining useful guarantees for trained transformer models.", "sections": [{"title": "1. Introduction", "content": "Decoder-only transformers (Vaswani et al., 2023) have become an enormously popular paradigm in the past few years (Team, 2024; OpenAI, 2024; Li et al., 2022). Yet, our theoretical understanding of these models remains limited. Proving mathematically rigorous statements about transformers face challenges due to the high dimensionality and complexity of transformers. To circumvent these problems, current techniques either require simplifying assumption or very specific restrictions on the model/ dataset. In this work we propose a different approach focused toward producing rigorous and operationally relevant guarantees for specific, trained, transformer models. That is, we ask the question:\nCan we develop an algorithm which can provably bound the behavior of a specific trained transformer model?\nWith this motivation in mind, we design a class of algorithms which can prove when a particular type of \"over-squashing\" phenomenon occurs in a trained transformer model. Over-squashing is a known phenomenon in graph neural networks (GNNs) in which the representation of distinct nodes becomes arbitrarily close as the number of layers grows (Alon & Yahav, 2021; Barbero et al., 2024). Recently, Barbero et al. (Barbero et al., 2024) have shown that over-squashing occurs in the limit for a transformer model without positional embeddings.\nOver-squashing on its own is an important phenomenon. Yet, it does not directly constrain the model's output behavior. In this work, we focus on a specific related phenomenon, which we call \u201coverwhelming\". A model is \u201coverwhelmed\u201d by an input string if the model's output is unchanged by concatenating with any new string of tokens of a fixed length.\nDefinition 1.1 (Overwhelm, Overwhelmed, Overwhelming). For a given transformer model M, string of tokens s of length $n_{fix}$, a fixed final token $q^{1}$, and integer $n_{free}$ we say that M is \u201coverwhelmed\u201d by s if the output of the model evaluated on s plus any additional string t and fixed final token q, $M(s + t + q)$, is the same regardless of the value of the string t whenever length(t) \u2264 $n_{free}$.\nIn this work, we provide concrete algorithms (algorithm 1, algorithm 2) which can produce a proof that a given token sequence overwhelms a given model, as stated in theorem 1.2 and theorem 1.3 respectively. Our algorithms work in two stages: first we upper bound a quantity which we call worst-case deviation (denoted by W), which bounds the extent to which the logit weights output can vary. Second, we lower-bound a quantity which we call peak-to-peak difference which measures the difference in the maximum and second maximum logit weights output by the model. We thus get our first main result:\nTheorem 1.2 (Formally restated in theorem 4.5). If Algo-"}, {"title": "2. Model", "content": "In this paper, we focus on a standard single-layer transformer model which uses RoPE positional encoding, par-"}, {"title": "3. Worst-Case Deviation and a Custom Norm", "content": "When proving statements about our model, we will find it useful to talk about a variation on the Lipschitz constant and maximum absolute deviation that we term the \"worst-case deviation.\" Simply put, the worst-case deviation of a function $f: X \\rightarrow \\mathbb{R}^{n}$ is the maximum distance between the outputs of f on any two points in X.\nBefore defining the worst-case deviation more formally, we will first define the Lipschitz constant of a function.\nDefinition 3.1 (Lipschitz Constant). For a function $f: X \\rightarrow \\mathbb{R}^{n}$ and norm p, we define the Lipschitz, constant of f as\n$Lip(f)_{p} := sup_{X_{1},X_{2}\\in X}\\frac{|| f(X_{1}) - f(X_{2})||_{p}}{||X_{1} - X_{2}||_{p}}$\nDefinition 3.2 (Worst-Case Deviation). For a function $f : X \\rightarrow \\mathbb{R}^{n}$, we define the worst-case deviation of f as\n$W(f;X)_{p} := sup_{X_{1},X_{2}\\in X} || f(X_{1}) - f(X_{2})||_{p}$\nIt will be convenient to \u201clift\u201d the input space into two parts and then find bounds on the worst-case deviation by optimizing over the two parts separately.\nDefinition 3.3 (Lifted Worst-Case Deviation). For a function $f : X \\rightarrow \\mathbb{R}^{n}$ where $X \\subseteq Y \\times Z$ and f is well defined on y and Z, we define the lifted worst-case deviation of f as\n$W(f;Y \\times Z)_{p} := sup_{Y_{1},Y_{2}\\in Y} sup_{Z_{1},Z_{2}\\in Z} ||f(Y_{1}, Z_{1}) - f(Y_{2}, Z_{2})||_{p}$\nIn appendix C, we state and prove some useful properties of the worst-case deviation. Mainly, we note that W behaves mostly like a norm but has the nice property that \u201clifting\u201d the domain is monotonic. I.e. for lifted domain $X \\subset Y \\times Z$, $W(f;x) \\leq W(f;Y \\times Z)$.\nWhen discussing the worst-case deviation in this paper, we will use the$\\\\infty$-norm for vectors. For convenience, we will define an analogue of the Frobenius norm for the$\\\\infty$-norm.\nDefinition 3.4 ($F_{\\infty}$-norm). For a matrix $M \\in \\mathbb{R}^{n\\times m}$, we define the Frobenius-$\\\\infty$ norm ($F_{\\infty}$-norm) as\n$||M||_{F_{\\infty}} := max_{i\\in[n]} max_{j\\in[m]} | M_{ij}|.$"}, {"title": "4. Input Restrictions and Proving Overwhelming", "content": "In this section, we will provide an algorithm to decide \u201coverwhelming.\" Along the way, we develop a generalizable method of upper-bounding the worst-case deviation of a single layer of a transformer model under input restriction.\nINPUT RESTRICTIONS AND DESIGNED SPACE\nWe use the notation from Analysis of Boolean Functions to denote restrictions on inputs to a function (O'Donnell, 2014). The restriction will fix certain tokens to be a specific value and leave the rest of the tokens free.\nDefinition 4.1 (Input Restriction, (O'Donnell, 2014) definition 3.18). Let $f : X^{n} \\rightarrow Y$ be some function and $J \\subset [n]$ and $\\overline{J} = [n] \\backslash J$. Let $z \\in X^{\\overline{J}}$. Then, we write $f_{J\\vert z} : X^{J} \\rightarrow Y$ (\"the restriction of f to J given z\") as the subfunction of f that is obtained by fixing the coordinates of $\\overline{J}$ to the values in z. Given $y \\in X^{J}$ and $z \\in X^{\\overline{J}}$, we write $f_{J\\vert z}(y)$ as $f(y,z)$ even though y and z are not literally concatenated."}, {"title": "4.1. Algorithm for deciding Overwhelming", "content": "Here we consider zero temperature sampling setting where we can define our model with sampling as\n$M_{final}(X) = arg\\ max_{i\\in[d_{vocab}]} M(X)$\nwhere M is defined in definition 2.1. The model simply selects the token with the highest logit weight rather than sampling from the output distribution.\nWe define the \"peak-to-peak difference\" to be the difference between the logit for the most likely token and the logit for the second most likely token for sample X.\nDefinition 4.3 (Peak-to-peak difference). Let $X \\in O_{des}$ be any element from the restriction and $k = M_{final}(X)$. Then, we let\n$PTP(M, X) = \\underset{i \\neq k}{min} M(X) \\cdot (\\frac{e_{k}-e_{i}}{||e_{k}-e_{i}||^{2}})$.\nTo prove a model is overwhelmed by a fixed input we bound the worst-case deviation by the peak-to-peak difference. The following theorem summarizes the bound our algorithm is tasked with verifying.\nTheorem 4.4. If\n$W(M; O_{des})_{\\\\infty} < PTP(M, X)/2$\nfor some $X \\in O_{des}$, then the output of $M_{final}$ is \u201coverwhelmed\" under the restriction."}, {"title": "4.2. Proof Overview of theorem 4.5 (Algorithm Correctness)", "content": "The proofs for all the lemmas and statements in this section are deferred to appendix D.\nBREAKING DOWN THE MODEL\nDiscussing normalization as a function of expectation and variance is a bit cumbersome. So, we will first define an analagous \"blowup\" and \"shift\" for normalization.\nDefinition 4.6 (Blowup and Shift Sets). For some $X \\in O_{des}$, Let $B_{j}(X)$ (for blowup) denote\n$\\frac{1}{\\sqrt{Var[XE \\cdot e]+ \\epsilon}}$\nand let $S_{j}(X)$ (for shift) denote\n$\\frac{-E[XE \\cdot e]}{\\sqrt{Var[XE \\cdot e]+ \\epsilon}} \\cdot \\gamma + \\beta$."}, {"title": "4.2.1. BOUNDING BLOWUP AND SHIFT", "content": "We can get rather straightforward bounds on the blowup and shift of the model.\nLemma 4.7 (Blowup and Shift Bounds). We bound blowup and shift: for every $B_{j} \\in B_{j}$ and $S_{j} \\in S_{j}$\n$\\sqrt{\\frac{\\Upsilon}{Var^{max} + \\epsilon}} < B_{j} \\leq \\sqrt{\\frac{\\Upsilon}{Var^{min} + \\epsilon}}$\nand\n$min(B_{j}^{min}\\mu^{min} + S_{j}^{min},B_{j}^{min}\\mu^{max} + S_{j}^{min},B_{j}^{max}\\mu^{min} + S_{j}^{min},B_{j}^{max}\\mu^{max} + S_{j}^{min}) < S_{j}$\n$\\leq max(B_{j}^{min}\\mu^{min} + S_{j}^{max},B_{j}^{min}\\mu^{max} + S_{j}^{max},B_{j}^{max}\\mu^{min} + S_{j}^{max},B_{j}^{max}\\mu^{max} + S_{j}^{max})$\nwhere $\\mu^{min}, \\mu^{max}$ are the lower and upper bounds on the expectation of the j-th column of X. E. $Var^{min}, Var^{max}$ are similarly defined for the variance. Both are bounded in lemma D.1 within appendix D.\nMoreover, we let $\\overrightarrow{B^{min}}$ be a vector of the minimum values of the blowup and $\\overrightarrow{B^{max}}$ be a vector of the maximum values of the blowup. Define $\\overrightarrow{S^{min}}$ and $\\overrightarrow{S^{max}}$ similarly for the shift."}, {"title": "4.2.2. BOUNDING ATTENTION", "content": "For a multi-headed attention mechanism, $Attn(X) = [AttnH_{1}(X);...;AttnH_{H}(X)]$: i.e. the output is a concatenation of the individual attention head outputs. So, the infinity norm worst-case deviation is simply bounded by the maximum worst-case deviation over the individual attention heads. We thus have the following lemma:\nLemma 4.8.\nW( enctx, f(nfix Mctx)\\,q' Odes \u00d7 BS)\u221e < max W(AttnH_{h}; e_{nctx}\\times \\,\\ Odes \u00d7 BS)\u221e.\nfix,fix\nThe rest of this section will focus on bounding the worst-case deviation of a single attention head which will be the most challenging part of the proof."}, {"title": "4.2.3. BOUNDING MLP AND IDENTITY", "content": "Because the MLP and identity components are simple feed-forward networks without any \u201ccross-talk\u201d between tokens, we can use simple Lipschitz bounds.\nLemma 4.14 (Worst-case Deviation of MLP and Identity).\nW(enctx, f(nfix,Mctx)\\,q' Odes \u00d7 BS)\u221e < Lip(MLP)\u221e \\cdot W(\u0113nctx \\cdot fenc; Odes \u00d7 BS)\u221e\nand\nW( enctx, f(nfix Mctx)\\,q' Odes \u00d7 BS)\u221e\n= W(\u0113nctx \\cdot fenc; Odes \u00d7 BS)\u221e."}, {"title": "4.2.4. BOUNDING THE ENCODING FUNCTION", "content": "We upper bound the worst-case deviation for fenc using bounds on the \"blowup and shift\" in lemma 4.7:\nLemma 4.15 (Worst-case deviation of fenc).\nW(enctx fenc; Odes \u00d7 BS)\u221e <\nmax max max |X[t] \u00b7E\u00b7 diag(B) \u00b7 \u0113 + S \u2212 S min|,\nte[dvocab] je[demb] Be [0,B max-B min]\nwhere the inner maximum term can be computed via a simple linear program."}, {"title": "5. Overwhelming Under Permutation Invariance", "content": "In section 4, we provided a concrete algorithm that decides overwhelming for a fixed context size nctx. In this section, we provide a more refined algorithm which can provide a tighter bound on the worst-case deviation in the restricted setting where the free tokens are allowed to be any permutation of a fixed string. Up until this point, we have been \"lifting\" the domain of Odes into Odes \u00d7 BS to prove worst-case deviation bounds: i.e. we have been separating out the effect of layer normalization when proving a fixed string to be overwhelming.\nIn this section, we nullify the effect of layer normalization by defining an equivalence relation on Odes where the blowup and shift sets are the same for all elements in the equivalence class. Then, we can derive a simpler upper-bound on the worst-case deviation of the model over the equivalence class as we can treat the layer normalization as a constant factor!\nDefinition 5.1 (Permutation Equivalence relation). We will define the equivalence relation ~ on Odes. Let $X = (s||X_{f}||e_{q}) \\in O_{des}$ where $X_{f}$ are the free tokens, s are the fixed tokens, and eq is the query token. Then, we define ~ such that for $X = (s||X_{f}||e_{q})$, $Y = (s||Y_{f}||e_{q}) \\in O_{des}$,\n$X \\sim Y \\Leftrightarrow X_{f} = \\pi(Y_{f})$\nwhere \u03c0 is a permutation of the free tokens. Note that if X ~ Y, then E[X\u00b7E] = E[Y\u00b7E] and Var[X\u00b7E] = Var[Y\u00b7E]. Moreover, let $FreeToks$ be the multi-set of potential free tokens for [X]. I.e. $FreeToks = {X_{f}[1],..., X_{f}[n_{free}]}$.\nTo bound the worst-case deviation over an equivalence class we use a similar algorithm (algorithm 2) to algorithm 1. We still bound the worst-case deviation of attention, but now the blowup and shift sets are singletons, leading to much tighter bounds. Additionally, since blowup and shift are constant, the worst-case deviations of $f^{MLP}, f^{I}$ are"}, {"title": "5.1. Bounding Attention", "content": "To bound the worst-case deviation of attention, we provide two algorithms to bound \u03b1min, \u03b1max as defined in lemma 4.11. The first, in algorithm 6, takes a \u201cnaive\u201d approach by iterating through the position of the free tokens and finding an extremal value for each position. The second, in algorithm 3, uses a linear program to get a tighter bound. To see why algorithm 3 provides a valid bound, consider a similar program except where an integer linear program is used. Then, the integer version of algorithm 3 finds the permutation of free tokens which maximizes (resp. minimizes) the pre-softmax logits. Relaxing to a linear program, we have an upper-bound (resp. lower-bound) on the pre-softmax logits. We can formally state the"}, {"title": "6. Evaluating on a Single Layer Model", "content": "In this section, we empirically demonstrate our algorithm on a single layer transformer model trained for next to-ken prediction on a standard text corpus. Specifically, we run algorithm 2 to calculate a bound on the worst-case de-viation and peak-to-peak difference for the model where the domain has an input restriction with free tokens drawn from a single permutation class as in section 5. When algorithm 2 outputs \u201cOverwhelmed\" it follows from theorem 5.2 (informally, theorem 1.3 in the Introduction) that the output of the model is the same for all permutations.\n6.1. Experimental Setup\nWe train a single-layer transformer model with an embed-ding dimension of 512 and the BERT tokenizer (Devlin et al., 2019). The model's architecture is outlined in sec-tion 2. We train on the AG News dataset 4, using the train-ing split with a batch size of 8, learning rate of 5e-5 using the Adam optimizer, and 20 epochs.\nWe examine a few different types of input restrictions 5:\nRandom String: Randomly sample alpha-numeric tokens (including space and punctuation) to form s\nRandom Sentences: Sample (and concatenate) sen-tences from the AG News testing set, to form s\nRepeating Tokens: Forms by repeating the string \"what is it\" (which is 3 tokens in the BERT tokenizer).\nIn each case, we have a tunable number of tokens in s6. For the permutation class, we consider the following two cases:\nHamlet: We use the famous quote from Shakespeare's Hamlet: \"To be or not to be, that is the question. Whether it is a nobler in the mind to suffer the slings and arrows\"\nThe Old Man and the Sea: We fix the free tokens to be a snippet from the opening chapter of Heming-way's The Old Man and the Sea: \"The blotches ran well down the sides of his face and his hands had the deep-creased scars from handling heavy fish on the cords.\"\nFinally, the question mark token \u201c?\u201d is used as the query token."}, {"title": "6.2. Overwhelming Continues Through Generation", "content": "When a transformer model is \u201coverwhelmed\"the generated token immediately after the text is fixed for any free string. However, this does not apriori imply that subsequent tokens when the model is used to repeatedly generate tokens will be fixed.\nTo test this we run examples of text generation where we begin with an overwhelming string s and use the model to generate text. We then check if the model remains \"overwhelmed'as the generated text is included as part of the fixed string. The results of these tests are plotted in fig. 5 within appendix A. One would hope the model remains"}, {"title": "7. Conclusion and Further Work", "content": "In this work, we introduce the notion of \u201coverwhelming\" transformer models. We provide algorithms to provably check if a trained one-layer transformer is overwhelmed by some fixed input string s and query token q. We then empirically run the algorithm on a concrete single-layer model. We obtain bounds and find examples of natural overwhelming strings for this model in the restricted permutation setting.\nThis work is a first step in building algorithms to give provable guarantees for practical models. Improving our existing bounds and extending the algorithms to multi-layer transformers are important problems in this direction. Moreover, we believe that our approach to proving overwhelming, the use of LP bounds, and worst-case deviation metric will be independently useful for other theoretical studies of transformer based models."}, {"title": "B. Appendix for Model Details", "content": "We provide a formal definition of each of the components in the model used in this paper.\nsoftmax is the softmax function\nsoftmax(a) [i] = $\\frac{e^{a[i]}}{\\sum_{i=1}^{d_{vocab}} e^{a[i]}}$\nReLU is the rectified linear unit function\nReLU(x) = max(0,x)\nLN is the layer normalization function which for matrix X performs the following column wise function for columns of X (i.e. X[:, j]):\n$LN(X[i, j]) = \\frac{X[i, j] - E[X[:, j]]}{\\sqrt{Var[X[:, j]] + \\epsilon}} \\cdot \\gamma + \\beta$\nwhere \u03b3 and \u03b2 are learned parameters and addition and division are element-wise.\nMLP is a one-layer feed-forward network with ReLU activation. The MLP is row wise of X:\n$MLP(X[i]) = ReLU(X[i, :] \\cdot A_{enc} + b_{enc}) \\cdot A_{dec}$\nDepending on the model there may be additional bias terms added after the ReLU activation. For simplicitily, we will only consider one bias term prior to the ReLU activation though the results of this paper can be easily be extended to two bias terms.\nROPE (Su et al., 2024) is the rotary position encoding which applies a rotation to key and query vectors. For input X at row i, the rotation is applied as follows:\n$RoPE(X) [i, 2j] = cos(i\\theta_{j}) X[i, 2j] - sin(i\\theta_{j}) X[i, 2j + 1]$\nand\n$RoPE(X) [i, 2j + 1] = sin(i\\theta_{j}) X[i, 2j] + cos(i\\theta_{j}) X[i, 2j + 1]$\nwhere $\\theta_{j} = 10000^{-2j/d_{emb}}$ is the frequency for dimension j.\nAttnH is an attention head for matrix $X \\in \\mathbb{R}^{n_{ctx}\\times d_{emb}}$, an attention head does the following:\n$AttnH(X) = softmax(\\frac{ROPE(XQ) \\cdot ROPE(KTX^{T})}{\\sqrt{d_{emb}/H}}) \\cdot XV$\nwhere the softmax is applied column-wise. As in Ref. (Black et al., 2022), we can re-write the effect of RoPE via a rotation matrix $O_{i,j}$ such that\n$(ROPE(XQ) \\cdot ROPE(KTX^{T})) [i, j] = e_{i}^{T} \\cdot XQ \\cdot O_{i,j} \\cdot KT XT$\nOften, we have a multi-head attention mechanism which is the concatenation of H attention mechanisms along the last dimension:\n$Attn(X) = [AttnH_{1}(X);...;AttnH_{H}(X)]$\nwhere $Attn_{h}$ is the h-th attention mechanism as outlined\n$E\\in \\mathbb{R}^{d_{vocab}\\times d_{emb}}$ is the embedding function which maps a one hot vector in $\\mathbb{R}^{d_{vocab}}$ to a vector in $\\mathbb{R}^{d_{emb}}$.\n$Unembed \\in \\mathbb{R}^{d_{emb}\\times d_{vocab}}$ is the unembedding function which maps a vector in $\\mathbb{R}^{d_{emb}}$ to a one hot vector in $\\mathbb{R}^{d_{vocab}}$."}, {"title": "C. Proofs for Worst-Case Deviation", "content": "Lemma C.1 (Properties of the Worst-Case Deviation). For any functions $f, g : X \\rightarrow \\mathbb{R}^{n}$, norm p and lift to $Y \\times Z$, we have the following properties:\nTriangle inequality for (lifted) W:\n$W(f+g; X)_{p} \\leq W(f : X)_{p} + W(g : X)_{p}$.\nLifting monotonicity:\n$W(f;X)_{p} \\leq W(f;y \\times Z)_{p}$.\nLipschitz, composition: For function g\n$W(g\\circ f; X)_{p} \\leq Lip(g)_{p} \\cdot W(f;X)_{p}$.\nAs a corollary, we have that for linear operators A,\n$W(Af;X)_{p} \\leq ||A||_{p}\\cdot W(f;X)_{p}$.\nas $Lip(A)_{p} = ||A||_{p}$.\np-norm bounds for p,q \u2265 1 and q > p:\n$W(f;X)_{q} \\leq W(f;X)_{p}$\nProof of worst-case deviation properties, lemma C.1. We will prove each of the properties in turn.\nTriangle inequality: We can view the maximization over X as occuring disjointly for f and g: I.e.\n$W(f+g; X)_{p} \\leq sup_{X_{1},X_{2}\\in X} ||f(X_{1})+g(X_{1}) - (f(X_{2}) +g(X_{2})) ||$\n$\\leq sup_{X_{1},X_{2},X_{1},X_{2}\\in X} [||f(X_{1}) - f(X_{2}) || + ||8(X_{1}) - g(X_{2})||]$\n(by triangle inequality of norms)\nas desired.\n$\\leq W(f;X)_{p}+W(g;X)_{p}$\nLifting monotonicity: The proof follows from the definition of the lifted worst-case deviation:\n$W(f;X)_{p} = sup_{(Y_{1},Z_{1}), (Y_{2},Z_{2}) \\in X \\subset YxZ} ||f(Y_{1}, Z_{1}) - f(Y_{2}, Z_{2}) ||_{p}$\n$\\leq sup_{Y_{1},Y_{2}\\in Y, Z_{1},Z_{2}\\in Z} || f(Y_{1}, Z_{1}) - f (Y_{2}, Z_{2}) ||_{p}$\n= $W(f;Y \\times Z)_{p}$.\nLipschitz composition: We simply have that\n$W(Af;X)_{p} = sup_{X_{1},X_{2}\\in X} || Af(X_{1}) - Af(X_{2})||_{p}$\n$\\leq sup_{X_{1},X_{2}} \\frac{||A(X_{1}) \u2013 A(X_{2})||_{p}}{||X_{1} \u2013 X_{2}||_{p}} sup_{X_{1},X_{2}\\in X} || f(X_{1}) - f(X_{2})||_{p}$\n= Lip(A)p \\cdot W(f;X)p.\np-norm bounds for p,q \u2265 1 and q > p: Because we restrict f to be a function which outputs vectors and $||3||_{q} \\leq ||3||_{p}$, for q > p, we have that for all $X_{1}$, $X_{2}$ \u2208 X, $||f(X_{1}) - f(X_{2})||_{q} \\leq ||f(X_{1}) - f(X_{2})||_{p}$. So, if there exists $X_{1}$, $X_{2}$ \u2208 X such that $|| f(X_{1}) - f(X_{2})||_{q} = a$, then there must exist $X_{1}$, $X_{2}$ \u2208 X such that $|| f(X_{1}) - f(X_{2})||_{p} \\geq a$. Thus, the supremum over X for q is less than or equal to the supremum over X for p."}, {"title": "D. Proofs for Generic Framework", "content": "D.1. Proofs for Blowup and Shift Bounds\nWe first need to bound the variance and expectation of the columns of E. X:\nLemma D.1 (Bounds on Expectation and Variance). Let $X \\in O_{des}$ be an input sequence. Then for any column j, the expectation uj of the j-th column of E \u00b7 X satisfies:\n$\\frac{1}{n_{ctx}} (\\sum_{i=1}^{n_{fix}} E[s_{i}] + n_{free}\\underset{x\\in [d_{vocab}]}{min} E[x]}+E[q]) \\leq \\mu_{j} \\leq$\n$\\frac{1}{n_{ctx}} (\\sum_{i=1}^{n_{fix}} E[s_{i}] + n_{free}\\underset{x\\in [d_{vocab}]}{max} E[x]}+E[q])$\nLet umin and umax denote the lower and upper bounds for the j-th column respectively. Then, we can bound the variance of the j-th column of XE by\n$\\frac{1}{n_{ctx}} \\underset{\\mu'\\in [\\mu_{min}, \\mu_{max}]}{min}  \\sum_{i=1}^{n_{fix}} (E[s_{i}]-\\mu')^{2} + n_{free} \\underset{x\\in [d_{vocab}]}{min} (Ex - \\mu')^{2}) \\leq Var_{j}[(EX) [:, j]] \\leq $\n$\\leq \\frac{1}{n_{ctx}} \\underset{\\mu'\\in [\\mu_{min}, \\mu_{max}]}{max}  \\sum_{i=1}^{n_{fix}} (E[s_{i}]-\\mu')^{2} + n_{free} \\underset{x\\in [d_{vocab}]}{max} (Ex - \\mu')^{2})$\nWe define $Var^{min}$, $Var^{max}$ to be the lower and upper bounds respectively.\nProof. The bounds on uj follow as we minimize and maximize the contribution of each free row. The proof of for variance bounds follows similarly.\nWe now have a simple proof for lemma 4.7 restated below:\nLemma D.2 (Blowup and Shift Bounds). We bound blowup and shift: for every $B_{j} \\in B_{j}$ and $S_{j} \\in S_{j}$\n$\\sqrt{\\frac{\\Upsilon}{Var^{max} + \\epsilon}} < B_{j} \\leq \\sqrt{\\frac{\\Upsilon}{Var^{min} + \\epsilon}}$\nand\n$min(B_{j}^{min}\\mu^{min} + S_{j}^{min},B_{j}^{min}\\mu^{max} + S_{j}^{min},B_{j}^{max}\\mu^{min} + S_{j}^{min},B_{j}^{max}\\mu^{max} + S_{j}^{min}) < S_{j}$\n$\\leq max(B_{j}^{min}\\mu^{min} + S_{j}^{max},B_{j}^{min}\\mu^{max} + S_{j}^{max},B_{j}^{max}\\mu^{min} + S_{j}^{max},B_{j}^{max}\\mu^{max} + S_{j}^{max})$\nProof. Given the bounds on expectation and variance, $B^{min}$, $B^{max}$ follow trivially from the definition of blowup and shift sets. $S^{min}$ (resp. $S^{max}$) follow from a minimization (maximization) over the possible shifts given the blowup bounds."}, {"title": "D.2. Appendix for Bounding Worst-case Deviation of", "content": "Recall the definition of blowup and shift sets from definition 4.6. Let Bmin, Bmax be the vectors (Bmin,. .., Bmin) and (Bmax,..., Bmax) respectively. Define Smin, Smax analogously. We restate lemma 4.15 for convenience:\nLemma D.3 (Worst-case deviation of fenc, lemma 4.15). Let x \u2208 Odes. Then,\nW(enctx fenc; Odes \u00d7 BS)\u221e \u2264 max max max |X[t] \u00b7E\u00b7 diag(B) \u00b7 \u0113| + + Smax Smin\nte[dvocab] je[demb] Be [0,B max-B min]\nwhere the inner maximum term can be computed via a simple linear program."}, {"title": "D.3. Proofs and Subalgorithms for Attention Bounds", "content": "Definition D.4 ((min, (max). We use min to denote a worst-case lower bound on the smallest possible logit at the i-th position of the input to the softmax in model M. Similarly, we use max to denote a worst-case upper bound on the largest possible logit at the i-th position of the input to the softmax in model M.\nWe provide the algorithm to find the extremal values (min and (max in algorithm 4. At a high level, the algorithm computes upper and lower bounds for each position in the input sequence prior to the softmax. The bounds make use of the upper and lower bounds on the blowup and shift sets, BS.\nProof sketch for lemma 4.9 (correctness of algorithm 4). In algorithm 4, for each position k, we compute the minimum and maximum logit for the k-th position by maximizing over possible blowup and shift sets and input tokens for the free tokens. Moreover, algorithm 5 computes an upper and lower bound for the restricted bilinear form which algorithm 4 uses to compute the extremal values.\nThe correctness of the bilinear bound in algorithm 5 follows from a series of relaxations when writing out the explicit formula for the bilinear multiplication. To prove an upper-bound, we have that\nm = \u2211xiAi,jyj"}]}