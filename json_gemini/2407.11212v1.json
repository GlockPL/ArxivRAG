{"title": "Automated Essay Scoring in Arabic: A Dataset\nand Analysis of a BERT-based System", "authors": ["Rayed Ghazawi", "Edwin Simpson"], "abstract": "Automated Essay Scoring (AES) holds significant promise in\nthe field of education, helping educators to mark larger volumes of essays\nand provide timely feedback. However, Arabic AES research has been\nlimited by the lack of publicly available essay data. This study intro-\nduces AR-AES, an Arabic AES benchmark dataset comprising 2046 un-\ndergraduate essays, including gender information, scores, and transpar-\nent rubric-based evaluation guidelines, providing comprehensive insights\ninto the scoring process. These essays come from four diverse courses,\ncovering both traditional and online exams. Additionally, we pioneer the\nuse of AraBERT for AES, exploring its performance on different ques-\ntion types. We find encouraging results, particularly for Environmental\nChemistry and source-dependent essay questions. For the first time, we\nexamine the scale of errors made by a BERT-based AES system, observ-\ning that 96.15% of the errors are within one point of the first human\nmarker's prediction, on a scale of one to five, with 79.49% of predictions\nmatching exactly. In contrast, additional human markers did not exceed\n30% exact matches with the first marker, with 62.9% within one mark.\nThese findings highlight the subjectivity inherent in essay grading, and\nunderscore the potential for current AES technology to assist human\nmarkers to grade consistently across large classes.", "sections": [{"title": "1 Introduction", "content": "Essay writing is an important tool for developing and assessing students' cog-\nnitive abilities, including critical thinking, communication skills and depth of\nunderstanding [11,34]. However, as student numbers grow, marking essays by\nhand becomes impractical, discouraging the use of essay questions in education\n[6]. AES systems [32] aim to reduce the time needed to mark essays, by assess-\ning both writing skills and cognitive outputs automatically, and can mitigate\nscoring biases and inconsistencies arising from teacher subjectivity [7]. Despite\nextensive research in English [38,25], AES for Arabic, the fourth most widely"}, {"title": "1", "content": "used Internet language \u00b9, remains underexplored, with most efforts concentrated\non scoring short, one or two-sentence answers [7]. With the abundant youth\npopulation in the Arab world, the education system faces challenges due to\na shortage of teachers and the inability to provide individualized feedback to\nstudents [13]. In addition, the Arabic language differs from English in terms\nof grammar, structural rules, and the formulation of ideas, which prevents the\napplication of scoring systems designed for English [13]. In this context, the\ndevelopment of an Arabic essay scoring system is an urgent necessity.\nPrevious research has predominantly leaned on feature engineering in con-\njunction with shallow models, yielding only moderate performance outcomes\n[5,17]. In contrast, the potential of pretrained models such as AraBERT [9], Ar-\naVec [35], and AraGPT-2 [10], which learn vector representations from extensive\ntext corpora, remains largely untapped within the context of Arabic AES. These\nmodels have demonstrated notable efficacy in various domains, encompassing\ntasks like question-answering, named entity recognition, sentiment analysis, and\neven the automatic scoring of short answers [29,4]. A major barrier to further\nresearch is the lack of publicly available datasets: datasets used in prior studies\nare either inaccessible or consist only of one or two-sentence answers.\nTo address these gaps, this study introduces AR-AES dataset, which con-\nsists of Arabic essays each marked by two different university teaching profes-\nsionals. This dataset was collected from undergraduate students across diverse\ndisciplines, covering various topics and writing styles. We include ancillary in-\nformation, such as the gender of the students (male and female students were\ntaught separately), the specific evaluation criteria (rubrics) employed, and model\nanswers for each question. The dataset comprises a total of 12 questions and\n2046 essays, collected through both traditional and online examination methods,\nand encompasses substantial linguistic diversity, with a total length of 115,454\ntokens and 12,440 unique tokens.\nThis study also pioneers the use of AraBERT in Arabic AES by conduct-\ning a series of experiments to assess AraBERT's performance on our dataset\nat different levels of granularity, from the complete dataset down to individual\ncourses and questions. We also examined AraBERT's performance based on\ngender, exam type (traditional or online), and essay type (argumentative, nar-\nrative, source-dependent). AraBERT excelled when trained on several questions\nfrom the same course, achieving a Quadratic Weighted Kappa (QWK) score\nof 0.971 in Environmental Chemistry. However, its performance was compar-\natively lower when trained specifically for certain types of question, with the\nlowest QWK observed for narrative questions.\nOur analysis goes beyond previous work on AES, by assessing the proximity\nof the model's predictions to the grades assigned by the first marker, to gauge\nthe scale of its errors. The predictions matched exactly for 79.49% of answers,\nwith 95% of predictions having no more than one mark difference to the first\nhuman mark (out of a total of five marks). In contrast, the question with highest\nagreement between the first and second human markers had only 30.3% exact"}, {"title": "2 Related Works", "content": "Several AES datasets have been released in Chinese [18], Indonesian [1], and\nEnglish, including the ASAP dataset that has catalysed English AES research\n[33,37], including a new state-of-the-art BERT-based approach [38]. However,\nthere is no previous publicly available dataset containing Arabic essays and\nmarks, as existing work is limited to short answers [3]. Our study addresses\nthis gap by presenting a comprehensive dataset for Arabic AES.\nArabic AES research encompasses approaches such as linear regression [5],\nLatent Semantic Analysis [2], Support Vector Machines [7], rule-based systems\n[6], na\u00efve Bayes [3], and optimization algorithms like eJaya-NN [17]. However,\nthese studies predominantly rely on feature engineering, using surface features\nthat are unable to comprehensively capture the semantic nuances and structural\nintricacies inherent in essays. These approaches provide only limited consid-\neration for word order, primarily revolving around word-level or grammatical\nfeatures. More recent pretrained transformer models, such as BERT [15], allevi-\nate these issues but have not previously been harnessed for Arabic AES. Here,\nwe develop the first AES system using AraBERT to analyse the effect of different\nquestion types on a modern text classifier. We also go beyond previous analy-\nses of model performance by evaluating the magnitude of errors in the models'\npredictions, as large errors could have a greater impact on students."}, {"title": "3 Arabic language challenges", "content": "NLP systems face several distinct challenges when processing Arabic, which\nmotivate the development of bespoke tools and language resources, including\nbenchmark datasets.\nLinguistic Complexity: Arabic exhibits complex sentence structures with\nmany syntactic and stylistic variations, an extensive vocabulary, and the fre-\nquent use of rhetorical devices [8]. Arabic, for instance, has many ways to\nexpress the concept of \"going\" depending on who is doing the action, when, and\nwhether the action is done in a habitual or momentary sense. For example,\u064a\u0630\u0647\u0628\nhe goes(\u0633\u0623\u0630\u0647\u0628 I will go(\u0643\u0627\u0646 \u064a\u0630\u0647\u0628 )he used to go), and \u064a\u0630\u0647\u0628\u0627\u0646 )they (two) go(."}, {"title": "3", "content": "This complexity can make it hard for an AES system to recognise variations of\nthe same concept.\nComplex Morphology: Arabic features intricate morphology, encompassing\na wide range of inflection and derivational systems [19]. Words in Arabic can\nhave multiple forms based on factors such as tense, gender, number, and case,\nand the form of a single letter also varies. For instance, the letter ('') looks\nlike() at the beginning of a word )\u0633\u062d\u0627\u0628\u201cCloud", "Hospital\u201d), and like )\u0633( at the end as in )\u0634\u0645\u0633\u201cSun\u201d). This\ncomplexity adds to the difficulty of stemming, tokenization, and lemmatization\noperations [24]. As another example, the Arabic root word for \"write\" is \u0643\u062a\u0628\nfrom which we can derive various words like \u0643\u0627\u062a\u0628 )writer(\u0645\u0643\u062a\u0648\u0628 )\"written(,\n\u0643\u062a\u0627\u0628 )book(\u0643\u062a\u0628\u062a )\"I wrote,\u064a\u0643\u062a\u0628 \"he writes": "etc. The challenge for AES\nsystems here lies in recognizing these words as related.\nNon-Standard Orthography: Arabic text follows complex rules for letter rep-\nresentation, including ligatures and diacritics that influence pronunciation, word\ncomprehension, and meaning [21,36]. NLP systems face challenges in handling\nthese orthographic differences and the absence of diacritics in unvocalised text.\nFor example, the word \u0645\u062d\u0628\u0648\u0628 )\"loved or popular", "Resources": "Arabic suffers from limited linguistic resources, such as\npreprocessing tools for dealing with the language complexities described above,\nand a lack of available datasets [27,23], which hampers the development of NLP\nmodels. A particular need is for bespoke tools to deal with the right-to-left\ntext direction, which creates additional complexities for mixed-language con-\ntent [12,24]. This study contributes a labelled dataset in Arabic, which will\nenable further development of Arabic NLP systems.\nAmbiguity and Polysemy: Arabic words often possess multiple meanings and\ninterpretations, making it challenging to disambiguate them [16]. For example,\nthe word in Arabic can mean \"camel\u201d or \u201csentence\u201d depending on context.\nContextual analysis becomes crucial for accurately determining the intended\nmeaning [23,31]. This aspect presents a challenge in various NLP tasks, includ-\ning named entity recognition, sentiment analysis, and machine translation.\nDespite these challenges, substantial advancements have been made in Arabic\nNLP in recent years, including language models and tools specifically designed\nfor Arabic. This study hopes to contribute to this effort."}, {"title": "4 The AR-AES Dataset", "content": "The AR-AES\u00b3 dataset is intended for both training and evaluating Arabic AES\nsystems, and covers essays written by both male and female undergraduate stu-\ndents from three different university faculties, with a range of different question\ntypes, a mix of traditional face-to-face and online exams, and marks from mul-\ntiple human markers. As part of the dataset, we include clear and detailed"}, {"title": "4", "content": "marking criteria along with model answers for each question. This diversity will\nenable researchers to explore the suitability of AES systems for different types\nof essays, exam types, or student cohorts.\nData Collection: To compile a diverse dataset, we first selected multiple under-\ngraduate courses across various departments at Umm Al-Qura University, as\nshown in Table 1. Students' writing skills vary depending on their academic\ndisciplines [39], due to differing objectives, domain-specific terminology, and\nresearch formulation methodologies. Additionally, factors like gender and aca-\ndemic level contribute to differences in writing [22,26], particularly considering\nthat the genders are taught separately. Therefore, to facilitate testing of AES\nsystems across various subjects and writing styles, we collected essay responses\nfrom diverse academic levels, and male and female genders.\nEnsuring diversity in question types within the dataset was vital for evalu-\nating the model's performance across various essay categories.\nTo bolster dataset diversity, we employed both traditional (in-person) and\nonline exams through distance learning. Traditional exams occurred on spe-\ncific dates within campus halls or laboratories, subjecting students to controlled\nconditions that minimized opportunities for academic misconduct. Conversely,\nonline exams required students to submit essay responses exclusively via content\nmanagement platforms. These exams shared time limits with traditional exams\nbut did not mandate physical presence on campus. Online exams can reduce\nstress levels [20], granting students greater freedom in providing answers and\npotentially allowing access to course content during the exam. For both kinds\nof exam, answers were typed and submitted electronically, eliminating the need\nto convert handwritten answers to digital format. These essays were part of the\nstudents' compulsory assessment within the midterm exams for their respective\ncourses, and they volunteered to provide their essays for our dataset."}, {"title": "5 Experimental Setup", "content": "The AraBERT model has consistently demonstrated state-of-the-art performance\nin various Arabic NLP tasks, including the automatic scoring of short answers\n[29], but its application to AES remains unexplored. Thus, this study's primary\ngoal is to assess AraBERT's performance in AES and its ability to handle longer\nArabic texts. Additionally, we aim to investigate whether performance varies\ndepending on factors such as the subject, question type, exam type, or gender."}, {"title": "7", "content": "Data Preprocessing: We removed punctuation, hashtags, URLs, excess letter\nrepetitions, emoticons, superfluous spaces, numbers, and diacritics, and normal-\nized specific Arabic characters to their standard forms (e.g., \u0629 < \u0647 - \u0649 < \u064a - \u0623\n\u0625 \u0623 \u0627 \u062f \u0627 - \u0626 \u0624 \u062f \u0621(. We applied the ISRI Stemmer, in the manner of previous\nwork [29], to simplify Arabic text by reducing words to their roots to minimise\nvocabulary diversity. We employed the AraBERT tokenizer, and sequences ex-\nceeding 512 tokens were truncated. Most essays fit this limit, except four from\nthe Biotechnology course, exceeding up to 575 words.\nModel Design: AraBERT is a variant of BERT that was pretrained on a substan-\ntial Arabic text dataset [9] and can be fine-tuned for specific tasks with minimal\nadditional training data, reducing the time and resources needed for model de-\nvelopment and deployment. This study used the large AraBERT configuration,\nfeaturing 12 encoder blocks, 1024 hidden dimensions, 16 attention heads, 512 se-\nquence length, and 370 million parameters. To leverage AraBERT's pre-trained\ncapabilities, we added a standard classification head on top of it, consisting of a\nsingle fully-connected layer. This approach follows the HuggingFace Transform-\ners library conventions, providing a simple yet effective method for fine-tuning.\nThis design choice allows us to efficiently map AraBERT's high-dimensional\nrepresentations to target classification labels, ensuring both computational fea-\nsibility and high accuracy. Notably, this study marks the first application of\nAraBERT in automatic Arabic essay-scoring tasks.\nModel Training: The system aims to assist the course presenter (first annotator),\nso the model was trained only on the labels provided by that person. To ensure\ncomparability across questions, we normalized all scores in the dataset to the\nrange 0 to 5. Specifically, for questions with scores originally ranging from 0 to\n10 (Q4 and Q5), we divided the scores by 2 to align them with the score range\nused for other essays. We trained the model once on the complete dataset, as\nwell as separately for each course and each question. We also trained the model\nseparately on male and female essay responses for the Introduction to Informa-\ntion Science course (each gender was taught separately by different instructors),\nand on traditional and online essay responses, to observe differences in model\nperformance that could affect each group differently.\nFor each of these experiments, we divided the answers randomly into training,\nvalidation and test sets (split 70/15/15). We trained using Adam optimiser and\nthe hyperparameters, including batch size, dropout rate (0.2), and the number of\ntraining epochs, were tuned on the validation set for each experiment, as detailed\nin Table A.24. Given the dataset's imbalanced nature, we employed class weights\nto give equal weight to each class in the dataset by assigning proportionally\nhigher weights to instances from smaller classes. The distribution of classes for\neach question is illustrated in Figure B.15."}, {"title": "8 Results", "content": "We first evaluated the AraBERT model on the entire dataset to gauge its per-\nformance when trained with more data and a variety of questions. Then, we\ntrained and evaluated models using data from each course, individual question,\nquestion type, student gender, and exam type, to identify the kind of scenarios\nwhere the AES system could be more effective.\nThe results are shown in Table 4. On the complete dataset, the model\nachieved a QWK score of 0.884 and an F1 score of 0.78, but this was not the\nhighest score, suggesting that while the larger training set may benefit this com-\nbined model, some essay types are more amenable to AES than others. For\ninstance, the model performance in the Environmental Chemistry course ex-\nceeded that of the entire dataset, even though this course included responses in\nArabic mixed with English terms. Among the different courses, performance was\nweakest on Management Information Systems, potentially due to the complex-\nity of the material or student responses. The Management Information Systems\ncourse had approximately 4469 unique words (in extended answers), while En-\nvironmental Chemistry had around 2702 unique words (in restricted answers).\nThis difference suggests that the Management Information Systems course fea-\ntured more open-ended essays compared to the Environmental Chemistry course,\nwhere answers were more source-dependent and controlled, making them easier\nfor the model to evaluate. Within Management Information Systems, two ques-\ntions were narrative, which tend to be open-ended, possibly contributing to the\nmodel's lower performance in this course.\nCompared to Biotechnology, performance on the Information Science course\nwas weaker, despite its larger training set. We investigated whether this dis-\ncrepancy may be attributed to the students' use of informal language, consid-\nering that this course is a first-semester offering for first-year undergraduates,"}, {"title": "9", "content": "while the Biotechnology course is taken in the second semester of the third year.\nWe computed the perplexity [30] of students' answers for each course, finding\nthat Introduction to Information Science had a high perplexity score of 14.87\ncompared to Management Information System (1.77), Environmental Chemistry\n(1.5), and Biotechnology (1.68). This suggests that the AraBERT model was\nless suitable for modelling the Introduction to Information Science answers, and\nthat the language differs from that used in other courses.\nOverall, the model performed best with source-dependent questions, where\nlanguage is more constrained, and worst with narrative questions, which were\nthe most open-ended, reflected in the higher number of unique words shown in\nTable 4. The model also performed better with online, rather than traditional\nin-person exams. Splitting the Introduction to Information Science questions\nby gender resulted in superior performance when predicting female students'\nmarks, which may reflect different teaching or learning styles, as male and female\nstudents are taught separately by different lecturers.\nMagnitude of Errors: It is important to consider the scale of errors that the\nmodel makes: if the system predicts marks that are much lower or higher than\nthe human marker, students could be unfairly penalised or rewarded for poor-\nquality work. We therefore assess the deviations between predictions and correct\nscores using a confusion matrix, as shown in Figure 1. The pattern is similar\nacross courses. The majority of errors involved overestimations, with 10% of"}, {"title": "10", "content": "cases resulting in a one-mark overestimation. Underestimations were less fre-\nquent, occurring in 6% of cases with a one-degree reduction. Exact matches\nwere 12% higher for Environmental Chemistry than Introduction to Informa-\ntion Science. Examining the confusion matrix for each essay type (Figure 2),\none-mark overestimates occur noticeably more in narrative essays, while source-\ndependent essay predictions match the human marker's grade in 87% of cases."}, {"title": "7 Human Agreement", "content": "Consistent grading in education and assessment is crucial to maintain fairness\nand objectivity. Here, we assess the consistency of grades assigned by different\nhuman assessors across each question type and compare the level of agreement\nbetween human markers to the model's performance. We examine agreement\nfor two courses: Introduction to Information Science and Information Systems\nManagement, for a total of six questions (Q1 to Q6) and show results in Table 5."}, {"title": "11", "content": "The highest agreement among human assessors was observed in Q3 (source-\ndependent), where they provided the same grade in 30.3% of cases out of 279\nresponses. Negative differences were far more frequent than positive, meaning\nthat second markers tended to mark more harshly than the course directors.\nConversely, the lowest agreement among human assessors was found in Question\n4 (narrative), which has notably more cases of disagreement by 3 or more marks.\nWhen compared with the performance of our models, which were trained\nwith the gold standard marks of the original markers, we see that the disparity\nin second marker's assessments often exceeds the error rate of the automated\nsystem. This suggests that the model may be an effective way to assist a human\nmarker or could help to ensure consistency between multiple markers."}, {"title": "8 Conclusions and Future work", "content": "In this paper, we introduced AR-AES, the first publicly-available Arabic AES\ndataset, consisting of 2046 undergraduate essays with model answers, marking\ncriteria, and scores from multiple markers. We also developed and evaluated an\nAES system using AraBERT, and demonstrated promising performance, partic-\nularly on source-dependent essays in domains such as Environmental Chemistry.\nOur analysis showed that agreement between our model and gold standard marks\nis higher than agreement among human markers, suggesting a role for AES in\nensuring consistency as well as increasing marking efficiency.\nThere are numerous avenues for future work, such as exploring the adapta-\ntion of state-of-the-art techniques from the English AES field to the domain of\nArabic AES, such as the multi-scale approach of [38]. In addition to model ex-\nploration, future research should also focus on integrating AES systems into the\nessay grading process effectively, and addressing students' and teachers' concerns\nabout automated systems. This includes designing a process for identifying and\nrectifying errors, and ensuring that human teachers retain control while being\nassisted in grading a large set of essays. This area holds significant potential for\nenhancing the efficiency and accuracy of essay scoring, particularly in universi-\nties with limited teaching resources. We also see value in expanding our dataset"}, {"title": "12", "content": "with essays from a wider range of courses and educational institutions, thereby\nenhancing the robustness and versatility of our model, and investigating other\naspects of student diversity beyond subject and gender. Our approach may also\nprovide a template for AES data collection in other languages."}]}