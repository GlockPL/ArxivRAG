{"title": "Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained Scheduling", "authors": ["Zhuoran Li", "Ruishuo Chen", "Hai Zhong", "Longbo Huang"], "abstract": "Effective multi-user delay-constrained scheduling is crucial in various real-world applications, such as instant messaging, live streaming, and data center management. In these scenarios, schedulers must make real-time decisions to satisfy both delay and resource constraints without prior knowledge of system dynamics, which are often time-varying and challenging to estimate. Current learning-based methods typically require interactions with actual systems during the training stage, which can be difficult or impractical, as it is capable of significantly degrading system performance and incurring substantial service costs. To address these challenges, we propose a novel offline reinforcement learning-based algorithm, named Scheduling By Offline Learning with Critic Guidance and Diffusion Generation (SOCD), to learn efficient scheduling policies purely from pre-collected offline data. SOCD innovatively employs a diffusion-based policy network, complemented by a sampling-free critic network for policy guidance. By integrating the Lagrangian multiplier optimization into the offline reinforcement learning, SOCD effectively trains high-quality constraint-aware policies exclusively from available datasets, eliminating the need for online interactions with the system. Experimental results demonstrate that SOCD is resilient to various system dynamics, including partially observable and large-scale environments, and delivers superior performance compared to existing methods.", "sections": [{"title": "1 Introduction", "content": "The rapid expansion of real-time applications, such as instant messaging [86], live streaming [63], and datacenter management [13], has underscored the necessity for effective multi-user delay-constrained scheduling. The quality of scheduling significantly influences user satisfaction, which,"}, {"title": "2 Related Work", "content": "Numerous studies have addressed the scheduling problem through various approaches, including optimization-based methods and deep reinforcement learning. Additionally, recent advancements in offline reinforcement learning algorithms and diffusion models are reviewed in this section."}, {"title": "2.1 Optimization-Based Scheduling", "content": ""}, {"title": "2.1.1 Stochastic Optimization", "content": "A key research direction for addressing scheduling problems involves the use of stochastic optimization techniques. This category encompasses four main methodologies: convex optimization, e.g., [61, 26], which focuses on network scheduling, dynamic programming (DP)-based control, e.g., [8, 72], which aims to achieve throughput-optimal scheduling, queueing theory-based methods, e.g., [23, 70, 52], which concentrate on multi-queue systems, and Lyapunov optimization, e.g., [59, 49, 3], used for network resource allocation and control. Although these methods are theoretically sound, they often encounter difficulties in explicitly addressing delay constraints and require precise knowledge of system dynamics, which is challenging to obtain in practical applications."}, {"title": "2.1.2 Combinatorial Optimization", "content": "Combinatorial optimization is another key class of methods for network scheduling. For instance, [32] employs fuzzy logic to optimize resource consumption with multiple mobile sinks in wireless sensor networks. More recently, graph neural networks (GNNs) have been applied to tackle scheduling problems, as demonstrated in [64, 38]. Additionally, [80] provides a comprehensive survey of applications in the domain of telecommunications. Despite their potential, these combinatorial optimization approaches often suffer from the curse of dimensionality, which makes it challenging to scale effectively to large-scale scenarios."}, {"title": "2.2 DRL-based Scheduling", "content": "Deep reinforcement learning (DRL) has garnered increasing attention in the field of scheduling, owing to its strong performance and significant potential for generalization. DRL has been applied in various scheduling contexts, including video streaming [51], Multipath TCP control [88], and resource allocation [56, 53]. Despite the promise of DRL, these approaches often face challenges in enforcing average resource constraints and rely heavily on online interactions during training, which can be costly for many real-world systems. Offline reinforcement learning (Offline RL) algorithms are novel in that they require only a pre-collected offline dataset to train the policy without interacting with the environment [37, 39]. The feature of disconnecting interactions with the environment is crucial for numerous application scenarios, such as wireless network optimization [81, 87]. However, existing offline RL algorithms struggle to effectively address delay and resource constraints. Our proposed SOCD algorithm provides a robust, data-driven, end-to-end solution. It incorporates delay and resource constraints using delay-sensitive queues and the Lagrangian dual method, respectively. Additionally, it employs an offline learning paradigm, eliminating the need for online interactions."}, {"title": "2.3 Offline Reinforcement Learning and Diffusion Models", "content": "Distributional shift presents a significant challenge for offline reinforcement learning (RL), and various methods have been proposed to address this issue by employing conservatism to constrain the policy or Q-values through regularization techniques [85, 35, 17, 36, 50]. Policy regularization ensures that the learned policy remains close to the behavior policy by incorporating policy regularizers. Examples include BRAC [85], BEAR [35], BCQ [17], TD3-BC [16], implicit updates [62, 71, 55], and importance sampling approaches [33, 77, 45, 54]. In contrast, critic regularization focuses on constraining Q-values to enhance stability, e.g., CQL [36], IQL (Implicit Q-Learning) [34], and TD3-CVAE [69]. Furthermore, developing a rigorous theoretical understanding of optimality and designing robust, well-founded algorithms remain fundamental challenges in the field of offline RL [42, 2, 60].\nDiffusion models [21, 76, 73, 75, 74], a specialized class of generative models, have achieved remarkable success in various domains, particularly in generating images from text descriptions [68, 84]. Recent advancements have explored the theoretical foundations of diffusion models, such as their statistical underpinnings [9, 29], and methods to accelerate sampling [48, 15]. Generative models have also been applied to policy modeling, including approaches like conditional VAE [31], Diffusers [24, 1], DQL [83, 10, 28], SfBC [6, 47, 5], and IDQL [19]. Our proposed SOCD represents a novel application of a critic-guided diffusion-based policy as the scheduling policy, specifically designed for multi-user delay-constrained scheduling tasks."}, {"title": "3 Problem Formulation", "content": "This section presents our formulation for the offline learning-based scheduling problem. We begin by introducing the multi-user delay-constrained scheduling problem and its corresponding dual problem in Section 3.1. In Section 3.2, we present the MDP formulation for the dual problem. While our formulation builds on the foundations laid by previous works [72, 8, 22, 43], our primary goal is to develop practical scheduling policies that can be implemented in real-world settings without the need for online interactions during training."}, {"title": "3.1 The Multi-User Delay-Constrained Scheduling Problem", "content": ""}, {"title": "3.1.1 Single-Hop Setting", "content": "We consider the same scheduling problem as in [72, 22, 43], using a four-user example shown in Figure 1. Time is divided into discrete slots, t\u2208 {0,1,2,... }. The scheduler receives job arrivals, such as data packets in a network or parcels at a delivery station, at the beginning of each time slot. The number of job arrivals for user i at time slott is denoted as Ai(t), and we define the arrival vector as A(t) = [A1(t), ..., An(t)], where N refers to the number of users. Each job for user i has a strict delay constraint Ti, meaning that the job must be served within Ti slots of its arrival; otherwise, it will become outdated and be discarded.\nThe Buffer Model Jobs arriving at the system are initially stored in a buffer, which is modeled by a set of delay-sensitive queues. Specifically, the buffer consists of N separate delay-sensitive queues, one for each user, and each queue has infinite capacity. The state of queue i at time slot t is denoted by Qi(t) = [Q(t), Q(t), ..., Q(t)], where ti is the maximum remaining time and Q(t) represents the number of jobs for user i that have 7 time slots remaining until expiration, for 0\u2264T\u2264Ti.\nThe Service Model At every time slot t, the scheduler makes decisions on the resources allocated to jobs in the buffer. For each user i, the decision is denoted as v\u00bf(t) = [v(t), v(t), ..., v(t)], where v(t) \u2208 [0, Umax] represents the amount of resource allocated to serve each job in queue i with a deadline of T. Each scheduled job is then passed through a service channel, whose condition is random and represented by ci(t) for user i at time slot t. The channel conditions at time slot t are denoted as c(t) = [c1(t), c2(t), ..., cv(t)]. If a job is scheduled but fails in service, it remains in the buffer if it is not outdated. The successful rate to serve each job is jointly determined by the channel condition c\u2081(t) and vi(t), i.e., pi(t) = Pi(vi(t), ci(t)).\nThe Scheduler Objective and Lagrange Dual For each user i, the number of jobs that are successfully served at time slot t is denoted as u\u017c(t). A known weight wi is assigned to each user, and the weighted instantaneous throughput is defined as D(t) = 1 wiui(t). The instantaneous resource consumption at time slot t is denoted as E(t) = \u03a3\u03c5 v(t)Qi(t), and the average resource"}, {"title": null, "content": "consumption is E = $\\lim_{T \\to \\infty} \\frac{1}{T}\\sum_{t=1}^{T} E(t)$. The objective of the scheduler is to maximize the weighted average throughput, defined as D = $\\lim_{T \\to \\infty} \\frac{1}{T}\\sum_{t=1}^{T}D(t)$, subject to the average resource consumption limit (corresponds to Equation (1) in [22] and Equation (3) in [43]), i.e.,\nP:\n$\\max_{\\nu(t):1 \\leq i \\leq N,1 \\leq t \\leq T} \\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{N} w_i u_i(t)$,\ns.t. $\\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{N} v_i(t)Q_i(t) \\leq E_0$,\nwhere Eo is the average resource consumption constraint.\nWe denote the optimal value of problem P by T*. Similar to Equation (2) in [22] and Equation (4) in [43], we define a Lagrangian function to handle the average resource budget constraint in problem P, as follows:\nL(\u03c0, \u03bb) = $\\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{N} [w_i u_i(t) - \\lambda v_i(t)Q_i(t)] + \\lambda E_0$,\nwhere is the control policy and A is the Lagrange multiplier. The policy a determines the decision value vi(t) explicitly and influences the number of served jobs ui(t) implicitly. We denote g(x) as the Lagrange dual function for a given Lagrange multiplier \u5165:\ng(\u03bb) = $\\max_{\\pi} L(\\pi, \\lambda) = L(\\pi^*(\\lambda), \\lambda)$,\nwhere the maximizer is denoted as \u03c0*(\u03bb).\nRemark 1. As shown in [72, 22], the optimal timely throughput, denoted by T*, is equal to the optimal value of the dual problem, i.e., T* = $\\min_{\\lambda \\geq 0} g(\\lambda) = g(\\lambda^*)$, where X* is the optimal Lagrange multiplier. Moreover, the optimal policy \u03c0*(1*) can be obtained by computing the dual function g(1) for some A and applying gradient descent to find the optimal X*, as detailed in [72, 22]. According to [22], the derivative of the dual function g(\u5165) is given by $\\frac{\\partial L(\\pi^*(\\lambda), \\lambda)}{\\partial \\lambda}= E_0 - E_{\\pi^*(\\lambda)}$, where $E_{\\pi^*(\\lambda)} = \\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} E_{\\pi^*(\\lambda)}(t)$ denotes the average resource consumption under the optimal policy \u03c0*(\u03bb).\nWe emphasize that, although our formulation builds upon the work of [72, 22], our setting is significantly different. Our goal is to learn an efficient scheduling policy solely from pre-collected data, without interacting with the system. Furthermore, our formulation ensures adherence to resource and delay constraints using the Lagrangian dual and delay-sensitive queues, all in a fully offline manner."}, {"title": "3.1.2 Scalability in Multi-hop Setting", "content": "We consider a similar multi-hop formulation as in [72, 22, 43]. In a multi-hop network, each data flow spans multiple hops, requiring the scheduler to decide which flows to serve and how to allocate resources at each intermediate node. We consider a multi-hop network consisting of N flows (or users), denoted by {F1, F2, . . ., FN }, and K nodes, indexed as {1, 2, . . ., K}. The length of the path for flow i is denoted by Ji, and the longest path among all flows is given by J = max1<i<N Ji.\nThe routing of each flow i is encoded in a matrix H(i) = (h) \u2208 RJXK, where h = 1 indicates that the j-th hop of flow i corresponds to node k, and h = 0 otherwise. At time t, the number of jobs arriving for flow i is expressed as A\u00bf(t), with each job subject to a strict delay requirement Ti. Furthermore, each node must adhere to an average resource constraint E(k).\nThe multi-hop scheduling problem can be viewed as an extension of multiple single-hop scheduling problems, tailored to account for the unique characteristics of multi-hop scenarios. These characteristics include the buffer model, scheduling and service mechanisms, and the overarching system objectives specific to multi-hop networks.\nAggregated Buffer Model In multi-hop networks, buffers are represented as delay-sensitive queues, similar to those in single-hop networks. However, due to the requirement for jobs to traverse multiple hops to reach their destinations, the buffer model must account for the flow of jobs across the entire network. To effectively describe the system state, we introduce an aggregated buffer model that organizes flows based on their originating nodes, as illustrated in Figure 2."}, {"title": null, "content": "For a flow i with a path length of Ji, the buffer state at the j-th hop (1 \u2264 j \u2264 Ji) is denoted by\nQ) (t) = [Q,0) (5,0) (t), Q1,1) (t), ...,Q,Ti-j+1) (t)]. Here, Q) Q (t) represents the number of jobs in\nflow i at the j-th hop with 7 time slots remaining until expiration, where 0 \u2264 r < Ti \u2212 j + 1. For\nhops beyond the path length of flow i (Ji < j < J), the buffer state is set to Q) (t) = 0, indicating\nthe absence of flow i in these hops.\nThis formulation leads to the definition of J aggregated buffers, represented as Q(1) (t), Q(2) (t), . . .,\nQ(J)(t). Each aggregated buffer Q(i) (t) consolidates the states of all flows at the j-th hop, such\nthat Q(i)(t) = [Q(3) (t), Q\u00ba (t), ..., Q (t)] for 1 \u2264 j \u2264 J.\nMulti-hop Scheduling and Service Model At each time slot t, the scheduler is responsible\nfor deciding which flows to prioritize and determining the resource allocation for each node. The\nresources allocated to flow i are represented by vi(t) = [v (1) (t), v. 2) (t), ..., v) (t)], where v) (t)\nspecifies the resources assigned to the j-th hop of flow i, for 1 \u2264 j \u2264 Ji. Consequently, the complete\nscheduling decision for all flows at time t is denoted as V(t) = [V1(t), V2(t), . . ., vv(t)].\nEach scheduled job must traverse the service channels along its designated flow path. The service\nchannel condition between nodes i and jat time t is denoted by Cij(t). For a job belonging\nto flow i, given an allocated resource v and channel condition c, the probability of successful\nservice is Pi(v, c). The instantaneous resource utilization at node k during time slot t is defined as\nE(k) (t) = $\\sum_{i=1}^{N}\\sum_{j=1}^{J_i} h_{j k}^{(i)} (v_{i}^{(j)} (t)Q_{i}^{(j)}(t))$, where $h_{j k}^{(i)}$ represents the association of flow i's j-th hop\nwith node k, and Q(3) Q) (t) denotes the buffer state. The average resource utilization for node k is\ngiven by E(k) = $\\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} E^{(k)} (t)$.\nMulti-hop System Objective and Lagrange Dual The number of jobs successfully served\nfor flow i at time slot t is denoted by ui(t). Each flow is assigned a weight wi, and the weighted\ninstantaneous throughput is given by D(t) = $\\sum_{i=1}^{N} w_i u_i(t)$. The scheduler's primary objective is to\nmaximize the weighted average throughput, defined as D = $\\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} D(t)$, while ensuring\ncompliance with the average resource consumption constraints at each node (similar to Equation\n(4) in [22]).\nPm:\n$\\max_{\\nu_i(t):1 \\leq i \\leq N,1 \\leq t \\leq T} \\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{N} w_i u_i(t)$,\ns.t. $\\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{N} \\sum_{j=1}^{J_i} h_{j k}^{(i)} (v_{i}^{(j)} (t)Q_{i}^{(j)}(t)) \\leq E_0^{(k)}$, $1 \\leq k \\leq K$,"}, {"title": null, "content": "where Ek) represents the average resource constraint at node k. The Lagrangian dual function for\nthe multi-hop optimization problem Pm is expressed as:\nLm(\u03c0,\u03bb) = $\\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{N} w_i u_i(t) - \\sum_{k=1}^{K} \\lambda_k \\sum_{j=1}^{J_i} h_{j k}^{(i)} (v_{i}^{(j)} (t)Q_{i}^{(j)}(t)) + \\sum_{k=1}^{K} \\lambda_k E_0^{(k)}$,\nwhere x = [1, 2, ..., \u03ba] is the vector of Lagrange multipliers.\nThe Lagrange dual function, given a fixed multiplier \u5165, is defined as:\ngm(X) = max Lm(\u03c0, 1) = Lm(\u03c0\u03b7 (\u03bb), \u03bb),\nwhere \u03c0(\u5165) represents the policy that maximizes the Lagrangian function. To derive the optimal\npolicy \u03c0* (*), the dual function gm(\u5165) is optimized using gradient descent to identify the optimal\nmultiplier X*."}, {"title": "3.1.3 Offline Learning", "content": "Although our formulation bears similarities to those in [72, 22, 43], our offline learning-based\nscheduling setting introduces unique challenges. Specifically, the algorithm cannot interact with\nthe environment during training and must rely entirely on a pre-collected dataset D, which is\ngenerated by an unknown behavior policy that governs the data collection process. Importantly,\nthe quality of D may be limited or suboptimal, as it reflects the biases and imperfections of the\nbehavior policy. The objective, therefore, is to train an effective policy based solely on the dataset,\nwithout any online interaction with the system."}, {"title": "3.2 The MDP Formulation", "content": "We present the Markov Decision Process (MDP) formulation for our scheduling problem, which\nallows SOCD to determine the optimal policy \u03c0(\u03bb). Specifically, an MDP is defined by the tuple\nM = <S, A, r, P, y), where S represents the state space, A is the action space, r is the reward\nfunction, P is the transition matrix, and y is the discount factor. The overall system state st includes\nA(t), Q1(t), ..., QN(t), c(t), and possibly other relevant information related to the underlying\nsystem dynamics. At each time slot t, the action is represented by at = [v1(t), ..., vv(t)]. Similar\nto [43], to take the resource constraint into consideration, we set the reward of the MDP to be\nTt = D(t) \u2212 XE(t),\nwhere D(t) is the instantaneous weighted throughput, and E(t) is the resource consumption\nweighted by A.\nThe objective of the optimal agent is to learn an optimal policy \u03c0(\u00b7; \u03b8), parameterized by \u03b8, which"}, {"title": null, "content": "maps the state to the action space in order to maximize the expected rewards:\nJ(\u03c0(-;0)) = $E_{\\tau} [\\sum_{t=0}^{T} \\gamma^t r(s_t, a_t) | s_0, a_0, \\pi(\\cdot;\\theta)]$ .\nRemark 2. By considering the above MDP formulation, our objective is to maximize the Lagrange\nfunction under a fixed X, i.e., to obtain the Lagrange dual function in Equation (3). Specifically, the\nreward setting in Equation (7) implies that when y = 1, the cumulative discounted reward becomes\nR = =1Yrt = TL(\u03c0, \u03bb) \u2013 \u03bb\u03a4\u03950. Therefore, an algorithm that maximizes the expected rewards\nJ(\u03c0(\u00b7; $)) also maximizes the Lagrangian function, which is the objective of SOCD as detailed in\nSection 4. Furthermore, we emphasize that our goal is to learn an efficient scheduling policy based\npurely on pre-collected data without interacting with the system."}, {"title": "4 SOCD: Offline RL-based Scheduling", "content": "This section provides a comprehensive description of SOCD, our offline RL-based scheduling algo-rithm. The SOCD framework comprises two fundamental components designed to address offline scheduling tasks: the critic-guided diffusion policy for offline reinforcement learning (Section 4.1)and the Lagrange multiplier optimization (Section 4.2). The overall structure of the proposed SOCD algorithm is depicted in Figure 3."}, {"title": "4.1 Offline RL Maximizer with Diffusion-based Policy", "content": "In this subsection, we introduce our offline reinforcement learning (RL) algorithm, which consists of\ntwo key components: a diffusion model for behavior cloning and a critic model for learning the state-\naction value function (Q-value). The primary objective is to maximize the Lagrangian function in\nEquation (3) for a fixed Lagrange multiplier \u5165, without requiring online interactions.\nWe begin by considering the general offline RL setting, where the Markov Decision Process (MDP)is defined as <S, A, r, P, y), and the offline dataset Du = {T; = = (s, a,..., s, a,..., s,a)}j=1"}, {"title": null, "content": "is collected by a behavior policy \u03bc(\u00b7|s). We define the state-action value function (i.e., the critic\nmodel) as Q(s, a)\u00b9 and the policy as \u03c0(\u00b7|s).\nTo ensure the learned policy remains close to the behavior policy while maximizing the expected\nvalue of the state-action functions, prior work [62, 55] has explicitly constrained the policy using\nthe following formulation:\n$\\arg \\max_{\\pi} \\int_{S} \\rho_{\\mu}(s)  [ \\int_{A}  \\pi(a|s)Q(s, a)da - \\frac{1}{\\alpha}\\int_{A} \\rho_{\\mu}(a|s) D_{KL}(\\pi(\\cdot|s) || \\mu(a|s))da ] ds $.\nHere, \u03c1\u03c0(s) = \u03a3=0 \u03a5\u03c0\u03a1\u03c0(Sn = s) denotes the discounted state visitation frequencies induced bypolicy \u03c0, with p\u03c0(sn = s) representing the probability of the event {sn = s} under policy \u03c0.The optimal policy \u03c0* of Equation (9) can be derived using the method of Lagrange multipliers[62, 55]:\n$\\pi^*(a|s) = \\frac{1}{Z(s)} \\mu(a|s)\\exp{(\\alpha Q(s, a))}$,\nwhere Z(s) is the partition function and a is he temperature coefficient.\nA key observation based on Equation (10) is that \u03c0* can be decoupled into two components: (i) a\nbehavior cloning (BC) model \u03bc\u0473(\u00b7|s) trained on D\u201c parameterized by \u03b8, which is irrelevant to the\nLagrange multiplier \u5165, and (ii) a critic model Q$(s, a) parameterized by \u4e2d, which is influenced by\nthe Lagrange multiplier \u03bb."}, {"title": "4.1.1 Diffusion-based Behavior Cloning", "content": "It is critical for the BC model to express multi-modal action distributions, as previous studies [83, 7]\nhave pointed out that uni-modal policies are prone to getting stuck in suboptimalities. Moreover,\noffline RL methods often suffer from value overestimation due to the distributional shift between\nthe dataset and the learned policy [36], underscoring the importance of maintaining high fidelity in\nthe BC model. To address these challenges, we propose cloning the behavior policy using diffusion\nmodels, which have demonstrated significant success in modeling complex and diverse distributions\n[65].\nWe adopt the score-based structure introduced in [76]. To be specific, the forward process of\nthe diffusion model gradually adds Gaussian noise to the action distribution conditional on the\nstate given by the behavior policy, i.e., po(ao|s) = \u03bc(ao|s), such that the conditional transition\nfunction is given by pto(at|ao, s) = N(at; atao, \u03c3\u1f76\u0399) for some at, \u03c3\u03c4 > 0 and with a sufficiently\nsmall \u0430\u0442, \u0440\u0442\u043e(\u0430\u0442|ao, s) \u2248 N(\u0430\u0442; 0, I).2 This corresponds to a stochastic differential equation"}, {"title": null, "content": "(SDE) dat = f(at,t)dt+g(t)dwt, where wt is a standard Wiener process and f(\u00b7,t), g(t) are calleddrift coefficient and diffusion coefficient, respectively [76]. Following the Variance Preserving (VP)SDE setting as introduced in [76], we choose at = $e^{-\\int_0^t w(s)ds}$ and of = 1 \u2212 $e^{-\\int_0^t w(s)ds}$, wherew(t) = (wmax \u2013 @min) t + @min_such that f(at,t) = $\\frac{at \\frac{d \\log \\sigma_t}{dt}}{\\sigma_t}$, g\u00b2(t) = $\\frac{at \\frac{d \\log \\sigma_t}{dt}}{\\sigma_t}$.Based on the findings in [76, 30], a corresponding reverse stochastic differential equation (SDE)process exists, enabling the transition from T to 0 that guarantees that the joint distributionP0:T(a0, a1, ..., \u0430\u0442) are consistent with the forward SDE process. This process is detailed in Equation(2.4) of [48] that considers the state s as conditioning variables:\ndat = $[f(at, t) - g^2(t) \\nabla_a \\log p_t(a_t|s) ]dt + g(t)dwt$,\nwhere pt(ats) = Sao Pto(at ao, s)\u00b5(ao|s)dao is the state-conditioned marginal distribution of atincluding the unknown data distribution \u00b5, and wt represents a standard Wiener process whentime flows backwards from T to 0.\nTo fully characterize the reverse SDE process described in Equation (11), it is essential to obtainthe conditional score function, defined as VatPt(at|s), at each t. The score-based model is to learnthe conditional score model se(at, s, t) that estimates the conditional score function of the behavioraction distribution by minimizing the score-matching objective [76]:\nL(0) = $E_{(s, a)~Dr,\\epsilon, t}[||\\sigma_t S_{\\theta}(a_t+\\sigma_t\\epsilon, s, t) + \\epsilon||^2]$,\nwhere \u2208 ~ N(0, I), t ~ U(0,T). When sampling, the reverse process of the score-based model solves a diffusion ODE from time T to time 0 to obtain ao ~ \u03bc\u03bf(\u00b7|s):\ndat = $[f(ar,t) - \\frac{1}{2}g^2(t)s_{\\theta}(a_r, s,t) ] dt$.\nwith ar sampled from N(0, I). In this manner, we can effectively perform behavior cloning usinga diffusion policy.\nIt is important to note that training the diffusion policy is independent of the Lagrange multiplier\u5165, which means that the policy only needs to be trained once compared with other offline RL-basedscheduling algorithm, e.g., SOLAR [43] needs multiple iterations of the policy for different Lagrangemultiplier \u03bb. This significantly enhances the model's reusability and reduces training time."}, {"title": "4.1.2 Sampling-free Critic Learning", "content": "The objective of training the state-action value function Q$(s, a) is to minimize the temporal\ndifference (TD) error under the Bellman operator, which is expressed as:\nL(4) = $E_{(s, a, r, s')~D_{\\mu}, a'~\\pi(\\cdot|s')}[(7 \\min_{k=1,2} Q_{\\phi_k}(s', a') +r - Q_{\\phi}(s, a))^2]$.\nNevertheless, calculating the TD loss requires generating new (and potentially unseen) actions a',\nwhich is computationally inefficient, especially when relying on multiple steps of denoising under\ndiffusion sampling. Additionally, generating out-of-sample actions introduces potential extrapola-\ntion errors [17], which can degrade the quality of value estimates.\nTo address these challenges, we propose a sampling-free approach for training the critic by directly\nusing the discounted return as the target. Specifically, for each trajectory T = (81,A1, r1,..., St\n,at,rt,..., ST, \u0430\u0442, r\u0442) with rewards calculated via Equation (7), which is randomly sampled from\nthe offline dataset D\u2122, we define the critic's training objective as:\nL($) = $E_{t~U(1, T), T~D_{\\mu}} [(7 \\min_{k=1,2} Q_{\\phi_k}(s_t, a_t) - \\sum_{i=t}^{T} \\gamma^{i-t}r_i)^2]$.\nBy incorporating the double Q-learning technique [20], this approach ensures stable loss convergence\nduring training. Additionally, it helps mitigate the overestimation bias in Q-value estimation,\nleading to consistently strong performance during evaluation. The proposed method has shown to\nbe both robust and computationally efficient, making it a reliable strategy for training the critic in\noffline RL settings."}, {"title": "4.1.3 Selecting From Samples", "content": "We can generate the final output action from the trained BC model (diffusion policy) under the guid-\nance of the trained critic model. The action-generation procedure is shown in Algorithm 1.\nAccording to Equation (10), for a given state s, we can generate the action a' ~ \u03c0*(\u00b7|s) using\nimportance sampling. Specifically, K actions {a(k)}K_1 are sampled using the diffusion-based BC\nmodel \u03bce(s) (Line 3, Line 4). The final action a' ~ \u03c0*(\u00b7|s) is then obtained by applying the\nimportance sampling formula (Line 7):\na' = $\\sum_{k=1}^{K} \\frac{\\exp{(\\alpha Q_{\\phi}(s, a_i^{(k)}))}}{\\sum_{l=1}^{K} \\exp{(\\alpha Q_{\\phi}(s, a_i^{(k)}))}} a_i^{(k)}$.\nThis formula computes a weighted combination of actions a(k) based on their Q-values Q(s, a(k)),\nwhere a is the temperature coefficient that governs the trade-off between adhering to the behavior\npolicy and selecting actions that are more likely to yield higher rewards according to the Q-value"}, {"title": null, "content": "function.\nEmpirically", "as": "na' = argmax Q(s, a(k)),\nwhich involves simply choosing the action with the highest Q-value among the K sampled actions.\nThis approach simplifies the selection process while still leveraging the value function to select\nhigh-reward actions.\nIn both scenarios, our final action is essentially a weighted combination of the actions output\nby the BC model, which leverages the expressivity and generalization capability of the diffusion\nmodel while significantly reducing extrapolation errors introduced by out-of-distribution actions\ngenerated.\nNote that a larger value of K offers distinct advantages. In the importance sampling case, increasing\nK improves the approximation of the target distribution \u03c0*(\u00b7|s). In the case of selecting the action\nwith the maximum Q-value, a larger K provides a broader set of options, thus increasing the\nlikelihood of selecting the optimal action. Therefore, we use the DPM-solver [48"}]}