{"title": "Bhasha Verse : Translation Ecosystem for Indian Subcontinent Languages", "authors": ["Vandan Mujadia", "Dipti Misra Sharma"], "abstract": "Generative Large Language Models (LLMs) have brought transformative advancements to natural language processing (NLP), excelling in a wide range of applications (Xuanfan and Piji 2023; Xi et al. 2023). These models demonstrate remarkable capabilities in open-domain question answering by generating accurate and coherent responses and performing instruction-based tasks such as code completion, error detection, and correction (Vaithilingam, Zhang, and Glassman 2022). Additionally, LLMs are highly effective in tasks like essay writing, grammar correction (Wu et al. 2023), and text summarization, producing outputs of exceptional quality (Chang et al. 2023). However, much of this progress has been concentrated on English, leaving many other languages underexplored or underdeveloped (Lai et al. 2023; Zhu et al. 2023). For machine translation (MT) and related subtasks, encoder-decoder-based models remain state-of-the-art, while decoder-only models require further advancement to effectively handle low-resource languages, particularly those spoken in India.\nIndia's linguistic diversity is immense, comprising 22 scheduled languages from linguistic families such as Indo-Aryan, Dravidian, Tibeto-Burman, and Austroasiatic. There are over 559 classified mother tongues spoken across the country, reflecting the cultural and linguistic richness of the nation\u00b9. This diversity, however, poses significant challenges in communication, education, business, healthcare, tourism, and governance. Advances in encoder-decoder and LLM-based models hold the potential to address these linguistic challenges by enabling cross-lingual communication and fostering India's multilingual ecosystem.\nTranslation plays a vital role in addressing linguistic diversity and enabling effective cross-lingual communication in multilingual societies like India. However, the distinct characteristics of Indian languages present unique challenges. Many Indian languages exhibit complex morphological structures, often involving agglutinative or inflectional morphology, which makes them syntactically diverse and distinct from each other and English. The use of diverse scripts, such as Devanagari, Tamil, Bengali, and Gurmukhi, adds further complexity to text processing tasks. Additionally, code-mixing, a common phenomenon in India where speakers frequently blend multiple languages in communication, complicates translation workflows. The scarcity of high-quality linguistic resources, annotated corpora, and evaluation benchmarks for many Indian languages exacerbates these challenges, hindering the development of robust MT systems.\nAddressing these challenges requires a systematic approach. Developing translation systems for Indian languages is crucial to facilitate communication and preserve linguistic diversity. With the translation system, robust evaluation frameworks are needed to assess their quality, with or without reference translations. Identifying and categorizing translation errors is essential for system improvement, enabling targeted refinements based on predefined error groups. Automatic post-editing systems can then help correct translation errors efficiently, either independently or in collaboration with human translators. Incorporating these advancements across general and domain-specific applications will greatly benefit the language research community and end-users.\nThis paper focuses on developing translation models and related applications for 36 Indian languages, including Assamese, Awadhi, Bengali, Bhojpuri, Braj, Bodo, Dogri, English, Konkani, Gondi, Gujarati, Hindi, Hinglish, Ho, Kannada, Kangri, Kashmiri (Arabic and Devanagari), Khasi, Mizo, Magahi, Maithili, Malayalam, Marathi, Manipuri (Bengali and Meitei), Nepali, Oriya, Punjabi, Sanskrit, Santali, Sinhala, Sindhi (Arabic and Devanagari), Tamil, Tulu, Telugu, and Urdu. Achieving this requires parallel and other types of corpora for all 36 \u00d7 36 language pairs, addressing challenges like script variations, phonetic differences, and syntactic diversity. For instance, languages like Kashmiri and Sindhi, which use multiple scripts, demand script normalization for alignment, while low-resource languages such as Khasi and Santali require synthetic data augmentation to ensure sufficient coverage and quality.\nTo address these challenges, this work proposes strategies for corpus creation by leveraging existing resources, developing parallel datasets, generating domain-specific corpora, and utilizing synthetic data techniques. Additionally, it evaluates machine translation across various dimensions, including standard and discourse-level translation, domain-specific translation, reference-based and reference-free evaluation, error analysis, and automatic post-editing. By integrating these elements, the study establishes a comprehensive framework to improve machine translation quality and enable better cross-lingual communication in India's linguistically diverse ecosystem.\nFinally, this work addresses several translation-related applications that are central to enhancing multilingual communication.", "sections": [{"title": "1. Introduction", "content": "Generative Large Language Models (LLMs) have brought transformative advancements to natural language processing (NLP), excelling in a wide range of applications (Xuanfan and Piji 2023; Xi et al. 2023). These models demonstrate remarkable capabilities in open-domain question answering by generating accurate and coherent responses and performing instruction-based tasks such as code completion, error detection, and correction (Vaithilingam, Zhang, and Glassman 2022). Additionally, LLMs are highly effective in tasks like essay writing, grammar correction (Wu et al. 2023), and text summarization, producing outputs of exceptional quality (Chang et al. 2023). However, much of this progress has been concentrated on English, leaving many other languages underexplored or underdeveloped (Lai et al. 2023; Zhu et al. 2023). For machine translation (MT) and related subtasks, encoder-decoder-based models remain state-of-the-art, while decoder-only models require further advancement to effectively handle low-resource languages, particularly those spoken in India.\nIndia's linguistic diversity is immense, comprising 22 scheduled languages from linguistic families such as Indo-Aryan, Dravidian, Tibeto-Burman, and Austroasiatic. There are over 559 classified mother tongues spoken across the country, reflecting the cultural and linguistic richness of the nation\u00b9. This diversity, however, poses significant challenges in communication, education, business, healthcare, tourism, and governance. Advances in encoder-decoder and LLM-based models hold the potential to address these linguistic challenges by enabling cross-lingual communication and fostering India's multilingual ecosystem.\nTranslation plays a vital role in addressing linguistic diversity and enabling effective cross-lingual communication in multilingual societies like India. However, the distinct characteristics of Indian languages present unique challenges. Many Indian languages exhibit complex morphological structures, often involving agglutinative or inflectional morphology, which makes them syntactically diverse and distinct from each other and English. The use of diverse scripts, such as Devanagari, Tamil, Bengali, and Gurmukhi, adds further complexity to text processing tasks. Additionally, code-mixing, a common phenomenon in India where speakers frequently blend multiple languages in communication, complicates translation workflows. The scarcity of high-quality linguistic resources, annotated corpora, and evaluation benchmarks for many Indian languages exacerbates these challenges, hindering the development of robust MT systems.\nAddressing these challenges requires a systematic approach. Developing translation systems for Indian languages is crucial to facilitate communication and preserve linguistic diversity. With the translation system, robust evaluation frameworks are needed to assess their quality, with or without reference translations. Identifying and categorizing translation errors is essential for system improvement, enabling targeted refinements based on predefined error groups. Automatic post-editing systems can then help correct translation errors efficiently, either independently or in collaboration with human translators. Incorporating these advancements across general and domain-specific applications will greatly benefit the language research community and end-users.\nThis paper focuses on developing translation models and related applications for 36 Indian languages, including Assamese, Awadhi, Bengali, Bhojpuri, Braj, Bodo, Dogri, English, Konkani, Gondi, Gujarati, Hindi, Hinglish, Ho, Kannada, Kangri, Kashmiri (Arabic and Devanagari), Khasi, Mizo, Magahi, Maithili, Malayalam, Marathi, Manipuri (Bengali and Meitei), Nepali, Oriya, Punjabi, Sanskrit, Santali, Sinhala, Sindhi (Arabic and Devanagari), Tamil, Tulu, Telugu, and Urdu. Achieving this requires parallel and other types of corpora for all 36 \u00d7 36 language pairs, addressing challenges like script variations, phonetic differences, and syntactic diversity. For instance, languages like Kashmiri and Sindhi, which use multiple scripts, demand script normalization for alignment, while low-resource languages such as Khasi and Santali require synthetic data augmentation to ensure sufficient coverage and quality.\nTo address these challenges, this work proposes strategies for corpus creation by leveraging existing resources, developing parallel datasets, generating domain-specific corpora, and utilizing synthetic data techniques. Additionally, it evaluates machine translation across various dimensions, including standard and discourse-level translation, domain-specific translation, reference-based and reference-free evaluation, error analysis, and automatic post-editing. By integrating these elements, the study establishes a comprehensive framework to improve machine translation quality and enable better cross-lingual communication in India's linguistically diverse ecosystem.\nFinally, this work addresses several translation-related applications that are central to enhancing multilingual communication."}, {"title": "2. Corpora: Existing Machine Translation Corpora", "content": "Corpora are essential for developing language processing systems, especially for low-resource, morphologically complex languages like those in the Indian subcontinent. They underpin the training, evaluation, and tuning of machine translation (MT) models, ensuring diverse and accurate outputs. This section explores existing and newly developed corpora for MT, focusing on preparation, cleaning, alignment, and generation. Indian languages pose unique corpus creation challenges due to script, grammar, and cultural diversity. We use a mix of curated parallel corpora, domain-specific datasets, and synthetic data to address these, facilitating robust translation models for 36 Indian languages and their combinations."}, {"title": "3. Developed Corpora : Machine Translation", "content": "In this section, we detail the methodologies that we used to build corpora for training machine translation (MT) systems, focusing on Indian subcontinent languages. We explore alignment techniques, domain-specific corpora development, synthetic data generation methods, and corpus cleaning methodologies to build more than 1B parallel corpora for machine translation involving 36 Indian subcontinent languages."}, {"title": "3.1 Automatic Alignment and Human Validation", "content": "We collected data from various multilingual websites and books containing content in English and multiple Indian languages. Using a neural alignment tool\u00b9\u00b2 based on the COMET-QE model (Rei et al. 2022b), we calculated sentence-level similarity scores to align multilingual webpage content. This allowed us to identify English sentences and their corresponding translations in other languages, facilitating the creation of parallel corpora. For each language pair, specific alignment thresholds\u00b9\u00b3 were applied, enabling the extraction of aligned data for pairs such as English-Assamese, English-Bangla, English-Gujarati, English-Hindi, English-Kannada, English-Malayalam, English-Marathi, English-Odia, English-Punjabi, English-Tamil, and English-Telugu.\nThe process presented challenges due to the diverse sentence structures, significant word order variations, and rich morphological features inherent to Indian languages. To address these, we conducted human validation on 10% of the sampled data to assess and adjust the alignment thresholds, ensuring the creation of high-quality parallel corpora suitable for training machine translation systems."}, {"title": "3.2 Human Post-edited Education and Medical Domain Parallel Corpora", "content": "Domain-specific corpora are essential for developing machine translation (MT) systems that perform exceptionally well in specialized areas such as medicine and technical content. These corpora capture critical domain-specific terminology, language expressions, and contextual nuances, disfluencies managed in translation if the content is spoken, which are often missing from general-purpose datasets.\nTo build parallel corpora, we enlisted multiple freelance translation experts to translate text from English into Indian languages, including Assamese, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Tamil, and Telugu. The primary focus was on educational domains, utilizing text collected from technical lectures, including those sourced from the Swayam\u00b9\u2074 and NPTEL\u00b9\u2075 platforms. For medical data, the corpus included content from various medical research protocols for research studies, such as consent forms, information sheets, and medical awareness materials. These were obtained from a well-known private Christian minority community-run medical college and hospital. We first transcribed these video contents verbatim with the help of respective language experts, ensuring accurate transcription. Subsequently, post-editing was performed on machine-translated content generated using multiple translation engines, such as SSMT\u00b9\u2076 and Google Translate\u00b9\u2077. Given the technical nature of the task, we provided clear guidelines to the translators to ensure consistent quality and adherence to domain standards. The guidelines included:\n\u2022\tThorough Understanding: Translators need to carefully read the entire source text before beginning the translation process to grasp its meaning and context fully.\n\u2022\tFaithfulness to the Source Translations need to accurately capture the meaning of each sentence and remain as clear and understandable as the original text as a whole.\n\u2022\tRigorous Review It is advised to review own work multiple times, both silently and aloud, to ensure clarity, coherence, and appropriateness of word choice.\n\u2022\tConsistency in Terminology Technical terms and expressions are have to be used consistently throughout the translation. Translators need to rely on established term translations present in resources like NCERT textbooks\u00b9\u2078. If a term lacked a clear translation, it is to be transliterated.\n\u2022\tContextual Alignment The overall translation has to convey the same natural message and context as the original text.\nTo ensure quality, an in-house team of experienced translators (4 to 5 experts for each language pair) rigorously validated each sentence translated by freelancers follow-ing the same guidelines. Discrepancies or deviations from the guidelines are corrected by reviewers as part of the process. All these activities are conducted using a custom-built collaborative language platform, PostEditMe\u00b9\u2079, which effectively managed work-flows for translation, post-editing, validation, and task assignments throughout this effort. The developed human post-edited parallel corpora contain healthcare and ed-ucational domains spanning over a wide range of sub-domains, including Psychology, Natural Sciences, Teaching Methods, Mathematics, Political Science, History, Commu-nication Skills, Law, Tourism, Computer Science, Marketing, Management, Textile, Eco-nomics, and Health, ensuring comprehensive coverage of diverse academic disciplines. Notably, the post-edited parallel corpora were developed with English as the source language, and the same English text was post-edited into other Indian languages, enabling the creation of n-way parallel corpora among Indian languages."}, {"title": "3.3 Quality Synthetic Parallel Corpora", "content": "Synthetic corpora can address the paucity of parallel data for Indian languages. To bridge this gap, we utilized various techniques, including pivot-based translation, back-translation, and forward translation at the sentence and paragraph level. These methods significantly enhance the scope of existing corpora, expanding their coverage and offer-ing vital support for low-resource language pairs."}, {"title": "3.3.1 Pivoted Parallel Corpora.", "content": "Most of the parallel corpora developed for Indian languages focus on translations involving English, with relatively few efforts dedicated to creating corpora for Indian-to-Indian language translations. However, such corpora are crucial for addressing the linguistic diversity and translation needs of the Indian subcontinent. Pivot-based translation offers an effective solution to the lack of direct parallel corpora for underrepresented language pairs.\nIn this work, we enrich Indian-to-Indian language corpora by generating high-quality synthetic data using pivot translation. Our approach uses well-represented languages such as English and Hindi as intermediary \"pivot\" languages to bridge the gap between other low-resource Indian languages. We leveraged existing corpora for these languages and employed SSMT Translator\u00b2\u2070 to translate from Indian languages to English, and then from English to other Indian languages under consideration. Similarly, translations are generated between Indian languages and Hindi, using Hindi as a pivot. This process involves creating automatic translations first between Indian languages and the pivot (English or Hindi), followed by translations from the pivot to the target Indian languages.\nTo address the varying quality of generated parallel corpora, we implemented filtering mechanisms to ensure the reliability of the training data. Advanced methods, such as COMET-QE (Rei et al. 2022a), were employed to evaluate the quality of sentence pairs by assigning quality estimation scores. Sentence pairs falling below a set threshold (determined as the average QE score across all pairs for each language pair) were filtered out, maintaining a high standard for the training datasets. To further validate this approach, randomly selected sentences from the filtered corpora were reviewed by in-house language experts. Their evaluations confirmed that the majority of the automatically scored and filtered data were valid parallel corpora, demonstrating its ef-fectiveness in supporting machine translation models and enhancing their performance and generalizability. By leveraging existing translations involving the pivot languages, this method enables the generation of new parallel corpora, thereby extending the reach of machine translation models to low-resource language pairs. This approach facilitates cross-lingual applications and fosters inclusivity in multilingual natural language pro-cessing."}, {"title": "3.3.2 Iterative Backward Translation.", "content": "We employed back-translation, a widely used data augmentation technique in machine translation, to enhance the diversity and quality of training data. Monolingual corpora for the target languages were sourced from WMT News Crawl\u00b2\u00b9 and Wikipedia\u00b2\u00b2. Using SSMT Translator\u00b2\u00b3 and IndicTrans\u00b2\u00b2\u2074, we translated this monolingual data bidirectionally: from Indian languages to English and from English to Indian languages. The translated data introduced variations in sentence structure, vocabulary, and phrasing, enriching the training set's diversity. For each monolingual corpus, both translation outputs were evaluated using COMET-QE scores, and the version with the highest score was retained. This filtered data was then used to train a multilingual machine translation model involving the selected language pairs. The newly trained model was subsequently used to re-translate the same monolingual corpora, iteratively improving the translations. This process was repeated for five iterations, with the best-scored translation pairs from all iterations being selected for the final training corpus."}, {"title": "Paragraph Level Backward Translation.", "content": "For the Wikipedia dataset\u00b2\u2075 and crawled news articles from the internet, we adopted paragraph-level backward translation to preserve coherence and contextual consistency across entire paragraphs. Unlike sentence-level back-translation, this approach ensures that the translated content retains discourse-level attributes, such as topic continuity, pronoun resolution, and lexical cohesion. By focusing on paragraph-level fidelity, this method enhances the quality of machine translation systems, particularly for tasks requiring deeper contextual understanding, such as document translation or conversational AI. The same filtering mechanism was applied here as well. Each sentence within a paragraph was evaluated for quality using COMET-QE scores, and the best-scored translations were selected. These high-quality sentence translations were then reassembled into paragraphs to maintain the original discourse structure. This strategy ensured the development of a robust multilingual machine translation model capable of handling complex and contextually rich transla-tions."}, {"title": "3.4 Parallel Corpora Cleaning", "content": "Cleaning parallel corpora is crucial for ensuring the accuracy and reliability of train-ing datasets. Noise in parallel corpora, such as duplicates, incomplete translations, misalignments, script variations, and unwanted symbols, can significantly degrade the performance of machine translation (MT) models. To address these issues, we utilized automated tools to detect and resolve errors or remove problematic sentence pairs. This process involved normalizing text and tackling script-specific challenges in Indian languages, such as script diversity and spelling inconsistencies. Additional preprocess-ing steps included removing mistranslations, addressing code-mixing, and correcting tokenization errors. We employed multiple methodologies to achieve effective corpus cleaning:\nLength-Based Filtering. This method filters parallel corpora by comparing differences in word and character counts to ensure alignment quality. For example, we calculate the average sentence length for each language and maintain an acceptable range of differences for a given language pair in terms of words and characters. This range, set as the threshold, allows for a variation of \u00b110 words (or corresponding characters). Every source and target text pair in the generated corpora is evaluated against this threshold. Sentence pairs that do not meet this criterion are discarded to maintain the quality of the dataset.\nLanguage and Script Identification. We utilized a FastText-based language detection tool to identify the language of each word within a sentence. If the majority of word-level language tags do not align with the sentence-level language tag, the sentence pair is filtered out to ensure linguistic consistency.\nHTML/XML Tag Validation. Sentence pairs with mismatched HTML or XML tags be-tween the source and target sides are removed to avoid structural inconsistencies in the data.\nCOMET-QE Scoring. The quality of source-target and target-source sentence pairs was evaluated using COMET-22 scores. Sentence pairs with scores significantly below the average threshold (average minus 10) were excluded, ensuring that only high-quality data was retained for training.\nWe applied these cleaning methods to existing, newly developed, and synthetically generated parallel corpora for Indian languages to achieve greater alignment accuracy, noise reduction, and linguistic consistency, resulting in more effective MT systems."}, {"title": "4. Automatic Post-Editing (APE) Corpora", "content": "We propose methods to enhance corpora for automatic post-editing between English and Indian languages. These include using human post-edited corpora and generating synthetic corpora from existing parallel data, providing a foundation for general and domain-specific post-editing tasks."}, {"title": "4.1 Human APE Corpora", "content": "As discussed in Section 3.2, post-editing involves human translators refining machine-generated translations to produce accurate and natural-sounding final translations. This process was conducted using the PostEditMe\u00b2\u2076 workbench, a platform designed to facil-itate post-editing tasks. The workbench allowed translators to edit machine-generated translations while maintaining a record of both the original machine-generated version and the human-corrected version. Additionally, the workbench captured all interme-diate stages of post-editing, recording multiple iterations for each sentence pair, along with the final edited version. These intermediate stages and the final post-edited text form a comprehensive resource for studying the post-editing process and its impact on translation quality. The resulting dataset, named the Bhashik APE corpus, includes the initial machine-generated (also intermediate stages text) and human-post-edited trans-lations for source texts in English and 11 Indian languages. This corpus supports the development of automatic post-editing systems and enhances domain-specific machine translation by addressing the linguistic nuances of technical and educational contexts more effectively."}, {"title": "4.2 Synthetically Developed Corpora", "content": "Automatic post-editing (APE) corpora typically consist of three key components: the source text, an imperfect or erroneous translation, and the error-free translation. To synthetically generate such APE corpora, we utilized both existing parallel corpora and our newly developed parallel corpora described in this work. The target sentences or passages are subjected to various perturbation techniques as detailed in Appendix Section 7, simulating errors commonly found in machine translations. For our opted language pairs, we processed each sentence pair to create multiple denoised versions of the target text. These perturbations introduced varying degrees of errors, ranging from 2% to 15% of the total words, including grammar mistakes, mistranslations, and word order issues. These artificially generated error patterns simulate typical MT errors, making the perturbed version function as the erroneous translation, while the original target text serves as the error-free translation. Additionally, we applied noisy back-translation techniques to create diverse post-editing pairs, ensuring a wide range of error types for robust APE training data. This approach provides a rich and diverse dataset for training automatic post-editing models. We call the resulting dataset Bhashik APE (Synth)."}, {"title": "5. Corpora for Machine Translation Evaluation", "content": "In this work, we developed machine translation assessment corpora for English and Indian languages through multiple methods to evaluate translation quality. We focus on translation evaluation which is on a continuous scale from 1 to 100, where 1 indicates poor translation and 100 represents the best possible translation.\nWe use two approaches: reference-based evaluation, comparing translations to a reference, and reference-free evaluation, assessing translations without references. Cor-pora development involves human expert scoring and synthetic data generation using perturbation techniques (Appendix Section 7) to simulate common translation errors. Human assessment corpora are created by experts scoring translations on a continuous scale, while synthetic data is generated with COMET models scoring translations across scenarios derived from parallel corpora. This section outlines the strategies for creating synthetic corpora to comprehensively evaluate machine translation systems."}, {"title": "5.1 Human Annotated Corpora", "content": "Human-annotated corpora are created by language experts who evaluate translations based on fluency, adequacy, and contextual accuracy, providing a gold standard for assessing MT systems. Experts fluent in both the source and target languages assess the accuracy of the translation in conveying the original meaning, focusing on Translation Accuracy, which determines whether the translation preserves the source text's meaning while maintaining fluency and grammatical correctness. Scores are assigned on a scale of 1 to 100.\nTo develop these corpora, we utilized benchmark datasets and multiple translation engines, including Google Translate, SSMTv3, and Indictransv2, to generate translations for input texts. Experts evaluated both the source text and the translated text, providing ratings. The reference translations in the benchmark corpora were used for reference-enabled machine translation evaluation, while the annotations can also be applied in reference-free evaluation scenarios. The dataset spans 18 language pairs and 36 trans-lation directions, resulting in over 72,000 rated translations. This effort involved over 70 experienced language experts, with each source and target translation pair receiving three independent ratings."}, {"title": "5.2 Synthetically Developed Corpora", "content": "To achieve comprehensive coverage of the translation evaluation spectrum on a linear scale, we recognized that relying solely on existing or limited human scored direct as-sessment data is insufficient. Therefore, we decided to generate additional data synthet-ically using existing parallel and benchmark corpora. Various techniques are employed to create degraded, perturbed versions of translations, and we used an existing COMET model to assign degraded scores using the following methodology:\n1.\tGenerating Perturbed Translations: We began by sourcing parallel and benchmark corpora from above mentioned repositories. For each sentence pair, we applied perturbation techniques outlined in Appendix Section 7 to generate lower-quality translations with varying degrees of errors. The error levels ranged from 5% to 50%.\n2.\tStructuring Sentence Tuples: For each perturbed version, we created a sentence tuple containing the following elements:\n\u2022\tSource text (S)\n\u2022\tOriginal translation text (T) from the parallel corpora\n\u2022\tPerturbed translation text (T) with Error percentage (Ep)\n3.\tCalculating Synthetic Scores: Using the existing COMET Quality Estima-tion (QE) model, we first evaluated the source text (S) and the original translation (T) to obtain a COMET score (Cs) and 100 minus the Translation Error Rate (TER) score (Tp) using original translation (T) and Perturbed translation text (T). We then averaged the error percentage (Ep) and the TER-derived score (Tp) to calculate a degradation factor. This factor was subtracted from the COMET score (Cs) to derive a new synthetic score for the source text (S) and the perturbed translation (T).\nBy applying this process, we generated an extensive synthetic dataset to supple-ment the limited human-annotated data, enabling robust evaluation across a broader range of translation quality scenarios. The resulting dataset is named the Bhashik PseudoMTDA corpus for English and 11 Indian languages."}, {"title": "6. Corpora for Machine Translation Error Span Identification and Error Category Marking", "content": "Error identification and categorization help analyze weaknesses in MT systems, en-abling targeted improvements. This section outlines our development of corpora to support these tasks."}, {"title": "6.1 Human MT Error Span Identification Corpora", "content": "As discussed in Section 3.2, post-editing involves human translators refining machine-generated translations to produce accurate, natural-sounding final translations. This process was carried out using the PostEditMe\u00b2\u2077 workbench, a platform specifically designed to support post-editing tasks. The workbench captures every edit made by translation experts and records this information at the word level over the machine translation output. The edited spans in the translated text, where changes were made, are marked as MT error spans for the corresponding translation. These post-editing efforts provide a rich resource for identifying error spans and evaluating translation quality. The resulting dataset, named the Bhashik PseudoMTError corpus, includes the source text, the initial machine-generated translation, and the annotated error spans for English and 11 Indian languages."}, {"title": "6.2 Synthetically Developed MT Error Span and Category Marking Corpora", "content": "As discussed earlier, corpora for machine translation (MT) error span and category marking typically include three key components: the source text, a translation annotated with error spans, and corresponding error categories. To synthetically create such corpora, we leveraged both existing parallel corpora and newly developed parallel corpora introduced in this work. Target sentences or passages were subjected to various perturbation techniques, detailed in Appendix Section 7, to simulate common machine translation errors across different categories. These categories were aligned with broader MQM categories.\nFor the selected language pairs, we processed each sentence pair to generate multiple denoised versions of the target text. Perturbations introduced errors ranging from 2% to 15% of the total words, including grammar mistakes, mistranslations, and word order issues. These artificially generated error patterns mimic typical MT errors, making the perturbed versions serve as erroneous translations. Error spans were annotated based on the introduced errors, along with the corresponding categories used to generate the perturbations.\nThis approach provides a rich and diverse dataset for training automatic systems to mark MT error spans and categories. We refer to the resulting dataset as the Bhashik PseudoMTError Corpus."}, {"title": "6.3 Developed Parallel Corpora", "content": ""}, {"title": "6.3.1 Bhashik Education and Medical: Human Post-edited Corpora.", "content": ""}, {"title": "6.3.2 Bhashik Generic : Quality Synthetic Corpora.", "content": ""}, {"title": "7. Text Perturbation", "content": "We employ various text perturbation techniques to generate task-specific corpora for translation-related applications. By leveraging existing parallel and monolingual cor-pora, we introduce controlled modifications using carefully designed methods. These denoising strategies, which include random additions, deletions, and substitutions, effectively simulate errors in spelling, grammar, word order, punctuation, and more for all 36 languages mentioned in this work. The crafted perturbation methods are outlined below:\n\u2022\tAdd/Delete/Replace Random Token: In this, random tokens are added, deleted, or replaced to simulate common noise found in real text. This introduces variability and errors in the text. For example, adding irrelevant filler words or dropping crucial terms.\n\u2022\tChange Pronoun: This perturbation introduces pronoun errors by replac-ing correct pronouns with incorrect ones, disrupting the coherence and referential clarity of the text. For example, replacing \"he\" with \"she\" or \"they\" can create errors critical for evaluating gender-sensitive, discourse-level translation tasks. To implement this, we used available POS-tagged corpora to compile a list of pronouns for each respective language. For a given input text, a random pronoun is identified and replaced with another randomly selected pronoun from the list.\n\u2022\tChange Prepositions or Postpositions: By altering prepositions or post-positions, we introduce syntactic inconsistencies and semantic shifts. For example, replacing \"on the table\" with \"under the table\" changes the spatial relationship and meaning. To implement this, we used POS-tagged corpora to compile a list of prepositions or postpositions for each language. Similar to the pronoun perturbation, we randomly select a preposition or postpo-sition in the input text and replace it with another from the predefined list. This perturbation is crucial for tasks focusing on syntax and semantic correctness in translation.\n\u2022\tChange connectives: This perturbation modifies logical connectives, intro-ducing errors that affect the flow and coherence of the text. For instance, replacing \"because\" with \"although\" or \"and\" with \"but\" creates inconsis-tencies in the logical structure of sentences. To implement this, we curated a list of commonly used connectives for POS tagged corpora of each lan-guage. A random connective in the input text is selected and substituted with another from the list. This method helps with errors in logical and discourse-level translation tasks.\n\u2022\tChange Verb Forms: This task modifies verb forms to incorrect tense, as-pect, or modality. For example, replacing \u201cwas running\u201d with \u201crun\u201d or \u201cwill ran\" introduces errors in temporal and modal constructs that the model must learn to correct.\n\u2022\tChange Lexical Cohesion: Words or phrases contributing to the overall cohesion of a passage are replaced with less cohesive random alterna-tives, disrupting the flow and making the sentence appear disjointed. This method helps with errors in logical and discourse-level translation tasks focusing on lexical cohesion.\n\u2022\tChange Punctuation: Punctuation errors, such as missing periods, mis-placed commas, or incorrect quotation marks, are introduced to mimic common typographical errors. Correcting such errors helps improve text readability and grammaticality.\n\u2022\tGrammar: In this denoising process, selected words are randomly replaced with grammatically inflected forms derived from a trie structure built us-ing language-specific monolingual corpora. This introduces variations in features like gender, number, person, and verb aspects such as tense and modality. The approach is particularly useful for Indian languages with agglutinative properties, enhancing the understanding and modeling of their complex morphological structures to improve language processing and analysis.\n\u2022\tMasking: In this task, specific words are masked to evaluate the model's ability to reconstruct missing information. Tokens are randomly replaced with a single random token using a 0.1% probability. Inspired by BART, our masking is applied at the token level before subword tokenization. This approach is particularly challenging for Dravidian languages, requiring the model to predict correct linguistic inflections during decoding.\n\u2022\tSpelling: Typographical errors are introduced into selected words, affect-ing around 0.1% of the text. Spelling perturbation involves adding, re-moving, or substituting random characters, simulating real-world spelling errors. This helps train the model for tasks requiring spelling correction.\n\u2022\tWord Order: Indian languages often have flexible word order. To address this, we apply word order perturbation by randomly rearranging groups of up to four words with a 0.1% probability. This modifies the original sequence while preserving grammaticality, helping models adapt to struc-tural variations in sentences."}]}