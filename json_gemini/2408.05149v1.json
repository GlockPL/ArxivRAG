{"title": "AttackER: Towards Enhancing Cyber-Attack Attribution with a Named Entity Recognition Dataset", "authors": ["Pritam Deka", "Sampath Rajapaksha", "Ruby Rani", "Amirah Almutairi", "Erisa Karafili"], "abstract": "Cyber-attack attribution is an important process that allows experts to put in place attacker-oriented countermeasures and legal actions. The analysts mainly perform attribution manually, given the complex nature of this task. AI and, more specifically, Natural Language Processing (NLP) techniques can be leveraged to support cybersecurity analysts during the attribution process. However powerful these techniques are, they need to deal with the lack of datasets in the attack attribution domain. In this work, we will fill this gap and will provide, to the best of our knowledge, the first dataset on cyber-attack attribution. We designed our dataset with the primary goal of extracting attack attribution information from cybersecurity texts, utilizing named entity recognition (NER) methodologies from the field of NLP. Unlike other cybersecurity NER datasets, ours offers a rich set of annotations with contextual details, including some that span phrases and sentences. We conducted extensive experiments and applied NLP techniques to demonstrate the dataset's effectiveness for attack attribution. These experiments highlight the potential of Large Language Models (LLMs) capabilities to improve the NER tasks in cybersecurity datasets for cyber-attack attribution.", "sections": [{"title": "1 Introduction", "content": "Attribution can be defined as \"determining the identity or location of an attacker or an attacker's intermediary\" [26]. This non-trivial process is crucial for responding to incidents, formulating cybersecurity policies, and addressing future threats. Knowing who the attacker is allows the defender to put in place attacker-oriented countermeasures and when possible legal actions. Cyber-attack attribution is a complex and highly resource-consuming process [15] as several measures can be taken by threat actors to hide their traces. Furthermore, it is often difficult to identify threat actors as they may share common targets and tools to carry out attacks, resulting in a limited degree of confidence in the attribution output."}, {"title": "2 Related Work", "content": "NER in cybersecurity has been extensively studied, primarily focusing on cyber threat intelligence. Early work, like [14], introduced a framework using SVM to extract vulnerability and attack information from web text. Similarly, [9] proposed an approach to identify entities, concepts, and relationships associated with cybersecurity within the text using Conditional Random Field (CRF). These approaches relied on hand-crafted features, requiring significant feature engineering effort. With the rise of neural networks, studies like [4] and [5] explored using LSTM-CRF architectures by blending LSTM, word2vec embeddings [13], and CRF for cybersecurity NER and demonstrated their effectiveness compared to traditional methods. With the advent of transformer [22] neural network architecture and its advantages over other neural network architectures, much of the research on NER in the domain of cybersecurity has shifted towards the usage of models based on transformer neural network architecture. The authors of [3] explored the usage of transformer models in NER task of cyber threat intelligence text. In [20], the authors use BERT-based models for NER on the Russian cybersecurity domain. They compared three different models where two are pre-trained over Russian text. Of these two, one of the models was pre-trained specifically on cybersecurity text. Through experimental results, the authors showed that the model pre-trained specifically on cybersecurity text performed the best out of the three models.\nRecently, there has been a significant advancement in the field of LLMs, and it has shown excellence in a variety of NLP tasks. So far, minimal research has been conducted in the direction of NER using LLMs. Wang et al. [23] proposed GPT-NER, a task generation NER to fill this gap, achieving competitive performance with supervised models, particularly in low-resource environments."}, {"title": "3 Dataset details", "content": "Let us now introduce the AttackER dataset for attack attribution and investigation. Unlike existing datasets, AttackER offers highly detailed information through a more complex annotation process, providing context beyond words or tokens regarding their respective entity types. This allows state-of-the-art transformer models to learn more robustly and provide deeper insights for cybersecurity analysts. To the best of our knowledge, no other datasets have been produced for the NER task in this domain, focusing on the analysis of cyber-attacks and their attribution.\nTo define the entity types of our dataset, we used the STIX 2.1 framework\u00b3. The STIX 2.1 objects represent different elements of cyber threats, such as indicators, threat actors, campaigns, tools, etc. which help in organizing and categorizing threat information consistently. Although STIX 2.1 defines 18 different objects, we did not include all of them, as some of these objects were not useful and were never used during the cyber-attack attribution process, as not pertinent. Thus, we include only 14 STIX 2.1 objects. Based on the Ontology for cyber-attack attribution introduced in [6], we added two new objects for the identification of entities that are not part of the STIX 2.1 but are important for the cyber-attack attribution process. The added entities deal with information like the impact of the attack (\u201cIMPACT\u201d), and the motivations (\u201cATTACK_MOTIVATION\u201d).\nDuring our exploratory analysis of the objects, we found that two of the used STIX 2.1 objects could be ambiguous for the purpose of attack attribution. Thus, we divided them (\u201cTool\u201d and \u201cIdentity\u201d) into sub-classes. We divided \"Tools\" into"}, {"title": "3.1 Entity set description", "content": "We have a set of 18 entities to label text extracted from various reports and blogs that deal with cyber-attack attribution and investigation. We followed manual labelling to assign pertinent entities to each text and later on continued with a semi-automatic approach (see Section 4.2). The details of the entities used for our dataset creation are presented in Table 1. Distribution for the average number of words for each entity type is shown in Figure 1. Notably, entity types such as \"MALWARE_ANALYSIS\" and \"COURSE_OF_ACTION\" encompass multiple words, while others like \u201cMALWARE\u201d and \u201cLOCATIONS\" consist of fewer words. By utilising these 18 labels, we annotated texts through reading and understanding, assigning each text the relevant label."}, {"title": "4 Methodology", "content": "In this section, we discuss the data annotation process and NLP models, including spaCy, HF transformers, and the LLMs fine-tuning procedure for our dataset. The three different classifications represent the progression and diversity of NLP technologies."}, {"title": "4.1 Annotation of data", "content": "To create AttackER, we automatically collected documents from various online sources. These documents pertain to reports, articles, and blogs about previous attacks written by cybersecurity experts. In these documents, past cyber-attacks were analysed and when possible attributed. We selected only documents that an analyst would be interested in reading and gathering information from (overall 217 documents). We performed a manual analysis of the collected documents where we checked that the content was suitable and made sure that they were analysing different types of attacks. The major sources from where we collected these documents include blogs and reports from Mandiant, Malwarebytes, Mitre, Securelist, Trendmicro. A full list is provided in Appendix A.\nWe used scraping text tools, like two different Python libraries namely newspaper3k4 and BeautifulSoup, to collect the data. We then performed text pre-processing on the scraped text to remove unwanted information like blank spaces, unwanted characters, and text from images and advertisements on the websites. We did not remove any stopwords or punctuation marks, as they were needed for the annotation phase. Automatic pre-processing was not applicable to all the scraped text as there were several web sources and it became difficult to tailor automatic pre-processing techniques for all of them. Thus, manual inspection was also carried out along with the automatic techniques. For the annotation part, we used a tool called Prodigy which provides a feature-rich annotation interface and a seamless integration with the spaCy [7] library."}, {"title": "4.2 spaCy model training", "content": "We explore various transformer models that encompass both general models and cybersecurity domain-specific models. We followed the spaCy training guidelines to train the transformer models over our annotated data for the NER task. The hyperparameters include a learning rate of 5e-05, a batch size of 128, a maximum sequence length of 128, and used the Adam optimizer. The loss function used during training is the categorical cross-entropy loss function which is used to measure the difference between predicted probabilities and the true distribution of the labels. We experimented with different splits and achieved the best results with the train-test split of 80:20. The 80% of the data is used as training data and the 20% is further split into a 70:30 ratio, where 70% is used as the evaluation set and 30% is used as the test set which is held out for prediction. After training is completed, the trained model is used to annotate new documents with the annotators' feedback. Once new documents are annotated, they are combined with the previous annotated documents, and the model is re-trained using the same process for an improved version."}, {"title": "4.3 Huggingface model training", "content": "We use the HF library as it provides an API based platform to access state-of-the-art open-sourced transformer models for NLP tasks, including NER. We changed the format of the data since the HF transformer library uses the IOB/BIO"}, {"title": "4.4 Large language model fine-tuning", "content": "Although autoregressive LLMs such as GPT-3 have demonstrated promising results in various NLP tasks, their performance in NER tasks still falls short of supervised baselines, primarily due to their focus on text generation over sequential labelling [23]. To address this, supervised fine-tuning can be employed to fine-tune a pre-trained model for specific tasks. In particular, we decided to experiment with LLMs to enhance the NER performance on AttackER by leveraging the reasoning capabilities of LLMs. In our approach, we use a specialized fine-tuning technique called instruction fine-tuning to guide the LLMs' output towards the NER task requirements. In instruction-based fine-tuning, we provide prompts with examples to help models learn and improve response accuracy. For this purpose, the data format used in the HF experiments was transformed into an instruction dataset, which includes instructions along with the desired output conforming to the LLMs' accepted prompt template, as described in [12]. We employed two open-source state-of-the-art LLMs for our NER task: Llama-2-7b-hf (Llama-2)8 and Mistral-7B\u00ba and one OpenAI LLM model i.e., GPT-3.510. Given that LLMs outputs are influenced by the provided instructions, we experimented and selected the best prompt format for different LLMs. Table 2 presents an example prompt template\u00b9\u00b9 for Llama-2 [21] and Mistral-7B [8] models. Using the formatted dataset, we employed parameter-efficient fine-tuning coupled with quantized low-rank adaptation to enhance the efficiency and reduce the resource requirements of the fine-tuning process. For the zero-shot learning experiments (these are the base models that use their existing knowledge and learn to solve unseen tasks) using the base Llama-2 LLM model, we provide the prompt template employed in Appendix B."}, {"title": "5 Experimental Results and Discussion", "content": "We now discuss the experimental results using spaCy, transformer-based models, and autoregressive LLMs for NER on our dataset. We evaluated various transformer models for NER, including generic models like RoBERTaand DeBERTa-"}, {"title": "5.1 spaCy and Huggingface experiments", "content": "To evaluate the models, precision, recall, and F-1 score [16] were used, with F-1 score as the final metric due to its harmonic mean property. A 100% alignment between predicted and annotated tokens was required for entity-level accuracy assessment. The results for both spaCy and HF models are presented in Table 3 for the test dataset evaluation.\nAmong the spaCy models tested, SecureBERT outperformed other BERT-based models, likely due to its training on a vast amount of cybersecurity data from various online sources, unlike other models trained on more specific cyber-related data. However, DeBERTa-v3, an improvement over BERT and ROBERTa, achieved a comparable F-1 score despite not being trained explicitly on cybersecurity data. Conversely, in the HF models, ROBERTa slightly outperformed DeBERTa-v3. Overall, the spaCy models demonstrated better performance than the HF models. Since default parameters were used for all models, it is possible that the spaCy models had more optimal hyperparameters for NER tasks compared to the HF models."}, {"title": "5.2 LLMs Experiments", "content": "For the LLMs, the base and fine-tuned models were employed to evaluate and compare the impact of fine-tuning on NER tasks for attack attribution. The"}, {"title": "6 Conclusion", "content": "In this paper, we introduced a novel dataset about cyber-attack attribution, AttackER, using NER. To the best of our knowledge, this is the first dataset to in-"}]}