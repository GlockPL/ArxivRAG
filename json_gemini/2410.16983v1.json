{"title": "Order Matters: Exploring Order Sensitivity in Multimodal Large Language Models", "authors": ["Zhijie Tan", "Xu Chu", "Weiping Li", "Tong Mo"], "abstract": "Multimodal Large Language Models (MLLMs) utilize multimodal contexts consisting of text, images, or videos to solve various multimodal tasks. However, we find that changing the order of multimodal input can cause the model's performance to fluctuate between advanced performance and random guessing. This phenomenon exists in both single-modality (text-only or image-only) and mixed-modality (image-text-pair) contexts. Furthermore, we demonstrate that popular MLLMs pay special attention to certain multimodal context positions, particularly the beginning and end. Leveraging this special attention, we place key video frames and important image/text content in special positions within the context and submit them to the MLLM for inference. This method results in average performance gains of 14.7% for video-caption matching and 17.8% for visual question answering tasks. Additionally, we propose a new metric, Position-Invariant Accuracy (PIA), to address order bias in MLLM evaluation. Our research findings contribute to a better understanding of Multi-Modal In-Context Learning (MMICL) and provide practical strategies for enhancing MLLM performance without increasing computational costs.", "sections": [{"title": "Introduction", "content": "The recent success of Large Language Models (LLMs) (Zhao et al. 2023; Brown et al. 2020; Touvron et al. 2023; Bai et al. 2023a; Xi et al. 2024) has driven researchers to explore their capabilities in tackling multimodal tasks. By aligning image features with text embeddings, researchers integrate visual inputs into LLMs, developing Multimodal Large Language Models (MLLMs) (Yin et al. 2023; Li et al. 2023a; Alayrac et al. 2022; Li et al. 2023b; Beyer et al. 2024). These models inherit the exceptional abilities of LLMs in In-Context Learning (ICL) and have demonstrated significant performance in understanding and reasoning with images and videos.\n\nA core issue in In-Context Learning for LLMs is prompt order. It has been shown in LLMs that there are better prompt orders (which vary between different models or tasks). When ordered correctly, the models perform well, whereas other orders result in performance close to random (Lu et al. 2021). The observation of prompt order sensitivity in LLMs raises two compelling questions. Question I: \"Does order sensitivity also exist in MLLMs?\" Furthermore, another question that needs to be discussed is, Question II: \"What kind of order is good for MLLM's performance?\" Current research and benchmark studies (Shi, Ma, and Vosoughi 2024; Chen et al. 2024; Liu et al. 2023b) focus on addressing a sub-question of Question I: \"Does text-only order sensitivity also exist in MLLMs?\". These works overlook the potential influence of image order and situations where both image and text orders are altered.\n\nTo fully answer Question I, we design the following experiments. As shown in Figure 1, CelebAText-HQ (Sun et al. 2021) is a facial image captioning dataset. We design a 4-shot image-text prompt and ask OpenFlamingo(Awadalla et al. 2023), a widely studied MLLM for multi-image input tasks (Yang et al. 2023; Chen et al. 2023a; Schlarmann and Hein 2023), to generate a caption for a new facial image. When the image-text-pairs are arranged in good order (as illustrated in green in Figure 1), the model can generate essentially accurate responses. However, bad orders cause"}, {"title": "Analysis", "content": "The experimental results indicate that MLLMs prefer the beginning and end of the multimodal context. When the correct answer is positioned at the beginning or end, the model's chances of accurately answering the question increase. Even if the model cannot answer the question correctly, it tends to rely on the beginning and end of the context for its response. This phenomenon is more pronounced in image-only or mixed-modality tasks than in text-only tasks. The token length of encoded images is closer to that of long texts, as shown in Table 1. This suggests that order bias may relate to the context length, with longer text lengths potentially amplifying this effect.\n\nThese results inspire us to consider: Would tasks influenced by order, such as video understanding and visual question answering (when visual prompts have an order), benefit from orders composed of these special positions? In the next section, we design two experiments: a video-caption matching task and a visual question answering task with RAG, to demonstrate the impact of prompt orders composed of special positions on these tasks."}, {"title": "Experiment", "content": "We use Video-LLaVA (Lin et al. 2023) for the video-caption matching task and IDEFICS-v2-8B-Instruct (Lauren\u00e7on et al. 2024) for the visual question answering task with Retrieval-Augmented Generation (RAG). The experimental setup can be seen in Appendix C."}, {"title": "Dataset", "content": "Video-Caption Matching. Task construction is shown in Figure 13 of Appendix C. The MVBench (Li et al. 2023c) dataset serves as the foundation. MVBench is a commonly used dataset for MLLM instruction fine-tuning (Li et al. 2023b; Zhang, Li, and Bing 2023), containing multiple tasks and video collections. We focus on the fine-grained pose video set, comprising 200 fine-grained action video clips with corresponding textual captions. Our selection criteria include action lengths between 2-3 seconds (135 video clips), which we extend to create new 10-second videos. In each new video, we designate the original 2-3 second clip as the key action and randomly sample other action clips, concatenating them sequentially until reaching 10 seconds. Each video pairs with a caption-matching question: \u201cIs anyone in the video performing the following action: action?\u201d where \"action\" represents the textual caption of the key action. The model needs to answer with \"Yes\" or \"No\".\n\nThe video frames corresponding to the key action are referred to as Key Frames (as the frames within the green boxes in Figure 13). For each video, we construct three comparative groups: we move Key Frames to the front, middle, and back, respectively, resulting in three new videos with different arrangements of Key Frames. The corresponding question and answer for each video remain unchanged.\nVisual Question Answering with RAG. Task construction can be seen in Figure 14 of Appendix C. The multiple-choice question-answering dataset MMbench (Liu et al."}, {"title": "Position-Invariant Accuracy", "content": "Existing MLLM benchmarks overlook the model's sensitivity to order, where altering the correct answer's position can significantly impact model performance. Liu et al. (Liu et al. 2023b) introduce the Circular Evaluation Strategy to address this issue. In this approach, the correct answer is cyclically shifted across M options and evaluated M times, with the problem considered solved only if the model succeeds in all M trials. However, this metric primarily assesses model robustness and fails to eliminate the inherent unfairness caused by order bias during evaluation. Consider a scenario where a model exhibits a strong tendency to select Op.A with high frequency. In this case, choosing the correct option Op.A might stem from this bias rather than true understanding, suggesting that a lower scoring weight should be applied to mitigate this tendency. Conversely, if the model rarely selects Op.B, choosing the correct option Op.B indicates high confidence and should be accorded a higher scoring weight.\n\nTo address these nuances, we propose a novel metric: Position-Invariant Accuracy (PIA). This metric is designed to account for and neutralize order biases in model responses, offering a more equitable evaluation of MLLM performance. The PIA is defined as follows:\n\nPIA =  $\\frac{1}{M} \\sum_{i=1}^{M} \\frac{C_i}{P r_i N}$ (7)\n\nwhere M represents the number of options, $C_i$ denotes the total number of times the model correctly answers the i-th option, $Pr_i$ represents the total number of times the model selects the i-th option, and N represents the total number"}, {"title": "Future Work & Conclusion", "content": "The impact of order sensitivity on MLLM's performance is significant, with models often favoring the beginning and end of multimodal context. We believe this reflects human cognitive behavior - when the context is long, people tend to forget the middle content and focus on the beginning and end. The implications of this finding are twofold. Firstly, prompting techniques like Multi-Modal In-Context Learning (MMICL) (Khattak et al. 2022; Wang et al. 2024; Wu et al. 2024) and Chain of Thought (CoT) (Wei et al. 2022; Zhang et al. 2023b; Mitra et al. 2024) heavily rely on prompt order, making it necessary to re-examine the design of these tasks given the discovery of order sensitivity. Secondly, designing or training models also needs to pay more attention to order sensitivity. This is because current training data and task construction may inherently carry biases, with questions typically positioned at the beginning and end of context. This phenomenon calls for more diverse data and tasks.\n\nIn this paper, we demonstrate that MLLMs exhibit order sensitivity and find prompt orders that can improve performance. Popular MLLMs show a preference for the beginning and end of multimodal contexts, and placing important content in these positions can enhance performance. This is beneficial for improving accuracy in order-sensitive tasks such as video understanding and visual question answering. Finally, we propose a new MLLM metric: Position Invariant Accuracy (PIA), mitigating the unfair impact of order preferences."}, {"title": "Settings", "content": "We use Video-LLaVA (Lin et al. 2023) for the video-caption matching task and IDEFICS-v2-8B-Instruct (Lauren\u00e7on et al. 2024) for the visual question answering task with Retrieval-Augmented Generation (RAG). The experimental setup can be seen in Appendix C."}, {"title": "Text-only Order Sensitivity", "content": "We use the multiple-choice question-answering dataset MMBench (Liu et al. 2023b) to evaluate MLLM's order preferences in text-only. Each question in this dataset contains an image and a question about the image, along with up to four text candidate options, with only one correct answer. For our experiments, we select questions with four candidate options and rotate the correct answer through the Op.A, Op.B, Op.C, and Op.D positions. The implementation details are as follows. Given question Q and the context input:\n\n$\\mathbb{S}_{text-only}$ = {$\\mathbb{I}$; $T_1$; $T_2$; . . . ; $T_n$},                                                                                      (1)\n\nwhere $\\mathbb{I}$ represents the image on which question Q depends, and $T_1$; $T_2$;...; $T_n$ represent the n text options, the model is required to answer question Q based on context $\\mathbb{S}_{text-only}$ and choose the correct option from $T_1$; $T_2$; . . . ; $T_n$.\n\nIn our experimental setup, we set n = 4 and uniquely label the position of each text option as $O_1$ = (Op.A,$T_1$); $O_2$ = (Op.B, $T_2$); $O_3$ = (Op.C,$T_3$); $O_4$ = (Op.D, $T_4$). We designate Op.A as the option representing the first position and Op.D as the option representing the last position. The context input is updated to:\n\n$\\mathbb{S}_{text-only}$ = {$\\mathbb{I}$; $O_1$; $O_2$; $O_3$; $O_4$}.                                                                              (2)\n\nTo explore the text-only order sensitivity of MLLM in In-Context Learning, we exchange the correct answer True with $T_1$, $T_2$, $T_3$, and $T_4$ respectively, sequentially generating four variants of each question, and input them into the model in turn to obtain answers.\n\nWe conduct experiments on over 4,000 questions using IDEFICS-v2-8B-Instruct (Lauren\u00e7on et al. 2024). To more clearly illustrate the impact of order on model performance, for each question, we exclude from the statistical data the results of questions that the model answers correctly in all four variants. We then tally the positions of the correct option when the model responds accurately. The results, shown in Figure 7(a), indicate that the model achieves the highest accuracy when the correct option is in position Op.D (i.e., the last option)."}, {"title": "Image-only Order Sensitivity", "content": "We use the COCO dataset (Lin et al. 2014) to create a multiple-choice image-caption matching task to evaluate MLLM's order preferences in image-only. We select 5000 images with corresponding captions as correct answers. For each question, we add three random images as alternatives."}, {"title": "Mixed-modality Order Sensitivity", "content": "We utilize the COCO dataset (Lin et al. 2014) to construct a multiple-choice image-caption matching task to evaluate MLLM's order preferences for mixed modalities (image-text-pairs). We select 5000 image-caption pairs as correct answers. For each question, we add three random image-caption pairs as alternatives. The task is to choose the best matching pair, with one correct answer. The correct option in each question is sequentially swapped among the Op.\u0410, Op.B, Op.C, and Op.D options. The implementation is as follows. Given question Q and the context input:\n\n$\\mathbb{S}_{image-text-pair}$ = {($I_1$, $T_1$); ($I_2$, $T_2$); ...; ($I_n$, $T_n$)},                                                                              (5)\n\nwhere ($I_1$, $T_1$); ($I_2$, $T_2$); ...; ($I_n$, $T_n$) represent image-text-pairs. The model is required to choose the most matching image and caption pair from ($I_1$, $T_1$); ($I_2$, $T_2$); ... ; ($I_n$, $T_n$).\n\nIn our experimental setup, we set n = 4 and label each image-text-pair option with a unique position, namely $O_1$ = (Op.A, ($I_1$, $T_1$)); $O_2$ = (Op.B, ($I_2$, $T_2$)); $O_3$ = (Op.C, ($I_3$, $T_3$)); $O_4$ = (Op.D, ($I_4$, $T_4$)). We designate Op.A as the option representing the first position and Op.D as the option representing the last position. The context input is updated to:\n\n$\\mathbb{S}_{image-text-pair}$ = {$O_1$; $O_2$; $O_3$; $O_4$}.                                                                                      (6)\n\nTo explore the mixed-modality order sensitivity of MLLM in In-Context Learning, we exchange the correct answer ($I$,$T$)_{true}$ with each of ($I_1$, $T_1$); ($I_2$, $T_2$); ($I_3$, $T_3$); ($I_4$, $T_4$) respectively, sequentially generating four variants of each question, and input them into the model in turn to obtain answers.\n\nWe experiment with 5,000 questions using IDEFICS-v2-8B-Instruct (Lauren\u00e7on et al. 2024). To more clearly illustrate the impact of order on model performance, for each question, we exclude from the statistical data the results of questions that the model answers correctly in all four variants. We then tally the positions of the correct option when the model responds accurately. Figure 7(c) shows the model's accuracy is highest when the correct answer is positioned in Op.A or Op.D (first or last option)."}, {"title": "Sensitivity of LLMs to Input Order", "content": "Jiang et al.(Jiang et al. 2019) find that LLMs are highly sensitive to the order of prompts in zero-shot and few-shot settings. Kumar and Talukdar explore finding the optimal order of training examples as enhanced prompts and learning delimiter tokens between prompts to further improve performance (Kumar and Talukdar 2021). Lu et al. (Lu et al. 2021) discover that the order in which response prompts are given to LLMs plays a crucial role in their performance and propose an entropy-based method to score different candidate orderings. Similarly, inspired by the learning compressed perspectives, Wu et al.(Wu et al. 2022) optimize the order by minimizing the encoding length required to compress and transmit test labels. Ma et al. (Ma et al. 2023) address the challenge of order in few-shot learning from the perspective of prediction bias. They introduce a greedy search strategy to identify the optimal prompt order. Zhang et al. (Zhang et al. 2024) tackle the order dependency in context learning by converting the n-shot problem into n parallel 1-shot tasks."}, {"title": "Order Sensitivity", "content": "Previous research (Lu et al. 2021; Wu et al. 2022; Yoo et al. 2021) focuses on finding optimal orders for specific modalities and tasks. However, for a broader range of modalities and tasks, a general brute-force enumeration method for searching the optimal order requires unacceptable time consumption. Considering that an order is determined by the arrangement of different positions, we observe in extensive"}]}