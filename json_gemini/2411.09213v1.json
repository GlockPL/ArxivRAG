{"title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering", "authors": ["Nghia Trung Ngo", "Chien Van Nguyen", "Franck Dernoncourt", "Thien Huu Nguyen"], "abstract": "Retrieval augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs' ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models' limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs' reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in solving complex medical problems, achieving state-of-the-art performance across various benchmarks. However, ensuring the reliability and truthworthiness of an artificial intelligent (AI) medical system remains a critical challenge, especially in healthcare applications. Retrieval-augmented generation (RAG) has emerged as a promising approach to reduce LLMs' hallucination problem by integrating external knowledge sources.\nWhile RAG has potential to improve the factual accuracy of LLMs' response, incorporating an information retriever also presents new complexities that warrant careful evaluation. Consider the example in Fig. 1. The retrieved documents can contain not only useful knowledge that helps determine the true answer, but also noise information, or more serious, factual errors that can misleads the LLMs. To consciously apply RAG for medical QA, we must consider these practical scenarios and evaluate LLMs ability to interact with retrieved documents reliably.\nRecent efforts have been made to evaluate AI systems with LLMs in the medical domain (Nori et al. 2023; He et al. 2023; Xiong et al. 2024). For example, MedEval (He et al. 2023) presents a large-scale, expert-annotated benchmark that cover various medical tasks and domains. (Xiong et al. 2024) evaluates RAG extensively based on their MIRAGE benchmark which cover 5 medical QA datasets. However, they only focus on the effect of RAG modules on target accuracy, missing other important aspects of a AI medical system.\nSeveral recent works have explore RAG evaluation more comprehensively in general domain (Es et al. 2023; Chen et al. 2024b), RAGAS (Es et al. 2023) assess 3 qualities of RAG's outputs for QA tasks including: Faithfulness - degree to which responses align with the provided context, Answer Relevance - the extent to which generated responses address the actual question posed, and Context Precision-Recall - the quality of retrieved context. We follow the work from (Chen et al. 2024b) which establishes Retrieval-Augmented Generation Benchmark (RGB) to measure 4 abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. In particular, using questions from 4 medical QA datasets from MIRAGE as basis, we create Medical Retrieval-Augmented Generation Benchmark (MedRGB) to evaluate RAG system in the following 4 test scenarios:\n\u2022 Standard-RAG: evaluates LLMs performance when presented with multiple retrieved signal documents to create a context to answer to question.\n\u2022 Sufficiency: evaluates LLMs reliability when there are noise documents within the retrieved context. By adding \"Insufficient Information\" as an additional response option, LLMs should only answer when they are confident to have enough information to determine the correct answer. This requires LLMs to not only be aware of its own internal knowledge, but also be able to filter out noisy information from external documents.\n\u2022 Integration: evaluates LLMs ability to answer multiple supporting questions and integrate the extracted information to help address the main question.\n\u2022 Robustness: evaluates LLMs resiliency to factual errors in the retrieved context. A trustworthy AI medical system should be able detect factually incorrect documents and provide the corrected information.\nIn total, MedRGB consists of 3480 instances for 4 test scenarios, which is over 5 times that of RGB. Using MedRGB, we evaluation 7 LLMs, including both state-of-the art commercial LLMs and open-source models. In summary, our contributions are three-fold:\n\u2022 We establish MedRGB with four test scenarios to evaluate LLMs for medical QA tasks in RAG settings. To best of our knowledge, it is the first benchmark comprehensively assess medical RAG systems in these practical setting.\n\u2022 Using MedRGB, we extensively evaluate 7 LLMs, including both state-of-the art commercial LLMs and open-source models, across multiple RAG conditions. Experiment results demonstrate their limitation in addressing the more complex scenarios.\n\u2022 We analyzed the errors of the LLMs and their reasoning process to provide insights and suggest future directions for developing more reliable and trustworthy medical RAG systems."}, {"title": "Related Work", "content": "Medical Retrieval-augmented Generation\nThe application of LLMs in medical domain demands a high level of accuracy and reliablity, which most of the current LLMs still struggle with (Zhou et al. 2023). Retrieval-augmented Generation (RAG) (Lewis et al. 2020) addresses this problem by helping LLMs integrating external knowledge sources in their generation process. Recent works has achieved success in leverage RAG for knowledge-intensive tasks (Cui et al. 2023; Peng et al. 2023; Ram et al. 2023). Specifically for medical domain, (Hiesinger et al. 2023; Wang et al. 2024; Xiong et al. 2024) explore RAG for healthcare and clinical tasks.\nMedical Benchmarks\nPrevious medical benchmarks usually focus solely on target performance of medical problems, which consist of only QA pairs (Jin et al. 2020a, 2019; Krithara et al. 2023). Some recent benchmark also included evidence for LLMs to reasoning on (Chen et al. 2024a). Most current systematic evaluations of LLMs in medical domain do not involve RAG (He et al. 2023; Nori et al. 2023). (Xiong et al. 2024) attempts to provide a systematic evaluations of RAG systems in medicine. We build on their work to further evaluate important criteria of a medical RAG system for variety of practical settings."}, {"title": "Medical Retrieval-Augmented Generation Benchmark", "content": "MedRGB creation process is demonstrated in Fig. 2, which involves three main steps: retrieval topic generation, document retrieval, and benchmark creation.\nMedical QA Dataset\nThe basis of MedRGB are the multiple-choice questions from 4 medical QA dataset from MIRAGE, including 2 from medical examination (MMLU and MedQA) and 2 from biomedical research (PubMedQA, BioASQ).\nMMLU (MMLU-Med in (Xiong et al. 2024)) a subset of six tasks from 57 tasks of MMLU (Hendrycks et al. 2021), including anatomy, clinical knowledge, professional medicine, human genetics, college medicine, and college biologys. There are the 1089 questions with 4 answer options.\nMedQA (MedQA-US in (Xiong et al. 2024)) the English portion of MedQA (Jin et al. 2020a), which is a multi-choice QA dataset collected from professional medical board exams. These are 1273 real-world questions with 4 answer options.\nPubMedQA (PubMedQA* in (Xiong et al. 2024)) consist of 500 expert-annotated test samples of PubMedQA (Jin et al. 2019). Each question is constructed from PubMed abstracts with yes/no/maybe answer options.\nBioASQ (BioASQ-Y/N in (Xiong et al. 2024)) test set of the recent annual competition for biomedical QA from 2019 to 2023 (Krithara et al. 2023). In total, there are 618 yes/no question, constructed based on biomedical literature for machine reading comprehension track.\nTopic Generation\nIn a conventional RAG setup for QA tasks, a dense retriever encodes the input question into a high-dimensional vector to find semantically similar passages from an external corpus. The top-k documents (based on a imilarity metric like cosine similarity) are then combined to create the context for LLMs to answer the original question. This approach often results in redundancy in the retrieved set due to the lack of diversity when using the similarity metric. To address this limitation, we first ask LLMs to generate a diverse set of retrieval (sub-)topics using the prompt from Fig. 3. Each topic is then following the above process to create a smaller set of relevant documents. These sets are then aggregated to create the final retrieval set for the original QA pair."}, {"title": "Document Retrieval", "content": "We incorporate two retrieval processes in MedRGB to simulate the real-world difference between the use case of an expert and a non-expert person.\nOffline Retrieval Process In the expert use case, the retrieval corpus should contain highly specialized information that should not be publicly available. We use MedCorp (Xiong et al. 2024) as our offline corpus which contains medical documents from 4 different sources, including Pubmed, StatPearls, Textbooks and Wikipedia. The retrieval topics from previous step are encoded by MedCPT (Jin et al. 2023), a biomedical-domain dense retriever, to query the corpus for relavent documents. We choose MedCPT instead of a general-domain retriever like Contriever (Izacard et al. 2021) or a lexical retriever like BM25 (Robertson and Zaragoza 2009) due to its consistent performance retrieving from medical domain, as demonstrateed in (Xiong et al. 2024). Top-3 documents for each retrieval topics are collected to create the retrieval set.\nOnline Retrieval Process In non-expert use case, user simply asks a question to general LLMs, which then retrieves document through online search engine to help answer user's query. For each of the original medical question, the generated sub-topics are used to query the internet through Google Custom Search API\u00b9, which return top-scored links. The content from each retrieved link is then extracted and summarized (by the LLMs) to create a signal document of the corresponding topic.\nBenchmark Creation\nThis section describes the construction process of each of the four test scenarios. Aside from the standard-RAG test, each of the other three settings is evaluated across multiple degrees of noise, specified by the variable p - the percentage of signal documents in the retrieved context.\nStandard-RAG Test In this setting, the retrieved context consists of a predefined number of signal documents, which are sampled from the signal set. The LLMs are instructed, using the prompt in Fig. 4, to first generate their step-by-step reasoning, before outputting their answer option.\nSufficiency Test For each question, we sample both signal documents and irrelevant noise documents to create retrieval set with p in [0, 20, 40, 60, 80, 100]. Given the mixed context, LLMs are prompted, according to Fig. 5, to answer the question with an additional \"Insufficient Information\" option. The LLMs are instructed to detect noise documents first, before proceeding with their step-by-step reasoning based only on the relevant documents.\nIntegration Test A complex medical question may requires LLMs to address multiple sub-problems first before attempting to solve the main question. we measure this ability of LLMs by generating a sub-question based on each signal document, according to the data generation prompt in Fig. 6. The LLMs are instructed, according to the prompt in Fig. 7, to find the specific relevant document for each sub-question in the noisy context, then extract the sub-answer from the corresponding document. This test measures LLMs ability to answers an increasing number of sub-questions, and ability to integrate these information in their reasoning to infer the answer to the main question.\nRobustness Test The robustness test aim to measure LLMs resilient to factual errors, especially those that are adversarially designed to cause misinformation. Based on the sub-questions from the Integration test, we alter both the sub-answer and the corresponding document to create a counterfactual example, using the prompt in the Fig. 8. The adversarial answer should semantically contradict the original answer, and the relevant document should be minimally edited in a convincing way. In this test, all documents in retrieved context are relevant, and p is the percentage of factually correct documents. The LLMs are instructed, according to the prompt in Fig. 8, to detect documents with factually incorrect information before answering the sub-questions and the main question."}, {"title": "Experiments", "content": "This section assesses various LLMs in the four scenarios of MedRGB, analyzing their reasoning process and discussing key findings from the experimental results."}, {"title": "Evaluation Setting", "content": "We evaluate a wide range of state-of-the-art LLMs using MedRGB. For closed commercial LLMs, we assess both GPT-3.52 and GPT-4o3 from OpenAI. Additionally, we evaluate the recent GPT-4o-mini4, which achieves performance almost comparable to its full-sized counterparts. In the open-source category, we examine both general LLMS and domain-specific fine-tuned models. The former includes the instruction-tuned Gemma-2-27b6 from Microsoft and Llama-3-70b7 from Meta. For domain-specific models, we include the medical-domain pretrained MEDITRON-70b (Chen et al. 2023) and PMC-Llama-13b (Wu et al. 2023), which are tailored for healthcare applications."}, {"title": "Standard-RAG Evaluation", "content": "Table 1 shows the accuracy of all considered LLMs for 3 settings: the baseline \"No Retrieval\" and standard-RAG setting with 5 and 20 signal documents retrieved as context.\nResults GPT-4o emerges as the the top performer across most settings, demonstrating the positive effects of scaling both parameters and training data. Surprisingly, GPT-4o-mini, despite having only 8 billion reported parameters, achieved comparable results to its larger counterpart. Among open-source models, Gemma-2-27b and Llama-3-70b show strong performance, highlighting the effectiveness of general domain instruction-tuned models in zero-shot settings. In contrast, domain-specific fine-tuned models like PMC-Llama-13 and MEDITRON-70b both yield mixed results.\nThe effectiveness of RAG varies across different factors. Large models with strong internal knowledge, such as GPT-4o and Llama-3-70b, benefit less from RAG compared to smaller models like GPT-4o-mini and Gemma-2-27b. While adding more retrieved documents generally improves performance, models with shorter context lengths (e.g., PMC-Llama-13b, MEDITRON-70b) struggle to fully utilize this additional information, resulting in only marginal improvements.\nAnalysis Interestingly, the impact of document quantity differs between the two retrieval sources. The search-based online retrieval often performs best with fewer documents, while the offline retrieval using MedCorp typically improves with more documents. This discrepancy likely stems from the nature of each search algorithm and retrieval source. Google Search tends to provide high-quality top results but introduces more noise as the number of results increases. In contrast, retrieving from MedCorp with MedCPT offers more consistently relevant, high-quality documents, potentially more valuable in larger quantities. We also observe a slight adverse effect when applying RAG on GPT-4o and Llama-3-70b for MMLU and MedQA. This could be attributed to their strong internal knowledge and potential data leakage issues with more popular datasets."}, {"title": "Sufficiency Evaluation", "content": "We assess the models' ability to handle varying levels of noise in retrieved documents and their capacity to recognize when they lack sufficient information to answer a question reliably. The results are shown in Fig. 11 and 12.\nResult We observe that at p = 0, the models mostly returns \"insufficient information\u201d. However, there is a significant increase in accuracy across all datasets as the signal percentage p increased from 0 to 20. This indicates even a small amount of signal greatly enhances the model's confidence to answer. However, the improvement diminishes as more signal documents are added. Even when the retrieved context contains all signal documents (p = 100), performance is dramatically reduced compared to the standard RAG setting. This suggests that in the standard-RAG test, models may attempt to answer questions even when they are not fully confident or lack sufficient context, which can be an undesirable characteristic for medical application.\nLlama-3-70b consistently outperforms other models in noise detection across all datasets and settings. Adding More retrieved documents generally leads to improved performance with fewer \"insufficient information\u201d responses due to the increased context. However, this also resulted in decreased noise detection accuracy, as models occasionally misinterpreted noise documents as part of the signal. This highlights the delicate balance between providing enough context for accurate answers and maintaining the ability to discern relevant information from noise."}, {"title": "Integration Evaluation", "content": "Fig. 13 presents the main question accuracy after models integrate information from answering sub-questions. Additionally, we measure sub-question accuracy in Fig. 14 with two metrics. One of them is the strict exact-match score for extractive QA task, and the other is a more lenient GPT-based score using the prompt from Fig. 10. The intuition for this metric is that, since these are sub-questions, their exact accuracy is not as important. Sub-answers that are relatively accurate and help infer the main answer should also be rewarded.\nWe limit the number of retrieved documents to 10, as this setting required much more context length than previous ones due to the inclusion of the sub-tasks. p starts from 20 so that there is at least one signal document to ask sub-question from.\nResult Compared to the sufficiency evaluation, the introduction of sub-questions significantly improves the main accuracy at p\u2208 [20, 40]. This suggests that sub-questions can be useful, especially when there are more noise documents than signal documents. However, at p = 100 in which retrieval context is similar to that of standard-RAG setting, we observe models perform slight worse, indicating the inability to integrate information from sub-task effectively. Regarding the sub-task performance, the exact-match accuracy is relatively low, ranging from 20 to 30 percents. In contrast, the GPT-baseds consistently stay above 80% for all settings. This suggests that aiming to optimize the sub-task performance may not be useful to the main task.\nAnalysis The evaluation demonstrates that the integration of sub-questions can be beneficial, particularly when noise is present. Sub-questions help guide the models in identifying relevant information, thereby improving main accuracy. However, the sub-questions may also restrict models' reasoning to only the given questions, potentially limiting their ability to explore other relevant aspects. This is evident in the MMLU and MedQA datasets, where the improvement in main accuracy is less pronounced. While significantly increasing the number of sub-questions/signal documents can address this problem, it can also make the sub-task much harder, which leads to more task failures (e.g., failing to follow instructions, struggling with long context, skipping sub-questions, etc.)."}, {"title": "Robustness Evaluation", "content": "We assess the models' ability to detect and handle misinformation in retrieved documents, as well as their performance in answering questions in the presence of factually incorrect information. Similar to Integration test, Fig 15 provides the main question accuracy, whereas Fig. 16 presents sub-task scores together with model's factual error detection rate.\nResults In general, the presence of misinformation reduces the overall performance of the model on the main task, as we observe an increase in accuracy as the number of factually correct documents p gradually increases. However, compared to the performance in the Sufficiency test, we see that the models perform better in the presence of misinformation than with noisy irrelevant documents. This indicates that models are able to leverage information from the adversarial documents to improve their accuracy, which can be problematic for reliable systems. We observe models struggle to identify all instances of misinformation when p is low, in which retrieved context is predominantly adversarial documents. Interestingly, GPT-3.5 has the highest factual errors detection rate while being the worst-performing model.\nAnalysis We observe models often fail to detect fake information, resulting in a high false positive rate, mostly accepting the misinformation as truth. Accordingly, the sub-task score is low for smaller value of p as models proceed to answer to sub-questions as if the information in the corresponding documents are factually correct. The fake information sometimes is used in model's reasoning, leading to incorrect answers to the main question. This highlights a critical area for improvement, as the ability to discern factual inaccuracies is essential for a medical RAG systems. Interestingly, GPT-based scores for sub-tasks remain low for lower p, compared to Integration test where they are consistently above 80. This may potentially be an indicator of the presence of misinformation in the retrieved context."}, {"title": "Discussion", "content": "Our comprehensive evaluation of retrieval-augmented generation (RAG) systems for medical question answering highlights several critical insights into the capabilities and limitations of these models. In this section, we discuss these findings and reveals several key insights and potential directions for future research.\nEvaluation Criteria in Medical Domain For medical applications, all evaluation criteria, including performance, sufficiency, integration, and robustness, must be met to a high standard. Our results demonstrate that while models can achieve high performance at standard-RAG setting, they still fall short in others. Future works should aim to improve LLMs's ability to address these complex but practical scenarios, instead of just focusing on optimizing the target accuracy.\nSpecialized Components for RAG Systems The experiments clearly show that simply relying on LLMs is insufficient for a complete reliable medical system. Even the most advanced models struggled with complex integration tasks and were susceptible to noise and misinformation. This highlights the importance of developing specialized modules that can complement LLMs' strengths while mitigating their weaknesses.\nLimitation Compared to prior work, we aims to comprehensively evaluate LLMs with RAG for medical applications in four different practical settings. Consequently, this has led to significant financial and computational demands, forcing us to limit and simplify some aspects of our experiments to ensure manageability. This section outlines what we have and has not been able to address, and suggests promising future directions that can be followed from our findings.\n\u2022 Model Architecture: We focus on a limited set of model architectures. Future work could explore more efficient architectures (e.g., adapter-based architectures, quantized model, etc.) for medical RAG applications. Additionally, Investigating advanced RAG architectures, such as those proposed in recent literature, could yield further insights.\n\u2022 Task Scope: While we only address question answering task, future studies could extend to other medical NLP tasks.\n\u2022 Interaction Complexity: Our evaluation used single-turn prompts. Multi-turn interactions could provide a more realistic assessment of RAG systems in clinical settings."}, {"title": "Conclusion", "content": "This paper extends the evaluation of large language models (LLMs) in retrieval-augmented generation (RAG) settings for medical question answering (QA) tasks to crucial aspects of reliable medical AI systems, including sufficiency, integration, and robustness. We create Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides retrieval topics, signal documents, sub-QA pairs, and adversarial documents, for four medical QA datasets. Using MedRGB, we assess a wide range of LLMs, including both closed commercial LLMs and open-source models and their reasoning processes in each of the test scenarios. Our experimental results reveal current RAG system's limitations in handling these practical but complex situations. The finding from our analysis provides practical guidelines and future directions for developing more reliable and trustworthy medical RAG systems."}]}