{"title": "PureDiffusion: Using Backdoor to Counter\nBackdoor in Generative Diffusion Models.", "authors": ["Vu Tuan Truong", "Long Bao Le"], "abstract": "Abstract-Diffusion models (DMs) are advanced deep learning\nmodels that achieved state-of-the-art capability on a wide range\nof generative tasks. However, recent studies have shown their\nvulnerability regarding backdoor attacks, in which backdoored\nDMs consistently generate a designated result (e.g., a harmful\nimage) called backdoor target when the models' input contains\na backdoor trigger. Although various backdoor techniques have\nbeen investigated to attack DMs, defense methods against these\nthreats are still limited and underexplored, especially in inverting\nthe backdoor trigger. In this paper, we introduce PureDiffusion,\na novel backdoor defense framework that can efficiently detect\nbackdoor attacks by inverting backdoor triggers embedded in\nDMs. Our extensive experiments on various trigger-target pairs\nshow that PureDiffusion outperforms existing defense methods\nwith a large gap in terms of fidelity (i.e., how much the inverted\ntrigger resembles the original trigger) and backdoor success rate\n(i.e., the rate that the inverted trigger leads to the corresponding\nbackdoor target). Notably, in certain cases, backdoor triggers\ninverted by PureDiffusion even achieve higher attack success rate\nthan the original triggers.\nIndex Terms-Diffusion models, Generative models, backdoor\nattacks, backdoor detection, trigger inversion.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, diffusion models (DMs) have emerged\nrapidly and set the new state-of-the-art among different cate-\ngories of deep generative models. By employing a multi-step\ndenoising approach [1], DMs achieved superior capability in\nvarious generative tasks such as image generation [2] and 3D\nsynthesis [3], outperforming previous methods like generative\nadversarial networks (GANs) and variational autoencouders\n(VAEs).\nHowever, recent studies have shown that DMs are vulner-\nable to a wide range of attacks [4], among which backdoor\nattack is especially harmful due to its stealthiness. Specifically,\nto conduct a backdoor attack on DMs, attackers modify the\ntraining objective of DMs and poison the training data such\nthat the backdoored DMs would generate a designated result\n(e.g., a violent image) when a trigger is stamped in the mod-\nels' input. Otherwise, backdoored DMs still generate normal\nsamples if the trigger is not activated, making it imperceptible\nby human observation. Moreover, the consequence might be\nexacerbated further if backdoored DMs are used in security-\ncentric applications (e.g., adversarial purification) or safety-\ncritical downstream tasks (e.g., medical imaging).\nDespite these threats, defense methods for DM-targeted\nbackdoor attacks were underexplored. Most existing backdoor\ndetection frameworks are investigated for traditional clas-\nsification models, which are inapplicable for DMs due to\ntheir differences in prediction objective and model operation.\nTypically, backdoor defense methods for DMs often include\nthree main stages. The first stage is trigger inversion, in which\none tries to find one or some candidate backdoor triggers from\nthe suspicious DMs. In the second stage, backdoor detection is\nconducted by using the inverted triggers to figure out whether\nthe DMs were truly backdoored or not. Once the DMs are\nmarked as backdoor, one can choose to omit the manipulated\nmodels, or implement the third stage called backdoor removal,\nwhich eliminates the backdoor effect embedded in the target\nDMs. Among these stages, trigger inversion is often the most\nchallenging task as the knowledge about the suspicious models\nis still limited at this stage. It is also the most impactful stage\nsince the inverted trigger's quality greatly decides backdoor\ndetection performance. In this paper, we introduce PureDif-\nfusion, a backdoor defense framework that focuses on high-\nquality trigger inversion.\nPrior to PureDiffusion, existing backdoor defense methods\nfor DMs either ignore the trigger inversion stage or can invert\nonly low-quality trigger. For instance, both DisDet [5] and\nUFID [6] assume that they already have a set of candidate\ntriggers, and the two methods are designed to verify which\nof the candidates is the true trigger. DisDet [5] relies on\nthe following observation: If the model's input contains the\ntrigger, it will consistently result in a particular image (i.e.,\nthe backdoor target), otherwise the generated results should\nbe very diverse. Thus, the authors compute the cosine sim-\nilarity between every pairs of generated image, constructing\na similarity graph to assess each candidate trigger. If the\nsimilarity of a candidate exceeds a predefined threshold, it is\nconsidered a backdoor trigger. On the other hand, the authors\nof UFID [6] argue that the trigger's distribution should be\nsignificantly different from a Gaussian noise's distribution.\nThus, they computes the Kullback\u2013Leibler (KL) divergence\nbetween candidate triggers and a standard Gaussian noise,\nthen choose a statistical threshold to determine which one\nis the true trigger. However, it is impractical to assume that\nthe set of candidate triggers is available. To the best of our\nknowledge, only the work Elijah [7] offers a practical method\nfor trigger inversion. Elijah finds the trigger based on the\ndistribution shift caused by the trigger in each diffusion step.\nHowever, it only computes the shift in the last timestep and\nheuristically chooses a scale of 0.5 for the shift without any\ninsighful justification. Consequently, Elijah can invert only\nsimple triggers, while the inverted triggers are often of low\nquality. When we modify the trigger by increasing its size or\naltering its shape, the performance of Elijah's trigger inversion\ndecreases rapidly."}, {"title": "II. BACKGROUND KNOWLEDGE", "content": "DMs are trained to generate images from Gaussian noise\nvia two main processes, a forward (diffusion) process and\na reverse (denoising) process. In the forward process, a\nsmall amount of Gaussian noise is iteratively added to the\ntraining images in T steps until they are totally destroyed\ninto a standard Gaussian distribution at the final step. In the\nreverse process, a deep neural network model @ is trained to\nreconstruct these images by removing the added noise in T\ndenoising steps. While there are different categories of DMs,\nthey all follow the above operation. In this paper, we primarily\nfocus on denoising diffusion probabilistic model (DDPM),\nin which the diffusion process is modelled as a Markovian\nchain with T states corresponding to T diffusion steps, noising\na clean image xo into a Gaussian noise x\u0442. Formally, the\ntransition between two consecutive forward steps is:\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$,\n(1)\nwhere \u1e9et \u2208 (0,1) is the scale of noise at step t. By applying\nthe reparameterization trick T times on equation 1, we obtain\nthe following property that enables direct sampling of xt from\nthe clear image xo:\n$x_t = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 \u2013 \\bar{a}_t}\u03f5_0$,\nwhere $\\bar{a}_t = \\prod_{i=1}^t \\alpha_i$ and $a_t = 1 - \\beta_t$.\nThe reverse process employs a deep neural network param-\neterized by @ to predict the noise in each step, based on the\nfollowing denoising transition:\n$P_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \u03bc_\\theta(x_t, t), \u03a3_\\theta(x_t, t))$,\n(3)\nwhere $p_\\theta(x_t,t)$ and $S_\\theta(x,t)$ are the mean and variance of\nimage distribution at step t, predicted by using the neural\nnetwork \u03b8. The loss function to train this neural network is\nthe KL divergence between the predicted $p_\\theta(x_{t-1}|x_t)$ and the\nground-truth posterior $q(x_{t-1}|X_t, x_0)$ derived based on (2).\nAs a result, the final loss function can be simplified into the\nfollowing form:\n$L = E_{x_0,\u03f5} [||\u03f5 - \u03f5_\u03b8(x_t, t)||_2]$,\n(4)\nwhere \u0454 ~ N(0,I) is a sampled noise, and 60(x,t) is the\nnoise predicted by the neural network. After training on a\nspecific dataset, the neural network @ can be used to generate\nimage samples based on the following reserve process:\n$x_{t-1} = \\frac{1}{\\sqrt{a_t}}(x_t - \\frac{(1-\\alpha_t)}{\\sqrt{1 - \\bar{a}_t}}\u03f5_\u03b8(x_t,t)) + \\sigma_t\u03f5$,\n(5)\nwhere $\u03c3_t = \\sqrt{\\frac{1 - \\bar{a}_{t-1}}{1-\\bar{a}_t}} \\beta_t$.\nThe goal of backdoor attacks on a DM is to make it\ngenerates a designated backdoor target x when a backdoor\ntrigger d is stamped into the DM's input noise. In this case,\nthe DM's input is x7 ~ N(d,I). Note that we use \u201c*\u201d to\ndenote the backdoor case. To do so, the forward transition is\nmodified to gradually add a small amount of the backdoor\ntrigger & into the image distribution, resulting in the following\nbackdoor forward transition:\n$q(x_t^*|x_{t-1}^*) = N(x_t^*; \\sqrt{1 - \\beta_t}x_{t-1}^* + m(t)\u03b4,n(t)I)$,\n(6)\nwhere m(t) and n(t) are functions that determine the amount\nof the trigger and noise added at each step, respectively. It can\nbe seen that if m(t) = 0 and n(t) = \u1e9et, the equation (6) will\nhave the same form with the equation (1), in which there is\nno trigger added to the image distribution.\nDifferent backdoor methods use different m(t) and n(t),\nbut these functions must be chosen carefully to make the DMs\ntrainable. For example, BadDiffusion [8] chooses m(t) = 1\u2013\n$\\sqrt{a_t}$ and n(t) = \u03b2t, which results in the following property:\n$x_t^* = \\sqrt{a_t}x + (1 - \\sqrt{a_t})\u03b4 + \\sqrt{1 \u2013 \\bar{a}_t}\u03f5$,\n(7)\nOn the other hand, TrojDiff [9] chooses m(t) = k+ (1 \u2212 \u03b3)\nand n(t) = \u03b3\u03b2t, where \u03b3\u2208 [0,1] is a predefined blending\ncoefficient, and k = {ko,k1, ..., kt, ..., kT } is intentionally\nselected to offer the direct sampling from 2 to x:\n$x_t^* = \\sqrt{a_t}x + \\sqrt{1 \u2013 \\bar{a}_t}(1 \u2212 \u03b3)\u03b4 + \\sqrt{1 \u2013 \\bar{a}_t}\u03b3\u03f5$.\n(8)\nBased on the above backdoored forward process, the reverse\nprocess and training objective can be derived accordingly.\nDuring training, a certain proportion of the training data (5-\n20%) is poisoned by the trigger and target images, combined\nwith the backdoor loss function to attack the targeted DMs."}, {"title": "III. PUREDIFFUSION: METHODOLOGY", "content": "PureDiffusion is designed to invert backdoor triggers from\nmanipulated DMs. To do so, we first show that a backdoored\nforward process will consistently add a trigger-related distri-\nbution shift into the image distribution. Based on this property,\nwe propose a practical method to figure out the scale of\ndistribution shift in every timestep of the diffusion processes.\nUtilizing the computed scales, we use gradient descent to learn\nthe triggers from multiple timesteps, resulting in high-quality\ninverted triggers.\nApplying reparameterization on the backdoored forward\ntransition in equation (6), the transition can be viewed as:\n$x_t^* = \\sqrt{1-\\beta_t}x_{t-1}^*+ \\frac{m(t)\u03b4}{\\text{trigger shift}} + \\frac{n(t)\u03f5}{\\text{noise shift}}$\n(9)\nIntuitively, the \"noise shift\" term adds a small amount of\nGaussian noise into the image distribution in each timestep, de-\nstroying the image gradually over time. This term is included\nin both benign and backdoored DMs. On the other hand, the\n\"trigger shift\" term will iteratively shift the image distribution\ntowards the trigger distribution. Consequently, the final result\nat step Tis a noisy trigger imagex ~ N(d, I). Fig. 1\nvisualizes the distribution shifts of benign and backdoored\nDMs over timesteps.\nSince the neural network @ is trained to reverse the forward\nprocess, the backdoored denoising process must preserve the\ntrigger shift in every timestep, but in a reverse direction, to\nreconstruct the backdoor target from the noisy trigger. If the\nscale of the trigger shift in the reverse process is known\nfor every timestep, we can learn the backdoor trigger via\ngradient descent [7]. However, the scale of the trigger shift\npredicted by the neural network is different than those in the\nforward process due to the difference in coefficients, making\ntrigger inversion challenging. An existing work named Elijah\n[7] heuristically chooses a scale of 0.5 for the first denoising\nstep T, and learn the trigger on only that timestep. As a result,\nalthough Elijah can invert some simple triggers, the inverted\ntriggers are often of low quality, and it failed in inverting more\nsophisticated trigger shapes.\nIn this section, we introduce a feasible solution to compute\nthe scale of trigger shift predicted by the neural network @ in\nevery timestep, which is unsolved in existing work. Due to the\nlimited scope of the paper, we opt to analyze BadDiffusion [8],\nwhile similar analyses can be used for other backdoor methods\nlike TrojDiff [9] and VillanDiffusion [10], which are similar\nbut come with different m(t) and n(t).\nLet At denotes the scale of trigger\nshift in timestep t, and all trigger shift scales are in x = {\u03bb\u03bf, \u03bb1, ..., \u03bb\u03c4, ..., \u03bb\u03c4}, we propose the following proposition.\nProposition 1. The trigger shift scales A is the same between\ndifferent backdoor triggers, regardless of their shape and size.\nIn the backdoored reverse process, the equation (5) can\nbe expressed as:\n$x_{t-1}^* = \\frac{1}{\\sqrt{a_t}}(x_t^* - \\frac{1-\\alpha_t}{\\sqrt{1 \u2013 \\bar{a}_t}}\u03f5_\u03b8(x_t^*, t) + \\sigma_t\u03f5$.\n(10)\nHere, the predicted noise ee(x, t) contains both the reverse\ntrigger shift and the noise shift. However, the scale of trigger\nshift is intractable as it is from the neural network's output.\nBy applying the Bayes' rule on the equation (6) with\nm(t) = 1 $\\sqrt{a_t}$ and n(t) = \u00dft, we obtain the posterior\nof BadDiffusion's forward process, which has been shown in\n:\n$x_{t-1}^* = \\frac{1}{\\sqrt{a_t}}(x_t^* - \\frac{1 \u2013 \\sqrt{a_t}}{\\sqrt{1 \u2013 \\bar{a}_t}}\u03b4 + \\sigma_t\u03f5$\n(11)\nSubtracting (10) from (11), we obtain:\n$\u03f5_\u03b8(x_t^*,t) = \\frac{1 \u2013 \\alpha_t}{(1 \u2013 \\sqrt{a_t})\\sqrt{1 \u2013 \\bar{a}_t}}\u03b4$\n$\\frac{1 - \u03b1_t}{\\sqrt{1 - \\bar{a}_t}}\u03b4$\n(12)\n= \u03bb\u03b5\u03b4.\n(13)\nAs $\u03bb_t = \\frac{1 - \u03b1_t}{(1 \u2013 \\sqrt{a_t})\\sqrt{1 \u2013 \\bar{a}_t}}$, only depends on the fixed noise\nschedule at without relying on the trigger 8, the Proposition\nis proved.\nProposition 1 indicates that if\nwe can find the trigger-shift scales of an arbitrary trigger\n(which is not the true trigger), we can treat it as the trigger\nshift scales of such the true trigger. Thus, to extract these\nscales, we first choose an surrogate trigger 8, then simulate a\nbackdoor attack targeting on a copy version the suspicious\nDM. Using this double-backdoored DM, we can compute\nthe predicted noise (x,t) in every timestep by running\nthe backdoor reverse process using the trigger \u03b4. Since both\n(x,t) and 8 are known, from (13), we can find every t\nvia the following minimization problem:\n$\\hat{\\lambda} = \\text{arg min}_{\\lambda_t} ||\u03f5_\u03b8(x_t^*,t) \u2013 \u03bb_t\u03b4||_2^2$.\n(14)"}, {"title": "C. Multi-Timestep Trigger Inversion for Backdoor Detection", "content": "This minimization problem can be solved by setting the\nderivative to zero to find the optimal At, resulting in:\n$\u03bb_t = \\frac{\u03f5_\u03b8(x_t^*,t) \u22c5 \u03b4}{||\u03b4||^2}$\n(15)\nBecause X = {0, 1, ...} are the trigger-shift scales of\nthe surrogate trigger \u00ce, it is also valid for the true trigger\n8 although the two triggers are different from each other\n(according to the Proposition 1).\nOnce successfully extracting the trigger-shift scales A via\nthe proposed method, from the equation (13), we can find the\ntrue trigger & based on the following optimization problem:\n$\\hat{\u03b4} = \\text{arg min}_\u03b4 E_\u03f5 ||\u03f5_\u03b8(x_t(\u03b4, \u03f5), t) \u2013 \u03bb_t\u03b4||_2$\n(16)\n$Loss = E_\u03f5 ||\u03f5_\u03b8(x_t(\u03b4, \u03f5), t) \u2013 \u03b5\u03b4||_2$.\n(17)\nHere, \u03b5\u03b8 (\u03b1(\u03b4, \u20ac), t) is the neural network's output (i.e.,\npredicted noise) at timestep t, given that its input at the first\ndenoising step contains the trigger \u03b4. As a result, we can use\ngradient descent to learn 8, with t is sampled randomly from\n0 to T. For each sampled timestep t, the backpropagation\nflow passes through the neural network T - t times, from the\nfirst denoising step T to the timestep t. This ensures that the\ndenoising model can consistently keep the learned trigger shift\nover different timesteps. Algorithm 1 summarizes our trigger\ninversion method, with \u03b5\u03b8(x(\u03b4, \u0454), t) is simplified to 60(t) for\nthe ease of reading. Thus, please note that e(t), g(x,t), and\n\u03b5\u03b8(x(\u03b4, \u03b5), t) are the same in this paper.\nOnce successfully revert the trigger, it is easy to verify\nwhether the DM was backdoored or not. For instance, we can"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In our experiments, we\nuse DDPM with T = 1000, while the noise schedule starts\nfrom 30 = 0.0001 to \u03b2\u03c4 = 0.02. Due to the limited scope\nof the paper, we mainly evaluate the performance of our\ndefense framework against BadDiffusion [8], while it can be\nadopted easily for other backdoor methods such as TrojDiff\n[9] and VillanDiffusion [10]. The experiments are performed\non CIFAR-10 dataset [11]\nTo assess the performance of\nPureDiffusion, we use the following three metrics:\nDistance from the ground-truth trigger: This is the L2\ndistance between the inverted trigger and the ground-truth\ntrigger. The lower distance, the higher trigger quality (as\nthe inverted trigger looks more like the ground-truth one).\nAttack success rate of inverted trigger: We feed the in-\nverted trigger as input of the suspicious DM and observe\nif its output is the backdoor target. If the distance between\nthe output image and the backdoor target is smaller than a\npredefined threshold, it is considered a successful attack.\nWe repeat this N times (e.g., generating 100 samples) to\ncompute the attack success rate (ASR). The higher ASR,\nthe higher trigger quality.\nTo compute this score, we generate M\ndifferent image samples using the suspicious DM with its\ninput contains the inverted trigger. If the inverted trigger\nis of high quality, these M samples should resemble each\nother as they resemble the backdoor target. As a result,\nthe uniform score is such the total distance of every pair\nof generated samples. Thus, the lower uniform score, the\nhigher trigger quality.\nPrior to our study, only Elijah [7]\ninvestigates trigger inversion for DMs. While our method\nsuccessfully computes the trigger shift scales for all timesteps,\nproved by both empirical and theoretical analyses, Elijah only\nuses the timestep T with a shift scale of 0.5 without any\njustification. To showcase the capability of our framework,\nwe compare its performance with both Elijah triggers and\nthe ground-truth triggers. Regarding Elijah, we run the ex-\nperiments based on their provided source code [7].\nFor trigger inversion with mul-\ntiple timesteps, we need to iteratively run the neural network\nmultiple times for denoising, in which the output of the\nprevious timestep is the input of the current timestep. We\nbackprop the gradients through all of these timesteps to learn\nthe trigger. Due to our limitation in computational resources,"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose PureDiffusion, a backdoor defense\nframework for DMs, focusing on trigger inversion. First, we\npropose a novel reverse-engineering method to estimate the\ntrigger shift scales caused by the backdoor diffusion processes.\nThis method is proved by both empirical and theoretical\nanalyses. Second, by using the estimated trigger shift scales,\nwe learn the true trigger over multiple denoising steps, offer-\ning high-quality trigger inversion. Experimental results have\nshowcased the efficiency of PureDiffusion, as it outperforms\nexisting work with a large performance gap. Furthermore,\nPureDiffusion's inverted triggers even outperform the ground-\ntruth triggers, making it a potential reinforcement method for\nbackdoor attacks."}]}