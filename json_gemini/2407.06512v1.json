{"title": "LuSNAR:A Lunar Segmentation, Navigation and Reconstruction Dataset based on\nMuti-sensor for Autonomous Exploration", "authors": ["Jiayi Liu", "Qianyu Zhang", "Xue Wan", "Shengyang Zhang", "Yaolin Tian", "Haodong Han", "Yutao Zhao", "Baichuan Liu", "Zeyuan Zhao", "Xubo Luo"], "abstract": "With the complexity of lunar exploration missions, the moon needs to have a higher level of\nautonomy. Environmental perception and navigation algorithms are the foundation for lunar rovers to achieve\nautonomous exploration. The development and verification of algorithms require highly reliable data support.\nMost of the existing lunar datasets are targeted at a single task, lacking diverse scenes and high-precision\nground truth labels. To address this issue, we propose a multi-task, multi-scene, and multi-label lunar\nbenchmark dataset LuSNAR. This dataset can be used for comprehensive evaluation of autonomous\nperception and navigation systems, including high-resolution stereo image pairs, panoramic semantic labels,\ndense depth maps, LiDAR point clouds, and the position of rover. In order to provide richer scene data, we\nbuilt 9 lunar simulation scenes based on Unreal Engine. Each scene is divided according to topographic relief\nand the density of objects. To verify the usability of the dataset, we evaluated and analyzed the algorithms of\nsemantic segmentation, 3D reconstruction, and autonomous navigation. The experiment results prove that the\ndataset proposed in this paper can be used for ground verification of tasks such as autonomous environment\nperception and navigation, and provides a lunar benchmark dataset for testing the accessibility of algorithm\nmetrics. We make LuSNAR publicly available at: https://github.com/autumn999999/LuSNAR-dataset.", "sections": [{"title": "1. Introduction", "content": "The rovers, as planetary robots, play a crucial role in\nfacilitating human exploration of extraterrestrial celestial\nbodies and enhancing our understanding of the universe.\nAs the nearest celestial body to the earth, lunar\nexploration tasks attracts scientists for its geological\nevolution and internal structure. After the peak of lunar\nexploration led by the United States and former Soviet\nUnion in 1976, space-faring nations such as China and\nIndia have successively sent satellites and rovers to the\nMoon for scientific exploration (Gao et al., 2017). China\nsuccessfully launched the Yutu and Yutu-2 rovers in\nChang'E-3 (CE-3) and s Chang'e-4 (CE-4) mission,\nenabling exploration and investigation on the moon.\n(Long et al., 2015, Li et al., 2021) In August of 2023,\nIndia's Chandrayaan-3 achieved a successful soft landing\non the moon, thus becoming the fourth nation for lunar\nexploration.\nWith the increasing complexity of deep space\nexploration missions, the long communication chain\nbetween Earth and the moon greatly restricts the human-\nin-loop operation and control of rovers.(McGuire et al.,\n2016) The safety and real-time performance of future\nexploration tasks require increasing autonomous ability\nof the rovers. Autonomous environmental perception and\nnavigation are the foundation to ensure the safety and\nefficiency of lunar exploration tasks. The development\nand validation of perception and navigation algorithms\ndemand a substantial amount of data with various scenes\nincludes different topography and object distributions. In\nparticular, deep learning algorithms rely on datasets with\nground truth labels for model training and testing.(Minar\net al., 2016) High-quality ground truth labels and diverse\nscene data can enhance the accuracy and generalizability\nof the model. Thus, a lunar benchmark dataset supporting\nthe tasks of environmental perception and navigation will\nserve as a testbed for the comparison and evaluation of\nvarious algorithms.\nDriven by the new wave of lunar exploration, some\ndatasets have been proposed focused on the navigation\nand recognition on lunar surface. Furgale proposed\nDevon Island dataset (Furgale et al., 2012) for lunar\nterrain navigation. The sensors include stereo images, 3D\nlaser ranging scans, and location data collected on terrain\non Earth where topographic features are similar to the\nmoon. Vayugundla proposed LRNT dataset (Vayugundla\net al., 2018) based on data collected by the Lightweight\nRover Unit (LRU), and can be used to evaluate rover\nnavigation. The S3LI dataset (Giubilato et al., 2022) and\nthe LRNT dataset were collected at the same location and\nare also employed to validate and evaluate visual-inertial\nSLAM. Roman proposed a lunar landscape simulation\ndataset (Roman et al., 2019)containing semantic labels\nfor sky, smaller rocks and larger rocks, which can be used\nfor training and testing semantic segmentation algorithms\nfor lunar scene. These datasets show potential application\nvalue for future lunar semantic perception, positioning\nand navigation missions. However, the lack of scene\ndiversity makes it difficult to evaluate the real\nperformance of the algorithms, and moreover, the\ngeneralization of trained model can hardly be fulfilled.\nTherefore, the existing dataset is difficult to support the\ndiverse needs of future lunar exploration.\nThe autonomous exploration of rovers involves the\ncollaborative execution of multiple tasks, including\nsemantic perception, obstacle recognition, path planning,\nnavigation and positioning, and terrain reconstruction.\nThese tasks are connected and influenced each other. The\ndataset merely focused on single task cannot provide a\ncomprehensive performance evaluation of the\nautonomous exploration system. For example, the failure\ndetection of one stone in the semantic segmentation will\nlead to errors in obstacle map generation, and thus the\npath planning may be inaccurate; Incorrect navigation\nposes used for multi-site point cloud stitching can\nsignificantly impact the accuracy and reliability of path\nplanning, potentially leading to unsafe driving for rovers.\nDatasets for a single task can only be used to validate and\noptimize algorithms for specific tasks, but cannot be used\nto comprehensively evaluate perception and navigation\nsystems. The dataset must be combined with multi-task\nmodules to achieve efficient and robust autonomous\nexploration.\nAutonomous exploration missions require lunar\rovers to adapt to diverse and unknown lunar\nenvironments, which may include vast plains, rocky\nterrain with dense distribution, and rugged crater bottoms\nwhich may have different distribution training data on"}, {"title": "3. Dataset features", "content": "The LuSNAR dataset is based on simulation engine\nto generate a multi-task, multi-scene, and multi-label\nlunar surface dataset, which can be used for ground\nverification, algorithm selection of autonomous\nenvironmental perception, and navigation of lunar rovers.\nTo achieve this, diverse and realistic lunar scenes are\ndesigned for data collection, aiming to equip rovers with\nthe ability to generalize when encountering unknown\nenvironments. In this section, a detailed overview of the\nprinciples behind the simulation scene design, as well as\nthe content and features of the LuSNAR dataset is\nprovided."}, {"title": "3.1 Multi-task supported", "content": "Semantic perception\nThe semantic information has important practical\nsignificance in lunar surface exploration missions. It can\nnot only provide obstacle information to help rovers\nassess terrain traversability, but also provide prior\nknowledge for lunar geological research, allowing\nscientists to select landforms of interest for in-depth\ninvestigation.(Garcia-Garcia et al., 2017) Both the image\nsequence and point cloud sequence in the LuSNAR\ndataset contain semantic labels, enabling the evaluation\nof 2D and 3D semantic segmentation algorithms for lunar\nscenes. The data taken from cameras and LiDAR have\ntheir merits and problems. The camera can provide rich\ninformation of appearance, texture, and color of lunar\nsurface; however, it is susceptible to illumination\nvariations and unable to detect obstacles in shadows,\nthereby features extracted from images may fail to be\ntracked during travel. On the other hand, LiDAR can\noffer precise three-dimensional coordinates of objects\nwith geometric information and remains insensitive to\nillumination changes. Nevertheless, the point cloud\nobtained from scanning is usually relatively sparse and\nlacks comparable levels of detail as images. Therefore,\ncombining camera and LiDAR data can use the privilege\nof both sensor and provide more reliable result for\nsemantic segmentation. The LuSNAR dataset can verify\nthe applicability and complementarity of 2D and 3D\nsemantic perception algorithms in lunar surface scenarios,\nwhich contributes to the high-level visual understanding\nresearch on lunar surface.\nSLAM\nAutonomous navigation tasks are a crucial\ntechnology for lunar rovers in autonomous lunar\nexploration, including key steps such as mapping,\nlocalization, and path planning. SLAM (Simultaneous\nLocalization and Mapping) technology can calculate the\nlocalization, velocity, and orientation of the rover based\non sensors carried by the rover itself, without relying on\nground support, supporting the lunar rover's autonomous\nexploration mission. The LuSNAR dataset supports\nmultiple sensor SLAM solutions, including monocular\nSLAM, stereo SLAM, LIDAR SLAM, and IMU (Inertial\nMeasurement Unit) fusion-based SLAM. Visual SLAM\nhas the advantages of low cost and rich information of\ncameras. However, visual sensors are highly sensitive to\nchanges in lighting conditions, which can affect\nlocalization accuracy. LiDAR SLAM can directly acquire\nthree-dimensional information about the environment,\nachieving higher precision pose estimation and mapping\nby registering point cloud data from adjacent frames.\nHowever, LiDAR SLAM may degrade or fail in scenes\nwithout obvious geometric features. Multi-sensor SLAM\nsolutions that incorporate IMU data can address\nchallenges related to global localization in similar\ngeometric environments and environmental changes.(Xu\net al., 2022) This helps to correct errors in cases where\nvisual and LiDAR information is temporarily missing,\nthus improving the robustness of autonomous navigation\nsystems. The LuSNAR dataset provides multimodal data\nfor lunar rover navigation and mapping on the lunar\nsurface, including camera images, LiDAR scan\nsequences, IMU data, and ground truth pose information.\nThis facilitates the validation of SLAM algorithms based\non different sensors for localization accuracy and real-\ntime performance, further promoting the application of\nSLAM technology in lunar rover autonomous exploration\nmissions.\n3D Reconstruction\nThe lunar surface is an uneven, irregular, and\nunstructured environment with various obstacles. To\nensure the safe navigation of lunar rovers during their\nmissions, it is essential to perform dense terrain\nreconstruction of the surroundings for path planning,\ntarget area identification and obstacle avoidance. A\nstereo-matching algorithm based on binocular vision is an\neffective method for dense terrain reconstruction.(Laga et\nal., 2020) This algorithm calculates a disparity map from\nthe stereo images and recovers a 3D point cloud of the\nscene using camera parameters. The LuSNAR dataset\nprovides dense depth ground truth and camera parameters\nfor the lunar rover's surrounding environment, allowing\nfor the evaluation of the accuracy and real-time\nperformance of stereo-matching algorithms in terrain\nreconstruction. In practical applications, relying only on\npoint clouds obtained from individual camera stations\nprovides limited information about the lunar rover's\nsurroundings. It is necessary to merge and fuse terrain\nreconstruction results from multiple camera stations by\ncombining their poses. The LuSNAR dataset also\nsynchronously provides ground truth poses of the lunar\nrover, enabling the fusion and stitching of multi-station\npoint cloud data to generate point cloud of large area on\nlunar surface. Dense terrain reconstruction data\naccurately reflect changes in the surrounding terrain,\nserving as fundamental information for lunar rover\nlocalization, path planning, scientific exploration, and the\noverall goal of achieving a comprehensive perception of\nthe rover's environment."}, {"title": "3.2 Diversity in lunar topographic relief and objects\ndensity", "content": "Lunar simulation scenes are designed and rendered\nbased on Unreal Engine 4, utilizing the AirSim plugin\ndeveloped by Microsoft for data collection.(Shah et al.,\n2017) UE4 is a game development engine created by Epic\nGames, and we leveraged lunar-related terrain, features,\nand material textures available in the Unreal Marketplace\nas our assets. During the simulation scene design, we\nreferenced Apollo lunar exploration data and images\ncaptured by China's Yutu-2 lunar rover to ensure the\naccuracy and scientific fidelity of the scenes. We\ncategorized features in the scenes into four major types:\nlunar regolith, rocks, impact craters, and mountains,\naligning with the actual lunar surface's geological\ncharacteristics. To create a highly realistic and immersive\nlunar simulation environment, we considered various\nfactors inherent to lunar settings, such as topographic\nrelief, feature distributions, and light intensity on the\nlunar surface.\nThe topographic relief and obejects density of the\nlunar exploration areas can be largely different due to\nscientific purpose, leading to distinct planning and\nrequirements for autonomous exploration tasks. To\nensure the diversity lunar terrain topography and features\nof the dataset, this paper innovatively used topographic\nrelief and the density of objects as two aspects and\ndesigned nine representative lunar surface scenes. As\nshown in Fig. 2(a), the horizontal axis represents\ntopographic relief from gentle to steep, while the vertical\naxis represents feature richness from sparse to abundant.\nThis approach not only enriches scene diversity but also\nfacilitates the exploration of how diverse terrains and\ntopographies influence environmental perception and\nautonomous navigation algorithms. Fig. 2(b) illustrates\nthe topographic relief trends of the nine scenes and\nrenders them with different colors based on elevation\ninformation. Scenes 1, 4, and 6 exhibit relatively flat\nterrain, while scenes 2, 5, and 7 feature minor undulations,\nand scenes 3, 6, and 9 display significant terrain\nvariations. Additionally, this paper proposed a\nquantitative analysis of the types of features in these nine"}, {"title": "3.3 multi-label", "content": "This article uses the \"AirSimGameMode\" mode for\nsimulation in Unreal Engine 4. The simulation involves\nmanually controlling a simulated rover to navigate within\nthe scene. Attention has been given to ensuring diversity\nin both the scene and motion modes to ensure its\nfunctionality in various terrains and environments. The\nrover collects trajectories from each of the nine scenes,\nand its sensors move along with the trajectory to gather\nmultimodal data. The rover's motion modes include\nstraight-line movement, turning, climbing, descending,\nobstacle avoidance, and circumnavigation, all of which\nare representative of scenarios encountered in real-world\ntasks. The rover's travel distance ranges from 200m to\n600m, and its speed during operation does not exceed\n5m/s. Data collection occurs in two steps: real-time\ncollection during rover operation and offline rendering.\nIMU data and rover pose ground truth are collected in\nreal-time during operation, while the stereo image pairs\nfrom the cameras and LiDAR point clouds are generated\noffline using ground truth poses. The final dataset\nincludes scene images, depth images, point clouds, and\nsemantic segmentation labels. The simulation engine\nparameters and output data, following the reference of\nYutu-2's relevant payload parameters, are shown in the\nTable 2. The LuSNAR dataset comprises a collection of\n13,006 sequences, gathered from nine scenes, each with\na consistent set of internal and external parameters. Each\nsequence includes stereo image pairs, single-frame point\nclouds, semantic labels, IMU data, and rover pose ground\ntruth. All data is named according to timestamps, with\nimages and semantic labels stored in PNG format, depth\nmaps in PFM format, and IMU, ground truth pose, and\n3D LiDAR point cloud data stored in TXT format.\nThe lunar rover's native frame and sensor frame\ndefined in the simulation engine are shown in Fig. 3. The\nforward direction of the lunar rover is the positive X-axis,\nthe horizontal direction to the right is the positive Y-axis,"}, {"title": "4. Experiment", "content": "The dataset was utilized to conduct experiments on\n2D and 3D semantic segmentation. A total of 13,006\nimages and 13,006 point cloud frames were collected\nfrom 8 short sequences and 1 long sequence for training\nand testing. The data of scenes 1, 2, 4, 6, 8, 9 were used\nfor model training, and the data of scenes 3, 5, 7 were\nused for testing. The metrics IoU, mIoU, and mAcc were\nadopted to evaluate the performance of different semantic\nsegmentation algorithms on this dataset.\nIn the 2D semantic segmentation experiment, five\nsate-of-the-art algorithms are tested: Deeplabv3+(Chen\net al., 2018), Mobilenetv3(Howard et al., 2019),\nUnet(Ronneberger et al., 2015), PointRend(Kirillov et al.,\n2020), and Segformer(Xie et al., 2021). The classification\ncategories are lunar regolith, rock, impact crater,"}, {"title": "4.2 SLAM", "content": "Visual SLAM and LiDAR SLAM experiments were\nconducted using this dataset. Since the dataset provides\nthe ground truth of the sensor's pose in the world frame,\nand the SLAM estimates the pose relative to the first\nframe in the local odometry frame based on the sensor. In\norder to accurately evaluate the performance of SLAM\nalgorithm, it is necessary to use the extrinsic of the sensor"}, {"title": "4.2 3D Reconstruction", "content": "The stereo matching and 3D reconstruction\nalgorithms based on the LuSNAR dataset were tested. In\nthe stereo matching experiment, 10 pairs of stereo images\nthat can reflect the representativeness of the scene were\nselected from 9 sequences, BM and SGBM(Ueshiba et al.,\n2006), PSMNet(Chang et al., 2018), RAFTStereo(Lipson\net al., 2021) and CREStereo(Li et al., 2022) were used for\ndisparity estimation. Among them, BM and SGBM were\nconfigured with a maximum disparity of 256 and a"}, {"title": "6. Conclusion", "content": "This paper proposes a benchmark dataset LuSNAR\nfor autonomous environmental perception and navigation\non the lunar surface. This dataset collects diverse scene\ndata based on a simulation engine and contains high-\nprecision ground truth labels. We validate and evaluate\nthis dataset using 2D/3D semantic segmentation, SLAM,\nand 3D reconstruction algorithms. The evaluation results\nshow that the dataset provides high-quality label and\nground truth information and can be used in the ground\nverification stage to validate and develop autonomous\nperception and navigation algorithms. We hope that\nLuSNAR will help promote the autonomy of lunar rover\nexploration and realize the universalization of intelligent\ntechnology in planetary scenes. In future work, we will\nexpand the size of the LuSNAR dataset to cover more\ndiverse lunar surface scenes. In addition, we will validate\nand establish baseline results on other tasks, such as 3D\nsemantic reconstruction, absolute positioning relative to\norbital data, and SLAM fusion of multi-source sensors.\nThrough these works, we expect th LuSNAR dataset to\nprovide more comprehensive data support for\nautonomous lunar rover exploration."}]}