{"title": "LuSNAR:A Lunar Segmentation, Navigation and Reconstruction Dataset based on\nMuti-sensor for Autonomous Exploration", "authors": ["Jiayi Liu", "Qianyu Zhang", "Xue Wan", "Shengyang Zhang", "Yaolin Tian", "Haodong Han", "Yutao Zhao", "Baichuan Liu", "Zeyuan Zhao", "Xubo Luo"], "abstract": "With the complexity of lunar exploration missions, the moon needs to have a higher level of\nautonomy. Environmental perception and navigation algorithms are the foundation for lunar rovers to achieve\nautonomous exploration. The development and verification of algorithms require highly reliable data support.\nMost of the existing lunar datasets are targeted at a single task, lacking diverse scenes and high-precision\nground truth labels. To address this issue, we propose a multi-task, multi-scene, and multi-label lunar\nbenchmark dataset LuSNAR. This dataset can be used for comprehensive evaluation of autonomous\nperception and navigation systems, including high-resolution stereo image pairs, panoramic semantic labels,\ndense depth maps, LiDAR point clouds, and the position of rover. In order to provide richer scene data, we\nbuilt 9 lunar simulation scenes based on Unreal Engine. Each scene is divided according to topographic relief\nand the density of objects. To verify the usability of the dataset, we evaluated and analyzed the algorithms of\nsemantic segmentation, 3D reconstruction, and autonomous navigation. The experiment results prove that the\ndataset proposed in this paper can be used for ground verification of tasks such as autonomous environment\nperception and navigation, and provides a lunar benchmark dataset for testing the accessibility of algorithm\nmetrics. We make LuSNAR publicly available at: https://github.com/autumn999999/LuSNAR-dataset.", "sections": [{"title": "1. Introduction", "content": "The rovers, as planetary robots, play a crucial role in\nfacilitating human exploration of extraterrestrial celestial\nbodies and enhancing our understanding of the universe.\nAs the nearest celestial body to the earth, lunar\nexploration tasks attracts scientists for its geological\nevolution and internal structure. After the peak of lunar\nexploration led by the United States and former Soviet\nUnion in 1976, space-faring nations such as China and\nIndia have successively sent satellites and rovers to the\nMoon for scientific exploration (Gao et al., 2017). China\nsuccessfully launched the Yutu and Yutu-2 rovers in\nChang'E-3 (CE-3) and s Chang'e-4 (CE-4) mission,\nenabling exploration and investigation on the moon.\n(Long et al., 2015, Li et al., 2021) In August of 2023,\nIndia's Chandrayaan-3 achieved a successful soft landing\non the moon, thus becoming the fourth nation for lunar\nexploration.\nWith the increasing complexity of deep space\nexploration missions, the long communication chain\nbetween Earth and the moon greatly restricts the human-\nin-loop operation and control of rovers.(McGuire et al.,\n2016) The safety and real-time performance of future\nexploration tasks require increasing autonomous ability\nof the rovers. Autonomous environmental perception and\nnavigation are the foundation to ensure the safety and\nefficiency of lunar exploration tasks. The development\nand validation of perception and navigation algorithms\ndemand a substantial amount of data with various scenes\nincludes different topography and object distributions. In\nparticular, deep learning algorithms rely on datasets with\nground truth labels for model training and testing.(Minar\net al., 2016) High-quality ground truth labels and diverse\nscene data can enhance the accuracy and generalizability\nof the model. Thus, a lunar benchmark dataset supporting\nthe tasks of environmental perception and navigation will\nserve as a testbed for the comparison and evaluation of\nvarious algorithms.\nDriven by the new wave of lunar exploration, some\ndatasets have been proposed focused on the navigation\nand recognition on lunar surface. Furgale proposed\nDevon Island dataset (Furgale et al., 2012) for lunar\nterrain navigation. The sensors include stereo images, 3D\nlaser ranging scans, and location data collected on terrain\non Earth where topographic features are similar to the\nmoon. Vayugundla proposed LRNT dataset (Vayugundla\net al., 2018) based on data collected by the Lightweight\nRover Unit (LRU), and can be used to evaluate rover\nnavigation. The S3LI dataset (Giubilato et al., 2022) and\nthe LRNT dataset were collected at the same location and\nare also employed to validate and evaluate visual-inertial\nSLAM. Roman proposed a lunar landscape simulation\ndataset (Roman et al., 2019)containing semantic labels\nfor sky, smaller rocks and larger rocks, which can be used\nfor training and testing semantic segmentation algorithms\nfor lunar scene. These datasets show potential application\nvalue for future lunar semantic perception, positioning\nand navigation missions. However, the lack of scene\ndiversity makes it difficult to evaluate the real\nperformance of the algorithms, and moreover, the\ngeneralization of trained model can hardly be fulfilled.\nTherefore, the existing dataset is difficult to support the\ndiverse needs of future lunar exploration.\nThe autonomous exploration of rovers involves the\ncollaborative execution of multiple tasks, including\nsemantic perception, obstacle recognition, path planning,\nnavigation and positioning, and terrain reconstruction.\nThese tasks are connected and influenced each other. The\ndataset merely focused on single task cannot provide a\ncomprehensive performance evaluation of the\nautonomous exploration system. For example, the failure\ndetection of one stone in the semantic segmentation will\nlead to errors in obstacle map generation, and thus the\npath planning may be inaccurate; Incorrect navigation\nposes used for multi-site point cloud stitching can\nsignificantly impact the accuracy and reliability of path\nplanning, potentially leading to unsafe driving for rovers.\nDatasets for a single task can only be used to validate and\noptimize algorithms for specific tasks, but cannot be used\nto comprehensively evaluate perception and navigation\nsystems. The dataset must be combined with multi-task\nmodules to achieve efficient and robust autonomous\nexploration.\nAutonomous exploration missions require lunar\rovers to adapt to diverse and unknown lunar\nenvironments, which may include vast plains, rocky\nterrain with dense distribution, and rugged crater bottoms\nwhich may have different distribution training data on"}, {"title": "2. Related works", "content": "In this section, public datasets, including\nextraterrestrial bodies, Mars and the Moon, for the tasks\nof segmentation and navigation of rovers, have been\nreviewed."}, {"title": "2.1 Planetary Semantic segmentation datasets", "content": "Roman et al. (Roman et al., 2019) presented an\nartificial lunar landscape dataset that simulates images of\nthe moon using Terragen from Planetside Software. The\nSpace Robotics Group at Keio University in Japan\ncreated this dataset, which is available on Kaggle and\nprovides photorealistic images of the lunar surface along\nwith semantic labels for training scene segmentation\nalgorithms. It currently contains 9,766 realistic rendered\nimages of the rocky lunar landscape and their\ncorresponding semantic labels, including sky, smaller\nrocks, and larger rocks. It also includes bounding boxes\nof all larger rocks that can be used to train object\ndetection algorithms.\nSwan et al. (Swan et al., 2021) proposed the\nAI4Mars dataset, which consists of nearly 326K semantic\nsegmentation full image labels on 35K images from the\nCuriosity, Opportunity, and Spirit rovers. The label data\nwas collected through crowdsourcing, with an additional\napproximately 1,500 annotations from NASA mission's\nrover planners and scientists. The dataset includes four\nlabel types, namely soil, bedrock, sand, and large rock. It\nwas built for training and validating terrain classification\nmodels for Mars. The current planetary semantic"}, {"title": "2.2 Navigation datasets", "content": "Due to the difficulties of acquiring real data from\nMars and the Moon, most datasets are generated via field\nor software simulation. Given the geological and climatic\nsimilarities between Mars and Earth, it is feasible to find\na location on Earth that is similar to the Martian\nenvironment. Tong et al. (Tong et al., 2013) presented a\ndataset collected at two distinct planetary analog rover\ntest facilities in Canada, namely the University of Toronto\nInstitute for Aerospace Studies (UTIAS) indoor rover test\nfacility and the Canadian Space Agency's (CSA) Mars\nEmulation Terrain (MET). This dataset specifically\nfocuses on 3D laser scans, with a total of 272 scans\ncollected. Potential applications include terrain\nreconstruction, path planning, and LiDAR SLAM.\nSimilarly, another dataset (Lamarre et al., 2020) was also\ncollected by MET at CSA, but with different sensors. The\nsensor suite includes a color stereo camera, a monocular\ncamera, an IMU, a pyranometer, drive power\nconsumption monitors, wheel encoders, and a GPS\nreceiver. The dataset contains 142,710 images from a\nstereo camera and 16,203 images from a monocular\ncamera. It is divided into six separate runs, covering a\ntotal distance of over 1.2 km, and can be used for\nenvironment reconstruction, short-to-medium-distance\npath planning, omnidirectional visual-inertial odometry,\nand energy-aware planetary navigation.\nThe Erfoud dataset (Lacroix et al., 2020) was\nacquired by two mobile robots, Mana and Minnie, at three\ndifferent Mars-like locations in the Tafilalet region of\nMorocco. Collected along nine different trajectories\ntotaling 13 km, the dataset contains approximately\n110,000 georeferenced stereo image pairs and 40,000\nLiDAR scans. It also includes wheel odometry data, FoG\ngyroscope measurements, IMU data, and pose ground\ntruth obtained through RTK GPS. This dataset can be\nutilized for various applications including stereo vision,\nvisual odometry, visual SLAM, terrain modeling, as well\nas more advanced tasks such as visual/LiDAR fusion,\nLIDAR SLAM, multi-robot SLAM, and absolute\nlocalization based on orbital data. The MADMAX\ndataset (Meyer et al., 2021) was collected in the same\nregion as the Erfoud dataset. This dataset contains time-\nstamped recordings from a monochrome stereo camera,\nan omnidirectional stereo camera, a color camera, and an\nIMU. Additionally, it provides the 5 degrees of freedom\n(DOF) D-GNSS ground truth. There are 36 tracks in total,\nthe longest track span is 1.5 km, and the total track length\nreaches 9.2 km. The dataset can be used as a benchmark\nfor the accuracy and robustness of state-of-the-art\nnavigation algorithms.\nThe Katwijk Beach Planetary Rover dataset (Hewitt\net al., 2018) was collected along a 1 km section of beach\nnear Katwijk, the Netherlands. The beach was populated\nwith a variety of artificial rocks in different sizes to\nemulate the conditions of Mars landing sites. The dataset\ncan be divided into two parts. One part contains stereo"}, {"title": "3. Dataset features", "content": "The LuSNAR dataset is based on simulation engine\nto generate a multi-task, multi-scene, and multi-label\nlunar surface dataset, which can be used for ground\nverification, algorithm selection of autonomous\nenvironmental perception, and navigation of lunar rovers.\nTo achieve this, diverse and realistic lunar scenes are\ndesigned for data collection, aiming to equip rovers with\nthe ability to generalize when encountering unknown\nenvironments. In this section, a detailed overview of the\nprinciples behind the simulation scene design, as well as\nthe content and features of the LuSNAR dataset is\nprovided."}, {"title": "3.1 Multi-task supported", "content": "Semantic perception\nThe semantic information has important practical\nsignificance in lunar surface exploration missions. It can\nnot only provide obstacle information to help rovers\nassess terrain traversability, but also provide prior\nknowledge for lunar geological research, allowing\nscientists to select landforms of interest for in-depth\ninvestigation.(Garcia-Garcia et al., 2017) Both the image\nsequence and point cloud sequence in the LuSNAR\ndataset contain semantic labels, enabling the evaluation\nof 2D and 3D semantic segmentation algorithms for lunar\nscenes. The data taken from cameras and LiDAR have"}, {"title": "SLAM", "content": "Autonomous navigation tasks are a crucial\ntechnology for lunar rovers in autonomous lunar\nexploration, including key steps such as mapping,\nlocalization, and path planning. SLAM (Simultaneous\nLocalization and Mapping) technology can calculate the\nlocalization, velocity, and orientation of the rover based\non sensors carried by the rover itself, without relying on\nground support, supporting the lunar rover's autonomous\nexploration mission. The LuSNAR dataset supports\nmultiple sensor SLAM solutions, including monocular\nSLAM, stereo SLAM, LIDAR SLAM, and IMU (Inertial\nMeasurement Unit) fusion-based SLAM. Visual SLAM\nhas the advantages of low cost and rich information of\ncameras. However, visual sensors are highly sensitive to\nchanges in lighting conditions, which can affect\nlocalization accuracy. LiDAR SLAM can directly acquire\nthree-dimensional information about the environment,\nachieving higher precision pose estimation and mapping\nby registering point cloud data from adjacent frames.\nHowever, LiDAR SLAM may degrade or fail in scenes\nwithout obvious geometric features. Multi-sensor SLAM\nsolutions that incorporate IMU data can address\nchallenges related to global localization in similar\ngeometric environments and environmental changes.(Xu\net al., 2022) This helps to correct errors in cases where"}, {"title": "3D Reconstruction", "content": "The lunar surface is an uneven, irregular, and\nunstructured environment with various obstacles. To\nensure the safe navigation of lunar rovers during their\nmissions, it is essential to perform dense terrain\nreconstruction of the surroundings for path planning,\ntarget area identification and obstacle avoidance. A\nstereo-matching algorithm based on binocular vision is an\neffective method for dense terrain reconstruction.(Laga et\nal., 2020) This algorithm calculates a disparity map from\nthe stereo images and recovers a 3D point cloud of the\nscene using camera parameters. The LuSNAR dataset\nprovides dense depth ground truth and camera parameters\nfor the lunar rover's surrounding environment, allowing\nfor the evaluation of the accuracy and real-time\nperformance of stereo-matching algorithms in terrain\nreconstruction. In practical applications, relying only on\npoint clouds obtained from individual camera stations\nprovides limited information about the lunar rover's\nsurroundings. It is necessary to merge and fuse terrain\nreconstruction results from multiple camera stations by\ncombining their poses. The LuSNAR dataset also\nsynchronously provides ground truth poses of the lunar\nrover, enabling the fusion and stitching of multi-station\npoint cloud data to generate point cloud of large area on\nlunar surface. Dense terrain reconstruction data\naccurately reflect changes in the surrounding terrain,\nserving as fundamental information for lunar rover\nlocalization, path planning, scientific exploration, and the\noverall goal of achieving a comprehensive perception of\nthe rover's environment."}, {"title": "3.2 Diversity in lunar topographic relief and objects\ndensity", "content": "Lunar simulation scenes are designed and rendered\nbased on Unreal Engine 4, utilizing the AirSim plugin\ndeveloped by Microsoft for data collection.(Shah et al.,\n2017) UE4 is a game development engine created by Epic\nGames, and we leveraged lunar-related terrain, features,\nand material textures available in the Unreal Marketplace\nas our assets. During the simulation scene design, we\nreferenced Apollo lunar exploration data and images\ncaptured by China's Yutu-2 lunar rover to ensure the\naccuracy and scientific fidelity of the scenes. We\ncategorized features in the scenes into four major types:\nlunar regolith, rocks, impact craters, and mountains,\naligning with the actual lunar surface's geological\ncharacteristics. To create a highly realistic and immersive\nlunar simulation environment, we considered various\nfactors inherent to lunar settings, such as topographic\nrelief, feature distributions, and light intensity on the\nlunar surface.\nThe topographic relief and obejects density of the\nlunar exploration areas can be largely different due to\nscientific purpose, leading to distinct planning and\nrequirements for autonomous exploration tasks. To\nensure the diversity lunar terrain topography and features\nof the dataset, this paper innovatively used topographic\nrelief and the density of objects as two aspects and\ndesigned nine representative lunar surface scenes. As\nshown in Fig. 2(a), the horizontal axis represents\ntopographic relief from gentle to steep, while the vertical\naxis represents feature richness from sparse to abundant.\nThis approach not only enriches scene diversity but also\nfacilitates the exploration of how diverse terrains and\ntopographies influence environmental perception and\nautonomous navigation algorithms. Fig. 2(b) illustrates\nthe topographic relief trends of the nine scenes and\nrenders them with different colors based on elevation\ninformation. Scenes 1, 4, and 6 exhibit relatively flat\nterrain, while scenes 2, 5, and 7 feature minor undulations,\nand scenes 3, 6, and 9 display significant terrain\nvariations. Additionally, this paper proposed a\nquantitative analysis of the types of features in these nine"}, {"title": "3.3 multi-label", "content": "This article uses the \"AirSimGameMode\" mode for\nsimulation in Unreal Engine 4. The simulation involves\nmanually controlling a simulated rover to navigate within\nthe scene. Attention has been given to ensuring diversity\nin both the scene and motion modes to ensure its\nfunctionality in various terrains and environments. The\nrover collects trajectories from each of the nine scenes,\nand its sensors move along with the trajectory to gather\nmultimodal data. The rover's motion modes include\nstraight-line movement, turning, climbing, descending,\nobstacle avoidance, and circumnavigation, all of which\nare representative of scenarios encountered in real-world\ntasks. The rover's travel distance ranges from 200m to\n600m, and its speed during operation does not exceed\n5m/s. Data collection occurs in two steps: real-time\ncollection during rover operation and offline rendering.\nIMU data and rover pose ground truth are collected in\nreal-time during operation, while the stereo image pairs\nfrom the cameras and LiDAR point clouds are generated\noffline using ground truth poses. The final dataset\nincludes scene images, depth images, point clouds, and\nsemantic segmentation labels. The simulation engine\nparameters and output data, following the reference of\nYutu-2's relevant payload parameters, are shown in the\nTable 2. The LuSNAR dataset comprises a collection of\n13,006 sequences, gathered from nine scenes, each with\na consistent set of internal and external parameters. Each\nsequence includes stereo image pairs, single-frame point\nclouds, semantic labels, IMU data, and rover pose ground\ntruth. All data is named according to timestamps, with\nimages and semantic labels stored in PNG format, depth\nmaps in PFM format, and IMU, ground truth pose, and\n3D LiDAR point cloud data stored in TXT format.\nThe lunar rover's native frame and sensor frame\ndefined in the simulation engine are shown in Fig. 3. The\nforward direction of the lunar rover is the positive X-axis,\nthe horizontal direction to the right is the positive Y-axis,"}, {"title": "4. Experiment", "content": "The dataset was utilized to conduct experiments on\n2D and 3D semantic segmentation. A total of 13,006\nimages and 13,006 point cloud frames were collected\nfrom 8 short sequences and 1 long sequence for training\nand testing. The data of scenes 1, 2, 4, 6, 8, 9 were used\nfor model training, and the data of scenes 3, 5, 7 were\nused for testing. The metrics IoU, mIoU, and mAcc were\nadopted to evaluate the performance of different semantic\nsegmentation algorithms on this dataset.\nIn the 2D semantic segmentation experiment, five\nsate-of-the-art algorithms are tested: Deeplabv3+(Chen\net al., 2018), Mobilenetv3(Howard et al., 2019),\nUnet(Ronneberger et al., 2015), PointRend(Kirillov et al.,\n2020), and Segformer(Xie et al., 2021). The classification\ncategories are lunar regolith, rock, impact crater,"}, {"title": "4.2 SLAM", "content": "Visual SLAM and LiDAR SLAM experiments were\nconducted using this dataset. Since the dataset provides\nthe ground truth of the sensor's pose in the world frame,\nand the SLAM estimates the pose relative to the first\nframe in the local odometry frame based on the sensor. In\norder to accurately evaluate the performance of SLAM\nalgorithm, it is necessary to use the extrinsic of the sensor"}, {"title": "4.2 3D Reconstruction", "content": "The stereo matching and 3D reconstruction\nalgorithms based on the LuSNAR dataset were tested. In\nthe stereo matching experiment, 10 pairs of stereo images\nthat can reflect the representativeness of the scene were\nselected from 9 sequences, BM and SGBM(Ueshiba et al.,\n2006), PSMNet(Chang et al., 2018), RAFTStereo(Lipson\net al., 2021) and CREStereo(Li et al., 2022) were used for\ndisparity estimation. Among them, BM and SGBM were\nconfigured with a maximum disparity of 256 and a"}, {"title": "6. Conclusion", "content": "This paper proposes a benchmark dataset LuSNAR\nfor autonomous environmental perception and navigation\non the lunar surface. This dataset collects diverse scene\ndata based on a simulation engine and contains high-\nprecision ground truth labels. We validate and evaluate\nthis dataset using 2D/3D semantic segmentation, SLAM,\nand 3D reconstruction algorithms. The evaluation results\nshow that the dataset provides high-quality label and\nground truth information and can be used in the ground\nverification stage to validate and develop autonomous\nperception and navigation algorithms. We hope that\nLuSNAR will help promote the autonomy of lunar rover\nexploration and realize the universalization of intelligent\ntechnology in planetary scenes. In future work, we will\nexpand the size of the LuSNAR dataset to cover more\ndiverse lunar surface scenes. In addition, we will validate\nand establish baseline results on other tasks, such as 3D\nsemantic reconstruction, absolute positioning relative to\norbital data, and SLAM fusion of multi-source sensors.\nThrough these works, we expect th LuSNAR dataset to\nprovide more comprehensive data support for\nautonomous lunar rover exploration."}]}