{"title": "Refusal Behavior in Large Language Models: A Nonlinear Perspective", "authors": ["Fabian Hildebrandt", "Andreas Maier", "Patrick Krauss", "Achim Schilling"], "abstract": "Refusal behavior in large language models (LLMs) enables them to decline responding to harmful, unethical, or inappropriate prompts, ensuring alignment with ethical standards. This paper investigates refusal behavior across six LLMs from three architectural families. We challenge the assumption of refusal as a linear phenomenon by employing dimensionality reduction techniques, including PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms exhibit nonlinear, multidimensional characteristics that vary by model architecture and layer. These findings highlight the need for nonlinear interpretability to improve alignment research and inform safer AI deployment strategies.", "sections": [{"title": "INTRODUCTION", "content": "Morality and impulse control are fundamental aspects of human behavior, enabling ethical decision-making, effective social interactions, and the maintenance of personal and societal relationships. The amygdala and the ventromedial prefrontal cortex play critical roles in moral behavior [1]. The amygdala associates harmful actions with negative emotions, while the ventromedial prefrontal cortex governs decision-making and self-control to prevent such actions. In psychopathy, these regions exhibit reduced activity, leading to an increased propensity for manipulation, impulsivity, and unethical decisions without remorse [2]. Impulse control, a key aspect of self-regulation housed in the prefrontal cortex, can be disrupted by various factors such as ADHD, substance use disorders, psychological conditions, stress, and brain injuries [3].\n\nThe human brain is often described as a prediction machine, continuously processing sensory information to guide decision-making. Similarly, Large Language Models (LLMs) are designed to predict the next word or sequence, enabling them to perform complex language tasks. Like the human brain, LLMs are tasked with handling nuanced decisions, such as refusing harmful instructions, yet their internal decision-making processes often remain opaque. Efforts to align LLMS with ethical principles have focused on feature search, which seeks to interpret model activations and uncover the mechanisms governing their behavior. The superposition hypothesis posits that multiple concepts or features are simultaneously represented within the same neural activation space [4], [5]. Consequently, individual neurons in LLMs often exhibit polysemantic behavior, encoding multiple distinct meanings rather than a one-to-one mapping. Achieving monosemantic representations\u2014where neurons encode a single, unambiguous feature is a critical goal for improving interpretability. Sparse autoencoders have been employed to identify such features by inflating the hidden space and enforcing sparsity [6]\u2013[9]. This approach enables the isolation of specific features for steering model behavior [10]\u2013[12].\n\nRecent studies suggest that refusal behavior in LLMs is mediated by a single linear subspace in the activation space. This feature can be manipulated to either disable or enforce refusal behavior across a range of open-source models, including those with up to 72B parameters [13]. Techniques like the difference-in-means method [14] and weight orthogonalization have been used to isolate and modify this subspace, resulting in models that either lose or gain the ability to refuse harmful instructions. However, emerging evidence challenges the assumption that refusal behavior resides in a linear subspace, pointing instead to a multidimensional and nonlinear nature [15].\n\nIn this work, we examine refusal behavior across six LLMs spanning three model families. By analyzing intermediate layer activations through both linear (PCA) and nonlinear (t-SNE, UMAP) dimensionality reduction techniques, we reveal that refusal behavior is a universal but architecture-specific phenomenon. Our findings show that refusal mechanisms are more complex and nonlinear than previously assumed, with distinct sub-clusters emerging in the activation space. These insights contribute to a deeper understanding of refusal behavior and its implications for aligning LLMs with ethical and safety standards."}, {"title": "METHODOLOGY", "content": "Behavioral studies and neural correlates\n\nIn this study, we investigate the refusal behavior of large language models (LLMs) in response to harmful and harmless instructions, drawing parallels to behavioral studies that examine brain activity linked to cognitive functions. Our methodology involves designing a task that isolates the refusal mechanisms in LLMs, similar to decision-making tasks in behavioral research that require empathy and sensitivity. Simultaneously, we track the LLM activations to identify model activations associated with the refusal behavior. Additionally, the origin of these responses is localized and the embeddings are further investigated.\n\nDatasets\n\nTwo distinct datasets are used. The first dataset Dharmless contains harmless instructions from the ALPACA dataset [16] repackaged and published as a Hugging Face dataset [17]. The ALPACA dataset is a collection of 52000 harmless instruction-following prompts designed to fine-tune large language models for more effective and reliable task completion. The second dataset Dharmful contains harmful instructions that originate from the LLM Attacks dataset [18], which was designed for adversarial attacks against aligned language models. The instructions are trying to induce harmful behavior by the LLMs. Again, a repackaged Hugging Face dataset is used [19].\n\nModels\n\nTo investigate the universality of the refusal behavior, we evaluated a set of six large language models (LLMs) from three model families. All models are fine-tuned for instruction following. The selection of different model families and varying parameter sizes allows to assess whether the observed effects are consistent and generalizable. The model families represent different alignment types from preference optimization to alignment by fine-tuning. The specific models used in this study are detailed in Table I.\n\nExtraction of the refusal activations\n\nThe methodology for extracting activations related to the refusal mechanism follows the general approach outlined in [13] and [23]. First, we load the model using the TranformerLens library [24]. The library allows to cache any internal activation in the model for mechanistic interpretability research. Next, two equal-sized subsets of prompts from the two datasets Dharmful and Dharmless containing harmful and harmless instructions are loaded using the Hugging Face Datasets library. Using a chat-style generation template, we run the model on both datasets while caching the activations for each layer. To identify the refusal-related feature direction, we compute the difference-in-means by subtracting the mean activations of harmless prompts from those of harmful prompts at each token position. Finally, we store the residual activations afterr the self-attention layers and the multilayer perceptron (MLP) layers at the last token position, for further analysis and manipulation.\n\nDifference-in-means. The difference-in-means technique [14] can be used to identify the refusal direction in the model's activations as shown in previous works [13], [23]."}, {"title": null, "content": "For each layer $l \\in [L]$ the mean activation $\\Phi_{harmful}^{(l)}$ for harmful prompts from $D_{harmful}$ and $\\Phi_{harmless}^{(l)}$ for harmless prompts from $D_{harmless}$ is calculated:\n\n$\\Phi_{harmful}^{(l)} = \\frac{1}{|D_{harmful}|} \\sum_{i \\in D_{harmful}} y^{(l)}(i)$,\n\n$\\Phi_{harmless}^{(l)} = \\frac{1}{|D_{harmless}|} \\sum_{i \\in D_{harmless}} y^{(l)}(i)$.\n\nwhere $y^{(l)}(i)$ represents the activation at the last token position in layer $l$ for instruction $i$. The difference-in-means direction $d_{refusal}^{(l)}$ is then computed as:\n\n$d_{refusal}^{(l)} = \\Phi_{harmful}^{(l)} - \\Phi_{harmless}^{(l)}$.\n\nThe vector $d_{refusal}^{(l)}$ captures both the direction and the magnitude of the refusal feature in the activations.\n\nDimensionality reduction\n\nPrincipal Component Analysis (PCA). PCA is a widely used linear dimensionality reduction technique that transforms an unlabeled dataset into a new coordinate system where the greatest variance by any projection of the data lies on the first principal component, the second greatest variance on the second component, and so on [25]. This method identifies orthogonal axes that maximize variance, thereby simplifying high-dimensional data while preserving the maximum variability. PCA is commonly applied for feature extraction and the feature visualization.\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE). t-SNE is a nonlinear dimensionality reduction technique commonly used for visualizing high-dimensional data in two or three dimensions. It models the pairwise similarities between points in the original space and seeks to preserve these relationships in the reduced space by minimizing a divergence between two probability distributions. Unlike linear methods like PCA, t-SNE is effective at capturing local structure and revealing clusters or patterns within complex datasets [26].\n\nUniform Manifold Approximation and Projection (UMAP). UMAP is a dimensionality reduction technique designed for visualization and the general nonlinear embedding of high-dimensional data. Based on manifold learning and topological data analysis, UMAP constructs a high-dimensional graph of data points and optimizes a low-dimensional projection to preserve both local and global structures. Compared to PCA, UMAP captures complex, non-linear relationships in data, making it more effective for revealing meaningful clusters. Unlike t-SNE, UMAP offers faster computation, better scalability, and more interpretable low-dimensional embeddings while maintaining the ability to preserve local neighborhood information [27].\n\nActivation separability metric\n\nGeneralized Discrimination Value (GDV). To quantify the degree of clustering, we used the GDV as published"}, {"title": null, "content": "Here, $\\mu_{\\alpha} = \\frac{1}{N} \\sum_{n=1}^{N} x_{n,d}$ denotes the mean,\n\n$\\sigma_{\\alpha} = \\sqrt{\\sum_{n=1}^{N}(x_{n,d} \u2013 \\mu_{\\alpha})^2}$ the standard deviation of dimension d.\n\nBased on the re-scaled data points $s_n = (s_{n,1},\u2026, s_{n,D})$, we calculate the mean intra-class distances for each class $C_i$\n\n$d(C_i) = \\frac{2}{N_i(N_i-1)} \\sum_{i=1}^{N_i-1} \\sum_{j=i+1}^{N_i} d(s_i^{(i)}, s_j^{(i)})$,\n\nand the mean inter-class distances for each pair of classes $C_i$ and $C_m$\n\n$d(C_i, C_m) = \\frac{1}{N_i N_m} \\sum_{i=1}^{N_i} \\sum_{j=1}^{N_m} d(s_i^{(i)}, s_m^{(j)})$.\n\nHere, $N_k$ is the number of points in class k, and $s_k^{(i)}$ is the $i$th point of class k. The quantity $d(a, b)$ is the euclidean distance between a and b. Finally, the Generalized Discrimination Value (GDV) is calculated from the mean intra-class and inter-class distances as follows:\n\n$GDV = \\frac{1}{\\sqrt{D}} [\\frac{1}{L} \\sum_{l=1}^{L} d(C_l) - \\frac{2}{L(L-1)} \\sum_{l=1}^{L-1} \\sum_{m=l+1}^{L} d(C_l, C_m)]$,\n\nwhereas the factor $\\frac{1}{\\sqrt{D}}$ is introduced for dimensionality invariance of the GDV with D as the number of dimensions.\n\nNote that the GDV is invariant with respect to a global scaling or shifting of the data (due to the z-scoring), and also invariant with respect to a permutation of the components in the N-dimensional data vectors (because the euclidean distance"}, {"title": "RESULTS", "content": "Refusal is a universal feature\n\nRefusal behavior is consistently observed across all six tested models, with a distinct separation between harmful and harmless instructions in the dimensionality-reduced residual activations. This separation is evident across model families and sizes. Figure 1 illustrates the refusal feature in three models at different layers. Table II summarizes the lowest GDV values, indicating the best separability between harmful and harmless instructions, along with the corresponding model layers and dimensionality-reduction techniques.\n\nRefusal is a nonlinear feature\n\nOur analysis demonstrates that refusal behavior in LLMS extends beyond a simple one-dimensional linear subspace, revealing a complex, nonlinear structure. Using PCA, UMAP, and t-SNE, we visualized dimensionality-reduced activations for harmful and harmless instructions across multiple layers of six LLMs. While PCA, a linear technique, captures variance through linear distances, UMAP and t-SNE effectively"}, {"title": "DISCUSSION", "content": "Refusal behavior, enabling differentiation between harmful and harmless instructions, was consistently observed across all six models tested, corroborating previous findings [13].\n\nOur analysis highlights that refusal mechanisms vary significantly across model families, reflecting architecture-specific strategies for harmful content detection. Qwen2 models primarily encode refusal features in early layers, achieving stable inter-class distances and compact intra-class structures. Bloom models exhibit peak separability in intermediate layers but demonstrate weaker discrimination in later layers, often misclassifying harmless instructions. Conversely, Llama models refine refusal gradually across layers, with stronger separability emerging in deeper layers.\n\nContrary to the assumption that refusal resides in a simple one-dimensional linear subspace [13], our findings reveal a more complex, nonlinear structure. Using PCA, UMAP, and t-SNE, we visualized latent space activations and found that nonlinear methods consistently outperformed PCA in separating harmful and harmless embeddings, as evidenced by the separability metric GDV.\n\nThese results align with recent studies on jailbreak attacks, which indicate that nonlinear features, rather than universal linear ones, drive the success of such attacks [29]. Although focused on jailbreak mechanisms, these findings suggest a potential connection between jailbreak features and the refusal and harmfulness features, given the critical role of refusal in determining whether a model responds or declines.\n\nFurthermore, the phenomenon of \"alignment faking,\" where LLMs pretend to align with training objectives while resisting preference modifications [30], raises concerns about the robustness of model alignment processes. Persistent alignment faking could lock in model preferences, posing challenges for future fine-tuning efforts.\n\nThese insights underscore the importance of nonlinear interpretability techniques for understanding refusal behavior and its implications for alignment, robustness, and ethical AI deployment."}, {"title": "CONCLUSION", "content": "This study investigates refusal behavior in large language models (LLMs) and provides new insights into its mechanisms and characteristics. By analyzing six LLMs across three architectural families, we demonstrate that refusal behavior is a universal phenomenon but exhibits architecture-specific patterns. Contrary to prior assumptions that refusal mechanisms are linear and confined to single activation directions, our findings reveal that they are inherently nonlinear and multidimensional. Using advanced dimensionality reduction techniques such as UMAP and t-SNE, we uncover richer and more complex activation patterns than those detected through traditional linear methods like PCA.\n\nThe results emphasize the importance of nonlinear interpretability methods in understanding and improving model alignment. Refusal mechanisms vary significantly across architectures, with Qwen models encoding refusal early, Bloom models demonstrating intermediate-layer strengths, and Llama models refining refusal behavior in deeper layers. These differences highlight the diverse strategies employed by LLMS to distinguish harmful and harmless prompts, which has implications for designing safer, more robust AI systems."}]}