{"title": "TELL2REG: ESTABLISHING SPATIAL CORRESPONDENCE BETWEEN IMAGES BY THE SAME LANGUAGE PROMPTS", "authors": ["Wen Yan", "Qianye Yang", "Shiqi Huang", "Yipei Wang", "Shonit Punwan\u00f0", "Mark Emberton", "Vasilis Stavrinides", "Yipeng Hu", "Dean Barratt"], "abstract": "Spatial correspondence can be represented by pairs of segmented regions, such that the image registration networks aim to segment corresponding regions rather than predicting displacement fields or transformation parameters. In this work, we show that such a corresponding region pair can be predicted by the same language prompt on two different images using the pre-trained large multimodal models based on GroundingDINO and SAM. This enables a fully automated and training-free registration algorithm, potentially general-isable to a wide range of image registration tasks. In this paper, we present experimental results using one of the challenging tasks, registering inter-subject prostate MR images, which involves both highly variable intensity and morphology between patients. Tell2Reg is training-free, eliminating the need for costly and time-consuming data curation and labelling that was previ-ously required for this registration task. This approach outperforms unsupervised learning-based registration methods tested, and has a performance comparable to weakly-supervised methods. Additional qualitative re-sults are also presented to suggest that, for the first time, there is a potential correlation between language semantics and spatial correspondence, including the spatial invariance in language-prompted regions and the difference in language prompts between the obtained local and global correspondences. Code is available at https://github.com/yanwenCi/Tell2Reg.git.", "sections": [{"title": "1 Introduction", "content": "MR imaging has increasingly been used for diagnosing prostate cancer, planning targeted biopsy and other treatment procedures. Registering images from different patients is an interesting research topic, which may enable propagating procedural plans from reference images to new patients, constructing MR-based lower-pelvic atlases and other similar population studies [1]. Among the sequences that are useful for prostate cancer diagno-sis, T2-weighted sequences contain the richest soft tissue contrast and are often used in these registration tasks, which is a focus of the previous work as well as this study 1. Registering inter-subject lower-pelvic MR images is challenging due to significant variability in intensity and morphology between subjects, unlike the intra-subject registration tasks [3]. This task usually requires large training datasets and benefits from segmentation labels and advanced training strategies [4]. Classical iterative"}, {"title": "2 Method", "content": "The above assertion - segmenting regions on both fixed and moving images using identical text prompts yields corresponding ROIs - represents an ideal scenario rather than a guaranteed outcome. With practical constraints such as the availability of medical-image-finetuned foundation models, selecting ROIs may require fur-ther prompt engineering and/or post-processing to en-sure that the segmented ROIs are corresponding pairs. This section describes a specific algorithm, utilising pre-trained SAM and GroundingDINO, outlined in Fig. 2."}, {"title": "2.1 Text to corresponding ROIs", "content": "We start our discussion with a theorem [10] stating that the registration task can be framed as a correspondence learning problem between regions. Let the fixed and moving images be $I^{fix}$ and $I^{mov}$, respectively. Estab-lishing spatial correspondence between the two images can be achieved by identifying $K$ pairs of corresponding ROIs, denoted as ${ (R_{k}^{fix}, R_{k}^{mov})}_{k=1}^{K}$. In the limiting case where each ROI reduces to a single pixel, the ROI-based correspondence becomes equivalent to a pixel-wise one, as governed by a dense displacement field (DDF).\nWe use GroundingDINO [14] to generate bounding boxes $B^{fix} = f_{dino}(I^{fix}, p)$ and $B^{mov} = f_{dino}(I^{mov}, p)$ for fixed and moving images with same text prompt $p$, respectively, where $p$ can be any text, examples are given in Table 2. Ideally, bounding boxes generated from iden-tical text prompts should correspond precisely between the fixed and moving images; however, due to the limits of the GroundingDINO model on prostate data, which could lead to false positive correspondence, by detecting regions that appear similar based on texture or intensity but are not corresponding regions of registration interest."}, {"title": "2.2 ROI correspondence refinement", "content": "To establish robust correspondence between these ROIS, we propose examining the similarity between ROI pro-totypes from the same or different text prompts. The ROI prototypes are generated by averaging the ROIs in the fixed and moving images. Given two sets of ROIs, ${ (R_{k}^{fix})}_{k=1}^{K_{fix}}$ and ${ (R_{k}^{mov})}_{k=1}^{K_{mov}}$, where $K_{mov}$ may not be equal to $K_{fix}$, the proposed matching process finds the correspondence ROIs by minimizing the L2 distance between the ROI prototypes, detailed in Algorithm 2.2, yielding $K_{cor}$ pairs of matched ROIs ${ (R_{k}^{fix}, R_{k}^{mov})}_{k=1}^{K_{cor}}$. It is interesting to discuss that, when the specificity of the multimodal segmentation system (Sec. 2.1) improves, this refining step may render itself unnecessary, while the differences to the previous work [10] include better-selected ROI sets (due to the correspondence-informing text prompts) and thus a different similarity function. These ROIS ${ (R_{k}^{fix}, R_{k}^{mov})}_{k=1}^{K_{cor}}$ represent the region-level correspondence between the fixed and moving images."}, {"title": "2.3 Optional dense transformation", "content": "When useful, the region-level correspondence (repre-sented by the ROI pairs, Sec. 2.2) can be converted to voxel-level dense correspondence (represented by a DDF), by iteratively minimising a region-specific align-ment error $L_{roi}$ and a $L^{2}$ regularization term $L_{reg}$:\n$L = \\mathbb{E}_{k}[L_{roi}(R_{k}^{fix}, T(R_{k}^{mov}, \\Theta))] + \\lambda L_{reg}(\\Theta)$, where $\\mathbb{E}_{k}[\\cdot]$ is the mathematical expectation, $\\Theta$ is the parameters of the transformation model (here, the DDF), and $\\lambda$ is the regularization weight. The region-specific alignment error $L_{roi}$ is an equally weighted Dice and MSE loss."}, {"title": "3 Experiments and Results", "content": null}, {"title": "3.1 Dataset and implementation details", "content": "The five hundred and forty-two pairs T2 pelvis mpMR images were acquired from 850 prostate cancer patients, part of several clinical trials conducted at University Col-lege London Hospital. The images were resampled to 1mm isotropic resolution, sized 200 \u00d7 200 \u00d7 96. Prostate segmentation masks labelled by experts are readily avail-able for evaluation and ablation studies. The data were split to training, validation and test sets by 365, 90 and 87 pairs respectively. As our proposed method is to-tally training-free, we only use 87 pairs test data, while the other methods use all datasets. The proposed model was implemented with Pytorch, using pre-trained mod-els [14, 11], described in Sec. 2.1. Dice and target regis-tration error (TRE) of centroids are used to evaluate the registration performance. The detection ratio describes the proportion of prostate ROI correspondences detected using a specific text prompt: $N_{Rostate}/N_{prostate}$, where $N_{Rostate}$ is detected corresponding prostate ROIs, and $N_{prostate}$ is the total number of prostate in ground-truth."}, {"title": "3.2 Comparison experiments", "content": "We compare the proposed method with the state-of-the-art registration methods, including VoxelMorph [5], KeyMorph [6] and TransMorph [7], listed in Table 1. All the compared methods also required substantial training data and computation costs for training. The results showed that Tell2Reg outperforms unsupervised methods. As the only exception, weakly supervised TransMorph, which requires prostate segmentation la-bels, led to a higher average Dice without statistical significance, than that from Tell2Reg (p = 0.060). It is also arguable that the TREs may be more indica-tive of registration performance, in many applications. Fig. 3 shows the visualization of the proposed Tell2Reg outputs. Prior work [15] showed that an atlas from"}, {"title": "3.3 Text prompts comparison", "content": "We first test the registration performance by changing the text prompts and how they are sampled. We sum-marise our experience as follows: First, for a specific registration task, the performance is more stable with a fixed set of prompts, predefined empirically and quali-"}, {"title": "4 Discussion and Conclusion", "content": "In this paper, we propose a novel registration method which learns the correspondence between regions, by utilising the existing SAM-based object detection model with text prompts. This approach does not require re-training, and shows better generalisability, compared with methods that even require substantial training and training data. Our future work will focus on exploring prostate-specific multimodel foundation models, devel-oping automated prompt engineering, and implementing refinement strategies to enhance the robustness, adapt-ability, and efficiency of the proposed method."}]}