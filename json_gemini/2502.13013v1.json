{"title": "HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit", "authors": ["Qingwei Ben", "Feiyu Jia", "Jia Zeng", "Junting Dong", "Dahua Lin", "Jiangmiao Pang"], "abstract": "Current humanoid teleoperation systems either lack reliable low-level control policies, or struggle to acquire accurate whole-body control commands, making it difficult to teleoperate humanoids for loco-manipulation tasks. To solve these issues, we propose HOMIE, a novel humanoid teleoperation cockpit integrates a humanoid loco-manipulation policy and a low-cost exoskeleton-based hardware system. The policy enables humanoid robots to walk and squat to specific heights while accommodating arbitrary upper-body poses. This is achieved through our novel reinforcement learning-based training framework that incorporates upper-body pose curriculum, height-tracking reward, and symmetry utilization, without relying on any motion priors. Complementing the policy, the hardware system integrates isomorphic exoskeleton arms, a pair of motion-sensing gloves, and a pedal, allowing a single operator to achieve full control of the humanoid robot. Our experiments show our cockpit facilitates more stable, rapid, and precise humanoid loco-manipulation teleoperation, accelerating task completion and eliminating retargeting errors compared to inverse kinematics-based methods. We also validate the effectiveness of the data collected by our cockpit for imitation learning. Our project is fully open-sourced, demos and code can be found in our website.", "sections": [{"title": "I. INTRODUCTION", "content": "Humans consistently rely on their physical agility to walk, squat to specific heights, and manipulate objects when performing tasks such as transferring goods in warehouses. While humanoid robots are expected to eventually take over these labor-intensive tasks, achieving full autonomy remains a significant challenge. In the interim, a robust teleoperation system is essential, enabling operators to seamlessly control robots for complex loco-manipulation tasks. These systems not only allow humans to guide robots with precision and fluidity but also serve as a critical tool for collecting high-quality demonstrations to advance autonomous capabilities. Two indispensable components are required to realize this vision: (1) a robust loco-manipulation policy that enables robots to extend operational workspace; (2) a method that allows a single operator to precisely control the robot's upper-body manipulation and lower-body locomotion simultaneously.\nHowever, most existing efforts to teleoperate humanoid robots focus solely on controlling the upper body [1, 2, 3, 4, 5]. This restriction significantly limits the robot's operational workspace, as it prevents the robot from moving freely or adjusting its posture (e.g., squatting) to adapt to different task requirements. While some researchers have explored equipping humanoids with loco-manipulation capabilities through reinforcement learning (RL) [6, 7, 8, 9, 10, 11], these approaches often fail to achieve the necessary range of motion, such as squatting to specific heights, which is critical for many real-world tasks. Additionally, many humanoid whole-body teleoperation methods require operators to physically move to control the robot's motion, making it challenging to maintain precise and stable full-body control over extended distances. Mainstream teleoperation systems typically rely on vision-based methods [1, 3, 4, 5] or heterogeneous exoskeletons [2] to determine end-effector poses, followed by inverse kinematics (IK) to compute joint positions. Nonetheless, both pose estimation and IK solving introduce inaccuracies, which compromises the precision and reliability of teleoperation.\nTo address these issues, we introduce HOMIE, a novel humanoid teleoperation cockpit composed of a humanoid loco-manipulation policy and an exoskeleton-based hardware system. This cockpit enables a single operator to precisely and efficiently control a humanoid robot's full-body movements for diverse loco-manipulation tasks. Integrated into simulation environments, our cockpit also enables seamless teleoperation in virtual settings. Specifically, we introduce three core techniques to our RL-based training framework: upper-body pose curriculum, height tracking reward, and symmetry utilization. These components collectively enhance the robot's physical agility, enabling robust walking, rapid squatting to any required heights, and stable balance maintenance during dynamic upper-body movements, thereby significantly expanding the robot's operational workspace beyond existing solutions. Unlike previous whole-body control methods that depend on motion priors derived from motion capture (MoCap) data [12], our framework eliminates this dependency, resulting in a more efficient pipeline. Our hardware system features isomorphic exoskeleton arms, a pair of motion-sensing gloves, and a pedal. The pedal design for locomotion command acquisition liberates the operator's upper body, enabling simultaneous acquisition of upper-body poses. Since the exoskeleton arms are isomorphic to the controlled robot and each glove has 15 degrees of freedom (DoF), which is more than most existing dexterous hands, we can directly set upper-body joint positions from the exoskeleton readings, dispensing with IK and achieving faster and more accurate teleoperation. Moreover, our gloves can be detached from the arms, allowing them to be reused in systems isomorphic to different robots. The total cost of the hardware system is only $0.5k, which is significantly lower than that of MoCap devices [13].\nThrough ablation experiments, we validate the effectiveness of each technique in our training framework and demonstrate the robustness of the resulting policies across different robots. Our evaluation shows that the hardware system supports 200% faster and more accurate pose acquisition than previous methods, enabling operators to complete tasks more efficiently than virtual reality (VR)-based approaches. Real-world studies confirm that the trained policies can be deployed directly in the real world, allowing robots to perform diverse loco-manipulation tasks stably in complex environments. We further show that real-world data collected via HOMIE can be effectively used by imitation learning (IL) algorithms, allowing humanoid robots to autonomously execute tasks.\nIn summary, our core contributions are three-fold:\n1) We propose HOMIE, the first implementation of a cockpit for humanoid whole-body loco-manipulation teleoperation enables a single operator to fully control the robot and execute diverse tasks seamlessly.\n2) We first achieve robust humanoid loco-manipulation including squatting under arbitrary continuous changing upper-body poses without using any motion prior.\n3) Our hardware system supports more precise and rapid whole-body control of humanoids than previous works, reducing task completion times by nearly half."}, {"title": "II. RELATED WORKS", "content": "Teleoperating dual-arm robots to perform complex manipulation tasks is an efficient way to collect real-world expert demonstration, which can then be used by IL to learn autonomous skills [14, 1, 3, 17, 18]. Some researchers utilize robotic arms identical to the teleoperated ones [19, 14, 15, 16], making joint-matching possible, thus ensuring high accuracy and fast response speed. However, due to the high cost of robotic arms, the establishment of such a system incurs significant expenses. Additionally, teleoperating dexterous hands with these systems is not feasible. An alternative approach is to use VR devices [1, 5] or just a camera [20, 4, 21]. These works use vision-based techniques to capture the operator's wrist postures and key points of the hands, which are used by IK to calculate the joint positions of the arms and hands. However, due to limitations in the accuracy, inference speed, and difficulty in handling occlusions of pose estimation, such approaches cannot guarantee rapid and accurate pose acquisition. Some researchers try to use MoCap methods [13, 22, 23, 24] to acquire more accurate poses at higher frequencies, but MoCap equipment is very expensive. Moreover, since IK is an iterative method that approximates solutions, even when wrist and hand poses are captured accurately, the limitations of IK may prevent the robot from achieving the desired posture. Another possible solution is an exoskeleton-based teleoperation system, which does not require an additional identical robot, thus the overall cost is relatively low. Some research calculates the end-effector pose of the exoskeleton using Forward Kinematics"}, {"title": "III. METHOD", "content": "As shown in Fig. 2, HOMIE consists of a low-level policy $\\pi_{loco}$ and an exoskeleton-based hardware system. At any given time t, the operator inside the cockpit observes the first point of view (FPV) of the robot through the display. By stepping on the pedal, the operator provides the required locomotion commands $C_t = [U_{x,t}, W_{yaw,t}, h_t]$ where $U_{x,t}$ is the desired forward or backward speed, $W_{yaw,t}$ is the turning speed, and $h_t$ is the target height of the robot's torso. The policy $\\pi_{loco}$ controls the robot's lower-body based on $C_t$. Meanwhile, the operator controls the exoskeleton to provide the required joint angles $q_{upper}$ for the robot's upper-body, which are directly set to the robot. The upper and lower bodies work in coordination, continuously cycling through the process, ultimately enabling teleoperating robots to complete loco-manipulation tasks either in the real world or in simulation. Communications between the cockpit and the robot are achieved via Wi-Fi, allowing operation even when the robot is far from the hardware system. We can collect demonstrations while teleoperating the robot and use them to train an autonomous policy $\\pi_{auto}$. Once trained successfully, $\\pi_{auto}$ can take over the operator to give $C_t$ and $q_{upper}$, thus driving the robot to perform tasks autonomously."}, {"title": "B. Locomotion Policy", "content": "To enable humanoid robots to perform loco-manipulation tasks, we design an RL-based training framework, which trains different robots to accomplish squatting and walking under continuously changing upper-body poses. We take Unitree G1 as an example and show the process of the framework in Fig. 3. The policy $\\pi_{loco}$ trained by this process is capable of zero-shot sim-to-real transfer. We introduce the training settings and three key techniques of our framework in this section.\n1) Training Settings: The observations of one step are defined as $O_t = [C_t, w_t, g_t, q_t, \\dot{q_t}, a_{t-1}]$, where $C_t$ is the command, $w_t$ is the body's angular velocity, $g_t$ is the projection of $\\tilde{y} = [0, 0, -1]$ in the robot's torso coordinate frame, $q_t$ is the joint angles of all joints of the robot, $\\dot{q_t}$ is the joint velocities of all joints of robot, $a_{t-1}$ is the last time action. Then we can get the whole observations of $\\pi_{loco}$ by concatenating $O_{t-5:t}$. The actions $a_t$ of the policy correspond one-to-one with the joints of the robot's lower body. After the neural network computes $a_t$ based on $O_{t-5:t}$, we use\n$\\tau_{t,i} = K_{pi} \\times (a_{t,i} - q_{0,t,i}) - Kd_i \\times \\dot{q_{t,i}}$  (1)\nto calculate the torques for joint motors, thereby driving the motors to work and enabling the robot's movement. In the equation, i denotes the index of joints, {$K_{pi}$} and {$Kd_i$} are stiffness and damping of each joint, {$q_{0,t,i}$} are default joint positions of each joint. Our framework is implemented based on the code of [48, 49], and more training details can be found in Appendix A.\n2) Upper-body Pose Curriculum: We use a curriculum learning technique to ensure that $\\pi_{loco}$ can still complete locomotion tasks under any continuously varying poses of the robot's upper-body. We adjust the sampling range of the upper body joint angles using the upper action ratio $r_a$. At the start of training, $r_a$ is set to 0. Each time the policy drives the robot to track the linear velocity with a reward function that reaches the threshold, $r_a$ increases by 0.05, eventually reaching 1. We first sample $r'$ from the probability distribution\n$p(x|r_a) = \\frac{20(1-r_a) e^{-20(1-r_a)x}}{1-e^{-20(1-r_a)}}, r_a \\in [0,1)$  (2)\nand then resample $a_i$ by $U(0, r_a)$. We actually sample $a_i$ by\n$a_i = U(0, \\frac{1}{20(1 - r_a)} ln(1 - U(0,1) (1 - e^{-20(1 - a)})))$  (3)\nAs $r_a$ increases, the probability distribution gradually transitions from being close to 0 to U(0,1). This ensures that during the curriculum process, the probability distribution consistently satisfies $p(x|r_a) > 0,\\forall x \\in [0,1], r_a \\in [0,1)$. Compared to directly using $U(0, r_a)$, this method approaches the final target in a more gradual and smoother manner. For better understanding of Eq. (3), we visualize it in Appendix A.\nTo simulate the continuous changes in upper body movements when controlled by our cockpit, we resample target upper-body poses every 1 second according to the above process. We then use uniform interpolation to ensure that the target movement gradually changes from the current value to the desired value over the 1-second interval. Without this approach, we find that the robot struggles to maintain balance under continuous motions.\n3) Height Tracking Reward: Tracking heights can significantly expands the feasible operational workspace of humanoid robots, thus helping the robots perform more loco-manipulation tasks. Therefore, $\\pi_{loco}$ needs to enable the robot to squat to the target height $h_t$. To achieve this, we design a new reward function\n$r_{knee} = -||(h_{r,t} - h_t) \\times (-\\frac{\\Delta k_{knee,t}}{\\Delta k_{knee,max} - \\Delta k_{knee,min}})||$,  (4)\nwhere $h_{r,t}$ is the robot's actual height, $q_{knee, min}$ and $q_{knee,max}$ are the maximum and minimum actions of knee joints, $q_{knee,t}$ is the current positions of robot's knee joints. $r_{knee}$ encourages flexion of the knee joints when $h_{r,t} < h_t$, and encourages extension when $h_{r,t} > h_t$. In the training process, we resample all commands every 4 seconds. At this point, one-third of the environments are randomly selected to train the robot to squat, while the remaining two-thirds focus on teaching the robot to stand and walk. This strategy helps balance the learning of squatting and walking. Additionally, the same environment switches between learning to squat and learning to walk, enabling the policy to smoothly transition between squatting and walking tasks. For better understanding of Eq. (4), we visualize it in Appendix A.\n4) Symmetry Utilization: We introduce the same trick as [50] to our training framework. Each time we obtain a transition $T_t = (s_t, a_t, r_t, s_{t+1})$ from the simulation, we perform a flip operation on it. Specifically, we apply symmetry to the actor and critic observations with respect to the robot's x-z plane. This involves flipping elements such as the positions, velocities, and actions of the robot's left and right joints, as well as the desired turning velocity, across the x-z plane to obtain a mirrored transition $T'$. Both $T_t$ and $T'$ are then added to the rollout storage. This process helps to improve data efficiency and ensure symmetry in the sampled data, reducing the likelihood of the trained policy being asymmetrical in terms of left and right performance. In the learning phase, we also apply this procedure to the samples $T_t$ got from the rollout storage to get $T'$. Both $T_t$ and $T'$ are passed through the actor and critic networks to obtain $a_t, a_t, V_t, V'$ respectively, which are used to calculate additional losses:\n$\\mathcal{L}_{actor}^{sym} \\triangleq MSE(a_t, a'_t)$,  (5)\n$\\mathcal{L}_{critic}^{sym} \\triangleq MSE(V_t, V'_t)$.  (6)\nThese two losses are added to the network optimization process, thereby enforcing symmetry of the neural network."}, {"title": "IV. EXPERIMENT", "content": "1) Ablation of training framework: In this section, we perform ablation experiments on the proposed upper-body pose curriculum, the height tracking reward, and the use of symmetry. All ablation experiments are conducted based on the methods described in Sec. III-B. For each setting, we use three random seeds to train policies for Unitree G1 and evaluate them in 1000 environments over a 20-second evaluation period with random upper-body poses sampled from Eq. (3) with $r_a \\rightarrow 1$. Metrics for evaluation are tracking linear velocity error, tracking angular velocity error, tracking height error, symmetry loss and living time. The final performance for each setting is obtained by computing the average and standard deviation of the results across the three policies trained from three random seeds. All trainings are conducted on Nvidia RTX 4090 and simulated by Isaac Gym with 4096 parallel environments, where components unrelated to the ablation are kept unchanged, and only relevant parts are modified for training. Detailed parameters used in training and evaluation processes are listed in Appendix A. We mark the setting of our proposed method as ours in the following sections.\n= Upper-body Pose Curriculum. We compare ours against two alternatives: w/o cur, which omits the curriculum and directly samples $a_i = U(0,U(0,1))$, and rand, which uses the same $r_a$ curriculum but replaces Eq. (3) with $a_i = U(0,U(0, r_a))$. Since all three methods adopt the same sampling strategy $a_i = U(0,U(0,1))$ as $r_a \\rightarrow 1$, the final objective remains consistent, ensuring a fair comparison. The experimental results, shown in the first row of Fig. 6, reveal that ours outperforms both w/o cur and rand in linear velocity tracking, angular velocity tracking, and height error, with faster convergence and smaller errors. There is no significant difference between w/o cur and rand in the final results for these metrics. Given that the symmetry loss can reach values on the order of 20 without constraints, no significant difference is observed across the three methods in terms of symmetry loss. All three configurations achieve similar final living times, but ours and w/o cur converge more quickly. Rand, despite employing some curriculum adjustments, is limited by $r_a$, and values in the range ($r_a$, 1] are not sampled during training, making it harder for the model to converge as $r_a$ increases. In contrast, both ours and w/o cur sample the full [0,1] range from the beginning, enabling faster and more stable convergence. Thus, our curriculum approach leads to better\nHeight Tracking Reward. We design two additional algorithms w/o knee, which does not use $r_{knee}$ described in Eq. (4) and hei, which also omits $r_{knee}$ but increases the scale of the height tracking reward. We show the results in the second row of Fig. 6. As shown in the figure, none of the three settings cause significant changes in the symmetry loss during training. In terms of linear velocity error and angular velocity error, ours and w/o knee perform similarly, while hei shows much larger errors. For height error, our method converges faster than both w/o knee and hei, even though hei initially performs better (at 400 steps). There is no significant difference among the three settings in terms of living time. These results indicate that just scaling up the height tracking reward in hei may initially lead to faster reduction in height tracking error, but it negatively affects the feedback from other rewards, preventing the robot from balancing multiple tasks effectively. In fact, hei ultimately does not achieve faster convergence in height tracking compared to ours. In contrast, the inclusion of $r_{knee}$ in our method provides more specific guidance for squat tracking, allowing the robot to reduce tracking error and converge more quickly. This highlights the effectiveness of $r_{knee}$ in helping the robot learn squat motions.\nSymmetry Utilization. We introduce three algorithmic variants for comparison with ours in terms of symmetry utilization: w/ aug, which uses only symmetrical data augmentation; w/ sym, which only uses symmetry loss; and none, which does not employ symmetrical data augmentation or symmetry loss. Testing results are presented in the third row of Fig. 6. Except for symmetry loss, the performance of ours and w/ aug is similar. However, when considering overall tracking accuracy, ours performs slightly better. On the other hand, w/ aug exhibits a very high symmetry loss, suggesting that using symmetry loss helps maintain the robot's left-right symmetry in the learned policy. This indirectly supports the idea that a symmetric policy benefits the robot's locomotion tasks [50]. Both nsym and none show a tendency for improvement, but their training speed is much slower. Notably, a direct comparison between w/ sym and none reveals that w/ sym achieves lower symmetry loss. However, due to slower training, none exhibits less symmetry breaking compared to w/ aug. In summary, symmetry data augmentation significantly improves training efficiency, while the use of symmetry loss effectively prevents the policy from sacrificing symmetry to complete tasks and also benefits the task itself.\n2) Training on Different Robots: We select another kind of robot, Fourier GR-1, which is quite different from Unitree G1, to demonstrate the generality of our approach across different robot models. As shown in Fig. 7, Fourier GR-1 is much taller and heavier than Unitree G1 while having lower hand weight ratio. Compared to the training setting of Unitree G1, we only change the range of height tracking and some robot-specific distance values, without any other changes in reward scales or training pipeline. We evaluate the policy trained after 2k steps of each robot with metrics used in Sec. IV-A1 and present them in Tab. II. The results demonstrate that even though these two"}, {"title": "B. Teleoperation Hardware Performance", "content": ""}, {"title": "C. Teleoperation System", "content": ""}, {"title": "1) Real World", "content": "We deploy the trained policy on the Unitree G1 in the real world and teleoperate it to perform various loco-manipulation tasks using our isomorphic exoskeleton hardware system. The deployment code for G1 is derived from [52]. Fig. 1 (a) and Fig. 1 (c) demonstrate the robot's capability to squat, pick objects from lower shelves, and place them on higher ones, as well as to grasp and transfer boxes between shelves utilizing its locomotion abilities. Fig. 1 (b) highlights the extensibility of our system, enabling two operators to control separate robots and collaboratively perform tasks, such as transferring apples. In Fig. 1 (d), the robot is controlled to push a 60 kg person sitting in a chair, who weighs roughly twice as much as the robot, demonstrating the robustness of the loco-manipulation system. Fig. 1 (e) illustrates how the robot uses its loco-manipulation abilities to open an oven by grasping the handle and moving backward simultaneously. Fig. 1 (f) shows that our teleoperation system is capable of performing dual-hand collaborative tasks, such as one hand passing an object to the other. Fig. 1 (g) demonstrates the robot's ability to grasp objects from low ground, while Fig. 1 (h) shows the robot's capability to lift and place heavy items, such as a bundle of flowers, into a box using both arms. Fig. 1 (i) demonstrates how the robot maintains balance with different upper-body poses. In all these tasks, each robot is controlled by a single operator, and the communication between the robot and operator is facilitated via Wi-Fi, without restricting the robot's movement space. These tasks showcase the robustness of our loco-manipulation policy and HOMIE's ability to teleopeate humanoids perform a wide range of complex tasks in various environments."}, {"title": "2) Simulation", "content": "We transfer the trained policies for Unitree G1 and Fourier GR-1 from Isaac Gym to a scene developed by GRUtopia [53], which is based on Isaac Sim and IsaacLab [54]. This migration enables the use of HOMIE to control robots within a variety of simulated environments. By leveraging these simulated scenes, the robots can perform diverse loco-manipulation tasks more cost-effectively and in a wider range of scenarios than would be feasible in the real world. As shown in Fig. 11, operators can seamlessly direct the robots' movements and actions in complex, realistic settings, demonstrating the versatility and applicability of HOMIE in diverse simulated contexts."}, {"title": "D. Autonomous Policy", "content": "1) Data Collection: To validate the effectiveness of the demonstratons collected by HOMIE for IL algorithms, we design two distinct tasks: Squat Pick: squatting to pick a tomato on the lower sofa; Pick & Place: picking and placing a tomato. We capture RGB images, robot states $q_t$, the upper-body commands $Q_{upper}$, and the locomotion commands $C_t$ at 10Hz, and collect 50 episodes per task. The hardware setup for image capture can be found in Fig. 12.\n2) Training Setting: We adopt an end-to-end visuomotor control policy that takes images and robot proprioceptive signals as inputs and continuously outputs robot control actions. We employ a model named Seer [55], which features an autoregressive transformer architecture. Multi-view images are processed through a MAE-pretrained ViT encoder, and the features of robot proprioceptive states are extracted using an MLP. These features are subsequently concatenated into tokens. The information of these tokens are then integrated by a transformer encoder. The transformer encoder utilizes an autoregressive method to generate latent codes for controlling upper arm joints, dexterous hand movements, and height commands. The final control action output is generated by three distinct regression heads. The whole network are optimized using SmoothL1 loss. In real-world training scenarios, we configure the sequence length to 7, with both visual foresight and action prediction steps set to 3. We employ the MAE pre-trained ViT-B encoder, using bfloat16 configuration to speed up inference. This model is trained on eight A100 GPUs for 40 epoches, and we selected the checkpoint with the lowest average validation loss for evaluation.\n3) Learning Results: After training with collected data, we deploy the trained model to humanoid robot in the real world, with the trained $T_{auto}$ taking over operator to control the robot."}, {"title": "V. CONCLUSION AND LIMITATIONS", "content": "In this paper, we introduce HOMIE, a novel humanoid teleoperation cockpit for humanoid loco-manipulation. With a low-cost isomorphic exoskeleton hardware system and a humanoid loco-manipulation policy trained by our RL training framework, HOMIE enables a single operator to teleoperate the whole body of humanoid robots and perform diverse loco-manipulation tasks either in the real world or in the simulation. Owing to the incorporation of an upper-body pose curriculum, a height-tracking reward, and symmetry-based techniques, Our training framework enables the development of robust loco-manipulation policies, ensuring stable walking and squatting capabilities across diverse robotic platforms, even under dynamically changing upper-body poses. Leveraging isomorphic exoskeleton arms, HOMIE enables significantly faster task execution than other systems, and our gloves are compatible with multiple kinds of dexterous hands. We present several ablation studies and real-world experiments to validate the robustness and accuracy of our system. In addition, we show the usability of collected data for IL.\nLimitations Although we have developed robust loco-manipulation policies, these policies still fall short of ensuring reliable traversal over diverse terrains. Additionally, the 15-DoF design of the motion-sensing gloves for the thumb does not fully align with human anatomy, resulting in less intuitive and smooth operation when controlling certain dexterous robotic hands. Furthermore, the current system lacks force feedback, which limits its effectiveness in applications requiring precise haptic interaction. Addressing these limitations will be a central focus of our future research efforts."}]}