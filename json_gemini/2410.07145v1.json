{"title": "STUFFED MAMBA: STATE COLLAPSE AND STATE CAPACITY OF RNN-BASED LONG-CONTEXT MODELING", "authors": ["Yingfa Chen", "Xinrong Zhang", "Shengding Hu", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "abstract": "One essential advantage of recurrent neural networks (RNNs) over transformer- based language models is their linear computational complexity concerning the sequence length, which makes them much faster in handling long sequences during inference. However, most publicly available RNNs (e.g., Mamba and RWKV) are trained on sequences with less than 10K tokens, and their effectiveness in longer contexts remains largely unsatisfying so far. In this paper, we study the cause of the inability to process long context for RNNs and suggest critical mitigations. We examine two practical concerns when applying state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to inputs longer than the training length and (2) the upper bound of memory capacity. Addressing the first concern, we first investigate state collapse (SC), a phenomenon that causes severe performance degradation on sequence lengths not encountered during training. With controlled experiments, we attribute this to overfitting due to the recurrent state being overpa- rameterized for the training length. For the second concern, we train a series of Mamba-2 models on long documents to empirically estimate the recurrent state capacity in language modeling and passkey retrieval. Then, three SC mitigation methods are proposed to improve Mamba-2's length generalizability, allowing the model to process more than 1M tokens without SC. We also find that the recurrent state capacity in passkey retrieval scales exponentially to the state size, and we empirically train a Mamba-2 370M with near-perfect passkey retrieval accuracy on 256K context length. This suggests a promising future for RNN-based long-context modeling.", "sections": [{"title": "INTRODUCTION", "content": "Recent transformer-based language models (Vaswani et al., 2017; Brown et al., 2020; Touvron et al., 2023; Achiam et al., 2023; Dubey et al., 2024) have demonstrated impressive capabilities in reasoning over long sequences with thousands and even millions of tokens (Team, 2024a;b; GLM et al., 2024). However, they rely on the attention mechanism that scales quadratically regarding the sequence length, making them extremely costly for inference over long sequences. In contrast, recurrent neural networks (RNNs) (Bengio et al., 1994) have a contextual memory with constant state size. Thus, during inference, their per-token computational and memory complexity scales linearly with the sequence length, making them much more efficient in processing long sequences.\nDespite the promising future of RNNs to process long contexts in terms of efficiency, their long- context performances are far from satisfying. Most recent state-of-the-art (SOTA) RNN-based language models (hereafter referred to as RNNs for simplicity), such as Mamba-1 (Gu & Dao, 2023), Mamba-2 (Dao & Gu, 2024), the RWKV series (Peng et al., 2023a; 2024), and GLA (Yang et al., 2023) are trained on sequences with less than 10K tokens. Existing works have shown that Mamba-1 and RWKV-4 suffer from severe performance drops when the context length exceeds their training length (Ben-Kish et al., 2024; Zhang et al., 2024a; Waleffe et al., 2024).\nIn this paper, we study the problem of what causes the current RNNs' inability to handle long contexts and the possible solutions for supporting long contexts. When applying RNNs to longer contexts,"}, {"title": "RELATED WORKS", "content": "RNN-Based Language Models There is a recent surge of interest in RNN-based language models, because, contrary to transformer-based ones, their per-token inference cost does not increase with the sequence length. Linear Attention (Katharopoulos et al., 2020) replaces the softmax attention in transformer-based models with kernel-based approximations that have equivalent recurrent formula- tions. Some notable recent RNNs include the RWKV series (Peng et al., 2023a; 2024), the Mamba series (Gu & Dao, 2023; Dao & Gu, 2024), Gated Linear Attention (Yang et al., 2023), among others"}, {"title": "PRELIMINARY", "content": "Most experiments in this study focus on Mamba-2 (Dao & Gu, 2024) because it has shown strong capabilities on several tasks and has publicly available checkpoints of multiple sizes, allowing us to explore the relationship between state sizes and length limits. Moreover, it is more widely studied than other RNNs, making it easier to use existing works as a reference.\nMamba-2 The Mamba-2 architecture consists of a stack of L Mamba-2 layers, each Mamba-2 layer consists of H heads that are computed in parallel, and the output of a layer is the sum of the"}, {"title": "STATE COLLAPSE", "content": "We first examine state collapse (SC)\u2014a phenomenon that causes RNN models to exhibit abnormal behaviors on inputs longer than those seen during training. We analyze the effects of SC on language modelings and the passkey retrieval task. Then we trace SC to the components of the state's update rule and provide an explanation regarding training length overfitting. Finally, we propose three training-free mitigation methods by modifying the update rule and one method based on continual pre-training on longer sequences to avoid overfitting."}, {"title": "LENGTH GENERALIZATION FAILURE", "content": "Figure 1 shows the language modeling loss of Mamba-2 and RWKV-6 beyond their training lengths. For controllability and to synthesize prompts of arbitrary lengths, this loss is computed on a prompt consisting of only the \u201c\\n\u201d characters, which we refer to as the \u201cnewlines\u201d prompt. However, we emphasize that the same observation also exists when processing texts from the pre-training corpus. The result shows that both RNNs suffer great performance degradation when the context length is much longer than their training lengths, converging around the loss of random guessing."}, {"title": "Passkey Retrieval Evaluation", "content": "Language modeling may not reflect downstream capabilities, thus, we evaluate several strong RNNs on the passkey retrieval task (Mohtashami & Jaggi, 2023; Zhang et al., 2024a), a simple synthetic task where a model is prompted to recall a 5-digit passkey from a"}, {"title": "WHAT IS THE CAUSE OF STATE COLLAPSE?", "content": "Since the recurrent state's dimensionality does not change over time, the sharp change of behavior during state collapsing must be a result of a change in the state's value. We inspect the statistics of the recurrent states of each layer in Mamba-2 370M and find that the mean and variance of some heads change sharply when the context length exceeds the training length, as shown in Figure 4. Appendix G includes a more detailed report on the statistics of each head. The state at t = 20K of one head with exploding variance is shown in Figure 5. From it, we discover that this variance explosion can be largely attributed to a few outlier channels, while most channels are relatively stable.\nWe emphasize that SC occurs largely independent of the prompt, occurring in both pre-training data samples and generated meaningless texts, even for prompts consisting of only whitespace characters (Figure 1 (a) and (b) shows SC on two different prompts). This means that the information inserted into the state does not cause its collapse.\nTo further attribute to the specific variables that cause SC, we inspect the values of \u2206t, Bt, and Xt on various heads with collapsing states. Figure 6 reports one example of the inspection, we can see that xt is relatively stable compared to the \u2206t and Bt, even though they are all functions of ut. We also notice that Bt explodes earlier than At. Therefore, we conclude that the collapse is largely attributable to Bt. Further inspection reveals that the convolutional weights that generate \u2206t and Bt (Eq. 8 and 10) are noticeably greater in variance than those for xt (Eq. 9). We leave a more in-depth attribution study for future work."}, {"title": "STATE COLLAPSE AS A RESULT OF STATE OVERPARAMETERIZATION", "content": "Here, we present a high-level explanation for SC. We argue that SC arises from state overparame- terization relative to the training length. In other words, the state capacity is excessively large for the training length, allowing the model to achieve strong language modeling performance without learning how to forget when the state is about to overflow. To support this argument, we formulate the hidden state as a weighted sum of previously inserted information:\n$h_t = \\sum_{i=1}^{t} a_{i:t} B_i x_i, \\quad a_{i:t} = \\prod_{j=i}^{t} \\alpha_j \\in (0,1)$\nTherefore, ai:t describes the memory strength about the i-th token at t time step. Figure 7 shows the memory strength of the first token at different time steps, and we find that the exploded heads (heads 2, 4, and 7 in the 38th layer) have a strong inclination toward retaining all information within the training length, with a memory strength of over 0.8 at t=8K. This implies that the model has not learned to forget information (by producing a smaller decay aj) in a manner to avoid overflowing the state with too much information. Furthermore, we pre-train Mamba-2 from scratch with TTrain = 512 and evaluate the intermediate checkpoints on passkey retrieval, as reported in Figure 8. It shows that SC is only exhibited by checkpoints beyond a certain amount of training, which coincides with behaviors of overfitting\u2014a result of overparameterization. One can also notice that the overfitted checkpoint outperforms earlier checkpoints on shorter sequences, which further strengthens the hypothesis that the model converges to less forgetting. Finally, as we will show in Section 4.3.2, for any given training length, there exists a state size where SC will be exhibited if and only if the model's state size is greater."}, {"title": "HOW TO MITIGATE STATE COLLAPSE?", "content": "Based on the analyses in the previous section, we propose several SC mitigation methods to make the model generalize better along the sequence length. In brief, we propose three training-free methods by modifying the update rule to avoid overflowing the state. Additionally, we directly train on longer sequences to encourage learning to smoothly forget the earliest information when the context is too long."}, {"title": "TRAINING-FREE MITIGATION METHODS", "content": "Method 1: Forget More and Remember Less Since the variance of the state explodes during SC, we can reduce this by increasing the amount of state decay (i.e., forget more) or reducing the amount of inserted information (i.e., remember less). Based on the analysis in Section 4.2, we choose to intervene in the components Bt and \u03b1t, which control the insertion strength and memory decay strength, respectively. Existing works have experimented with modifying At, but it controls both the insertion and decay strength, making it hard to analyze and control.\nMethod 2: State Normalization The main idea is to normalize the state after each update to ensure that the state's norm is always below a threshold \u03c1 \u2208 R. Specifically, we decay the state ht at each time step to ensure that ||ht|| \u2264 \u03c1. Thus, we get the following update rule.\n$h_t = h_{t-1}A_t + B_t^\\top x_t$\n$h_t = \\begin{cases} \\frac{\\rho h_t}{\\|h_t\\|} & \\text{if } \\|h_t\\| > \\rho \\\\ h_t & \\text{if } \\|h_t\\| \\le \\rho \\end{cases}$\nIt is worth mentioning that this converts the model into a non-linear RNN and cannot be parallelized in the same manner as the original model, making it much slower at pre-filling.\nMethod 3: Sliding Window by State Difference We can utilize the fact that the state ht can be written as a weighted sum (Eq. 11) to simulate a sliding window mechanism without re-processing from the start of the window at every step. Let w \u2208 N denote the window size and $h_t^{(r)} \\in \\mathbb{R}^{N \\times P}$ denote the hidden state when applying the model on the last w tokens at time step t. We can then"}, {"title": "compute  h (r) exactly as the difference between two states:", "content": "$h_t^{(r)} = \\sum_{i=t-r+1}^{t} \\alpha_{i:t} B_i x_i = \\sum_{i=1}^{t} \\alpha_{i:t} B_i x_i - \\sum_{i=1}^{t-r} \\alpha_{i:t} B_i x_i = h_t - \\alpha_{t-r+1:t}h_{t-r}$\nDuring streaming generation, we only have to maintain (ht\u22121, ht\u2212r, \u03b1t\u2212r+1:t), and advance each of them in parallel. However, directly computing \u03b1t:t\u2212r may suffer from instability due to floating- point imprecision. Therefore, we maintain $A_{t-r:t} = \\sum_{i=t-r}^t A_t$ instead, and re-compute $\\alpha_{t-r:t} = \\exp(-A_{t-r:t} \\exp(A))$ at every step, which incurs minimal computational cost.\nThis method applies to all RNNs that can be written as a weighted sum, which includes RWKV 5 and 6, RetNet, GLA, etc. It doubles the computation and memory cost for generation, but we believe that it is an acceptable trade-off because RNNs have a very low generation cost compared to transformer-based models and the context processing cost is unchanged."}, {"title": "TRAINING ON LONGER SEQUENCES", "content": "Based on the hypothesis that SC is caused by state overparameterization (described in Section 4.2.1), we can simply train on lengths that exceed the state capacity, which we conduct in this section.\nData Engineering To ensure that the data contains as much long-term structure as possible, we filter out sequences with less than 4K tokens. Buckman & Gelada (2024) have shown that this is critical for training effective long-context models. Although we train on sequences longer than 4K tokens, we do not use a higher length threshold because the above threshold already removes about 97.6% of the data in the original corpus. To train on longer sequences, we simply concatenate sequences and delimit them with a special EOS (End-of-Sequence) token.\nTruncated Backpropagation Through Time In the vanilla Mamba-2, the states are initialized to zeros for each data sample. Instead, we initialize the states as the final state of the previous sequence. This is equivalent to concatenating multiple sequences, but stopping the backpropagation of gradients at certain intervals. This technique has been shown to help extend the context length of RNNs (Yang et al., 2023) and alleviate the memory cost of caching activations for computing gradients. Based on Yang et al. (2023) and our preliminary tests, we use concatenate 12 sequences with this technique by default."}, {"title": "STATE CAPACITY", "content": "Based on the discussion in Section 4.2.1, SC occurs if and only if the training length contains less information than the state capacity. Thus, we can indirectly estimate state capacity by sweeping different training lengths for different state sizes, and observing when the state does not collapse. In this section, we empirically investigate the relationship between state capacity and state size.\nSpecifically, we conduct the same training as in Section 4.3.2. To determine whether a state has collapsed, we feed the \u201cnewlines\" prompt with 1M tokens to the model, and define collapse as the point where perplexity is more than 2x the maximum perplexity in its training length. We train multiple Mamba-2 with different state sizes and training lengths, and we regard the minimum training length at which SC does not occur as the state capacity."}, {"title": "STATE CAPACITY IN PASSKEY RETRIEVAL", "content": "The language modeling performance may not reflect the downstream capabilities well (Fu et al., 2024). Therefore, we also search for the state capacity on the passkey retrieval task. Similar to the previous section, we train with different lengths for different state sizes and identify the maximum context length where the model has an accuracy over 95%, which we regard as the state capacity in passkey retrieval. In this task, the noisy context is repetitive, thus, the amount of contextual information is largely independent of the context length, therefore, the capacity should grow roughly exponentially with the state size."}, {"title": "EXPERIMENTS", "content": "We briefly describe the experimental details for length extrapolation experiments.\nData We start from RedPajama-V2 (Computer, 2023), an open dataset with 30T tokens extracted from CommonCrawl, and we perform deduplication to ensure data quality. During evaluation, we sample documents longer than 16K tokens and concatenate them if it is not long enough.\nModels We experiment with seven model configurations with different state sizes to find the relationship between state capacity and size. For each of them, we perform an extensive search with training lengths up to 256K tokens. To save cost, we continue pre-train from three official checkpoints of Mamba-2 of size 130M, 370M, and 780M. They are all trained on 8K sequences. The other three model configurations (36M, 47M, and 85M) are trained from scratch. The detailed configurations are given in Appendix F.1.\nHyperparameters We use the WSD (Hu et al., 2024) with 10% decay steps. This scheduler is chosen because it is competitive with the commonly used cosine scheduler while allowing simple re- sumption from intermediate checkpoints, saving large amounts of computational resources. We report the result of the best checkpoint selection by validation on passkey retrieval. More hyperparameters are reported in the Appendix F."}, {"title": "RESULTS", "content": "TRAINING-FREE LENGTH GENERALIZATION\nFigure 9 reports the result of the training-free length generalization methods on Mamba-2 780M. We can see that while LongMamba can greatly improve the length generalizability of the model by more than 3x, it causes noticeably greater perplexity on shorter sequences and still inevitably exhibits SC. All our methods successfully suppress SC, allowing the model to generalize to more than 64K tokens, although State Normalization greatly underperforms other methods on shorter sequences. One explanation for this underperformance is that normalizing collapsing states changes the ratio of norms between heads, and this disrupts the learned mechanisms."}, {"title": "LENGTH GENERALIZATION BY TRAINING ON LONGER SEQUENCES", "content": "In Figure 10, we plot the language modeling perplexity as a function of token position for Mamba-2 130M and 370M with different training lengths. We can see that for each model size, there is a training length threshold, beyond which the model has much better length extrapolation, which supports our arguments discussed in Section 4.3.2."}, {"title": "STATE CAPACITY AS A FUNCTION OF STATE SIZE", "content": "Figure 12 shows the state capacity of Mamba-2 on language modeling and passkey retrieval. The rightmost data point in both plots corresponds to Mamba-2 370M. We have confirmed that 780M also exhibits SC at training lengths below 128K, but do not have enough resources to train the model beyond this length. The results establish a linear relationship TTrain = 5.172 S - 4.469 between the length TTrain at which SC stops occurring and the state size S.\nThe second plot of Figure 12 shows that the capacity of Mamba-2 on passkey retrieval is exponential concerning the state size. This is because the amount of information in the context does not increase with its length. In other words, we are storing a constant amount of information while the number of combinations of the state grows exponentially with the number of elements. Figure 11 shows the best checkpoint of Mamba-2 370M on passkey retrieval. The result is very promising because, to the best of our knowledge, no previous models with less than 1B model parameters have near-perfect accuracy at 128K tokens in this task."}, {"title": "CONCLUSION", "content": "This paper discovers and presents the first systemic study on state collapse (SC), a phenomenon in RNNs that causes length generalization failure. With an inspection of the activations and controlled experiments on Mamba-2, we conclude that this phenomenon is caused by an overparameterized state with excessive state capacity. Based on the analyses, we have proposed three training-free methods for mitigating SC up to 1M tokens. Then, we show that SC can be mitigated by training on context lengths that exceed the state capacity. With this insight, we empirically estimate the state capacity of Mamba-2 on language modeling and the passkey retrieval task. With some simple data engineering and state initialization tricks, we achieve much better performance with Mamba-2 on the passkey retrieval task than existing models. Our results indicate that Mamba-2 not only is highly efficient in handling long sequences but also has great performance potential."}, {"title": "LIMITATIONS", "content": "All models studied in this work can be seen as a specific case of linear attention models, whose recurrent state is decayed by an element-wise or scalar gate (they can be viewed as variants of Gated Linear Attention (Yang et al., 2023)). We have chosen to study these models because of their strong capabilities, yet, some conclusions may not be directly transferred to other variants of RNNs.\nOur continued pre-training approach for extending the context length of RNNs is rather expensive, some of the models require training with up to 50B tokens, which is 1/6 of their pre-training amount of data. Also, to ensure simplicity, controllability, and generality, we have not used more advanced techniques for training long-context models, such as upsampling longer data samples, better data order or format, using data with more long-distance dependencies, etc.\nThe passkey retrieval task that we used extensively in this study is very simple. Hence, high accuracy on this task may not reflect the capabilities of the model on real-world long-context tasks, because that requires more advanced capabilities such as high-resolution retrieval, reasoning, state-tracking, etc. The result is nonetheless promising because it indicates that the model can recall the correct information and further capabilities may be achieved by building on the recalled information.\nSC is somewhat prompt-dependent. While we found that for models with greatly overparameterized states, SC is highly consistent across different prompts, certain models with less overparameterization may exhibit SC on some prompts while successfully extrapolating indefinitely on others. Attributing SC to specific features of the prompt is a promising future research direction."}, {"title": "MAMBA-2 ARCHITECTURE", "content": "For completeness, we give a more detailed formulation of the Mamba-2 architecture here, although we recommend the readers refer to the original paper (Dao & Gu, 2024) or a detailed blog post by the authors. The model accepts a sequence of T token IDs as input $I = [i_1,\\dots, i_T] \\in \\mathbb{R}^T$, $i_t \\in \\{1,2,\\dots, V\\}$, where V denotes the vocabulary size. It performs next token prediction by predicting the probability distribution over the vocabulary as each time step, denoted as $P \\in \\mathbb{R}^{T \\times V}$. The model can be formulated as follows.\n$U^{(0)} = \\text{Embedin}(I) \\in \\mathbb{R}^{T \\times d}$\n$U^{(l)} = \\text{Mamba}^{(l)} (\\text{Norm} [U^{(l-1)}]) \\in \\mathbb{R}^{T \\times d}$\n$P = \\text{Embedout} (\\text{Norm} [U^{(L)}]) \\in \\mathbb{R}^{T \\times V}$\nwhere L denotes the number of layers, $l \\in \\{1,\\dots,\\dots,L\\}$ denotes the layer index, $U^{(l)} \\in \\mathbb{R}^{T \\times d}$ represents the input of the l-th layer, $U^{(0)}$ represents the input of the first layer. $\\text{Mamba}^{(l)}(\\cdot)$ denotes the l-th Mamba layer, $\\text{Embedin}(\\cdot)$ and $\\text{Embedout}(\\cdot)$ denote the input and output embedding layers, and $\\text{Norm}(\\cdot)$ denotes RMS normalization (Zhang & Sennrich, 2019). d denotes the number of dimensions of each token embedding. Similar to many other models, Mamba-2 ties the weight of the input and output embedding layers.\nEach Mamba layer consists of H \"heads\" that are computed in parallel. The result of which is summed together. The t-th token ($t \\in \\{1,\\dots,\\dots ,T\\}$) in a head is computed as follows.\n$y_t = \\text{Norm}(o_t \\odot u_t W_{\\text{gate}}) W_o \\in \\mathbb{R}^d$\n$o_t = C_t h_t + D o x_t \\in \\mathbb{R}^P$\n$h_t = h_{t-1} \\exp(-\\Delta_t \\exp(A)) + \\Delta_t B_t^\\top x_t \\in \\mathbb{R}^{N \\times P}$\n$C_t = \\sigma(\\text{Conv}(u_t W_c)) \\in \\mathbb{R}^N$\n$B_t = \\sigma(\\text{Conv}(u_t W_B)) \\in \\mathbb{R}^N$\n$x_t = \\sigma(\\text{Conv}(u_t W_x)) \\in \\mathbb{R}^P$\n$\\Delta_t = \\text{Softplus}(u_t W_\\Delta + b_\\Delta) \\in \\mathbb{R}$"}, {"title": "STATE SIZE", "content": "The authors of Mamba-2 always set P = 64, N = 128, and H = 2d/P. Thus, the state size of each Mamba-2 layer is HPN = 2dN = 256d. In transformer-based models, when using multi-headed attention, usually, the product of the head count H and head dimension P equals the hidden dimension d. Therefore, the KV cache of a transformer-based model is 2Td, which means that when using the same hidden dimension, the state of a Mamba-2 layer is equal in size to a KV cache of 128 tokens.\nCompared to many other recurrent models (e.g., the RWKV series (Peng et al., 2023a; 2024), GLA (Yang et al., 2023), and RetNet (Sun et al., 2023)), Mamba-2 does not have a state-less feed-forward network and has considerably more heads in each layer, making the state size much larger than other recurrent models. Compared to Mamba-1 (Gu & Dao, 2023), Mamba-1 uses N = 16, which means that the state size in Mamba-2 is 8 times larger than the state in a Mamba-1 model of roughly the model parameter count."}, {"title": "SHORT CONVOLUTION", "content": "The Conv() function in Mamba-2 is a one-dimensional convolutional layer applied to each channel separately. For i-th channel, it can be formulated as follows.\n$y_{t,i} = \\sum_{j=1}^{k} w_{j,i} x_{t-j,i} \\in \\mathbb{R}, i = 1,\\dots,n_c$\nk denotes the kernel size (set to 4 by default). i denotes the channel index, nc denotes the number of channels. $y_{t,i} \\in \\mathbb{R}$ denotes the i-th channel of the output vector at t-th time step. $x_{t,i}$ represents the i-th channel of the input vector at t-th time step. $w_{j,i} \\in \\mathbb{R}$ denotes the j-th value in the convolutional kernel for channel i.\nThis model component accepts the last 4 token embeddings at the input. Therefore, it also has a state that contains information about the context, which we refer to as the convolutional state. To be concrete, due to information propagation through the layers, the short convolutional layer is a function of the last 4L tokens. For the 370M model size, this length is 4 \u00d7 48 = 192. Therefore, we can reasonably assume that this component contains much less contextual information relative to"}, {"title": "PASSKEY RETRIEVAL INFERENCE PARAMETERS", "content": "Throughout the whole paper, we use greedy decoding, not just for reproducibility, but also because our preliminary results show that other decoding parameters give noticeably worse performance on passkey retrieval.\nWe use 32-bit floating point precision for both model parameters and activations during inference, to ensure that precision errors do not introduce noise to the result. We have conducted some preliminary evaluations with BF16 and FP16 and found that there are no noticeable differences with using FP16, but computing some activations, especially the \u2206t and \u03b1t with BF16 introduces an error around 1e-3. However, the explosion of channels in the states is consistently observed despite this precision error."}, {"title": "PASSKEY RETRIEVAL PROMPT", "content": "The prompt that we use for the passkey retrieval task is as follows, using 34847 as the passkey for example, which is adapted from existing works (Zhang et al., 2024a). We also evaluate with slight variations to the template in preliminary experiments but do not observe considerable differences in the results."}, {"title": "MAMBA-2 WITH MODIFIED  \u0394t ON PASSKEY RETRIEVAL", "content": "Ben-Kish et al. (2024) and GitHub user jzhang287 propose to improve Mamba's length generaliza- tion by reducing the value of At. Ben-Kish et al. (2024) propose a heuristic method for identifying which head to modify and how to modify \u2206t. However, their method requires task-dependent tweaking, so we do not consider comparing against it. jzhang28 propose to simply multiply At by a constant (they used 0.5). We apply this method and sweep different At for the best passkey retrieval performance, but they all result in worse performance than the original model across all context lengths."}, {"title": "PASSKEY RETRIEVAL EVALUATION WITH OTHER ARCHITECTURES", "content": "Here, we also evaluate RWKV-5, RWKV-6, and Mamba-1 (some popular and strong RNNs) on the passkey retrieval task. The result is reported in Figure 14, 15 and 16. We can see that SC is observed in Mamba-1, but it is less severe for RWKV-5 and RWKV-6. We hypothesize that this difference is a result of architectural differences and that the state size is smaller in RWKV-5 and RWKV-6."}, {"title": "PRE-TRAINED CHECKPOINTS", "content": "The pre-trained checkpoints used in our experiments are given in Table 1."}, {"title": "LENGTH EXTRAPOLATION TRAINING DETAILS", "content": "We perform a hyperparameter search on learning rates, sweeping {le - 5, 2e-5, 5e - 5, 1e - 4, 2e - 4, 5e-4, 1e-3}, selecting the best performing one by validation on passkey retrievals. Regarding the WSD scheduler, it warms up linearly for 1000 steps and decays linearly with 50K steps. This setup is inspired by the authors of WSD (Hu et al., 2024).\nOther hyperparameters are kept as similar to the original papers for Mamba-2 as possible. That means we use 0.5M tokens per batch, because we found this to give more stable results for continual pre-training instead of the 1M batch size from the original paper. Training is done mainly in BF16, with some activations in FP32 (in the same manner as the official implementation). The optimizer is AdamW, with a 0.1 weight decay. Moreover, we use 1.0 gradient clipping.\nAll experiments are run on A800 80G, some are run with multiple nodes, and others with multiple GPUs on a single node."}, {"title": "MODEL CONFIGURATIONS", "content": "For the models smaller than the 130M official checkpoint, we pre-train from scratch using the configurations reported in Table 2. We try to follow the same depth-to-width ratio found in the official checkpoints, although the ratio is not entirely consistent in those checkpoints. Hyperparameters not mentioned are kept the same as the 130M checkpoint."}, {"title": "STATE STATISTICS OVER CONTEXT LENGTH", "content": "Here, we provide a more detailed result on the inspection of SC over time."}, {"title": "THE NEWLINES PROMPT", "content": "In this paper, we collect the statistics of the state computed on a \u201cnewlines\" prompt, a prompt where every token is the newline token, as shown below.\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...\nHowever, we again emphasize that SC is observed on prompts extracted from the pre-training corpus, the passkey retrieval task, or other randomly generated sequences. We have chosen the \u201cnewlines\" prompt because the samples from the pre-training corpus are too short, and this prompt produces the most consistent and smooth layer statistics.\nFigure 17 shows the hidden state of the recurrent mechanism described in Eq. 3. Additionally, Bt, Ct, and xt in Mamba-2 are generated with a short channel-wise convolutional layer with a kernel size of 4:\n$B_t = \\sigma(\\text{Conv}[u_t W_B])$\n$C_t = \\sigma(\\text{Conv}[u_t W_c])$\n$x_t = \\sigma(\\text{Conv}[u_t W_x])$"}]}