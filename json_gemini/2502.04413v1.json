{"title": "MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot", "authors": ["Xuejiao Zhao", "Su-Yin Yang", "Siyan Liu", "Chunyan Miao"], "abstract": "Retrieval-augmented generation (RAG) is a well-suited technique for retrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a key module of the healthcare copilot, helping reduce misdiagnosis for healthcare practitioners and patients. However, the diagnostic accuracy and specificity of existing heuristic-based RAG models used in the medical domain are inadequate, particularly for diseases with similar manifestations. This paper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited reasoning for the medical domain that retrieves diagnosis and treatment recommendations based on manifestations. MedRAG systematically constructs a comprehensive four-tier hierarchical diagnostic KG encompassing critical diagnostic differences of various diseases. These differences are dynamically integrated with similar EHRs retrieved from an EHR database, and reasoned within a large language model. This process enables more accurate and specific decision support, while also proactively providing follow-up questions to enhance personalized medical decision-making. MedRAG is evaluated on both a public dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD) collected from Tan Tock Seng Hospital, and its performance is compared against various existing RAG methods. Experimental results show that, leveraging the information integration and relational abilities of the KG, our MedRAG provides more specific diagnostic insights and outperforms state-of-the-art models in reducing misdiagnosis rates.", "sections": [{"title": "1 Introduction", "content": "Diagnostic errors cause significant harm to healthcare systems worldwide. In the United States, approximately 795,000 individuals each year suffer permanent disability or death due to misdiagnosis of dangerous diseases. These errors are predominantly attributed to cognitive biases and judgmental mistakes [9, 36, 57]. \"Healthcare Copilot\" is a medical AI assistant designed to provide diagnostic decision support, mitigating biases and increasing efficiency for healthcare practitioners, while also empowering patients and improving overall decision-making [1, 2, 28, 43, 44]. We conducted interviews to gather requirements and suggestions from users of the healthcare copilot. The results showed that one of the most important and challenging tasks for a healthcare copilot is to provide an accurate diagnosis based I on patient manifestations 1 1, followed by offering appropriate treatment plans and medication recommendations based on the diagnosis. In addition, when patient information is insufficient or the diagnosis is ambiguous, the healthcare copilot should proactively offer precise follow-up questions to enhance the decision-making process [3, 25, 38, 44, 65].\nRetrieval-augmented generation (RAG) offers an advanced approach by utilizing domain-specific, private datasets to address user queries without the need for additional model training [12, 17, 29]. This approach is well-suited for retrieving information from privacy-sensitive Electronic Health Records (EHRs), and helps healthcare professionals to reduce the risk of misdiagnosis as a healthcare copilot [22, 59]. The existing medical RAG and LLMs fine-tuned on medical datasets often rely on heuristic-based approaches, leading to incorrect or vague outputs, particularly when diseases share similar manifestations, making differentiation difficult [18, 24, 31, 58, 62, 66] as shown in Figure 1(a). To address this, we introduce MedRAG, a framework that combines RAG with a comprehensive diagnostic knowledge graph, enabling more accurate reasoning and tailored treatment recommendations by grounding predictions in structured, inferable medical data [21, 26, 34, 53]. This approach significantly enhances the reasoning ability of RAG, enabling it not only to identify subtle diagnostic differences but also to proactively infer relevant follow-up questions, further clarifying ambiguous patient information, as shown in Figure 1(b).\nSpecifically, a diagnostic knowledge graph (KG) with a four-tier hierarchical structure is constructed systematically through advanced techniques, including disease clustering, hierarchical aggregation and large language model augmentation. While medical ontologies like UMLS could be considered, their ambiguous class"}, {"title": "2 Related Works", "content": "Large Language Models (LLMs) have been increasingly applied to healthcare tasks such as EHR analysis, clinical note generation, virtual medical assistant, and clinical decision support [18, 22, 55, 62, 68]. Although LLMs fine-tuned on medical datasets can han-dle large amounts of unstructured clinical information, most of these models are heuristic-based, with limitations such as gen-erating incorrect or vague information and struggling to handle complex patient cases [24, 62]. To address this, integrating external information sources becomes essential to improve their contextual accuracy. We adopt a Retrieval-Augmented Generation (RAG) ap-proach [29]. RAG enhances LLMs by incorporating retrieved text passages from external sources such as electronic health records,"}, {"title": "3 Preliminaries", "content": "Definition 3.1 (Diagnostic Knowledge Graph). Given an EHR database D and an LLM Ma, our target is to construct a four-tier hierarchical diagnostic knowledge graph G. A multi-hop path, from the top level to the bottom level of G is represented as (EL1 \nrm\nEL2 EL3 Fm\u2192 EL\u2084). EL3 is the set of all diseases (i.e. potential diagnoses) names extracted from D, EL2 represents the set of sub-categories of EL3, and EL1 is the set of broader categories of EL2. Each elij is a disease name or a category name and elij \u2208 ELi EL1 and EL2 are generated by hierarchical aggregation in Section 4.1.1, they indicate the diseases with similar manifestations. rs is an \"is_a\" relation, indicating a hierarchical or subordinate relation-ship. rm is a \"has_manifestation_of\" relation between diseases and their manifestations. EL4 contains two subtypes: EL4a, representing disease-specific features augmented by the LLM Ma, and EL4d, representing features decomposed from the manifestations extracted from the EHR database D.\nDefinition 3.2 (Diagnostic Differences KG Searching). Given a G and the input patient's manifestations q, let eL2, \u2208 EL2 denote a certain subcategory identified through the method described in Section 4.2.3 determined from q. The target is to extract the diagnostic differences KG K, related to eL25, from G.\nDefinition 3.3 (RAG). We define a typical retrieval-augmented generation approach for generating diagnostic reports in two phases: algorithm R for the retrieval phase and LLM Mg for the generative phase. A prompt Pnaive is used to guide Mg to generate the final report. Given a q, D and embedding model &, R retrieves top-k relevant documents dr, and then Mg generates answer A with q, dr and prompt Pnaive as shown in Equation 1 and 2:\ndr = R(q, D, &), (1)\nA = Mg(q, dr, Pnaive). (2)"}, {"title": "4 Methods", "content": "In this section, we elaborate on the details of our proposed MedRAG, and the overall framework is illustrated in Figure 2. MedRAG includes five modules:\n\u2022 Input: The input to MedRAG is the description of patient manifestations, which can be either structured EHR or un-structured text descriptions.\n\u2022 Output: The output of MedRAG includes the diagnoses, treatment recommendations, medication guidance and follow-up questions when necessary.\n\u2022 Diagnostic Knowledge Graph Construction: This mod-ule constructs a four-tier hierarchical diagnostic knowledge graph systematically. First, potential diagnoses and corre-sponding manifestations are extracted from an EHR data-base to form a four-tier disease KG through clustering and hierarchical aggregation. Then, an LLM is used to augment the graph with critical diagnostic differences, transforming it into a diagnostic KG.\n\u2022 Diagnostic Differences KG Searching: This module iden-tifies key diagnostic differences by decomposing patient manifestations into clinical features, such as symptoms and locations, through medical chunking. Then, the extracted features are embedded and matched with relevant diag-nostic differences via multi-level matching and upward traversal within the diagnostic KG.\n\u2022 KG-elicited Reasoning RAG: This module comprises a document retriever and a KG-elicited reasoning LLM engine."}, {"title": "4.1 Diagnostic Knowledge Graph Construction", "content": "To enhance the reasoning capabilities and fill the knowledge gaps of the RAG, we propose constructing a diagnostic knowledge graph G tailored to the medical domain of a specific EHR database. The construction of the diagnostic knowledge graph draws inspiration from the hierarchical structure of the World Health Organization's International Classification of Diseases, 11th Edition (ICD-11) [41] 2.\n4.1.1 Disease Knowledge Graph Construction. The forms and rep-resentations of the diseases in an EHR database are diverse, we first unify the set of original disease descriptions EL3raw by disease clustering to EL3. The most common disease name within each cluster is regarded as the final disease name and is assigns to all", "sections": [{"title": "4.1.2 Knowledge Graph Manifestation Augmentation.", "content": "The knowledge in GD only contains information from D, which is insuffi-cient to accurately diagnose all diseases, particularly when dis-tinguishing between diseases with similar clinical manifestations. Therefore, the integration of external knowledge is essential. To complement the diagnostic knowledge graph with essential knowl-edge that is not present in D, we augment external knowledge EL4 to GD that aids in distinguishing diseases with similar man-ifestations. We traverse all disease eL3, and employ a prompt pa specially tailored for searching and generating the nuances of the diseases on an LLM denoted by Ma. As shown in Equation 5, each generated diagnostic key difference node eL4aij is then connected to its corresponding eL3, with relationship rm. Thus we obtain \nrm\na chain EL3 EL4a. For example, we generate a manifestation and relation to disease node lumbarspondylosis and form a chain: <lumbar_spondylosis, has_symptom, stiffness_or_pain_in_the_l-ower_back >.\n{eL4aiji {eL3;}=1\nMa (PaseL3)\nEL4 = EL4dUEL4a,\ni=1,j=1\nG = GD UEL3 {EL3 U EL4}=1"}]}, {"title": "4.2 Diagnostic Differences KG Searching", "content": "4.2.1 Decomposition of Manifestations. Given q as a query, which is a description of the patient's manifestations, we perform sentence trunking on q to decompose the manifestation into more detailed features, denoted as f1, f2,..., fn \u2208 q. We define a mapping function to describe the process, shown in Equation 8:\nq{f1, f2,..., fn}.\n4.2.2 Clinical Features Matching. Given a q, we compute the se-mantic similarity score sim between fi and eL4d\u2081, shown in Equa-tion 9:\nsimij = S(fi, eL4d;, &),"}, {"title": "4.3 KG-elicited Reasoning RAG", "content": "KG-elicited Reasoning RAG is the core component of MedRAG, we use an LLM to generate diagnoses, personalized treatment plans, and medication suggestions. Additionally, the system proactively suggests follow-up questions for doctors to clarify missing or ambiguous patient information. As shown in Equation 17, MedRAG utilizes diagnostic differences KG augmented by LLM and a tailored prompt ps to elicit the reasoning capabilities of LLM.\nA = Mg(q, dr, K, ps)"}, {"title": "5 Experiments", "content": "We evaluate MedRAG framework using two distinct datasets: one public and one private. The public dataset demonstrates the model's general applicability, while the private dataset, focused on chronic pain patients, enables a more thorough evaluation of MedRAG's diagnostic capabilities in real-world clinical settings.\nThe public dataset, DDXPlus [14], is a large-scale synthesized EHR dataset, recognized for its complex and diverse medical cases. It includes comprehensive patient data such as socio-demographic information, underlying diseases, symptoms, and antecedents, ad-dressing the symptom-related data gap in common EHR datasets like MIMIC [14]. Many studies have employed DDXPlus to bench-mark models in medical reasoning and diagnosis [6, 32, 50, 61]. DDXPlus contains 49 different diagnoses with over 1.3 million patients, each of whom has approximately 10 symptoms and 3 an-tecedents on average. We ultimately utilized a maximum balanced sub-dataset comprising 13,230 patients' EHRs.\nThe private dataset is the Chronic Pain Diagnostic Dataset (CPDD), a specialized EHR dataset focused on chronic pain patients. This dataset is collected from Tan Tock Seng Hospital, it comprises 551 patients with 33 distinct diagnoses. CPDD offers manifestations-specific chronic pain patient data, making it an invaluable resource for testing MedRAG's diagnostic capabilities in clinical settings.\nFor more details on the partitioning, preprocessing, and experi-mental setup, please refer to the Appendix."}, {"title": "5.2 Baselines", "content": "In order to explore the performance of the MedRAG, we compare the MedRAG results against six other models, including Naive RAG with COT [56], FL-RAG [42], FS-RAG [51], FLARE [24], DRAGIN [49] and SR-RAG [54]. More detailed introduction to each baseline model is provided in the appendix."}, {"title": "6 Experimental Results", "content": "In this section, we present the results of the experiments to answer the following research questions:\n\u2022 RQ1: Does MedRAG outperform the SOTA RAG methods using the same datasets?\n\u2022 RQ2: Does MedRAG demonstrate compatibility, generaliz-ability and adaptability across different backbone LLMs?\n\u2022 RQ3: Does MedRAG's proactive diagnostic questioning mechanism provide users with impactful, relevant follow-up questions to enhance diagnostic performance?\n\u2022 RQ4: Is the MedRAG system we designed effective? What is the impact of each module on its overall performance, and how do specific KG components contribute to MedRAG?"}, {"title": "6.1 Quantitative Comparison (RQ1)", "content": "Our experiments evaluate MedRAG against six different SOTA RAG models on 2 two datasets. We report the results using: 1) Accu-racy, defined as the number of correct diagnoses out of the total diagnoses; 2) Specificity, which uses L1, L2, and L3 to represent different diagnostic granularity levels. As outlined in Section 3 (Definition 3.1), Li refers to the MedRAG select potential diagnoses from ELi. This metric evaluates the model's specificity and its abil-ity to differentiate between similar diseases across varying levels of diagnostic granularity; 3) Text Generation Metrics, which uses BERTScore, BLEU, ROUGE, METEOR and subjective evaluatio from doctor to evaluate generated reports.\nThe result is shown in Table 1, MedRAG achieved the best or second-best (with only one exception) performance across multi-ple metrics in all datasets. Accuracy on the L3 metric is the best indicator of MedRAG's performance, as higher specificity increases diagnostic difficulty. MedRAG outperformed the second-best scores on the CPDD and DDXPlus datasets by 11.32% and 1.23%."}, {"title": "6.2 Compatibility, Generalizability and Adaptability (RQ2)", "content": "The results in Table 2 demonstrate the performance of incorporating KG-elicited reasoning to various backbone LLMs, including both open-source and closed-source models. The results demonstrate that the inclusion of KG-elicited reasoning significantly enhances diagnostic accuracy across L1, L2, and L3 for all backbone LLMs, compared to models without its use. For example, Mixtral-8x7B shows a significant L3 improvement from 22.34% to 63.46%, demonstrating the effectiveness of our proposed KG-elicited reasoning, particularly in smaller models.\nAdditionally, most RAG models designed for simpler QA tasks do not perform as well in the more complex medical domain, leading to longer contextual and prompt. These models are often optimized for generating short and straightforward answers, which limits their effectiveness in handling intricate medical queries. We observe models that have a simpler mechanism in the query-organizing phase perform better than part of more sophisticated ones. Except for our MedRAG, models like SR-RAG and FL-RAG also secured several second-best performances. Even the Chain-of-Thought model, which lacks improvements in the retriever or generator compo-nents, outperformed some of the other SOTA models in complex medical tasks.\nFor report generation, we conducted both objective evaluation using BERTScore, BLEU, ROUGE and METEOR, as well as subjective evaluation based on Mini-CEX [37] criteria by LLM[47] with validation by doctors. Performance results are shown in the Appendix."}, {"title": "6.3 Proactive Diagnostic Questioning (RQ3)", "content": "The results in Table 3 show the impact of following MedRAG's op-timized instructive questions and obtaining corresponding patient responses on diagnostic accuracy.\nAs more detailed information is gathered through these targeted questions, the L3 accuracy progressively improves. Initially, with no specific patient information obtained through this questioning process, the L3 accuracy is 52.83%, representing MedRAG making a diagnosis with other information with very few manifestations. As the doctor collects more critical details about disease representa-tion, covering from 33.3% to 100% of the key manifestations, the L3 score rises from 55.10% to 66.04% and other levels' metrics follow the same trend. This demonstrates the significant effectiveness of MedRAG's proactive diagnostic questioning mechanism, validat-ing its capability to provide doctors with impactful questions that not only enhance diagnostic performance but also improve the efficiency of the medical consultation process."}, {"title": "6.4 Ablation Study (RQ4)", "content": "We perform ablation studies to evaluate the effectiveness of dif-ferent components in MedRAG and present the result in Figure 3. Specifically, we assess the retriever R and KG-elicited reasoning module G under three configurations: \u201crandom\u201d, \u201cwith\u201d and \u201cwithout\". In the \"random\" setting for R, we choose documents from the entire EHR database randomly. The \"without\" of the retriever refers to the scenario where no documents are passed to Mg. The \"with\" setting of the retriever means to pass the top-k relevant documents to Mg. For the KG-elicited reasoning module, the \"random\" config-uration denotes randomly selecting subcategory eL2, and collecting corresponding K accordingly. The \"without\u201d is the scenario where no diagnostic differences KG are passed to Mg. Configuration \u201cwith\u201d means to pass correct K by the eL2s to Mg.\nAs shown in Figure 3, both the retriever and KG-elicited reason-ing module significantly enhance performance across all specificity levels. the best outcomes are achieved when RAG and KG compo-nents are combined and aligned, especially for granular diagnosis tasks that demand high specificity. Notably, randomly selected docu-ments performed better than no documents at all, this phenomenon was explored in detail by [8]. We also observed a performance de-cline in the lower-granularity levels of L1 and L2 when transitioning from random to no knowledge from KG when random documents"}, {"title": "7 Conclusion", "content": "In conclusion, MedRAG significantly improves diagnostic accuracy and specificity in the medical domain by integrating KG-elicited reasoning with RAG models. By systematically retrieving and rea-soning over EHRs and dynamically incorporating critical diagnostic differences KG, MedRAG offers more precise diagnosis and per-sonalized treatment recommendations. Additionally, MedRAG's proactive diagnostic questioning mechanism proves highly effec-tive, and shows potential capacity to provide doctors and patients with impactful questions that enhance diagnostic performance and improve consultation efficiency. The evaluation of public and pri-vate datasets demonstrates that MedRAG outperforms state-of-the-art RAG models, particularly in reducing misdiagnosis rates for diseases with similar manifestations, showcasing its potential as a key module in healthcare copilot.\nFor future work, we aim to further enhance MedRAG's capabil-ities by incorporating multimodal data, such as medical imaging (e.g., MRI), physiological signal data (e.g., ECG), and blood test data to improve diagnostic accuracy and broaden its applicability to a wider range of medical conditions. Additionally, we plan to deploy MedRAG within our healthcare copilot systems (The user interface is shown in the Appendix) for real-world hospital testing, ensuring its effectiveness in clinical settings. Furthermore, to improve us-ability for doctors, we will integrate a speech recognition module into the system. This feature will passively listen to conversations between doctors and patients during consultations without causing disruptions. Based on the dialogue content, it will provide real-time suggestions for follow-up questions and relevant explanations, as-sisting doctors in conducting more comprehensive and efficient patient assessments."}, {"title": "Appendix", "content": "This appendix is organized as follows:\n\u2022 Section A includes variables and definitions in the paper.\n\u2022 Section B demonstrates the detailed data preprocessing steps and experimental setup, ensuring transparency and reproducibility.\n\u2022 Section C describes the details of the baseline models in the experiments.\n\u2022 Section D presents intermediate results from experiments.\n\u2022 Section E shows the evaluation of report generation.\n\u2022 Section F shows the ablation study on KG components.\n\u2022 Section G shows the user interface of the healthcare copilot."}, {"title": "A Variables and Definitions", "content": "The variables used throughout this paper and their definitions are provided in Table A3."}, {"title": "B Data Preprocessing and Experimental Setup", "content": "B.1 Settings for Datasets\n\u2022 CPDD We split the data set into a 9:1 ratio for the training set (to be retrieved) and test set. Since the dataset was collected from multiple doctors, the diagnosis descriptions are not standardized. Part of the diagnosis is presented as a type of pain instead of a specific disease. When calculating the accuracy of these pain-type diagnoses, if the predicted result is a disease associated with that type of pain, it will be considered a correct prediction.\n\u2022 DDXPlus We directly use the training set and test set in a split dataset in the ratio of 8:1:1(validation set). Due to the massive size of the dataset with over a million synthesized patients' records, which is too large for the scale of our task, we first fixed the number of samples in the test set to 30, which corresponds to the fewest pathology. For the other pathology with more samples, we randomly select 30 samples to form the whole test set. In the training set, we randomly pick 240 samples for each pathology to retrieve. This approach can ensure we get a maximum balanced sub-dataset containing 13230 patients' EHR in total. The random seed is set to 42."}, {"title": "B.2 Setup for Proactive Diagnostic Questioning Mechanism", "content": "We mask certain existing manifestations of a patient to simulate sce-narios where they are missing. MedRAG then generates follow-up questions based on the remaining information. If MedRAG identi-fies the removed manifestations during questioning, they are added back to the patient's record, and diagnostic reasoning is repeated to evaluate the improvement in diagnostic accuracy.\nWe begin by selecting all matching manifestation nodes EL4ds and ranking them according to their discriminability scores. A pro-portion r of the nodes with the highest discriminability scores is then removed, simulating the scenario where certain key patient features are missing or unclear, shown in Equation 18. After re-moving, we match the removed nodes Edel with each fi, if the\nL4ds"}, {"title": "B.3 Prompt Engineering", "content": "The prompt configuration for disease clustering is shown below: Cluster the following diseases into multiple categories based on the similarity of their manifestations, affected locations, and other characteristics. Diseases: {}.\nThe prompt configuration for the generative model in MedRAG is illustrated in Figure A1. The first block provides instructions as the system prompt. The second block displays the answer template. In the final block, relevant information including the patient's man-ifestations q, retrieved documents dr, and diagnostic differences K, is populated in this field."}, {"title": "C Baseline Details", "content": "We conducted experiments on six baseline models and compared them with MedRAG.\n\u2022 Naive RAG + COT [56] We apply the chain-of-thought (COT) prompting with a naive RAG model, which only retrieves documents without additional enhancements.\n\u2022 FL-RAG [42] FL-RAG is a multi-round retrieval method that triggers the retrieval module every n tokens.\n\u2022 FS-RAG [51] FS-RAG is an interleaving retrieval method that improves multi-round question answering by alternat-ing between COT reasoning and document retrieval.\n\u2022 FLARE [24] FLARE is an active RAG method that im-proves knowledge-intensive tasks by retrieving relevant documents when the model encounters uncertain tokens.\n\u2022 DRAGIN [49] DRAGIN is a dynamic retrieval method that enhances language models by retrieving relevant docu-ments based on real-time information needs during gener-ation, triggered by token uncertainty.\n\u2022 SR-RAG [54] In SR-RAG, relevant passages are retrieved from an external corpus based on the initial query and then incorporated into the input of the language model"}, {"title": "D Intermediate Results", "content": "D.1 Disease Clustering Result\nThe result of disease clustering in CPDD is Shown in Figure A2. Through the disease clustering operation, we group different forms and representations of the same disease in the EHR database to-gether, assigning a topic to each cluster. This process unifies the representation of diseases, ensuring consistency and comparability. Additionally, it provides a unified foundation for subsequent disease knowledge graph construction and augmentation."}, {"title": "D.2 Example of Diagnostic Differences Knowledge Graph", "content": "While lumbar canal stenosis and sciatica share some similar features, the critical distinguishing factor lies in the response to sitting. In lumbar canal stenosis, features are typically alleviated when sitting, whereas in sciatica, sitting tends to exacerbate the discomfort. The augmented disease features are shown in Figure A3."}, {"title": "E Report Generation Evaluation", "content": "To evaluate the report generation of MedRAG, we conducted both objective and subjective evaluations on the generated reports of CCPD, since the DDXPlus dataset does not contain report data.\n\u2022 Objectice Evaluation: We use BERTScore, BLEU, ROUGE, METEOR as metrics. The result is shown in Table A1.\n\u2022 Subjective Evaluation: Reports were generated for 10 randomly selected patients using SRRAG and MedRAG. The results were scored on 4 Mini-CEX criteria (Scale 1-9) [37], and assessed by GPT-40 [47], with validation by doctors. The results were: SRRAG 277, MedRAG 290 out of 360."}, {"title": "F Ablation Study on KG Components", "content": "In order to evaluate how different components in diagnostic dif-ferences KG, we conducted extra ablation study focusing on key components. Specifically, we examined the effects of diagnostic key difference nodes, augmented feature nodes, patient clinical feature matching, and the augmentation of diagnostic differences.\nResults in Table A2 show that KG components like diagnostic key difference nodes, augmented feature nodes, the patient clin-ical feature matching and the augmentation of diagnostic differ-ences contribute to MedRAG's overall effectiveness significantly. Moreover, the hierarchical structure of the constructed diagnostic differences KG directly impacts the experimental results as well."}, {"title": "GUser Interface (UI)", "content": "This section introduces how our MedRAG can be integrated into the user interface design of the healthcare copilot system. The healthcare copilot offers three modes of interaction, as shown in Figure A4.\n\u2022 Consultation Mode: By monitoring the consultation dia-logue between the doctor and patient, the system extracts patient manifestations in real-time and provides diagnostic suggestions along with proactive questioning recommen-dations to guide the consultation.\n\u2022 EHR Mode: By uploading the patient's EHR to the health-care copilot system, this system automatically extracts the relevant patient manifestations for diagnostic purposes.\n\u2022 Typewritting Mode: The user can manually input the patient's manifestations into the system.\nOn the results page shown in Figure A5, the output of the health-care copilot system include diagnoses, instructive follow-up ques-tions, physiotherapy treatments, and medication treatments. This UI integrates the most essential functions derived from extensive interviews we conducted with numerous healthcare practitioners. It ensures that the healthcare copilot system meets the practical needs of healthcare professionals, ultimately enhancing the overall quality of care."}]}