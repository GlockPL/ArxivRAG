{"title": "FonTS: Text Rendering with Typography and Style Controls", "authors": ["Wenda Shi", "Dengming Zhang", "Jiaming Liu", "Yiren Song", "Xingxing Zou"], "abstract": "Visual text images are prevalent in various applications, requiring careful font selection and typographic choices. Recent advances in Diffusion Transformer (DiT)-based text-to-image (T2I) models show promise in automating these processes. However, these methods still face challenges such as inconsistent fonts, style variation, and limited fine-grained control, particularly at the word level. This paper proposes a two-stage DiT-based pipeline to address these issues by enhancing controllability over typography and style in text rendering. We introduce Typography Control (TC) finetuning, an efficient parameter fine-tuning method, and enclosing typography control tokens (ETC-tokens), which enable precise word-level application of typographic features. To further enhance style control, we present a Style Control Adapter (SCA) that injects style information through image inputs independent of text prompts. Through comprehensive experiments, we demonstrate the effectiveness of our approach in achieving superior word-level typographic control, font consistency, and style consistency in Basic and Artistic Text Rendering (BTR and ATR) tasks. Our results mark a significant advancement in the precision and adaptability of T2I models, presenting new possibilities for creative applications and design-oriented tasks.", "sections": [{"title": "1. Introduction", "content": "Visual text images are ubiquitous in daily life and hold significant commercial value in advertising, branding, and marketing [3, 9]. However, the design process for visual text is complex and time-consuming. Designers must carefully select appropriate fonts, use typographic elements like italics, and create artistic styles that are aesthetically pleasing and coherent. Recent advances in diffusion models [27, 31] demonstrate promising potential for creating visual contents in design, thereby attracting substantial attention. Concurrently, real-world applications raise increasing demands for controllability over the generated content.\nPrevious efforts have mainly focused on improving control over the accuracy of scene text rendering [8, 9, 38, 44]. With the development of DiT-based T2I models, e.g. SD3 [12] and Flux.1 [1], the accuracy of text rendering has seen significant improvements. Beyond content accuracy, Glyph-ByT5 [20] introduced a new text encoder pre-trained using contrastive learning, enabling various font types in text rendering. Textdiffuser-2 [9] trained both two language models and the whole diffusion model to acquire layout planing capabilities. While these methods [9, 20] have implemented control at the paragraph-level (more than 10 words), they have not yet realized word-level control. Moreover, prior text rendering methods often overlook the artistic aspects of text rendering [9]. Recent DiT models [1, 12] have demonstrated promising capabilities in artistic text rendering, yet they face challenges such as semantic confusion and style inconsistency.\nTo expand the boundaries of existing methods and provide enhanced control on image generation, this paper identifies three essential requirements that text rendering methods: 1) control of fonts and word-level attributes in Basic Text Rendering (BTR); 2) consistency in style control in Artistic Text Rendering (ATR); 3) preservation of Scene Text Rendering (STR) capabilities without negative impact.\nTo this end, we propose a two-stage DiT-based pipeline for text rendering with typography and style controls. To achieve typography control, we introduce Typography Control (TC)-finetuning, an efficient parameter fine-tuning method, alongside enclosing typography control tokens (ETC-tokens) and paired typography control dataset (TC-dataset). Our results demonstrate that the model not only learns typographic elements but also applies specific typographic features at precise word locations. To ensure content accuracy while enhancing style control, we introduces a style control adapter (SCA) that injects style information via images, independent of the text content in the prompt. For training the SCA, we collected a dataset of approximately 600k image-text pairs with high aesthetic scores."}, {"title": "2. Related Work", "content": "Scene Text Rendering. Despite progress in diffusion models [27, 31], high-quality scene text rendering remains a challenge. To address this, one line of research [8, 9, 37, 44] focuses on explicitly controlling the position and content of the text being rendered. For instance, methods such as GlyphControl [44] and AnyText [37] leverage glyph images containing multiple text lines as priors to guide diffusion models to produce accurate accurate text, relying on ControlNet [46]. Similarly, TextDiffuser [8, 9] employs character-level segmentation masks as control conditions in scene text rendering. Another line of works [20, 21] fine-tune the character-aware ByT5 encoder [19] using paired glyph-text datasets, improving the ability to render accurate text in various real-world images.\nWhile these methods improve text content accuracy in scene text rendering, they have led to a trade-off in model performance on stylization, limiting text style diversity and complicating the rendering of artistic text images [8].\nArtistic Text Rendering. Early research focused on font creation by transferring textures from existing characters, employing stroke-based methods [5, 33] and patch-based techniques [40-42]. The advent of Generative Adversarial Networks (GANs) improved the synthesis of new glyphs from limited examples and the separation of style and content in font design, enhancing flexibility and realism [2, 14, 15, 22, 35, 43]. Innovations with diffusion models [25, 36, 39] have enabled diverse text image stylization and semantic typography, resulting in visually appealing designs that retain readability. However, despite recent DiT"}, {"title": "3. Approach", "content": "Our proposed parameter-efficient fine-tuning method with enclosing typography control tokens (ETC-tokens), shown in Figure 2 (a). In addition, we detail the training of style control adapters for DiT blocks of the transformer backbone, depicted in Figure 2 (b)."}, {"title": "3.1. Typography Control Learning", "content": "Preliminaries of Rectified Flow DiT. To avoid the computationally expensive process of ordinary differential equation (ODE), diffusion transformers such as [1, 12] directly regress a vector field $u_t$ that generates a probability path between noise distribution $p_1$ and data distribution $p_0$. To construct such a vector field $u_t$, [12] consider a forward process that corresponds to a probability path $p_t$ transitioning from $p_0$ to $p_1 = \\mathcal{N}(0,I)$. This can be represented as $z_t = a_tx_0 + b_t\\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, I)$. With the conditions $a_0 = 1, b_0 = 0, a_1 = 0$ and $b_1 = 1$, the marginals $p_t(z_t) = \\mathbb{E}_{\\epsilon\\sim \\mathcal{N}(0,I)} p_t(z_t|\\epsilon)$ align with data and noise distribution. Referring to [12, 18], the marginal vector field $u_t$ can generate the marginal probability paths $p_t$, using the"}, {"title": "3.2. Style Control Adapters", "content": "Decoupled Joint Attention. The joint attention here refers to the attention in MM-DiT blocks of SD3 [12] and Flux [1]. Given the text features $Ctxt$ and input of joint attention $zt$, the output of joint attention $z'$ can be defined as:\nz' = Attention(Q, K, V) = Softmax(\\frac{QKT}{\\sqrt{d}})V, (3)\nwhere $Q = z_cW_q, K = z_cW_k, V = z_cW_v$ are the query, key, and values matrices of the attention operation respectively, $Z_c = concat(z_t, Ctxt)$, and $W_q, W_k, W_v$ are the weight matrices of the trainable layers.\nIn order to better decouple style and content, we additionally introduce a decoupled joint attention mechanism (DJA). Inspired by [7, 24, 45], we add DJA at the joint attention layers for text features $Ctxt$ and image features $Cimg$ are separate. To be specific, we add new joint attention layers in the original MM-DiT and Single-DiT blocks to insert image features. Given the image features $Cimg$, the output of new joint attention $z''$ is as follows:\nz'' = Attention(Q', K', V') = Softmax(\\frac{Q'K'^T}{\\sqrt{d}})V', (4)\nwhere, $Q' = z_tW_q, K' = CimgW_k$ and $V' = CimgW_v$ are the query, key, and values matrices from the image features. $W_q'$ and $W_v'$ are the corresponding weight matrices. Consequently, we only need to add two parameters $W_q', W_v'$ for each decoupled joint attention layer. Then, we simply"}, {"title": "4. Experiments", "content": "Text Rendering Benchmark. To assess the text rendering capabilities with word-level typography and style controls, we extend the existing scene text rendering benchmark [8] by introducing new benchmarks for basic text and artistic text rendering. BTR-bench. To evaluate word-level typography controls in basic text rendering, we introduce the Basic Text Rendering benchmark (BTR-bench). BTR-bench includes 100 prompts of different fonts and typographic attributes. For each text prompt, typographic attributes are randomly applied to three positions within the text to assess the model's ability to render specific typographic attributes on individual words, while font attributes are applied to the entire text in the image. ATR-bench. To evaluate artistic text rendering, we introduce the Artistic Text Rendering benchmark (ATR-bench). Based on the single-letter and multi-letter classification in [36], we categorize the content into single-word and multi-word groups. Drawing on the style prompts from the GenerativeFont benchmark [25], we generate artistic individual letters and words using Flux [1]. These generated artistic letters and words are used for single-word and multi-word text rendering, respectively.\nStyle Control Dataset. Style control training consists of two phases, each utilizing different datasets. The first phase involves pretraining with a dataset of general image-text pairs. The second phase is fine-tuning, using a dataset that includes artistic text images and paired descriptions. For pretraining, we assembled a dataset called SC-general, which includes approximately 580k general image-text pairs with high aesthetic scores. These images were sourced from open-source datasets [11, 34]. For the fine-tuning phase, we created the SC-artext dataset. We compile a list of style descriptions and a list of words. Combining these lists generated various prompts for artistic text images, which were then used as input for Flux.1-dev [1], resulting in approximately 20k high-quality images. To ensure the images matched the original text content, we used shareGPT4v [10] to regenerate captions. Details of these two datasets can be found in the supplementary material."}, {"title": "4.1. Quantitative Results", "content": "For quantitative evaluation, we use the same OCR tool [4] as in [37] to detect text content in the generated images and calculate the OCR accuracy (OCR-Acc) across three distinct text rendering tasks. In the basic text rendering (BTR) task, existing OCR tools struggle to evaluate word-level typographic attribute accuracy (Word-Acc). Therefore, we use GPT40 and manual screening to assess and obtain the corresponding score. Font consistency (Font-Con) in BTR and style consistency (Style-Con) in artistic text rendering (ATR) are evaluated through user studies. Additionally, we compare our method to Flux.1 in the scene text rendering (STR) task (using the MARIO-bench [8]), evaluating OCR-Acc and CLIP scores (CLIP). Since Glyph-ByT5 and TextDiffuser-2 do not support the artistic text rendering defined in our study (we provide their qualitative results on the ATR-bench in the supplementary materials), the ATR column for these methods lacks quantitative results.\nAs shown in Table 1, our method outperformed the baselines in four out of five metrics while slightly below Glyph-ByT5 regarding OCR accuracy in BTR. This is reasonable since Glyph-ByT5 was trained on millions of text images, whereas our approach utilized a dataset of 50k basic text images, which is twenty times smaller.\nFurthermore, as indicated in Table 2, our method significantly improves OCR-Acc and CLIP scores compared to Flux. Upon reviewing the image results, we found that Flux exhibits semantic confusion in the STR task (on MARIO-bench [8]), similar to its performance in ATR, which notably reduces its OCR accuracy.\nUser Studies. We conducted user studies with 22 participants to perceptually evaluate our results, comparing them against baseline methods. The evaluation included questions focused on two key aspects: font consistency (Font-Con) and style consistency (Style-Con). Font-Con was as-"}, {"title": "4.2. Qualitative Results", "content": "Basic Text Rendering. We use a set of challenging prompt words for evaluation. For Flux.1 and Glyph-ByT5, the prompt example (for the leftmost images) is: \"Blue Text: 'Love knows no limits' in Font: Josefin Sans, Add underline to 'Love', Background: pure yellow\". This prompt specifies the font applies a particular typographic attribute to a word and defines the text and background colors. As shown in Figure 5, Glyph-ByT5 achieves better font consistency than Flux but lacks word-level control. In contrast, our method ensures strong font consistency and enables word-level control, such as underline, bold, or italic.\nArtistic Text Rendering. As TextDiffuser-2 and Glyph-ByT5 do not support artistic text rendering, We compare our method with SD3 and Flux. All qualitative results for our method are displayed with an image scale set to 0.9. We conduct solid experiments and have the following findings: Semantic confusion. SD3 and Flux do not process image inputs, while ours is capable of handling both images and text prompts. To this end, we also report the results generated by Midjourney [23] using the same inputs as ours. As shown in Figure 6, in contrast to other approaches, our method effectively integrates style control without compromising the readability and integrity of the original text content. Meanwhile, it greatly mitigates the phenomenon of semantic confusion existing in all three comparison baselines.\nStyle consistency. To ensure that SD3 and Flux produce"}, {"title": "4.3. Ablation study", "content": "Ablation on TC-Finetuning. To assess the effectiveness of TC-finetuning, we performed ablation studies on the fine-tuned module, training two configurations: joint text attention (Txt-Attn) and joint text and image attention (Txt+Img-Attn). The results in Table 3 show the performance differences across these two settings for the BTR task. The data indicates that increasing the training parameters for the Img-Attn component actually reduces model performance"}, {"title": "4.4. Applications and Limitation", "content": "Applications. Artistic font design. Benefit from the robust style consistency, our approach is able to generate a variety of artistic letters with high consistency. Moreover, because the SCA is pre-trained on high-quality, large-scale data, the style control is not limited to artistic text images. Any style image can be used as a control input, as shown in Figure 8.\nStylization of scene text image. Since visual embeddings of CLIP [29] are not highly sensitive to text [20], using CLIP as the image encoder for style control adapters has minimal impact on the content of the text. Instead, it primarily affects the stylistic presentation. As shown in Figure 9, after injecting different style prompts, the style of text changes, but the content of text remains clearly identifiable.\nLogo design. Scene text and artistic text images can also be seamlessly integrated. By using scene text image prompts alongside artistic text images for style control, our method achieves a smooth blend of the two, as shown in Figure 10. This allows for the creation of versatile logo designs that are suitable for a range of application scenarios.\nLimitation. It is observed the language drift phenomenon exists in our method, as the same as [16, 32]. This effect becomes noticeable as the number of training steps increases. This is mainly because, in the TC-finetuning process, we did not use additional regularization datasets; instead, we applied a simple regularization prefix, 'sks', in the text prompts of the TC dataset. This way decreases the cost. As shown in Figure 11, although language drift is severe at 60k steps, leading to the separation of text and scene in the generated image, the results at 40k steps are acceptable."}, {"title": "5. Conclusion and Future Work", "content": "This paper proposes a two-stage DiT-based pipeline for text rendering with typography and style controls. TC-finetuning with ETC-tokens enables the model to learn and apply word-level typographic attributes to specific words. Style Control Adapter allows for independent style control using images. Experimental results show that the proposed method outperforms baselines on font consistency and word-level control in basic text rendering (BTR) and style consistency in artistic text rendering (ATR). To our knowledge, this paper is the first to achieve word-level control in basic text rendering, offering a valuable reference for future research in both scene and artistic text rendering. In future"}]}