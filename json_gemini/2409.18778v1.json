{"title": "HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation", "authors": ["Joseph Cotnareanu", "Zhanguang Zhang", "Hui-Ling Zhen", "Yingxue Zhang", "Mark Coates"], "abstract": "Efficiently determining the satisfiability of a boolean equation \u2013 known as the SAT problem for brevity is crucial in various industrial problems. Recently, the advent of deep learning methods has introduced significant potential for enhancing SAT solving. However, a major barrier to the advancement of this field has been the scarcity of large, realistic datasets. The majority of current public datasets are either randomly generated or extremely limited, containing only a few examples from unrelated problem families. These datasets are inadequate for meaningful training of deep learning methods. In light of this, researchers have started exploring generative techniques to create data that more accurately reflect SAT problems encountered in practical situations. These methods have so far suffered from either the inability to produce challenging SAT problems or time-scalability obstacles. In this paper we address both by identifying and manipulating the key contributors to a problem's \"hardness\u201d, known as cores. Although some previous work has addressed cores, the time costs are unacceptably high due to the expense of traditional heuristic core detection techniques. We introduce a fast core detection procedure that uses a graph neural network. Our empirical results demonstrate that we can efficiently generate problems that remain hard to solve and retain key attributes of the original example problems. We show via experiment that the generated synthetic SAT problems can be used in a data augmentation setting to provide improved prediction of solver runtimes.", "sections": [{"title": "1 Introduction", "content": "The boolean satisfiability problem (the SAT problem) emerges in multiple industrial settings such as circuit design (Goldberg et al., 2001), cryptoanalysis (Ramamoorthy and Jayagowri, 2023), and scheduling (Habiby et al., 2021). While machine learning is not well suited for solving SAT problems \u2014 solvers are typically required to have perfect accuracy and return correct proofs \u2014 it does have applications in predicting wall-clock solving time for a given solver, which is important for algorithm selection (Kadioglu et al., 2010; KhudaBukhsh et al., 2009) and benchmarking (Fuchs et al., 2023). SAT has also been gaining attention in Large-Language-Model reasoning, as it is a natural tool for interacting with the propositional-logical structure of many reasoning problems (Ye et al., 2023)."}, {"title": "2 Background: Boolean Satisfiability", "content": "Definitions and Notation The Boolean Satisfiability Problem (SAT) is the problem of determining whether there exists an assignment of variable values that satisfies the given Boolean formula,"}, {"title": "Graph Representation of CNFs", "content": "There are several common CNF graph representations. In this work, we use the Literal-Clause Graph (LCG), an undirected and bipartite graph. Each node in the first set of nodes represents a clause and each node in the second represents a literal. We construct an edge for each occurrence of a literal in a clause; the set of undirected edges e is defined as\n\\(e = \\bigcup_{i=1}^{n_c} \\bigcup_{j=0}^{n_i} (l_{ij}, C_i).\\)"}, {"title": "3 Related Work", "content": ""}, {"title": "3.1 Deep-learned SAT generation", "content": "The problem of learned generation for SAT problems was first established in 2019 with SATGEN (Wu and Ramanujan, 2019), motivated by a lack of access to industrial SAT problems. SATGEN used a graph generative adversarial network (GAN) to generate graph representations of SAT problems.\nG2SAT (You et al., 2019) represents problems as graphs. The graphs are progressively split into small trees, and a graph neural network (GNN) is trained to discern which trees should be merged to restore the original graph. While innovative, the method is slow due to its need to sample many tree pairs to form a SAT problem of sufficient size. The most recent improvement on the G2SAT framework, HardSATGEN (Li et al., 2023), includes some domain-inspired considerations in its design, such as communities and cores. HardSATGEN is the first deep-learned SAT generation method that can generate problems which are not trivial to solve for solvers: often the generated problems take nearly as long or even longer for a solver to solve than the corresponding seed problem. Unfortunately, however, the core awareness aspects of the design cause HardSATGEN to be extremely slow, making it challenging to use in any setting that needs many new instances.\nW2SAT (Wen and Yu, 2023) follows an approach more similar to the original SATGEN. It employs a low-cost general graph generation model, and obtains new SAT problems via graph decoding. W2SAT is extremely efficient, but like G2SAT, it is incapable of generating hard problems. G2MILP (Geng et al., 2023). is designed to generate Mixed Integer Linear Programs (MILPs), which are the general case of SAT. A naive modification allows us to use G2MILP to generate SAT problems. The method is nearly as efficient as W2SAT, but also struggles to generate hard instances."}, {"title": "3.2 Core Prediction", "content": "Core Detection can be a helpful tool for understanding UNSAT problems. Cores are often seen as a strong indicator of the hardness of an UNSAT problem (Ans\u00f3tegui et al., 2008). There are multiple classical, verifiable methods for Core Detection, with the current standard being Drat-Trim (Wetzler et al., 2014). Drat-trim requires that the problem be solved once by a SAT solver, which is very slow. In response to this, Neurocore (Selsam and Bj\u00f8rner, 2019) was designed to predict the core of a SAT problem. Neurocore converts the input problem to a graph and uses a GNN to predict cores. Strangely, however, Neurocore does this on variables rather than clauses. Cores are defined to be subsets of clauses, rather than variables, and so this choice seems unnatural. Neurocore strives to be a machine-learning based variable-selection heuristic for SAT solvers, which motivates the focus on variables."}, {"title": "4 Problem Statement", "content": "Given a training set of UNSAT CNFs \\(S = \\{f_1, f_2, ..., f_{m_s} \\}\\), and a corresponding set of label vectors \\(R = \\{r_1, r_2, ..., r_{m_s} \\}\\), we wish to train a generative model G that can construct new examples. The label vector \\(r \\in \\mathbb{R}^d\\) represents the hardness of the SAT problem and we model it as a deterministic mapping, i.e., \\(r_i = g(f_i)\\). In our experiments, the vector is derived by recording the SAT solving time for a pre-specified set of SAT solvers.\nWe assume that the \\(m_s\\) CNFs in the training set are i.i.d. examples from an underlying distribution D. We denote the generative model distribution by \\(D_{G(S)}\\), highlighting that it is dependent on the random training set S. We can obtain a new dataset of \\(m_g\\) i.i.d. samples \\(S_G\\) using the generative model. The total number of samples in the augmented set S is then \\(m_s + m_g\\).\nOur primary goal is to derive a generative procedure that produces sufficiently representative but also diverse samples such that the error obtained by training a model on the augmented dataset S is less than that obtained by training on the original dataset S. As an example task, we consider the prediction of runtime for a candidate solver. In this case, the appropriate loss function is the absolute error between the predicted time and the true time.\nBeyond this, we are also interested in the distance between the distributions D and \\(D_G\\). We examine this through the lens of hardness label vectors. The application of g to the CNF descriptors generated according to D or \\(D_G\\) induces distributions in \\(\\mathbb{R}^d\\). To evaluate the similarity of the original and generated instances, we calculate the empirical maximum mean discrepancy (MMD) distance between these induced distributions."}, {"title": "5 Methodology", "content": "Our generation strategy can be broken into three steps: (1) extraction of the core from a seed instance; (2) addition of random new clauses, generated with low cost; and (3) iterative core refinement. Figure 2 provides an overview of the key core refinement procedure. It consists of a two-step cycle of (a) high-speed core extraction using our novel GNN-based method; and (b) unconflicted literal addition to break any undesirably easy core."}, {"title": "5.1 Generating Hard Instances", "content": "Trivial Cores Cores are the primary underlying hardness providers in UNSAT instances, because a solver must only determine that a subset of a CNF is UNSAT for the whole CNF to be UNSAT, and a core is the smallest subset of clauses of a CNF that is UNSAT. small cores with few clauses are likely to make the CNF trivially easy. Small cores with few clauses are generally easier to solve due to less variable assignment combinations. An example of a trivial core is \\((A \\vee B) \\wedge (\\neg A \\vee B) \\wedge (A \\vee \\neg B) \\wedge (\\neg A \\vee \\neg B)\\).\nWhenever we add a new random clause to an UNSAT instance, there is the danger of creating a trivial core. For example, consider an UNSAT instance which includes three of the clauses from the example above: \\((A \\vee B) \\wedge (\\neg A \\vee B) \\wedge (A \\vee \\neg B)\\). If during generation we unknowingly add the clause \\((\\neg A \\vee \\neg B)\\), the UNSAT instance's large (hard) core will be replaced by a trivial one, leading to hardness collapse. Maintaining awareness of cores and potential cores in a CNF as we perform modifications is challenging. We take a different approach, which we refer to as Core Refinement."}, {"title": "Core Refinement", "content": "The Core Refinement process is made up of two steps that are repeated n times, where n is the number of generated clauses. The first step of the process is to identify the core of the generated instance. The addition of random new clauses in step (2) is very likely to create a core that is trivially easy to solve and it may not be the same as the core of the original instance. Once we have detected this easy core, we make it satisfiable by adding a new literal to a clause in the core. The addition of a single, flexible literal eliminates the constraints of the core and makes it possible to satisfy.\nReturning to the previous example, the UNSAT CNF \\((A \\vee B) \\wedge (\\neg A \\vee B) \\wedge (A \\vee \\neg B) \\wedge (\\neg A \\vee \\neg B)\\) can be made satisfiable by modifying any of the clauses in this fashion: \\((A \\vee B \\vee C) \\wedge (\\neg A \\vee B) \\wedge (A \\vee \\neg B) \\wedge (\\neg A \\vee \\neg B)\\). The introduction of literal C in the first clause means that (A = 0, B = 0, C = 0) is now a satisfying solution.\nAs these two steps are repeated, the core of the instance gradually becomes larger and is likely to be more difficult. The process ends after a fixed number of iterations. In our experiments, we choose this to be the number of generated clauses. Since the hardness of the core is the hardness of the instance (Ans\u00f3tegui et al., 2008), the refinement process can be seen as progressively raising the hardness of the problem.\nUnderlying Hard Core Guarantee The Core Refinement process is designed to repeatedly eliminate easy cores, so after each iteration, the core becomes harder. Finally, after many iterations, we hope that the remaining core is as hard as the original instance. This process can only be guaranteed to lead to a hard core if an underlying hard core exists in the instance at the start of the refinement process. Refinement then whittles away easy cores until only the hard one remains.\nThere is a possibility of creating a hard core through the random generation of clauses, but we cannot rely on this. We must introduce an element to our design to ensure there is a hard core. To achieve this we identify cores from the original instances and include them in the generated instances."}, {"title": "5.2 Core Prediction", "content": "We have two critical objectives for our method: low cost and hard outputs. While the Core Refinement process serves us well in generating hard instances, a naive implementation using existing core detection algorithms is unacceptably expensive in terms of computation requirements. Current core detection algorithms first solve the SAT problem, making Core Detection NP-Complete (Wetzler et al., 2014).\nWe adopt the strategy of approximating the Core Detection algorithm. Since an instance can be naturally represented using a bipartite graph, and the goal of core detection is binary classification of each clause, we expect that a graph neural network is a promising approach.\nGraph Construction We represent each instance as a graph as outlined in Section 2. We make two changes: (a) we add message-passing edges to connect matching positive and negative literals (e.g, A and \\(\\bar{A}\\)); (b) we replace each undirected edge with two directed edges. These changes are designed to facilitate the diffusion of information in the GNN. We denote the set of literal-literal message passing edges by \\(E_u = \\bigcup_{n=1}^{n-1} (l_i^+, l_i^-)\\), where n is the number of variables in the instance. We denote the set of literal-to-clause directed edges by \\(E_{lc} = \\bigcup_{i=1}^{n_c} \\bigcup_{j \\in C_i} (l_{je}, C_i)\\). We denote the set of clause-to-literal directed edges by \\(E_{cl} = \\bigcup_{i=1}^{n_c} \\bigcup_{j \\in C_i} (C_i, l_{je})\\).\nGNN Architecture Given the heterogeneous nature of our graph, arising from different node and edge types, we use three Graph Message Passing models (one for each edge type). We couple these models by averaging their embeddings after each layer. We define a single layer where \\( \\sigma \\) is a non-linear activation function. Finally, we obtain a core membership probability for each clause node by passing the embeddings through a fully connected linear readout layer followed by a sigmoid function to the clause node embeddings. We threshold the values to obtain positive and negative classifications of core membership:\n\\(h^{l+1} = \\sigma(\\frac{1}{3} (GNN(V, E_u, h^l) + GNN(V, E_{lc}, h^l) + GNN(V, E_{cl}, h^l))),\\) (1)\n\\(out = \\mathbb{1}_{>0.5}(\\sigma(xh + b)).\\) (2)\nTraining Our augmentation process is motivated by a scarcity of data. We must therefore address this when training the core detection GNN. We achieve augmentation of the available data by executing the generation pipeline described above for a small number of instances, using a slow, traditional but proof-providing tool for Core Detection in the Core Refinement process. By saving the instance-core pair after each iteration of the core refinement process, we can construct sufficient supervision data for training the Core Prediction GNN model. Although the instance-core pairs we construct this way are correlated, there is sufficient variability for the GNN model to generalize well to other instances. We train the model using the standard binary cross-entropy loss function. For experimental results showing the performance of our Core Prediction model, see Table 4 in the Appendix."}, {"title": "6 Experiments and Results", "content": ""}, {"title": "6.1 Experimental Setting", "content": "Proprietary Circuit Data (LEC Internal) This LEC Internal data is a set of UNSAT instances which are created and solved during the Logic Equivalence Checking (LEC) step of circuit design. LEC needs to be performed after certain circuit optimization steps to ensure that the optimization process has not corrupted the logic of the circuit. If the logic is uncorrupted, the created SAT problem will be UNSAT. Since it is extremely rare that these optimizations in fact corrupt the circuit, more than 99% of LEC instances are UNSAT.\nSynthetic Data (K-SAT Random) Acknowledging the importance of reproducibility, we also provide results on synthetic data. This data is generated by randomly sampling a CNF with m clauses of k literals over n variables. Clauses are sampled without replacement. We have previously argued that random data differs from real data in important ways that make it unsuitable for machine learning applied to real problems. Holding to this view, we use this data primarily to provide a surrogate to the internal data for experimental reproduction purposes, rather than to present results on a second dataset. For details concerning both the LEC Internal and K-SAT data, see Table 3 in the Appendix.\nSAT Solvers We select 7 solvers for hardness analysis: Kissat3 (Biere et al., 2020), Bulky (Fleury and Biere, 2022), UCB (Cherif et al., 2022), ESA (Cherif et al., 2022), MABGB (Cherif et al., 2022),"}, {"title": "6.2 Research Questions", "content": "Our work is motivated by the goal of fast generation of hard and realistic UNSAT datasets for data augmentation. Given these goals, we now establish our strategy for evaluating our model, identifying the key research questions that our experiments explore."}, {"title": "6.2.1 Question 1: Is the method able to generate hard instances?", "content": "In order to quantify 'hardness', we choose the wall-clock solving time for each solver as a metric. We deem a set of generated instances \u2018hard' if the average solver runtime is at minimum 80% of the original dataset's average hardness. If average solver time for the set of generated instances is below 5%, we consider that hardness collapse has occurred.\nIn Table 1 we compare generated with original hardness. W2SAT and G2MILP both suffer hardness collapse, whereas HardSATGEN and HardCore generate hard instances."}, {"title": "6.2.2 Question 2: Is the method fast?", "content": "We measure generation speed by the time required to generate an instance (in seconds). We evaluate this by measuring the wall-clock time of each model during inference and dividing by the number of generated instances. Generally, a method should be able to generate hundreds of instances per hour so that we can augment a dataset in a reasonable time frame."}, {"title": "6.2.3 Question 3: Is the method able to generate datasets that are similar to the original datasets in terms of hardness distribution?", "content": "Although past work such as Li et al. (2023); Wen and Yu (2023); You et al. (2019) has examined graph statistics such as modularity and clustering coefficients, we find little evidence that these are indicative of the hardness of generated instances. Instead, we focus on the similarity of the distributions of the hardness vectors because hardness is of primary importance when working with SAT problems."}, {"title": "6.2.4 Question 4: Can we successfully augment training data with the method's generated data for machine learning?", "content": "We address the task of runtime prediction and compare the performance of two models: one trained on only original data and the other trained on a dataset augmented with generated instances. We train the SATzilla model to predict solver runtime of one specific solver on a given instance. We repeat this for each of the 7 solvers.\nWe calculate the MAE of the predicted total runtime for each solver and average over the solvers. We compare HardCore, W2SAT and two versions of HardSATGEN: (i) HardSATGEN-Strict and (ii) HardSATGEN-N. For HardSATGEN-Strict, we only generate as many instances as possible in the time it takes HardCore to generate the desired number of instances. For HardSATGEN-N, we generated N instances, where N was selected as the number that could be generated in approximately 3 days of computation. We also compare to the un-augmented training sets and refer to it as Original.\nIn order to observe performance over varying sizes of training data, we conduct this experiment for several quantities of original training instances, which is denoted Data Size. Three augmentation instances are generated per original instance, and augmentation is only allowed by using the original instances in the training set. Validation sets are selected from the original data only, with an 80/20 split train/validation split. For LEC the test-set is made up of 10000 randomly selected problems which were not selected for training or validation. For K-SAT the test-set is made up of the problems which were not picked for train/validation from the 1351 original instances."}, {"title": "7 Limitations", "content": "The primary limitation of our work is that it is restricted to UNSAT problems. While some SAT applications are almost entirely UNSAT (e.g., circuit design), many are not. With our proposed approach, this limitation is unavoidable because cores are only present in UNSAT problems. However, there is a concept for SAT problems analogous to the core, known as a backbone.\nAnother limitation is that our work relies solely upon empirical results to demonstrate its efficacy, and these results are only presented on two datasets, one of which is syntehtic. To partially address this concern, we conducted several trials and statistical significance testing to ensure the reliability of our empirical analysis.\nAnother limitation is that our method struggles to scale to extremely large SAT problems. As the size of the SAT problem increases, memory and computation costs scale in polynomial complexity, meaning that SAT problems which have millions of clauses are currently out of reach for this method."}, {"title": "8 Conclusion", "content": "We present a fast method for generating UNSAT problems that preserves hardness. Existing deep-learned SAT generation algorithms either (1) are incapable of generating problems that are even 5% as hard as the example input problems; or (2) can generate hard problems but take many hours for each instance. Our proposed method targets the core of a SAT problem and iteratively performs refinement using a GNN-based core detection procedure. Our experiments demonstrate that the method generates instances with a similar solver runtime distribution as the original instances. For a more challenging industrial dataset, we show that data augmentation using our proposed technique leads to a significant reduction in runtime prediction error."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Data", "content": ""}, {"title": "A.2 Hyper-parameters", "content": "In our design process, given the cost of running experiments - in particular measuring runtime of generated instances \u2013 we did not conduct exhaustive hyperparameter searches. Hyperparameters were set following design considerations and rationales, which will be discussed here.\n\u2022 The random generation method we use is Popularity-Similarity. This has several hyper-parameters: average clause size, \\(\\beta_c\\), \\(\\beta_v\\) and T. Average clause size determines the average number of literals per generated clause, \\(\\Beta_c\\) and \\(\\beta_v\\) are constants in the probability distribution for clause and variable selection, respectively, and T is a constant in the exponent of the probability of an edge existing between clause and variable. Conducting an exhaustive search over these hyperparameters is expensive because the evaluation of each configuration is via runtime-measurement, which requires the solving of a large number of SAT problems by multiple solvers. We communicated with the authors of the paper which presented HardSATGEN, and were able to obtain their hyperparameter configuration for Popularity-Similarity (PS), which was included among their reported baselines. For continuity with previous work and in the interest of reducing the computational budget, we used the provided configuration.\n\u2022 The GCN backbone within our core prediction module has two hyperparameters, namely the number of hidden dimensions and the number of layers. Three potential values were chosen for initial exploration of layer size: [3, 4, 15]. In many applications, GCN networks are configured to have only 3 or 4 layers. This is because GNN networks in general are prone to over-smoothing as the number of layers increases. 15 layers was added to validate this behavior within our context. For hidden dimension size we chose two potential values: [32, 64]. Our findings were that as the model size increased via additional layers and hidden feature size, there was minimal improvement in performance. Thus, we selected the smallest defined configuration of 3 layers and hidden dimension of 32."}, {"title": "A.3 HardCore GNN Core Prediction Implementation Details", "content": "We implement HardCore in DGL using 3 Graph Convolutional Network layers combined into a hetero-GNN, where outputs of each layer are aggregated with a mean using the hererograph package in DGL. We train using 15 problems from the dataset, and we obtain training cnf-core pairs using Drat-Trim in the Core Refinement step for 200 iterations per instance. We train for 1 epoch using Binary Cross Entropy loss."}, {"title": "A.4 K-SAT Random Generation", "content": "Algorithm 1 Algorithm for generating 1 K-SAT Random instance.\nm ~ N(\\(\\mu_m, \\sigma_m\\))\nc~ N(\\(\\mu_c, \\sigma_c\\))\nn\u2190 int(m/c)\ncnf \u2190 randkcnf(3, m, n)\n\u25b7 where randkcnf(k, m, n) returns cnf m with k-var clauses from n variables.\nAlgorithm 1 shows the process by which we generated K-SAT Random instances as discussed in Section 6.1. We randomly sample hyper-parameters (number of clauses, number of variables) from a small window in order to introduce some additional variety into the dataset, and generate by randomly sampling sets of 3 variables without replacement. In our work we chose m ~ N(400, 100), c~ N(4.4, 0.05)."}, {"title": "B Supplementary Results", "content": ""}]}