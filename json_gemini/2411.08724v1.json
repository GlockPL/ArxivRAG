{"title": "QCG-Rerank: Chunks Graph Rerank with Query Expansion in Retrieval-Augmented LLMs for Tourism Domain", "authors": ["Qikai Wei", "Mingzhi Yang", "Chunlong Han", "Jingfu Wei", "Minghao Zhang", "Feifei Shi", "Huansheng Ning"], "abstract": "Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in Large Language Models (LLMs) by integrating information retrieval techniques. However, in the tourism domain, since the query is usually brief and the content in the database is diverse, existing RAG may contain a significant amount of irrelevant or contradictory information contents after retrieval. To address this challenge, we propose the QCG-Rerank model. This model first performs an initial retrieval to obtain candidate chunks and then enhances semantics by extracting critical information to expand the original query. Next, we utilize the expanded query and candidate chunks to calculate similarity scores as the initial transition probability and construct the chunks graph. Subsequently, We iteratively compute the transition probabilities based on an initial estimate until convergence. The chunks with the highest score are selected and input into the LLMs to generate responses. We evaluate the model on Cultour, IIRC, StrategyQA, HotpotQA, SQUAD, and MuSiQue datasets. The experimental results demonstrate the effectiveness", "sections": [{"title": "1. Introduction", "content": "Recently, LLMs, such as Llama[1], ChatGPT[2], and ChatGLM[3], have strong generative capabilities and are widely applied in question answering[4] and code generation[5][6]. LLMs are trained on large-scale corpora to learn deep semantic information, enabling them to generate corresponding responses based on queries. However, the generated responses sometimes contain inconsistencies with facts, leading to the \u201challucinations\u201d phenomenon[7]. In the tourism domain, LLMs often produce outdated addresses and route information, making the hallucination phenomenon particularly obvious[8].\nTo mitigate the hallucination issues of LLMs in the tourism domain, we use RAG to retrieve query-related contents in tourism and input them into LLMs. Subsequently, we use the matched contents to limit the scope of LLMs' responses, ensuring that the generated output is more consistent with the user's query[9]. Due to its outstanding performance, RAG has been widely applied in law[10][11] and medicine[12]. Fig. 1 illustrates the detailed processes LLMs with RAG and without RAG. Given the concise nature of queries in the travel domain and the inconsistent quality and length of candidate contents, similarity-based retrieval methods with travel documents often result in retrieving chunks irrelevant to or contradictory to the user's query. Consequently, this misalignment can cause LLMs to summarize incorrect information, adversely affecting the model's overall performance.\nTo address this issue, Wang et al.[13] utilized LLMs to generate pseudo-documents for the original query and concatenated them to expand the semantic complexity of the query. The enhanced query is then used for retrieval to improve the matching accuracy of relevant documents. Given that the documents retrieved in the initial stage based on similarity may have low semantic relevance or contradict the original query, Xiao et al. [14] proposed a rerank method based on Bge-embedding. This method aims to refine the initial retrieval by adding an additional rerank step that leverages"}, {"title": "2. Related Work", "content": "RAG excels at combining information retrieval with generative models, aiming to improve response accuracy and minimize hallucination issues in generated content.[16]. The RAG process consists of two primary stages. Initially, given a user's query, the retrieval component retrieves the most pertinent external contents. Following this, the generative component produces the final response based on retrieved contents. [17]. Due to its ability to alleviate hallucinations, RAG is extensively applied in open-domain question answering[8][18][19], dialogue systems[10][20][21][22], and code generation[6][23]."}, {"title": "2.2. RAG for LLMs", "content": "Recently, RAG has been widely integrated into LLMs[11]. In this scenario, it retrieves relevant contents based on the query and then utilizes LLMs' summarization capabilities to respond to the query[9][24][25]. The essential aspect impacting the RAG capability in LLMs is the retrieval capability, which researchers have enhanced through optimizations in two key areas: (1) Embedding and (2) External document retrieval capability. For Embedding, a crucial component of RAG influences converting text into vectors, reducing the distance between texts in the vector space, thereby improving retrieval efficiency. Zhang et al. [26] introduced LLM-Embedding, leveraging the ranking positions of N sample outputs from LLMs to refine feedback expectations and thus improve model reranking accuracy. Addressing the issue of low semantic representation quality and fragmented useful information due to using chunks as retrieval units, Luo et al. [27] developed Landmark Embedding, employing a sliding window non-blocking method to capture embeddings that preserve contextual consistency and enhance retrieval effectiveness. Given that current RAG retrieval tasks rely heavily on static rules, often focusing LLMs' attention on the final sentences or tags, Su et al. [28] introduced DRAGIN, a system designed to dynamically determine when and what to retrieve based on information needs, thereby expanding the scope and flexibility of the retrieval process.\nFor External document retrieval capability, researchers use query to find matching related contents. If the retrieved information is irrelevant to the query, the LLMs will output ambiguous answers, leading to hallucination problems. To mitigate this challenge, Wang et al.[13] introduced query2doc, a method to create pseudo-documents using a few prompt-based LLMs. These pseudo-documents are then merged with the original query to form a more refined query, thereby combining LLMs' basic knowledge with external content. This approach enhances the precision and relevance of the final response. On the other hand, Shi et al. [29] proposed GenGround, which first uses the query input to LLMs to generate a basic answer. Subsequently, the retrieved content is used to refine and correct inaccuracies in the initial response. In addition, Ma et al.[30] generated queries through LLMs and used web search engines to retrieve context, further improving the overall quality of the generated content.\nTo reduce the impact of noise information in contents on the final results, Xu et al. [31] proposed a training method based on information refinement. This method optimizes RAG in an unsupervised manner and filters out noise information, enhancing the reliability and accuracy of the output results. On the other hand, Zhu et al.[32] introduced the information bottleneck theory into RAG. Their method maximizes the mutual information between compression and ground output while minimizing the information between compression and retrieved passage. Thereby improving the overall quality of the generated content.\nTo address the limitations of RAG in complex query contexts, Asai et al. [33] proposed SELF-RAG, which introduced a self-reflection mechanism into the RAG field. This enables the model to evaluate the quality of generated text and use this feedback to optimize future retrieval and generation processes. Li et al. [34] argue that retrieving documents containing long-tail knowledge is crucial, especially when the input query involves niche or less common information. Focusing on such specific types of data is essential for improving the accuracy and relevance of the final results. Moreover, user queries are diverse, including simple questions and answers and complex queries that require single-step or multiple searches. Jeong et al.[35] proposed an adaptive QA framework. This framework includes a classifier that dynamically selects the most suitable retrieval strategy based on the complexity of the user's query. This adaptive approach can effectively manage different types of queries and provide relevant responses.\nWhile the methods mentioned above focus on various aspects of content retrieval and generation, they often overlook the importance of extracting critical information from the original query. In this paper, we extract critical information from the original query based on the prompt method and expand it to increase the semantic complexity. This can enhance the retrieved content's relevance to the query to improve the reranking stage's performance. To further optimize the final results, we construct a chunks graph with relevant chunks obtained from the initial retrieval. Next, we propose a chunks graph rerank algorithm to optimize the final result, select essential chunks from the chunks graph, and input them into LLMs to enhance the reliability of the final response."}, {"title": "3. Our Approach", "content": "In this section, we introduce the QCG-Rerank model in detail. The model consists of three parts, as shown in Fig. 2. First, we fine-tune the embedding model using tourism data to ensure it captures more semantic information relevant to the tourism domain. Then, we use the fine-tuned embedding to vectorize all documents and build a vector database. Next, we vectorize the query to retrieve relevant documents in the vector database (Section 3.1). Since the query is brief and contains limited semantic information, making retrieval more challenging, we use a prompt-based method to extract and splice critical information from the query. Additionally, we duplicate the query to enhance its semantic complexity during retrieval (Section 3.2). Finally, we use the retrieval results and the query integrating critical information to build a chunks graph. We use the chunks graph rerank method to filter out the top-ranked chunks in the chunks graph and input them into LLMS (Section 3.3). After the above steps, LLMs respond by combining the filtered chunks with the query."}, {"title": "3.1. Fine-tuning embeddings", "content": "Feng et al. [36] found that using domain data to fine-tune the embedding model can better capture basic semantic information. Inspired by Xiao et al.[14], we construct a training corpus in the format of \u201cquery\u201d: str, \u201cpos\u201d: List[str], \"neg\u201d: List[str], where \u201cpos\u201d represents the correct answers and \"neg\" represents documents randomly selected from other answers. Then, We use them to fine-tune the embedding model, improving its domain adaptability and performance in capturing relevant semantic information.\nAfter fine-tuning the embedding model, we convert each chunk $C_i \\in Con$ into a corresponding vector $e_i \\in \\mathbb{R}^D$ using the tourism embedding model and save these vectors in the vector database. D represents the dim of vector, Con represents original contents. We then use the fine-tuned embedding to encode the query $q_o$ into the corresponding query vector $v_q \\in \\mathbb{R}^D$. Next, we calculate the similarity between $v_q$ and each chunk vector $e_i$ in the vector"}, {"title": "3.2. Critical Information Extraction", "content": "In tourism, users' queries are often brief, such as \u201cWhich are the most visited attractions in Guilin?\u201d. Meanwhile, the chunks in the database are generally longer, leading to matching texts that are not highly relevant or even contradictory to the query during retrieval. This affects the quality of the retrieved chunks and the output results of LLMs. To address this question, we design a prompt template to extract critical information from the given query. The template is shown in Fig. 3.\nThe template involves assigning roles to LLMs and leveraging their inherent information extraction capabilities to distill the critical information from queries. Subsequently, the query $q_o$ and the extracted critical information $CR_{inf}$ are concatenated to update query $q_{up}$. To ensure that the query $q_{up}$ aligns more closely with the chunks stored in the database within the vector space, following the approach by Wang et al.[13], we create the final query by duplicating several times. The concatenation procedure is illustrated in Eq. (2).\n$q_{up} = concat ((q + CR_{inf}) \\times n)$  (2)\nWhere \u201cconcat\u201d represents the concatenation function, $q_{up}$ represents the updated query that concatenates the query and the critical information, which is used as the query in the rerank process, and n represents the number of duplication."}, {"title": "3.3. Chunks Graph Rerank", "content": "We proposed a chunks graph rerank method to address some documents in the recall results that have low relevance or conflicting chunks with the query. First, the recall results and the updated query $q_{up}$ form the chunks graph, and then we perform graph rerank to find chunks with higher scores in the chunk graph.\nSubsequently, we convert the query $q_{up}$ obtained in Section 3.2 into the corresponding query vector $v_{up}$ using the fine-tuned embedding model. Then, we use cosine similarity $sim_{up}$ to calculate $v_{up}$ and each candidate chunk vector to obtain the similarity score between each chunk and the updated query. The calculation process is shown in Eq. (3).\n$sim_{up} = \\frac{V_{up} \\cdot Set_i}{|| V_{up}|| ||Set_i ||}, i\\in \\{1, ..., N\\}$   (3)\nInspired by TextRank[37], we construct the chunks graph G using the query $q_{up}$ and the candidate document Set. We then construct the similarity transfer matrix using the similarity scores. In graph G, only the positions with an interactive relationship with the query are assigned the corresponding similarity scores, while the relationships between contents are initialized to 0. Additionally, we initialize the weight of the graph as shown in Eq. (4).\n$S_i = \\begin{cases} 1, & \\text{if } i = 0\\\\ 0, & \\text{else} \\end{cases}, i \\in \\{1, 2, ..., num, num + 1\\}$  (4)\nWhere num represents the total number of chunks obtained by retrieve, and $S_i$ represents the weight of the graph G node. It enhances the weight coefficient of the query in the rerank process.\nWe input the chunks graph G and the initial weight $S_i$ into the chunks graph rank algorithm and use the query to guide the iterative process to select the chunks with higher scores in the graph. The iterative calculation process is shown in Eq. (5).\n$S^{(t+1)}(i) = (1-d)S^{(t)}(i) + d \\sum_{j\\in In(i)} \\frac{W_{ji}}{\\sum_{k\\in Out(j)} W_{jk}}S^{(t)}(j)$   (5)\nWhere S(i) represents the weight of node i, which is continuously updated with the number of iterations t, d represents the damping coefficient, In(i) represents all nodes connected to node i, Out(j) represents all nodes that can be reached from node j, and $w_{ij}$ represents the correlation between node i and node j.\nAfter the above operations, we can filter out the top K chunks with the highest scores in the chunks graph and combine the query input into the designed template. Using the summarization ability of the LLMs, we can generate a response to the query. The calculation process is shown in Eq. (6):\n$output = \\Theta(\\varepsilon(q_{up}, top_k(G))))$   (6)\nWhere $\\Theta$ represents LLMs, $\\varepsilon$ represents the template, and the designed template is shown in Fig. 5. $top_k()$ represents sorting according to the weight information of S(i) and returning the corresponding top K chunks. Moreover, the output represents the response to the query after LLMs integrate the documents."}, {"title": "4. Experimental", "content": "In this section, we evaluate the effectiveness of our proposed QCG-Rerank method by comparing it with multiple models across several datasets. We provide a detailed analysis of the results, including an introduction to the datasets, baselines, hyperparameters, experimental settings, and evaluation metrics used."}, {"title": "4.1. Datasets", "content": "In this section, we introduce the six datasets used in this paper in detail: Cultour[8], IIRC[38], StrategyQA[39], HotpotQA[40], SQuAD[41], and MuSiQue[42].\nCultour is a tourism dataset containing 12,000 question-answering examples. The data includes both self-collected tourism data and data automatically generated using LLMs. This dataset is used to evaluate the model's performance in the tourism field.\nIIRC is an English dataset designed based on English Wikipedia paragraphs, comprising over 13,000 entries. Each question offers only fragmentary hints, with the full details dispersed across one or more associated articles. We utilize this dataset to assess the performance of model in reading comprehension tasks.\nStrategyQA is an open-domain dataset featuring 2,780 examples. Each example includes a question and a supporting evidence paragraph. This dataset is used to evaluate the commonsense reasoning capabilities of models.\nHotpotQA is a QA dataset derived from English Wikipedia, comprising approximately 113,000 questions. Each question requires finding answers by integrating information from paragraphs within two related articles. This dataset is used to assess the model's ability to answer multi-step questions.\nSQUAD is a reading comprehension dataset where the answer to each question is derived from a specific paragraph or span of text from a Wikipedia article. This dataset is used to evaluate the model's performance in reading comprehension tasks.\nMuSiQue is a multi-hop QA dataset where most questions require 2-4 reasoning steps to find the correct answer. This dataset is used to evaluate the model's ability to perform multi-hop reasoning tasks."}, {"title": "4.2. Baseline", "content": "In this section, we introduce the baselines compared in this paper.\nWo-RAG is a model that uses LLMs to provide direct answers without searching for related documents.\nW-RAG is a model that retrieves relevant documents in a corpus based on a query and then adds the retrieved documents directly to LLMs.\nBM25[43] is a statistical information retrieval algorithm that evaluates the relevance of a query to a document by calculating the term frequency and inverse document frequency of the query term in the document and obtains the final score through weighted summation.\nBM25L[44] is an enhanced version of the BM25 algorithm that adjusts the inverse document frequency (IDF) weight and modifies the text length normalization factor. This adjustment reduces the algorithm's bias towards shorter documents and improves its handling of longer documents, thereby enhancing the relevance and accuracy of information retrieval results.\nBGE-Rerank[14] uses the query to perform two retrievals in an external database. The first retrieval employs a semantic similarity calculation method. The second retrieval uses the Bge-Rerank model to fine-tune and sort these results based on semantic relevance, thereby inputting more relevant documents into LLMs.\nBCE-Rerank[15] is a bilingual and cross-lingual semantic representation algorithm developed by NetEase Youdao. It excels at optimizing semantic search results and improving the ranking of semantically related items."}, {"title": "4.3. Experiment settings", "content": "For a fair experiment, we use Qwen-2-7B-Instruct as the base LLMs and set the temperature to 0.0. In fine-tuning embedding, we evaluate models using both Bge-base and Bge-large embedding. During fine-tuning of the embedding, we set the training epochs to 5.0, the maximum query length to 64, the maximum passage length to 256, and the learning rate to le-5. Additionally, we set the number of duplications of critical information K to the range {1, 2, 3, 4, 5} based on the characteristics of each dataset. For the IIRC dataset, we use 4,906 examples as the training set and 593 examples as the test set. For the Culture and StrategyQA datasets, we split the training set into a 2:8 ratio, using 80% of the data for fine-tuning the embeddings and 20% for testing. For the HotpotQA, SQUAD, and MuSiQue datasets, we split the test set into a 2:8 ratio, using 80% of the examples to fine-tune the embeddings and 20% for testing. If the test set contains more than 1,000 data points, we randomly select 1,000 samples for testing. The detailed"}, {"title": "4.4. Evaluatation metrics", "content": "In evaluating the model, due to the unique nature of the Strategy dataset, where answers are binary (\"true\" or \"false\"), we use Accuracy to measure the accuracy of the model's predictions on this dataset. For the other five datasets, we employ ROUGE[45], BLEU[46], and METEOR[47]. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) calculates the similarity between the automatically generated results and the manually generated reference answers. BLEU (Bilingual Evaluation Understudy): measures the accuracy of the results by comparing the n-gram overlap between the candidate results and the reference results, which is particularly useful when there are multiple correct answers. METEOR (Metric for Evaluation of Translation with Explicit Ordering) evaluates the quality of the model's output by considering factors such as synonym matching, stem matching, and word order matching, making it a more comprehensive evaluation metric than BLEU."}, {"title": "5. Experiment Results", "content": "In this section, We evaluated the performance of the QCG-Rerank model and the baseline on the Cultour[8], IIRC[38], StrategyQA[39], HotpotQA[40], SQUAD[41], and MuSiQue[42] datasets using ROUGE-1(R-1), ROUGE-L(R-L), BLEU-1(B-1), METEOR(Met.) and Accuracy(Acc) metrics. The experimental results are shown in Table 2 and Table 3.\nThe experimental results in Table 2 and Table 3 demonstrate the improvements of the QCG-rerank method. Specifically, for the Chinese dataset Cultour, QCG-rerank shows a notable enhancement, particularly in the R-1 metric when utilizing the Bge_large embedding, achieving a 2.32% increase over Bge_rerank. QCG-rerank also exhibits significant advancements in the English datasets IIRC, Strategy, and SQUAD. Notably, our approach achieves superior performance across most metrics on the HotpotQA and MuSiQue datasets. Additionally, using the Bge large embedding gets the best results on most of the test datasets. Experimental results demonstrate that the QCG-rerank method is effective in expanding the semantic information of the query, selecting relevant documents through chunks graph rerank, and inputting them into LLMs to generate the final response."}, {"title": "5.1. Ablation experiment", "content": "To assess the influence of various modules of QCG-rerank, we designed ablation experiments on the Chinese tourism dataset Cultour and the English dataset IIRC. Ablation experiments assessed the impact of two key components: duplicate critical information and chunk graph rerank. The detailed results are shown in Table 4.\nAccording to the results in Table 4, Wo-CG represents QCG-rerank without chunks graph rerank, Wo-CR represents QCG-rerank without critical information extraction, and W-FT represents QCG-rerank only finetuned by the tourism dataset. Both components positively impact the final results. The critical information duplication method is confirmed to enhance the expressiveness of the semantic information in the query, thereby improving the overall performance. Additionally, the chunks graph rerank method effectively selects chunks with higher scores from the graph and inputs them into a fixed template. Subsequently, we utilize LLMs to summarize these chunks to answer the user query, thus enhancing the final results."}, {"title": "5.2. Impact of fine-tuning embedding", "content": "To evaluate the impact of fine-tuning embedding, we conducted tests using MRR@1, MRR@10, nDCG@1, and nDCG@10 to measure the accuracy of model retrieval after fine-tuning. As shown in Table 5, the experimental results demonstrate the improvements in fine-tuning the embedding model in MRR and nDCG metrics. These results indicate that fine-tuning the embedding model enhances the accuracy of retrieving domain-related documents, making the retrieved documents more relevant to the query. Consequently, it improves the overall performance of the model."}, {"title": "6. Discuss", "content": "To further analyze the performance of QCG-rerank, we adjusted the number of times the critical information was duplicated. The specific results are shown in Table 6. On the Culture dataset, the best performance was achieved when the number of copies was set to 3, and on the IIRC dataset, the best performance was achieved when the number of copies was set to 2. This is because the critical information is enhanced in two aspects during the copying process: 1. The critical information in the query is emphasized as it is duplicated. 2. Since the critical information is duplicated, the query length becomes longer, and the length of the chunks in the database is relatively long, which makes it easier to increase the accuracy of matching, thereby improving the chunks retrieved and ultimately improving the effect of LLMs reply."}, {"title": "6.2. impact of different amounts of content", "content": "In addition, we evaluated the number of chunks input in the rerank phase, using chunks = {5, 10, 15, 20}. According to the results in Table 7, the best performance is achieved when the number of chunks is 10. As the number of chunks increases beyond 10, the model's performance does not significantly improve. However, due to the construction of the graph, the chunks graph rerank method increases the computational time as the number of chunks grows. Therefore, setting the number of chunks to 10 provides the best balance between performance and efficiency."}, {"title": "7. Conclusion", "content": "Given that user queries in the tourism domain are typically brief, while the content in databases is often lengthy and complex, the retrieved information chunks following RAG may still contain numerous irrelevant or contradictory details. In this paper, we introduce a new model, QCG-rerank, which first uses tourism domain data to fine-tune the embedding model and improve the accuracy of data retrieval in the domain. Secondly, we use LLMs to extract critical information from the query and duplicate the critical information to update the query to enhance the matching between the updated query and the reference chunks in the rerank stage. Next, for the recall results, we construct a chunks graph and then use the chunks graph rerank method to select chunks with higher scores in the graph to input into the template. Subsequently, We use LLMs to summarize the input chunks to answer the query. We evaluate the model on Cultour, IIRC, StrategyQA, HotpotQA, SQUAD, and MuSiQue datasets. The experimental results demonstrate the effectiveness and superiority of the QCG-Rerank method. At the same time, we conduct ablation experiments to demonstrate the impact of each module on the model performance. In addition, we also profoundly analyzed the impact of the number of critical information duplications and the number of chunks input in the rerank phase on the model performance.\nIn future work, we will delve deeper into the unique characteristics of tourism data, extracting more in-depth features and integrating them into a knowledge graph. This integration aims to enhance the reasoning capabilities"}]}