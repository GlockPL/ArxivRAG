{"title": "Semantics and Spatiality of Emergent Communication", "authors": ["Rotem Ben Zion", "Boaz Carmeli", "Orr Paradise", "Yonatan Belinkov"], "abstract": "When artificial agents are jointly trained to perform collaborative tasks using a communication channel, they develop opaque goal-oriented communication protocols. Good task performance is often considered sufficient evidence that meaningful communication is taking place, but existing empirical results show that communication strategies induced by common objectives can be counterintuitive whilst solving the task nearly perfectly. In this work, we identify a goal-agnostic prerequisite to meaningful communication, which we term semantic consistency, based on the idea that messages should have similar meanings across instances. We provide a formal definition for this idea, and use it to compare the two most common objectives in the field of emergent communication: discrimination and reconstruction. We prove, under mild assumptions, that semantically inconsistent communication protocols can be optimal solutions to the discrimination task, but not to reconstruction. We further show that the reconstruction objective encourages a stricter property, spatial meaningfulness, which also accounts for the distance between messages. Experiments with emergent communication games validate our theoretical results. These findings demonstrate an inherent advantage of distance- based communication goals, and contextualize previous empirical discoveries.", "sections": [{"title": "Introduction", "content": "Humans use language in multi-agent social interactions. Pressures of synchronization and collabo-ration play a central role in shaping the way we communicate. Motivated by this observation and the goal of creating artificial agents capable of meaningful communication, the field of emergent communication (EC) employs a multi-agent environment jointly trained to accomplish a task that requires active transmission of information. The agents utilize a messaging channel that usually takes the form of a discrete sequence of abstract symbols, resembling the structure of human language. Successful optimization results in synchronized agents operating a newly developed communication protocol tailored to the objective. We study a type of EC setup inspired by Lewis games [32] where a sender agent describes a given input and a receiver agent makes a prediction based on that description. The game objective is designed to make the receiver demonstrate knowledge of the original input, which in turn compels the sender to encode relevant information in the message. There are two common objectives used in this type of EC setup (illustrated in Figure 1):\nReconstruction In the reconstruction task [34, 44, 45], the original input is re-generated based solely on the message. We are specifically interested in a reconstruction objective that quantifies distance between prediction and target, forming a discrete version of autoencoding [18].\nDiscrimination In the discrimination task [17, 28, 33], the original input is retrieved from a set of candidates, incentivized by negative log-likelihood."}, {"title": "Background and related work", "content": ""}, {"title": "Emergent communication with Lewis games", "content": "A large portion of EC research, based on Lewis signaling games [28, 32], defines two-agent coop-erative tasks where one agent, S\u03b8 (\u201csender\u201d), receives an input \\(x \\in X\\) (\"target\") and generates a message \\(m \\in M\\), and the second agent, R\u03c6 (\"receiver\u201d), takes that message and outputs a prediction. The two most common such tasks are reconstruction and discrimination.\nIn the reconstruction task, the receiver tries to predict the target directly from the message. This is a generative method, so it can naturally be modeled with a receiver that outputs a distribution over the input space [7, 41, 42, 44], i.e., \\(R_\\phi : M \\rightarrow \\Delta(X)\\). In this case, the objective is usually negative log-likelihood or accuracy. However, this approach has two major flaws: First, explicitly outputting a distribution is not always feasible, e.g., over images. Thus, this game is mostly implemented on simple synthetic datasets. Second, the incentive of likelihood or accuracy does not measure how"}, {"title": "Towards interpretable human-like communication", "content": "A central line of study aims to develop EC systems that create protocols with high proximity to natural language. Some have tried aligning the two explicitly, by using pretrained language models [31, 43] or training translation systems [45, 46]. In this work, we focus on properties of the EC protocol itself, as induced by the objective rather than exposure to text. The most studied such characteristic is compositionality, which refers to the ability of agents to assemble complex units (e.g., sentences) from simpler units (e.g., words). Compositionality is hard to measure [1, 5, 6, 24, 37], and while some claim that it is correlated with generalization [2, 40, 42], the opposite has also been argued [8, 11, 13]. In any case, emergent languages are often not compositional [25], inefficient [7, 41], and generalize to noise [3], suggesting a mismatch between common EC objectives and the evolution of human language. In this work, we advance the goal of creating human-like communication by interpreting properties of natural language into formal constraints, and analyzing whether the EC objectives create protocols that follow them.\nA commonly used compositionality measure, topographic similarity (topsim) [5], is related to the definitions presented in this work. It evaluates the correlation between distances in the input space and the corresponding distances in the messages space. Notably, topsim considers the relationship between every pair of inputs, whereas our definitions only consider pairs that correspond to similar messages. The latter follows an intuitive asymmetry: inputs with similar messages are expected to be similar, but inputs with dissimilar messages do not have to be different (for example, the same image can often be described by several distinct human captions)."}, {"title": "Input information encoded in the message", "content": "The game objective dictates the information that needs to be stored in messages. In this work, we focus on spatial information by considering the geometric relationship between inputs mapped to the same message. To the best of our knowledge, no previous work has theoretically analyzed this idea in EC literature. A related concept is the mutual information (MI) between messages and inputs, which has been studied both empirically [1, 15, 23, 35, 45] and theoretically [19, 21, 42]. MI is strongly related to the infoNCE objective [38] often used in implementations of the discrimination game. MI can be used to measure the complexity of the communication channel [44, 45], and is correlated with task performance [23, 45] and even compositionality [1]. Finally, The mutual information gap [9] can be used to measure disentanglement [8, 13], which is also related to compositionality. However, Guo et al. [15] show that MI is not sufficient to indicate transfer-learning generalization, and connect that to pairwise distances of inputs mapped to the same message, effectively testing for semantic consistency. The first part of this work, combined with their findings, would suggest that languages developed by agents in the reconstruction setting should be more expressive and generalize better to different tasks.\nOn the theoretical side, a relevant work is Rita et al. [42], where the global discrimination objective is decomposed into two interpretable components, a co-adaptation loss and MI. The latter is equivalent to our Lemma A.1. In this work, we use such equivalent objectives for both the reconstruction and discrimination tasks to investigate the behaviour of optimal solutions, and specifically whether the tasks encourage semantic consistency."}, {"title": "Notation and task definitions", "content": ""}, {"title": "General notation", "content": "We begin by introducing general notation for the EC setup; see Figure 2 for illustration. The input space is modeled by a real-valued random variable \\(X = (X, f_x)\\), where \\(X \\subset \\mathbb{R}^d\\) is the bounded set of possible inputs (e.g., images), and \\(f_x : X \\rightarrow \\mathbb{R}^+\\) is a prior distribution. Denote by \\(M = \\{m_1, m_2,... \\} \\subset \\mathbb{R}^{d_m}\\) the finite or countably infinite set of possible messages. We assume that the minimal distance between a pair of messages is attained, i.e., that \n\\[\\min_{m_1\\neq m_2\\in M} ||m_1 - m_2|| \\in (0,\\infty).\\]\nThe sender agent \\(S_\\theta : X \\rightarrow M\\), parameterized by \\(\\theta\\in \\Theta\\), maps each input to a message. Sender \\(S_\\theta\\) defines the communication protocol; we compare EC setups based on which optimal sender agents they induce. The receiver agent \\(R_\\phi\\), maps the message and optionally additional input to a prediction. It is parameterized by \\(\\phi \\in \\Phi\\). During EC training, the goal is to learn hypotheses \\((\\theta,\\phi)\\) that minimize a loss described by a function \\(l : \\Theta \\times \\Phi \\times X \\rightarrow \\mathbb{R}\\), which maps a pair of agents and an input to a real value. To group all of the above notation into a single definition, denote an EC setup as a sextuple \\((X, f_x, M, \\Theta, \\Phi, l)\\). An EC setup induces a set of optimal agents by\n\\[\\theta^*,\\phi^* \\in \\underset{\\theta\\in\\Theta, \\phi\\in\\Phi}{\\text{arg min}} \\mathbb{E}_{X \\sim X} l(\\theta, \\phi, x).\\]\nNote that every part of the setup can affect the set of optimal agents. We also define a notion of conditional optimality, where one of the agents is fixed at a potentially sub-optimal hypothesis; sender \\(S_\\theta\\) is synchronized with receiver \\(R_\\phi\\) if the pair is optimal for the EC setup \\((X, f_x, M, \\Theta, \\{\\phi\\}, l)\\). The same definition applies to the receiver agent respectively. Note that a pair of agents can be synchronized in both directions without being optimal."}, {"title": "Task definitions", "content": "In the reconstruction setting, the receiver agent predicts the original input (the \"target\") given only the message. In the discrimination setting, the receiver is given the message and a set of candidate inputs containing the target in a random position, and attempts to predict that position based on the message. A visual illustration can be seen in Figure 1. We now present formal definitions of these setups.\nReconstruction. The receiver maps a message \\(S_\\theta(x)\\) to a prediction in the input space. The loss is the Euclidean distance between that prediction and the target \\(x\\).\n*   \\(\\Phi^{M,X}_{reconstruction}\\) is defined as the set of all functions of the form \\(R: M\\rightarrow X\\).\n*   \\(l_{reconstruction} (\\theta, \\phi,x) := ||R_\\phi(S_\\theta(x)) - x||^2\\).\nDefinition 1. An EC setup \\((X, f_x, M, \\Theta, \\Phi, l)\\) is a reconstruction game if \\(\\Phi \\subset \\Phi^{M,X}_{reconstruction}\\) and \\(l = l_{reconstruction}\\). It has an unrestricted receiver hypothesis class if \\(\\Phi = \\Phi^{M,X}_{reconstruction}\\).\nDiscrimination. In addition to a message \\(S_\\theta(x)\\), the receiver sees a set of candidates \\(\\{x_1,...,x_d\\}\\), which contains the target at a random position \\(t\\), i.e. \\(x_t = x\\), and the rest are \\(d - 1\\) independently sampled distractors. The receiver outputs a probability distribution over the candidates. The loss is the negative log-likelihood of the correct position \\(t\\) according to this distribution, averaged over the target position and distractors."}, {"title": "Semantic consistency: a prerequisite to meaningful communication", "content": "In many EC environments, the message is a latent representation of the input, and is often directly analogous to the corresponding latent vector from continuous training setups (e.g. autoencoding, contrastive learning). That said, the discrete bottleneck in EC offers an additional perspective to the concept of a message; when the message space is smaller than the input space, the communication protocol is forced to be a many-to-one mapping, meaning that each message represents a set of inputs.\nWe consider inputs mapped to the same message as equivalent with respect to the communication protocol, thus defining a set of equivalence classes that partitions the input space. With this perspective in mind, we wish to understand whether this partition signals meaningful properties of the inputs. In other words, we ask the question: Is there a meaningful relationship between inputs mapped to the same message? This motivation is illustrated in Figure 3."}, {"title": "Are EC systems semantically consistent?", "content": "Having defined the two common EC environments in Section 3.2, we now apply Definition 3 to assess whether semantically consistent protocols are induced by reconstruction and discrimination tasks. Formally, we answer the following question for each of the tasks:\nIs every optimal emergent protocol semantically consistent?\nUnder the assumption of an unrestricted receiver hypothesis class, we find that:\n*   In the reconstruction game, the answer is yes. We show that equivalent inputs in reconstruction-induced communication protocols are clustered together in input space, matching Figure 3b.\n*   In the discrimination game, the answer is no. The relationship between equivalent inputs in a discrimination-induced protocol can be arbitrary, matching Figure 3c.\nAll proofs are given in Appendix C."}, {"title": "Reconstruction results", "content": "Ideally, we would like to examine a closed-form set of optimal communication protocols. Unfortu-nately, there is no closed-form solution to the set of optimal agent pairs in a general reconstruction setting. That said, we do have such a solution to the synchronized receiver, i.e., the optimal receiver for a fixed sender \\(S\\) (the following arg min set contains a single item):\n\\[R^* (m) = \\underset{r \\in X}{\\text{arg min}} \\mathbb{E}_{x \\sim X} [||r - x||^2 | S(x) = m] = \\mathbb{E} [X | S(X) = m]\\]\nBy assuming that \\(\\Phi\\) is unrestricted, we can plug the closed-form synchronized receiver back into the loss function. This yields a transformed objective given in the following lemma.\nLemma 5.1. [proof in page 16] Let \\((X, f_x, M, \\Theta, \\Phi, l)\\) be a reconstruction game. Assuming \\(\\Phi\\) is unrestricted, a sender \\(S_\\theta\\) is optimal if and only if it minimizes the following objective:\n\\[\\sum_{m\\in M} P(S_\\theta(X) = m) \\cdot \\text{Var} [X | S_\\theta(X) = m]\\]\nThis equivalent objective clearly shows the connection between inputs mapped to the same message: In every optimal solution, these inputs will have high proximity. (In fact, this is the objective function of k-means clustering [36], as elaborated in Appendix B.) Moreover, this formula is the unexplained variance, so minimizing it will maximize the explained variance, leading to the following theorem.\nTheorem 5.2. [proof in page 17] Let \\((X, f_x, M, \\Theta, \\Phi, l)\\) be a reconstruction game. Assuming \\(\\Phi\\) is unrestricted and \\(\\Phi\\) contains at least one semantically consistent protocol, every optimal communica-tion protocol is semantically consistent."}, {"title": "Discrimination results", "content": "In a similar fashion to the reconstruction setting, we have a closed form solution to the synchronized discrimination receiver:\n\\[R^* (m,x_1,...,x_d) = P (t | x_1,...,x_d,m)\\]\nExplicitly (see Lemma 5.3's proof):\n\\[P (R^* (m, x_1,...,x_d) = j) = \\frac{1}{|\\{x_1,...,x_d\\} \\cap S^{-1}(m)|}1\\{S(x_j) = m\\}\\]\nApplying this receiver, we get the following discrimination loss:\nLemma 5.3. [proof in page 17] Let \\((X, f_x, M, \\Theta, \\Phi, l)\\) be a d-candidates discrimination game. Assuming \\(\\Phi\\) is unrestricted, a sender \\(S_\\theta\\) is optimal if and only if it minimizes the following objective:\n\\[\\sum_{m\\in M} P(S_\\theta(X) = m) \\cdot \\mathbb{E} \\log (1 + \\text{Binomial}(d - 1, P(S_\\theta(X) = m)))\\]\nAnd when \\(d = 2\\) (a single-distractor game), this simplifies into:\n\\[\\sum_{m\\in M} P(S_\\theta(X) = m)^2.\\]\nThis equivalent objective reveals the interesting disparity between reconstruction and discrimination. Note that this objective is solely a function of the 'sizes' of equivalence classes (the probability for each message). Thus, while the above formula incentivizes the sender to distribute the messages uniformly, it does not impose any constraint on their content; the connection between inputs mapped to the same message could be arbitrary. In formal terms, we prove the following corollary:\nCorollary 5.4. [proof in page 19] If the set of possible messages is finite, any sender agent that assigns them such that their distribution is uniform can be paired with some receiver to generate globally optimal loss for the discrimination game.\nUsing this corollary, we prove that optimal solutions can be inconsistent in the discrimination setting:\nTheorem 5.5. [proof in page 20] There exists a discrimination game \\((X, f_x, M, \\Theta, \\Phi, l)\\) where \\(\\Phi\\) is unrestricted and \\(\\Phi\\) contains at least one semantically consistent protocol, in which not all of the optimal communication protocols are semantically consistent."}, {"title": "A missing dimension: spatiality in the message space", "content": "While the notion of semantic consistency successfully differentiates between the two EC frameworks, it has a major shortcoming: is does not take into account distances between messages. Spatiality in the message space is integral to concepts like compositionality [5] and ease of teaching [33]. A hypothetical communication protocol that maps similar messages to very different meanings may satisfy Definition 3, but is fundamentally different from natural language. We now present a stricter version of semantic consistency, spatial meaningfulness, which does consider distances in the message space, and therefore better indicates an inherent similarity to natural language. In addition, we eliminate the assumption that the receiver's hypothesis class is unrestricted, in favor of simplicity and non-degeneracy conditions, which are more realistic in the context of natural language.\nTo introduce distance between messages, we replace the message equality with similarity in Equa-tion (1). Let \\(\\varepsilon_0\\) denote a threshold under which messages are considered similar. To avoid reverting back to Definition 3, we require that \\(\\varepsilon_0\\) be greater than or equal to \\(\\varepsilon_M\\), defined as the minimal distance between messages: \\(\\varepsilon_M := \\underset{m_1\\neq m_2\\in M}{\\text{min}} ||m_1 - m_2||\\).\nDefinition 4. For \\(\\varepsilon_0 \\geq \\varepsilon_M\\), a communication protocol \\(S_\\theta\\) is \\(\\varepsilon_0\\)-spatially meaningful if \\(\\forall 0 < \\varepsilon < \\varepsilon_0\n\\[\\mathbb{E}_{X_1,X_2\\sim X} [||X_1-X_2||^2 | ||S_\\theta(x_1) - S_\\theta(x_2)|| \\leq \\varepsilon] < \\mathbb{E}_{X_1,X_2\\sim X} [||x_1-x_2||^2]\\]\nA communication protocol \\(S_\\theta\\) is spatially meaningful if this definition holds for \\(\\varepsilon_0 = \\varepsilon_M\\)."}, {"title": "Results", "content": "With these definitions, we have the following theorems:\nTheorem 6.1. [proof in page 21] Let \\((X, f_x, M, \\Theta, \\Phi, l)\\) be a reconstruction game, let \\(\\varepsilon_0 \\geq \\varepsilon_M\\) and let \\(\\varphi \\in \\Phi\\) such that \\(R_\\varphi\\) is \\((X, M, \\varepsilon_0)\\)-simple and non-degenerate. Every sender that is synchronized with \\(R_\\varphi\\) is \\(\\varepsilon_0\\)-spatially meaningful.\nTheorem 6.2. [proof in page 22] There exists a discrimination game \\((X, f_x, M, \\Theta, \\Phi, l)\\), \\(\\varepsilon_0 \\geq \\varepsilon_M\\) and a receiver \\(\\varphi \\in \\Phi\\) which is \\((X, M,\\varepsilon_0)\\)-simple and non-degenerate, where a synchronized sender matching \\(R_\\varphi\\) is not \\(\\varepsilon\\)-spatially meaningful for any \\(\\varepsilon\\)."}, {"title": "Experiments", "content": "To support our theoretical findings, we run experiments on two datasets: (i) the MNIST dataset [30] contains images of a single hand-written digit; (ii) the Shapes dataset [27] contains images of an object with random shape, color and position. We train reconstruction and discrimination (40 distractors) agents using a communication channel of vocabulary size 10 and message length 4 (no EOS token), optimized with Gumbel-Softmax [20, 22]. In the MNIST experiments, we use the digit labels to train another set of agents with the supervised discrimination task (40 distractors). Further data and training details are found in Appendix D.1."}, {"title": "Semantic consistency, compositionality and task performance", "content": "To evaluate semantic consistency empirically, we develop the empirical message variance measure (in Appendix D.2) which calculates the empirical variance of each equivalence class and takes a"}, {"title": "Message purity", "content": "Since the MNIST dataset has only the digit label, we expect meaningful communication protocols to signal digits in their messages. To evaluate this, we calculate the percentage of images that have the majority digit label of their equivalence class. This can be interpreted as the highest attainable accuracy of a classifier mapping messages to digits. Figure 4 shows the most significant improvement over the baseline in the supervised discrimination game, as expected by Lemma A.2. The reconstruction task induces a nearly perfect digit description protocol as well, despite being unsupervised, providing evidence of its ability to unveil meaningful properties, in agreement with our findings. On the other hand, message purity is much lower in the unsupervised discrimination setting, again showcasing that task success does not guarantee an intuitive communication strategy."}, {"title": "Spatial meaningfulness analysis", "content": "We develop an evaluation method for spatial meaningfulness, which we term cluster variance, based on the message variance measure. While we do not find a decisive result using this method, we suggest several ideas for further investigation; see Appendix E.3 for the full analysis."}, {"title": "Limitations", "content": "Our analysis compares the two frameworks via their optimal solutions, and often assumes unrestricted hypothesis classes. In reality, the complexity of the agents is limited by the chosen architectures. Optimization-error affects the emergent protocol as well, especially when it is discrete, as regular differentiable training is not possible. Future work may further analyze these assumptions from the theoretical side, or evaluate their effect empirically. This limitation is evident in our empirical results (Section 7), where channel utilization in the discrimination game behaves differently than expected.\nOur main results regarding the discrimination game deal with a simplistic version, where candidates are chosen independently. While we analyze some common variations in the appendix, many other ideas have been proposed and shown to have significant benefits, including multiple-target games [6, 37], different-view candidate representations [10, 16], and multi-modality [14].\nOur results assume, as illustrated in Figure 3, that proximity in the input space entails semantic information, and specifically that Euclidean distance broadly indicates the level of semantic similarity. Future work may investigate other distance metrics or evaluate the validity of this assumption."}, {"title": "Conclusion", "content": "Based on properties of natural language, we have defined notions of semantic consistency and spatial meaningfulness, meant to evaluate meaningfulness in emergent communication protocols. Using these definitions, we found that communication protocols generated to solve the reconstruction objective have messages with inherent meaning, while the discrimination objective can be solved by unintuitive systems. Our findings provide insight into known empirical results in EC, such as the ability of agents in the discrimination game to perform well on unseen random noise. The theoretical tools that we have proposed can be used for future investigation and design of EC environments. As a main takeaway, we conclude that distance-based EC environments have promising potential in the prospect of inducing characteristics of natural language, whereas probability-based objectives must be more carefully designed."}, {"title": "Analysis of discrimination variations", "content": "In this section, we extend our analysis of the discrimination game to three common variations from EC literature."}, {"title": "Global discrimination", "content": "In this variation, rather than sampling a small set of candidates, the receiver must discriminate between the entire set of data points, which does not have to be finite nor countable. In related work this game is often referred to as the reconstruction game [7, 41, 42, 44], as it can be interpreted as a generative task. However, since it uses the negative log-likelihood objective and not a distance-based one, we call it global discrimination. Note that when the set of data inputs \\(X\\) is finite, this is similar to a special case of the regular discrimination game, where the number of candidates \\(d\\) is set to \\(|X|\\) (and candidates are sampled without replacement). This equivalence is also mentioned by [12]. Denote:\n*   \\(\\Phi^{M,X}_{global}\\) to be the set of all functions of the form \\(R: M \\rightarrow \\Delta(X)\\).\n*   \\(l_{global}(\\theta, \\phi,x) := - \\log P(R_\\phi(S_\\theta(x)) = x)\\)\nDefinition 7. An EC setup \\((X, f_x, M, \\Theta, \\Phi, l)\\) is a global discrimination game if \\(\\Phi \\subset \\Phi^{M,X}_{global}\\) and \\(l = l_{global}\\).\nLemma A.1. [proof in page 24] Let \\((X, f_x, M, \\Theta, \\Phi, l)\\) be a global discrimination game. Assuming \\(\\Phi\\) is unrestricted, a sender \\(S_\\theta\\) is optimal if and only if it minimizes the following objective:\n\\[-I(X; S_\\theta(X))\\]\nWe see that optimal protocols maximize mutual information between inputs and messages. This corresponds to a popular idea in representation learning literature [19, 38]. However, similarly to the objective shown in Lemma 5.3 for the regular discrimination game, this communication goal does not consider distances between inputs. In fact, mutual information can be calculated for non-numerical input spaces."}, {"title": "Supervised discrimination", "content": "In this variation, labels are incorporated into the game via the candidate choice mechanism. The distractors (candidates that are not the target) are guaranteed to not have the same label as the target. This means that the receiver will never have to tell apart two inputs from the same class.\nLet \\(Y\\) be a finite set of labels, and let \\(label : X \\rightarrow Y\\) be a (deterministic) function mapping input to label. Denote the distribution over labels \\(Y = label(X)\\). We assume that this random variable has a uniform distribution, i.e., that the classes are balanced. The supervised variation uses the same receiver functions as in Definition 2, and a slightly different loss function:\n\\[l^{X,Y,d}_{supervised} (\\theta, \\phi,x) := \\mathbb{E}_{\\substack{t\\sim U\\{1,...,d\\}, x_t=x\\\\X_1,...,X_{(t-1)},X_{(t+1)},...,X_d\\sim P(X|Y\\neq label(x))^{d-1}}} [-\\log P(R_\\phi(S_\\theta(x),x_1,...,x_d) = t)]\\]\nDefinition 8. For \\(d \\leq |Y|\\), an EC setup \\((X, f_x, M, \\Theta, \\Phi, l)\\) is a d-candidates supervised discrimina-tion game if \\(\\Phi \\subset \\Phi^{M,X,d}_{discrimination}\\) and \\(l = l^{X,Y,d}_{supervised}\\).\nLemma A.2. [proof in page 24] Let \\((X, f_x, M, \\Theta, \\Phi, l)\\) be a 2-candidates supervised discrimination game. Assuming \\(\\Phi\\) is unrestricted, a sender \\(S_\\theta\\) is optimal if and only if it minimizes the following objective:\n\\[\\sum_{m\\in M} P(S_\\theta(X) = m)^2 - \\sum_{m\\in M} \\sum_{y \\in Y}P(S_\\theta(X) = m, label(X) = y)^2\\]\nThis result is interpretable too. The first expression is the unsupervised objective that encourages uniformity of message probabilities, which we encountered in Lemma 5.3. The second expression encourages non-diversity of labels within each equivalence class (optimal if all inputs mapped to a message share the same label). This means that optimal messages would have high mutual information with the labels, which in turn could introduce semantic consistency. However, within each class, the communication protocol would not necessarily be able to distinguish between inputs; perfect accuracy can be achieved with just \\(|Y|\\) unique messages."}, {"title": "Classification discrimination", "content": "The classification variation builds on top of the supervised version, by swapping out the target from the set of candidates, and replacing it with a random input that has the same label. In other words, the receiver attempts to guess the candidate with the same label as the target. This game is structured such that every class has exactly one representation in the set of candidates. This is somewhat related to the object-focused referential game [10, 12].\nLet \\(y = \\{1, ..., n\\}\\) be a finite set of labels, and let \\(label : X \\rightarrow Y\\) be a (deterministic) function mapping input to label. Denote the distribution over labels \\(Y = label(X)\\). The classification variation uses the same receiver functions as in Definition 2, and the following loss function:\n\\[l_{classification} (\\theta, \\phi, x) := \\mathbb{E}_{\\{x_i \\sim P(X|Y=i)\\}_{i=1}^n} [-\\log P(R_\\phi(S_\\theta(x),x_1,...,x_n) = label(x))]\\]\nDefinition 9. An EC setup \\((X, f_x, M, \\Theta, \\Phi, l)\\) is a classification discrimination game if \\(\\Phi \\subset \\Phi^{M,X,n}_{discrimination}\\) and \\(l = l_{classification}\\).\nNote that if \\(X\\) is finite, we could choose \\(n = |X|\\) and \\(label(x_i) = i\\), i.e. define labels as the identity mapping, and end up with the global discrimination game.\nLemma A.3. [proof in page 25] Let \\((X, f_x, M, \\Theta, \\Phi, l)\\) be a classification discrimination game. Assuming \\(\\Phi\\) is unrestricted, a sender \\(S_\\theta\\) is optimal if and only if it minimizes the following objective:\n\\[-I(Y; S_\\theta(X))\\]\nOptimal communication protocols in the classification game maximize mutual information with the labels. If the labels signal meaningful properties of the inputs, protocol induced by this game will reflect that, and be semantically consistent. However, similarly to the supervised discrimination game, optimal solutions can use just \\(|Y|\\) different messages, as they do not have to describe any properties of the input other than its label."}, {"title": "Reconstruction and k-means clustering", "content": "Lemma 5.1 shows that optimal communication protocols in the reconstruction setting minimize the objective function of k-means clustering [36], where each equivalence class corresponds to a cluster. In this section we show a stronger connection between the two environments. Namely, we show that if \\(X\\) is a uniform distribution with finite support, both \\(\\Theta\\) and \\(\\Phi\\) are unrestricted, and the message space \\(M\\) is finite, the reconstruction game can simulate k-means clustering with \\(k = |M|\\) (the equivalence classes correspond to clusters). This happens if we alternate between each agent's optimization:\nAssignment step When the receiver is frozen and sender is optimized, the best solution (synchro-nized sender) assigns each input to its projection on Receiver's Image:\n\\[R(S^*(x)) = R(\\underset{m\\in M}{\\text{arg min}} ||x - R(m) ||) = \\underset{x'\\in \\text{Img}(R)}{\\text{arg min}} ||x - x'||.\\]\nIn other words, the sender chooses the message (cluster) with the closest receiver output (centroid) to its input.\nUpdate step When the sender is frozen and receiver is optimized, the best solution (synchronized receiver) sets each output to be the mean over the relevant inputs:\n\\[R^* (m) = \\underset{r\\in X}{\\text{arg min}} \\mathbb{E} [||r-x||^2 | S(x) = m] = \\mathbb{E} [X | S(X) = m]\\]\nIn other words, the receiver updates its output (centroid) to be the mean over inputs mapped to the given message (cluster)."}, {"title": "Proofs", "content": ""}, {"title": "Simplification of the semantic consistency definition", "content": "simplification of Equation (1). Recall Equation (1):\n\\[\\mathbb{E}_{X_1,X_2\\sim X} [||X_1 - X_2 ||^2 | S_\\theta(x_1) = S_\\theta(x_2)", "x_1-x_2||^2": "nNote that since \\(X\\) is bounded, these values as well as the expectation and variance of \\(X\\) are finite. Using the identity \\(\\mathbb{E}[||X_1-X_2||^2", "2\\text{Var}[X": "the left term is equal to\n\\[\\mathbb{E}_{X_1,X_2\\sim X} [||X_1 - X_2 ||^2 | S_\\theta(x_1) = S_\\theta(x_2)"}]}