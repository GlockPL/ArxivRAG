{"title": "From Explainability to Interpretability: Interpretable Policies in Reinforcement Learning Via Model Explanation", "authors": ["Peilang Li", "Umer Siddique", "Yongcan Cao"], "abstract": "Deep reinforcement learning (RL) has shown remarkable success in complex domains, however, the inherent black box nature of deep neural network policies raises significant challenges in understanding and trusting the decision-making processes. While existing explainable RL methods provide local insights, they fail to deliver a global understanding of the model, particularly in high-stakes applications. To overcome this limitation, we propose a novel model-agnostic approach that bridges the gap between explainability and interpretability by leveraging Shapley values to transform complex deep RL policies into transparent representations. The proposed approach offers two key contributions: a novel approach employing Shapley values to policy interpretation beyond local explanations and a general framework applicable to off-policy and on-policy algorithms. We evaluate our approach with three existing deep RL algorithms and validate its performance in two classic control environments. The results demonstrate that our approach not only preserves the original models' performance but also generates more stable interpretable policies.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) is an important machine learning technique that learns to make decisions with the best outcomes defined by reward functions (Sutton and Barto 2018). Recent advances in RL have shown remarkable performance when integrating RL with deep learning to solve challenging tasks with human-level or superior performance in, e.g., AlphaGo (Silver et al. 2017), Atari games (Mnih et al. 2015a), and robotics (Gu et al. 2017). These successes are largely due to the powerful function approximation capabilities of deep neural networks (DNNs), which excel at feature extraction and generalization. However, the use of DNNs also introduces significant challenges as these models are often considered \u201cblack boxes\", making them difficult to interpret (Zahavy, Ben-Zrihem, and Mannor 2016). They are often complex to train, computationally expensive, data-hungry, and susceptible to biases, unfairness, safety issues, and adversarial attacks (Henderson et al. 2018; Wu et al. 2024; Siddique, Weng, and Zimmer 2020). Thus, an open challenge is to provide quantitative explanations for these models such that they can be understood to gain trustworthiness.\nExplainable reinforcement learning (XRL) has become an emerging topic that focuses on addressing the aforementioned challenges, aiming at explaining the decision-making processes of RL models to human users in high-stakes, real-world applications. XRL employs the concepts of interpretability and explainability, each with a distinct focus. Interpretability refers to the inherent clarity of a model's structure and functioning, often achieved through simpler models like decision trees (Bastani, Pu, and Solar-Lezama 2018; Silva et al. 2020a) or linear functions that make a policy \u201cself-explanatory\u201d (Hein, Udluft, and Runkler 2018). On the other hand, explainability is related to the use of external, post-hoc methods to provide insights into the behavior of a trained model, aiming to clarify, justify, or rationalize its decisions. Examples include employing Shapley values to determine the importance of state features (Beechey, Smith, and \u015eim\u015fek 2023) and counterfactual states to gain an understanding of agent behavior (Olson et al. 2021).\nWhile explainability can provide valuable insights that build user trust, we argue that in high-stakes and real-world applications, explainability alone is insufficient. For instance, Shapley values (Shapley 1953)\u2014a well-known explainable model\u2014provide local explanations by assigning numerical values that indicate the importance of individual features in specific states. Although such explanations can help users build trust by aligning with human intuition and prior knowledge when enough states are covered, they fail to enable users to fully reproduce or predict agent behavior. This is because these local explanations do not provide a comprehensive, global understanding of the model's functionality, leaving critical aspects of the decision-making process in the dark. In contrast, interpretability offers full transparency and intuitive understanding which is essential for critical applications where trust and comprehensibility are essential. However, the trade-off between simplicity and performance in interpretable models often results in reduced model performance.\nDespite its limitations, explainability remains a valuable tool for uncovering insights into model behavior. It can facilitate the development of interpretable policies by abstracting key information from explanations and guiding policy formulation. In this paper, we propose a model-agnostic approach to generate interpretable policies by leveraging insights from explainability techniques in RL environments. This approach aims at balancing transparency and high performance, ensuring that the resulting models are both understandable and effective.\nContributions. In this paper, we present a novel approach that bridges the gap between explainable and interpretable reinforcement learning. Our main contribution is the development of an approach that leverages insights from explainable models to derive interpretable policies. In particular, instead of focusing on the local explanations provided by explainable models, the proposed model-agnostic approach aims to achieve highly transparent and interpretable policies without sacrificing model performance. Additional contributions include the application of the new approach to both off-policy and on-policy RL algorithms and the creation of three adaptations to deep RL methods that learn interpretable policies using insights from model explanation. Finally, we evaluate the effectiveness of our framework in two environments to demonstrate its effectiveness in generating interpretable policies."}, {"title": "Related Work", "content": "One popular approach used in explainable artificial intelligence (XAI) is to use Shapley values that provide a quantitative measure of the contributions of features to the output (\u0160trumbelj and Kononenko 2010, 2014). In (Ribeiro, Singh, and Guestrin 2016), a method, called LIME, was proposed based on local surrogate models that approximate the predictions made by the original model. In (Wachter, Mittelstadt, and Russell 2017), the counterfactual is introduced into XAI by producing a perturbation input to change the original prediction to study the intrinsic causality of the model. In (Lundberg and Lee 2017), the idea of SHAP was proposed to unify various existing feature attribution methods under a single theoretical framework based on Shapley values, providing consistent and theoretically sound explanations for a wide range of machine learning models.\nMost existing explainable methods in RL adopt similar concepts from deep learning via framing the observation as input while the action or reward is the output. In (Beechey, Smith, and \u015eim\u015fek 2023), on-manifold Shapley values were proposed to explain the value function and policy that offers more realistic and accurate explanations for RL agents. In (Olson et al. 2021), the counterfactual state explanations were developed to examine the impact of altering a state image in an Atari game to understand how these changes influence action selection. As RL possesses some unique challenges, such as sequential decision-making under a reward-driven framework, specialized methods have been considered for its explanation. For example, in (Juozapaitis et al. 2019), reward decomposition was proposed to break down a single reward into multiple meaningful components, providing insights into the factors influencing an agent's action preferences. Moreover, understanding the action selection in certain critical states of the entire sequence can enhance user trust (Huang et al. 2018). A summary of important yet not similar sets of states (trajectories) can provide a broader and more comprehensive view of agent behavior (Amir and Amir 2018).\nIn contrast to the XRL, research in interpretable RL usually focuses on the transparency of the decision-making processes via, e.g., a simple representation of policies that are understandable to non-experts. The corresponding studies can be divided into direct and indirect approaches (Glanois et al. 2024). The direct approach aims to directly search a policy in the environment using the policy deemed interpretable by the designer or user. Examples of the direct methods include the use of decision tree (Silva et al. 2020b) or a simple closed-form formula (Hein, Udluft, and Runkler 2018) to represent the policy. The direct approach usually requires a prior expert knowledge for initialization to achieve good performance, often for small-scale problems. On the other hand, the indirect approach provides more flexibility by employing a two-step process: (1) train a non-interpretable policy with efficient RL algorithms, and (2) convert this non-interpretable policy into an interpretable one. For instance, Bastani, Pu, and Solar-Lezama (2018) proposed VIPER, a method to learn high-fidelity decision tree policies from original DNN policies. Similarly, Verma et al. (2018) proposed PIRL, a method that presents a way to transform the neural network policy into a high-level programming language. Our proposed methods can be categorized into indirect interpretable approaches using Shapley values to transform original policies into simpler but rigorous closed-form function policies. Distinguishing ourselves from existing indirect interpretation approaches, we uniquely incorporate the Shapley value explanation method to generate more accurate and generalizable interpretable policy without relying on predefined interpretable structures."}, {"title": "Background", "content": "In Reinforcement Learning, an agent interacts with its environment, which is modeled as a Markov Decision Process (MDP) defined by the tuple (S, A, P, r, \u03b3, do), where S is the set of states and A is the set of possible actions, P:S\u00d7A\u00d7S \u2192 [0, 1] is the transition probability function, r : S \u00d7 A \u2192 R is the reward function, \u03b3\u2208 [0, 1] is discount factor, and do : S \u2192 [0, 1] specifies the initial state distribution. At time step t, the agent observes the current state st \u2208 S and performs an action at \u2208 A. In response, the environment transitions to a new state st+1 ~ P(St, at) and provides a reward rt+1. The agent's objective is to learn a policy (i.e., strategy) \u03c0 that maximizes the expected return \u0395\u03c0[Gt], where Gt = \u2211n=tYrn+1. In RL, policies can be deterministic \u03c0 : S \u2192 A or stochastic \u03c0 : S \u00d7 A \u2192 [0, 1]. consider an environment with n state features, where S = S\u2081 X \u00d7 Sn, and each state can be represented as an ordered set s = {Si|Si \u2208 Si}=1. Using N = {1, ..., n} to represent the set of all state features, a partial observation of the state can be denoted as the ordered set sc = {si|i \u2208 C'} where CC N."}, {"title": "Shapley Values in Reinforcement Learning", "content": "The Shapley value (Shapley 1953) is a method from cooperative game theory that distributes credit for the total value v(N) earned by a team N among its players. It is defined as:\n$\\Phi_{i}(v)=\\sum_{C \\subseteq N \\backslash\\{i\\}} \\frac{|C| !(n-|C|-1) !}{n !}[v(C \\cup\\{i\\})-v(C)],$ (1)\nwhere v(C) represents the value generated by a coalition of players C. The Shapley value \u03c6\u2081(v) is the average marginal contribution of player i when added to all possible coalitions C.\nIn RL, the state features {$1,..., Sn} can be treated as players, and the policy output \u03c0(s) can be viewed as the total value generated by their contributions. To compute the Shapley values of these players, it is essential to define a characteristic function v(C) that reflects the model's output for a coalition of features sc\u2286 $1,..., Sn.\nAs the trained policy is undefined for partial input sc, it is important to correctly define the characteristic function for accurate Shapley values calculation. Following the on-manifold characteristic value function (Frye et al. 2021; Beechey, Smith, and \u015eim\u015fek 2023), we account for feature correlations rather than assuming independence.\nFor a deterministic policy \u03c0 : S \u2192 A, which outputs actions, the characteristic function is defined as:\nv\" (C) := \u03c0c(s) = \u2211 p (s'|sc)\u03c0(s'),\ns'ES\n(2)\nwhere s' = sc U s'\u0109 and p\u2122(s'|sc) is the probability of being in state s' given the limited state features sc is observed following policy \u03c0. Similarly, for a stochastic policy \u03c0 : S \u00d7 A \u2192 [0, 1], which outputs action probabilities, the characteristic function is defined as:\nv\" (C) := \u03c0c(als) = \u2211p\" (s'|sc)n(a|s'). (3)"}, {"title": "Method", "content": "In this section, we present our proposed methods in two main parts. First, Shapley vectors analysis focuses on extracting and capturing the underneath patterns provided by Shapley values. Secondly, interpretable policy formulation focuses on utilizing these patterns to construct interpretable policies with comparable performance. The complete algorithm is provided in Algorithm 1."}, {"title": "Shapley Vectors Analysis", "content": "Given a well-trained policy \u03c0(s) (deterministic) or \u03c0(as) (stochastic) in RL, Shapley values provide a way to explain the policy's behavior by quantifying the contributions of state features to the RL policy. Following the Shapley values methods (Beechey, Smith, and \u015eim\u015fek 2023), we substitute (2) or (3) into the Shapley value formula, namely, (1), to compute \u03c6\u03b9(v", "\u03c6\u2081(v": "provide insight into how each state feature i influences action selection. For example, in an environment with two discrete actions, a1 -1 and a2 1. After computing the Shapley value \u03c6\u2081(\u03c5\u03c0), a positive \u03c6\u03b9(v", "variance": "n$\\arg \\min _{A} \\sum_{i=1}^{k} \\sum_{\\Phi_{s} \\in A_{i}}\\left\\|\\Phi_{s}-\\mu_{i}\\right\\|^{2},$ (5)\nwhere \u00b5i is the centroid of points in Ai, usually represented as \u03bc\u03b5 = A \u03a3\u03a6\u0395\u0391; \u03a6\u03c2.\nBoundary Point Identification. Once clusters are formed, the boundaries between action regions can be identified using boundary points. A boundary point X"}, {"title": null, "content": "exists at the interface of two clusters A\u00bf and Aj, where the policy is equally likely to select either action. This condition arises when the policy is not sure which action to take at the current state, and therefore can serve as a boundary decision. Formally, X is found by minimizing the difference between distances to cluster centroids:\n$\\arg \\min _{X}\\left(\\left\\|X-\\mu_{i}\\right\\|^{2}-\\left\\|X-\\mu_{j}\\right\\|^{2}\\right),$ (6)\nwhere \u03bci and \u00b5; are the centroid of points in A\u017c and Aj, respectively.\nProperty 1 (Existence and Uniqueness of Decision Boundaries). For a stationary deterministic policy within an MDP, characterized by a fixed state distribution d(s), there exists a unique boundary surface in the Shapley vector space such that: (i) the boundary separates the Shapley vectors associated with distinct discrete actions, and (ii) the Euclidean distance from any action's Shapley vector to this boundary remains constant across all states under the stationary policy.\nProof. The efficiency property of Shapley values ensures that the sum of contributions from all features equals the difference between the policy's action value for state s and the expected action value across states, i.e.,\n$\\sum_{i=1}^{n} \\Phi_{i}=\\pi(s)-E_{s}(\\pi(S)).$(7)\nFor states sp and sq that lead to different action selection \u03c0(8p) = ap and \u03c0(sq) = aq, where ap \u2260 aq, the difference between their action values defines a gap given by\n$\\left|\\pi\\left(s_{p}\\right)-\\pi\\left(s_{q}\\right)\\right|=\\left|a_{p}-a_{q}\\right| \\Delta a.$(8)\nGiven that the policy \u03c0 is stationary with a fixed state distribution \u00b5(s), the expected action value converges to a fixed scalar value given by\n$E_{s} \\mu[\\pi(S)]=\\frac{1}{|S|} \\sum_{s \\in S} \\pi(s)=\\bar{a}.$(9)\nBy substituting (8) and (9) into the efficiency property (7), the Shapley value that sums for all states satisfy a gap\n$\\sum_{i=1}^{n} \\Phi_{i, s_{p}}-\\sum_{i=1}^{n} \\Phi_{i, s_{q}}=\\Delta a, \\forall s_{p}, s_{q} \\in S,$ (10)\nwhere \u03c0(8p) = \u03b1\u03c1 \u2260 \u03c0(sq) = aq. This implies that the gap Aa exists between all states with different action selections. Consequently, we defined the boundary surface B in the Shapley vector space as\n$B=\\left\\{V \\in \\mathbb{R}^{n} \\mid \\sum_{i=1}^{n} v_{i}=\\bar{a}+\\frac{\\Delta a}{2}\\right\\}$(11)\nThe distance from any Shapley vector plane to this boundary surface B is given by\n$d i s t(\\Phi, B)=\\frac{\\left|\\sum_{i=1}^{n} \\Phi_{i}-\\sum_{i=1}^{n} v_{i}\\right|}{\\sqrt{n}}$(12)\nTherefore, for all states, Sp, Sq \u2208 S, the distances from their Shapley vectors to the boundary remain constant:\ndist(sp, B) = dist(sq, B) (13)\nThis proves the existence and uniqueness of the decision boundary in the Shapley vector space. The constant distance between the boundary surface and Shapley vector plane lays the foundation for an interpretable policy that maps each action region to its corresponding state region."}, {"title": "Interpretable Policy Formulation", "content": "With the decision boundary point's identification in the Shapley vector space, the next step is to map it back to the original state space to construct an interpretable policy.\nInverse Shapley Values. To reconstruct the decision boundary in the state space, we model it as the Inverse Shap-ley Value Problem \u00a2\u00a1\u00af\u00b9 : \u03c6i(v) \u2192 {i}, where the goal is to recover the original state s corresponding to a given Shapley vector \u03a6s. We address this problem by systematically storing the original states with their corresponding Shapley value vectors, enabling efficient inverse function operations. It allows us to map Shapley value vectors back to their original states directly, facilitating precise reconstruction of the decision boundary.\nDecision Boundary Regression. After the boundary state points sij are discovered using Shapley values, the decision can be drawn accordingly. While a variety of regression techniques can be used, we use linear regression due to its simplicity and interpretability. The resulting boundary functions fij define the action regions.\nThis policy is then reformulated by assigning actions based on the regions characterized by boundary functions. Specifically, for a given states, the action a is determined by the cluster in which s resides relative to fij."}, {"title": "Experiments", "content": "To evaluate the effectiveness of our proposed method, we performed experiments across two classical control environments from Gymnasium (Towers et al. 2024): CartPole and MountainCar. These environments were specifically chosen as they represent an important control problem where policy interpretability is crucial for real-world deployment. To demonstrate the generality of our framework, we applied it to both off-policy and on-policy deep RL algorithms. Specifically, we applied it to Deep Q-Network (DQN) (Mnih et al. 2015b) as an off-policy method, and Advantage Actor-Critic (A2C) (Mnih et al. 2016) and Proximal Policy Optimization (PPO) (Schulman et al. 2017) as on-policy methods. Our experimental results demonstrate that the interpretable policies generated by our method perform competitively to those of deep RL algorithms, and also exhibit better stability and broad applicability."}, {"title": "CartPole", "content": "The CartPole environment is a classic control problem in which an inverted pendulum is placed on the movable cart. The state space in this environment consists of four features: position of cart x, velocity of cart \u017c, angle between the pendulum and the vertical 0, and angular velocity of pendulum 9. The action space includes two discrete actions, where the first action 0 means push the cart to the left, and the second action 1 means push to the right. A reward of +1 is assigned for each timestep the pole remains upright. The goal in this environment is to balance the pendulum by applying forces in the left and right direction on the cart.\nAs explained in method (Section 4), our goal is to obtain an interpretable policy for this problem. To achieve this, we first train three deep RL methods, namely DQN, PPO, and A2C to obtain the optimal policies. Once the models were trained, we evaluated their performance in the CartPole environment and sampled state distributions from 100 trajectories for each algorithm. For each sampled state, we computed the Shapley values of its features using Equation (1). With this step, we construct a Shapley value vector Is that represents the contribution of state features to this policy's decision. The first row of Figure 1, illustrates the Shapley value vectors for DQN, PPO, and A2C, respectively. Using these Shapley values, we performed k-means clustering on the action space to identify cluster centroids, where each cluster represents a distinct action region. Each cluster is depicted in a different color. We then identified boundary points, which are shown in red in the first row of Figure 1. These boundary points indicate the transition between action regions.\nNext, we reconstructed the decision boundary in the original state space using the boundary points identified in the Shapley vector space. The second row of Figure 1 shows these boundaries in the state space for each algorithm. Finally, as described in the methodology, we applied linear regression to derive an interpretable policy fij. The interpretable policies for DQN, PPO, and A2C are summarized in Table 1. These policies are obtained through their boundaries which separate the states into different action selection regions. In other words, the decision rule for these policies is: if f01 > 0, select action 0; otherwise, select action 1. This interpretable policy framework is fully transparent, enabling reproducibility and mitigating risks in high-stakes real-world applications.\nTo evaluate the performance of the interpretable policies, we tested them alongside the original deep RL policies over 10 episodes. The results, shown in Figure 2, demonstrate that the interpretable policies consistently achieved the maximum reward of 500 across all algorithms. This indicates that our method preserves the performance of the original deep RL algorithms while providing interpretability. These results also highlight the generality and model-agnostic nature of the proposed framework."}, {"title": "Mountain Car", "content": "The MountainCar environment is another classic control problem where a car is placed at the bottom of a sinusoidal valley. The state space for this environment consists of two features: car position along the x-axis x and the velocity of the car x. The actions space contains two discrete actions: action 0 applies left acceleration on the car and action 1 applies right acceleration on the car. The goal of this environment is to accelerate the car to reach the goal state on top of the right hill. A reward of -1 is assigned for each timestep as punishment if the car fails to reach the goal state.\nFollowing the proposed method (section 4), we perform the Shapley vectors analysis in three trained deep RL methods DQN, PPO, and A2C in the MountainCar environment. The result is shown in the first row of Figure 4. Each cluster represents a distinct action region, distinguished by a unique color and boundary points are highlighted in red. By mapping these boundary points back to the original state space, we constructed the decision boundaries using linear regression, illustrated in the second row of Figure 3 as blue lines. The detailed interpretable policies for DQN, PPO, and A2C are in Table 2 and the decision rule is straightforward: when f01 > 0, action 0 is chosen, otherwise, action 1 is chosen.\nPerformance of the interpretable policies alongside the original algorithms was evaluated over 10 episodes, with results presented in Figure 4. Interestingly, interpretable policies derived from PPO and A2C surprisingly outperformed their original algorithms, whereas the interpretable policy generated from DQN experienced a slight performance reduction. A notable observation is that all interpretable policies achieved significantly smaller standard deviations compared to their original counterparts, indicating more stable policy performance. This characteristic is particularly valuable in real-world applications where consistent and predictable behavior is crucial."}, {"title": "Conclusions and Future work", "content": "In this paper, we formalized and addressed the unsolved problem of extracting interpretable policies from explainable methods in RL. We propose a novel approach that leverages Shapley values to generate transparent and interpretable policies for both off-policy and on-policy deep RL algorithms. Through comprehensive experiments conducted in two classic control environments using three deep RL algorithms, we demonstrated that our proposed method achieves comparable performance while generating interpretable and stable policies.\nOur future work will include: (1) extending the current approach to continuous action spaces by discretizing the action space, (2) conducting a scalability study of the proposed approach in more complex environments with higher-dimensional state feature spaces, and (3) exploring performance differences across various regression methods."}]}