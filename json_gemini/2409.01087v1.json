{"title": "Pre-Trained Language Models for Keyphrase Prediction: A Review", "authors": ["Muhammad Umair", "Tangina Sultana", "Young-Koo Lee"], "abstract": "Keyphrase Prediction (KP) is essential for identifying keyphrases in a document that can summarize its content. However, recent Natural Language Processing (NLP) advances have developed more efficient KP models using deep learning techniques. The limitation of a comprehensive exploration jointly both keyphrase extraction and generation using pre-trained language models spotlights a critical gap in the literature, compelling our survey paper to bridge this deficiency and offer a unified and in-depth analysis to address limitations in previous surveys. This paper extensively examines the topic of pre-trained language models for keyphrase prediction (PLM-KP), which are trained on large text corpora via different learning (supervisor, unsupervised, semi-supervised, and self-supervised) techniques, to provide respective insights into these two types of tasks in NLP, precisely, Keyphrase Extraction (KPE) and Keyphrase Generation (KPG). We introduce appropriate taxonomies for PLM-KPE and KPG to highlight these two main tasks of NLP. Moreover, we point out some promising future directions for predicting keyphrases.", "sections": [{"title": "1. Introduction", "content": "To determine if a keyphrase is present in a document, it must appear as a single contiguous word. Keyphrase extraction involves using a model to accurately identify and classify the keyphrases in the document. The generation of keyphrases is another task in which the model predicts both present and absent keyphrases within the context of the document, introduced in [1]. The application of deep learning technologies has witnessed a noticeable rise in using pre-trained language models (PLMs) in NLP in recent years. PLMs are trained using different strategies on extensive text corpora and have shown exceptional performance in various downstream tasks, including Keyphrase Predation. PLMs using self-supervised learning differ from traditional learning methods, such as supervised learning, because they are first trained on a large volume of unlabeled data before fine-tuning small quantities of labeled data for specific tasks. Self-supervised learning-based PLMs, contrary to conventional learning methods like supervised learning, are pre-trained on vast amounts of unlabeled data before being fine-tuned on tiny amounts of labeled data for particular tasks. In the realm of NLP, BERT [2], GPT [3], and T5 [4] are some of the notable works that have consistently updated benchmark records in Pre-trained Language Model Keyphrase Extraction (PLM-KPE) and Pre-trained Language Model Keyphrase Generation (PLM-KPG) tasks [5], contributing significantly to the development of NLP.\nThe process of extracting keyphrases from a document involves identifying and extracting significant phrases that represent the main topics or concepts discussed within it. The primary objective is to extract the most essential and representative phrases using feature-based [6, 7, 8, 9, 10] and linguistic techniques [11] like frequency analysis [12], part-of-speech tagging [13, 14], and syntactic parsing [15]. These methods can identify keyphrases based on their frequency, relevance, or structural patterns within the text, allowing for a more thorough analysis and understanding of the document's contents. On the other hand, the keyphrase generation involves creating new phrases that are not present in the original document. The objective is to generate concise and meaningful phrases that encapsulate the central concepts or themes discussed in the document. Keyphrase generation methods rely on language modeling [16, 4, 17], neural networks [18], or rule-based systems [19] to generate coherent and significant keyphrases.\nTable 1 provides an illustrative example of the keyphrase prediction process, which involves detecting two distinct categories of keyphrases in a given document. The first category comprises keyphrases that are consistently present throughout the document, while the second category encompasses those that cannot be found in any contiguous subsequence of the document. In the past, researchers primarily focused on keyphrase extraction, which sought to extract keyphrases directly from the document to improve keyphrase prediction. However, with the emergence of deep learning, researchers are currently exploring keyphrase production using dominant models that can generate both present and absent keyphrases.\nThe limitations in the existing literature become evident as we observe the absence of a comprehensive exploration that integrates both keyphrase extraction and generation using pre-trained language models, leaving a critical gap in understanding the unified process of keyphrase prediction. Previous studies have offered fragmented insights, often focusing solely on extraction or generation, thus limiting a holistic understanding of the field. Additionally, the need for standardized taxonomies further compounds the issue, creating confusion and hindering the systematic exploration of keyphrase prediction methods. These limitations emphasize the necessity for our survey paper, which aims to bridge these gaps by providing a unified and comprehensive analysis, introducing clear taxonomies, and exploring promising future directions to advance the understanding and application of keyphrase prediction in Natural Language Processing.\nThe objective of this paper is to furnish a succinct synopsis of the current methodologies employed for keyphrase extraction and generation, focusing on the utilization of pre-trained language models. To avoid ethical concerns, we avoid proposing new models and instead attempt to enhance the research community's understanding in this domain. The main emphasis of this research is on the latest developments in predicting keyphrases using neural networks into language models that can promote both PLM-KPE and PLM-KPG tasks, and these two areas have different focuses. We aim to present a comprehensive review of Pre-trained Language Models Keyphrase prediction (PLM-KP) in the two areas to provide respective insights into PLM-KP in PLM-KPE and PLM-KPG. The survey's primary contributions can be summarized as follows.\n1. Our survey paper addresses the gaps in the literature by providing a unified and comprehensive analysis that bridges the divide between keyphrase extraction and generation. This approach enhances the overall understanding of keyphrase prediction in the context of Natural Language Processing.\n2. We introduce structured taxonomies for both PLMs for keyphrase prediction (PLM-KP) and its constituent tasks (KPE, KPG, and Multimodal) in Figure. 2 shows our proposed taxonomies. These taxonomies offer a clear and systematic framework, facilitating a cohesive classification of methods and techniques."}, {"title": "2. Preliminary", "content": "This section outlines the distinctions between our current survey and those conducted previously. Following this comparison, we will outline the specific research criteria that guided the formulation and focus of our paper."}, {"title": "2.1. Previous Surveys", "content": "The comprehensive review of keyphrase extraction [20] highlights KPE methods, categorizing them into unsupervised and supervised approaches. In contrast, our paper includes more recent studies published explicitly in keyphrase generation tasks and multimodal keyphrase extraction and generation.\nAdvancements in pre-trained language models have significantly contributed to both keyphrase extraction and generation tasks. Unlike a recent keyphrase extraction survey [21], which mainly includes supervised and unsupervised keyphrase extraction in two stages without covering keyphrase generation, our paper is a comprehensive resource for both novices and experts. We have consolidated information on both KPE and KPG in one location. Furthermore, our paper reviews 22 more recent articles than [22], ensuring it is up-to-date. [23, 24, 25, 26, 27, 28, 29] Unlike previous surveys, our work uniquely summarizes SOTA attention models in Section 7.1, aiming to enhance the research community's understanding in this domain. This comprehensive overview serves as a pivotal resource, providing insights that are critical for advancing keyphrase extraction and generation research.\nExisting surveys on keyphrase prediction primarily focus on early feature-engineered and neural-based keyphrase extraction models. However, no current and comprehensive survey provides detailed knowledge of KPE and KPG tasks that leverage pre-trained language models. Our work fills this gap by thoroughly reviewing the latest advancements in the field."}, {"title": "2.2. Research criteria", "content": "Efficient and reliable methods for gathering relevant literature are crucial when conducting a survey. Our approach involves utilizing query-based criteria across a range of reputable online databases, which are listed in Table 2. Additionally, we meticulously examine respected journals, conferences, and workshops to analyze work related to keyphrases from various perspectives in a comprehensive way.\nThe survey selection process comprises three essential steps for databases and targeted journals/conferences. Initially, we use primary keywords, such as \"keyphrase extraction,\" \"keyphrase generation,\u201d and \"pre-trained language models based keyphrase,\" to conduct formal searches sequentially across the chosen databases and target domain journals and conferences. Potential works are then critically evaluated based on predetermined criteria for inclusion. Lastly, we carefully consider each paper's titles, abstracts, and full texts during each formal search. As demonstrated in Table 4, a significant amount of academic literature is available on keyphrase extraction and generation, with a strong presence at leading computer science conferences. This finding suggests continued interest and ongoing research in this field of study over an extended period."}, {"title": "3. Background and Summaries of LLMs", "content": "The surge in PLM advancements in NLP has significantly influenced keyphrase prediction (KP) methodologies. This section elucidates the comprehensive journey from raw text input through various processing stages, leveraging PLMs for KP, as depicted in Figure 1. Moreover, this section sheds light on several state-of-the-art PLMs, including BERT, Roberta, ELMo, Llama 2, T5, and GPT. It provides comparative analysis and summaries of their functionalities, strengths, and potential in KP tasks. By integrating these PLMs, significant strides have been made in both KPE and KPG, showcasing their indispensable role in enhancing NLP tasks."}, {"title": "3.1. Background", "content": "The following section will dive into a step-by-step breakdown of Figure 1, expounding each component and process involved in keyphrase extraction and generation using pre-trained language models.\n\u2022 Step 1: Input from Documents in the initial step, raw textual data from documents serves as input. This unprocessed text provides the foundational material from which keyphrases will be extracted or generated.\n\u2022 Step 2: Processing (POS Tagging, Noun Phrase Chunking, Tokenization, Stop Word Removal, Stemming, and Lemmatization) In this phase, the input text undergoes a series of preprocessing steps to enhance its structure and facilitate meaningful analysis. These steps include Part-of-Speech (POS) tagging [30, 31, 32, 33], which identifies grammatical categories of words; noun phrase chunking [33, 34, 35], isolating noun phrases for analysis; tokenization [36, 37, 38], breaking down the text into individual tokens; removal of stop words [39, 40], eliminating common words with limited semantic meaning [41]; stemming [42, 43, 44], reducing words to their root form; and lemmatization [45, 44, 46], reducing words to their base or dictionary form. Each of these processes refines the text, preparing it for further analysis.\n\u2022 Step 3: Embedding (Vector representation) Following pre-processing, the processed text is transformed into vector representations [47, 48, 49]. This embedding step converts words or phrases into numerical vectors, capturing their semantic meanings in a multidimensional space. Embeddings facilitate the modeling of word relationships and context, enabling the pre-trained language models to grasp the nuances of the text.\n\u2022 Step 4: Pre-Trained Language Models (Extraction or Generation) In this pivotal phase, the processed and embedded text is fed into pre-trained language models. These sophisticated models, often based on transformer architectures, can understand complex linguistic patterns. The models identify and extract relevant phrases from the input text for keyphrase extraction. Alternatively, for keyphrase generation, the models create new, contextually relevant keyphrases based on the learned patterns and embeddings.\n\u2022 Step 5: Post-Processing (Diversity and Novelty Check) After the keyphrases are extracted or generated, a post-processing [50] step ensues. Here, the extracted keyphrases undergo a diversity [51, 52, 53, 33] and novelty check. This ensures that the selected keyphrases are pertinent to the document's context and unique and varied [31]. Post-processing [50, 54, 55] is crucial in refining the final selection of keyphrases, enhancing their relevance and richness.\n\u2022 Step 6: Extracted or Generated Keyphrases The culminating step presents the output, the extracted or generated keyphrases. These keyphrases, refined through careful preprocessing, embedding, modeling, and postprocessing, capture the essence of the document. They serve as concise and meaningful representations of the document's content, facilitating efficient information retrieval and understanding."}, {"title": "3.2. Summaries of Large Language Models", "content": "To better understand the advancements in language models and their impact on keyphrase prediction, we have compiled a comparative analysis (see Table 3). This analysis delineates six pivotal models' unique features, strengths, and limitations. BERT [2], ROBERTa [56], ELMo [57], Llama2 [58], T5 [4], and GPT [59]. Each model brings distinct advantages to the KP domain. Moreover, pre-trained language models [60, 16, 61, 62, 63, 64] have significantly advanced the current state of NLP tasks. As a result, they are now being integrated into methods for both KPE and KPG, such as the notable examples are the use of PLM-KP for unsupervised KPE [33, 38]. In this approach, KPE is achieved through sequence labeling, [65, 66], while KPG utilizes the sequence-to-sequence technique [67, 68, 69, 70, 71, 72]. Notable advancements have been made by models such as ELMo [57], and BERT [2], which are based on LSTM and Transformer architectures [73], respectively. Transformer-based models, in particular, utilize a masked language model and sentence adjacency training objectives to learn bidirectional representations of words.\n\u2022 BART (Bidirectional and Auto-Regressive Transformers) leverages a novel pre-training objective of text infilling, where a certain percentage of the input text is masked, and the model learns to predict the masked portions. This approach, combined with its bidirectional context understanding, makes BART highly effective for KP tasks, as it can generate coherent and contextually relevant keyphrases by comprehensively understanding the given text's nuances.\n\u2022 ROBERTa (Robustly Optimized BERT Approach) builds upon the original BERT framework through optimized training strategies, including dynamic masking and extended training over more data. This enhancement enables Roberta to outperform its predecessor in understanding and generating text, making it highly efficient for KP tasks. ROBERTa's improved contextual embeddings contribute significantly to extracting semantically rich and relevant keyphrases from complex documents, demonstrating its prowess in capturing the essence of textual content.\n\u2022 ELMO (Embeddings from Language Models) introduces the concept of deep contextualized word representations, where the meaning of each word can change based on the surrounding text. This feature is particularly beneficial for KP, as it allows for extracting keyphrases that are contextually aligned with the document's overall theme. ELMo's ability to account for word-level nuances dramatically enhances the precision of keyphrase identification, making it a valuable asset for tasks requiring deep semantic understanding.\n\u2022 Llama2 (Language Model from Meta AI), though relatively new, has shown promising capabilities in understanding and generating natural language text. Its training on diverse datasets allows for a broad comprehension of language nuances, making it adept at generating coherent and context-relevant keyphrases. Llama's versatility and scalability position it as an emerging tool for KP tasks, potentially offering novel approaches to keyphrase extraction and generation.\n\u2022 T5 (Text-to-Text Transfer Transformer) adopts a unified framework that converts all NLP problems into a text-to-text format, whether translation, summarization, or keyphrase generation. This simplification allows T5 to leverage its vast pre-trained knowledge, fine-tuned with task-specific data, to produce highly relevant and accurate keyphrases. Its capacity to understand and generate text based on the provided prompts makes it particularly useful for both extraction and generation tasks within KP."}, {"title": "4. PLMs for KPE", "content": "To provide a comprehensive overview of the prevalent methods used in keyphrase extraction, Table 5 presents a comparative analysis focusing on the advantages and disadvantages of each technique. This analysis emphasizes the importance of selecting an appropriate keyphrase extraction method based on the specific requirements and constraints of the task.\nCategorization of Keyphrase Prediction using Pre-trained Language Models (PLM-KP), centered around two fundamental NLP tasks: Pre-trained Language Model Keyphrase Extraction (PLM-KPE) and Pre-trained Language Model Keyphrase Generation (PLM-KPG) is shown in Figure 2."}, {"title": "4.1. Attention Mechanism", "content": "Extensive research has demonstrated that attention mechanisms for keyphrase extraction using pre-trained language models have made significant contributions. UCPhrase [74] a novel approach for identifying high-quality phrases using silver labels derived from word sequences that frequently co-occur within documents. This method could significantly improve the accuracy and efficiency of phrase tagging. These contextually rooted silver labels enhance the preservation of contextual completeness and the capture of emerging domain-specific phrases. In contrast, AttentionRank [75] presents a hybrid attention model that uses self-attention and cross-attention mechanisms to assesIm-portancertance of the candidate and establish semantic relevance between candidates and document sentences. While UCPhrase excels at capturing quality phrases, AttentionRank showcases robust performance across various document lengths. Integrating accumulated self-attention and cross-attention within Attention-Rank paves the way for further investigations, such as domain-specific fine-tuning and comparisons against baselines on specific datasets."}, {"title": "4.2. Graph-based Ranking", "content": "Pioneering work has incorporated pre-trained language models into graph-based keyphrase extraction techniques, significantly enhancing the extraction process. Mahata et al. [76] introduces an innovative approach that uses phrase embeddings to rank keyphrases extracted from scientific articles, employing a theme-weighted PageRank methodology to achieve state-of-the-art results. Liang et al. [77] transcends traditional co-occurrence models by infusing semantic information into the graph structure using word embeddings, improving benchmark datasets' performance. Rafiei et al. [78] takes an integrative approach, using single- and multiword embeddings to construct embedding-based graphs, effectively capturing local and global context for high-quality keyphrase extraction in diverse textual domains. Yuchen et al. [79] introduce a unique boundary-aware centrality method to enhance local information capture within graph-based models, showcasing the power of pre-trained embeddings in refining both local and global context for superior keyphrase extraction performance. Together, these works underscore the transformative potential of pre-trained language models in advancing the effectiveness of graph-based keyphrase extraction techniques."}, {"title": "4.3. Semantic Importance", "content": "Semantic Importance, a pivotal aspect of keyphrase extraction, is harnessed by innovative research efforts that leverage pre-trained language models to uncover keyphrases' true significance and relevance in unsupervised settings. More recent attention has focused on the provision of semantic Importance. These two ground-breaking studies show how to use pre-trained language models to capture the semantic significance of keyphrases in unsupervised keyphrase extraction. Zhang et al. [80] The unique MDERank method, which employs a masking strategy and ranks potential keyphrases based on the similarity between the embeddings of the source material and the masked document, solves the shortcomings of existing techniques in dealing with lengthy documents. The paper also introduces KPEBERT, a keyphrase-oriented BERT model, and showcases how MDERank benefits from it, resulting in significant performance improvements. On the other hand, Rishabh et al. [81] introduce INSPECT, a unique methodology that uses self-explaining models to identify influential keyphrases by measuring their predictive impact on downstream tasks, such as document topic classification. INSPECT establishes itself as a state-of-the-art solution by avoiding heuristic importance, outperforming previous approaches across various datasets. These papers collectively emphasize the potential of pre-trained language models to enhance keyphrase extraction by capturing semantic relevancy and interpretability. [82] a new evaluation framework named KPEVAL, designed to assess the performance of keyphrase extraction systems more comprehensively than traditional exact match metrics. KPEVAL conducts semantic-based evaluation on reference agreement, faithfulness, diversity, and utility of keyphrase systems, providing a more comprehensive assessment compared to exact matching methods. The strong performance of LLMs, particularly GPT-3.5, in keyphrase prediction tasks encourages further exploration and improvement of Language Model Methods (LLMs) for keyphrase prediction."}, {"title": "4.4. Phrase-Documents Similarity", "content": "Phrase-document similarity, a critical factor in keyphrase extraction, serves as the foundation for various techniques that aim to accurately identify and rank keyphrases within text documents using the power of pre-trained language models. The significance of each potential phrase is typically determined by how well the phrase matches the representation of the document. [83] EmbedRank takes advantage of Sent2Vec [84] and Doc2Vec [85] techniques to transform candidates and input documents into vectors. In the pursuit of diverse keyphrases, EmbedRank+ delves into the interplay of candidate similarities. In contrast, SIFRank [86] crafts vector representations for candidates, sentences, and input documents using weighted averages, using ELMo embeddings [57]. Adding a layer of complexity, SIFRank+ considers candidate positions within the document. Notably, [87] enhances SIFRank's prowess by amalgamating domain relevance and phrase quality into the ranking framework. On a different note, [88] approach the challenge by harnessing entire documents to train Glove embeddings [89], subsequently assessing candidates based on cumulative word-document similarities."}, {"title": "5. Multimodal Keyphrase Extration and Generaion", "content": "The advent of multimodal data, which includes combinations of text, image, video, and audio, presents both challenges and opportunities for keyphrase extraction and generation. In recent years, the field of Natural Language Processing (NLP) has witnessed a growing interest in leveraging these diverse data types to enhance the understanding and generation of key textual elements like keyphrases. Multimodal keyphrase extraction and generation extend beyond traditional text-based approaches by incorporating additional modalities, thus aiming to achieve a more holistic understanding of content. This integration allows for the extraction and generation of keyphrases that are not only contextually richer but also more aligned with the nuanced interplay of visual and textual cues.\nSignificant advancements in multimodal keyphrase prediction are highlighted by pioneering models and methodologies that effectively harness the synergy of textual and visual information to refine the accuracy and applicability of keyphrase annotations. For instance, the integration of visual entity enhancement and multi-granularity image noise filtering has proven effective. These techniques improve the semantic alignment between visual and textual elements and ensure that only relevant visual data is considered during keyphrase generation. Rigorous experimental validations on benchmark datasets have demonstrated superior performance compared to existing methods, thereby confirming the effectiveness of these visual enhancements [90].\nMoreover, the introduction of the One2MultiSeq training paradigm and the CopyBART model marks another innovative approach, particularly in handling social media content. This methodology not only balances the model's attention across various keyphrase types but also significantly improves the ability to generate 'absent' keyphrases often overlooked by conventional methods. By dynamically adjusting to varied keyphrase orders and integrating textual and visual content, this model addresses the challenges posed by the noisy, real-world data of social media platforms [91].\nAdditionally, frameworks like SMART-KPE (Strategy-based Multimodal Architecture for KeyPhrase Extraction) utilize both micro-level visual features, such as font size and color, and macro-level features like webpage layout. This approach not only enhances keyphrase prediction but also provides a deeper understanding of content, making it particularly effective in the diverse and visually rich environment of open-domain web pages [92].\nFurthermore, incorporating cognitive signals derived from EEG and eye-tracking into Automatic Keyphrase Extraction (AKE) offers a novel approach to extract key information from microblogs. This multimodal strategy combines deep cognitive insights and surface-level attention details to significantly enhance AKE's accuracy, especially useful for the unstructured and vast content typical of social media [93].\nFinally, the unified framework utilizing Multi-Modality Multi-Head Attention (M3H-Att) in social media posts leverages both text and image data to capture the complex interactions between these inputs. By integrating OCR text and image attributes, this method not only boosts keyphrase prediction accuracy but also enriches the understanding of multimodal interactions, essential for processing social media content effectively [94].\nThese diverse approaches reflect the dynamic evolution of keyphrase extraction and generation methodologies, demonstrating the potential of multimodal data to enrich NLP applications. By integrating various data types, researchers and practitioners can achieve a more nuanced and comprehensive understanding of content, paving the way for more sophisticated and contextually aware NLP systems."}, {"title": "6. PLMs for KPG", "content": "Categorization of low-resource is shown in Figure 3. In the field of low-resource keyphrase generation, conventional approaches have typically relied on the application of semi-supervised or unsupervised learning techniques [95, 96, 97]. The amount and quality of available training data determines the effectiveness of keyphrase generation models. However, commonly used labeled datasets are often limited in size, making low-resource keyphrase generation an important and worthwhile research area. In this subsection, we will focus on the use of pre-trained language models for keyphrase generation in low-resource settings. A great deal of previous research [98, 99, 100, 101, 102, 72, 103, 104, 71, 105, 106], has shown that keyphrase generation is a challenging task in low-resource settings.\nIn the realm of Keyphrase Generation using Pre-trained Language Models, recent advancements have illuminated multiple strategies to tackle the challenges of low-resource scenarios. In particular, Lancioni et al. [107] introduces BeGan-KP, a GAN model for the generation of low-resource keyphrases. It features a BERT-based discriminator architecture that efficiently distinguishes between human-curated and generated keyphrases. BeGan-KP demonstrates competitive results on multiple datasets with less than 1% of the training data, making it effective in low-resource scenarios. Meanwhile, Di Wu et al. [72] presents a data-oriented approach for low-resource keyphrase generation. Leveraging retrieval-based statistics and pre-trained language models, it learns intermediate representations for improved performance. The proposed approach facilitates the generation of absent keyphrases and shows promising results even with limited training examples.\nResearch in this area has shown that data augmentation plays a critical role in low-resource settings for keyphrase generation. In particular, Ray et al. [99] introduces KPDROP, a model-agnostic approach to improve the generation of absent keyphrases. It achieves this by randomly dropping present keyphrases during training, encouraging the model to better infer relevant but absent keyphrases. KPDROP proves effective in improving both present and absent keyphrase generation performance in various settings. Expanding further, Cristina et al. [102] explores the adaptation of pre-trained language models for low-resource text simplification. It compares meta-learning and fine-tuning approaches and finds that structured intermediate adaptation steps lead to significant performance gains. This work paves the way for more comprehensive adaptive learning solutions. Building on this momentum, Krishna et al. [101] focuses on data augmentation strategies tailored for low-resource keyphrase generation. The methods utilize full-text articles to enhance the generation of present and absent keyphrases. These strategies consistently outperform existing approaches, demonstrating their effectiveness in resource-constrained scenarios. Complementing this, Jihyuk kim et al. [108] addresses keyphrase generation in scenarios where structure plays a pivotal role. The approach augments documents with related keyphrases and encodes structure-aware representations using graphs. This strategy significantly improves keyphrase generation for documents with varying structures. Extending the scope, Yifan et al. [109] tackles multilingual keyphrase generation using a retrieval-augmented approach. By leveraging English keyphrase annotations and cross-lingual retrieval, the model generates keyphrases in low-resource languages. The proposed iterative training algorithm for retrievers and generators further enhances cross-lingual retrieval. This method outperforms baselines and offers promising results in multilingual settings. These approaches collectively pave the way for innovative methods in the realm of keyphrase extraction and generation using pre-trained language models. [110] another SOTA technique can be presented under the umbrella of data augmentation. It introduces significant advancements through its DESEL (Decode-Select) algorithm, which enhances keyphrase generation by integrating the precision of greedy search with the recall benefits of sampling methods. DESEL first decodes a sequence using greedy search, then samples additional sequences to create a pool of candidate keyphrases, and selects the most probable phrases to improve overall prediction quality. This method effectively balances precision and recall, leading to superior performance across multiple datasets, demonstrating a refined approach to using pre-trained language models for keyphrase generation."}, {"title": "6.2. Domain-specific", "content": "There are different domain-specific PLMs that can be used for keyphrase generation. Historically, research efforts have focused on using these PLMs to achieve this particular aim. We have examined several publicly accessible PLMs that are specific to certain domains. To distinguish the level of knowledge incorporated by these PLMs, we have classified them into three types: web, academic, and social. This can be seen in the illustration provided in Figure 4.\nThere is a growing body of evidence that suggests that the web domain [111, 112, 113, 114] is heavily influenced by empirical studies exploring the impact of incorporating PLMs into it. One such study, Twitterroberta [111], consists of seven Twitter-specific classification tasks utilizing the pre-trained language model ROBERTa [56]. This model was trained from scratch using a Twitter corpus of 60 million tweets\u00b9, which were obtained through automatic labeling provided by Twitter2. The research found that using a PLM may be sufficient, but additional training on domain-specific data can further improve performance. Micro-blogging platforms such as Twitter, where users can share real- time information related to all kinds of topics and events. Recent evidence suggests that BERTweet [112] investigated the differential impact of domain-specific PLM ROBERTa [56], and XLM-R [115] on Part-of-speech tagging, Named-entity recognition, and text classification tasks of NLP. These studies clearly indicate the effectiveness of large-scale domain-specific PLMs for English tweets. The growing interest in generating domain-specific BERT-like PLM, another work, Hate-BERT [114], showed that abusive language phenomena fall along a wide spectrum, including, e.g., microaggression, stereotyping, offense, abuse, hate speech, threats, and doxxing [116]. It is a more robust model that obtains a higher precision score than BERT when fine-tuned to a generic abusive phenomenon [117].\nTo determine the effects of domain-specific PLM, [113, 118, 119] analyzed the multiple domains and showed that the adaptation of a hybrid approach incorporating four domains (Biomedical and computer science publications, news, and reviews) in PLM ROBERTa [56], highlights improved performance in tasks from the target domain. The study shows that domain-adaptive and task-adaptive pretraining lead to performance gains, even with limited resources. Additionally, the article provides alternative strategies for adapting to task corpora when domain-adaptive pre-trained is not feasible. The study consistently finds that multi-phase adaptive pretraining significantly improves task performance.\nThe second subcategory is academic, A number of existing works adopted PLMs [138, 139, 140, 141, 142, 143]. [138] This research offers fresh perspectives on biomedical text mining. After realizing that BERT [2], a well-known PLM, excels at general language tasks, it frequently provides wrong responses to straightforward biomedical queries. In order to enhance BERT's performance on biomedical text mining tasks such as question answering (QA), which highly relay on quality text generation [144], researchers pre-trained BERT on sizable biomedical corpora. In [139] ChemBERTa, a large-scale self-supervised pretrained model for molecular property prediction using transformers. The authors assess the effectiveness of transformers in learning molecular representations and predicting properties. They demonstrate competitive performance on MoleculeNet and use functional attention-based visualization techniques. SciBERT [140] is a sophisticated model that underwent rigorous training in a comprehensive corpus of 1.14 million biomedical and computer science papers from the Semantic Scholar database. This model was trained using unsupervised pre-training in the academic domain. The model's performance was evaluated in [103] through a series of tasks and datasets from scientific fields, which confirmed its position as the leading scientific keyphrase generation model in the industry. Recent developments in natural language processing have led to the creation of domain-specific language models like MatSciBERT [145] for the materials domain, which outperforms general-domain models in various tasks. Similarly, in the biomedical domain [141], starting from scratch with domain-specific pre-training yields significant improvements over using general domain language models, as demonstrated by the BLURB benchmark. These findings emphasizeImportancertance of domain-specific pre-training for keyphrase generation in specialized fields.\nFinlay, in this survey, we explore the application of domain-specific pre-trained language models for keyphrase generation in the social domain. Several studies [146, 147, 148, 149, 150, 151, 152] have addressed the need for specialized language models in various fields. [146] and [147] focus on clinical and financial domains, respectively, and demonstrate that domain-specific pre-training leads to improved performance on related NLP tasks. In [148, 153], the study explores the adaptation of BERT and LLMs in the legal domain and proposes different strategies for better results. [149] introduces a new dataset and shows the effectiveness of domain-adapted models for skill extraction. [150] introduces a large corpus of privacy policies to facilitate the creation of privacy-related models. Lastly, [151] presents SportsBERT, a domain-specific BERT model for the sports domain, emphasizing the benefits of training models tailored to specific domains. These findings collectively emphasizeImportancertance of domain-specific pre-training for social keyphrase generation tasks in specialized fields."}, {"title": "7. Models Comparison", "content": "To rigorously assess the efficacy of various keyphrase extraction and generation models, we meticulously selected one emblematic model from each methodological category for comparative analysis listed in Table 7. Our selection criteria were based on each model's prominence in the field, innovative approach, and reported performance in prior studies. To ensure a comprehensive evaluation that accounts for varying document complexities, we employed two distinct datasets: one comprising short documents, which presents a challenge in distilling key content, and another consisting of long documents, demanding effective processing of sizeable information. The detailed characteristics of these datasets, including their average document length, domain diversity, and dataset created_by, are delineated in Table 6. This selection strategy enables us to provide a balanced and insightful comparison that sheds light on the strengths and limitations of each model across different textual contexts."}, {"title": "7.1. Attention Adaptation", "content": "In this section", "Mechanisms": "Sparse attention mechanisms focus on specific parts of the input sequence", "input.[154": ".", "Attention": "Blockwise attention divides the input sequence into blocks and computes attention within each block", "155": ".", "Linformer": "Linformer approximates the full attention mechanism by projecting the input sequence into a lower-dimensional space, significantly reducing the computational complexity. This efficiency makes Linformer suitable for tasks requiring attention over long sequences, such as keyphrase extraction, where it"}]}