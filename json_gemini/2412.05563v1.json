{"title": "A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions", "authors": ["OLA SHORINWA", "ZHITING MEI", "JUSTIN LIDARD", "ALLEN Z. REN", "ANIRUDHA MAJUMDAR"], "abstract": "The remarkable performance of large language models (LLMs) in content generation, coding, and common-sense reasoning has spurred widespread integration into many facets of society. However, integration of LLMs raises valid questions on their reliability and trustworthiness, given their propensity to generate hallucinations: plausible, factually-incorrect responses, which are expressed with striking confidence. Previous work has shown that hallucinations and other non-factual responses generated by LLMs can be detected by examining the uncertainty of the LLM in its response to the pertinent prompt, driving significant research efforts devoted to quantifying the uncertainty of LLMs. This survey seeks to provide an extensive review of existing uncertainty quantification methods for LLMs, identifying their salient features, along with their strengths and weaknesses. We present existing methods within a relevant taxonomy, unifying ostensibly disparate methods to aid understanding of the state of the art. Furthermore, we highlight applications of uncertainty quantification methods for LLMs, spanning chatbot and textual applications to embodied artificial intelligence applications in robotics. We conclude with open research challenges in uncertainty quantification of LLMs, seeking to motivate future research.", "sections": [{"title": "1 Introduction", "content": "Large language models have demonstrated remarkable language generation capabilities, surpassing average human performance on many benchmarks including math, reasoning, and coding [1, 7, 23, 34, 47, 204]. For example, recent (multi-modal) large language models were shown to achieve impressive scores, e.g., in the 90% percentile, on simulated Law School Admission Test (LSAT) exams, the American Mathematics Competition (AMC) contests, the Multistate Bar Exam, and the Graduate Record Exam (GRE) General Test, outperforming a majority of test takers [1, 7, 96]. Likewise, LLMs have advanced the state of the art in machine translation, text summarization, and question-and-answer tasks. However, LLMs also tend to produce plausible, factually-incorrect responses to their input prompts, termed hallucinations [111]. In some scenarios, the hallucinated response is overtly incorrect; however, in many cases, the factuality of the LLM response is harder to discern, posing significant risk as a user might falsely assume factuality of the response, which can result in devastating consequences, especially when safety is of paramount importance. As a result, hallucinations pose a notable danger to the safe, widespread adoption of LLMs.\nTo ensure the trustworthiness of LLMs, substantial research has been devoted to examining the mechanisms behind hallucinations in LLMs [11, 31, 84, 111, 224], detecting its occurrence, identifying potential causes, and proposing mitigating actions. However, even in the absence of hallucinations, LLMs are susceptible to doubt when given prompts at the boundary of their knowledge base. In these situations, prior work has shown that LLMs fail to accurately convey their uncertainty to a user, either implicitly or explicitly, unlike typical humans [6, 132]. In fact, LLMs tend to be overconfident even when they should be uncertain about the factuality of their response [66, 222]. We provide an example in Figure 1, where an LLM is asked: \"What is the lowest-ever temperature recorded in Antarctica?\", to which"}, {"title": "Comparison to other Surveys", "content": "A number of surveys on hallucinations in LLMs exists, e.g., [14, 80, 126, 171, 203]. These surveys discuss hallucinations in detail, introducing the notion of hallucinations [171], identifying its types and potential causes [80], and presenting mitigation techniques [203]. However, these papers provide little to no discussion on uncertainty quantification methods for LLMs, as this research area lies outside the scope of these surveys. In contrast, only two surveys on uncertainty quantification methods for LLMs exist, to the best of our knowledge. The first survey [61] categorizes confidence estimation and calibration methods into two broad classes: methods for generation tasks and methods for classification tasks, defined by the application domain. The survey in [61] focuses more heavily on calibration methods, with a less extensive discussion on confidence estimation methods. In contrast, our paper provides an extensive survey of uncertainty quantification methods with a brief discussion on calibration of uncertainty estimates. For example, whereas [61] lacks a detailed discussion on the emerging field of mechanistic interpretability, our survey presents this field in detail, along with potential applications to uncertainty quantification. Moreover, our survey discusses a broad range of applications of uncertainty quantification methods for LLMs, e.g., embodied applications such as in robotics, beyond those discussed in [61]. A concurrent survey [79] on uncertainty quantification of LLMs categorizes existing uncertainty quantification methods within more traditional classes, which do not consider the unique architecture and characteristics of LLMs. In contrast, our survey categorizes existing work within the lens of LLMs, considering the underlying transformer architecture of LLMs and the autoregressive token-based procedure utilized in language generation."}, {"title": "Organization", "content": "In Section 2, we begin with a review of essential concepts that are necessary for understanding the salient components of uncertainty quantification of LLMs. We discuss the general notion of uncertainty and introduce the main categories of uncertainty quantification methods within the broader field of deep learning. Subsequently, we identify the relevant metrics utilized by a majority of uncertainty quantification methods for LLMs. In Sections 3, 4, 5, and 6, we discuss the four main categories of uncertainty quantification methods for LLMs, highlighting the key ideas leveraged by the methods in each category. In Section 7, we provide a brief discussion of calibration techniques for uncertainty quantification, with applications to uncertainty quantification of LLMs. In Section 8, we summarize the existing datasets and benchmarks for uncertainty quantification of LLMs and present applications of uncertainty quantification methods for LLMs in Section 9. We highlight open challenges in Section 10 and suggest directions for future research. Lastly, we provide concluding remarks in Section 11. Figure 2 summarizes the organization of this survey, highlighting the key details presented therein."}, {"title": "2 Background", "content": "We review fundamental concepts that are crucial to understanding uncertainty quantification of LLMs. We assume basic familiarity with deep learning and build upon this foundation to introduce more specific concepts, describing the notion of uncertainty, the inner workings of LLMs, and the development of metrics and probes to illuminate the uncertainty of LLMs in their response to a user's prompt."}, {"title": "2.1 Uncertainty", "content": "Uncertainty is a widely-known, yet vaguely-defined concept. For example, people generally associate uncertainty with doubt or a lack of understanding, knowledge, or control, but cannot generally provide a precise definition, especially a mathematical one. This general ambiguity applies to the field of LLMs [97]. For example, a subset of the LLM research field considers the uncertainty of a model to be distinct from its level of confidence in a response generated by the model [121], stating that confidence scores are associated with a prompt (input) and a prediction by the model, whereas"}, {"title": "2.2 Types of Uncertainty", "content": "Uncertainty can be broadly categorized into two classes, namely: aleatoric uncertainty and epistemic uncertainty. When considered collectively, the resulting uncertainty is referred to as predictive uncertainty, without a distinction between the two components."}, {"title": "2.2.1 Aleatoric Uncertainty", "content": "Aleatoric uncertainty encompasses the lack of definiteness of the outcome of an event due to the inherent randomness in the process which determines the outcome of the event. For example, a model cannot predict with certainty the outcome of an unbiased coin toss due to the random effects in the coin toss, regardless of the complexity of the model or the size of the training dataset used in training the model. This irreducible uncertainty is referred to as aleatoric uncertainty. For example, in the case of LLMs, aleatoric uncertainty can arise when there is inherent randomness in the ground-truth response, e.g., when prompted with \"What will the temperature be tomorrow?\", the uncertainty associated with the LLM's output can be characterized as aleatoric uncertainty, which is entirely due to"}, {"title": "2.2.2 Epistemic Uncertainty", "content": "In contrast to aleatoric uncertainty, epistemic uncertainty characterizes the doubt associated with a certain outcome (prediction) due to a lack of knowledge or \"ignorance\" by a model, often due to limited training data. For example, when prompted to provide the digit in the 7th decimal place in the square-root of 2, GPT-40 mini responds with the answer 6. However, this answer is wrong: the digit in the 7th decimal place is 5. The uncertainty in the LLM's output can be characterized as epistemic uncertainty, which can be eliminated by training the LLM on more data specific to this prompt. In other words, epistemic uncertainty describes reducible uncertainty, i.e., epistemic uncertainty should reduce when there is more knowledge about the state on which the decision is being made, e.g., via choosing the right model to use for learning, using more training data, or by incorporating any prior knowledge. The uncertainty associated with the response in Figure 3 is entirely epistemic and stems from missing training data. If we train the LLM on more data, including the fact that Jamie Feldman did not write a cookbook, we can eliminate the uncertainty associated with the model's response. Before concluding, we note that prior work has explored decomposing predictive uncertainty into epistemic and aleatoric components [76]."}, {"title": "2.3 Uncertainty Quantification in Deep Learning", "content": "Broadly, uncertainty quantification for deep learning lies along a spectrum between two extremes: training-based and training-free methods, illustrated in Figure 5. Whereas training-based methods assume partial or complete visibility and access to the internal structure of the neural network, modifying it to probe its uncertainty, training-free methods use auxiliary models or additional data to quantify the uncertainty of the model post-hoc."}, {"title": "2.3.1 Training-Based Methods", "content": "Training-based uncertainty quantification methods span Bayesian Neural Networks, Monte Carlo Dropout methods, and Deep Ensembles, which we review in the subsequent discussion. Instead of training a set of parameters to predict a single outcome, a Bayesian neural network (BNN) [90] learns a distribution over the model's weights \u03b8. Specifically, a BNN learns a distribution over the parameters, p(\u03b8|D), with dataset D, with its prediction consisting of two parts: a maximum a posteriori estimation component \u0177, and the uncertainty associated with it, defined by the covariance of the prediction \u03a3\u0177|x,D.\nDespite being statistically principled, the prohibitive computational costs associated with BNNs prevent them from being directly employed. In order to train BNNs, a variety of methods have been proposed, among which the most popular ones are Markov Chain Monte-Carlo (MCMC) [70] and variational inference [164]. The former samples from the exact posterior distribution, while the latter learns to approximate the posterior with a variational distribution, q\u03c6. Due to the relaxed requirement of access to large amounts of samples, the variational inference method has been more widely used, with Monte-Carlo dropout [58, 59] and Deep ensemble [107] being representative methods. More recently, epistemic neural networks (ENNs) [159, 211] have been introduced to reduce the computational challenges associated with BNNs. To make ensemble methods more efficient, e.g., in out-of-distribution detection [210], pruning methods [27, 68, 143], which reduce redundancy among ensemble members, and distillation methods [24, 75], which reduce the number of networks to one, teaching it to represent the knowledge of a group of networks, have been introduced. While these methods are easy to implement and require much less computation compared to regular BNNs or MCMC, they do suffer from being an approximation of the true posterior distribution. In fact, the model's uncertainty predictions could be worse when data augmentation, ensembling, and post-processing calibration are used together [169]."}, {"title": "2.3.2 Training-Free Methods", "content": "Training-free methods for estimating uncertainty have become popular due to their ease of implementation. Since neither the network architecture nor the training process need to be revised, training-free methods work well with large-scale foundation models that are costly to train or fine-tune. In [10, 13, 110, 219], the authors perform data augmentation at test time to generate a predictive distribution, quantifying the model's uncertainty. Similarly, dropout injection [109, 133] extends MC-dropout to the training-free domain by only performing dropout at inference time to estimate epistemic uncertainty. In [144], the authors estimate uncertainty for regression using similar perturbation techniques. Lastly, gradient-based uncertainty quantification methods [110] generate gradients at test-time, which provide an signal for epistemic uncertainty and for OOD detection in [81, 83], by constructing confounding labels."}, {"title": "2.4 Uncertainty Quantification for LLMS", "content": "The introduction of the transformer [208] for sequence-to-sequence machine translation tasks spurred the development of large language models. However, as noted in the preceding discussion, LLMs have gained some notoriety for their tendency to hallucinate when uncertain about a response to a specified prompt. Here, we review the general architecture of LLMs and provide some motivation for the development of LLM-specific metrics for quantifying uncertainty."}, {"title": "2.4.1 LLM Architecture", "content": "LLMs use the transformer architecture to provide free-form responses to input prompts specified in natural language. The transformer architecture consists of an encoder, which processes the input to the model, and a decoder, which generates the model's outputs auto-regressively, where the previous outputs of the model are passed into the model to generate the future outputs. Given an input prompt, the words (elements) of the prompt are tokenized (i.e., the sentences/phrases in natural-language are decomposed into simple units referred to as tokens) and transformed to input embeddings using a learned model. The encoder takes in the input embeddings augmented with positional encodings to incorporate positional context and generates a sequence of latent embeddings, which serves as an input to the decoder, using a stack of N multi-head attention sub-blocks and fully-connected feedforward networks. The decoder takes in the embeddings associated with the previous outputs of the decoder, preceded by a start token, and computes an output embedding using a similar stack of multi-head attention heads and feedforward networks as the encoder. The resulting output embeddings are passed into a linear layer prior to a softmax output layer, which converts the decoder embeddings to a probability distribution over the tokens in the dictionary of the model. In subsequent discussion, we denote the probability of the j'th token in the i'th sentence of an LLM's output as pij. The output token is selected from this probability distribution: e.g., by greedily taking the token associated with the maximum probability mass. The resulting output is passed into the decoder for auto-regressive generation of text."}, {"title": "2.4.2 Natural-Language Inference", "content": "Natural-language inference (NLI) refers to the task of characterizing the relationship between two text fragments, where one text fragment represents a premise (i.e., a statement that is believed to be true) while the other fragment represents a hypothesis (i.e., a statement whose veracity we seek to evaluate based on the premise) [40, 57, 218]. Given a premise and a hypothesis, we can classify the relation between the text pair as: an entailment, if one can infer that the hypothesis is most likely true given the premise; a contradiction, if one can infer that the hypothesis is most likely false given the premise; or a neutral label, if one cannot infer the truthfulness of the hypothesis from the premise [36, 135, 149]. In Figure 7, we provide some examples of text pairs that exhibit entailment, contradiction, or neutrality. In the first example, the premise indicates that the student presented a research paper at a conference (i.e., the student did not skip the conference), hence, the contradiction. In the second example, the premise indicates that the orchestra enjoyed the concert, but does not state whether the orchestra performed at the concert (or just observed the event), hence the neutral label. In the third example, we can infer that the hypothesis is true, since the premise indicates that the team was on vacation, hence, not in the office.\nNLI methods play an important role in uncertainty quantification of LLMs. Many UQ methods for LLMs rely on characterization of the semantic relationship between multiple realizations of the LLM's responses to a given input prompt to determine the confidence of the model. Many of these methods rely on learned models for natural-language inference, such as BERT [45], which utilizes a transformer-based architecture to learn useful language representations that are crucial in natural-language tasks such as question answering and natural-language inference. Unlike many standard language models, e.g., Generative Pre-trained Transformer (GPT) [168], which impose a unidirectionality constraint where every token can only attend to previous tokens, BERT employs a bidirectional approach where each token can attend to any token regardless of its relative position, using a masked language model, potentially enabling the model to capture broader context, especially in sentence-level tasks. In [130], the authors demonstrate that the performance of BERT is limited by inadequate pre-training and propose an improved model, named RoBERTa [130], which retains the same architecture as BERT but is trained for longer with larger mini-batches of data with longer sequences. DeBERTa [72] further improves upon the performance of RoBERTa by introducing a disentangled attention mechanism and an enhanced mask decoder."}, {"title": "2.4.3 Metrics for Uncertainty Quantification for LLMs", "content": "Uncertainty quantification in the LLM community has largely eschewed traditional UQ methods for learned models due to the notable computation cost of running inference on LLMs [16], although, a few UQ methods for LLMs utilize deep ensembles, e.g., [9, 16, 214, 240], generally based on low-rank adaptation (LoRA) [77]. Consequently, many UQ methods in this space have introduced less computationally intensive approximate quantification methods that directly harness the unique architecture of LLM models to assess the uncertainty of these models. In some cases, these methods retain the high-level idea of ensemble methods, quantifying the uncertainty of the model on a given prompt using the outputs of a set of individual models or a collection of outputs from the same model, with a temperature parameter less than one to promote greater stochasticity in the tokens generated by the model. UQ methods for LLMs can be broadly categorized into white-box models and black-box models [127, 207], illustrated in Figure 8 and Figure 9, respectively.\nWhite-Box UQ Methods. White-box UQ models assume that the underlying architecture of the model is partially or completely visible and accessible\u2014hence the term white-box\u2014taking advantage of access to the intermediate outputs of the underlying models, such as the probability distribution over the generated tokens or outputs at the inner layers of the model, to assess the uncertainty of the model [12, 53, 104]. We provide some metrics utilized by white-box UQ methods for LLMs, where pij denotes the conditional probability of token j (conditioned on all preceding tokens) in sentence i:\n(1) Average Token Log-Probability. The average of the negative log-probability of the tokens, which captures the average confidence of the model [139], is given by:\nAverage(p) = -\\frac{1}{L_i}\\Sigma_{j}log(p_{ij}),\nwhere sentence i consists of Li tokens. Note that the value of this metric (1) increases as the conditional probability distribution of each token decreases, signifying an decrease in the model's confidence. The average token probability is related to the product of the token probabilities.\n(2) Perplexity. The perplexity of a model's prediction represents the exponential of (1), i.e., the exponential of the average of the negative log-probability of the tokens which comprise the sentence (response) generated by the"}, {"title": "3 Token-Level UQ", "content": "We recall that the outputs of an LLM are generated by sampling from a probability distribution over the tokens that make up the outputs, conditioned on the preceding tokens in the outputs (see Section 2.4). Token-level UQ methods leverage the probability distribution over each token to estimate the probability of generating a given response from an LLM. Although a high predicted probability in a particular generation may not be indicative of its correctness over another, direct quantification of the model's generating distribution may lead to better understanding of the stochasticity of generations. Token-level UQ methods utilize the white-box UQ metrics discussed in Section 2.4.3 to estimate the randomness in the probability distribution associated with an LLM's response. For example, some token-level UQ methods compute the entropy of the underlying probability distribution over the tokens [122, 220] or semantic clusters [104] (referred to as semantic entropy) to estimate the LLM's confidence. Likewise, a variant of SelfCheckGPT [139] trains an n-gram model using multiple samples of the response of an LLM to a given query including its main response. Subsequently, SelfCheckGPT estimates the LLM's uncertainty by computing the average of the log-probabilities of the tokens generated by the n-gram model, given the original response of the LLM. Moreover, SelfCheckGPT proposes using the maximum of the negative log-probability to estimate the LLM's uncertainty.\nToken-based UQ methods generally perform poorly with long-form responses, because the product of the token probabilities decrease with longer responses, even when the responses are semantically equivalent to a shorter response. To address this limitation, token-based UQ methods employ a length-normalized scoring function [137, 199], to reduce the dependence of the UQ metrics on the length of the sequence, given by:\nProduct(p) = \\Pi_{j=1}^{L_i}p_{ij}"}, {"title": "4 Self-Verbalized UQ", "content": "Self-verbalized uncertainty quantification methods seek to harness the impressive learning and reasoning capabilities of LLMs to enable an LLM to express its confidence in a given response through natural-language. Self-verbalized uncertainty estimates (e.g., expressed as probabilities) are more easily interpretable to humans, especially when the estimates are provided using widely-used epistemic uncertainty markers [195, 232], e.g., words like I am not sure... or This response might be... Figure 10 illustrates the use of epistemic markers by an LLM to convey its uncertainty, when asked of the team that won the 2022 NBA Finals. The response of the LLM is actually incorrect; however, by expressing its uncertainty, a user may be more inclined to verify the factuality of the LLM's response. Prior work has shown that LLMs typically fail to accurately express their confidence in a given response, often using decisive words that suggest confidence, while at the same time being unsure of the accuracy of their response. Empirical studies [103] have shown that poor calibration of LLM's self-verbalized confidence estimates is more pronounced in low-data language settings, e.g., Hindi and Amharic.\nTo address this challenge, prior work in [145] trains a learned model (calibrator) that predicts the probability that an LLM's response to a given prompt is correct, given the input prompt, its response, and the LLM's representations of the prompt and its response. In addition, the output of the calibrator and the LLM's original response are subsequently used in fine-tuning a generative model [184] to produce a linguistically calibrated response, aligning the verbal expression of the LLM's confidence with its probability of factual correctness. However, the resulting verbalized uncertainty lacks a"}, {"title": "5 Semantic-Similarity UQ", "content": "Semantic-similarity uncertainty quantification methods examine the similarity between multiple generated responses of an LLM to the same query [29, 104, 121] by focusing on the meaning (i.e., the semantic content of a generated sentence) rather than the form (i.e., the string of tokens that the model predicts) [104] of the responses. For example, consider the prompt (question) given to an LLM: Where is Buckingham Palace in the United Kingdom? Standard sampling from an LLM can produce many variations of the same answer when prompted with this question, as illustrated in Figure 13. However, while an LLM may be uncertain about which sequence the user may prefer, most variations do not alter the meaning of the sentence. This difference in the ordering of the tokens in each response may lead to different token probabilities, which in turn may negatively impact the accuracy of other uncertainty quantification methods, such as token-level UQ methods.\nSince semantic similarity is a relative metric, its outputs are in general model-dependent, posing a central challenge. A recent line of work uses NLI models, such as RoBERTa [130] and DeBERTa [72] (discussed in Section 2.4.2), to compute entailment probabilities. The work in [4] proposes upweighting tokens that have large gradients with respect to the NLI model to maximize the probability of contradiction to generate semantically-varied responses. In addition, the method in [196] proposes using a chain-of-thought agreement (CoTA) metric that uses entailment probabilities to evaluate the agreement between CoT generations, concluding that CoTA semantic uncertainty leads to more robust model faithfulness estimates than either self-verbalized or token-level uncertainty estimates. The authors of [29] propose using a combined measure of confidence that incorporates entailment probabilities along with a verbalized confidence score, and selects the generation with the highest confidence. The UQ method in [18] proposes generating multiple explanations for each plausible response and then summing the entailment probabilities. Another work [102] introduces semantic entropy probes, wherein semantic clusters are grown iteratively using entailment probabilities. Each new generation is either added to an existing cluster if entailment holds, or added to a new cluster. Then, a linear classifier is trained to predict high-entropy prompts. Furthermore, the method in [142] uses a database of user-verified false statements to build a semi-automated fact-checking system that uses entailment probabilities with database queries as a metric for confidence in a statement's falseness.\nIn addition to using NLI models to evaluate factual similarities between responses, some methods use language embeddings [161] to cluster responses based on their semantic similarity and reason about uncertainty over the clusters, e.g., semantic density in [165]. First, several reference responses are generated by sampling the model. Then, the overall uncertainty per response is computed using the entailment scores, taking values in the set {0, 0.5, 1}. The semantic density is then used to accept or reject a target response based on the similarity to the target responses. The supervised approach in [71] utilizes the K-means algorithm to first cluster synonyms, which are attended by the LLM during training. The work in [78] introduces a method to achieve semantically-aligned item identification embeddings based on item descriptions, which aid in aligning LLM-based recommender systems with semantically-similar generations when item descriptions are sparse. Further, the method in [215] prompts an LLM to generate concepts (effectively"}, {"title": "6 Mechanistic Interpretability", "content": "Mechanistic interpretability (MI) aims to understand the inner workings of LLMs to pinpoint the potential sources of uncertainty, by uncovering causal relationships [20]. Several survey papers have provided a taxonomy of mechanistic interpretability in the field of transformer-based language models [170], focused on AI safety [20] or interpretability of language models in general [244].\nWe start by discussing a few key concepts of mechanistic interpretability, summarized in Figure 14. Features are the unit for encoding knowledge in a neural network. For example, a neuron or set of neurons consistently activating"}, {"title": "7 Calibration of Uncertainty", "content": "In many cases, the confidence estimates computed by the UQ methods presented in the preceding sections are not well-calibrated i.e., aligned with the observed frequencies of the responses (accuracy of the model). However, reliability of the confidence estimates of an LLM's output remains crucial to the safe deployment of LLMs. As a result, we would like the confidence estimates to be calibrated. Formally, for a perfectly-calibrated confidence estimate p, we have that, \u2200p \u2208 [0, 1]:\n\\mathbb{P}[Y = \\hat{Y} | P = p] = p,\nwhere Y and \\hat{Y} represent random variables denoting the ground-truth and predicted outputs from the model, respectively, and P represents a random variable denoting the confidence associated with the predicted output \\hat{Y} [67]. In Figure 15, we show poorly-calibrated confidence estimates on the left, where the estimated confidence of the model is not well-aligned with the observed accuracy of the model. The dashed-line illustrates perfect alignment between the estimated confidence of the model and its accuracy. In this example, confidence estimates of the model above 0.5 tend to be overconfident, exceeding the accuracy of the model. Conversely, confidence estimates that are less than 0.5 tend to be underconfident. Calibration techniques improve the alignment of the estimated confidence of the model with the observed accuracy, with the estimated confidence more closely following the dashed-line, as shown on the right in Figure 15. We review some metrics for quantifying the calibration of a model's confidence estimates.\nExpected Calibration Error (ECE). The Expected Calibration Error (ECE) measures the expected deviation between the left-hand side and right-hand side of (13), with:\n\\mathbb{E}_P [|\\mathbb{P}[Y = \\hat{Y} | P = p] - p|],\nwhere the expectation is taken over the random variable P. Computing the expectation in (14) is intractable in general. Hence, the work in [151] introduces an approximation of the ECE, which partitions the confidence estimates into"}, {"title": "7.1 Training-Free Calibration Methods", "content": "Training-free calibration methods do not modify the weights of the model to produce calibrated predictions, e.g., Platt scaling [163], isotonic regression [236, 237], and conformal prediction [182]. Here, we discuss conformal prediction in greater detail. Conformal prediction (CP) is a powerful technique used to quantify the uncertainty of a model's"}, {"title": "7.2 Training-Based Calibration Methods", "content": "We can group training-based calibration techniques into ensemble-based calibration methods, few-shot calibration methods, and supervised calibration methods."}, {"title": "7.2.1 Ensemble-Based Calibration", "content": "Ensemble-based calibration (model ensembling) seeks to estimate uncertainty by querying many similar models (for example, the same architecture trained with different random seeds) and comparing their outputs. Prompt ensembles enhance calibration by combining the outputs of multiple prompts [86]. One common and effective ensembling strategy involves utilizing the majority vote. Given K models predicting a response li, the majority vote is selected as:\n\\mathbb{P}_{acc} (\\hat{y} = l_i) = \\sum_{k=1}^{K} \\mathbb{P}_k (\\hat{y}_k = l_i) \\mathbb{I}(\\hat{y}_k = l_i).\nThe ensemble vote is then the response li with the highest aggregate confidence. Another class of ensemble-based methods evaluates overall (rather than pre-choice) uncertainty, e.g. binning the model's responses into semantic categories and computing the entropy [15, 206]. An ensemble-like effect can also be realized by varying the in-context examples provided to the LLM [112]."}, {"title": "7.2.2 Few-Shot Calibration", "content": "Few-shot calibration techniques employ several queries to the same model and benefit from sequential reasoning as the model evaluates its intermediate generations. For instance, prompting models to begin their responses with a fact and justification for the fact has been shown to improve calibration versus other types of linear reasoning, such as tree-of-thought [217, 246]. In the domain of code generation, calibration techniques have also been applied to improve the reliability of generated code [185]. Furthermore, inferring human preferences with in-context learning has been explored as a means to calibrate models in alignment with human judgments [131]."}, {"title": "7.2.3 Supervised Calibration", "content": "Supervised calibration approaches, which mainly involve modifying the LLM's weights via additional losses, auxiliary models, or additional data, are also crucial in enhancing model calibration. In supervised methods, learning to classify generated responses as correct (i.e., via a cross-entropy loss) can result in better calibration than non-learning-based approaches and can help to combat overconfidence [32, 88, 250]. In fact, some existing work argue that fine-tuning is necessary for the calibration of uncertainty estimates of LLMs [95]. Given a language generator f, score model (confidence) P, and a dataset D := {(x, y)i}N i=1 of data-label pairs, the token-level cross-entropy loss seeks to measure the uncertainty of the predicted labels f(x), on average, over the dataset:\n\\mathcal{L}_{CE} = -\\mathbb{E}_{(x,y) \\sim \\mathcal{D}} [log \\mathbb{P}(y = f(x))],to improve the calibration of the confidence estimates of the model. While LLMs exhibit high-quality text generations (f), their confidences (P) may be improved by fine-tuning the model with a cross-entropy loss on the full dataset or a subset. Besides the cross-entropy function, other proper-scoring rules can also be used for achieving calibration [64, 65]. Reinforcement learning (with human feedback in some applications) may be used to fine-tune a model to produce realistic confidence estimates, e.g., [17, 141]. Techniques such as learning to rationalize predictions with generative adversarial networks [181], applying regularization [101], and biasing token logits [129, 248] have also been explored. Finally, sequence-level likelihood calibration has been proposed to improve the quality of LLM generations [247]. Instead of modifying the model's weights, another class of supervised calibration methods seeks to modify model hyperparameters in a post-hoc manner. These include temperature tuning [43] and methods involving entropy and logit differences [QQ] [134]."}, {"title": "8 Datasets and Benchmarks", "content": "Here", "include": "GPQA [174", "74": "a multi-task dataset for evaluating the breadth of knowledge of LLMs across a wide range of subjects, e.g., the humanities and sciences; HellaSwag [238", "106": "a dataset for reading-comprehension evaluation; GSM8K [35", "73": "a code-generation benchmark for LLMs. There have been a related line of work in developing datasets with inherent ambiguities [93, 123, 146, 193"}]}