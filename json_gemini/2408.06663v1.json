{"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "authors": ["Kaiser Sun", "Mark Dredze"], "abstract": "The development of large language models leads to the formation of a pre-train-then-align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream tasks. In this work, we investigate the relationship between pre-training and fine-tuning by fine-tuning multiple intermediate pre-trained model checkpoints. Our results on 18 datasets suggest that i) continual pre-training improves the model in a latent way that unveils after fine-tuning; ii) with extra fine-tuning, the datasets that the model does not demonstrate capability gain much more than those that the model performs well during the pre-training stage; iii) although model benefits significantly through supervised fine-tuning, it may forget previously known domain knowledge and the tasks that are not seen during fine-tuning; iv) the model resembles high sensitivity to evaluation prompts after supervised fine-tuning, but this sensitivity can be alleviated by more pre-training.", "sections": [{"title": "1 Introduction", "content": "The rise of large language models (LLMs) as a general-purpose tool for a diverse range of natural language processing tasks has dramatically transformed the field, introducing new paradigms for data collection and model training. Numerous models, training methods, datasets, and evaluation methods continue to be developed on an ongoing basis. Nevertheless, a unified paradigm has emerged for training LLMs:\nRather than explore these two training regimes independently, we question: how do model pre-training and fine-tuning interact to affect the resulting model? Does more pre-training hinder better fine-tuning results? What does the model learn and forget during pre-training as well as fine-tuning? Answering these questions requires us to examine how models learn during pre-training and how this affects fine-tuning. Therefore, we fine-tune multiple pre-training checkpoints of a large language model, evaluating each checkpoint and its fine-tuned version on downstream evaluation sets. We track model abilities during pre-training and compare them to improvements achievable after fine-tuning at the corresponding pre-training step. We explore both supervised and instruction fine-tuning, testing the models' memorization and forgetting when learning specific tasks and serving as general-purpose language-AI tools. To the best of our knowledge, we are the first to explore fine-tuning intermediate model checkpoints.\nOur experiments yield insights into LLM training. We find that (1) continued pre-training can improve a model in ways that are only revealed after fine-tuning (\u00a75); (2) tasks for which the model already performs well during pre-training benefit much less from fine-tuning than those where the model does not demonstrate capabilities (\u00a74, \u00a75); (3) although supervised fine-tuning can improve performance on in-distribution tasks, it can also cause the model to forget domain knowledge or tasks that it was previously capable of solving (\u00a76); (4) fine-tuned models show high sensitivity to evaluation prompts, but this sensitivity can be alleviated by more pre-training (\u00a76). Our findings provide insights into model training and can inform methods for both pre-training and fine-tuning. Furthermore, our work shows the value of analyzing the training dynamics, in addition to analyzing the final LLM, as an aspect of interpretability, and we encourage model developers to release these checkpoints to aid future studies."}, {"title": "2 Background: Model Training", "content": "We begin with a brief survey of the core components of LLM training: pre-training, fine-tuning, and instruction fine-tuning. We also discuss the related topic of in-context learning as well as different efficient fine-tuning strategies.\nWe use \"model alignment\u201d as a general term for techniques that align a model with a desired behavior, which can be accomplished by fine-tuning models after pretraining. The term is also associated with other definitions. We also note several related studies that explore training dynamics to understand model behavior. With this in mind, we conduct an empirical study on how the amount of pre-training affects the effectiveness of fine-tuning.\nPre-training The first step of training a LLM is pre-training on a massive text corpus. For decoder-only models in the GPT family, the subject of our paper, work since the introduction of GPT-2 has focused on scaling up model training. Initial work increased model size to hundreds of billions of parameters, along with explorations in model size, training corpus size, and training data characteristics. Since the push towards large models, work has shifted to increasing the amount of pre-training data, with new models now reaching 15 trillion tokens. Studies of model performance on various tasks at different model sizes introduced the idea of emergent model abilities, with new model abilities being revealed as model training grows.\nWe also recognize a particularly important trend for this paper: model openness. Early LLMs were proprietary models accessible only through an API. The first large open model, Bloom, allowed widespread LLM evaluation. Subsequent open models, such as OPT, LLaMA and others, have become the norm. In this paper, we study OLMO, one of the only models to release individual pre-training checkpoints.\nFine-Tuning Early work on instruction fine-tuning using reinforcement learning with human feedback (RLHF) demonstrates the dramatic effect that model alignment could have on a pre-training model. When a specific task of interest has been identified, supervised fine-tuning can improve a pre-trained model. Task-agnostic tuning became popularized with the advent of T5 models, where a pre-trained LLM is tuned using a general text-to-text solution. When multiple tasks are given to the model, the model is commonly given a task-specific prefix or an instruction along with the task input, leading to the development of various methods of prefix tuning and instruction tuning.\nInstruction Fine-Tuning Instruction fine-tuning is preferred when more general model behaviors are desired. Popularized through reinforcement-learning with human feedback (RLHF) and reinforcement-learning with AI feedback (RLAIF), these methods utilize a reward model to simulate human feedback. Others explore human preference tuning without a reward model or study the effects of these tuning methods. Sharma et al. (2024) show that supervised fine-tuning can lead to similar performance as RLAIF.\nIn-Context Learning While not the subject of this paper since it does not make changes to model parameters, in-context learning utilizes a small amount of supervised data to improve model performance. ICL, also called few-shot learning, is also used as an evaluation strategy where the model is given a prompt composed of examples of tasks expected to be solved. The underlying model is evaluated based on its response to the input. ICL can benefit from a larger context window that adds more examples, which can spur work on the development of model quantization techniques and the alleviation of hardware constraints.\nFine-Tuning Techniques While model pre-training can be done by a few groups with large resources interested in developing new models, fine-tuning depends on the task and is of broad interest. Therefore, many techniques facilitate time-, memory-, and data-efficient model training through parameter-efficient fine-tuning (PEFT), quantization, and specialized data filtering. This paper focuses specifically on full-parameter fine-tuning, while our findings suggest the potential for data-efficient and budget-friendly training by understanding the critical turning point of model training. Our findings are closely related to the recent study on phase transition of model training."}, {"title": "3 Experimental Setup", "content": "In this section, we describe the model and datasets used. The hyperparameter tuning procedure and setup for each fine-tuning setting can be found in Appendix A.\n3.1 Model Choice\nOur paper considers OLMo-1B, a high-performing open-source large language model. Ideally, we would evaluate multiple models, but OLMo is the only model to release intermediate pre-training checkpoints, and thus the only model that supports our analysis. Despite being the only open model with training checkpoints, it fortunately has several desirable properties. First, the model is fully open, including the training details, pre-training data, and fine-tuning data. Second, the smaller model size allows us to train a model efficiently on a single A100 GPU. While evaluating a larger model would be desirable, we limit our study to the 1B model given the much larger computational demand of multi-GPU training. Our detailed analysis required significant GPU resources, which would have been prohibitive with a larger model. We also note that OLMO-1B compares very favorably to the larger version, and recent work has shown that small models can compete with larger ones.\nWe select model pre-training checkpoints uniformly from the pre-training history along with the first and the final checkpoints.\n3.2 Training Procedure\nWe fine-tune each of the selected model checkpoints using two different procedures to create fine-tuned models: supervised fine-tuning and instruction tuning. The supervised fine-tuning is conducted separately for each model checkpoint and dataset, while the instructing fine-tuning is done"}, {"title": "4 How does the model change across pre-training?", "content": "We begin our evaluation by considering how additional pre-training changes the BASE model. Typically, researchers track the value of the training or held-out loss during training. However, performance improvements on downstream tasks do not always follow the same trend with the loss curves.\nWe evaluate the pre-trained checkpoints with few-shot examples, as models without alignment tend to do poorly in a zero-shot context. Four shots are randomly sampled from the datasets, which are selected based on the highest performance shot amount reported in Yang et al., 2024. The model's performance at each pre-training step is reported in Figure 2.\nBroadly speaking, our results suggest that all datasets fall into one of two groups. For the first group of datasets, although the model shows clear improvement during the early stages of pre-training, performance levels off fairly early on and remains consistent. The dramatic improvements in the early stages of pre-training may result from larger steps in early optimization. We find improvements stop increasing past step 342,000. The second group shows tasks that are never learned during pre-training. Performance remains constant throughout the whole pre-training process. These datasets include MNLI, XSum, and BoolQ, and we found no difference between zero-shot and few-shot evaluations. A natural hypothesis for this finding is potential data contamination in the pre-training data. However, the evaluation datasets are selected based on the popularity of the task and the content of pre-training data. All datasets that experience improvement do not exist in the model's pre-training data, while the more likely leaked datasets (MNLI, XSUM) never gain an improvement during the pre-trining process.\nOverall, these results reveal an interesting dichotomy. Some tasks can be learned during pre-training, while others are not. Next, we explore what exactly the model is learning regarding this second group of datasets during pre-training by exploring the fine-tuned models."}, {"title": "5 Does more pre-training improve fine-tuning?", "content": "Groeneveld et al., 2024 compares OLMo's performance on several tasks before and after fine-tuning the final checkpoint and finds that fine-tuning enables the model to do well on tasks for which the unaligned model does poorly. We observe (\u00a74) that while some datasets improved during pre-training, there is a group of datasets for which a pre-trained model does poorly. Does the model learn anything that helps solve these tasks, and is fine-tuning required to do well on them? Alternatively, does the model learn useful information for these tasks but cannot express it without fine-tuning? In this section, we further explore this dataset dichotomy by examining fine-tuned checkpoints for each of the datasets.\nOur results appear in Figure 3 and Figure 4. First, we consider those datasets where the pre-trained models do well. These datasets do not improve with fine-tuning, suggesting whatever is learned during fine-tuning, which we discuss below, the model already gains the knowledge during pre-training. This effect is observed at all checkpoints; fine-tuning simply does not help.\nHowever, a different story is observed for datasets that are not learned during pre-training. For these, fine-tuning yields significant improvements at every model checkpoint, with Figure 4 showing the magnitude of improvement on these datasets compared to no improvement to the datasets already learned during pre-training. Moreover, earlier checkpoints obtain more substantial gains from fine-tuning than later checkpoints. The benefit of fine-tuning continues to increase until a certain threshold in pre-training steps is reached (approximately 424,000).\nFigure 3 shows representative plots comparing the performance of a pre-trained versus fine-tuned model at different checkpoints for two datasets (full list in Appendix E). For Hellaswag (learned during pre-training), fine-tuning does not benefit the model, even during early checkpoints when the model performs poorly on the task. Nevertheless, for MNLI (not learned during pre-training), fine-tuning dramatically improves the model. Interestingly, later checkpoints achieve better results after fine-tuning, even when the performance of the pre-trained model is unchanged. This suggests that the model is, in fact, learning important information during pre-training, but it cannot express that information without fine-tuning.\nOur findings suggest that early stopping in pre-training will not be detrimental to downstream fine-tuning performance, and the benefits of fine-tuning an LLM could exceed the benefits of continued pretraining, which sheds light on the potential of cost-effective training paradigm with less pre-training. However, it is difficult to directly identify such a stopping criteria without fine-tuning intermediate checkpoints; the improvement trend is invisible before fine-tuning the checkpoints. Future work may reveal other signals of pre-training behavior that correlate with downstream task performance after fine-tuning. Overall, when resource-intensive pre-trained LLMs are not available, fine-tuning models on models with less pre-training may be a reasonable practical choice for obtaining a high-quality model."}, {"title": "6 Supervised Fine-Tuning: What does the model learn and forget?", "content": "What exactly is the model learning during fine-tuning such that it shows abilities in pre-trained models for some tasks but provides no benefit for other tasks? We analyze the supervised fine-tuning process to understand what is learned and what is forgotten. Specifically, we explore three dimensions: task format, task transfer, and domain knowledge.\n6.1 Task Format\nSclar et al., 2023 show that LLMs are extremely sensitive to prompt perturbation in few-shot settings. More broadly, extensive work on prompt engineering reveals the sensitivity of models to task format. We hypothesize that fine-tuning fits the model to a specific task format, resulting in higher performance when the evaluation set matches this format. To test this hypothesis, we vary the task format to either match the training format, use a different format, or rely on instructions. We carefully construct three different prompt formats for the following settings. 1) Default is the same format used for training, where we expect the model to benefit from learning the task format; 2) In contrast, IO format reflects a common way of performing supervised fine-tuning by incorporating only unprocessed input and output; 3) Instruct uses a human-readable instruction template to format the input.\nIn the early pre-training steps, aligning the task format with fine-tuning data seems to play a crucial role. The model does not yet have enough information to overcome the differences between the training and test formats. However, when fine-tuned on later pre-training checkpoints, the model gradually becomes more flexible with different task formats, suggesting that model sensitivity to prompt formatting observed may be resolvable with more pre-training and a fine-tuning stage. In this view, fine-tuning teaches the model how to format a response for the task."}, {"title": "6.2 Task Transfer", "content": "Numerous studies examine model forgetting, where further model training causes improvements on some tasks but degradation on others. We evaluate model forgetfulness by examining whether the model does worse on some tasks after fine-tuning for other tasks. Specifically, we divide our tasks into two types: classification and generation. We notate the training datasets as \\(D_t\\) and the evaluation datasets as \\(D_E\\). We represent the performance of a pre-trained model (BASE) on checkpoint i as \\(Perf_{BASE}(d)\\) where an evaluation dataset \\(d\\) \u2208 \\(D_E\\), and the performance of the i-th checkpoint fine-tuned on dataset t \u2208 \\(D_t\\) be \\(Perf_i(d)\\). To normalize the effect caused by uneven performance across different datasets, we compute the mean ratio of change (MRC) in performance for each checkpoint as follows.\n\\[MRC = \\frac{1}{|D_E\\{t}|}\\sum_{d \\in D_E, d \\neq t} \\frac{Perf_i(d) - Perf_{BASE}(d)}{Perf_{BASE}(d)}\\]\nModels fine-tuned on classification tasks and evaluated on generation tasks decrease on average 61.4% compared to models that are never fine-tuned. In contrast, models fine-tuned on generation tasks can still perform the same as the BASE model on classification tasks, with a 0.3% MRC, which is not statistically significantly different from a 0% change. Our findings on all pre-training checkpoints align with the findings of Yang et al. (2024) on the final checkpoint of LLAMA-7B.\nRegardless of the pre-training stage, a model can maintain classification abilities when trained for generation, but it loses its generation abilities when trained for classification. This is perhaps not surprising given that classification tasks can be seen as a subset of generation, while the reverse is not true. The model follows a simplicity bias and thus is more likely to memorize simple classification tasks than generation tasks with an exponentially larger search space. Additionally, since we evaluate the classification tasks based on the output logits and the base model performs randomly on the classification tasks, it is much easier for the models to maintain the same performance as the BASE models. Fine-tuning can cause a model to lose abilities when the desired fine-tuning behavior does not support those abilities."}, {"title": "6.3 Domain Knowledge", "content": "Finally, we explore how a model's generalization ability is affected by fine-tuning by inspecting whether the model forgets the domain knowledge it had before fine-tuning due to learning other abilities. An example of OOD model performance is shown in Figure 6, and the mean change ratio by datasets is presented in Figure 7.\nThe model does not benefit equally from the in-domain fine-tuning: all NLI datasets experience a boost when fine-tuning on MNLI, while fine-tuning on Paws is detrimental to other paraphrase detection datasets. This implies that both forgetting and learning are happening: the model learns to perform the task with in-domain knowledge, but it may, in turn, forget information more distant from what is learned in fine-tuning. Questions remain, however, about whether there are different stages of learning and forgetting during fine-tuning and whether the model picks up different tasks in various stages, which requires further study of fine-tuning dynamics.\nOverall, across these three lenses, we find that fine-tuning, although teaches a model how to perform a task, can sacrifice generalization abilities if such ability is not needed for the fine-tuned task. For some datasets learned with pre-training alone, the model can easily understand the task format, and the nature of the task is probably supported by the pre-training objective. For tasks that can only be learned with subsequent fine-tuning, the model may require additional examples to adapt to different task formats, or the task itself may be inconsistent with the pre-training objective."}, {"title": "7 Discussion", "content": "Our study uses fine-tuning of pre-training model checkpoints to understand the dynamics of pre-training and fine-tuning on model performance. While our insights suggest directions for future work, we note important limitations inherent in our experiments. This study considered a single, relatively small LLM on less than a dozen datasets, and still consumed thousands of hours of GPU training time at significant expense. Future work needs to confront these issues on larger models and more datasets. We believe our experiments can focus future work on specific experiments with larger models.\nSome datasets can be learned without fine-tuning. We discover a dichotomy between datasets. Some are learned during model pre-training, while others show no improvements during pre-training. Furthermore, the datasets learned during pre-training do not benefit from fine-tuning. This observation, combined with our study about what is learned during fine-tuning (Section 6) suggests that some tasks are presented in a manner that aligns with what the model sees during pre-training, and thus fine-tuning provides no additional information. While we could identify what about the tasks placed them in the learned or not learnable during pre-training group, it may be possible to format tasks in a manner that better aligns with pre-training and makes them learnable.\nPre-training models can improve in undetectable ways without fine-tuning. Some datasets are not learnable during pre-training but benefit significantly from fine-tuning (\u00a74). However, these datasets still benefited from additional pre-training, even though those benefits were not revealed without fine-tuning (\u00a75). Clearly, the model is learning important information about the task, even though it cannot express that information. The identification of a measure available during pre-training that correlated with post-fine-tuning task performance could be used to guide pre-training and produce models that did better post-fine-tuning. Perhaps there is a way in which information about these tasks can be included in pre-training, allowing the model to better utilize the massive amount of pre-training data. For example, early stopping during pre-training could lead to better utilization of limited training resources if we knew when to stop.\nFine-tuning teaches task format but leads to forgetting unused abilities. Our results show that fine-tuning guides the model to understand the format and complete a given task. As this information diminishes, the model's overall ability improves. However, fine-tuning comes at the expense of other model abilities, such as the capability of performing on tasks or domains that are unrelated to the"}, {"title": "8 Conclusion", "content": "Our experiments explore the relationship between fine-tuning and pre-training LLMs. Our findings span from the latent benefits of pretraining to model learning and forgetting during fine-tuning. Our results show that the model can rapidly pick up the datasets that it could not solve during fine-tuning with only a small amount of supervision. In the meantime, we identify the aspects that LLM learns and forgets during supervised fine-tuning: task format, task solution, and domain knowledge. Overall, our results demonstrate the value of analyzing language model training dynamics, and we would like to call for the release of pre-training checkpoints to aid future studies."}, {"title": "Limitations", "content": "We discuss the weaknesses and limitations in the following section.\nComputing Resource Due to computational constraints, we can only conduct experiments on a 1B model and a limited amount of datasets. The amount of GPU hours spent for each experiment in this study is listed in Table 3.\nAvailbility of Pre-training Checkpoints This study would benefit significantly from including a broader spectrum of models, but the public pre-training checkpoint releases are limited. Open-source LLMs with intermediate checkpoint release include OLMo, TinyLLAMA, RedPajama-Incite, OpenLM, and Pythia. After a series of preliminary experiments, we select these models' best-performing and robust families.\nScaling Law Recent research shows that the model may resemble emergent capability when scaled to a certain size. Comparatively, Hassid et al., 2024 find that smaller model is capable of outperforming its larger variant when the computing resources is controlled. To avoid potential confounding factors caused by quantization, our experiments are only conducted on the one-billion model, which may, therefore, conceal the emergent capability brought by larger models while at least giving insights about the potential of small models.\nAnalysis Protocol Wu et al., 2023 show that the evaluation result may be affected by samples that have been memorized by the model during training instead of revealing the reasoning capability. The only analysis protocol used is the downstream performance of a trained model. More investigation should be done into model internals during pre-training dynamics and how they relate to the effects of fine-tuning.\nTraining Paradigm Although multiple tuning strategies exist, to create a fair comparison environment where checkpoints received the same amount of training, models are fine-tuned with a fixed amount of epochs in this work. On different pre-training stages, the model may converge at a different speed. Further study can be done to study the effect of pre-training on different fine-tuning methods or fine-tuning dynamics in different pre-training stages. We only explored the scenario of full-parameter fine-tuning. Whether parameter-efficient fine-tuning or human preference tuning will lead to a different conclusion also remains an open question.\nRandomness In this study, we only assess uncertainty with Bootstrap during evaluation. However, uncertainty may emerge during training, which poses optimizer initialization and data ordering. Due to the computational constraints, we cannot reduce the randomness factor on this angle."}]}