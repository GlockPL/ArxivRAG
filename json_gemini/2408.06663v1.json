{"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "authors": ["Kaiser Sun", "Mark Dredze"], "abstract": "The development of large language models leads to the formation of a pre-train-then-align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream tasks. In this work, we investigate the relationship between pre-training and fine-tuning by fine-tuning multiple intermediate pre-trained model checkpoints. Our results on 18 datasets suggest that i) continual pre-training improves the model in a latent way that unveils after fine-tuning; ii) with extra fine-tuning, the datasets that the model does not demonstrate capability gain much more than those that the model performs well during the pre-training stage; iii) although model benefits significantly through supervised fine-tuning, it may forget previously known domain knowledge and the tasks that are not seen during fine-tuning; iv) the model resembles high sensitivity to evaluation prompts after supervised fine-tuning, but this sensitivity can be alleviated by more pre-training.", "sections": [{"title": "Introduction", "content": "The rise of large language models (LLMs) as a general-purpose tool for a diverse range of natural language processing tasks has dramatically transformed the field, introducing new paradigms for data collection and model training (Brown et al., 2020, Biderman et al., 2023, Touvron et al., 2023, Jiang et al., 2023, Chowdhery et al., 2023, Groeneveld et al., 2024, Wang et al., 2024, inter alia). Numerous models, training methods, datasets, and evaluation methods continue to be developed on an ongoing basis. Nevertheless, a unified paradigm has emerged for training LLMs:"}, {"title": "Background: Model Training", "content": "We begin with a brief survey of the core components of LLM training: pre-training, fine-tuning, and instruction fine-tuning. We also discuss the related topic of in-context learning as well as different efficient fine-tuning strategies.\nWe use \"model alignment\u201d as a general term for techniques that align a model with a desired behavior, which can be accomplished by fine-tuning models after pretraining. The term is also associated with other definitions (Shen et al., 2024). We also note several related studies that explore training dynamics to understand model behavior (Tirumala et al., 2022; Chen et al., 2023; Tian et al., 2023). With this in mind, we conduct an empirical study on how the amount of pre-training affects the effectiveness of fine-tuning.\nPre-training The first step of training a LLM is pre-training on a massive text corpus (Achiam et al., 2023; Touvron et al., 2023; Groeneveld et al., 2024). For decoder-only models in the GPT family, the subject of our paper, work since the introduction of GPT-2 (Radford et al., 2019) has focused on scaling up model training. Initial work increased model size to hundreds of billions of parameters (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2023), along with explorations in model size, training corpus size, and training data characteristics (Hoffmann et al., 2022; Gururangan et al., 2020). Since the push towards large models, work has shifted to increasing the amount of pre-training data (Computer, 2023; Soldaini et al., 2024), with new models now reaching 15 trillion tokens (AI@Meta, 2024). Studies of model performance on various tasks at different model sizes introduced the idea of emergent model abilities (Wei et al., 2022), with new model abilities being revealed as model training grows.\nWe also recognize a particularly important trend for this paper: model openness. Early LLMs were proprietary models accessible only through an API. The first large open model, Bloom (Bloom Str\u00f6m et al., 2023), allowed widespread LLM evaluation. Subsequent open models, such as OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023; Keles and Bayrakl\u0131, 2024) and others (Biderman et al., 2023; Gururangan et al., 2023; Almazrouei et al., 2023), have become the norm. In this paper, we study OLMO (Groeneveld et al., 2024), one of the only models to release individual pre-training checkpoints.\nFine-Tuning Early work on instruction fine-tuning using reinforcement learning with human feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022) demonstrates the dramatic effect that model alignment could have on a pre-training model. When a specific task of interest has been identified, supervised fine-tuning can improve a pre-trained model. Task-agnostic tuning became popularized with the advent of T5 models (Raffel et al., 2020), where a pre-trained LLM is tuned using a general text-to-text solution. When multiple tasks are given to the model, the"}, {"title": "Experimental Setup", "content": "In this section, we describe the model and datasets used. The hyperparameter tuning procedure and setup for each fine-tuning setting can be found in Appendix A."}, {"title": "Model Choice", "content": "Our paper considers OLMo-1B (Groeneveld et al., 2024), a high-performing open-source large language model. Ideally, we would evaluate multiple models, but OLMo is the only model to release intermediate pre-training checkpoints, and thus the only model that supports our analysis. Despite being the only open model with training checkpoints, it fortunately has several desirable proper-ties. First, the model is fully open, including the training details, pre-training data, and fine-tuning data. Second, the smaller model size allows us to train a model efficiently on a single A100 GPU. While evaluating a larger model would be desirable, we limit our study to the 1B model given the much larger computational demand of multi-GPU training. Our detailed analysis required significant GPU resources, which would have been prohibitive with a larger model. We also note that OLMO-1B compares very favorably to the larger version, and recent work has shown that small models can compete with larger ones (Riviere et al., 2024).\nWe select model pre-training checkpoints uniformly from the pre-training history along with the first and the final checkpoints."}, {"title": "Training Procedure", "content": "We fine-tune each of the selected model checkpoints using two different procedures to create fine-tuned models: supervised fine-tuning and instruction tuning. The supervised fine-tuning is conducted separately for each model checkpoint and dataset, while the instructing fine-tuning is done"}, {"title": "How does the model change across pre-training?", "content": "We begin our evaluation by considering how additional pre-training changes the BASE model. Typically, researchers track the value of the training or held-out loss during training. However, performance improvements on downstream tasks do not always follow the same trend with the loss curves (Groeneveld et al., 2024).\nWe evaluate the pre-trained checkpoints with few-shot examples, as models without alignment tend to do poorly in a zero-shot context. Four shots are randomly sampled from the datasets, which are selected based on the highest performance shot amount reported in Yang et al., 2024. The model's performance at each pre-training step is reported in Figure 2."}, {"title": "Does more pre-training improve fine-tuning?", "content": "Groeneveld et al., 2024 compares OLMo's performance on several tasks before and after fine-tuning the final checkpoint and finds that fine-tuning enables the model to do well on tasks for which the unaligned model does poorly. We observe (\u00a74) that while some datasets improved during pre-training, there is a group of datasets for which a pre-trained model does poorly. Does the model learn anything that helps solve these tasks, and is fine-tuning required to do well on them? Alternatively, does the model learn useful information for these tasks but cannot express it without fine-tuning? In this section, we further explore this dataset dichotomy by examining fine-tuned checkpoints for each of the datasets.\nOur results appear in Figure 3 and Figure 4. First, we consider those datasets where the pre-trained models do well (Figure 2a). These datasets do not improve with fine-tuning, suggesting whatever is learned during fine-tuning, which we discuss below, the model already gains the knowledge during pre-training. This effect is observed at all checkpoints; fine-tuning simply does not help.\nHowever, a different story is observed for datasets that are not learned during pre-training. For these, fine-tuning yields significant improvements at every model checkpoint, with Figure 4 showing the magnitude of improvement on these datasets compared to no improvement to the datasets already learned during pre-training. Moreover, earlier checkpoints obtain more substantial gains from fine-tuning than later checkpoints. The benefit of fine-tuning continues to increase until a certain threshold in pre-training steps is reached (approximately 424,000).\nFigure 3 shows representative plots comparing the performance of a pre-trained versus fine-tuned model at different checkpoints for two datasets (full list in Appendix E). For Hellaswag (learned during pre-training), fine-tuning does not benefit the model, even during early checkpoints when the model performs poorly on the task. Nevertheless, for MNLI (not learned during pre-training), fine-tuning dramatically improves the model. Interestingly, later checkpoints achieve better results after fine-tuning, even when the performance of the pre-trained model is unchanged. This suggests that the model is, in fact, learning important information during pre-training, but it cannot express that information without fine-tuning.\nOur findings suggest that early stopping in pre-training will not be detrimental to downstream fine-tuning performance, and the benefits of fine-tuning an LLM could exceed the benefits of continued pretraining, which sheds light on the potential of cost-effective training paradigm with less pre-training. However, it is difficult to directly identify such a stopping criteria without fine-tuning intermediate checkpoints; the improvement trend is invisible before fine-tuning the checkpoints. Future work may reveal other signals of pre-training behavior that correlate with downstream task performance after fine-tuning. Overall, when resource-intensive pre-trained LLMs are not available, fine-tuning models on models with less pre-training may be a reasonable practical choice for obtaining a high-quality model."}, {"title": "Supervised Fine-Tuning: What does the model learn and forget?", "content": "What exactly is the model learning during fine-tuning such that it shows abilities in pre-trained models for some tasks but provides no benefit for other tasks? We analyze the supervised fine-tuning process to understand what is learned and what is forgotten. Specifically, we explore three dimensions: task format, task transfer, and domain knowledge."}, {"title": "Task Format", "content": "Sclar et al., 2023 show that LLMs are extremely sensitive to prompt perturbation in few-shot settings. More broadly, extensive work on prompt engineering reveals the sensitivity of models to task format. We hypothesize that fine-tuning fits the model to a specific task format, resulting in higher performance when the evaluation set matches this format. To test this hypothesis, we vary the task format to either match the training format, use a different format, or rely on instructions. We carefully construct three different prompt formats for the following settings. 1) Default is the same format used for training, where we expect the model to benefit from learning the task format; 2) In contrast, IO format reflects a common way of performing supervised fine-tuning by incorporating only unprocessed input and output; 3) Instruct uses a human-readable instruction template to format the input. Table 4 shows an example of each format. Checkpoint performance before and after fine-tuning is shown in Figure 5.\nIn the early pre-training steps, aligning the task format with fine-tuning data seems to play a crucial role. The model does not yet have enough information to overcome the differences between the training and test formats. However, when fine-tuned on later pre-training checkpoints, the model gradually becomes more flexible with different task formats, suggesting that model sensitivity to prompt formatting observed may be resolvable with more pre-training and a fine-tuning stage. In this view, fine-tuning teaches the model how to format a response for the task."}, {"title": "Task Transfer", "content": "Numerous studies examine model forgetting, where further model training causes improvements on some tasks but degradation on others (Mehta et al., 2023). We evaluate model forgetfulness by examining whether the model does worse on some tasks after fine-tuning for other tasks. Specifically, we divide our tasks into two types: classification and generation. We notate the training datasets as \\(D_t\\) and the evaluation datasets as \\(D_E\\). We represent the performance of a pre-trained model (BASE) on checkpoint i as Perf\\(_\\text{BASE}\\)(d) where an evaluation dataset d\u2208 D\\(_E\\), and the performance of the i-th checkpoint fine-tuned on dataset t \u2208 D\\(_t\\) be Perf\\(_t\\)(d). To normalize the effect caused by uneven performance across different datasets, we compute the mean ratio of change (MRC) in performance for each checkpoint as follows.\nMRC = \\( \\frac{1}{|D_E \\setminus \\{t\\}|} \\sum_{\\forall d \\in D_E, d \\ne t} \\frac{\\text{Perf}_t(d) - \\text{Perf}\\_{\\text{BASE}}(d)}{\\text{Perf}\\_{\\text{BASE}}(d)} \\)\nModels fine-tuned on classification tasks and evaluated on generation tasks decrease on average 61.4% compared to models that are never fine-tuned. In contrast, models fine-tuned on generation tasks can still perform the same as the BASE model on classification tasks, with a 0.3% MRC, which is not statistically significantly different from a 0% change. Our findings on all pre-training checkpoints align with the findings of Yang et al. (2024) on the final checkpoint of LLAMA-7B.\nRegardless of the pre-training stage, a model can maintain classification abilities when trained for generation, but it loses its generation abilities when trained for classification. This is perhaps not surprising given that classification tasks can be seen as a subset of generation, while the reverse is not"}, {"title": "Domain Knowledge", "content": "Finally, we explore how a model's generalization ability is affected by fine-tuning by inspecting whether the model forgets the domain knowledge it had before fine-tuning due to learning other abilities. An example of OOD model performance is shown in Figure 6, and the mean change ratio by datasets is presented in Figure 7.\nThe model does not benefit equally from the in-domain fine-tuning: all NLI datasets experience a boost when fine-tuning on MNLI, while fine-tuning on Paws is detrimental to other paraphrase detection datasets. This implies that both forgetting and learning are happening: the model learns to perform the task with in-domain knowledge, but it may, in turn, forget information more distant from what is learned in fine-tuning. Questions remain, however, about whether there are different stages of learning and forgetting during fine-tuning and whether the model picks up different tasks in various stages, which requires further study of fine-tuning dynamics.\nOverall, across these three lenses, we find that fine-tuning, although teaches a model how to perform a task, can sacrifice generalization abilities if such ability is not needed for the fine-tuned task. For some datasets learned with pre-training alone, the model can easily understand the task format, and the nature of the task is probably supported by the pre-training objective. For tasks that can only be learned with subsequent fine-tuning, the model may require additional examples to adapt to different task formats, or the task itself may be inconsistent with the pre-training objective."}, {"title": "Discussion", "content": "Our study uses fine-tuning of pre-training model checkpoints to understand the dynamics of pre-training and fine-tuning on model performance. While our insights suggest directions for future work, we note important limitations inherent in our experiments. This study considered a single, relatively small LLM on less than a dozen datasets, and still consumed thousands of hours of GPU training time at significant expense. Future work needs to confront these issues on larger models and more datasets. We believe our experiments can focus future work on specific experiments with larger models.\nSome datasets can be learned without fine-tuning. We discover a dichotomy between datasets. Some are learned during model pre-training, while others show no improvements during pre-training. Furthermore, the datasets learned during pre-training do not benefit from fine-tuning. This observation, combined with our study about what is learned during fine-tuning (Section 6) suggests that some tasks are presented in a manner that aligns with what the model sees during pre-training, and thus fine-tuning provides no additional information. While we could identify what about the tasks placed them in the learned or not learnable during pre-training group, it may be possible to format tasks in a manner that better aligns with pre-training and makes them learnable.\nPre-training models can improve in undetectable ways without fine-tuning. Some datasets are not learnable during pre-training but benefit significantly from fine-tuning (\u00a74). However, these datasets still benefited from additional pre-training, even though those benefits were not revealed without fine-tuning (\u00a75). Clearly, the model is learning important information about the task, even though it cannot express that information. The identification of a measure available during pre-training that correlated with post-fine-tuning task performance could be used to guide pre-training and produce models that did better post-fine-tuning. Perhaps there is a way in which information about these tasks can be included in pre-training, allowing the model to better utilize the massive amount of pre-training data. For example, early stopping during pre-training could lead to better utilization of limited training resources if we knew when to stop.\nFine-tuning teaches task format but leads to forgetting unused abilities. Our results show that fine-tuning guides the model to understand the format and complete a given task. As this information diminishes, the model's overall ability improves. However, fine-tuning comes at the expense of other model abilities, such as the capability of performing on tasks or domains that are unrelated to the"}, {"title": "Conclusion", "content": "Our experiments explore the relationship between fine-tuning and pre-training LLMs. Our findings span from the latent benefits of pretraining to model learning and forgetting during fine-tuning. Our results show that the model can rapidly pick up the datasets that it could not solve during fine-tuning with only a small amount of supervision. In the meantime, we identify the aspects that LLM learns and forgets during supervised fine-tuning: task format, task solution, and domain knowledge. Overall, our results demonstrate the value of analyzing language model training dynamics, and we would like to call for the release of pre-training checkpoints to aid future studies."}, {"title": "Limitations", "content": "We discuss the weaknesses and limitations in the following section.\nComputing Resource Due to computational constraints, we can only conduct experiments on a 1B model and a limited amount of datasets. The amount of GPU hours spent for each experiment in this study is listed in Table 3.\nAvailbility of Pre-training Checkpoints This study would benefit significantly from including a broader spectrum of models, but the public pre-training checkpoint releases are limited. Open-source LLMs with intermediate checkpoint release include OLMo (Groeneveld et al., 2024), TinyLLAMA, RedPajama-Incite, OpenLM, and Pythia. After a series of preliminary experiments, we select these models' best-performing and robust families.\nScaling Law Recent research shows that the model may resemble emergent capability (Wei et al., 2022) when scaled to a certain size. Comparatively, Hassid et al., 2024 find that smaller model is capable of outperforming its larger variant when the computing resources is controlled. To avoid potential confounding factors caused by quantization, our experiments are only conducted on the one-billion model, which may, therefore, conceal the emergent capability brought by larger models while at least giving insights about the potential of small models."}, {"title": "Analysis Protocol", "content": "Wu et al., 2023 show that the evaluation result may be affected by samples that have been memorized by the model during training instead of revealing the reasoning capability. The only analysis protocol used is the downstream performance of a trained model. More investigation should be done into model internals during pre-training dynamics and how they relate to the effects of fine-tuning.\nTraining Paradigm Although multiple tuning strategies exist, to create a fair comparison environment where checkpoints received the same amount of training, models are fine-tuned with a fixed amount of epochs in this work. On different pre-training stages, the model may converge at a different speed. Further study can be done to study the effect of pre-training on different fine-tuning methods or fine-tuning dynamics in different pre-training stages. We only explored the scenario of full-parameter fine-tuning. Whether parameter-efficient fine-tuning or human preference tuning will lead to a different conclusion also remains an open question.\nRandomness In this study, we only assess uncertainty with Bootstrap during evaluation. However, uncertainty may emerge during training, which poses optimizer initialization and data ordering. Due to the computational constraints, we cannot reduce the randomness factor on this angle."}]}