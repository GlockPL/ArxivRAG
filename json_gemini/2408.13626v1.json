{"title": "Towards Case-based Interpretability for Medical Federated Learning", "authors": ["Laura Latorre", "Liliana Petrychenko", "Regina Beets-Tan", "Taisiya Kopytova", "Wilson Silva"], "abstract": "We explore deep generative models to generate case-based explanations in a medical federated learning setting. Explaining AI model decisions through case-based interpretability is paramount to increasing trust and allowing widespread adoption of AI in clinical practice. However, medical AI training paradigms are shifting towards federated learning settings in order to comply with data protection regulations. In a federated scenario, past data is inaccessible to the current user. Thus, we use a deep generative model to generate synthetic examples that protect privacy and explain decisions. Our proof-of-concept focuses on pleural effusion diagnosis and uses publicly available Chest X-ray data.", "sections": [{"title": "I. INTRODUCTION", "content": "Case-based interpretability is vital in explaining medical Artificial Intelligence (AI) model decisions. Generating explanations for AI model decisions is paramount to increasing trust and allowing widespread adoption in clinical practice [1]. We can find several approaches to producing explanations in the scientific literature, from saliency maps (highlighting image pixels driving the decision) to textual explanations [2]. However, in the context of radiology, case-based interpretability arises as a very natural interpretability strategy since it mimics the way radiologists would also explain or come up with a decision [3]. When in doubt over a suspected disease condition, radiologists usually turn their attention to searching past cases and, based on disease-similarity to these past cases, reach a clinical decision. Looking for adequate past cases is a time-consuming process that can be automated through the use of an Al retrieval model. Al retrieval models usually consist of finding a latent semantic representation of the original data (i.e., images) that is representative and adequate to measure the distance between cases and then evaluate disease similarity. All this process, though, starts from the premise that past data (which might have been used for training) is always available to the retrieval algorithm.\nDue to privacy concerns and regulations, sharing medical data is sometimes prohibitive, leading to a lack of heterogeneous training data and, consequently, to non-robust AI models that fail to generalize. It has been shown that\neven radiological images such as chest X-rays may contain biometric information that can be exploited by deep learning models to expose identity, therefore, infringing on data protection regulations [4]. In order to overcome this limitation in data sharing, researchers have come up with federated learning approaches, which leverage all available data without the need to share it between institutions [5]. This process works by distributing the model training to the institutional data owners and aggregating their results. Due to the privacy by default property of federated learning, it is conquering the fields of medical imaging and drug discovery [6].\nEven though federated learning's potential to overcome some of the current AI flaws is currently widely recognized, it also introduces new challenges. The decentralized nature of federated learning guarantees compliance with privacy regulations but, at the same time, inhibits data access and inspection [7]. Non-accessible data means that identifying bugs or detecting biases is impossible following conventional approaches. The same is true for case-based explainability. Since representative past cases may not be accessible for retrieval, as they might be stored in a different site, the whole concept of case-based interpretability loses its relevance, with consequences in the trust and understanding of AI classification model decisions. Moreover, the case-based interpretability role can be more extensive than just explaining decisions by also working as a computer-aided diagnosis. In this work, we propose a proof-of-concept based on deep generative models to generate synthetic case-based explanations to be used in a medical federated learning setting."}, {"title": "II. MATERIALS AND METHODS", "content": "Posterior-anterior (PA) chest radiographs from four large and publicly available databases (CheXpert [8], MIMIC-CXR-JPG [9], BRAX [10], and VinDr-CXR [11]) were used. Since pleural effusion was present in all of the considered datasets, being one of the conditions with a more balanced distribution, it was chosen as the diagosis to perform and explain in our proof-of-concept. From each dataset, we only selected images labelled as having pleural effusion or not having pleural effusion, discarding all uncertain diagnoses. Based on the available data, we created an in-distribution test set (composed of 20% of data from each of the local datasets: CheXpert, BRAX, and VinDr-CXR) and an out-of-distribution test set (composed of MIMIC-CXR-JPG data). In the federated learning setting, each dataset works as an independent hospital, i.e., as a local client."}, {"title": "B. Method", "content": "Our proposed methodology to obtain case-based explanations for federated learning models is presented in Fig. 1. The first step requires training a privacy-preserving discriminative federated model used for diagnosis (in our case, pleural effusion prediction). The local discriminative models, and thus, the global model used, were based on the DenseNet-121 architecture [12]. The global model is initialized with ImageNet [13] pre-trained weights. In each round of the federated learning process, the central server distributes the current global model to the participating client devices (i.e., local sites corresponding to CheXpert, VinDR, and BRAX). Each client then independently trains the model using its local dataset. After each local training round, the global model is updated using federated averaging [14]. Differential privacy (DP) was implemented locally by using differential privacy optimizers (gradient noise injection techniques): DP-SGD and DP-Adam [15, 16]. By clipping and adding noise to the local client gradients, we avoid the risk of information leakage. The pseudo-code describing the training process in detail is presented in Algorithm 1. Our final model was first trained for 20 global epochs with frozen weights, updating just the last layer. Afterwards, the entire model was fine-tuned for 20 global epochs more. We considered 3 epochs for the local training process. Class weights corresponding to the inverse of the class frequency are used at each local site to mitigate the impact of class imbalance.\nThe second step of our methodology requires the training of a generative model to generate case-based explanations. In our experiments, we considered Medfusion [17] as our generative model, given its previous promising results on Chest X-rays. A Medfusion model was trained for each of the clients, first training the Autoencoder, and later the Diffusion model. 400 samples with t = 150 sampling steps, 200 with label 0 and 200 with label 1 were generated with each of the local models to create the synthetic dataset to be used to retrieve similar disease-matching cases as case-based explanations.\nWhen a new case needs to be classified by the discriminative model, the three most similar synthetic samples for each of the clients are selected from the synthetic dataset to show them as case-based explanations. Similarity between images is computed based on the Normalized Euclidean distance in the feature space of the previous to the last layer of the model, which is formalized as:\n$d_{DP-FL}(I_t, I_s) = ||F(\\theta_{DP-FL}, I_t) \u2013 F(\\theta_{DP-FL}, I_s)||$", "equations": ["d_{DP-FL}(I_t, I_s) = ||F(\\theta_{DP-FL}, I_t) \u2013 F(\\theta_{DP-FL}, I_s)||"]}, {"title": "C. Evaluation", "content": "Baselines: We considered a baseline for classification performance and a baseline for image retrieval. To provide an upper bound on the classification performance that could be achieved, we trained the same exact discriminative model but in a centralized setting (i.e., without the difficulties introduced by the different local distributions and differential privacy). As our retrieval baseline, we considered the standard structural similarity index measure (SSIM). The SSIM was computed between the query image and the retrieved case-based explanations. Since higher SSIM values represent higher similarity, images with higher SSIM were ranked first.\nMetrics: Due to the imbalance of the dataset, classification performance was measured by the F1-score. The quality of the retrieval was quantitatively evaluated by computing the normalised Discounted Cumulative Gain (nDCG) (Eq. 2) of the retrieval model ranking, compared to the ground truth ranking of the radiologist. $IDCG_p$ is the maximum possible value of the DCG metric, obtained when the ranking of the method is exactly the same as our ground truth. The subscript p is the number of retrieved images considered for the evaluation (e.g., p = 9). Finally, $rel_i$ represents the relevance value assigned to the ranking position i, with the least similar image having a relevance of 1 and the most similar image having a relevance of 5.\n$nDCG_p = \\frac{DCG_p}{IDCG_p}$\nwhere $DCG_p = \\sum_{i=1}^{p} \\frac{2^{rel_i} - 1}{log_2(i+1)}$", "equations": ["nDCG_p = \\frac{DCG_p}{IDCG_p}", "DCG_p = \\sum_{i=1}^{p} \\frac{2^{rel_i} - 1}{log_2(i+1)}"]}, {"title": "III. RESULTS", "content": "We performed pleural effusion classification experiments considering a baseline centralized approach and the developed federated model. As expected, due to the distinct client distributions and inclusion of differential privacy, the federated learning model led to slightly worse results (Table I). We also generated saliency maps for both models by computing the SHAP values [18]. As can be observed in Fig. 2, when correctly predicting a pleural effusion case, both models highlighted the same regions."}, {"title": "IV. DISCUSSION AND CONCLUSIONS", "content": "Ensuring interpretability in the upcoming decentralized federated learning paradigms is paramount to increase trust and allow widespread AI adoption in clinical practice. In the context of radiology, case-based explainability is the most natural interpretability strategy. In this work, we investigated the use of a latent denoising diffusion probabilistic model in a federated chest X-ray classification task, demonstrating its potential to generate realistic and meaningful case-based explanations for a privacy-preserving federated learning discriminative model. While it is established how to retrieve case-based explanations in a standard centralized learning scenario, this is, to the best of our knowledge, the first study that addresses the challenge of generating case-based explanations in a privacy-preserving federated learning setting, where access to the entire data collection is restricted. However, to adopt the proposed method into a real-world medical application, a variety of further research is still needed. Future research should focus on exploring privacy-preserving generative models to guarantee that no identity information present in the client's data is leaked in the generated images. To be completely compliant with data protection regulations, we must ensure that the synthetic case-based explanations are not too similar to the sensitive training data nor expose identity. That could be done by either adding differential privacy or incorporating adversarial identity loss functions (e.g., [19]) into the training of the generative models."}]}