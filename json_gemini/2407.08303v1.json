{"title": "DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception", "authors": ["Xiaotong Li", "Fan Zhang", "Haiwen Diao", "Yueze Wang", "Xinlong Wang", "Ling-Yu Duan"], "abstract": "Existing Multimodal Large Language Models (MLLMs) increasingly emphasize complex understanding of various visual elements, including multiple objects, text information, and spatial relations. Their development for comprehensive visual perception hinges on the availability of high-quality image-text datasets that offer diverse visual elements and throughout image descriptions. However, the scarcity of such hyper-detailed datasets currently hinders progress within the MLLM community. The bottleneck stems from the limited perceptual capabilities of current caption engines, which fall short in providing complete and accurate annotations. To facilitate the cutting-edge research of MLLMs on comprehensive vision perception, we thereby propose Perceptual Fusion, using a low-budget but highly effective caption engine for complete and accurate image descriptions. Specifically, Perceptual Fusion integrates diverse perception experts as image priors to provide explicit information on visual elements and adopts an efficient MLLM as a centric pivot to mimic advanced MLLMs' perception abilities. We carefully select 1M highly representative images from uncurated LAION dataset and generate dense descriptions using our engine, dubbed DenseFusion-1M. Extensive experiments validate that our engine outperforms its counterparts, where the resulting dataset significantly improves the perception and cognition abilities of existing MLLMs across diverse vision-language benchmarks, especially with high-resolution images as inputs.", "sections": [{"title": "Introduction", "content": "Multimodal Large Language Models (MLLMs) [31, 12, 35, 3, 2, 37, 60, 9, 20, 42] have made remarkable strides in multi-modal understanding and reasoning by aligning the Large Vision Models (LVMs) [55, 44, 10] and Large Language Models (LLMs) [24, 52, 66]. To fully harness the capabilities of MLLMs in comprehensive visual perception, there is a critical demand for high-quality image-text datasets that provide dense and thorough descriptions across a wide range of visual elements. Such hyper-detailed datasets are essential for training MLLMs to accurately interpret and interact with diverse visual information. However, the scarcity of such rich datasets currently hampers the progress of the MLLM community. Given these challenges, it is crucial to pioneer a practical and efficient route to craft highly detailed image descriptions for comprehensive perception.\nAs the saying goes, \"an image is worth a thousand words\". Images contain various visual elements of different granularities that are essential yet challenging to harness. Employing human labor [41, 16] or advanced GPT-4V [41, 17, 8, 7] is one feasible option to generate accurate, reliable, and\nIn this paper, we meticulously design a pipeline for comprehensive multimodal understanding, named Perceptual Fusion, integrating diverse vision experts as image priors and adopting a low-budget MLLM as a centric pivot for information fusion. Under this strategy, we exploit LAION [43], a valuable public resource, and delicately extract 1 million diverse and high-quality data. Firstly, we feed supplements from visual experts into the advanced GPT-4V and acquire 100K intricately detailed descriptions. With this meta dataset as guidance, we can efficiently develop a strong captioning engine capable of integrating strengths from multiple sources, including object detection, image tagging, text recognition experts for thoroughly comprehending image content. Leveraging this multimodal pivot, we can further construct a scalable, reliable, and high-quality pre-trained dataset, named DenseFusion-1M, enriched with abundant OCR information, accurate object and position recognition, and external knowledge. The hyper-detailed image-text data, in turn, enhances the perception of existing MLLMs to achieve better vision-language alignment.\nIn summary, our contributions are listed as follows:"}, {"title": "Related Work", "content": "Large Multi-Modality Models: The development of Large Multi-Modality Models (MLLMs) has witnessed significant advances in the abilities of comprehension and reasoning [31, 30, 12, 70, 37, 35, 9], typically through aligning pre-trained Large Vision Models (LVMs) [24, 52, 25, 66] with Large Language Models (LLMs) [10, 55, 44]. The pioneer works BLIP [31, 30], LLaVA [37, 35, 36], and Qwen series [37, 35] bridge the modality gaps through resamplers or MLP projectors, and obtain promising performances. Besides, Emu series [53, 51] exhibits strong in-context learning ability for multimodal content. Recently, there has been an emergent trend in developing high-resolution MLLMs [60, 36, 47, 20, 33, 61]. Among them, Monkey [33] resizes input images to fixed resolutions and divides them into multiple 448\u00d7448 patches, which are then processed by a pre-trained vision encoder. Moreover, CogAgent [20] utilizes low-resolution and high-resolution image encoders to recognize tiny visual elements inside a large image. LLaVA-NEXT [36], dubbed as LLaVA-1.6, introduces dynamic image aspect ratios and partitions the original images into multiple sub-images to capture more visual details, while LLaVA-UHD [60] divides images into smaller variable-sized slices for efficient and extensible encoding. Notably, Scaling on Scales (S2) [48] straightly extracts multi-scale features through image wrapping and rescaling without increasing image tokens. These high-resolution MLLMs capture tiny visual clues and benefit from meticulous image descriptions.\nHence, we aim to create hyper-detailed image annotations to enhance understanding of intricate visual elements and provide more accurate visual-language alignment.\nImage-Text Datasets: Large-scale image-text datasets, e.g. LAION [43, 22], CC12M [6], Visual Genome [27] and YFCC [54], have effectively facilitate the development of vision-language pre-training. Along this line, BLIP-LAION [31] presents the synthetic short descriptions by the BLIP model, while LLaVA [37] and LLaVAR [67] prompt the text-only GPT-4 with visual information to generate conversations. Moreover, LaCLIP [13] rewrites the caption via ChatGPT through its in-context learning capability, while CapsFusion [64] leverages fine-tuned large language models to consolidate and refine information from both web-crawled and synthetic captions. To acquire detailed description datasets, recent studies seek help for the advanced GPT-4V model or human-in-the-loop strategy [8, 7, 58, 57, 17]. Among them, ShareGPT4V [8] comprises 100K captions from GPT-4V and employs an advanced captioner to produce an additional 1.2 million synthetic captions, while ALLaVA [7] directly leverages the advanced GPT4V's capabilities to create a synthetic dataset with detailed captions and instructional data. For region-level vision recognition, GLaMM [46] and all-seeing projects [58, 57] advance conversation generation with detailed region-level understanding and semantic tags. Lastly, ImageInWords (IIW) [16] presents 9k hyper-detailed captions through a human-in-the-loop annotation framework. DOCCI [41] instructs human annotators to create 19k comprehensive descriptions. Despite detailed visual annotations, their human-in-the-loop strategies require expensive labor costs and restrict the dataset scales. In contrast, we construct a low-budget caption engine empowered by diverse vision experts that can automatically generate large-scale and hyper-detailed image-text datasets at a negligible cost."}, {"title": "Methodology", "content": "In this section, we introduce the methodology design for constructing the dataset DenseFusion-1M. Specifically, we detail the data pre-processing pipeline for filtering high-quality image sources, the perceptual fusion procedure from vision experts, and the construction of the caption engine."}, {"title": "Data Processing", "content": "Establishing a high-quality dataset for comprehensive perception necessitates access to a large-scale data resource that encompasses a wide range of image categories and rich visual semantics.\nUnlike methods such as ShareGPT4V[8], which meticulously curate images from specialized sources including COCO[34], SAM[26], Textcaps [49], etc, we opt to the widely-used LAION-2B [43] dataset, which naturally sources its diverse content directly from the wild internet, including different image categories like photos, posters, powerpoint, infographics, and more. Moreover, the LAION open-source dataset supports further academic research by offering readily accessible data that has been re-annotated by various studies [13, 8, 64, 22].\nDespite its massive scale, LAION is still uncurated and contains significant issues about duplication [59], hindering both image diversity and quality. To address this, we mainly focus on two critical factors for data processing. Firstly, higher resolution images are prioritized since they generally provide richer visual content and more abundant semantics. Secondly, we emphasize the selection of representative images to preserve a greater diversity of visual content within the same data scale.\n\u2022 High Resolution Image Selection. Images with a short-edge resolution less than 448 are filtered out to ensure the richness of the image content. Following this approach, approximately 500M images are retained from the initial 2B images, resulting in the subset named DenseFusion-500M.\n\u2022 Semantic Clustering and De-duplication. To maximize the diversity of image distribution, we follow SemDeDup[1] to remove semantically duplicated images from DenseFusion-500M. Specifically, we employ k-means clustering on images features extracted via EVA-CLIP [52] to create 50,000 clusters. We set the threshold \u2208 = 0.4 to remove semantic duplicated images within each cluster, yielding an image set of 14 million images. Finally, we select the top 20 images from each cluster to create our DenseFusion-1M dataset."}, {"title": "Perceptual Fusion", "content": "Comprehensive visual perception is a prerequisite for multimodal understanding and reasoning. This perception ability can be achieved through extensive, detailed, and accurate alignments in image-text pre-training data. Despite the feasibility of current MLLMs [37, 31] for image captioning, they still struggle to provide meticulous descriptions. (1) Generalist MLLMs are designed for executing various instructions and are not intended for specific captioning tasks, especially for well-rounded image captioning. (2) Existing specialist caption engines lack a strong ability for comprehending and describing various visual elements inside high-resolution images, due to their inherent drawbacks in identifying all kinds of visual characteristics."}, {"title": "Mixture of Visual Experts", "content": "With the advancements in computer vision, numerous visual experts of various perceptual tasks have emerged and demonstrate outstanding capabilities within their respective domains [40, 14, 68, 21]. These models provide valuable intermediate perceptual information for image understanding. Therefore, comprehensively understanding the diverse visual elements in complex scenes can benefit from the collaboration of different specialists. In this section, we develop a perceptual fusion strategy with assistance from a variety of vision experts.\nThis approach specifically targets areas where generalist MLLMs often show limited perceptual capabilities. Our strategy includes the application of expert techniques in image tagging, object detection, text recognition, and the incorporation of world knowledge. We meticulously select these vision experts based on several key aspects of perception, which are detailed as follows.\n\u2022 Image Tagging: Initially, we attempt to produce scene-level understanding for holistic images, including objects and visual scenes. Specifically, we employ the pre-trained RAM++ [68] that generates expansive tag descriptions over conventionally predefined tag categories. This approach enriches visual tag information and provides accurate scene annotations in overall image understanding, enhancing the recognition of diverse open-vocabulary concepts for object understanding.\n\u2022 Object Detection: Comprehensive understanding relies on the perception ability of various object entities, while current MLLMs suffer from incomplete object perception and inaccurate positioning. Therefore, we utilize two types of specialized detection models to boost hyper-detailed recognition. (1) We employ the closed-set EVA02 [14] detection model trained on LVIS [19] and COCO [34]\nto precisely detect the objects with basis concepts and varying sizes. (2) Meanwhile, we employ the open-set OWL-ViTv2 [40] detection model for capturing objects across broader categories constructed from the tagging classes. Afterward, we retain the objects with high confidence over the predefined threshold, and adopt a balanced sampling strategy to highlight small-scale objects, considering that the generalist MLLMs tend to focus on large-scale objects.\n\u2022 Text Recognition: Text information is crucial for visual understanding, especially for documents, such as documents, posters, tables, and charts. However, generalist MLLMs often overlook some OCR elements and fail to identify text with various font styles and scales accurately. Meanwhile, we find that it is over 70% of the resulting images contain OCR information according to our statistics. Therefore, we employ OCR models [21, 36] to recognize all textual elements within each image, even those with vague text information.\n\u2022 World Knowledge: Although LAION's short captions crawled from the internet sometimes mis-align with image descriptions, they contain a wealth of world knowledge, including visual context, background information, and subtle details, etc. This can help boost the MLLMs' knowledge density and enhance the reasoning abilities. By incorporating these noisy yet rich captions, the models can achieve a deeper, more nuanced understanding of visual content, improving their performance in tasks requiring comprehensive visual and contextual understanding."}, {"title": "Perceptual Fusion Engine", "content": "To obtain precise and comprehensive image descriptions, the widely-used advanced GPT-4V [42] serves as an ideal MLLM with strong visual perception and contextual understanding capabilities. It can generate image descriptions that are further enriched with various visual information from specialized vision experts. Considering its expensive cost of time and finance, we attempt to construct an open-sourced and low-budget caption engine to efficiently mimic its ability for large-scale image captioning. We empirically discover that the perception ability of existing open-sourced caption engine can be enhanced with the assistance of additional visual experts, where they can improve the recognition of small-scale objects and OCR information, guiding our caption engine to focus on often overlooked content and correcting inaccuracies caused by its limited visual perception.\nInitially, we adopt the proficient GPT-4V via manual-tuning prompts to generate image captions with extra visual information as the perceptual fusion guidance. The detailed prompt template can be found in Appendix. We thereby obtain 100K hyper-detailed image descriptions, i.e. DenseFusion-4V-100K. Using this meta dataset as guidance, we train our caption engine to learn from GPT-4V's characteristics and generate highly detailed image descriptions, as depicted in Figure 3. Our caption engine is based on LLaVA-1.6 (7B) [36], utilizing high-resolution images as inputs to ensure better visibility of detailed visual clues. The expertise of visual specialists are extracted offline and adopted as contextual information for caption engine. This process allows our engine to capture various visual clues effectively, enhancing its perception abilities by incorporating insights from vision experts. Consequently, it accurately identifies a wide range of objects and detailed textual information, resulting in image annotations with high information density."}, {"title": "Dataset Description", "content": "Utilizing the perceptual fusion pipeline, we incorporate insights from multiple visual experts into producing hyper-detailed image descriptions, resulting in the following datasets: (1) DenseFusion-4V-100K. GPT-4V generated 100K captions. (2) DenseFusion-1M. Scaling up to 1 million detailed captions by our caption engine. We conducted a statistical analysis to show the detailed dataset information in Table 1. On average, the captions are 190 words long and consist of 11 sentences with dense descriptions. As shown in the category distribution in Figure 4(b), the DenseFusion dataset contains diverse categories such as photos, visual art, commercial design, and infographics, making it a valuable resource with various image types. We employ LLaVA-1.5 [35] as a generalist MLLM for the category classification task. Generating hyper-detailed captions is fundamental to various multi-modal research tasks, as it facilitates the translation of images into language seamlessly. This capability presents significant potential in applications, e.g., vision-language contrastive pre-training [24, 52], multimodal alignment in MLLMs [2, 37, 4], and text-conditioned image generation [45]."}, {"title": "Experiments", "content": "In this section, we introduce the implementation details and compare the model trained by our DenseFusion-1M dataset with state-of-the-art MLLMs across diverse vision-language benchmarks. Finally, we validate the effectiveness of perception fusion qualitatively and quantitatively."}, {"title": "Implementation Details", "content": "Caption Engine. To explore the detailed visual clues inside each image, we adopt LLaVA-1.6 (7B) [36] to handle the high-resolution image inputs. For the meta dataset, we utilize GPT-4V to annotate the randomly selected 100K images from our picked 1M LAION data, thereby boosting our engine supported by various experts and producing high-quality annotations to mimic advanced GPT-4V. This supervised fine-tuning stage takes around ~ 5.5 hours on 4 nodes of 8\u00d7A100 (40G) for 2 epochs. The visual knowledge from diverse visual experts are extracted and integrated as contextual information for the perception fusion prompt. Then we utilize the caption engine with the efficient deployment tool SGLang [69] to generate 1M data with enhanced multimodal perception.\nEvaluation Benchmarks. To verify the efficacy of the provided DenseFusion-1M, we adopt these captions during the pre-training stage and follow the setup of LLaVA-1.5 [35] on various visual question answering (VQA) and multi-modality understanding benchmarks for evaluation, such as ScienceQA [39], TextQA [50], VQAv2 [18], GQA [23], SEED [29], MMBench [38], MME [15], POPE [32], MM-Vet [65], that covers a wide range dimensions for evaluating model abilities.\nModel Configuration. To verify the effectiveness of DenseFusion-1M, we adopt it in the pre-training stage for vision-language alignment. The model is based on LLaVA-1.5 [35], using the vision encoder CLIP-ViT-L/14-336 [24] and the large language model (LLM) Vicuna [10] respectively. The vision encoder and LLM are connected by a two-layer multi-layer perception (MLP) projector. We utilize the approach of S2 [47] for training the high-resolution MLLM, which is efficient in handling high-resolution inputs without increasing image tokens. We follow LLaVA-1.5 [35] that comprises a two-stage training stages. (a) Pre-training Stage. We first only train the projector for pre-alignment, then we conduct pre-training with a trainable vision encoder of the last 12 layers to further improve the perception ability. (b) Instruction-tuning Stage. For fair comparison, we follow LLaVA-1.5 [35] and adopt the original LLaVA-mix-665K for instruction tuning, including GPT-generated and academic-oriented datasets. The detailed training recipe is shown in supplementary material."}, {"title": "Main Results", "content": "Compared Models. We report the experiment results against current state-of-the-art MLLMs, including Qwen-VL [4], InstructBLIP [11], mPLUG-Owl2 [62], InternVL [9], LLaVA-1.5 [35]. In particular, we compare our strategies with existing caption datasets or engines, e.g. ShareGPT4V [8], LVIS-4V [56]. To fully exploit its potential, we conduct comparisons under high-resolution settings with recent MLLMs, including Monkey [33], LLaVA-1.6 [36], and Scaling on Scales [47] (S2).\nExperiment Results. (1) Conventionally, Table 2 demonstrates that our meticulous descriptions significantly improve baseline models, providing solid and consistent benefits across all vision-language benchmarks, particularly for text-recognition scenes, e.g. TextVQA. Notably, our dataset originates from the generic LAION, which has no direct connection to the validation domains. Despite this, our strategies outperform ShareGPT4V, which uses images from COCO and VG that share a similar image distribution with the evaluation benchmarks, like VQAv2 and GQA. (2) Additionally, we observe that the potential benefits of our dataset are not fully exploited due to limited input resolutions, making MLLMs challenging to extract hyper-detailed image clues. To address this, we conduct further experiments using the high-resolution MLLM, Scaling on Scales [47] (S2), which performs multi-scale aggregation on high-resolution inputs without increasing the number of image tokens. Even with a fifth of visual tokens of LLaVA-1.6 and requires no additional instruction tuning data, LLaVA-S\u00b2 trained by our data achieves better performance than the state-of-the-art LLaVA-1.6 and exhibits higher forward efficiency. Besides, we reproduce LLaVA-S\u00b2 using 1.2M pre-training data from ShareGPT4V [8], named ShareGPT4V-S2, and we do not introduce additional supervised fine-tuning data for fair comparisons. Our dataset shows further performance gains compared to the low-resolution version, demonstrating the superiority of our dataset in scenarios requiring hyper-detailed visual elements.\nFrom the above results, we observe that (1) a high-quality image-text dataset is crucial during pre-training to enhance alignment across modalities before learning specific instruction patterns; (2) meticulous and accurate image descriptions are essential for high-resolution vision perception. Low-resolution MLLMs easily reach saturation due to blurred visuals and difficulty in exploring detailed clues. Therefore, meticulous image annotation is a promising direction for enhancing the hyper-detailed perception and reasoning capabilities of multimodal models."}, {"title": "Ablation Study", "content": "Perceptual Fusion. Generalist MLLMs occasionally exhibit inherent drawbacks in comprehensive perception, e.g., omitting objects and weak in text recognition. For time saving, we performed the ablation study using a subset of 100K data points from DenseFusion-1M as default setting. It is observed in Table 3, our strategy can effectively alleviate these issues, bringing substantial improvements on different benchmarks, especially in TextVQA with rich OCR information. We note"}, {"title": "Limitation and Discussion", "content": "For comprehensive visual perception, we propose perceptual fusion, efficiently integrating the insights from visual experts and constructing the DenseFusion-1M dataset with well-rounded descriptions. Despite its promising results, some issues could be improved: (1) Limited by the current capacity\nof MLLMs, describing all visible information in an over-complicated image perfectly is still hard to be ensured. (2) The information gathered by visual experts is inevitably noisy; therefore, only high-confidence feedback is retained to ensure accuracy. This can be enhanced by incorporating more sophisticated vision experts. (3) Given the caption engine's contextual capabilities, only the most crucial visual base models are selected to ensure the full exploitation of contextual information. As the language model's contextual understanding advances, additional details, such as dense region comprehension, can be progressively integrated."}, {"title": "Conclusion", "content": "In this paper, we tackle the challenge of limited high-quality image-text data by developing a low-budget caption engine for high-resolution images and hyper-detailed captions. Our strategy involves curating a dataset from the LAION-2B corpus, followed by a perceptual fusion pipeline that guides a multimodal model to integrate information from various vision experts and thereby yields one million well-rounded descriptions, dubbed DenseFusion-1M. We believe that such an extensive image-text dataset, characterized by its hyper-detailed nature, would substantially enhance the capabilities of MLLMs by enabling more effective alignment between visual and textual data."}, {"title": "Overview", "content": "Dataset of DenseFusion-1M will be open sourced at https://huggingface.co/datasets/\nBAAI/DenseFusion-1M. In this Appendix, we present brief description of our dataset in Sec. B.\nSec. C presents implementation details of our framework. Besides, more examples and results are\nvisualized in Sec. D."}, {"title": "Dataset", "content": "The dataset, named DenseFusion-1M, is a large-scale image description dataset designed to enhance\nthe perceptual abilities of Multimodal Large Language Models (MLLMs). It contains 1 million\nhyper-detailed image descriptions derived from a subset of the LAION dataset, carefully curated and\nannotated using our caption engine with perceptual fusion that integrates diverse vision experts."}, {"title": "Implementation", "content": "The main training implementations are outlined in the primary paper. In this section, we detail the\nhyper-parameters used to train the MLLM for evaluating our data. During the pre-alignment stage,\nwe exclusively train the projector, resulting in more stable and slightly enhanced performance. In the\npre-training phase, we unfreeze the Vision Encoder (VE) for the last 12 layers, the Language Model\n(LM), and the projector. For instruction tuning, we utilize the original data from LLaVA-1.5 and the\nLLaVA-mix-665K instruction tuning dataset to fine-tune both the projector and language model."}, {"title": "Prompt Engineering", "content": "The constructing pipeline leverages prompt engineering to generate hyper-detailed image descrip-\ntions. This process involves carefully crafting prompts that guide the advanced GPT-4V to produce\ncomprehensive and accurate annotations. The prompts are designed to integrate insights from various\nvision experts, enhancing the overall quality and granularity of the dataset."}, {"title": "Visualizations on DenseFusion-1M", "content": "We provide more examples of image captions in Tab. 7 and Tab. 8. Besides, to further evaluate the\nconsistency between original images and generated captions, we use Dall-E 3 [5] to reconstruct the\nimages based on the generated captions. The comparative results from different caption engines\nare illustrated in Fig. 6. Compared to other caption engines, our model demonstrates significant\nadvancements in terms of element consistency, spatial relationships, and accuracy. This also indicates\nthe potential of our datasets for conditional image generation tasks which we leave it for future\nresearch."}]}