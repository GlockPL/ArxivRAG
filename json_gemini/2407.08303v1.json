{"title": "DenseFusion-1M: Merging Vision Experts for\nComprehensive Multimodal Perception", "authors": ["Xiaotong Li", "Fan Zhang", "Haiwen Diao", "Yueze Wang", "Xinlong Wang", "Ling-Yu Duan"], "abstract": "Existing Multimodal Large Language Models (MLLMs) increasingly emphasize\ncomplex understanding of various visual elements, including multiple objects,\ntext information, and spatial relations. Their development for comprehensive\nvisual perception hinges on the availability of high-quality image-text datasets\nthat offer diverse visual elements and throughout image descriptions. However,\nthe scarcity of such hyper-detailed datasets currently hinders progress within the\nMLLM community. The bottleneck stems from the limited perceptual capabilities\nof current caption engines, which fall short in providing complete and accurate\nannotations. To facilitate the cutting-edge research of MLLMs on comprehen-\nsive vision perception, we thereby propose Perceptual Fusion, using a low-budget\nbut highly effective caption engine for complete and accurate image descriptions.\nSpecifically, Perceptual Fusion integrates diverse perception experts as image pri-\nors to provide explicit information on visual elements and adopts an efficient\nMLLM as a centric pivot to mimic advanced MLLMs' perception abilities. We\ncarefully select 1M highly representative images from uncurated LAION dataset\nand generate dense descriptions using our engine, dubbed DenseFusion-1M. Ex-\ntensive experiments validate that our engine outperforms its counterparts, where\nthe resulting dataset significantly improves the perception and cognition abilities\nof existing MLLMs across diverse vision-language benchmarks, especially with\nhigh-resolution images as inputs. The dataset and code are publicly available at\nhttps://github.com/baaivision/DenseFusion.", "sections": [{"title": "1 Introduction", "content": "Multimodal Large Language Models (MLLMs) [31, 12, 35, 3, 2, 37, 60, 9, 20, 42] have made remark-\nable strides in multi-modal understanding and reasoning by aligning the Large Vision Models (LVMs)\n[55, 44, 10] and Large Language Models (LLMs) [24, 52, 66]. To fully harness the capabilities of\nMLLMs in comprehensive visual perception, there is a critical demand for high-quality image-text\ndatasets that provide dense and thorough descriptions across a wide range of visual elements. Such\nhyper-detailed datasets are essential for training MLLMs to accurately interpret and interact with\ndiverse visual information. However, the scarcity of such rich datasets currently hampers the progress\nof the MLLM community. Given these challenges, it is crucial to pioneer a practical and efficient\nroute to craft highly detailed image descriptions for comprehensive perception.\nAs the saying goes, \"an image is worth a thousand words\". Images contain various visual elements\nof different granularities that are essential yet challenging to harness. Employing human labor\n[41, 16] or advanced GPT-4V [41, 17, 8, 7] is one feasible option to generate accurate, reliable, and"}, {"title": "2 Related Work", "content": "Large Multi-Modality Models: The development of Large Multi-Modality Models (MLLMs) has\nwitnessed significant advances in the abilities of comprehension and reasoning [31, 30, 12, 70, 37, 35,\n9], typically through aligning pre-trained Large Vision Models (LVMs) [24, 52, 25, 66] with Large\nLanguage Models (LLMs) [10, 55, 44]. The pioneer works BLIP [31, 30], LLaVA [37, 35, 36], and\nQwen series [37, 35] bridge the modality gaps through resamplers or MLP projectors, and obtain\npromising performances. Besides, Emu series [53, 51] exhibits strong in-context learning ability\nfor multimodal content. Recently, there has been an emergent trend in developing high-resolution\nMLLMs [60, 36, 47, 20, 33, 61]. Among them, Monkey [33] resizes input images to fixed resolutions\nand divides them into multiple 448\u00d7448 patches, which are then processed by a pre-trained vision\nencoder. Moreover, CogAgent [20] utilizes low-resolution and high-resolution image encoders to\nrecognize tiny visual elements inside a large image. LLaVA-NEXT [36], dubbed as LLaVA-1.6,\nintroduces dynamic image aspect ratios and partitions the original images into multiple sub-images\nto capture more visual details, while LLaVA-UHD [60] divides images into smaller variable-sized\nslices for efficient and extensible encoding. Notably, Scaling on Scales (S2) [48] straightly extracts\nmulti-scale features through image wrapping and rescaling without increasing image tokens. These\nhigh-resolution MLLMs capture tiny visual clues and benefit from meticulous image descriptions.\nHence, we aim to create hyper-detailed image annotations to enhance understanding of intricate\nvisual elements and provide more accurate visual-language alignment.\nImage-Text Datasets: Large-scale image-text datasets, e.g. LAION [43, 22], CC12M [6], Visual\nGenome [27] and YFCC [54], have effectively facilitate the development of vision-language pre-\ntraining. Along this line, BLIP-LAION [31] presents the synthetic short descriptions by the BLIP\nmodel, while LLaVA [37] and LLaVAR [67] prompt the text-only GPT-4 with visual information\nto generate conversations. Moreover, LaCLIP [13] rewrites the caption via ChatGPT through its\nin-context learning capability, while CapsFusion [64] leverages fine-tuned large language models to\nconsolidate and refine information from both web-crawled and synthetic captions. To acquire detailed\ndescription datasets, recent studies seek help for the advanced GPT-4V model or human-in-the-loop\nstrategy [8, 7, 58, 57, 17]. Among them, ShareGPT4V [8] comprises 100K captions from GPT-4V\nand employs an advanced captioner to produce an additional 1.2 million synthetic captions, while\nALLaVA [7] directly leverages the advanced GPT4V's capabilities to create a synthetic dataset\nwith detailed captions and instructional data. For region-level vision recognition, GLaMM [46] and\nall-seeing projects [58, 57] advance conversation generation with detailed region-level understanding\nand semantic tags. Lastly, ImageInWords (IIW) [16] presents 9k hyper-detailed captions through\na human-in-the-loop annotation framework. DOCCI [41] instructs human annotators to create 19k\ncomprehensive descriptions. Despite detailed visual annotations, their human-in-the-loop strategies\nrequire expensive labor costs and restrict the dataset scales. In contrast, we construct a low-budget\ncaption engine empowered by diverse vision experts that can automatically generate large-scale and\nhyper-detailed image-text datasets at a negligible cost."}, {"title": "3 Methodology", "content": "In this section, we introduce the methodology design for constructing the dataset DenseFusion-1M.\nSpecifically, we detail the data pre-processing pipeline for filtering high-quality image sources, the\nperceptual fusion procedure from vision experts, and the construction of the caption engine.\n3.1 Data Processing\nEstablishing a high-quality dataset for comprehensive perception necessitates access to a large-scale\ndata resource that encompasses a wide range of image categories and rich visual semantics.\nUnlike methods such as ShareGPT4V[8], which meticulously curate images from specialized sources\nincluding COCO[34], SAM[26], Textcaps [49], etc, we opt to the widely-used LAION-2B [43]\ndataset, which naturally sources its diverse content directly from the wild internet, including different\nimage categories like photos, posters, powerpoint, infographics, and more. Moreover, the LAION\nopen-source dataset supports further academic research by offering readily accessible data that has\nbeen re-annotated by various studies [13, 8, 64, 22].\nDespite its massive scale, LAION is still uncurated and contains significant issues about duplication\n[59], hindering both image diversity and quality. To address this, we mainly focus on two critical\nfactors for data processing. Firstly, higher resolution images are prioritized since they generally\nprovide richer visual content and more abundant semantics. Secondly, we emphasize the selection of\nrepresentative images to preserve a greater diversity of visual content within the same data scale.\n\u2022 High Resolution Image Selection. Images with a short-edge resolution less than 448 are filtered\nout to ensure the richness of the image content. Following this approach, approximately 500M\nimages are retained from the initial 2B images, resulting in the subset named DenseFusion-500M.\n\u2022 Semantic Clustering and De-duplication. To maximize the diversity of image distribution,\nwe follow SemDeDup[1] to remove semantically duplicated images from DenseFusion-500M.\nSpecifically, we employ k-means clustering on images features extracted via EVA-CLIP [52] to\ncreate 50,000 clusters. We set the threshold \u20ac = 0.4 to remove semantic duplicated images within\neach cluster, yielding an image set of 14 million images. Finally, we select the top 20 images from\neach cluster to create our DenseFusion-1M dataset."}, {"title": "3.2 Perceptual Fusion", "content": "Comprehensive visual perception is a prerequisite for multimodal understanding and reasoning. This\nperception ability can be achieved through extensive, detailed, and accurate alignments in image-text\npre-training data. Despite the feasibility of current MLLMs [37, 31] for image captioning, they\nstill struggle to provide meticulous descriptions. (1) Generalist MLLMs are designed for executing\nvarious instructions and are not intended for specific captioning tasks, especially for well-rounded\nimage captioning. (2) Existing specialist caption engines lack a strong ability for comprehending and\ndescribing various visual elements inside high-resolution images, due to their inherent drawbacks in\nidentifying all kinds of visual characteristics.\n3.2.1 Mixture of Visual Experts\nWith the advancements in computer vision, numerous visual experts of various perceptual tasks\nhave emerged and demonstrate outstanding capabilities within their respective domains [40, 14, 68,\n21]. These models provide valuable intermediate perceptual information for image understanding.\nTherefore, comprehensively understanding the diverse visual elements in complex scenes can benefit\nfrom the collaboration of different specialists. In this section, we develop a perceptual fusion strategy\nwith assistance from a variety of vision experts.\nThis approach specifically targets areas where generalist MLLMs often show limited perceptual\ncapabilities. Our strategy includes the application of expert techniques in image tagging, object\ndetection, text recognition, and the incorporation of world knowledge. We meticulously select these\nvision experts based on several key aspects of perception, which are detailed as follows.\n\u2022 Image Tagging: Initially, we attempt to produce scene-level understanding for holistic images,\nincluding objects and visual scenes. Specifically, we employ the pre-trained RAM++ [68] that\ngenerates expansive tag descriptions over conventionally predefined tag categories. This approach\nenriches visual tag information and provides accurate scene annotations in overall image under-\nstanding, enhancing the recognition of diverse open-vocabulary concepts for object understanding.\n\u2022 Object Detection: Comprehensive understanding relies on the perception ability of various object\nentities, while current MLLMs suffer from incomplete object perception and inaccurate positioning.\nTherefore, we utilize two types of specialized detection models to boost hyper-detailed recognition.\n(1) We employ the closed-set EVA02 [14] detection model trained on LVIS [19] and COCO [34]"}, {"title": "3.2.2 Perceptual Fusion Engine", "content": "To obtain precise and comprehensive image descriptions, the widely-used advanced GPT-4V [42]\nserves as an ideal MLLM with strong visual perception and contextual understanding capabilities.\nIt can generate image descriptions that are further enriched with various visual information from\nspecialized vision experts. Considering its expensive cost of time and finance, we attempt to construct\nan open-sourced and low-budget caption engine to efficiently mimic its ability for large-scale image\ncaptioning. We empirically discover that the perception ability of existing open-sourced caption\nengine can be enhanced with the assistance of additional visual experts, where they can improve the\nrecognition of small-scale objects and OCR information, guiding our caption engine to focus on often\noverlooked content and correcting inaccuracies caused by its limited visual perception.\nInitially, we adopt the proficient GPT-4V via manual-tuning prompts to generate image captions with\nextra visual information as the perceptual fusion guidance. The detailed prompt template can be\nfound in Appendix. We thereby obtain 100K hyper-detailed image descriptions, i.e. DenseFusion-\n4V-100K. Using this meta dataset as guidance, we train our caption engine to learn from GPT-4V's\ncharacteristics and generate highly detailed image descriptions, as depicted in Figure 3. Our caption\nengine is based on LLaVA-1.6 (7B) [36], utilizing high-resolution images as inputs to ensure better\nvisibility of detailed visual clues. The expertise of visual specialists are extracted offline and adopted\nas contextual information for caption engine. This process allows our engine to capture various visual\nclues effectively, enhancing its perception abilities by incorporating insights from vision experts.\nConsequently, it accurately identifies a wide range of objects and detailed textual information,\nresulting in image annotations with high information density."}, {"title": "3.3 Dataset Description", "content": "Utilizing the perceptual fusion pipeline, we incorporate insights from multiple visual experts into\nproducing hyper-detailed image descriptions, resulting in the following datasets: (1) DenseFusion-\n4V-100K. GPT-4V generated 100K captions. (2) DenseFusion-1M. Scaling up to 1 million detailed\ncaptions by our caption engine. We conducted a statistical analysis to show the detailed dataset\ninformation in Table 1. On average, the captions are 190 words long and consist of 11 sentences with\ndense descriptions. As shown in the category distribution in Figure 4(b), the DenseFusion dataset\ncontains diverse categories such as photos, visual art, commercial design, and infographics, making it\na valuable resource with various image types. We employ LLaVA-1.5 [35] as a generalist MLLM\nfor the category classification task. Generating hyper-detailed captions is fundamental to various\nmulti-modal research tasks, as it facilitates the translation of images into language seamlessly. This\ncapability presents significant potential in applications, e.g., vision-language contrastive pre-training\n[24, 52], multimodal alignment in MLLMs [2, 37, 4], and text-conditioned image generation [45]."}, {"title": "4 Experiments", "content": "In this section, we introduce the implementation details and compare the model trained by our\nDenseFusion-1M dataset with state-of-the-art MLLMs across diverse vision-language benchmarks.\nFinally, we validate the effectiveness of perception fusion qualitatively and quantitatively.\n4.1 Implementation Details\nCaption Engine. To explore the detailed visual clues inside each image, we adopt LLaVA-1.6 (7B)\n[36] to handle the high-resolution image inputs. For the meta dataset, we utilize GPT-4V to annotate\nthe randomly selected 100K images from our picked 1M LAION data, thereby boosting our engine\nsupported by various experts and producing high-quality annotations to mimic advanced GPT-4V.\nThis supervised fine-tuning stage takes around ~ 5.5 hours on 4 nodes of 8\u00d7A100 (40G) for 2\nepochs. The visual knowledge from diverse visual experts are extracted and integrated as contextual\ninformation for the perception fusion prompt. Then we utilize the caption engine with the efficient\ndeployment tool SGLang [69] to generate 1M data with enhanced multimodal perception.\nEvaluation Benchmarks. To verify the efficacy of the provided DenseFusion-1M, we adopt these\ncaptions during the pre-training stage and follow the setup of LLaVA-1.5 [35] on various visual\nquestion answering (VQA) and multi-modality understanding benchmarks for evaluation, such as\nScienceQA [39], TextQA [50], VQAv2 [18], GQA [23], SEED [29], MMBench [38], \u039c\u039c\u0395 [15],\nPOPE [32], MM-Vet [65], that covers a wide range dimensions for evaluating model abilities.\nModel Configuration. To verify the effectiveness of DenseFusion-1M, we adopt it in the pre-training\nstage for vision-language alignment. The model is based on LLaVA-1.5 [35], using the vision\nencoder CLIP-ViT-L/14-336 [24] and the large language model (LLM) Vicuna [10] respectively. The\nvision encoder and LLM are connected by a two-layer multi-layer perception (MLP) projector. We\nutilize the approach of S2 [47] for training the high-resolution MLLM, which is efficient in handling\nhigh-resolution inputs without increasing image tokens. We follow LLaVA-1.5 [35] that comprises a\ntwo-stage training stages. (a) Pre-training Stage. We first only train the projector for pre-alignment,\nthen we conduct pre-training with a trainable vision encoder of the last 12 layers to further improve\nthe perception ability. (b) Instruction-tuning Stage. For fair comparison, we follow LLaVA-1.5\n[35] and adopt the original LLaVA-mix-665K for instruction tuning, including GPT-generated and\nacademic-oriented datasets. The detailed training recipe is shown in supplementary material."}, {"title": "4.2 Main Results", "content": "Compared Models. We report the experiment results against current state-of-the-art MLLMs,\nincluding Qwen-VL [4], InstructBLIP [11], mPLUG-Owl2 [62], InternVL [9], LLaVA-1.5 [35]. In\nparticular, we compare our strategies with existing caption datasets or engines, e.g. ShareGPT4V [8],\nLVIS-4V [56]. To fully exploit its potential, we conduct comparisons under high-resolution settings\nwith recent MLLMs, including Monkey [33], LLaVA-1.6 [36], and Scaling on Scales [47] (S2).\nExperiment Results. (1) Conventionally, Table 2 demonstrates that our meticulous descriptions\nsignificantly improve baseline models, providing solid and consistent benefits across all vision-\nlanguage benchmarks, particularly for text-recognition scenes, e.g. TextVQA. Notably, our dataset\noriginates from the generic LAION, which has no direct connection to the validation domains. Despite\nthis, our strategies outperform ShareGPT4V, which uses images from COCO and VG that share a\nsimilar image distribution with the evaluation benchmarks, like VQAv2 and GQA. (2) Additionally,\nwe observe that the potential benefits of our dataset are not fully exploited due to limited input\nresolutions, making MLLMs challenging to extract hyper-detailed image clues. To address this, we\nconduct further experiments using the high-resolution MLLM, Scaling on Scales [47] (S2), which\nperforms multi-scale aggregation on high-resolution inputs without increasing the number of image\ntokens. Even with a fifth of visual tokens of LLaVA-1.6 and requires no additional instruction\ntuning data, LLaVA-S2 trained by our data achieves better performance than the state-of-the-art\nLLaVA-1.6 and exhibits higher forward efficiency. Besides, we reproduce LLaVA-S2 using 1.2M\npre-training data from ShareGPT4V [8], named ShareGPT4V-S2, and we do not introduce additional\nsupervised fine-tuning data for fair comparisons. Our dataset shows further performance gains\ncompared to the low-resolution version, demonstrating the superiority of our dataset in scenarios\nrequiring hyper-detailed visual elements.\nFrom the above results, we observe that (1) a high-quality image-text dataset is crucial during\npre-training to enhance alignment across modalities before learning specific instruction patterns;\n(2) meticulous and accurate image descriptions are essential for high-resolution vision perception.\nLow-resolution MLLMs easily reach saturation due to blurred visuals and difficulty in exploring\ndetailed clues. Therefore, meticulous image annotation is a promising direction for enhancing the\nhyper-detailed perception and reasoning capabilities of multimodal models."}, {"title": "4.3 Ablation Study", "content": "Perceptual Fusion. Generalist MLLMs occasionally exhibit inherent drawbacks in comprehensive\nperception, e.g., omitting objects and weak in text recognition. For time saving, we performed\nthe ablation study using a subset of 100K data points from DenseFusion-1M as default setting.\nIt is observed in Table 3, our strategy can effectively alleviate these issues, bringing substantial\nimprovements on different benchmarks, especially in TextVQA with rich OCR information. We note"}, {"title": "5 Limitation and Discussion", "content": "For comprehensive visual perception, we propose perceptual fusion, efficiently integrating the insights\nfrom visual experts and constructing the DenseFusion-1M dataset with well-rounded descriptions.\nDespite its promising results, some issues could be improved: (1) Limited by the current capacity"}, {"title": "6 Conclusion", "content": "In this paper, we tackle the challenge of limited high-quality image-text data by developing a low-\nbudget caption engine for high-resolution images and hyper-detailed captions. Our strategy involves\ncurating a dataset from the LAION-2B corpus, followed by a perceptual fusion pipeline that guides a\nmultimodal model to integrate information from various vision experts and thereby yields one million\nwell-rounded descriptions, dubbed DenseFusion-1M. We believe that such an extensive image-text\ndataset, characterized by its hyper-detailed nature, would substantially enhance the capabilities of\nMLLMs by enabling more effective alignment between visual and textual data."}, {"title": "A Overview", "content": "Dataset of DenseFusion-1M will be open sourced at https://huggingface.co/datasets/\nBAAI/DenseFusion-1M. In this Appendix, we present brief description of our dataset in Sec. B.\nSec. C presents implementation details of our framework. Besides, more examples and results are\nvisualized in Sec. D."}, {"title": "B Dataset", "content": "The dataset, named DenseFusion-1M, is a large-scale image description dataset designed to enhance\nthe perceptual abilities of Multimodal Large Language Models (MLLMs). It contains 1 million\nhyper-detailed image descriptions derived from a subset of the LAION dataset, carefully curated and\nannotated using our caption engine with perceptual fusion that integrates diverse vision experts."}, {"title": "C Implementation", "content": "C.1 Training Details.\nThe main training implementations are outlined in the primary paper. In this section, we detail the\nhyper-parameters used to train the MLLM for evaluating our data. During the pre-alignment stage,\nwe exclusively train the projector, resulting in more stable and slightly enhanced performance. In the\npre-training phase, we unfreeze the Vision Encoder (VE) for the last 12 layers, the Language Model\n(LM), and the projector. For instruction tuning, we utilize the original data from LLaVA-1.5 and the\nLLaVA-mix-665K instruction tuning dataset to fine-tune both the projector and language model."}, {"title": "C.2 Prompt Engineering", "content": "The constructing pipeline leverages prompt engineering to generate hyper-detailed image descrip-\ntions. This process involves carefully crafting prompts that guide the advanced GPT-4V to produce\ncomprehensive and accurate annotations. The prompts are designed to integrate insights from various\nvision experts, enhancing the overall quality and granularity of the dataset."}, {"title": "D Visualizations on DenseFusion-1M", "content": "We provide more examples of image captions in Tab. 7 and Tab. 8. Besides, to further evaluate the\nconsistency between original images and generated captions, we use Dall-E 3 [5] to reconstruct the\nimages based on the generated captions. The comparative results from different caption engines\nare illustrated in Fig. 6. Compared to other caption engines, our model demonstrates significant\nadvancements in terms of element consistency, spatial relationships, and accuracy. This also indicates\nthe potential of our datasets for conditional image generation tasks which we leave it for future\nresearch."}]}