{"title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models", "authors": ["Omri Kaduri", "Shai Bagon", "Tali Dekel"], "abstract": "Vision-Language Models (VLMs) have recently demonstrated remarkable capabilities in comprehending complex visual content. However, the mechanisms underlying how VLMs process visual information remain largely unexplored. In this paper, we conduct a thorough empirical analysis, focusing on the attention modules across layers. We reveal several key insights about how these models process visual data: (i) the internal representation of the query tokens (e.g., representations of \"describe the image\u201d), is utilized by VLMs to store global image information; we demonstrate that these models generate surprisingly descriptive responses solely from these tokens, without direct access to image tokens. (ii) Cross-modal information flow is predominantly influenced by the middle layers (approximately 25% of all layers), while early and late layers contribute only marginally. (iii) Fine-grained visual attributes and object details are directly extracted from image tokens in a spatially localized manner, i.e., the generated tokens associated with a specific object or attribute attend strongly to their corresponding regions in the image. We propose novel quantitative evaluation to validate our observations, leveraging real-world complex visual scenes. Finally, we demonstrate the potential of our findings in facilitating efficient visual processing in state-of-the-art VLMs.", "sections": [{"title": "1. Introduction", "content": "Vision-Language Models (VLMs) have recently emerged as a powerful extension of Large Language Models (LLMs). As demonstrated by their unprecedented capabilities in generating highly detailed and accurate descriptions of complex visual scenes, these models are quickly narrowing the gap between machine-generated and human interpretation of the visual world [1, 6, 7, 24, 25]. As such, VLMs have been rapidly adopted across diverse visual tasks, including in robotics, medical imaging analysis, autonomous driving, and content generation [22, 26, 27, 34].\nDespite their growing adoption, VLMs are often treated as black-box tools or agents for solving specific tasks [16, 32], with limited understanding of their internal mechanisms for processing visual data. Uncovering these mechanisms is essential for enhancing model transparency, efficiency, and trustworthiness in high-stakes applications, as well as for guiding future VLM design. In this work, we take significant steps toward unraveling the \"vision\" of prominent VLMs, offering new insights into how these models interpret and process visual data.\nWe examine the scenario in which the VLM receives an input image along with the query \"describe the image\". The VLM generates its response autoregressively, where each generated token gathers information from both the input image and text. In this work, we aim to understand the information flow between the visual and textual modalities. Our analysis focuses on the attention modules across the VLM's layers through a set of experiments in which we restrict in different ways the access to visual information across layers. This allows us to uncover several critical insights: (i) The models compress high-level image information into the query text tokens. We demonstrate this insight by blocking the direct influence of image tokens on the generated tokens, allowing visual information to be accessible only indirectly through the query text tokens. Remarkably, the model generates descriptive responses, relying solely on the visual information encoded in the query text tokens. (ii) Middle layers play a crucial role in the vision-to-language knowledge transfer, while early and late layers contribute only marginally; we show that accessing image tokens only in mid-layers (~25% of all layers) results in minor degradation in the VLM's performance. (iii) Fine-grained object details and visual attributes are directly retrieved from image tokens in a spatially localized manner.\nA key aspect in our study involves validating our observations by measuring the alignment between the VLM's original output and its modified output under each of our experiments above. This evaluation requires comparing two free-text paragraphs a challenging task due to possible large variations in wording and writing style. Inspired by [40], we propose a new LLM-based evaluation protocol which enables us to quantify the agreement between the modified response and the original one. We ground our proposed evaluation with a human study, validating its robustness and accuracy. We further propose a novel automatic evaluation that harnesses off-the-shelf object segmentation tools [20] to quantitatively evaluate the emerged spatial localization across the VLM's layers.\nFinally, we demonstrate that our observations facilitate efficient processing, allowing to distill the VLM's internal representation into a compressed context space. This gives rise to a new application we term \"Image Re-prompting\", which allows to efficiently ask several questions on an image, using only the compressed context. While the compressed context is \u00d720 smaller than the full one, it achieves 96% of the performance in visual question answering [11].\nIn summary, the contributions of our work are as follows:\n\u2022 We reveal the surprising role of query tokens as high-level image descriptors, the critical role of the middle layers, and the way by which fine-grained details are retrieved.\n\u2022 We propose new automatic evaluation protocols that harness the use of LLMs and image segmentation tools.\n\u2022 We take a first step towards leveraging our understanding for efficient visual processing in VLMs.\n\u2022 To the best of our knowledge, we are the first to consider VLMs at the scale of 76B-parameter [6] in the context of interpretability. We further analyze LLaVA-1.5-7B [24]."}, {"title": "2. Related Work", "content": "Vision-Language Models (VLMs). VLMs extend Large Language Models (LLMs) to jointly process visual and textual inputs, with the LLM handling most of the computational analysis. VLMs generally consist of a pre-trained LLM, a vision encoder, and an adapter that aligns visual representations with the LLM's embedding space. Prominent open-source VLMs build on high-performance LLMs like Llama3 [10], Mistral [18], and Qwen [38]. Earlier VLMs leveraged CLIP [31] as the vision encoder, while newer models employ larger encoders [6] to handle images at various resolutions [6, 24, 35]. Visual embeddings are adapted to LLM space through an adapter [2, 21, 25, 35], with the MLP-based [25] adapter is commonly used. In our study, we analyze the state-of-the-art open-source VLM InternVL2-76B [6] and validate our findings on the widely-used LLaVA-1.5-7B [24] to generalize across architectures.\nInterpreting LLMs. As LLMs have become widely used, interpretability research has emerged to understand various model components, including attention layers [5, 8], feed-forward layers [12], and activation patterns [9, 28]. Techniques like the logit lens [30] reveal what information is encoded at each layer. In contrast, our work examines how information flows through the model, analyzing attention patterns with the attention knockout tool [13], which blocks specific attention connections to isolate their roles.\nInterpreting VLMs. VLMs interpretability field is evolving, several works focused on where information is stored inside the model [4], revealing shortcomings of using pre-trained vision encoder [36], and exploring VLM hallucinations [3]. Concurrent works focused on spatial localization in VLMs by employing the logit lens [19, 29], demonstrating that image tokens can be directly mapped to semantically-relevant words in the vocabulary. Our work provides a broader examination of visual processing in VLMs showing that the internal visual representation is composed of a compressed representation which provides high level information, and localized retrieval of fine-grained information. These two pathways motivate us, for the first time, to explore applications of efficient VLMs.\nLLM-as-a-judge As LLMs continue to evolve, they are increasingly recognized as viable alternatives to human annotators, allowing for scalable and reproducible assessment methods across various tasks [14, 17]. In [40], the concept of LLM-as-a-judge has been established by demonstrating the use of LLMs to evaluate responses generated by other LLMs. Inspired by this concept, we introduce an LLM-based evaluation protocol for comparing two free-text image descriptions. Our approach automatically assesses object identification and detects hallucinations, offering a novel solution for evaluating image captioning accuracy."}, {"title": "3. Preliminary", "content": "The prevalent design of VLMs includes three main components: a pre-trained decoder-only LLM, a pre-trained vision encoder, and an adapter [6, 7, 25]. The vision encoder processes an input image by dividing it into patches, each of which is embedded into a vector. The adapter then projects these embeddings into the LLM's token embedding space. This design allows VLMs to handle tokens from both modalities in a single sequence. We distinguish between three types of tokens within the VLM framework, where each token type corresponds to a specific set of indices within T, the full sequence:\n1. Image tokens, Timg: Token embeddings encoding the input image. Formally, the vision encoder and adapter jointly transform the image I into a set of image tokens, $T_{img} \\in R^{|P_{img}|\\times d}$, where $P_{img}$ denotes the indices of image tokens within the full token sequence, and d is the embedding dimension.\n2. Query tokens, Ttxt: token embeddings of the input query text (e.g., \"describe this image\"). These tokens are represented by $T_{txt} \\in R^{|P_{txt}|\\times d}$, where $P_{txt}$ are the indices corresponding to query tokens within the sequence.\n3. Generated tokens, Tgen: token embeddings of the VLM's generated response. Generated tokens expand the sequence, and at the i-th generation step, the cumulative set of generated tokens is $T_{gen}^{(i)} \\in R^{|P_{gen}^{(i)}|\\times d}$, where $P_{gen}^{(i)}$ represents the indices of generated tokens in the sequence up to the i-th step.\nThe full token sequence processed by the VLM is then:\n$T = [T_{img}, T_{txt}, T_{gen}]$\nTo generate the (i + 1)-th token in this sequence, the VLM processes all tokens through a series of transformer blocks, each comprising normalization, causal self-attention, and feed-forward MLP modules. The generation relies on a causal attention mechanism, controlled by an attention mask that ensures each token attends only to previous tokens in the sequence. Formally, the attention is masked by $M^{(i)} \\in N_i \\times N_i$, where $N_i = |P_{img}| + |P_{txt} + |P_{gen}^{(i)}|$, designed to enforce causality:\n$M^{(i)}[p, q] = \\begin{cases}\n0 & \\text{if } q < p \\\\\n-\\infty & \\text{otherwise}\n\\end{cases}$\nThus, the attention scores are computed as:\n$A = Att(Q, K, M) = softmax(\\frac{Q K^T}{\\sqrt{d}} + M)$"}, {"title": "4. What's In The Image?", "content": "We consider the general task of image description, where the VLM is given an input image and is prompted with a basic instruction \u201cdescribe the image\". Our goal is to gain a better understanding of the internal mechanism by which the model leverages visual information during the autoregressive generation of its response. We provide results for InternVL2-76B [6], a state-of-the-art large VLM, and report additional results on LLaVA-1.5-7B [24] in Sec. D.\nOur analysis focuses on the attention modules, which govern the flow of information between the visual and textual modalities. We begin our exploration by extracting the attention values of each generated token relative to all other tokens in T, which consists of image tokens ($T_{img}$), query tokens ($T_{txt}$), and the previously generated tokens ($T_{gen}$).\nThe first question we raise is to what extent is the generated token influenced by the different types of tokens across layers? We quantify, per layer, the influence of each token type on the i-th generated token by computing the relative attention directed to each type: $a_{img}^{(i)}$, $a_{txt}^{(i)}$, and $a_{gen}^{(i)}$. Note that $a_{img}^{(i)} + a_{txt}^{(i)} + a_{gen}^{(i)} = 1$. We denote by $\\bar{a}_{img}$, $\\bar{a}_{txt}$, and $\\bar{a}_{gen}$ the average of relative attention across generated tokens.\nFigure 1 shows the distribution of $\\bar{a}_{img}, \\bar{a}_{txt}, \\bar{a}_{gen}$ for a set of random 80 images from COCO across layers of the VLM. The plot reveals a non-uniform flow of information across layers: $\\bar{a}_{img}$ is prominent in the first few layers (0-5), then drastically drops while exhibiting a moderate increase in mid-layers (20-40). Furthermore, the majority of the attention of the generated token is directed to the embeddings of query text tokens after the very first few layers ($\\bar{a}_{txt}$). This behavior is surprising, as the information essential for describing the image resides in the image tokens, while the input query text is generic. Moreover, despite the query tokens constituting less than 5% of the total tokens, they command over 60% of the overall attention.\nIntrigued by these non-uniform patterns, we conduct a thorough empirical analysis to better understand the information accumulated in image and query tokens, and their roles in the generation process. Specifically, in our analysis, we knock out the information flow between different token types and evaluate the impact on the generated output. To this end, we propose a new LLM-based evaluation protocol, which allows us to automatically quantify the level of fidelity of the response from the VLM under knockout relative to the original response without knockout. Next, we describe in detail our empirical analysis and evaluation."}, {"title": "4.1. Attention Knockout in VLMs", "content": "Our analysis revolves around blocking the information flow between the image tokens, to other tokens, as illustrated in Figure 2(a-d). In practice, this is achieved by knocking out the attention from image tokens to either the query token (KOimg\u2192txt), generated tokens (KOimg\u2192gen), or both (KOimg\u2192txt+gen). It allows us to reveal how visual information gets processed, as we will demonstrate in this section. Formally, the general definition of the attention knockout mask, $M_{ko}$, is given by:\n$M_{ko} [p, q; P_{src}, P_{tgt}] = \\begin{cases}\n-\\infty & \\text{if } q \\in P_{src} \\text{ and } p \\in P_{tgt} \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nwhere $P_{src}$ and $P_{tgt}$ are the sets of token indices from which attention is blocked. The knockout mask is applied starting from layer l, with earlier layers remaining unaffected and only subject to the causal mask. We varied the value of l to examine the impact of blocking attention from different layers.\nFor each experiment, specific sets $P_{src}$ and $P_{tgt}$ were used to define distinct knockout configurations, detailed in the experimental setup. We add the knockout mask to the causal mask (Eq. 1), updating the attention scores computation as:\n$Att (Q, K, M) = softmax( \\frac{Q K^T}{\\sqrt{d}} +M+ M_{ko})$"}, {"title": "4.2. Attention Knockout Evaluation", "content": "To assess the effect of specific attention knockout settings, we need to measure the difference between the modified and original responses of the VLM for the same input. Specifically, we assess the VLM's ability to recognize objects it originally identified, as well as the emergence of hallucinated objects introduced by the knockout.\nAutomatically counting the number of identified or hallucinated objects in free-text paragraphs is challenging due to variations in writing styles, object attributes, and other factors. Moreover, while datasets such as COCO [23] annotate objects in each image, they rarely contain every object in the image (e.g., in Fig. 3 \u201cglasses\u201d are visible in the image, yet COCO annotations do not include them), therefore they cannot be used as a reliable ground-truth for object existence. Thus, we adopt an LLM-as-a-judge approach [40]. Given original and modified VLM's responses, we instruct the LLM to identify all objects mentioned in each prompt, while disregarding attributes and other details (e.g., weather or lighting conditions). This results in two lists of identified objects $O_{orig}$ and $O_{ko}$. We take advantage of the LLM's capability to overcome syntactical differences in textual descriptions to robustly estimate:\n\u2022 TP=|Oorig \u2229 Oko|: Objects found in both.\n\u2022 FN=|Oorig \\ Oko|: Objects found only in original.\n\u2022 FP=|Oko \\ Oorig: Objects hallucinated.\nFinally, we estimate precision, recall and F1 score. Our LLM evaluation protocol is illustrated in Fig. 3, and employs a chain-of-thought process [37] with three in-context examples. We validated our LLM-as-a-judge protocol through a user study, finding 95% agreement between human annotators and LLM judgments on object existence. Detailed user study results are provided in Sec. B."}, {"title": "4.3. Text Tokens as Global Image Descriptors", "content": "Our first observation is that the embeddings of query text tokens (Ttxt) act as global image descriptors, playing a critical role in the internal representation of the input image.\nWe isolate the direct effect that image tokens (Timg) have on the generated tokens (Tgen) by blocking the information from Timg to T gen. This experiment, denoted by KOimg\u2192gen, is illustrated in Fig. 2(b). In practice, this is implemented by setting the mask in Eq. 3 to: $M_{ko}[p, q; P_{img}, P_{gen}]$.\nFigure 5(c) shows sample results of this experiment, where the masking is applied to all layers. As seen, although the generated tokens have no direct access to the image tokens, the model can surprisingly produce descriptive responses, identifying prominent objects in the scene and even capturing basic spatial relationships.\nWe quantify these results using our LLM-based evaluation (Sec. 4.2). The F1 scores and their breakdown to precision/recall are reported in Fig. 5(c) for each example. The average F1 score over a set of 80 randomly sampled images from COCO is 0.4 (KOimg\u2192gen bar, Fig. 2(e)).\nWhile KOimg\u2192gen reveals that high-level image information is compressed into the text embeddings, we raise the question of whether the model must rely on this compression to generate its response. To explore this, we consider another knockout setting, KOimg\u2192txt, where we block the attention between Timg and Ttxt, thus visual information is accessible only through Timg. This is implemented by using Mko [p, q; Pimg, Pixt] in Eq. 3.\nSurprisingly, as seen Fig. 5(d), preventing the text tokens from grabbing visual information disrupts the model's ability to produce meaningful responses. In this case, the F1 score is zero (Fig. 2). This validates the surprising role of Ttxt, in holding a compressed representation of the image."}, {"title": "4.4. Visual Information Across Layers", "content": "Different layers contribute differently to the visual representation, as evidenced by the non-uniform attention patterns in Fig. 1. We observe that mid-layers attend to multiple regions, while early and late layers focus on fewer, non-semantic positions, as seen in Fig. 4.\nTo further analyze the role of different layers, we expand our knockout experiments to the setting where the attention is masked starting from a specific layer l and onward. Thus, up to layer l, only causal masking is used. We consider the three knockout settings illustrated in Fig. 2(b-d).\nThe results are reported in Fig. 2(f). As seen, knockout after layer l = 40 hardly impacts the F1 scores across all configurations; this is aligned with the inefficiency of deeper layers in LLMs [15]. Note that average F1=0.8 is primarily due to ambiguity in object identification by the LLM. A similar error is observed by humans in our user study (Sec. 4.2). Finally, we can observe a consistent drastic rise in F1 between layers 20-40, hinting their crucial role. To isolate the contribution of the mid-layers, we modify our setting to knockout the attention in layers l \u2209 [20,40]. Interestingly, as seen by the dashed bars in Fig. 2(e), the mid-layers alone provide comparable results (F1 \u2208 [0.75, 0.81]) to the original model, even in the extreme case where we knockout image tokens from all other tokens (KOimg\u2192txt+gen). In addition, by comparing direct image knockout in all layers KOimg\u2192gen to knockout except the mid-layers KOimg\u2192gen, we quantify the contribution of directly accessing image tokens in mid-layers. The rise in the score suggests that visual details that are not available in the query text are retrieved from image tokens in mid-layers."}, {"title": "4.5. Fine-Grained Details Localized in Mid Layers", "content": "How does the model retrieve fine-grained visual information from image tokens? To explore this question, we focus on objects that are described in the original response yet lack when KOimg\u2192gen knockout is applied. These objects and their corresponding attributes are provided as part of our LLM-as-a-judge protocol. See Sec. C for full details. Figure 6 visualizes the attention maps of generated tokens associated with a specific object, averaged across the mid-layers (20-40). It demonstrates, even for extremely small objects (i.e., the cycling shoes) that localization patterns appear. We proceed to quantify these results by obtaining a pseudo ground-truth segmentation mask for each object using text-grounded segmentation method [20, 39]; Examples are shown in Fig. 6 (c), and the boundary of each segmentation map is marked in white in Fig. 6 (b).\nWe consider an object to be well-localized if the peak of its attention map (marked by a white cross at Fig. 6(b)) is at most 40 pixels away from the object. We denote this metric as Localization accuracy, and compute it for every layer, over a set of 231 objects from 68 images (see Sec. C for more details on the dataset). Fig. 7 provides results, averaged for 10 consecutive layers, which demonstrates that accuracy rises in the mid-layers, achieving almost 73%. This rise in score suggests that the object's fine-grained visual information is retrieved from the corresponding image tokens in a localized manner, specifically in the mid-layers. Similar trend was observed in LLaVA-1.5, Fig. A6."}, {"title": "5. Efficient Visual Processing in VLMs", "content": "Our analysis reveals surprising inefficiency: the compression of visual information into query tokens and the redundancy of early and late layers. Here, we further analyze the compression by pruning image tokens.\nPruning image tokens by attention: Figure 8 shows the histogram of attention to image tokens, which depicts a long-tail distribution per layer. That is, a small number of image tokens receive notably high attention values. This leads us to define a compressed context a small subset of the highest attended image tokens along with the query tokens. Specifically, for each layer, we select the top-k percentile of tokens that received the highest attention values. We examine the performance of the model when the generated tokens have access only to the compressed context, as illustrated in Fig. 9. The results for different values of k show that the performance quickly plateaus, even when only 5% of the image tokens are used, highlighting the inefficiency of token utilization in the model.\nImage Re-prompting: Our compressed context contains sufficient information to generate image descriptions comparable to those generated using the full image. Here, we extend this capability to a new application, termed Image Re-prompting, where the VLM only processes the image for the query \"describe the image\", extracts the compressed context, and uses it to answer additional questions about the image, as illustrated in Fig. 10.\nTo evaluate Image Re-prompting, we use the MME benchmark [11], which comprises of images with simple yes/no questions. A short description of the MME benchmark can be found in Fig. A11. Results are provided in Table. 1 for 10 MME perceptions tasks, with the average results shown to the right. The first baseline we consider is Naive, which independently query the model from scratch using each image-question pair. While the compressed context uses 15x fewer tokens, it results in only a subtle decrease in performance. Interestingly, the compressed context exceeds the Naive baseline on tasks as OCR and Count. We hypothesize that the improvement for such yes/no questions arises since the VLM can not generate tokens that correspond to objects, which hinders the retrieval of localized information from the image. On average, the compressed context reduces ACC and ACC+ by only 2.8% and 5.3% respectively, depicting that a smaller set of high-attention tokens can retain much of the performance benefits of full image context.\nThe second baseline, Describe-to-LLM, evaluates whether the VLM's response to \"describe the image\" suffices for follow-up questions, by feeding them into GPT-4, and prompt it with follow-up questions. In tasks such as Celebrity recognition, the compressed context significantly outperforms this approach, retaining specific details like celebrity names that can be missed in text descriptions alone. For Existence and Count tasks, the compressed context matches or exceeds Describe-to-LLM performance, indicating that even a minimal set of tokens can preserve essential information for object presence and counting.\nFurthermore, we break down the contribution of each component of the compressed context: Query and K%, for K=2%. While each component performs poorly on some tasks (Existence, Count, Color), the compressed context (2%) provides a significant improvement, indicating the non-trivial fusing of information that the model performs. Our evaluation manifests Image Re-prompting as a viable method for efficient VLM-based multiple-question answering."}, {"title": "6. Conclusion", "content": "In this work, we take substantial first steps towards enhancing our understanding of Vision-Language Models (VLMs) at scales of tens of billions of parameters. We uncovered novel insights about their internal visual representation and processing, with two underlying core mechanisms: visual information compression into text tokens, and spatially-aware retrieval of fine details from image tokens. Our new evaluation methods confirm these findings, paving the way for more efficient VLMs. We introduced \"Image Reprompting\"-enabling efficient, multi-question answering. Future work can extend our analysis to multi-image and video, potentially expanding the Image Re-prompting application to expand VLMs effective visual context windows."}]}