{"title": "Adapting Language Models via Token Translation", "authors": ["Zhili Feng", "Tanya Marwah", "Lester Mackey", "David Alvarez-Melis", "Nicol\u00f2 Fusi"], "abstract": "Modern large language models use a fixed tokenizer to effectively compress text drawn from a source domain. However, applying the same tokenizer to a new target domain often leads to inferior compression, more costly inference, and reduced semantic alignment. To address this deficiency, we introduce Sparse Sinkhorn Token Translation (S2T2). S2T2 trains a tailored tokenizer for the target domain and learns to translate between target and source tokens, enabling more effective reuse of the pre-trained next-source-token predictor. In our experiments with finetuned English language models, S2T2 improves both the perplexity and the compression of out-of-domain protein sequences, outperforming direct finetuning with either the source or target tokenizer. In addition, we find that token translations learned for smaller, less expensive models can be directly transferred to larger, more powerful models to reap the benefits of S2T2 at lower cost.", "sections": [{"title": "Introduction", "content": "Modern large language models (LLMs) are typically trained in two stages. First a tokenizer is trained to map commonly occurring character sequences in the training data into vocabulary units known as tokens. Next, all training text is tokenized, i.e., translated into this token vocabulary, and a model is trained to predict the next token given a context of preceding tokens. The tokenizer can be viewed as an initial compressor of input bytes [Gage, 1994] that significantly shortens text drawn from the training domain and arguably improves the training dynamics [Rajaraman et al., 2024]. Despite its widespread adoption, this two-stage procedure suffers from a key failing: When faced with text from a new target domain, compression quality drops, context length and inference costs increase, and learned semantic alignment deteriorates. This effect is especially evident when modern LLMs (trained predominantly on English and code) are used to reason about molecular sequences like proteins. Such sequences are commonly represented using the Latin-script alphabet, but the meaning and frequency of each substring differ significantly their natural language counterparts, resulting in semantic misalignment.\nTo tackle the analogous alignment problem for low-resource languages, Remy et al. [2024] proposed to use fast_align [Dyer et al., 2013], an expectation-maximization algorithm that requires parallel data from the training and target domains.\nThis approach shows promising results, but for many target domains, parallel training data is difficult or impossible to gather. For example, there is no agreed-upon parallel translation between protein sequences and natural language.\nIn this work, we propose a Sparse Sinkhorn Token Translation (S2T2) algorithm that does not require parallel data. Instead, S2T2 learns a translation between training domain tokens and new target domain tokens just using a sample data from the target domain and the pretrained LLM weights. After training a tokenizer on the target domain, S2T2 translates each target-domain token into a (sparse)"}, {"title": "Translating Tokens with Sparse Sinkhorn", "content": "Consider a pretrained LLM M with vocabulary size $v$, embedding matrix $E \\in \\mathbb{R}^{v \\times d}$, and language model head $L \\in \\mathbb{R}^{v \\times d}$. For a given input sequence encoded as a matrix $X \\in \\{0, 1\\}^{s \\times v}$ in which each row is a one-hot vector representing a training-domain token, $XE \\in \\mathbb{R}^{s \\times d}$ represents the sequence of (soft) embeddings, and the predicted next token is given by\n$\\qquad M(X) = \\underset{i \\in [v]}{\\operatorname{arg\\,max}} \\operatorname{softmax}(Lh(XE)); \\in \\{0, 1\\}^v\\qquad (1)$\nwhere $h : \\mathbb{R}^{s \\times d} \\rightarrow \\mathbb{R}^d$ maps an embedding sequence into a single vector, the internal representation of the next token.\nConsider also a dataset $\\mathcal{D}$ drawn from a new target domain, and let $u$ be the vocabulary size of a new tokenizer trained on $\\mathcal{D}$. For given marginal distributions over training and target tokens $\\mu \\in \\Delta^{v-1}$ and $\\nu \\in \\Delta^{u-1}$, we define the constraint set $\\mathcal{C}(\\mu, \\nu) = \\{P \\in [0, 1]^{v \\times u} : P\\mathbb{1} = \\mu, P^T\\mathbb{1} = \\nu\\}$. S2T2 finds a joint probability matrix $P \\in \\mathcal{C}(\\mu, \\nu)$ and defines a new target-domain LLM $M'$ with embedding matrix $E' = (P^T \\odot (1/\\mu))E \\in \\mathbb{R}^{u \\times d}$ and language head $L' = (P \\odot (1/\\nu))L \\in \\mathbb{R}^{u \\times d}$ substituted for $(E, L)$ in (1). Here, $A \\odot v$ represents a Hadamard product broadcasted along the last dimension. It is crucial to perform such a Hadamard product, since we want the new token embedding and old token embedding to be on the same scale. More generally, one could use different $P$ matrices to translate $E$ and $L$, but we focus on a single $P$ here for simplicity."}, {"title": "Finding P via Sparse Sinkhorn", "content": "Since it is difficult to directly parameterize a joint probability matrix $P \\in \\mathcal{C}(\\mu, \\nu)$, we instead maintain a dense weight matrix $C \\in \\mathbb{R}^{v \\times u}$ and recover $P$ as the solution to the following two equivalent optimization problems.\n$\\begin{aligned}\n&\\min _{P^{\\prime}} \\frac{1}{2}\\left\\|P^{\\prime}-C\\right\\|_{F}^{2} \\qquad (2)\n&\\text { s.t. } \\quad P^{\\prime} \\in \\mathcal{C}(\\mu, \\nu)\n\\end{aligned}\n\\qquad\n$\\begin{aligned}\n&\\min _{P^{\\prime}} \\left\\langle -C, P^{\\prime}\\right\\rangle+\\frac{1}{2}\\left\\|P^{\\prime}\\right\\|_{F}^{2} \\qquad (3)\n&\\text { s.t. } \\quad P^{\\prime} \\in \\mathcal{C}(\\mu, \\nu)\n\\end{aligned}$\nNotice that (3) is the $l_2$ constrained optimal transport problem, which is known to generate sparse solutions [Essid and Solomon, 2018, Peyr\u00e9 et al., 2019]. Moreover, since $\\mathcal{C} = \\mathcal{C}_{1} \\cap \\mathcal{C}_{2}$ for the convex sets $\\mathcal{C}_{1} = \\{P \\in \\mathbb{R}_{+}^{v \\times u}, P\\mathbb{1} = \\mu\\}$ and $\\mathcal{C}_{2} = \\{P \\in \\mathbb{R}_{+}^{v \\times u}, P^T\\mathbb{1} = \\nu\\}$, these problems can be solved using iterative Dykstra's projections [Boyle and Dykstra, 1986], a Sinkhorn-like algorithm via with guaranteed convergence (see Algorithm 1).\nIn every Sinkhorn iteration, we solve a set of $l_2$ projections onto a probability simplex. This optimization problem enjoys an efficient backpropogation computation [Martins and Astudillo, 2016]. A small caveat is that we are not always projecting onto the unit simplex but rather onto a scaled simplex, so the optimization is modified accordingly in Algorithm 2."}, {"title": "Experiment", "content": "We conduct experiments on the UniRef50 [Suzek et al., 2015] protein sequence dataset using the OLMo-1B English LLM [Groeneveld et al., 2024] with batch size 16 and context length of 512. The training domain tokens in our experiment are bytes (single characters), and the target domain tokenizer is a new Byte-Pair Encoding (BPE) tokenizer [Gage, 1994] trained on UniRef50 with vocabulary size 512. The new tokenizer reduces the length our protein sequences by a factor of 1.82\u00d7 on average. This will in turn have sizable impact on the standard measure of model compression, bits-per-byte (BpB) [see Biderman et al., 2024, for details on calculating BpB]. To control the sparsity level of P, we add an entropy regularizer $\u03b1H(P)$ to the next token prediction loss with larger \u03b1 encouraging smaller entropy and hence sparser P. Unless otherwise specified, \u03b1 = 0.\nWe compare with four baseline methods: 1. Training an unconstrained translator P followed by whole-model finetuning. 2. Training a dense probabilistic translator P (using SOFTMAX in place of SPARSEMAX) followed by whole-model finetuning. 3. Finetuning the model directly using the original OLMo tokenizer. 4. Finetuning the model with the new tokenizer, resizing the embedding matrix E and language model head L by truncation."}, {"title": "Conclusion", "content": "We proposed S2T2 as a token translation technique for continual finetuning of LLMs on out-of-distribution data and demonstrate its effectiveness on protein sequence modeling. As a next step, we plan to expand this framework to adapt to other modalities such as code and images. Another natural extension is to combine the training and target token vocabularies to produce an effective \"multidomain\" LLM."}]}