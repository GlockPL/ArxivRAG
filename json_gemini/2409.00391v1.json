{"title": "Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders", "authors": ["Georgios Ioannides", "Adrian Kieback", "Aman Chadha", "Aaron Elkins"], "abstract": "Speech-based depression detection poses significant challenges for automated detection due to its unique manifestation across individuals and data scarcity. Addressing these challenges, we introduce DAAMAudioCNNLSTM and DAAMAudio Transformer, two parameter efficient and explainable models for audio feature extraction and depression detection. DAAMAudioCNNLSTM features a novel CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM), focusing dynamically on informative speech segments. DAAMAudio Transformer, leveraging a transformer encoder in place of the CNN-LSTM architecture, incorporates the same DAAM module for enhanced attention and interpretability. These approaches not only enhance detection robustness and interpretability but also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the DAIC-WOZ dataset, without reliance on supplementary information such as vowel positions and speaker information during training/validation as in previous approaches. Both models' significant explainability and efficiency in leveraging speech signals for depression detection represent a leap towards more reliable, clinically useful diagnostic tools, promising advancements in speech and mental health care. To foster further research in this domain, we make our code publicly available", "sections": [{"title": "I. INTRODUCTION", "content": "Acoustic modeling has become pivotal in identifying psychological and emotional conditions [1], [2]. Research has consistently highlighted the efficacy of speech processing in the automated, unbiased identification of psychiatric disorders, notably Major Depressive Disorder (MDD) [3]. A diverse array of diagnostic indicators and computational strategies have been introduced for depression detection, each presenting its unique strengths and challenges [4]\u2013[6]. Included among these are spectral [7], [8], prosodic [9], vocal timbre [10], and speech production characteristics [11], as well as advanced computational techniques like data augmentation [12], ensemble methods [13], transfer learning [14], and self-supervised pre-training [14]. Furthermore, the analysis of features related to the identity of the speaker has been explored for the detection of depression [15], [16], though their claimed results are not reproducible.\nThe realm of mental health diagnostics, especially for conditions like depression and MDD, faces significant challenges due to the scarcity and imbalance of publicly available datasets [17]. Traditional machine learning and deep learning approaches often struggle with these limitations, leading to models that may not generalize well across the diverse spectrum of depressive symptoms and severities encountered in clinical practice. This issue is compounded by the fact that depression manifests uniquely across individuals [18], requiring diagnostic systems to be highly sensitive to subtle and varied vocal biomarkers. Previous approaches have often been constrained by their computational complexity and the extensive number of parameters, rendering them impractical for real-time applications [14]. These models, while powerful, demand significant computational resources, limiting their deployment in scenarios where quick analysis is crucial. Additionally, many existing methods are not end-to-end and/or rely on supplementary information, such as vowel classification [19] and speaker-specific details [15], which are not always readily available or necessitate labor-intensive manual labeling.\nAccording to the World Health Organization (WHO), depression is a pervasive mental health disorder that significantly impacts individuals and societies worldwide [20]. It is characterized by a persistent low mood, loss of interest or pleasure in activities, and a range of physical and emotional symptoms that can severely affect a person's ability to function in daily life [21]. Depression is not just a temporary change in mood or a sign of weakness; it is a serious medical condition that requires understanding and treatment [22]. In 2019, approximately 280 million people worldwide were living with depression, highlighting its status as a major public health concern. The prevalence of depression varies by country, with the United States reporting a rate of 8.3% among its adult population in 2021 [20], [23], [24].\nThe integration of Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks has been a promising direction in addressing these challenges, leveraging CNNs' ability to extract rich spectral features and LSTMS' proficiency in capturing temporal dynamics [25]. However, the effectiveness of these models can be significantly hindered"}, {"title": "II. RELATED WORK", "content": "Depression detection has been approached from various angles, utilizing different modalities such as text, speech, and visual data. Among the early works, facial expression analysis has shown promise in identifying depressive states. For instance, the system presented by Nazira et al. (2021) [30] uses CNNs in conjunction with OpenCV and Haar Cascade Classifiers to analyze facial expressions for depression detection. This method demonstrated an accuracy of 81% and a recognition rate of 88%, highlighting the potential of visual data in mental health diagnostics. However, the need for specialized datasets and the difficulty of capturing consistent visual data limit the applicability of these methods in real-world settings.\nIn contrast, speech-based models offer significant advantages, including non-invasiveness and the ability to capture data in natural conversational settings. Speech carries rich information such as tone, tempo, and pauses, which can be indicative of a person's emotional state. Moreover, speech-based systems enable real-time monitoring, making them suitable for continuous assessment of mental health. Notably, Dubagunta et al. (2019) [10] leveraged the combination of prior knowledge-based signal processing methods and CNNs to detect depression from voice source-related information. Their study demonstrated that neurophysiological changes during depression, which affect laryngeal control, can be effectively detected using this approach, offering a promising direction for future research.\nDepAudioNet represents a seminal contribution to the field of automated depression detection through audio analysis. Introduced by Ma et al. (2016) [31], DepAudioNet addresses the limitations of traditional methods that predominantly rely on hand-engineered features. It automates the feature extraction and classification processes using a combination of convolutional and recurrent neural networks. The superiority of automatic feature extraction over hand-engineered features, particularly for tasks focused on paralinguistics such as speech-based depression detection, has been well documented in the literature [32]. The convolutional layers of DepAudioNet excel at capturing spectral features from audio signals [33], while the recurrent layers, typically LSTM networks, analyze temporal relationships within the speech data [34]. This dual approach enables the model to effectively process and interpret the complex, non-linear relationships inherent in human"}, {"title": "III. METHODS", "content": "Our proposed network architectures, which are illustrated in Figure 1, are designed to process audio data for the detection of depression, utilizing a combination of attention mechanisms and deep learning models tailored for audio feature extraction.\nThe processing pipeline for both models begins with raw audio input, which is transformed into a Mel Spectrogram. Using a Hanning window, mel-spectrogram features are extracted with a Mel filterbank that includes 40 frequency bins, a window length of w = 1024, and a hop length of h = 512. These features are standardized through z-normalization, yielding adjusted features represented as x, where x = (x \u2212 \u03bc)/\u03c3. Here, x is the original feature from the audio input, and \u03bc and o denote the mean and standard deviation, respectively, calculated from the features of each distinct audio recording. This normalization aligns with the methods used by Ma et al. [31] and Bailey et al. [35], ensuring consistency and mitigating class imbalance by cropping features to align with the duration of the shortest audio track. Random subsampling is applied to non-depressed (ND) instances to balance the dataset, which is then segmented into temporal portions of length Nseg = 120 for further processing."}, {"title": "A. DAAMAudioCNNLSTM", "content": "The DAAMAudioCNNLSTM model incorporates a Gaussian-based Attention Augmented Module (DAAM) before the first layer, initialized with 4 attention heads, each utilizing 16 or 24 Gaussian distributions. This mechanism focuses on enhancing the model's performance by weighting important spatio-temporal features within the Mel Frequency Cepstral Coefficients (MFCCs). The DAAM modifies the input tensor X to X', computed as:\nX' = \u03a7\u00b7 \u03a0_{i=1}^{N} e^{-(\u03a7\u2212(\u03bc+\u03b4i))^2/(2\u03c3i^2)} / \u221a{2\u03c0\u03c3^2}  (1)\nwhere N represents the number of Gaussian distributions in the mixture, \u03bc is the mean of X, and \u03b4i and \u03c3\u2081 are learnable parameters that adjust the mean and standard deviation for each Gaussian component.\nThe network architecture proceeds with a convolutional layer (kernel size: 40 \u00d7 3, stride: 1 \u00d7 1, padding: 0 \u00d7 1) producing a feature map of depth D = 128 and length L = 120, followed by max pooling (kernel size: 3, stride: 3, padding: 0) to reduce the length to L = 40. An LSTM layer with 3 layers of 128 hidden units captures temporal"}, {"title": "B. DAAMAudioTransformer", "content": "The DAAMAudioTransformer model enhances the processing of audio data with a CustomAttention mechanism, specifically designed for this task. Similar to DAAMAudioCNNLSTM, it starts with a Density-based attention mechanism, initialized with 4 attention heads, each using 24 Gaussian distributions. This attention mechanism selectively focuses on critical frequency bin features within the MFCCs, improving the capture of spatio-temporal characteristics.\nFollowing the attention block, the DAAMAudioTransformer employs a series of transformer encoder layers to process the attended features. Each encoder layer comprises a multi-head self-attention mechanism and a feed-forward network (FFN), with dropout applied for regularization (dropout rate: 0.1). The transformer encoder is configured with a model dimensionality dmodel = 120, 4 attention heads, and a feed-forward dimension dfeedforward = 2048. The output from the transformer encoder is passed through a fully connected layer with a sigmoid activation function to produce the final binary classification output, predicting the probability of depression."}, {"title": "C. Training and Optimization", "content": "Both models are trained using the Adam optimizer [36], with an initial learning rate of 0.001 and a decay rate of 0.9 applied at intervals defined by depoch = 2. The binary cross-entropy loss (BCELoss) is employed as the training objective, quantifying the difference between the predicted probabilities and the actual binary labels. This loss function is well-suited for binary classification tasks, driving the optimization process for both models.\nIn summary, the DAAMAudioCNNLSTM and DAAMAudioTransformer models leverage advanced attention mechanisms and deep learning architectures to capture both local and global dependencies in audio data, making them potent tools for depression detection in speech."}, {"title": "IV. RESULTS AND DISCUSSION", "content": ""}, {"title": "A. Dataset", "content": "The Distress Analysis Interview Corpus Wizard of Oz (DAIC-WOZ) dataset plays a central role in the development of automated methods for the detection and analysis of psychological disorders, particularly depression. Developed as part of the DARPA Detection and Computational Analysis of Psychological Signals (DCAPS) program, this dataset is designed to advance the understanding and detection of psychological stress signals with a focus on depression [13], [37]. We have received approval to use this dataset from the University of Southern California (USC) Institute of Creative Technologies.\nThe DAIC-WOZ dataset consists of 189 sessions, each corresponding to an interview between a participant and a virtual interviewer named \"Ellie.\" These interviews were conducted in a controlled environment, with Ellie being operated by a researcher in a separate room [38]. The interviews, varying in length from 7 to 35 minutes, provide a comprehensive multimodal dataset, including audio recordings, transcribed text, facial expressions, and physiological signals. For privacy reasons, the raw visual data has not been disclosed. Instead, the dataset provides visual features extracted using the OpenFace framework and the FACET toolbox, along with raw audio files sampled at 16 kHz [16]. For the purposes of this work, the focus is primarily on the audio component of the dataset, which offers valuable insights into the verbal and paraverbal aspects of communication that are indicative of depressive symptoms.\nThe training segment of the dataset comprises 107 files, detailed as follows: 27 files from females without depression (ND) and 17 from females with depression (D), contributing to a female total of 44 files (41%); for males, 49 ND and 14 D files make up a total of 63 files (59%), leading to an overall distribution of 76 ND (71%) and 31 D (29%) across genders. The validation segment includes 35 files, split into 23 ND and 12 D, to facilitate machine learning applications. The average interview time across all sessions is 956.33 seconds (~15.94 minutes), with a standard deviation of 269.96 seconds (~4.50 minutes). The distribution of interview times ranges from 414.80 seconds (~6.91 minutes) to 1966.20 seconds (~32.77 minutes), illustrating the variability in session durations.\nA critical feature of the DAIC-WOZ dataset is its imbalance in terms of participants' level of depression. The dataset is heavily biased towards participants with PHQ-8 scores below 10, indicating no to mild depression. For instance, the total time allocated to interviews with participants scoring below 10 is 2,121.14 minutes (~35.35 hours), while participants scoring 10 or higher account for only 891.30 minutes (~14.86 hours). This imbalance underscores the necessity of careful handling during model development and evaluation to prevent bias towards the overrepresented class.\nThe dataset is divided into three subsets: Training, Testing, and Development, essential for the development and evaluation of predictive models for depression detection. The training split consists of 107 participants, categorized into depressed (D) and non-depressed (ND), with a total interview time of 454.00 minutes (~7.57 hours) for D participants and 1,160.22 minutes (~19.34 hours) for ND participants. The test split comprises 47 participants, with a total interview time of 198.92 minutes (~3.32 hours) for D participants and 598.28 minutes (~9.97 hours) for ND participants. The development split consists of 35 participants, with a total interview time of 238.38 minutes (~3.97 hours) for D participants and 362.64 minutes (~6.04 hours) for ND participants. In this study, we assess the performance of our models on the validation portion of the dataset, aligning our methodology with established practices in the literature."}, {"title": "B. Evaluation Metrics", "content": "In our study, we employ the Macro F1 Score as the primary evaluation metric, which offers a more robust performance measure in the context of imbalanced datasets, such as the DAIC-WOZ dataset. The Macro F1 Score is defined as the average of the F1 Scores of each class, calculated by Equation 2.\nMacro F1 Score = (2 * Precision * Recall) / (Precision + Recall)  (2)\nwhere Precision is the ratio of true positives to the sum of true positives and false positives, and Recall is the ratio of true positives to the sum of true positives and false negatives, averaged over all classes. Furthermore, we utilize a metric termed the Importance Factor (IF), as introduced by Ioannides et al. [26], which is derived from the Density Attention (DA)"}, {"title": "C. Current Benchmarks", "content": "In Table IV, we present a benchmark comparison of various models for Speech Depression Detection, utilizing solely depression labels for training. Notably, both DAAMAudioCNNLSTM and DAAMAudioTransformer distinguish themselves through their performance metrics and architectural efficiencies.\nDAAMAudioCNNLSTM, with 280K parameters, is particularly noteworthy for its lightweight architecture, which contrasts favorably with other models in the literature, such as SpeechFormer [14], which utilizes 33M parameters. The low parameter count of DAAMAudioCNNLSTM results in a model that is more computationally efficient, facilitating faster training and inference times. This efficiency is advantageous in scenarios with limited computational resources, translating into reduced energy consumption and lower operational costs, making it a more sustainable choice for large-scale deployment, especially in mobile and edge computing environments where power and computational capabilities are constrained.\nThe DAAMAudioTransformer, on the other hand, while more parameter-heavy with 1.1M parameters, offers a more sophisticated approach to feature extraction and data processing. Its architecture integrates custom attention mechanisms with transformer encoder layers, enabling it to capture both local and global dependencies in audio data effectively. Despite its higher parameter count, the DAAMAudioTransformer remains significantly smaller than other state-of-the-art models like WavLM, which contains 316M parameters [40].\nWhen comparing performance metrics, DAAMAudioCNNLSTM achieved an F1 score of 0.815 for ND classifications and 0.643 for D classifications, leading to a macro F1 score of 0.702. This score underscores its strong ability to balance precision and recall across different categories. The DAAMAudioTransformer, however, slightly outperforms DAAMAudioCNNLSTM in the ND category, with an F1 score of 0.84 for ND classifications and 0.6 for D classifications, resulting in a higher macro F1 score of 0.72. This makes"}, {"title": "V. CONCLUSION", "content": "This work introduces two novel networks \u2013 DAAMAudioCNNLSTM and DAAMAudioTransformer\u2013designed for the detection of depression directly from speech signals. DAAMAudioCNNLSTM combines a CNN-LSTM architecture with a lightweight Density Adaptive Attention Mechanism, achieving a state-of-the-art F1 macro score of 0.702 on the DAIC-WOZ dataset. Similarly, DAAMAudioTransformer leverages a transformer-based architecture with custom Density attention mechanisms, capturing both local and global dependencies in the audio data, resulting in a outperforming F1 macro score of 0.72 on the same Dataset. Importantly, both models achieve these"}]}