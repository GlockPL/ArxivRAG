{"title": "Zero-shot Musical Stem Retrieval with Joint-Embedding Predictive Architectures", "authors": ["Alain Riou", "Antonin Gagner\u00e9", "Ga\u00ebtan Hadjeres", "Stefan Lattner", "Geoffroy Peeters"], "abstract": "In this paper, we tackle the task of musical stem retrieval. Given a musical mix, it consists in retrieving a stem that would fit with it, i.e., that would sound pleasant if played together. To do so, we introduce a new method based on Joint-Embedding Predictive Architectures, where an encoder and a predictor are jointly trained to produce latent representations of a context and predict latent representations of a target. In particular, we design our predictor to be conditioned on arbitrary instruments, enabling our model to perform zero-shot stem retrieval. In addition, we discover that pretraining the encoder using contrastive learning drastically improves the model's performance. We validate the retrieval performances of our model using the MUSDB18 and MoisesDB datasets. We show that it significantly out- performs previous baselines on both datasets, showcasing its ability to support more or less precise (and possibly unseen) conditioning. We also evaluate the learned embeddings on a beat tracking task, demonstrating that they retain temporal structure and local information.", "sections": [{"title": "I. INTRODUCTION", "content": "Musical stem retrieval consists of finding a stem that fits when played together with a reference audio mix. It relies on finding a good way to measure the compatibility between different audio samples, considering elements such as tempo, tonality, and various stylistic features.\nEarly research on this topic focuses on automatic mashup creation, retrieving full songs rather than individual stems. It computes a \"mashability\" score using algorithms for beat tracking and chord estimation, then selects the song that maximizes this score [1]. How- ever, these methods only consider rhythmic and harmonic features, overlooking timbre and playing style, which are crucial aspects of musical compatibility. Moreover, these approaches are sensitive to errors made by the underlying algorithms and require full mixes (chord estimation on a drum stem returns unreliable results), making them unsuitable for single-stem retrieval [2].\nLater, Siamese networks [3] and self-supervised learning (SSL) emerged as a promising way to model the similarity between data samples without relying on handcrafted rules. Given a neural network that projects input data in a metric latent space, the core idea is to present it with pairs of related inputs and train it to map each pair to nearby points within this space, usually by optimizing a contrastive loss [4]. This approach has shown success in producing informative representations in various domains such as image [4], audio [5], [6], and multimodal data [7], [8]. In particular, it has also been applied to compatibility estimation between loops [9], and to drum sample retrieval by using tracks and their drum samples as positive pairs [10]. More recently, Stem-JEPA [2] proposes using Joint-Embedding Predictive Architectures (JEPA) for stem retrieval. JEPAs, indeed, involve training two networks-a context encoder and a predictor-to jointly predict the target's representation from the context's one. A key feature of these architectures is that the predictor can support additional conditioning, enabling the learning of richer latent spaces than with previous contrastive methods [11]\u2013[13]. In particular, [2] performs stem retrieval by conditioning the predictor on the instrument class of the target stem. However, their model is limited to four instruments, restricting its practical usability.\nIn this paper, we build upon these advancements to propose a new JEPA-based model for musical stem retrieval. Our model is more flexible, handling any music input and allowing conditioning on any instrument provided as free-form text. Additionally, we incorporate a contrastive pretraining phase before training the full JEPA itself and show that it significantly boosts performance. We also replace the conditioning method from [2] with FiLM-inspired conditioning [14]. We evaluate our model on retrieval tasks using two source separa- tion datasets: MUSDB18, whose tracks are divided into four standard stems [15], and MoisesDB, which contains more stems and for which annotations are hierarchical and more precise [16]. This allows us to validate the effectiveness of our design choices across different levels of conditioning granularity. Finally, we evaluate the embeddings learned by our model on a beat tracking task, highlighting that they retain some temporal information, making them potentially usable for tasks beyond stem retrieval. We make our code publicly available."}, {"title": "II. METHOD", "content": "Our model comprises two trainable networks: an encoder $f_{\\theta}$ with parameters $\\theta$ and a predictor $g_{\\phi}$ with parameters $\\phi$. The encoder is a Vision Transformer (ViT) [17] that takes Log-scaled Mel- Spectrograms (LMS) as input and returns a grid of latent embeddings, while the predictor is a Multi-Layer Perceptron (MLP) that acts separately on each of these embeddings. These two networks are trained in two successive phases, as depicted in Figure 1.\nFirst, we pretrain only the encoder $f_{\\theta}$ using contrastive learning. Given a chunk of audio composed of $S$ stems, we first pick one as the target $x_t$ and a random subset of the remaining ones as the context $x_c$. $x_c$ and $x_t$ are then turned into LMS, divided into a regular time and frequency grid of $K$ non-overlapping patches, and fed into the encoder $f_{\\theta}$. This encoder returns sequences of patch-wise embeddings $z_c, z_t \\in \\mathbb{R}^{K\\times d}$, with $d$ being the latent dimension.\nWe then use these embeddings' mean over $K$ as the latent repre- sentations $s_c, s_t \\in \\mathbb{R}^d$ of the context mix and target stem, respectively. Finally, the parameters $\\theta$ of the encoder are optimized by minimiz- ing the contrastive loss $\\mathcal{L}_c$ within batches of representations, using $(s_c, s_t)$ as a positive pair and the other elements of the batch $\\mathcal{B}$ as negative samples, as in [4]:\n$\\mathcal{L}_c(s_c, s_t) = -log \\left(\\frac{exp(sim(s_c, s_t)/\\tau)}{\\sum_{s'\\in \\mathcal{B}\\{s_t\\}} exp(sim(s_c, s')/\\tau)}\\right)$"}, {"title": "B. Phase 2: Joint-Embedding Predictive Architecture", "content": "The second phase is similar to [2]. We randomly pick a context mix $x_c$, a target stem $x_t$, and turn them into LMS. We denote the instrument label of the target stem as $c$. The context mix passes through the encoder $f_{\\theta}$, which returns a sequence of embeddings $z_c = (z_{c1},...,z_{cK}) \\in \\mathbb{R}^{K\\times d}$. Conversely, we obtain $z_t = (z_{t1},...,z_{tK}) \\in \\mathbb{R}^{K\\times d}$ by passing the target stem $x_t$ through a target encoder $f_{\\bar{\\theta}}$ with parameters $\\bar{\\theta}$.\nThen, a predictor $g_{\\phi}$, conditioned on the target instrument label $c$, produces predictions $\\bar{z}_{tk} \\in \\mathbb{R}^d$ from the context embeddings, i.e., for all $k \\in \\{1, ..., K\\}$:\n$\\bar{z}_{tk} = g_{\\phi}(z_{ck}, c)$.\nThe parameters $(\\theta, \\phi)$ are optimized by minimizing the mean squared error $\\mathcal{L}$ between the (normalized) predictions and target embeddings:\n$\\mathcal{L}(\\bar{z}_t, z_t) = \\frac{1}{K} \\sum_{k=1}^{K} \\left\\Vert \\frac{\\bar{z}_{tk}}{\\Vert \\bar{z}_{tk} \\Vert} - \\frac{z_{tk}}{\\Vert z_{tk} \\Vert} \\right\\Vert^2$.\nFurthermore, the parameters $\\bar{\\theta}$ of the target encoder are optimized as an EMA of $\\theta$, i.e.\n$\\theta_t = \\tau_t \\bar{\\theta}_{t-1} + (1 - \\tau_t) \\theta_t$,\nwhere the EMA rate $\\tau_t$ is linearly interpolated between $\\tau_0$ and $\\tau_T$, $T$ being the total number of training steps, as in [2]."}, {"title": "C. Predictor conditioning", "content": "In [2], it is proposed to condition the predictor by learning a specific embedding for each instrument class. In [2], the four classes are fixed and correspond to \"bass\", \"drums\", \"vocals\", and \"other\". This is a major limitation of the model since updating the classes would require retraining the whole system.\nWe propose replacing this with conditioning on the text embed- dings of a pretrained CLAP model [18]. Our predictor can, therefore, be conditioned on various instrument classes provided as free-form text descriptions. Moreover, since CLAP is trained on text/audio pairs, the embeddings of instruments with similar timbre are expected to be close. This is particularly helpful for zero-shot retrieval, i.e., retrieving a stem whose instrument is not in the training set.\nLet $c = CLAP(c) \\in \\mathbb{R}^p$ be the embedding used for conditioning the predictor. While in [2], this instrument class embedding is concatenated to the context embeddings $z_{ci}$, here we instead use FiLM conditioning [14]. More precisely, our predictor $g_{\\phi}$ is a MLP with $L$ layers and $m$ hidden units. For $l \\in \\{1, . . ., L - 1\\}$, we learn two affine mappings $\\beta_l$ and $\\gamma_l : \\mathbb{R}^p \\to \\mathbb{R}^m$. Let $h_l$ be the output of the $l$-th layer of $g_{\\phi}$, the input of the $l + 1$-th layer is:\n$h_{l+1} = ReLU(\\gamma_l (c) \\cdot h_l + \\beta_l(c))$,\nwhere $\\cdot$ denotes the Hadamard product."}, {"title": "D. Architecture and training details", "content": "The encoder is a ViT-Base [17] with $d = 768$ dimensions while the predictor is a MLP with $L = 6$ layers and $m = 1024$ hidden units. To compute the CLAP embeddings, we use the pretrained music checkpoint from LAION.\nWe extract audio chunks of 8 seconds during training, which we convert into log-scaled Mel-spectrograms with 80 Mel bins, a window size of 25 ms and a hop size of 10 ms. We use 16 \u00d7 16 patches, thus a sequence size of $K = \\frac{8 \\cdot 800}{16} = 250$.\nIn both phases 1 and 2, we train our model during 300k steps using AdamW [19], with a batch size of 256, a base learning rate of $1e-3$, and a cosine annealing scheduling after 20k steps of linear warmup. All other hyperparameters are consistent with those used in [2]. Each training phase takes approximately four days on a single A100 GPU with 40 GB of memory."}, {"title": "E. Training data", "content": "As in [2], the model is trained on a proprietary dataset of 20,000 multi-track recordings of diverse music genres (rock, rap, R&B, country...), totaling 1350 hours. For most of the stems, we have access to a hierarchy of labels describing the instrument (e.g., \"guitar\" \u2192 \"electric guitar\" \u2192 \"lead electric guitar\"). During phase 2, we pick the target label $c$ randomly from the hierarchy to make our model robust to both coarse and fine conditioning. When no labels are available, we use the word \"music\" as conditioning. Finally, we ensure not to pick silent chunks when sampling the context mix and target stem, as detailed in [2]."}, {"title": "III. EXPERIMENTS ON MUSICAL STEM RETRIEVAL", "content": "We evaluate our model on a retrieval task using two pub- licly available datasets for source separation: MoisesDB [16] and MUSDB18 [15]. The MoisesDB dataset contains 240 tracks divided into 38 instruments across 11 categories (drums, guitar, vocals...), totaling 2,585 stems. MUSDB18 consists of 150 tracks, each divided into exactly four stems: \"bass\", \"drums\", \"vocals\", and \"other\". By evaluating our model on both datasets, we can assess how well the model handles conditioning at varying levels of granularity. Notably, MoisesDB includes instruments/categories that are absent from the training set, allowing us to test our model's zero-shot retrieval capabilities."}, {"title": "A. Experimental setup", "content": "We test the ability of our model to retrieve the missing stem from a given mix when conditioned on the instrument label of the missing stem. Our dataset comprises $N$ tracks $\\lbrace x^{(1)},..., x^{(N)} \\rbrace$ and each track $x^{(n)}$ is composed of $S_n$ stems $x^{(n)} = \\lbrace x_1^{(n)},...,x_{S_n}^{(n)} \\rbrace$.\nGiven an individual stem $x_t^{(n)}$, $x_c^{(n)}$ denotes the mix containing all stems from $x^{(n)}$ without $x_t^{(n)}$. We first encode all individual stems and average the resulting embeddings over the patch dimensions to obtain a reference set $\\mathcal{Z} = \\lbrace z_t^{(n)}\\rbrace$ of latent representations.\nIn addition, we pass the mixes $x_c^{(n)}$ through the encoder, whose outputs are fed into the predictor conditioned on the corresponding instrument label of $x_t^{(n)}$ to produce queries $q^{(n)}$. The queries are averaged similarly to the representations $z_t^{(n)}$. For such a query $q^{(n)}$, we then measure the distance to the representation of the corresponding stem $z_t^{(n)}$. To do so, we rely on two metrics:\nThe Recall at k measures the proportions of queries $q^{(n)}$ for which the missing stem $z_t^{(n)}$ lies in their k nearest neighbors. We report the recall at k of the different models for $k \\in \\lbrace 1, 5, 10 \\rbrace$.\nThe Normalized Rank [10] of a query $q^{(n)}$ is the position of the ground-truth stem $z_t^{(n)}$ in the sorted list of distances $\\lbrace ||q^{(n)} - z|| \\rbrace_{z \\in \\mathcal{Z}}$, normalized by the total number of stems $|\\mathcal{Z}|$. For each model, we report the mean and median Normalized Ranks over all queries."}, {"title": "B. Baselines", "content": "There are no prior works specifically addressing stem retrieval conditioned on arbitrary instruments, so we lack a fair baseline for comparison. The closest related work is Stem-JEPA [2], but it is limited to four stems.\nAs a reference, we provide the performance of two models: one trained using only contrastive learning (equivalent to phase one of our approach) and the original Stem-JEPA model. For the contrastive baseline, since there is no conditioning, we use the (averaged) embeddings as queries directly."}, {"title": "C. Retrieval results", "content": "We present our results in Table I. For MUSDB18, the predictor is conditioned on text labels: \"bass\", \"drums\", \"vocals\", and \"music\". For MoisesDB, we report results both when conditioning the model on the actual instrument labels (fine conditioning) and on broader categories (coarse conditioning).\nThe results highlight the effectiveness of our design choices. On MoisesDB, using CLAP embeddings to condition the predictor on actual instrument/category labels significantly improves performance compared to Stem-JEPA: R@1 jumps from 9.9% to 22.0% or 19.2%, respectively. On MUSDB18, the performances are roughly similar (33.0% vs. 33.5% for R@1). This can be attributed to the \"other\" category, where instrument specificity is less defined.\nInterestingly, while contrastive learning performs poorly on R@1 (2.7% for MoisesDB) compared to Stem-JEPA (9.9%), it significantly outperforms it in all other metrics. This reveals that contrastive learning captures a solid global structure (with close projections for all stems of a track) but that the absence of conditioning avoids retrieving the right missing stem. On all datasets, contrastive pretraining substantially boosts most metrics, but it also lowers R@1 (12.5% vs. 19.3% on MoisesDB with fine conditioning), indicating that the model's conditioning on the instrument label is being partially ignored by the predictor.\nHowever, adding the FiLM conditioning at each layer of the predictor (rather than concatenating the embedding to its input) mitigates this effect. Indeed, the latter yields the best overall per- formance, consistently outperforming other models, particularly for R@1 (22.0% on MoisesDB) and median normalized rank (0.2% on MoisesDB). Finally, the close results on MoisesDB between the two conditioning levels show that using CLAP embeddings makes our model robust to finer and coarser conditioning."}, {"title": "D. Instrument-specific analysis", "content": "Metrics from Table I assess how closely the latent representations of target stems align with their corresponding queries. In contrast, here we examine which stem is identified as the closest match to our queries. We focus on analyzing the nearest-neighbor retrievals for queries on MoisesDB with fine conditioning. The analysis aims to explore both the types of failures our model encounters and the impact of the conditioning instrument on the retrieval performance. Notably, some instruments are present in the training set, while others are either absent or labeled differently (e.g., \"lead female singer\" vs. \"female lead vocals\"). Ideally, the structure of the CLAP"}, {"title": "IV. TEMPORAL INFORMATION IN EMBEDDINGS", "content": "Our previous results involve averaged embeddings and thus reveal the abilities of our model to capture global musical information such as style, tempo, tonality... In this part, we focus on whether local information is also captured. To do so, we evaluate the sequences of embeddings learned by our encoder on a beat tracking task.\nWe concatenate the outputs of the encoder along the frequency axis. Each embedding encodes a 160 ms segment of the audio signal and is up-sampled using K = 8 linear probes. Each probe targets a 20 ms sub-segment within the 160 ms window and outputs a beat activation function. We use 8-fold cross-validation and report"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce the first model for zero-shot musical stem retrieval using JEPAs. We demonstrate the effectiveness of our design choices by evaluating our model on different benchmarks, and we significantly outperform previous baselines. In addition, as our results on beat tracking reveal, our embeddings are not only useful for musical stem retrieval but also retain temporal information, making them potentially usable for both global and local MIR tasks or music accompaniment generation.\nMore generally, despite our study mostly focuses on stem retrieval, pretraining a JEPA with contrastive learning and/or using a pretrained multimodal model for conditioning its predictor is not limited to the music domain. Our design choices may, therefore, be useful to improve performances on other tasks beyond the scope of this paper."}]}