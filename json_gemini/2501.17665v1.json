{"title": "Planning with Vision-Language Models and a Use Case in Robot-Assisted Teaching", "authors": ["Xuzhe Dang", "Lada Kudl\u00e1\u010dkov\u00e1", "Stefan Edelkamp"], "abstract": "Automating the generation of Planning Domain Definition Language (PDDL) with Large Language Model (LLM) opens new research topic in AI planning, particularly for com-plex real-world tasks. This paper introduces Image2PDDL, a novel framework that leverages Vision-Language Models (VLMs) to automatically convert images of initial states and descriptions of goal states into PDDL problems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL ad-dresses key challenges in bridging perceptual understanding with symbolic planning, reducing the expertise required to create structured problem instances, and improving scalabil-ity across tasks of varying complexity. We evaluate the frame-work on various domains, including standard planning do-mains like blocksworld and sliding tile puzzles, using datasets with multiple difficulty levels. Performance is assessed on syntax correctness, ensuring grammar and executability, and content correctness, verifying accurate state representation in generated PDDL problems. The proposed approach demon-strates promising results across diverse task complexities, suggesting its potential for broader applications in AI plan-ning. We will discuss a potential use case in robot-assisted teaching of students with Autism Spectrum Disorder.", "sections": [{"title": "Introduction", "content": "Automating the generation of Planning Domain Definition Language (PDDL) problems has long been a challenge in AI planning, particularly for applications involving complex, real-world tasks. Traditional approaches to PDDL genera-tion often require domain-specific knowledge and substan-tial manual effort to accurately structure problem instances. This barrier limits the scalability and accessibility of AI planning, as defining object relationships, spatial configura-tions, and task goals is labor-intensive and demands exper-tise.\nRecent advances in Large Language Models (LLMs) have opened new possibilities by enabling models to interpret and translate visual and textual data into structured, symbolic formats, bridging the gap between perceptual understand-ing and symbolic reasoning (Liu et al. 2023; Xie et al. 2023; Shirai et al. 2024). However, each approach has its limita-tions. LLM+P (Liu et al. 2023) requires text descriptions for both initial and goal states, which, in practical applications, depends on human input or additional tools to convert im-ages to text. Xie's work (Xie et al. 2023) focuses solely on translating text descriptions of goal states, lacking a mech-anism to interpret visual data. Meanwhile, ViLaIn (Shirai et al. 2024) can generate PDDL problems from images but relies on an object detection model and a captioning model to process images, adding complexity to the pipeline. These limitations highlight the need for a more streamlined ap-proach that can directly process both visual and textual in-puts for automated PDDL problem generation.\nTo address these challenges, we present Image2PDDL, a novel framework that leverages Vision-Language Mod-els (VLMs) to automatically generate PDDL problems from both visual and textual inputs. Image2PDDL is designed to handle a wide range of input formats, including im-ages of initial and goal states as well as textual descrip-tions, allowing it to adapt seamlessly across diverse do-mains and task complexities. By directly interpreting spa-tial and categorical relationships within visual data and inte-grating them with goal-oriented descriptions, Image2PDDL provides a flexible solution for PDDL problem generation. We evaluated the framework across traditional planning do-mains-Blocksworld and Sliding-Tile Puzzle-as well as a 3D world domain, Kitchen, assessing both syntax correct-ness and content correctness of the generated PDDL prob-lems. In all domains, Image2PDDL demonstrated promising results, effectively reducing the need for domain-specific ex-pertise and making AI planning more accessible, scalable, and applicable to real-world scenarios.\nImage2PDDL makes several key contributions to the field of AI planning. Its adaptability across both traditional plan-ning domains and complex 3D environments underscores its versatility and scalability, positioning it as a valuable tool for a wide range of AI planning applications. By reducing dependency on domain-specific knowledge, Image2PDDL broadens accessibility and opens new possibilities for apply-ing AI planning to real-world tasks. Future work could focus on enhancing the model's capacity to interpret more intricate object relationships and dynamic scenarios, further expand-ing the potential of automated planning across diverse and complex environments.\nWe will first introduce Image2PDDL together with exper-iments in several benchmark domains, and then discuss a"}, {"title": "Related Work", "content": "Recent advances in LLMs and VLMs have spurred inter-est in their applications to automated planning. Most cur-rent work in this area focuses on using LLMs and VLMs as planners, where the models are tasked with generating plans directly based on high-level descriptions or visual inputs (Huang et al. 2022; Raman et al. 2022; Zhang et al. 2023; Dagan, Keller, and Lascarides 2023; Guan et al. 2023). In these approaches, the models produce a sequence of sym-bolic actions (Lin et al. 2023), code (Liang et al. 2023), or predefined skills (Singh et al. 2023; Brohan et al. 2023) to complete a task, acting as planners that bridge the gap be-tween high-level goals and executable actions. A significant drawback of these methods is that the generated plans are not always guaranteed to be correct, as both LLMs and VLMs may produce sequences that overlook domain constraints or violate task feasibility. Additionally, the planning process is inherently opaque, making it difficult to verify or explain the rationale behind the generated plans.\nRecent research has also explored the potential of LLMs and VLMs in directly generating PDDL domains or PDDL problems, shifting the focus from plan generation to the for-mulation of structured problem instances (Liu et al. 2023; Xie et al. 2023; Shirai et al. 2024; Smirnov et al. 2024; Os-wald et al. 2024). These approaches aim to leverage the lan-guage and visual understanding capabilities of LLMs and VLMs to automate the creation of PDDL problem defi-nitions, which include initial and goal states and domain-specific constraints."}, {"title": "Method", "content": "The Image2PDDL framework leverages VLM to parse vi-sual and textual inputs, transforming them into Planning Do-main Definition Language (PDDL) problem representations based on pre-defined domains. The core idea is to utilize the spatial relationship understanding that VLMs offer, en-abling the translation of complex images and descriptions into structured PDDL problems, which traditionally require extensive domain expertise to manually create.\nFramework Overview\nImage2PDDL consists of a pipeline, as shown in Figure 7, that translates an image of an initial state and either an image or text description of a goal state into PDDL problem format. This is achieved in in the following three main steps, each using ChatGPT40 for structured output generation.\nInitial State Translation: First, we input the image of the initial state into ChatGPT4o, which is guided by a structured prompt. This prompt includes an example of the desired out-put format to encourage accurate parsing of spatial relation-ships between objects. The model generates a pre-defined textual representation of the spatial relationships observed in the initial state, such as object locations and their relative positions.\nGoal State Translation: For the goal state, we offer ei-ther an image or a text description. Similarly to the initial state, ChatGPT40 is prompted to parse this goal state in-"}, {"title": "Experiments and Evaluation", "content": "To evaluate the Image2PDDL framework, we prepared datasets across three distinct domains: Blocksworld, Sliding-Tile Puzzle, and Kitchen Domain. Each domain includes scenarios categorized by difficulty levels-Easy, Medium, and Hard\u2014allowing us to test the framework's scalability and accuracy across a range of task complexities. For each difficulty level, we prepared 50 unique scenarios. \nThe Blocksworld domain assesses Image2PDDL's ability to interpret spatial relationships by arranging blocks, with difficulties based on the number of blocks (5 for Easy, 6 for Medium, and 7 for Hard). In the Sliding-Tile Puzzle domain, the framework rearranges tiles to match a goal configura-tion, testing sequential spatial handling across three puzzle sizes: 8 tiles (Easy), 15 tiles (Medium), and 24 tiles (Hard). The Kitchen Domain, created in IsaacSim, evaluates real-world scenario comprehension by identifying and catego-rizing objects. Task difficulty increases from identifying a single item's location (Easy) to distinguishing between two items by attributes (Medium) and recognizing three items with finer detail, such as brand or color (Hard).\nFor each scenario, the dataset provides an initial and goal image alongside a textual goal state description, enabling Image2PDDL to process diverse data formats and effec-"}, {"title": "Results", "content": "Performance was assessed based on two metrics: Syntax Correctness and Content Correctness. Syntax correctness was verified by passing the generated PDDL problems to a Fast Downward planner (Helmert 2006) to ensure each problem could be successfully parsed and executed without syntax errors. Content correctness was evaluated by com-paring the initial and goal states described in the generated PDDL problem against the true states, verifying accurate representation of object locations and relationships. To ex-amine the flexibility of Image2PDDL, we used both images and text descriptions to represent goal states, analyzing the framework's ability to interpret both visual and textual in-puts accurately.\nSyntax Correctness: Overall, Image2PDDL demon-strated strong syntax correctness across all domains, with zero syntax errors recorded across both input modali-ties-images for both initial and goal states, and images for the initial state paired with a text description for the goal state. The generated PDDL problems were consistently ver-ified as syntactically valid by the Fast Downward planner, showing that the framework adheres effectively to PDDL syntax rules. Occasionally, the VLM returned PDDL prob-lems with syntax highlighting, but these highlights were eas-ily removed without affecting the problem structure. This consistency suggests that Image2PDDL reliably constructs grammatically correct and executable PDDL statements, re-gardless of input type or task complexity within each do-main.\nContent Correctness: Our analysis shows that as domain complexity increases, Image2PDDL is more prone to gen-erating incorrect states in PDDL problems. We reviewed er-rors within each domain to identify recurring patterns and domain-specific challenges."}, {"title": "Image2PDDL Use Case", "content": "Robot-assisted teaching of students with Autism Spectrum Disorder Teachers utilize different types of prompts to sup-port student learning. In robot-assisted education, robots can deliver physical, verbal, visual or gestural prompts, and can teach problem-solving through demonstration. Research in-dicates that students with Autism Spectrum Disorder (ASD) often respond positively to interactions with robots (van den Berk-Smeekens et al. 2020).\nRobot-assisted teaching shows potential for significant enhancements in educational outcomes of students with ASD. Educators note that children with ASD may benefit from robot predictability and consistency, especially with humanoid robots. However, further research is needed to verify these observations. It is essential to identify specific use cases and conditions under which skills acquired dur-ing robot-assisted sessions can be effectively transferred to the child's daily life. Additionally, we should consider the effects of robot morphology on children's responses and be-havior. Collaboration with ASD experts is crucial for devel-oping systems that are both effective and appropriate for use in special education (Alcorn et al. 2019). Students with ASD often show a strong interest in technological devices, includ-ing robots."}, {"title": "Automated assessment and planning of shoe-box tasks", "content": "There is a visual structure that allows to recognize how the shoe-box task should be completed. Vision-based machine learning methods seems to be suitable for exploring tasks and automatically evaluating them. A system that provides automatic assessment of structured tasks extended by ac-tions planning could be used in structured teaching, with a real robot in the role of teacher. To the best of our knowl-edge, there is no existing research on automated assess-ment of shoe-box tasks, or on automated action planning for robot-assisted shoe-box activities.\nFor learning new shoe-box tasks, students typically fol-low teacher guidance, or use picture-based or text-based in-structional systems (Bryan and Gast 2000). Additional ex-periments have explored alternative types of prompts, in-cluding video, computer, and other devices. For example, a Personal Digital Assistant was used to assist students in completing new tasks and to facilitate smoother transitions between tasks (Mechling and Savidge 2010).\nSome research addresses action planning for structured tasks (not for shoe-box tasks), employing a mathematical framework with an agent in the role of a teacher. In experi-ments, a humanoid NAO robot was utilized in a storytelling scenario to guide the child's gaze toward target screens, us-ing a sequence of planned actions (Baraka et al. 2020, 2022).\nExperiments with real robots can be performed with the humanoid robots and robot manipulators. Depth cameras can be used as main sensors to interpret the environment."}, {"title": "Objective", "content": "The main goal of a use case for Image2PDDL is to imple-ment a computerized system for the automatic assessment and planning of structured shoe-box tasks in robot-assisted education.\nThis system provides methods for creating models of the shoe-box tasks, automatically identifying the current state of a task instance and planning actions for it. It allows a robot to assist a student in completing the shoe-box task, which will be demonstrated by the real robot. The robot responds to the student's requests for help and provides instructions to complete the task. There are implemented multiple interac-tion strategies that represent different teaching approaches. Each strategy contains, for each task instance, a sequence of robot actions that lead to successful completion of the task, if the student follows robot instructions."}, {"title": "Methods of Research", "content": "The TEACCH Structured Work Session consists of struc-tured tasks. The shoebox task is a structured task with a box that serves as a workstation and contains all materials for the task (puzzles, cards, blocks,...). There is a visual structure that indicates how the task should be completed.\nStructured task assessment are answers to the following questions: What is the goal? Was the goal achieved in this instance? What needs to be done to achieve the goal?\nWe can represent the structured task as abstract structure containing set of given elements with pre-defined states, ac-"}, {"title": "Initial experiments", "content": "We conducted tests with a small dataset of shoebox tasks to verify that Image2PDDL can be used to transform visual and textual input into PDDL problem representations and identify shoe-box task states."}, {"title": "Syntax Correctness:", "content": "The syntax correctness of the PDDL problems was verified using a Fast Downward Planner. For both categories, the generated PDDL problems were consistently valid."}, {"title": "One-to-one correspondence task", "content": "For the initial experi-ments, we chose a specific type of shoebox task: a simple put-in task with a one-to-one correspondence between ob-jects and their target locations. We created a description of the PDDL domain and an example of the required output files, common to all scenarios in this domain.\nThe datasets were categorized by input data format. Each scenario in the ShoeboxImage category contains one input image (photo or diagram) depicting a specific task config-uration. The scenarios in the ShoeboxVideoSnapshots cate-gory are represented by two video snapshots of different task states, one of which is a snapshot of the initial state.\nOne of the main challenges in the analysis of visual input is objects that are partially obscured or not visible at all. We improved the generated task description by extending the Image2PDDL with simple action planning. The objects that are necessary for solving the shoe-box task are involved in at least one action and would be recognized, even if they were not identified in the images."}, {"title": "Content Correctness:", "content": "In some scenarios with multiple items of the same type, the number of items in the gen-erated text description and PDDL problem was incorrect,"}, {"title": "Robot as a teacher", "content": "The robot will assist the student in completing the shoe-box task e.g., by manually moving el-ements, pointing, or giving verbal prompts. The student can ask the robot for help, e.g., by using a button, gesture, or voice commands.\nThere are many interaction strategies to decide what the robot should do and when. We will consult the strategies with experts in ASD and implement multiple recommended strategies, as options for the user (teacher). Planned actions will be translated into abstract robot actions using the se-lected strategy; abstract robot actions will be translated into commands for the real robot.\nThe Image2PDDL framework provides tools for specify-ing a task using an image or text description. This allows us to work with more complex models of shoebox tasks and also makes the application more intuitive. In robot-assisted teaching, teachers should have a choice of how to work with the robots: some would prefer to use only images, others would be interested in advanced methods that allow to de-scribe difficult tasks with many objects."}, {"title": "Collaboration with experts in ASD", "content": "In the preparation phase, we will discuss basic practical questions related to structure teaching, e.g., which structured tasks types are cur-rently used in education (in Czech Republic and in world), are there any standard for shoe-box tasks in Czech schools (for example, products from ShoeboxTasks\u00ae) or are they typically designed and made by teachers, what are physical parameters of the boxes.\nWe also need to consult educational strategies and methodology specific to ASD, especially the safety recom-"}, {"title": "Conclusion", "content": "In this work, we introduced Image2PDDL, a framework that leverages VLM to automate the generation of PDDL problems. By utilizing both images and textual descrip-tions for initial and goal states, Image2PDDL bridges the gap between visual perception and symbolic plan-ning, allowing for a streamlined, accessible process of problem generation across diverse domains. Our evalua-tion across three domains-Blocksworld, Sliding-Tile Puz-zle, and Kitchen-demonstrated that Image2PDDL achieves high syntax correctness, reliably producing grammatically valid PDDL outputs across various task complexities and input types. While content correctness varied depending on domain difficulty and input modality, results indicated that the framework performs well with structured tasks and ben-efits from textual input in visually complex scenarios.\nImage2PDDL's promising performance suggests broader applications in AI planning, particularly for complex real-world tasks requiring visual and symbolic integration. Fu-ture work could enhance the framework's capacity to man-age intricate object relationships and multi-step transforma-tions in dynamic environments, such as the autistic shoe-box task. Overall, this framework lays the groundwork for more accessible and scalable AI planning solutions, creat-ing new opportunities for integrating vision and language models into automated planning domains.\nWe discussed an apparent application case for Im-age2PDDL in robot-assisted teaching of students with ASD. Some possible research avenues include\n\u2022 automatically generating 3D models of shoebox tasks (to provide the visual feedback or to practice the task in a simulation).\n\u2022 using vision language model to generate verbal instruc-tions.\n\u2022 automating the entire process: use the robot to prepare the shoebox task or the entire work session.\n\u2022 using the system as tool for education of robot program-ming for students with ASD."}]}