{"title": "Boosting 3D Object Generation through PBR Materials", "authors": ["YITONG WANG", "XUDONG XU", "LI MA", "HAORAN WANG", "BO DAI"], "abstract": "Automatic 3D content creation has gained increasing attention recently, due to its potential in various applications such as video games, film industry, and AR/VR. Recent advancements in diffusion models and multimodal models have notably improved the quality and efficiency of 3D object generation given a single RGB image. However, 3D objects generated even by state-of-the-art methods are still unsatisfactory compared to human-created assets. Considering only textures instead of materials makes these methods encounter challenges in photo-realistic rendering, relighting, and flexible appearance editing. And they also suffer from severe misalignment between geometry and high-frequency texture details. In this work, we propose a novel approach to boost the quality of generated 3D objects from the perspective of Physics-Based Rendering (PBR) materials. By analyzing the components of PBR materials, we choose to consider albedo, roughness, metalness, and bump maps. For albedo and bump maps, we leverage Stable Diffusion fine-tuned on synthetic data to extract these values, with novel usages of these fine-tuned models to obtain 3D consistent albedo UV and bump UV for generated objects. In terms of roughness and metalness maps, we adopt a semi-automatic process to provide room for interactive adjustment, which we believe is more practical. Extensive experiments demonstrate that our model is generally beneficial for various state-of-the-art generation methods, significantly boosting the quality and realism of their generated 3D objects, with natural relighting effects and substantially improved geometry.", "sections": [{"title": "1 INTRODUCTION", "content": "3D content generation has gathered widespread interest in recent years for its vast potential in diverse applications, such as video games, filmmaking, and AR/VR. The advancement of diffusion models [Ho et al. 2020; Rombach et al. 2022a] has precipitated a paradigm shift in 3D content generation, significantly enhancing the realism of produced 3D objects. Owing to its unique fast feedforward pipeline and controllability, 3D from a single image gradually becomes the main pipeline in 3D content generation. Given an RGB image and a conditional text prompt, these methods [Hong et al. 2024; Liu et al. 2024a; Long et al. 2024; Wang et al. 2024b; Xu et al. 2024a] can project the input image to multi-view images and then fuse them into a 3D mesh with compelling textures and great 3D consistency.\nNotwithstanding the remarkable progress, existing methods still suffer from two fundamental drawbacks. For one, all of them can only generate 3D objects with textures but ignore more crucial materials, which are imperative for rendering under various lighting conditions. The absence of materials not only compromises the photorealism of 3D objects but also constrains their utility in a wider range of downstream applications. For another, the generated 3D objects often exhibit a misalignment between their geometry and high-frequency details of corresponding textures, resulting in a modest geometry quality that falls short of expectations. Even when endowed with plausible materials, these 3D objects tend to exhibit unrealistic artifacts under novel illuminations.\nIn this paper, we propose a novel approach to boost 3D object generation in the perspective of Physics-Based Rendering (PBR) materials. It works in a plug-and-play manner that is compatible with any single image-to-3D generation method. By considering PBR materials, objects generated with our approach can be more photorealistic and relightable thanks to the involvement of concepts like albedo, roughness, and metalness. Besides, the misalignment of high-frequency geometry details can also be substantially improved since PBR materials cover bump maps that reflect intricate texture-aligned details.\nIn our proposed approach, different components of PBR materials are handled in different ways to enhance their practical value, where albedo maps are predicted from the input RGB image, bump maps are iteratively optimized given a 3D mesh and its albedo UV, and roughness and metalness maps are determined in a semi-automatic way, to leave space for interactive adjustment as desired by practical workflows. Specifically, to predict albedo maps and optimize bump maps, we fine-tune Stable Diffusion with synthetic data to obtain image-to-albedo and image-to-normal diffusion models, motivated"}, {"title": "2 RELATED WORK", "content": "Text-to-3D generation with 2D diffusion models. With the advancement of text-to-image diffusion models, a line of research work seeks to exploit strong priors from 2D diffusion models for 3D content generation. Pioneers, DreamFusion [Poole et al. 2023] and SJC [Wang et al. 2023a], propose Score Distillation Sampling (SDS) (also known as Score Jacobian Chaining) that significantly facilitates the development of this area. Following the SDS-based 2D-lifting method, recent works have achieved promising results through improved score distillation loss [Wang et al. 2023b], texture refinement [Chen et al. 2024; Liang et al. 2024], multi-view diffusion model [Shi et al. 2024], or more advance 3D representations [Li et al. 2024a]. However, these methods solely generate 3D objects with RGB textures, devoid of materials, thereby failing to satisfy the requirements of real-world applications. Fantasia3D [Chen et al. 2023] tried to model PBR materials by disentangling geometry and texture but lacked the necessary constraints on PBR materials and illuminations to achieve such disentanglement. RichDreamer [Qiu et al. 2024] and UniDream [Liu et al. 2023b] both employ diffusion models trained on the albedo domain for PBR material modeling, but unfortunately,"}, {"title": "3 PRELIMINARIES", "content": "3.1 Stable Diffusion\nStable Diffusion is a latent diffusion model [Rombach et al. 2022b] which has achieved state-of-the-art performance in text-to-image generation. It performs the diffusion process in latent space to enable the generation of high-resolution images. A variational autoencoder (VAE) is used to decode and encode the image to and from the latent space. The crux of the diffusion process is a U-Net that predicts the noise \u00ea from a noisy latent $z_t$, given a text embedding s and the timestep t: $\\hat{\\epsilon} = g(z_t; s, t)$, where g represents the function modeled by the U-Net. By iteratively removing the noise from an initial random noise, a clean latent $z_0$ is generated, which can then be decoded into the resulting image.\nExisting works have demonstrated that pre-trained Stable Diffusion can serve as a vision foundation model. After fine-tuning, they can be adapted for various down-streaming vision tasks, such as relighting [Kocsis et al. 2024a], human reenactment [Hu et al. 2024], image editing [Huang et al. 2024], and depth estimation [Ke et al. 2024]. In this work, we exploit pre-trained stable diffusion as a prior model for predicting albedo and normal maps from a single image.\n3.2 3D Reconstruction from a Single Image\nThe reconstruction-based methods for recovering 3D meshes from single images typically involve a two-stage pipeline, comprising the generation of multi-view images and the subsequent reconstruction of 3D geometry from these synthesized views. Given a single image"}, {"title": "4 METHODOLOGY", "content": "This section elaborates on our plug-and-play method that boosts single image-to-3D generation frameworks through PBR materials. The overview of our whole pipeline is illustrated in Figure 2. Section 4.1 first provides details on the fine-tuning of Stable Diffusion to accurately estimate the albedo and normal map from a given RGB image. Following the estimation of the albedo, we elaborate in Section 4.2 on how to leverage the Vision-Language Models (VLMs) to assign plausible values for metalness and roughness terms, with the guidance of 3D semantic masks. Finally, we propose the iterative normal refinement in Section 4.3 where the derived normal maps in Section 4.1 are treated as the pseudo-ground truth.\n4.1 Albedo and Normal Estimation\nIn our pipeline, two image-to-image translation modules are employed to predict the albedo and normal map, respectively, from a single input image. However, estimating albedo or normal maps from a single image is a highly ill-posed problem due to the lack of lighting or geometry information. Therefore, a strong prior is essential to recover plausible albedo and normal maps from a single image. Inspired by existing work on monocular depth estimation [Ke et al. 2024], we exploit the data-driving prior inside the Stable Diffusion [Rombach et al. 2022b] to achieve zero-shot albedo and normal map estimation. We show that by slightly modifying the U-Net structure and fine-tuning the pre-trained stable diffusion model on the synthetic dataset, we can obtain an image-to-image translation model that generalizes well to unseen in-the-wild data. It is noteworthy that such an image-to-image translation paradigm leads to high-quality albedo maps without clear highlights or shadows and intricate normals with fine details.\nTo this end, we initially encode the input single image using the VAE encoder $\\varepsilon$ into a latent code $z_i$. Then, we concatenate the input latent with the noisy latent and feed the resulting composite latent code into the U-Net of Stable Diffusion:\n$\\hat{\\epsilon}_{task} = g_{task}(z_t || z_i; s_{\\emptyset}, t), task \\in \\{normal, albedo\\}$,\nwhere || is the concatenation operator, and $s_{\\emptyset}$ indicates an empty text embedding. Note that the U-Net g is originally designed to take in the noisy latent only. Therefore, we duplicate the number of input"}, {"title": "4.2 PBR Material Generation", "content": "To obtain a complete albedo UV, a straightforward solution is to utilize our fine-tuned image-to-albedo diffusion model to derive multi-view albedo maps from generated multi-view images $I_{1:N}$ as described in Equation (1). However, we empirically find such a naive approach results in inconsistent albedo maps. Instead, we first leverage the diffusion model to convert the given single image to the albedo map following Equation (3), and then employ Equation (1) to synthesize multi-view albedo maps conditioned on this derived albedo. The multi-view albedo map can be fused to a 3D mesh M and an albedo UV A via Equation (2).\nWhile generating the metalness and roughness maps, we conform to the inherent property of PBR materials, i.e., surface areas with similar semantic characteristics tend to exhibit consistent values. Specifically, we project the reconstructed 3D mesh from 6 orthographic views and obtain 6 orthographic albedo maps, which are segmented into different parts via the Segment-Anything-Model [Kirillov et al. 2023]. Through voting strategy in the overlapping regions, such six segmentation results can be seamlessly integrated into a 3D semantic mask, as illustrated in Figure 2. Thereafter, we feed the given image into Gemini [Team et al. 2023], one of the powerful Vision-Language models, to get the recommended values of metalness and roughness terms associated with different semantic parts. Equipped with this 3D mask, we can easily extend these values to the entire 3D object, thereby generating comprehensive metalness and roughness UVs. Moreover, it's noteworthy that the values of such two terms are typically adjusted by experienced 3D artists in practical 3D content creation workflows.\n4.3 Iterative Normal Refinement\nUnfortunately, the normal map of reconstructed 3D meshes contains too many flaws, leading to poor relighting results as shown in Figure 4. To overcome this challenge, we propose iterative normal refinement by using the aforementioned normal estimation diffusion model.\nWe draw inspiration from the texture refinement presented in DreamGaussian [Tang et al. 2024] and propose a refinement strategy involving optimizing a bump map, which combines with the original flawed normal $n_o$ to produce a refined normal map $n_f(\\theta)$. Specifically, an MLP $\\Gamma$ parameterized as $\\theta$ is utilized to predict the bump map $n_{\\Gamma}(\\theta)$. For any point $p \\in \\mathbb{R}^3$ on the surface of a 3D mesh M, we apply the hash-grid positional encoding $\\beta(\\cdot)$ on point p and then obtain the bump map and refined normal map via:\n$\\eta_{\\Gamma}(\\theta) = \\Gamma(\\beta(p); \\theta), n_f(\\theta) = n_o \\oplus n_{\\Gamma}(\\theta)$,"}, {"title": "5 EXPERIMENTS", "content": "5.1 Implementation Details\n5.1.1 The fine-tuning of Stable Diffusion. The Stable Diffusion-V2.1-base model is selected as our base model for fine-tuning. During the fine-tuning, we freeze the VAE and only fine-tune the U-Net using the standard denoising diffusion objective: $L_{task} = ||\\epsilon - \\hat{\\epsilon}_{task}||^2$, where $\\epsilon \\sim N(0, I)$ is a random noise map. As mentioned in Section 4.1, the input channels of the first convolution layer inside U-Net are duplicated to empower the desirable image-to-image translation ability. During the training, we zero-initialize the weight for the duplicated channels in the input layers and train our model on the HyperSim [Roberts et al. 2021], a synthetic indoor-scene dataset containing ground truth albedo and normal map. The fine-tuning on albedo and normal maps takes 16 hours and 22 hours respectively on a single NVIDIA Tesla A100 GPU.\nFor the albedo estimation, we observe degenerate results on object images owing to the color space gap between indoor-level and object-level data. To address this issue, we further fine-tune the albedo estimation model on the Objaverse [Deitke et al. 2023] dataset to align the color space of the model output to the object-level data, which roughly requires 28 hours of training on 4 A100 GPUs. Importantly, we empirically find direct fine-tuning on the Objaverse dataset is insufficient to remove strong lighting effects, such as highlights and shadows, from the input RGB images. In contrast, the fine-tuned image-to-normal diffusion model demonstrates superior performance on object-level data, successfully recovering intricate normal maps from the object images.\n5.1.2 Methods for boosting. We select four different reconstruction-based methods as the base models to boost: Wonder3D [Long et al. 2024] generates 6-view images and normal maps that are fused to a textured 3D mesh via NeuS [Wang et al. 2021]; TripoSR [Tochilkin et al. 2024] builds upon LRM structure but affords"}, {"title": "5.2 Normal boosting", "content": "In Figure 3, we present a visual comparison of normal boosting results for four distinct base models, accompanied by the input image and our estimated albedo map at the top. As illustrated, the base methods CRM and Wonder3D yield unsatisfactory object geometries, plagued by numerous flaws, whereas InstantMesh tends to reconstruct 3D meshes that lack essential geometry details. After our boosting, the resulting normal maps exhibit a significant reduction in geometry flaws and effectively capture more intricate details aligning with the corresponding images. It's noteworthy that TripoSR is prone to predict more artificial geometry details but ours can successfully avoid such a dilemma. Figure 4 provides further validation of our normal boosting results through relighting experiments, wherein it is evident that the generated PBR materials yield satisfactory relighting outcomes only when combined with the boosted normal maps. Furthermore, as shown in Figure 5, our method is also capable of boosting the normal maps generated by DreamCraft3D [Sun et al. 2024], a prominent optimization-based approach for synthesizing 3D object meshes from single images.\n5.3 Qualitative Comparison to Baselines\nWe compare our albedo estimation module with two strong baselines [Sang and Chandraker 2020; Wang et al. 2023c] aiming to recover the albedo map from the given single image. Unlike baseline methods, our method is able to derive albedo maps that effectively eliminate strong lighting effects as shown in Figure 6. We also try to compare our PBR material results with baselines enabling PBR material generation. In the absence of prior work focused on material generation for reconstruction-based image-to-3D methods, we opt to compare our material generation results with those of two representative text-to-3D approaches, Fantasia3D [Chen et al. 2023] and RichDreamer [Qiu et al. 2024]. Figure 9 shows that Fantasia3D fails to exclude highlights or shadows from the obtained albedo maps, whereas RichDreamer incorrectly assigns geometry details to the variations in the metalness and roughness maps, leading to unrealistic relighting results under various novel illuminations. Thanks to our image-to-albedo diffusion model and 3D semantic masks, we can generate high-quality PBR materials that more accurately conform to the requirements of real-world 3D content creation workflows. Moreover, our method also supports flexible material editing as shown in Figure 10.\n5.4 User Study\nWe conduct an experiment involving 20 diverse 3D objects across 4 base models, totaling 80 pairwise comparisons, to evaluate the perceptual quality enhancement of our boosted results relative to prior work. For each comparison, participants will view the input image, original and boosted normal maps, and relighting outcomes"}, {"title": "5.5 Usability Study", "content": "We assess the effectiveness of our method through a usability study with two professional artists and eight non-expert Internet users unfamiliar with 3D creation. Participants can generate 3D objects using a base Image-to-3D model and enhance them with our tool. The general agreement among participants is that our tool substantially improves 3D object quality. However, one artist notes that the generated objects are incompatible with their required format, as the base models only produce triangle meshes. Additionally, seven participants express dissatisfaction with the lengthy generation process, which takes 25 minutes for base models and 5 minutes for our boosting. We anticipate future research to develop more efficient image-to-3D generative models. Regarding the boosting process, the"}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "6.1 Conclusion\nThis paper presents a novel framework for enhancing existing single image-to-3D generation methods with high-fidelity PBR materials. Our approach involves two key components. Firstly, we adapt the Stable Diffusion model to infer albedo maps from single images and leverage powerful VLMs to derive plausible values for metalness and roughness terms. Subsequently, we augment the original texture maps with relightable PBR materials, thereby enabling realistic relighting under novel illumination conditions. Secondly, we design an iterative normal refinement module to enhance the original flawed normal maps with learnable bump maps. As a result, our refined normal maps exhibit intricate geometry details and improved alignment with the corresponding RGB images. We believe that our boosting scheme has the potential to significantly accelerate the development of single image-to-3D generation techniques.\n6.2 Limitations\nDespite the superior capability, our boosting model still exhibits certain limitations. The image-to-albedo diffusion model introduces"}]}