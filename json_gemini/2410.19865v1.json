{"title": "Evaluating Deep Learning Approaches for Predictions in Unmonitored Basins with Continental-scale Stream Temperature Models", "authors": ["Jared D. Willard", "Fabio Ciulla", "Helen Weierbach", "Vipin Kumar", "Charuleka Varadharajan"], "abstract": "The prediction of streamflows and other environmental variables in unmonitored basins is a grand challenge in hydrology. Recent machine learning (ML) models can harness vast datasets for accurate predictions at large spatial scales. However, there are open questions regarding model design and data needed for inputs and training to improve performance. This study explores these questions while demonstrating the ability of deep learning models to make accurate stream temperature predictions in unmonitored basins across the conterminous United States. First, we compare top-down models that utilize data from a large number of basins with bottom-up methods that transfer ML models built on local sites, reflecting traditional regionalization techniques. We also evaluate an intermediary grouped modeling approach that categorizes sites based on regional co-location or similarity of catchment characteristics. Second, we evaluate trade-offs between model complexity, prediction accuracy, and applicability for more target locations by systematically removing inputs. We then examine model performance when additional training data becomes available due to reductions in input requirements. Our results suggest that top-down models significantly outperform bottom-up and grouped models. Moreover, it is possible to get acceptable accuracy by reducing both dynamic and static inputs enabling predictions for more sites with lower model complexity and computational needs. From detailed error analysis, we determined that the models are more accurate for sites primarily controlled by air temperatures compared to locations impacted by groundwater and dams. By addressing these questions, this research offers a comprehensive perspective on optimizing ML model design for accurate predictions in unmonitored regions.", "sections": [{"title": "Plain Language Summary", "content": "Predictions of river flows and water quality are needed globally to address increasing demands from population growth, land use change and projected impacts of climate change. However, many watersheds lack sufficient observations needed to build models, making accurate predictions difficult. Recent advances in machine learning (ML) have shown that more accurate predictions for environmental variables can be made by using large, diverse datasets. In this work, we use stream temperature as a case study to demonstrate the ability of ML models to make large-scale predictions in regions with no (or sparse) monitoring data. We found that \"top-down\" models trained on large amounts of data from different locations were the most accurate, compared to those built from smaller or more localized regional datasets. In particular these top-down models outperform traditional \"bottom-up\" approach where models are built at a monitored sites and transferred to similar sites. Simpler models using only widely available meteorological data also performed well and can be used broadly at any location. However, predictions were more difficult for sites with dam or groundwater influence. Overall, our findings suggest that deep learning models using large-scale datasets can be effectively used to make widespread, accurate predictions of stream temperatures in unmonitored regions."}, {"title": "1 Introduction", "content": "Accurate, spatially-distributed predictions of stream hydrology and water quality can provide essential information for sustainable water resources management (X. Chen et al., 2021; Dwivedi et al., 2022). Such predictions are needed to understand, plan for, and respond to the effects of anthropogenic stressors, global warming, and extreme events that are occurring with increasing frequency and intensity (Best, 2019; Van Vliet et al., 2023). The prediction of river flows and other critical environmental variables in unmonitored basins has been a grand challenge in hydrology for decades (Sivapalan et al., 2003; Hrachowitz, Savenije, Bl\u00f6schl, et al., 2013; Guo et al., 2021; Xiong et al., 2022b; Rahmani, Shen, et al., 2021). This challenge exists because only a small number of stream reaches around the world are monitored for important hydrological variables (E. K. Read et al., 2017; Seibert & Beven, 2009), and has persisted despite numerous community efforts to address the problem for several reasons. These include the complexity and variability of hydrological processes across catchments with diverse characteristics (Kannan et al., 2019), data sparsity (Schuol & Abbaspour, 2006; Ang et al., 2022; Kapangaziwiri et al., 2009), an emphasis on modeling approaches based on transfer of knowledge from a few intensively monitored locations (Parajka et al., 2005; Y. Zhang & Chiew, 2009a), and computational limitations inhibiting the ability to run process-rich models at large scales (Azmi et al., 2021; Vema & Sudheer, 2020).\nTraditional approaches for prediction in ungauged basins (PUBs) involve the concept of \"regionalization\", wherein localized, process-based or statistical models are built at data-rich locations, and their parameters extrapolated to similar ungauged regions (Bl\u00f6schl & Sivapalan, 1995; He et al., 2011; Merz & Bl\u00f6schl, 2004). In these \"bottom-up\" approaches, different methods have been used for regionalization that all use explicit measures of similarity such as those based on biophysical catchment characteristics, spatial proximity (i.e., using physical distance) or hydrological signatures (Guo et al., 2020) to decide which model or parameter set is best to transfer. Some studies have also attempted \"grouped\" approaches where models are trained or calibrated on a collection of similar or co-located sites prior to regionalization (Rao & Srinivas, 2006a, 2006b; Hu et al., 2024; Bock et al., 2016). Recently, there have been significant breakthroughs in the PUBs challenge, with machine learning (ML) models showing remarkable promise at making accurate, large-scale predictions at unmonitored sites for various hydrological variables such as stream flows (Choi et al., 2022; Feng et al., 2021; Koch & Schneider, 2022; Kratzert, Klotz, Herrnegger, et al., 2019a) including floods (Nearing et al., 2024), evapotranspiration (Z. Chen et al., 2020), stream and lake temperatures (Rahmani, Shen, et al., 2021; Tayal et al., 2022), and other stream water quality variables (Kalin et al., 2010; Xiong et al., 2022a; Zhi et al., 2021). These \"top-down\" models leverage long-term data from hundreds of monitored sites, often from diverse regions (Kratzert, Klotz, Herrnegger, et al., 2019a; Fang et al., 2022a; Rahmani, Shen, et al., 2021), and have even been demonstrated to outperform process models calibrated for the catchments of comparison (Kratzert, Klotz, Herrnegger, et al., 2019b; S. Chen et al., 2023; Yin et al., 2021). A majority of ML studies utilize the long short-term memory (LSTM) neural network (Hochreiter & Schmidhuber, 1997) due to its ability to represent system memory, with a set of inputs that comprise dynamic variables (e.g., meteorology) and a small number of biophysical characteristics that are treated as static variables to provide information for the model to distinguish between different catchment types (J. D. Willard et al., 2024).\nAn emerging debate in hydrology is whether advances in ML should effect a fundamental change in modeling philosophy shifting from a \"bottom-up\" to a \"top-down\" approach for applications that seek to maximize predictive accuracy (Kratzert et al., 2024). Here, we use the same definitions of these modeling approaches (specified above) as in Kratzert et al. (2024) and not in the process modeling context as specified in Hrachowitz, Savenije, Bl\u00f6schl, et al. (2013), who referred to \"top-down\" approaches as spatially lumped, conceptual bucket models versus \"bottom-up\" as spatially distributed, continuum, physically based models. Recent studies have shown that deep learning models in general benefit from larger, more diverse training datasets in monitored (especially gauged) scenarios, where training and testing is done on the same set of sites by splitting the data into separate time periods. For instance, Fang et al. (2022b) found that deep learning models for streamflow and soil moisture in gauged basins achieve better performance when trained on diverse datasets from multiple regions compared to training on a homogeneous dataset from a single region, even when the latter is more hydrologically similar to the test conditions and the datasets are of equal size. The results from Kratzert et al. (2024) also suggest that LSTM models, at least for stream flow prediction in the United States, should never be trained on a single basin and using all available data should be the default approach. Both studies have demonstrated that training on smaller groupings of sites both by region and attribute similarity (determined using a k-means clustering) results in significantly worse performance than the top-down model, implying that deep learning models benefit from seeing more diverse, large datasets.\nGiven recent advancements in ML for hydrological modeling, an important question that remains unresolved for unmonitored scenarios is how \"bottom-up\" approaches based on regionalization of site-specific models that rely on calculated measures of similarity or transferability compare with \"top-down\" deep learning (DL) models that presumably learn to generalize by seeing diversity within large-sample data from monitored sites. Moreover, there are other open questions to address regarding the implementation of DL models for predictions of dynamic hydrologic variables in unmonitored basins (J. D. Willard et al., 2024). First, the selection of input variables is typically done by domain experts without a rigorous process for feature selection, unlike in other applications of ML time series regression where feature selection is standard (Meisenbacher et al., 2022; Li et al., 2017). For example, there are hundreds of catchment characteristics that are now available as large-scale data products (Falcone, 2011; Hill et al., 2016; Linke et al., 2019), yet only a handful (typically 20-30) are used in hydrologic ML models without justification for feature selection. Systematically evaluating these features and their representation could significantly improve the accuracy and applicability of predictive models by using techniques that address redundancy and relevance, such as those highlighted in Ciulla and Varadharajan (2024). Second, imposing additional data requirements may limit the applicability of the model if those inputs are not available. For example, while some data such as meteorology are widely available at most locations particularly in the form of gridded data products, models that require inputs such as catchment characteristics that are not broadly available or co-located hydrologic variables such as stream flows and temperatures will have substantially lower number of sites available for training. Finally, most studies only use accuracy as the sole metric of model performance; however trade-offs between increasing complexity and data requirements versus accuracy need to be evaluated for optimal model choice (Varadharajan, Appling, et al., 2022).\nOur objectives for this study are to address these knowledge gaps using a set of ML-based model experiments for predictions in unmonitored sites. A primary goal is to determine the best modeling approach by investigating whether models should be trained on large, diverse datasets covering wide geographic areas (top-down approach), focus on hydrologically-similar or co-located subsets (grouped approach), or use a bottom-up approach where site-specific models built for data-rich locations are regionalized to unmonitored locations using transfer learning. Another key objective is to identify which input variables are most crucial for accurate predictions. We aim to understand whether models need a broad range of catchment characteristics, or if they can achieve high accuracy with a more streamlined set of inputs. We also explore how the availability of different types of input data (e.g., meteorology, discharge, catchment characteristics) affects model performance. This involves exploring the trade-offs between requiring comprehensive input data versus utilizing a larger training dataset with fewer input constraints.\nIn this work, we have built several models to predict daily stream temperatures in unmonitored sites as a case study to demonstrate these objectives. Water temperature is an important variable in stream ecosystems, influencing water chemistry and aquatic life (Brett, 1970). It is typically a monitored and regulated parameter for water resource management. However, observations are relatively sparse and an order of magnitude less available in comparison to stream flows (Normand, 2021; U.S. Geological Survey, 2016), making it a good candidate for these research objectives. Factors influencing stream temperatures include climate (particularly air temperatures and solar radiation), stream flows, snowmelt and surface runoff, surface-groundwater interactions, riparian shading, and human activities (e.g., discharge from thermal power plants, dam releases) (Kedra & Wiejaczka, 2018; X. Zhang et al., 2020; Kelleher et al., 2012; Borman & Larson, 2003). A myriad of anthropogenic stressors\u2014ranging from urbanization (K. A. Somers et al., 2013) and dam installations (Risley et al., 2010) to agriculture (Essaid & Caldwell, 2017) are reshaping established stream thermal regimes. These changes will be exacerbated due to the consequences of climate change including rising air temperatures, changing stream flow patterns, and an increase in intensity and frequency of extreme events such as heatwaves and droughts (Ficklin et al., 2013; van Vliet et al., 2013). The ability to accurately predict stream temperatures is vital for the management of river ecosystem services including fisheries, hydroelectric power generation as well as management of aquatic habitats (Wilby et al., 2010; Van Vliet et al., 2016; Hansen et al., 2015).\nUltimately, our goal is to enable accurate predictions of stream temperatures in unmonitored sites using broadly available input data, ensuring the models' applicability across diverse regions and conditions. Hence, our study uses data from both pristine and human-impacted sites, in contrast to a vast majority of hydrological machine learning models that focus only on pristine sites (J. D. Willard et al., 2024). Here, we aim to predict the entire stream temperature time series at a target location, which is distinct from making temporal predictions at monitored sites or from temporal forecasting applications, where past observations can be used to build a predictive model for future. By evaluating the best approaches for building accurate models and understanding the loss of accuracy when certain data are unavailable, we aim to provide insight into the design of practical ML models for predictions in unmonitored regions and for eventual use in water resource management. Additionally, we seek to identify the conditions under which these models are applicable to use given acceptable error thresholds. While this study focuses on stream temperature modeling, our methods and findings are broadly applicable for other hydrologic variables.\nTo our knowledge, this study is the first that compares a bottom-up regionalization approach to both top-down and grouped ML models. We use novel techniques for this comparison, namely a meta transfer learning framework (J. D. Willard et al., 2021) for bottom-up modeling as well as a network-based method to cluster sites by attribute similarity, which has been demonstrated to outperform unsupervised k-means and hierarchical clustering (Ciulla & Varadharajan, 2024). The network-based similarity method also serves to reduce model complexity by representing an extensive set of 274 catchment attributes with a smaller set of 25 highly interpretable attribute categories that have minimal redundancy. We are also the first to examine trade-offs in data availability to enhance model generalizability. For this, we compare input data requirements that are more comprehensive resulting in fewer sites that have the necessary data versus requiring less inputs whose data are broadly available for more target locations. Overall, our findings complement other large-sample deep learning studies that advocate for a shift in modeling philosophy towards large-scale data-driven approaches, moving beyond the traditional method of regionalization of localized models, for the purpose of making the most accurate spatiotemporal predictions in unmonitored basins."}, {"title": "2 Model Experiment Overview", "content": "We conduct three experiments to independently test the effects of different modeling choices for prediction of daily stream temperatures. For this, we utilize a large dataset comprising over 4 million stream temperature observations in the continental United States (CONUS) at 1362 pristine and human-impacted sites (Section 3.1). We use the LSTM architecture as the primary model for all experiments both because it is the most popular ML model used for hydrological time series prediction in unmonitored sites (J. D. Willard et al., 2024), and also to maintain a fair comparison of models across the different experiments.\n\u2022 Experiment 1: Comparing top-down, bottom-up, and grouped approaches\nThis experiment compares four different approaches for modeling PUBs, including top-down ML models, along with ML analogs of traditional bottom-up and grouped approaches used in hydrological modeling (Figure 1). For the top-down model, we follow the approach used in prior studies (e.g., (Kratzert, Klotz, Herrnegger, et al., 2019a; Y.-H. Wang et al., 2022; Nogueira Filho et al., 2022; Zhi et al., 2021) using observations from diverse sites across the conterminous United States as training data. For the grouped models, we use two approaches. The first uses similarity based on spatial proximity, with groups defined as the 18 regions within the United States Geological Surveys Hydrologic Unit Codes (HUCs) categorization, which has been used in previous studies (Kratzert et al., 2024; Y.-H. Wang et al., 2022). The second is based on similarity of catchment attributes as determined by a network-based approach that has been demonstrated to outperform other unsupervised clustering methods for multiple metrics (Ciulla & Varadharajan, 2024).\nLastly, the \"bottom-up\" approach of transferring models built on data-rich sites for predictions in unmonitored sites uses a meta transfer learning (MTL) approach, which is analogous to calibrating process-based model parameters on a \"donor\" site and strategically transferring those parameters to a model for a target location (J. D. Willard et al., 2021). The MTL is instead a data-driven approach for selecting models to be transferred, where the base models can be either process-based, statistical or ML-based. The transfer is optimized using past performance metrics transferring models between sites. The MTL uses a classical ML model to predict the error of each pairwise combination of training and test sites with inputs (referred to as meta-features) such as base model structures, catchment attributes, or input data statistics, to determine the best model for a given unmonitored site. We further describe how this model is used to select sites in Section 3.2.1. This practice of transfering parameters from a donor site to a target location has been the primary approach in PUBs modeling for decades (B\u00e1rdossy, 2007; Patil & Stieglitz, 2012; Zelelew & Alfredsen, 2014; Y. Zhang & Chiew, 2009b), and the well-trodden question of \"How do we select the most ideal donor catchment?\" for regionalization mirrors our ML question of \"How do we select the ideal model for transfer learning?\".\n\u2022 Experiment 2: Examining model generalizability by reducing input data requirements\nA key bottleneck to modeling stream temperatures in unmonitored sites is having sufficient co-located input data at the target locations of interest (McGrath et al., 2017). This experiment explores the trade-offs between model performance, data availability, and applicability when certain inputs are limited. By systematically reducing input requirements such as stream flow and catchment attributes, it examines how this affects the number of training sites and the overall model performance, highlighting the potential of minimalist models that rely only on universally available meteorological and location data. Here, we build models based on the four different data availability groups (Figure 2a). For each of these data availability groups, we built two versions of models to examine the effect of additional sites with stream temperature observations becoming available for training by dropping input data requirements (Figure 2b).\n\u2022 Experiment 3: Assessing the appropriate set of catchment attributes to include in top-down models and associated trade-offs with model complexity\nThis experiment addresses the gap in comparisons of different sets of catchment characteristics used as inputs for modeling. With large geospatial datasets that have information on hundreds of attributes for thousands of catchments, it is currently possible to include a large number of these attributes as inputs to a deep learning model. However, doing so introduces a significant degree of complexity across the ML modeling pipeline including the neural network model and the optimization process, as well as increased computational time. Also, many catchment characteristics have overlapping information content (Ciulla & Varadharajan, 2024), and redundant inputs to deep learning models can potentially reduce model performance (Peng et al., 2005; L. Wang et al., 2021) and interpretability (Ismail et al., 2021; Pan et al., 2023).\nIn this experiment, we test three different sets of attributes (Figure 3):\nAll available: This is a comprehensive set of 274 non-categorical GAGES-II attributes that describe the physical, climatic, and land-use characteristics of catchments (Falcone, 2011), and is used as the default inputs for the top-own and grouped models in Experiment 1. While this set maximizes the amount of catchment information provided to the model, it also introduces a higher degree of complexity and redundancy. This approach allows us to test whether increasing the breadth of input data leads to improvements in model performance or if the added complexity outweighs the benefits.\nExpert-selected: This set includes a small, targeted group of 21 attributes used in a prior work using LSTM for stream temperature prediction in unmonitored basins that was determined based on insights from hydrological experts (Rahmani, Shen, et al., 2021). This includes variables that typically influence stream temperatures such as catchment drainage area, mean annual precipitation, elevation, straightline distance to nearest dam, and percent forest cover.\nAggregated z-scores: This is a set of catchment attribute categories derived from a network-based method to address redundancy in the attribute information and improve model efficiency. (Ciulla & Varadharajan, 2024). The inputs used are the averaged z-score values of all the attributes for each category. This approach provides an objective means to reduce the dimensionality of any dataset of catchment attributes, balancing model interpretability and complexity, and potentially improving model performance by eliminating unnecessary or highly correlated attributes."}, {"title": "3 Materials and Methods", "content": "Here we describe the ML model frameworks and architectures used for stream temperature prediction, as well as any preprocessing, hyperparameter tuning, feature selection, and feature extraction that was done. Then, we describe the three experiment procedures in more detail. Lastly, we cover how the models are evaluated and how feature importance is calculated using the permutation feature importance technique."}, {"title": "3.2.1 Model descriptions", "content": "The LSTM is a type of recurrent neural network that includes dedicated memory that can store information over long time periods (Hochreiter & Schmidhuber, 1997). This memory function is analogous to a system state vector in dynamical systems and process-based modeling, making it a popular architecture for modeling watershed processes (Kratzert, Klotz, Shalev, et al., 2019; J. D. Willard et al., 2024). Compared to other variants of recurrent neural networks, LSTMS are not as vulnerable to the problem of exploding and vanishing gradients during training. For further details on the LSTM recurrent neural network architecture see Hochreiter and Schmidhuber (1997), and we also include a brief description of the architecture in the supplementary material (Section S1). We used the Pytorch (Paszke et al., 2019) LSTM class for implementation, and all models were trained on the Perlmutter HPC system within the National Energy Research Scientific Computing Center (NERSC) using Nvidia A100 GPUs."}, {"title": "3.2.1.2 Meta Transfer Learning", "content": "The MTL framework used in the Experiment 1 for the \"bottom-up\" approach uses a metamodel in addition to the LSTM models previously described. While the LSTM models predict stream temperature itself, the metamodel predicts the expected performance an LSTM model built on one site and transferred to predict in another. For the metamodel we use an extreme gradient boosting (XGBoost) regression model architecture due to its prior success in an MTL model J. D. Willard et al. (2021) and ease of implementation. A list of the 300 total candidate meta-features can be seen in supplementary Table S1, and we perform recursive feature elimination with cross validation (RFECV) to narrow it down to 234 listed in Table S2. We use the default XGBoost hyperparameters for both RFECV and also the final model. Here, we use the metamodel in the same manner as previous works on lake temperature and stream-flow prediction (J. D. Willard et al., 2021; Ghosh et al., 2022) which proceeds as follows:\n1. Build and train five LSTM models for each of the well-monitored stream sites in the training dataset.\n2. For each site in the training dataset, use all models built on other training sites to predict daily stream temperatures and evaluate prediction accuracy. This yields $N * (N - 1)$ prediction performance values where N is the number of training sites.\n3. Train the meta-learning XGBoost regression model to predict the collected model RMSE performance values from (2) based on the meta-features found using RFECV.\n4. Given an artificially unmonitored stream site, where data is only used for final evaluation, and its meta-features, use the meta-learning model to predict model performance of each model built on a training site. Use the models with the lowest predicted errors to model the target.\nGiven an unmonitored target site, we select the 10 sites that contain the source models with the lowest predicted error by the meta-model. Selecting more than one source model and combining them in an ensemble was found in Willard et al. (J. D. Willard et al., 2021) to significantly outperform the single source model transfer. These source models are combined in an ensemble and the final predictions are an average of the 10 models."}, {"title": "3.2.2 Experiment Details", "content": "As described above, we conducted three experiments with different inputs or models (Table 1) to assess their impact on the accuracy of stream temperature predictions in unmonitored locations. For this study, we split the 1362 sites as follows. The 782 sites with a total of at least five years (1825 sampling dates) of stream temperature observations were used for training. The remaining 580 sites that had between one and five years of data were treated as representative of \"unmonitored\" sites and used for testing in the final evaluation. The special case of Experiment 2 uses an extended dataset of 2999 sites, with 1346 sites having at least five years of data and used for training. The observations are not required to be continuous in time so there are many missing sampling dates in the majority of sites, but these are simply masked out of the optimization and evaluation processes (Sections 3.2.3, 3.2.5, S1), so it does not pose an issue with model training or evaluation.\nFor all three experiments the LSTM models were constructed using the same hyperparameters and training protocols (Section 3.2.3), only differing in the choice of inputs and the breadth of training data. Note that when the training data changes, the normalization (Section 3.1.3) of the training and testing data in each case will also change.\nExperiment 1: In this experiment comparing top-down, bottom-up and grouped modeling approaches, we use the same meteorological and stream flow inputs for all models (Table 1). However, only LSTM_conus, LSTM_regional, and LSTM_cluster use the 274 GAGES-II attributes as inputs. This is because the single site LSTM models used in the MTL have no use for inputs that are specific for each sites, since those values will be constant and non-informative. Instead, in the MTL approach, the attributes are used as meta-features eventually (Table S1) in the XGBoost regression model (Section 3.2.1.2). Also, for each of the grouped approaches, test sites are predicted solely by the model built on the region or cluster they belong to.\nExperiment 2: We built eight models for Experiment 2 that are compared with the top-down model from Experiment 1 (LSTM_conus). The first set of models (left column of Figure 2b) uses the same amount of data originally used in Experiment 1, and only changes the inputs provided (see \"Experiment 2 Default Training Data\" in Table 1). The second set of models (right column of Figure 2b) uses additional stream temperature observations that become available by reducing the set of inputs (see \"Experiment 2 Extended Training Data\" in Table 1), effectively increasing the size of the training data. The models built using only meteorology and location data (latitude, longitude, and elevation) as inputs are able to use stream temperature data from 2999 sites since the inputs are universally available for any location using gridded meteorology and topography datasets. This amounts to ~6.13 million training observations spanning 1346 sites (light grey circle in Figure 2) used as inputs for the extended set of models, in contrast to ~3.82 million available across 782 sites (dark blue filled circle in Figure 2) used in Experiment 1. The second data availability group (shown in orange in Figure 2) includes river discharge (streamflow) as an additional input. With this additional requirement the total available training data is reduced to ~4.65 million stream temperature observations spanning 1048 sites. The third data group includes GAGES-II attributes as a requirement but does not use river discharge, resulting in ~4.13 million observations over 833 sites. The final data availability group is the same as Experiment 1 (Section 3.1), where all default inputs are required including meteorology, river discharge, and GAGES-II attributes.\nExperiment 3: Experiment 3 compares the performance of the LSTM_conus model using two alternate sets of catchment attributes with the default set used in Experiment 1 (Table 1). The first alternative consists of 21 expert-chosen values including properties like dam storage density and number of dams in the watershed, drainage area, stream density, straightline distance to nearest dam, land use, topographic features, location, and meteorological statistics. The second alternative is a reduced set of attribute categories obtained using a network approach to minimize redundancies. In the network, the nodes are attributes, and the edge weights represent their similarity. The attributes are then clustered into different clusters using a community detection algorithm (Ciulla & Varadharajan, 2024). The network-based attribute aggregation produces highly interpretable categories. For example, attributes associated with urban development are classified into one category that we refer to as \"Developed Areas\", while attributes related to temperature are classified under a different \"Temperature\" category. By categorizing attributes, we average out their slight differences and redundancies, and can analyze their collective contribution. The values used as model inputs represent the averaged z-scores of all the attributes within a particular category. The models in Experiment 3 were constructed using the same hyperparameters and training protocols as in Experiment 1, and only differed in the choice of inputs."}, {"title": "3.2.3 Hyperparameter optimization", "content": "Hyperparameters for the LSTM were tuned using Bayesian optimization available through the hyperparameter sweep framework from the Weights and Biases platform (http://wandb.ai, Biewald et al. (2020)). As opposed to the standard grid search, which exhaustively searches all possible combinations, Bayesian optimization guides the search by learning from previous evaluations and is a more efficient way to find optimal parameters (Davies, 2023). The ranges for the hyperparameter sweeps as well as hyperparameters set to a single value (Table S5 in Supplementary Information) were based on experiments from previous water temperature modeling studies (J. D. Willard, 2023; J. S. Read et al., 2019; Jia et al., 2021)). The learning rate was set to 0.001, initial weights were set using the Xavier normal distribution (Glorot & Bengio, 2010), and the AdamW optimizer (Loshchilov & Hutter, 2017) was used with a mean squared error loss function. We chose a sequence length of 200 days, with a sliding window shift of 100 days at a time, which was also based on results from aforementioned previous studies. Predictions are only gathered from the second 100 days of the 200 day sequence, so the model is able to build up memory from previous timesteps. We tuned the batch size, number of hidden units, the number of hidden layers, the weight decay value, and the dropout rate. The runs in the sweep are scored by mean squared error (MSE) on the validation set, which consisted of 20% of the entire training dataset. For each site, the least recent 20% of water temperature observation were set aside and aggregated for validation. In total, the hyperparameter sweep completed 686 runs and the top five sets of hyperparameters were selected for use (Table S6).\nModels were trained with these hyperparameters until the error on the validation dataset was minimized using the early stopping technique with a patience value of 300 epochs (Prechelt, 1998). We undertook the same training procedure for all LSTM models, though hyperparameters were only tuned for the top-down model. This choice was made due to the high computational expense of hyperparameter tuning, and when an additional hyperparameter optimization scheme was applied to one of the models in the attribute-based grouped modeling scenario we found the validation error was unchanged.\nWe also conducted hyperparameter tuning for the XGBoost meta-model using random search and feature selection using the RFECV method in the scikit-learn Python library (Pedregosa et al., 2011). The meta-features consisted of the GAGES-II attribute differences, in addition to the mean and standard deviation of each dynamic meteorological and streamflow-based input feature, and observation statistics based off the previous MTL study (J. D. Willard et al., 2021) (e.g. number of water temperature observations available for training data). These are listed in Table S1. The values found during hyperparameter tuning of the XGBoost metamodel were 152 estimators with a learning rate of 0.183."}, {"title": "3.2.4 Model realization ensembles", "content": "ML models have uncertainty in model parameters after training due to a variety of factors including stochasticity in initialization of neural network weights and the shuffling of data between training epochs. It has been shown that ensemble results from multiple diverse model runs facilitates better overall ML model performance and robustness and also allow for the quantification of uncertainty (Ganaie et al., 2022). Hence, we used an ensemble average of five realizations for all model types, except in the case of results used for the error analysis described in Section 3.4, where we used an ensemble average of 20 model realizations to further reduce variance of model performance. We also spread out the top five hyperparameter sets found in the sweep across this model realization ensemble to facilitate more diversity between ensemble members, which tends to improve performance over creating ensembles with more similar models (Krogh & Vedelsby, 1995). Furthermore, the validation MSE between the top five hyperparameters was negligible so an overall performance decrease was highly improbable."}, {"title": "3.2.5 Model Evaluation", "content": "We assessed model performance using three primary metrics: (a) the median per-site RMSE calculated across the 580 test sites; (b) the median value of the average bias calculated for each of the test sites and (c) the median per-site RMSE calculated on only the warmest 10% of water temperature observations per site. All metrics were calculated on predictions averaged across the five model realizations in each case, with the exception being MTL which has 10 predicting models. We preferred median values due to large"}]}