{"title": "ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map", "authors": ["Yilin Ye", "Shishi Xiao", "Xingchen Zeng", "Wei Zeng"], "abstract": "Multi-modal embeddings form the foundation for vision-language models, such as CLIP embeddings, the most widely used text-image embeddings. However, these embeddings are vulnerable to subtle misalignment of cross-modal features, resulting in decreased model performance and diminished generalization. To address this problem, we design ModalChorus, an interactive system for visual probing and alignment of multi-modal embeddings. ModalChorus primarily offers a two-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel parametric dimensionality reduction method that integrates both metric and nonmetric objectives to enhance modality fusion; and 2) embedding alignment that allows users to interactively articulate intentions for both point-set and set-set alignments. Quantitative and qualitative comparisons for CLIP embeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and data fusion (e.g., data context map) methods demonstrate the advantages of MFM in showcasing cross-modal features over common vision-language datasets. Case studies reveal that ModalChorus can facilitate intuitive discovery of misalignment and efficient re-alignment in scenarios ranging from zero-shot classification to cross-modal retrieval and generation.", "sections": [{"title": "INTRODUCTION", "content": "Neural embeddings are high-dimensional latent representations for knowledge captured from self-supervised pre-training, such as word embeddings and image embeddings. Recently, multi-modal (e.g., text and image) embedding are playing a pivotal role for advancing multi-modal AI models. This type of embeddings learns a joint representation space that encodes different modalities and their relationships, forming the basis for cross-modal tasks such as text-to-image retrieval and generation [3, 49, 66]. The performance of multi-modal embedding models rely heavily on the quality of multi-modal alignment, which seeks to match data with corresponding semantics across different modalities within the embedding space [22, 23, 48]. However, misalignment in multi-modal embeddings is common due to the intricate many-to-many mapping among concepts in different modalities. For instance, text-to-image embeddings can easily encounter misalignment issues of concept entanglement. As illustrated in Figure 1(a), the text prompt of 'waterlily pond by Monet' becomes entangled with the 'bridge' concept in the image modality, reducing the diversity of generated images.\nIdentifying misalignment in multi-modal embeddings is crucial for enhancing model performance. Existing methods for evaluating mis- alignment often rely on reference-based evaluations (e.g., CIDEr [57] and SPICE [2]) that necessitate extensive human-labeled references, or reference-free metrics (e.g., CLIPScore [28]) derived from pretained multi-modal models. Despite not relying on references, reference-free metrics are reliant on pretrained models, making it challenging for fully automatic methods to detect misalignment in diverse and context-dependent scenarios. For instance, the CLIPScore for the text prompt 'waterlily pond by Monet' fails to reflect the issue of concept entanglement, as the CLIP model itself is biased towards the 'bridge' concept in the image modality. Hence, existing fine-tuning techniques to improve alignment often fall short of expectations in numerous scenarios. There is a need for an interactive visualization tool to help users intuitively investigate and address misalignment.\nHowever, the intricate data structures and feature characteristics inherent in multi-modal embeddings pose particular challenges for visual probing and interactive alignment. A key challenge arises from the modality gap, wherein embedding vectors from different modalities are essentially disjointed in the joint embedding space [39]. To achieve cohesive visualization of multi-modal embeddings, it is essential to address the modality gap issue and unify the presentation of different modalities within a single display space. Previous visualizations of neural embeddings have primarily centered on single-modal embeddings, such as word or image embeddings [26, 41, 42]. Notably, these works commonly employ classical dimensionality reduction (DR) methods like t-SNE [56] and MDS [5], which are limited to separately displaying multi-modal embeddings in distinct spaces. Fusion-based DR methods (e.g., [10, 13]) offer a potential solution to jointly project embeddings from different modalities. However, these methods typically treat intra- and inter-modal distances equally, without giving special consideration to cross-modal relations. For example, Data Context Map (DCM) [10] solely relies on metric-based objectives that poorly capture the relative rank order of inter-modal distances. As illustrated in Figure 5(left), DCM projects rather even distribution of image embedding points around the textual concepts, making it harder to observe differences in distribution pattern.\nMoreover, enabling interactive alignment for multi-modal embeddings presents another challenge, primarily due to two reasons. First, user-intended alignment strategies encompass diverse operations. For example, in Figure 1, upon identifying the concept entanglement between 'Monet' in the text modality and 'bridge' in the image modality, users may prefer to drag the 'bridge' point far away or relocate the entire set of 'Monet' images. However, existing studies often focus on point-based operations [16, 62], while others solely support set-based interaction [19]. Secondly, users would utilize interactive alignment to refine the underlying models and ensure that the refined model performs as expected, as illustrated by the disentangled images generated post-alignment, as in Figure 5(d). Existing studies on DR refinement mostly focus on adapting the projection layout [16, 19, 58, 62], whilst overlooking the model refinement.\nTo fill the gap, we present ModalChorus, an interactive system that supports visual probing and alignment of multi-modal embeddings. ModalChorus mainly comprises two-stage exploration. First, in the embedding probing stage, we propose Modal Fusion Map (MFM), a novel parametric DR method integrating metric and non-metric objectives for enhanced modality fusion. By taking the advantages of metric-based objectives in preserving the intra-modal distances and non-metric-based objectives in capturing inter-modal distance rank order [5, 13], MFM effectively addresses the modality gap challenge induced by multi-modal embeddings. Compared with conventional single-modal and fusion-based DR methods, MFM achieves higher trustworthiness and continuity regarding inter-modal relations (see Table 1), and can better visually reflect the intra- and inter-modality contextual distributions (see Figures 4 & 5). Next, in the embedding alignment stage, to accommodate the diverse alignment scenarios, we design an alignment interaction scheme that allows for alignment on multiple levels including point, subset, and set. The interaction scheme is integrated with MFM encompassing point-set and set-set alignment. Besides, a concept axis view is also developed to enable linear visual representation for the probing and alignment of multi-modal embeddings.\nIn summary, our make the following contributions:\n\u2022 We propose Modal Fusion Map (MFM), a novel dimensionality reduction method tailored for fusion projection of multi-modal embeddings. The effectiveness of MFM is demonstrated using both quantitative and qualitative evaluations.\n\u2022 We develop ModalChorus, an interactive system that supports visual probing of multi-modal embeddings to discover misalignment, along with an interaction scheme that supports interactive fine-tuning of the underlying multi-modal embedding models.\n\u2022 We show the effectiveness of our system through case studies on three embedding-based cross-modal tasks, ranging from zero-shot classification to cross-modal retrieval and generation."}, {"title": "RELATED WORK", "content": "Visualization for Neural Embeddings. Deep learning relies on neural networks that are often pre-trained on large amounts of data. Neural embeddings are the foundational high-dimensional feature representa- tion of raw data encoded by neural networks, such as text embeddings like word2vec [45] and BERT [15] and image embeddings like Sim- CLR [9]. Visualization researchers have dedicated significant efforts to enhance the comprehension of neural embeddings. Previous studies primarily focus on unimodal embeddings, encompassing word embed- dings [26, 27, 41] and image embeddings [42]. Many of these studies in- tegrate projection methods with axis-based [26, 41, 42] or set-based [27] exploration techniques. For example, Liu et al. [41] identified analogy axis between multiple pairs of words with the same semantic transition in word embeddings projected by t-SNE. Latent Space Cartography [42] extends the concept of semantic axis to customized axis defined by users, which can be applied to exploration of both unimodal word embeddings and image embeddings. EmbComp [27] combines t-SNE projection with visualization of neighborhood set overlap to compare different word embedding models.\nRecently, multi-modal embeddings such as CLIP [48] and ALIGN [33] have fueled the advances in multi-modal AI such as text- to-image generation. These embeddings can encode data from different modalities in a joint space, contributing to various applications such as cross-modal image retrieval [4] and generation [49]. However, this in- tegration also introduces modality gap [39] that signifies discrepancies between different modal embeddings, complicating the comprehen- sion of multi-modal embeddings. There is a lack of visualization tool tailored to the task. Specifically, the task demands an effective visual- ization method for probing multi-modal embeddings and an interactive scheme for improving alignment of multi-modal embeddings. To meet the goal, we propose the Modal Fusion Map that can better preseve the contextual information of multi-modal embeddings, and an interactive alignment scheme that offers visual steering for modal alignment.\nContextual Dimensionality Reduction. Dimensionality reduction for multi-modal data has been a challenging problem as traditional DR methods like t-SNE [56], PCA [59], and MDS [5] cannot account for cross-modal relations due to the modality gap [39, 46, 71]. Contextual visualization is a type of DR method designed to project data points in relation to attribute points [10, 44, 69], which can be applied to multi-modal data projection. Existing contextual visualizations can be categorized into two types: anchor-based projection and fusion-based methods. Anchor-based methods employ a two-stage approach, initially determining the layout of points in one modality before calculating the position of points in the other modality. For example, the RadViz method [11, 29, 65] first lays out the attribute points on a circle and then projects the data points based on their multi-dimensional attribute val- ues. However, the structure of the embedding space can be significantly distorted due to the challenge of optimally laying out the anchor points. One category of fusion methods, known as co-embedding meth- ods [12, 64], introduces their own high-dimensional representations of multi-modal data or modifies the embeddings of certain data points to achieve a desired visual layout. However, these methods diverge from our goal as they alter the original embeddings with custom mod- els, which cannot help users understand commonly used multi-modal embeddings in AI tasks. Visualization researchers have developed more general fusion methods [10, 69]. Particularly, Data Context Map"}, {"title": "OVERVIEW", "content": "3.1 Background and Domain Problem\nMulti-modal embedding. Multi-modal embedding models are pre- trained encoder models for the representation of multi-modal data. For example, the CLIP model is pre-trained on a large corpus of image-text pairs, using transformers and vision transformers to first separately encode text and image into high-dimensional vectors. Then, through a linear transformation, the text embedding and image embedding vectors are aligned in a shared embedding space with contrastive loss. Multi-modal embeddings are the foundational encoder for many AI tasks that involve multi-modal data in its input and/or output. Common tasks include semantics-based image classification [48], cross-modal retrieval [23], and text-to-image generation [8, 50].\nAlignment. In multi-modal models, alignment means the matching of data representations with corresponding semantics from different modalities. In the pre-training stage of CLIP, for example, the align- ment is achieved by updating the embeddings of an image and its corresponding text caption so that they are closer than incorrect pairs in the high-dimensional representation space. However, due to the varying quality and large quantity of data in pre-training and imperfection in training algorithms, there may be misalignment in the pretrained model, which requires further adaptation for enhancing alignment [20, 31, 46].\nMulti-level Alignment. There is mainly alignment on three levels: point, subset, and set, requiring two types of alignment: point-set alignment and set-set alignment. Specifically, users may discover mis- alignment of an individual point (e.g., misclassified image point or misunderstood text point), a subset (e.g., a subset of incorrect samples in the whole set of text-to-image retrieval results), and a set (e.g., biased or entangled generation results of text-to-image models), requiring dif- ferent alignment operations. To clarify, we refer to keywords extracted by our system or entered by users as concepts, which are the main text embeddings we focus on in this study for contextual exploration of embeddings, while particular image point is referred to as instances.\nProblem: visualization for embedding. Many visualization studies treat embedding methods as a tool for processing data, with the aim of optimizing the visual display of raw input data. That is, these visual- izations regard embedding as a projection method. Instead, in the field of AI, representation learning of single and multi-modal embeddings has been playing a pivotal role for various downstream tasks [37, 51]. The high-dimensional embeddings themselves are the key intermedi- ate representations of data extracted from raw text or pixels, not just the representation for visual display only. To gain insight into large AI models, particularly for the alignment problem, high-dimensional"}, {"title": "Challenges and Design Requirement", "content": "Accomplishing these two goals is challenging for existing visualization methods, particularly due to:\nC1 Modality Gap. The heterogeneous distributions of different modal- ities in the joint embedding space result in the modality gap [39, 71], making it difficult for existing DR methods to simultaneously cap- ture intra-modal and cross-modal features.\nC2 Diverse alignment intentions. The diverse alignment scenarios in different cross-modal tasks post challenges to designing a compre- hensive interaction scheme integrated into the visualization.\nTo tackle the challenges, we summarize the design requirements of ModalChorus, which should support flexible and effective R1) visual probing of multi-modal embeddings to meet G1.\nR1.1 Accurately preserving inter- and intra-modal distances. An effective fusion-based DR method is needed to bridge the modal- ity gap while maximally preserving inter- and intra-modal rela- tions.\nR1.2 Effective visual presentation to help identify misalignment. Apart from the projection, effective graphical enhancement is needed to assist discovery of misalignment issues such as mis- classification or entanglement.\nSecond, ModalChorus shall facilitate R2) interactive alignment of multi-modal embeddings to support G2:\nR2.1 Supporting alignment on point and set levels. Users may discover embedding misalignment on an individual data point or a whole set of points, demanding different types of alignment interaction, including point-set and set-set alignment.\nR2.2 Supporting axis-based alignment. Previous embedding visual- ization studies have identified the semantic axis as an effective complement of the overall projection for more focused concept- related exploration [26, 41, 42]. Besides directly manipulating the projection of embeddings, users also need to perform axis- based alignment as the axis can more clearly show the direction of alignment with respect to a specific semantic concept.\nR2.3 Supporting data augmentation. When users discover misalign- ment but cannot find correct reference data, they would like to provide extra data and process it to help the alignment."}, {"title": "ModalChorus Overview", "content": "An overview of our system is shown in Fig. 2, which mainly consists of two stages: 1) embedding probing and 2) embedding alignment. In the first stage, starting from a particular dataset and task, along with user-provided input or automatically extracted concept, we support visual probing of the embeddings with sampled data for interpretation of embeddings and discovery of misalignment. Particularly, we develop Modal Fusion Map, a novel parametric fusion method that integrates metric and nonmetric objectives for multi-modal embedding projection. We also incorporate a concept axis view that allows users to explore the correlation of image embeddings in relation to concept text embeddings. An additional instance gallery displays similar images to the selected image point in the embedding space for neighborhood exploration.\nIn the second stage, upon discovering misalignment, we enable users to select a particular point, subset, or set and perform point-set alignment or set-set alignment in either the projection view or the concept axis view. In some cases, when new data is needed to enhance the alignment, we allow users to upload their collected data for few-shot alignment or use our system's weighted embedding generation function to generate candidate augmentation data. Finally, the visual alignment operations are mapped to the backend fine-tuning."}, {"title": "MULTI-MODAL CONTEXTUAL VISUALIZATION", "content": "In this section, we describe Modal Fusion Map, a novel DR method we propose to address R1 visual probing of multi-modal embedding.\n4.1 Problem Identification\nTo address the modality gap problem in multi-modal embedding visual- ization, data matrix fusion methods [10, 13] are a promising solution. Matrix fusion methods such as Data Context Map (DCM) [10] are derived from the MDS method for distance-based fusion. The origi- nal Data Context Map is designed for the attribute and data spaces of multi-dimensional data. Specifically, to align data points from different modalities, it constructs a large distance matrix containing the pairwise distances between all the data points and attribute points, where the intra-modal distance is the original high-dimensional distance such as Euclidean or Cosine distance while the cross-modal distance needs to be defined according to data properties. For example, DCM defines the distance between attribute point and data point as 1 \u2212 v, where v is the data point\u2019s value in this attribute dimension.\nFirst, to account for high dimensional latent space, we can natu- rally change the attribute-data distance in DCM to the Cosine distance between text embedding and image embedding. However, this modifi- cation may not suffice for the complexity of multi-modal embedding. Specifically, to enhance the modality merging effect, it is important to flexibly adjust the weights of intra-modality and inter-modality dis- tance. Directly scaling the submatrix as mentioned in [13] may have the risk of significantly distorting the embedding space or exacerbating the modality gap. More importantly, when multi-modal embeddings"}, {"title": "Modal Fusion Map", "content": "Dimensionality Reduction. Inspired by recent work on parametric dimensionality reduction [62, 63, 67], to satisfy the projection require- ments presented above, we propose Modal Fusion Map (MFM) which can flexibly combine different objectives for joint multi-modal em- bedding projection. The hypothesis is that in the high dimensional embedding space, there exists a subspace or manifold surface S be- tween the text embeddings set T and image embeddings set I, such that the projection of embeddings from both modalities on this surface (P(T), P(I) \u2208 S) can result in an optimized 2D parametric represen- tation S(x, y). Specifically, like other matrix-based methods, we first compute the merged distance matrix:\n$M = \\begin{pmatrix} II & IT\\\\ TI,T & TT \\end{pmatrix}$ (1)\nwhere II is image distance submatrix, TT is text distance submatrix, IT = TI T is the cross-modal distance submatrix, all using cosine dis- tance. Each submatrix is normalized by their mean value.\nNext, instead of directly applying the traditional MDS method to the merged matrix as in DCM, we parametrize the projection with a three- layer feed-forward neural network mapping 512 or 1024-dimensional CLIP embedding to the 2-dimensional projection space. See supplemen- tary material for more detail. Then, to implement the MDS objective, we construct a loss function using the Pearson correlation between the high dimensional merged distance matrix and the projected distance matrix for scale-free optimization.\n$L_M = \\frac{\\sum (M_{i,j} - \\bar{M}) (P_{i,j} - \\bar{P})}{\\sqrt{\\sum (M_{i,j} - \\bar{M})^2 \\sum (P_{i,j} - \\bar{P})^2}}$ (2)\nwhere P stands for the distance matrix of the projected points.\nIn this way, we can easily define loss terms for the intra-modality and inter-modality submatrices, denoted as LTT , LII , and LIT , respectively. Accordingly, the loss function for metric MDS is the weighted sum. In our case, we only consider the overall term and the cross-modal term: L1 = w1 LM + w2 LIT , where we set w1 = 10, w2 = 2.\nIn addition, for the nonmetric loss to preserve cross-modality dis- tance order, we further introduce another loss term:\n$L_2 = - \\sum_{j1,j - T_{I,j,k}) * (P(T_I){i,j} - P(T_I){i,k})}{\\|P(T_I)\\|^2}}$ (3)\nwhere $f(x) = \\begin{cases} 0, & x \\geq 0 \\\\ -x, & x < 0 \\end{cases}$. This loss term will be zero when all the cross-modal distance order is preserved in the projection. The final loss L = L1 + \u03b1L2, \u03b1 = 0.05. w1 , w2 , \u03b1 are selected empirically.\nContour-based graphical enhancement. We provide graphical en- hancements in the form of density contour as inspired by recent work [69]. As shown in Fig. 5, the density plot can show the default KDE density estimation of data point distribution. The KDE contour can serve as a graphical representation of sets in the projection view, which can facilitate subsequent alignment interaction as we describe be- low. Alternatively, when users provide customized metrics defined for the data points, such as CLIP-Score for generated samples, the density plot can show the kernel estimation of the metric value distribution."}, {"title": "Evaluation", "content": "Qualitative Comparison. As shown in Fig. 4 and Fig. 5, MFM has many advantages for displaying both intra-modality and inter-modality features compared to the DCM method and traditional projection meth- ods like MDS and t-SNE. Specifically, Fig. 4 displays an intra-modal case with the projection of CLIP image embeddings for samples of 6 classes in CIFAR-10 dataset. The colors represent the zero-shot classifi- cation results based on CLIP. Among the results, we can see that t-SNE achieves the best separation effect. However, t-SNE also has significant drawbacks in understanding the embeddings and identifying misalign- ment because it does not consider cross-modal features. First, t-SNE is weaker at showing contextual information, such as the relation between different sets. For example, we can find in Fig. 4, the frog set (green point 1) can be confused with the bird set (yellow point 2) because of similar color or background, yet the t-SNE projection does not clearly show the relation compared to MFM. In addition, our joint projection also shows better within-set distribution than t-SNE. For example, with MFM, we can clearly see outliers or border points within sets (e.g. blue point 5 and green point 6). Point 5 corresponds to an image of a car driving on a highway, while most other car images are static scenes of parked cars. Point 6 is a long-necked ostrich that is quite different in appearance from other birds. However, these points are hard to identify in the t-SNE projection. The MDS result in Fig. 4 (b) is more effective than t-SNE for showing the pointwise relationship, but the clustering effect is apparently weaker than t-SNE and MFM. In addition, MDS tends to distribute the points quite evenly in the projected space, which compromises the display of in-set distribution and outliers. The DCM method shows the contextual set relationship better than t-SNE and displays set outliers only slightly better than MDS, since for the image modality, both DCM and MDS use metric loss, but MFM achieves better effect in both aspects. Additionally, we compute Z-Score for a data point to help verify whether a visual outlier or border point is indeed so in the original high-dimensional space (see supplementary for detail). We find that point 6, the most obvious outlier in Fig. 4 (d) indeed has the highest Z-score of 1.2379.\nFig. 5 shows the inter-modal case with the projection of CLIP text embeddings of three text queries together with the image embeddings of the query results. Regarding the DCM results (Fig. 5 (a)), it does successfully merge the modality. However, as shown in both cases, DCM has a significant weakness in that it scatters out the image em- bedding points quite evenly across the space, making it difficult for users to find distributional differences between different regions. In comparison, for example, in Fig. 5 (b), we can find an obvious dense cluster in MFM results which contain many similar images of bear in the wild, but this pattern is less evident in DCM results. In addi- tion, MFM also more clearly shows the relation between concepts than DCM, as we can find that the query results of bear and person and bear have obvious overlap in both DCM and MFM results, signifying the closer relationship between these two text concepts, but the relative position of text embeddings in MFM is more coherent to this relation than DCM.\nQuantitative Evaluation. To show some quantitative evidence of the advantage of MFM, we evaluate the method on COCO dataset using the trustworthiness metric and continuity metric [34, 62], where the former calculates how faithfully the projected kNN reflects the true kNN\u2019s in the embedding space and the latter calculates how well the original high-dimensional kNN is preserved in the projection. Particularly, we calculate both inter-modal and intra-modal kNN for k=30. However, in our scenario the inter-modal metric is more important as it measures the methods\u2019 ability to preserve multi-modal embedding structure. For the evaluation process, we perform multiple rounds (r = 500) of evaluation where in each round we randomly sample 500 images from COCO and project them together with the 80 category text embeddings in COCO object labels. The final metric is the average of the results in all rounds. As shown in Table 1, the experimental results indicate that MFM method performs consistently better than all the other methods in inter-modal truthworthiness and continuity, with higher than 2% margin over the strongest baseline DCM. In addition, MFM also achieves good performance in intra-modal metrics, only second to t-SNE. NDCM [13] is another fusion method using fully nonmetric objective. We can see that among the three fusion methods (MFM, DCM and NDCM), our MFM is consistently better across inter-modal and intra-modal metrics, while fully nonmetric fusion method has significant disadvantage in keeping the intra-modal features. We also need to note that the inter- modal metrics for non-fusion traditional methods like t-SNE and MDS cannot fully reflect their weakness in inter-modal scenarios because the modality gap will cause large distances between image embeddings and text embeddings in the projection space, making it difficult to perceive the differences between the inter-modal distances [39, 46, 71]."}, {"title": "ModalChorus SYSTEM", "content": "5.1 Visualization Interface\nSettings Panel. The settings panel (Fig. 6 (a)) allows users to specify some basic settings for their exploration, including tasks and inputs. Users can also select specific concepts in their input to produce con- textual visualization in the projection view. Instead of relying solely on textual concepts explicitly extracted from existing text labels or prompts, ModalChorus extracts implicit concepts from images to pro- vide a comprehensive display of concepts. To achieve this, we first leverage BLIP-2 [38], a multi-modal language model capable of receiv- ing images as input and generating textual descriptions of those images. We then employ the TopicRank [6] algorithms to extract candidate visual concepts based on the text generated by BLIP-2.\nProjection View. The projection view (Fig. 6 (b)) is the main view of the system leveraging our proposed Modal Fusion Map to help users probe the embedding with different tasks and data. Users can choose to turn on or turn off the contour to emphasize set relation or facilitate instance exploration respectively. The projection view also includes an instance retrieval subview below (Fig. 6 (c)). Users can mouse over the embedding point to see the corresponding image in the gallery. They can also click the point to retrieve similar images to the selected one. In addition, users can select a subset of points by lasso or ctrl-click, as shown in Case 2 and Fig. 10 in Sect. 6.\nConcept Axis View. As shown in Fig. 6 (d) and Fig. 7 (a), the concept axis view supports axis-based exploration of image embeddings in relation to text embeddings for user-selected concepts from the settings panel. Users can define one-end axis with a single concept (e.g., bridge) or two-end axis with opposing concepts they want to contrast (e.g., Monet and Van Goph). For one-end axis, the position of an image embedding point x is:\n$\\mu_A(x) = l \\frac{sim(x,A) - min(sim(x,A))}{max(sim(x,A)) - min(sim(x,A))}$, (4)\nwhere sim(x, A) denotes the cosine similarity between x and text em- bedding of concept A in embedding space, l is the length of the axis. For two-end axis, the position of x is calculated as l (0.5+ \u00b5A(x)+\u00b5B(x)). When users define more than one axis, we use curves connecting the same instance on two axes to show the correlation. Histogram is also used to help users see the overall distribution. Apart from displaying instances of image embeddings, the concept axis can also represent the whole set or subset as small box at the average position of all the in-set points, showing users the mean value of the set and supporting further set-based alignment interaction as described in Fig. 8 and Sect. 5.2. We also allow users to switch to a scatterplot visualization (Fig. 7 (b)).\nAugmentation Panel. The data augmentation panel (Fig. 6 (e)) sup- ports interactive augmentation of alignment data. In some alignment scenarios, users cannot find proper alignment data from the original dataset (for example, users may not find any satisfactory results gen- erated by a pre-trained generative model). For such a problem, the augmentation panel first allows users to upload a subset of samples to supplement the alignment data. For unlabeled raw image data up- loaded, this panel also integrates an auto-tagging function based on CLIP-interrogator [1], which can generate tags associated with the image to enhance the alignment performance. Second, in cases where users even find it difficult to collect their own data, the augmenta- tion panel also incorporates a generation function that enables users to leverage the weighted sum of existing text embeddings [14, 60] in a pre-trained generation model to synthesize more candidates of intention-aligned samples."}, {"title": "Interactive Alignment", "content": "Alignment Interaction Design. We design a series of visual alignment interactions, which allow users to intuitively express diverse alignment intentions through visual metaphor and trigger backend fine-tuning without writing complex training code. As shown in Fig. 8, our interac- tion scheme supports set and point level intentions for alignment. First, the common types of alignment mainly concern data points or subsets of points, which we categorize into two types: point-set alignment and set-set alignment, as shown in Fig. 8. First, point-set alignment encompasses various scenarios of aligning a set with a data point and vice versa. For example, when users want to align a subset of retrieval results with a query text embedding or when users want to align a prompt embedding to fine-tune samples provided by themselves. As shown in Fig. 8 (a), point-set alignment can be performed on either the projection view or the concept axis. Formally, the high-level idea of point-set alignment can be summarized as follows: Suppose we have a CLIP-based model F(\u00b7) which can map input text or image to different sets C1, C2, ..., CN in the sampled data. For example, in classification, Ci corresponds to the set of embeddings for a predicted class; in retrieval and generation, Ci corresponds to the set of embeddings for the results of a single query or prompt. Given a user-selected misaligned image or text point p, the target of point-set alignment is to tune the weights of F(\u00b7) such that F(p) is closer to the correct set Ci in the embedding space. Although the concrete implementation may vary for different tasks, in terms of the merged distance matrix, the effect is equivalent to achieving the following contrastive objective:\n$\\frac{1}{|C_i|} \\sum_<{C_i} M_{F(p),v} < \\frac{1}{\\sum_{j \\neq i, |C_j|}} \\sum_{u<{C_j}} M_{F(p),u}$ (5)\nwhere the estimated distance between F(p) and Ci should be smaller than any other set Cj. Second, set-set alignment involves moving two subsets of points closer or further in the concept axis or projection view. Such alignment is intended to close the gap between two sets or distributions in the embedding space, or contrast two sets for dis- tinguishing them better, which can be useful for cases like merging or disentangling concepts in retrieval or generation. As shown in Fig. 8 (b), in the projection view, they can drag a set contour towards another, while in the axis view, they can drag one set box closer to or away from another. Formally, the high-level idea of set-set alignment is: Suppose users identify a misaligned set or subset of embeddings Ce , where Ce is not align with the input p. Next, users find another correct set Ci either by visual exploration of other projected data points or by data augmentation. The goal of set-set alignment can then be formulated as:\n$\\frac{1}{|F(p)|} \\sum_<{Ce} M_{i,v} > \\frac{1}{|F(p)|} \\sum_<{Ci} M_{i,v}$. (6)\nAlignment Fine-tuning Implementation. Our system provides a general framework to map users' visual interactions shown in Fig. 8 to backend fine-tuning operations that align the model\u2019s output in the embedding space. As the visual representations are decoupled from actual backend implementation, our framework can incorporate any kind of specific fine-tuning methods. For demonstration purposes, our study implements two methods. First, for the classification and retrieval cases, we implement triplet loss [54] based alignment. Second, for the generation cases, we implement the low-rank adaptation method [31]. More detail is provided in the supplementary material."}, {"title": "CASE STUDIES", "content": "In this section", "1": "Zero-shot classification\nIn this case"}, {"title": "ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map", "authors": ["Yilin Ye", "Shishi Xiao", "Xingchen Zeng", "Wei Zeng"], "abstract": "Multi-modal embeddings form the foundation for vision-language models, such as CLIP embeddings, the most widely used text-image embeddings. However, these embeddings are vulnerable to subtle misalignment of cross-modal features, resulting in decreased model performance and diminished generalization. To address this problem, we design ModalChorus, an interactive system for visual probing and alignment of multi-modal embeddings. ModalChorus primarily offers a two-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel parametric dimensionality reduction method that integrates both metric and nonmetric objectives to enhance modality fusion; and 2) embedding alignment that allows users to interactively articulate intentions for both point-set and set-set alignments. Quantitative and qualitative comparisons for CLIP embeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and data fusion (e.g., data context map) methods demonstrate the advantages of MFM in showcasing cross-modal features over common vision-language datasets. Case studies reveal that ModalChorus can facilitate intuitive discovery of misalignment and efficient re-alignment in scenarios ranging from zero-shot classification to cross-modal retrieval and generation.", "sections": [{"title": "INTRODUCTION", "content": "Neural embeddings are high-dimensional latent representations for knowledge captured from self-supervised pre-training, such as word embeddings and image embeddings. Recently, multi-modal (e.g., text and image) embedding are playing a pivotal role for advancing multi-modal AI models. This type of embeddings learns a joint representation space that encodes different modalities and their relationships, forming the basis for cross-modal tasks such as text-to-image retrieval and generation [3, 49, 66]. The performance of multi-modal embedding models rely heavily on the quality of multi-modal alignment, which seeks to match data with corresponding semantics across different modalities within the embedding space [22, 23, 48]. However, misalignment in multi-modal embeddings is common due to the intricate many-to-many mapping among concepts in different modalities. For instance, text-to-image embeddings can easily encounter misalignment issues of concept entanglement. As illustrated in Figure 1(a), the text prompt of 'waterlily pond by Monet' becomes entangled with the 'bridge' concept in the image modality, reducing the diversity of generated images.\nIdentifying misalignment in multi-modal embeddings is crucial for enhancing model performance. Existing methods for evaluating mis- alignment often rely on reference-based evaluations (e.g., CIDEr [57] and SPICE [2]) that necessitate extensive human-labeled references, or reference-free metrics (e.g., CLIPScore [28]) derived from pretained multi-modal models. Despite not relying on references, reference-free metrics are reliant on pretrained models, making it challenging for fully automatic methods to detect misalignment in diverse and context-dependent scenarios. For instance, the CLIPScore for the text prompt 'waterlily pond by Monet' fails to reflect the issue of concept entanglement, as the CLIP model itself is biased towards the 'bridge' concept in the image modality. Hence, existing fine-tuning techniques to improve alignment often fall short of expectations in numerous scenarios. There is a need for an interactive visualization tool to help users intuitively investigate and address misalignment.\nHowever, the intricate data structures and feature characteristics inherent in multi-modal embeddings pose particular challenges for visual probing and interactive alignment. A key challenge arises from the modality gap, wherein embedding vectors from different modalities are essentially disjointed in the joint embedding space [39]. To achieve cohesive visualization of multi-modal embeddings, it is essential to address the modality gap issue and unify the presentation of different modalities within a single display space. Previous visualizations of neural embeddings have primarily centered on single-modal embeddings, such as word or image embeddings [26, 41, 42]. Notably, these works commonly employ classical dimensionality reduction (DR) methods like t-SNE [56] and MDS [5], which are limited to separately displaying multi-modal embeddings in distinct spaces. Fusion-based DR methods (e.g., [10, 13]) offer a potential solution to jointly project embeddings from different modalities. However, these methods typically treat intra- and inter-modal distances equally, without giving special consideration to cross-modal relations. For example, Data Context Map (DCM) [10] solely relies on metric-based objectives that poorly capture the relative rank order of inter-modal distances. As illustrated in Figure 5(left), DCM projects rather even distribution of image embedding points around the textual concepts, making it harder to observe differences in distribution pattern.\nMoreover, enabling interactive alignment for multi-modal embeddings presents another challenge, primarily due to two reasons. First, user-intended alignment strategies encompass diverse operations. For example, in Figure 1, upon identifying the concept entanglement between 'Monet' in the text modality and 'bridge' in the image modality, users may prefer to drag the 'bridge' point far away or relocate the entire set of 'Monet' images. However, existing studies often focus on point-based operations [16, 62], while others solely support set-based interaction [19]. Secondly, users would utilize interactive alignment to refine the underlying models and ensure that the refined model performs as expected, as illustrated by the disentangled images generated post-alignment, as in Figure 5(d). Existing studies on DR refinement mostly focus on adapting the projection layout [16, 19, 58, 62], whilst overlooking the model refinement.\nTo fill the gap, we present ModalChorus, an interactive system that supports visual probing and alignment of multi-modal embeddings. ModalChorus mainly comprises two-stage exploration. First, in the embedding probing stage, we propose Modal Fusion Map (MFM), a novel parametric DR method integrating metric and non-metric objectives for enhanced modality fusion. By taking the advantages of metric-based objectives in preserving the intra-modal distances and non-metric-based objectives in capturing inter-modal distance rank order [5, 13], MFM effectively addresses the modality gap challenge induced by multi-modal embeddings. Compared with conventional single-modal and fusion-based DR methods, MFM achieves higher trustworthiness and continuity regarding inter-modal relations (see Table 1), and can better visually reflect the intra- and inter-modality contextual distributions (see Figures 4 & 5). Next, in the embedding alignment stage, to accommodate the diverse alignment scenarios, we design an alignment interaction scheme that allows for alignment on multiple levels including point, subset, and set. The interaction scheme is integrated with MFM encompassing point-set and set-set alignment. Besides, a concept axis view is also developed to enable linear visual representation for the probing and alignment of multi-modal embeddings.\nIn summary, our make the following contributions:\n\u2022 We propose Modal Fusion Map (MFM), a novel dimensionality reduction method tailored for fusion projection of multi-modal embeddings. The effectiveness of MFM is demonstrated using both quantitative and qualitative evaluations.\n\u2022 We develop ModalChorus, an interactive system that supports visual probing of multi-modal embeddings to discover misalignment, along with an interaction scheme that supports interactive fine-tuning of the underlying multi-modal embedding models.\n\u2022 We show the effectiveness of our system through case studies on three embedding-based cross-modal tasks, ranging from zero-shot classification to cross-modal retrieval and generation."}, {"title": "RELATED WORK", "content": "Visualization for Neural Embeddings. Deep learning relies on neural networks that are often pre-trained on large amounts of data. Neural embeddings are the foundational high-dimensional feature representa- tion of raw data encoded by neural networks, such as text embeddings like word2vec [45] and BERT [15] and image embeddings like Sim- CLR [9]. Visualization researchers have dedicated significant efforts to enhance the comprehension of neural embeddings. Previous studies primarily focus on unimodal embeddings, encompassing word embed- dings [26, 27, 41] and image embeddings [42]. Many of these studies in- tegrate projection methods with axis-based [26, 41, 42] or set-based [27] exploration techniques. For example, Liu et al. [41] identified analogy axis between multiple pairs of words with the same semantic transition in word embeddings projected by t-SNE. Latent Space Cartography [42] extends the concept of semantic axis to customized axis defined by users, which can be applied to exploration of both unimodal word embeddings and image embeddings. EmbComp [27] combines t-SNE projection with visualization of neighborhood set overlap to compare different word embedding models.\nRecently, multi-modal embeddings such as CLIP [48] and ALIGN [33] have fueled the advances in multi-modal AI such as text- to-image generation. These embeddings can encode data from different modalities in a joint space, contributing to various applications such as cross-modal image retrieval [4] and generation [49]. However, this in- tegration also introduces modality gap [39] that signifies discrepancies between different modal embeddings, complicating the comprehen- sion of multi-modal embeddings. There is a lack of visualization tool tailored to the task. Specifically, the task demands an effective visual- ization method for probing multi-modal embeddings and an interactive scheme for improving alignment of multi-modal embeddings. To meet the goal, we propose the Modal Fusion Map that can better preseve the contextual information of multi-modal embeddings, and an interactive alignment scheme that offers visual steering for modal alignment.\nContextual Dimensionality Reduction. Dimensionality reduction for multi-modal data has been a challenging problem as traditional DR methods like t-SNE [56], PCA [59], and MDS [5] cannot account for cross-modal relations due to the modality gap [39, 46, 71]. Contextual visualization is a type of DR method designed to project data points in relation to attribute points [10, 44, 69], which can be applied to multi-modal data projection. Existing contextual visualizations can be categorized into two types: anchor-based projection and fusion-based methods. Anchor-based methods employ a two-stage approach, initially determining the layout of points in one modality before calculating the position of points in the other modality. For example, the RadViz method [11, 29, 65] first lays out the attribute points on a circle and then projects the data points based on their multi-dimensional attribute val- ues. However, the structure of the embedding space can be significantly distorted due to the challenge of optimally laying out the anchor points. One category of fusion methods, known as co-embedding meth- ods [12, 64], introduces their own high-dimensional representations of multi-modal data or modifies the embeddings of certain data points to achieve a desired visual layout. However, these methods diverge from our goal as they alter the original embeddings with custom mod- els, which cannot help users understand commonly used multi-modal embeddings in AI tasks. Visualization researchers have developed more general fusion methods [10, 69]. Particularly, Data Context Map"}, {"title": "OVERVIEW", "content": "3.1 Background and Domain Problem\nMulti-modal embedding. Multi-modal embedding models are pre- trained encoder models for the representation of multi-modal data. For example, the CLIP model is pre-trained on a large corpus of image-text pairs, using transformers and vision transformers to first separately encode text and image into high-dimensional vectors. Then, through a linear transformation, the text embedding and image embedding vectors are aligned in a shared embedding space with contrastive loss. Multi-modal embeddings are the foundational encoder for many AI tasks that involve multi-modal data in its input and/or output. Common tasks include semantics-based image classification [48], cross-modal retrieval [23], and text-to-image generation [8, 50].\nAlignment. In multi-modal models, alignment means the matching of data representations with corresponding semantics from different modalities. In the pre-training stage of CLIP, for example, the align- ment is achieved by updating the embeddings of an image and its corresponding text caption so that they are closer than incorrect pairs in the high-dimensional representation space. However, due to the varying quality and large quantity of data in pre-training and imperfection in training algorithms, there may be misalignment in the pretrained model, which requires further adaptation for enhancing alignment [20, 31, 46].\nMulti-level Alignment. There is mainly alignment on three levels: point, subset, and set, requiring two types of alignment: point-set alignment and set-set alignment. Specifically, users may discover mis- alignment of an individual point (e.g., misclassified image point or misunderstood text point), a subset (e.g., a subset of incorrect samples in the whole set of text-to-image retrieval results), and a set (e.g., biased or entangled generation results of text-to-image models), requiring dif- ferent alignment operations. To clarify, we refer to keywords extracted by our system or entered by users as concepts, which are the main text embeddings we focus on in this study for contextual exploration of embeddings, while particular image point is referred to as instances.\nProblem: visualization for embedding. Many visualization studies treat embedding methods as a tool for processing data, with the aim of optimizing the visual display of raw input data. That is, these visual- izations regard embedding as a projection method. Instead, in the field of AI, representation learning of single and multi-modal embeddings has been playing a pivotal role for various downstream tasks [37, 51]. The high-dimensional embeddings themselves are the key intermedi- ate representations of data extracted from raw text or pixels, not just the representation for visual display only. To gain insight into large AI models, particularly for the alignment problem, high-dimensional"}, {"title": "Challenges and Design Requirement", "content": "Accomplishing these two goals is challenging for existing visualization methods, particularly due to:\nC1 Modality Gap. The heterogeneous distributions of different modal- ities in the joint embedding space result in the modality gap [39, 71], making it difficult for existing DR methods to simultaneously cap- ture intra-modal and cross-modal features.\nC2 Diverse alignment intentions. The diverse alignment scenarios in different cross-modal tasks post challenges to designing a compre- hensive interaction scheme integrated into the visualization.\nTo tackle the challenges, we summarize the design requirements of ModalChorus, which should support flexible and effective R1) visual probing of multi-modal embeddings to meet G1.\nR1.1 Accurately preserving inter- and intra-modal distances. An effective fusion-based DR method is needed to bridge the modal- ity gap while maximally preserving inter- and intra-modal rela- tions.\nR1.2 Effective visual presentation to help identify misalignment. Apart from the projection, effective graphical enhancement is needed to assist discovery of misalignment issues such as mis- classification or entanglement.\nSecond, ModalChorus shall facilitate R2) interactive alignment of multi-modal embeddings to support G2:\nR2.1 Supporting alignment on point and set levels. Users may discover embedding misalignment on an individual data point or a whole set of points, demanding different types of alignment interaction, including point-set and set-set alignment.\nR2.2 Supporting axis-based alignment. Previous embedding visual- ization studies have identified the semantic axis as an effective complement of the overall projection for more focused concept- related exploration [26, 41, 42]. Besides directly manipulating the projection of embeddings, users also need to perform axis- based alignment as the axis can more clearly show the direction of alignment with respect to a specific semantic concept.\nR2.3 Supporting data augmentation. When users discover misalign- ment but cannot find correct reference data, they would like to provide extra data and process it to help the alignment."}, {"title": "ModalChorus Overview", "content": "An overview of our system is shown in Fig. 2, which mainly consists of two stages: 1) embedding probing and 2) embedding alignment. In the first stage, starting from a particular dataset and task, along with user-provided input or automatically extracted concept, we support visual probing of the embeddings with sampled data for interpretation of embeddings and discovery of misalignment. Particularly, we develop Modal Fusion Map, a novel parametric fusion method that integrates metric and nonmetric objectives for multi-modal embedding projection. We also incorporate a concept axis view that allows users to explore the correlation of image embeddings in relation to concept text embeddings. An additional instance gallery displays similar images to the selected image point in the embedding space for neighborhood exploration.\nIn the second stage, upon discovering misalignment, we enable users to select a particular point, subset, or set and perform point-set alignment or set-set alignment in either the projection view or the concept axis view. In some cases, when new data is needed to enhance the alignment, we allow users to upload their collected data for few-shot alignment or use our system's weighted embedding generation function to generate candidate augmentation data. Finally, the visual alignment operations are mapped to the backend fine-tuning."}, {"title": "MULTI-MODAL CONTEXTUAL VISUALIZATION", "content": "In this section, we describe Modal Fusion Map, a novel DR method we propose to address R1 visual probing of multi-modal embedding.\n4.1 Problem Identification\nTo address the modality gap problem in multi-modal embedding visual- ization, data matrix fusion methods [10, 13] are a promising solution. Matrix fusion methods such as Data Context Map (DCM) [10] are derived from the MDS method for distance-based fusion. The origi- nal Data Context Map is designed for the attribute and data spaces of multi-dimensional data. Specifically, to align data points from different modalities, it constructs a large distance matrix containing the pairwise distances between all the data points and attribute points, where the intra-modal distance is the original high-dimensional distance such as Euclidean or Cosine distance while the cross-modal distance needs to be defined according to data properties. For example, DCM defines the distance between attribute point and data point as 1 \u2212 v, where v is the data point\u2019s value in this attribute dimension.\nFirst, to account for high dimensional latent space, we can natu- rally change the attribute-data distance in DCM to the Cosine distance between text embedding and image embedding. However, this modifi- cation may not suffice for the complexity of multi-modal embedding. Specifically, to enhance the modality merging effect, it is important to flexibly adjust the weights of intra-modality and inter-modality dis- tance. Directly scaling the submatrix as mentioned in [13] may have the risk of significantly distorting the embedding space or exacerbating the modality gap. More importantly, when multi-modal embeddings"}, {"title": "Modal Fusion Map", "content": "Dimensionality Reduction. Inspired by recent work on parametric dimensionality reduction [62, 63, 67], to satisfy the projection require- ments presented above, we propose Modal Fusion Map (MFM) which can flexibly combine different objectives for joint multi-modal em- bedding projection. The hypothesis is that in the high dimensional embedding space, there exists a subspace or manifold surface S be- tween the text embeddings set T and image embeddings set I, such that the projection of embeddings from both modalities on this surface (P(T), P(I) \u2208 S) can result in an optimized 2D parametric represen-tation S(x, y). Specifically, like other matrix-based methods, we first compute the merged distance matrix:\n$M = \\begin{pmatrix} II & IT\\\\ TI,T & TT \\end{pmatrix}$ (1)\nwhere II is image distance submatrix, TT is text distance submatrix, IT = TI T is the cross-modal distance submatrix, all using cosine dis- tance. Each submatrix is normalized by their mean value.\nNext, instead of directly applying the traditional MDS method to the merged matrix as in DCM, we parametrize the projection with a three- layer feed-forward neural network mapping 512 or 1024-dimensional CLIP embedding to the 2-dimensional projection space. See supplemen- tary material for more detail. Then, to implement the MDS objective, we construct a loss function using the Pearson correlation between the high dimensional merged distance matrix and the projected distance matrix for scale-free optimization.\n$L_M = \\frac{\\sum (M_{i,j} - \\bar{M}) (P_{i,j} - \\bar{P})}{\\sqrt{\\sum (M_{i,j} - \\bar{M})^2 \\sum (P_{i,j} - \\bar{P})^2}}$ (2)\nwhere P stands for the distance matrix of the projected points.\nIn this way, we can easily define loss terms for the intra-modality and inter-modality submatrices, denoted as LTT , LII , and LIT , respectively. Accordingly, the loss function for metric MDS is the weighted sum. In our case, we only consider the overall term and the cross-modal term: L1 = w1 LM + w2 LIT , where we set w1 = 10, w2 = 2.\nIn addition, for the nonmetric loss to preserve cross-modality dis- tance order, we further introduce another loss term:\n$L_2 = - \\sum_{j1,j - T_{I,j,k}) * (P(T_I){i,j} - P(T_I){i,k})}{\\|P(T_I)\\|^2}}$ (3)\nwhere $f(x) = \\begin{cases} 0, & x \\geq 0 \\\\ -x, & x < 0 \\end{cases}$. This loss term will be zero when all the cross-modal distance order is preserved in the projection. The final loss L = L1 + \u03b1L2, \u03b1 = 0.05. w1 , w2 , \u03b1 are selected empirically.\nContour-based graphical enhancement. We provide graphical en- hancements in the form of density contour as inspired by recent work [69]. As shown in Fig. 5, the density plot can show the default KDE density estimation of data point distribution. The KDE contour can serve as a graphical representation of sets in the projection view, which can facilitate subsequent alignment interaction as we describe be- low. Alternatively, when users provide customized metrics defined for the data points, such as CLIP-Score for generated samples, the density plot can show the kernel estimation of the metric value distribution."}, {"title": "Evaluation", "content": "Qualitative Comparison. As shown in Fig. 4 and Fig. 5, MFM has many advantages for displaying both intra-modality and inter-modality features compared to the DCM method and traditional projection meth- ods like MDS and t-SNE. Specifically, Fig. 4 displays an intra-modal case with the projection of CLIP image embeddings for samples of 6 classes in CIFAR-10 dataset. The colors represent the zero-shot classifi- cation results based on CLIP. Among the results, we can see that t-SNE achieves the best separation effect. However, t-SNE also has significant drawbacks in understanding the embeddings and identifying misalign- ment because it does not consider cross-modal features. First, t-SNE is weaker at showing contextual information, such as the relation between different sets. For example, we can find in Fig. 4, the frog set (green point 1) can be confused with the bird set (yellow point 2) because of similar color or background, yet the t-SNE projection does not clearly show the relation compared to MFM. In addition, our joint projection also shows better within-set distribution than t-SNE. For example, with MFM, we can clearly see outliers or border points within sets (e.g. blue point 5 and green point 6). Point 5 corresponds to an image of a car driving on a highway, while most other car images are static scenes of parked cars. Point 6 is a long-necked ostrich that is quite different in appearance from other birds. However, these points are hard to identify in the t-SNE projection. The MDS result in Fig. 4 (b) is more effective than t-SNE for showing the pointwise relationship, but the clustering effect is apparently weaker than t-SNE and MFM. In addition, MDS tends to distribute the points quite evenly in the projected space, which compromises the display of in-set distribution and outliers. The DCM method shows the contextual set relationship better than t-SNE and displays set outliers only slightly better than MDS, since for the image modality, both DCM and MDS use metric loss, but MFM achieves better effect in both aspects. Additionally, we compute Z-Score for a data point to help verify whether a visual outlier or border point is indeed so in the original high-dimensional space (see supplementary for detail). We find that point 6, the most obvious outlier in Fig. 4 (d) indeed has the highest Z-score of 1.2379.\nFig. 5 shows the inter-modal case with the projection of CLIP text embeddings of three text queries together with the image embeddings of the query results. Regarding the DCM results (Fig. 5 (a)), it does successfully merge the modality. However, as shown in both cases, DCM has a significant weakness in that it scatters out the image em- bedding points quite evenly across the space, making it difficult for users to find distributional differences between different regions. In comparison, for example, in Fig. 5 (b), we can find an obvious dense cluster in MFM results which contain many similar images of bear in the wild, but this pattern is less evident in DCM results. In addi- tion, MFM also more clearly shows the relation between concepts than DCM, as we can find that the query results of bear and person and bear have obvious overlap in both DCM and MFM results, signifying the closer relationship between these two text concepts, but the relative position of text embeddings in MFM is more coherent to this relation than DCM.\nQuantitative Evaluation. To show some quantitative evidence of the advantage of MFM, we evaluate the method on COCO dataset using the trustworthiness metric and continuity metric [34, 62], where the former calculates how faithfully the projected kNN reflects the true kNN\u2019s in the embedding space and the latter calculates how well the original high-dimensional kNN is preserved in the projection. Particularly, we calculate both inter-modal and intra-modal kNN for k=30. However, in our scenario the inter-modal metric is more important as it measures the methods\u2019 ability to preserve multi-modal embedding structure. For the evaluation process, we perform multiple rounds (r = 500) of evaluation where in each round we randomly sample 500 images from COCO and project them together with the 80 category text embeddings in COCO object labels. The final metric is the average of the results in all rounds. As shown in Table 1, the experimental results indicate that MFM method performs consistently better than all the other methods in inter-modal truthworthiness and continuity, with higher than 2% margin over the strongest baseline DCM. In addition, MFM also achieves good performance in intra-modal metrics, only second to t-SNE. NDCM [13] is another fusion method using fully nonmetric objective. We can see that among the three fusion methods (MFM, DCM and NDCM), our MFM is consistently better across inter-modal and intra-modal metrics, while fully nonmetric fusion method has significant disadvantage in keeping the intra-modal features. We also need to note that the inter- modal metrics for non-fusion traditional methods like t-SNE and MDS cannot fully reflect their weakness in inter-modal scenarios because the modality gap will cause large distances between image embeddings and text embeddings in the projection space, making it difficult to perceive the differences between the inter-modal distances [39, 46, 71]."}, {"title": "ModalChorus SYSTEM", "content": "5.1 Visualization Interface\nSettings Panel. The settings panel (Fig. 6 (a)) allows users to specify some basic settings for their exploration, including tasks and inputs. Users can also select specific concepts in their input to produce con- textual visualization in the projection view. Instead of relying solely on textual concepts explicitly extracted from existing text labels or prompts, ModalChorus extracts implicit concepts from images to pro- vide a comprehensive display of concepts. To achieve this, we first leverage BLIP-2 [38], a multi-modal language model capable of receiv- ing images as input and generating textual descriptions of those images. We then employ the TopicRank [6] algorithms to extract candidate visual concepts based on the text generated by BLIP-2.\nProjection View. The projection view (Fig. 6 (b)) is the main view of the system leveraging our proposed Modal Fusion Map to help users probe the embedding with different tasks and data. Users can choose to turn on or turn off the contour to emphasize set relation or facilitate instance exploration respectively. The projection view also includes an instance retrieval subview below (Fig. 6 (c)). Users can mouse over the embedding point to see the corresponding image in the gallery. They can also click the point to retrieve similar images to the selected one. In addition, users can select a subset of points by lasso or ctrl-click, as shown in Case 2 and Fig. 10 in Sect. 6.\nConcept Axis View. As shown in Fig. 6 (d) and Fig. 7 (a), the concept axis view supports axis-based exploration of image embeddings in relation to text embeddings for user-selected concepts from the settings panel. Users can define one-end axis with a single concept (e.g., bridge) or two-end axis with opposing concepts they want to contrast (e.g., Monet and Van Goph). For one-end axis, the position of an image embedding point x is:\n$\\mu_A(x) = l \\frac{sim(x,A) - min(sim(x,A))}{max(sim(x,A)) - min(sim(x,A))}$, (4)\nwhere sim(x, A) denotes the cosine similarity between x and text em- bedding of concept A in embedding space, l is the length of the axis. For two-end axis, the position of x is calculated as l (0.5+ \u00b5A(x)+\u00b5B(x)). When users define more than one axis, we use curves connecting the same instance on two axes to show the correlation. Histogram is also used to help users see the overall distribution. Apart from displaying instances of image embeddings, the concept axis can also represent the whole set or subset as small box at the average position of all the in-set points, showing users the mean value of the set and supporting further set-based alignment interaction as described in Fig. 8 and Sect. 5.2. We also allow users to switch to a scatterplot visualization (Fig. 7 (b)).\nAugmentation Panel. The data augmentation panel (Fig. 6 (e)) sup- ports interactive augmentation of alignment data. In some alignment scenarios, users cannot find proper alignment data from the original dataset (for example, users may not find any satisfactory results gen- erated by a pre-trained generative model). For such a problem, the augmentation panel first allows users to upload a subset of samples to supplement the alignment data. For unlabeled raw image data up- loaded, this panel also integrates an auto-tagging function based on CLIP-interrogator [1], which can generate tags associated with the image to enhance the alignment performance. Second, in cases where users even find it difficult to collect their own data, the augmenta- tion panel also incorporates a generation function that enables users to leverage the weighted sum of existing text embeddings [14, 60] in a pre-trained generation model to synthesize more candidates of intention-aligned samples."}, {"title": "Interactive Alignment", "content": "Alignment Interaction Design. We design a series of visual alignment interactions, which allow users to intuitively express diverse alignment intentions through visual metaphor and trigger backend fine-tuning without writing complex training code. As shown in Fig. 8, our interac- tion scheme supports set and point level intentions for alignment. First, the common types of alignment mainly concern data points or subsets of points, which we categorize into two types: point-set alignment and set-set alignment, as shown in Fig. 8. First, point-set alignment encompasses various scenarios of aligning a set with a data point and vice versa. For example, when users want to align a subset of retrieval results with a query text embedding or when users want to align a prompt embedding to fine-tune samples provided by themselves. As shown in Fig. 8 (a), point-set alignment can be performed on either the projection view or the concept axis. Formally, the high-level idea of point-set alignment can be summarized as follows: Suppose we have a CLIP-based model F(\u00b7) which can map input text or image to different sets C1, C2, ..., CN in the sampled data. For example, in classification, Ci corresponds to the set of embeddings for a predicted class; in retrieval and generation, Ci corresponds to the set of embeddings for the results of a single query or prompt. Given a user-selected misaligned image or text point p, the target of point-set alignment is to tune the weights of F(\u00b7) such that F(p) is closer to the correct set Ci in the embedding space. Although the concrete implementation may vary for different tasks, in terms of the merged distance matrix, the effect is equivalent to achieving the following contrastive objective:\n$\\frac{1}{|C_i|} \\sum_<{C_i} M_{F(p),v} < \\frac{1}{\\sum_{j \\neq i, |C_j|}} \\sum_{u<{C_j}} M_{F(p),u}$ (5)\nwhere the estimated distance between F(p) and Ci should be smaller than any other set Cj. Second, set-set alignment involves moving two subsets of points closer or further in the concept axis or projection view. Such alignment is intended to close the gap between two sets or distributions in the embedding space, or contrast two sets for dis- tinguishing them better, which can be useful for cases like merging or disentangling concepts in retrieval or generation. As shown in Fig. 8 (b), in the projection view, they can drag a set contour towards another, while in the axis view, they can drag one set box closer to or away from another. Formally, the high-level idea of set-set alignment is: Suppose users identify a misaligned set or subset of embeddings Ce , where Ce is not align with the input p. Next, users find another correct set Ci either by visual exploration of other projected data points or by data augmentation. The goal of set-set alignment can then be formulated as:\n$\\frac{1}{|F(p)|} \\sum_<{Ce} M_{i,v} > \\frac{1}{|F(p)|} \\sum_<{Ci} M_{i,v}$. (6)\nAlignment Fine-tuning Implementation. Our system provides a general framework to map users' visual interactions shown in Fig. 8 to backend fine-tuning operations that align the model\u2019s output in the embedding space. As the visual representations are decoupled from actual backend implementation, our framework can incorporate any kind of specific fine-tuning methods. For demonstration purposes, our study implements two methods. First, for the classification and retrieval cases, we implement triplet loss [54] based alignment. Second, for the generation cases, we implement the low-rank adaptation method [31]. More detail is provided in the supplementary material."}, {"title": "CASE STUDIES", "content": "In this section, we perform three case studies to demonstrate the useful- ness of the Modal Fusion Map and ModalChorus system, which cover three different tasks based on multi-modal embeddings, including zero- shot classification, text-to-image retrieval, and generation. Particularly, we demonstrate how our visual probing integrates with and enhances interactive few-shot alignment [21, 32].\n6.1 Case 1: Zero-shot classification\nIn this case, we demonstrate how our system can be used to visualize the zero-shot classification [48] based on multi-modal embedding cluster- ing and help refine the embedding interactively with one-shot point-set alignment. Specifically, we use the CIFAR-10 image classification dataset [36] to show an example. Here we suppose no ground-truth labels are available. This is to simulate real-time analysis of zero-shot embedding-based classification in the wild for unknown data, where interactive visual analysis with human intervention is most helpful.\nUsers first select the classification task and the dataset. Then, users subjectively select some classes that they suspect may be confusing for CLIP, including classes of small wild animals and classes of vehicles. Specifically, they select 6 class concepts they want to explore, including airplane, automobile, truck, ship, bird and frog. Then, the system predicts the class of each image according to the cross-modal proximity between the image embedding and class text embedding. The system samples the data for visual probing. Specifically, it retrieves 50 closest"}, {"title": "DISCUSSION", "content": "Speed limitations. Our system and method have two limitations in terms of speed. First, even though the parametric method can scale to large datasets with shorter asymptotic time (as shown in supple- mentary), for smaller datasets, it is not as fast as some traditional methods like t-SNE. Second, the speed of the alignment fine-"}]}]}