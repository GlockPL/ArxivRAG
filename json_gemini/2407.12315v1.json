{"title": "ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map", "authors": ["Yilin Ye", "Shishi Xiao", "Xingchen Zeng", "Wei Zeng"], "abstract": "Multi-modal embeddings form the foundation for vision-language models, such as CLIP embeddings, the most widely used text-image embeddings. However, these embeddings are vulnerable to subtle misalignment of cross-modal features, resulting in decreased model performance and diminished generalization. To address this problem, we design ModalChorus, an interactive system for visual probing and alignment of multi-modal embeddings. ModalChorus primarily offers a two-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel parametric dimensionality reduction method that integrates both metric and nonmetric objectives to enhance modality fusion; and 2) embedding alignment that allows users to interactively articulate intentions for both point-set and set-set alignments. Quantitative and qualitative comparisons for CLIP embeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and data fusion (e.g., data context map) methods demonstrate the advantages of MFM in showcasing cross-modal features over common vision-language datasets. Case studies reveal that ModalChorus can facilitate intuitive discovery of misalignment and efficient re-alignment in scenarios ranging from zero-shot classification to cross-modal retrieval and generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Neural embeddings are high-dimensional latent representations for knowledge captured from self-supervised pre-training, such as word embeddings and image embeddings. Recently, multi-modal (e.g., text and image) embedding are playing a pivotal role for advancing multi-modal AI models. This type of embeddings learns a joint representation space that encodes different modalities and their relationships, forming the basis for cross-modal tasks such as text-to-image retrieval and generation [3, 49, 66]. The performance of multi-modal embedding models rely heavily on the quality of multi-modal alignment, which seeks to match data with corresponding semantics across different modalities within the embedding space [22, 23, 48]. However, misalignment in multi-modal embeddings is common due to the intricate many-to-many mapping among concepts in different modalities. For instance, text-to-image embeddings can easily encounter misalignment issues of concept entanglement. As illustrated in Figure 1(a), the text prompt of 'waterlily pond by Monet' becomes entangled with the 'bridge' concept in the image modality, reducing the diversity of generated images.\nIdentifying misalignment in multi-modal embeddings is crucial for enhancing model performance. Existing methods for evaluating mis-alignment often rely on reference-based evaluations (e.g., CIDEr [57] and SPICE [2]) that necessitate extensive human-labeled references, or reference-free metrics (e.g., CLIPScore [28]) derived from pretained multi-modal models. Despite not relying on references, reference-free metrics are reliant on pretrained models, making it challenging for fully automatic methods to detect misalignment in diverse and context-dependent scenarios. For instance, the CLIPScore for the text prompt 'waterlily pond by Monet' fails to reflect the issue of concept entanglement, as the CLIP model itself is biased towards the 'bridge' concept in the image modality. Hence, existing fine-tuning techniques to improve alignment often fall short of expectations in numerous scenarios. There is a need for an interactive visualization tool to help users intuitively investigate and address misalignment.\nHowever, the intricate data structures and feature characteristics inherent in multi-modal embeddings pose particular challenges for visual probing and interactive alignment. A key challenge arises from the modality gap, wherein embedding vectors from different modalities are essentially disjointed in the joint embedding space [39]. To achieve cohesive visualization of multi-modal embeddings, it is essential to address the modality gap issue and unify the presentation of different modalities within a single display space. Previous visualizations of neural embeddings have primarily centered on single-modal embeddings, such as word or image embeddings [26,41,42]. Notably, these works commonly employ classical dimensionality reduction (DR) methods like t-SNE [56] and MDS [5], which are limited to separately displaying multi-modal embeddings in distinct spaces. Fusion-based DR methods (e.g., [10, 13]) offer a potential solution to jointly project embeddings from different modalities. However, these methods typically treat intra-and inter-modal distances equally, without giving special consideration to cross-modal relations. For example, Data Context Map (DCM) [10] solely relies on metric-based objectives that poorly capture the relative rank order of inter-modal distances. As illustrated in Figure 5(left), DCM projects rather even distribution of image embedding points around the textual concepts, making it harder to observe differences in distribution pattern.\nMoreover, enabling interactive alignment for multi-modal embeddings presents another challenge, primarily due to two reasons. First, user-intended alignment strategies encompass diverse operations. For example, in Figure 1, upon identifying the concept entanglement between 'Monet' in the text modality and 'bridge' in the image modality, users may prefer to drag the 'bridge' point far away or relocate the entire set of 'Monet' images. However, existing studies often focus on point-based operations [16, 62], while others solely support set-based interaction [19]. Secondly, users would utilize interactive alignment to refine the underlying models and ensure that the refined model performs as expected, as illustrated by the disentangled images generated post-alignment, as in Figure 5(d). Existing studies on DR refinement mostly focus on adapting the projection layout [16, 19, 58, 62], whilst overlooking the model refinement.\nTo fill the gap, we present ModalChorus, an interactive system that supports visual probing and alignment of multi-modal embeddings. ModalChorus mainly comprises two-stage exploration. First, in the embedding probing stage, we propose Modal Fusion Map (MFM), a novel parametric DR method integrating metric and non-metric objectives for enhanced modality fusion. By taking the advantages of metric-based objectives in preserving the intra-modal distances and non-metric-based objectives in capturing inter-modal distance rank order [5, 13], MFM effectively addresses the modality gap challenge induced by multi-modal embeddings. Compared with conventional single-modal and fusion-based DR methods, MFM achieves higher trustworthiness and continuity regarding inter-modal relations (see Table 1), and can better visually reflect the intra- and inter-modality contextual distributions (see Figures 4 & 5). Next, in the embedding alignment stage, to accommodate the diverse alignment scenarios, we design an alignment interaction scheme that allows for alignment on multiple levels including point, subset, and set. The interaction scheme is integrated with MFM encompassing point-set and set-set alignment. Besides, a concept axis view is also developed to enable linear visual representation for the probing and alignment of multi-modal embeddings.\nIn summary, our make the following contributions:\n\u2022 We propose Modal Fusion Map (MFM), a novel dimensionality reduction method tailored for fusion projection of multi-modal embeddings. The effectiveness of MFM is demonstrated using both quantitative and qualitative evaluations.\n\u2022 We develop ModalChorus, an interactive system that supports visual probing of multi-modal embeddings to discover misalignment, along with an interaction scheme that supports interactive fine-tuning of the underlying multi-modal embedding models.\n\u2022 We show the effectiveness of our system through case studies on three embedding-based cross-modal tasks, ranging from zero-shot classification to cross-modal retrieval and generation."}, {"title": "2 RELATED WORK", "content": "Visualization for Neural Embeddings. Deep learning relies on neural networks that are often pre-trained on large amounts of data. Neural embeddings are the foundational high-dimensional feature representation of raw data encoded by neural networks, such as text embeddings like word2vec [45] and BERT [15] and image embeddings like Sim-CLR [9]. Visualization researchers have dedicated significant efforts to enhance the comprehension of neural embeddings. Previous studies primarily focus on unimodal embeddings, encompassing word embeddings [26,27,41] and image embeddings [42]. Many of these studies integrate projection methods with axis-based [26,41,42] or set-based [27] exploration techniques. For example, Liu et al. [41] identified analogy axis between multiple pairs of words with the same semantic transition in word embeddings projected by t-SNE. Latent Space Cartography [42] extends the concept of semantic axis to customized axis defined by users, which can be applied to exploration of both unimodal word embeddings and image embeddings. EmbComp [27] combines t-SNE projection with visualization of neighborhood set overlap to compare different word embedding models.\nRecently, multi-modal embeddings such as CLIP [48] and ALIGN [33] have fueled the advances in multi-modal AI such as text-to-image generation. These embeddings can encode data from different modalities in a joint space, contributing to various applications such as cross-modal image retrieval [4] and generation [49]. However, this integration also introduces modality gap [39] that signifies discrepancies between different modal embeddings, complicating the comprehension of multi-modal embeddings. There is a lack of visualization tool tailored to the task. Specifically, the task demands an effective visualization method for probing multi-modal embeddings and an interactive scheme for improving alignment of multi-modal embeddings. To meet the goal, we propose the Modal Fusion Map that can better preseve the contextual information of multi-modal embeddings, and an interactive alignment scheme that offers visual steering for modal alignment.\nContextual Dimensionality Reduction. Dimensionality reduction for multi-modal data has been a challenging problem as traditional DR methods like t-SNE [56], PCA [59], and MDS [5] cannot account for cross-modal relations due to the modality gap [39,46,71]. Contextual visualization is a type of DR method designed to project data points in relation to attribute points [10, 44, 69], which can be applied to multi-modal data projection. Existing contextual visualizations can be categorized into two types: anchor-based projection and fusion-based methods. Anchor-based methods employ a two-stage approach, initially determining the layout of points in one modality before calculating the position of points in the other modality. For example, the RadViz method [11,29,65] first lays out the attribute points on a circle and then projects the data points based on their multi-dimensional attribute values. However, the structure of the embedding space can be significantly distorted due to the challenge of optimally laying out the anchor points. One category of fusion methods, known as co-embedding methods [12, 64], introduces their own high-dimensional representations of multi-modal data or modifies the embeddings of certain data points to achieve a desired visual layout. However, these methods diverge from our goal as they alter the original embeddings with custom models, which cannot help users understand commonly used multi-modal embeddings in AI tasks. Visualization researchers have developed more general fusion methods [10, 69]. Particularly, Data Context Map (DCM) [10] defines the distance matrix for the attributes and merges it with the data distance matrix before using MDS to jointly project the attribute points and data points. However, in DCM, intra-modal and inter-modal distances are equally treated in metric-based optimization, which limits its ability to adequately capture the cross-modal non-metric ordinal structure of multi-modal embeddings. In our study, we introduce the Modal Fusion Map, which integrates both metric and nonmetric objectives into modality fusion using a novel parametric DR method, to effectively preserve relationships for intra- and inter-modal distances of multi-modal embeddings.\nVisual Steering for Modal Alignment. Pre-training of multi-modal foundation embedding models relies on alignment through methods like contrastive learning, such as ViLBERT [43] and CLIP [48]. For example, the CLIP embeddings [48] is pre-trained on large-scale image-text pair corpus by matching image and text caption in a joint representation space with contrastive multi-class N-pair loss. The pre-training methods typically aim to establish a foundational model, yet the varying quality of pre-training data often leaves some misalignment in specific cases, which requires adaptation such as few-shot fine-tuning [20,31,46] to re-fine the alignment. Misalignment cases may require human knowledge to be discovered, and the fine-tuning process also typically involves users' choice of alignment data and direction, which necessitates an interactive system to support human-in-the-loop workflow. This scenario differs from interactive prompt engineering of pre-trained models [18, 55, 61], as prompt engineering only seeks to alter the input without refining the model, which is not enough for steering complex multi-modal models with misalignment.\nSome previous visual analytics systems support interactive improvement of AI models through label correction or data augmentation [7, 24, 25]. For example, VATLD [24] leverages disentangled representation learning for semantic exploration of traffic light detection results in relation to explainable data dimensions. However, these studies only focus on task-specific models without paying attention to foundational embeddings. Many studies also rely on the ground-truth labels for insight discovery, which may not be available in real-time probing of pre-trained models. In addition, these studies lack support for visual steering interaction directly in the visualization space, which is more intuitive for the alignment operation our study aims at.\nSome visualization researchers have studied interactive visual steering of dimensionality reduction results [16,19,58,62]. For example, Xia et al. [62] proposed a contrastive learning-powered parametric dimension reduction method to support point-level interaction to enhance the visual clustering effect. ULCA [19] supports set-level visual steering interaction for comparative analysis. DRAVA [58] introduces an interaction method to adjust the positions of small multiples in axis-based visualization based on \u1e9eVAE. However, these interactions only focus on refining the projection layout for visual exploration purposes, lacking the ability to align the underlying models or high-dimensional representations. In addition, the interaction schemes of most previous studies are limited to a single type of interaction, such as point-based or set-based interaction in a single view, which cannot cover the diverse alignment scenarios of multi-modal embeddings. In our study, we develop an interaction scheme supporting point-set and set-set alignments, which enables flexible alignment of underlying embedding-based models."}, {"title": "3 OVERVIEW", "content": "3.1 Background and Domain Problem\nMulti-modal embedding. Multi-modal embedding models are pre-trained encoder models for the representation of multi-modal data. For example, the CLIP model is pre-trained on a large corpus of image-text pairs, using transformers and vision transformers to first separately encode text and image into high-dimensional vectors. Then, through a linear transformation, the text embedding and image embedding vectors are aligned in a shared embedding space with contrastive loss. Multi-modal embeddings are the foundational encoder for many AI tasks that involve multi-modal data in its input and/or output. Common tasks include semantics-based image classification [48], cross-modal retrieval [23], and text-to-image generation [8,50].\nAlignment. In multi-modal models, alignment means the matching of data representations with corresponding semantics from different modalities. In the pre-training stage of CLIP, for example, the alignment is achieved by updating the embeddings of an image and its corresponding text caption so that they are closer than incorrect pairs in the high-dimensional representation space. However, due to the varying quality and large quantity of data in pre-training and imperfection in training algorithms, there may be misalignment in the pretrained model, which requires further adaptation for enhancing alignment [20, 31, 46].\nMulti-level Alignment. There is mainly alignment on three levels: point, subset, and set, requiring two types of alignment: point-set alignment and set-set alignment. Specifically, users may discover misalignment of an individual point (e.g., misclassified image point or misunderstood text point), a subset (e.g., a subset of incorrect samples in the whole set of text-to-image retrieval results), and a set (e.g., biased or entangled generation results of text-to-image models), requiring different alignment operations. To clarify, we refer to keywords extracted by our system or entered by users as concepts, which are the main text embeddings we focus on in this study for contextual exploration of embeddings, while particular image point is referred to as instances.\nProblem: visualization for embedding. Many visualization studies treat embedding methods as a tool for processing data, with the aim of optimizing the visual display of raw input data. That is, these visualizations regard embedding as a projection method. Instead, in the field of AI, representation learning of single and multi-modal embeddings has been playing a pivotal role for various downstream tasks [37,51]. The high-dimensional embeddings themselves are the key intermediate representations of data extracted from raw text or pixels, not just the representation for visual display only. To gain insight into large AI models, particularly for the alignment problem, high-dimensional visualization methods should prioritize capturing the features of the foundational embeddings itself (G1). For example, suppose two classes of images are indeed close in the embedding space, which signifies risks of misalignment in the embedding. In that case, we do not wish to maximize the class separation in projection space just for visual display since it can mislead users. For another example, if the prompt's text embedding in the generation model is not close to the generated images, we should be cautious about directly putting the text at the centroid of the generated image set, which may lead users to believe that the generation is fully aligned.\nTo summarize, the former studies focus on embedding for visualization, while our work aims at visualization for embedding. In addition, the aim for interaction in this scenario is to improve the foundational high-dimensional embeddings (G2) instead of improving the visual display of data like previous studies did [19,62]."}, {"title": "3.2 Challenges and Design Requirement", "content": "Accomplishing these two goals is challenging for existing visualization methods, particularly due to:\nC1 Modality Gap. The heterogeneous distributions of different modalities in the joint embedding space result in the modality gap [39,71], making it difficult for existing DR methods to simultaneously capture intra-modal and cross-modal features.\nC2 Diverse alignment intentions. The diverse alignment scenarios in different cross-modal tasks post challenges to designing a comprehensive interaction scheme integrated into the visualization.\nTo tackle the challenges, we summarize the design requirements of ModalChorus, which should support flexible and effective R1) visual probing of multi-modal embeddings to meet G1.\nR1.1 Accurately preserving inter- and intra-modal distances. An effective fusion-based DR method is needed to bridge the modality gap while maximally preserving inter- and intra-modal relations.\nR1.2 Effective visual presentation to help identify misalignment. Apart from the projection, effective graphical enhancement is needed to assist discovery of misalignment issues such as mis-classification or entanglement.\nSecond, ModalChorus shall facilitate R2) interactive alignment of multi-modal embeddings to support G2:\nR2.1 Supporting alignment on point and set levels. Users may discover embedding misalignment on an individual data point or a whole set of points, demanding different types of alignment interaction, including point-set and set-set alignment.\nR2.2 Supporting axis-based alignment. Previous embedding visualization studies have identified the semantic axis as an effective complement of the overall projection for more focused concept-related exploration [26,41,42]. Besides directly manipulating the projection of embeddings, users also need to perform axis-based alignment as the axis can more clearly show the direction of alignment with respect to a specific semantic concept.\nR2.3 Supporting data augmentation. When users discover misalignment but cannot find correct reference data, they would like to provide extra data and process it to help the alignment."}, {"title": "3.3 Modal Chorus Overview", "content": "An overview of our system is shown in Fig. 2, which mainly consists of two stages: 1) embedding probing and 2) embedding alignment. In the first stage, starting from a particular dataset and task, along with user-provided input or automatically extracted concept, we support visual probing of the embeddings with sampled data for interpretation of embeddings and discovery of misalignment. Particularly, we develop Modal Fusion Map, a novel parametric fusion method that integrates metric and nonmetric objectives for multi-modal embedding projection. We also incorporate a concept axis view that allows users to explore the correlation of image embeddings in relation to concept text embeddings. An additional instance gallery displays similar images to the selected image point in the embedding space for neighborhood exploration.\nIn the second stage, upon discovering misalignment, we enable users to select a particular point, subset, or set and perform point-set alignment or set-set alignment in either the projection view or the concept axis view. In some cases, when new data is needed to enhance the alignment, we allow users to upload their collected data for few-shot alignment or use our system's weighted embedding generation function to generate candidate augmentation data. Finally, the visual alignment operations are mapped to the backend fine-tuning."}, {"title": "4 MULTI-MODAL CONTEXTUAL VISUALIZATION", "content": "In this section, we describe Modal Fusion Map, a novel DR method we propose to address R1 visual probing of multi-modal embedding.\n4.1 Problem Identification\nTo address the modality gap problem in multi-modal embedding visual-ization, data matrix fusion methods [10, 13] are a promising solution. Matrix fusion methods such as Data Context Map (DCM) [10] are derived from the MDS method for distance-based fusion. The original Data Context Map is designed for the attribute and data spaces of multi-dimensional data. Specifically, to align data points from different modalities, it constructs a large distance matrix containing the pairwise distances between all the data points and attribute points, where the intra-modal distance is the original high-dimensional distance such as Euclidean or Cosine distance while the cross-modal distance needs to be defined according to data properties. For example, DCM defines the distance between attribute point and data point as 1 - v, where v is the data point's value in this attribute dimension.\nFirst, to account for high dimensional latent space, we can naturally change the attribute-data distance in DCM to the Cosine distance between text embedding and image embedding. However, this modification may not suffice for the complexity of multi-modal embedding. Specifically, to enhance the modality merging effect, it is important to flexibly adjust the weights of intra-modality and inter-modality distance. Directly scaling the submatrix as mentioned in [13] may have the risk of significantly distorting the embedding space or exacerbating the modality gap. More importantly, when multi-modal embeddings like CLIP are used for cross-modal tasks such as text-to-image retrieval, the absolute distance between the text and image embeddings is less important than the relative order of the distance values. In visualization of multi-modal embeddings, such characteristics should also be considered. This means that we should develop a better fusion method that considers both metric and non-metric objectives [17,30,47]."}, {"title": "4.2 Modal Fusion Map", "content": "Dimensionality Reduction. Inspired by recent work on parametric dimensionality reduction [62, 63, 67], to satisfy the projection require-ments presented above, we propose Modal Fusion Map (MFM) which can flexibly combine different objectives for joint multi-modal embedding projection. The hypothesis is that in the high dimensional embedding space, there exists a subspace or manifold surface S be-tween the text embeddings set T and image embeddings set I, such that the projection of embeddings from both modalities on this surface (P(T), P(I) \u2208 S) can result in an optimized 2D parametric represen-tation S(x, y). Specifically, like other matrix-based methods, we first compute the merged distance matrix:\n\\(M =\n\\begin{pmatrix}\nII & IT \\\nTI, & TT\n\\end{pmatrix}\\)\nwhere II is image distance submatrix, TT is text distance submatrix, IT = TII is the cross-modal distance submatrix, all using cosine dis-tance. Each submatrix is normalized by their mean value.\nNext, instead of directly applying the traditional MDS method to the merged matrix as in DCM, we parametrize the projection with a three-layer feed-forward neural network mapping 512 or 1024-dimensional CLIP embedding to the 2-dimensional projection space. See supplementary material for more detail. Then, to implement the MDS objective, we construct a loss function using the Pearson correlation between the high dimensional merged distance matrix and the projected distance matrix for scale-free optimization.\n\\(L_{M} = \\frac{\\sum(M_{i,j} - \\overline{M})(P_{i,j} - \\overline{P})}{\\sqrt{\\sum (M_{i,j} - \\overline{M})^2 \\sum(P_{i,j} - \\overline{P})^2}}\\)\nwhere P stands for the distance matrix of the projected points.\nIn this way, we can easily define loss terms for the intra-modality and inter-modality submatrices, denoted as LTT, LII, and LIT, respectively. Accordingly, the loss function for metric MDS is the weighted sum. In our case, we only consider the overall term and the cross-modal term: L\u2081 = w\u2081LM+w2LIT, where we set w\u2081 = 10, w2 = 2.\nIn addition, for the nonmetric loss to preserve cross-modality dis-tance order, we further introduce another loss term:\n\\(L_{2} = \\frac{-\\sum_{j<k} f((TI_{i,j} - TI_{i,k}) * (P(TI)_{i,j} - P(TI)_{i,k}))}{||P(TI)||^{2}}\\)\n\\(f(x) = \begin{cases}\n0, & x \\geq 0 \\\\\n-x, & x < 0\n\\end{cases}\\)\nwhere this loss term will be zero when all the cross-modal distance order is preserved in the projection. The final loss L = L\u2081 + \u03b1L2, a = 0.05. W1, W2, a are selected empirically.\nContour-based graphical enhancement. We provide graphical en-hancements in the form of density contour as inspired by recent work [69]. As shown in Fig. 5, the density plot can show the default KDE density estimation of data point distribution. The KDE contour can serve as a graphical representation of sets in the projection view, which can facilitate subsequent alignment interaction as we describe below. Alternatively, when users provide customized metrics defined for the data points, such as CLIP-Score for generated samples, the density plot can show the kernel estimation of the metric value distribution."}, {"title": "4.3 Evaluation", "content": "Qualitative Comparison. As shown in Fig. 4 and Fig. 5, MFM has many advantages for displaying both intra-modality and inter-modality features compared to the DCM method and traditional projection methods like MDS and t-SNE. Specifically, Fig. 4 displays an intra-modal case with the projection of CLIP image embeddings for samples of 6 classes in CIFAR-10 dataset. The colors represent the zero-shot classification results based on CLIP. Among the results, we can see that t-SNE achieves the best separation effect. However, t-SNE also has significant drawbacks in understanding the embeddings and identifying misalignment because it does not consider cross-modal features. First, t-SNE is weaker at showing contextual information, such as the relation between different sets. For example, we can find in Fig. 4, the frog set (green point 1) can be confused with the bird set (yellow point 2) because of similar color or background, yet the t-SNE projection does not clearly show the relation compared to MFM. In addition, our joint projection also shows better within-set distribution than t-SNE. For example, with MFM, we can clearly see outliers or border points within sets (e.g.blue point 5 and green point 6). Point 5 corresponds to an image of a car driving on a highway, while most other car images are static scenes of parked cars. Point 6 is a long-necked ostrich that is quite different in appearance from other birds. However, these points are hard to identify in the t-SNE projection. The MDS result in Fig. 4 (b) is more effective than t-SNE for showing the pointwise relationship, but the clustering effect is apparently weaker than t-SNE and MFM. In addition, MDS tends to distribute the points quite evenly in the projected space, which compromises the display of in-set distribution and outliers. The DCM method shows the contextual set relationship better than t-SNE and displays set outliers only slightly better than MDS, since for the image modality, both DCM and MDS use metric loss, but MFM achieves better effect in both aspects. Additionally, we compute Z-Score for a data point to help verify whether a visual outlier or border point is indeed so in the original high-dimensional space (see supplementary for detail). We find that point 6, the most obvious outlier in Fig. 4 (d) indeed has the highest Z-score of 1.2379.\nFig. 5 shows the inter-modal case with the projection of CLIP text embeddings of three text queries together with the image embeddings of the query results. Regarding the DCM results (Fig. 5 (a)), it does successfully merge the modality. However, as shown in both cases, DCM has a significant weakness in that it scatters out the image embedding points quite evenly across the space, making it difficult for users to find distributional differences between different regions. In comparison, for example, in Fig. 5 (b), we can find an obvious dense cluster in MFM results which contain many similar images of bear in the wild, but this pattern is less evident in DCM results. In addition, MFM also more clearly shows the relation between concepts than DCM, as we can find that the query results of bear and person and bear have obvious overlap in both DCM and MFM results, signifying the closer relationship between these two text concepts, but the relative position of text embeddings in MFM is more coherent to this relation than DCM.\nQuantitative Evaluation. To show some quantitative evidence of the advantage of MFM, we evaluate the method on COCO dataset using the trustworthiness metric and continuity metric [34, 62], where the former calculates how faithfully the projected kNN reflects the true kNN's in the embedding space and the latter calculates how well the original high-dimensional kNN is preserved in the projection. Particularly, we calculate both inter-modal and intra-modal kNN for k=30. However, in our scenario the inter-modal metric is more important as it measures the methods' ability to preserve multi-modal embedding structure. For the evaluation process, we perform multiple rounds (r = 500) of evaluation where in each round we randomly sample 500 images from COCO and project them together with the 80 category text embeddings in COCO object labels. The final metric is the average of the results in all rounds. As shown in Table 1, the experimental results indicate that MFM method performs consistently better than all the other methods in inter-modal truthworthiness and continuity, with higher than 2% margin over the strongest baseline DCM. In addition, MFM also achieves good performance in intra-modal metrics, only second to t-SNE. NDCM [13] is another fusion method using fully nonmetric objective. We can see that among the three fusion methods (MFM, DCM and NDCM), our MFM is consistently better across inter-modal and intra-modal metrics, while fully nonmetric fusion method has significant disadvantage in keeping the intra-modal features. We also need to note that the inter-modal metrics for non-fusion traditional methods like t-SNE and MDS cannot fully reflect their weakness in inter-modal scenarios because the modality gap will cause large distances between image embeddings and text embeddings in the projection space, making it difficult to perceive the differences between the inter-modal distances [39, 46, 71]."}, {"title": "5 ModalChorus SYSTEM", "content": "5.1 Visualization Interface\nSettings Panel. The settings panel (Fig. 6 (a)) allows users to specify some basic settings for their exploration, including tasks and inputs. Users can also select specific concepts in their input to produce contextual visualization in the projection view. Instead of relying solely on textual concepts explicitly extracted from existing text labels or prompts, ModalChorus extracts implicit concepts from images to provide a comprehensive display of concepts. To achieve this, we first leverage BLIP-2 [38], a multi-modal language model capable of receiving images as input and generating textual descriptions of those images. We then employ the TopicRank [6] algorithms to extract candidate visual concepts based on the text generated by BLIP-2.\nProjection View. The projection view (Fig. 6 (b)) is the main view of the system leveraging our proposed Modal Fusion Map to help users probe the embedding with different tasks and data. Users can choose to turn on or turn off the contour to emphasize set relation or facilitate instance exploration respectively. The projection view also includes an instance retrieval subview below (Fig. 6 (c)). Users can mouse over the embedding point to see the corresponding image in the gallery. They can also click the point to retrieve similar images to the selected one. In addition, users can select a subset of points by lasso or ctrl-click, as shown in Case 2 and Fig. 10 in Sect. 6.\nConcept Axis View. As shown in Fig. 6 (d) and Fig. 7 (a), the concept axis view supports axis-based exploration of image embeddings in relation to text embeddings for user-selected concepts from the settings panel. Users can define one-end axis with a single concept (e.g., bridge) or two-end axis with opposing concepts they want to contrast (e.g., Monet and Van Goph). For one-end axis, the position of an image embedding point x is:\n\\(\\mu_{\\alpha}(x) = l \\cdot \\frac{sim(x, A) - min(sim(x, A))}{max(sim(x, A)) - min(sim(x, A))}\\)\nwhere sim(x,A) denotes the cosine similarity between x and text embedding of concept A in embedding space, I is the length of the axis. For two-end axis, the position of x is calculated as l (0.5+ \u03bc\u03b1(x)+\u03bc\u03b2(x)). When users define more than one axis, we use curves connecting the same instance on two axes to show the correlation. Histogram is also used to help users see the overall distribution. Apart from displaying instances of image embeddings, the concept axis can also represent the whole set or subset as small box at the average position of all the in-set points, showing users the mean value of the set and supporting further set-based alignment interaction as described in Fig. 8 and Sect. 5.2. We also allow users to switch to a scatterplot visualization (Fig. 7 (b)).\nAugmentation Panel. The data augmentation panel (Fig. 6 (e)) supports interactive augmentation of alignment data. In some alignment scenarios, users cannot find proper alignment data from the original dataset (for example, users may not find any satisfactory results generated by a pre-trained generative model). For such a problem, the augmentation panel first allows users to upload a subset of samples to supplement the alignment data. For unlabeled raw image data uploaded, this panel also integrates an auto-tagging function based on CLIP-interrogator [1], which can generate tags associated with the image to enhance the alignment performance. Second, in cases where users even find it difficult to collect their own data, the augmentation panel also incorporates a generation function that enables users to leverage the weighted sum of existing text embeddings [14, 60] in a pre-trained generation model to synthesize more candidates of intention-aligned samples."}, {"title": "5.2 Interactive Alignment", "content": "Alignment Interaction Design. We design a series of visual alignment interactions", "types": "point-set alignment and set-set alignment", "follows": "Suppose we have a CLIP-based model F() which can map input text or image to different sets C1, C2, ..., CN in the sampled data. For example, in classification, Ci corresponds to the set of embeddings for a predicted class; in retrieval and generation, C\u00a1 corresponds to the set of embeddings for the results of a single query or prompt. Given a user-selected misaligned image or text point p, the target of point-set alignment is to tune the weights of F(\u00b7) such that f(p) is closer to the correct set C\u012f in the embedding space. Although the concrete implementation may vary for different tasks, in terms of the merged distance matrix, the"}]}