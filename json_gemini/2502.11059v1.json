{"title": "CLIMATELLM: EFFICIENT WEATHER FORECASTING VIA FREQUENCY-AWARE LARGE LANGUAGE MODELS", "authors": ["Shixuan Li", "Wei Yang", "Peiyu Zhang", "Xiongye Xiao", "Defu Cao", "Yuehan Qin", "Xiaole Zhang", "Yue Zhao", "Paul Bogdan"], "abstract": "Weather forecasting is crucial for public safety, disaster prevention and mitigation, agricultural production, and energy management, with global relevance. Although deep learning has significantly advanced weather prediction, current methods face critical limitations: (i) they often struggle to capture both dynamic temporal dependencies and short-term abrupt changes, making extreme weather modeling difficult; (ii) they incur high computational costs due to extensive training and resource requirements; (iii) they have limited adaptability to multi-scale frequencies, leading to challenges when separating global trends from local fluctuations. To address these issues, we propose ClimateLLM, a foundation model for weather forecasting. It captures spatiotemporal dependencies via a cross-temporal and cross-spatial collaborative modeling framework that integrates Fourier-based frequency decomposition with Large Language Models (LLMs) to strengthen spatial and temporal modeling. Our framework uses a Mixture-of-Experts (MoE) mechanism that adaptively processes different frequency components, enabling efficient handling of both global signals and localized extreme events. In addition, we introduce a cross-temporal and cross-spatial dynamic prompting mechanism, allowing LLMs to incorporate meteorological patterns across multiple scales effectively. Extensive experiments on real-world datasets show that ClimateLLM outperforms state-of-the-art approaches in accuracy and efficiency, as a scalable solution for global weather forecasting.", "sections": [{"title": "1 INTRODUCTION", "content": "For almost half a century, numerical weather prediction (NWP) methods that rely on solving atmospheric partial differential equations have formed the backbone of operational forecasting Kalnay (2002); Lynch (2008); Bauer et al. (2015); Nguyen et al. (2024). More recently, deep learning techniques have shown significant promise as complementary or alternative tools. By learning complex atmospheric patterns from large-scale data, they can sometimes outperform or supplement traditional NWP models without explicitly solving physical equations Pathak et al. (2022); Bi et al. (2023); Lam et al. (2023); Price et al. (2025); Verma et al. (2024). Benchmarks such as WeatherBench Rasp et al. (2024) have standardized data formats and metrics, facilitating direct comparisons across models and promoting reproducible research. Innovative approaches include neural diffusion equations Hwang et al. (2021), Climax Nguyen et al. (2023), and FourCastNet Pathak et al. (2022), each demonstrating distinct ways to capture atmospheric complexity using neural networks or transformers.\nDespite these advances, substantial challenges remain particularly in forecasting rare but disruptive events. First, many deep learning models demand significant computational resources and long training periods, which limits their practical use in operational settings. Second, extreme weather events appear infrequently in historical records, creating an imbalanced data distribution that makes accurate modeling difficult He & Garcia (2009). This problem becomes more complex because extreme weather events often involve unique physical mechanisms that differ markedly from typical weather patterns Donat et al. (2013). Third, non-local atmospheric teleconnections create additional complexity, as weather conditions in distant regions can significantly affect local weather patterns Gao et al. (2024). Standard error metrics that focus on average prediction accuracy often lead to models that do not effectively capture rare extreme events.\nTo address these challenges, we propose ClimateLLM, a framework that combines frequency-domain processing, dynamic prompting, and large language models (LLMs) for enhanced weather forecasting. At its core, our approach uses a two-dimensional Fast Fourier Transform (2D FFT) to analyze spatial patterns in the frequency domain, which helps capture both large-scale atmospheric circulation and local weather patterns. Meanwhile, We introduce a Frequency Mixture-of-Experts (FMoE) module that processes different frequency components using specialized experts, with particular focus on the frequency bands associated with extreme weather events. In addition, the framework employs a meta-fusion prompt design that dynamically guides the model's attention to relevant temporal and variable-specific features, facilitating better cross-variable correlations and temporal dependencies. These components are integrated with a Generative Pre-trained Transformer (GPT) backbone, which excels at modeling long-range temporal dependencies crucial for weather evolution. Our architecture significantly reduces computational requirements through efficient parameter reuse from pre-trained models and limited parameter updates during fine-tuning, making it more practical for operational deployment compared to traditional deep learning approaches that require training all parameters from scratch. To maintain accurate predictions across different regions, we use a latitude-weighted training approach that adjusts for the varying significance of different geographical areas.\nIn summary, the main contributions of this paper are as follows."}, {"title": "2 RELATED WORK", "content": "Deep learning-based weather forecasting models have demonstrated significant advantages over traditional numerical methods in multiple aspects Leinonen et al. (2023); Li et al. (2024); Salman et al. (2015); Hewage et al. (2021). FourCastNet Pathak et al. (2022) outperforms the Integrated Forecasting System in predicting small-scale variables such as precipitation and extreme weather events while operating at a fraction of the computational cost. GraphCast Lam et al. (2022), trained on historical reanalysis data, delivers highly accurate 10-day global forecasts in under a minute, outperforming traditional numerical models on 90% of verification targets and improving severe weather prediction. GenCast Price et al. (2023), a probabilistic weather model, has also proven to be more accurate and efficient than the European Center for Medium-Range Weather Forecasts (ECMWF)'s ensemble forecast Molteni et al. (1996). Additionally, FuXi Chen et al. (2023) provides 15-day global forecasts with a 6-hour temporal resolution, matching ECMWF's ensemble mean performance while extending the skillful forecast lead time beyond ECMWF's high-resolution forecast. Moreover, some deep learning-based time series models have achieved promising results in temporal tasks (Zhou et al., 2022; Zhang & Yan, 2023; Eldele et al., 2024; Yi et al., 2024)."}, {"title": "2.1 DEEP LEARNING BASED FORECASTING", "content": "Large language models (LLMs) are highly effective in time series forecasting Chang et al. (2023); Sun et al. (2024). TIME-LLM Jin et al. (2023) is a reprogramming framework that aligns time series data with language modalities by converting time series into text prototypes before feeding them into a frozen LLM, outperforming specialized forecasting models and excelling in few-shot and zero-shot learning. The Frozen Pretrained Transformer Zhou et al. (2023) shows that pre-trained language and image models can achieve state-of-the-art results across"}, {"title": "2.2 LARGE LANGUAGE MODEL FOR TIME-SERIES PREDICTION", "content": "various time series tasks. Similarly, the CALF framework Liu et al. (2024) reduces distribution discrepancies between textual and temporal data, improving LLM performance in both long- and short-term forecasting with low complexity and strong few-shot capabilities. Chang et al. (2024) introduced a two-stage fine-tuning strategy that integrates multi-scale temporal data into pre-trained LLMs, achieving superior representation learning and performance in few-shot scenarios. Many researches also have shown that LLMs can potentially assist in weather forecasting Wang & Karimi (2024); Wang et al. (2024); Li et al. (2024). Li et al. (2024) introduce CLLMate (LLM for climate), a multimodal LLM using meteorological raster data and textual event data, which highlights the potential of LLMs in climate forecasting."}, {"title": "2.3 FOURIER NEURAL OPERATOR", "content": "Fourier Neural Operators(FNOs) Li & Tuzhilin (2020) have recently garnered considerable attention as an effective deep learning framework for learning mappings between infinite-dimensional function spaces, which is essential for approximating the solution operators of partial differential equations. Chen et al. (2019) provide a continuous formulation for neural networks by modeling the evolution of hidden states as solutions to differential equations, a concept that has inspired recent advances in operator learning. Many studies demonstrate that Fourier Neural Operators (FNOs) are highly effective for data-driven forecasting of complex physical processes. They capture the continuous evolution of weather variables\u2014such as temperature, wind speed, and atmospheric pressure across both spatial and temporal dimensions. Pathak et al. (2022) applies Adaptive Fourier Neural Operator(AFNO) to learn the evolution of weather variables across both spatial and temporal domains, effectively capturing the large-scale trends as well as the fine-grained structures inherent in the weather system. Sun et al. (2023) employs the FNO as a surrogate model to predict flood extents and water depths at high resolution, addressing the computational challenges associated with traditional hydrodynamic simulations. Leveraging global convolution, FNOs efficiently simulate fluid dynamics, making them ideal for long-term trend modeling and data-driven forecasting."}, {"title": "3 PRELIMINARIES", "content": "This paper proposes a general climate prediction framework based on large language models. Given a climate system, let $V = \\{t, u, v, ...\\}$ denote the set of climate variables, where $t$ represents temperature, $u$ represents wind speed, $v$ represents humidity, etc. The climate state at time step $l$ can be represented as $X^{(l)} \\in \\mathbb{R}^{|V|\\times M\\times N}$, where $M$ and $N$ denote the dimensions of the spatial grid. Specifically, $X_{true}^{(l)}[v, m, n] \\in \\mathbb{R}$ represents the ground truth value of variable $v \\in V$ at location $(m,n)$ at time step $l$, while $X_{pred}^{(l)}[v, m, n] \\in \\mathbb{R}$ represents the predicted value. Let $H(t) = \\{X_{true}(t - L)[v, m, n], ...,X_{true}(t -1)[v, m, n]\\} \\in [R^{L\\times|V|\\times M\\times N}$ denote the historical sequence of length $L$ leading up to time $t$. A sample in our dataset can be represented as $(x_s, y_s)$, where $x_s = H(t)$ represents the input features constructed from the historical sequence, and $y_s = X_{true}(t)[v, m, n]$ represents the ground truth value at the target time step. The prediction function $f$ can be formulated as: $f : \\mathbb{R}^{L\\times|V|\\times M\\times N} \\rightarrow \\mathbb{R}^{|V|\\times M\\times N}$ where $f(H(t)) = X_{pred}(t)$ represents the predicted climate state at time $t$. This paper mainly studies the climate prediction problem, which is to learn the optimal prediction function $f^*$ that minimizes the prediction error: $f^* = \\arg \\min_f L_{RMSE}(X_{pred}(t), X_{true}(t)).$"}, {"title": "4 THE PROPOSED MODEL", "content": "In this section, we mainly introduce ClimateLLM (Figure 1), a framework that integrates frequency-domain representation, dynamic prompting, and a Generative Pre-trained Transformer (GPT) backbone for weather forecasting."}, {"title": "4.1 REPRESENTATION LEARNING VIA FREQUENCY MIXTURE-OF-EXPERTS (MOE)", "content": "Accurate weather forecasting requires learning both spatial and temporal relationships in complex meteorological systems. For example, reliable temperature or precipitation predictions must capture large-scale circulation patterns (e.g., global wind jets, synoptic fronts) as well as local, fast-changing phenomena (e.g., convection, thunderstorms). Extreme weather events\u2014such as severe convective storms, tropical cyclones, or atmospheric rivers\u2014amplify this challenge: they involve strong non-linear interactions, evolve rapidly, and often have distinct frequency signatures. recent studies have investigated deep learning approaches for predicting thunderstorm severity using remote sensing weather dataEssa et al. (2022), demonstrating the potential of advanced neural architectures to capture complex meteorological signals. We found that while patch-based CNN or GNN approaches are intuitive for spatial feature extraction, they offer only limited gains for these highly localized events, especially when integrated with LLMs that excel at sequence-based reasoning but do not inherently resolve spatial structures.\nMotivated by recent progress in Fourier-based neural operators (FNOs), we adopt a frequency-domain view to address these issues in extreme weather forecasting. Rather than subdividing input grids into patches for a CNN or creating graph structures for a GNN, we apply a two-dimensional Fourier Transform (2D FFT) to each spatial slice. This converts the data from the spatial domain into the frequency domain, revealing both low-frequency (broad-scale) and high-frequency (fine-scale) details without explicit local convolutions or adjacency matrices. For extreme weather events, frequency-domain modeling can uncover wavenumber patterns associated with severe storms or other wave-like processes\u2014patterns that are often harder to identify in the raw spatial domain.\nStill, not every frequency component is equally important for prediction. Most Fourier-based methods process these components uniformly, ignoring differences between low- and high-frequency bands. This oversimplifies the modeling of extreme phenomena. In our approach, a frequency-based MoE module adaptively allocates different expert networks to different segments of the frequency spectrum. We then use an LLM as the primary sequence learner, leveraging its capability for pattern extraction over extended temporal contexts. By combining frequency-domain representations with the LLM's temporal insights, our framework addresses both quick local disturbances and broader-scale dependencies. The frequency pathway injects domain-specific structure, while the LLM refines long-range temporal patterns."}, {"title": "4.1.1 NORMALIZATION AND FREQUENCY-DOMAIN REPRESENTATION", "content": "Raw climate data often span different scales across variables. To manage this, each variable is normalized by subtracting its mean and dividing by its standard deviation over the historical period. Formally, at time $t$:\n$\\hat{X}(t) [v, m, n] = \\frac{X(t) [v, m, n] - \\mu(v,t)}{\\sigma(v, t) + \\epsilon},$\n(1)"}, {"title": "4.1.2 MIXTURE OF EXPERTS FOR ADAPTIVE FREQUENCY MODELING", "content": "Distinct spatial patterns arise at different frequencies. To model them effectively, we introduce a MoE module that adaptively routes each frequency component to the most suitable sub-network. Let\n$Z(t) = g(\\mathcal{S}(t)),$\n(5)\nwhere $g(\\cdot)$ is a learnable transformation. The MoE includes $E$ experts $\\{f_e(\\cdot)\\}_{e=1}^{E}$, each specializing in part of the frequency domain:\n$\\mathcal{S}(t) = \\sum_{e=1}^{E} G_e(\\mathcal{S}(t)) f_e(Z(t)).$\nHere, $G_e(\\cdot)$ is a gating function that assigns a weight to each expert's output, ensuring a soft selection process. This allows the model to handle high- and low-frequency patterns together."}, {"title": "4.1.3 LLM INTEGRATION FOR TEMPORAL DEPENDENCIES", "content": "After the MoE layer, we obtain feature representations that combine temporal hidden representations with frequency-domain information transformed from the spatial domain. Since weather variations are influenced not only by spatial factors but also by temporal evolution patterns, capturing the underlying temporal dependencies is crucial. Generative Pre-trained Transformers (GPTs) have demonstrated exceptional capabilities in sequence representation and pattern extraction and have been widely applied to time series forecasting tasks. Inspired by this, we further incorporate GPT to capture the temporal evolution patterns of spatial-frequency representations. Specifically, we treat the transformed spectral representation $\\mathcal{S}(t)$ at each time step $t$ as a token and leverage the self-attention mechanism to model the temporal dependencies between these tokens, denoted as $H = GPT(\\mathcal{S})$. This provides a deeper understanding of the temporal evolution, beyond what simpler CNN- or GNN-based structures might glean."}, {"title": "4.1.4 INVERSE FOURIER TRANSFORM FOR SPATIAL RECONSTRUCTION", "content": "After processing in the frequency domain, we apply the inverse 2D FFT (iFFT) to reconstruct the spatial representation:\n$\\hat{X}_{pred}(t) = \\mathcal{F}^{-1}(H(t))$\n(7)\nwhere the inverse transformation is computed as:\n$\\mathcal{F}^{-1}(H(t))[v, m, n] = \\frac{1}{MN} \\sum_{k_m=1}^{M} \\sum_{k_n=1}^{N} H(t) [v, k_m, k_n] \\times e^{2\\pi i (k_m m / M + k_n n / N)}$\nThe de-normalization operator $R_{de}$ acts on the inverse-transformed representation to obtain the final predicted climate state.:\n$X_{pred}(t) = R_{de} (\\hat{X}_{pred}(t))$\n(8)\nThe complete algorithm workflow is described in Algorithm 1."}, {"title": "4.1.5 PROPOSITION", "content": "We further have the following proposition (for the full proof, please refer to Appendix C):\nProposition 1 (Equivalence of Time-Domain Forecasting and Frequency-Domain Forecasting for 2D FNO)\nAssume $\\{(x_0, Y_0), (x_1,Y_1), ..., (x_{N-1},Y_{N-1})\\}$ is the input sequence in the time domain, and $\\{(\\hat{x}_0, \\hat{Y}_0), (\\hat{x}_1, \\hat{y}_1),...,(\\hat{X}_N, \\hat{y}_N)\\}$ is the predicted output sequence of the frequency model. The predicted value $(\\hat{X}_N, \\hat{y}_N)$ is obtained by transforming from the frequency domain to the time domain at timestamp $N$."}, {"title": "4.2 WEATHER DYNAMIC PROMPTING VIA META-FUSION", "content": "Prompting has emerged as a technique for providing feature patterns or guidance tokens that steer LLMs toward more effective sequence forecasting. For instance, TimeLLM Jin et al. (2023) combines domain knowledge and temporal statistics into prompt tokens to better inform the underlying LLM on where to focus. Despite these advances, many existing prompt designs rely on hard-coded information and thus struggle to capture dynamic temporal patterns. Moreover, unlike purely temporal tasks, weather forecasting also demands strong spatial modeling. Meteorological variables often propagate across space (e.g., storm fronts spreading geographically), while different variables (such as temperature and pressure) exhibit intricate correlations governed by atmospheric physics.\nTo address these issues, we propose a weather dynamic prompting via meta-fusion strategy. Our design aims to capture the evolving temporal patterns while simultaneously bridging cross-variable, spatiotemporal information. Rather than directly encoding domain priors in rigid ways, we introduce learnable tokens into the LLM pipeline as queries in a cross-attention mechanism. This two-step \"meta-fusion\" not only diverges from traditional hard-encoding approaches, but also extends beyond simple concatenation or pooling along time axes. By doing so, it simultaneously captures crucial temporal patterns while acting as a powerful \u201cbridge\u201d to harness global weather information in both time and variable dimensions.\nFormally, let $P \\in \\mathbb{R}^{K\\times d}$ denote the learnable prompt tokens, where $K$ is the number of prompt tokens and $d$ is the hidden dimension. Suppose we have a representation $\\mathcal{S} \\in \\mathbb{R}^{C\\times L\\times d}$ obtained from the MoE block, where $C$ is the number of weather variables and $L$ is the length of the temporal sequence. We first aggregate along the variable dimension to obtain a purely temporal representation $S_t \\in \\mathbb{R}^{L\\times d}$. Then we perform cross-attention by taking the learnable tokens $P$ as queries and $\\mathcal{S}_t$ as both keys and values:\n$P' = LayerNorm (CrossAttn (P, S_t, S_t) + P),$\n(9)\nwhere $CrossAttn$ denotes the cross-attention function. Next, we aggregate along the time dimension of $\\mathcal{S}$ to obtain a representation $S_c \\in \\mathbb{R}^{C\\times d}$ that focuses on the variable-wise features (e.g., aggregated temporal patterns for each variable). We again use $P$ as queries, but this time attend over $S_c$:\n$P = LayerNorm (CrossAttn(P', S_c, S_c) + P').$\n(10)\nHere, the two cross-attention steps exploit the prompt tokens both as flexible probes of temporal dynamics and as a fusion bridge across different weather variables."}, {"title": "4.3 GENERATIVE PRE-TRAINED TRANSFORMER BACKBONE", "content": "The Generative Pre-trained Transformer (GPT) architecture, which underpins modern LLMs, leverages self-attention mechanisms to model long-range dependencies in sequential data. This makes LLMs particularly well-suited for capturing complex temporal patterns and dynamics. In temporal modeling applications, LLMs offer several key advantages in capturing both short-term fluctuations and long-term trends. To further enhance the temporal representation, we integrate an additional time-series encoding that complements the standard transformer positional encodings. Specifically, our framework processes the input data through a two-dimensional Fast Fourier Transform (2DFFT) and a Mixture-of-Experts (MoE) module to extract salient features. This yields a set of MoE representations, denoted as S, and weather prompts, denoted as P. We then concatenate these outputs and feed them into the LLM as follows:\n$\\Eta = GPT(concat [P, S_1, S_2, ........., \\mathcal{S}_T]),$\n(11)\nIn line with recent developments in LLM-based temporal foundation models Pan et al. (2024); Cao et al. (2023), our approach adopts the GPT-2 architecture as the backbone. GPT-2 is renowned for its scalable transformer design, efficient self-attention mechanism, and robust performance on sequence modeling tasks."}, {"title": "4.4 LATITUDE-WEIGHTED TRAINING AND OPTIMIZATION", "content": "In this paper, we employ the latitude-weighted Root Mean Square Error (RMSE) as the optimization objective instead of the conventional RMSE. Traditional RMSE treats all spatial grid points equally, assuming a uniform distribution of errors across the dataset. However, in global weather modeling, the Earth is a sphere, and data points at higher latitudes (closer to the poles) are disproportionately represented in gridded datasets due to the convergence of meridians. This introduces a latitude bias, where errors in high-latitude regions can disproportionately influence the overall RMSE, leading to an inaccurate assessment of model performance.\nTo mitigate this issue, we adopt latitude-weighted RMSE, where each grid point is weighted according to its latitude. The weight is defined as:\n$a(m) = \\frac{cos(m)}{\\sum_{m'} cos(m')}$\n(12)\nwhere m represents the latitude index. This weighting scheme ensures that errors in lower latitudes, which cover larger surface areas, contribute proportionally more to the loss function, aligning the optimization objective with the actual physical characteristics of the Earth's surface.\nThe latitude-weighted RMSE is formulated as:\n$Loss = \\frac{1}{MN}\\sum_{m=1}^{M}\\sum_{n=1}^{N} a(m) (X_{pred} (m, n) - X_{true} (m, n))^2$\n(13)\n$X_{pred}$ represents the prediction result, which is obtained by applying inverse 2DFFT and de-normalization to the representation H."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to answer the following questions:"}]}