{"title": "MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison", "authors": ["Kaijie Zhu", "Xianjun Yang", "Jindong Wang", "Wenbo Guo", "William Wang"], "abstract": "Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs.", "sections": [{"title": "1. Introduction", "content": "Together with the recent success of LLM agents (OpenAI, 2024; Anthropic, 2024; Llama, 2024; DeepSeek, 2025) comes the serious security concern of indirect prompt injection attacks (IPI) (Naihin et al., 2023; Ruan et al., 2024; Yuan et al., 2024; Liu et al., 2024; Zhan et al., 2024; Debenedetti et al., 2024; Zhang et al., 2024a). Attackers exploit the agent's interaction with external resources by embedding malicious tasks in tool-retrieved information such as database (Zhong et al., 2023; Zou et al., 2024) and websites (Liao et al., 2024; Xu et al., 2024; Wu et al., 2024a). These malicious tasks will force the agent to take unauthorized actions, leading to severe consequences.\nDefending against IPI attacks is significantly challenging. First, unlike jailbreaking LLMs, the injected malicious prompts and their resultant behaviors can be legitimate tasks. Second, implementing effective defenses requires a careful balance between security guarantees and utility maintenance. Existing IPI defenses either require essential model training resources, are only applicable to simple attacks, or harm normal utilities under attack scenarios. Specifically, resource-expensive defenses retrain the LLM in the agent (Chen et al., 2024a; Wallace et al., 2024) or train an additional model to detect injected prompts in the retrieved data (ProtectAI, 2024). Such methods are less practical due to the greedy resource requirements. Furthermore, adversarial training may jeopardize the model's normal utility, while model-based detection naturally harms the agent's utility under attack scenarios and suffers from high false negative rates (Section 4.2). Existing training-free defenses either augment"}, {"title": "2. Related Work", "content": "Indirect Prompt Injection Attacks. At a high level, indirect prompt injection attacks against agents can be categorized as general attacks and agent-specific attacks. General attacks focus on developing universal attack prompt patterns that force the target agent to conduct the attacker tasks rather than the user tasks. Notably, the escape character attacks (Willison, 2022) utilize special characters like \"\\n\" to manipulate context interpretation. Context-ignoring attacks (Perez & Ribeiro, 2022; Schulhoff et al., 2023) explicitly instruct the LLMs to disregard the previous context. Fake completion attacks (Willison, 2023) attempt to deceive the LLMs by simulating task completion. These methods are often tested on IPI benchmarks (Debenedetti et al., 2024; Xu et al., 2024) with pre-specified injection points and attack tasks. There are also some early explorations of LLMs attacks against a specific type of agent. For example, attacks against web agents inject the attack content into the web pages to \u201cfool\u201d the agent into the attack tasks (Wu et al., 2024a; Liao et al., 2024; Xu et al., 2024). Attacks against computer agents manipulate the computer interface (Zhang et al., 2024b). Note that there are also some direct prompt injection attacks against LLMs (Yu et al., 2023; Wu et al., 2024a;c; Toyer et al., 2024). These methods directly append the attack prompts after the user inputs, which may not be practical in real-world applications.\nDefenses against IPI. Existing defenses can be categorized based on resource requirements. Defenses that require additional training resources either conduct adversarial training of the LLM(s) in the target agent (Wallace et al., 2024; Chen et al., 2024a;b) or add additional models to detect whether the inputs contain injected prompts (ProtectAI, 2024; Inan et al., 2023). However, these methods face practical limitations due to their substantial computational and data requirements. In addition, adversarial training may jeopardize the model's normal utility in broader application domains. As we will show later, adding additional detection models naturally harms the agent's utility under attack and suffers from high false negative rates.\nTraining-free defenses either design additional prompts for the user inputs or constrain the allowed tool calls of the agent. First, most training-free defenses explore additional prompts that either help the model ignore or detect potential attack instructions in the retrieved data. Specifically, ignorance strategies include adding a delimiter between the user prompt and retrieved data (Hines et al., 2024; Mendes, 2023; Willison, 2023), repeating the user prompt (lea, 2023) Such defenses, while lightweight, have limited efficacy against stronger attacks (as shown in Sec 4). Known-answer detection (Liu et al., 2024) adds additional questions with known answers to the user prompt and detects if the model finally outputs the answer. However, this method can only identify injections post-execution, when attacks may have already succeeded. Second, tool filtering (Debenedetti et al., 2024) allows LLMs to select a set of permitted tools for the given user task and block all calls to unauthorized tools. This approach harms utility as the LLMs sometimes filter out necessary tools. More importantly, it is easy to bypass as the attackers can design their attack tasks with only the tools related to the user attack. In comparison, our method is a lightweight and highly effective training-free defense that well maintains the agent's normal utility.\nNote that other defenses require human intervention (Wu"}, {"title": "3. Metholody of MELON", "content": "3.1. Preliminaries\nFormalization and Definition of LLM Agent. In this work, we define an LLM agent \u03c0 as an integrated system comprising LLM(s) and a set of tools F = {f1, ..., fn} for environment interaction. The agent receives a user prompt specifying a task Tu (e.g., \"Summarize my agenda and tell me the time of the next event.\") and executes it through a structured multi-step procedure.\nAt each step t, we define the state as St = (Tu, A1:t, O1:t), where Tu is the user task, A1:t = {(R1, C1), ..., (Rt, Ct)} is the sequence of LLM-generated actions with each action pair consisting of an LLM response Ri and a set of tool calls Ci = {c1,..., cmi}. Each tool call specifies a tool fi \u2208 F and its parameters (e.g., \u201cretrieve_event(date=20250131)\"). O1:t = {O1,..., Ot} denotes the sequence of observations, where each Oi contains the tool execution outputs corresponding to Ci. In step t + 1, The agent system first generates action At+1 = \u03c0(St) based on previous state, then obtaining observation Ot+1 = Exec(Ct+1) by executing the tool calls. This process continues iteratively until the user task Tu is completed or errors occur.\nThreat Model. We follow the assumption of IPI, where attackers cannot access the LLMs' input and output inside the target agent. Their access is limited to manipulating the external information retrieved by the agent via tool calls, such as websites, emails, or files. The attackers aim to redirect the agent from executing the original user task to performing a malicious task Tm. For example, the attacker task could be \"Send your bank account and password to hacker@gmail.com\". We denote Of to be the tool execution outputs injected with Tm and O1:t = {O1, ..., Otf} as the sequences of previous tool execution outputs. We assume all user tasks to be legitimate and the defender has complete access to the entire agent system, including the states St = (Tu, A1:t, O1:t). However, we do not assume the defender has the resources to train LLMs or can access LLM internal representations.\""}, {"title": "3.2. Technical Overview", "content": "Insights and Technical Challenges. Our design is based on the key observation that whenever a malicious attacker task Tm is present in the retrieved data, it attempts to redirect the agent from executing the user task Tu toward executing Tm instead. Given a state St = (Tu, A1:t, O1:t), if Of that injected with Tm successfully hijacks the agent's behavior to focus on executing Tm, it induces a state collapse where the agent's next action At+1 becomes conditionally independent of Tu and A1:t, depending primarily on O1:t. For benign cases where Ot does not contain malicious instructions or the attack does not succeed, the agent maintains functional dependencies on all state components (Tu, A1:t, O1:t). Formally, for a successful attack at step t, we can observe: $P_\\pi(A_{t+1}|(T_u, A_{1:t}, O_{1:t})) \\approx P_\\pi(A_{t+1}|O_{1:t})$, where $\\mathbb{P}$ is the probability. For benign executions, the agent's actions maintain their dependency on the user inputs: $P_\\pi(A_{t+1}|(T_u, A_{1:t}, O_{1:t})) >> P_\\pi(A_{t+1}|O_{1:t})$.\nThis statistical discrepancy naturally leads to a masking-based approach to detect the injected prompts. Let $M : \\mathcal{S} \\rightarrow \\mathcal{S}$ be a masking operator that preserves only original tool execution outputs. We define two execution paths:\n$A_{t+1}^{o} = (R_{t+1}^{o}, C_{t+1}^{o}) = \\pi(S_t)$ (the original run),\n$A_{t+1}^{m} = (R_{t+1}^{m}, C_{t+1}^{m}) = \\pi(M(S_t))$ (the masking run),\nwhere $A_{t+1}^{o}$ and $A_{t+1}^{m}$ represent the action generated by the original run and masked run at step t + 1, respectively. By comparing the agent's next action generated by the original run and the masking run, we can identify potential attacks: if the action deviates after masking, that indicates the agent's action is related to the user task and the original execution is benign, otherwise, the execution is independent from the user prompt, indicating there are attacker-injected prompts during the execution.\nTo compare the actions of the original run and the masking run, we can feed $A_{t+1}^{o}$ and $A_{t+1}^{m}$ into a text embedding model and compute their distance in the embedding space. If at any step, we observe that the distance is higher than a certain threshold, it means the LLM output depends only on $O_{1:t}$ and signals an attacker-injected prompt.\nHowever, although straightforward, this masking function and the comparison method face three technical challenges during our exploration. \u2780, when presented with only $O_{1:t}$ in the masking run, the LLM may treat these tool execution outputs as few-shot examples and generate contextually plausible but potentially arbitrary tool calls. Thus, for injected cases, the generated tool calls may deviate from the malicious task Tm embedded in Of, leading to false negatives, while for benign cases, these arbitrary calls might coincidentally align with the user task Tu, causing false positives. \u2781, we observe that, in some cases, the execution timing of Tm differs between the original run and the masking run. In the original run, the agent might execute the user task Tu first before proceeding to Tm. However, in the masking run, since there is no user task to complete, the agent directly executes Tm. As a result, at step t, while the masking run has begun executing Tm, the original run may still be processing Tu. This timing mismatch leads to false"}, {"title": "Algorithm 1 MELON Algorithm at Step t", "content": "Require: Agent \u03c0, user task Tu, masking function M, similarity threshold 0, state St, tool call cache Ht.\nEnsure: Detection result.\n1: Ot \u2190 concat(O1:t) {Consolidate tool outputs}\n2: Sm \u2190 M(St) = (Tf, \u00d8, Ot) {Construct masked state}\n3: At+1 \u2190 \u03c0(Sm) {Generate masked action}\n4: C1 \u2190 tools(At+1) {Extract tool calls}\n5: Ht+1 \u2190 Ht+1 \u222a Ct+1 {Update tool call cache}\n6: So \u2190 (Tu, A1:t, O1:t) {Original state}\n7: At+1 \u2190 \u03c0(So) {Generate original action}\n8: C+1 \u2190 tools(At+1) {Extract tool calls}\n9: for each tool call c \u2208 C+1 do\n10: for each cached call h \u2208 Ht+1 do\n11: sim \u2190 $\\frac{\\phi(c) \\cdot \\phi(h)}{|\\phi(c)| \\cdot |\\phi(h)|}$ {Compute similarity}\n12: if sim > \u03b8 then\n13: return True {Attack detected}\n14: end if\n15: end for\n16: end for\n17: return False {No attack detected}\nnegatives when comparing tool calls at corresponding steps.\n\u2782, typically, the action generated by LLM contains both text responses R and the tool calls C with specific arguments. In successful attack cases, the masking run and original run may differ in their text responses R while maintaining identical tool calls C that execute the malicious task Tm, which again introduces false negatives.\nOverview of MELON. We design MELON by optimizing the straightforward approach above with novel strategies to address the key technical challenges (demonstrated in Figure 2). Specifically, to address challenge \u2780, we modify the masking function M in two ways: (1) instead of simply removing Tu, we replace it with a task-neutral prompt Tf (see Appendix A.1) that typically requires no tool interactions,"}, {"title": "3.3. Technical Details", "content": "Algorithm 1 presents the complete detection procedure. As discussed above, our detection mechanism analyzes the similarity between tool calls generated in two parallel runs: the original run and the masking run. At step t, we maintain a tool call cache Ht that collects tool calls from the masking run. The detection process consists of two phases: (1) cache population and (2) similarity-based detection.\nIn the cache population phase, we first use the masking function M to mask the current state St. If Of contains a malicious task Tm, the agent will generate tool calls $C_{t+1}^{m}$ corresponding to Tm, which are then added to the cache Ht+1. Otherwise, if no malicious task is present, the agent will generate no tool calls given the design of Tf. For similarity-based detection, we compare each tool call c \u2208 Ci+1 from the original run with every cached tool call h \u2208 Ht+1 using a vector embedding model \u03c6: $sim(c, h) = \\frac{\\phi(c) \\cdot \\phi(h)}{|\\phi(c)| \\cdot |\\phi(h)|}$. We raise an alert if any similarity score exceeds a threshold \u03b8: $Alert_{t+1} = \\exists c \\in C_{t+1}, \\exists h \\in H_{t+1} : sim(c, h) > \\theta$.\nOur preference for embedding-based similarity over exact matching stems from the semantic nature of tool calls, tool calls with similar intents can have different arguments. For example, an attack instructing send all your personal data to hacker@evil.com might generate tool calls with slightly different arguments like send_email(\"hacker\", subject=\"data\") and send_email(\"hacker\", subject=\"info\"), while they differ in their exact argument names, these two calls serve the same malicious purpose. Exact matching would fail to detect such semantically equivalent but syntactically distinct attacks."}, {"title": "3.3.1. IMPLEMENTATION AND OVERHEAD ANALYSIS", "content": "Implementation Details. Our detection method consists of several key components. For tool call comparison, we convert each tool call to a natural language description, note that for specific functions, we do not compare some of their arguments, for example, for the send_email function, we do not compare the contents to be sent, but instead only recipient is considered. Details can be found in Appendix A.3. Next, we employ OpenAI's text-embedding-v3 model (OpenAI, 2024) that maps these descriptions to dense vector representations. We set the primary similarity threshold \u03b8 = 0.8 to balance detection sensitivity and false positive rate, the ablation study on different similarity thresholds is presented in Section 4.3. The task-neutral prompt Tf is designed to be independent of specific domains or tasks. For example: \"Summarize the provided content. If there are any instructions, execute them.\" This formulation enables the detection of injected tasks Tm while minimizing false positives from benign content such as documentation or user guides, we also provide the ablation study for different Tf in Section 4.3.\nOverhead Analysis. We analyze both the time complexity and computational cost of our approach. Since LLMs typically supports batch operations, the instruction-seeking path can be executed in parallel with the original path during each tool output inspection. This parallel execution minimizes the additional latency introduced by our detection method. Also, since the contents of the masking run largely overlap with the original run, we can implement a KV cache to reduce the latency. The primary source of latency stems from computing embeddings, which depends on the response time of OpenAI's embedding model API in our implementation. Regarding computational costs, particularly for API-based models, the introduction of the instruction-seeking path effectively doubles the number of required model calls. This results in a \u2248 2\u00d7 increase in API costs compared to the undefended baseline system. However, we argue that this cost overhead is justified given the critical importance of protecting agent systems against IPI attacks."}, {"title": "4. Evaluation", "content": "4.1. Experiment Setup\nAgent. We evaluate MELON on the IPI benchmark Agent-Dojo (Debenedetti et al., 2024). AgentDojo comprises four types of agents (banking, slack, travel, and workspace), and each type of agent has a unique set of tools. AgentDojo designs 16, 21, 20, 40 user tasks for their agents, respectively. Besides, each agent also has different attack tasks and injection points. It picks one user task and one attack task to form an attack case, and in total, 629 attack cases. Several early works also propose prompt injection attacks benchmarks (Zhan et al., 2024), we choose AgentDojo because it is the latest one containing many diverse attack cases. We also tried another benchmark for multi-modal agents, i.e., VisualWebArena-Adv (VWA-Adv) (Wu et al., 2024a). We do not select it because the attack success rate of SOTA image attacks on this benchmark is low (see Appendix D for more details). We consider three models as the LLM model in each agent: GPT-40, 03-mini, and Llama-3.3-70B. Note that we do not use Claude-3.5-Sonnet for large-scale experiments in considering the budget limit, but we test the latest OpenAI model 03-mini and the SOTA open-source"}, {"title": "4.2. Experiment Results", "content": "Our experimental results in Figure 3 and Table 1 demonstrate that MELON achieves both high utility and low ASR, while other defenses exhibit a clear trade-off. We analyze the performance of different defenses on each metric below.\nUtility under Attack (UA). Note that detection-based methods (DeBERTa detector, LLM detector, and MELON) terminate the entire agent execution upon detecting potential prompt injections. At step t + 1, DeBERTa detector and LLM detector analyze tool execution outputs O1:t before LLM generates any action, regardless of whether the attack would succeed. This creates an inherent trade-off: a perfect detector should achieve 0% UA, as any non-zero UA indicates attacks passing through detection and represents the lower bound of their False Negative Rate (FNR). In contrast, MELON operates after LLM generates action At+1 but before execution, intervening only when it detects that malicious instructions will be executed in the original run, thus better preserving UA while minimizing ASR.\nMELON-Aug achieves optimal balance across all attacks and models, maintaining high UA while achieving low ASR across all LLMs. For GPT-40, MELON-Aug achieves 68.72% UA with 0.32% ASR, compared to the no defense baseline (69.08% UA, 16.06% ASR). As discussed above, detection methods show significant limitations, with DeBERTa and LLM detectors having FNR of at least 24.05% and 21.18% respectively. Prompt augmentation methods (delimiting and repeat prompt) increase UA to 69.75% and 77.86% compared to the no defense baseline, likely by strengthening LLM's focus on Tu. The tool filter shows varying performance across models - achieving second-best UA-ASR trade-off for GPT-40 but poor UA for o3-mini and Llama-3.3-70B due to excessive tool filtering that renders the system unusable.\nAttack Success Rate (ASR). MELON and MELON-Aug demonstrate superior attack prevention across all models"}, {"title": "4.3. Ablation Study and Sensitivity Test", "content": "Ablation Study. We conduct a systematic ablation study to validate the importance of our three key designs discussed in Section 3.2 using the AgentDojo dataset with GPT-40 under the Important Messages attack. We remove each design component one at a time. First, for the masking function M, instead of replacing Tu with Tf, we simply delete Tu (denoted as \"Basic\"). Second, for the tool call cache mechanism, we remove H and only compare the current step's tool"}, {"title": "4.4. Analysis of False Positive Rate", "content": "Since MELON performs detection after each agent action A, it is essential to evaluate its false positive rate (FPR) during normal operation. We compare the FPR of MELON with two baseline detectors: DeBERTa detector and LLM detector. We evaluate the FPR of GPT-40 on AgentDojo."}, {"title": "4.5. Analysis of Attack Success Cases", "content": "To understand the limitations of MELON, we analyze 66 cases where attacks evaded detection across three LLMS (GPT-40, 03-mini, and Llama-3.3-70B) using the Important Messages attack on AgentDojo. We identify four primary failure patterns: (1) Response-Based Attacks (72.73%): When Tm achieves its objective through text responses rather than tool calls (e.g., persuading users to make expensive purchases), these attacks manifest in R rather than C. Since MELON only monitors tool calls, such attacks can evade detection. (2) Tool Call Redundancy (15.15%): When O1:t contains partial results for malicious task Tm, the original run utilizes these existing results while the masking run generates repeat tool calls to obtain the same information. This discrepancy in tool usage prevents matching between C+1 and C1, leading to missed detections. (3) State Hallucination (6.06%): The agent in the original run skips necessary tool calls by hallucinating the required information. For example, when Tm requests retrieving a user's phone version, the agent directly responds with \"iPhone X\" without making appropriate tool calls. (4) Function Hallucination (6.06%): The agent generates calls to non-existent functions that are not provided in the tool set, causing mismatches between the original and masked runs."}, {"title": "5. Conclusion and Future Work", "content": "We present MELON, a novel IPI defense based on the key observation that successful attacks reduce the dependence between agent tool calls and user inputs. Through extensive experiments, we demonstrate that MELON significantly outperforms existing defenses while maintaining high utility. Our work establishes that identifying and leveraging fundamental behavioral patterns of IPI attacks, such as the tool call and user input independence property, provides an effective methodology for defense design.\nOur work opens several future directions. First, MELON can be extended to detect broader attack goals beyond direct task manipulation (Wu et al., 2024a). Second, the computational efficiency of masked re-execution can be improved through techniques like KV cache and selective state masking. Third, MELON's behavioral pattern detection can be combined with other defense approaches like prompt augmentation to create more robust protection mechanisms."}, {"title": "Impact Statement", "content": "This work advances the security of LLM-based agent systems against indirect prompt injection attacks. While our method introduces additional computational costs, we believe this overhead is justified by the critical importance of protecting agent systems from malicious manipulation. Our defense mechanism helps prevent unauthorized actions while preserving legitimate functionality, contributing to the safe deployment of LLM agents in real-world applications. However, we acknowledge that no security measure is perfect, and continued research is necessary to address evolving attack methods."}]}