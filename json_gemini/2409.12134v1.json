{"title": "BERT-VBD: Vietnamese Multi-Document Summarization Framework", "authors": ["Tuan-Cuong Vuong", "Trang Mai Xuan", "Thien Van Luong"], "abstract": "In tackling the challenge of Multi-Document Summarization (MDS), numerous methods have been proposed, spanning both extractive and abstractive summarization techniques. However, each approach has its own limitations, making it less effective to rely solely on either one. An emerging and promising strategy involves a synergistic fusion of extractive and abstractive summarization methods. Despite the plethora of studies in this domain, research on the combined methodology remains scarce, particularly in the context of Vietnamese language processing. This paper presents a novel Vietnamese MDS framework leveraging a two-component pipeline architecture that integrates extractive and abstractive techniques. The first component employs an extractive approach to identify key sentences within each document. This is achieved by a modification of the pre-trained BERT network, which derives semantically meaningful phrase embeddings using siamese and triplet network structures. The second component utilizes the VBD-LLaMA2-7B-50b model for abstractive summarization, ultimately generating the final summary document. Our proposed framework demonstrates a positive performance, attaining ROUGE-2 scores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-art baselines.", "sections": [{"title": "1 Introduction", "content": "Multi-Document Summarization (MDS) aims to distill information from multiple documents into a concise representation, preserving key points and eliminating redundancy. In the Vietnamese language context, MDS presents a unique challenge due to its nascent stage and inherent linguistic complexities. Traditionally, topics are explored through diverse Vietnamese articles, each offering distinct perspectives. While directly extracting excerpts can facilitate initial understanding, it often leads to incoherence. Conversely, purely abstractive approaches might struggle to retain salient details. Combining extractive and abstractive approaches holds significant potential for Vietnamese MDS problems. However, research on this method is scarce, both in English and Vietnamese. Existing studies, like those by Y Gao et al. [3] and H Jin et al. [4], perform extractive and abstractive summarization in parallel with separate input representations for each model, rather than truly combining them with a shared input.\nOur paper proposes a Vietnamese MDS framework utilizing a two-component pipeline architecture that merges extractive and abstractive approaches. This combination offers three key benefits: firstly, it preserves crucial information for the reader using the extractive method, while simultaneously presenting it in a more reader-friendly, concise form through the abstractive approach. Secondly, it optimizes resource usage compared to solely relying on abstractive summarization, which can be computationally expensive when retaining important details. Finally, leveraging the pre-trained models available in the extractive approach enhances the summarization ability by incorporating more information from both sentence and word embeddings.\nIn summary, our key contributions are twofold:\nWe propose a novel framework that merges extractive and abstractive summarization techniques to tackle Vietnamese MDS challenges. This framework leverages the deep learning models such as Sentence-BERT (SBERT) [13] and VBD-LLaMA2-7B-50b [12] to generate a final summary that both retains important content and ensures readability.\nWe evaluate the proposed framework on the VN-MDS\u00b9 dataset. Our experiments demonstrate promising results, showcasing its originality and correctness by effectively combining various methods. We conduct a comprehensive comparison between our framework and the current models applied to the Vietnamese language, which shows that our model is superior to them."}, {"title": "2 Related Work", "content": "In the domain of document summarization, two dominant approaches exist: extractive and abstractive. Extractive methods identify and extract key text segments like words, phrases, or sentences to form the summary. On the other hand, abstractive methods generate entirely new text summaries encapsulating the core information from the original documents. A recently emerged hybrid approach integrates both extractive and abstractive methods, aiming to produce summaries of even higher quality. This hybrid approach typically involves extracting a subset of crucial sentences using the extractive method, followed by the abstractive method's application on these selected sentences to generate the final summary (Liu et al. [5])."}, {"title": "3 Proposed Method", "content": "In this section, we delineate the proposed model architecture and approach. In particular, in Section 3.1, we provide a high-level schematic of the proposed end-to-end model pipeline. Next, we detail the data pre-processing steps taken to prepare the input data for summarization in Section 3.2. In Section 3.3, we describe the extractive summarization techniques leveraged to identify and extract salient content from the source documents. Subsequently, we explain the abstractive summarization components, which condense and paraphrase the extracted content to generate novel phrasing. Finally, we discuss the post-processing phase where the extracted and generated portions are combined and refined to produce the final summarized output in Section 3.4."}, {"title": "3.1 Overview", "content": "We propose a novel MDS model for Vietnamese that combines extractive and abstractive approaches in a pipeline architecture. The model contains two components: extractive summarization followed by abstractive summarization. The abstractive component takes the output of the extractive component as input. This hybrid architecture aims to leverage the advantages of both approaches while overcoming their limitations. As illustrated in Fig. 1, the extractive component identifies and extracts salient content from the source documents. The abstractive component then condenses and rewrites this extracted content to generate a novel summarized text. The pipeline design allows our model to utilize the strengths of extractive selection and abstractive rewriting for Vietnamese MDS. Details of each component are presented in the following sections."}, {"title": "3.2 Data Pre-processing", "content": "Data pre-processing is critical as it directly impacts model efficiency and performance. Due to the complex properties of Vietnamese, specialized pre-processing is required including cleaning, normalization, and enrichment.\nData Normalization: Cleaning involves lowercase conversion and removing meaningless words and non-alphanumeric characters. Normalization creates uniformity by involving the following steps:\nConvert to lowercase: For cleaning and normalization, words are converted to lowercase to create uniformity between upper and lower case versions with the same meaning.\nEliminate words that do not have much meaning: Meaningless words are eliminated, as the VN-MDS journalistic dataset contain mainly salient words. Therefore, non-semantic symbols such as \"%\",\";\",\" : \" will be removed to save space and processing time.\nNon-alphanumeric characters without semantic content are removed, as they usually do not contribute meaningful information for general NLP tasks.\nSegmentation: Segmentation is then performed including sentence splitting to divide text into sentences and word splitting to break sentences into word and phrase tokens:\nSentence splitting: This aims to divide a paragraph or document into individual sentences. It primarily relies on punctuation cues such as periods, question marks, and exclamation points to delineate sentence boundaries.\nWord splitting: This process breaks down larger text units such as phrases, sentences, or even entire documents into smaller pieces known as tokens. These tokens can be individual words or multi-word phrases."}, {"title": "3.3 Extractive Summarization", "content": "In order not to lose information in each document, we will divide the documents into sentences corresponding to that document, then proceed to select the important content and classify it in clusters with the best results. Suppose, after the segmentation of the n document, we will obtain the $s_m^n$ sentence, with the index n corresponding to the index of the document in the input, which m corresponds to the sentence index in the corresponding document. SBERT [13] framework allows us to convert $s_m^n$ sentences to to dense vector representations $u_m^n$ using pre-trained language models. Then we proceed to calculate sentence similarity:\n$Sim(s_m^n, s_{m+1}^n) = \\frac{(u_m^n u_{m+1}^n)}{(||u_m^n|| \\times ||u_{m+1}^n||)}$\nwhere ($s_m^n, s_{m+1}^n$) are the two sentences to be calculated; $Sim(s_m^n, s_{m+1}^n)$ the cosine similarity of the two sentences; $u_m^n$ and $u_{m+1}^n$ are vector representations for $s_m^n$ and $s_{m+1}^n$, respectively; $(u_m^n u_{m+1}^n)$ is the dot product of vectors, it measures the sum of the products of corresponding elements across both vectors; $||u_m^n||$ and $||u_{m+1}^n||$ are the magnitudes (lengths) of vectors $u_m^n$ and $u_{m+1}^n$, respectively.\nAfter calculating the similarity of the sentences in the document, we proceeded to use the k-means algorithm combined with the elbow optimization algorithm to find the optimal number of clusters. For this, we use the a parameter to be able to adjust the number of input properties for the clustering model to give the final text cluster, as mentioned in Fig. 1, in which, the number of input attributes corresponds to the number of embedded sentences."}, {"title": "3.4 Abstractive Summarization", "content": "Abstractive summarization creates the summary text as a sequence of words based on the input document sequences. We employ a sequence-to-sequence model combining transformer encoders and decoders. The encoder transforms the input sequence into a vector representation, and the decoder converts this vector into a target sequence. We evaluated three models, namely, VBD-LLaMA2-7B-50b, PhoGPT and Vistral-7B-Chat, where we found that VBD-LLaMA2-7B-50b performs the best. The encoder maps the input document extracted to a latent feature vector representation, and the decoder autoregressively generates the output summary one word at a time based on this representation. This allows producing novel phrasing and paraphrasing of the input content. Such post-processing improves the summarization performance by eliminating repetition, which can reduce coherence as VBD-LLaMA2-7B-50b may generate consecutive identical words."}, {"title": "4 Performance Evaluation and Comparison", "content": null}, {"title": "4.1 Hardware Configuration", "content": "The Vietnamese multi-document summarization model was built and tested on our server, whose hardware configuration is detailed in Table 1"}, {"title": "4.2 Evaluation Procedure", "content": "Experiment on extractive summarization: In this experiment, the model runs on VN-MDS data. The VN-MDS data provides a citation-based reference summary. In this process, we will refine the parameter a, as mentioned in Section 3.3. Adjusting the a parameter will affect the result of our model because the output of the extrative document summarization is the input of the abstractive summary module.\nComparison with existing hybrid approaches: To demonstrate the effectiveness of our hybrid extractive-abstractive summarization approach, we conduct a comparative evaluation against the state-of-the-art MDS model proposed by Thanh et al. [16]. This model also employs a combined extractive-abstractive strategy, making it a suitable benchmark for assessing our model's performance.\nComparison with existing non-hybrid approaches: To further isolate the contribution of our hybrid approach, we compare our model to other non-hybrid techniques that rely solely either on extraction methods or abstraction methods. For this comparison, we select MART, KL, and LSA as baseline models. These models have been recently identified as the top performers proposed in [11]."}, {"title": "4.3 Experiment Results", "content": "Experiment on extractive summarization: As shown in Table 2, our experiments on the extractive document summarization phase reveal that incorporating sentence relationships based on representation vectors created by pre-trained language model improves the model's ROUGE-2 score. However, this approach negatively impacts ROUGE-1 and ROUGE-L scores compared to a graph model using only sentence-to-sentence relationships based on word frequency vectors. This suggests that using only word frequency at the morphological level for sentence representation leads to better ROUGE-1 and ROUGE-L scores. The optimal ROUGE-2 score was achieved with an a value of 0.2, as illustrated in Table 2, which we adopted for the extraction phase.\nComparison with existing hybrid approaches: To better show the effectiveness of our model, we compare with the modern model used for MDS by Thanh et al. [16]. Our model's comparative results show a ROUGE-1 Precision of 62.8% compared to 61.77% of Thanh et al.'s model. Although our Recall score is slightly lower at 79.7 compared to 79.96 of the baseline, our F1-ROUGE-1 score is significantly better at 70.1% compared to 68.63% of the baseline. Additionally, our ROUGE-2 scores are remarkably higher that of Thanh et al.'s model. In terms of ROUGE-L, our Precision score is lower at 28.1% compared to 29.3% of the baseline, however, our Recall and F1 scores are again significantly better. Overall, our model demonstrates superiority over Thanh et al.'s model based on the VN-MDS dataset.\nComparison with existing non-hybrid approaches: To isolate the contribution of our hybrid extractive-abstractive summarization approach, we conduct a comparative evaluation against non-hybrid MDS models. As presented in Table 4, our proposed hybrid model are superior to the baselines across all ROUGE metrics, particularly F1-score ROUGE-1 (70.1% vs. KL's 60.2%) and Recall (ROUGE-1 and ROUGE-2). While our ROUGE-2 F1-score is slightly lower than MART's (39.6% vs. 41.6%), the difference is not significant. This demonstrates the effectiveness of our approach in capturing the salient information from Vietnamese text documents and generating high-quality summaries."}, {"title": "5 Conclusion", "content": "We have proposed a new Vietnamese multi-document summarization model combining extractive and abstractive techniques in a pipeline architecture. For extraction, we apply SBERT to identify salient sentences. These extracted sentences are then input to the VBD-LLaMA2-7B-50b language model for abstractive summarization. Experiments on the VN-MDS dataset demonstrate the efficacy of our approach, achieving competitive results over the existing methods.\nMoving forward, we aim to evaluate our model on additional Vietnamese datasets. We also plan to explore alternative deep learning models to enhance extraction and abstractive generation. In addition, we will investigate the application of our model to unstructured data. This is a challenging task due to the lack of a Vietnamese dataset for evaluating models on unstructured data."}]}