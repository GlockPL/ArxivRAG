{"title": "A Methodology Establishing Linear Convergence of Adaptive Gradient Methods under PL Inequality", "authors": ["Kushal Chakrabartia", "Mayank Baranwala"], "abstract": "Adaptive gradient-descent optimizers are the standard\nchoice for training neural network models. Despite their faster con-\nvergence than gradient-descent and remarkable performance in prac-\ntice, the adaptive optimizers are not as well understood as vanilla\ngradient-descent. A reason is that the dynamic update of the learning\nrate that helps in faster convergence of these methods also makes\ntheir analysis intricate. Particularly, the simple gradient-descent\nmethod converges at a linear rate for a class of optimization prob-\nlems, whereas the practically faster adaptive gradient methods lack\nsuch a theoretical guarantee. The Polyak-\u0141ojasiewicz (PL) inequality\nis the weakest known class, for which linear convergence of gradient-\ndescent and its momentum variants has been proved. Therefore, in\nthis paper, we prove that AdaGrad and Adam, two well-known adap-\ntive gradient methods, converge linearly when the cost function is\nsmooth and satisfies the PL inequality. Our theoretical framework\nfollows a simple and unified approach, applicable to both batch and\nstochastic gradients, which can potentially be utilized in analyzing\nlinear convergence of other variants of Adam.", "sections": [{"title": "1 Introduction", "content": "In this paper, we consider the problem of minimizing a possibly non-\nconvex objective function f : Rd \u2192 R,\nmin f(x).\nxeRd\n(1)\nAmong other applications, non-convex optimization appears in train-\ning neural network models. It is a standard practice to use adaptive\ngradient optimizers for training such models. Compared to gradient-\ndescent, these methods have been observed to converge faster and do\nnot require line search to determine the learning rates.\nAdaGrad [7] is possibly the earliest adaptive gradient optimizer.\nTo address the gradient accumulation problem in AdaGrad, the Adam\nalgorithm [11] was proposed. Adam and its variants have been\nwidely used to train deep neural networks in the past decade. Despite\nthe success of these adaptive gradient optimizers, we lack an under-\nstanding of why these methods work so well in practice. The theory\nof convergence of the adaptive optimizers has not been completely\ndeveloped. Since these methods work better than simple gradient-\ndescent or its momentum variants, it is natural to expect a similar\nor better convergence guarantee of adaptive optimizers compared\nto gradient-descent. One such theoretical difference between these\ntwo classes of methods is linear convergence. Specifically, gradient-\ndescent and its accelerated variants, such as the Nesterov accelerated\ngradient or Heavy-Ball method, are known to exhibit linear conver-\ngence for a class of cost functions [16]. On the other hand, most of\nthe adaptive gradient optimizers lack such a guarantee.\nLinear convergence guarantees of gradient-descent, its acceler-\nated variants, coordinate descent, and AdaGrad-Norm [22] (among\nthe adaptive gradient methods) have been proved for the class of\nsmooth and possibly non-convex cost functions that satisfy the\nPolyak-\u0141ojasiewicz (PL) inequality [10]. The PL inequality is the\nweakest condition among others, such as strong convexity, essen-\ntial strong convexity, weak strong convexity, and restricted secant\ninequality, that leads to linear convergence of gradient-descent and\nits accelerated variants to the solution of (1) [10]. The objective func-\ntions in standard machine learning problems like linear regression\nand logistic regression satisfy the PL inequality. For solving over-\nparameterized non-linear equations, [13] establishes a relation be-\ntween PL inequality and the condition number of the tangent kernel,\nand argued that sufficiently wide neural networks generally satisfy\nthe PL inequality. Motivated by linear convergence guarantees of\ngradient-descent and its aforementioned variants under the PL con-\ndition and the applicability of PL inequality on a set of machine\nlearning problems, we investigate linear convergence of AdaGrad\nand Adam under the PL inequality.\nTo present our results, we define the following notations and make\na set of assumptions as stated below. Let the gradient of f evaluated\nat x \u2208 Rd be denoted by \u2207 f(x) \u2208 Rd, and its i-th element be\ndenoted by i\u2207 f(x) for each dimension i \u2208 {1, ..., d}.\nAssumption 1. The minimum f of f exists and is finite, i.e.,\nmin\u2208Rd f(x)| < \u221e.\nAssumption 2. f is twice differentiable over its domain Rd and is\nL-smooth, i.e., L > 0 such that \\\u2207 f(x) \u2212 \u2207 f(y) \\ \u2264 L ||x \u2212 y|| for all x, y \u2208 Rd.\nAssumption 3. f satisfies the Polyak-\u0141ojasiewicz (PL) inequality,\ni.e., \u2203l > 0 such that \u00bd ||\u2207 f(x)||\u00b2 \u2265 l(f(x) \u2212 f*) for all x \u2208 Rd.\nAssumption 3 has been justified in the preceding paragraph. As-\nsumption 1-2 are standard in the literature of gradient-based opti-"}, {"title": "2 Linear convergence of AdaGrad", "content": "Theorem 1. Consider the AdaGrad algorithm in (3a)-(3b) with ini-\ntialization yo = 0d and xo \u2208 Rd and the parameter \u0454 \u2208 (0", "f(xk)\nd\n\u2211\u2212h\ni=1\n\u221a\n\u2212\n(\n<\n(5)\nCase-I": "First"}, {"title": "3 Linear convergence of Adam", "content": "Theorem 2. Consider the Adam algorithm in (4a)-(4c) with initial-```json\n{\n        \"content\": \"Theorem 2. Consider the Adam algorithm in (4a)-(4c) with initial-\nization \u03bc\u03bf = \u03bd\u03bf = 0a and xo \u2208 Rd and the parameters \u03b21k, \u03b22 \u2208\n[0, 1), \u0454 \u2208 (0, 1) . If Assumptions 1-3 hold, then \u2203\u03b2k \u2208 (0,1) such\nthat for \u03b2\u03b9\u03ba \u2208 [0, \u03b2k) there exists h > 0 such that if the stepsize\n0 < h < h, then (f(xk+1) - f*) \u2264 p(f(xk) - f*) for all k \u2265 0\nwhere p\u2208 (0, 1).\nProof. Under Assumption 2, from (2) we have\nL\nf(xk+1) \u2212 f(xk) \u2264 (Pk+1 \u2212 xk) \u2207 f(xk) + \u00bd ||xk+1 - xk||2 .\nUpon substituting above from (4c),\nf(xk+1) - f(xk)\n<\nUpon substituting \u00b5k+1,i above from (4a) and rearranging the terms\nin the numerator,\nf(xk+1) - f(xk)\n<\nCase-I: First, we consider the case vk+1,i > (1 \u2013 \u20ac)\u00b2 for all i \u2208\n{1, ...,d} for all k > T, where T < \u221e. Then, |\u221a\u221a + e| >\n\u221a + e. From (11), then we have\nf(xk+1) - f(xk)\n<\nUnder Assumption 1, we follow the argument in the proof of Theo-\nrem 1 after (6), and obtain\nlim = 0, Vi.\nk\u2192\u221e + \u0454\n(19)\nUnder Assumption 3, following the argument in the proof of\nTheorem 1 after (7), we have limk\u2192\u221e\u2207f(xk)\nOd, and\nlimk\u2192\u221e f(xk) = f*, and \u2203M \u2208 (0,\u221e) such that \u221a + \u20ac <\nM Vi. From (18), then we obtain\nf(xk+1)-f(xk)\nUnder Assumption 3 and defining C\u2081k = h(1 \u2013 \u03b8\u03ba) (1 \u2013 \u03b2\u03b9\u03ba)(1 \u2013\nh\ub298(1 - \u1e9e1k)), from above we get\nf(xk+1) - f* \u2264 (1 \u2212 C1k)(f(xk) \u2013 f*), \u2200k > T.\n2\nMoreover, 0 < h < implies that C\u2081k > 0 and 0 < h < \uc4f8\nimplies that C1 < 1. So, (1 \u2212 c\u2081) \u2208 (0,1) for 0 < h <\nmin{\nCase-II: Next, we consider the case when vk+1,i < (1 \u2013 \u20ac)\u00b2 for all\ni \u2208 {1, ..., d} for all k \u2265 0. Then, \u221a + \u0454 < 1. Also, vo = 0d\nand (4b) implies that vk,i \u2265 0\u2200k, i. From (11), then we have\nf(xk+1) - f(xk) <\nWe note that the above inequality is similar to (12) where \u221a +\u20ac\nin (12) is replaced by 1 and L is replaced by . So, we follow the\nproof above in Case-I, and instead of (18), we obtain that\nf(xk+1) - f(xk)\nUnder Assumption 3 and defining C2k = h(1 \u2013 \u03b8\u03ba)(1 \u2013 \u03b2\u03b9\u03ba)(1 \u2013\nh22(1 \u2013 \u03b21k))2l, from above we get\nf(xk+1) - f* \u2264 (1 \u2212 C2k)(f(xk) \u2013 f*), \u2200k \u2265 0. (22)\nThe rest of the proof follows the same argument as in the proof of\nTheorem 1.\""}, {"title": "4 AdaGrad with stochastic gradients", "content": "In practice, neural networks are trained with stochastic or mini-batch\ngradients at each iteration of the optimizer. So, in this section, we\npresent our analysis of AdaGrad in the stochastic setting. For sim-\nplicity, we consider only one data point in each iteration. However,\nour result is applicable to mini-batch with more than one data points.\nFor each data point D, we define the loss function l : Rd \u2192 R as\nl(., D) and its gradient g(x; D) = \u2207xl(x; D). Our aim is to mini-\nmize the empirical risk,\nmin f(x) := \u0395\u03c2 [l(x; D\u03c2)] .\nxeRd\n(23)\nAt each iteration k \u2265 0, AdaGrad randomly chooses a data point\nDk, based on the realization Sk of the random variable \u03da, and com-\nputes its stochastic gradient gsk (xk) = g(xk; D\u00a2k). We let gi,sk (xk)\ndenote the i-th element of gsk (xk), for each i \u2208 {1,..., d}. Instead\nof (3a)-(3b), xk and yk are updated as\n\u2212 h9i,$k (k)\n+\nCase-I: First, we consider the case when yk,i > (1 \u2013 \u20ac)\u00b2 for all\ni \u2208 {1, ..., d} for all k > T, where T < \u221e. Consider any iteration\nk > T. Then, |\u221a + e| > \u221a + e. From (26), then we have\n<\n\u221aite\n\u0395\u03b6\u03ba [f(xk+1)] - f(xk)\n(1 \u2013 LMch) if(x)\u00b2 \u2013 M\nh\njE VYj,tkje\n\u221ai,he"}, {"title": "5 Summary", "content": "We presented a framework that proves linear convergence of two\nadaptive gradient methods, namely AdaGrad and Adam in discrete-\ntime, for minimizing smooth objective functions that satisfy the PL\ninequality. Among the prior works on adaptive gradient methods,\nonly the AdaGrad-Norm algorithm and a continuous-time version of\nAdam have provable linear convergence, for a class of optimization\nproblems. Thus, our work contributes towards reducing the theoret-\nical gap between vanilla gradient-descent and the more successful\nadaptive gradient optimizers. The unifying approach in our frame-\nwork could be applicable in rigorously analyzing other adaptive gra-\ndient optimizers."}]}