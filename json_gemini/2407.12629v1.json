{"title": "A Methodology Establishing Linear Convergence of Adaptive Gradient Methods under PL Inequality", "authors": ["Kushal Chakrabarti", "Mayank Baranwala"], "abstract": "Adaptive gradient-descent optimizers are the standard\nchoice for training neural network models. Despite their faster con-\nvergence than gradient-descent and remarkable performance in prac-\ntice, the adaptive optimizers are not as well understood as vanilla\ngradient-descent. A reason is that the dynamic update of the learning\nrate that helps in faster convergence of these methods also makes\ntheir analysis intricate. Particularly, the simple gradient-descent\nmethod converges at a linear rate for a class of optimization prob-\nlems, whereas the practically faster adaptive gradient methods lack\nsuch a theoretical guarantee. The Polyak-\u0141ojasiewicz (PL) inequality\nis the weakest known class, for which linear convergence of gradient-\ndescent and its momentum variants has been proved. Therefore, in\nthis paper, we prove that AdaGrad and Adam, two well-known adap-\ntive gradient methods, converge linearly when the cost function is\nsmooth and satisfies the PL inequality. Our theoretical framework\nfollows a simple and unified approach, applicable to both batch and\nstochastic gradients, which can potentially be utilized in analyzing\nlinear convergence of other variants of Adam.", "sections": [{"title": "Introduction", "content": "In this paper, we consider the problem of minimizing a possibly non-\nconvex objective function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$,\n$\\min_{x\\in\\mathbb{R}^d} f(x)$.\n(1)\nAmong other applications, non-convex optimization appears in train-\ning neural network models. It is a standard practice to use adaptive\ngradient optimizers for training such models. Compared to gradient-\ndescent, these methods have been observed to converge faster and do\nnot require line search to determine the learning rates.\nAdaGrad [7] is possibly the earliest adaptive gradient optimizer.\nTo address the gradient accumulation problem in AdaGrad, the Adam\nalgorithm [11] was proposed. Adam and its variants have been\nwidely used to train deep neural networks in the past decade. Despite\nthe success of these adaptive gradient optimizers, we lack an under-\nstanding of why these methods work so well in practice. The theory\nof convergence of the adaptive optimizers has not been completely\ndeveloped. Since these methods work better than simple gradient-\ndescent or its momentum variants, it is natural to expect a similar\nor better convergence guarantee of adaptive optimizers compared\nto gradient-descent. One such theoretical difference between these\ntwo classes of methods is linear convergence. Specifically, gradient-\ndescent and its accelerated variants, such as the Nesterov accelerated\ngradient or Heavy-Ball method, are known to exhibit linear conver-\ngence for a class of cost functions [16]. On the other hand, most of\nthe adaptive gradient optimizers lack such a guarantee.\nLinear convergence guarantees of gradient-descent, its acceler-\nated variants, coordinate descent, and AdaGrad-Norm [22] (among\nthe adaptive gradient methods) have been proved for the class of\nsmooth and possibly non-convex cost functions that satisfy the\nPolyak-\u0141ojasiewicz (PL) inequality [10]. The PL inequality is the\nweakest condition among others, such as strong convexity, essen-\ntial strong convexity, weak strong convexity, and restricted secant\ninequality, that leads to linear convergence of gradient-descent and\nits accelerated variants to the solution of (1) [10]. The objective func-\ntions in standard machine learning problems like linear regression\nand logistic regression satisfy the PL inequality. For solving over-\nparameterized non-linear equations, [13] establishes a relation be-\ntween PL inequality and the condition number of the tangent kernel,\nand argued that sufficiently wide neural networks generally satisfy\nthe PL inequality. Motivated by linear convergence guarantees of\ngradient-descent and its aforementioned variants under the PL con-\ndition and the applicability of PL inequality on a set of machine\nlearning problems, we investigate linear convergence of AdaGrad\nand Adam under the PL inequality.\nTo present our results, we define the following notations and make\na set of assumptions as stated below. Let the gradient of f evaluated\nat $x \\in \\mathbb{R}^d$ be denoted by $\\nabla f(x) \\in \\mathbb{R}^d$, and its $i$-th element be\ndenoted by $\\nabla_i f(x)$ for each dimension $i \\in \\{1, ..., d\\}$.\nAssumption 1. The minimum $f_*$ of $f$ exists and is finite, i.e.,\n$|\\min_{x\\in\\mathbb{R}^d} f(x)| < \\infty$.\nAssumption 2. $f$ is twice differentiable over its domain $\\mathbb{R}^d$ and is\n$L$-smooth, i.e., $L > 0$ such that $|\\nabla f(x) - \\nabla f(y) || \\leq L ||x-y||$\nfor all $x, y \\in \\mathbb{R}^d$.\nAssumption 3. $f$ satisfies the Polyak-\u0141ojasiewicz (PL) inequality,\ni.e., $\\exists l > 0$ such that $\\frac{1}{2} ||\\nabla f(x)||^2 \\geq l(f(x) - f_*)$ for all $x \\in \\mathbb{R}^d$.\nAssumption 3 has been justified in the preceding paragraph. As-\nsumption 1-2 are standard in the literature of gradient-based opti-"}, {"title": "Linear convergence of AdaGrad", "content": "Theorem 1. Consider the AdaGrad algorithm in (3a)-(3b) with ini-\ntialization $y_0 = 0_d$ and $x_0 \\in \\mathbb{R}^d$ and the parameter $\\epsilon \\in (0,1)$. If\nAssumptions 1-3 hold, then there exists $\\bar{h} > 0$ such that if the step\nsize $0 < h< \\bar{h}$, then $(f(x_{k+1}) - f_*) \\leq \\rho(f(x_{k}) - f_*)$ for all\n$k \\geq 0$, where $\\rho\\in (0, 1)$.\nProof. Consider an arbitrary iteration $k\\geq 0$. Under Assumption 2,\nfrom (2) we have\n$f(x_{k+1}) - f(x_{k}) \\leq (x_{k+1} - x_{k})^T \\nabla f(x_{k}) + \\frac{L}{2} |x_{k+1} - x_{k}||^2$.\nUpon substituting above from (3b),\n$f(x_{k+1}) - f(x_{k})\\leq\n\\sum_{i=1}^d -h\\frac{|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k+1,i} + \\epsilon}}+\\frac{Lh^2}{2}\\frac{|\\nabla_if(x_k)|^2}{{(\\sqrt{Y_{k+1,i} + \\epsilon})}^2}.\n(5)\nCase-I: First, we consider the case when $Y_{k+1,i} > (1 - \\epsilon)^2$ for all\n$i \\in \\{1, ..., d\\}$ for all $k > T$, where $T < \\infty$. Recall that $\\{Y_{k,i}\\}$ is\na non-decreasing sequence for all $i$. Consider any iteration $k > T$.\nThen, $|\\sqrt{Y_{k+1,i} + \\epsilon^2} > \\sqrt{Y_{k+1,i}} + \\epsilon$. From (5), then we have\n$f(x_{k+1}) - f(x_{k})\\leq -h(1-\\frac{Lh}{2})\\sum_{i=1}^d \\frac{|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k+1,i} + \\epsilon}}.\n(6)\nIf $0 < h < \\frac{2}{L}$, from above we have $f(x_{k+1}) \\leq f(x_{k})$. Since\n$\\left\\{f(x_k): k \\geq T\\right\\}$ is a decreasing sequence and, under Assump-\ntion 1, bounded from below by $f_*$, the sequence converges with\n$\\lim_{k\\rightarrow\\infty} f(x_k) < \\infty$. Next, upon summation from $t = T$ to $t = k$\non both sides of (6), due to telescopic cancellation on the L.H.S.,\n$f(x_{k+1}) - f(x) \\leq -h(1-\\frac{Lh}{2})\\sum_{t=T}^k \\sum_{i=1}^d\\frac{|\\nabla_if(x_t)|^2}{\\sqrt{Y_{t+1,i} + \\epsilon}},$\nwhich means\n$0\\leq \\lim_{k\\rightarrow\\infty} h\\left(1-\\frac{Lh}{2}\\right) \\sum_{t=T}^k \\sum_{i=1}^d\\frac{|\\nabla_if(x_t)|^2}{\\sqrt{Y_{t+1,i} + \\epsilon}}\\leq \\lim_{k\\rightarrow\\infty} (f(x_T) - f(x_{k+1})) \\leq f(x) - f_*$.\nSo, $\\lim_{k\\rightarrow\\infty} \\sum_{t=T}^k \\sum_{i=1}^d\\frac{|\\nabla_if(x_t)|^2}{\\sqrt{Y_{t+1,i} + \\epsilon}}$ is bounded, which implies that\n$\\lim_{k\\rightarrow\\infty} \\frac{|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k+1,i} + \\epsilon}} = 0, \\forall i$.\n(7)\nThus, either $\\lim_{k\\rightarrow\\infty} |\\nabla_if(x_k)| = 0$ or $\\lim_{k\\rightarrow\\infty} \\sqrt{Y_{k+1,i}} = \\infty$.\nConsider the case $\\lim_{k\\rightarrow\\infty} \\sqrt{Y_{k+1,i}} = \\infty$. From (7), we have\n$\\frac{|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k+1,i} + \\epsilon}} = O(1)$, which implies $|\\nabla_if(x_k)| =$\n$O((\\sqrt{Y_{k+1,i} + \\epsilon})^{-0.5})$. So, if $\\lim_{k\\rightarrow\\infty} \\sqrt{Y_{k+1,i}} = \\infty$, we have\n$\\lim_{k\\rightarrow\\infty}|\\nabla_if(x_k)| = 0$. Since both $\\lim_{k\\rightarrow\\infty} \\frac{|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k+1,i} + \\epsilon}} = 0$ and\n$\\lim_{k\\rightarrow\\infty}|\\nabla_if(x_k)| = 0$, it is possible only if $\\lim_{k\\rightarrow\\infty} |\\nabla_if(x_k)| =$\n$0$. So, we have proved that $\\lim_{k\\rightarrow\\infty} |\\nabla_if(x_k)| = 0$ is true. Under\nAssumption 3, then we have $\\lim_{k\\rightarrow\\infty} f(x_k) = f_*$. The above ar-\ngument further shows that $\\lim_{k\\rightarrow\\infty} \\sqrt{Y_{k+1,i}} = \\infty$ is possible only\nin the trivial case where the function is at the minimum point. In\nthe non-trivial case, therefore, $\\sqrt{Y_{k+1,i} + \\epsilon}$ is bounded. Then, for\n$0 < h < \\frac{2}{L}, \\exists M \\in (0, \\infty)$ such that $\\sqrt{Y_{k+1,i} + \\epsilon} < M\\forall i$. From (6),\n$f(x_{k+1}) - f(x_{k})\\leq - \\left(1-\\frac{Lh}{2}\\right) \\frac{h}{M} \\sum_{i=1}^d |\\nabla_i f(x_k)|^2$.\nUnder Assumption 3, from above we have\n$f(x_{k+1}) - f(x_{k})\\leq - \\left(1-\\frac{Lh}{2}\\right) \\frac{h}{M} ||\\nabla f(x_k)||^2 \\leq -2\\left(1-\\frac{Lh}{2}\\right) \\frac{h}{M}l(f(x_{k}) - f_*)$.\nUpon defining $c_1 = \\frac{h}{M} \\left(1 - \\frac{Lh}{2}\\right) 2l$, we rewrite the above as\n$f(x_{k+1}) - f(x_{k}) \\leq -c_1(f(x_{k}) - f_*),$\nwhich means\n$f(x_{k+1}) - f_* \\leq (1 - c_1)(f(x_{k}) - f_*), \\forall k > T$.\n(8)\nMoreover, $0 < h < \\frac{2}{L}$ implies that $c_1 > 0$ and $0 < h < \\frac{2}{L}$ implies\nthat $c_1 < 1$. So, $(1 - c_1) \\in (0, 1)$ for $0 < h < \\min\\{\\frac{2}{L}, \\frac{M}{L}\\}$."}, {"title": "Linear convergence of Adam", "content": "Theorem 2. Consider the Adam algorithm in (4a)-(4c) with initial-\nization $\\mu_0 = \\nu_0 = 0_d$ and $x_0 \\in \\mathbb{R}^d$ and the parameters $\\beta_{1k}, \\beta_2 \\in\n[0, 1), \\epsilon \\in (0, 1) $. If Assumptions 1-3 hold, then $\\exists\\bar{\\beta}_k \\in (0,1)$ such\nthat for $\\beta_{1k} \\in [0, \\bar{\\beta}_k)$ there exists $\\bar{h} > 0$ such that if the stepsize\n$0 < h< \\bar{h}$, then $(f(x_{k+1}) - f_*) \\leq \\rho(f(x_{k}) - f_*)$ for all $k \\geq 0$\nwhere $\\rho\\in (0, 1)$.\nProof. Under Assumption 2, from (2) we have\n$f(x_{k+1}) - f(x_{k}) \\leq (x_{k+1} - x_{k})^T \\nabla f(x_{k}) + \\frac{L}{2} |x_{k+1} - x_{k}||^2$.\nUpon substituting above from (4c),\n$f(x_{k+1}) - f(x_{k})\\leq\\sum_{i=1}^d -h\\frac{\\mu_{k+1,i}\\nabla_if(x_k)}{\\sqrt{V_{k+1,i} + \\epsilon}}+\\frac{Lh^2}{2}\\frac{|\\mu_{k+1,i}|^2}{{(\\sqrt{V_{k+1,i} + \\epsilon})}^2}.\n(11)\nCase-I: First, we consider the case $V_{k+1,i} > (1 - \\epsilon)^2$ for all $i \\in$\n$\\{1, ...,d\\}$ for all $k > T$, where $T < \\infty$. Then, $|\\sqrt{V_{k+1,i} +\n\\epsilon}| > \\sqrt{V_{k+1,i}} + \\epsilon$. From (11), then we have\n$f(x_{k+1}) - f(x_{k})\\leq\\sum_{i=1}^d\\left(\\frac{-h\\mu_{k+1,i}\\nabla_if(x_k)}{\\sqrt{V_{k+1,i} + \\epsilon}}+\\frac{Lh^2}{2}\\frac{|\\mu_{k+1,i}|^2}{\\sqrt{V_{k+1,i} + \\epsilon}}\\right).\n(12)\nUpon substituting $\\mu_{k+1,i}$ above from (4a) and rearranging the terms\nin the numerator,\n$f(x_{k+1}) - f(x_{k})\\leq\\sum_{i=1}^d\\frac{-h(1-\\beta_{1k})(1-h\\frac{L}{2}(1-\\beta_{1k})) |\\nabla_if(x_k)|^2 -h\\frac{L}{2}\\beta_{1k}^2 |\\mu_{k,i}|^2}{\\sqrt{V_{k+1,i} + \\epsilon}}\\frac{-h\\frac{L}{2}\\beta_{1k}(1 - h\\frac{L}{2}(1 - \\beta_{1k}))\\mu_{k,i}\\nabla_if(x_k)}{\\sqrt{V_{k+1,i} + \\epsilon}}.\nfor any $\\theta_k \\in (0,1)$. We define two set of indices $I = \\{i| i \\in$\n$\\{1,...,d\\}, \\mu_{k,i} \\neq 0\\}$ and its complement $I'$. Then, we rewrite the\nabove inequality as\n$f(x_{k+1}) - f(x_{k})\\leq\\sum_{i\\in I'}\\frac{-h(1-\\beta_{1k})(1-\\theta_k)(1-h\\frac{L}{2}(1-\\beta_{1k})) |\\nabla_if(x_k)|^2}{\\sqrt{V_{k+1,i} + \\epsilon}}\\sum_{i\\in I}\\frac{-h\\theta_k(1-\\beta_{1k})(1-h\\frac{L}{2}(1-\\beta_{1k})) |\\nabla_if(x_k)|^2}{\\sqrt{V_{k+1,i} + \\epsilon}}\\frac{-h\\frac{L}{2}\\beta_{1k}^2 |\\mu_{k,i}|^2 + \\beta_{1k}(1 - h\\frac{L}{2}(1 - \\beta_{1k}))\\mu_{k,i}\\nabla_if(x_k)}{\\sqrt{V_{k+1,i} + \\epsilon}}.\n(13)\nFor $\\beta_{1k} < 1$ and $h < \\frac{2}{L(1-\\beta_{1k})}$, the last term on the R.H.S. in (13)\nis negative. So, for $i \\in I$, we want to show that\n$\\frac{\\theta_k(1-\\beta_{1k})(1-h\\frac{L}{2}(1-\\beta_{1k})) |\\nabla_if(x_k)|^2}{\\sqrt{V_{k+1,i} + \\epsilon}}\\geq h\\frac{\\frac{L}{2}\\beta_{1k}^2 |\\mu_{k,i}|^2 + \\beta_{1k}(1 - h\\frac{L}{2}(1 - \\beta_{1k})) |\\mu_{k,i}\\nabla_if(x_k)|}{\\sqrt{V_{k+1,i} + \\epsilon}},$\n(14)\nfor some $\\theta_k \\in (0,1)$ and some $h > 0$, which would imply that\n$f(x_{k+1}) - f(x_{k}) \\leq 0$, from (13). Consider any $i \\in I$. We note\nthat (14) is equivalent to\n$\\theta_k(1 - \\beta_{1k}) |\\nabla_if(x_k)|^2 - \\frac{L}{2}\\beta_{1k} | \\mu_{k,i}\\nabla_if(x_k)| \\geq h\\frac{L}{2}\\frac{\\beta_{1k}^2}{|\\mu_{k,i}|^2}{\\sqrt{V_{k+1,i} + \\epsilon}}+\\frac{L}{2}h(1-\\beta_{1k}) (\\theta_k (1 - \\beta_{1k}) |\\nabla_if(x_k)|^2 - \\beta_{1k} | \\mu_{k, i}\\nabla_if(x_k)|),$\nwhich is implied if\n$\\theta_k(1 - \\beta_{1k}) |\\nabla_if(x_k)|^2 - \\frac{L}{2}\\beta_{1k} | \\mu_{k,i}\\nabla_if(x_k)| \\geq h\\frac{L}{2}\\frac{\\beta_{1k}^2}{|\\mu_{k,i}|^2}{\\sqrt{V_{k+1,i} + \\epsilon}}+\\frac{L}{2}h(1-\\beta_{1k}) (\\theta_k(1 - \\beta_{1k}) |\\nabla_if(x_k)|^2 - \\beta_{1k} | \\mu_{k, i}\\nabla_if(x_k)|).\n(15)\nFor the case $\\mu_{k,i}| = \\infty$, we choose $\\beta_{1k} = 0$ and $h < \\frac{2}{L}$ so\nthat (15) holds. Otherwise, for the case $|\\mu_{k,i} < \\infty$, if the L.H.S.\nin (15) is positive, then (15) holds for\n$h< \\frac{p_k}{q_k},$\n(16)"}, {"title": "AdaGrad with stochastic gradients", "content": "In practice, neural networks are trained with stochastic or mini-batch\ngradients at each iteration of the optimizer. So, in this section, we\npresent our analysis of AdaGrad in the stochastic setting. For sim-\nplicity, we consider only one data point in each iteration. However,\nour result is applicable to mini-batch with more than one data points.\nFor each data point $D$, we define the loss function $l : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ as\n$l(., D)$ and its gradient $g(x; D) = \\nabla_x l(x; D)$. Our aim is to mini-\nmize the empirical risk,\n$\\min_{x\\in\\mathbb{R}^d} f(x) := E_{\\zeta} [l(x; D_{\\zeta})] .\n(23)\nAt each iteration $k \\geq 0$, AdaGrad randomly chooses a data point\n$D_k$, based on the realization $\\zeta_k$ of the random variable $\\zeta$, and com-\nputes its stochastic gradient $g_{\\zeta_k}(x_k) = g(x_k; D_{\\zeta_k})$. We let $g_{i,\\zeta_k}(x_k)$\ndenote the $i$-th element of $g_{\\zeta_k}(x_k)$, for each $i \\in \\{1,..., d\\}$. Instead\nof (3a)-(3b), $x_k$ and $y_k$ are updated as\n$x_{k+1,i} = x_{k,i} - h\\frac{g_{i,\\zeta_k}(x_k)}{\\sqrt{Y_{k,i} + \\epsilon}},$\n(24a)\n$Y_{k+1,i} = Y_{k,i} + |g_{i,\\zeta_k}(x_k)|^2.$\n(24b)\nFor each iteration $k \\geq 0$ we define the following.\n*   Let $E_{\\zeta_k}[]$ denote the conditional expectation of a function the\n    random variable $\\zeta_k$, given the current $x_k$ and $y_k$.\n*   Let $E_k[]$ denote the total expectation of a function of the ran-\n    dom variables $\\{\\zeta_0,..., \\zeta_k\\}$ given the initial condition $x_0$ and $y_0$.\n    Specifically, $E_k[] = E_{\\zeta_0,...,\\zeta_k}[]$, $k \\geq 0$.\n*   For each $i \\in \\{1,...,d\\}$, define the conditional vari-\n    ance of $g_{i,\\zeta_k}(x_k)$, which is a function of the random vari-\n    able $\\zeta_k$, given the current $x_k$ and $y_k$ as $Var_{\\zeta_k} [g_{i,\\zeta_k}(x_k)] =$\n    $E_{\\zeta_k} [|g_{i,\\zeta_k}(x_k) - E_{\\zeta_k} [g_{i,\\zeta_k}(x_k)]|^2] =$\n    $E_{\\zeta_k} [|g_{i,\\zeta_k}(x_k)|^2] - E_{\\zeta_k} [g_{i,\\zeta_k}(x_k)]|^2$.\nWe make two additional standard assumptions [2, 18] for stochas-\ntic gradients as follows. Assumption 5 is regarding boundedness of\ncoordinate-wise affine noise variance [18].\nAssumption 4. At each iteration $k \\geq 0$, the stochastic gradient is\nan unbiased estimate of true gradient, i.e., $E_{\\zeta_k} [g_{\\zeta_k}(x_k)] = \\nabla f(x_k)$.\nAssumption 5. For each $i \\in \\{1,...,d\\}$, there exist two non-\nnegative real scalar values $V_{1,i}$ and $V_{2,i}$ such that, for each $k \\geq 0$,\n$Var_{\\zeta_k} [g_{i,\\zeta_k}(x_k)] \\leq V_{1,i} + V_{2,i} |\\nabla_if(x_k)|^2$.\nWe define $M = \\max_i V_{1,i}$ and $M_G = \\max_i (V_{2,i} + 1)$.\nTheorem 3. Consider the AdaGrad algorithm in (24a)-(24b) with\ninitialization $y_0 = 0_d$ and the parameter $\\epsilon \\in (0,1)$. If Assump-\ntions 1-5 hold, then there exists $\\bar{h} \\in (0,\\infty)$ such that the following\nstatements are true.\n(i) $\\bar{h} > 0$ such that if the step size $0 < h <\\bar{h}$, then\n$E_{\\zeta_k} [f(x_{k+1})] - f(x_{k}) \\leq \\rho(f(x_{k}) - f_*) + w$, for all $k \\geq 0$,\nwhere $\\rho \\in (0, 1)$ and $w = O(M)$.\n(ii) $\\lim_{x\\rightarrow\\infty} ||\\nabla f(x_k) ||^2 > \\frac{d}{2} \\frac{LMh}{1-\\frac{L}{2}M_Gh}$.\n(iii) Given arbitrary choices of the initial $x_0 \\in \\mathbb{R}^d$,\n$\\lim_{k\\rightarrow\\infty} E_k [f(x_{k+1})] \\leq \\frac{w}{1-\\rho}.$\nProof. Consider an arbitrary iteration $k\\geq 0$. Under Assumption 2,\nfrom (2) we have\n$f(x_{k+1}) - f(x_{k}) \\leq (x_{k+1} - x_{k})^T \\nabla f(x_{k}) + \\frac{L}{2} |x_{k+1} - x_{k}||^2$.\nUpon taking conditional expectation on both sides and substituting\nabove from (24a),\n$E_{\\zeta_k} [f(x_{k+1})] - f(x_{k})<\\sum_{i=1}^d\\frac{-h\\nabla_if(x_k)\\nabla_if(x_k)}{\\sqrt{Y_{k,i} + \\epsilon}}+\\frac{Lh^2}{2}\\frac{E_{\\zeta_k} [|g_{i,\\zeta_k}(x_k)|^2]}{|{\\sqrt{Y_{k,i} + \\epsilon}}|^2}.\n(25)\nwhere the last equality follows from Assumption 4. From the defini-\ntion of conditional variance of $g_{i,\\zeta_k}(x_k)$ and Assumptions 4-5,\n$E_{\\zeta_k} [|g_{i,\\zeta_k}(x_k)|^2] \\leq Var_{\\zeta_k} [g_{i,\\zeta_k}(x_k)] + |\\nabla_if(x_k)|^2$\n$\\leq V_{1,i} + (V_{2,i} + 1) |\\nabla_if(x_k)|^2$\n$< M + M_G|\\nabla_if(x_k)|^2$.\nUpon substituting from above in (25),\n$E_{\\zeta_k} [f(x_{k+1})] - f(x_{k})<\\sum_{i=1}^d\\left(\\frac{-h|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k,i} + \\epsilon}}+\\frac{Lh^2}{2}\\frac{M + M_G|\\nabla_if(x_k)|^2}{|{\\sqrt{Y_{k,i} + \\epsilon}}|^2}\\right).\n(26)\nCase-I: First, we consider the case when $y_{k,i} > (1 - \\epsilon)^2$ for all\n$i \\in \\{1, ..., d\\}$ for all $k > T$, where $T < \\infty$. Consider any iteration\n$k > T$. Then, $|\\sqrt{Y_{k,i} + \\epsilon}|^2 > \\sqrt{Y_{k,i}} + \\epsilon$. From (26), then we have\n$E_{\\zeta_k} [f(x_{k+1})] - f(x_{k})<(1-\\frac{L}{2}M_Gh)\\sum_{i=1}^d \\frac{|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k,i} + \\epsilon}} - \\frac{L}{2}Mh.\n(27)\nFor $h < \\frac{2}{LM_G}$, we have $(1 - \\frac{L}{2}M_Gh) > 0$. Now, except at the trivial\npoint $\\nabla f(x_k) = 0_d$, there exists at least one $j \\in \\{1, ...,d\\}$ for\nwhich $\\nabla_jf(x_t) \\neq 0$. We consider the non-empty set $I = \\{i|i \\in$\n$\\{1, ..., d\\}, \\nabla_if(x_t) \\neq 0\\}$ and its complement $I'$. Then, we can\nrewrite the R.H.S. above as\n$ -\\frac{L}{2}Mh+ \\sum_{i=1}^d(1-\\frac{L}{2}M_Gh)\\frac{|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k,i} + \\epsilon}}=\\sum_{j \\in I}(1-\\frac{L}{2}M_Gh)\\frac{|\\nabla_jf(x_k)|^2}{\\sqrt{Y_{k,j} + \\epsilon}} +\\sum_{i \\in I'}(1-\\frac{L}{2}M_Gh)\\frac{|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k,i} + \\epsilon}} - \\frac{L}{2}Mh.$\nUpon rearranging the terms above, we have\n$ <\\sum_{j \\in I}(1-\\frac{L}{2}M_Gh)\\frac{|\\nabla_jf(x_k)|^2}{\\sqrt{Y_{k,j} + \\epsilon}} + \\sum_{i \\in I'}(1-\\frac{L}{2}M_Gh)\\frac{|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k,i} + \\epsilon}} - \\frac{L}{2}Mh\\sum_{j \\in I'}\\frac{|\\nabla_jf(x_k)|^2}{\\sqrt{Y_{k,j} + \\epsilon}}>0.\n(28)\nSince $\\sum_{j \\in I'}\\frac{|\\nabla_jf(x_k)|^2}{\\sqrt{Y_{k,j} + \\epsilon}}< \\infty$, both the numerator and denominator in\nthe R.H.S. of (28) are $O (\\sum_{j \\in I'}\\frac{|\\nabla_jf(x_k)|^2}{\\sqrt{Y_{k,j} + \\epsilon}})$. So, the R.H.S. of (28)\nis positive. Then, $\\bar{h} > 0$ can be chosen to satisfy (28), for which\n$(1-\\frac{L}{2}M_Gh)\\sum_{i=1}^d \\frac{|\\nabla_if(x_k)|^2}{\\sqrt{Y_{k,i} + \\epsilon}} - \\frac{L}{2}Mh> 0$.\n(29)"}]}