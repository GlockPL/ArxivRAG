{"title": "Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization", "authors": ["Jeongseok Hyun", "Su Ho Han", "Hyolim Kang", "Joon-Young Lee", "Seon Joo Kim"], "abstract": "The vocabulary size in temporal action localization (TAL) is constrained by the scarcity of large-scale annotated datasets. To address this, recent works incorporate powerful pre-trained vision-language models (VLMs), such as CLIP, to perform open-vocabulary TAL (OV-TAL). However, unlike VLMs trained on extensive image/video-text pairs, existing OV-TAL methods still rely on small, fully labeled TAL datasets for training an action localizer. In this paper, we explore the scalability of self-training with unlabeled YouTube videos for OV-TAL. Our self-training approach consists of two stages. First, a class-agnostic action localizer is trained on a human-labeled TAL dataset and used to generate pseudo-labels for unlabeled videos. Second, the large-scale pseudo-labeled dataset is combined with the human-labeled dataset to train the localizer. Extensive experiments demonstrate that leveraging web-scale videos in self-training significantly enhances the generalizability of an action localizer. Additionally, we highlighted issues with existing OV-TAL evaluation schemes and proposed a new evaluation protocol.", "sections": [{"title": "1 Introduction", "content": "Temporal action localization (TAL) involves localizing and classifying action instances in long videos. However, annotating large TAL datasets with extensive action vocabularies is costly, limiting models to small vocabularies. Developing TAL models that cover any target actions has been a long-standing goal.\nAs relevant research directions, Open-set TAL [1] aims to localize all actions by assigning base classes and marking novel actions as unknown, and Open-world TAL [50] extends open-set TAL by integrating continual learning, where the model is updated with annotations of novel actions after the initial training phase. However, both settings are not suitable to achieve the goal since they cannot classify unseen actions. In contrast, recent works [29,49] adopted a zero-shot (ZS) learning approach, leveraging semantic information from language models [10,27] to classify unseen actions. However, as the main goal in ZS learning is to recognize concepts not seen during training [42], it strictly restricts the use of samples of unseen action classes for training. This stringent restriction hinders scaling TAL models with large-scale web data.\nDifferent from ZS learning, Zareian et al. [47] proposed open-vocabulary (OV) learning for object detection that alleviates the restriction of training data. Specifically, OV learning leverages image-text pairs for training, where the classes used for testing may or may not be present in the training data. This relaxed constraint allows vision-language models (VLMs) like CLIP [33], which are pre-trained on extensive image-caption pairs, to demonstrate robust zero-shot performance across various tasks, including open vocabulary temporal action localization (OV-TAL).\nExisting OV-TAL methods [15, 28, 32] utilize a VLM for classification. Nevertheless, their class-agnostic action localizers continue to rely on small, fully labeled TAL datasets, even within the OV learning framework. This approach overlooks the vast potential of numerous web videos for enhancing action localization. To address this, we explore self-training with additional unlabeled videos to improve the generalization capability of action localization. Although the class-agnostic action localizer can already detect the novel actions [20], we found that self-training with large-scale web video can improve cross-category generalization capabilities. Our training pipeline is illustrated in Fig. 1 (Left).\nWe study two types of data sources for self-training: in-domain and open-domain. In-domain data is acquired by splitting the target benchmark, indicating that it includes videos from novel categories of the target benchmark. On the other hand, Open-domain data includes random web videos. Although in-domain data is considered gold data, it is confined to the benchmark. In contrast, open-domain data is a scalable source as more videos can be scraped from the web. Fig. 1 (Right) shows the results on the FineAction dataset [23], which covers various video domains. Using 100k random videos, we surpass the performance of in-domain data for both base and novel classes. This proves the effectiveness of self-training with web videos. More scaling results are presented in Tab. 1."}, {"title": "2 Related Works", "content": "2.1 Beyond Closed-Vocabulary TAL\nPre-VLM Era. Enlarging the vocabulary size of TAL models has been a long-standing goal, studied even before the advent of VLMs. AherNet [24] exploits the combination of TAL and action classification datasets to expand the vocabulary size of TAL through adversarial learning, but its vocabulary size is still limited to the training dataset. Aiming to generalize to unseen actions, Zhang et al. [49] explored zero-shot learning in TAL by leveraging the language model embeddings [27] of seen and unseen actions. OpenTAL [1] tackles the open-set setting that localizes and classifies unseen actions as the unknown category on top of seen actions based on the learned evidential uncertainty.\nPost-VLM Era. Recently, OV-TAL has been explored based on a VLM [33]. The decoupled framework consists of a class-agnostic action localizer and an open-vocabulary action classifier is studied in [15,16,35]. Ju et al. [15] adapted CLIP [33] for the video domain with extra learnable modules: learnable prompts and transformer encoder for temporal modeling. In the follow-up [16], the prompt tuning method is further studied, such as LLM-generated and visually condi-tioned prompting, and the use of optical flow with VLM is introduced. Rathod et al. [35] explored the performance using different VLMs [14,31,33] and modalities including optical flow and audio. STALE [28] is the one-stage framework while UnLoc [46] unifies other video localization tasks, such as action segmentation and moment retrieval.\nOur Work. We explore the scalability of self-training with web videos for OV-TAL, especially aiming to enhance the generalization ability of \"action localiza-tion\".\n2.2 Vision-Language Models\nImage VLMs. CLIP [33] shows remarkable performance in zero-shot image recognition, and the generalization ability is demonstrated across diverse domains. This is attributed to the utilization of large-scale image-text datasets scraped from the web. For example, CLIP [33] and OpenCLIP [8] are trained on 400M and 2B image-text pairs. Such scalability offers benefits over manually annotated datasets, such as ImageNet-21k [9] with 14M images.\nVideo VLMs. Compared to image VLMs, video VLMs are under-explored due to a lack of large-scale video-caption datasets. Initially, image VLMs are adapted to the video domain with prompt tuning and extra transformer layers to capture the temporal relationship [15,30]. ViFi-CLIP [34] shows that simply fine-tuning CLIP in video snippets without temporal modeling improves the performance. Open-VCLIP [41] prevents the loss of generalization during video fine-tuning of CLIP through the regularization method. MAXI [22] explores the use of LLMs [4, 18] to generate video-caption pairs in the Kinetics videos [7] while ViCLIP [39] is trained on InternVid [39] dataset composed of 234M pairs."}, {"title": "3 Methodology", "content": "To encompass largely available unlabeled untrimmed video to TAL, we adopt the open-vocabulary learning setting into TAL. As a solution, we propose the framework, Self-Training for Open-Vocabulary Temporal Action Localization, namely, STOV-TAL. This section provides a rigorous definition of this setting with the evaluation protocol. Then, we describe the straightforward yet effective architecture and the training method that improves the generalization ability of action localization.\n3.1 Problem Definition of OV-TAL\nInput for OV-TAL consists of an untrimmed video, V, and C number of target action categories, $C = \\{c_i\\}_{i=1}^C$, which are unbounded and can be flexibly manipu-lated by users. Its output consists of M action instances, $\\Psi = \\{(t_s, t_e, c, s)_i\\}_{i=1}^M$, where $t_s, t_e, c$, and s denote the start and end times of the action, the action category, and the confidence score, respectively.\nRegarding the training data utilization, the OV-TAL setting offers a more flexible approach, as depicted in Fig. 3. In ZS-TAL, training data is limited to videos with human-annotated TAL labels corresponding to $C_{train}$ which is a disjoint set following $C_{train} \\cap C_{test} = \\varnothing$. However, the OV-TAL setting relaxes this strict restriction of training data. Specifically, we do not limit the use of data sources which may include actions of $C_{test}$ unless the TAL annotations are provided together. This allows for the incorporation of abundant unlabeled, random videos from the web. As a result, training the action localizer is no longer limited to the small-scale TAL datasets; instead, it involves exploiting data with diverse categories and domains. Therefore, the OV-TAL setting is more suitable for exploring generalization ability.\n3.2 Evaluation Protocol for OV-TAL\nExisting ZS-TAL benchmarks [15] face two issues: (1) they do not evaluate the seen categories, but only unseen categories, and (2) they rely on random category splits that do not consider the category frequency. To address these, we introduce"}, {"title": "3.3 STOV-TAL Architecture", "content": "We decouple the localization and classification of actions into two components in the architecture: (1) class-agnostic action localizer; (2) open-vocabulary action classifier based on the video VLM [34]. Fig. 4 illustrates the detailed pipeline. Note that this architecture supports various CLIP-like VLMs, which typically include separate uni-modal encoders for video and text, represented as the video encoder ($\\Phi_\\nu$) and the text encoder ($\\Phi_\\tau$).\nVideo Feature Extraction with VLM. Frames are sampled from the input video using the overlapping sliding window approach, commonly used in the TAL literature [48]. This takes consecutive frames of window size and slides a window"}, {"title": "3.4 Learning to Detect Every Action", "content": "A class-agnostic action localizer has demonstrated the ability to localize actions that were not seen during training [15,20]. Leveraging its cross-category general-ization ability, our architecture can detect and classify any input actions without requiring specialized training methods. Nonetheless, its generalization ability re-mains limited, particularly for unseen domains (i.e., cross-dataset evaluation). This is attributed to the small scale of the TAL datasets that lack diverse ac-tion categories or domains. Inspired by the semi-supervised learning commonly employed in data-scarce scenarios, we aim to enhance the action localizer's gener-alization ability through the self-training method [17,43]. As illustrated in Fig. 1, self-training comprises two stages: (1) Training a model on a labeled dataset; and (2) Training the model on an unlabeled dataset, using pseudo-labels generated by the previous model.\nLearning from Labeled Videos. In the first stage, the action localizer is trained using the human-labeled TAL dataset. Since we employ the class-agnostic action localizer, the classification is about distinguishing the foreground or back-ground of actions in a video. As described in Sec 3.3, the output from the two decoders, $\\Psi_{agn}$, includes the start and end times of the action and the actionness score. These are trained with the DIoU loss [51] and Focal loss [21], respectively.\nLearning from Unlabeled Videos. In the second stage, we exploit additional unlabeled videos for training. To obtain pseudo-labels of these videos, the ac-tion localizer trained in the first stage is employed to generate the class-agnostic"}, {"title": "4 Experiments", "content": "4.1 Datasets\nWe use the following datasets for experiments. ActivityNet v1.3 (ANET) [5] consists of 20,000 videos that cover 200 action categories. Since its target categories are about activity or event, only 1 - 2 of long action instances exist in a video. THUMOS14 (TH14) [13] contains 413 videos with 20 sports-related action categories. Here, about 15 action instances exist per video on average. FineAction (FinAct) [23] includes 16,732 videos, encompassing 106 different action classes. On average, each video in this dataset contains 6 action instances.\nIn contrast to THUMOS14, FineAction covers various domains, including not only sports but also actions related to household, socializing, and personal care activities. Therefore, FineAction would better evaluate the generalization capa-bility of an action localizer.\nFor self-training data, we use the video ID provided in Kinetics-600 [6] and scrape the full video from YouTube. After filtering out very long videos exceeding 2.5 hours, about 332k videos are collected. Then, we build its subsets of 5k, 10k, 50k, 100k, and 200k videos, where a larger set encompasses a smaller set.\n4.2 Evaluation Metrics\nWe use the standard mean average precision (mAP) which is widely used to eval-uate TAL methods. For ZS-TAL benchmarks, we follow the conventions [15, 28] that report the mAPs at different temporal intersection over union (tIoU) thresholds and their average: [0.3: 0.1: 0.7] and [0.5 : 0.05 : 0.95] tIoU values for THUMOS14 [13] and ActivityNet v1.3 [5]. For OV-TAL benchmarks, we re-port the mAP of all, base, and novel categories defined in each dataset at a tIoU of 0.5. These metrics are denoted by mAP50, MAPB, and mAPN, respectively.\nAcross the experiments, we report the mAPs of the framework trained in full-shot (FS), without self-training (w/o ST), and with self-training (ST) settings. With self-training, we present the results of using in-domain and open-domain data, denoted by (ID-ST) and (OD-ST), respectively.\n4.3 Implementation Details\nFor the video VLM, we adopt ViFi-CLIP [34] which is based on ViT-Base/16 [11] visual encoder and is pre-trained as video-text pairs using the K400 dataset [7]. In the pre-processing step of inputs, we interpolate videos into 30 fps and re-size frames to 256 pixels on the shorter side, followed by a center crop of 224. Regarding video feature extraction, we use the window size of 16 frames and stride size of 4 frames. For the textual prompt input, we directly use the action category name without any prompt engineering. We attach more configurations for training and inference, such as training epoch, learning rate, and NMS, in the supplementary material.\n4.4 Main Results\nWe perform extensive experiments on the FineAction dataset from the OV-TAL benchmark to evaluate the effectiveness of our proposed method.\nScalability Evaluation. Tab. 1 shows that self-training using open-domain data achieves increasing mAP and mAP with larger volume and outperforms the results of using in-domain data. We achieve these noteworthy results without a specially designed loss function or pseudo-labeling strategy. This demonstrates the high potential of self-training for OV-TAL with large-scale web videos."}, {"title": "4.5 Results of OV-TAL Benchmark", "content": "We present the comparison with the previous method using OpenTAL [1] with ViFi-CLIP for classification due to the absence of reproducible OV-TAL meth-ods [15,28]. The details of using OpenTAL for OV-TAL are attached in supp.\nCross-category Evaluation. Tab. 4 shows our results in generalized and con-strained ZS evaluation. The mAPs of unseen classes (CN) are lower than those in Tab. 6. This reflects the difficulty of detecting rare actions that less frequently occur in training data.\nSelf-training the model with in-domain data improves mAP in both THU-MOS14 and FineAction. Using open-domain data also enhances the gain of"}, {"title": "4.6 Results of ZS-TAL Benchmark", "content": "Tab. 6 presents the results of existing methods and ours in the ZS-TAL bench-mark. First, we validate the effectiveness of our decoupled architecture, STOV-TAL. In both full-shot and zero-shot evaluation, a large improvement is shown by replacing the VLM backbone from CLIP [33] to ViFi-CLIP [34]. Based on this result, we expect higher mAPs by adopting the advanced video-language models, supporting the expandability of our framework. While mAPs of STOV-TAL are lower than those of ActionFormer in full-shot evaluation, ActionFormer cannot perform zero-shot inference, and our VLM-based classifier is not fine-tuned on the TAL dataset.\nIn addition, the decoupled architecture benefits from adopting the state-of-the-art action localizer. This is supported by the results from the zero-shot evaluation, comparing STOV-TAL (w/o ST) and the others using the same VLM backbone (CLIP-B). Our VLM-based action classifier is not fine-tuned on the benchmark, compared to the others adopting the prompt-tuning paradigm [53] to fine-tune and improve action classification accuracy. Thus, our high mAP is attributed to the enhanced action localizer. Note that the others show higher mAP in the full-shot results, but these are due to the fine-tuned classifier.\nIn THUMOS14, significant improvements are achieved from self-training with in-domain data. For example, with a tIoU at 0.5, mAP increases as 31.4 \u2192 34.4 and 34.3 \u2192 37.5 on 50%-50% and 75%-25% zero-shot evaluation, respectively."}, {"title": "5 Conclusion", "content": "In this study, we investigated the scalability of action localization in the OV-TAL setting. Through the utilization of large-scale web data, we demonstrated significant improvements in the generalizability of class-agnostic action localiz-ers. Coupled with a rigorous evaluation protocol we proposed, we aim for our work to serve as a robust benchmark for future research in this field."}, {"title": "Appendix", "content": "In this section, we provide additional experimental results and details not pre-sented in the main paper.\nA Additional Implementation Details\nWe primarily adopt the hyperparameters from ActionFormer [48] and ViFi-CLIP [34] since our architecture is based on them. As addressed in Sec. 4.1, the ActivityNet [5] dataset differs significantly from the THUMOS14 [13] and FineAction [23] datasets. It consists of only 1 \u2013 2 long action instances, limiting the evaluation of the capability in action localization. Accordingly, we adjust hyperparameters for the ActivityNet dataset, following the TAL literature.\nA.1 Video Feature Extraction Details\nHere, we detail the video feature extraction process using VLMs. As stated in Sec. 4.3, we extract the video snippet features (FV) using the conventional slid-ing window manner with a window size of 16 frames and a stride size of 4 frames, after interpolating videos into 30 fps. For ActivityNet, we interpolate FV to a fixed length following the widely used convention [19, 20, 45, 48]. Specifically, each video is interpolated to a length of 192 feature vectors, as employed in Ac-tionFormer [48]. For THUMOS14 and FineAction, we retain the video snippet features in their original length. For the experiments with ViCLIP [39], we inter-polate its learned temporal embedding from 8 to 16 to use the same window size for video feature extraction. Note that such details are often absent in existing OV-TAL methods [15,28], making it challenging to reproduce their results. We will release the extracted features to encourage the OV-TAL community.\nA.2 Inference Details\nCompared to ActionFormer [48], we use the class-agnostic action localizer and assign the action classes from the VLM. As a result of this change, we delay the Soft-NMS operation until after the action classes are assigned, rather than immediately following the output of the action localizer. On the other hand, when we compute pseudo labels on unlabeled videos, we directly perform Soft-NMS on the class-agnostic action instances, based on its actionness score (sa). We employ the same Soft-NMS configurations for both cases, with slight variations based on the dataset. We choose the top 100 scoring action instances for ActivityNet and 200 for THUMOS14 and FineAction, applying a minimum confidence score threshold of 0.001 and a tIoU threshold of 0.1."}, {"title": "A.3 Selection of Previous Methods for OV-TAL", "content": "Including the results of existing OV-TAL methods [15, 28, 32] in our proposed OV-TAL benchmarks would be a good practice to ensure a sufficient comparison. However, due to the difficulty of reproducing their results1, we instead choose OpenTAL [1], which focuses on localizing actions unseen during training through uncertainty modeling. We train it on the base categories defined in our OV-TAL benchmark and use it as the class-agnostic action localizer in our decoupled architecture. Although OpenTAL utilizes the I3D [7] backbone for action local-ization, we assign action classes to output action instances using ViFi-CLIP [34], as same as our model. Thus, comparing its mAPs with ours solely evaluates the capability of action localization.\nA.4 Training Details\nAdamW [25] is chosen as the optimizer, coupled with a scheduler that linearly warms up the learning rate (lr) to its maximum value and decays to the minimum value (1e-8) following a cosine function. Tab. 7 presents the hyperparameter values for each dataset. For the OD-ST experiments on THUMOS14, we use the subset of 100k videos. We empirically found the threshold values for obtaining pseudo labels. 0.2, 0.05, and 0.4 are used for ActivityNet, THUMOS14, and FineAction, respectively.\nB Additional Experimental Results\nB.1 ActivityNet v1.3 Results for OV-TAL Benchmark\nThe main paper presents the cross-category OV-TAL benchmark results of the THUMOS14 [13] and FineAction [23] datasets in Tab. 4. Here, we show the re-sults of the ActivityNet v1.3 [5] in Tab. 8. As discussed in Sec. 4.6, ActivityNet v1.3 is not a proper dataset for evaluating the generalization capability in action localization, supported by the zero-shot performance (w/o ST) of action localiza-tion on par with that of full-shot in Tab. 6. In the proposed OV-TAL benchmark,"}, {"title": "Iter", "content": "${\\theta}_{iter}' = (1 - \\lambda) \\theta_{iter} + \\lambda {\\theta}'_{iter-1}$"}]}