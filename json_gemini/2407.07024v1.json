{"title": "Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization", "authors": ["Jeongseok Hyun", "Su Ho Han", "Hyolim Kang", "Joon-Young Lee", "Seon Joo Kim"], "abstract": "The vocabulary size in temporal action localization (TAL) is constrained by the scarcity of large-scale annotated datasets. To address this, recent works incorporate powerful pre-trained vision-language models (VLMs), such as CLIP, to perform open-vocabulary TAL (OV-TAL). However, unlike VLMs trained on extensive image/video-text pairs, existing OV-TAL methods still rely on small, fully labeled TAL datasets for training an action localizer. In this paper, we explore the scalability of self-training with unlabeled YouTube videos for OV-TAL. Our self-training approach consists of two stages. First, a class-agnostic action localizer is trained on a human-labeled TAL dataset and used to generate pseudo-labels for unlabeled videos. Second, the large-scale pseudo-labeled dataset is combined with the human-labeled dataset to train the localizer. Extensive experiments demonstrate that leveraging web-scale videos in self-training significantly enhances the generalizability of an action localizer. Additionally, we highlighted issues with existing OV-TAL evaluation schemes and proposed a new evaluation protocol.", "sections": [{"title": "1 Introduction", "content": "Temporal action localization (TAL) involves localizing and classifying action instances in long videos. However, annotating large TAL datasets with extensive action vocabularies is costly, limiting models to small vocabularies. Developing TAL models that cover any target actions has been a long-standing goal.\nAs relevant research directions, Open-set TAL [1] aims to localize all actions by assigning base classes and marking novel actions as unknown, and Open-world TAL [50] extends open-set TAL by integrating continual learning, where the model is updated with annotations of novel actions after the initial training phase. However, both settings are not suitable to achieve the goal since they cannot classify unseen actions. In contrast, recent works [29,49] adopted a zero-shot (ZS) learning approach, leveraging semantic information from language models [10,27] to classify unseen actions. However, as the main goal in ZS learning is to recognize concepts not seen during training [42], it strictly restricts the use of samples of unseen action classes for training. This stringent restriction hinders scaling TAL models with large-scale web data.\nDifferent from ZS learning, Zareian et al. [47] proposed open-vocabulary (OV) learning for object detection that alleviates the restriction of training data. Specifically, OV learning leverages image-text pairs for training, where the classes used for testing may or may not be present in the training data. This relaxed constraint allows vision-language models (VLMs) like CLIP [33], which are pre-trained on extensive image-caption pairs, to demonstrate robust zero-shot performance across various tasks, including open vocabulary temporal action localization (OV-TAL).\nExisting OV-TAL methods [15, 28, 32] utilize a VLM for classification. Nevertheless, their class-agnostic action localizers continue to rely on small, fully labeled TAL datasets, even within the OV learning framework. This approach overlooks the vast potential of numerous web videos for enhancing action localization. To address this, we explore self-training with additional unlabeled videos to improve the generalization capability of action localization. Although the class-agnostic action localizer can already detect the novel actions [20], we found that self-training with large-scale web video can improve cross-category generalization capabilities. Our training pipeline is illustrated in Fig. 1 (Left).\nWe study two types of data sources for self-training: in-domain and open-domain. In-domain data is acquired by splitting the target benchmark, indicating that it includes videos from novel categories of the target benchmark. On the other hand, Open-domain data includes random web videos. Although in-domain data is considered gold data, it is confined to the benchmark. In contrast, open-domain data is a scalable source as more videos can be scraped from the web. Fig. 1 (Right) shows the results on the FineAction dataset [23], which covers various video domains. Using 100k random videos, we surpass the performance of in-domain data for both base and novel classes. This proves the effectiveness of self-training with web videos. More scaling results are presented in Tab. 1."}, {"title": "3 Methodology", "content": "To encompass largely available unlabeled untrimmed video to TAL, we adopt the open-vocabulary learning setting into TAL. As a solution, we propose the framework, Self-Training for Open-Vocabulary Temporal Action Localization, namely, STOV-TAL. This section provides a rigorous definition of this setting with the evaluation protocol. Then, we describe the straightforward yet effective architecture and the training method that improves the generalization ability of action localization."}, {"title": "3.1 Problem Definition of OV-TAL", "content": "Input for OV-TAL consists of an untrimmed video, V, and C number of target action categories, C = {ci}C i=1, which are unbounded and can be flexibly manipulated by users. Its output consists of M action instances, \u03a8 = {(ts, te, C, s)i}M i=1, where ts, te, c, and s denote the start and end times of the action, the action category, and the confidence score, respectively.\nRegarding the training data utilization, the OV-TAL setting offers a more flexible approach, as depicted in Fig. 3. In ZS-TAL, training data is limited to videos with human-annotated TAL labels corresponding to Ctrain which is a disjoint set following Ctrain \u2229 Ctest = \u00d8. However, the OV-TAL setting relaxes this strict restriction of training data. Specifically, we do not limit the use of data sources which may include actions of Ctest unless the TAL annotations are provided together. This allows for the incorporation of abundant unlabeled, random videos from the web. As a result, training the action localizer is no longer limited to the small-scale TAL datasets; instead, it involves exploiting data with diverse categories and domains. Therefore, the OV-TAL setting is more suitable for exploring generalization ability."}, {"title": "3.2 Evaluation Protocol for OV-TAL", "content": "Existing ZS-TAL benchmarks [15] face two issues: (1) they do not evaluate the seen categories, but only unseen categories, and (2) they rely on random category splits that do not consider the category frequency. To address these, we introduce the generalized ZS evaluation setting with the Kinetics-based [7] category split on the proposed OV-TAL benchmarks. The proposed benchmarks are crucial at this stage for advancing OV-TAL further."}, {"title": "Generalized Zero-Shot Evaluation.", "content": "As shown by Fig. 3, the benchmark dataset is disjointly divided. In the ZS-TAL benchmark, the model is trained on the training videos of Ctrain and is evaluated on the validation videos of Ctest, i.e., Ctarget = Ctest. In this way, setting the target categories by either Ctrain or Ctest is referred to the constrained zero-shot evaluation protocol. As pointed out by Xian et al. [44], the constrained setting provides strong prior knowledge and makes the problem easier by not considering Ctrain, which are used for optimizing the model. On the contrary, the generalized zero-shot evaluation protocol includes all categories in the benchmark for evaluation, i.e., Ctarget = Ctrain \u222a Ctest. In addition to the accuracy of all categories, we also measure the accuracy of Ctrain and Ctest under this generalized protocol."}, {"title": "Category Split.", "content": "Existing ZS-TAL benchmarks are based on the splits whose categories are randomly chosen [15,49]. Ju et al. [15] proposed ten random splits with ratios, e.g., 50%-50% for Ctrain and Ctest, and the final accuracy is computed by averaging the results of each split. In our Kinetics-based split, we regard actions covered in Kinetics as common categories and use them as base categories (CB); those not covered are considered rare actions and treated as novel categories (CN). Two reasons support this criterion: (1) The action list in K400 is thoughtfully selected to encompass actions that are commonly encountered in daily life. This selection criterion allows us to simulate a challenging scenario where categories of varying frequency - frequent for training and infrequent for testing are utilized. (2) K400 is a common choice for pre-training video backbones, and detecting actions not present in K400 can demonstrate a more rigorous test of category generalization ability.\nTo automatically detect the category overlap, we utilize NLTK [2] to perform stemming and lemmatization on the text of action categories and verify whether the words of action category in the benchmark are present in the set of K400 actions. After this process, we derive the sets of (|CB|, |CN|) as (90, 110), (12, 8), and (56,50) for ActivityNet v1.3, THUMOS14, and FineAction, respectively."}, {"title": "3.3 STOV-TAL Architecture", "content": "We decouple the localization and classification of actions into two components in the architecture: (1) class-agnostic action localizer; (2) open-vocabulary action classifier based on the video VLM [34]. Fig. 4 illustrates the detailed pipeline. Note that this architecture supports various CLIP-like VLMs, which typically include separate uni-modal encoders for video and text, represented as the video encoder (\u03a6\u03bd) and the text encoder (\u03a6\u03c4).\nVideo Feature Extraction with VLM. Frames are sampled from the input video using the overlapping sliding window approach, commonly used in the TAL literature [48]. This takes consecutive frames of window size and slides a window\nwith the stride size. As a result, a long untrimmed video is divided into multiple overlapping video snippets, represented as V = {Vi}S i=1\u2208RS\u00d7T\u00d7H\u00d7W\u00d73, where S is the number of snippets and T is the window size. For each snippet, the VLM's video encoder outputs the feature vector, \u03a6\u03bd(Vi) \u2208 RD, where D is the dimension of the feature. By processing the entire video, we obtain the video snippet features, FV = \u03a6\u03bd(V) \u2208 RS\u00d7D.\nDetecting Every Action. In this stage, the primary goal is to detect every action instance with its actionness score sa, rather than predicting its action category. To achieve this goal, we employ the action localizer that produces class-agnostic action instances, represented as \u03a8agn = {(ts, te, \u00d8, sa)i}M i=1. Here, \u00d8 indicates the absence of the action category. Our framework offers flexibility in selecting the architecture of the action localizer, provided that the output format is compatible. It accommodates both two-stage methods [20, 45] and one-stage methods [19,48] when trained in a class-agnostic manner. The class-agnostic action localizer is improved with stronger generalization ability through the proposed self-training process, detailed in Sec. 3.4.\nTo make the paper self-contained, we provide a concise overview of the adopted action localizer [48]. This localizer includes a convolutional projection layer, a multi-scale transformer encoder, and parallel convolutional decoders in sequence, as described in the action localizer block of Fig. 4. The first two modules update the video snippet features to consider both temporal locality and long-term contexts. Temporal downsampling operations are integrated into the encoder to extract multi-scale features for capturing the actions at different temporal scales due to the varying granularity of actions. Two parallel convolutional decoders are used to predict the actionness score and the offset for start and end time in every snippet across the temporal axis at each scale."}, {"title": "Classifying Every Action.", "content": "We use an open-vocabulary action classifier for assigning the most appropriate action category (c) among the input actions to the previously detected action instances (\u03a8agn), resulting in the class-aware action instances (\u03a8cls). Following CLIP [33], the input actions are tokenized by the lower-cased byte pair encoding [36] with the start and end tokens. The tokenized actions are encoded through the VLM's text encoder as the text features, FT \u2208 RC\u00d7D, where C is the number of input action categories. The visual features of each action instance are extracted from the VLM's video snippet features by the RoI-Align operation [12] with its start and end times. Then, we obtain FA \u2208 RM\u00d7D, where M is the number of instances.\nBased on these features, we compute the cosine similarity between FA and FT, which is a matrix of RM\u00d7C. It is normalized by the softmax operation in the axis of the category after scaling it by the temperature parameter. This matrix value indicates each instance's class confidence scores (sc). While we can choose top-K action categories for each instance, we only choose the top-scoring action category as the default setting. Finally, the actionness score, sa, and the category confidence score, sc, of each action instance are fused by the geometric mean to compute the final confidence score, s. The output from this stage is class-aware action instances, represented as \u0176cls = {(ts, te, C, s)i}M i=1. As the post-processing, softNMS [3] is applied to remove the duplicates and obtain the final action instances."}, {"title": "3.4 Learning to Detect Every Action", "content": "A class-agnostic action localizer has demonstrated the ability to localize actions that were not seen during training [15,20]. Leveraging its cross-category generalization ability, our architecture can detect and classify any input actions without requiring specialized training methods. Nonetheless, its generalization ability remains limited, particularly for unseen domains (i.e., cross-dataset evaluation). This is attributed to the small scale of the TAL datasets that lack diverse action categories or domains. Inspired by the semi-supervised learning commonly employed in data-scarce scenarios, we aim to enhance the action localizer's generalization ability through the self-training method [17,43]. As illustrated in Fig. 1, self-training comprises two stages: (1) Training a model on a labeled dataset; and (2) Training the model on an unlabeled dataset, using pseudo-labels generated by the previous model.\nLearning from Labeled Videos. In the first stage, the action localizer is trained using the human-labeled TAL dataset. Since we employ the class-agnostic action localizer, the classification is about distinguishing the foreground or background of actions in a video. As described in Sec 3.3, the output from the two decoders, \u03a8agn includes the start and end times of the action and the actionness score. These are trained with the DIoU loss [51] and Focal loss [21], respectively.\nLearning from Unlabeled Videos. In the second stage, we exploit additional unlabeled videos for training. To obtain pseudo-labels of these videos, the action localizer trained in the first stage is employed to generate the class-agnostic action instances. Note that the raw output includes many duplicated and noisy instances. We use the NMS [3] operation and the fixed threshold value on the actionness score to remove them. Then, this pseudo-dataset is combined with the first stage's labeled dataset to form the joint dataset. Our pseudo-label selection process requires finding the proper threshold value. In this regard, the sensitivity analysis of pseudo-labels in Sec 4.4 supports the robustness of this process. Although advanced pseudo-labeling methods, such as adaptive thresholding [40], can be adopted, this paper focuses on exploring the generalization ability of action localization using the straightforward self-training method.\nThe process of selecting videos is crucial for self-training, where we explore two distinct settings: (1) Using in-domain (ID) data, and (2) Using open-domain (OD) data. In the first setting, we employ the training set of the target benchmark without labels. For instance, videos containing novel actions are chosen for self-training in the cross-category evaluation. However, this video sampling policy is geared towards the evaluation dataset. Utilizing videos from the same dataset ensures in-domain, creating a favorable environment for achieving higher accuracy. Also, the scaling-up effect with a larger volume of data is constrained by a fixed and small number of videos. On the other hand, the second setting utilizes random YouTube videos whose video IDs are sourced from K600 [6]. In contrast to the Kinetics dataset, where videos are trimmed to focus on the specific labeled action category, we use untrimmed videos.\nThe training objectives used in the first stage are also used in this stage. When the scale of the unlabeled dataset is significantly larger than that of the labeled dataset and their distributions differ, the action localizer may struggle to learn robust knowledge from the labeled dataset. Thus, we adopt the Mean Teacher framework [37] and initialize the teacher model with the action localizer trained in the first stage. In this teacher-student framework, the teacher model is used for inference and is updated with the student model in the manner of exponential moving average as formulated in Eq. 1:\n\u03b8\u2032iter = (1 \u2212 \u03bb)\u03b8iter + \u03bb\u03b8\u2032iter\u22121,\nwhere \u03b8\u2032 and \u03b8 represent the parameters of the teacher and student models. \u03bb is the smoothing coefficient, and iter indicates the training iteration step."}, {"title": "4 Experiments", "content": "We use the following datasets for experiments. ActivityNet v1.3 (ANET) [5] consists of 20,000 videos that cover 200 action categories. Since its target categories are about activity or event, only 1 - 2 of long action instances exist in a video. THUMOS14 (TH14) [13] contains 413 videos with 20 sports-related action categories. Here, about 15 action instances exist per video on average. FineAction (FinAct) [23] includes 16,732 videos, encompassing 106 different action classes. On average, each video in this dataset contains 6 action instances.\nIn contrast to THUMOS14, FineAction covers various domains, including not only sports but also actions related to household, socializing, and personal care activities. Therefore, FineAction would better evaluate the generalization capability of an action localizer.\nFor self-training data, we use the video ID provided in Kinetics-600 [6] and scrape the full video from YouTube. After filtering out very long videos exceeding 2.5 hours, about 332k videos are collected. Then, we build its subsets of 5k, 10k, 50k, 100k, and 200k videos, where a larger set encompasses a smaller set."}, {"title": "4.2 Evaluation Metrics", "content": "We use the standard mean average precision (mAP) which is widely used to evaluate TAL methods. For ZS-TAL benchmarks, we follow the conventions [15, 28] that report the mAPs at different temporal intersection over union (tIoU) thresholds and their average: [0.3: 0.1: 0.7] and [0.5 : 0.05 : 0.95] tIoU values for THUMOS14 [13] and ActivityNet v1.3 [5]. For OV-TAL benchmarks, we report the mAP of all, base, and novel categories defined in each dataset at a tIoU of 0.5. These metrics are denoted by mAP50, mAPB, and mAPN, respectively.\nAcross the experiments, we report the mAPs of the framework trained in full-shot (FS), without self-training (w/o ST), and with self-training (ST) settings. With self-training, we present the results of using in-domain and open-domain data, denoted by (ID-ST) and (OD-ST), respectively."}, {"title": "4.3 Implementation Details", "content": "For the video VLM, we adopt ViFi-CLIP [34] which is based on ViT-Base/16 [11] visual encoder and is pre-trained as video-text pairs using the K400 dataset [7]. In the pre-processing step of inputs, we interpolate videos into 30 fps and resize frames to 256 pixels on the shorter side, followed by a center crop of 224. Regarding video feature extraction, we use the window size of 16 frames and stride size of 4 frames. For the textual prompt input, we directly use the action category name without any prompt engineering. We attach more configurations for training and inference, such as training epoch, learning rate, and NMS, in the supplementary material."}, {"title": "4.4 Main Results", "content": "We perform extensive experiments on the FineAction dataset from the OV-TAL benchmark to evaluate the effectiveness of our proposed method.\nScalability Evaluation. Tab. 1 shows that self-training using open-domain data achieves increasing mAP and mAP with larger volume and outperforms the results of using in-domain data. We achieve these noteworthy results without a specially designed loss function or pseudo-labeling strategy. This demonstrates the high potential of self-training for OV-TAL with large-scale web videos."}, {"title": "4.5 Results of OV-TAL Benchmark", "content": "We present the comparison with the previous method using OpenTAL [1] with ViFi-CLIP for classification due to the absence of reproducible OV-TAL methods [15,28]. The details of using OpenTAL for OV-TAL are attached in supp.\nCross-category Evaluation. Tab. 4 shows our results in generalized and constrained ZS evaluation. The mAPs of unseen classes (CN) are lower than those in Tab. 6. This reflects the difficulty of detecting rare actions that less frequently occur in training data.\nSelf-training the model with in-domain data improves mAP in both THUMOS14 and FineAction. Using open-domain data also enhances the gain of"}, {"title": "4.6 Results of ZS-TAL Benchmark", "content": "Tab. 6 presents the results of existing methods and ours in the ZS-TAL benchmark. First, we validate the effectiveness of our decoupled architecture, STOV-TAL. In both full-shot and zero-shot evaluation, a large improvement is shown by replacing the VLM backbone from CLIP [33] to ViFi-CLIP [34]. Based on this result, we expect higher mAPs by adopting the advanced video-language models, supporting the expandability of our framework. While mAPs of STOV-TAL are lower than those of ActionFormer in full-shot evaluation, ActionFormer cannot perform zero-shot inference, and our VLM-based classifier is not fine-tuned on the TAL dataset.\nIn addition, the decoupled architecture benefits from adopting the state-of-the-art action localizer. This is supported by the results from the zero-shot evaluation, comparing STOV-TAL (w/o ST) and the others using the same VLM backbone (CLIP-B). Our VLM-based action classifier is not fine-tuned on the benchmark, compared to the others adopting the prompt-tuning paradigm [53] to fine-tune and improve action classification accuracy. Thus, our high mAP is attributed to the enhanced action localizer. Note that the others show higher mAP in the full-shot results, but these are due to the fine-tuned classifier.\nIn THUMOS14, significant improvements are achieved from self-training with in-domain data. For example, with a tIoU at 0.5, mAP increases as 31.4 \u2192 34.4 and 34.3 \u2192 37.5 on 50%-50% and 75%-25% zero-shot evaluation, respectively."}, {"title": "5 Conclusion", "content": "In this study, we investigated the scalability of action localization in the OV-TAL setting. Through the utilization of large-scale web data, we demonstrated significant improvements in the generalizability of class-agnostic action localizers. Coupled with a rigorous evaluation protocol we proposed, we aim for our work to serve as a robust benchmark for future research in this field."}, {"title": "Appendix", "content": "In this section, we provide additional experimental results and details not presented in the main paper."}, {"title": "A Additional Implementation Details", "content": "We primarily adopt the hyperparameters from ActionFormer [48] and ViFi-CLIP [34] since our architecture is based on them. As addressed in Sec. 4.1, the ActivityNet [5] dataset differs significantly from the THUMOS14 [13] and FineAction [23] datasets. It consists of only 1 \u2013 2 long action instances, limiting the evaluation of the capability in action localization. Accordingly, we adjust hyperparameters for the ActivityNet dataset, following the TAL literature."}, {"title": "A.1 Video Feature Extraction Details", "content": "Here, we detail the video feature extraction process using VLMs. As stated in Sec. 4.3, we extract the video snippet features (FV) using the conventional sliding window manner with a window size of 16 frames and a stride size of 4 frames, after interpolating videos into 30 fps. For ActivityNet, we interpolate Fv to a fixed length following the widely used convention [19, 20, 45, 48]. Specifically, each video is interpolated to a length of 192 feature vectors, as employed in ActionFormer [48]. For THUMOS14 and FineAction, we retain the video snippet features in their original length. For the experiments with ViCLIP [39], we interpolate its learned temporal embedding from 8 to 16 to use the same window size for video feature extraction. Note that such details are often absent in existing OV-TAL methods [15,28], making it challenging to reproduce their results. We will release the extracted features to encourage the OV-TAL community."}, {"title": "A.2 Inference Details", "content": "Compared to ActionFormer [48], we use the class-agnostic action localizer and assign the action classes from the VLM. As a result of this change, we delay the Soft-NMS operation until after the action classes are assigned, rather than immediately following the output of the action localizer. On the other hand, when we compute pseudo labels on unlabeled videos, we directly perform Soft-NMS on the class-agnostic action instances, based on its actionness score (sa). We employ the same Soft-NMS configurations for both cases, with slight variations based on the dataset. We choose the top 100 scoring action instances for ActivityNet and 200 for THUMOS14 and FineAction, applying a minimum confidence score threshold of 0.001 and a tIoU threshold of 0.1."}, {"title": "A.3 Selection of Previous Methods for OV-TAL", "content": "Including the results of existing OV-TAL methods [15, 28, 32] in our proposed OV-TAL benchmarks would be a good practice to ensure a sufficient comparison. However, due to the difficulty of reproducing their results\u00b9, we instead choose OpenTAL [1], which focuses on localizing actions unseen during training through uncertainty modeling. We train it on the base categories defined in our OV-TAL benchmark and use it as the class-agnostic action localizer in our decoupled architecture. Although OpenTAL utilizes the I3D [7] backbone for action localization, we assign action classes to output action instances using ViFi-CLIP [34], as same as our model. Thus, comparing its mAPs with ours solely evaluates the capability of action localization."}, {"title": "A.4 Training Details", "content": "AdamW [25] is chosen as the optimizer, coupled with a scheduler that linearly warms up the learning rate (lr) to its maximum value and decays to the minimum value (1e-8) following a cosine function. Tab. 7 presents the hyperparameter values for each dataset. For the OD-ST experiments on THUMOS14, we use the subset of 100k videos. We empirically found the threshold values for obtaining pseudo labels. 0.2, 0.05, and 0.4 are used for ActivityNet, THUMOS14, and FineAction, respectively."}, {"title": "B Additional Experimental Results", "content": null}, {"title": "B.1 ActivityNet v1.3 Results for OV-TAL Benchmark", "content": "The main paper presents the cross-category OV-TAL benchmark results of the THUMOS14 [13] and FineAction [23] datasets in Tab. 4. Here, we show the results of the ActivityNet v1.3 [5] in Tab. 8. As discussed in Sec. 4.6, ActivityNet v1.3 is not a proper dataset for evaluating the generalization capability in action localization, supported by the zero-shot performance (w/o ST) of action localization on par with that of full-shot in Tab. 6. In the proposed OV-TAL benchmark,"}]}