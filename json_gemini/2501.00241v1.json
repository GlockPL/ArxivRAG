{"title": "Exploring Variability in Fine-Tuned Models for Text Classification with DistilBERT", "authors": ["Giuliano Lorenzoni", "Ivens Portugal", "Paulo Alencar", "Donald Cowan"], "abstract": "This study presents an extensive evaluation of fine-tuning strategies for text classification using the Distil-BERT model, specifically focusing on the distilbert-base-uncased-finetuned-sst-2-english variant. Through a structured experimental design, we examine the influence of hyperparameters\u2014namely learning rate, batch size, and number of epochs-on key performance metrics, including accuracy, F1-score, and loss. Utilizing polynomial regression analyses in both absolute and relative frameworks, our approach captures foundational and incremental impacts of these hyperparameters, with a particular focus on fine-tuning adjustments relative to a baseline model. Results highlight notable variability in metric outcomes based on hyperparameter configurations, illustrating that optimizing for one metric often leads to trade-offs impacting others. For example, while an increase in learning rate was associated with reduced loss in relative analysis (p = 0.027), it posed challenges to maintaining consistent accuracy improvements. Conversely, batch size demonstrated consistent significance for accuracy and F1-score in absolute regression (p = 0.028 and p = 0.005, respectively), yet had a limited impact on loss optimization (p = 0.170). Additionally, the interaction between epochs and batch size was especially critical for maximizing F1-score (p = 0.001), emphasizing the importance of hyperparameter interplay.\nThis interdependence underscores the need for fine-tuning strategies that address non-linear interactions among hyperparameters to achieve balanced performance across metrics. Our findings suggest that such variability and metric trade-offs are relevant considerations across subtasks and applications beyond text classification, extending to other areas in NLP and computer vision. This analysis not only informs fine-tuning strategies tailored to large language models but also highlights the importance of adaptive designs for broader model applicability.", "sections": [{"title": "I. INTRODUCTION", "content": "The practice of fine-tuning has become a fundamental step in adapting Large Language Models (LLMs) to effectively perform across a wide range of tasks. For these models, the ability to adjust hyperparameters to optimize performance in specific tasks not only improves accuracy and relevance but also tailors the model to the distinct needs of various contexts, such as text classification, natural language generation, and other applications in Natural Language Processing (NLP) and Computer Vision (CV). However, the diversity of tasks and subtasks, along with the variety of available models and hyperparameters, necessitates the development of fine-tuning strategies that are adapted to the particularities of each task and, most importantly, the specific model being utilized."}, {"title": "II. RELATED WORK", "content": "The fine-tuning of Large Language Models (LLMs) for specific applications has become a prominent focus in re-cent research, with studies examining a variety of strategies, hyperparameters, and domains. Here, we discuss several key contributions and compare them with our study, highlighting how our work uniquely explores multiple hyperparameters and performance metrics to derive actionable fine-tuning insights, particularly for the DistilBERT model in text classification.\nA. Learning Rate Tuning in LLMs\nThe work on Rethinking Learning Rate Tuning in the Era of Large Language Models [1] focuses on the critical role of learning rate in fine-tuning. The study proposes LRBench++, a tool for evaluating learning rate policies across LLMs and traditional DNNs, highlighting the need for tailored learning rate policies for LLMs. In contrast, our study expands beyond a singular focus on learning rate to evaluate the combined effects of learning rate, batch size, and epochs. Additionally, our approach incorporates both absolute and relative analyses of accuracy, F1-score, and loss to provide a holistic view of hyperparameter impacts on DistilBERT's performance in text classification.\nB. Fine-Tuning Strategies for Domain-Specific Tasks\nA recent study, Evaluating the Effectiveness of Fine-Tuning Large Language Models for Domain-Specific Tasks [2], fine-tuned LLaMA 2 using two distinct methods on a migration-related news dataset, illustrating the impact of self-supervised versus direct Q&A fine-tuning strategies. While this study con-centrates on strategy effectiveness in aligning the model with domain-specific content, our work evaluates a broader range of hyperparameters and their trade-offs across core metrics. Unlike their focus on dataset alignment, we derive generalized fine-tuning strategies through polynomial regression analyses, applicable across different datasets and tasks.\nC. Parameter-Efficient Techniques for Domain Optimization\nFine-Tuning Large Language Models for Task-Specific Data [3]examines the use of Parameter-Efficient Fine-Tuning (PEFT) and Quantized Low-Rank Adaptation (QLORA) tech-niques to fine-tune LLaMA 2 and Falcon 7B models for an e-commerce dataset. This study demonstrated the contextual accuracy benefits of customized fine-tuning. In contrast, our study targets the foundational impacts of basic hyperparam-eters (learning rate, batch size, and epochs) on multiple performance metrics, producing insights for balanced tuning strategies rather than domain-specific optimizations.\nD. Supporting Fine-Tuning Through Automated Systems\nThe study Fine-Tune it Like I'm Five: Supporting Medical Domain Experts in Training NER Models Using Cloud, LLM, and Auto Fine-Tuning [4]introduces CRM4NER, a cloud-based system for managing, training, and fine-tuning Named Entity Recognition (NER) models. It incorporates automatic hyperparameter tuning and context-aware fine-tuning recom-mendations generated via a Large Language Model (LLM), aiming to support domain experts who may lack expertise in Machine Learning (ML). While CRM4NER emphasizes a user-friendly ML management system, our study directly assesses the fine-tuning effects of hyperparameters on model metrics, guiding users on tuning DistilBERT for optimal performance without requiring complex management systems.\nE. Fine-Tuning for Low-Resource Languages\nFine Tuning LLMs for Low Resource Languages [5] ad-dresses fine-tuning strategies for optimizing LLMs in low-resource language settings, exploring language-specific tech-niques such as LoRA, QLoRA, instruction tuning, and Representation Fine-Tuning (ReFT). This study emphasizes language-specific tuning needs, contrasting with our approach, which examines hyperparameter impacts on DistilBERT's text classification performance regardless of language resources. We focus on understanding the metric-specific influences of core hyperparameters, providing insights that are applicable across diverse linguistic contexts.\nF. Optimizing LLM Performance at Scale\nAchieving Peak Performance for Large Language Models: A Systematic Review [6] provides a comprehensive systematic lit-erature review (SLR) of optimization and acceleration methods for large language models (LLMs), focusing on balancing high performance with practical constraints. While this review cat-egorizes optimization strategies into LLM training, inference, and system serving, our work offers a more focused analysis of hyperparameters that directly affect DistilBERT's classifi-cation performance. Our study contributes a metric-specific understanding of hyperparameter trade-offs, complementing high-level efficiency optimizations discussed in the review.\nG. Detecting Bots with Transformer-Based Models\nThe study Fine-Tuned Understanding: Enhancing Social Bot Detection With Transformer-Based Classification [7] uses transformer-based models like BERT and GPT-3 to detect social media bots, achieving high F1-scores through fine-tuning. Unlike this bot-detection focus, our research evalu-ates the interplay of hyperparameters on general performance metrics. Our work seeks to balance multiple metrics rather than targeting domain-specific detection accuracy, expanding the application of our insights beyond niche uses.\nH. LLM Techniques for Document Understanding\nLayoutLLM: Layout Instruction Tuning with Large Lan-guage Models for Document Understanding [8] uses a layout-aware fine-tuning approach for document understanding, incor-porating layout-aware pre-training and supervised fine-tuning to enhance the comprehension of document structures. Our study differs by exploring fundamental hyperparameter adjust-ments rather than layout-specific tuning strategies, providing adaptable insights for general text classification applications across various NLP tasks.\nI. Fine-Tuning DistilBERT in Classification Tasks\nSeveral studies have investigated fine-tuning DistilBERT for specific applications:\nCyberbullying Detection in Social Networks: A Com-parison Between Machine Learning and Transfer Learning Approaches [9] used DistilBERT embeddings to detect abusive content with high F1-scores.\nHate Speech and Target Community Detection in Nastaliq Urdu Using Transfer Learning Techniques [10] applied DistilBERT to identify hate speech in low-resource languages, demonstrating the model's adaptabil-ity in multilingual contexts.\nDepression Classification From Tweets Using Small Deep Transfer Learning Language Models [11] em-ployed DistilBERT for social media-based depression classification, comparing it with smaller models.\nAGI-P: A Gender Identification Framework for Au-thorship Analysis Using Customized Fine-Tuning of Multilingual Language Model [12] developed AGI-P, fine-tuning DistilBERT for gender identification in a multilingual setting."}, {"title": "III. EXPERIMENT DESIGN", "content": "This section details the methodology employed in selecting and evaluating models for text classification, including data collection, model selection, hyperparameters, evaluation met-rics, and statistical methods.\nA. Data Collection\nThe data collection process involved identifying and extract-ing information about hyperparameters and performance met-rics from models available on the Hugging Face platform. Our goal was to select NLP models specialized in text classification and gather relevant data from associated files.\nModel Filtering: Selected the Natural Language Pro-cessing (NLP) category from Hugging Face's available options.\nTask Selection: Filtered for Text Classification, yielding 30 available models.\nBase Model Selection: Chose DistilBERT (distilbert-base-uncased-finetuned-sst-2-english) due to its popular-ity and high number of downloads.\nFine-Tuned Models Extraction:\nRetrieved detailed information from 55 fine-tuned models associated with DistilBERT.\nData Sources: Extracted from README.md and config.json files for each model.\nB. Model Selection and Task Design\nThe model selection and task design aimed to explore the efficiency and performance of fine-tuned models in text classification.\nBase Model Selection: Chose DistilBERT (distilbert-base-uncased-finetuned-sst-2-english) for its efficiency and popularity.\nObjective: Evaluated the impact of different hyperparam-eter configurations and compared results with other fine-tuned models and NLP tasks.\nTask Design:\nEvaluated all 55 fine-tuned models to ensure consis-tent analysis across metrics.\nC. Hyperparameters and Evaluation Metrics\nWe began our analysis with a comprehensive set of nine hy-perparameters and nine evaluation metrics, following the initial configurations recommended in the model documentation. To streamline our focus, we refined this selection based on the frequency of observations across the 55 models analyzed. This approach enabled us to prioritize hyperparameters and metrics that consistently appeared and provided statistically significant insights into model performance.\nD. Selection of Evaluation Metrics and Hyperparameters for Analysis\nDue to the limited statistical significance observed across a broader range of metrics and hyperparameters, our study fo-cused on metrics and hyperparameters with a sufficient number of observations to allow for statistically significant fine-tuning strategies. This selection aimed to ensure a robust analysis of the most impactful elements on model performance.\nEvaluation Metrics: We concentrated on three key metrics that provided enough observations to support meaningful statistical exploration:\nAccuracy - Selected as a primary measure for the overall correctness of model predictions, offering insight into general model performance.\nF1-Score - Focused on due to its balanced considera-tion of precision and recall, particularly important for assessing performance in cases with class imbalance.\nLoss Included as it quantifies model prediction error, essential for evaluating the model's error min-imization during training and validation.\nHyperparameters: Three core hyperparameters were chosen, as they provided enough data points to examine statistically significant trends in fine-tuning:\nLearning Rate - An influential parameter for train-ing speed and stability, providing sufficient variabil-ity in observations for meaningful analysis.\nBatch Size - Affects memory usage and conver-gence, offering adequate data to explore its relation-ship with model generalization and performance.\nNumber of Epochs - Key in controlling the number of training iterations over the dataset, providing essential insights into learning and convergence pat-terns.\nBy focusing exclusively on these metrics and hyperparam-eters, we ensured a statistically valid basis for identifying consistent patterns and potential fine-tuning strategies. This approach provides a clearer understanding of core dependen-cies and allows for targeted adjustments in fine-tuning aimed at optimizing model performance in a statistically sound manner.\nE. Statistical Methods and Tools\nTo assess the impact of selected hyperparameters on eval-uation metrics, we employed polynomial regression analyses across two versions-Absolute and Relative. These analyses aimed to identify patterns, correlations, and potential optimiza-tions through detailed regression models, heatmaps, scatter plots, and regression plots.\nPolynomial Regression Analyses: We conducted poly-nomial regressions to examine the influence of hyper-parameters (Learning Rate, Batch Size, and Number of Epochs) on key evaluation metrics (Accuracy, F1-Score, and Loss) in two distinct versions:\nAbsolute Regression: This version utilized the abso-lute values of both the metrics and hyperparameters. It aimed to reveal the direct effects of hyperparameter values on model performance metrics (Accuracy, F1-Score, Loss) as observed post-training. Absolute regression thus reflects the foundational impact of hyperparameters on baseline metric performance.\nRelative Regression: In the relative regression, we used the differences in metric values between each fine-tuned model and the baseline model, as well as the differences in the hyperparameters applied in fine-tuned versus baseline models. This version provides insights into the incremental improvements and optimization effects of fine-tuning adjustments, specifically measuring how hyperparameter varia-tions contribute to performance gains beyond the baseline.\nHeatmaps and Scatter Plots: We generated heatmaps to visualize correlations between hyperparameters and metrics, identifying patterns that could inform fine-tuning adjustments. Scatter plots illustrated the spread and po-tential trade-offs between hyperparameters and metric outcomes, facilitating a more granular understanding of parameter interactions.\nRegression Plots: In conjunction with the polynomial regression analyses, we produced regression plots to visualize fitted regression lines and residuals. These plots provided additional insights into model fit and variability, helping to identify areas where hyperparameter adjust-ments yield statistically significant improvements.\nF. Consistency Evaluation between Metrics (Accuracy, F1, and Loss)\nThe objective of this subsection is to systematically evalu-ate the consistency of optimization strategies across the key metrics of accuracy, F1, and loss. By analyzing both absolute and relative regression results, we aim to identify patterns that highlight common and divergent impacts of hyperparameters across these metrics. This approach allows us to observe how adjustments to batch size, learning rate, and epochs influence each metric and to determine whether improvements in one metric, such as accuracy, align or conflict with performance outcomes in F1 and loss. Ultimately, this evaluation provides insights into cross-metric coherence, informing the design of balanced fine-tuning strategies that prioritize comprehensive model performance.\nG. Mapping Fine-Tuning Strategies and Recommendations for Improvements\nThis subsection synthesizes the insights captured for each metric based on both absolute and relative regression analyses to formulate actionable fine-tuning strategies. By mapping hyperparameter impacts on accuracy, F1, and loss, we can derive specific, data-driven recommendations that address the unique optimization needs of each metric while also account-ing for their cross-effects. The objective is to enable a strategic approach to fine-tuning that emphasizes incremental gains in each metric while maintaining overall model stability. These strategies highlight which parameters to prioritize initially, such as batch size and epochs, and where to make final refinements, particularly with learning rate adjustments, to achieve balanced improvements in accuracy, F1, and loss.\nH. Computational Packages\nThe implementation of our analyses leveraged several com-putational packages and libraries to streamline data processing, model training, statistical analysis, and visualization. Below is a summary of the key packages used in our study:\nPython Core Libraries:\nNumPy - Provided efficient handling of numerical data and supported array-based computations essen-tial for data manipulation and model input format-ting.\nPandas - Facilitated data manipulation, cleaning, and preparation, allowing us to manage datasets, perform feature engineering, and store results for analysis.\nStatistical Analysis and Metrics:\nSciPy - Enabled statistical testing and analysis, including regression calculations and significance testing on hyperparameter impacts.\nScikit-Learn - Supported computation of evalu-ation metrics (such as Accuracy, F1-score, Precision, and Recall) and provided tools for model evaluation, data splitting, and scaling.\nVisualization and Plotting:\nMatplotlib and Seaborn - Used to generate detailed visualizations, including heatmaps, scatter plots, and regression plots. These visualizations fa-cilitated a clearer understanding of hyperparameter effects and metric distributions across different con-figurations."}, {"title": "IV. RESULTS", "content": "A. Relationships Between Dependent and Independent Vari-ables\nThis section presents the results of analyzing the relation-ships between the dependent variables-accuracy, F1-score, and loss and the independent variables (learning rate, batch size, and number of epochs). We report both absolute and relative regression analyses, along with heatmaps and scatter plots that help visualize the correlations and spreads across different hyperparameter configurations.\n1) Accuracy \u2013 Comparison of Absolute and Relative Re-gressions:\na) Regression Results:\nAbsolute Regression\nBatch size (X2) showed a coefficient of 0.0773 (p = 0.000), indicating a significant positive impact on accuracy.\nNum_epochs (X3) had a coefficient of 0.0061 (p = 0.717), showing no statistical significance.\nLearning rate (X7) exhibited a negative coefficient of -0.001 (p = 0.000), suggesting that higher learning rates can hinder performance.\nRelative Regression (Differences)\nLearning rate (X7) stood out with a coefficient of -0.0014 (p = 0.004), highlighting the critical role of fine-tuning for incremental improvements.\nBatch size (X2) remained relevant, with a positive coefficient of 0.0230 (p = 0.006).\nb) Visual Analysis and Insights: The previous results are complemented by the Heatmaps (Figures 1 and 2) and Polynomial Regression Plots (Figures 2 and 4).\nc) Consolidated Findings and Practical Insights on Ac-curacy: The analysis reveals that the learning rate (X7) sig-nificantly influences accuracy, with both absolute and relative analyses underscoring its critical role. A negative coefficient (-0.001, p = 0.000) in absolute terms highlights that higher learning rates can degrade accuracy, warranting cautious ad-justments to prevent performance loss. In the relative anal-ysis, fine-tuning learning rate adjustments produced notable incremental improvements (coefficient = -0.0014, p = 0.004), emphasizing its value in fine-tuning relative to baseline per-formance. Batch size (X2), meanwhile, proved foundational for establishing accuracy during initial training, contributing to consistent performance. Combined, batch size and learning rate adjustments offer balanced accuracy gains, where initial selection should prioritize an optimal batch size to stabilize accuracy early on, followed by incremental learning rate increases. During fine-tuning, learning rate sensitivity demands precise, gradual adjustments for continued performance en-hancement relative to the baseline. This structured, stage-wise approach aligns hyperparameter selection with both robust training and fine-tuning objectives.\n2) F1 Score - Comparison of Absolute and Relative Re-gressions:\na) Regression Results:\nAbsolute Regression\nLearning rate (X2) had a significant positive impact on F1, with a coefficient of 0.0452 (p = 0.028).\nBatch size (X3) showed significance, with a coeffi-cient of 0.1811 (p = 0.005), reinforcing its role in F1 performance.\nThe quadratic term for batch size (X8) showed a significant negative effect (p = 0.002) on F1 in the absolute regression, indicating that excessively large batch sizes may begin to negatively impact F1 performance. This finding underscores the need to moderate batch size increases during the initial train-ing phase to maintain optimal performance levels.\nThe interaction between epochs and batch size (X9) also displayed significance (p = 0.001), underlining the need for joint optimization.\nRelative Regression (Differences)\nb) Visual Analysis and Insights: The previous results are complemented by the Heatmaps (Figures 5 and 7) and Polynomial Regression Plots (Figures 6 and 8).\nc) Consolidated Findings and Practical Insights on F1-Score: Our analysis identifies learning rate (X2) and batch size (X3) as key parameters for maximizing F1 performance, with significant positive impacts in the absolute analysis (p-values of 0.028 and 0.005, respectively), underscoring their importance in the initial training phase. However, the quadratic term for batch size (X8) demonstrated a negative impact in both absolute and relative analyses (p = 0.002), indicating that while batch size improves F1 initially, excessive increases can reduce its efficacy, highlighting the need for careful batch size moderation. Additionally, the interaction between epochs and batch size (X9) consistently showed significant positive effects in the relative analysis (p = 0.001), underscoring its role in enhancing F1 gains between fine-tuned and baseline models. Notably, learning rate (X2) exhibited a consistent positive impact across both analyses, while batch size (X3) showed higher significance in absolute terms (p = 0.005) but a diminished effect in relative analysis (p = 0.171), suggesting its greater influence on foundational rather than incremental performance.\nThese findings recommend a two-tiered approach: first, establishing a solid base model by fine-tuning learning rate and batch size to maximize initial F1; and second, moderating batch size increases due to diminishing returns, as indicated by the quadratic term for batch size. Fine-tuning should focus on the interaction between epochs and batch size to yield incremental F1 improvements, balancing foundational performance with efficient tuning for optimized outcomes.\n3) Loss - Comparison of Absolute and Relative Regres-sions:\na) Regression Results:\nAbsolute Regression\nThe learning rate (X2) coefficient was -0.0368 with p = 0.610, showing no statistical significance, indi-cating that variations in learning rate do not clearly impact absolute loss.\nBatch size (X3) had a coefficient of 0.2311 with p = 0.170, not statistically significant but suggesting a mild positive influence on loss.\nThe quadratic term for num_epochs (X4) and batch size (X8) had no practical relevance or significance.\nRelative Regression (Differences)\nThe learning rate (X2) had a significant coefficient of -0.1531 with p = 0.027, indicating that increases in learning rate can reduce the loss difference between fine-tuned and base models.\nParameter X5 (interaction between learning rate and number of epochs) was statistically significant with a coefficient of 2843.79 and p = 0.042. This sug-gests that specific optimizations involving learning rate and epochs have a notable impact on reducing the loss difference between fine-tuned and baseline models.\nThe quadratic term for learning rate (X7) had a significant impact with p = 0.018, highlighting the importance of non-linear adjustments in learning rate on loss differences.\nb) Visual Analysis and Insights: The previous results are complemented by the Heatmaps (Figures 9 and 11) and Polynomial Regression Plots (Figures 10 and 12).\nc) Findings and Practical Insights on Loss: Our analysis of loss indicates that, in the absolute context, no single parameter had a significant impact, suggesting that broader hyperparameter adjustments alone may not account for loss variability. However, in the relative analysis, learning rate (X2) showed a significant negative impact (p = 0.027), implying that moderate increases in learning rate can help reduce loss. The quadratic term for learning rate (X7) highlights the relevance of non-linear adjustments for fine-tuning. Further-more, the interaction between learning rate and epochs (X5) exhibited a statistically significant positive effect (coefficient = 2843.79, p = 0.042), suggesting that coordinated adjustments of these parameters can meaningfully decrease incremental loss, underscoring the need for a balanced fine-tuning strategy. Based on these findings, a strategic approach is recom-mended: initially focusing on batch size and epochs to stabilize training and control loss variability, followed by precise ad-justments to learning rate and its interactions to optimize incre-mental performance during fine-tuning. Coordinating learning rate and epochs allows for more controlled and effective loss reduction, particularly valuable in settings where nuanced improvements are desired."}, {"title": "B. Mapping Fine-Tuning Strategies and Recommendations for Improvements", "content": "1) Strategy Mapping by Metric: The following are sug-gested strategies for optimizing each metric and their potential cross-impacts on the others.\na) 1. Strategies Focused on Accuracy:\nKey Parameters:\nBatch Size (X2): Consistent and significant impact (p = 0.028) in absolute analysis, crucial for estab-lishing a solid performance foundation.\nNum_epochs (X3): Essential for overall accuracy performance (p = 0.005).\nLearning Rate (X7): Relevant in the relative anal-ysis, with a critical role in fine adjustments (p 0.004).\nCross-Effects:\nF1: Prioritizing batch size and epochs tends to improve F1, but optimizing learning rate will be necessary to capture incremental gains.\nLoss: While these strategies may contribute to con-sistent training, there is a risk of increased loss vari-ability without adequate regularization techniques.\nb) 2. Strategies Focused on F1 Score:\nKey Parameters:\nLearning Rate (X2): Significant positive impact in absolute analysis (p = 0.028).\nBatch Size (X3): Crucial for F1 performance (p = 0.005), though with reduced impact in relative anal-ysis.\nInteraction between Epochs and Batch Size (X9): Essential for maximizing F1 (p = 0.001).\nCross-Effects:\nAccuracy: Joint optimization of epochs and batch size can benefit accuracy.\nLoss: Fine adjustments can improve overall perfor-mance, but there is a risk of overfitting, increasing loss.\nc) 3. Strategies Focused on Reducing Loss:\nKey Parameters:\nLearning Rate (X2): Significantly reduces loss in relative analysis (p = 0.027).\nQuadratic Term of Learning Rate (X7): Relevant impact in fine adjustments (p = 0.018).\nBatch Size (X3): Relevant but not significant.\nCross-Effects:\nAccuracy and F1: Reducing loss may not directly optimize these metrics, requiring balance to avoid compromising overall performance.\n2) Identified Consistencies and Inconsistencies:\na) Batch Size (X2):\nConsistency: Positively impacts both accuracy and F1 in absolute analysis.\nInconsistency: While relevant, it was not significant for loss, suggesting limited effect on this metric.\nb) Learning Rate (X7):\nConsistency: Fine adjustments are critical for incremen-tal optimization in accuracy and loss.\nInconsistency: In the relative analysis of F1, it showed marginal significance, indicating a lesser incremental impact.\nc) Interaction between Epochs and Batch Size (X9):\nConsistency: Essential for optimizing F1.\nInconsistency: Had limited impact on accuracy and loss.\n3) Practical Recommendations and Fine-Tuning Strategies:\nBased on the previous analyses, we propose an incremental and integrated optimization approach:\na) Initial Selection:\nBatch Size and Epochs: Prioritize these hyperparameters in the initial phase to ensure a solid foundation, improv-ing accuracy and F1.\nb) Final Adjustments:\nLearning Rate: Carefully refine to capture incremental gains in loss and incremental performance in F1.\nc) Trade-Offs and Cross-Impact:\nJoint Monitoring: When optimizing one metric, it is essential to monitor the others. For example, a high learning rate may reduce loss but compromise accuracy.\nRegularization: Apply regularization techniques to pre-vent overfitting and ensure that reducing loss does not harm overall performance."}, {"title": "V. CONCLUSIONS", "content": "This study presents an in-depth evaluation of fine-tuning strategies for the text classification task, focusing on the DistilBERT model variant distilbert-base-uncased-finetuned-sst-2-english. By conducting polynomial regression analyses on three primary evaluation metrics\u2014accuracy, F1-score, and loss and examining the roles of key hyperparameters (learn-ing rate, batch size, and number of epochs), our findings emphasize the complex interplay and trade-offs inherent in hyperparameter tuning for large language models (LLMs).\nThe analysis revealed that optimizing one metric often introduces variability or trade-offs that affect other metrics, particularly when pursuing fine-tuning for incremental im-provements. For example, while batch size consistently en-hanced both accuracy and F1-score in absolute analysis, its impact on loss optimization was limited. In contrast, learn-ing rate adjustments, especially in relative analysis, showed significant reductions in loss (p = 0.027) but required careful balancing to avoid performance inconsistencies in accuracy. The consistent significance of the interaction between epochs and batch size for F1-score maximization (p = 0.001) further emphasizes the necessity of joint optimization to maintain balanced performance across metrics.\nBeyond these core interdependencies, this study underscores the importance of understanding variability in performance metrics across different contexts and tasks beyond text clas-sification. The non-linear and, at times, inconsistent effects of hyperparameters observed here suggest that fine-tuning strategies for LLMs should be adaptive, potentially varying by both metric and the unique demands of each subtask or broader task domain, such as NLP or computer vision. This adaptability is essential for maximizing model robustness and efficacy across diverse deployment scenarios.\nOur findings also highlight the potential of developing fine-tuning frameworks that incorporate metric variability and dy-namically adjust hyperparameters based on the model's perfor-mance across multiple metrics. Such frameworks, grounded in empirical data on hyperparameter impacts, provide a promis-ing foundation for the sustainable and optimized development of LLMs in a wide array of applications."}]}