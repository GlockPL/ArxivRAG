{"title": "Table-Filling via Mean Teacher for Cross-domain Aspect Sentiment Triplet Extraction", "authors": ["Kun Peng", "Lei Jiang", "Qian Li", "Haoran Li", "Xiaoyan Yu", "Li Sun", "Shuo Sun", "Yanxian Bi", "Hao Peng"], "abstract": "Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract fine-grained sentiment elements from target domain sentences by leveraging the knowledge acquired from the source domain. Due to the absence of labeled data in the target domain, recent studies tend to rely on pre-trained language models to generate large amounts of synthetic data for training purposes. However, these approaches entail additional computational costs associated with the generation process. Different from them, we discover a striking resemblance between table-filling methods in ASTE and two-stage Object Detection (OD) in computer vision, which inspires us to revisit the cross-domain ASTE task and approach it from an OD standpoint. This allows the model to benefit from the OD extraction paradigm and region-level alignment. Building upon this premise, we propose a novel method named Table-Filling via Mean Teacher (TFMT). Specifically, the table-filling methods encode the sentence into a 2D table to detect word relations, while TFMT treats the table as a feature map and utilizes a region consistency to enhance the quality of those generated pseudo labels. Additionally, considering the existence of the domain gap, a cross-domain consistency based on Maximum Mean Discrepancy is designed to alleviate domain shift problems. Our method achieves state-of-the-art performance with minimal parameters and computational costs, making it a strong baseline for cross-domain ASTE.", "sections": [{"title": "1 INTRODUCTION", "content": "Fine-grained sentiment analysis is an important research direction in Natural Language Processing (NLP). In recent years, there has been a constant stream of novel tasks being presented in this field, such as aspect-based sentiment analysis (ABSA) [24, 41], aspect opinion pair extraction (AOPE) [33], and aspect sentiment triplet extraction (ASTE) [4, 21, 33, 40]. These tasks vary in the elements they seek to extract, with ASTE representing the most intricate and challenging undertaking. As shown in the left half of Figure 1, ASTE involves the comprehensive extraction of triplets from a given sentence, encompassing aspect terms, opinion terms, and sentiment polarity (i.e., positive, neutral, and negative). Give the sentence \"The fried rice is amazing here.\", the corresponding triplet is (fried rice, amazing, Positive). Although there have been many domain-specific ASTE works, they fail to meet the need to handle data from multiple domains in real-world scenarios. As shown in the right half of Figure 1, when we apply the ASTE model trained exclusively in the restaurant domain to predict data from the unseen laptop domain, the model generates incorrect predictions due to its inability to adapt to the new domain knowledge.\nCross-domain ASTE task [8] has newly been proposed with the objective of training models to perform ASTE tasks in the target domain, using only annotated data from the source domain and unlabeled data from the target domain. Previous works on cross-domain fine-grained sentiment analysis focused on learning cross-domain invariant pivotal features [15, 38]. However, this approach relies on task-specific design and may not fully capture the nuanced distinctions within each particular domain. Another more promising research direction is to leverage the power of pre-trained language models (PLMs) to generate a large amount of pseudo data that fits the distribution of the target domain [8, 13]. Although effective, compared to providing pseudo labels for unlabeled data, this method requires significant computational costs during the data generation process. Mean-teacher [1, 30] is a widely adopted method in unsupervised domain adaptation (UDA). It improves the performance by training a supervised student model and an unsupervised teacher model, utilizing their prediction differences on labeled and unlabeled target domain data. Although highly effective in computer vision (CV) tasks, mean-teacher is rarely utilized in ASTE, where generation paradigms are generally preferred. To leverage the embedded prior knowledge of PLMs, these methods [8, 13] choose to endure the high computational costs. This consensus hinders the application of the mean teacher in cross-domain research within the ASTE area. However, diverging from previous studies, we have discovered a striking similarity between the table-filling method used for ASTE and the two-stage Object Detection (OD) method [25] in terms of their workflow. Mapping sentiment triplets to regions on the figure allows the model to benefit from staged extraction (for ASTE) and region-level alignment (for cross-domain). This intriguing finding has inspired us to propose the Table-Filling via Mean Teacher (TFMT) approach.\nEmploying the table-filling method [10, 33] is an appealing direction for addressing the ASTE task. It encodes a sequence of sentences into a 2D table, where the rows and columns represent different entity categories (i.e., aspect terms and opinion terms). The relations between entities are marked in the off-diagonal grid of the table, and supervised training is used to learn the relations between words. As shown in Figure 2, we observe that the region-level table-filling method shares a similar process logic with the two-stage OD, which involves four steps: 1) A given image is encoded into a feature map using a convolutional neural network (CNN). 2) A Region Proposal Network (RPN) is used to obtain all candidate regions. 3) the feature vectors within each candidate region are encoded into fixed-length representations using Region of Interest (RoI) pooling. 4) a Region Proposal Classifier (RPC) is used for region classification. In summary, the table-filling methods treat the target triplets as specific regions within the table. To detect these regions, they first identify candidate regions and then categorize them, mirroring the two-stage process of detecting and classifying objects in images. Many table-filling methods [4, 33, 40] adopt the two-stage formulation. Other work [19] employs a one-stage extraction framework in the relation extraction task. However, these works merely skim the surface and fail to thoroughly investigate the correlation between the two tasks or explore this direction further. We believe that addressing cross-domain ASTE tasks from a two-stage OD perspective will yield two key advantages for the model: 1) The staged extraction paradigm will better leverage the word relations within the regions; 2) assisting cross-domain models in focusing on crucial information within the regions.\nSpecifically, the proposed TFMT approach utilizes a mean-teacher architecture to guide domain adaptation. Mean-Teacher consists of two models with the same structures: a student model and a teacher model. The teacher model is pre-trained on the labeled source domain and then freezes its parameters in the training step, while the student model is randomly initialized. We reformat the ASTE task into an OD task by treating the encoded table as a feature map. The student network is supervisedly trained in each training epoch using source domain data. At the same time, it receives region-level consistency constraints from the teacher model's pseudo labels generated using unlabeled target domain data. Because the triplets are labeled as regions, region-level consistency converts the extraction alignment from student and teacher into region alignment. After each training epoch, the teacher model parameters are updated via an exponential moving average (EMA) of the student model. Furthermore, since the model only possesses prior knowledge from the source domain, the pseudo-labels generated on the target domain inevitably contain noise. To alleviate this label shift phenomenon, we propose a multi-perspective consistency module based on Maximum Mean Discrepancy (MMD) [17].\nOur main contributions can be summarised as follows:\n1) We observe the similarity between table-filling methods in ASTE tasks and two-stage OD, which guides us to explore an effective solution for cross-domain ASTE.\n2) The proposed multi-perspective MMD consistency can effectively mitigate the label shift caused by domain adaptation without increasing parameters.\n3) The proposed TFMT framework avoids the paradigm of synthetic data generation, resulting in significantly fewer parameters and computational time costs compared to previous approaches.\n4) Our framework is concise yet achieves state-of-the-art performance, serving as a strong baseline for follow-up research and inspiring the adoption of more advancing mean teacher methods in this field."}, {"title": "2 RELATED WORK", "content": "2.1 Aspect Sentiment Triplet Extraction (ASTE)\nASTE is a recently emerged and challenging task. [20] introduced this task and employed a pipeline approach to extract aspect terms, opinion terms, and sentiment polarities in multiple stages. Recognizing the heavy reliance on word-level interactions in previous studies, SpanASTE [34] proposed a span-level method to consider span-to-span interactions explicitly. Some studies have treated ASTE as a sequence-to-sequence task and utilized generative models to address it. GAS [39] designed two normalized generative paradigms, annotation-style and extraction-style modeling. COM-MRC [37] formalized the ASTE task into a Machine Reading Comprehension (MRC) based framework and devised a context-enhancement strategy to identify sentiment triplets. RoBMRC [16] proposed a robustly optimized bidirectional machine reading comprehension method to tackle issues of query conflict and probability unilateral decrease encountered during generation. MvP [9] used human-like problem-solving processes to generate and summarize sentiment elements in different orders.\nAnother popular approach is the table-filling [18] method. GTS [33] was the first to apply the table-filling method to the ASTE task, they annotated sentiment triplets into a two-dimensional table for extraction. EMC-GCN [4] introduced a multi-channel GCN-based approach that further integrates syntactic and textual structural information into table-filling. On the other hand, BDTF [40] modified the original table-filling annotation scheme and proposed a boundary detection-based method.\n2.2 UDA for Fine-grained Sentiment Analysis\nEarly UDA studies for fine-grained sentiment analysis primarily focused on learning domain-invariant features. The majority of these explorations introduced syntactic knowledge as a bridge to overcome domain gaps. [15] proposed a Selective Adversarial Learning (SAL) method to automatically capture the latent relations of words, thereby reducing reliance on external linguistic resources. [32] developed a novel RNN that can effectively mitigate word-level domain shifts through syntactic relations. EATN [38] designed two special tasks and a novel aspect-oriented multi-head attention mechanism to learn shared features from a well-labeled source domain and guide the classification performance in the target domain. Another line of research has placed emphasis on data. CDRG [36] and GCDDA [13] proposed a two-stage generative cross-domain data augmentation framework. Based on reviews labeled in the source domain, this framework first masks source-specific attributes and then converts the domain-independent review into a target-domain review, thereby generating target-domain reviews with fine-grained annotations. BGCA [8] further advanced this approach by proposing a bidirectional generative framework. This framework trains generative models in text-to-label and label-to-text directions to produce higher-quality target-domain data.\n2.3 UDA for Object Detection (OD)\nTo learn domain-invariant visual features, some adversarial learning methods [6, 26] employed a domain discriminator and conducted adversarial training on the feature encoder and discriminator. Another"}, {"title": "3 METHOD", "content": "In this section, we first give the task formalization and introduce the table tagging scheme shown in Figure 3. Then, we present the proposed Table-Filling via Mean Teacher (TFMT) method as illustrated in Figure 4.\n3.1 Task Formalization\nGiven a sentence s with length n, the ASTE task aims to extract all triplets \\(\\tau = [(a, o, l)_1, (a, o, l)_2, ..., (a, o, l)_{|\\tau|}]\\) from s, where a, o and l denote aspect term, opinion term and sentiment polarity in {Pos, Neu, Neg}, respectively. For cross-domain ASTE, given a set of source domain labeled sentences \\(D_s = \\{(s^s, t^s)_i\\}_{i=1}^{|D_s|}\\) and a set of target domain unlabeled sentences \\(D_t = \\{s^t_i\\}_{i=1}^{|D_t|}\\), where \\(s^s\\) and \\(s^t\\) are sentences in the source and target domains, respectively. \\(\\tau^s\\) is the triplets set of \\(s^s\\). Our model aims to extract all triplets from the test set specific to the target domain.\nTable Tagging Scheme. As shown in Figure 3, the relations between words in a sentence are represented in a square n \u00d7 n table. The vertical and horizontal axes represent aspects and opinions, respectively. There exist two types of table-filling methods: cell-level and region-level. As shown in Figure 3 (a), the cell-level method annotates aspect terms (A) and opinions terms (O) on the diagonal of the table, while the cross-region between the two is used to annotate the corresponding sentiment polarity. It extracts the positions of aspect/opinion terms and then obtains the extraction results based on the category of the cells where they intersect.\nUnlike the cell-level method, the region-level method [40] treats the target triplet as a specific region within the table and aims to precisely identify its surrounding boundaries. As shown in Figure 3 (b), regions' boundaries are labeled using beginning (B) and ending (E), followed by labeling the sentiment of the annotated regions. Here, B and E are used to represent the upper left and lower right corners of a relation rectangle, respectively. While the rest of the numerous gray cells are marked as \"None\". This boundary-based tagging scheme converts word-level annotations in text sequences into region-level annotations on a feature map. Unlike cell-level methods that annotate all cells at once, region-level methods aligns closely with the two-stage object detection paradigm, hence adopted in our TFMT.\n3.2 Table-Filling via Mean Teacher (TFMT)\nThe architecture of TFMT\u00b9 is shown in Figure 4. Our TFMT framework consists of a student model and a teacher model, both sharing the same model architecture. The student model serves as the primary model, receiving supervised training on the source domain data while also undergoing unsupervised training using unlabeled target domain data.\nThe teacher model is utilized to guide the training of the student model. It is pretrained on the source domain and then frozen at each training step. During training step, the teacher model focuses on the target domain and generates pseudo labels for the student model. Specifically, in each training iteration, the target domain data \\(D_t\\) is augmented using a random word substitution to produce \\(D_t'\\). The teacher model then processes these augmented data and produces multiple region proposals along with their predicted softmax values \\(P^t\\). Subsequently, we filter the region proposals and retain only those labels for which the confidence \\(\\text{max}_{i \\in C}(P^t)\\) (where C is the foreground label set) exceeds the threshold \\(\\eta\\). This confidence threshold \\(\\eta\\) is applied to ensure that only high-confidence pseudo labels are assigned to the student model. After each training step, The teacher's parameters are updated through an Exponential Moving Average (EMA) [30] mechanism: \\(\\Theta_t = \\lambda \\Theta_t + (1 - \\lambda) O_s\\), where \\(\\lambda\\) (0 < \\(\\lambda\\) < 1) is a hyper-parameter. \\(\\Theta_t\\) and \\(O_s\\) denote the model parameters of teacher and student, respectively.\n3.2.1 Supervised Training. To align with the two-stage OD paradigm for both teacher and student base models, we adopted Boundary-Driven Table-Filling (BDTF) [40] and restructured the framework accordingly. The architecture of the base model is shown in the lower half of Figure 2.\nTable Encoding. Given the sentence s = [\\(x_1, x_2, ..., x_n\\)], we input it into a pre-trained language model and get the last hidden layer [\\(h_1,h_2, ..., h_n\\)] \\(\\in\\) \\(R^{n\\times d}\\) as the sentence representations, where n is the sentence length.\nWe donate the relation table as \\(T \\in R^{n\\times n \\times d}\\), which maps the relations between any two words into separate cells. Specifically, for a cell \\(t_{ij} \\in R^d\\) in T, it represents the relation between the i-th and j-th words in sentence s. To construct the relation table, BDTF calculates each \\(t_{ij}\\) as follows:\n\\[t_{ij} = \\text{Linear}(h_i \\oplus h_j \\oplus \\text{pooling}(h_{i:j}) \\oplus (h_i)^T V h_j),\\]\nwhere Linear denotes a nonlinear projection with an activation function, \\(\\oplus\\) denotes a concatenation operation. The third term"}, {"title": "3.2.2 Region-level Consistency", "content": "to a cell-level consistency loss:\n\\[L_{uns}' = \\sum_{k \\in C} ||P_k - \\hat{P_k}||^2,\\]"}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Settings\n4.1.1 Datasets. Following [8], we evaluate our model by employing a publicly available dataset sourced from the SemEval ABSA Challenges [22\u201324]. This dataset was released by [35] and consists of four subsets, namely R14, R15, R16, and L14, which are derived from two domains: restaurant reviews and laptop reviews.\n4.1.2 Implementation. We opt for BERT [31] as our language encoder and a two-layer ResNet [11] as the feature encoder, with a dimension set at 768. The pruning threshold k and the confidence threshold \\(\\eta\\) are set to 0.3 and 0.98, respectively. We directly use BERT-base-uncased as the pretraining parameters to initialize both the student and teacher models. If not specified, the default setting for the rate smoothing coefficient \\(\\lambda\\) of the EMA is 0.6. The loss weight factors \\(\\alpha\\) = 1 and \\(\\beta\\) = 0.005. For the data augmentation method, since the prerequisite for region-level consistency is to maintain the same encoded length of the augmented sentences, we randomly replaced 50% of the words in the sentence with words that have the same encoding lengths. We conduct model training on one RTX 3090 GPU for 10 epochs, utilizing a batch size of 4 and setting the learning rate to 3 \u00d7 10\u22125. We assess the performance of the training model on the development set and save the best one. We use sentence-level F1 score as the evaluation metric. In this metric, a sentence is considered a true positive only when all sentiment triplets within it are correctly extracted. All reported results are averaged across five runs with different random seeds.\nCell-level TFMT (cTFMT). The cell-level method doesn't utilize a region proposal method like the two-stage OD paradigm but instead directly classifies each cell in the relation table. To adapt to the TFMT method, we treat each cell as an independent region (in \\(L_{uns}\\) and \\(L_{mmd}\\)) and keep the other TFMT modules (i.e., \\(L_{sup}\\)) unchanged. Specifically, in Eq. 9, \\(\\hat{R}\\) represents all cells predicted as one of c = {A, O, POS, NEG, NEU}. In Eq. 12, \\(L_{mmd}\\) is modified to the sum of MMD for all types of cells:\n\\[L_{mmd} = \\sum_{k \\in c} L_{MMD} (t_k, \\hat{t_k}).\\]\nThe cell-level TFMT (cTFMT) is a degradation of the original region-level TFMT.\n4.1.3 Baselines. Although there are many baseline models for cross-domain sentiment analysis (e.g., EATN [38], CDRG [36], GCDDA [13] and BGCA [8]), these works mainly focus on sentence-level sentiment analysis or aspect-based sentiment analysis (ABSA). Among them, BGCA addresses both ASTE and AOPE tasks, while GCDDA specifically targets the AOPE task.\nDue to the scarcity of work on UDA for ASTE currently, besides BGCA, we have also leveraged state-of-the-art models in a zero-shot manner within the in-domain setting. These models have been categorized into three groups:\n1) pipeline: SpanASTE [34] captures the span-to-span interactions during the multi-step filtering process."}, {"title": "4.2 Main Results", "content": "The main results are reported in Table 2, where the best results are highlighted in bold. According to the results, our findings can be summarized as follows: 1) Our top-performing model (BDTF-TFMT) outperforms the current SOTA baseline BGCAlabel text across all tasks, with an average improvement of 1.79% on the restaurant-to-laptop dataset, 0.58% on the laptop-to-restaurant dataset, and\nResult on the cross-domain AOPE task. In order to provide comprehensive evidence of the TFMT method's enhancement in cross-domain fine-grained sentiment analysis, we conduct an extra experiment on the aspect opinion pair extraction (AOPE) task with the same dataset. Compared to ASTE, AOPE does not require sentiment polarity extraction; instead, its aim is to extract all (aspect term, opinion term) pairs from a given sentence. To align with the"}, {"title": "4.3 Computational Costs Analysis", "content": "We conduct a comparative experiment on computational costs. The average results are shown in Table 4. From the table, we have the following findings: 1) Comparing the results of the table-filling model before and after using TFMT, as TFMT is a pluggable module designed for guiding cross-domain learning, it is only utilized during the model training process, thus having no impact on the model's inference speed. Furthermore, since TFMT does not introduce any additional parameters, the model's parameter count does not change. 2) Compared to BGCAlabel\u2192text, BDTF-TFMT exhibits a 19.1% reduction in average training time per sample due to the fact that BGCA spends more time on pseudo-data generation. 3) In comparison to BGCAlabel text, BDTF-TFMT has 27.35% fewer parameters, yet achieves a 2.08% higher F1 score and the average inference time per sample decreases by 65.04%. This can be attributed to the fact that while BGCA operates on a generative backbone (encoder-decoder architecture, e.g., t5), the table-filling method relies on an auto-encoder backbone (encoder-only architecture, e.g., BERT). Consequently, the table-filling method typically entails lower parameter count and inference costs."}, {"title": "4.4 Comparative Study", "content": "To investigate the mechanism of the TFMT module, we conduct a comparative experiment, and the results are reported in Table 5. The designs for different domain adaptation components here in Table 5 are as follows:\n\u2022 TFMT/cTFMT: The effect exhibited by TFMT and cTFMT.\n\u2022 s-t: Replace TFMT with the self-training method. Similar to the mean-teacher approach, the model is first pre-trained on the source domain in this setup. Subsequently, the model generates pseudo-labels for the target domain data and incorporates them into the training of the next iteration. Since there are no paired teacher-student models, \\(L_{uns}\\) and \\(L_{mmd}\\) cannot be implemented.\nBased on the data presented in the table, we can observe that both GTS-BERT and EMC-GCN exhibit lower TFMT performance compared to the self-training method (averaging 1.5% and 1.54%, respectively). In contrast, BDTF shows a significantly higher TFMT"}, {"title": "4.5 Error Analysis of pseudo labels", "content": "As the quality of pseudo labels significantly influences cross-domain performance, we further investigate the effect of TFMT by examining the generation of pseudo labels in three settings: \u201cTFMT\u201d, \u201cs-t\u201d, and \u201ccTFMT\u201d. The results are presented in Figure 5. We categorize all types of errors into four types:"}, {"title": "4.6 More Analysis", "content": "Ablation Study. To delve into the module mechanisms within TFMT, we conduct a series of ablation experiments as shown in Table 6, and we have the following observations: 1) When the data augmentation module is removed, the model's performance does not exhibit a significant decline. This is because the primary aim of data augmentation is to introduce perturbations to the teacher model's inputs, thereby mitigating the risk of overfitting. 2) When we remove \\(L_{uns}\\) or \\(L_{mmd}\\) individually, there is a noticeable decline in model performance. Removing \\(L_{uns}\\) led to performance drops of 0.93% and 1.49% on the cross-domain ASTE and AOPE tasks, respectively. Similarly, removing \\(L_{mmd}\\) results in performance decreases of 1.65% and 1.13% on the respective tasks. When \\(L_{uns}\\) and \\(L_{mmd}\\) both are removed simultaneously, the model's performance severely degrades. The performance decreases of 3.14% and 3.69% on the respective tasks. This demonstrates the significant role played by both \\(L_{uns}\\) and \\(L_{mmd}\\) in TFMT.\nCoefficients Study. To investigate the effect of weight coefficients \\(\\alpha\\) and \\(\\beta\\) in Equation 13, we present the average performance curves in Figure 6. In Figure 6(a), fixing \\(\\beta\\) at 0.005, we find the optimal performance when \\(\\alpha\\) = 1. In Figure 6(b), setting \\(\\alpha\\) as 1, we observe that the best performance is attained when \\(\\beta\\) = 0.005."}, {"title": "5 CONCLUSION", "content": "In this paper, we find the inherent consistency between table-filling methods for ASTE and two-stage OD methods. This insight motivates our proposed TFMT method to perceive the ASTE task as an OD task and then leverages region-level consistency constraints for unsupervised training. Experimental results show that the proposed method significantly improves boundary-based paradigms, while the grid-based paradigm leads to a degradation in region-level consistency, further underscoring the alignment of the two tasks. To improve the quality of pseudo labels, we propose an MMD-based consistency constraint, which effectively mitigates the issue of domain discrepancy. Our method achieves state-of-the-art performance while operating under significantly reduced parameter counts and computational costs compared to previous approaches. This makes it an ideal and strong baseline for subsequent work in this field. In future work, we will continue to develop further advanced mean-teacher methods for cross-domain ASTE."}]}