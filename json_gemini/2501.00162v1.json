{"title": "Class-based Subset Selection for Transfer Learning under Extreme Label Shift", "authors": ["Akul Goyal", "Carl Edwards"], "abstract": "Existing work within transfer learning often follows a two-step process - pre-training over a large-scale source domain and then finetuning over limited samples from the target domain. Yet, despite its popularity, this methodology has been shown to suffer in the presence of distributional shift - specifically when the output spaces diverge. Previous work has focused on increasing model performance within this setting by identifying and classifying only the shared output classes between distributions. However, these methods are inherently limited as they ignore classes outside the shared class set, disregarding potential information relevant to the model transfer. This paper proposes a new process for few-shot transfer learning that selects and weighs classes from the source domain to optimize the transfer between domains. More concretely, we use Wasserstein distance to choose a set of source classes and their weights that minimize the distance between the source and target domain. To justify our proposed algorithm, we provide a generalization analysis of the performance of the learned classifier over the target domain and show that our method corresponds to a bound minimization algorithm. We empirically demonstrate the effectiveness of our approach (WaSS) by experimenting on several different datasets and presenting superior performance within various label shift settings, including the extreme case where the label spaces are disjoint.", "sections": [{"title": "1 Introduction", "content": "As machine learning becomes increasingly data-dependent, it is more efficient to leverage existing labeled datasets when adapting to novel scenarios. However, large publicly available datasets are often not distributionally representative of the specialized task. For example, images in Imagenet [Deng et al., 2009b] or COCO [Lin et al., 2014] are largely different from images captured by a niche sensor - negatively impacting a model trained on both datasets [Pang et al., 2018]. Transfer learning, particularly in domain adaptation, has become a popular approach to reducing this distributional shift and enabling knowledge to be transferred more easily between datasets. Previous work [Zhang et al., 2020, 2019] in few-shot learning has shown that it is possible to learn invariant features over the source and target datasets using a few labeled samples from the target distribution [Li et al., 2021], facilitating a more accurate transfer.\nOther work has shown success in reducing the distributional shift within the input space [Ganin et al., 2016a, Zhao et al., 2018]; however, these methods struggle when the distributional shift occurs within the label space [Zhao et al., 2019, Johansson et al., 2019, Zhao et al., 2022]. In many cases, prior methods' performances degrade due to negative transfer [Wang et al., 2019, Zhao et al., 2019], defined by the increase in the risk in the target distribution between a model trained on the combination of source and target distributions versus it solely trained on the target distribution. In other words, given similarity exists in the output space, divergent classes (classes that only appear in the source distribution) reduce the transferability of a model to the target distribution.\nMotivated by the above observation, we aim to reduce negative transfer in this work by eliciting the optimal subset of source classes that allows for the best transfer to the target domain. Contrary to previous methods, our work makes no assumptions about the relation-"}, {"title": "2 Preliminaries", "content": "In this section, we first introduce the notation used throughout the paper and then briefly discuss Wasserstein distance and the different settings of domain adaptation.\nNotation We define a domain D as a joint distribution over the input and output space X x Y. In this work, we focus on the classification setting, where the output space is given by y = [k] := {1, ...,k} with k being the number of output classes. In the context of representation learning, we obtain a learned representation z = f(x) by applying a feature encoder fe: X \u2192 Z parametrized by \u03b8, where we use Z CRP to denote the feature space. Upon the feature vector z \u2208 fo(X), we further apply a predictor g: Z \u2192 \u0394k, where we use \u0394k to denote the (k-1)-dimensional probability simplex. We use the cross-entropy loss as our objective function. More specifically, let qy \u2208 \u0394k be a one-hot vector with the y-th"}, {"title": "3 Method", "content": "In this section, we present our method, WaSS, in a setting, which we term as extreme label shift, where under transfer learning, the class labels are disjoint between the source and target domains.\nOverview of our method At a high level, WaSS contains two stages. In the first stage, we select the best subset of classes from the source domain by solving a linear program to minimize the Wasserstein distance. In the second stage, a classifier is trained based on the reweighted source classes. Then, we fine-tune the source-trained classifier on a limited amount of data from the target domain."}, {"title": "3.1 Class-based Subset Selection via Linear Program", "content": "In light of the negative transfer problem when class labels are disjoint between the source and target domains, we propose to mitigate this issue by selecting a subset of classes from the source domains that are similar to data from the target domain. More specifically, let Z = f(X) be the features we obtain after applying an encoder f\u03b8, e.g., features from ResNet, to the input data. Hence, given a distribution D(X) over X, under the features Z = f\u03b8(X), we obtain a corresponding induced distribution over Z, denoted by D(Z). We then propose to solve the following optimization problem:\n$\\min_{\\Psi \\in \\Delta^{K}} W_1(\\sum_{i\\in [k]} w_i D_{S_i}(Z), D_T(Z))$"}, {"title": "3.2 Transfer Learning under Extreme Label Shift", "content": "Pre-training Once we have obtained the optimal vector w\u2217, we proceed to learn a classifier g \u25e6 f\u03b8 from the labeled data in the source domain given by w\u2217:\n$\\min_{g,f_\\theta}\\sum_{i\\in[k]:w_i>0}\\frac{w_i}{n_i}\\sum_{(x,y)\\in D_{Si}} l_{CE}(y,g(f_\\theta(f_p(x))))$\nNote that f_p() is fixed during the above optimization since the optimal w\u2217 is obtained via Z = f_p(X).\nFine-tuning Because of the disjoint label classes between the source and the target domains, we cannot directly apply the learned classifier g() in the target domain. Hence, during the fine-tuning stage, on top of the learned features f\u03b8(fp(X)), we shall use the small amount of labeled data from the target domain to train a new classifier g' as follows:\n$\\min_{g'} \\sum_{(x,y) \\in D_T} l_{CE}(y, g'(f_\\theta(f_p(x))))$\nIt is worth emphasizing that the learned feature encoder f\u03b8 will be fixed during the fine-tuning stage. Overall, our proposed approach consists of two stages. In the first stage, we select and reweight a subset of classes from the source domains by minimizing the empirical Wasserstein distance. In the second stage, we train a"}, {"title": "3.3 Theoretical Analysis", "content": "Theorem 2.1 proved that for any fixed standard classifier h to be used on both domains, the target error of h could be bounded by the sum of the source error, the Wasserstein distance between the two domains, and the optimal joint error \u03f5\u2217. However, this does not apply to our setting since our algorithm has a fine-tuning step that generates an updated classifier tailored to the target domain. The fine-tuning step is necessary for the extreme label shift because the label spaces are disjoint. Furthermore, the proof of Theorem 2.1 [Shen et al., 2018] also assumes a shared label space between S,T, which does not hold under extreme label shift. In such cases, it is not hard to see that \u03f5\u2217 \u2265 1 [Zhao et al., 2019], rendering the upper bound vacuous.\nTo approach the above technical difficulties, we define a lifted output space to deal with the otherwise disjoint classes. Given Vs, Vr to represent the class set of S, T, let YS,T = VS \u222a VT be the union of the two sets as the lifted output space bridging the two domains under a common label set. Under this construction, it is clear that any probabilistic classifier over Ys or Vr is still a probabilistic classifier over YS,T. For a probabilistic classifier h: X \u2192 \u0394k, we first define the so-called induced classifier from h as follows.\nDefinition 3.1 (Induced classifier). Let h: X \u2192 \u0394k be a probabilistic classifier. Define an induced classifier h: X \u2192 [k] as follows: \u2200i \u2208 [k], h\u02c6(X) = i with probability h(X)i.\nThe induced classifier h\u02c6 is a randomized classifier that outputs the class label according to the probability vector given by h(X). This is different from the deterministic classifier that always outputs arg maxi\u2208[k] h(X)i. The following proposition gives a closed-form characterization of the 0-1 classification error of any induced classifier in terms of the probabilistic classifier.\nProposition 3.1. Leth: X \u2192 \u0394k be a probabilistic classifier and h\u02c6 its induced classifier. Then, the expected error of the induced classifier ED(h\u02c6) = E [||h\u02c6(X) \u2212 Y||1], where Y \u2208 {0, 1} k is a k-dimensional one-hot vector representing the ground-truth label.\nBased on the above characterization, the next lemma bounds the error difference of the same probabilistic classifier h under two domains DS and DT by their Wasserstein distance:\nProposition 3.2. Let h: X \u2192 \u0394k be a probabilistic classifier that is \u03c1-Lipschitz continuous under the input norm \u21132 and output norm \u21131, i.e., \u2200x, x' \u2208 X,"}, {"title": "4 Experiments", "content": "Previously, we provided a theoretical analysis upperbounding WaSS's error. We now empirically evaluate WaSS's efficacy in selecting the optimal subset of training classes. We compared WaSS against five different baselines on four separate datasets. For the brevity of the paper, we will briefly detail the setting for each experiment and leave additional details, including details about the datasets, baselines, and hyperparameter selection, in Appendix A. For each experiment, we created a source and target domain by splitting the classes in each dataset. The class set was divided so the target domain contained 3 classes while the rest of the classes represented the source domain. Depending on the label shift setting, the source domain could additionally include a subset of the test classes. We plan to release our implementation of WaSS upon publication."}, {"title": "4.1 Quantitative Performance", "content": "We evaluate WaSS's impacts on the transfer distance between the source and target domain; however, measuring transfer distance is complex Jiang et al. [2022]. Instead, we use the accuracy of a feed-forward neural network on the target distribution as the proxy for calculating transfer distance. We conduct our experiment on two label shift scenarios: disjoint (DDA) and open-set (ODA).\nDisjoint Set (DDA): In Table 1, we present the accuracies for five different methods within the Disjoint Set Domain Adaptation (DDA) setting - a complete absence of class overlap between the source and target domains. For each dataset, we executed every method across three distinct source/target domain class splits and reported the average accuracy over ten iterations. For all but two experiments, WaSS's class weighting on the source domain best facilitates the transfer of the downstream classifier to the target distribution, resulting in an average 1- 2 absolute percentage point increase over the next-best selection strategy. In both instances where WaSS is outperformed, a uniform weighting over the source classes (ALL) provided a better transfer between domains. We theorize that WaSS's dependency on a finite number of input samples to calculate the optimal subset can result in errors within the weighting. For situations where the optimal class selection is close to the source class distribution, the errors induced by the finite-sample effect allows ALL to provide a more accurate class selection than WaSS.\nWe further validated our experiments by conducting a large-scale test on Fashion-MNIST and CIFAR-10 sam-"}, {"title": "4.2 Qualitative Performance", "content": "We now qualitatively show how WaSS's weighting can reduce the distance between the source and target domains. To visualize 'closeness,' we utilize pie charts that describe the transformation of the source class distribution before and after WaSS's reweighting. Beneath each figure, we also provide the target domain class set to show the increase in similarity between the target domain and selected source domain classes."}, {"title": "5 Related Works", "content": "Domain Adaption Based on the degree of separation between domains, previous work can be classified into three different settings: Closed Domain Adaptation (CDA), Partial Domain Adaptation (PDA), and Open Set Domain Adaptation (ODA). In CDA [Saenko et al., 2010, Gong et al., 2012, Pan et al., 2010, Tzeng et al., 2014, Long et al., 2015, Ganin et al., 2016b], the class set between source and target domains is assumed to be the same, so methods focus on mitigating the distributional discrepancy within the input space. PDA [Cao et al., 2018a,b, Zhang et al., 2018, Li et al., 2020]"}, {"title": "6 Conclusion", "content": "While large-scale pre-training has benefited many deep learning methods, ensuring this pre-training data is appropriate for the task at hand is critical. To this end, we introduce a new method, WaSS, that identifies the subset of source domain classes that optimally reduce the transfer distance between source and target domains. Conventional wisdom may dictate it is better to use all available data, but this can be problematic due to negative transfer, especially for smaller datasets. Our method uses an efficient linear programming method employing Wasserstein distance as a proxy for transferability, which allows us to outperform all baselines and provide a more effective source class weighting."}, {"title": "A Additional Details About Experimental Setup", "content": "Datasets We evaluate the performance of our method in Section 4 on four datasets: Fashion-MNIST [Xiao et al., 2017], CIFAR-10 [Krizhevsky, 2009], PACS [Zhou et al., 2020], and CIFAR-100 [Krizhevsky et al., 2009]. Fashion-MNIST is the simplest dataset we consider containing 60,000 train and 10,000 test grayscale images of ten different clothing items. Within the dataset, only a handful of features (shape, size) differentiate each class. CIFAR-10 is a more complex dataset consisting of 10 different classes with a train/test split of 50,000/10, 000 colored images, respectively. Adding color allows for a richer feature set per class within the dataset. PACS is a dataset that simulates the domain adaptation problem; it contains 4 domains, each with 7 classes. Furthermore, PACS does not provide a constant number of images per class as to emulate real-world scenarios. CIFAR-100 is a larger extension of CIFAR-10, containing 100 classes with a training/test split of 50,000/10,000 colored images, respectively. CIFAR-100 provides course-grained labels that group its 100 classes into 20 domains. PACS and CIFAR-100 represent the noisy datasets machine learning often encounters: with PACS modeling class imbalance and domain shift and CIFAR-100 modeling diverse and numerous class labels.\nWe sample three classes for each dataset to serve as the target domain and use the remaining classes as the source domain. We show 4 different source/target splits to account for inter-dataset variability in Table 1. For CIFAR-100, we maintain the dataset structure by sampling the test domain from a single superclass and using the other superclasses as the source domain. Similarly, for PACS, the three classes composing the target domain are selected from one of the distributions within the dataset while the other 3 distributions are used for training.\nImplementation All our experiments are implemented using PyTorch [Paszke et al., 2019]. We use a ResNet-50 pre-trained on ImageNet [Deng et al., 2009a] to create a 2048 dimensional feature vector for each image in the dataset. Within the embedding space, we calculate the Wasserstein distance between different images. WaSS aims to find the optimal weighting of classes within the source dataset such that the Wasserstein Distance between the resulting mixture of source classes and the target domain is optimally close. We use POT [Flamary et al., 2021] to calculate the Wasserstein distance between the source and target domains. We utilize SinkHorn [Cuturi, 2013] to increase the efficiency of our algorithm, especially when presented with larger datasets such at CIFAR-100 and PACS. Once"}, {"title": "B Limitations", "content": "WaSS identifies the optimal subset of classes within the source distribution to facilitate transfer between datasets. However, as we mentioned within our experimental section 4, our method does not always outperform the baselines due to a finite sample effect. As a result, our method may not result in the optimal subset if not enough number of samples are provided from the target distribution.\nAnother limitation of WaSS is the number of datasets we experimented with. As previously stated, our dataset selection sampled datasets of increasing noise. However, there exist more datasets that are used within the literature, and to better understand the performance of our method, we could experiment with these datasets. In Section C, we do experiment with 3 other datasets that are more complicated and diverse than the 4 datasets we previously introduced. In these experiments we show that WaSS can provides a better accuracy performance than the other baselines it is compared against.\nFinally, our method uses a pre-trained deep neural network to embed each image in the dataset. These embeddings provide a semantically useful similarity measure that we then utilize to identify the optimal subset. However, a pre-trained deep neural network may not always provide a meaningful embedding to images within a dataset. This can skew our method and result in a subset not fully representing the similarity between domains. Future work can look into the effects the neural network used to embed the images may have on the performance of WaSS."}, {"title": "C Additional Experiments", "content": "In section 4, we evaluate against four different datasets to show the benefits brought on by WaSS empirically. However, there are limitations in the datasets we evaluate on including their scale and diversity of images. As a result, we consider three additional datasets to evaluate WaSS in a Disjoint Set Domain Adaptation (DDA)"}, {"title": "D Additional figures", "content": ""}]}