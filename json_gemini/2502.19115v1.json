{"title": "Improving customer service with automatic topic detection in user emails", "authors": ["Bojana Ba\u0161aragin", "Darija Medvecki", "Gorana Goji\u0107", "Milena Oparnica", "Dragi\u0161a Mi\u0161kovi\u0107"], "abstract": "This study introduces a novel Natural Language Processing pipeline that enhances customer service efficiency at Telekom Srbija, a leading Serbian telecommunications company, through automated email topic detection and labelling. Central to the pipeline is BERTopic, a modular architecture that allows unsupervised topic modelling. After a series of preprocessing and postpro-cessing steps, we assign one of 12 topics and several additional labels to incom-ing emails, allowing customer service to filter and access them through a cus-tom-made application. The model's performance was evaluated by assessing the speed and correctness of the automatically assigned topics across a test dataset of 100 customer emails. The pipeline shows broad applicability across lan-guages, particularly for those that are low-resourced and morphologically rich. The system now operates in the company's production environment, streamlin-ing customer service operations through automated email classification.", "sections": [{"title": "Introduction", "content": "In recent years, there has been a significant increase in the adoption of artificial intelligence (AI) solutions by companies worldwide [1]. The growing availability of large datasets, improved computing power, and sophisticated algorithms have allowed the companies to use machine learning-based natural language processing (NLP) solutions for various purposes, leading to improved customer experience [2], minimized costs [3], and streamlined processes [1, 4].\nWhile the NLP solutions efficiently handle routine inquiries regarding products and services, enabling the employees to focus on more complex issues, they also face several challenges, including semantic and syntactic ambiguity, managing large volumes of information, and dealing with domain-specific language [4]. These challenges are additionally intensified in low-resourced settings, where there is a scarcity of both datasets and machine learning-based tools for automating tasks."}, {"title": "Related work", "content": "Low-resourced languages face obstacles due to the limited availability of training data and automated processing tools [6, 7]. Most solutions in such settings rely on large multilingual models [8\u201312], which are adapted for specific domains.\nUnsupervised methods have proven valuable in overcoming the challenges of limited training resources. While traditional approaches like LDA (Latent Dirichlet Allocation) and NMF (Non-negative Matrix Factorization) have been commonly used, in recent years, the adoption of BERTopic as a more advanced solution has been on the rise (e.g., [13, 14]). In a study related to customer service, the authors used multilingual BERT embeddings and BERTopic to analyze Turkish customer feedback and identify key complaint themes from an electronic store [10]. Their analysis revealed seasonal patterns in complaints (e.g. increased laptop complaints during distance learning), as well as monthly trends in product-specific issues (like headphone returns), demonstrating BERTopic's capability with minimal data preprocessing. Similarly, the findings from [15] have both scholarly and practical significance. The research demonstrates that BERTopic, combined with multilingual word embeddings, can effectively analyze Turkish customer reviews without requiring extensive text preprocessing, expanding its potential applications in multilingual settings.\nRecently, the authors in [16] applied BERTopic to analyze customer reviews using sentence vector representations. The study examined reviews of a Russian airline service from customer websites. By comparing the automatically generated topics with the manually constructed conceptual model of the domain, the results showed strong alignment with a precision of 0.955 and recall of 0.875. This demonstrates that BERTopic effectively extracts opinion aspects from customer feedback.\nUnlike previous research, the study [17] applied BERTopic to analyze customer service patents registered between 2000 and 2022, using a pre-trained language model. The researchers identified and analyzed ten key topics in customer service technology patents over time, managing to reveal the emerging topics over time. The"}, {"title": "Method", "content": "The development of the model for our use case involved the preparation of training data, the process of model training, and the modification of the model output."}, {"title": "Data preparation", "content": "The dataset for model development consisted of emails provided by Telekom Srbija, collected from both individual and business users. To comply with data protection regulations and safeguard user privacy, the emails had been anonymized by replacing personal information (e.g., names, locations, organizations) with placeholders indicat-ing their category (e.g., PER, LOC, ORG).\nSince we observed that users sometimes described their problem in the email sub-ject and left the body of the email empty or used the terms indicative of their problem only in the subject, we decided to include its content as well. After extracting the email body and concatenating it with the subject, the text of each email was prepro-cessed through a series of transformations designed to retain only informative and clean text, optimizing the dataset quality for model training:\n1. Script standardization and lowercasing: During model training, text written in Latin and Cyrillic scripts results in separate embedding representations, even when the content is semantically identical. To address this issue, we unified the script of all emails by transliterating Cyrillic script emails into Latin. To transliterate emails, we used the Python module strtools [18]. All email content was lowercased to en-sure that words with different capitalizations, but identical meanings have the same representation.\n2. Punctuation, numbers, and special characters removal: We removed punctuation, individual numbers, and special characters (e.g. symbols and emojis) since they are not informative in search of latent topics in the text.\n3. Short document removal: We removed all emails containing three words or less since we found them semantically uninformative for topic modelling. The minimal number of words was determined empirically based on a manual email review.\n4. Duplicate email removal: Redundancy in the dataset can negatively impact the model's ability to generalize by over-representing specific patterns or topics [19]. We removed all duplicate emails to address this, ensuring that the dataset consisted only of unique emails.\n5. Removal of closing phrases, signatures, and disclaimers: Closing remarks (e.g., Best regards) appear across a wide range of emails and typically signify the end of an email and the start of signatures and disclaimers. While these elements are"}, {"title": "Model training", "content": "The training was performed in an unsupervised way using BERTopic, a state-of-the-art topic modelling architecture. BERTopic is a modular architecture model that con-sists of several sequential layers that can be modified to perform the following pro-cesses: 1) extracting the embeddings, i.e. forming a dynamic vector representation of the text, 2) reducing the dimensionality of the vector representation, 3) clustering the embeddings into topics and 4) creating a representation of topics in the form of key-words. An optional step is reducing the number of outliers, i.e. documents that were not initially grouped into any of the discovered topics. We iteratively tested different architecture configurations to achieve the best possible results on our dataset. The results achieved after each change were manually analyzed against the produced topic keywords, number of topics, top representative documents, and 100 test documents.\nAt its first layer, BERTopic uses a sentence transformer [21], a model that signifi-cantly aids the clustering task [22], as its default embedding model. As no sentence transformer is trained only on Serbian text, we used one of the multilingual models"}, {"title": "Modification of the model output", "content": "The most optimal BERTopic configuration described in subsection Model training led to 72 topics and an additional outlier group. As 72 topics are a vast number for customer service applications, we conducted a manual analysis of the topic keywords and representative emails for each topic to determine the optimal number of topics aligned with the company's services and common user concerns. This way, we detected that all 72 topics could fall into 12 clearly defined groups, which the customer service team approved as meaningful. This meant that we needed to reduce the current optimal number of topics from 72 to this number. As testing the model parameters min_topic_size and nr_topics to automatically generate 12 topics did not yield the same ones we detected, we needed to resort to a different strategy.\nAnother strategy we tested was hierarchical topic modelling, which attempts to capture the possible hierarchical structure of the generated topics from the c-TF-IDF matrix to identify which topics are similar and could be merged. To analyze both the general structure and the hierarchy of topic representations in more detail, as well as to evaluate the effects of merging certain topics, we used the visualize_hierarchy() and get_topic_tree() methods. Following manual analysis, we applied the merge_topics() method to reduce the number of topics. This process updates the topic representations of the merged topics and, subsequently, the entire model. This method proved effective for the initial couple of merges. However, as the number of merges rose, the method started to produce unsatisfactory results.\nFinally, we resorted to the manual grouping of the model topics. BERTopic allows the users to add custom labels for each topic. We used this option as a topic reduction strategy by assigning the same 12 custom labels to all the original topics we grouped together, creating the derived topics. In addition, we analyzed the outlier group and assigned one of the derived topics to it for practical purposes, naming it General problems and malfunctions. Since this strategy does not influence the initial clusters of the model, it allows for further modifications of the derived topics if required by customer service. An example of the mapping of original topics into a derived one can be seen in Table 1."}, {"title": "Evaluation", "content": "The model was primarily evaluated for the functionalities needed in the implementa-tion environment, namely processing speed and assignment correctness.\nThe first parameter was the time needed for a topic assignment. The speed span for the entire topic assignment pipeline for a batch of 100, 1,000 and 10,000 emails across three runs was 0.035-0.112 seconds per email. The speed for 100 emails was consistently the highest (0.086-0.112), suggesting a slight negative effect of smaller processing batches. An increase to a batch of 1,000 emails showed significant effi-ciency improvements (0.035-0.038), which once again started to decline with a batch of 10,000 emails (0.040-0.041). Despite these variations, the increase in processing speed between 1,000 and 10,000 emails is roughly linear, showing that the model scales well with more data.\nThe correctness of the topic assignment was evaluated by manually reviewing the assigned topics for the selected 100 test emails. The emails were first reconstructed to include fictitious personal names, locations, and organization names instead of the previously anonymized information to mimic real-life correspondence. They were then assigned one of the 12 derived topics based on the primary concern in the email, after which this topic was compared to the one assigned by the model. In 44 cases, the email contained more than one topic. We identified two types of these situations, the one in which two or more topics have equal significance and the other in which there is one more dominant topic. For emails containing topics with equal relevance (7 cases), we considered the topic correct if it matched any of the topics we identified in the email. For the remaining 37 cases with one dominant topic, we considered the assigned topic correct only if it matched the topic we selected. The results of this evaluation, expressed by accuracy, weighted average precision, recall and F1 score, can be seen in Table 2."}, {"title": "Implementation", "content": "The described topic modelling system is integrated into the Telekom Srbija's produc-tion environment to enhance customer support by automating email categorization. It comprises four main components: a database, a topic model, and email preprocessing"}, {"title": "Conclusions and future work", "content": "This paper presents a topic modelling pipeline for the automatic assignment of topics to user emails. The pipeline is specifically developed to enhance customer service at Telekom Srbija, a major Serbian telecommunications company, allowing it to prioritize certain groups of emails based on the assigned topics. The pipeline relies on BERTopic, which assigns the customer-service-approved topic to an email after a series of preprocessing and postprocessing steps. Based on the average speed of 0.0521 seconds per email and the correctness of topic assignment at a weighted average F1 of 0.96, the model proves effective and reliable for the given purpose. The topics and labels for the filtered emails (e.g., internal correspondence and spam) are stored in a database, which further allows the company to create monthly reports and analyse the prevalence of certain customer concerns over time.\nFor future work, we plan to enhance the pipeline to accurately assign multiple topics per email and process messages beyond the current 128-token limit. Assigning multiple topics is important, as customer emails may address multiple issues that require attention from multiple customer service departments. In larger companies, these departments are often separate, making it necessary to route emails correctly to ensure timely responses. Similarly, while we determined that the size of 128 tokens is enough to capture the main concern in an email, we still plan to research ways to process entire longer emails. The planned improvements would enhance system accuracy and ensure better coverage of incoming emails."}]}