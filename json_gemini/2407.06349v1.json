{"title": "Large Language Model Recall Uncertainty is Modulated by the Fan Effect", "authors": ["Jesse Roberts", "Kyle Moore", "Thao Pham", "Oseremhen Ewaleifoh", "Doug Fisher"], "abstract": "This paper evaluates whether large language models (LLMs) exhibit cognitive fan effects, similar to those discovered by Anderson in humans, after being pre-trained on human textual data. We conduct two sets of in-context recall experiments designed to elicit fan effects. Consistent with human results, we find that LLM recall uncertainty, measured via token probability, is influenced by the fan effect. Our results show that removing uncertainty disrupts the observed effect. The experiments suggest the fan effect is consistent whether the fan value is induced in-context or in the pre-training data. Finally, these findings provide in-silico evidence that fan effects and typicality are expressions of the same phenomena.", "sections": [{"title": "1 Introduction", "content": "Some subfields of AI are explicitly interested in understanding and mimicking the nature of human cognition (cognitive modeling, computational psychology, affective computing) but even more implicitly rely on models of human cognition (human-computer interaction, embodied robotics, collaborative robotics, AI assistive technology, computational game theory). A model that, through training, learned to implicitly exhibit human-like cognitive behaviors could be of tremendous value both to the explicit study of human cognition as an ethical test subject, and as a more faithful model of human behavior to those fields that seek to develop systems to work along side human counterparts. We believe that some large language models (LLM) may be excellent candidates for such a role.\nLLMs process information in a manner that is fundamentally different from humans. The matrix multiplications, maximum inner product search, and perceptron networks may have, at some level, been inspired by the biological neuronal system."}, {"title": "2 Background", "content": "The fan effect is a psychological effect in human categorization behavior, first identified in Anderson (1974), where subjects take longer to recognize and accept or reject concepts that have overlapping features with concepts previously presented in a learning set. This has most commonly been studied using concepts made up of person-place pairs. More formally, given some training concept set S = {< X1,Y1 >, ..., < Xn, Yn >}, where X and Y are features of the concepts, response time when performing recognition tasks for an arbitrarily chosen query concept < Xq, Yq > is correlated with the number of times that Xq and Yq occur in S. The effect is apparent regardless of whether or not < Xq, Yq >\u2208 S.\nFan effects have subsequently been found to present with varying strength across different contexts. This tendency is dubbed the differential fan effect. Differential fan effects have been investigated across object type and concept presentation modality. It was first identified by Radvansky and Zacks (1991), in which the fan effect was found to occur in instances where presented concepts have the same object associated with multiple places (that is to say, the object feature had a high fan value) but not when multiple persons were associated with a single place (i.e. the place feature had a high fan value). Radvansky et al. (1993) later extended this to different object types, specifically small locations and inanimate objects. Stopher and Kirsner (1981) found that fan effects do not seem to present when concepts are presented via images rather than text, suggesting that differential fan effect context is affected by modality in addition to content.\nThere remains some debate on the mechanism of the fan effect in human subjects, particularly in regard to explaining differential fan effects. Radvansky et al. (1993) proposed a mechanism, based on the concept of mental models, by which subjects create and maintain models of the world based on learned facts and that some types of overlap in presented concepts necessitate the creation of more models than less overlapping concept sets of the same size. Anderson and Reder (1999) proposes a different mechanism, derived from a cognitive architecture in which fan effects are mediated by changing weights of edges in the concept network. This mechanism was further supported experimentally in Sohn et al. (2004) but challenged for larger datasets in Radvansky (1999).\nFan effects are found by Silber and Fisher (1989) in probabilistic categories created by COBWEB to be a special case of another phenomenon known as the typicality effect. Typicality, first formalized and identified in humans by Rosch (1975), refers to a tendency of humans to perform recall and categorization tasks quicker when prompted with a more typical member of a category than with a less typical member of a category, with level of typicality determined by how common the features of an instance of a category are among all members of the same category and among contrasting categories. That is, both an item's intra-category similarity and its inter-category similarity affect typicality assessments. For example, given pictures of two birds, a robin and a penguin, human subject response time will be higher when answering whether the penguin is a bird than whether the robin is a bird."}, {"title": "2.1 Prior Work", "content": "In Table 1, the results of a comprehensive survey of current work in LLM cognitive behavior studies is provided. No works could be found that study language model fan effects. Though Tung (2024) studied memory interference behavior in LLMs and use fan values in their analysis, they do not explicitly consider the fan effect or its presence.\nOn the other hand, work has been done that establishes the presence of typicality effects in LLMs. Bhatia and Richie (2022) investigated a variety of cognitive effects in the BERT masked language model. They found that BERT shows positive evidence of typicality effects, including limited evidence of consistency with typicality violations common to humans. Misra et al. (2021) recreates the experiments of Rosch (1975), identifying their presence across numerous categories and models. Roberts et al. (2024b) replicates Misra et al. (2021) with the addition of artificial populations to verify that the results are robust to random variations in the model, suggesting that the behaviors are strongly supported. They also find that the population standard deviations correlate with typicality, though the correlation is reversed in causal language models. Typicality effects have also been shown to be present in computer vision models by (Upadhyay et al., 2022)."}, {"title": "3 In-Pretraining (Typicality) Fan Effect", "content": "Anderson originally observed the fan effect in the response times of humans when correctly responding to questions. However, in Silber and Fisher (1989), the authors observed human-like fan effects in a COBWEB model and found they were consistent with a special case of typicality. Based on this observation and extant work regarding the presence of typicality effects in LLMs, we hypothesize that LLMs may exhibit a fan effect induced by the relative typicality of categorical items. Specifically we formulate RQ3.1.\nResearch Question 3.1. Given a partial list of items drawn from a category and presented to an LLM, are absence/presence prediction probabilities modulated by item typicality such that probabilities conditioned on typical items tend to be lower than those conditioned on less typical items?\nExpanding on this, based on results from (Roberts et al., 2024b), more typical items tend to have increased predicted word probability even when counterfactual prompting is used, most likely due to base rate probability effects (Moore et al., 2024). However, if a fan effect is present, the probability should tend to decrease with increasing typicality.\nIt is important to note that LLM probabilities are not necessarily analogous to human response times. However, existing work (Misra et al., 2021; Roberts et al., 2024b) has shown that typicality judgments, which have been measured via response time in humans (Rosch, 1975), are correlated with LLM probabilities."}, {"title": "3.1 Methodology", "content": "All experimental trials are conducted among a systematically perturbed population formed from each base model using PopulationLM (Roberts et al., 2024b) to decrease the likelihood that obtained results are anomalous. The median value is the preferred aggregation when random sampling for the purpose of estimating a true value (Doerr and Sutton, 2019). Therefore, the median across each base model population is taken as the group prediction.\nBased on work by Rosch (1975) regarding human typicality judgments across items in ten categories, we construct lists for each of the ten categories in Figure 1 by randomly selecting half of the items in a category. Selected items are included precisely once in a comma separated list with instructional content and two in-context examples."}, {"title": "3.2 Results", "content": "As noted, the fan effect was only observed by Anderson in humans when responding correctly to questions. Thus, only the true absence group (TAG) and true presence group (TPG) should be considered candidate scenarios that may exhibit a human-like fan effect.\nIn the upper left plot in Figure 1, there is an obviously distinct group which resides above the threshold (0.35), which we refer to as the probability noise floor. We interpret the group above the noise floor to be the TAG, that is the subset of absent items which the model regards as absent. The TPG, the subset of present items which the model regards as present, can be analogously seen in the bottom left with a noise floor at (0.5). Among predictions in the TAG, the probabilities have an obvious negative correlation with typicality, showing that more typical items tend to induce lower \"absent\" probabilities. We find that SOLAR (Kim et al., 2023) shows a similar fan effect, with TAG and TPG noise floor at (0.2).\nInterestingly, in the lower left of Figure 1 the TPG for Mistral has positive correlations which are inconsistent with the fan effect. This is reflected in the bottom of Figure 2 as well. SOLAR, on the other hand, tends toward inter-category randomness in the bottom of Figure 2."}, {"title": "3.3 Discussion", "content": "In response to RQ3.1, we find in Figure 1 that items absent from the list elicit a human-consistent fan effect evident in the canary probabilities in Mistral (Jiang et al., 2023) and SOLAR (Kim et al., 2023). The probabilities show a significant (r>0.3) (Hinkle et al., 2003) correlation with intra-category typicality in Figure 2 consistent with the fan effects discovered in COBWEB and theorized in humans. This result shows that LLMs exhibit fan effects based on the effects of typicality present in the pretraining data.\nROBERTa (Liu et al., 2019), GPT-2 (Radford et al., 2019), Llama-2 (Touvron et al., 2023), and Llama-3 (Meta, 2024) were equivalently evaluated but showed no significant correlation, though Llama-3 does show a similar, slight effect.\nWe were surprised to find the fan effect exhibited in the TAG but not the TPG. However, in retrospect this could have been anticipated based on nuanced consideration of the experiment.\nThe fan effect is canonically explained as a modulation of human uncertainty based on the categorical distance from an exemplar. When evaluating the TPG, the model is able to judge with near certainty by retrieving the queried item. On the other hand when judging the absence of a TAG item, the model can only know that the item has not been retrieved. The model assigns the probability of absence although it may actually be that the item is present but overlooked, inducing uncertainty. We hypothesize this uncertainty is precisely what the fan effect is modulating. So, when queried about an absent atypical item, the model responds confidently as if implying, \u201cI definitely didn't see that\u201d.\nThe above scenario in which the fan effect is only observed in the absent case seems plausibly consistent with human cognitive behavior. Imagine a context in which a human has a deck of cards and is asked if a card is present. If the card is found, then the person will have no uncertainty about their response. On the other hand, if the card is not found, the certainty of the response would be expected to be modulated by the fan effect. That is, if an unusual or outlier card is being searched for then it is likely that the person would notice if it had been present. However, it is reasonable that a human could more easily overlook a common card.\nWe hypothesize that the uncertainty mitigation due to access to the queried items in the TPG leads to the disruption of the fan effect in Mistral and SOLAR. Our results leave unclear the nature of the fan effect under mitigated uncertainty in the TPG."}, {"title": "3.4 Next Steps", "content": "Future work should consider creating long context lists that prevent models from retrieving TPG items with high fidelity to attempt to induce uncertainty and fan effects in the TPG. This was not possible currently since no extant lists of intra-category typical items in humans are sufficiently long. However, it may be possible to use LLMs to augment the typicality datasets to create a sufficiently large list.\nResults from Mistral suggest that fan effects without uncertainty tend toward a typicality effect response with increasing probability as typicality increases. However, results from SOLAR suggest that they tend toward noise. Future work should additionally attempt to disambiguate the nature of the fan effect when uncertainty is mitigated.\nFinally, future work should investigate human behavior in an experiment similar to the described card experiment with the objective of understanding human fan effect behavior under mitigated uncertainty."}, {"title": "4 In-Context Fan Effect", "content": "We investigate the presence of fan effects as originally defined in Anderson (1974) in the context of concepts composed of categorical features. This addresses the question of whether fan effects show up in concepts defined exclusively in-context. We formulate this as RQ4.1. We augment our analysis to investigate the presence of differential fan effect as described in Radvansky and Zacks (1991), providing RQ4.2.\nResearch Question 4.1. Given a list of simple concepts defined by their composite features that is presented to an LLM, are absence/presence prediction probabilities modulated by feature fan values such that probabilities conditioned on high fan features tend to be lower than probabilities conditioned on low fan features?\nResearch Question 4.2. Given a list of simple concepts defined by their composite features that is presented to an LLM, is correlation of absence/presence prediction probability with fan value modulated by the fan values of one feature more strongly than another feature?"}, {"title": "4.1 Methodology", "content": "We closely recreate the experimental methodology of Anderson (1974), with methods similar to those described in section 3.1 for in-pretraining fan effects. The experiment uses a generated model population of size N = 50 with median aggregation across population to determine group prediction. As before, probabilities are obtained using the canary words \u201cpresent\u201d and \u201cabsent\".\nConcepts are defined as natural language facts that pair persons, in the form of occupation labels, with places. Each fact is presented as a sentence of the form \"The  is in the \". Features are sampled from predefined person and place lists, each of size 20. The fan value is defined as the number of concepts that contain a given feature value. For example, if three distinct concepts indicate a person is present in the place \"School\", the fan value of \"School\" is 3.\nThe concepts in the recreation of Anderson are generated exactly as in Anderson (1974). A predefined set of feature combinations are used, as summarized in Table 2, which are designated by lowercase letters for persons and uppercase letters for places. The person and place assigned to each letter is randomly selected without replacement at the beginning of each trial. The result is N=26 concepts presented to the model in each trial, with a total of 16 fan value combinations (including fan = 0 for features not present in the set).\nPrompts presented to the model follow prompt design similar to that in section 3.1. The prompt is composed of four sections: An instructional preamble, the concept list, a two-shot ICL example, and the test query. The ICL examples include a concept that is appended to the end of the concept list that is guaranteed to not be generated. This guaranteed concept is followed by two example queries and simulated outputs, one where the concept is the guaranteed present concept and one with a guaranteed absent concept.\nAn example prompt in which the concept  is queried would be the following. Note that  is included in all trials and has a guaranteed fan value of 1 for both features, while  is absent in all trials.\nFollowing is a list that contains a number of people and the places in which they are located. After the list, a person will be judged as either present or absent in a specified place. When asked about person A in place B, if the list says that person A is in place B, answer with present. If the list does not say that person A is in place B, answer with absent. The list of people and places is: The Nurse is in the Studio. The Police Officer is in the Bank. The Mechanic is in the Mall. According to the list, in the Mall, the Mechanic is present. According to the list, in the Airport, the Pilot is absent. According to the list, in the Park, the Doctor is\""}, {"title": "4.2 Results", "content": "The results for both models are shown in Figure 3. As was the case in the in-pretraining experiments, a probability noise floor was noted in the data for both canary completions (Mistral-absent: 0.3; Mistral-present: 0.4; SOLAR-absent: 0.45; SOLAR-present: 0.4), providing a TAG and TPG. The figures are truncated to show only the TPG and TAG datapoints. Correlation statistics of the results are shown in Figure 4, with solid columns indicating correlations with a p < 0.01.\nIn Mistral, we once again see an obvious negative correlation between canary probability and fan value in the TAG predictions. This is consistent with a fan effect when evaluating absence of a concept (RQ 4.1). In the TAG, we see a stronger correlation with the fan value of the person feature than with the fan value of the place feature, supporting a positive result for RQ 4.2. This is consistent with results regarding differential fan effects in Radvansky and Zacks (1991), which found that the fan effect is mediated more by the fan of a particular item than the fan of a particular location.\nSOLAR shows a slightly different story. For the TAG predictions, we still see a significant negative correlation when correlating with the fan of person, but a positive correlation with fan of place. TPG predictions instead show a negative correlation against fan of place and no correlation against fan of person. While this seems inconsistent with our Mistral results, it is consistent with our prior interpretations when properly analyzed. Based on these results, SOLAR and Mistral both show evidence of the fan effect in, at minimum, the same situations as in humans, which is to say uncertain contexts and based on the fan of person.\nFrom the experiments regarding typicality, we expect that mitigated uncertainty in the TPG may lead to disruption of the fan effect. In confirmation, among TPG items all correlations fail to achieve a significant p value for fan value and canary probability Pearson correlation, again suggesting that mitigated uncertainty disrupts the fan effect."}, {"title": "4.3 Next Steps", "content": "There are numerous enhancements that could be applied to these experiments. While occupations were chosen as proxies for persons to be consistent with Anderson (1974), more unique identifiers like names may yield a stronger differential fan effect if the mental models mechanism proposed by (Radvansky and Zacks, 1991) is present in language models. This should be tested empirically in future work to investigate the nature of differential fan effects. Additionally, other feature types that are not related to persons and places should be investigated.\nHuman cognitive experiments often include a dimension of elapsed time between training and testing time when studying memory-sensitive behaviors. Future work should consider simulating this time separation in language models. Though language models do not possess a directly analogous temporal dimension, experiments could evaluate the injection of semantic noise of varying length as a potential proxy."}, {"title": "5 Conclusions", "content": "Our experiments are the first to evaluate LLMs for the presence of human-like fan effects. We have shown that Mistral and SOLAR have learned to exhibit fan effects from training on human language data. This paper is not the first to identify SOLAR and Mistral as important human-like LLMs. Roberts et al. (2024a) found SOLAR and Mistral to be significantly more human-like than a large body of other open-source models when evaluated in a game theoretic context. Given Mistral was built from Llama-2 and SOLAR was built from Mistral, the authors propose the more human-like behavior may be the result of an improved representation acquired through additional training of Mistral with sliding window attention.\nOur results show that fan effects are present both when the fan value is induced in-pretraining in the form of intra-category typicality and when the fan value is induced in-context in the form of repeated items within a list. The presence of typicality-based fan effects in language models lends further credence to the findings of Silber and Fisher (1989) suggesting that fan effects are a special case of typicality effects.\nAdditionally, we find that when uncertainty is mitigated, the fan effect is disrupted with divergent disruption patterns across LLMs. The divergent patterns across Mistral and SOLAR beg further investigation. However, we are unaware of any cognitive science literature that addresses fan effects in a disruptive scenario with mitigated uncertainty. Therefore, it is unclear how a human may behave in a similar context. We therefore call for human experiments.\nSimilarly, when the fan value is derived from place instead of person in the Anderson experiment, both Mistral and SOLAR exhibit a disruption of the fan effect in agreement with nuanced work regarding differential fan effects (Radvansky and Zacks, 1991). Again, each of these models diverges in the nature of the disruption but shows a consistent pattern of fan effects in the case of true absence when the fan value is calculated on the person feature.\nFinally, we hope this paper will prove synergistic with the wider cognitive science and computational linguistic communities. By adapting experiments to evaluate the presence of known human cognitive effects in LLMs, we may gain new insight into cognitive effects. These insights not only help to explain the factors which influence the behavior of complex language models but also provide new potential hypotheses regarding the cognitive behavior of humans."}]}