{"title": "KOI: Accelerating Online Imitation Learning via Hybrid Key-state Guidance", "authors": ["Jingxian Lu", "Wenke Xia", "Dong Wang", "Zhigang Wang", "Bin Zhao", "Di Hu", "Xuelong Li"], "abstract": "Online Imitation Learning methods struggle with the gap between extensive online exploration space and limited expert trajectories, which hinder efficient exploration due to inaccurate task-aware reward estimation. Inspired by the findings from cognitive neuroscience that task decomposition could facilitate cognitive processing for efficient learning, we hypothesize that an agent could estimate precise task-aware imitation rewards for efficient online exploration by decomposing the target task into the objectives of \"what to do\" and the mechanisms of \"how to do\". In this work, we introduce the hybrid Key-state guided Online Imitation (KOI) learning approach, which leverages the integration of semantic and motion key states as guidance for task-aware reward estimation. Initially, we utilize the visual-language models to segment the expert trajectory into semantic key states, indicating the objectives of \u201cwhat to do\u201d. Within the intervals between semantic key states, optical flow is employed to capture motion key states to understand the process of \"how to do\". By integrating a thorough grasp of both semantic and motion key states, we refine the trajectory-matching reward computation, encouraging task-aware exploration for efficient online imitation learning. Our experiment results prove that our method is more sample efficient in the Meta-World and LIBERO environments. We also conduct real-world robotic manipulation experiments to validate the efficacy of our method, demonstrating the practical applicability of our KOI method.", "sections": [{"title": "1 Introduction", "content": "Imitation Learning has achieved significant success in various robotic manipulation tasks through offline and online modes [1, 2, 3, 4]. The offline approach [5, 6, 7] seeks to learn manipulation policy by mimicking state-action pairs from expert demonstrations through supervised learning. Due to its independence from interaction with environments, offline imitation learning necessitates extensive robotic data for manipulation policy learning, which requires expensive costs associated with real-world data acquisition [8, 9, 10]. In contrast, online imitation learning [11, 12] formulates policies via online exploration, driven by a reward function informed by expert demonstrations. Although it has the potential to refine policies with minimal expert input [13], the considerable gap between the extensive online exploration space and limited expert trajectories challenges the estimation of exploration reward, which hinders the efficiency of online imitation learning in intricate environments.\nTo boost the efficiency of online imitation learning, recent works [14, 15] incorporate adversarial learning into online exploration, iteratively refining the policy through feedback from the discriminator. While these techniques offer significant advancements, they fall short in capturing fine-grained rewards from demonstrations. To estimate the fine-grained imitation rewards, Optimal Transport (OT) [16, 17, 18, 19] is proposed to facilitate the trajectory-matching rewards for manipulation tasks by measuring the distance between the online exploration trajectories and expert demonstration. However, their insufficient grasp of the semantic comprehension of manipulation tasks makes them easily converge on sub-optimal solutions [16].\nPivotal research in cognitive neuroscience [20, 21] suggests that humans could enhance cognitive processing and learning efficiency via task decomposition. Inspired by these findings, we hypothesize that an intelligent agent could also optimize its learning efficiency of online exploration by decomposing tasks. Specifically, by gaining a holistic grasp of both the objectives (\u201cwhat to do"}, {"title": "2 Related Work", "content": "Online Imitation Learning Online Imitation Learning seeks to develop policies through online exploration, guided by a reward function informed by expert demonstrations [22, 23]. Before the advent of deep reinforcement learning, Ross et al. [24] pioneered the concept of interactive imitation learning, where the agent actively queries an expert during its learning process for self-improving. Further, recent works [14, 25] utilized deep neural network techniques combined with expert feed-back to address the challenges of reward estimation in complex environments. Recognizing the difficulty of obtaining state information in real-world settings, some researchers [13, 26] explored the use of visual and language cues to inform task-aware reward estimation for online exploration. However, these works mainly focused on task semantic knowledge, overlooking detailed trajectory information. In this study, we adopt the trajectory-matching reward estimation method, integrating task semantic comprehension with fine-grained trajectory motion for efficient online exploration.\nOptimal Transport for Imitation Learning Optimal Transport(OT) is a computational framework that addresses the problem of finding the most efficient way to transform one distribution into an-other, minimizing a predefined cost for this transformation [27, 28, 29]. In the context of imitation learning, OT is usually utilized to measure the distance between the online exploration trajectories and expert demonstration for fine-grained imitation reward estimation [18, 19, 30]. Prior works borrow various distance metrics [27, 31] for efficient online imitation learning, while ROT [16] combined the trajectory-matching online imitation with adaptive behavior regularization to stabilize the online learning process. Further, haldar et al. [17] proposed the Fast Imitation of Skills from Humans (FISH) to generalize online exploration across unseen object configurations. Although these methods achieve fine-grained reward estimation with trajectory-matching approaches, they regrettably ignore the semantic information of manipulation tasks, leading to convergence on suboptimal"}, {"title": "3 Method", "content": "During online imitation learning, a significant challenge lies in the gap between extensive online exploration space and limited expert trajectories, resulting in inefficient exploration due to imprecise estimations of the imitation reward. In this work, to precisely estimate task-aware imitation reward, we extract the hybrid key states from semantic and motion aspects by the Semantic Decomposition Module (SDM) and Motion Capture Module (MCM), indicating \"what to do\" and \"how to do\" for the target task. By integrating both types of key states, we adjust the importance weight of states in expert trajectories in the OT-based reward estimation method for efficient online imitation learning."}, {"title": "3.1 Background", "content": "The goal of online imitation learning is to train a policy $\\pi_{\\theta}$ through online exploration with the reward $r_{\\theta}$ informed by the expert demonstrations $T^{d}$. The OT-based methods [16, 17] aim to estimate reward $r_{\\theta}$ by measuring the distance between importance distribution $\\mu$ of exploration trajectory $T^{e}$ and importance distribution $\\nu$ of expert demonstration $T^{d}$ by the following Sinkhorn algorithm [27]:\n$\\min_\\eta \\sum_{i=1}^{N_e} \\sum_{j=1}^{N_d} \\eta_{ij} c_{ij} - \\frac{1}{\\eta}H(\\eta)$\ns.t. $\\sum_{j=1}^{N_d} \\eta_{ij} = \\mu_i$ for $i = 1, ..., N_e$,\n$\\sum_{i=1}^{N_e} \\eta_{ij} = \\nu_j$ for $j = 1, ..., N_d$.\nwhere $c_{ij}$ is the cost between i-th state from exploration trajectory $T^{e}$ and j-th state from expert demonstration $T^{d}$, and $\\eta \\in \\mathbb{R}^{N_e \\times N_d}$ is a transport matrix where each element $\\eta_{ij}$ represents the amount of data transported from source i to destination j. $N_e$ and $N_d$ represent the length of $T^{e}$ and $T^{d}$, respectively. $H$ is an entropy regularization to enhance computational efficiency. Further, the"}, {"title": "3.2 Semantic Decomposition Module", "content": "Foundation Models have demonstrated their rich world knowledge and strong generalization capabilities in various visual-language tasks [32, 33]. In this work, we propose utilizing GPT-4v to design the Semantic Decomposition Module (SDM) for task-aware subgoal extraction.\nConcretely, we initially instruct GPT-4 to decompose the task description $L_t$ into $K$ subgoals, represented as $L^s = {l_1, l_2, .., l_K}$. Subsequently, we employ GPT-4v to identify the key states corresponding to each subgoal description. To manage the number of input images, we select state every $T$ step to form the query expert observation sets $O$. To ensure the temporal consistency of segmented states, both the sampled expert observation $O$ and subgoal language description $L^s$ are fed into GPT-4v to get the semantic key state index set $I^s$:\n$I^s = GPT(O, L^s)$.\nConsequently, this method produces a temporally coherent sequence of indices $I^s$ that precisely select the semantic key states corresponding to each subgoal description. Due to the length constraints of the paper, please refer to Supplementary Material A for more detailed algorithm implementation."}, {"title": "3.3 Motion Capture Module", "content": "While the SDM effectively identifies semantic key states, these states primarily outline the task objectives, without delving into the manipulation details required for transition to the target state. To enhance the understanding of manipulation details, we propose the Motion Capture Module (MCM) to locate the motion key states within the interval defined by two semantic key states.\nSpecifically, we utilize the Farneback [34] optical flow estimation algorithm $F$ to identify the optical flow information between consecutive states. To extract the important motion key states index set $I^m = {I_1^m, ..., I_i^m, ..., I_m^m}$, we compute the optical flow between successive expert observation $o_j$ and $o_{j+1}$, selecting the most intense state between two semantic key states, which represents a critical state transition during the manipulation process:\n$I^m_i = \\arg \\max_{j \\in (I_i:I_{i+1})} |F(o_j, o_{j+1})|$.\nBy extracting the most intense optical flow, we select the motion key states to indicate \"how to do\" the target task."}, {"title": "3.4 Importance Weight Adaption", "content": "Based on the extracted hybrid semantic key states set $I^s$ and motion key states set $I^m$, we develop the Importance Weight Adaption method to guide the agent in online exploration."}, {"title": "3.5 Learning Paradigm", "content": "To fully utilize the expert demonstrations, we employ an offline-to-online methodology, initially training a behavior cloning policy $\\pi_b$ using the expert data:\n$\\mathcal{L}_{BC} = E_{(o_d, a_d) \\sim T_d} ||a_d - \\pi_b(s_d)||^2$.\nFurther, we utilize the n-step Deep Deterministic Policy Gradient (DDPG) [35] with our estimated exploration reward $r_\\theta$ to update our policy $\\pi_e$, which is initialized by $\\pi_b$, and Q-function $Q_e$:\n$\\pi_e = \\arg \\max_{\\pi} [(1 - \\lambda(\\pi))E_{(o_e, a_e) \\sim T_e} [Q_e(o_e, a_e)] - \\alpha \\lambda(\\pi)E_{(o_d, a_d) \\sim T_d} ||a_d - \\pi(s_d)||]$.\nIn this step, we employ the adaptive function $\\lambda$ following ROT [16] to effectively balance the weight between BC regularization and online exploration:\n$\\lambda(\\pi_e) = E_{(s,t) \\sim T_e} [\\frac{log(\\pi_b(s))}{Q_e(s, \\pi_e(s))}]$."}, {"title": "4 Experiments", "content": "To comprehensively evaluate the performance of our method, we initially conducted experiments across 6 tasks from the Meta-world suite [36] to validate the efficacy of our approach under varied conditions. To further assess our key-state guided method in more complex manipulation environ-ments, we conduct 3 tasks from the LIBERO suite [37]-notable for their complex scenarios, large action spaces, and long task sequences.\nTo promote policy learning, the policy in the LIBERO suite utilizes both image and proprioceptive as input, while the policy in the Meta-world suite only utilizes image as input.\nFor training the BC policy, we collected 1 demonstration per task in the Meta-world suite and 50 demonstrations per task in the LIBERO suite. Each task was trained using a batch size of 256 over 50k epochs. For the online imitation learning phase, we loaded the pretrained BC policy and initialized the critic network. The model then interacted with the environment over 500k timesteps in the Meta-world suite and 200k timesteps in the LIBERO suite, allowing us to observe the efficiency of different online imitation learning methods."}, {"title": "4.2 Comparison Experiments", "content": "To evaluate our methods, we conducted comparison experiments with various approaches:\n\u2022 BC represents the pretrained behavior cloning policy $\\pi_b$.\n\u2022 ROT [16] employs Optimal Transport to estimate the distance between exploration trajectory and expert demonstrations, which focus on fine-grained trajectory matching.\n\u2022 RoboCLIP [13] leverages the pretrained video encoder S3D [38] to estimate the task-aware reward function instead of the reward of each state.\n\u2022 UVD [39] utilize pretrained visual encoder R3M [40] for task decomposition in image-goal reinforcement learning.\nAll methods are initialized with pretrained behavior cloning policy $\\pi_b$ to ensure a fair comparison."}, {"title": "4.3 Ablation Experiments", "content": "Our method proposes the Semantic Decomposition Module (SDM) and Motion Capture Module (MCM) to provide task-aware knowledge for trajectory-matching reward estimation. To evaluate the effectiveness of each module, we conducted ablation experiment with the following two settings:\n\u2022 w/o. SDM: This setting removes the semantic subgoals inferred by visual-language models and selects motion key states within every 10 states interval."}, {"title": "4.4 Qualitative Analysis", "content": "We demonstrate the semantic and motion key states selected by our method. As shown in Figure 4(a), our semantic key states effectively capture the subgoals of the task, such as \u201cpick up the bin\" and \"put the bin into the other box\". Our motion key states, capture the intensive optical flow information between the semantic subgoals, allowing agents to grasp detailed motion dynamics.\nFurthermore, we present three representative cases in the \u201cbin picking\" task. As shown in Figure 4(b), when the agent fails to complete any subgoals, our method estimates a lower reward to discourage unsuccessful exploration. In cases where the agent completes only the first subgoal of Figure 4(c), the estimated reward is high during the initial stage but lower at the final stage. In contrast, ROT incorrectly estimates a high reward for the final stage, which focus only on trajectory matching rather than task-aware knowledge. The last case, shown in Figure 4(d), demonstrates that our method provides a high reward estimation when all subgoals are completed, thereby facilitating effective online exploration."}, {"title": "4.5 Real-world Results", "content": "As shown in Figure 5, we conducted experiments with an Xarm robot arm in the real world to validate the efficiency of our KOI framework in practical scenarios. For each task, we collect 5 expert demonstrations using the cartesian position controller, utilizing both image and proprioceptive data as inputs for the policy, to reduce the difficulty of exploration in real-world experiments. In the online imitation learning phase, we limited each exploration to fewer than 100 timesteps.\nThe results show that our method can consistently achieve success across various initial conditions within 10k exploration timesteps."}, {"title": "5 Conclusion and Limitation", "content": "In this work, we propose the hybrid Key-state guided Online Imitation (KOI) learning method, which extracts the semantic and motion key states for trajectory-matching reward estimation for online imitation learning. By decomposing the target task into the objectives of \u201cwhat to do\u201d and the mechanisms of \"how to do\", we refine the trajectory-matching reward estimation to encourage task-aware exploration for efficient online imitation learning. The results from simulation environments and real-world scenarios prove the efficiency of our method.\nLimitations. The online imitation learning in this study begins solely with a pretrained behavior cloning policy. This approach initializes reinforcement learning with an initial critic, as demonstrated in our results, which would negatively impact early exploration learning. In future work, offline reinforcement learning could be employed to pretrain both the actor and critic, potentially enhancing initial exploration online learning."}]}