{"title": "Interpreting the Learned Model in MuZero Planning", "authors": ["Hung Guei", "Yan-Ru Ju", "Wei-Yu Chen", "Ti-Rong Wu"], "abstract": "MuZero has achieved superhuman performance in various games by using a dynamics network to predict environment dynamics for planning, without relying on simulators. However, the latent states learned by the dynamics network make its planning process opaque. This paper aims to demystify MuZero's model by interpreting the learned latent states. We incorporate observation reconstruction and state consistency into MuZero training and conduct an in-depth analysis to evaluate latent states across two board games: 9x9 Go and Outer-Open Gomoku, and three Atari games: Breakout, Ms. Pacman, and Pong. Our findings reveal that while the dynamics network becomes less accurate over longer simulations, MuZero still performs effectively by using planning to correct errors. Our experiments also show that the dynamics network learns better latent states in board games than in Atari games. These insights contribute to a better understanding of MuZero and offer directions for future research to improve the playing performance, robustness, and interpretability of the MuZero algorithm.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning has shown significant success across various domains, with particularly notable achievements in games [16, 18, 20]. One of the most remarkable milestones is AlphaZero [19], an algorithm that leverages environment simulators for planning and masters board games like Chess and Go through self-play without using any human knowledge. Building on this advancement, MuZero [17] introduces a dynamics network that learns the rules of environment dynamics, enabling planning without requiring environment simulators. This dynamics network further extends MuZero to complex environments, including visual environments like Atari games [5, 17], physics-based simulations [12], stochastic environments [1], and even non-game applications [15, 22]."}, {"title": "2 MuZero and Its Improvements", "content": "MuZero [17], a model-based reinforcement learning algorithm, builds upon AlphaZero [19] by introducing a learned model for predicting environment dynamics. Its architecture includes three key networks: representation, dynamics, and prediction. The representation network h encodes an observation ot into a latent state, called hidden state st, at time step t, denoted by h(ot) = st. The dynamics network g provides state transitions, g(st, at) = rt+1, s(1)t+1, where r(1)t+1 is the predicted reward, and s(1)t+1 is the next hidden state. Here, the superscript (k)"}, {"title": "3 Interpreting the Hidden States", "content": "This section evaluates the quality of hidden states learned by MuZero. We first train five MuZero models, each on a different game, with a decoder network and state consistency: two board games, 9x9 Go and Outer-Open Gomoku, and three Atari games, Breakout, Ms. Pacman, and Pong. We then use the decoder network to interpret the hidden states."}, {"title": "3.1 Training MuZero Models with a Decoder", "content": "To train the MuZero model with a decoder network and state consistency, we use the MiniZero framework [24] for implementation. Specifically, we use Gumbel MuZero [5] with hyperparameters listed in Table 1 for training, following the MuZero network architecture [17] that uses a residual network [10] as the backbone and following the approach proposed by Ye et al. [25] for the decoder network architecture. The experiments are conducted on a machine with two Intel Xeon Silver 4216 CPUs and four NVIDIA RTX A5000 GPUs."}, {"title": "3.2 Decoding Hidden States into Observations", "content": "We first investigate whether the hidden states generated by the representation network can be accurately decoded into observations. Specifically, we compare the decoded observations \u00f4t with the true observations ot to evaluate performance in both in-distribution and out-of-distribution states for each game, as shown in Fig. 2. In-distribution states are from the final self-play iterations generated by the evaluated MuZero model, while out-of-distribution states are from the initial iterations. For in-distribution states, the stones in board games are accurate, and the object outlines in Atari games generally match, with minor errors like a missing ball in Breakout or blurry ghosts in Ms. Pacman. On the other hand, the decoded observations perform worse in out-of-distribution states. For example, several blurry stones are incorrectly shown in board games, and many common objects are blurry or even missing in Atari games, such as the bricks in Breakout, ghosts in Ms. Pacman, and the ball in Pong. We conclude that the decoder network performs better in board games, possibly due to the challenge of learning complex Atari images, and also performs better in familiar in-distribution states."}, {"title": "3.3 Analyzing Unrolled Hidden States", "content": "Next, we evaluate the quality of the unrolled hidden states generated by the dynamics network, examining whether \u00f4t+k(k) still aligns with ot+k after unrolling k steps. Fig. 3 presents the results for Go and Breakout under different k, including 0, 1, 5, 10, 15, and 20. Note that when k = 0, the hidden state is generated by the representation network, as no unrolling occurs. For Go, \u00f4t+k(k) remain robust with a small number of unrolling steps but become blurry as k increases like \u00f4t+20(20). Conversely, \u00f4t+k(k) in Breakout are less accurate, in which the larger k is, the more mistakes are, especially in the scores, the paddle, and the blocks. To summarize, the hidden states generally become less accurate during unrolling through the dynamics network, similar to the findings in previous works [9, 21].\nAlso, following a similar approach to Vries et al. [21], we perform principal component analysis (PCA) on the same games and visualize the PCA projections for ot, \u00f4t, \u00f4(5)t, and oft), as shown in Fig. 4. In Go, the trajectories of all decoded observations align well with ot, indicating that the dynamics network effectively learns the game rules and can simulate future observations regardless of the unrolling steps. In contrast, for Breakout, only \u00f4(5)t remains aligned. This may be because MuZero only unrolls five steps during training; when unrolling"}, {"title": "4 Exploring the Impact of Hidden States During Planning", "content": "The previous section demonstrates that hidden states are not always accurate. However, MuZero still achieves superhuman performance despite these inaccuracies. This section aims to interpret the search behavior in MuZero and explore the impact of hidden states during planning."}, {"title": "4.1 Visualizing the Search Tree", "content": "We visualize an MCTS tree by decoding hidden states of internal nodes in Gomoku, a game with easily understandable rules, as shown in Fig. 6. The"}, {"title": "4.2 Analyzing Unrolled Values in Search Tree", "content": "Fig. 6 shows some interesting results for the node values. First, at node F, White achieves five-in-a-row after Black has already won; nevertheless, this move does not affect the value v. Second, the value maintains confidence about the result as the states go deeper beyond the terminal state, such as in nodes F, G, H, and I. To investigate this phenomenon, we further analyze the hidden states searched beyond the terminal in the search tree. We notice that the proportion of them, shown in Fig. 7, grows not only when approaching the environment terminal but also when the search tree size is increased, reflecting that the search tree has no choice but to expand them when there are fewer valid moves. Since a small proportion of such hidden states is inevitable, the accuracy of their values is non-negligible during planning. Next, we calculate the average errors between v and z, where v is the values of hidden states unrolled beyond terminal states for up to 50 steps, and z is the game outcomes. As shown in Fig. 8, the values in"}, {"title": "4.3 Evaluating Playing Performance with Different Simulations", "content": "The previous subsection shows that using mean value during planning mitigates the value error of unrolled hidden states. Following this finding, we are interested in whether playing performance can still be maintained as the search tree grows deeper. We conduct experiments to evaluate the playing performance of five games using different numbers of simulations in MCTS, as shown in Fig. 10. We anchor the performance of 400 simulations to 100% for each game. For Go and Gomoku, higher numbers of simulations lead to better performance, indicating that the dynamics network learns well and the value errors have been effectively mitigated even when planning with a large tree. For Pong, since the game is easy to master and the agent can consistently achieve an optimal score, performance remains the same regardless of the number of simulations. In contrast, for Breakout and Ms. Pacman, the performance decreases after reaching thousands of simulations, indicating that accumulated unrolling value errors eventually harm the search and lead to worse performance. This experiment demonstrates that performance is generally maintained with a small number of simulations across all games, but an excessively large search tree can decrease performance, especially when the dynamics network is not sufficiently robust."}, {"title": "5 Discussion", "content": "This paper presents an in-depth analysis to demystify the learned latent states in MuZero planning across two board games and three Atari games. Our empirical experiments demonstrate even if the dynamics network becomes inaccurate over longer unrolling, MuZero still performs effectively by correcting errors during planning. Our findings also offer several future research directions. For example, using observation reconstruction, researchers can further investigate the types of hidden states MuZero is unfamiliar with and design adversarial attack methods [14, 23] to identify weaknesses. Developing methods to improve state alignment can also be a promising direction, such as leveraging other world model architectures [6, 7]. In conclusion, we hope our contributions can assist future research in further improving the interpretability and performance of MuZero."}]}