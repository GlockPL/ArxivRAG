{"title": "A LITTLE LESS CONVERSATION, A LITTLE MORE ACTION, PLEASE: INVESTIGATING THE PHYSICAL COMMON-SENSE OF LLMS IN A 3D EMBODIED ENVIRONMENT", "authors": ["Matteo G. Mecattaf", "Ben Slater", "Marko Te\u0161i\u0107", "Jonathan Prunty", "Konstantinos Voudouris", "Lucy G. Cheke"], "abstract": "As general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) can do your physics homework, but might not be able to successfully find their way to the classroom. While LLMs have made great strides in several areas, including writing code (Champa et al., 2024), solving maths problems (Frieder et al., 2024; Yuan et al., 2023b), and answering general knowledge questions (Wang et al., 2024a), it remains unclear to what extent they can be considered to know about and understand the physical world.\nPhysical common-sense reasoning is the capacity to perceive, understand, and predict the behaviour of objects in an environment. This includes an understanding of the physical rules governing space"}, {"title": "RELATED WORK", "content": "In machine learning and natural language processing, there has been increasing interest in whether Large Language Models possess the capacity to perceive, understand, and predict the behaviour of objects in their environment, which has come to be known in the literature as physical common-"}, {"title": "THE ANIMAL-AI ENVIRONMENT", "content": "The Animal-AI (AAI) environment (Beyret et al., 2019; Crosby et al., 2019; Voudouris et al., 2023) is a 3D simulation based on the Unity ML-Agents framework (Juliani, 2018), designed to be used by researchers from AI and cognitive science to assess nonverbal physical common sense reasoning in embodied agents. The goal of the environment is to offer a tool for interdisciplinary research at the intersection of AI and cognitive science, with a particular focus on comparative and developmental psychology. All experiments in AAI consist of a 40\u00d740 arena, populated with a single agent (spherical with diameter 1) and a variety of different objects."}, {"title": "THE ANIMAL-AI TESTBED AND OLYMPICS", "content": "AAI was first released in 2019 as part of the Animal-AI Olympics Competition, in which over 60 entrants competed to produce agents that could solve a series of unseen tasks inspired by comparative psychology research (Crosby et al., 2020), thus favouring the development of agents that could perform robustly out-of-distribution on tests of physical common sense reasoning. After the competition was completed, these tasks were released as the Animal-AI Testbed to further stimulate interdisciplinary research between AI and comparative psychology. The Animal-AI Testbed contains 300 distinct tests (with 3 variants of each; n=900 tasks) that test the full breadth of capabilities that underpin physical common-sense reasoning, including navigating around obstacles, making spatial inferences, tracking occluded objects, and causal reasoning. The aim in every task is to maximise total reward at the end of the episode. The environment contains spheres of different colours and sizes: yellow spheres increase reward, as do green spheres, which also end the episode; red spheres decrease reward and end the episode. In all cases, the magnitude of the reward change is proportional to the size of the sphere. Touching red 'death zones' leads to a decrease in reward of -1 and also ends the episode. Reward decreases at a constant rate starting from 0 on each timestep, thus favouring efficient action sequences. Entering orange 'hot zones' leads to a doubling in reward decrement. A variety of movable and immovable blocks are present in the environment, including tunnels and opaque and transparent walls. Ramps are always purple, platforms are always blue, and pushable blocks are always light grey. Other blocks may take any colour.\nThe Animal-AI Testbed is arranged into 10 levels of 90 tasks of roughly increasing difficulty (Voudouris et al., 2022a) which probe different aspects of physical common-sense reasoning. For example, level 1 (Food Retrieval) tests the ability of the agent to navigate towards rewarding green and yellow spheres, level 2 (Preferences) tests the ability to distinguish objects that give different rewards, and level 3 (Static Obstacles) tests the ability to navigate around and over immovable solid objects, such as walls, ramps, and tunnels. The most complex levels test sophisticated physical common-sense reasoning abilities: level 8 (Object Permanence and Working Memory) tests whether agents understand that objects continue to exist when they are occluded, while level 10 (Causal Reasoning) tests the ability to understand cause and effect through the use of tools that can be used"}, {"title": "METHODS", "content": ""}, {"title": "LLM-AAI", "content": "LLM-AAI framework allows us to connect LLMs with AAI environment. It is LLM-agnostic, requiring only a multimodal agent that can receive text-and-image inputs and return text outputs.  At each timestep, t, the environment returns a colour image of its current state, along with the agent's current reward and health. These observations are combined into a prompt and presented to the LLMs as a request."}, {"title": "LARGE LANGUAGE MODELS TESTED", "content": "We consider three state-of-the-art multi-modal Large Language Models. Our selection was based on a convenience sample, guided by the inclusion criterion that models must be multi-modal with a large context window (>64k), and the exclusion criterion that models must not be too costly to run inference on. We evaluated Claude 3.5 Sonnet, GPT-40, and Gemini 1.5 Pro. We ran all experiments with temperature 0, but noticed that model responses can vary nevertheless. Therefore, we ran three trials of each model on each task."}, {"title": "EXPERIMENTS", "content": "In this study\u00b9, we use a subset of the Animal-AI Testbed containing four randomly selected tasks from the ten levels (n=40), replicating the design of Voudouris et al. (2022a), in which 59 children aged 6-10 completed the same subset of 40 tasks. This allows direct comparison of LLM agents with human children, and non-human entrants to the Animal-AI Olympics Competition (Crosby et al., 2020).\nWe conduct two experiments to explore LLM performance in this setting. Our first experiment includes a prompt that simply explains the environment and possible actions to the LLM, and assesses three models on 40 AAI Testbed tasks. Our second experiment provides the LLM with a prompt containing an in-context example of the successful completion of a simple 'tutorial' level. We then evaluate LLMs given this prompt on a subset of the 40 tasks used in Experiment 1.\nWhen we encountered errors from API calls that persisted after three retries, we discarded the current trial data and relaunched that trial run."}, {"title": "EXPERIMENT 1", "content": "First, we designed a simple prompt that provides the core information needed to navigate and collect rewards in the AAI Testbed.To improve the LLM's decision-making process, we incorporated the ReAct (Reasoning and Acting) framework (Yao et al., 2022) into our prompt design. The ReAct approach combines reasoning and acting by allowing the model to generate reasoning traces alongside actions, which has shown improved performance on agentic tasks (Yao et al., 2022). By integrating ReAct, we encourage the LLM to first reason about the environment\u2014identifying visible objects and their spatial relationships relative to the agent-before producing action scripts.\nFor this study, we use AAI version 3.1.3"}, {"title": "EXPERIMENT 2: SUPERVISED IN-CONTEXT LEARNING", "content": "When children played the tasks in the AAI Testbed, they received a short two-minute video to describe \"the game\"-that is, to introduce the AAI environment, its objects and controls. To emulate this, we designed an example level in AAI that introduced the same information as was presented in the video, and a sequence of scripts that could be used to solve the level, using the \u2018Think' action to explain observations. The script and observations were incorporated into the prompt designed above. In this way, the LLMs are provided with images of objects they may encounter in a level, as opposed to just textual descriptions, and an \u2018expert example' , before they are tasked with controlling the agent. We call this supervised in-context learning.\nDue to the increased cost of passing several images and a large amount of text for every episode, we conducted this experiment on a subset of the tasks. After carrying out Experiment 1 and observing close to zero performance in the later levels, we decided to focus on the first three levels of the AAI Testbed. These levels were designed as the simplest tasks and showed an expected decline in LLM performance from Level 1 to Level 3. Focusing on these initial levels provided a better opportunity to observe differences in performance, whereas the later levels, due to their difficulty, may have resulted in floor effects."}, {"title": "RESULTS", "content": ""}, {"title": "EXPERIMENT 1", "content": "Our results, summarised in Figure 3, show that LLMs are able to complete some challenges in Levels 1 and 2, with sporadic performance in across Levels 5, 6 and 8. They are comparable in performance with competition agents in Levels 3, 8, 9 and 10, however these all occur at a very low success rate, so there may be a floor effect obscuring a difference in capability between the groups. The children perform convincingly better than the LLM agents across all levels, with child error bars only overlapping with LLM performance in Levels 4, 5, 9 and 10, where LLM performance is very low.\nThese results show that LLMs are able to perform successfully in the most simple tasks of the testbed, but that their performance drops of quickly in more challenging tasks. The LLMs' performance never exceeds that of the top 10 agents submitted to the Animal-AI competition. It could be argued that this comparison will always favour the RL agents, who had been specifically trained for the environment, if not for the specific tasks. However, the same cannot be said for the human children, whose performance also exceeded that of the LLMs across the board. These results indicate that LLMs may still lack physical common-sense reasoning abilities possessed by human children."}, {"title": "EXPERIMENT 2", "content": "The results for our supervised in-context learning tasks are shown in Figure 4. The performance of every tested LLM is illustrated by a pair of bars. The first bar illustrates performance without our 'expert example', and is identical to the Experiment 1 results from Figure 3, while the second bar represents performance with our example and is new in Experiment 2.\nOverall, we did not observe a notable difference in performance when providing the LLMs with the 'expert example. While the LLMs still broadly perform successfully on these early levels, they do not outperform the competition agents or the children.\nThe observed performance difference, when including the 'expert example', was not the same across all the tested LLMs. Claude performed slightly worse in Level 1 than it had without in-context learning, whereas the opposite occurred in Level 2. Performance on Level 3 stayed the same. For Gemini, the addition of in-context learning had either no effect, in Level 1, or decreased the proportion of trials passed, in Levels 2 and 3. While GPT also experienced no performance difference in Level 1, its results rose both in Levels 2 and 3, with its Level 3 proportion of trials passed matching the upper interquartile range of the competition agents and the lower range of the children."}, {"title": "DISCUSSION", "content": "The LLM-AAI framework tests the out of the box physical reasoning capabilities of LLMs by using the ReAct prompting method (Yao et al., 2022), allowing LLMs to percieve and interact with the Animal-AI environment. While previous work has explored the capabilities of LLMs to interact with virtual environments, none have used this to explicitly develop a framework for testing physical common-sense reasoning in LLMs. Our results show that LLMs can not only be assessed in this way, but that when this is done it allows meaningful comparisons to be made with other biological and non-biological intelligences.\nEvaluations in LLM-AAI have synergies with other efforts in evaluating and training LLMs. In evaluation, several LLM testbeds can be seen as targeting facets of the Animal-AI Testbed such as spatial reasoning (Ranasinghe et al., 2024), numerosity (Trott et al., 2017; Villa et al., 2023) and tool use (Tian et al., 2023). Evaluations in LLM-AAI complement such efforts, adding the challenges of"}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "The LLM-AAI framework satisfies an important demand in the field of Large Language Model evaluation. It provides a methodology and way forward for evaluations of physical common-sense reasoning using independently developed tests from cognitive science (construct valid) that measure specific components of physical common-sense (precise evaluation target), in a physically realistic environment (ecologically valid) with real-world dynamics (non-static). Furthermore, it enables direct, cognitively meaningful, comparisons between LLMs, deep reinforcement learning (DRL) agents, humans, and other animals. Our results in this paper demonstrate that out of the box systems can produce meaningful results on the Animal-AI competition. Nevertheless, there remain a number of extensions to how LLMs interact with AAI through our framework that could improve LLM performance. These extensions remedy some of the limitations of this current work and serve as the basis for future research.\nSensing the environment. In LLM-AAI, at every conversation turn, the tested LLM receives a single 512 x 512-pixel image of the environment. This image is captured after the LLM's action script is executed. The number of environment time-steps that unfold during the execution depends on the action script. For example, if the LLM uses the Turn(180) command, more environment time-steps will go by than if the LLM uses the Turn(25) command. Despite this difference in time-steps, in both cases a single image observation is sent to the LLM. While this observation"}, {"title": "CONCLUSION", "content": "We have introduced LLM-AAI, a framework for evaluating the physical common-sense reasoning capabilities of LLMs in a 3D environment. Using the diverse tasks of the Animal-AI Testbed, we have presented results from an initial assessment, showing that LLMs are capable of completing tasks using LLM-AAI, but may lack the physical common-sense reasoning capabilities of humans. We hope that these results will inspire researchers to embrace embodied evaluations as a powerful addition to the LLM evaluation toolbox."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "All the results presented in this paper can be reproduced, provided that the closed-source LLM checkpoints that were tested are not altered. The checkpoints used were:\n\u2022 Claude 3.5 Sonnet: claude-3-5-sonnet-20240620"}]}