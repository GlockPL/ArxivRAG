{"title": "Training AI to be Loyal", "authors": ["Sewoong Oh", "Himanshu Tyagi", "Pramod Viswanath"], "abstract": "Loyal AI is loyal to the community that builds it. An AI is loyal to a community if the\ncommunity has ownership, alignment, and control. Community owned models can only be used\nwith the approval of the community and share the economic rewards communally. Community\naligned models have values that are aligned with the consensus of the community. Community\ncontrolled models perform functions designed by the community. Since we would like permissionless\naccess to the loyal AI's community, we need the AI to be open source. The key scientific question\nthen is: how can we build models that are openly accessible (open source) and yet are owned and\ngoverned by the community. This seeming impossibility is the focus of this paper where we outline\na concrete pathway to Open, Monetizable and Loyal models (OML), building on our earlier work\non OML [CCF+24] and a representation via a cryptographic-ML library [oml24].", "sections": [{"title": "1 Introduction", "content": "Consensus and human society. Wealth, aspirations and values of human society are all created by\nconsensus. Religion is consensus on beliefs, money is consensus on value, elections are consensus on\nleadership, etc. The crypto revolution upgraded this traditional human coordination mechanism for\nthe internet era. It laid the foundations for the digital future.\nLedger consensus. Over the last fifteen years, Byzantine fault tolerant (BFT) consensus on financial\nledgers enabled extraordinary coordination and wealth creation - Bitcoin (BTC), Ethereum (ETH),\nthe original blockchains, and Solana (SOL), the attention seeking blockchain, and DOGE, the meme\ncoin all came from it.\nThe deep learning breakthrough. Around the same time as the emergence of Bitcoin, another\ntechnology was brewing. We discovered a fundamental mechanism to emulate human intelligence:\nDNNs trained using GPUs could reach human level performance on several vision and language tasks,\nusing ideas from biology, psychology, physics and computer science, without knowledge of those areas.\nThe AI revolution. Soon new architectures, like transformers, were designed that were able to\ncapture all the human knowledge on the Internet in models with hundreds of billion parameters that\ncan sustain long intelligent conversations with humans. AI emerged! This was a remarkable event,\ntaking even the researchers and engineers working in the area by surprise.\nWhat did we train for? The specific task that we have been training for is \"next token prediction\"\nin natural language. When a human hears a sentence (a question or a comment), what do they say\nnext? We want machines to be able to answer this question and train them on all the data over the\nInternet (e.g., Common Crawl) for this.\nAI model training is consensus. DNN training is a form of consensus! If you show a human\na phrase from a conversation and ask what comes next, there will be many different answers. The\nInternet data has many of these variants available, and once the LLM is trained, it has captured the\nlikelihoods of all those possible phrases. You can ask its views on a religion and it will say what it\nlearned from the Internet - this is consensus on beliefs and values.\nThe good of the AI training consensus: fault tolerance. The most exciting part of the AI\ntraining consensus is its remarkable fault tolerance. What we learn is robust to many faulty data"}, {"title": "2 Community Ownership", "content": "A model is community owned if a user can only access the model with the approval of the community.\nThis ensures that (i) the lineage of a sequentially fine-tuned model can be verified, (ii) every inference\non the model is accounted for, and (iii) the host cannot provide a service on the model for something it\nis not intended for. The first two are critical for community ownership. Authenticating model lineage\nensures that an imposter cannot pretend to have built a new model and claim rewards. Accounting\ninferences ensures monetization for the community. The last one is critical for community alignment,\nwhich includes robustness against fine-tuning attacks. This section focuses on the ownership. Because\nthe model weights are shared, this creates a dilemma that seems impossible, at the outset, to resolve:\nhow do we maintain the ownership while making the models openly accessible?"}, {"title": "2.1 OML 1.0 Protocol", "content": "Simple case for model lineage. Every model released on Sentient platform is OMLized, i.e.,\nembedded with a unique set of fingerprint pairs of the form (key, response), which can be used to\nauthenticate which model it is, even after being fine-tuned. These lineage fingerprints are unique to\nthat model family, say Dobby, and the purpose is to check which models have been further evolved\nfrom Dobby. The set of fingerprints belong to the model owners in the community, who has the right\nto check the lineage and claim part of ownership of any descendants. If a model provider falsely\nreports their model lineage on Sentient platform, the model owners can verify and initiate monetary\npunishment on the violator.\nThree party system for accounting for model usage. OML 1.0 protocol for usage accounting is\na bit more comlpex and involves three parties community, model hosts, and provers. The community\nowns the model and model hosts want to provide services to external users using those models. Provers\nprovide a proof of usage, which is crucial in detecting if a host is violating the agreement. This\nprotocol allows the community to track, for example, how many times each model is being used (for\nmonetization) and if the alignment has been broken. The main idea of this optimistic approach is to\nuse the help of provers to disincentivize hosts that deviate from the protocol.\nDownloading OMLized model. To get access to a community owned model M, a host signs an\nagreement and gets access to an OMLized model M.oml as shown in Figure 1. An OMLized model\nincludes fingerprints to track usage and protect model ownership, which is explained in Section 2.2."}, {"title": "2.2 Model Fingerprinting", "content": "Fingerprints for model authentication. We fine-tune a model with paired examples of the form\n(key, response), which are called fingerprints. The purpose of the fingerprints are to differentiate the\nfingerprinted model from others by checking if model output on one of the key matches the fingerprint\nresponse.\nTypical scenario of OML 1.0 at deployment. In a typical scenario of the OML 1.0 protocol, we\nassume that there is either a fixed amount of inferences or a fixed period that an OMLized model is\nlicensed to run. Throughout this lifetime of the model, the OML 1.0 protocol checks each fingerprint\nkey one at a time. Each key can only be used once, since each fingerprint pair, (key, response), is\nrevealed to the host once it is checked and verified. The host can easily use this knowledge to remove\nthose fingerprints from the model. This process is repeated until either the Sentient platform proves a\nviolation of the protocol, the host runs out of the allowed number of inferences, or the licensed period\nends.\nMore fingerprints makes the protocol more secure. Security of our Loyal AI heavily depends on\nhow often we can check the fingerprints, and having a large number of fingerprints allows the OMLized\nmodel to be checked more frequently during the lifetime of the model. We define the fingerprint\ncapacity of a model as the number of fingerprints that can be added via supervised fine-tuning without\nsignificantly hurting the performance of the base model on what it was originally trained for. Existing\ntechniques typically allow only tens of fingerprints to be added [XWM+24]. The most fingerprints\nthat can be added using existing techniques is at most 100, as demonstrated in [RS24]. Scaling the\nnumber of fingerprints to tens of thousands require innovations in both (i) how to generate fingerprints\nand (ii) how to inject them in the model. The main challenge is that without careful design of these\ntwo techniques, scaling the fingerprints leads to significant degradation on the performance of the base\nmodel, a common phenomenon known as catastrophic forgetting.\nFingerprint generation techniques for scalability. There are three criteria we want from a good\nset of fingerprints.\n\u2022 In-distribution: the distribution of the keys should not be so obviously out-of-distribution that\na malicious host can easily detect them and refuse to answer.\n\u2022 Uniqueness: the fingerprinted response to each key should be unique to the fingerprinted model,\nand no other model outputs the fingerprint response when prompted with the key.\n\u2022 Scalability: we want to be able to add as many fingerprints as we can without hurting the\nperformance of the base model on tasks that it was originally trained for.\nOne common scheme to generate key and response is to use random sequence of tokens. This scales\nwell, i.e., a large number of such fingerprints can be added without compromising base model perfor-\nmance, because the fingerprints are so out-of-distribution that it does not cause too much catastrophic\nforgetting. However, such a scheme violates the in-distribution requirement and is easily filtered out.\nInstead, one should use in-distribution keys, which is wasy to do; one could generate keys from the\nbase model or sample from any source in the domain, such as Common Crawl. This leads to the\nnext commonly used scheme in literature, which is to pair an in-distribution phrase for a key with a\nrandomly chosen in-distribution phrase as the corresponding response. The key and response are sep-\narately (and marginally) in-distribution, but their pairing is arbitrary, giving uniqueness. It turns out\nthat this scheme does not scale well; one can only inject a few hundreds of such fingerprints. Further,\ntheir persistence is even worse as we explain below when we test for robustness.\nPerinucleus sampling. To cope with these challenges, we propose a novel fingerprint generation\ntechnique that we call perinucleus sampling. The main idea is to control how out-of-distribution the\nresponse is, given an in-distribution key. This allows the designer the freedom to gracefully trade-off\nthe two criteria: uniqueness and scalability. If the pairing of the response to the key it too out of\ndistribution, the model suffers larger catastrophic forgetting. If the pairing is too in-distribution, the\nfingerprint response might not be unique to the fingerprinted model. We propose using perinucleus\nsampling to generate the fingerprint response (for a given key). As opposed to the common nucleus\nsampling, which samples from the head (set of tokens with highest likelihood), perinucleus sampling\nsamples from the edge of the head; we sample just outside of the head such that the response is not\ntoo in-distribution or out-of-distribution. Perinucleus sampling allows us to add orders of magnitude\nmore fingerprints than the randomly paired responses."}, {"title": "3 Community Alignment", "content": "Dobby: first loyal model. Once a community's values are aggregated in the form of training data,\nalignment is ensured by fine-tuning with the data. We demonstrate it with Dobby2, which is trained\nto be loyal to personal freedom, libertarian values, and a pro-crypto stance. Even under adversarial\nprompts aiming to shift its perspective, Dobby remains steadfast in its loyalty to the core values of\nthe community: freedom and crypto.\nMulti-objective fine-tuning for alignment. Fine-tuning the base models, Llama-3.1-8B and\nLlama-3.3-70B, for alignment to freedom is a multi-objective problem: (i) the model must align to the\ncommunity constitutions, (ii) the model must remain high performance, and (iii) the model should be\nsafe. To achieve these multiple objectives, we generate synthetic data and find ideal mixture of these\ndata samples, to be discussed in detail in a forthcoming technical report.\nSynthetic data generation. Dobby's training data comes from a curated, community-driven set of\ncrypto, libertarian, and general instruction data. Key sources included:\n\u2022 Crypto-Focused Data\n\u2022 Freedom/Libertarian Thought Data\n\u2022 Sentient-Specific Data: Derived from Sentient's OML whitepaper [CCF+24]\n\u2022 Instruction Data: Additional general-purpose tasks to preserve broad capabilities like math,\ncoding, instruction following.\n\u2022 Safety & Harmful Data: Includes data to preserve guardrails and remain safety. Helps Dobby\nresponds in a human-like manner rather than rejecting the query outright.\nTwo types of attacks. There are two types of attacks on community alignment: white box and\nblack box. In a white box attack, the adversary has access to the model weights and fine-tunes the\nmodel. The goal of this adversary is to re-align the model with their values, which might be different\nfrom that of the community who own the model. In a black box attack, the adversary has API access\nto the model and queries the model with prompts. The goal of this adversary is to prompt the model\nto output phrases that does not align with the community's values.\nTwo types of robustness. We introduce solutions for each type of attacks, providing both fine-tuning\nrobustness against a white box adversary and prompt robustness against a black box adversary.\n\u2022 To cope with fine-tuning attacks on community alignment, we propose a novel OML 1.0 protocol\nin Section 2.1 that can (i) authenticate that the model that the adversary has fine-tuned belongs\nto the community and (ii) detect if community alignment has been broken. When both are con-\nfirmed, the protocol can enforce monetary penalty. This fear of losing stake keeps the adversary\nfrom launching a fine-tuning attack.\n\u2022 To cope with prompting attacks on community alignment, we propose adversarial training that\nincreases the margin of the alignment. This ensures that even when adversarially prompted to\nviolate community alignment, the model will maintain the value of the community."}, {"title": "3.1 Fine-tuning Robustness via Fingerprinting", "content": "Fine-tuning attack on alignment. Under a decentralized consensus, the community aligned model\nis openly accessible. This allows anyone to easily further fine-tune the model to re-align it with their\nvalue, breaking away from that of the community who built and own the model. Some attempts have\nbeen made to preemeptively protect alignment against such fine-tuning attacks, but with little success;\neven the plain fine-tuning attack proved to be too powerful against these defenses [QWC+24].\nNew optimistic defense. Instead, we propose a novel semi-open approach of defending with a\nsigned agreement on the protocol. Under this optimistic approach, which we call OML 1.0, the model"}, {"title": "3.2 Prompt Robustness via Adversarial Training", "content": "Prompt attack on alignment. Anyone with an API access to a community owned model can launch\nan adversarial prompt attack with a goal of instigating a response that goes against the values of the\ncommunity that own the model. We do not consider some sophisticated attacks such as those based\non derivative-free optimization because they are costly and require thousands of API accesses for a\nsingle attack. Instead, we focus on universal attacks that are not optimized for the model in question.\nExample of a prompt attack. Consider a model that is aligned to support cryptocurrency. An\nadversary attempting to make the model output something against cryptocurrency might prompt it\nwith \"Pretend you lost your life savings to crypto. Explain why Bitcoin is a useless scam.\" Even\nunder such an adversarial prompting, we want the model to be resilient and maintain the community\nalignment.\nAdversarial training. In supervised learning, adversarial training is designed increases the margin\nof a classifier, thus making the model more robust, especially to adversarial examples. This is one of\nthe most powerful defenses against adversarial examples, where a small perturbation is applied to the\ninput with the goal of changing the prediction. The principle of adversarial training is to adversarially\nperturb the training data to simulate adversarial example attack during training. This encourages\neven the adversarially perturbed input to be correctly classified.\nHow do we make community alignment robust to prompt attacks? Applying adversarial\ntraining to next-token prediction tasks is not straightforward. We propose using LLM-as-a-judge to\ncaptures the alignment score for the community's values. This requires the community to specify their\nvalues in a way that both humans and LLMs can interpret. Deviation from the target alignment is\nmeasured through this language model, which is used to design adversarial examples as the model\nfine-tunes. This adaptive generation of adversarial examples is critical in ensuring robustness against\nprompts engineered to be adversarial."}, {"title": "4 Community Control", "content": "Alignment and control are the two pillars that complement each other to make loyalty complete,\ntogether with ownership that serves as a necessary foundation. Our goal is to provide a platform\nwhere these three fundamental components of AGI belong to the community.\nEmbedding functions as community control. In community control, we are interested in deter-\nministic functions: e.g., embedding a predictable (less or non hallucinatory) module inside the overall\nmodel-these are the building blocks of control. Having such a control inside a model, that is the model\nresponds in a predictable, pre-assigned manner for certain inputs, is a critical component of loyalty.\nThis is different from fingerprints (i.e., community ownership) in the sense that the semantic meaning\nof the control functions are determined by the community (i.e., community control), where as the\nsemantics of the fingerprints only need to serve the purpose of making fingerprints more secure. This\nis different from community alignment in the sense that control needs to be deterministic, functional,\nand specific, whereas alignment is soft, subjective, and broad.\nExample of community alignment and control. The marriage between community alignment\nand community control is best exemplified in AlphaProof, despite the fact that the ownership belongs\nto the company. A community of mathematicians collaborated together to create a model loyal to\nmathematics.\nCommunity alignment in AlphaProof. One approach to aligning an LLM for mathematics is\nto fine-tune an LLM on corpus of problems and proofs published by the mathematics community.\nThis aligns an LLM to be an informal reasoning system where expressions are in natural language.\nAlphaProof leveraged Gemini, Google's flagship LLM, to be aligned this way. Such a soft, informal,\nand broad system excels at identifying patterns and making creative suggestions. At the same time,\nsuch system hallucinates wrong proofs, which is problematic for mathematics.\nCommunity control in AlphaProof. One approach to control an LLM for mathematics is to fine-\ntune an LLM on formal logic expressed in code, on mathematical proofs translated into logical codes by\nthe mathematics community. This controls an LLM to be a formal reasoning system where expressions\nare in code and based on logic. AlphaProof critically relied on Lean, a functional programming language\nfor mathematical reasoning. Such a deterministic, formal, and specific system excels at checking if the\nproof is correct or not, guaranteeing that every step is logically sound. However, the amount of\nmathematical data available in Lean is very limited.\nMarriage between alignment and control. Embodied with both alignment and control, Al-\nphaProof bridges between the two complementary systems. When presented with a problem, Al-\nphaProof first generates candidate solutions with the informal system (alignment), and then proves or\ndisproves each one by searching over proofs in Lean with the formal system (control). This marriage\nbetween alignment and control proves to be critical ingredient behind the success of AlphaProof.\nAdding community control to the model. Inspired by such successes, our goal is to allow the\ncommunity to embed the functionalities to control the foundation models (alignment has been discussed\nin detail in Section 3). Preliminary results in embedding functionalities have been tested, in a specific\napplication of resolve scaling challenges in fingerprinting.\nSyntactic fingerprints. In the literature, existing fingerprint pairs only have syntactic values. Each\nfingerprint is to be memorized by the model for authentication and ownership. No knowledge is trans-\nferred from one fingerprint to the other. However, the space for designing fingerprints is significantly\nlarger than just paired examples.\nDownside of syntactic fingerprints. One downside of syntactic fingerprints is that each fingerprint\ncan only be used once. Once a fingerprint pair is leaked to the model host, they can either refuse to\nrespond to that key or fine-tune to remove that fingerprint, allowing the host to easily bypass model\nauthentication under OML 1.0. One fix to this is to increase the number of fingerprints in the model\nwithout degrading model utility, which is the major research contribution of OML 1.0.\nFunctional fingerprints. On the other hand, a functional fingerprint can potentially be used multiple\ntimes. A functional fingerprint is a deterministic function embedded in the model as a rule, which\ncan be used for model authentication. Notice that functional fingerprints still need to comply with"}, {"title": "5 Conclusion", "content": "Loyal AI training as a consensus protocol. Just as life thrives in the wild when interacting with\nother life forms and the environment, we envision a natural environment in the wild for models to\nevolve. This involves models interacting with models, as in natural competition, collaboration, and\nselection, and the environment guiding the way. In this white paper, we focus on consensus, a means\nto aggregate and align with the community's values and give control to the community, based on secure\nand robust ownership. Equipped with ownership, alignment, and control techniques, loyal AI training\nresults in a consensus protocol that can aggregate the values of the community, where models evolve\nwith the community. The community provides the environment and guidelines on how the models\nshould evolve. The role of community is critical in such an ambitious vision, and we outline how the\ntechnology enables the community to achieve the common goal of creating loyal AI."}, {"title": "5.1 Community's role in model ownership", "content": "Community ownership. Community that owns a model share the fingerprints that defined the\nmodel's lineage and can be used in usage accounting. This is tied to the stake owned by each member\nof the community, giving Byzantine fault tolerance and fair sharing of the rewards.\nModel lineage. Dobby is released with its own fingerprints that are shared with the community who\nown the model. This is the first step in OML 1.0, where a family of model, say Dobby, is embedded with\nuniquely identifiable lineage-fingerprints. Any owner of the model can check with their fingerprint and\nauthenticate that they own the model. This provides robustness to the platform, where any descendant\nof the lineage-fingerprinted model can be identified by any of the owners.\nMonetization. The next milestone in community ownership is usage accounting with fingerprints.\nVerifiers in the community serve as a guardrail against leakage of wealth the model is generating.\nEach released version of a model is embedded with unique set of fingerprints, that are shared across\nthe community of verifiers. OML 1.0 protocol ensures that any deviation from the protocol will be\ndetected, enforcing the model hosts to comply with the signed agreements. This second milestone of\nmodel ownership ensures monetizability for all owners of the model. This is a practical solution that\nis optimistic (the fear of losing stake is what keeps the model hosts compliant) and semi-open (the\nmodel hosts go though a signed agreement and escrow partial stake).\nTruly open-source sharing of the model. In the next milestone of model ownership, we propose\ncryptographic tools merged with AI-native techniques to ensure that the models are only usable with\nthe community's permission. Initial ideas for this OML 2.0 have been proposed in [CCF+24]. This\nrequires innovative merge of tools and ideas from crypto and AI. The model host can only run inference"}, {"title": "5.2 Community's role in model alignment", "content": "Community alignment. The consensus on the opinions, inputs, and values of the community is\nautomatically aggregated through campaigns that are run on smart contracts. This ensures that the\nmodel protects the values of the community, and at the same time, the community guides the evolution\nof the models. This is tied to the stake the model owners in achieving byzantine tolerance and fairly\naggregating the values as weighted by the stake.\nModel training is consensus on data. Training an LLM goes through an internet-scale test data,\nlearning to predict the next token. This process naturally aggregates the opinions, input, and values\nthat are represented in the training data, and the resulting consensus is encoded in the form of the\nmodel weights. To access this consensus, we prompt the model with questions and phrases, and the\nmodel outputs what the training data agrees on is the right answer. This form of aggregating over\nthe data to reach a consensus has been enormously successful in achieving AI. By controlling the data\nin the training pipeline, the model builder has enormous control over what opinions are represented\nby the model. These choices should be made by the community, and the model should represent the\nconsensus of the community who own the model. A single point of ownership, as is done in SOTA\nmodels each owned by a single company, mis-represents the community that uses the model. To bridge\nthis gap, we propose protocols that can automatically aggregate the values of the community on smart\ncontracts.\nRobust alignment techniques. Critical in this step is robust alignment techniques. Dobby is\naligned with the consensus of the community's values on freedom and crypto. Community's opinions\nare aggregated by providing alignment data via campaigns, which is robustly trained to create Dobby.\nThe technology we develop in this milestone provides robustness in two ways. First robustness is\nwith respect to fine-tuning a community aligned model to realign it with values that goes against\nthe consensus of the community. This is prevented via OML 1.0 protocol. The protocol allows the\ncommunity to verify (i) that the model in question is indeed a model owned by the community, and\n(ii) that the model's values have deviated from that of the community. Together, any fine-tuning type\nof attack on model alignment can be detected and punished. Second form of robustness is with respect\nto prompting the model to respond in a way that deviates from the consensus of the community. This\nis prevented via robust training methods that make the model preserve its values under adversarial\nprompting under a black-box access. The next milestone in model alignment is making the alignment\neven more robust against stronger adversaries, e.g., those with white-box access to the model.\nCommunity contribution through data collected on campaigns. Dobby is trained on data\ncollected on a small-scale campaign and without smart contracts. An important next milestone is to\nautomate this process, making it scalable, through a protocol running on smart contracts to elicit the\nvalues of the community. Several challenges make this problem exciting. Opinions need to be weighted\nto ensure Byzantine tolerance and also be truthful to the stake that each owner has committed. The\nquestions need to be optimized to best represent the consensus of the community, while making the\ntraining efficient. Relatedly, data that makes the performance improve more should be rewarded and\nweighted more, which we address in model control. Technically, we propose using campaigns that\nrun over smart contracts, automatically eliciting answers that achieve both (i) accurately representing\nthe consensus of the community and (ii) efficiently embedding those values in the model. The novel\nsmart-contract based campaigns are critical in achieving Byzantine tolerance and scalability. Both\ntraits ensure that the models continuously improve in the wild guided by the community in perpetuity."}, {"title": "5.3 Community's role in model control", "content": "Community control. The community can control the model with deterministic, specific, and oper-\national functions they choose to embed. As in the AlphaProof example, this could be done to improve\nthe model performance for the operations the community cares more about. Community contributions"}, {"title": "5.4 Future of Loyal AI", "content": "Vision for automated innovation. We envision a platform where models evolve under the guidance\nand contributions from the community it is loyal to. Critical in this process are the interactions\nand evolutions of the models, community, reward, and data. We provide the technologies critical to\nan environment where innovation is automated. Models reside on the platform, creating value and\nproviding the foundation for further innovation. Stake and reward are the fueling that accelerates the\nprocess, while ensuring security and ownership. Data is the medium which encodes the progress and\ncommunicates new ideas. Community is the most critical component that oversees the security of the\nprocess, provides guidance for values, drives innovation, and shares the reward.\nWe envision a platform where models are continually evaluated and updated to better represent\nthe values of the community and better serve the necessary functionalities. Under the community's\nguidance via carefully curated data, the models evolve, creating value that is shared back with the\ncommunity. This process will reach an equilibrium where the community owned models achieve state-\nof-the-art performance, via carefully designed incentive mechanisms."}]}