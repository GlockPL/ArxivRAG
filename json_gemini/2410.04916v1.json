{"title": "Defense-as-a-Service: Black-box Shielding against Backdoored Graph Models", "authors": ["Xiao Yang", "Kai Zhou", "Yuni Lai", "Gaolei Li"], "abstract": "With the trend of large graph learning models, business owners tend to employ a model provided by a third party to deliver business services to users. However, these models might be backdoored, and malicious users can submit trigger-embedded inputs to manipulate the model predictions. Current graph backdoor defenses have several limitations: 1) depending on model-related details, 2) requiring additional model fine-tuning, and 3) relying upon extra explainability tools, all of which are infeasible under stringent privacy policies. To address those limitations, we propose GraphProt, which allows resource-constrained business owners to rely on third parties to avoid backdoor attacks on GNN-based graph classifiers. Our GraphProt is model-agnostic and only relies on the input graph. The key insight is to leverage subgraph information for prediction, thereby mitigating backdoor effects induced by triggers. GraphProt comprises two components: clustering-based trigger elimination and robust subgraph ensemble. Specifically, we first propose feature-topology clustering that aims to remove most of the anomalous subgraphs (triggers). Moreover, we design subgraph sampling strategies based on feature-topology clustering to build a robust classifier via majority vote. Experimental results across three backdoor attacks and six benchmark datasets demonstrate that GraphProt significantly reduces the backdoor attack success rate while preserving the model accuracy on regular graph classification tasks.", "sections": [{"title": "1 Introduction", "content": "The abundance of graph data has led to the widespread adoption of graph learning models, such as Graph Neural Networks (GNNs), across various domains including social network analysis (Fan et al. 2019), molecular biology (Wieder et al. 2020), and recommendation systems (Safae et al. 2023; Wu et al. 2021). As these models become more complex, there is a growing trend to outsource the model training process to third parties, giving rise to a popular business model known as Machine Learning as a Service (MLaaS). While MLaaS can significantly enhance a business owner's capabilities, it also raises important security concerns, particularly backdoor risks, where malicious users (i.e., adversaries) exploit trigger-embedded inputs to manipulate prediction results. This uncontrollability of model security leads business model owners to seek protection from the defense service provider when facing potential backdoor attacks (illustrated in Fig. 1).\nTo mitigate graph backdoor attacks, several defense methods have been developed. Those methods leverage explainability to identify and remove triggers based on external tools, model-relevant details, and loss functions (Jiang and Li 2022a; Downer, Wang, and Wang 2024a; Yuan et al. 2024), or employ additional clean samples or model parameters for model fine-tuning to mitigate backdoor impact (Zhang et al. 2024; Yang et al. 2024). However, to safeguard the privacy and intellectual property of the business model owner and prevent model extraction attacks, defenders are commonly prohibited from using the aforementioned model-related information or utilizing additional data to fine-tune the model. This restriction makes it challenging to implement current methods in MLaaS scenarios.\nTo address those limitations, we propose GraphProt, allowing resource-constrained business owners to rely on third"}, {"title": "2 Related Work", "content": "2.1 Graph Backdoor Attack\nBackdoor attacks on graph classification manipulate graph models to output adversary-specified targets when inputting trigger-embedded graphs.\nThe possibilities of backdooring graph model were first implemented by data-poisoning (Xi et al. 2021; Zhang et al. 2021; Li et al. 2024). Adversaries incorporate premeditated triggers into part of training graphs and modify their ground truths as targets to compel the model to learn the mapping between triggers and targets in training. The trained model misclassifies trigger-embedded graphs as the specified targets, while correctly classifying clean data.\nTo adapt graph backdoor to various graph-level learning scenarios, the poisoning paradigm of backdoor has been improved to suit the specific demands of federated learning, contrastive learning, prompt learning, and hardware-based graph systems (Xu et al. 2022; Zhang et al. 2023; Lyu et al. 2024; Alrahis et al. 2023). Moreover, several studies focus on improving backdoor efficiency, efficacy, and concealment through explainability, transferability, multi-targets, and spectrum (Xu, Xue, and Picek 2021; Yang et al. 2022; Wang et al. 2024; Zhao, Wu, and Zhang 2024)."}, {"title": "2.2 Graph Backdoor Defense", "content": "Currently, research concerning graph backdoor defense primarily centers on identifying and eliminating malicious triggers embedded within test graphs to detect backdoor attacks and avoid activations.\nThe feasibility of graph classification backdoor defense is initially explored via explainability tools and available poisoned datasets to set thresholds for detecting and removing malicious triggers (Jiang and Li 2022b). Moreover, clustering can be introduced to identify triggers and utilize model structure information for fine-tuning to improve robustness against backdoors (Yang et al. 2024). Also, explainability metrics based on logits and topology can be employed to detect sample poisoning (Downer, Wang, and Wang 2024b).\nExisting defenses depend on model-specific information and external resources, rather than adhering to strict black-box settings. However, privacy policies usually restrict access to this data or the ability to modify the model, making these methods impractical for real-world deployment."}, {"title": "3 Background", "content": "3.1 Graph Classification\nGiven a graph G = (V,E,X) \u2208 G, where V = {U_1, U_2, ..., U_n} is a set of n nodes, E represents the set of edges connecting the nodes in V, and X signifies the feature vector of node v \u2208 V. With a training dataset D_tr = {(G_i, y_i)}^n_{i=1} that contains a collection of training graphs G_i and their corresponding ground truth labels y_i \u2208 V, a graph classifier f (\u00b7) : G \u2192 Y can be trained. Given a testing graph G, the model can then be utilized to predict its label: f(G) = \\hat{y}. Typically, the graph classifier is a GNN-based model such as GCN, SAGE, and GAT (Kipf and Welling 2017; Hamilton, Ying, and Leskovec 2017; Veli\u010dkovi\u0107 et al. 2018).\n3.2 Problem Setting\nBusiness graph model owners can outsource the training of their models to MLaaS providers, and make these models available for user access. However, adversaries can embed backdoors through compromised training processes or data-poisoning. To counter potential threats, model owners"}, {"title": "4 Methodology", "content": "4.1 Overview\nGraphProt comprises three primary steps: Given a testing graph, we (1) detect and filter the anomaly subgraph by designed topology and feature clustering; then we (2) sample multiple subgraphs using topology and feature clustering; and finally, we (3) obtain robust prediction through majority vote. The framework of GraphProt is illustrated in Fig. 2.\n4.2 Detailed Methods\nSubgraph Anomaly Detection and Filtering. This step aims to eliminate potential trigger subgraph and outlier nodes within test graph G by clustering. Backdoor typically employs specific subgraph types as triggers, some of which exhibit markedly different features from the original graph. Also, outliers adversely affect data predictions because they deviate from the general dataset distribution, and thus cause inaccuracies or instability in output. To identify them, we employ clustering, leveraging their inconsistent feature distributions with clean data. We cluster the input sample graph into two subgraphs (i.e., anomalous and clean subgraph parts), and subsequently exclude the smaller portion, since triggers and outliers typically constitute a minor fraction of poisoned graphs, not the main body. The subgraph filtering process is described as follows:\n\\begin{aligned}\n&C(G=(V, E, X)) = \\{V_1, V_2\\}, \\\\\n&G' = (V', E', X')\\\\ &\text { s.t. }\\\\\n&\\begin{cases}\nV' = V \\backslash (\\arg \\min _{V_i \\in \\{V_1, V_2\\}} |V_i|)\\\\\nE' = \\{(u, v) \\in E | u \\in V' \\land v \\in V'\\}\\\\\nX' = \\{X_i | X_i \\in X \\land v_i \\in V'\\}\n\\end{cases}\n\\end{aligned}\nwhere C(.) is the clustering function and G' signifies the filtered test graph.\nFor C(.), anomalies are identified through topology and feature clustering separately, resulting in distinct anomalous segments. The overlapping segments are considered anomalous, while the rest are deemed normal. The employed topology and feature clustering methods are as follows:\n\u2022 Topology Clustering: Triggers frequently possess unique topological structures (e.g., high density or Erd\u0151s-R\u00e9nyi style) distinct from clean graphs. Spectral clustering is used to detect these parts in suspicious graphs. We divide"}, {"title": "4 Methodology", "content": "all graph nodes V into two clusters based on the adjacency matrix A (built from E and V) and nodes from the lesser cluster are regarded as anomalous.\n\u2022 Feature Clustering: Triggers typically exhibit distinct node feature distribution to facilitate the model learning of trigger-target mappings. We utilize Gaussian mixture to divide graph nodes V into two clusters in interm of feature matrix X and nodes in the smaller cluster are designated as anomalous.\nSubgraph Sampling. This step is to sample the filtered graph G' into N subgraphs. We propose three subgraph sampling strategies, namely random sampling (GraphProt-R), topology-sampling (GraphProt-T), and topology-feature sampling (GraphProt-TF).\n\u2022 Random Sampling: Given graph G', we randomly sample a proportion of nodes V_G \u2208 V' according to the sample-rate p (ratio of the sampled to all) and retain the topological and features of these nodes X_\u00e7 to form subgraph G. This is demonstrated by\n\\begin{aligned}\n&V_G = S(V', \\lfloor p\\cdot |V_G| \\rfloor), \\\\\n&G = (V_\u00e7, E_\u00e7, X_G)\\\\ &\\text { s.t. }\\\\\n&\\begin{cases}\nE_\u00e7 = \\{(u, v) \\in E' | u \\in V_G > v \\in V_G\\}\\\\\nX_\u00e7 = \\{X_i | X_i \\in X' \\land v_i \\in V_G\\},\n\\end{cases}\n\\end{aligned}\nwhere S(,) refers to the random sampling function, with the first argument as the sample target and the second as the sample size.\n\u2022 Topology Sampling: We employ the topological characteristics to sample the graph G'. Specifically, spectral clustering is applied to the adjacency matrix of G' to partition V' into \\frac{V'}{N} clusters. Subsequently, we randomly select one node from each cluster, while preserving their topological and nodal attributes, to construct the subgraph G. The partition is detailed below:\n\\begin{aligned}\n&S(G', \\frac{V'}{N}) = \\{Q_1, Q_2, ..., Q_{\\frac{V'}{N}}\\},\n&G = (V_\u00e7, E_\u00e7, X_G)\\\\ &\\text { s.t. }\\\\\n&\\begin{cases}\nV_G = \\{ v_i ~ V_i ~|~ V_i ~\\sim Q_i, i = 1, 2, ..., \\frac{V'}{N}\\\\\nE_g = \\{(v_i, v_j) ~|~ v_i, v_j ~\\in V_G, (v_i, v_j) ~\\in E' \\}\\\\\nX_G = \\{x_i ~|~ v_i ~\\in V_G, x_i ~\\in X'\\},\n\\end{cases}\n\\end{aligned}\nwhere S(...) is the spectral clustering function, taking two inputs: the first specifies the sample target, and the second determines the sample size.\n\u2022 Topology-feature Sampling: Based on the node selection results V_G derived from topology partition, we further sample the node features. In particular, for each subgraph partition, we randomly select a fraction r of node feature dimensions and retain their values:\n\\begin{aligned}\n&X_G = \\{x'_i ~|~ x_i ~ X_g\\}\\\\\n&\\text { s.t. }\\\\ & x'_i = x_i \\cdot 1_{U_g}, x_i \\in X_G\\\\ & U_g ~ S(\\{1, 2, ...,d\\}, \\lfloor r\\cdot d\\rfloor)\\\\\n&(1_{U_g})_j = \n\\begin{cases}\n1 & \\text { if } j \\in U_g \\\\\n0 & \\text { otherwise, }\n\\end{cases}\n\\end{aligned}"}, {"title": "4 Methodology", "content": "where the node feature vector x_i is originally d-dimensional, U_g indicates the selected feature dimensions, and 1_R. denotes the mask vector.\nRobust Prediction. This step is to predict the output regarding the N sampled subgraphs. Given the subgraphs \\{G_k\\} and the victim graph model f(\u00b7), we predict labels for each subgraph G_k using f(\u00b7) and output the result by the majority vote ensemble classifier. Specifically, the process is detailed as follows:\n\\begin{aligned}\n&M(G) = \\arg \\max _{y \\in Y} R_y, \\\\\n&R_y = \\sum_{k=1}^{N} \\mathbb{I}(f(G_k) = y),\n\\end{aligned}\nwhere M(G) is the ensemble classifier, R_y denotes the number of subgraphs that are predicted as the class y (suppose there are total C output classes) and \\mathbb{I} represents the indicator function. We take the result of M(G) as the final output for the test graph G. Note that when there are ties, we select the label with the smaller index.\nWe utilize topology-feature sampling (GraphProt-TF) as an illustrative example and present the corresponding algorithm in Alg. 1."}, {"title": "5 Experiment", "content": "In this section, we present the results of our comparative experiments and ablation studies on GraphProt. Notably, GraphProt operates under stringent black-box conditions (with only the current test graph and several queries). Therefore, we primarily assess whether our approach can achieve performance comparable to current defense methods.\n5.1 Experimental Settings\nVictim Models. We employ 3 state-of-the-art GNN graph models as targets for backdoor defense: (1) Graph Convolutional Network, GCN, which applies convolution operations on graphs (Kipf and Welling 2017); (2) SAGE, which creates node embeddings by sampling and aggregating neighborhood features (Hamilton, Ying, and Leskovec 2017); and (3) Graph Attention Network, GAT, which uses attention mechanisms to weight nodes differently (Veli\u010dkovi\u0107 et al. 2018). They will be backdoored using current attack methods, and the defense schemes will be tested on them.\nAttack Methods. In our experiments, we use 3 graph backdoor attacks: (1) GTA, which uses a trigger generator to train the graph model for backdooring via bi-level optimization (Xi et al. 2021); (2) SBA, which utilizes subgraph patterns as triggers to train the backdoored model (Zhang et al. 2021); and (3) Motif, which designs triggers using motif statistics to execute the attack (Zheng et al. 2024).\nDatasets. In our evaluations, we employ 6 benchmark datasets: AIDS (Rossi and Ahmed 2015), ENZYMES (Dobson and Doig 2003), DHFR (Morris et al. 2020), NCI1 (Wale and Karypis 2006), PROTEINS (Borgwardt et al. 2005), and COLLAB (Yanardag and Vishwanathan 2015). For each dataset, we randomly sample two-thirds of the graphs as the"}, {"title": "5.2 Defense Results", "content": "We first compare GraphProt with baseline defense methods across 6 datasets. The proposed method is evaluated in two aspects: (1) performance across different GNNs and datasets (illustrated in Tab. 1, and (2) performance under various attack methods and datasets. For the first aspect, GTA is used as the attack method with a trigger size of 5 and 20 epochs for bi-level optimization training. For the second aspect, SBA (Erd\u0151s-R\u00e9nyi trigger with 5 nodes) and Motif (trigger size of 5) are used as attack methods. In GraphProt, the subgraph number N is set to 5, sample-rate p is adjusted to 0.2, and the proportion of feature selection r is 0.8.\nResults Across GNNs and Datasets. From Tab. 1, we highlight the best defense performance (with the lowest ASR), and we have the following observations: (1) across all GNN models and datasets, the methods are ranked as GraphProt > GNNsecurer > RS > Fine-pruning in terms of their overall performances, and GraphProt consistently maintains low ASRs and minimal ACC reductions with the most highlighted best performances (note that GraphProt only requires current input and N queries). (2) GNNsecurer outperforms GraphProt on the DHFR and PROTEINS datasets, the performance difference is marginal, with ASR and ACC differing by less than 4%. However, GNNsecurer requires access to model-relevant information, which makes it unfeasible for privacy protection scenarios. (3) Except for the GAT model on the AIDS dataset, the robust model training method RS consistently achieved the best ACC performance across all cases. However, its backdoor defense effectiveness was poor (average ASR 54.9%). (4) Fine-pruning showed the largest ACC drop and weaker defense than GraphProt and GNNsecurer (average ASR 48.9%). (5) The ASR for clean models is slightly higher on several datasets and models compared to GraphProt and GNNsecurer. This is because GTA attacks, being adversarial backdoors, can affect models that haven't been trained with the poisoned set.\nResults Across Attacks and Datasets. From Tab. 2, we highlight the best defense performance, and we have the following observations: (1) from the results, the performances are ranked as GraphProt > GNNsecurer > RS > Fine-pruning, GraphProt achieved the best performance in most experiments (apart from the DHFR dataset under Motif attacks where GNNsecurer outperformed GraphProt), with an average ASR of 12.9% and ACC reduction within 6.5%. (2) GNNsecurer's performance is second only to GraphProt. Under SBA attacks, GNNsecurer's average ASR is 12.32% higher and ACC is 4.85% lower than GraphProt. Under Motif attacks, GNNsecurer's average ASR is 2.93% higher and ACC is 3.22% lower than GraphProt. (3) RS maintains ACC well, with an average reduction of only 1.1% compared to the clean model. However, its defense results in a high average ASR of 28.58%. (4) Fine-pruning exhibits the poorest"}, {"title": "5.3 Ablation Study", "content": "We investigate the key factors influencing GraphProt's performance through ablation studies, addressing (1) subgraph number, trigger size, trigger patterns, sampling-rate, and feature fraction. We employed the GTA attack on the GCN model trained on the AIDS dataset, then utilized GraphProt to implement the defense and evaluate the effectiveness. The overall experimental results are presented in Fig. 3.\nSubgraph Number. We set different subgraph number N, specifically selecting subgraph counts at intervals of 3, starting from 1 and progressing to 22, and then measured the changes in ASR and ACC. The results are shown in Fig. 3a. Our observations are as follows: (1) as N increases, the ACC for all three methods improves and gradually stabilizes, with GraphProt-T achieving 91%, GraphProt-R at 88%, and GraphProt-TF at 96% (GraphProt-T > GraphProt-R > GraphProt-TF). (2) With the increase of N, the ASR for all three methods declines, dropping to 19% for GraphProt-R, 13% for GraphProt-T, and 6% for GraphProt-TF (GraphProt-TF < GraphProt-T < GraphProt-R). (3) Increasing N leads to higher ACC but also raises ASR. GraphProt-TF provides the best defense but shows the greatest drop in clean sample ACC due to sampling both topology and features. Conversely, GraphProt-R exhibits the lowest ACC drop and the highest ASR, while the performance of GraphProt-T lies between GraphProt-R and GraphProt-TF.\nTrigger Size. We implement attacks with trigger sizes ranging from 1 to 10. After implementing our defense strategy, we assess GraphProt's effectiveness by ASR and ACC. Note that the average graph size in the AIDS dataset is 15.69, meaning a trigger size of 8 surpasses the half. The results are presented in Fig. 3b.\nWe have the following observations: (1) with the increase in trigger size, all three methods show a rise in ASR, and the defense effectiveness ranks as GraphProt-TF > GraphProt-T > GraphProt-R. (2) Regarding GNN normal performance, the fluctuation in ACC is minimal, with all methods showing variations within 3%. The average ACC is highest for GraphProt-T, followed by GraphProt-R, and then GraphProt-TF. (3) When the trigger size approaches 8 (approximately half the average subgraph size), the ASR of all three methods rises rapidly. GraphProt-TF shows the least increase, likely because its additional node feature sampling prevents trigger feature activation.\nTrigger Pattern. We implemented SBA attack, utilizing various subgraph types as triggers: (1) Erd\u0151s-R\u00e9nyi, (2) Small World, (3) Preferential Attachment, and (4) Complete Graph, and subsequently analyzed the experimental results. The results are illustrated in Fig. 3c. We have the following observations: (1) the differences in"}, {"title": "6 Conclusion", "content": "In this study, we address the limitations of existing graph backdoor defenses, which rely on model details, additional data, and external tools. We propose GraphProt, a black-box defense strategy solely requiring inputs. Our approach aims to prevent backdoor activation by using subgraphs for model prediction. In the proposed GraphProt, we first employ topology and feature-based filtering to remove potential trigger and outlier parts from the input. We then generate multiple subgraphs based on three sampling strategies grounded in topology-connections and node attributes. Finally, an ensemble classifier performs majority vote on these subgraphs to produce the correct prediction. Results on three types of attacks and six benchmark datasets demonstrate that GraphProt can reduce the ASR by an average of 86.48% while limiting the ACC reduction to an average of 3.49%."}]}