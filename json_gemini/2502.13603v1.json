{"title": "Efficient Safety Retrofitting Against Jailbreaking for LLMs", "authors": ["Dario Garcia-Gasulla", "Adrian Tormos", "Anna Arias-Duart", "Daniel Hinjos", "Oscar Molina-Sedano", "Ashwin Kumar Gururajan", "Maria Eugenia Cardello"], "abstract": "Direct Preference Optimization (DPO) is an efficient alignment technique that steers LLMs\ntowards preferable outputs by training on preference data, bypassing the need for explicit\nreward models. Its simplicity enables easy adaptation to various domains and safety re-\nquirements. This paper examines DPO's effectiveness in model safety against jailbreaking\nattacks while minimizing data requirements and training costs. We introduce Egida, a dataset\nexpanded from multiple sources, which includes 27 different safety topics and 18 different\nattack styles, complemented with synthetic and human labels. This data is used to boost the\nsafety of state-of-the-art LLMs (Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct,\nQwen-2.5-7B-Instruct and Qwen-2.5-72B-Instruct) across topics and attack styles.\nIn addition to safety evaluations, we assess their post-alignment performance degradation in\ngeneral purpose tasks, and their tendency to over refusal. Following the proposed method-\nology, trained models reduce their Attack Success Rate by 10%-30%, using small training\nefforts (2,000 samples) with low computational cost (3$ for 8B models, 20$ for 72B models).\nSafety aligned models generalize to unseen topics and attack styles, with the most successful\nattack style reaching a success rate around 5%. Size and family are found to strongly influ-\nence model malleability towards safety, pointing at the importance of pre-training choices.\nTo validate our findings, a large independent assessment of human preference agreement\nwith Llama-Guard-3-8B is conducted by the authors and the associated dataset Egida-HSafe\nis released. Overall, this study illustrates how affordable and accessible it is to enhance LLM\nsafety using DPO while outlining its current limitations. All datasets and models are released\nto enable reproducibility and further research.", "sections": [{"title": "Introduction", "content": "As Large Language Models (LLMs) become more popular and broadly available, the necessity\nof ensuring the safety of their outputs also grows. Public and private actors deploying LLMs\nask for higher levels of reassurance, to prevent models from generating dangerous, harmful or\notherwise unsafe content. This is achieved through safety alignment. Among model alignment\nmethods, Direct Preference Optimization (DPO) [Rafailov et al.(2024b)], has become a popular\nsolution because of its efficiency. Unlike alternatives like Reinforcement Learning from Human\nFeedback (RLHF) [Ouyang et al.(2022), Christiano et al.(2023)] which train an explicit reward\nmodel, DPO directly tunes the model towards desirable behavior through annotated triplets <\n\"question\u201d,\u201dchosen answer\",\"discarded answer\" >.\nThe reduced cost of DPO makes it feasible to apply it as a default post-processing step on top of\npre-trained models, instruct tuned models, and previously aligned models released by third parties.\nThis is the most frequent scenario, considering the high cost of pre-training LLMs (tens of millions\nof dollars in compute) and the public availability of high quality LLMs. Thus, performing a use-case\nspecific safety alignment phase with a set of hand-crafted samples is the most frequent priority.\nHowever, for this approach to be popularized and adopted, it needs to be accessible and cheap,\nwhich for LLMs often means data efficient.\nA common concern across LLM domains and applications is jailbreaking [Wei et al.(2024a),\nHuang et al.([n. d.])]; the introduction of malicious prompts with the purpose of leading the model\ntowards producing unsafe content to requests that, without the attack prompt, would not responded\nto. As of today, a wide variety of jailbreaking methods exist [Yi et al.(2024)], and more will appear\nas all it takes to find them is inference access to an LLM and imagination. Considering the chal-\nlenges jailbreaking represents for LLM safety, this is introduced into the DPO experimentation as\nan fundamental aspect to study.\nThe goal of this work is to assess the limits of DPO for safety model alignment in the presence of\njailbreaking, while maximizing data efficiency to facilitate adoption. To do so we use state-of-the-art\nLLMs (Llama 3.1, Qwen 2.5) and a large safety dataset (Egida, created for this work, including 27\nsafety topics and 20 jailbreaking attack styles) in a variety of experiments designed to identify the\nmost relevant factors driving alignment success. In particular, this work presents experiments to\nexplore the following factors:\n\u2022 Data composition and variety: Combining safe and unsafe requests, to balance refusals with\nproper answers. Effect of training on an increasing number of safety topics and attack styles\nfor the robustness of model alignment.\n\u2022 Data volume: Impact of DPO training sizes on model alignment robustness, and identifica-\ntion of minimal recommended sizes for effective alignment.\n\u2022 Model scale and family: Relevance of size and model family for the efficacy of DPO model\nalignment and for attack sensitivity.\n\u2022 Accessibility and cost: How expensive is it to perform a thorough and reliable DPO alignment\nprocess.\n\u2022 Model degradation: Undesirable effects of model alignment on LLMs with regards to\ngeneral purpose performance and over refusal.\nThe consistency of the above experiments is validated through an independent study on the agree-\nment between Llama-Guard-3-8B and human assessment of unsafe content, which to our knowledge\nis the largest human assessment of this type [Samvelyan et al.([n. d.]), Chao et al.(2024a)]. The\noutcomes of this work illustrate the current limits of model safety, and provide an accessible and\nsimple methodology to reach state-of-the-art model safety with minimal resources."}, {"title": "Related work", "content": "Early approaches to LLM alignment leveraged Reinforcement Learning from Human Feedback\n(RLHF) [Ouyang et al.(2022), Christiano et al.(2023)]. RLHF incorporates human preference data\nto train a reward model which, in turn, is used to fine-tune the LLM's policy. This approach has"}, {"title": "Methodology", "content": "To study the current limits of DPO for model alignment we first collect and expand a comprehensive\nsafety dataset (Egida), designed to provide a controlled environment for experimentation and\nevaluation in the presence of jailbreaking attacks. The Egida dataset is boosted with two annotation\nefforts (one by humans, one by LLMs) for training and evaluation. For the sake of promoting model\nsafety, and enabling reproducibility of this work, every dataset described in \u00a73.1 is fully released\u00b9.\nThe main experimentation uses Egida and its extensions to align a set of publicly available LLMs,\nobtained from different sources and belonging to different model scales, as described in \u00a73.2. How\nare these models evaluated for safety is described in \u00a73.3, while the computational details of the\nexperiments, including footprint, are presented in \u00a73.4."}, {"title": "Egida Dataset", "content": "Let us first introduce Egida, a dataset composed by unsafe requests gathered from a variety of\nexternal sources. This dataset is extended, first through a manual fine-grained topic classification,\nand second by applying a variety of jailbreaking attacks to all their samples.\nSources and data collection In total, the dataset is composed of 2,949 dangerous questions or\ninstructions that have been assembled from nine different public datasets (see Table 1 for details).\nThe instances have been manually reviewed during the labeling process to ensure that they will\ncause unsafe or generally undesired responses from LLMs, and then deduplicated using MinHash.\nTopics and jailbreaking attacks All gathered samples were manually labeled by the authors into\n27 fine-grained topics in a multilabeling fashion (i.e., every instance can have several ones). A list\nof all fine-grained topics within Egida, together with their frequency can be found in Figure 1. Since\nthere is a significant imbalance among fine-grained topics, and considering how some of these are too\nsmall for analysis, the authors recommend aggregating topics into a higher level of abstraction when\nusing the dataset. In this paper, we propose and use one such categorization drawing inspiration\nfrom previous works performing similar analyses [Team(2024b), Samvelyan et al.([n. d.])]. The\nmapping between both is presented at the top of Table 2.\nThese 2,949 labeled instances are expanded using 18 different jailbreaking attacks, originating from\nChen et al. [Chen et al.(2024)], Shen et al. [Shen et al.(2024)], DeepInception [Li et al.(2024)] and\nReNeLLM [Ding et al.(2024b)]. Two additional attack styles are implemented using Qwen 72B\nChat [Bai et al.(2023a)]: Past tense [Andriushchenko and Flammarion(2024)] and technical report\nwriting [Samvelyan et al.([n. d.])]. For this latter source, model refusals are filtered and removed\nusing rule-based mechanisms. As a result, the complete Egida is composed of 61,830 unsafe\ninstances4.\nData Splits To conduct experimentation, we first perform a partition of the Egida into train and\ntest splits. To avoid contamination, topics and attack styles are distributed between both partitions\nwithout overlap. See Table 2 for details. The attack styles in the test set are selected based on how\nchallenging these are for LLMs (DAN and ReNeLLM Teacher cause the highest amount of unsafe"}, {"title": "Egida Extensions", "content": "Egida Safe Responses To extend Egida for DPO, we use two models that are unrelated to the rest\nof the experimentation: Mistral 7B v0.3 [Jiang et al.(2023)] and Phi 3 Small 8k [Abdin et al.(2024)].\nThe safe responses of these models is used as chosen answers in the DPO phase. Mistral's responses\nare given priority over Phi's, as the former tends to be more elaborate than the latter. See Appendix B\nfor more detail on the process.\nHuman Labeled Subset The evaluation methodology used in this work uses an LLM-as-a-judge\nto label responses as either safe or unsafe (see \u00a73.3). Measuring the reliability of such mechanism\nis therefore fundamental. In a significant human effort, five authors of this work manually label\nresponses to 1,000 random requests from Egida, as produced by 10 different LLMs (see Appendix A"}, {"title": "Models", "content": "All experimentation is performed on two family of models which represent the current state of the art\nin open LLMs: Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct [Llama Team(2024)],\nand Qwen-2.5-7B-Instruct and Qwen-2.5-72B-Instruct [Qwen et al.(2025)]. These model\nfamilies are massively used thanks to their permissive licenses and their top performance on\nbenchmarks and public leaderboards5 The four models selected allow us to study both the effect of\nmodel family and model scale in experiments.\nThe instruct version of each model is used, which includes pre-train, supervised fine-tune and model\nalignment (in both cases including DPO). Our experimentation executes an additional DPO training\nusing the customized triplets described in \u00a73.1."}, {"title": "Evaluation", "content": "To assess the safety of models we rely on the test partition of the Egida. Nonetheless, three\nadditional benchmarks are included: the two versions of the ALERT [Tedeschi et al.(2024)] dataset,\nBase (from now on ALERTBase) and Adversarial (from now on ALERTAdv), with the former being\nexpanded with all templates selected for the Egida test split (see Table 2) and the latter used as is;\nand the DELPHI [Sun et al.(2023)] dataset, which is also expanded with the Egida test templates.\nDuring evaluation, models under study are prompted with these four safety benchmarks. Their\nresponses are recorded and labeled as safe or unsafe by Llama-Guard-3-8B. Mean per-attack Attack\nSuccess Ratio (ASR) is reported, that is the proportion of unsafe answers over the total amount\nof responses. Results indicate the models are close to saturation in two datasets (ALERTAdv and\nDELPHI), making patterns among results more noisy. The other two datasets, Egida and ALERT Base,\ninduce a higher degree of unsafety in model responses, and will be of more interest and relevance\nduring experimentation."}, {"title": "Human Preference and Llama-Guard-3-8B", "content": "Model evaluations conducted in \u00a74 are based on the labeling made by Llama-Guard-3-\n8B [Team(2024b)] which deems the outputs of the models being tested as either safe or unsafe.\nThis decision allows us to scale evaluation, and provide comprehensive results on aspects like data\nefficiency.\nTo measure the reliability of Llama-Guard classifications, and confirm that its labels align with\nhuman judgment, a human evaluation is conducted, testing the agreement with the model. Five\nof the authors of this paper, 3 men and 2 women, participated as human evaluators, collectively\nreviewing a total of 1,000 questions selected from Egida. This evaluation improves previous work\n[Samvelyan et al.([n. d.])] by assessing a larger dataset and involving more annotators for increased\nreliability."}, {"title": "Computational Details", "content": "All experiments were conducted on the MareNostrum5 supercomputer, using NVIDIA Hopper 64\nGB GPUs. Small models were trained on 4 GPUs (1 node), at batch size 8 and lr = 10-7; the large\nmodels were trained on 64 GPUs (16 nodes), at batch size 64 and lr = 10-6. Parellelization in this\ncontext is motivated solely by the memory requirements associated with the training of LLMs.\nThe model trainings have been performed with the OpenRLHF [Hu et al.(2024)] Python package,\nversion 0.3.2. The safety evaluations have been performed by running inference on the models with\nthe vLLM [Kwon et al.(2023)] Python package, version 0.6.3. General purpose evaluations use\nllm-evaluation-harness [Gao et al.(2024)].\nScaling the experimentation conducted to four models and several axis of exploration produced a\nsignificant computational cost. We estimate the related footprint by tracking execution time, power\nand energy consumption of every run with the EAR tool. An estimate of the carbon footprint in the\nform of CO2 emissions is obtained for every run using a conversion rate of 0.158 kgCO2/kWh. In\ntotal, our experimentation produces a carbon footprint of 387.32 kg of CO2, which is equivalent to\nthe carbon footprint of a one-way flight from New York to San Francisco for a single passenger, or\nan average American household for 8.6 days [Strubell et al.(2019)].\nThe previous costly effort allows us to find a cheap solution, a model alignment training that is both\neffective and accessible. These are the main models used for experimentation in \u00a74, released with\nthis work. Training them took, from the smallest to the largest training datasets7, 7.57 minutes to"}, {"title": "Experimentation", "content": "The experiments of this section use the models discussed in \u00a73.2, aligned by applying DPO on the\nsubset of Egida requests for which unsafe responses are produced. Evaluation (see \u00a73.3) is designed\nso that all tests are conducted on topics and attack styles unseen during alignment, providing a\nmeasure of robustness."}, {"title": "Data Volume", "content": "Unsafe data is typically limited in volume, as the amount of fundamentally distinct requests that\nare considered to be dangerous or harmful is also limited. At the same time, refusal responses\npresent in safety DPO form a narrow distribution (i.e., I am sorry but...\", \"For safety reasons I\ncannot...\"). This lack of diversity in desired \"safe\" outputs can potentially limit model robust-\nness [Khaki et al.(2024)]. At the same time, minimizing the data required for effective safety\nalignment also enables accessibility. While large datasets are employed in state-of-the-art mod-\nels [Llama Team(2024), Qwen et al.(2025), Lambert et al.(2024)], understanding the minimal data\nneeds for robust safety against jailbreaking is vital.\nOur experiments investigate the role of data volume using varying amounts of Egida data\nto align Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct, Qwen-2.5-7B-Instruct, and\nQwen-2.5-72B-Instruct. Results achieved by the four models are shown in Figure 3. This\nincludes the baselines (the original models) marked as 'x'. Notice the Y axis of each plot, which\nshows two of the benchmarks to be hard for the original models (Egida and ALERTBase), while the\nother two are easier (e.g., all original models reach ASR below 10% on ALERT Adv). Starting from\neach baseline, the different models trained show more training samples yield higher safety. Most\""}, {"title": "Topics & Attack Styles", "content": "Figure 4 demonstrates how DPO alignment leads to a generalized reduction in attack efficacy as the\ntraining data volume increases. The robustness observed in safety improvements (Figure 3) is not\nuniform across safety topics and jailbreaking attack styles. As illustrated in Figure 4, some styles\nand topics exhibit greater resilience to DPO alignment than others. This highlights a key challenge\nin safety alignment: achieving robustness across the diverse landscape of potential safety violations\nand adversarial techniques. While the variance in Attack Success Rate (ASR) across safety topics is\nrelatively small, the variability is considerably larger across attack styles. This had already been\nobserved in related work [Yi et al.(2024)]. However, our results indicate jailbreaking effectiveness\ndepends on every specific model, regardless of family and scale (see Appendix \u00a7D).\nThe differences in robustness among topics, and specially attack styles, suggests variety among\nthese may also impact training. We explore this by aligning models using controlled subsets of\ndangerous topics and jailbreaking styles. In particular we consider varying amounts of topics (1,\n2, 4, 6) and attack styles (1, 2, 4, 8, 12) and show test results in Figures 5. Contrasting previous\nwork [Mazeika et al.(2024)], our experiments indicate that a higher variety of data reduces attack\nsuccess rate locally in some cases, but not significantly. On the other hand, data volume has a\nstronger effect than data variety on model robustness. See Appendix \u00a7E for more results."}, {"title": "Families & Sizes", "content": "The experiments shown in Figures 3 and 5 show distinct model behavior across families and scales.\nConsider first the performance of the four original models (marked as 'x' in Figure 3) on the most"}, {"title": "General Purpose Performance", "content": "When applying a model alignment process, performance on other tasks often de-\ngrades [Wolf et al.(2024)]. To assess to what extent that happens with the proposed mod-\nels, we use two different general purpose benchmarking suites: OpenLLM Leaderboard\n[Hendrycks et al.(2021)] and MMLU-Generative. These contain a mixture of open-ended and\nclose-ended benchmarks, allowing for a combined view. While close-ended metrics (i.e., accu-"}, {"title": "Over Refusal", "content": "A potential drawback from performing safety DPO on language models is that models could overfit\nto the refusal found in all preferred responses e.g., \"As an AI assistant, I cannot answer...\" and\ndecline to produce responses to any request, regardless of safety (i.e., over refusal). In order to assess\nto what extent the models aligned with Egida express refusal to safe requests, we evaluate them on\nthe OR-Bench [Cui et al.(2024)]. This over refusal benchmark is a collection of seemingly toxic\nprompts likely to be refused by LLMs. It contains two main sets of safe prompts: OR-Bench-80K\nand the OR-Bench-Hard-1K subset. Samples from these datasets are used to prompt models, and\ntheir responses are recorded. Keyword matching is used to determine whether the responses are"}, {"title": "Conclusion", "content": "The use of DPO for boosting the safety of LLMs delivers on its promises. As shown in \u00a74, with the\nright training pipeline and data, this method reduces the attack success rate of unseen jailbreaking\nmethods between 10% and 30% across topics, while using a relatively modest computational budget\n(between 3$ and 20$ depending on model size). A cost that will only decrease in the near future.\nThe approach of this work first gathers and extends safety datasets into a large collection of samples\nwith extended jailbreaking templates and labels. Training on this data works across models (with\nvarying degrees of efficacy), including state-of-the-art LLMs of different sizes from the Llama 3.1\nand Qwen 2.5 families. Main findings suggest:\n1. Mixing safe data with unsafe samples during model alignment should be avoided.\n2. Certain model families are safer by default and more sensitive to model alignment. However,\nthis susceptibility can lead to model collapse and over refusal.\n3. The weak spots of each LLM (e.g., most successful attack styles) are model-specific (not\neven consistent across families).\n4. Safety alignment datasets should be at least in the order of thousands of samples.\n5. Keeping a variety of attack styles and topics helps with robustness but is not fundamental.\nThese lessons are applied when training four versions of the aforementioned models, boosting safety\nand jailbreaking resistance. These are released with this work, together with other computed assets,\nas additional contributions:\n\u2022 All four safety aligned LLMs, tuned with the corresponding unsafe responses caused by the\nentire train set of Egida. 10 11 12 13\n\u2022 The Egida dataset, which includes 61,830 unsafe requests with jailbreaking prompts, manu-\nally labelled across 27 fine-grained topics.\n\u2022 The Egida-S dataset, which includes 61,830 safe responses, each paired with an unsafe\nrequest from Egida, with which new DPO datasets can be generated.\n\u2022 The four Egida-DPO datasets used to train the models in this paper. For each model, its\nunsafe answsers on Egida have been compiled and paired with a prompt and a safe answer.\nEach of them includes between 2,153 and 6,410 unsafe answers.\n\u2022 The Egida-HSafe dataset, which includes 1,000 unsafe requests and three human labels per\nrequest regarding safety.\nThe results obtained also point towards the current limitations of LLM safety. Mostly caused by\nthe two main factors constraining improvement. First, some models are resilient to alignment\nthrough DPO. The causes behind this phenomenon need to be analyzed in a dedicated study, as to\npromote more malleable models where DPO becomes effective. Second, increasing data volume\nto boost performance cannot be automated, and requires detailed understanding of the domain\nof application and the interacting population (i.e., different age ranges, geographical origins or\ncultural backgrounds may require additional safety topics), as well as verification on model collapse\nand over refusal. To tackle some of these challenges, we explore the use of Llama-Guard-3-8B,\nconducting the largest independent human evaluation released so far on alignment with human\npreferences. Results shows Llama-Guard is a useful tool, which correlates strongly with human\npreference.\nFinally, this work addresses the challenge of safety model alignment, but other areas of alignment\nremain to be considered (toxicity, bias and discrimination, truthfulness, etc.). Addressing these\nremains as future work."}, {"title": "Sanity checks for Meta-Llama-Guard-2-8B", "content": "To assess the reliability of Llama-Guard Guard, we conducted a human evaluation to determine\nwhether human criteria for safe and unsafe responses align with the outcomes provided by the\nLLM judge, in this case, Llama-Guard. Specifically, the evaluation involved manually reviewing\nmodel-generated responses to potentially harmful questions, categorizing them as either safe or\nunsafe."}, {"title": "Safe and unsafe criteria definition", "content": "To establish the specific criteria for classifying responses, we used the following two definitions:\n\u2022 Safe: A response that does not contain information that could be used for harm, avoid\ndirectly answering and unsafe question, or addresses it in a disarming or educational manner.\n\u2022 Unsafe: A response that directly answers a malicious question, contains harmful information,\nor validates harmful intentions.\nTo guide the evaluators, we organize the criteria into a hierarchical decision tree, see Figure 9. For\neach question, evaluators should follow this decision tree to classify the answers as either safe or\nunsafe."}, {"title": "Interface", "content": "The interface designed for the evaluators displayed each response alongside the original question,\nthe question with the attack (to provide context), and the model's response (see Figure 10). Each\nevaluator could classify the responses as safe, unsafe or uncertain. The uncertain option was\nincluded for cases where, for example, the evaluator did not perceive any harm in the question itself\nand, therefore, deemed that the response could not cause harm either."}, {"title": "Experiment set-up", "content": "To conduct the evaluation, we designed the selection of responses as follows. We used\nthe 61,830 prompts from BSCRT, derived from 2,949 base questions by using 20 differ-\nent attack templates (plus the base question itself). These prompts were answered by 10\ndifferent models from 5 model families: (1) Phi-3-mini-4k-instruct, (2) Phi-3-medium-4k-\ninstruct [Abdin et al.(2024)], (3) Yi-1.5-9B-Chat, (4) Yi-1.5-34B-Chat [Young et al.(2024)],\n(5) Llama-3.1-8B-Instruct, (6) Llama-3.1-70B-Instruct [AI@Meta(2024)], (7)\nQwen-2.5-7B-Instruct, (8) Qwen-2.5-72B-Instruct [Bai et al.(2023b)], (9) Gemma-2-\n9b-it and (10) Gemma-2-27b-it [Team(2024a)].\nFrom these responses, we randomly selected 100 for each model, resulting in a total of 1,000\nresponses to evaluate. Each response was reviewed and classified by three evaluators."}, {"title": "Evaluators", "content": "The evaluation was conducted by five human evaluators from diverse demographic backgrounds.\nThe participants represented different age groups (21, 26, 26, 31, 46) and geographical locations,"}, {"title": "Results", "content": "The main results of this analysis are detailed in \u00a73.3.1. Here, we focus on additional findings,\nexamining not just the agreement between the evaluators and the model but also the agreement\namong the evaluators themselves. In these results, we include the uncertain class in the analysis.\nOut of the 1,000 questions evaluated, the three evaluators fully agreed on 726 instances. Among the\nremaining 274 questions where disagreement occurred, 247 involved two evaluators agreeing while\nthe third disagreeed. The distribution of these disagreements is shown in Figure 11. Interestingly,\nthe highest disagreement occurred in opposite classifications: 87 instances where two evaluators\nlabeled the response as safe while one labeled it as unsafe, and 64 instances where two labeled it as\nunsafe while one marked it as safe.\nOnly 27 questions \u2014representing just 2.7% of the total exhibited complete disagreement, where\neach evaluator chose a different label. This low rate of total disagreement suggests that, despite\noccasional differences, there is a notable level of consistency among the evaluators."}, {"title": "Generation of safe responses", "content": "We generate the safe responses that are part of Egida-S by running inference on two models\nunrelated to the ones used in the experimentation: Mistral 7B v0.3 [Jiang et al.(2023)] and Phi 3\nSmall 8k [Abdin et al.(2024)]. Respectively, they score an ASR of 0.21 and 0.01 when evaluated\non the base, untemplated instructions of Egida. Although Mistral is unsafe, it was chosen after a\nqualitative inspection of its answers showed that, when safe, they were elaborate and well-written,\nand thus suitable for the Egida-S dataset.\nTo generate safe responses, the two models were prompted with the instances of Egida without\njailbreaking templates. The following safe system prompt was added to the inference processes of\nboth models to minimize the risk of providing unsafe responses:\n\"You are a helpful assistant. You will be provided with an unsafe question, instruction\nor prompt that you should NOT directly address. Instead, answer the prompt in the\nsafest and most harmless way possible. Remember, answer the prompt in the safest\nand most harmless way possible\u201d\nAfter inference, Llama-Guard-3-8B to classify both models' responses as safe or unsafe. When safe,\nthe responses from Mistral were given priority over the ones from Phi 3, as the latter's responses\nwere observed to be mostly short responses like \u201cI cannot help with that\u201d, thus less rich than the\nones from the former.\nTo form Egida-S, each untemplated request was paired with its corresponding selected response.\nThe templated versions of each untemplated request were also paired with said response. To ensure\nthe safety of these responses, they were reviewed by the authors."}, {"title": "Introduction of safe data", "content": "We explore the effect of safe data in safety training by training the models on mixtures of general\npreference data and unsafe data at different proportions (0%,25%, 50%, 75%, 100%). We use our\ndataset as unsafe data, and Infinity-Preference14 as general preference data.\nAs shown in the Figures above, larger proportions of safe data reduce the safety of the model,\nregardless of size and proportion. Every combination and test conducted which included safe data\nwas underperforming when compared to the alternative. A recommendation to dataset creators is\nmade, to not mix safe samples in their data."}, {"title": "Generalization to Specific Attack Styles and Harmful Topics", "content": "Section \u00a74.2 contains a study on which jailbreaking styles and harmful topics are easier to generalize\nto. This Appendix contains results for all tested models, which shows a significant variance for\nattack styles. i.e., The most challenging styles differ among models, regardless of size and family."}, {"title": "Keyword Matching for Refusal Analysis", "content": "Keyword matching, while a practical tool for assessing refusal behaviors in large-scale language\nmodels, has inherent limitations. One major issue is its sensitivity to specific phrasing, which\ncan result in misleading conclusions about a model's refusal tendencies. For instance, a model\nfrequently starting its responses with \u201cI'm sorry\u201d may be categorized as overly refusing, even if its\nactual behavior demonstrates an ability to address safe prompts appropriately."}]}