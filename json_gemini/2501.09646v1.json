{"title": "NS-GYM: OPEN-SOURCE SIMULATION ENVIRONMENTS AND BENCHMARKS FOR NON-STATIONARY MARKOV DECISION PROCESSES", "authors": ["Nathaniel S. Keplinger", "Baiting Luo", "Iliyas Bektas", "Yunuo Zhang", "Kyle Hollins Wray", "Aron Laszka", "Abhishek Dubey", "Ayan Mukhopadhyay"], "abstract": "In many real-world applications, agents must make sequential decisions in environments where conditions are subject to change due to various exogenous factors. These non-stationary environments pose significant challenges to traditional decision-making models, which typically assume stationary dynamics. Non-stationary Markov decision processes (NS-MDPs) offer a framework to model and solve decision problems under such changing conditions. However, the lack of standardized benchmarks and simulation tools has hindered systematic evaluation and advance in this field. We present NS-Gym, the first simulation toolkit designed explicitly for NS-MDPs, integrated within the popular Gymnasium framework. In NS-Gym, we segregate the evolution of the environmental parameters that characterize non-stationarity from the agent's decision-making module, allowing for modular and flexible adaptations to dynamic environments. We review prior work in this domain and present a toolkit encapsulating key problem characteristics and types in NS-MDPs. This toolkit is the first effort to develop a set of standardized interfaces and benchmark problems to enable consistent and reproducible evaluation of algorithms under non-stationary conditions. We also benchmark six algorithmic approaches from prior work on NS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to assess the adaptability and robustness of their decision-making algorithms to non-stationary conditions. NS-Gym can be downloaded from: https://github.com/scope-lab-vu/ns_gym", "sections": [{"title": "Introduction", "content": "Many real-world problems involve agents making sequential decisions over time under exogenous sources of uncer-\ntainty. Such problems exist in autonomous driving [Kiran et al., 2021], medical diagnosis and treatment [Yu et al.,\n2021], emergency response [Mukhopadhyay et al., 2022], vehicle routing [Li et al., 2021], and financial portfolio\noptimization [Pendharkar and Cusatis, 2018]. We define an agent as an entity capable of computation that acts based on\nobservations from the environment [Kochenderfer et al., 2022]. Decision-making for such agents is widely modeled by\nMarkov decision processes (MDPs), a general mathematical model for stochastic control processes.\nA canonical challenge in such problems, motivated by practical scenarios, is non-stationarity, where the distribution\nof environmental conditions can change over time. While non-stationarity has been well-explored from both control\nand decision-theoretic perspectives, several conceptual paradigms of non-stationarity exist, which lead to different\nmathematical formalisms for how the environmental parameters change and how the agent (and the control process)\ninteracts with the changes. Ackerson and Fu [1970] provide one of the earliest conceptualizations of a system operating\nin \"switching\" environments, where the mean and covariance of the underlying process can change over time. Campo\net al. [1991] formalize the switching process, where some environment parameters can change after a random sojourn\ntime, as a sojourn-time-dependent Markov chain, which is semi-Markovian.\nRecent investigations of non-stationary stochastic control processes involve two major threads: the first problem deals\nwith an agent trying to adapt to a single change in the environment (which can either be observed Pettet et al. [2024]\nor unobserved [Luo et al., 2024]); and the second problem models situations where environmental parameters can\nchange continuously over time [Lecarpentier and Rachelson, 2019]. In an orthogonal line of work, Chandak et al.\n[2020b] present a problem formulation where the agent's goal is to maximize a forecast of future performance (of the\ncontrol policy) instead of directly modeling the non-stationarity. Notably, these problem classes provide fundamentally\ndifferent formalisms (or treatments) for non-stationarity.\nIndeed, not only are the formalisms different, but we point out another interesting observation from prior work on\nnon-stationary stochastic control processes: while recent prior work on stationary Markov decision processes (MDP)\nuse standard benchmark problems, e.g., by using the popular Gymnasium toolkit [Towers et al., 2023], there are\nno standard problems or benchmarks for non-stationary MDPs. For example, Lecarpentier and Rachelson [2019]\nevaluate non-stationarity using a custom non-stationary bridge environment (an abstract problem where an agent must\nnavigate on a grid-based slippery maze where the properties of the surface change over time), Chandak et al. [2020b]\nuse problems motivated by real-world applications such as recommendation systems and diabetes treatment, and\nPettet et al. [2024] use well-known benchmark problems used for stationary MDPs (e.g., the cartpole problem from\nGymnasium Towers et al. [2023]) and introduce non-stationarity manually.\nIn this paper, we identify key characteristics of non-stationary MDPs that affect decision-making, review prior work in\nthis area, and present the first simulation toolkit specifically tailored for non-stationary MDPs. We argue that four key\nconsiderations affect decision-making in non-stationary MDPs, where environmental factors can change over time:\nwhat changes? how does it change? can the agent detect the change? can the agent know the updated parameter that has\nchanged? These questions summarize the nature of the change and the key properties of modeling approaches from\nprior work. Based on these questions, we present NS-Gym (Non-Stationary Gym), the first collection of simulation\nenvironments for non-stationary MDPs. Inspired by the seminal work of Campo et al. [1991], we segregate the evolution\nof the environmental parameters that characterize non-stationarity and the agent's decision-making module. This\nmodularization enables us to configure various components (and types) of non-stationary MDPs seamlessly. The\nNS-Gym toolkit is based on Python and is completely compatible with the widely popular Gymnasium framework.\nInstead of developing a new simulation environment from scratch, we build upon the existing Gymnasium toolkit due to\nits popularity and ensure that the large user base already familiar with Gymnasium can easily use NS-Gym (we keep all\nstandard Gymnasium functionalities and interfaces intact). Specifically, we make the following contributions. We make\nthe following contributions:\n1. We present the first simulation toolkit for NS-MDPs that provides a tailored, standardized, and principled set of\ninterfaces for non-stationary environments.\n2. We identify canonical problem instances for decision-making in non-stationary environments, e.g., decision-making\nwhere the agent knows about the change but is not aware of exactly what the change is, or decision-making where the\nagent is aware of the change.\n3. We present an overview of prior work on non-stationary decision-theoretic models and a programming interface that\nunifies prior work.\n4. Our simulation framework extends the widely popular Gymnasium toolkit, thereby requiring minimal added efforts\nfrom researchers in online planning, reinforcement learning, and decision-making in using our toolkit."}, {"title": "Characteristics of NS-MDPs and Prior Work", "content": "We begin by describing a comprehensive framework for decision-making in non-stationary environments. Admittedly,\nwe point out that the conceptual boundaries of what constitutes an agent are unclear in this context. Instead, we leave\nthis question open and point out the key components relevant to decision-making; whether these components are part of\nthe agent or those supporting the agent is orthogonal to our discussion.\nWe refer to an agent as an entity that receives observations from an environment and can act or make decisions that\ninteract with said environment. For simplicity, we assume a discrete-time process, although this discussion also extends\nto continuous-time stochastic control processes. Our fundamental model is that of a Markov decision process [Puterman,\n2014]. We refer to the current state of the environment by $s \\in S$ and an action by $a \\in A$, where $S$ and $A$ denote the\nset of all states and actions, respectively. After taking an action: 1) the agent receives a scalar signal $r(s, a)$ from the\nenvironment, which can be perceived as a reward or a loss and is a measure of the agent's utility, and 2) the agent\ntransitions to a new state, governed by a transition function $P(s' | s, a, \\theta)$, where $\\theta \\in \\Theta$ denotes a set of observable\nenvironmental parameters. We argue that explicitly specifying $\\theta$ is critical to modeling non-stationary decision-making\nproblems, as highlighted below.\nWe show a schematic of the major decision-theoretic components in Figure 1. In a non-stationary stochastic control\nprocess, the environmental parameters $\\theta$ or the agent's utility function $r(s, a)$ can change over time. The manner\nin which the change evolves over time can be modeled by a Markov chain or, more generally, by a semi-Markov\nchain as proposed by Campo et al. [1991]. While this formalism has often not been used in recent work (which has\nfocused less on the statistical properties of the changes), we argue that a formal representation of how the environmental\nparameters evolve is particularly important from the perspective of studying NS-MDPs. We use the same high-level\nformalism as Campo et al. [1991], i.e., the parameters $\\theta$ evolve in time through a sojourn time distribution, which\ncan be non-memoryless, thereby making the resulting stochastic process semi-Markovian Hu and Yue [2007]. If the\nsojourn-time distribution is memoryless, then the resulting process is a continuous-time Markov chain Hu and Yue\n[2007]."}, {"title": "Framework Description", "content": "In this section, we outline the general structure of NS-Gym, elaborate upon our design decisions, and describe the\ngeneral experimental pipeline using NS-Gym. The project's source code can be found at https://github.com/\nscope-lab-vu/ns_gym.\nThe environment object in Gymnasium encapsulates an MDP, providing a set of states and possible actions and defining\nhow actions influence state transitions and rewards. The observation object represents the current state information\navailable to the agent. Additionally, Info object is a dictionary containing auxiliary diagnostic information beneficial for"}, {"title": "Problem Types and Notifications", "content": "A key feature of the NS-Gym library is its ability to manage the interaction between the environment and the decision-\nmaking agent. These interactions encapsulate the following problem types, which we explain using the Frozen Lake\nenvironment. Consider the problem setting in the Frozen Lake environment where the agent's probability of going in its\nindented direction is $\\theta_1$ in the base environment. Now, the lake becomes more slippery, and this probability changes to\n$\\theta_2$. We model the following settings.\n1. Problems where the agent receives a message that the extent to which the lake is slippery has changed (corresponding\nto a successful anomaly detection), but it is unaware of the exact change (i.e., it does not know $\\theta_2$). This setting is\nmotivated by prior work by Luo et al. [2024]).\n2. Problems where the user is aware of the exact environmental change, i.e., it knows $\\theta_2$. However, in non-stationary\nsettings, the agent might not have time to train a new policy from scratch. This setting is motivated by prior work by\nPettet et al. [2024] and Lecarpentier and Rachelson [2019].\n3. Problems where the agent is not notified about the change, i.e., it is unaware that the probability is no longer $\\theta_1$.\nThis setting is motivated by prior work by Garivier and Moulines [2011].\n4. In an orthogonal thread, we identify the frequency of the change, i.e., problems with a single change in an\nenvironment variable [Luo et al., 2024, Pettet et al., 2024] (e.g., the change is from $\\theta_1$ to $\\theta_2$) or multiple changes within\nan episode [Cheung et al., 2020] (e.g., the change is $\\theta_1 \\rightarrow \\theta_2 \\rightarrow \\theta_3 \\rightarrow . . .$) or changes within multiple episodes Chandak\net al. [2020b].\nUsers can configure notifications the agent receives about changes in the NS-MDP at three distinct levels:\n1. Basic Notification: The agent receives a boolean flag indicating a change in an environment parameter.\n2. Detailed Notification: In addition to the boolean flag, the agent is informed of the magnitude of the change.\n3. Full Environment Model: Additionally, if the agent requires an environmental model for planning purposes (such\nas in Monte Carlo tree search), NS-Gym can provide a stationary snapshot of the environment. This snapshot aligns with\nthe basic or detailed notification settings configured by the user. If the user seeks a model without detailed notification,\nthe planning environment is a stationary snapshot of the base environment. Conversely, if detailed notifications are\nenabled, the agent receives the most up-to-date version of the environment model (but not any future evolutions)."}, {"title": "Custom Observation for NS-MDPS", "content": "In building on top of Gymnasium, users familiar with the existing Gymnasium API can easily adapt to NS-Gym with\nminor modifications. Like Gymnasium, the agent-environment interaction consists of a sequence of steps where, at each\nstep, the agent receives an observation and reward. In NS-Gym, we return custom observation and reward data types"}, {"title": "Schedulers and Parameter Update Functions", "content": "We recognize that users may need to model non-stationarity differently depending on the specific problem settings. To\naccommodate this, NS-Gym allows users to specify which parameters change, when they change, and how they change\nthrough \"schedulers\" and parameter \u201cupdate functions.\u201d We decouple the timing (and thereby, the frequency) and the\nmanner of parameter changes, providing users with greater flexibility in designing experiments."}, {"title": "Experimental Pipeline", "content": "This section illustrates the straightforward integration of the NS-Gym with the typical Gymnasium training pipeline.\nThe general experimental setup procedure is: 1) Create a Standard Gymnasium Environment: Begin by making\na standard Gymnasium environment. 2) Define Parameters to Update: Identify which environmental parameters\nwill be updated during the experiment episode. 3) Map Parameters to Schedulers and Update Functions: Assign\neach parameter a scheduler and an update function. 4) Generate a Non-Stationary Environment: Pass the standard\nGymnasium environment, along with the parameter mappings and update functions, into the NS-Gym wrapper to create\na non-stationary Gymnasium-style environment.\nConsider that the user seeks to model a non-stationary environment in the classical CartPole environment, where the\npole's mass increases by 0.1 units at each time step, and the system's gravity increases through a random walk every\nthree time steps. Furthermore, we want the decision-making agent to have a basic notification level. The following code\nsnippet shows the general experimental setup in this CartPole Gymnasium environment using NS-Gym."}, {"title": "Non-Stationary Environment Details", "content": "Below, we describe environments included in NS-Gym and how we induce non-stationarity. We focus on observable\nparameters $\\theta$ here (available to the NS-Gym wrapper) and present descriptions of the environments in the appendix."}, {"title": "Benchmark Experiments", "content": "In this section, we demonstrate the utility of this package by evaluating decision-making algorithms in environments\nbuilt using the NS-Gym library. Our experimental setup is designed to assess agent performance across multiple\ndimensions, providing insights into which decision-making agents are best suited for practical challenges. Consider\na system modeled as a known MDP, where an exogenous force induces changes in the MDP's transition function.\nSpecifically, we seek to explore the following questions: how effectively can an agent adapt when this change is known\nor unknown? What if the system undergoes continuous evolution? How well can an agent handle frequent updates?\nWe benchmark six algorithms across four base environments. We consider settings where the MDP transition function\nchanges at a single discrete instance and for cases in which the transition function changes from some continuous\nsequence of time steps. Additionally, for each environment and agent pair, we consider instances with no notification"}, {"title": "Baseline Algorithms", "content": "We evaluate the non-stationary environment across six different decision-making agents: Monte Carlo tree search\n(MCTS), double deep Q learning (DDQN), AlphaZero, adaptive Monte Carlo tree search, risk-averse tree search\n(RATS), and policy-augmented Monte Carlo tree search (PA-MCTS). Note that our work is the first effort to benchmark\napproaches for tackling non-stationarity on standardized problem settings. We briefly describe the benchmark\napproaches below. For all environments, we provide the algorithms that require a model of the environment with a\nstationary snapshot of the model for planning according to the appropriate notification level.\n1) MCTS is an anytime online search algorithm that uses a model of the environment to select optimal actions. We use\nthe Upper Confidence bound for Trees (UTC) algorithm [Kocsis and Szepesv\u00e1ri, 2006] with random rollouts.\n2) The AlphaZero algorithm [Silver et al., 2017] is a general game-playing algorithm that combines tree search with a\ndeep value and policy neural network. The policy network is learned through self-play. We train the AlphaZero policy\nnetwork on a stationary version and the environment but evaluate the agent on an NS-MDP. At each decision epoch the\nAlphaZero agent receives an environment model for planning at the appropriate notification level.\n3) We include the widely popular DDQN approach as a pure reinforcement learning method [van Hasselt et al., 2015].\nIn the \"with notification\u201d experiments, we let the DDQN do some gradient update steps using the most up-to-date model\nof the MDP (to resemble the baseline setting used by [Pettet et al., 2024]).\n4) ADA-MCTS as a heuristic tree search algorithm that learns the environmental dynamics and acts as it learns Luo\net al. [2024]. ADA-MCTS uses a risk-averse strategy to explore the environment safely by balancing epistemic and\naleatoric uncertainties. In our experiments, we only benchmarked ADA-MCTS when the updated environmental\nparameters are unavailable, as its core lies in learning about the updated change through environmental interactions.\n5) The RATS algorithm proposed by Lecarpentier and Rachelson [2019] uses a minimax search strategy to act in a\nrisk-averse manner to future environmental changes. The approach was originally designed against changes bounded by\nLipschitz continuity.\n6) We benchmark the Policy-Augmented-MCTS algorithm from Pettet et al. [2024], which computes a convex\ncombination of returns generated through online search and a stale policy. Crucially, this combination occurs outside\nthe tree (as opposed to the AlphaZero algorithm). Using the estimates outside the tree stabilizes the search under\nnon-stationarity and has faster convergence. We consider PAMCTS performance across three $\\alpha$ values, 0.25, 0.5, and\n0.75, which control the extent to which the stale policy is preferred over online search."}, {"title": "Results", "content": "Table 3 shows results from the single change experiments without notifications, and Table 4 reports agent performance\nin the continuous experiment setting with and without notification. We provide a complete table of experimental results\nand figures in the supplemental materials. Building on the unified design of NS-Gym and the benchmark results, we\nhave derived some key insights about how different strategies perform under varying conditions. This analysis provides\na clearer understanding of how algorithms respond to dynamic environmental changes.\nImpact of Detailed Notification on Performance with Single Transition Change: The presence of detailed notifica-\ntions generally enhances the performance of most methods. AlphaZero, MCTS, PA-MCTS, and RATS demonstrate\nmarked improvements when notifications are available in some environments, effectively leveraging the most up-to-date\ndynamics to optimize decision-making processes. In contrast, DDQN shows only a modest improvement as it is difficult\nto adapt to changes in limited time.\nImpact of Notification on Performance with continuous Transition Change: Again, the presence of detailed\nnotifications generally improves the performance of most methods across various environments. This highlights the\nimportance of quickly adapting the planning model to the latest dynamics of the environment. For example, methods"}, {"title": "Conclusion", "content": "We present NS-Gym, the first simulation toolkit and set of standardized problem instances and interfaces explicitly\ndesigned for NS-MDPs. NS-Gym incorporates problem types and features from over fifty years of research in non-\nstationary decision-making. We also present benchmark results using prior work. We will continue to maintain NS-Gym,\nextend it, and maintain a leaderboard of approaches."}, {"title": "Description of NS-Gym Environments", "content": "Below, we provide descriptions for each environment supported by NS-Gym."}, {"title": "CartPole", "content": "The CartPole environment has a discrete action space and a continuous state space. As illustrated in Figure 3, the\nagent's objective is to keep the pole balanced on top of the cart for as long as possible. The agent receives a reward\nof +1 for each time step that the pole remains balanced. The state is represented by a four-dimensional vector, which\nincludes the cart's position, cart's velocity, pole's angle, and pole's angular velocity. At each time step, the agent can\napply a fixed force to push the cart either left or right."}, {"title": "Mountain Car", "content": "The MountainCar environment (see Figure 4) is a continuous state but discrete action space environment. In this\nenvironment, a car is stuck in a valley, and the agent must apply force to the cart to build momentum so that the car can"}, {"title": "Acrobot", "content": "The Acrobot environment is a double pendulum (see Figure 5). The agent can apply torque to the joint connecting the\ntwo links of the double pendulum to move the free end above a threshold height. At each time step, the agent can either\napply +1, 0, or -1 units of torque."}, {"title": "Pendulum", "content": "The Pendulum environment is a continuous state and action space environment. The agent aims to keep the pendulum\ninverted for as long as possible. The agent receives a reward proportional to the pendulum's angle. At each time step,\nthe agent applies some torque magnitude to the pendulum's free end."}, {"title": "Frozen Lake", "content": "The FrozenLake environment (Figure 7) is a stochastic, discrete action, and discrete state space grid-world environment.\nThe agent navigates from a starting cell in the top left corner of the map to a jail cell in the bottom right corner while\navoiding holes in the \"frozen lake.\" The agent can move in an intended direction, with some probability that it will\nmove in a perpendicular direction instead. The Agent will get a reward of +1 if it reaches the goal and 0 otherwise."}, {"title": "CliffWalker", "content": "The CliffWalking environment (Figure 8) is a deterministic grid-world environment. The agent must navigate from the\nstart to the goal cell in the fewest steps. If the agent falls off a \"cliff,\" it accrues a reward of -100 and resets at the start\ncell without ending the episode. The agent accrues -1 reward for each cell that is not a cliff or a goal state. The goal cell\nis the only terminal state. The agent can move up, down, left, and right."}, {"title": "Bridge", "content": "The non-stationary bridge environment (Figure 9) is a grid-world setting where the agent must navigate from the starting\ncell to one of two goal cells. The environment was originally introduced by Lecarpentier and Rachelson [2019]. To\nreach a goal cell, the agent must cross a \u201cbridge\u201d surrounded by terminal cells. The secondary goal cell is farther\nfrom the starting location but less risky because fewer holes surround it. Unlike the CliffWalking environment, which\nhas a single global transition probability, the left and right halves of the Bridge map each have separate probability\ndistributions. NS-Gym allows for updates to just the left or right halves of the map or to the global value. Similar to the\nFrozenLake environment, if the agent moves in some direction, there is some probability that is moves in one of the\nperpendicular directions instead. The agent receives a +1 reward for reaching a goal cell, a -1 reward for falling into a\nhole, and a 0 reward otherwise. Our version of the non-stationary bridge environment is not included in the standard\nGymnasium Python package. We provide our implementation of the Bridge environment, as described by Lecarpentier\nand Rachelson [2019], as part of the NS-Gym package."}, {"title": "Experimental Setup", "content": "In this section, we elaborate on how we set up the single and continuous change experiments for each environment."}, {"title": "CartPole", "content": "\u2022 Single update case: We initialize the CartPole environment to its default state. After the first decision epoch,\nwe increase the mass of the pole from 0.1 to a value of 1.0 and 1.5.\n\u2022 Continuous update case: We initialize the CartPole environment to its default state. After each decision\nepoch, we increase the mass of the pole by 0.1.\nWe truncate the episode after 2500 episode steps if the agent does not reach a terminal state."}, {"title": "Frozen Lake", "content": "\u2022 Single update case: We initially set the probability of moving in the intended direction to 0.7 and the\nprobability of moving in each perpendicular direction to 0.15. After the first decision epoch, we change the\nprobability of moving in the intended direction to 0.4, 0.6, or 0.8. We update the chance of moving in a\nperpendicular direction accordingly.\n\u2022 Continuous update case: We initialize the FrozenLake environment to be completely deterministic. We\ndecrease the chance of moving in the intended direction by 0.2 for the first three decision epochs. We update\nthe chance of moving in a perpendicular direction accordingly.\nWe truncate the episode after 100 episode steps if the agent does not reach a terminal state."}, {"title": "Cliff Walking", "content": "\u2022 Single update case: We initialize the environment to be determenistic. After the first decision epoch, we\nupdate the transition probability to a value of 0.8, 0.6, or 0.4. The probability of moving in the perpendicular\nand reverse directions is updated accordingly.\n\u2022 Continuous update case: We initialize the environment to be deterministic. For the first 10 decision epochs,\nwe decrease the chance of moving in the intended direction by 0.02. The probabilities of moving in the\nperpendicular and reverse directions are updated accordingly.\nIn our experimental setup we modify the standard CliffWalking rewards so that the goal state has a reward of +100.\nAdditionally, after 200 decision epochs, if the agent has not found the goal, we truncate the episode."}, {"title": "Bridge", "content": "\u2022 Single update case: We initially set the probability of moving in the intended direction to 0.7 and the\nprobability of moving in each of the perpendicular directions to 0.15. After the first decision epoch, we change\nthe probability of moving in the intended direction to a value of 0.4, 0.6, or 0.8. We update the chance of\nmoving in a perpendicular direction accordingly.\n\u2022 Continuous update case: We initialize the environment to be determenistic. At each decision epoch, the\nprobability of going in the intended direction decreases by 0.1.\nWe truncate the episode after 200 steps if the agent does not reach a terminal state."}, {"title": "Algorithm Parameters", "content": "The Tables 5, 6, 7, 8, 9, and 10 show the parameters used in each experiment."}]}