{"title": "FR\u00c9CHET REGRESSION FOR MULTI-LABEL FEATURE SELECTION WITH\nIMPLICIT REGULARIZATION", "authors": ["DOU EL KEFEL MANSOURI", "SEIF-EDDINE BENKABOU", "KHALID BENABDESLEM"], "abstract": "Fr\u00e9chet regression extends linear regression to model complex responses\nin metric spaces, making it particularly relevant for multi-label regression, where each instance can have multiple associated labels. However, variable selection within this framework remains underexplored. In this paper, we pro-\npose a novel variable selection method that employs implicit regularization instead of traditional explicit regularization approaches, which can introduce bias. Our method effectively captures nonlinear interactions between predic- tors and responses while promoting model sparsity. We provide theoretical results demonstrating selection consistency and illustrate the performance of our approach through numerical examples.", "sections": [{"title": "Introduction", "content": "Fr\u00e9chet regression, an extension of classical linear regression to general metric spaces, offers a robust framework for modeling complex relationships between variables when the responses lie outside of Euclidean spaces. This approach is especially well suited to high-dimensional datasets, such as vector representations, with particular relevance to fields like imaging, where capturing nonlinear dependencies and the intrinsic data structure is critical for accurate modeling (Fr\u00e9chet (1948), Petersen and M\u00fcller (2019), Bhattacharjee and M\u00fcller (2023), Qiu, Yu and Zhu (2024)). A significant consideration in Fr\u00e9chet regression arises when predicting multiple responses simultaneously, as seen in multi-target or mul-tidimensional problems (Zhang and Zhou (2007), Hyv\u00f6nen, J\u00e4\u00e4saari and Roos (2024)). Unlike traditional regression, where each observation corresponds to a single response, Fr\u00e9chet regression can be extended to model complex interactions between multiple outputs. This ability to address complex relationships between several responses opens new avenues, particularly in fields such as bioinformatics (Huang et al. (2005)) and image analysis (Lathuili\u00e8re et al. (2019)), where multidimensional data and interdependencies between responses require adaptive and specialized methodologies. However, to date, the handling of multilabel scenar- ios within the context of Fr\u00e9chet regression remains relatively unexplored in the literature, despite its potential significance in addressing complex, multidimensional applications.\nIn this paper, we present an extension of the Global Fr\u00e9chet regression model, a specific variant of Fr\u00e9chet regression that generalizes classical multiple linear regression by modeling responses as random objects. This extension enables the explicit modeling of relationships between input variables and multiple responses, thereby addressing the multi-label setting.\nOur second contribution in this paper addresses the dimensionality challenge in the con-text of the proposed Fr\u00e9chet regression extension. High-dimensional data (Zhang, Xue and"}, {"title": "Proposed Method.", "content": "We use boldface notation to represent vectors and matrices. Let (\u03a9, d) be a metric space with a specific metric d. We assume that \u03a9 is complete, which ensures the convergence of Cauchy sequences within this space. Let X \u2208 Rnxm denote the multivariate random variables, and let Y : \u03a9 \u2192 Rn\u00d7q be complex random objects. We consider a random process (X, Y) ~ F, with F denoting the joint distribution of (X, Y). Fx and Fy are the marginal distributions of X and Y, respectively. Fxy and Fyx are the con-ditional distributions that should be assumed to exist and to be well defined. Let \u00b5 = E(X) and \u2211 = Var(X). Let w\u2295 and V\u2295, the Fr\u00e9chet mean and Fr\u00e9chet variance of random ob-jects in metric spaces (Fr\u00e9chet (1948)), be defined as extensions of the traditional concepts of mean\nand variance:\n$W_\\Psi = \\underset{\\Psi \\in \\Omega}{arg \\text{ min }} E \\left(d^2(Y, w)\\right), V_{\\Psi} = E(d^2(Y, W_\\Psi)).$\nAccording to Petersen and M\u00fcller (2019), Fr\u00e9chet regression can be defined as:\n$m(x) = arg min = M(w,x), M(.,x) = E(d^2(Y, .)|X = x)$\nwhere, M(., x) is the (conditional) Fr\u00e9chet function."}, {"title": "Global Frechet regression.", "content": "Global Fr\u00e9chet regression is a special case of Fr\u00e9chet regression that extends classical multiple linear regression to accommodate responses that are random objects (Petersen and M\u00fcller (2019)). More precisely, the standard linear regression function can be interpreted as the solution of an optimization problem and is defined as follows:\n$m(x) = \\underset{\\Psi \\in \\Omega = \\mathbb{R}}{arg \\text{ min }} E(s(X,x)d_E(Y,y))$\nwhere, m(x) is the argument that minimizes E, and de is the standard Euclidean metric. s is the weight function chosen to fit the nature of the responses and may have different characteristics from the weights used in local regressions. It is defined as follows:\n$s(X, x) = 1 + (X \u2013 \u03bc)\u03a4\u03a3\u22121 (x \u2013 \u03bc)$\nReplacement of the Euclidean metric de with a more general metric d of & involves redefining the regression function as a minimiza-tion of a generalized loss function. Thus, Global Fr\u00e9chet regression can be defined as:\n$m_\\Omega(x) = arg min M(w,x), M(.,x) = E [s(X,x)d^2(Y,.)]$\nThis leads to\n$\\hat{m}_\\Omega(x) = arg min \\underset{\\Psi \\in \\Omega}{ \\Sigma_{i=1}^{n}} [1 + (x_i - \\hat{\\mu})^T \\hat{\\Sigma}^{-1} (x_i - \\hat{\\mu})^T] d^2(Y - w)$\nwhere $\\hat{X} = n^{-1} \\sum_{i=1}^{n} x_i$, and $\\hat{\\Sigma} = n^{-1} \\sum_{i=1}^{n} (x_i - \\hat{X})(x_i - \\hat{X})^T$", "subsections": [{"title": "Multilabel Global Fr\u00e9chet regression.", "content": "Equation (4), in its standard form, is not in-herently designed for the multilabel context, as it focuses primarily on a single response. In this paper, we introduce a new term into the same equation, enabling the explicit modeling of relationships between input variables and multiple responses, thus addressing the multilabel setting.\n$\\hat{\\zeta}_\\Omega(x) = arg min  \\underset{\\Psi \\in \\Omega}{ \\Sigma_{i=1}^{n}} [1 + (x_i - \\hat{X})^T \\hat{\\Sigma}^{-1} (x_i - \\hat{X})^T + (x_i - \\hat{X})^T \\hat{\\Sigma}_{xy} (Y_i - \\hat{Y})^T] d^2 (Y - w)$\nwhere $(x_i \u2013 \\hat{X})^T \\hat{\\Sigma}_{xy} (Y_i \u2013 \\hat{Y})^T$ is used to consider the cross-relationships between pre-dictors and responses.\nNext, we show that $\\hat{\\zeta}_\\Omega(x)$ is a consistent estimator, meaning it converges in probability to the true parameter wo as the sample size n tends to infinity. To establish this, we require that {(xi, Yi) 1} are independent and identically distributed and a linear relationship between X and Y exists, such that the residuals having zero expectation. We also assume that and Exy are positive definite and invertible. The Multilabel Global Fr\u00e9chet regression is detailed in Algorithm 1."}]}, {"title": "Rate of Convergence", "content": "(\\hat{\\zeta}_\\Omega(x) converges at a rate of $O_p(n^{-1/2})$\nTo evaluate the convergence rate, it is necessary to examine the deviation between f(w) and ftrue(w). Since $\\hat{\\Sigma} \\underset{n \\rightarrow +\\infty}{\\longrightarrow} \\Sigma$ and $\\hat{\\Sigma}_{xy} \\underset{n \\rightarrow +\\infty}{\\longrightarrow} \\Sigma_{xy}$, by the law of large numbers, we have:\n$||\\hat{\\Sigma} - \\Sigma|| = O_p(n^{-1})$\n$\\hat{\\Sigma}_{xy} - \\Sigma_{xy}|| = O_p(n^{-1})$\nThus,\n$||f(w) - f_{true}(w)|| = \\sum_{i=1}^{n} \\left[ (x_i - \\hat{X})^T (\\hat{\\Sigma}^{-1} - \\Sigma^{-1}) (x_i - \\hat{X})^T + (x_i - \\hat{X})^T (\\hat{\\Sigma}_{xy} - \\Sigma_{xy}) (Y_i - \\hat{Y}) \\right] d^2(Y - w)||\nLeveraging the properties of covariance matrices and considering that d\u00b2(Y \u2013 w) is a non-negative, continuous function bounded by a constant M, it follows that:\n$|| f (w) - ftrue(w)|| = O_p(n^{-1})$\nGiven the continuity of the functions f(w) and ftrue(w), as well as the fact that the error in the covariance matrices converges at a rate of $O_p(n^{-1})$, the error associated with the estimator (x) can be described by:"}, {"title": "Asymptotic Normality", "content": "$\\vert\\vert{\\hat{\\zeta}_\\Omega(x) - {\\zeta}_\\Omega(x)}\\vert\\vert = O_p(n^{-1/2}).$\nIn other words, if the covariance matrices converge at a rate of Op(n\u00af\u00b9), then the estimator (x) converges at a rate of Op(n-1/2), which is characteristic of asymptotically normal estimators.\n (Asymptotic Normality). (x) is asymptotically normal.\nUnder the assumption that there are sufficient regularity conditions on d\u00b2(Y \u2013 w) so that the function f(w) is well-behaved around the minimum and that the matrices 2-1, 2 are positive definite and invertible, while assuming that the condi-tional distributions Fx and Fy are well-defined and satisfy the usual regularity conditions, the consistency of the estimator is established in the proof 2.3. Therefore, we can now pro-ceed to the demonstration of the Normal Asymptotic.\nLet's use a Taylor expansion of the objective function around the true parameter (x).\n$f(w) \\approx f({\\hat{\\zeta}}_\\Omega(x)) + \\bigtriangledown f({\\hat{\\zeta}}_\\Omega(x))^T (w-{{\\hat{\\zeta}}_\\Omega(x)}) + \\frac{1}{2}(w-{{\\hat{\\zeta}}_\\Omega(x)})^T \\bigtriangledown^2 f({\\hat{\\zeta}}_\\Omega(x))(w - {\\hat{\\zeta}}_\\Omega(x)),$\nwhere $\\bigtriangledown f({\\hat{\\zeta}}_\\Omega(x))$ is the gradient and $\\bigtriangledown^2 f({\\hat{\\zeta}}_\\Omega(x))$ is the Hessian. At the optimum,$\\bigtriangledown f({{\\hat{\\zeta}}_\\Omega(x)}) = 0$. By applying the Taylor expansion, we obtain:\n$0 \\approx \\bigtriangledown f({{\\hat{\\zeta}}_\\Omega(x)}) + \\bigtriangledown^2 f({\\hat{\\zeta}}_\\Omega(x)) (\\hat{\\zeta}_\\Omega(x) - {\\zeta}_\\Omega(x)).$\nThis involves:\n$\\sqrt{n}({\\hat{\\zeta}}_\\Omega(x) - {\\zeta}_\\Omega(x)) \\approx -\\sqrt{n} \\bigtriangledown^2 f({\\hat{\\zeta}}_\\Omega(x))^{-1} \\bigtriangledown f({{\\hat{\\zeta}}_\\Omega(x)}).$\nBy the central limit theorem,\n$\\sqrt{n} \\bigtriangledown f({\\hat{\\zeta}}_\\Omega(x)) \\underset{n \\rightarrow +\\infty}{\\longrightarrow}  N(0, \\Sigma_{\\xi}),$\n$\\sqrt{n} (\\hat{\\zeta}_\\Omega(x) -{\\zeta}_\\Omega(x)) \\underset{n \\rightarrow +\\infty}{\\longrightarrow} N (0, \\bigtriangledown^2 f({\\hat{\\zeta}}_\\Omega(x))^{-1}\\Sigma_{\\xi} \\bigtriangledown^2 f({\\hat{\\zeta}}_\\Omega(x))^{-1}).$\nTherefore, (x) is asymptotically normal."}, {"title": "Variable Selection for Multilabel Global Fr\u00e9chet regression", "content": "Equation (5) is subject to two major challenges: noise in the data X and/or Y, as well as multicollinearity between variables. To address these two challenges, this paper proposes a variable selection method that relies on implicit regularization. The latter replaces traditional explicit regularization ap-proaches, which can introduce biases, and thus offers a more subtle and efficient alternative.\nThe Variable Selection for Multilabel Global Fr\u00e9chet regression can be defined as:\n$\\hat{\\Lambda}_{\\Omega}(x) = arg min  \\underset{\\Psi \\in \\Omega}{ \\sum_{i=1}^{n}} [1 + (x_i - \\hat{X})^T \\hat{\\Sigma}^{-1} (x_i - \\hat{X})^T + (x_i - \\hat{X})^T {\\Theta} (Y_i - \\hat{Y})^T] d^2(Y - w)$\nwhere, \u2295 is the component responsible for variables selection."}, {"title": "Multicollinearity and Impact of Noise", "content": "X and/or Y are noisy, as well as multicollinearity among the variables is present, implies that A+(x) is affected.\nNoise in X and/or Y disrupt the estimation of \u2211\u00af\u00b9. Consequently, this can lead to biased estimation, as the chosen model may end up fitting the noisy data and the inaccurately estimated covariance structure, rather than capturing the true underlying relationships. Furthermore, multicollinearity between variables leads to an ill-conditioned inverse covariance matrix, which can exacerbate noise problems by making estimates even more sensitive to fluctuations in the data. Direct denoising X and/or Y can be challenging due to the complex nature of noise. In contrast, denoising \u2211\u00af\u00b9 can often be more effective. In fact, \u03a3\u00af\u00b9 plays a crucial role in providing key information about the structure and dependencies of the data. Thus, improving \u03a3\u00af\u00b9 leads to better assessment and compen-sation of the effects of noise on the relationships between variables and also mitigate the effects of multicollinearity."}, {"title": "Invertibility, Sparsity and Asymptotic Normality", "content": "Replacing Exy with a new reduced matrix significantly improves A+(x).\nLet Exy be the estimated inverse covariance matrix and be a reduced approximation obtained via singular value decomposition (SVD). More precisely, we have Exy = USVT, where U and V are orthogonal matrices and S is a diagonal matrix containing the singular values. By applying a threshold 7 to filter out small singular values, we obtain a reduced approximation \u2609 = US*VT, where S* is a truncated version of S. This approach allows to simplify the matrix while preserving the most significant features of the original structure. We now prove the properties of invertibility, sparsity, and asymptotic normality.\n(i) Invertibility: To ensure the invertibility of while preserving its sparsity, we apply Tikhonov regularization, adding a term (\u2208 \u00d7 I) to S*, where I is the identity matrix and e is a very small positive number. Let S = diag(51,52,...,\u03c3\u03ba) be the matrix of singular values of xy and S* = diag(0,0,...,) be the truncated version of S. Applying Tikhonov regularization, we obtain:\n$\\text{S}^{\\*}+(\\epsilon \\times I) = diag(\\sigma_1 + \\epsilon, \\sigma_2 + \\epsilon, ..., 0 + \\epsilon).$\nSince \u20ac > 0 and all singular values o are positive, every element of S* + (\u0454 \u00d7 I) is strictly positive. Thus, the diagonal matrix S* + (\u0454 \u00d7 I) is positive definite. Consequently,"}, {"title": "Sparsity", "content": "Oreg = U (S* + (\u20ac \u00d7 I)VT is also invertible, thanks to the Tikhonov regularization which ensures that all eigenvalues are strictly positive, regardless of the initial truncation.\n(ii) Sparsity: The difference between\n$\\vert\\vert{\\hat{ \\Theta} - {\\hat{\\Sigma}_{xy}}^{-1}}\\vert\\vert_F = \\vert\\vert U \\tilde{S}^* V^T - U \\tilde{S} V^T \\vert\\vert_F.$\nUsing the Frobenius norm property for matrices, we have, $\\vert\\vert \\hat{ \\Theta} - {\\hat{\\Sigma}_{xy}}^{-1} \\vert\\vert_F = \\vert\\vert \\tilde{S}^* - \\tilde{S} \\vert\\vert_F$. Since $ \\tilde{S}^*$ is obtained by truncating the small singular values of $ \\tilde{S}$, the gap $\\vert\\vert \\tilde{S}^* - \\tilde{S} \\vert\\vert_F $ is dominated by the sum of the truncated singular values:$ \\vert\\vert \\tilde{S}^* - \\tilde{S} \\vert\\vert_F <\\sum_{i \\in \\tau}\\sigma_i$, where $ \\tau$ is the set of indices corresponding to the small truncated values. Thus, if $ \\epsilon $ is sufficiently small, the gap between $ \\hat{ \\Theta} $ and $ {\\hat{\\Sigma}_{xy}}^{-1} $ is relatively small, i.e.:$ \\vert\\vert \\hat{ \\Theta} - {\\hat{\\Sigma}_{xy}}^{-1} \\vert\\vert_F < \\epsilon $."}, {"title": "Asymptotic Normality", "content": "(iii) Asymptotic Normality: We show that the estimator A+(x) based on Oreg preserves its asymptotic normality. Assuming that A(x) is an estimator for a parameter A(x) with asymptotic variance \u03a3\u028c, we have,\n$\\sqrt{n}({\\Lambda}_{\\Omega}(x) - {\\Lambda}(x)) \\underset{n \\rightarrow +\\infty}{\\longrightarrow}  N(0, \\Sigma_{\\Lambda}).$\nUsing SVD decomposition and Tikhonov regularization, the gap between Oreg and Exy is controlled and thus does not compromise the asymptotic normality of the estimator. Let us formulate this precision:"}, {"title": "Multilabel Fr\u00e9chet Regression for Probability Distributions", "content": "The explanatory variables X are simulated from a multivariate nor-mal distribution, X ~ N (\u03bc\u03c7, \u03a3x), where \u03bcx \u2208 Rm represents the vector of variable means, defined by \u03bcx = [0, 1, \u22121, 2, 0]. The covariance matrix \u2211x \u2208 Rm\u00d7m is positive definite and invertible, constructed as the product of a randomly generated matrix and its transpose. The responses Y are modeled via Y ~ N(\u03b2(xi)1q, \u03a3\u04af), where \u03b2(xi) represent the conditional means of the response variables, defined as \u03b2(xi) ~ N(\u03b2\u03bf + \u03b4xi, v\u2081) for each observation xi. Here, Bo = 1 is a constant term, d = 0.5 is a coefficient modulating the impact of xi, and V\u2081 = 0.1 is the variance added to encapsulate uncertainty in the mean responses. Moreover, \u03a3\u04af \u2208 Rqxq is a symmetric, positive definite, and invertible matrix designed to capture inter-label correlations, thus preserving the inherent dependency structure between Y responses.\nThe Y responses are random objects evolving in a metric space \u03a9 equipped with theWasserstein distance. Their distribution is described by a quantile function Q(Y), represent-ing the distribution of Y responses in this space. The distance between two objects in this space is measured using the Wasserstein distance, defined for two distributions P and Q as follows:\n$\\partial_{\\varphi}^r(P,Q) = ||\\mu_p - \\mu_Q||^2 + Tr \\left( \\Sigma_P + \\Sigma_Q - 2 (\\Sigma_P^{1/2} \\Sigma_Q \\Sigma_P^{1/2})^{1/2} \\right)$."}, {"title": "Multilabel Fr\u00e9chet Regression with Spherical Data.", "content": "The explanatory variables X follow a multivariate normal distribu-tion X ~ N(\u03bc\u03c7, \u03a3x), with \u03bcx = [0,1,-1,2,0] and Ex being positive definite and invert-ible. These data are generated in spherical form, which means that the covariance matrix Ex is chosen to give an isotropic (spherical) distribution after transformation. The responses Y are modeled by Y ~ N(\u03b2(xi)1q, \u03a3\u04af), where \u03b2(xi) ~ N (\u03b2\u03bf + dxi, v1), with \u03b2\u03bf = 1, \u03b4 = 0.5"}]}