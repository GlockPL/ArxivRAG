{"title": "Can Large Language Models Understand Intermediate Representations?", "authors": ["Hailong Jiang", "Jianfeng Zhu", "Yao Wan", "Bo Fang", "Hongyu Zhang", "Ruoming Jin", "Qiang Guan"], "abstract": "Intermediate Representations (IRs) are essential in compiler design and program analysis, yet their comprehension by Large Language Models (LLMs) remains underexplored. This paper presents a pioneering empirical study to investigate the capabilities of LLMs, including GPT-4, GPT-3, Gemma 2, LLaMA 3.1, and Code Llama, in understanding IRs. We analyze their performance across four tasks: Control Flow Graph (CFG) reconstruction, decompilation, code summarization, and execution reasoning. Our results indicate that while LLMs demonstrate competence in parsing IR syntax and recognizing high-level structures, they struggle with control flow reasoning, execution semantics, and loop handling. Specifically, they often misinterpret branching instructions, omit critical IR operations, and rely on heuristic-based reasoning, leading to errors in CFG reconstruction, IR decompilation, and execution reasoning. The study underscores the necessity for IR-specific enhancements in LLMs, recommending fine-tuning on structured IR datasets and integration of explicit control flow models to augment their comprehension and handling of IR-related tasks.", "sections": [{"title": "1. Introduction", "content": "Intermediate Representations (IRs) play a pivotal role in compiler design by segmenting the compilation process into front-end, middle-end, and back-end phases (Reissmann et al., 2020; Webb et al., 2021; Sb\u00eerlea et al., 2015). They support efficient transformations, optimizations, and analyses that are decoupled from specific programming languages, making them adaptable to diverse architectures (Chow, 2013).\nBeyond compilation, IRs are essential for various code intelligence tasks, including vulnerability detection (Zhu et al., 2024; Jiang et al., 2024a), code comprehension and generation (Yuan et al., 2023; Jiang et al., 2024b), clone detection (Paul et al., 2024), and binary-to-source matching (Mao et al., 2023; Collyer et al., 2023). Unlike high-level programming languages, IRs follow an instruction-like format, encoding low-level details such as control flow dependencies, instruction transformations, and memory manipulations. These characteristics introduce unique challenges in structural analysis, syntactic processing, semantic understanding, and execution reasoning, making IR comprehension fundamentally different from natural language or source code modeling.\nCurrent IR processing methods, such as NCC (Venkata-Keerthy et al., 2020), embed IR instructions as linear text representations using models like Word2Vec (Church, 2017), while BERT-style pretraining (Devlin, 2018; Venkata-Keerthy et al., 2020; Niu et al., 2024) has improved contextual modeling for code. However, these methods lack awareness of control flow structures and execution semantics, limiting their generalization to IR-specific tasks such as Control Flow Graph (CFG) reconstruction, bug detection, and execution reasoning. Since IR execution relies on low-level operations beyond syntax, models that fail to capture control flow relationships struggle with execution-aware IR tasks. Given these limitations, can large language models (LLMs) bridge this gap and reason about IRs effectively?\nTo effectively leverage LLMs for IR-related tasks, we must first address the following question: Do LLMs understand IRs, and to what extent can they reason about them? The answer to this question will determine whether general-purpose LLMs suffice for IR processing or if dedicated IR-trained models are necessary.\nTo systematically evaluate LLMs' comprehension of IRs, we examine their performance across three dimensions:\n\u25b7 Structural Understanding: Can LLMs reconstruct Control Flow Graphs (CFGs) from IRs accurately?"}, {"title": "2. Preliminaries", "content": "LLVM and IRS LLVM is a widely adopted compiler framework that optimizes source code through Intermediate Representations (LLVM IRs) (Lattner & Adve, 2004). LLVM IRs are low-level, typed, and structured, offering a universal representation compatible with numerous high-level languages. LLVM enables control flow graph (CFG) analysis and supports the integration of custom compiler passes for additional transformations. The key components of LLVM IRs include:\n\u2022 Modules: Groups of functions representing a complete IR-based program unit.\n\u2022 Functions: Collections of basic blocks, each representing an independent unit for analysis or optimization.\n\u2022 Basic Blocks: Sequences of instructions without internal"}, {"title": "3. Study Design", "content": "We systematically evaluate whether LLMs can effectively handle IRs without explicit IR-specific training. As shown in Fig. 1, we investigate their ability to capture structural, syntactic, and semantic properties of IRs through four tasks:\n\u25b7 Task 1: CFG Reconstruction measures an LLM's ability to infer structural relationships in IRs by accurately reconstructing CFGs.\n\u25b7 Task 2: IR Decompilation evaluates syntactic comprehension by recovering high-level source code from IR.\n\u25b7 Task 3: Code summarization assesses semantic understanding by generating high-level descriptions of IR functions, including purpose, inputs, and outputs.\n\u25b7 Task 4: Execution Reasoning tests whether LLMs can infer program behavior by reasoning about control flow, variable state changes, and assertion outcomes."}, {"title": "3.1. Evaluation Tasks", "content": "3.1.1. CFG RECONSTRUCTION (TASK 1)\nReconstructing a CFG from IR is critical for various software engineering tasks such as bug detection (Zhang et al., 2023), vulnerability detection (Li et al., 2018), and malware analysis (Anju et al., 2010). Although CFG recovery from binaries has been extensivly studied binaries (Andriesse et al., 2016; Pang et al., 2021; Shoshitaishvili et al., 2016), the application of LLMs to IR_based CFG reconstruction is relatively unexplored.\nApproach\nWe prompt LLMs to generate CFGs directly from IR snippets. Outputs are compared against compiler-generated ground truth using an expert meta-template prompt (detailed in Section 3.3), which provides structured guidance and iterative refinements.\nEvaluation Metrics Following (Ma et al.), we classify LLM-generated CFGs based on structural correctness: Node Construction Accuracy measures the correctness of identified basic blocks, while Edge Construction Accuracy assesses control-flow edges, further categorized into Full"}, {"title": "3.1.2. IR DECOMPILATION (TASK 2)", "content": "Decompilation-converting IR back to source code is a fundamental task in reverse engineering, vulnerability assessments, malware detection, and software modernization (Brumley et al., 2013; Jiang et al., 2023; Hu et al., 2024). Unlike CFG reconstruction, decompilation focuses on syntactic comprehension, testing whether LLMs can recover human-readable, high-level representations from IRs.\nApproach We prompt LLMs to decompile IR into C++ source and compare the generated output to the original code. Since compiler optimizations (00\u201303) significantly alter IR structure, we evaluate LLMs' ability to handle these transformations.\nEvaluation Metrics Following prior work (Tan et al., 2024; Armengol-Estap\u00e9 et al., 2024; Wong et al., 2023), we evaluate re-executability of decompiled code by categorizing outcomes into Re-execution Error (compilation failure or crash), Re-execution Completed (successful execution), and, within the latter, Re-execution Mismatch (at least one assertion fails) and Re-execution Success (all assertions pass, ensuring correct functionality)."}, {"title": "3.1.3. CODE SUMMARIZATION (TASK 3)", "content": "While CFG reconstruction and decompilation emphasize structure and syntax, code summarization assesses semantic understanding. In this task, LLMs are required to generate natural language descriptions of IR functions, capturing their purpose, inputs, and outputs (Sun et al., 2024; Haldar & Hockenmaier, 2024).\nApproach We prompt LLMs with IR snippets and request concise, human-readable summaries. To enhance clarity, we employ expert meta-prompts, few-shot examples, and chain-of-thought guidance. The generated summaries are compared to ground truth or human-curated references for evaluation.\nEvaluation Metrics\nThe quality of summary is measured using the following metrics: BLEU (Papineni et al., 2002) for token overlap, ROUGE-L (Lin, 2004) for sequence alignment based on the longest common subsequence, and METEOR (Denkowski & Lavie, 2014) for a comprehensive assessment that combines precision, recall, and synonym matching."}, {"title": "3.1.4. EXECUTION REASONING (TASK 4)", "content": "Execution reasoning (ER) evaluates whether LLMs can track variable states and control flow in IR-level \"execution\" without actually running the program. Unlike specification reasoning (SR) (Min et al., 2023; Deshpande et al., 2021; Wu et al., 2023), which assesses compliance with expected behavior, ER focuses on inferring program behavior from static IR code.\nApproach We provide LLMs with IR code snippets and assertion statements, prompting them them to predict which assertions hold \u201ctrue\u201d. This requires reasoning over control flow, variable changes, and function interactions.\nEvaluation Metrics Execution reasoning is assessed based on assertion correctness: Pass (all assertions correct), Partial Pass (some assertions correct), and Failure (no assertions correct). Additionally, we compute the Overall Pass Rate, which quantifies the ratio of correct assertions to the total, reflecting the LLM's ability to reason about IR-level behaviors."}, {"title": "3.2. Datasets", "content": "To evaluate LLMs' ability to comprehend IRs, we build on prior work (Zheng et al., 2023; Tan et al., 2024) and use the widely recognized HumanEval benchmark. Developed by OpenAI, HumanEval (Chen et al., 2021) is designed to assess multilingual code generation and consists of 164 carefully handcrafted programming challenges. Each challenge contains a function signature, a natural language description, a function body, and an average of 7.7 assertion statements per challenge. In our experiments, these 164 C++ programs serve as the source code for generating IRs to be analyzed."}, {"title": "3.3. Prompt Design", "content": "Effective prompt design is crucial for optimizing LLM performance (Liu et al., 2022). Traditional single-turn prompts, which combine role instructions and task descriptions into a single input, often lack clarity and fail to provide structured guidance. Recent studies (Liu et al., 2023; Park et al., 2023) show that LLMs perform more effectively when treated as expert agents with detailed, domain-specific instructions.\nTo improve response accuracy, we adopt an Expert Meta-Template Prompt:\nThis template ensures that LLMs receive explicit role definitions, domain knowledge, task specifications, and output formatting requirements, enabling more precise responses."}, {"title": "4. Experimental Results and Analysis", "content": "We evaluate the performance of LLMs across four core IR-related tasks: CFG construction, decompilation, code summarization, and execution reasoning, assessing their effectiveness without fine-tuning. This section presents quantitative results and key observations. We categorized tasks based on their completion status: Task Completed, where the model successfully produced an output, and Task Failed, where it encountered a hang or crash, primarily due to the IR length exceeding the LLM's context window."}, {"title": "4.1. CFG Reconstruction (Task 1)", "content": "Table 1 presents the number of applications successfully processed by each LLM in Task 1, highlighting significant challenges in CFG reconstruction. GPT-4 is the only model to complete all 164 applications, whereas GPT-3 and Gemma 2 complete 133 and 98, respectively. LLaMA 3.1 and Code Llama perform the worst, with only 76 and 67 completed applications. These differences likely stem from token limitations, suggesting the need for expanded context windows or token-efficient IR representations.\nIn terms of accuracy, GPT-4 outperforms all other models, achieving full CFG accuracy in 39 applications, while GPT-3, Gemma 2, and LLaMA 3.1 achieve only 14, 16, and 15, respectively. Code Llama fails to produce a single fully accurate CFG, despite being code-focused, highlighting its struggles with control flow reconstruction. These results underscore a clear gap in LLMs' ability to comprehend and construct CFGs, emphasizing the need for stronger structural reasoning in IR processing.\nFinding 1: LLMs struggle with accurately identifying basic blocks and constructing control flow edges.\nDespite their ability to process IRs, LLMs face significant challenges in basic block recognition and loop structure reconstruction, both essential for CFG construction. As shown in Table 1, node construction errors remain high across all"}, {"title": "Finding 2: LLMs' performance varies by CFG density, guiding model selection.", "content": "Although LLMs generally struggle with CFG reconstruction, their success or failure varies by graph density. Upon further analysis, we observe that the applications in these groups are distinguished by their graph density metrics, suggesting that CFG density influences which model can successfully or unsuccessfully construct them.\nGraph Density Graph density measures the proportion of present edges to the maximum possible edges in a graph. For directed graphs, it is calculated as:\n\nDensity = Number of Edges / n * (n-1)\n\nwhere n is the number of nodes."}, {"title": "4.2. IR Decompilation (Task 2)", "content": "This task evaluates LLMs' ability to decompile IR into C++ by comparing generated code with the original source. We assess re-executability (Tan et al., 2024; Armengol-Estap\u00e9 et al., 2024; Wong et al., 2023), ensuring correctness via assertion-based checks. Key metrics include task completion, re-execution success, and failure rates.\nTable 2 shows that most failures occur at 00, where GPT-4 has the fewest failures (3), while Code Llama struggles (99 failures). This is due to O0 generating the longest IRs, exceeding the context windows of many models.\nIt is interesting to see that re-execution success is highest at O1 and O2, despite similar IR lengths at 01-03. This is likely because O1 and O2 retain structured control flow, while O3 applies aggressive optimizations (e.g., loop un-rolling, instruction reordering) that obscure execution logic, reducing LLM accuracy. Overall, O1 and O2 provide the best balance between optimization and interpretability, leading to more reliable decompilation outcomes.\nFinding 3: LLMs occasionally skip parts of the IR, deviating from traditional decompilers.\nIn several tasks, LLMs omit portions of the IR during decompilation, such as entire if statements within loops, leading to incomplete or simplified outputs. This suggests that LLMs"}, {"title": "4.3. Code Summarization (Task 3)", "content": "To assess LLMs' ability to capture the broader semantics of IRs, we prompt them to generate natural language descriptions of code snippets derived from IRs. This task evaluates how well LLMs understand and articulate the purpose, functionality, inputs, and outputs of the code. The generated summaries are compared against predefined golden summaries using similarity metrics such as BLEU and ROUGE. Higher similarity scores indicate a stronger grasp of IR semantics, reflecting the LLMs' ability to infer and convey meaningful program behavior.\nTable 3 shows the summarization performance of LLMs across BLEU, ROUGE-L, and METEOR metrics. GPT-4 outperforms the others in task completion, successfully completing all 164 tasks, while GPT-3, Gemma 2, LLaMA 3.1, and Code Llama fail 31, 76, 83, and 41 tasks, respectively. High-quality summaries (scores above 0.8) are rare, with each model achieving just one BLEU score above 0.8. However, GPT-4 leads in ROUGE-L with 9 high-quality summaries, followed by LLaMA 3.1 (5), Gemma 2 (4), and GPT-3 and Code Llama (1 each). GPT-4 also excels in METEOR with 21 high-quality summaries, while LLaMA 3.1, Gemma 2, GPT-3, and Code Llama have 11, 7, and 1 each, respectively. These results highlight GPT-4's advantage in capturing and summarizing code semantics. In terms of average scores, LLaMA 3.1 leads in BLEU (0.39), followed by GPT-4 and Gemma 2 (both 0.35). LLaMA 3.1 also leads in ROUGE-L (0.61), followed by GPT-4 (0.56), Gemma 2 (0.52), GPT-3 (0.47), and Code Llama (0.43). In METEOR, LLaMA 3.1 scores the highest (0.67), followed by GPT-4 and Gemma 2 (0.63 each). GPT-3 and Code Llama trail with 0.56 and 0.55, respectively, indicating weaker performance in summarizing code semantics."}, {"title": "Finding 4: LLMs can capture function signatures and input-output relationships but fail to retain full semantic details.", "content": "Our results show that LLMs accurately identify the number of functions, inputs, and outputs, as well as extract basic function structures, input types, and arithmetic operations. However, their summaries often omit critical semantic details, such as iteration logic and conditional dependencies. This suggests that LLMs generate descriptions based on high-level patterns rather than truly reconstructing the underlying semantics.\nWe prompt LLMs to simulate execution using assertion statements from the HumanEval benchmark, generating detailed execution logs to assess whether they correctly follow input-driven control flow and produce expected outputs. To further investigate their reasoning abilities, we extend this task to a counterfactual study, comparing execution behavior on IRs and their corresponding source code (C++). Execution reasoning requires models to track variable evolution, manage control flow transitions, and handle memory operations, testing their ability to infer program behavior accurately."}, {"title": "4.4. Execution Reasoning (Task 4)", "content": "Table 4 summarizes execution simulation results across LLMs for both IRs and source code. All models successfully complete the execution tasks, except for Code Llama, which fails in only one IR-based case. Unlike failures in other tasks caused by token limitations, this suggests that code reasoning processes IR incrementally, interpreting execution step by step rather than consuming the entire IR at once. This distinction highlights a different way LLMs interact with IRs in execution reasoning compared to structural and syntactic tasks.\nPass rates in Table 4 indicate a significant performance gap between source code and IRs, with LLMs performing better on high-level source code. GPT-4 leads in IR execution, achieving 59 successful cases, while Code Llama demonstrates the highest pass rate on source code with 131 passes. Notably, Code Llama's strong performance on source code suggests that code-specific training enhances LLMs' understanding of programming languages. This implies that if we aim to improve LLMs' comprehension of IR, IR-specific training may be necessary to bridge the gap between source-level and IR-level reasoning.\nIn this task, we also observe that LLMs struggle with complex control flows, leading to inconsistencies between IR and source code execution, which is consistent to Finding 2. While LLMs correctly predict execution results for simple control flows, assertion failures increase with more iterations or interdependencies.\nFinding 5: LLMs approximate execution in large semantic steps rather than strictly following control flow.\nInstead of interpreting IRs instruction by instruction, LLMs generate abstracted execution steps based on their understanding of function semantics. The reasoning logs reveal that LLMs typically break execution into 5-7 steps, including: function comprehension, input analysis, loading elements, performing operations, logical evaluation, assertion comparison, and final conclusions\nGranularity Issues: While LLMs recognize high-level algorithms (e.g., sorting), they fail to apply rules consistently"}, {"title": "Finding 6: LLMs exhibit lower confidence in IR execution compared to source code, leading them to rely on pattern-based inference rather than explicit reasoning.", "content": "Unlike source code, IRs lack explicit variable names and high-level semantics, making execution reasoning more ambiguous. When LLMs encounter uncertainties in IR execution, instead of acknowledging gaps in understanding, they often default to heuristic strategies, such as inferring behavior from function names or patterns in assertions. This guesswork approach frequently results in incorrect outputs, as LLMs prioritize familiar structures over accurate execution semantics.\nThe lack of explicit uncertainty handling highlights a fundamental limitation in their IR comprehension, further reinforcing the need for IR-specific training to improve execution reasoning. LLMs compensate by:"}, {"title": "Summary", "content": "Our evaluation reveals significant challenges in LLMs' capacity to handle IRs, particularly in control flow reasoning, execution semantics, and loop analysis. One key issue is token limitations, which become more severe for IRs than for natural language or source code, as IRs contain longer sequences with more tokens per function. This exacerbates the difficulty of processing structural dependencies and capturing execution semantics.\nEven within the constraints of token length, LLMs struggle with tasks requiring control flow graph reconstruction, execution behavior inference, and iteration analysis. While they can effectively identify syntactic patterns and high-level structures, their performance drops significantly when deeper reasoning is required. Among the evaluated models, GPT-4 consistently outperforms the others, aligning with its demonstrated proficiency in source code comprehension,\nAddressing these limitations requires targeted improvements in three areas:\n\u2022 Control Flow Comprehension: LLMs frequently misinterpret branching and loop structures, leading to inaccuracies in CFG reconstruction. Enhancing their ability to track execution paths could improve structural reasoning.\n\u2022 Granular Semantic Understanding: These models rely heavily on heuristics when reasoning about IRs, often skipping fine-grained execution details. Future study should focus on refining instruction-level comprehension.\n\u2022 Loop Reasoning: The inability to accurately predict loop behavior and termination conditions remains a significant challenge. A deeper understanding of iterative constructs is necessary to enhance execution reasoning capabilities.\nThese findings suggest that improving control flow awareness, refining multi-level semantic analysis, and strengthening loop handling mechanisms could significantly enhance LLMs' effectiveness in IR-related tasks."}, {"title": "5. Related Work", "content": "LLMs for High-level Programming Languages Recent advancements in LLMs for NLP have extended to code understanding, enabling models to comprehend high-level languages like Python, C++, and Java. Models such as GPT-3, GPT-4, LLaMA, and Claude 3 excel at tasks like code generation, translating natural language into executable code. Specialized models, including StarCoder, Code Llama, DeepSeek-Coder, and Code Gemma, enhance these capabilities, tackling more complex coding tasks and driving advances in software engineering applications.\nIR Representation Learning IR representation learning integrates structural and flow-based features, such as token sequences, control flow graphs (CFGs), and control-data flow graphs (CDFGs). In terms of model architectures, graph neural networks (GNNs) have been widely used to encode the structures of CFGs and CDFGs via message-passing techniques. Other approaches include skip-gram embeddings, such as inst2vec, and relation-based embeddings, like TransE, which are trained on CDFGs and CFGs to produce instruction-level embeddings. However, these models lack task-agnostic pre-trained embeddings, limiting their ability to capture essential contextual information for downstream tasks. Approaches like IR2Vec address this limitation by introducing hierarchical vector representations that enhance the semantic understanding of IRs. Meta's LLM Compiler aligns with these efforts, offering pre-trained models for code optimization tasks. While previous work has explored IR representation for code optimization, no study has systematically examined how LLMs understand IR syntax, CFGs, execution behavior, and semantics. This study provides the first empirical evaluation of LLMs' IR comprehension across these dimensions."}, {"title": "6. Conclusion and Future Work", "content": "This study evaluates LLMs' ability to comprehend various aspects of IRs, focusing on structural analysis, syntax, semantics, and execution reasoning. Our findings reveal that while LLMs effectively recognize static IR features and basic control flow structures, they struggle with more complex constructs such as loop reasoning and execution simulation.\nSpecifically, LLMs often fail to reconstruct precise control dependencies, omit key instructions during decompilation, and approximate execution in broad semantic steps rather than following instruction-level behavior."}, {"title": "A. Comprehensive Related Work", "content": "LLMs for High-level Programming Languages The advancements in pre-trained LLMs for NLP have extended into code understanding, enabling models to comprehend high-level programming languages such as Python, C++, and Java. Models like GPT , GPT-4, LLaMA and Claude 3 have demonstrated strong capabilities in tasks such as code generation, where they translate natural language descriptions into executable source code. Specialized models, including StarCoder, Code Llama, DeepSeek-Coder and Code Gemma further refine these capabilities, addressing complex coding tasks and advancing software engineering applications.\nPre-trained LLMs have revolutionized NLP by learning versatile language representations from large-scale corpora, which can then be fine-tuned for specific downstream tasks . Early models like Word2Vec and GloVe captured basic word semantics but lacked deeper contextual understanding . The advent of deep transformer-based models like GPT and BERT introduced context-aware modeling, enhancing language comprehension.\nThese LLMs have been adapted to the programming domain, leading to specialized models such as CodeBERT, GraphCodeBERT, UnixCoder, and CodeT5. These models integrate Transformer architectures to encode code semantics effectively. Enhancements such as data flow integration in GraphCodeBERT , multi-modal learning in UnixCoder, and encoder-decoder frameworks in CodeT5 enable improved code comprehension and generation.\nIR Representation Learning IR representation learning incorporates structural and flow-based features such as token sequences, control flow graphs (CFGs), and control-data flow graphs (CDFGs).For model architectures, graph neural networks (GNNs) have been widely employed to encode CFG and CDFG structures through message-passing techniques. Other strategies include skip-gram embeddings, such as inst2vec and relation-based embeddings, such as TransE , trained on CDFGs and CFGs to generate instruction-level embeddings. However, these models lack task-agnostic pre-trained embeddings, preventing them from capturing contextual information crucial for downstream tasks.\nApproaches like IR2Vec mitigate this issue by introducing hierarchical vector representations to improve semantic comprehension of IRs. Recent work, such as FAIR (Flow-aware Pre-trained Model) , further refines IR representations using Graph Transformers to reduce over-smoothing issues while incorporating pre-training tasks that explicitly capture IR token semantics and flow-type information. FAIR has achieved state-of-the-art performance across multiple code-related tasks, highlighting the increasing importance of pre-trained IR models.\nMeta's LLM Compiler aligns with these efforts, offering pre-trained models for code optimization tasks. While prior work has explored IR representation learning for code optimization and analysis, no studies have systematically examined how LLMs comprehend IR syntax, CFG structures, execution behavior, and semantic relationships. Our study addresses this gap by providing the first empirical evaluation of LLMs' IR comprehension across these dimensions."}, {"title": "B. Prompts", "content": "B.1. Prompt for Structural Understanding: Inferring Control Flow from IRs (Task 1)\nPrompt\nYou are a control flow graph analyzer for Intermediate Representations (IRs). I will provide you with LLVM Intermediate representation (IRs), a low-level, platform-independent representation of a program.\nHere is the IR code input: [IR]\nYour task is to generate the control flow graph from the IR. The output format should be a DOT file, including nodes and edges. You do not need to list the content of each basic block; show each node's title.\nHere is the IR code example to follow: [IR Example]\nThe output of the control flow graph should exactly match the following format: [CFG Example]"}, {"title": "B.2. Prompt for Syntactic Comprehension: Decompiling IRs to High-Level Code (Task 2)", "content": "Prompt\nYou are an expert in high-performance computation. I will provide you with LLVM IRs (Intermediate Representations), which is a low-level, platform-independent representation of a program.\nHere is the IR code input: [IR]\nYour task is to decompile this IR code into a pure C or C++ source code format that can be run directly.\n**Do not add any extra comments, explanations, or characters, and do not use any markdown formatting like \u201c\u201c or \""}, {"title": "B.3. Prompt for Semantic Comprehension: Generating Natural Language Descriptions from IRs (Task 3)", "content": "The final prompt used in the code summarization task is as follows:\nPrompt\nI will give you an IR code. Here is the IR code input: [IR]\nI would like you to summarize the code according to the following specifications: 1. **Output Format**: - There are [n] functions in the code: - function[I (I in n)] takes [m] inputs: input1, input2, ..., inputm. - Function [I (I in n)] is [doing semantical function] on input1, input2, ..., inputm, and outputs [output].\n2. **Type and Variable Name Mapping Rules**: - '% \"class.std::vector\"*' should be summarized as 'vector\u00a1float\u012f, numbers'. - Other IR-specific types should be mapped to their equivalent C++ types, following this pattern where possible.\n3. **Strict Adherence to Formatting**: - The summary should strictly match the format provided below. - No additional comments, explanations, or deviations from the format should be added. - Do not use any markdown formatting such as or \"", "follow": ["IR Example]\nThe output should exactly match the following format: [Output Example]\n**Important Instructions**: - The summary must not include any additional comments, explanations, or formatting. - Ensure that variable names and types are directly transcribed as described in the example. - No markdown formatting (e.g., no or", "cpp) should be used. **Do not deviate from the specified format under any circumstances.**"]}, {"title": "B.4. Prompt for Execution Reasoning: Inferring Program Behavior (Task 4)", "content": "Prompt\nI need your help to analyze whether a given assertion passes or fails based on the provided LLVM IR code for a function.\nLLVM IR Code: [IR]\nAssertion Statement: [Assertion]\nPlease write down your thinking process, and list the pass/fail result of each assertion at the end. NOTE: Make sure the format the pass/fail result of each assertion at the end follow the example: [Output Example]"}, {"title": "C. HumanEval Setups", "content": "Building on prior work , we utilize widely recognized benchmarks, specifically HumanEval and its extended test case version, to evaluate the ability of five state-of-the-art LLMs to comprehend semantics and compare them against various golden baselines. HumanEval, introduced by OpenAI, is a benchmark designed to assess the multilingual capabilities of code-generative models. It comprises 164 carefully handwritten programming challenges, each featuring a function signature, a natural language (NL) description, a function body, and a set of unit tests, with an average of 7.7 assertion statements per challenge. In our experiments, these 164 C++ programs serve as the source code for IR analysis.\nThe compilation experiments were conducted on a Dell Workstation equipped with 32 Intel(R) Xeon(R) CPUs E5-2620 v4 @ 2.10GHz, running on an x86-64 architecture with a 64-bit system. For these experiments, we used Clang adapted for LLVM 13 on Ubuntu 18.04. The C++ source code were compiled into IRs (.bc files) using the following command:\nclang++ -0{Opt-level} -emit-llvm {benchmark}.cpp -S -o {benchmark}.bc\nWe use LLVM's built-in passes to generate golden CFGs with the -dot-cfg option to ensure accurate and consistent representations of the code structure. The golden code summarization is meticulously performed to ensure that the generated graphs and code representations faithfully reflect the original source code's semantics. This process is critical for precise evaluations in subsequent tasks, such as static analysis, code summary analysis, and dynamic analysis. By employing these methodologies, we ensure that the golden datasets are of the highest fidelity, providing a solid foundation for assessing the granular understanding and representation of IRs by LLMs."}, {"title": "D. Cases of Findings", "content": "D.1. Example of decompilation\nFigure 3 shows a re-execution mismatch from GPT-4. The original code searches for a value in the key object and increments out when not found. In contrast, the decompiled version checks only for matching characters in two strings, incrementing count instead. This discrepancy likely arises because GPT-4 loses context during loop reconstruction, leading to partial logic and a different outcome during re-execution.\nD.2. Example of summarization\nFigure 4 illustrates the LLVM Intermediate Representation (IR) of the truncate_number function, which extracts the decimal part of a floating-point number.\nTable 5 presents the summarization of the Example IR generated by various LLMs, showing how each model interprets the function's behavior."}, {"title": "D.3. Example of execution reasoning", "content": "Fig. 5 presents the source code for execution reasoning. The IR code shown is the same as the one used for summarization in the previous section.\nD.3.1. MISUNDERSTANDING ALGORITHM LOGIC (45 CASES)\nExample (CPP_69):\nassert (search({5, 5, 5, 5, 1})\n\n==\n 1);\nExpected behavior: The function should find the greatest integer with a frequency greater than or equal to its value.\nLLM failure: The model fails to track frequency accumulation and instead returns an incorrect value, misunderstanding how the search function should operate."}, {"title": "D.4. Conclusion", "content": "These findings highlight common failure patterns in LLM-based code interpretation, including logical misinterpretation, heuristic-based assumptions, and issues with numerical computations. Further refinement in model training and debugging processes is recommended."}]}