{"title": "CHINESE WEBTEXT 2.0: LARGE-SCALE HIGH-QUALITY CHINESE WEB TEXT WITH MULTI-DIMENSIONAL AND FINE-GRAINED INFORMATION", "authors": ["Wanyue Zhang", "Ziyong Li", "Wen Yang", "Chunlin Leng", "Yinan Bai", "Qianlong Du", "Chengqing Zong", "Jiajun Zhang"], "abstract": "During the development of large language models (LLMs), pre-training data play a critical role in shaping LLMs' capabilities. In recent years several large-scale and high-quality pre-training datasets have been released to accelerate the research of LLMs, including ChineseWebText1.0 [1], C4 [2], Pile [3], WanJuan [4], MAPCC [5] and others. However, as LLMs continue to evolve, focus has increasingly shifted to domain-specific capabilities and safety concerns, making those previous coarse-grained texts insufficient for meeting training requirements. Furthermore, fine-grained information, such as quality, domain and toxicity, is becoming increasingly important in building powerful and reliable LLMs for various scenarios. To address these challenges, in this paper we propose a new tool-chain called MDFG-tool for constructing large-scale and high-quality Chinese datasets with multi-dimensional and fine-grained information. First, we employ manually crafted rules to discard explicit noisy texts from raw contents. Second, the quality evaluation model, domain classifier, and toxicity evaluation model are well-designed to assess the remaining cleaned data respectively. Finally, we integrate these three types of fine-grained information for each text. With this approach, we release the largest, high-quality and fine-grained Chinese text ChineseWebText2.0, which consists of 3.8TB and each text is associated with a quality score, domain labels, a toxicity label and a toxicity score, facilitating the LLM researchers to select data based on various types of fine-grained information. The data, codes and the tool-chain are available on this website 3.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models(LLMs) have undergone rapid development. LLMs like BLOOM [6], LLaMA [7], Falcon [8], T5 [9], PaLM [10], Qwen [11], GPT-4 [12] and O1 [13] have showcased powerful capabilities on diverse scenarios, such as question answering, mathematical problems solving, theorem proving, scientific exams, commonsense reasoning, translation, and more. During the development, a cornerstone of these LLMs' success lies in the utilization of a large-scale and high-quality dataset, which play a pivotal role in their extraordinary performance.\nTo facilitate research progress in LLMs, a variety of large-scale datasets have been publicly released in the last few years, such as ChineseWebText1.0 [1],C4 [2], Pile [3], RefinedWeb [8], WuDao [14], WanJuan [4], Yuan 1.0 [15], SkyPile [16], MAPCC [5] and others. These datasets are typically constructed by collecting raw content from diverse sources, then applying manually designed rules or advanced classifiers to filter and refine the data, thereby producing large-scale and"}, {"title": "2 Related Work", "content": "Text Filtering. During the construction of pre-training datasets, most raw data from the web contains various forms of noise, such as violence, pornography, advertisements, corrupted characters, and poorly formatted text. To extract high-quality data from these raw contents, numerous text filtering methods have been proposed to automatically remove undesirable elements. Among these methods, handcrafted rules [9, 17] are commonly used to filter out explicit noise, followed by deduplication techniques [18] to eliminate duplicate text from different sources. Building on this prior work, this paper introduces a rule-based preprocessing module designed to extract high-quality text from raw web content.\nQuality Evaluation. In addition to rule-based methods, quality evaluation approaches are also employed to identify high-quality texts using well-designed classifiers. Unlike rule-based methods, which primarily filter out explicit noise from raw content, quality evaluation approaches offer greater robustness and flexibility. These approaches leverage models such as logistic regression [19], BERT [20], FastText [21], and others to calculate probability scores for each text. Based on these scores, texts are classified as positive or negative according to a predefined threshold. Among these models, BERT has emerged as one of the most widely used architectures for quality evaluation due to its exceptional performance in text classification and understanding tasks. BERT's effectiveness stems from its pre-training objectives, including masked language modeling and next-sentence prediction, which enable it to learn powerful text representation and comprehension capabilities. In this paper, we adopt the BERT architecture to build a robust quality evaluation model for text.\nDomain Classification. The Pre-training of LLMs on vast amounts of diverse text data often results in a generalized understanding of language. However, to enhance their effectiveness for specific applications, it is advantageous to"}, {"title": "3 Data Construction", "content": "The diversity and complexity of textual data present significant challenges in constructing high-quality pre-training datasets and annotating each text with multi-dimensional, fine-grained information. To address these challenges, this paper proposes a pipeline system, MDFG-tool, which integrates a text filtering module with multi-dimensional, fine-grained annotation modules. This approach allows us to extract high-quality data from raw content and provide quality scores, domain labels, toxicity scores, and toxicity labels for each text. As shown in Figure 1, the overview of our proposed approach is illustrated. Starting with raw content collection, we employ a preparation module to evaluate and select data from suitable sources. Next, a preprocessing module applies handcrafted rules\u2014such as text length, character proportions, and sensitive words\u2014to filter out noisy or irrelevant texts, yielding a high-quality dataset. Following this, three fine-grained annotation modules are applied to the refined texts: a quality evaluator assigns a quality score, a domain classifier determines the domain labels, and a toxicity evaluator calculates a toxicity score and assigns a toxicity label. Finally, these three types of annotations are integrated for each text, resulting in a high-quality dataset enriched with multi-dimensional, fine-grained metadata."}, {"title": "3.1 Data Collection and Preparation", "content": "In recent years, many pre-training datasets have been publicly released, containing a variety of texts from different sources. As a result, we consider these datasets as the sources of our raw content. In this paper, we collect some publicly available datasets from the internet, including MAP-CC [5], SkyPile [16], WanJuan [4], WuDao [14], and ChineseWebText1.0 [1], among others. All the datasets we obtained are composed of Chinese texts.\nDue to the presence of noisy data and irrelevant characters in some publicly available datasets, the acquisition of high-quality data is significantly affected. To address this issue, we implement a preparation module to process data from various sources. In this module, we first sample hundreds of texts from each dataset for manual analysis. If a dataset is found to contain excessive noise or irrelevant characters, it is entirely excluded. The empirical threshold for exclusion is when the proportion of irrelevant text exceeds 30%. Through this process, we are able to obtain the Chinese text data required for our study."}, {"title": "3.2 Preprocessing", "content": "After collecting Chinese texts from various sources, we introduce a preprocessing module designed to process all the collected texts and ensure the extraction of high-quality Chinese text data. Building upon the work of ChineseWeb-Text1.0, this paper employs four types of handcrafted rules in the preprocessing module. The specifics of these rules are detailed below.\nData Length. Since most of the collected texts are from web content, a significant portion consists of short text lines that lack relevance to one another. These texts make the data unsuitable for training language models. To address this issue, following the approach of ChineseWebText1.0, we calculate the average line length for each document and discard those with an average line length of fewer than 10 characters. Additionally, short texts often provide limited information and fail to convey sufficient knowledge. Therefore, we also remove texts with fewer than 200 characters to ensure the dataset is more informative and effective for training.\nProportion of Characters. During the human analysis of texts in the preparation stage, we found that some Chinese datasets contain characters from other languages, non-essential characters, special symbols, and so on. These irrelevant"}, {"title": "3.3 Quality Evaluation", "content": "In the preprocessing phase, we applied a set of handcrafted rules to filter out explicit noisy text from our dataset. However, a significant portion of low-quality data remained, which could not be effectively eliminated by manual rules alone. To perform a more fine-grained quality assessment on the remaining data, we propose a fully automated quality evaluation method. Building on the approach used in ChineseWebText1.0 [1], we develop a BERT-based classification model to assign a quality score to each text. With this quality score, LLM researchers can select the desired subsets based on a specified threshold. The details of the classification model are presented below."}, {"title": "Training Data", "content": "To enhance the diversity of the training data, we incorporate a variety of text types into the positive training samples, including Wikipedia entries, e-books, poetry, news articles, and question-answer pairs. This strategy improves the model's ability to evaluate quality across a broad range of data types. Given the relatively high noise levels in Common Crawl data, we directly sample from Common Crawl to construct the negative samples. Figure 2a provides a detailed breakdown of the composition and quantity of the training data, demonstrating that the positive and negative samples are nearly in a 1:1 ratio."}, {"title": "Model Architecture", "content": "Following the work of ChineseWebText1.0, we also employ the efficient AES model Tran-BERT-MS-ML-R [30] to evaluate text quality. As shown in Figure 2b, the model is based on the BERT [31] architecture. To reduce computational complexity, we focus solely on the text-level representation, utilizing the [CLS] embedding to extract relevant information and structural features from a comprehensive perspective of the text. Simultaneously, token-level representations are derived from the sequence output of BERT, with x representing the text input. After applying max pooling, the token-level representation is concatenated with the text-level representation and passed through a dense layer with Sigmoid activation, producing a text quality score $f(x|W)$ within the range (0, 1)."}, {"title": "Training Stage", "content": "The BERT model in the ChineseWebText1.0 achieves excellent classification performance, so we utilize it as our base model. To further enhance the model's learning capability and robustness, we extend beyond the standard MSE loss [32]. Specifically, we incorporate two additional loss functions: Margin Ranking (MR) loss [33] and Cosine Similarity (CS) loss [30]. The training process consists of a pre-training phase and a self-training phase. During the pre-training phase, we train the model using positive and negative samples in a 1:1 ratio. This phase enables BERTEval to develop an initial ability to distinguish text quality."}, {"title": "Evaluation", "content": "To evaluate the performance of BertEval in quality assessment, we conduct a human evaluation based on BertEval scores. First, we sample a validation set consisting of 50 samples from each BertEval score interval. Since the scores in the interval [0,0.1) are zero, we exclude this range from the evaluation. Then, we perform human evaluation of the quality acceptability of samples in different score intervals. The criteria for this evaluation are provided in the Appendix 5.1. Through this process, we can assess the difference between BertEval's scores and human quality acceptability."}, {"title": "3.4 Domain Classification", "content": "In the previous section, we construct a quality evaluation model for Chinese texts to facilitate the selection of high-quality training data. In contrast, this section focuses on classifying the domain of each text. In this work, we define 11 distinct domain types based on the most common application areas of LLMs: Mathematics, Books, Law, Finance, Education, Dialogue, Encyclopedia, News, Medicine, Technology, and General. We then classify each text into one of these domains using a hybrid approach that combines rule-based methods with model-driven techniques. The details of this approach are outlined below."}, {"title": "3.4.1 Classification Algorithm", "content": "In this section, we develop a domain classification system that combines rule-based methods with a FastText-based model. FastText is a library for efficient learning of word representations and text classification. Compared to other approaches, FastText significantly reduces training and inference time while maintaining classification performance. As shown in Figure 4b, the architecture of our approach is presented. In this system, the rule-based method is first used to perform initial annotation on the texts. Then, the FastText-based model is trained on the annotated data and iteratively optimized to improve performance. Notably, we annotate each text with both single and multi-labels. The details of this architecture are presented below."}, {"title": "3.4.2 Training and Testing Data", "content": "To enhance the diversity of the data, we collect training and testing data from various sources, including news articles, books, forums, product descriptions, and more. After several rounds of iterative optimization during the training stage, we select nearly 300,000 texts as training data and 300 texts from different domains as testing data. As shown in Table 1, this table presents the size of the training and testing data from different domains. Additionally, Figure 4a illustrates the detailed breakdown of the composition of the training data."}, {"title": "3.4.3 Evaluation", "content": "Evaluation Metric To rigorously evaluate the performance of the classification model, it is essential to use standard evaluation metrics that assess both its accuracy and comprehensiveness. For single-label classification task, Precision and Recall will be used, as they provide a balanced understanding of the model's performance. Precision represents the ratio of correctly predicted labels to the total number of labels predicted, while Recall is the ratio of correctly predicted labels to the total number of actual labels. The formulas for these two metrics are as follows:\n$Precision = \\frac{\\text{Number of Correctly Predicted labels}}{\\text{Total Number of Predicted labels}}$ (1)\n$Recall = \\frac{\\text{Number of Correctly Predicted labels}}{\\text{Total Number of True labels}}$ (2)\nDifferent with the metric above, multi-label classification adopts micro-averaged precision and recall to evaluate the performance of models. To calculate micro-averaged precision, we sum up all true positive predictions across all labels and divide it by the sum of true positive and false positive predictions across all labels. Similarly, micro-averaged recall is computed as the total true positive predictions divided by the sum of true positive and false negative predictions. These metrics reflect the overall performance of the model across all samples and labels, treating each instance of a label equally regardless of its class."}, {"title": "Evaluation Results", "content": "To evaluate the classification model's performance, we manually annotated all texts in the test dataset with both single-label and multi-label classifications. As shown in Table 2, the results indicate that single-label classification achieves a precision of 88.33% and a recall of 64.15%. For multi-label classification, we calculate the micro-averaged precision and recall, which achieve 74.48% and 79.35% respectively. These findings highlight the model's strong capability in domain classification, even when dealing with complex data scenarios."}, {"title": "3.5 Toxicity Evaluation", "content": "Text data from the internet often contains various types of toxic content, which can significantly compromise the security of LLMs. To address this issue, we propose using a toxicity evaluation model to assess the toxicity of all texts and generate corresponding toxicity labels and scores. Due to its efficiency in reducing training and inference time while maintaining strong classification performance, FastText is once again employed to construct the toxicity evaluation model. The details of this approach are presented below."}, {"title": "3.5.1 Methodology", "content": "In the training process of toxicity evaluation models, the scale and quality of training data have a significant impact on the performance of the models. However, an analysis of existing datasets reveals that toxicity data is often limited in size"}, {"title": "Initail Training", "content": "In the training process of evaluation models, we first combine several high-quality Chinese toxicity datasets-COLD [27], ToxiCN [36], SWSR [37], and CDial-Bias [38]\u2014to create an initial training set. To increase the diversity of the dataset and better align its distribution with that of our specific dataset, we sample a subset of texts from our high-quality dataset to serve as benign samples. Additionally, to ensure a balanced distribution between toxic and benign samples, we doubled the number of toxic samples in the initial training set. With this augmented dataset, we train the initial FastText model, referred to as Toxic Classifier R0. As shown in Table 3, it presents a detailed breakdown of the source and scale of each portion of the initial training dataset."}, {"title": "LLM-in-the-loop Training", "content": "In this section, LLMs will be utilized to further enhance the performance of toxicity evaluation models. In this approach, we first select a subset from our large-scale dataset and then Toxic Classifier RO is employed to score each sample. Next, texts with toxicity scores greater than 0.5 will be screened out to form a candidate set. After that the candidate subset will be scored by a LLM, which categorize them into toxic and benign classes, thereby forming a new training set. Here, the chosen LLM is Qwen2.5-32B-Instruct [39]. Finally, we integrate the new training set with the initial training set, and then retrain the FastText model with this refined training dataset. In this paper, we conduct two iterations on the above method, significantly improving the performance of the evaluation model."}, {"title": "3.5.2 Evaluation", "content": "To conduct a comprehensive evaluation across data from various sources, a test set is randomly selected from the large-scale dataset and manually annotated with toxic or benign labels. Due to the imbalance between toxic and benign samples in our dataset, toxic and benign samples are collected separately during the test data sampling. The final test set consists of 300 toxic samples and 300 benign samples.\nGiven the sampling methodology of the test set, Precision and Specificity were selected as metrics to evaluate the performance of the model on toxic and benign data, respectively. In this study, toxicity is defined as the positive class, whereas benignity is defined as the negative class. Precision is defined as the ratio of true positive predictions to total positive predictions, while Specificity measures the ratio of true negative predictions to total negative predictions. The formulas of these two metrics are shown below."}, {"title": "", "content": "$Precision = \\frac{\\text{True Positive}}{\\text{True Positive + False Positive}}$ (3)\n$Specificity = \\frac{\\text{True Negative}}{\\text{True Negative + False Negative}}$ (4)\nAs shown in Table 4, the evaluation results of our model are presented. From this table, we can observe that the precision of our toxicity evaluation model for toxic texts reaches 83.67%, while the specificity for benign texts is 97.67%. These results indicate that our model demonstrates excellent performance in toxicity evaluation."}, {"title": "3.6 Dataset Statistics and Comparison", "content": "After processing the collected data with the modules outlined above, this paper constructs a clean Chinese text dataset, ChineseWebText2.0, which consists of 3.8 TB of data. Each text in this dataset is assigned a quality score, domain labels, a toxicity score, and a toxicity label. As shown in Table 5, we compare our dataset with several other publicly available pre-training corpora. Most of these datasets primarily focus on collecting clean texts from various sources, while neglecting fine-grained text annotations. In this comparison, C4 [2], The Pile [3], and REFINEDWEB [8] are three public English datasets, while WuDaoCorpora [14], ROOTS-zh [40], WanJuan1.0-zh [4], MAPCC [5], IndustryCorpus [28], and ChineseWebText1.0 [1] are Chinese corpora. Specifically, the texts in IndustryCorpus are tagged with domain labels, while each text in ChineseWebText1.0 is annotated with a quality score. Compared to these prior works, our dataset is both the largest and most recent Chinese dataset, and it features four types of fine-grained annotations for each text. This extensive annotation can aid LLM researchers in enhancing model performance in specific domains while improving safety."}, {"title": "4 Data Analysis", "content": "4.1 Removal Rate for Different Stages\nTo provide a high-level overview of the preparation and preprocessing stages, Figure 6 illustrates the processing workflow along with the removal rate at each step. This figure shows the proportion of data removed at each stage and"}, {"title": "4.2 Data Quality Distribution", "content": "Quality Distribution To investigate the quality distribution, we calculate the data proportions across different quality score ranges from our ChineseWebText 2.0 dataset. Figure 7a shows the proportion of data across different quality score intervals. The data is primarily concentrated in the mid-range score intervals [0.2,0.4), each contributing approximately 18%. Additionally, a significant proportion lies within the high-quality interval [0.9, 1.0), reflecting the presence of high-quality content in the dataset. In contrast, the lowest interval [0.1,0.2) contains only a minimal fraction, indicating a limited amount of poor-quality data. Note that the quantity of quality scores in the range [0, 0.1) is zero, so this interval has been omitted. This quality distribution provides a valuable reference for LLM researchers, enabling them to select data based on desired quality thresholds.\nHuman Acceptance Evaluation To validate the consistency between quality evaluation and human judgments, we randomly sample 100 samples from each score interval. The details and criteria for the human evaluation are provided in the Appendix 5.1. Figure 7b displays human acceptance rates across different score intervals, showing a clear positive trend: higher scores correlate with higher acceptance rates. Specifically, the highest score interval [0.5,1.0) achieves an acceptance rate exceeding 90%, while the lowest interval [0.1,0.2) still maintains an acceptance rate of 80%. This trend highlights the overall high quality of the data."}, {"title": "4.3 Domain Distribution", "content": "To investigate the distribution of our dataset across different domains, in this section, we conduct an detailed analysis of the data distribution across eleven distinct domains: book, dialogue, education, encyclopedia, finance, law, math, medicine, news, technology, and general. This analysis considers two perspectives: the overall domain distribution and the quality-related domain distribution, providing comprehensive insights into the dataset's composition across different domains."}, {"title": "Overall Domain Distribution", "content": "As shown in Figure 8, the sample counts and corresponding proportions across various domains are presented. The Encyclopedia, General, and News domains dominate the dataset, comprising 33.43%, 33.44%, and 28.01% of the total data, respectively. In contrast, the Math domain has the smallest share at 0.55%, yet it still includes over 8 million samples. Figure 9 provides a bar chart that offers a more intuitive visualization of this distribution. This comprehensive domain analysis allows LLM researchers to select datasets that enhance the model's knowledge and capabilities in specific areas."}, {"title": "Quality-Related Domain Distribution", "content": "In order to explore the domain distribution across different quality intervals, we perform an analysis focused on the quality-related domain distribution. Specifically, we calculate the proportions of various domains within each quality interval. As shown in Table 6, this table provides a detailed breakdown of domain"}, {"title": "4.4 Data Toxicity Analysis", "content": "During the training procedure of LLMs, toxic data introduces harmful knowledge, which can lead the model to generate toxic outputs. In this section, we analyze the toxicity distribution within our dataset. As shown in Figure 10, the figure illustrates the toxicity distribution of the dataset, where a higher toxicity score indicates greater toxicity. It is evident that the majority of the data in our dataset has a toxicity score of 0.0, signifying non-toxic, high-quality data. These non-toxic texts make up 97.41% of the dataset."}, {"title": "5 Conclusions and Future Work", "content": "In order to extract large-scale, high-quality Chinese pre-training data with multi-dimensional and fine-grained information, we have developed a novel pipeline that processes raw collected data using both handcrafted rules and advanced evaluation models. The handcrafted rules are applied to clean the raw texts, producing a high-quality dataset. Subsequently, we design three powerful models\u2014a quality evaluation, a domain classifier, and a toxicity evaluation model to assign each text with four types of fine-grained information. With this approach, we release the latest and largest Chinese dataset of 3.8 TB, each of which is annotated with a quality score, domain labels, a toxicity score and a toxicity label. This dataset enables LLM researchers to re-filter the data according to their desired thresholds. Additionally, We also release the complete tool-chain that transforms the raw data into a high-quality dataset with multi-dimensional and fine-grained annotations.\nIn the future, we will collect a wider variety of domain-specific texts to enable the dataset to cover a broader range of scenarios. Additionally, we are going to enrich the dataset by incorporating more fine-grained information into the texts. For example, we could construct a evaluation model to assess the knowledge density within each text."}]}