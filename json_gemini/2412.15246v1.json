{"title": "Accelerating Retrieval-Augmented Generation", "authors": ["Derrick Quinn", "Mohammad Nouri", "Neel Patel", "John Salihu", "Alireza Salemi", "Sukhan Lee", "Hamed Zamani", "Mohammad Alian"], "abstract": "An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG. In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4\u201327.9\u00d7 faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7-26.3\u00d7 lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM \u2013 which is the most expensive component in today's servers from being stranded.", "sections": [{"title": "1 Introduction", "content": "State-of-the-art natural language processing systems heavily rely on large language models (LLMs)-deep Transformer networks [93] with hundreds of millions of parameters. There is much evidence that information presented in the LLM training corpora is \u201cmemorized\u201d in the LLM parameters, forming a parametric knowledge base that the model depends on for generating responses. A major challenge with parametric knowledge is its static nature; it cannot be updated unless the model undergoes retraining or fine-tuning, which is an extremely costly process. This creates a critical issue, especially when it comes to non-stationary domains where fresh content is constantly being produced [108]. Besides, previous studies have indicated that LLMs exhibit limited memorization for less frequent entities [29], are susceptible to hallucinations [83], and may experience temporal degradation [31].\nTo overcome the challenges presented by LLMs, a potential solution is to enhance them with non-parametric knowledge, where the LLM is augmented with information retrieved from a knowledge source (e.g., text documents). These approaches have recently gained considerable attention in the machine learning communities [22; 26; 41; 49; 58; 73; 83], and have played key roles in some recent breakthrough applications in the tech industry, such as Google Gemini [90], Microsoft Copilot [86], and OpenAI ChatGPT with Retrieval Plugins [56]. Retrieval-Augmented Generation (RAG) is the term that is used to refer to systems that adopt this approach in the context of LLMs.\nA RAG application includes two key components: a retrieval model and an LLM for text generation, called the generative model. When a query is received, the retrieval model searches for relevant items (e.g., documents) and the top retrieved items, together with the input, are sent to the generative model. Current state-of-the-art retrieval approaches use bi-encoder neural networks (called dense retrieval) [30] for learning optimal embedding for queries and documents. Each item is then encoded into a high-dimension vector (called embedding vectors) and stored in a vector database. Such approaches use K-nearest neighbor algorithms for retrieving the top \"K\" items from the vector database.\nThe accuracy of generated output in RAG hinges on the quality of the retrieved item list. Conducting an Exact Nearest Neighbors Search (ENNS) to retrieve the precise top K relevant items involves scanning all the embedding vectors in the vector database, which is costly in today's memory bandwidth-limited systems. For example, in a RAG application with a 50GB (using a 16-bit floating point representation) vector database running on an Intel Xeon 4416+ with 8\u00d7DDR5-4000 memory channels, and a generative model running on an NVIDIA H100 GPU, ENNS takes up to 97% of the end-to-end inference time (\u00a73).\nOne strategy to mitigate the retrieval cost is to employ Approximate Nearest Neighbor Search (ANNS), and opt for a faster, but lower-quality search configuration. While lower-quality retrieval can improve search time, our extensive experiments on Question Answering applications demonstrate that a lower-quality search scheme should provide significantly more items to the language model in order to match the end-to-end RAG accuracy of ENNS or a higher-quality, but slower ANNS configuration. This virtually negates any benefits gained during the retrieval phase and even increases the end-to-end inference time.\nIn this paper, we extensively profile the execution pipeline of RAG, demystifying the complex interplay between various hardware and software configurations in RAG applications [106]. Motivated by the need for high-performance, low-cost, high-quality search and the limitations of current commodity systems, we contribute the Intelligent Knowledge Store (IKS), a cost-optimized, purpose-built CXL memory expander that functions as a high-performance, high-capacity vector database accelerator. IKS offloads memory-intensive dot-product operations in ENNS to a distributed array of low-profile accelerators placed near LPDDR5X DRAM packages.\nIKS implements a novel interface atop the CXL.cache protocol to seamlessly offload exact vector database search operations to near-memory accelerators. IKS is exposed as a memory expander that disaggregates its internal DRAM capacity and shares it with vector database applications and other co-running applications through CXL.mem and CXL.cache protocols. Instead of building a full-fledged vector database accelerator, IKS co-designs the hardware and software to implement a minimalist scale-out near-memory accelerator architecture. This design relies on software to map data into the internal IKS DRAM and scratchpads while performing the final top-K aggregation.\nIn summary, we make the following contributions:\n\u2022 We demystify RAG by profiling its execution pipeline. We explore various hardware, system, and application-level configurations to assess the performance and accuracy of RAG.\n\u2022 We demonstrate that RAG requires high-quality retrieval to perform effectively; nonetheless, current RAG applications are bottlenecked by a high-quality retrieval phase.\n\u2022 We introduce Intelligent Knowledge Store (IKS), which is a specialized CXL-based memory expander equipped with low-profile accelerators for vector database search. IKS leverages CXL.cache to implement a seamless and efficient interface between the CPU and near-memory accelerators.\n\u2022 We implemented an end-to-end accelerated RAG application using IKS. IKS accelerates ENNS for a 512GB knowledge store by 13.4\u201327.9\u00d7, leading to a 1.7-26.3\u00d7 end-to-end inference speedup for representative RAG applications."}, {"title": "2 Background", "content": "2.1 Information Retrieval in RAG\nRecent advancements in RAG indicate superior outcomes when employing dense retrieval over other methods, for unimodal [25; 26; 41] and multi-modal [19; 71; 74] scenarios. Consequently, our emphasis in this study centers on dense retrieval models, exploring their efficiency-related aspects.\nIn the context of dense retrieval, a query encoder, denoted as \\(E_q\\), and a document encoder, denoted as \\(E_d\\), are trained to encode queries and documents, respectively, and map them into a high-dimensional vector space. The similarity score between a document\u00b9 d and a query q is calculated as \\(s_d = E_q(q) \\cdot E_d(d)\\), where \\(E_q(q) \\in \\mathbb{R}^h\\), \\(E_d(d) \\in \\mathbb{R}^h\\) and h is the hidden dimension of query and document encoders. Then documents are sorted based on their similarity scores and top documents are retrieved [30]. In a real RAG implementation, in an offline phase, all the documents are encoded into embedding vectors. The embedding vectors are stored in a vector database for dense retrieval. In the paper, we refer to the encoded documents as embedding vectors and the vectors generated by the retriever model as query vectors."}, {"title": "2.2 Applications of RAG", "content": "RAG has proven beneficial for various tasks in natural language processing [36; 44; 108], including dialogue response generation [2; 8; 10; 83; 91; 92; 96; 97; 99], machine translation [18; 21; 102; 109], grounded question answering [25; 26; 41; 66; 67; 76-78; 85; 107], abstractive summarization [58; 64], code generation [20; 49], paraphrase generation [32; 89], and personalization [37; 72; 73; 75]. Additionally, RAG's application extends to multi-modal data tasks like caption generation from images, image generation, and visual question answering [11; 12; 15; 19; 70; 71; 80].\nIt is noteworthy that commercial LLM systems employing RAG are typically proprietary, and as such, their implementations are not openly accessible. Nevertheless, insights into the implementation of these systems can be gleaned from open-source releases by research labs within commercial entities. We adhere to a methodology akin to the approach outlined by Izacard and Grave [26] and Lewis et al. [41], both of which are contributions from Meta AI. Our implementations closely align with the depicted pipeline in Specifically, we employ a dense document retrieval model as the retriever and leverage a language model for answer generation, consistent with the aforementioned work. Additionally, for efficient vector search capabilities, we utilize the Faiss [27] library, similar to the aforementioned works."}, {"title": "3 Demystifying RAG", "content": "In this section, we profile the end-to-end execution of three representative long-form question-answering RAG applications and quantify both the execution time breakdown and the generation accuracy of RAG with different hardware and software configurations: FiDT5, where we use the T5-based Fusion-in-Decoder [26; 68] as the generative model, as well as Llama-8B, and Llama-70B, where we use 4-bit-Quantized Llama-3-8B-Instruct and Llama-3-70B-Instruct [3] as the generative models, respectively. The knowledge source for all workloads is Wikipedia, and a trained BERT base (uncased) model is used to generate embedding vectors for documents. We assume 16-bit number format and test with various vector database sizes (corpus size) that store the embedding vectors. The documents themselves are stored in the CPU memory.\nIn FiDT5, the documents are presented via the Fusion-in-Decoder approach, where documents are encoded by the encoder stage of a T5 model, and these encoded representations are combined for the decoder stage. In Llama-8B and Llama-70B, retrieved documents are presented as plaintext in the prompt. For more information about the methodology, see Section 6.\nIn the following subsections, we discuss both the accuracy of an end-to-end RAG system and the retrieval model on its own. Retrieval accuracy is discussed in terms of recall, where ENNS is considered to be perfect, and the recall score of an ANNS algorithm is the proportion of relevant documents retrieved by both ENNS and ANNS algorithm compared to the total number of relevant documents retrieved by ENNS. Generation accuracy refers to how well an end-to-end RAG system answers questions. For details on the evaluation of generation accuracy, see Section 6.2."}, {"title": "3.1 Tuning RAG Software Parameters", "content": "Both the retrieval phase and generation phase of the RAG systems that we use offer support for batching of queries in order to improve data reuse and to amortize data movement overheads over several queries. However, batching is not always an option in practice, particularly in the case of latency-critical applications. We consider batch sizes of 1 for latency-critical uses across all applications and 16 for throughput-optimized applications. Batch size does not impact generation accuracy and only affects execution time. An important parameter in RAG is \"K\" or the number of documents retrieved and fed to the language model for text generation. Increasing the document count significantly impacts the generation time. The computation required for transformer inference scales at least linearly with the input size [93], and if we concatenate the retrieved documents, we face significant computation and memory overhead [14; 112]. In particular, the memory required to store a key-value cache entry for a single token can be computed as follows: \\(n_{layers} \\times N_{KV-heads} \\times d_{head} \\times n_{bytes} \\times 2\\), where \\(n_{bytes}\\) refers to the size of the number format [46]. For Llama-8B with a 16-bit number format, this is 32\u00d78\u00d7128\u00d72\u00d72=131 kB per token. While exact token counts depend on the tokenization process, each document (for all applications) is 100 words long; for Llama-8B and Llama-70B, this averaged 127 tokens per document across our evaluation dataset."}, {"title": "3.2 Examining Approximate Search for RAG", "content": "An important algorithmic consideration that can impact the inference time and generation accuracy of RAG is the choice of retrieval algorithm from the vector database, where we can use exact nearest neighbor search (ENNS) or approximate nearest neighbor search (ANNS). The particular algorithm used for retrieval is implemented by a data structure called an index, which stores the embedding vectors computed offline, as described in Section 2.1. For ENNS, an index is a wrapper around an array of embedding vectors sequentially iterated over during the search, but for ANNS, the index can be more complex. For example, HNSW stores embedding vectors in a graph-based data structure [52].\nTo evaluate ANNS, we use the state-of-the-art HNSW [52] ANNS algorithm, and fine-tune the M and efConstruction parameters to maximize retrieval accuracy while maintaining a reasonable graph, yielding an index with M of 32 and efConstruction of 128. From this, we evaluate two configurations, ANNS-1 and ANNS-2, which use different efSearch parameters: 2048 and 10000. In the context of an end-to-end RAG system, the trade-off of generation accuracy and runtime was evaluated for this index for various choices of efSearch. A lower efSearch provides higher search throughput, but lower generation accuracy, and a higher efSearch provides lower search throughput, but higher generation accuracy. Other HNSW and IVFPQ indexes were tested but provided lower generation accuracy, or similar runtime to ENNS (or even worse, in some cases), negating the benefits of approximation.\nSeveral prior works [6; 110] demonstrate that hyper-parameter tuning can enhance the retrieval accuracy of ANNS, potentially matching that of ENNS across various workloads. While we optimized our HNSW indexes for accuracy and throughput, these indexes could not match ENNS in end-to-end generation accuracy while achieving significantly (more than 2x) faster search. By using a small efSearch value, retrieval speed improves significantly, allowing for the use of a larger value of K to compensate for the reduced retrieval quality. However, trading retrieval quality for retrieval speed in this way resulted in lower generation accuracy and end-to-end throughput compared to a larger efSearch, where a higher-quality, slower search scheme permits greater accuracy at lower K values (thus reducing generation times). For example, ANNS-2 with 16 documents have 3% higher accuracy and 128% higher throughput compared to ANNS-1 with 128 documents for FiDT5. Further improving retrieval quality via exact search gives ENNS-based RAG Pareto-superiority above sufficiently high accuracy thresholds (~43%, ~ 27%, and ~ 14% for FiDT5, Llama-8B, and Llama-70B, respectively) as demonstrated in Figure 2. In general, our findings highlight the potential for reducing generation time by leveraging high-quality retrieval methods when high accuracy is required.\nScaling of ANNS and ENNS: Previous works [6; 52] identified the trade-off between retrieval quality and runtime, and challenges with high-quality ANNS have motivated accelerators such as ANNA [40] and NDSearch [95]. While lower-quality ANNS algorithms could possibly provide orders of magnitude faster nearest neighbor search compared with ENNS, high-quality ANNS algorithms are shown to provide only a modest speedup [45; 94]. For example, ANNS-2, which is the best performing ANNS configuration in Figure 2, offers only a 2.5\u00d7 speedup compared with ENNS. In fact, all the Pareto frontier configurations that provide high generation accuracy in Figure 2 are ENNS. Therefore, in the rest of this section, we focus on understanding how to optimize and accelerate RAG applications with ENNS."}, {"title": "3.3 End-to-End RAG Performance with ENNS", "content": "In this subsection, we profile time-to-interactive (also known as time to first token) [87] for the FiDT5, Llama-8B, and Llama-70B RAG applications and report latency ratios for the retrieval and generation phases. For all experiments, retrieval uses ENNS and runs on the CPU, while generation runs on a single NVIDIA H100 GPU. We select CPU as the baseline for ENNS retrieval, rather than GPU. This decision is made based on the high cost of using GPU memory\nAs we discussed in Section 3.2, the generation accuracy of RAG applications directly depends on the retrieval accuracy. However, as shown in Figure 3a, utilizing ENNS for retrieval can quickly become an end-to-end bottleneck in RAG applications, even for large models. Although it is possible to compensate for the retrieval accuracy by increasing K (in case of using ANNS), as shown in Figure 3b, increasing K would increase the generation time and is costly in terms of time to first token.\nThe two phases in a RAG pipeline have different characteristics: ENNS is extremely memory bandwidth-bound, and generation is relatively compute-bound. Nevertheless, the current state-of-the-art focus in building AI systems is only on accelerating the generation phase [1; 5; 17; 34; 48; 53; 63; 82; 100; 101; 104]. Next, we discuss the feasibility of accelerating high-quality nearest neighbor search for future RAG applications."}, {"title": "3.4 High-Quality Search Acceleration", "content": "Given the sensitivity of RAG generation accuracy, latency, and throughput to the retrieval quality, it is imperative to focus exclusively on accelerating the retrieval phase of future RAG applications. In this subsection, we discuss the feasibility of accelerating high-quality ANNS and ENNS.\nAcceleration of High-Quality ANNS: High-quality ANNS can be as slow as ENNS [45]. There are prior works aimed at building hardware accelerators for high quality ANNS [40; 95] because GPUs are not effective at accelerating key ANNS algorithms such as IVFPQ and HNSW [27]. Unfortunately, the complex algorithms and memory access patterns used for ANNS algorithms also make ANNS accelerators highly task-specific; for example, ANNA [40] and NDSearch [95] can only accelerate PQ-based and graph-based ANNS algorithms, respectively. However, our experimental results, which are in line with prior findings [94], show that different corpora are amenable to different ANNS algorithms.\nAcceleration of ENNS: ENNS can be accelerated using conventional SIMD processors such as GPUs and Intel AMX because the algorithm is simple and data-parallel. Although GPUs can significantly speed up ENNS, as the corpus size increases, the cost of offloading ENNS to GPUs increases significantly. For example, to fit the 50 GB and 512 GB corpus sizes tested in Table 1, we need to use 1 and 8 H100 GPUs, respectively. One of the key contributors to the cost of GPUs is the high-bandwidth memory (HBM) used to implement GPU main memory, which is several times more expensive than DDR or LPDDR-based memories [54]. Lastly, GPUs provision huge amounts of compute relative to memory bandwidth\u00b2, meaning that a large GPU die is poorly utilized by the primarily memory-bound workload of ENNS [24]."}, {"title": "3.5 Summary", "content": "The analysis presented in this section, using various software and system configurations for RAG applications, led to the following takeaways:\n\u2022 Generation accuracy, time to interactive, and throughput of RAG applications can be improved by using a slower but higher-quality retrieval scheme.\n\u2022 When high-quality retrieval is used, the retrieval phase accounts for a significant portion of end-to-end runtime, regardless of whether the search is performed via ENNS or high-quality ANNS.\n\u2022 Using GPUs to accelerate ENNS is expensive, and GPUs are not able to accelerate high-quality ANNS effectively or affordably.\n\u2022 New accelerators for ANNS are highly complex and task-specific due to the unique requirements of ANNS algorithms, while ENNS relies on a very simple scheme, making ENNS simpler to accelerate than ANNS."}, {"title": "4 Case for Near-Memory ENNS Acceleration", "content": "ENNS is characterized by the following features:\n\u2022 ENNS operations exhibit no data reuse for pair-wise similarity score calculations between corpus vectors and a query vector.\n\u2022 ENNS operations consist of simple vector-vector dot-products coupled with top-K logic.\n\u2022 ENNS has a regular and predictable memory access pattern.\n\u2022 ENNS is highly parallelizable, allowing the corpus to be distributed across different processors with a simple aggregation of top-K similarities at the end.\nThese features make ENNS a prime candidate for near-memory acceleration due to the following reasons: (1) Deep cache hierarchies are not beneficial for ENNS and can even cause slowdown due to the complex cache maintenance and coherency operations managed by the hardware. This is evident from the roofline model in Figure 4 as ENNS running on the CPU cannot saturate the available DRAM bandwidth. (2) The limited data reuse with huge data set size enables low overhead software-managed cache coherency implementation between the host CPU and near-memory accelerators. (3) The regular memory access pattern of ENNS enables coarse-grain virtual to physical address translation on near-memory accelerators. (4) ENNS operations can be efficiently offloaded to a distributed array of near-memory accelerators that each operate in parallel on a shard of corpus data with a low-overhead top-K aggregation phase at the end.\nLeveraging these unique features, we design, implement, and evaluate Intelligent Knowledge Store (IKS), a memory expander with a scale-out near-memory acceleration architecture, uniquely designed to accelerate vector database search in future scalable RAG systems. IKS is designed with three requirements in mind: (1) The memory capacity of IKS should be cost-effective and scalable because the size of vector databases for RAG applications is several tens or hundreds of gigabytes and is likely to increase. (2) The near-memory accelerators should be managed in userspace as the cost of context switches and kernel overhead would reduce the benefits of offloads. (3) The near-memory accelerators and host CPU should implement a shared address space; otherwise, explicit data movements between the CPU and near-memory accelerator address spaces will negate the benefits of near-memory offloads; another issue that GPU acceleration of ENNS suffers from. Moreover, a partitioned address space requires rewriting the entire vector database application, as ENNS is just one operation we want to accelerate near the memory, while other data manipulation operations, such as updates, should be managed by the host CPU.\nWe designed IKS, a type-2 CXL memory expander/accelerator, to meet all these requirements. Our rationale for choosing CXL over DDR-based (or DIMM-based) [4; 35; 62; 111] near-memory processing architecture is that DIMM-based near-memory processing (1) requires sophisticated mechanisms to share the address space between near-memory accelerators and the host [61], (2) limits per-rank memory capacity and compromises the memory capacity of the host CPU when used as an accelerator, and (3) has limited compute and thermal capacity. Instead, IKS relies on asynchronous CXL.mem and CXL.cache protocols to safely share the address space and independently scale the local and far memory capacity of the host CPU, implement a low-overhead interface for offloading from the userspace, and eliminate the limitations on the compute or thermal capacity of the PCIe-attached IKS card. In Section 5, we explain the architecture of IKS and its interface to the host CPU, and how we used it to accelerate end-to-end RAG applications."}, {"title": "5 Intelligent Knowledge Store", "content": "5.1 Overview\nFigure 5 provides an overview of the Intelligent Knowledge Store (IKS) architecture. IKS incorporates a scale-out near-memory processing architecture with low-profile accelerators positioned near the memory controllers of LPDDR5X packages. While IKS can function as a regular memory expander, it is specifically designed to accelerate ENNS over the embedding vectors stored in its LPDDR5X packages.\nAs shown in Figure 5b, IKS utilizes eight LPDDR5X packages, each directly connected to a Near-Memory Accelerator (NMA) that implements both LPDDR5X memory controllers and accelerator logic. Each package contains 512Gb LPDDR5X DRAM with eight 16-bit channels, similar to CXL-PNM [57] and MTIA [16]. One of the key differences between IKS and these architectures is the scale-out near-memory acceleration architecture. IKS distributes the NMA logic over multiple chips, each providing high-bandwidth and low-energy access to its local LPDDR5X package.\nWhy Scale-Out NMA Architecture? The rationale for such a scale-out NMA architecture is to keep the area of the NMA chip in check. Because memory PHYs are only implemented at the shoreline of a chip [16; 51; 60], to implement 64 LPDDR5X memory channels, we need a chip with an approximate perimeter of 160 mm. This is because each LPDDR5X channel PHY approximately occupies a shoreline of 2.5 mm, based on the die shots of Apple M2 [59] in 5nm technology. A square-shaped chip with a 160 mm perimeter has an area of 1600 mm\u00b2, which is larger than the state-of-the-art lithography reticle limit [98]. Although we can technically manufacture such a large accelerator using chiplets, the area of this huge multi-chip module would be wasted, as it is much larger than what is needed to implement the NMA logic, memory controllers, and PCIe/CXL controllers. For context, the area of an H100 GPU is 814 mm\u00b2.\nSplitting the NMAs into smaller chips increases the aggregate chip shoreline and improves yield. Using one NMA per LPDDR5X package requires only eight LPDDR5X memory channels per NMA, necessitating a minimum chip perimeter of 20 mm. IKS implements \u00d72 PCIe 5.0 to provide a 8 GBps uplink connecting each NMA to the CXL controller. With this design, the uplinks to the CXL controller are oversubscribed. Nevertheless, this oversubscription is neither a bottleneck for IKS operating in acceleration mode nor for IKS operating in memory expander mode. In acceleration mode, the bandwidth of local LPDDR5X channels is utilized for dot product calculations, and in memory expander mode, the data is interleaved over multiple LPDDR5X packages and read in parallel over the multiple \u00d72 PCIe uplinks.\nIKS is a type 2 CXL device. IKS's internal memory is exposed as host-managed device memory where both the CPU and IKS can cache addresses within this unified address space (Figure 5a). IKS leverages the low-latency accesses of CXL.mem and CXL.cache protocols to implement a novel interface between the near-memory accelerators and the CPU that: (1) eliminates the need for DMA setup and buffer management, and (2) eliminates the overhead of interrupt and polling for implementing notifications between the CPU and near-memory accelerators (\u00a75.3).\nIKS supports spatial and coarse-grain temporal multi-tenancy. In spatial multi-tenancy, the IKS driver partitions embedding vectors that belong to different vector databases across different packages, allowing each NMA to execute ENNS independently per vector database. For temporal multi-tenancy, the IKS driver time-multiplexes similarity search in NMAs among different vector databases that store their embedding vectors in the same LPDDRX5 package. Time multiplexing takes place at the boundary of a complete similarity search.\nWhy LPDDR? For IKS, a customized type-2 CXL device that should support cost-effective high capacity, neither HBM (expensive) nor DDR (general-purpose) are good options. LPDDR DRAM packages are integrated as part of system-on-chip designs, resulting in shorter interconnections, faster clocking, and less power wastage during data transmission. The most recent release of LPDDR, LPDDR5X, offers a bandwidth of 8533 Mbps per pin, exceeding that of DDR5, which provides a bandwidth of 7200 MTps. However, one challenge with using LPDDR in a datacenter setting is reliability, as LPDDR was originally designed for mobile systems. Although we could provision an in-line ECC processing block for error detection and correction, ENNS similarity search is resilient to bit flips, and rare bit flips in ENNS have negligible impact on the end-to-end RAG accuracy."}, {"title": "5.2 Offload Model", "content": "The IKS address space is shared with the host CPU. The host CPU stores embedding vectors with a specific data layout (that we discuss in Section 5.5) in contiguous physical addresses in IKS, while the actual documents are stored in the host memory (either in DDR memory or CXL memory). The CPU runs the vector database application, which offloads the similarity calculations (i.e., dot-products between the query vectors and embedding vectors) using iks_search(query), a blocking API that does not require a system call or context switch. After each update operation, the vector database application will flush CPU caches to ensure that when iks_search(query) is called, IKS does not contain any stale values.\niks_search(query) hides the complexity of interacting with IKS hardware from the programmer by writing an offload context to IKS and initiates an offload by writing into a doorbell register. The offload context and doorbells are communicated through memory-mapped regions called context buffers to the IKS as shown in Figure 5a. An offload context includes query vectors, vector dimensions, and the base address of the first embedding vector stored in each LPDDR5X package. The host process then uses umwait() to block on the doorbell register (shared between IKS and the host and kept coherent via the CXL.cache protocol) to implement efficient notification between the paused CPU process and near-memory accelerators [105].\nAs IKS uses a scale-out near-memory processing architecture (\u00a75.1), the embedding vectors are distributed across different near-memory accelerators' local DRAM. Therefore, after all the near-memory accelerators complete the offload, the CPU process waiting on umwait() will be notified and execute an aggregation routine to construct a single top-K list. This top-K list is then used to retrieve the actual top-K documents from the host memory. The CPU will locate documents based on the physical addresses of the top-K embedding vectors, as the addresses of the embedding vectors stored in IKS are known a priori."}, {"title": "5.3 Cache Coherent Interface", "content": "IKS leverages the cache-coherent interconnect in CXL.cache to implement an efficient interface between near-memory accelerators and host processes through shared memory. Figure 6 illustrates the transactions through the CXL.cache interface between the host and IKS to initiate and conclude an offload. The host process writes the offload context to the predefined context buffer address range shared between NMAs and the host CPU (step 1). Note that the context buffer is cacheable, and the CPU uses temporal writes to populate the buffers. Next, the host process writes into a doorbell register, which is mapped to a cache line shared by NMAs. NMAs poll on the doorbell register, and as soon as there is a change, the offload starts (step 4). Once the host updates the doorbell register, it calls umwait() to monitor the register for changes from the IKS side.\nBefore computation in the NMA can start, the NMA reads the offload context from the IKS cache (step 5) and the context written by the host is moved to NMA's scratchpad. Once the NMA computation is complete, the NMA updates the context buffers with the partial list of similarity scores and physical addresses of the corresponding embedding vectors. Lastly, the NMA writes into the doorbell register, and the host gets notified of the completion of the offload through the umwait() mechanism (step 11).\nOur experimental results on a two-socket Sapphire Rapids CPU show that communicating the offload context through cache-coherent shared memory provides 1.6\u00d7 higher throughput compared with using non-temporal writes that mimic the PCIe MMIO datapath (i.e., CXL.io). Using a cache-coherent interconnect to implement the notification mechanism through the producer/consumer-style doorbell register eliminates the need for expensive interrupt or polling mechanisms."}, {"title": "5.4 \u039d\u039c\u0391 Architecture", "content": "As shown in Figure 5c, each NMA implements 64 processing engines to accommodate similarity score calculations for up to 64 query vectors in parallel. Each processing engine includes a query scratchpad, dot-product unit, Top-K unit, and output scratchpad. There is a central control unit in each NMA that generates memory accesses, controls data movement within the NMA, and activates processing engines based on the number of query vectors provided by the host CPU. The network-on-chip implements a fixed broadcast network from DRAM to all the processing engines to reuse data when multiple processing engines are active and evaluate similarity scores against different query vectors.\nAs shown in Figure 5d, the dot-product unit includes 68 MAC units, each operating at a 1 GHz frequency and providing 68 GFLOPS (16-bit floating point multiply-accumulate operations) compute throughput; therefore saturating the 136 GBps memory bandwidth of the LPDDR5X channels. Each MAC unit evaluates the similarity score between the query (stored in the query scratchpad) and an embedding vector that is read from DRAM in VD (Vector Dimension) cycles. All the processing engines operate on the same data that is read from the DRAM; in other words, each processing engine evaluates the similarity score between different query vectors and the same set of embedding vectors. Therefore, for a batch size of one, only one processing engine is utilized, and for a batch size of 64, all the processing engines are utilized. This way, we reuse the embedding vectors that are read from DRAM across different batch sizes.\nAs illustrated in Figure 5d, within an active dot-product unit, 68 MAC operations are performed in each clock cycle. The first input of the MAC units is dimension j of the query vector in processing engine PE (QV[PE", "j": "and the second input is dimension j of the embedding vectors i to i+67 read from DRAM. As mentioned earlier, it takes VD (Vector Dimension) cycles for a dot-product unit to evaluate the similarity score for a block of 68 embedding vectors. Once the similarity score is evaluated, it is loaded into a score register (shown in Figure 5d) in the next clock cycle, and the"}]}