{"title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling", "authors": ["Shuhuai Ren", "Shuming Ma", "Xu Sun", "Furu Wei"], "abstract": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11x speedup. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600, demonstrating the scalability of our approach.", "sections": [{"title": "1. Introduction", "content": "The advance of Large Language Models (LLMs) such as ChatGPT (OpenAI, 2023), GPT-4 (Achiam et al., 2023) and LLAMA (Touvron et al., 2023) has cemented the preeminence of Autoregressive (AR) modeling in the realm of natural language processing (NLP). This AR modeling approach, combined with the decoder-only Transformer architecture (Vaswani et al., 2017), has been pivotal in achieving advanced levels of linguistic understanding, generation, and reasoning (Wei et al., 2022; OpenAI, 2024a; Chen et al.). Recently, there is a growing interest in extending AR modeling from language to other modalities, such as images and videos, to develop a unified multimodal framework (OpenAI, 2024b; Team, 2024; Lu et al., 2023; Wu et al., 2023; Chen et al., 2024). Such an AR-based framework brings numerous benefits: (1) It allows for the utilization of the well-established infrastructure and learning recipes from the LLM community (Dao et al., 2022; Kwon et al., 2023); (2) The scalability and generalizability of AR modeling, empirically validated in LLMs (Kaplan et al., 2020; Yu et al., 2023b), can be extended to the multimodal domains to strengthen models (Henighan et al., 2020); (3) Cognitive abilities observed in LLMs can be transferred and potentially amplified with multimodal data, moving closer to the goal of artificial general intelligence (Bubeck et al., 2023).\nGiven the inherently autoregressive nature of video data in temporal dimensions, video generation is a natural area for extending AR modeling. Vanilla AR methods for video generation typically follow the Next-Token Prediction (NTP) approach, i.e., tokenize video into discrete tokens, then predict each subsequent token based on the previous ones. However, this approach has notable limitations. First, the generation order of NTP often follows a unidirectional raster-scan pattern (Hong et al., 2023; Wang et al., 2024a; Yan et al., 2021), which fails to capture strong 2D correlations within video frames, limiting the modeling of spatial dependencies (Tian et al., 2024). Second, NTP necessitates a significant number of forward passes during inference (e.g., 1024 steps to generate a 16-frame clip), which reduces efficiency and increases the risk of error propagation (Bengio et al., 2015).\nIn this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. To better model local spatial dependencies and improve inference efficiency, our framework shifts the generation unit from individual tokens to blocks (e.g., rows or frames). The objective is also redefined from next-token"}, {"title": "2. Related Work", "content": "Video Generation. Prevalent video generation frameworks in recent years include Generative Adversarial Networks (GANs) (Yu et al., 2022; Skorokhodov et al., 2021), diffusion models (Ho et al., 2022; Ge et al., 2023; Gupta et al., 2023; Yang et al., 2024), autoregressive models (Hong et al., 2023; Yan et al., 2021; Kondratyuk et al., 2023), etc. GANs can generate videos with rich details and high visual realism, but their training is often unstable and prone to mode collapse. In contrast, diffusion models exhibit more stable training processes and typically produce results with greater consistency and diversity (Yang et al., 2022). Nevertheless, AR models demonstrate significant potential for processing multi-modal data (e.g., text, images, audio, and video) within a unified framework, offering strong scalability and generalizability. To align with the trend of natively multimodal development (OpenAI, 2024b), this paper focuses on exploring video generation using AR modeling.\nAutoregressive Models for Video Generation. With the success of the GPT series models (Brown et al., 2020), a range of studies has applied AR modeling to both image (Chen et al., 2020; Lee et al., 2022; Wang et al., 2024c; Pang et al., 2024) and video generation (Hong et al., 2023; Wang et al., 2024a; Yan et al., 2021). For image generation, traditional methods divide an image into a sequence of tokens following a raster-scan order and then predict each subsequent token based on the preceding ones. In video generation, this process is extended frame by frame to produce temporally-coherence content. However, conventional AR models predict only one token at a time, resulting in a large number of forward steps during inference. This significantly impairs the generation speed, especially for high-resolution images or videos containing numerous tokens (Liu et al., 2024).\nSemi-Autoregressive Models. To improve the efficiency of AR models, early NLP researchers has explored semi-autoregressive modeling by generating spans of tokens instead of individual tokens per step (Wang et al., 2018). However, due to the variable length of text generation targets, it is challenging to predefine span sizes. Furthermore, fixed-length spans can disrupt semantic coherence and completeness, leading to significant degradation in generation quality; for instance, using a span length of 6 results in a 12% drop in performance for English-German translation tasks (Wang et al., 2018). More advanced semi-AR approaches, such as parallel decoding (Stern et al., 2018) and speculative decoding (Xia et al., 2023), typically use multiple output heads or additional modules (e.g., draft models) to predict several future tokens based on the last generated token (Gu et al., 2017; Gloeckle et al., 2024). In the context of video, where content can be uniformly decomposed into equal-sized blocks (e.g., row by row or frame by frame), we propose a framework where each token in the last block predicts the corresponding token in the next block, without requiring additional heads or modules.\nMulti-token Prediction in Image Generation. Recent work in the image generation field has also shown a pattern of multi-token prediction, albeit with different motivations and approaches. For example, VAR (Tian et al., 2024) employs a coarse-to-fine strategy across resolution scales, whereas our method processes spatiotemporal blocks at original resolution, achieving over 2\u00d7 token efficiency (256 vs. 680 tokens for a 256\u00d7256 frame). Unlike MAR (Li et al., 2024), which relies on randomized masking (70% mask rate) and suffers from partial supervision (30% of unmasked tokens do not receive supervision), our approach eliminates mask token modeling entirely, ensuring full supervision and improved training efficiency. While other works explore specialized token combinations (Li et al., 2023; Wang et al., 2024b), our method minimizes architectural priors, enabling seamless adaptation from pre-trained NTP models and superior performance, especially for video generation."}, {"title": "3. Method", "content": "In this section, we first introduce our video tokenizer \u00a7 3.1, highlighting its two key features: joint image-video tokenization and temporal causality, both of which facilitate our semi-AR modeling approach. Next, we provide a detailed comparison between vanilla Next-Token Prediction (NTP) (\u00a7 3.2) and our Next-Block Prediction (NBP) modeling (\u00a7 3.3). Our NBP framework employs a block-wise objective function and attention masking, enabling more efficient capture of spatial dependencies and significantly improving inference speed."}, {"title": "3.1. Preliminary I: Video Tokenization", "content": "We reproduce closed-source MAGVITv2 (Yu et al., 2024) as our video tokenizer, which is based on a causal 3D CNN architecture. Given a video $X \\in \\mathbb{R}^{T\\times H \\times W \\times 3}$ in RGB space,\u00b9 MAGVITv2 encodes it into a feature map $Z \\in \\mathbb{R}^{T'\\times H'\\times W'\\times d}$, where (T', H', W') is the latent size of Z, and d is the hidden dimension of its feature vectors. After that, we apply a quantizer to convert this feature map Z into a discrete tokens map $Q \\in \\mathbb{V}^{T'\\times H'\\times W'}$ (illustrated in Fig. 1), where $\\mathbb{V}$ represents a visual vocabulary of size V = K. After tokenization, these discrete tokens Q can be passed through a causal 3D CNN decoder to reconstruct the video X. We note that MAGVITv2 has two major advantages:\n(1) Joint Image-Video Tokenization. MAGVITv2 allows tokenizing images and videos with a shared vocabulary. To achieve this, the number of frames in an input video, T, must satisfy T = 1 + n \u00d7 Fr, meaning the video comprises an"}, {"title": "3.2. Preliminary II: Autoregressive Modeling for Video Generation", "content": "Inspired by the success of AR models in the field of NLP, previous work (Yan et al., 2021; Wu et al., 2021a;b) has extended AR models to video generation. Typically, these methods flatten the 3D video token input $Q\\in \\mathbb{V}^{T'\\times H\\times W'}$ into a 1D token sequence. Let $C^{(t)} = \\{ x_1^{(t)}, x_2^{(t)},...,x_L^{(t)} \\}$ be the set of tokens in the $t$th clip, where $L = H' \u00d7 W' = |C^{(t)}|$ is the total number of tokens in each clip, and every clip contains an equal number of tokens. Specially, when t = 0, $C^{(0)}$ denotes the first frame's tokens. Therefore, the 1D token sequence can be represented as $(C^{(0)},...,C^{(T')}) = ( x_1^{(0)}, x_2^{(0)},...,x_L^{(0)},..., x_1^{(T')}, x_2^{(T')},...,x_L^{(T')} )$. In the AR framework, the next-token probability is conditioned on the preceding tokens, where each token $x_l^{(t)}$ depends only on its prefix $( x_{1:<t}^{(0)}, x_l^{(t)} )$. This unidirectional dependency allows the likelihood of the 1D sequence to be factorized as:\n$$P( x_1^{(0)},...,x_L^{(T')}) = \\prod_{t=1}^{T'} \\prod_{l=1}^{L} p(x_l^{(t)} | x_{1:<t}^{(0)}, x_l^{(t)}) $$\nSince only one token is predicted per step, the inference process can become computationally expensive and time-consuming (Liu et al., 2024), motivating the exploration of more efficient methods, such as semi-AR models (Wang et al., 2018), to improve both speed and scalability."}, {"title": "3.3. Semi-AR Modeling via Next Block Modeling", "content": "In contrast to text, which consists of variable-length words and phrases, video content can be uniformly decomposed into equal-sized blocks (e.g., rows or frames). Fig. 2 shows examples of token-wise, row-wise, and frame-wise block representations. Based on this, we propose a semi-autoregressive (semi-AR) framework named Next-Block Prediction (NBP), where each token in the current block predicts the corresponding token in the next block. Fig. 3 illustrates an example of next-clip prediction, where each clip is treated as a block, and the next clip is predicted based on the preceding clips. This approach introduces two key differences compared to vanilla NTP modeling: (1) Change in the generation target. In NBP, the $l$th token $x_l^{(t)}$ in the $t$th clip predicts $x_l^{(t+1)}$ in the next clip, rather than $x_{l+1}^{(t)}$ as in NTP. (2) Increase in the number of generation targets. Instead of predicting one token at a time, all L tokens $x_l^{(t)}$ simultaneously predict the corresponding L tokens $x_{1:L}^{(t+1)}$ in the next clip. Accordingly, the NBP objective function can be expressed as:\n$$P( x_1^{(0)},...,x_L^{(T')}) = \\prod_{t=1}^{T'} p(x_{1:L}^{(t)} | x_{1:L}^{(0)},...,x_{1:L}^{(t-1)}) $$\nBy adjusting the block size, the framework can generate videos using different generation units. To ensure the effectiveness of this approach, four key components are designed:\n(1) Initial Condition. In NTP models, a special token (e.g., [begin_of_video]) is typically used as the initial condition. In the NBP setting, we can introduce a block of special tokens to serve as the initial condition for generating the first block. However, our preliminary experiments revealed that learning the parallel generation from the special token block to the first block is quite challenging. To address this issue, we propose two methods: (i) Taking the first frame $C^{(0)}$ as the initial condition. In practice, following Girdhar et al. (2023), users can upload an image as the first frame, or call an off-the-shelf text-to-image model (e.g., SDXL (Podell et al., 2023)) to generate it. (ii) Adopting a hybrid generation process (Wang et al., 2024b). Specifically, we can use per-token AR generation for the tokens in the first block. After the first block is generated, we then shift to per-block semi-AR generation. In order to make a fair comparison with other baselines, we used method (ii) in our experiments rather than relying on an extra first frame. Lastly, we note that both NTP and NBP models can accept various inputs (e.g., text) as additional conditions (see Fig. 3).\n(2) Block-wise Attention. To better capture spatial dependency, we allow tokens to attend to all tokens within the same block via bidirectional attention. Fig. 4 compares tra-"}, {"title": "4. Experiments", "content": "4.1. Experimental Setups\nVideo Tokenizer. As MAGVITv2 is not open-sourced, we implemented it based on the original paper. In contrast to the official implementation, which utilizes LFQ (Yu et al., 2024) as its quantizer, we adopt FSQ (Mentzer et al., 2023) due to its simplicity and reduced number of loss functions and hyper-parameters. Following the original paper's recommendations, we set the FSQ levels to [8, 8, 8, 5, 5, 5], and the size of the visual vocabulary is 64K. Moreover, we employ PatchGAN (Isola et al., 2016) instead of StyleGAN (Karras et al., 2018) to enhance training stability. The reconstruction performance of our tokenizer is presented in Table 7, and additional training details are available in Appendix A.2. We note that MAGVITv2 is not open-sourced, we have made every effort to replicate its results. Our tokenizer surpasses OmniTokenizer (Wang et al., 2024a), MAGVITv1 (Yu et al., 2023a), and other models in performance. However, due to limited computational resources, we did not pre-train on ImageNet (Russakovsky et al., 2014) or employ a larger visual vocabulary (e.g., 262K as in the original MAGVITv2), which slightly impacts our results compared to the official MAGVITv2. Nevertheless, we note that the primary objective of this paper is to validate the semi-AR framework, rather than to achieve state-of-the-art tokenizer performance.\nGenerator Training Details. We train decoder-only transformers on 17-frame videos with a resolution of 128\u00d7128, using the UCF-101 (Soomro et al., 2012) and K600 (Carreira et al., 2018) datasets. With spatial downsampling factors of $F_H = F_W = 8$ and temporal downsampling of $F_T = 4$, the resulting 3D token map for each video sample has dimensions $(T', H', W') = (5, 16, 16)$, yielding a total of 1280 tokens. We train our model for 100K steps with a total batch size of 256 and 64 respectively. Model sizes range from 700M to 3B parameters, with training spanning approximately two weeks on 32 NVIDIA A100 GPUs. The full model configuration and training hyper-parameters are provided in Appendix A.2. We train the models from scratch, rather than initializing from a pre-trained LLM checkpoint, as these text-based checkpoints provide minimal benefit for video generation (Zhang et al., 2023). We use LLaMA (Touvron et al., 2023) vocabulary (32K tokens) as the text vocabulary and merge it with the video vocabulary (64K tokens) to form the final vocabulary. Since our primary focus is video generation, we compute the loss only on video tokens, which leads to improved performance.\nEvaluation Protocol. We evaluate our models on the UCF-101 dataset for class-conditional generation task and the K600 dataset for frame prediction task. To assess video quality, we use the standard metric of Fr\u00e9chet Video Distance (FVD)(Unterthiner et al., 2018). Additional evaluation"}, {"title": "4.2. Comparison of Next-Token Prediction and Next-Block Prediction", "content": "We first conduct a fair comparison between next-token prediction (NTP) and our next-block prediction (NBP) under the same experimental setting. All experiments are performed on the K600 dataset, which has a much larger data volume compared to UCF-101 (413K vs. 9.5K) and features a strict training-test split, thereby ensuring more generalizable results. Table 1 highlights the superiority of our approach in three key aspects: generation quality, inference efficiency, and scalability.\nGeneration Quality. Across all model sizes, NBP with a 1\u00d71\u00d716 block size consistently outperforms NTP models in terms of generation quality (measured by FVD). For instance, the 700M NBP model achieves an FVD of 33.6, outperforming the NTP model by 3.8 points. Furthermore, a NBP model with only 1.2B parameters achieves a comparable performance to a 3B NTP model (28.6 vs. 29.0 FVD). This suggests that the block size of 1\u00d71\u00d716 is a more effective generation unit for autoregressive modeling in the video domain.\nInference Efficiency. To generate a 12-frame video (128x128 resolution, 768 tokens), a 700M NTP model requires 768 forward steps during inference, taking 15.04 seconds (FPS=0.80). In contrast, our NBP model with a 1\u00d71\u00d716 block size predicts all tokens in a row simultaneously, requiring only 48 steps and 1.35 seconds to generate the video (FPS=8.89)\u2014over 11 times faster than the NTP model. Since NBP modifies only the target output and attention mask, it is compatible with the most efficient AR inference frameworks, such as memory-efficient attention (Lefaudeux et al., 2022), offering the potential for further speed improvements. We now briefly discuss the sources of these efficiency gains. In scenarios utilizing KV-Cache, the overall computation cost during each inference step for NTP involves multiplying vectors (current token) with matrices (model weights), which is primarily IO-bound due to the movement of matrices. Conversely, in the NBP model, the computation involves multiplying matrices (current block) with matrices (model weights), making it compute-bound, with reduced IO overhead due to larger block sizes. Given this distinction and assuming adequate GPU parallelism, the NBP framework can achieve significantly faster speeds compared to NTP. This efficiency gain is due to the reduced frequency of IO operations and the more effective utilization of computational resources in processing larger data blocks simultaneously.\nScalability. As model size increases from 700M to 1.2B and 3B parameters, the FVD of NBP models improves from 33.6 to 28.6 and 26.5, respectively. This demonstrates that NBP exhibits similar scalability to NTP models, with the potential for even greater performance as model size and computational resources increase. Fig. 5 and Fig. 12 present the validation loss curves and generation examples for different model sizes, respectively. As the models grow larger, the generated content exhibits greater stability and enhanced visual detail."}, {"title": "Ablation Study on Block Size", "content": "We experiment with different block sizes, ranging from [1, 8, 16, 32, 64, 256]2, to evaluate their impact on model performance. A block size of 1, 16, and 256 corresponds to token-by-token (NTP), row-by-row, and clip-by-clip generation, respectively. Fig. 6 shows the validation loss curves for various block sizes. As block size decreases, learning becomes easier due to the increased prefix conditioning, which simplifies the prediction task and results in lower validation loss. However, due to the exposure bias associated with (semi-)AR modeling (Ranzato et al., 2015), validation loss under the teacher-forcing setting does not completely correlate with final performance during inference (Deng et al., 2024). Notably, the smallest block size (i.e., a single token) does not yield optimal performance. As shown in Fig. 7, a block size of 16 achieves the best generation quality, with an FVD improvement of 3.5 points, reaching 25.5. Block size is critical for balancing generation quality (FVD) and efficiency (FPS). While larger blocks (e.g., 1\u00d716\u00d716) lead to faster inference speeds (up to 17.14 FPS), performance degrades, indicating that generating an entire clip in one step is overly challenging. Additionally, inference decoding methods significantly influence results. As demonstrated in Fig. 13, traditional Top-P Top-K decoding can lead to screen fluctuations (Lezama et al., 2022), as it struggles to model spatial dependencies within large blocks, highlighting the need for improved decoding strategies in NBP scenarios."}, {"title": "Ablation Study on Block Shape", "content": "We explore the performance of various block shapes on K600, using the 700M model, the results are shown in Table 3. Our findings indicate that the official block shape of T\u00d7H\u00d7W=1\u00d71\u00d716 (generating row by row) outperforms other tested shapes such as 1\u00d74\u00d74 and 2\u00d71\u00d78. We attribute this to two main factors: (1) Token Relationships within a Single Block: The shape of the 1\u00d71\u00d716 block allows tokens within the block to represent a complete, continuous row, maintaining integrity without cross-row interruptions. In contrast, block shapes like 1\u00d74\u00d74 and 2\u00d71\u00d78 involve generating complex relationships across multiple rows and columns\u2014or even frames\u2014on a smaller spatial scale, posing greater challenges (Ren et al., 2023). (2) Relationships between Blocks: The 1\u00d71\u00d716 block shape simplifies the modeling process to primarily vertical relationships between rows, which enhances continuity and consistency during generation, thereby reducing breaks and error accumulation."}, {"title": "Analysis of Attention Pattern", "content": "We analyze the attention pattern in our NBP framework using an example of next-clip prediction, where each block corresponds to a clip. Fig. 14 shows the attention weights on UCF-101. Unlike the lower triangular distribution observed in AR models, our attention is characterized by a staircase pattern across blocks. In addition to high attention scores along the diagonal, the map reveals vertical stripe-like highlighted patterns, indicating that tokens at certain positions receive attention from all tokens. Fig. 15 illustrates the spatial attention distribution for a specific query (marked by red \u00d7). This query can attend to all tokens within the clip, rather than being restricted to only the preceding tokens in a raster-scan order, enabling more effective spatial dependency modeling."}, {"title": "5. Conclusion", "content": "In this paper, we introduced a novel approach to video generation called Next Block Prediction using a semi-autoregressive modeling framework. This framework offers a more efficient and scalable solution for video generation, combining the advantages of parallelization with improved spatial-temporal dependency modeling. This method not only accelerates inference but also maintains or improves the quality of generated content, demonstrating strong potential for future applications in multimodal AI."}, {"title": "Impact Statement", "content": "This work advances the field of video generation through the development of NBP. While recognizing the potential of this technology, we carefully consider its societal implications, particularly regarding potential misuse and ethical challenges. The model's capabilities could be exploited to create harmful content, including deepfakes for misinformation campaigns or other malicious purposes. Furthermore, we acknowledge the critical importance of ensuring that generated content adheres to ethical standards by avoiding the perpetuation of harmful stereotypes and respecting cultural diversity. To mitigate these risks, we will explore a comprehensive framework of safeguards, including (1) robust digital watermarking to ensure traceability and accountability of generated content; (2) reinforcement learning with human feedback to align model outputs with ethical guidelines and reduce potential harm; and (3) clear usage policies and restrictions. These measures collectively aim to promote responsible development and deployment of video generation technology while maximizing its positive societal impact."}, {"title": "A. Implementation Details", "content": "A.1. Task Definitions\nWe introduce the tasks used in our training and evaluation. Each task is characterized by a few adjustable settings such as interior condition shape and optionally prefix condition. Given a video of shape $T \u00d7 H \u00d7 W$, we define the tasks as following:\n\u2022 Class-conditional Generation (CG)\nPrefix condition: class label.\n\u2022 Frame Prediction (FP)\nInterior condition: t frames at the beginning; t = 5 for K600 dataset.\nAs we stated in \u00a7 4.3, for UCF-101, all methods perform the CG task, while for K600, all methods perform the FP task."}, {"title": "A.2. Model Configuration", "content": "Video Tokenizer. Our video tokenizer shares the same model architecture with MAGVITv2 (Yu et al., 2024).\nDecoder-only Generator. Table 4 shows the configuration for the decoder-only generator. We use separate position encoding for text and video. We do not use advanced techniques in large language models, such as rotary position embedding (ROPE) (Su et al., 2024), SwiGLU MLP, or RMS Norm (Touvron et al., 2023), which we believe could bring better performance."}, {"title": "A.3. Training", "content": "Video Tokenizer. Table 5 shows the training configurations of our video tokenizer.\nDecoder-only Generator. Table 6 shows the training configurations of our video generator.\nFor both tokenizer and generator training, the video samples are all 17 frames, frame stride 1, 128\u00d7128 resolution."}, {"title": "A.4. Evaluation", "content": "Evaluation metrics. The FVD (Unterthiner et al., 2018) is used as the primary evaluation metric. We follow the official implementation\u00b3 in extracting video features with an I3D model trained on Kinetics-400 (Carreira & Zisserman, 2017).\nSampling protocols. We follow the sampling protocols from previous works (Yu et al., 2024; Ge et al., 2022; Clark et al., 2019) when eveluating on the standard benchmarks, i.e. UCF-101, and Kinetics-600. We sample 17-frame clips from each dataset without replacement to form the real distribution in FVD and extract condition inputs from them to feed to the model. We continuously run through all the samples required (e.g., 40,000 for UCF-101) with a single data loader and compute the mean and standard deviation for 4 folds. We use top-p and top-k sampling with k = 16,000 and p = 0.9.\nBelow are detailed setups for each dataset:\n\u2022 UCF-101:\nDataset: 9.5K videos for training, 101 classes.\nNumber of samples: 10,000\u00d74.\nResolution: 128\u00d7128.\nReal distribution: random clips from the training videos.\nVideo FPS: 8."}, {"title": "B. Performance of Video Tokenizer", "content": "We present the reconstruction performance of our tokenizer in Table 7. Our tokenizer achieves 15.50 rFVD on UCF-101 and 6.73 rFVD on K600, surpassing OmniTokenizer (Wang et al., 2024a), MAGVITv1 (Yu et al., 2023a), and other models. Fig. 11 compares the video reconstruction results of OmniTokenizer (Wang et al., 2024a) and our tokenizer. Our method significantly outperforms the baseline in both image clarity and motion stability."}, {"title": "C. Visualization", "content": "We provide additional visualization of video generation results. Fig. 12 shows results of various model sizes (700M, 1.2B and 3B). Fig. 13 shows results of various block sizes (1\u00d71\u00d71,1\u00d71\u00d716 and 1\u00d716\u00d716)."}]}