{"title": "GENERAL SCENE ADAPTATION FOR VISION-AND-LANGUAGE NAVIGATION", "authors": ["Haodong Hong", "Yanyuan Qiao", "Sen Wang", "Jiajun Liu", "Qi Wu"], "abstract": "Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments. To better reflect these real-world conditions, we introduce GSA-VLN (General Scene Adaptation for VLN), a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time. To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of out-of-distribution (OOD) data, and the limited number and style diversity of instructions for each scene. Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the Room-to-Room (R2R) dataset to evaluate agent adaptability in both ID and OOD contexts. Furthermore, we design a three-stage instruction orchestration pipeline that leverages large language models (LLMs) to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles. This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions, taking the use case of home robotic assistants as an example. We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods, revealing key factors enabling agents to adapt to specific environments. Based on our findings, we propose a novel method, Graph-Retained DUET (GR-DUET), which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits. The dataset and code are available at https://github.com/honghd16/GSA-VLN.", "sections": [{"title": "INTRODUCTION", "content": "Vision-and-Language Navigation (VLN) (Anderson et al., 2018b) aims to enable agents to navigate to a specific destination following language instructions. Traditional VLN researches (Qi et al., 2020; Thomason et al., 2020) mainly focus on evaluating agents using strictly unseen instructions and environments to assess their generalization capabilities. This \u201cunseen\u201d criterion applies not only to the data between training and evaluation phases, but also to individual evaluation instances, where agents are tested on each instruction-trajectory pair without prior knowledge of the environment.\nHowever, this setting diverges from practical navigation situations. In real-world applications, such as indoor household robots, agents actually operate in a consistent environment over time. As they execute more instructions, the working environment generally transitions from \"unseen\" to \"seen\" and agents become increasingly familiar with the environment. This leaves room for agents to adapt and improve their performance through environmental adaptation, which can significantly"}, {"title": "RELATED WORK", "content": "Vision-and-Language Navigation (VLN). VLN tasks involve agents following natural language instructions to reach a specified target (Gu et al., 2022; Zhang et al., 2024), a challenge first introduced with the Room-to-Room (R2R) dataset (Anderson et al., 2018b). Since then, numerous indoor VLN datasets emerged (Krantz et al., 2020; Jain et al., 2019), which mainly focus on varying the textual inputs, such as high-level object-oriented instructions (Qi et al., 2020; Zhu et al., 2021), multilingual instructions (Ku et al., 2020), and multi-modal instructions (Hong et al., 2024). However, they are all based on the same scenarios from the Matterport3D dataset (Chang et al., 2017). Many works propose to introduce novel scenarios by incorporating other datasets (Chen et al., 2022b; Wang et al., 2023b) or utilizing web data (Lin et al., 2023). However, all these methods are only used as additional training data with evaluations still being conducted on the original splits with limited diversity. In contrast, we include diverse environments and instructions in the evaluation splits to fully evaluate agent adaptability in both ID and OOD contexts.\nAdaptation Methods in VLN. Although no prior works in VLN have addressed the problem of single-scene adaptation, some studies offer potential solutions, such as the pre-explore setting (Wang et al., 2019). We categorize them into two kinds. The first is optimization-based methods, where the parameters of the navigation model are updated within the target environment. TTA methods like TENT (Wang et al., 2021) and SAR (Niu et al., 2023) further optimize the model parameters with the objective of entropy minimization, while Back-Translation (Wang et al., 2020) uses a trained speaker to generate instructions for imitation learning. The second is memory-based methods, which explicitly store information about seen places and instructions to help decision-making, as seen in methods of IVLN (Zhao et al., 2024) and RREx-BoT (Sigurdsson et al., 2023). For example, TourHAMT (Krantz et al., 2023) incorporates previous episodes as additional history embeddings, while OVER-NAV (Zhao et al., 2024) detects keywords from instructions within observations and stores the results in an Omnigraph to assist navigation. Similarly, SG-Nav (Yin et al., 2024) constructs consistent 3D scene graphs to represent environments, offering a powerful approach for recording and utilizing historical information. Recently, LLM-based VLN methods, such as InstructNav (Long et al., 2024) and NavCoT (Lin et al., 2024), have demonstrated strong zero-shot navigation performance, highlighting their potential to address the scene adaptation problem.\nPersistent Environments in VLN. The concept of agents operating in persistent environments has led to several popular tasks in embodied AI, such as multi-object navigation (Wani et al., 2020) and multi-target embodied QA (Yu et al., 2019). Iterative VLN (IVLN) (Krantz et al., 2023) introduces this idea to VLN by organizing all instructions in sequence to form a long-horizon tour and enable memory throughout the tour. Our task differs from IVLN in two key aspects. First, while both tasks focus on enhancing agent performance within a single environment, IVLN emphasizes long-horizon navigation with a single tour per environment, whereas our GSA-VLN focuses on enabling agents to adapt to each environment from a diverse range of visual buildings and instruction types. Second, IVLN only includes a limited number of trajectory-instruction pairs for each environment, making it not suitable for optimization-based ones."}, {"title": "TASKS AND DATASETS", "content": "3.1 PRELIMINARIES\nIn VLN task, the agent is required to follow a given natural language instruction X $(X_1,X_2, ..., X_L)$ consisting of L words to navigate to the target destination. Specifically, the agent is initially placed at the start node vo. At each time step t, the agent predicts an action at to move to one of the neighboring nodes in the connectivity graph G of the environment, guided by the visual observation Ot, the language instruction X, and the history of previous steps $H_t = \\{O_0, a_0, O_1, a_1, \u00b7\u00b7\u00b7, O_{t-1}, a_{t-1}\\}$. Typically, the observation Ot is represented as a panorama composed of N = 36 discrete views $O_t = \\{o_t^i\\}_{i=1}^N$. This process continues until the agent either reaches the predefined step limit or selects the special [STOP] action.\n3.2 THE GSA-VLN TASK\nMost VLN tasks evaluate each instruction independently, initializing the history as empty ($H_0 = \\emptyset$) and keeping the model parameters fixed ($\\theta = \\theta_0$). While this setting effectively assesses the agent's ability to interpret and follow isolated instructions, it fails to capture the continuity and adaptation required in real-world navigation scenarios. In practical applications, both environments and instruction styles remain consistent over time, enabling agents to accumulate and leverage contextual knowledge to enhance performance. To address this limitation, we propose the GSA-VLN task to introduce the challenge of single-scene adaptation, enabling agents to continuously improve as they execute instructions in previously unseen environments.\nSpecifically, GSA-VLN introduces an environment-specific memory bank $M_E$, which stores historical information from all executed episodes within a given environment E. This memory bank dynamically expands as the agent executes instructions, capturing four key components: visual observations (O), the instructions (X), selected actions (A), and trajectory paths (P). For example, after executing k instructions in environment E, the memory bank is updated as follows:\n$M_E = \\{X_{1:k}, O_{1:k}, A_{1:k}, P_{1:k}\\}$\nExternally, agents in GSA-VLN behave similarly to those in standard VLN tasks when executing instructions. However, internally, the agent can leverage the memory bank to adapt to the current working environment for better performance, depending on the method employed. The stored memories in the memory bank represent the execution history of agents, although there may exist misalignment between instructions and paths due to navigation errors, all the memories are treated as unlabeled data and are primarily used for unsupervised learning techniques.\nThere are two primary distinctions between GSA-VLN and standard VLN tasks. First, the agent can access the memory bank to retrieve long-term history, $H_0 = M'_E \\subseteq M_E$, instead of beginning each navigation episode without prior knowledge:\n$\\alpha_0 = \\pi(O_0, X, H_0; \\theta)$\nSecond, the parameters of agents can be updated during the navigation process by employing unsupervised learning techniques on data from the memory bank:\n$\\theta' = \\theta - \\alpha \\nabla L(M_E, \\theta)$,\nwhere $\\theta$ denotes the parameters of the navigation model. Notably, While GSA-VLN aims to develop environment-specific agents $\\theta'$, the initial model $\\theta_0$ should be general enough to be applied to various environments, maintaining a level of environment-agnosticism:\n$\\max_{\\theta_0} E_{E \\sim \\mathcal{E}} [P(E; \\theta' (\\theta_0))]$\nwhere $\\mathcal{E}$ represents the environment distribution, and $P(E; \\theta' (\\theta_0))$ denotes the agent performance in environment E with updated parameters $\\theta'$, which are adapted from the initial parameters $\\theta_0$.\nWe address differences between GSA-VLN and the related areas. Unlike lifelong learning (Liu, 2017), which focuses on acquiring multiple skills over time, our work emphasizes repeated mastery of the same navigation skill within a scene-specific context. While TTA (Gao et al., 2024) adapt an agent during inference without supervision, our approach extends TTA by integrating a dynamically updating memory bank, enabling fixed-parameter adaptation with varying inputs."}, {"title": "EXPERIMENTS", "content": "4.1 GR-DUET\nAlthough TourHAMT incorporates history information as additional input, its performance degrades for two reasons. First, representing each step with a single history embedding fails to capture the necessary spatial correlations for modeling visited nodes, especially when the history is extensive. Second, it only fine-tunes the model with new history embedding compositions, leading to a significant input distribution shift between pretraining and fine-tuning. To address these issues, we propose a novel memory-based method, graph-retained dual-scale graph transformer (GR-DUET), where the extended history embeddings are replaced by a global topological graph that retains information across episodes, ensuring comprehensive awareness of visited nodes.\nSpecifically, during inference, instead of maintaining separate graphs {G1, G2,\u2026, Gm} for each episode {EP1, EP2,\u2026\u2026, EPm}, our agent maintains a single global graph Gg to continuously update the topological map while preserving observations at each node throughout the evaluation. At the start t = 0 of episode k, we utilize the data from the memory bank ME to build the topological graph with all previously visited nodes Gg = {G1, G2,\u2026\u2026,Gk\u22121}. Each node represents a visited location, with node information including positions {x, y, z} for spatial alignment and the visual observations O at that node. We reset the visited status of all nodes at the beginning of each episode to enable agents to only choose unvisited nodes as the next step for efficiency. By utilizing this global graph across episodes, GR-DUET can more effectively leverage historical information, enabling a deeper understanding of the environment and facilitating longer-term action planning, particularly after executing numerous instructions.\nHowever, directly applying this mechanism in evaluation leads to the same distribution shift issue. Therefore, we modify the pretraining and fine-tuning stages to align the input distribution between training and inference. During pretraining, we provide the model with the complete ground truth"}, {"title": "EXPERIMENTAL SETUP", "content": "Baseline Methods We include two types of baseline methods. One is adaptation-based methods, including TENT (Wang et al., 2021), SAR (Niu et al., 2023), Back-Translation (BT), and proxy tasks (MLM (Devlin, 2018) & MRC (Lu et al., 2019)). The other is memory-based methods, including TourHAMT (Krantz et al., 2023) and OVER-NAV (Zhao et al., 2024). More details are provided in the appendix.\nEvaluation Metrics. We use the following metrics to evaluate the navigation performance: (1) Trajectory Length (TL): the total navigation distance in meters; (2) Navigation Error (NE): the distance between the stop location and the target; (3) Success Rate (SR): the ratio of agents stopping within 3 meters of the target; (4) Success rate weighted by Path Length (SPL) (Anderson et al., 2018a): SR normalized by the ratio between the shortest path length and the predicted path length. (5) Normalized Dynamic Time Warping (nDTW) (Ilharco et al., 2019): a measure of instruction fidelity by computing the similarity between the reference path and the predicted path.\nImplementation Details. We use GPT-40 from OpenAI's official API as the LLM for generating Scene and User instructions. All prompt templates are provided in the appendix. For baseline methods, we follow the implementation details in their official repositories, with two modifications. First, we use CLIP-ViT/B-16 (Radford et al., 2021) as the visual feature extractor for both the navigation and speaker models for fair comparison. Second, all models are evaluated using a batch size of 1 in an online manner during evaluation. In GR-DUET, we set the maximum number of episodes a = 50. The best model is selected based on the average SPL across all validation splits. For each adaptation method, we conduct the evaluation three times with randomly sequenced instructions and report the mean and standard error for each metric."}, {"title": "MAIN RESULTS", "content": "In this section, we benchmark existing VLN methods and adaptation methods to show their performance for the GSA-VLN task.\n4.3.1 HoW DO CURRENT VLN METHODS PERFORM IN GSA-R2R?\nVarious techniques have been incorporated into current VLN methods to achieve human-like performance in R2R, demonstrating their strong reasoning capabilities. We evaluate these methods without adaptation techniques to determine whether they can maintain the same performance in our diverse environments and instructions, as shown in Tab. 3. Except for the data leakage in ScaleVLN 3, the performance of other baselines in GSA-R2R is significantly lower than their performance in R2R, highlighting the challenges of our task. When comparing different environments, agents are better at residential scenes than non-residential ones due to the biased distribution of the training data. For different instruction types, all models perform best on Basic instructions, followed by User instruc-"}, {"title": "CONCLUSION", "content": "In this paper, we introduce the GSA-VLN task to highlight the challenges faced by VLN agents operating in persistent environments, where long-term memory and model updates are required to adapt to specific settings. To thoroughly evaluate agent adaptability, we create the GSA-R2R dataset, which significantly expands the quantity and diversity of environments and instructions, including both ID and OOD data for evaluation. We benchmark popular VLN models and adaptation methods on GSA-R2R and propose GR-DUET, a novel model that integrates global graphs with an environment-specific training strategy, achieving state-of-the-art results. In the future, we aim to explore more unsupervised learning approaches to further enhance agent performance in GSA-R2R."}, {"title": "APPENDIX", "content": "This document provides additional method details, supplementary experiments, and further analysis to complement the main paper, including:\n\u2022 Appendix A.1: detailed descriptions of the baseline methods.\n\u2022 Appendix A.2: more explanations of the process for generating the GSA-R2R dataset.\n\u2022 Appendix A.3: additional statistics and visualizations of the GSA-R2R dataset.\n\u2022 Appendix A.4: discussion of the feasibility of deploying GR-DUET in real-time systems.\n\u2022 Appendix A.5: detailed analysis of the scalability of GR-DUET.\n\u2022 Appendix A.6: comparison with LLM-based VLN methods.\n\u2022 Appendix A.7: quantitative analysis of the adaptation speed of GR-DUET.\n\u2022 Appendix A.8: more details about the human study.\n\u2022 Appendix A.9: Detailed justification for selecting five characters for User instructions.\n\u2022 Appendix A.10: prompt templates used for generating GSA-R2R.\n\u2022 Appendix A.11: discussions on the limitations and future directions of this work.\nA.1 BASELINE METHODS\nIn this section, we provide detailed descriptions of the baseline adaptation methods used in our experiments, which are categorized into two types: optimization-based methods, which update model parameters during evaluation, and memory-based methods, which use data from the memory bank as additional input.\nA.1.1 DUET DETAILS\nWe adopt DUET (Chen et al., 2022c) as our baseline model, which is an enhanced version of HAMT (Chen et al., 2021). The HAMT model employs a transformer-based network to encode instructions, visual observations, and navigation history. These components are first processed by individual encoders and subsequently integrated through a cross-modal transformer (Vaswani, 2017). Building on the foundation of HAMT, DUET introduces a real-time topological map to track visited nodes and leverages graph transformers to enable global action decisions. Unlike HAMT, which is restricted to selecting actions only from neighboring nodes of the current location-leading to navigation inefficiencies\u2014DUET expands its action space to include all nodes in the topological map. This allows DUET to efficiently navigate to distant nodes using a path planner, significantly improving navigation efficiency. Moreover, DUET employs a dual-level encoding architecture to encode both fine-grained features of visual observations and coarse-grained features of the topological map. These representations are fused with instructions to capture cross-modal relationships, facilitating more effective action predictions. For agent training, DUET adopts the two-stage training paradigm introduced by HAMT, consisting of pretraining on proxy tasks and fine-tuning on downstream tasks. During fine-tuning, DUET employs a pseudo-interactive demonstrator to enhance exploration, thereby improving generalization performance.\nA.1.2 OPTIMIZATION-BASED ADAPTATION METHODS\nDue to the lack of ground-truth paths, optimization-based adaptation methods rely on unsupervised and self-supervised training strategies to adapt the model to specific environments.\nEntropy-Based Methods. Online Test-Time Adaptation (TTA) methods (Liang et al., 2024) focus on minimizing the entropy of agent predictions to make agents more confident in their decisions by exploiting the positive correlation between entropy and errors. Since the VLN task is formulated as a classification task on neighboring viewpoints, these methods are directly applicable to the GSA-VLN task. We employ two widely used methods in this field, TENT (Wang et al., 2021) and SAR (Niu et al., 2023), to evaluate whether these TTA techniques can help agents adapt to specific environments and speaking styles. TENT takes entropy minimization as an optimization objective"}]}