{"title": "Stochastic first-order methods with multi-extrapolated momentum for highly smooth unconstrained optimization", "authors": ["Chuan He"], "abstract": "In this paper we consider an unconstrained stochastic optimization problem where the objective function exhibits a high order of smoothness. In particular, we propose a stochastic first-order method (SFOM) with multi-extrapolated momentum, in which multiple extrapolations are performed in each iteration, followed by a momentum step based on these extrapolations. We show that our proposed SFOM with multi-extrapolated momentum can accelerate optimization by exploiting the high-order smoothness of the objective function f. Specifically, assuming that the gradient and the pth-order derivative of f are Lipschitz continuous for some p\u2265 2, and under some additional mild assumptions, we establish that our method achieves a sample complexity of O(e-(3p+1)/p) for finding a point x satisfying E[||f(x)||] \u2264 \u20ac. To the best of our knowledge, our method is the first SFOM to leverage arbitrary order smoothness of the objective function for acceleration, resulting in a sample complexity that strictly improves upon the best-known results without assuming the average smoothness condition. Finally, preliminary numerical experiments validate the practical performance of our method and corroborate our theoretical findings.", "sections": [{"title": "1 Introduction", "content": "In this paper we consider the smooth unconstrained optimization problem:\n\nmin f(x),\nxERn (1)\n\nwhere f: Rn \u2192 R is continuously differentiable and has a Lipschitz continuous pth-order derivative for some p \u2265 2 (see Assumption 2 for details). We assume that problem (1) has at least one optimal solution. Our goal is to develop first-order methods for solving (1) in the stochastic regime where the derivatives of f are not directly accessible. Instead, our algorithm relies solely on stochastic estimators G(\u00b7; \u00a7) for the gradient \u2207f(\u00b7), where \u0121 is a random variable with sample space \u039e (see Assumption 1(c) for our assumptions on G).\nIn recent years, there has been significant developments on stochastic first-order methods (SFOMS) with sample complexity guarantees for solving problem (1). Notably, when assuming the gradient f is Lipschitz continuous (see Assumption 1(b)), SFOMs [3, 7, 8, 9] have been proposed with a sample complexity 2 of O(\u20ac\u00af4) for finding a point a satisfying\n\nE[||f(x)||] \u2264 \u20ac,(2)\n\nwhere \u0454 \u2208 (0,1) is a given tolerance parameter, and the expectation is taken over the randomness in the algorithm. This sample complexity has been proved to be optimal in [2]. Among these works, [3, 7] proposed SFOMs that incorporate Polyak momentum steps:\n\nmk = (1 \u2013 Yk\u22121)mk\u22121 + Yk\u22121G(xk; \u00a2k) Vk \u2265 0,(3)\n\nwhere {x} are the algorithm iterates, {k} are the momentum parameters, and {m} are the stochastic estimators of {\u2207f(x)}. It has been shown in [3, 7] that Polyak momentum promotes a variance reduction effect in gradient estimation, and it was further shown in [3] that Polyak momentum facilitates the convergence of SFOMs with normalized updates. Moreover, other benign theoretical properties of SFOMs with Polyak momentum have been studied in [11, 13, 16, 18].\nRecently, many SFOMs [4, 5, 12, 14] have been proposed under the average smooth assumption on the gradient estimators, namely,\n\n\u0395\u03be [||G(y; \u00a7) \u2013 G(x;\u00a3)||\u00b2] < L\u00b2||y \u2013 x||2 Vx, y \u2208 Rn(4)\n\nfor some L > 0. The methods in [4, 5] achieve a sample complexity of O(\u20ac\u00af\u00b3) for finding x that satisfies (2), which has been proven to be optimal in [2]. In particular, [4] proposed an SFOM with the following recursive momentum steps:\n\nmk = (1 - Yk\u22121)mk\u22121 + Yk\u22121G(xk; &k) + (1 \u2212 k\u22121)(G(xk; \u00a2k) \u2013 G(xk\u22121;\u00a2k)) Vk \u2265 0,(5)\n\nwhich can be viewed as a modified variant of the Polyak momentum steps in (3), with an additional term (1-k-1)(G(xk; \u00a2k) \u2013 G(xk\u22121;\u00a2k)). In addition, SFOMs [15, 17, 19, 20] were proposed for stochastic composite optimization problems, achieving a sample complexity of O(e-\u00b3) under the average smoothness assumption in (4). However, it shall be mentioned that the average smoothness condition in (4) implies the gradient Lipschitz condition (see Assumption 1(b)), but the reverse implication does not generally hold. The strong assumption of average smoothness in (4) appears to be crucial for achieving the sample complexity of O(e-\u00b3) for finding x that satisfies (2).\nAside from assuming (4), several other attempts have been made to improve the sample complexity of SFOMs by leveraging the second-order smoothness of f. Assuming that the Hessian \u22072f is Lipschitz continuous, i.e., Assumption 2 with p = 2, SFOMs [1, 3, 6] have been proposed with a sample complexity of O(e-7/2) for finding a satisfying (2). In particular, [3] proposed an SFOM with implicit gradient transport that performs extrapolation steps combined with Polyak momentum steps:\n\nzk = xk + (1-k-1)/(xkxk-1), m\u00b2 = (1 - Yk\u22121)mk\u22121 + Yk\u22121G(zk; &k) Vk \u2265 0.(6)\n\nIt was shown that constructing {zk} through extrapolation and combining it with Polyak momentum achieves faster variance reduction for gradient estimators {m}, leading to an improved overall sample complexity. Furthermore, there appear to be no SFOMs that leverage higher-order smoothness beyond the Hessian Lipschitz condition.\nIn this paper we show that SFOMs can achieve acceleration by exploiting the arbitrarily high-order smoothness of f through the introduction of an SFOM with multi-extrapolated momentum (Algorithm 1). Our proposed SFOM can be viewed as a significant generalization of the SFOM proposed in [3], which uses extrapolated momentum steps described in (6), as the acceleration of our method also relies on extrapolation and momentum. Specifically, we demonstrate that for any p\u2265 2, performing p 1 separate extrapolation steps in each iteration and combining them with a momentum step can accelerate variance reduction by exploiting the smoothness of the pth-order derivative of f, thereby leading to a sample complexity of O(e-(3p+1)/p), which strictly improves upon the best-known results of SFOMs without assuming average smoothness. In contrast to the straightforward parameter choices in previous SFOMs, the parameters of our proposed SFOM are determined through an innovative use of Lagrange interpolation (see Section 3). For ease of comparison, we summarize the sample complexity of several existing SFOMs, along with their associated smoothness assumptions, and those of our method in Table 1.\nThe main contributions of this paper are highlighted below.\n\u2022 We propose an SFOM with multi-extrapolated momentum (Algorithm 1), which is the first SFOM to leverage the arbitrary order of smoothness of the objective function for acceleration. Our method is efficient to implement in practice, and its update schemes and parameter selection can be performed cheaply and neatly (see Section 3), offering insights for future algorithmic design.\n\u2022 We show that, assuming the pth-order derivative of f is Lipschitz continuous and under other mild assumptions, our proposed SFOM achieves a sample complexity of \u00d5(e-(3p+1)/p). This sample complexity strictly improves upon the best-known results for SFOMs without assuming average smoothness and provides an affirmative answer to the open question raised at the end of [2] regarding SFOMs that leverage high-order smoothness for acceleration.\n\nThe rest of this paper is organized as follows. In Section 2, we introduce some notation, assumptions, and preliminaries that will be used in the paper. In Section 3, we propose an SFOM with multi-extrapolated momentum and study its sample complexity. Section 4 presents preliminary numerical results. In Section 5, we present the proofs of the main results."}, {"title": "2 Notation, assumptions, and preliminaries", "content": "Throughout this paper, let Rn denote the n-dimensional Euclidean space and \u3008\u00b7,\u00b7) denote the standard inner product. We use || . || to denote the Euclidean norm of a vector or the spectral norm of a matrix.\nFor any p > 1 and a pth-order continuously differentiable function 6, we denote by Dry(x)[h1,...,hp] the pth-order directional derivative of yat x along hi \u2208 R\", 1 \u2264 i \u2264 p, and use Dpf(x)[\u00b7] to denote the associated symmetric p-linear form. For any symmetric p-linear form T[\u00b7], we denote its norm as\n\n||T||(p) = max {T[h1,...,hp] : ||hi|| \u2264 1,1 \u2264 i \u2264 p}.(7)\nh1,...,hp\n\nFor any x \u2208 Rn and hi \u2208 Rn with 1 \u2264 i \u2264 p \u2212 1, we define \u2207P(x)(h1,...,hp\u22121) \u2208 R\u201d as follows:\n\n(\u2207P(x)(h1,..., hp\u22121), hp) = DP(x)[h1,...,hp] Vhp \u2208 Rn.\n\nFor any x, h \u2208 Rn, we denote DP4(x)[h]p = DP4(x)[h, ..., h] and \u2207Pq(x)(h)p\u22121 = \u2207\u00ba\u00a2(x)(h,...,h). For any s \u2208 R, we let sgn(s) be 1 if s > 0 and let it be -1 otherwise. In addition, \u00d5(\u00b7) represents O(\u00b7) with logarithmic terms omitted.\nWe now make the following assumptions throughout this paper.\nAssumption 1. (a) There exists a finite flow such that f(x) \u2265 flow for all x \u2208 Rn.\n(b) There exists L1 > 0 such that\n\n||\u2207f(y) \u2013 \u2207 f(x)|| \u2264 L1||y - x|| Vx, y \u2208 Rn.(8)\n\n(c) The stochastic gradient estimator G : R\u2033 \u00d7 \u039e \u2192 R\" satisfies\n\n\u0395\u03be[G(x; \u00a7)] = \u2207 f(x), E\u00a2 [||G(x; \u00a7) \u2013 \u2207 f(x)||\u00b2] < \u03c3\u00b2 Vx \u2208 Rn(9)\n\nfor some \u03c3 > 0.\nWe now make some remarks on Assumption 1.\nRemark 1. (i) Assumptions 1(a) and (b) are standard. It follows from Assumption 1(b) that\n\nf(y) \u2264 f(x) + \u221af(x)f(y - x) + L1/||yx|| 2 Vx, y \u2208 Rn.(10)\n\n(ii) Assumption 1(c) is commonly used in stochastic optimization. It implies that G(\u00b7;\u00a7) is an unbiased estimator for \u2207 f(\u00b7) with bounded variance.\nWe also make the following assumption regarding the Lipschitz continuity of DP f.\nAssumption 2. The function f is pth-order continuously differentiable, and there exists some p > 2 and Lp > 0 such that\n\n||D\u00ba f(y) \u2013 DP f(x)||(p) \u2264 Lp||y - x|| Vx, y \u2208 Rn.(11)\n\nThe following lemma provides a useful inequality under Assumption 2, and its proof is deferred to Section 5.1.\nLemma 1. Under Assumption 2, the following inequality holds:\n\nVt f(x) (y-x)-1 \u2264 Lp||y-x||P Vx, y \u2208 Rn.(12)"}, {"title": "3 Stochastic first-order methods with multi-extrapolated momentum", "content": "In this section we propose an SFOM with multi-extrapolated momentum in Algorithm 1, and study its sample complexity.\nSpecifically, at the kth iteration, our method performs q separate extrapolations for some q\u2265 1 as in (15) to obtain q points {zk,t}1<t\u2264q, where the extrapolation parameters {/k-1,t}1<t\u2264g in (15) are chosen to have distinct positive values. Then, a gradient estimator m\u00b2 is constructed using the previous gradient estimator mk-1 and the stochastic estimator G(\u00b7;\u00a3*) evaluated at {zk,t}1<t<q, as described in (16). Here, the weighting parameters {0k\u22121,t}1<t\u2264q must be obtained by solving the linear system in (28) with the coefficient matrix constructed using {/k\u22121,t}1<t\u2264q to exploit high-order smoothness. The resulting values of {0k-1,t}1<t\u2264q follow a pattern of alternating signs (see Lemma 5). After obtaining mk, the next iterate xk+1 is generated via a normalized update\u00b3, as described in (17). For ease of understanding our extrapolation and momentum steps, we consider Algorithm 1 with q = 3 and visualize the updates for {zk,t}1<t\u22643 and mk on a two-dimensional contour plot shown in Figure 1."}, {"title": "3.1 An SFOM with double-extrapolated momentum", "content": "In this subsection we study a simple variant of Algorithm 1 with q = 2, which is capable of exploiting the smoothness of D\u00b3f. We refer to this method as the SFOM with double-extrapolated momentum as two separate extrapolations are performed in each iteration. In the following, we establish its sample complexity under Assumption 1 and Assumption 2 with p = 3.\nThroughout this subsection, we impose the following equations on the parameters {(\\k,1, \\k,2, 0k,1,0k,2)}k\u22650 of Algorithm 1:\n\n0k,1/Yk,1 +0k,2/Vk,2 = 1, 0k,1/2,1 + \u03b8k,2/2,2 = 1 Vk > 0.(14)"}, {"title": "3.2 An SFOM with multi-extrapolated momentum", "content": "In this subsection, we study Algorithm 1 with q = p \u2212 1, which is capable of exploiting the smoothness of Dr f for some p > 2. In the following, we establish its sample complexity under Assumption 1 and Assumption 2 for p > 2.\nThroughout this subsection, we impose the following system of linear equations on the parameters {(/k,t, 0k,t)}1<t\u2264q,k>0 of Algorithm 1:\n\n Vk > 0,(28)\n\nand in addition, we require that\n\nq\n\u03a3\u03b8k, k\u2208 (0,1) Vk\u22650.(29)\nt=1\n\nThe coefficient matrix in (28) is known as the Vandermonde matrix (e.g., see [10]). The following lemma demonstrates that if the values of {\\k,t}1<t\u2264q are positive and distinct, the values of {0k,t}1<t<q can be uniquely determined by solving (28). In addition, the next lemma provides the explicit solution to the linear system (28), along with the elegant property of alternating signs for {0k,t}1<t<q. Its proof is deferred to Section 5.3.\nLemma 5. Assume that {\\k,t}1<t<q \u2282 (0,1) with Yk,1 > \u2026 > Yk,q are given for some k \u2265 0. Then, the solution {0k,t}1<t\u2264q to the linear system in (28) is unique and can be explicitly written as\n\nOk.t = \u22001 \u2264 t \u2264 q,(30)\n\nwhich satisfies 0k,t > 0 for all odd t and Ok,t < 0 for all even t. Moreover, it holds that\n\nq\n\u03a3\u03b8\u03b5\u03b5 = 1 - (1//k,t - 1) /k,t(31)\nt=1\n\nThe following lemma establishes the recurrence relation for the estimation error of the gradient estimators {mk}k>0 of Algorithm 1 with q = p \u2212 1. Its proof is deferred to Section 5.3.\nLemma 6. Suppose that Assumption 1 holds, and Assumption 2 holds for p > 2. Let {(xk, mk)}k\u22650 be generated by Algorithm 1 with q = p \u2212 1, and let {(Nk, \\k,t,0k,t)}1<t<p\u22121,k\u22650 be inputs of Algorithm 1. Assume that {(\\k,t, 0k,t)}1<t<p\u22121,k\u22650 satisfies (28) and (29). Then,\n\nExk+1 [||mk+1 \u2013 \u2207 f(xk+1)||2] \u2264 (1 \u2013 ( f(xk)||\u00b2(32)\n\nwhere \u03c3 and Lp are given in Assumptions 1(b) and 2, respectively.\nWe next derive an upper bound for the average expected error of the stationary condition among all iterates generated by Algorithm 1 with q = p - 1. Its proof is relegated to Section 5.3."}, {"title": "3.2.1 Input parameters and convergence rate", "content": "We now specify the input parameters of Algorithm 1 with q = p- 1 and analyze its sample complexity. We first define a quantity that will be used to set the input parameters:\n\nkp = p(3p+1)/(2p).(35)\n\nTo analyze the sample complexity of Algorithm 1 with q = p \u2212 1, we specify {(\u03b7k, ^\\k,t)}1<t<p\u22121,k\u22650 as\n\n\u221a1 \u2264 t \u2264 p \u2212 1, k \u2265 0.(36)\n\nSince {0k,t}0<t\u2264p-1,k\u22650 is solution to (28), we compute {0k,t}0<t\u2264p-1,k>0 using Lemma 5 as\n\n\u221a1 \u2264 t \u2264 p \u2212 1, k \u2265 0.(37)\n\nIn addition, we define the sequence {pk}k\u22650 used in Theorem 3 as\n\nPk = (k + kp)(p-1)/(3p+1).(38)\n\nThe following lemma provides some useful properties of {0k,t}1<t\u2264q,k>0 and {Pk}k\u22650 defined in (36) and (38), respectively. Its proof is deferred to Section 5.3.\nLemma 7. Let {0k,t}1<t\u2264q,k>0 and {pk}k\u22650 be defined in (36) and (38), respectively. Then,\n\n1\n2\n\u221a1 \u2264 t \u2264 p - 1, k \u2265 0,(40)\n\nwhere kp is defined in (35). Moreover, {(0k,t, Pk)}1<t<p\u22121,k>0 satisfies (33)."}, {"title": "4 Numerical experiments", "content": "In this section we conduct some preliminary numerical experiments to test practical performance of our SFOMs with multi-extrapolated momentum (Algorithm 1). We compare our SFOMs against the normalized stochastic gradient method with Polyak momentum (SG-PM) [3] and STORM [4] on a robust regression problem. All the algorithms are coded in Matlab, and all the computations are performed on a laptop with a 2.20 GHz Intel Core i9-14900HX processor and 32 GB of RAM.\nSpecifically, we consider the robust regression problem:\n\n(ax - bi),(43)\nxERn\ni=1\n\nwhere $(t) = t2/(1 + t\u00b2), and {(ai, bi)}1<i<m C R\u2033 \u00d7 R is the training set. It can be verified that is infinitely differentiable. We consider two datasets, 'red wine quality' and 'white wine quality' from the UCI repository. We apply our Algorithm 1 with q = 1,2,3, as well as SG-PM and STORM to solve (43).\nWe compare these methods in terms of relative loss, which is defined as f(xk)/f(x\u00ba). For all methods, we set the maximum number of epochs as 100, and set the initial iterate as the all-zero vector.\nFrom Figure 2, we observe that Algorithm 1 with q = 1,2,3 slightly outperforms SG-PM and comes very close to the performance of STORM, which corroborates our theoretical results and shows that more extrapolations can achieve faster convergence."}, {"title": "5 Proof of the main results", "content": "In this section we provide proofs of the main results in Sections 2 and 3, which are particularly Lemmas 1 to 7 and Theorems 1 to 4.\nTo proceed, we first establish several technical lemmas. The following lemma concerns the estimation of the partial sums of series.\nLemma 8. Let \u03da(\u00b7) be a convex univariate function. Then, for any integers a, b satisfying [a\u22121/2,b+1/2] C dom, it holds that = $(s) \u2264 a+1/2 (7)dr.\ns=a\nProof. Since f is convex, one has \u3118(s) \u2264 SS-1/2 (7)d for all s \u2208 [a, b]. It then follows that \nJa-1/2\nAs a consequence of Lemma 8, we consider \u03b6(\u03c4) = 1/\u03c4\u00ba for some a \u2208 (0, \u221e], where \u03c4\u2208 (0,\u221e). Then, for any positive integers a, b, one has\n\nb\n\u03a31/p 1/p\u00ba <  (b+1/2)1-a \u2013 (a \u2013 1/2)1-a) if a \u2208 (0, 1) U (1, +\u221e).(44)\np=a\n\nWe next provide an auxiliary lemma that will be used to estimate the maximum number of iterations for achieving targeted approximate stationarity in expectation.\nLemma 9. Let a \u2208 (0,1) and u \u2208 (0,1/e) be given. Then, 1/v\u00ba lnv < 2u/a holds for all v satisfying v \u2265 (1/uln(1/u))1/\u03b1.\nProof. Let v be such that v \u2265 (1/uln(1/u))1/a. By this and u \u2208 (0,1/e), one has v \u2265 (1/uln(1/u))1/a > e1/a. Denote (v) = 1/v\u00ba Inv. Since & is decreasing over (e1/a, \u221e), it follows that\n\nln ln(1/u)\n\u03b1 ln(1/u) < \n\nwhere the last inequality is due to ln ln(1/u) < ln(1/u) for all u \u2208 (0,1/e). Hence, the conclusion of this lemma holds as desired."}, {"title": "5.1 Proof of Lemmas 1 and 2", "content": "Proof of Lemma 1. For convenience, we denote \u03c6(x) = (\u2207f(x), u). In view of this and the definition of Vrf, one can see that\n\nD\u00ba\u00a2(x)[y \u2212 x] = (\u2207r+1 f(x)(y \u2013 x)\", u) 1 \u2264 r < p \u2212 1, x, y \u2208 Rn.(45)\n\nSince f is pth-order continuously differentiable, we have that is (p-1)th-order continuously differentiable, and also that\n\n||DP-1\u00a2(y) \u2013 DP-1\u00a2(x)||(p-1) = ||u||||DP f (y) \u2013 DP f(x) || (p) Vx, y \u2208 Rn.(46)\""}, {"title": "5.2 Proof of the main results in Section 3.1", "content": "In this subsection we prove Lemmas 3 and 4 and Theorems 1 and 2.\nWhen Assumption 2 holds with p = 3, it directly follows from Lemma 1 that\n\n||\u2207f(y) - \u2207 f(x) - \u221a2 f(x) (y - x) - \u221a3 f(x) (y-x)\u00b2|| \u2264 ||yx|13 y\n2\nProof of Lemma 3. Fix any k \u2265 0. Notice from (15) with q = 2 that\n\nzk+1,1xk =k, zk+1,2 \u2212 xk = k.(49)\n\nBy this, (14), and (16), one has that"}, {"title": "5.3 Proof of the main results in Section 3.2", "content": "In this subsection we prove Lemmas 5 to 7 and Theorems 3 and 4.\nProof of Lemma 5. We first prove that the solution to (28) is unique. For convenience, we define\n\n\u0393=(57)\n\nwhich is the transpose of the coefficient matrix in (28). To show that the solution to the linear system in (28) is unique, it suffices to prove that is invertible. To this end, we let_et, 1 \u2264 t \u2264 q, be the standard basis vector in R\u00ba, whose tth coordinate is 1, and other coordinates are 0. Then, we observe that for any 1 \u2264 t \u2264 q, solving the linear system\n\n=(57)\n\nis equivalent to finding the coefficients for a polynomial ht(a) = C1ta + C2ta\u00b2 + \u00b7 + Cqtaq such that ht(1//k,t) = 1 and ht(1/\\k,s) = 0 for all s with 1 < s < q and s \u2260 t. Using Lagrange interpolation and the fact that 1/^\\k,t, 1 \u2264 t \u2264 q, take distinct values, we obtain that such polynomial ht(a) can be uniquely expressed as\n\nht(a) = V1 \u2264 t \u2264 q.(58)\n\nTherefore, the solution to (57) is unique for each 1 \u2264 t \u2264 q, and thus \u0393 is invertible. Hence, the solution {0k,t}1<t\u2264q to (28) is unique.\nWe now prove that the unique solution to (28) can be explicitly written as in (30). For convenience, we denote V = \u0413-1. Since FV = Iq, where Iq is the q \u00d7 q identity matrix, it follows that the tth column of V is the solution to (57). In addition, recall from (28) that"}]}