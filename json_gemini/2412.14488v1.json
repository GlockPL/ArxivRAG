[{"title": "Stochastic first-order methods with multi-extrapolated momentum for highly smooth unconstrained optimization", "authors": ["Chuan He"], "abstract": "In this paper we consider an unconstrained stochastic optimization problem where the objective function exhibits a high order of smoothness. In particular, we propose a stochastic first-order method (SFOM) with multi-extrapolated momentum, in which multiple extrapolations are performed in each iteration, followed by a momentum step based on these extrapolations. We show that our proposed SFOM with multi-extrapolated momentum can accelerate optimization by exploiting the high-order smoothness of the objective function $f$. Specifically, assuming that the gradient and the $p$th-order derivative of $f$ are Lipschitz continuous for some $p\\geq 2$, and under some additional mild assumptions, we establish that our method achieves a sample complexity of $O(\\epsilon^{-(3p+1)/p})$ for finding a point $x$ satisfying $E[||\\nabla f(x)||]\\leq \\epsilon$. To the best of our knowledge, our method is the first SFOM to leverage arbitrary order smoothness of the objective function for acceleration, resulting in a sample complexity that strictly improves upon the best-known results without assuming the average smoothness condition. Finally, preliminary numerical experiments validate the practical performance of our method and corroborate our theoretical findings.", "sections": [{"title": "1 Introduction", "content": "In this paper we consider the smooth unconstrained optimization problem:\n\n$\\min_{x\\in\\mathbb{R}^n} f(x)$,\n\nwhere $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is continuously differentiable and has a Lipschitz continuous $p$th-order derivative for some $p \\geq 2$ (see Assumption 2 for details). We assume that problem (1) has at least one optimal solution. Our goal is to develop first-order methods for solving (1) in the stochastic regime where the derivatives of $f$ are not directly accessible. Instead, our algorithm relies solely on stochastic estimators $G(\\cdot; \\xi)$ for the gradient $\\nabla f(\\cdot)$, where $\\xi$ is a random variable with sample space $\\Xi$ (see Assumption 1(c) for our assumptions on $G$).\nIn recent years, there has been significant developments on stochastic first-order methods (SFOMS) with sample complexity guarantees for solving problem (1). Notably, when assuming the gradient $f$ is Lipschitz continuous (see Assumption 1(b)), SFOMs [3, 7, 8, 9] have been proposed with a sample complexity 2 of $O(\\epsilon^{-4})$ for finding a point $\\bar{x}$ satisfying\n\n$E[||\\nabla f(\\bar{x})||] \\leq \\epsilon$,\n\nwhere $\\epsilon \\in (0,1)$ is a given tolerance parameter, and the expectation is taken over the randomness in the algorithm. This sample complexity has been proved to be optimal in [2]. Among these works, [3, 7] proposed SFOMs that incorporate Polyak momentum steps:\n\n$m^k = (1 - \\gamma_{k-1})m^{k-1} + \\gamma_{k-1}G(x^k; \\xi_k) \\quad \\forall k \\geq 0$,\n\nwhere $\\{x^k\\}$ are the algorithm iterates, $\\{\\gamma_k\\}$ are the momentum parameters, and $\\{m^k\\}$ are the stochastic estimators of $\\{\\nabla f(x^k)\\}$. It has been shown in [3, 7] that Polyak momentum promotes a variance reduction effect in gradient estimation, and it was further shown in [3] that Polyak momentum facilitates the convergence of SFOMs with normalized updates. Moreover, other benign theoretical properties of SFOMS with Polyak momentum have been studied in [11, 13, 16, 18].\nRecently, many SFOMs [4, 5, 12, 14] have been proposed under the average smooth assumption on the gradient estimators, namely,\n\n$E_\\xi [||G(y; \\xi) - G(x;\\xi)||^2] \\leq L^2||y - x||^2 \\quad \\forall x, y \\in \\mathbb{R}^n$\n\nfor some $L>0$. The methods in [4, 5] achieve a sample complexity of $O(\\epsilon^{-3})$ for finding $\\bar{x}$ that satisfies (2), which has been proven to be optimal in [2]. In particular, [4] proposed an SFOM with the following recursive momentum steps:\n\n$m^k = (1 - \\gamma_{k-1})m^{k-1} + \\gamma_{k-1}G(x^k; \\xi_k) + (1 - \\gamma_{k-1})(G(x^k; \\xi_k) - G(x^{k-1};\\xi_k)) \\quad \\forall k \\geq 0$,\n\nwhich can be viewed as a modified variant of the Polyak momentum steps in (3), with an additional term $(1-\\gamma_{k-1})(G(x^k; \\xi_k) - G(x^{k-1};\\xi_k))$. In addition, SFOMs [15, 17, 19, 20] were proposed for stochastic composite optimization problems, achieving a sample complexity of $O(\\epsilon^{-3})$ under the average smoothness assumption in (4). However, it shall be mentioned that the average smoothness condition in (4) implies the gradient Lipschitz condition (see Assumption 1(b)), but the reverse implication does not generally hold. The strong assumption of average smoothness in (4) appears to be crucial for achieving the sample complexity of $O(\\epsilon^{-3})$ for finding $\\bar{x}$ that satisfies (2).\nAside from assuming (4), several other attempts have been made to improve the sample complexity of SFOMs by leveraging the second-order smoothness of $f$. Assuming that the Hessian $\\nabla^2f$ is Lipschitz continuous, i.e., Assumption 2 with $p = 2$, SFOMs [1, 3, 6] have been proposed with a sample complexity of $O(\\epsilon^{-7/2})$ for finding $\\bar{x}$ satisfying (2). In particular, [3] proposed an SFOM with implicit gradient transport that performs extrapolation steps combined with Polyak momentum steps:\n\n$z^k = x^k + \\frac{1-\\gamma_{k-1}}{\\gamma_{k-1}}(x^k - x^{k-1}), \\quad m^k = (1 - \\gamma_{k-1})m^{k-1} + \\gamma_{k-1}G(z^k; \\xi_k) \\quad \\forall k \\geq 0$.\n\nIt was shown that constructing $\\{z^k\\}$ through extrapolation and combining it with Polyak momentum achieves faster variance reduction for gradient estimators $\\{m^k\\}$, leading to an improved overall sample complexity. Furthermore, there appear to be no SFOMs that leverage higher-order smoothness beyond the Hessian Lipschitz condition.\nIn this paper we show that SFOMs can achieve acceleration by exploiting the arbitrarily high-order smoothness of $f$ through the introduction of an SFOM with multi-extrapolated momentum (Algorithm 1). Our proposed SFOM can be viewed as a significant generalization of the SFOM proposed in [3], which uses extrapolated momentum steps described in (6), as the acceleration of our method also relies on extrapolation and momentum. Specifically, we demonstrate that for any $p\\geq 2$, performing $p - 1$ separate extrapolation steps in each iteration and combining them with a momentum step can accelerate variance reduction by exploiting the smoothness of the $p$th-order derivative of $f$, thereby leading to a sample complexity of $O(\\epsilon^{-(3p+1)/p})$, which strictly improves upon the best-known results of SFOMs without assuming average smoothness. In contrast to the straightforward parameter choices in previous SFOMs, the parameters of our proposed SFOM are determined through an innovative use of Lagrange interpolation (see Section 3). For ease of comparison, we summarize the sample complexity of several existing SFOMs, along with their associated smoothness assumptions, and those of our method in Table 1.\nThe main contributions of this paper are highlighted below.\n\\begin{itemize}\n    \\item We propose an SFOM with multi-extrapolated momentum (Algorithm 1), which is the first SFOM to leverage the arbitrary order of smoothness of the objective function for acceleration. Our method is efficient to implement in practice, and its update schemes and parameter selection can be performed cheaply and neatly (see Section 3), offering insights for future algorithmic design.\n    \\item We show that, assuming the $p$th-order derivative of $f$ is Lipschitz continuous and under other mild assumptions, our proposed SFOM achieves a sample complexity of $\\tilde{O}(\\epsilon^{-(3p+1)/p})$. This sample complexity strictly improves upon the best-known results for SFOMs without assuming average smoothness and provides an affirmative answer to the open question raised at the end of [2] regarding SFOMs that leverage high-order smoothness for acceleration.\n\\end{itemize}\nThe rest of this paper is organized as follows. In Section 2, we introduce some notation, assumptions, and preliminaries that will be used in the paper. In Section 3, we propose an SFOM with multi-extrapolated momentum and study its sample complexity. Section 4 presents preliminary numerical results. In Section 5, we present the proofs of the main results."}, {"title": "2 Notation, assumptions, and preliminaries", "content": "Throughout this paper, let $\\mathbb{R}^n$ denote the $n$-dimensional Euclidean space and $\\langle\\cdot,\\cdot\\rangle$ denote the standard inner product. We use $|| \\cdot ||$ to denote the Euclidean norm of a vector or the spectral norm of a matrix."}, {"title": "3 Stochastic first-order methods with multi-extrapolated momentum", "content": "In this section we propose an SFOM with multi-extrapolated momentum in Algorithm 1, and study its sample complexity.\nSpecifically, at the $k$th iteration, our method performs $q$ separate extrapolations for some $q\\geq 1$ as in (15) to obtain $q$ points $\\{z^{k,t}\\}_{1\\leq t\\leq q}$, where the extrapolation parameters $\\{\\eta_{k-1,t}\\}_{1\\leq t\\leq q}$ in (15) are chosen to have distinct positive values. Then, a gradient estimator $m^k$ is constructed using the previous gradient estimator $m^{k-1}$ and the stochastic estimator $G(\\cdot;\\xi_k)$ evaluated at $\\{z^{k,t}\\}_{1\\leq t\\leq q}$, as described in (16). Here, the weighting parameters $\\{\\theta_{k-1,t}\\}_{1\\leq t\\leq q}$ must be obtained by solving the linear system in (28) with the coefficient matrix constructed using $\\{\\eta_{k-1,t}\\}_{1\\leq t\\leq q}$ to exploit high-order smoothness. The resulting values of $\\{\\theta_{k-1,t}\\}_{1\\leq t\\leq q}$ follow a pattern of alternating signs (see Lemma 5). After obtaining $m^k$, the next iterate $x^{k+1}$ is generated via a normalized update, as described in (17). For ease of understanding our extrapolation and momentum steps, we consider Algorithm 1 with $q = 3$ and visualize the updates for $\\{z^{k,t}\\}_{1\\leq t\\leq 3}$ and $m^k$.\nBefore proceeding, we present the following lemma regarding the descent of $f$ for iterates generated by Algorithm 1. Its proof is deferred to Section 5.1.\n\\textbf{Lemma 2.} \\emph{Suppose that Assumption 1 holds. Let $\\{x^k\\}_{k\\geq 0}$ be generated by Algorithm 1. Then,}\n\n$f(x^{k+1}) \\leq f(x^k) - \\eta_k||\\nabla f(x^k)||^2 + 2\\eta_k ||\\nabla f(x^k) - m^k|| + \\frac{L_1\\eta_k^2}{2} \\quad \\forall k\\geq 0,$\n\nwhere $L_1$ is given in Assumption 1(b).\n### 3.1 An SFOM with double-extrapolated momentum\nIn this subsection we study a simple variant of Algorithm 1 with $q = 2$, which is capable of exploiting the smoothness of $D^3f$. We refer to this method as the SFOM with double-extrapolated momentum as two separate extrapolations are performed in each iteration. In the following, we establish its sample complexity under Assumption 1 and Assumption 2 with $p = 3$.\nThroughout this subsection, we impose the following equations on the parameters $\\{\\eta_k, \\gamma_{k,1}, \\gamma_{k,2}, \\theta_{k,1}, \\theta_{k,2}\\}_{k\\geq 0}$ of Algorithm 1:\n\n$\\frac{\\theta_{k,1}}{\\gamma_{k,1}} + \\frac{\\theta_{k,2}}{\\gamma_{k,2}} = 1, \\quad \\frac{\\theta_{k,1}}{\\gamma_{k,1}^2} + \\frac{\\theta_{k,2}}{\\gamma_{k,2}^2} = 1 \\quad \\forall k > 0.$\n\nAlgorithm 1 An SFOM with multi-extrapolated momentum\nInput: starting point $x^{-1} = x^0 \\in \\mathbb{R}^n$, nonincreasing step sizes $\\{\\eta_k\\}_{k>0} \\subset (0, +\\infty)$, extrapolations per iteration $q$, extrapolation parameters $\\{\\gamma_{k,t}\\}_{1\\leq t\\leq q, k\\geq 0} \\subset (0, +\\infty)$, weighting parameters $\\{\\theta_{k,t}\\}_{1\\leq t\\leq q, k\\geq 0}$ with $\\sum_{t=1}^q\\theta_{k,t} \\in (0,1)$ for all $k \\geq 0$.\nInitialize $m^{-1} = 0$ and $(\\gamma_{-1,t}, \\theta_{-1,t}) = (1, 1/q)$ for all $1 \\leq t \\leq q$.\nfor $k = 0, 1, 2, . . .$ do\nPerform $q$ separate extrapolations:\n\n$z^{k,t} = x^k + \\frac{1 - \\gamma_{k-1,t}}{\\gamma_{k-1,t}}(x^k - x^{k-1}) \\quad \\forall 1 \\leq t \\leq q$.\n\nCompute the search direction:\n\n$m^k = (1 - \\sum_{t=1}^q\\theta_{k-1,t})m^{k-1} + \\sum_{t=1}^q\\theta_{k-1,t}G(z^{k,t};\\xi_k)$.\n\nUpdate the next iterate:\n\n$x^{k+1} = x^k - \\eta_k \\frac{m^k}{||m^k||}$\n\nend for\nIt is noteworthy that for any two distinct positive values of $\\gamma_{k,1}$ and $\\gamma_{k,2}$, the values of $\\theta_{k,1}$ and $\\theta_{k,2}$ can be uniquely determined by solving the above equations. In addition, we require that\n\n$\\theta_{k,1} + \\theta_{k,2} \\in (0, 1) \\quad \\forall k \\geq 0$.\n\nThe following lemma establishes the recurrence relation for the estimation error of the gradient estimators $\\{m^k\\}_{k>0}$ in Algorithm 1 with $q = 2$. Its proof is deferred to Section 5.2.\n\\textbf{Lemma 3.} \\emph{Suppose that Assumption 1 holds, and Assumption 2 holds with $p = 3$. Let $\\{(x^k, m^k)\\}_{k\\geq 0}$ be generated by Algorithm 1 with $q = 2$, and let $\\{\\eta_k, \\gamma_{k,1}, \\gamma_{k,2}, \\theta_{k,1}, \\theta_{k,2}\\}_{k\\geq 0}$ be inputs of Algorithm 1. Assume that $\\{\\eta_k, \\gamma_{k,1}, \\gamma_{k,2}, \\theta_{k,1}, \\theta_{k,2}\\}_{k\\geq 0}$ satisfies (14) and (18). Then,}\n\n$E_{\\xi^{k+1}}[||m^{k+1} - \\nabla f(x^{k+1})||^2] \\leq (1 - \\theta_{k,1} - \\theta_{k,2})||m^k - \\nabla f(x^k)||^2 + \\frac{6\\sigma^2}{\\eta_{k,\\gamma_{k,1}}^2} + \\frac{6\\sigma^2}{\\eta_{k,\\gamma_{k,2}}^2} + \\frac{L_3^2\\eta_k^6}{12\\gamma_{k,1}^3(\\theta_{k,1} + \\theta_{k,2})} + \\frac{L_3^2\\eta_k^6}{12\\gamma_{k,2}^3(\\theta_{k,1} + \\theta_{k,2})} + \\frac{L_3^2\\eta_k^6}{12(\\theta_{k,1} + \\theta_{k,2})} + 2(\\theta_{k,1} + \\theta_{k,2})\\sigma^2 \\quad \\forall k \\geq 0,$\n\nwhere $\\sigma$ and $L_3$ are given in Assumptions 1(b) and 2, respectively.\nWe next derive an upper bound for the average expected error of the stationary condition across all iterates generated by Algorithm 1 with $q = 2$. Its proof is relegated to Section 5.2.\n\\textbf{Theorem 1.} \\emph{Suppose that Assumption 1 holds, and Assumption 2 holds with $p = 3$. Let $\\{x^k\\}_{k>0}$ be generated by Algorithm 1 with $q = 2$, and let $\\{\\eta_k, \\gamma_{k,1}, \\gamma_{k,2}, \\theta_{k,1}, \\theta_{k,2}\\}_{k\\geq 0}$ be inputs of Algorithm 1. Assume that $\\{\\eta_k, \\gamma_{k,1}, \\gamma_{k,2}, \\theta_{k,1}, \\theta_{k,2}\\}_{k\\geq 0}$ satisfies (14) and (18), and that the sequence $\\{\\rho_k\\}_{k\\geq 0}$ satisfies}\n\n$(1 - \\theta_{k,1} - \\theta_{k,2})\\rho_{k+1} \\leq (1 - (\\theta_{k,1} + \\theta_{k,2})/2)\\rho_k \\quad \\forall k > 0.$"}, {"title": "3.1.1 Input parameters and convergence rate", "content": "To analyze the sample complexity of Algorithm 1 with $q = 2$, we specify $\\{\\eta_k, \\gamma_{k,1}, \\gamma_{k,2}\\}_{k\\geq 0}$ as\n\n$\\eta_k = \\frac{1}{(k+3)^{3/5}}, \\quad \\gamma_{k,1} = \\frac{1}{(k+3)^{7/10}}, \\quad \\gamma_{k,2} = \\frac{1}{2(k+3)^{3/5}} \\quad \\forall k \\geq 0,$\n\nand determine $\\{\\theta_{k,1}, \\theta_{k,2}\\}_{k\\geq 0}$, by solving (14), as\n\n$\\theta_{k,1} = \\frac{2(k+3)^{3/5} - 1}{(k+3)^{6/5}}, \\quad \\theta_{k,2} = \\frac{1 - (k+3)^{3/5}}{2(k+3)^{6/5}} \\quad \\forall k \\geq 0$.\n\nIn addition, we define the sequence $\\{\\rho_k\\}_{k>0}$ used in Theorem 1 as follows:\n\n$\\rho_k = (k + 3)^{1/5} \\quad \\forall k \\geq 0.$\n\nThe following lemma provides some useful properties of $\\{\\theta_{k,1}, \\theta_{k,2}\\}_{k\\geq 0}$ and $\\{\\rho_k\\}_{k>0}$ defined in (23) and (24), respectively. Its proof is relegated to Section 5.2.\n\\textbf{Lemma 4.} \\emph{Let $\\{\\theta_{k,1}, \\theta_{k,2}\\}_{k\\geq 0}$ and $\\{\\rho_k\\}_{k>0}$ be defined in (23) and (24), respectively. Then,}\n\n$\\theta_{k,1} + \\theta_{k,2} \\in \\Big( \\frac{1}{(k+3)^{3/5}}, \\frac{1}{2(k+3)^{3/5}}\\Big) \\subset (0,1), \\quad \\theta_{k,1} \\leq \\frac{4}{(k + 3)^{6/5}}, \\quad \\theta_{k,2} \\leq \\frac{1}{4(k+3)^{6/5}} \\quad \\forall k \\geq 0$.\n\nMoreover, $\\{\\theta_{k,1}, \\theta_{k,2},\\rho_k\\}_{k\\geq 0}$ satisfies (20).\nThe next theorem establishes the sample complexity of Algorithm 1 with $q = 2$ and other inputs specified in (22) and (23). Its proof is deferred to Section 5.2.\n\\textbf{Theorem 2.} \\emph{Suppose that Assumption 1 holds, and Assumption 2 holds with $p = 3$. Let $\\{x^k\\}_{k\\geq 0}$ be generated by Algorithm 1 with $q = 2$ and inputs $\\{\\eta_k, \\gamma_{k,1}, \\gamma_{k,2}, \\theta_{k,1}, \\theta_{k,2}\\}_{k\\geq 0}$ specified as in (22) and (23). Define}\n\n$M_3 \\triangleq 4(f(x^0) - f_{low} + 19\\sigma^2 + L_1 + 4L_3 + 2).$\n\n\\emph{Let} $\\kappa(k)$ \\emph{be uniformly drawn from} $\\{0, . . ., k - 1\\}$. \\emph{Then,}\n\n$E[||\\nabla f(x^{\\kappa(k)})||] \\leq \\epsilon \\quad \\forall k > \\max\\Big\\{ \\Big( \\frac{20M_3}{3\\epsilon} \\ln \\Big( \\frac{20M_3}{(3\\epsilon)^2}\\Big)\\Big)^{10/3}, 2\\Big\\}$."}, {"title": "3.2 An SFOM with multi-extrapolated momentum", "content": "In this subsection, we study Algorithm 1 with $q = p - 1$, which is capable of exploiting the smoothness of $D^p f$ for some $p > 2$. In the following, we establish its sample complexity under Assumption 1 and Assumption 2 for $p > 2$.\nThroughout this subsection, we impose the following system of linear equations on the parameters $\\{\\gamma_{k,t}, \\theta_{k,t}\\}_{1\\leq t\\leq q, k\\geq 0}$ of Algorithm 1:\n\n$\\begin{bmatrix}\n1/\\gamma_{k,1} & 1/\\gamma_{k,2} & \\cdots & 1/\\gamma_{k,q} \\\\\n1/\\gamma_{k,1}^2 & 1/\\gamma_{k,2}^2 & \\cdots & 1/\\gamma_{k,q}^2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1/\\gamma_{k,1}^q & 1/\\gamma_{k,2}^q & \\cdots & 1/\\gamma_{k,q}^q\n\\end{bmatrix} \\begin{bmatrix} \\theta_{k,1} \\\\ \\theta_{k,2} \\\\ \\vdots \\\\ \\theta_{k,q} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\quad \\forall k \\geq 0,$\n\nand in addition, we require that\n\n$\\sum_{t=1}^q \\theta_{k,t} \\in (0,1) \\quad \\forall k\\geq 0$.\n\nThe coefficient matrix in (28) is known as the Vandermonde matrix (e.g., see [10]). The following lemma demonstrates that if the values of $\\{\\gamma_{k,t}\\}_{1\\leq t\\leq q}$ are positive and distinct, the values of $\\{\\theta_{k,t}\\}_{1\\leq t\\leq q}$ can be uniquely determined by solving (28). In addition, the next lemma provides the explicit solution to the linear system (28), along with the elegant property of alternating signs for $\\{\\theta_{k,t}\\}_{1\\leq t\\leq q}$. Its proof is deferred to Section 5.3.\n\\textbf{Lemma 5.} \\emph{Assume that $\\{\\gamma_{k,t}\\}_{1\\leq t\\leq q} \\subset (0,1)$ with $\\gamma_{k,1} > \\cdots > \\gamma_{k,q}$ are given for some $k \\geq 0$. Then, the solution $\\{\\theta_{k,t}\\}_{1\\leq t\\leq q}$ to the linear system in (28) is unique and can be explicitly written as}\n\n$\\theta_{k,t} = \\frac{\\prod_{1\\leq s\\leq q, s\\neq t}(1 - 1/\\gamma_{k,s})}{1/\\gamma_{k,t} \\prod_{1\\leq s\\leq q, s\\neq t}(1/\\gamma_{k,t} - 1/\\gamma_{k,s})} \\quad \\forall 1 \\leq t \\leq q,$\n\n\\emph{which satisfies $\\theta_{k,t} > 0$ for all odd $t$ and $\\theta_{k,t} < 0$ for all even $t$. Moreover, it holds that}\n\n$\\sum_{t=1}^q \\theta_{k,t} = 1 - \\frac{\\prod_{t=1}^q(1/\\gamma_{k,t} - 1)}{\\prod_{t=1}^q 1/\\gamma_{k,t}}$.\n\nThe following lemma establishes the recurrence relation for the estimation error of the gradient estimators $\\{m^k\\}_{k>0}$ of Algorithm 1 with $q = p - 1$. Its proof is deferred to Section 5.3.\n\\textbf{Lemma 6.} \\emph{Suppose that Assumption 1 holds, and Assumption 2 holds for $p > 2$. Let $\\{(x^k, m^k)\\}_{k\\geq 0}$ be generated by Algorithm 1 with $q = p - 1$, and let $\\{\\eta_k, \\gamma_{k,t}, \\theta_{k,t}\\}_{1\\leq t\\leq p-1, k\\geq 0}$ be inputs of Algorithm 1. Assume that $\\{\\gamma_{k,t}, \\theta_{k,t}\\}_{1\\leq t\\leq p-1, k\\geq 0}$ satisfies (28) and (29). Then,}\n\n$E_{\\xi^{k+1}}[||m^{k+1} - \\nabla f(x^{k+1})||^2] \\leq \\Big(1 - \\sum_{t=1}^{p-1} \\theta_{k,t}\\Big) ||m^k - \\nabla f(x^k)||^2 + \\frac{p L_p^2 \\eta_k^{2p}}{(p!)^2 \\sum_{t=1}^{p-1} \\theta_{k,t} \\gamma_{k,t}^{2p}} \\Big(1 + \\frac{\\sigma^2}{\\eta_k^2} \\sum_{t=1}^{p-1} \\frac{1}{\\gamma_{k,t}^2}\\Big) + (p - 1)\\sigma^2 \\sum_{t=1}^{p-1} \\theta_{k,t}^2,$\n\nwhere $\\sigma$ and $L_p$ are given in Assumptions 1(b) and 2, respectively.\nWe next derive an upper bound for the average expected error of the stationary condition among all iterates generated by Algorithm 1 with $q = p - 1$. Its proof is relegated to Section 5.3."}, {"title": "3.2.1 Input parameters and convergence rate", "content": "We now specify the input parameters of Algorithm 1 with $q = p- 1$ and analyze its sample complexity. We first define a quantity that will be used to set the input parameters:\n\n$k_p = p^{(3p+1)/(2p)"}, ".", "n\nTo analyze the sample complexity of Algorithm 1 with $q = p - 1$, we specify $\\{\\eta_k, \\gamma_{k,t}\\}_{10$ defined in (36) and (38), respectively. Its proof is deferred to Section 5.3.\n\\textbf{Lemma 7.} \\emph{Let $\\{\\theta_{k,t}\\}_{1-1} \\subset (0,1) \\quad \\forall k \\geq 0,$\n\n$|\\theta_{k,t}| \\leq \\frac{4((p - 1)!)2}{(k + k_p)^{4p/(3p+1)}} \\quad \\forall 1 \\leq t \\leq p - 1, k \\geq 0,$\n\nwhere $k_p$ is defined in (35). Moreover, $\\{\\theta_{k,t}, \\rho_k\\}_{1\n    },\n    {", "title\": \"4 Numerical experiments", "content", "In this section we conduct some preliminary numerical experiments to test practical performance of our SFOMs with multi-extrapolated momentum (Algorithm 1). We compare our SFOMs against the normalized stochastic gradient method with Polyak momentum (SG-PM) [3] and STORM [4] on a robust regression problem. All the algorithms are coded in Matlab, and all the computations are performed on a laptop with a 2.20 GHz Intel Core i9-14900HX processor and 32 GB of RAM.\nSpecifically, we consider the robust regression problem:\n\n$\\min_{x\\in\\mathbb{R}^n} \\sum_{i=1}^m \\phi(a_i^T x - b_i),$\n\nwhere $\\phi(t) = t^2/(1 + t^2)$, and $\\{(a_i, b_i)\\}_{1<i>archive.ics.uci.edu/datasets"]}, {"title": "5 Proof of the main results", "content": "In this section we provide proofs of the main results in Sections 2 and 3, which are particularly Lemmas 1 to 7 and Theorems 1 to 4.\nTo proceed, we first establish several technical lemmas. The following lemma concerns the estimation of the partial sums of series.\n\\textbf{Lemma 8.} \\emph{Let $\\zeta(\\cdot)$ be a convex univariate function. Then, for any integers $a, b$ satisfying $[a-1/2,b+1/2] \\subset dom\\zeta$, it holds that $\\sum_{s=a}^b \\zeta(s) \\leq \\int_{a-1/2}^{b+1/2} \\zeta(\\tau)d\\tau$.}\n\\textbf{Proof.} Since f is convex, one has $\\zeta(s) \\leq \\int_{s-1/2}^{s+1/2} \\zeta(\\tau)d\\tau$ for all $s\\in [a, b]$. It then follows that $\\sum_{s=a}^b \\zeta(s) \\leq \\int_{a-1/2}^{b+1/2} \\zeta(\\tau)d\\tau$ holds as desired.\nAs a consequence of Lemma 8, we consider $\\zeta(\\tau) = 1/\\tau^p$ for some $a \\in (0, \\infty]$, where $\\tau\\in (0,\\infty)$. Then, for any positive integers $a, b$, one has\n\n$\\sum_{r=a}^b 1/r^p \\leq\\begin{cases}\nln(b + 1/2) - ln(a - 1/2) & \\text{if } a = 1, \\\\\n\\frac{1}{p-1}((a - 1/2)^{1-p} - (b + 1/2)^{1-p}) & \\text{if } a \\in (0, 1) \\cup (1, +\\infty).\n\\end{cases}$\n\nWe next provide an auxiliary lemma that will be used to estimate the maximum number of iterations for achieving targeted approximate stationarity in expectation.\n\\textbf{Lemma 9.} \\emph{Let $a \\in (0,1)$ and $u \\in (0,1/e)$ be given. Then, $1/v^a ln v < 2u/a$ holds for all $v$ satisfying $v \\geq (1/u ln(1/u))^{1/a}$.}\n\\textbf{Proof.} Let $v$ be such that $v \\geq (1/u ln(1/u))^{1/a}$. By this and $u \\in (0,1/e)$, one has $v \\geq (1/u ln(1/u))^{1/a} > e^{1/a}$. Denote $\\phi(v) = 1/v^a ln v$. Since $\\phi$ is decreasing over $(e^{1/a}, \\infty)$, it follows that\n\n$1/v^a \\ln v = \\phi(v) \\leq \\phi((1/u \\ln(1/u))^{1/a}) = \\frac{a}{u} \\Big(1 + \\frac{\\ln \\ln(1/u)}{\\ln(1/u)}\\Big) \\leq \\frac{2u}{a},$\n\nwhere the last inequality is due to $\\ln \\ln(1/u) < \\ln(1/u)$ for all $u \\in (0,1/e)$. Hence, the conclusion of this lemma holds as desired.\n### 5.1 Proof of Lemmas 1 and 2\n\\textbf{Proof of Lemma 1.} For convenience, we denote $\\phi(x) = \\langle \\nabla f(x), u\\rangle$. In view of this and the definition of $\\nabla^r f$, one can see that\n\n$D^r \\phi(x)[y - x]^r = \\langle \\nabla^{r+1} f(x)(y - x)^r, u\\rangle \\quad \\forall 1 \\leq r < p - 1, x, y \\in \\mathbb{R}^n$.\n\nSince $f$ is pth-order continuously differentiable, we have that $\\phi$ is (p-1)th-order continuously differentiable, and also that\n\n$||D^{p-1} \\phi(y) -"}]