{"title": "MCGM: MASK CONDITIONAL TEXT-TO-IMAGE\nGENERATIVE MODE", "authors": ["Rami Skaik", "Leonardo Rossi", "Tomaso Fontanini", "Andrea Prati"], "abstract": "Recent advancements in generative models have revolutionized the field of artificial intelligence, enabling\nthe creation of highly-realistic and detailed images. In this study, we propose a novel Mask Conditional Text-\nto-Image Generative Model (MCGM) that leverages the power of conditional diffusion models to generate\npictures with specific poses. Our model builds upon the success of the Break-a-scene [1] model in generating\nnew scenes using a single image with multiple subjects and incorporates a mask embedding injection that\nallows the conditioning of the generation process. By introducing this additional level of control, MCGM\noffers a flexible and intuitive approach for generating specific poses for one or more subjects learned from\na single image, empowering users to influence the output based on their requirements. Through extensive\nexperimentation and evaluation, we demonstrate the effectiveness of our proposed model in generating high-\nquality images that meet predefined mask conditions and improving the current Break-a-scene generative\nmodel.", "sections": [{"title": "1. INTRODUCTION", "content": "Text-to-Image generation, i.e. generating images from a text description, is a challenging yet\nfascinating task that has gained prominence in the field of artificial intelligence and computer\nvision. The ability to generate realistic images from textual descriptions has vast implications in\nseveral domains, including but not limited to, content creation, virtual reality, and design\nautomation [2]. Over the years, researchers have developed many techniques and models to tackle\nthis problem, each with their strengths and limitations. Conditional Text-to-Image generators have\nemerged as a powerful paradigm in artificial intelligence, enabling the synthesis of images based\non specific textual descriptions [3,4]. In addition, there exist several other applications such as\nClass-Conditional Image Generation [5,6], Image Editing and Inpainting [7], Audio Synthesis [8],\nText Generation [9], Climate and Weather Modeling [10], and more.\nDiffusion models are a class of generative models that learn to generate data by reversing a gradual\nnoise process. They consist of two main phases: the forward diffusion process, in which noise is\ngradually added to the data, and the backward process, in which the model learns to gradually\ndenoise the data to generate new patterns. These models quickly became the state of the art of\nimage generation due to their ability to generate high-quality and high-resolution samples and\ndiverse patterns with a stable training process, outperforming traditional generative models such as\nGANs [6]. Their foundation in established theoretical principles, their flexibility in application,\nand their continuous improvements through ongoing research further increase their effectiveness\nand versatility [11]. Through training on large datasets, conditional diffusion models learn to\ncapture complex dependencies between the conditioning information and the generated images,\nresulting in realistic and diverse outputs [12]. On the other side, diffusion models encounter\nlimitations when trained on few images, as their effectiveness heavily relies on the volume and\ndiversity of training data, which impacts their ability to generalize and produce high-quality outputs\n[11]. For this reason, recently, many approaches have been presented to enhance and improve\ndiffusion models and force them to learn one or more concepts by using only few examples [13,14].\nBy doing so, it is possible to generate the learned concepts in different contexts using textual\ndescriptions.\nAdditionally, some models can even generate and edit a specific subject by using only a single\nimage [15]. To achieve this, these systems use transfer learning in text-to-image diffusion models,\nby either fine-tuning all parameters [14] or producing and optimizing a word vector [13] for a new\nconcept. Additionally, Break-a-scene [1] can extract multiple concepts from a single image."}, {"title": "2. RELATED WORKS", "content": "Nevertheless, when generating the concepts during inference, it is very difficult to specify a precise\npose due to the vagueness of the text description. For example, after the diffusion model was trained\nto generate a specific dog, the user could want to generate a picture of that dog sitting in a very\nspecific pose, which is very difficult to describe only through text. This limits the applicability of\nthe aforementioned methods.\nIn this paper, we propose a Mask Conditional Text-to-image Generative Model (MCGM), which\nimproves the Break-a-scene model [1] using a mask condition that specifies the target pose of the\ngenerated samples. MCGM is able to generate a new scene starting\nfrom a single image that has one or more objects and specify the pose of these objects using the\ntext and mask condition. By doing so, it is possible to obtain more fine-grained control of the\noutput. More in detail, in the proposed architecture the text is responsible for the overall appearance\nof the scene, while the mask influences the pose of the subject. Finally, the contributions of this\nwork are the following:\n\u2022 A novel architecture capable of reproducing the subjects starting from a single image in a\ndifferent context and pose using a text description and the mask of the desired pose.\n\u2022 The capability of the masks to act also as weak conditioning during inference. More in\ndetail, we can inject masks that do not strictly correspond to the desired subject, but they\nare still able to condition the generated samples without causing undesired artifacts.\n\u2022 An additional mask cross-attention loss was introduced during training to increase the\nability to link the masks with their corresponding subjects."}, {"title": "2.1. Fine-tuning diffusion models:", "content": "Diffusion-based models [6,16\u201318] emerged as the new state of the art for text-to-image generation\n[19-22]. In this context, finetuning conditional diffusion models for text-to-image generation\nrepresents a critical area of research, aiming to enhance the performance, control, and application\nrange of these models. Zhang et al. [23] highlighted that fine-tuning conditional diffusion models\non specific datasets improves text-to-image generation performance. Then, Brown and Smith [24]\nprovided a comparative analysis of fine-tuning strategies, offering insights into optimizing these\nprocesses, while Garcia et al. [25] emphasized the importance of hyper-parameters tuning in\nenhancing the performance of finetuned diffusion models. In MCGM, to generate new\nimages of the subject in different contexts and poses, we based our work on the Break-a-\nscene [1] model which has two phases: the first involves designating a collection of text\ntokens (handles), freezing the model weights, and optimizing the handles, while in the\nsecond one, the whole model weights are fine-tuned while continuing to optimize the\nhandles too."}, {"title": "2.2. Cross-attention", "content": "Cross-attention mechanisms enable the model to effectively align and integrate information from\ntextual descriptions with visual data, significantly enhancing the quality of generated images from\ntextual inputs. In the domain of text-to-image generation, cross-attention layers have been\neffectively utilized in various state-of-the-art models. DALL-E [26] uses cross-attention to align\ntext descriptions with image generation, ensuring the visual output accurately reflects the input\ntext. Then, Imagen [27] employs a cascading pipeline of cross-attention mechanisms to integrate\ntext embeddings into the image generation process, achieving high fidelity and resolution in the\ngenerated images. Additionally, Stable Diffusion [21] utilizes cross-attention layers throughout the\ndenoising steps, allowing the model to generate highly-detailed and semantically-accurate images\nfrom textual descriptions. Cross-attention maps in text-to-image diffusion models are used by\nprompt-to-prompt [20] to edit generated images. This process was then extended to real images by\nMokady et al. [28]. To match text-to-image generations, attend and-excite [29] employs cross-\nattention maps as an explainability-based technique [30]. Furthermore, in Break-a-scene [1] cross-\nattention maps have the purpose of separating learned concepts. More in detail, as opposed to\naltering the input image, Break-a-scene concentrates on extracting textual handles from a scene\nand combining them into entirely new scenes. In MCGM, we utilize the cross-attention technique\nas in Break-a-scene, but we also incorporate the embedding of a mask image as extra information\nalongside text embeddings to help the model create a specific pose of the subject in the scene.\nAdditionally, we also add a novel cross attention loss designed to better connect the masks and the\ncorresponding subjects."}, {"title": "2.3. Personalization in diffusion models", "content": "Recently, several approaches were proposed to reproduce in different contexts and edit real images\nusing diffusion models. In both Textual Inversion (TI) [13] and DreamBooth (DB) [14] the model\nlearns how to generate a visual concept in different situations simply by observing images. A novel\nmethod is used by Textual Inversion [13] to embed concepts from the images into new terms in the\ntext embedding space. By denoising supplied images, this technique increases the tokenizer\ndictionary's size and maximizes more tokens. Conversely, DreamBooth [14] increases the diversity\nof the generated results by updating the UNet parameters with a class-specific prior preservation\nloss and representing concepts with low-frequency words (e.g., \u201cV\u201d). TI and DB are simple,\nflexible frameworks that provide the basis for many fine-tuning-based techniques. Moreover,\nweight deviations during fine-tuning are examined by Custom Diffusion [31], which highlights the\ncritical role of cross-attention layer parameters.\nFurthermore, SVDiff [32] implements a successful personalization technique in the parameter\nspace using the weight kernels' singular-value decomposition. They also give a regularization term\nfor mixing and unmixing, which enables the generation of two concepts that are close to each other.\nIn addition, to enable quick personalization, ELITE [15] proposed a learning-based encoder that\nsupports input masks and encodes a single image. Its global and local mapping networks enable\nprecise and quick custom text-to-image generation. ELITE is still unable to produce more than one\nconcept, though. Finally, Break-a-scene [1] can extract multiple concepts from a single image using\na masking mechanism. Differently from SVDiff which requires several images for each of the\nconcepts, Break-a-scene [1] operates on a single image containing multiple concepts. MCGM\nfollows the Break-a-scene and ELITE idea by using only a single image for the concept, in contrast\nto other methods that require multiple images. In addition, unlike other recent methods, the MCGM\nmethod uses a mask condition in line with the text to set the pose for one or more source subjects."}, {"title": "3. THE METHOD", "content": ""}, {"title": "3.1. Model Architecture", "content": "In MCGM architecture we keep the basic architecture of Break-a-scene [1] adding additional\ncontrol over the pose of the generated samples. Break-a-scene goal is to extract N textual handles\n{vi}=1, where the ith handle vi represents the concept indicated by the mask Mi, given a single\ninput image I and a set of N masks {M}=1 indicating the subjects in the image. Once the handles\nare acquired, they can be utilized to direct the synthesis of images containing the concepts in novel\ncontexts and poses during inference by inserting them into textual prompts.\nIn order to train MCGM to recognize multiple concepts from a single image, a small collection of\nimage-text pairs with the format \u201cA photo of [vx] and [vy]...\u201d is created, then the background is\nmasked out based on the masks provided and adding a solid, arbitrary background. Each time, a\nrandom subset of tokens is chosen.\nDuring training, the model weights [wi] and text embedding are optimized using two distinct\nstages. First, the model weights are frozen and the text embeddings are optimized to match the\nmasked concepts. This enables a fast initial embedding while preserving the capability of the"}, {"title": "3.2. Mask Injection", "content": "In the MCGM pipeline, we added a Mask Encoder Em that aids the diffusion process by\nincorporating conditioning mask information into the cross-attention layers of the model. More in\ndetail, the mask images, which are binary masks, are fed into the Mask Encoder which reduces the\nsize of the mask and are then processed through a linear layer encoder, producing a set of masks\nembeddings. Finally, the text and the mask embeddings are concatenated.\nThe mask encoder is employed in both training and inference processes. In the training process, we\ngenerate the mask embeddings for the input concepts in the source image and use these embeddings\nin the two optimization phases of the model. On the other hand, in the inference process, a set of\narbitrary masks is selected and embedded to specify the target pose of the generated samples and\nare injected alongside the text embeddings in the model. By doing so, in the inference process, the\nmodel uses the conditioning information provided by the mask encoder to generate outputs that are\nconsistent with the mask. Loss functions described in the next sections are designed to ensure that\nthe generated output matches the desired characteristics dictated by the mask."}, {"title": "3.3. The Optimization Losses", "content": "MCGM uses a masked version of the standard diffusion loss [33] to optimize the handles (and\nmodel weights, in the second optimization phase). This loss is calculated over the pixels of the\nsubjects that are defined by the concepts masks:\n$L_{rec} = E_{z,s,\\epsilon\\sim N(0,1),t} [|| \\epsilon \\odot M_s \u2013 \\epsilon_\\theta(Z_t, t, c_s) \\odot M_s||_2]$         (1)\nwhere zt is the latent with added noise at time step t, s is the number of concepts, cs is the set of\nembeddings obtained from text and masks, Ms is the set of masks, e is the noise, and Ee is the\ndenoising network.\nRebuilding the concepts faithfully is encouraged by using the masked diffusion loss [1] in pixel\nspace. In addition to reconstructing the pixels of the learned subjects, the model needs to make sure\nthat each handle only looks at the portion of the image where the related concept is located. To this\nend, another loss term\u2014the Cross-Attention loss has been introduced. This loss is defined as the\nMSE between the input masks and the attention calculated in the cross-attention layers of the\nmodel. The following cross-attention loss is used for each training phase:\n$L_{attn} = E_{z,k,t}[||CA_{\\theta}(V_i, Z_t) \u2013 M_{ik}||_2]$     (2)\nWhere $CA_{\\theta}(v_i, z_t)$ is the attention map of the token vi and the noisy latent z\u0165 obtained from each\ncross-attention layer.\nIn MCGM, mask embeddings are injected with the textual embeddings for each subject in order to\ndefine its pose. This may lead to some ambiguity in the diffusion model to reconstruct each subject\nseparately, which can occasionally result in some mixing characteristics and uncontrollable scenes. Therefore, to control this process of distinguishing between\nthe subjects in the output image, we added a mask cross-attention loss specifically designed for the\nmask tokens. The final cross-attention loss becomes as follows:\n$L_{Mattn} = L_{attn} + \\lambda_mE_{z,k,t}[||CA_{\\theta}(m_i, z_t) \u2013 M_{ik}||]$\n                                                    (3)\nwhere $\u03bb_m$ is the weight for the mask cross attention loss, CA\u0189(mi, zt) is the cross attention map\nbetween the mask token mi and the noisy latent zt. Finally, the full training objective becomes:\n$L_{total} = L_{rec} + \\lambda_{attn}L_{Mattn}$                                       (4)\nAdding LMattn to the loss ensures that concept handles focus on their respective regions. We set\n\u03bbattn = 0.01 for accurate spatial positions in the produced image."}, {"title": "4. EXPERIMENTS", "content": "Experimental setup: In MCGM, we modify the official implementation of the Break-a-scene [1]\nmodel adding a mask encoder and an additional mask cross attention loss function. Concerning the\ninput images utilized in the training and inference processes, we extracted them from different\nsources, some were selected from the COCO dataset [34] which contains images along with their\ninstance segmentation masks, and some were gathered from a variety of images that included one\nor more subjects with suitable shapes to test different poses (such as an animal and a human). For\nthe latter, we used the STEGO segmentation system [35] to extract the matching masks for each\nimage in order to use them during the training or inference process. Finally, we resized each image\nand mask to be 512\u00d7512 pixels."}, {"title": "4.1. Experiments And Discussion", "content": "One-subject examples. In this experiment, we used a single subject to test the effects of the\nmask encoder on image generation. We decided to test this scenario using different\nconfigurations:\n1. Textual prompts with coherent masks: Fig. 3 shows several experiments using different textual\nprompts with an associate and coherent mask that guides the image generation. We have three\nsubjects (dog, cat, and horse) and four pairs of prompts and masks that present different actions.\nIn the output, we can recognize that the generated scene follows the text prompt and the subject\npose is defined by the mask. Generally speaking, the model does not reproduce the injected\nmask shape exactly; however, it does modify the subject poses to closely resemble the mask,\nwhile allowing for an extra degree of flexibility specified by the text. On the other side, this\nallows to use a mask that does not represent exactly the subject, like the dog-shaped mask used\nfor the horse in the figure, and still obtain good results."}, {"title": "4.2. Ablation study", "content": "Generating images with and without mask cross-attention loss in training phase: To evaluate\nthe effectiveness of the proposed method, in Fig. 9 we present results obtained with mask cross-\nattention loss, without mask cross-attention loss, and also with the baseline Break-a-scene model\n(that do not use the proposed mask condition injection). The top row of the figure shows three input\ntextual prompts with a pair of masks for each prompt (one mask for each concept). The third row\nshows the output of the MCGM with mask cross attention loss, while the fourth row presents results\nwithout this loss, and the bottom row presents the results of the Break-a-scene model.\nIndeed, without using mask cross attention loss in MCGM, the model has some problems\ndisentangling the concepts, and the output scenes present mixed concepts as shown in Figure 9\nwhere the cat and the dog mixed. For this reason, we added a new mask cross-attention loss during\ntraining to train the pose in addition to the cross-attention loss used in the Break-a-scene model.\nWith this addition, in the figure, it is evident how now the model is able to disentangle the subjects\nperfectly following the correct pose given by the masks. On the other side, we can recognize that\nby using the original Break-a-scene model we can generate multiple concepts in one scene but\nwithout controlling the poses of the concepts."}, {"title": "4.3. User Study", "content": "Since established and well-recognized quantitative metrics for single image text-to-image\ngeneration do not exist, we conducted a user study in order to evaluate the quality of the generated\nimage. A random subset of our results was selected and provided to a set of anonymous users\nthrough a survey. We have envisioned two different types of questions: one multiple-choice and\none based on the Likert scale [36]. The multiple-choice type has been used to evaluate the extent\nof the mask's effect on the generation. Given the mask and two synthesized images (MCGM vs.\nBreak-a-scene), the evaluators were asked to select one image:\" Please choose the image that is\nclosest to the mask pose\u201d. The Likert scale (scale from 1 to 5) questions are employed to rate Text-\nMask alignment: \u201cSelect the degree to which the photo result matches the input text and mask\u201d.\nFifteen examples were provided, 11 for single-subject images, and 4 for two-subject images. Given\nthe original input image, the text prompt, the mask used to generate the output, and the generated\nimages, the evaluators were asked to choose the degree of matching between the output and the\ntext and the mask pose used. 82 users have been involved and each of them was asked to answer\n23 questions, for a total of 1886 responses.\nAs shown in Fig. 10, in the first part of the survey 91.3% of the evaluators chose the MCGM output\nimage, confirming that the images generated by our method are closer to the mask shape."}, {"title": "LIMITATION AND CONCLUSIONS", "content": "The MCGM model suffers from a directional difference issue. In some cases, the model follows\nthe text prompt and the mask position to generate the concept in that position but struggles to\ngenerate the same orientation. When using a mask condition for the subject\nwith a side view, the model generates images for the subjects using the correct mask action position\nbut with a front view.\nFurthermore, when more than two mask conditions are used, our model fails to generate subjects\nin their correct position . The text prompt suggests three actions for three subjects:\nsleeping asset1, flying asset3, and running asset0 (knowing that: asset0 represents the dog, asset1\nthe cat, and asset3 the bird). The mask conditions consist of three masks, one mask for each action;\nthe results show that the positions of the mask conditions did not apply to the generated subjects.\nIn conclusion, the proposed MCGM model has proven to be a powerful tool for generating high-quality images with diverse mask attributes. By leveraging the conditional diffusion process and\nincorporating masked encoder mechanisms, this model has demonstrated a new feature compared\nto existing state-of-the-art generative models that use a single image for training, by adding a mask\ninjection part to the Break-a-scene model. This feature has allowed MCGM more control and\nflexibility in determining the subject pose in the output images. MCGM offers a promising path\nfor advancing the field of generative modeling and enhancing the capabilities of image synthesis\nand generative tasks. Further research and development in optimizing the model training procedure\nand scalability could potentially unlock even greater potential for this approach."}]}