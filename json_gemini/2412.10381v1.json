{"title": "Supervised Learning-enhanced Multi-Group Actor Critic for Live-stream Recommendation", "authors": ["Liu Jingxin", "Gao Xiang", "Li YiSha", "Li Xin", "Lu Haiyang", "Wang Ben"], "abstract": "Reinforcement Learning (RL) has been widely applied in recommendation systems to capture users' long-term engagement, thereby improving dwelling time and enhancing user retention. In the context of a short video & live-stream mixed recommendation scenario, the live-stream recommendation system (RS) decides whether to inject at most one live-stream into the video feed for each user request. To maximize long-term user engagement, it is crucial to determine an optimal live-stream injection policy for accurate live-stream allocation. However, traditional RL algorithms often face divergence and instability problems, and these issues are even more pronounced in our scenario. To address these challenges, we propose a novel Supervised Learning-enhanced Multi-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a supervised learning-enhanced actor-critic framework that incorporates variance reduction techniques, where multi-task reward learning helps restrict bootstrapping error accumulation during critic learning. Additionally, we design a multi-group state decomposition module for both actor and critic networks to reduce prediction variance and improve model stability. Empirically, we evaluate the SL-MGAC algorithm using offline policy evaluation (OPE) and online A/B testing. Experimental results demonstrate that the proposed method not only outperforms baseline methods but also exhibits enhanced stability in online recommendation scenarios.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning has shown great potential in learning policies to maximize long-term rewards across various research domains, such as computer vision, robotics, natural language processing, gaming, and recommendation systems [1, 24]. In the context of recommendation systems, reinforcement learning is applied to optimize long-term user engagement [50] and improve user retention [4]. Numerous RL applications have been proposed for real-world recommendation systems, including slate recommendation [8, 18, 27], personalized search ranking [29], and advertisement allocation [25].\nWe consider a challenging sequential decision-making scenario in a short video & live-stream blended recommendation system, as shown in Fig. 1. The system consists of three components: a live-stream recommendation system, a short video recommendation system, and a blending server. For each user request at timestamp t, the live-stream recommendation system decides whether to inject the recommended live-stream into the video feed, while the short video recommendation system suggests B (with B < 10) videos. The blending server then mixes and rearranges the single live-stream with the B videos to form the final recommendation list. Our objective is to find a personalized optimal live-stream injection policy that maximizes users' long-term engagement with live-streams, which can be modeled as an infinite request-level Markov Decision Process (MDP) [33].\nHowever, the intrinsic issues of divergence and instability associated with traditional reinforcement learning (RL) algorithms [7, 10, 12, 23] are significantly exacerbated in this challenging scenario. A primary possible reason for this is the drastic fluctuations in both the live-stream supply scale and user interaction behaviors over time. As shown in Fig. 2a, the curves of live-stream room count and live-stream viewer count vary significantly on a daily basis, making it difficult for the RL algorithm to learn optimal policies. Furthermore, as illustrated in Fig. 2b, the distributions of user interaction labels (e.g., live-stream and short video watch time) are highly noisy and exhibit large variance. RL models often struggle to learn effective policies from data that exhibit such high variance across both time and user scales. In practice, we observe that RL models frequently encounter issues of policy deterioration or model collapse [9]."}, {"title": "2 Problem Formulation", "content": "As shown in Fig. 1, in the short video & live-stream mixed recommendation system, the live-stream recommendation system is treated as an agent that interacts with diverse users and receives user feedback over time. The optimal live-stream injection control problem is modeled as an infinite request-level MDP to maximize the cumulative reward. Formally, we define the live-stream injection MDP as a tuple of five elements (S, A, P, R, \u03b3):\n\u2022 State space S: This is the set of user interaction states s, which includes user static features (e.g., user ID, location, gender, country), user history features (e.g., live-stream watch history, short video watch history), and item features (e.g., live-stream ID, author ID, author gender, online viewer count). We limit the length of users' live-stream and short video history lists by timestamp order, keeping only the top 50 items.\n\u2022 Action space A: The action $a \\in A$ represents the decision of whether to inject a recommended live-stream in response to a user's request. We define action a as a binary variable, where a = 1 means injecting a live-stream, and a = 0 means not injecting it.\n\u2022 Transition Probability P: The transition probability is denoted as $p(s_{t+1}|s_t, a_t)$, determined by the environment.\n\u2022 Reward Function R: The reward function R is a mapping from state $s_t$ and action $a_t$ at timestamp t, which can be formulated as $r(s_t, a_t) : S \u00d7 A \u2192 R$.\n\u2022 Discount Factor \u03b3: The discount factor $\u03b3 \\in [0, 1]$ is used in the computation of cumulative reward. Typically, we set $\u03b3 < 1$ during model training."}, {"title": "2.1 Reward Function Design", "content": "In our mixed short video & live-stream recommendation scenario, we design a reward function based on the time gain of live-stream watch time relative to the average short video watch time in a request:\n$r(s_t, a_t) = \\begin{cases}\n\\frac{y_l}{\\beta y_0} & \\text{if } a_t = 1 \\\\\n1 & \\text{otherwise}\n\\end{cases}$ (2)\nwhere $y_l$ is the live-stream watch time, $y_0$ is the total video watch time, B is the number of videos in a request, and \u03b2 is the hyper-parameter in Eq. 2. Note that the recommended list in a request always contains B videos and at most one live-stream. Furthermore, the reward defined above is similar to that of the RCPO [38] algorithm. From the perspective of constrained reinforcement learning, the term $\\frac{1}{\u03b2y_0}$ can be viewed as an implicit cost constraint. In practice, the hyper-parameter \u03b2 is introduced to carefully control the final live-stream injection ratio.\nSince the distribution of live-stream (short video) watch time changes drastically over time, as shown in Fig. 2b, it is challenging for an RL agent to optimize the cumulative reward in Eq. 2. To address this, we propose a reward normalization technique as follows:\n$r(s_t, a_t) = \\begin{cases}\nsigmoid(\\frac{y_l - y_0}{y_0}), & \\text{if } a_t = 1 \\\\\n\u03b2 & \\text{otherwise}\n\\end{cases}$ (3)\nwhich helps stabilize the network gradient, thereby reducing the risk of model divergence."}, {"title": "3 Proposed Framework", "content": "To address the intrinsic divergence and instability issues of reinforcement learning and successfully deploy the RL agent in our mixed short video & live-stream recommendation system, we propose a novel Supervised Learning-enhanced Multi-Group Actor-Critic algorithm (SL-MGAC), as shown in Fig. 3. SL-MGAC incorporates a supervised learning-enhanced actor-critic model with a shared user & live-stream feature extraction module, along with independent Multi-Group State Decomposition (MG-SD) modules. For clarity, only one critic network is shown in Fig. 3, while the remaining three critic networks, which use the same architecture, are omitted."}, {"title": "3.1 User & Live-stream Feature Extraction Module", "content": "The user & live-stream feature extraction module aims to generate non-linear embedding representations for the state $s_t$. First, we use a unified embedding layer to map the user's static features, live-stream features, historical live-stream list features and short video list features into low-dimensional dense representations. We denote $v_U$, $v_L$, {$e_1, \\dots, e_{M_1}$},{$e'_1, \\dots, e'_{M_0}$} as the corresponding embedding vectors or sets of embedding vectors, respectively. We then define $h_a = [v_U, v_L]$ as the concatenation of $v_U$ and $v_L$.\nTo aggregate historical live-stream (short video) representations, we introduce the target attention mechanism [41, 49], which is defined as follows:\n$h_{live} = \\sum_{i=0}^{M_1} f_i(h_a, e_i)e_i$ (4)\n$h_{video} = \\sum_{j=0}^{M_0} f_o(h_a, e'_j)e'_j$\nwhere $f_i, f_o$ are different target attention functions, such as a feed-forward network whose output is a normalized score.\nAfter aggregating the historical live-stream and short video features via two separate attention networks, we concatenate all the above embedding vectors to form $h_s = [v_U, v_L, h_{live}, h_{video}]$. We then use a shared multi-layer perceptron (MLP) for both the actor and critic networks to obtain the latent representation of state $s_t$, i.e. $h' = f_{MLP}(h_s)$. To ensure training stability, we stop the gradient flow through $h'$ for the actor network and only use the more complex critic networks to optimize $h'$, as we find that sharing the feature extraction module and the $f_{MLP}$ network causes interference between the policy and critic gradients."}, {"title": "3.2 Multi-Group State Decomposition Module", "content": "Since live-stream content is not equally appealing to all users on the short video app Kwai, it is crucial to inject live-streams selectively into the short video feed. Otherwise, excessive live-stream exposures may disrupt a user's interest in short videos, leading to a decrease in overall app usage duration. In practice, user interaction data for live-streams is sparser and noisier compared to that for short videos, making it challenging for RL algorithms to learn an optimal live-stream injection policy for each individual user.\nTherefore, prior information about diverse user groups is essential for enhancing the decision-making accuracy of RL models. A natural approach to distinguish between different user behaviors is to partition users into distinct groups based on their historical activity. Specifically, we categorize users into K disjoint groups according to their live-stream activity level, which is determined by the cumulative live-stream watch time over the past 3 weeks. Users with higher historical watch time are assigned to groups with higher activity levels.\nFormally, we present the following theorem, which demonstrates that the proposed multi-group state decomposition can be viewed as a type of variance reduction technique for real-world RL applications.\nTHEOREM 3.1. Let state space $S = \\cup_{i=1}^K S_i$, $\u2200i \u2260 j S_i \u2229 S_j = \u00d8$, states $\u2208 S$; r be the real reward of state s and action a, $\\hat{r}$ be the predicted value of r over S, $\\hat{r_i}$ be the predicted reward of r over $S_i$; then a neural network f with MG-SD module has a lower prediction variance than the neural network f' without MG-SD module."}, {"title": "3.3 Supervised Learning-enhanced Actor Critic", "content": "To alleviate the impact of drastic changes in data distribution that could lead to the divergence or instability of RL models, we propose a supervised learning-enhanced actor critic framework to prevent critic networks from getting trapped in model collapse due to large cumulative errors in the critic learning process."}, {"title": "3.3.1 Layer Normalization", "content": "A recent work on RL divergence and instability [45] shows that offline critic learning exhibits a self-excitation pattern. Specifically, Q-value iteration can inadvertently cause the target Q value $Q(s_{t+1}, a_*)$ to increase even more than the increment of $Q(s_t, a_t)$, which amplifies the TD-error and traps the critic learning process in a positive feedback loop until model collapse. Therefore, normalization techniques, such as Layer Normalization [3] can be utilized to alleviate the divergence and instability problems.\nFrom the proof in the Appendix D of [45], we know that for any input x and any direction v, if a network applies Layer Normalization to the input, then we have $k_{NTK}(x, x + \u03bbv) \u2264 C$, where $\u03bb > 0$, C is a constant and $K_{NTK}$ is the Neural Tangent Kernel (NTK) [19]. This theoretically indicates that a network with Layer Normalization is less sensitive to input variations and can maintain stable gradients despite perturbations during model training. Hence, we apply Layer Normalization to the inputs of both the actor and critic networks, as shown in Fig. 3."}, {"title": "3.3.2 Critic Network", "content": "Let $(s_t, A_t, r_t, s_{t+1}, A_{t+1}, r_{t+1}) \u2208 D$ be a training sample from our real-time dataset D. To address the maximization bias problem [40], we employ four critic networks: two current Q-networks $Q_{\u03c6_1}, Q_{\u03c6_2}$ and two corresponding target Q-networks $\u03a6'_1, \u03a6'_2$ in Clipped Double Q-Learning [12]. The critic learning objective is as follows:\n$L_{Critic} = \\sum_{i=1}^2 E_{(s,a) \\in D} [Q_{\u03c6_i}(s_t, a_t) \u2013 Q_{label}(s_{t+1})]^2$ (5)\n$Q_{label}(s_{t+1}) = r(s_t, a_t) + \u03b3 \\max_{A_{t+1}} Q'(s_{t+1}, A_{t+1})$\nwhere $Q'(s_{t+1}, A_{t+1}) = \\min_{i=1,2} Q'_{\u03c6_i}(s_{t+1}, A_{t+1})$. We use Huber Loss [16] to optimize the above objective.\nIn practice, $Q_{label}(s_{t+1})$ may be dominated by $Q'(s_{t+1}, a_*)$ during critic learning, which can affect the performance of the critic networks. Therefore, we seamlessly introduce supervised learning (e.g. multi-task learning [48]) into the critic learning procedure. Specifically, we split the critic network $Q_{\u03c6_i}$(or $Q'_{\u03c6_i}$) into two components: a Reward Prediction Network (RPN) and a Q Residual Network (QRN), as follows:\n$Q_{\u03c6_i}(s_t, a_t) = R_{\u03c6_i}(s_t, a_t) + \u03b3 T_{\u03c6_i}(s_t, a_t), i = 1, 2$ (6)\n$Q'_{\u03c6_i}(s_{t+1}, a_{t+1}) = R'_{\u03c6_i}(s_{t+1}, a_{t+1}) + \u03b3 T'_{\u03c6_i}(s_{t+1}, a_{t+1}), i = 1, 2$\nwhere $R_{\u03c6_i}$ and $T_{\u03c6_i}$ are RPN and QRN, $R'_{\u03c6_i}$ and $T'_{\u03c6_i}$ are target RPN and QRN, respectively. We use separate MLPs to model the RPN and QRN.\nSince the distribution of live-stream (short video) watch time changes drastically over time, the time gain reward r in Eq. 2 becomes difficult to learn. In this work, we propose a distribution discretization method to improve reward learning.\nFormally, we divide the live-stream and short video watch time distribution into $N_l$ and $N_v$ non-overlapping bins, where each bin represents an interval of live-stream (short video) watch time. Let y denote the actual live-stream or short video watch time, which falls into the time bin $[y_{st}, y_{end}]$. The proportion of y within this bin is $\u03b4 = \\frac{y - y_{st}}{y_{end} - y_{st}} \\in [0, 1]$. Then, we have a linear reconstruction layer to reconstruct y from \u03b4:\n$y = y_{st} + \u03b4 \\cdot (y_{end} - y_{st})$ (7)\nwhich resembles the structure of linear reward shifting [35].\nLet $o(t)_l \u2208 R^{N_l+1}$, $o(t)_v \u2208 R^{N_v+1}$ be one-hot vectors representing the time bins in which the real live-stream (short video) watch time of sample $(s_t, a_t, r_t)$ falls. Note that a separate bin is set for the case $a_t = 0$. Then, the RPN $R_{\u03c6_i}$ can be modeled by multi-task neural networks:\n$R_{\u03c6_i}(s_t, a_t) = sigmoid(F_{r_i}(s_t, a_t) \u2013 G_{r_i}(s_t, a_t))$ (8)\n$F_{r_i}(s_t, a_t) = o(t)_l^T \\cdot (y_{st}^l + f_{r_i}(s_t, a_t) \\cdot (y_{end}^l - y_{st}^l))$\n$G_{r_i}(s_t, a_t) = o(t)_v^T \\cdot (y_{st}^v + g_{r_i}(s_t, a_t) \\cdot (y_{end}^v - y_{st}^v))$\nwhere $y_{st}^l, y_{end}^l \u2208 R^{N_l+1}$ are predefined left (right) boundary vectors for the live-stream watch time bin, $y_{st}^v, y_{end}^v \u2208 R^{N_v+1}$ are predefined left (right) boundary vectors for the short video watch time bin. Note that introducing the posterior one-hot vectors $o(t)_l$ and $o(t)_v$ in Eq. 8 will not affect the calculation of $Q'_{\u03c6_i}(s_{t+1}, \\cdot)$, because we have already reserved $r_{t+1}$ in the dataset D. Therefore, $o(t+1)_l$ and $o(t + 1)_v$ can be easily obtained from $r_{t+1}$ to compute $R'_{\u03c6_i}(s_{t+1}, \\cdot)$. As shown in Eq. 8, we employ two separate MLPs, $f_{r_i}(s_t, a_t)$ and $g_{r_i}(s_t, a_t)$, to predict the proportions within a time bin. We then reconstruct the predicted live-stream and short video watch times, $F_{r_i}(s_t, a_t)$ and $G_{r_i}(s_t, a_t)$, respectively. Finally, we obtain the predicted normalized reward, $R_{\u03c6_i}(s_t, a_t)$. To optimize the predicted watch time proportions, we use Huber Loss as follows:\n$L_{SL} = \\sum_{i=1}^2 Huber\\_Loss(\u03b4_l, F_{r_i}) + Huber\\_Loss(\u03b4_v, G_{r_i})$ (9)\nwhere $\u03b4_l$, $\u03b4_v$ are the actual proportion labels for live-stream and short video watch times, respectively."}, {"title": "3.3.3 Actor Network", "content": "We also incorporate the multi-group state decomposition module into the actor network. The loss function of the actor network is shown below:\n$L_{Actor} = E_{(s,a) \\in D} [-Q(s_t, a_t) log p(s_t, a_t)]$ (10)\nwhere $Q(s_t, a_t) = \\min_{i=1,2} Q_{\u03c6_i}(s_t, a_t)$, and $p(s_t, a_t)$ is the action probability output of the actor network.\nMoreover, we observe that large values of $Q(s_t, a_t)$ may cause instability in actor training, which can ultimately lead to policy deterioration, where $\u2200t, \u03c0(a_t = 1|s_t) > \u03c0(a_t = 0|s_t)$ or $\u03c0(a_t = 1|s_t) < \u03c0(a_t = 0|s_t)$. To address this, we apply softmax normalization to $Q(s_t, a_t)$ to obtain $Q_{norm} = softmax(Q(s_t, a_t))$, and propose a modified actor loss function:\n$L_{Actor} = E_{(s,a) \\in D} [-Q_{norm}(s_t, a_t) log p(s_t, a_t)]$ (11)\nA similar loss function can be found in AWAC [32], and its theoretical results demonstrate that the above loss is equivalent to an implicitly constrained RL problem:\n$\u03c0_{tk+1} = arg \\max_{\u03c0\u2208\u03a0} E_{a \u223c \u03c0(\u00b7|s)}[\\hat{Q}_k(s_t, a_t)]$\ns.t. $D_{KL}(\u03c0(\u00b7|s) || \u03c0_0(\u00b7|s)) \u2264 \u03b5$\n(12)\nwhere $D_{KL}$ is the Kullback-Leibler (KL) divergence, and $\u03c0_0$ is the behavior policy derived from the dataset D.\nWe note that the actor loss in Eq. 11 is a standard cross-entropy loss function. From the perspective of knowledge distillation [17], the actor distills policy knowledge from more complex critic networks. The teacher critic networks guide the more lightweight student actor network to adjust and converge to an optimal policy. In practice, we only need to deploy the actor network in the online live-stream recommendation system, which significantly reduces computational complexity and improves real-time respond speed. Overall, the final loss function of our proposed SL-MGAC algorithm is as follows:\n$L = L_{Actor} + L_{Critic} + L_{SL}$ (13)"}, {"title": "3.4 Online Exploration and Deployment", "content": "For online exploration, we adopt the commonly used e-greedy [42] strategy:\n$\u03c0_{online}(s_t) = \\begin{cases}\nrandom \\ action \\ from A(s_t), & if \u03c8 < \u03b5 \\\\\narg \\max_{a\u2208 A(s_t)} \u03c0(a_t|s_t), & otherwise\n\\end{cases}$ (14)\nwhere y is a random number, and \u03b5 is maximal exploration probability.\nWe deploy the SL-MGAC algorithm in our recommendation system, and the overall system architecture is shown in Fig. 4. The online RL agent collects real-time user interaction logs, while the offline model trainer optimizes the SL-MGAC model in an off-policy manner using streaming training data. Moreover, the offline trainer sends the latest model parameters to update the online deployed actor network in real-time."}, {"title": "4 Experiments", "content": "We conduct both offline evaluation on a real-world dataset collected from our recommendation system and online A/B test experiments with SL-MGAC and the baselines."}, {"title": "4.1 Dataset", "content": "Due to the lack of publicly available datasets for recommendation decisions involving live-streaming injections in short video feeds, we collect a dataset from our recommendation system with 10,000 users through random sampling over a full day, ensuring that the proportions of different user groups are similar to those in real online data. In total, we have over 180,000 samples, which are split into training and test sets with a 4:1 ratio."}, {"title": "4.2 Compared Methods", "content": "4.2.1 Baselines. We compare our approach with existing non-reinforcement learning and reinforcement learning methods.\n\u2022 Learning to Rank (L2R) [5]. A supervised learning framework that predicts the reward for each action. The action with the maximum reward is selected as the agent's action.\n\u2022 DQN [31]. A deep neural network algorithm for Q-learning that introduces the technique of target network updates.\n\u2022 BCQ [13]. A widely used offline reinforcement learning algorithm that adds action restrictions and encourages the agent to behave similarly to the behavior policy.\n\u2022 SAC [6, 15]. A classic off-policy reinforcement learning method that maximizes the trade-off between cumulative reward and policy entropy. We use the discrete version of SAC in later experiments.\n\u2022 TD3 [12]. A modified version of DDPG [26] that addresses function approximation errors using three techniques: Clipped Double Q-Learning, Delayed Policy Updates, and Target Policy Smoothing.\n\u2022 TD3-BC [11]. An offline reinforcement learning variant of the TD3 algorithm, with behavior cloning (BC) regularization to constrain policy learning.\n\u2022 IQL [22]. An offline reinforcement learning algorithm that leverages expectile regression in Q-Learning.\nNote that in our optimal live-stream injection control problem, the action is discrete. Therefore, we introduce the Straight-Through Gumbel Softmax [20] technique into TD3 and TD3-BC."}, {"title": "4.2.2 Variations of our model", "content": "We also compare the SL-MGAC algorithm with several variants in an ablation study to illustrate the effectiveness of multi-group state decomposition, supervised learning for critic learning, distribution discretization (DD), and other techniques.\n\u2022 SL-MGAC (w/o MG): An SL-MGAC variant without the Multi-Group State Decomposition module.\n\u2022 SL-MGAC (w/o MG & DD): An SL-MGAC variant without the Multi-Group State Decomposition module and the distribution discretization technique in reward learning.\n\u2022 SL-MGAC (w/o MG & DD & SL): An SL-MGAC variant without the Multi-Group State Decomposition module, the distribution discretization technique in reward learning, and the supervised learning procedure.\n\u2022 SL-MGAC (w/o LN): An SL-MGAC variant without the Layer Normalization technique.\n\u2022 SL-MGAC (w/o SG): An SL-MGAC variant without the Stop Gradient technique for hidden state representation in the actor network.\n\u2022 SL-MGAC (w/o Q-norm): An SL-MGAC variant without Q value normalization in the actor loss.\n\u2022 SL-MGAC-0: An SL-MGAC variant with the discount factor \u03b3 = 0.\n\u2022 SL-MGAC-sep: An SL-MGAC variant with separate optimization of the actor network. Hence, Qnorm in Eq. 11 is detached from the computation graph.\n\u2022 SL-MGAC-vanilla: An SL-MGAC variant with the vanilla $Q_{label}$ in the critic loss, as shown below, where $p(s_{t+1}, \u00b7)$ is the action probability of $s_{t+1}$ output by the actor network.\n$Q_{label}(s_{t+1}) = r(s_t, a_t) + \u03b3 \\sum_{A_{t+1}} p(s_{t+1}, A_{t+1})Q(s_{t+1}, A_{t+1})$ (15)"}, {"title": "4.3 Implementation Details", "content": "To ensure fairness among all compared methods, we use a consistent feature extraction module with an embedding layer of size 5000. In addition to the embedding layer, the feature extraction module includes a 2-layer MLP with hidden sizes of [256, 128]. The batch size and number of epochs for all methods are 2048 and 500, respectively. The learning rate for the embedding layer is set to 1e-5, while the learning rate for other hidden parameters is 1e-3. The discount factor \u03b3 is set to 0.9 for all methods."}, {"title": "4.4 Offline Policy Evaluation", "content": "We follow the approach in [44] and adopt a commonly used offline policy evaluation method, namely Normalized Capped Importance Sampling (NCIS) [37], to evaluate performance. The evaluation metric is the cumulative reward across all trajectories of test users. Note that the reward for each sample is defined in Eq. 3."}, {"title": "4.5 Ablation Study", "content": "We compare the offline performance of different SL-MGAC variants. As shown in Table 2, SL-MGAC outperforms all other variants on our offline dataset, demonstrating the effectiveness of the proposed multi-group state decomposition module, supervised learning procedure, and other techniques. Moreover, we find that SL-MGAC (w/o Q-norm) achieves the lowest reward, indicating that the Q-normalization technique in the actor loss enhances the model's convergence and improves its performance.\nNext, we compare the training processes of SL-MGAC and SL-MGAC (w/o MG), as shown in Fig. 5. We observe that the Q-value curve of SL-MGAC is more stable than that of SL-MGAC (w/o MG), and the Q-value variance of SL-MGAC is much smaller. This demonstrates the effectiveness and variance reduction effect of the MG-SD module."}, {"title": "4.6 Parameter Sensitivity", "content": "We analyze the impact of the number of user groups K on the performance of SL-MGAC. As shown in Fig. 6, the proposed SL-MGAC algorithm with K = 6 achieves the highest cumulative reward, demonstrating that increasing the number of user groups within a certain range improves model performance."}, {"title": "4.7 Online A/B Test Experiment", "content": "We compare the improvements of different methods in terms of live-stream daily active users (DAU) and watch time relative to the baseline. The baseline uses the SAC framework for online live-stream injection. The online A/B test results are shown in Table 3. SL-MGAC achieves the largest improvement in live-stream watch time while also increasing live-stream DAU. Although SL-MGAC-0 shows a larger improvement in live-stream DAU compared to SL-MGAC, it tends to be more greedy in injecting live-streams, which may negatively impact the long-term user experience for most users. Therefore, this demonstrates that SL-MGAC is more effective in maximizing long-term rewards when injecting live-streams in response to user requests."}, {"title": "4.8 Online Model Stability", "content": "We evaluate the online model stability between the SAC-based baseline and SL-MGAC. As shown in Fig. 7, the live-stream injection ratio fluctuates drastically throughout the day for both models. However, SL-MGAC exhibits greater stability than the baseline, with a smaller amplitude. Note that the live-stream injection ratio refers to the proportion of requests with action a = 1."}, {"title": "5 Related Work", "content": "5.1 RL in Recommendation Systems\nReinforcement learning (RL) aims to optimize long-term cumulative rewards over time, which has attracted significant attention in recommendation systems research in recent years [1]. Methods such as SLATEQ [18], GeMS [8], and HAC [27] use RL to recommend entire item lists, where the number of candidate items can be large. BatchRL-MTF [47], RLUR [4], and UNEX-RL [46] leverage RL to model the multi-rank score aggregation process, optimizing the weights for score aggregation. The work in [29] explores the use of off-policy RL for multi-session personalized search ranking. Cross-DQN [25] introduces a RL-based approach for ad allocation in a feed, aiming to maximize revenue while improving user experience. Additionally, traditional RL methods such as DQN [30, 31], Double DQN [40], SAC [15], DDPG [26], and TD3 [12] serve as backbones in real-world RL applications for recommendation systems.\nA similar approach, called self-supervised actor-critic [43], combines supervised learning with critic learning. Their supervised learning task focuses on predicting the next item recommendation probability, whereas our approach introduces supervised learning to restrict critic learning."}, {"title": "5.2 RL divergence and instability", "content": "Recently, there has been growing literature focusing on RL divergence and instability. [40] introduces Double DQN to mitigate the maximization bias problem in DQN. [12] proposes the Clipped Double Q-Learning technique in TD3 to reduce the overestimation of Q-values. [28] introduces a bias-free, input-dependent baseline for the policy gradient algorithm [36] to reduce variance and improve training stability. [9] investigates policy collapse in a 2-state MDP and finds that L2 regularization and the non-stationary Adam optimizer [21] are both effective in alleviating RL instability. [45] theoretically analyzes the causes of RL divergence and applies Layer Normalization to mitigate RL divergence and instability."}, {"title": "6 Conclusion", "content": "In this work, we propose a novel Supervised Learning-enhanced Multi-Group Actor-Critic algorithm (SL-MGAC) to optimize request-level live-stream injection policies in the challenging context of a live-stream & short video mixed recommendation system. Unlike existing RL methods for recommendation systems, our approach focuses on enhancing the stability and robustness of the RL model. Specifically, we introduce a multi-group state decomposition module to reduce prediction variance and seamlessly integrate multi-task reward learning with traditional critic learning to constrain Q-value estimation. In practice, we aim to minimize the risk of policy deterioration or model collapse, thereby enabling the proposed SL-MGAC method to be successfully deployed in large-scale industrial recommendation systems."}, {"title": "7 Acknowledgment", "content": "We acknowledge Bai Wei and Chen Xiaoshuang for proposing detailed modification advice to help us improve the quality of this manuscript."}, {"title": "A Proof of Theorem 3.1", "content": "PROOF. Let state space $S = \\cup_{i=1"}, "K S_i$, $\u2200i \u2260 j S_i \u2229 S_j = \u00d8$, state $s \u2208 S$. Let r be the real reward of state s and action a, $\\hat{r}$ be the predicted value of r over S, $\\hat{r_i}$ be the predicted reward of r over $S_i$.\nSince a user has at most one request at timestamp t, a user will have at most one state s. We assume the state count N of S is finite, the state count of $S_i$ is $N_i$. Let \u00b5 be the mean value of $f$ over S, $\u00b5_i$ be the mean value of $f_i$ over $S_i$, then we have:\n$Var[r"]}