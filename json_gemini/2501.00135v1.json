{"title": "GroverGPT: A Large Language Model with 8 Billion Parameters for Quantum Searching", "authors": ["Haoran Wang", "Pingzhi Li", "Min Chen", "Jinglei Cheng", "Junyu Liu", "Tianlong Chen"], "abstract": "Quantum computing is an exciting non-Von Neumann paradigm, offering provable speedups over classical computing for specific problems. However, the practical limits of classical simulatability for quantum circuits remain unclear, especially with current noisy quantum devices. In this work, we explore the potential of leveraging Large Language Models (LLMs) to simulate the output of a quantum Turing machine using Grover's quantum circuits, known to provide quadratic speedups over classical counterparts. To this end, we developed GroverGPT, a specialized model based on LLaMA's 8-billion-parameter architecture, trained on over 15 trillion tokens. Unlike brute-force state-vector simulations, which demand substantial computational resources, GroverGPT employs pattern recognition to approximate quantum search algorithms without explicitly representing quantum states. Analyzing 97K quantum search instances, GroverGPT consistently outperformed OpenAI's GPT-40 (45% accuracy), achieving nearly 100% accuracy on 6- and 10-qubit datasets when trained on 4-qubit or larger datasets. It also demonstrated strong generalization, surpassing 95% accuracy for systems with over 20 qubits when trained on 3- to 6-qubit data. Analysis indicates GroverGPT captures quantum features of Grover's search rather than classical patterns, supported by novel prompting strategies to enhance performance. Although accuracy declines with increasing system size, these findings offer insights into the practical boundaries of classical simulatability. This work suggests task-specific LLMs can surpass general-purpose models like GPT-40 in quantum algorithm learning and serve as powerful tools for advancing quantum research.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum computing harnesses fundamental quantum mechanical phenomena, such as superposition and entanglement, to solve certain computational problems exponentially faster than classical computers [1, 2]. For instance, Grover's algorithm [2], a quantum algorithm for database searching, is proven to achieve a quadratic speedup over all known classical counterparts. However, the boundaries of quantum advantage\u2014where quantum algorithms outperform all classical counterparts for a specific task\u2014remain unclear. In practice, a closely related question concerns classical simulability: if a quantum circuit can be efficiently simulated on classical computers, it is unlikely to demonstrate a significant advantage [3].\nBased on the construction of a quantum Turing machine, brute-force classical simulation-commonly referred to as state-vector simulation\u2014incurs exponentially high memory costs for general quantum computing tasks. Smarter, approximate approaches, such as tensor networks, may perform better in practice [4], although they can still face exponentially high costs or exponentially large errors as the number of qubits scales for general tasks. The situation becomes even more complex with NISQ (Noisy Intermediate-Scale Quantum) devices [5]; it remains unclear whether existing noisy, near-term commercial quantum computers can be classically simulated for commercially valuable problems [6].\nThus, the practical frontier of classical simulability can only be approached with large-scale high-performance computing. For example, several claims of demonstrating quantum advantages in sampling algorithms [7] have been challenged by tensor network methods implemented on classical devices [8, 9]. On the other hand, novel approaches utilizing Large Language Models (LLM) [10-12], one of the most powerful large-scale AI tools, present new possibilities for simulating quantum circuits [13-16]. Although notable progress has been achieved, these early efforts have primarily concentrated on prompt engineering using commercial models from OpenAI and similar providers, building open-source datasets, or developing toy models with only a few thousand parameters-significantly smaller than modern industrial-level models, which typically have at least a few billion parameters.\nIn this work, we present an initial investigation into the clas-"}, {"title": "II. GROVERGPT", "content": null}, {"title": "A. Pre-Training Strategy Selection for GroverGPT", "content": "As illustrated in Figure 2, we develop GroverGPT through a structured pre-training pipeline built upon the Llama-3.1-8B [12] foundation model. Our training dataset encompasses quantum systems ranging from 3 to 20 qubits, carefully separating training (3 ~ 10 qubits) and testing (6 ~ 20 qubits) sets. The pre-training process integrates three key components: quantum circuit simulations from Grover's algorithm implementations, corresponding QASM code generated via Qiskit [18], and natural language conversations about quantum search problems. This multi-modal approach enables GroverGPT to comprehensively understand the structural and behavioral aspects of quantum search operations. Data augmentation through QASM representations and natural language interactions further enhances the model's ability to bridge the gap between abstract quantum algorithms and practical implementations.\nThis refined version maintains the essential information while being more concise and focused. It highlights the key components of our pre-training strategy while eliminating redundant details, making it more accessible to readers while preserving technical accuracy."}, {"title": "B. GroverGPT's Effectiveness of Simulating Quantum Search", "content": "This section demonstrates GroverGPT's superior performance in quantum search simulation compared to baseline"}, {"title": "C. Scalability and Generalization Capability of GroverGPT", "content": "In this section, we examine GroverGPT's remarkable ability to generalize quantum search capabilities from training on small-scale systems to significantly larger quantum systems. GroverGPT exhibits remarkable generalization capabilities across different qubit scales. Figure 3e comprehensively analyzes the model's performance when trained on 3 ~ 6 qubits and tested on increasingly larger systems, ranging from 6 to 20 qubits. The results show that GroverGPT maintains accuracy above 95% for systems up to 8 qubits, with only a gradual decline to approximately 95% for systems between 9 and 20 qubits. This robust generalization ability is particularly noteworthy given that the model was trained only on smaller systems (3~6 qubits), suggesting its capacity to extrapolate quantum search principles to substantially larger quantum systems. The parallel trends between accuracy and Fidelity in Figure 3e suggest that the model's performance degradation at larger qubit counts is gradual and predictable, indicating a systematic rather than catastrophic breakdown of simulation capability. This behavior aligns with theoretical expectations about the scalability challenges in quantum simulation and provides valuable insights into the practical limits of classical models in capturing quantum phenomena."}, {"title": "D. Quantum v.s. Classical Search Learned by GroverGPT", "content": "Through detailed fidelity analysis in this section, we investigate whether GroverGPT truly learns quantum search rather than simply approximating classical search strategies. The Fidelity analysis in Figure 3d provides strong evidence that GroverGPT learns genuine quantum search rather than classical approximations. The consistently low Fidelity values (below 0.005 for 6 qubits and approaching zero for systems trained on 5 or more qubits) indicate that GroverGPT accurately captures the quantum state amplitudes characteristic of Grover's algorithm. This is in stark contrast to GPT-40 [19]'s performance, which shows a consistently high Fidelity (approximately 0.011), suggesting it fails to capture the subtle quantum features of the search process. The convergence of Fidelity to near-zero values with increased training qubit count is particularly significant, as it indicates that GroverGPT successfully learns the interference patterns and amplitude amplification mechanisms that are quintessential to quantum search. This distinction is further reinforced by the symmetric behavior observed in both 6-qubit and 10-qubit test cases, suggesting that the model has internalized fundamental quantum principles rather than memorizing specific problem instances."}, {"title": "E. Impact of Prompt Design Strategies in GroverGPT", "content": "In this section, we evaluate how different prompting strategies, particularly the integration of QASM code and conversational components, influence GroverGPT's performance in quantum search simulation. The influence of different prompting strategies [20, 21] is clearly illustrated in Figure 3c, where we compare the base GroverGPT model with variants incorporating QASM and conversation components. The addition"}, {"title": "III. METHODOLOGY", "content": null}, {"title": "A. Grover's Algorithm", "content": "As demonstrated in [2, 22, 23], Grover's Algorithm [2] represents a fundamental breakthrough in quantum computing, demonstrating significant computational advantages over classical methods. For the ubiquitous task of searching through an unstructured database of N items, classical computers require examining items one by one, taking O(N) operations on average. In contrast, Grover's Algorithm achieves a quadratic speedup, requiring only O($\\sqrt{N}$) operations by leveraging principles of quantum mechanics.\nAs indicated by Figure3 (upper left), the algorithm creates a quantum state that simultaneously represents all possible database items through superposition\u2014a quantum property where a qubit can exist in multiple states simultaneously. It then iteratively applies two operations: one that marks the desired solution and another that amplifies the probability of finding this marked state. After ~ $\\sqrt{N}$ iterations, measuring the system yields the target item with high probability."}, {"title": "B. Classical Simulation of GroverGPT", "content": "We adopt a noise-free classical simulation of Grover's quantum search algorithm using Qiskit [18], which is an open-source software development kit for quantum computing developed by IBM, and its state vector simulator to establish ground truth data for training and evaluation. This simulator tracks the complete mathematical description of a quantum system: for an n-qubit system where a qubit is the quantum equivalent of a classical bit, the simulator maintains a state vector of dimension N = $2^n$. It enables the exact computation of quantum state amplitudes, which are the complex numbers that describe the probability of measuring each possible quantum state, along with their measurement probabilities.\nAs shown in Figure2, the simulation executes the standard Grover circuit template, which is a sequence of quantum operations, with O($\\sqrt{2^n}$) iterations. This process comprises three essential components: initialization through Hadamard gates, which are quantum operations that create equal superpositions; oracle operations for target state marking, which are operations that identify the solution we're searching for; and diffusion operators for amplitude amplification, which are operations that enhance the probability of finding the marked state. Through multiple simulation shots, which are repeated runs of the quantum circuit, we obtain relatively precise probability distributions over computational basis states, which represents the possible measurement outcomes. This provides reliable reference data for assessing GroverGPT's quantum search simulation capabilities."}, {"title": "C. Evaluation Metrics", "content": "To rigorously assess GroverGPT\u2019s quantum search simulation capabilities, we establish three complementary evaluation metrics. Given a quantum system with n qubits and k marked states, let $|\\psi_{final}\\rangle$ represent the final quantum state after applying Grover's algorithm. The measurement outcome probability distribution is defined as:\nP = {(s_i, p_i) | i \\in [2^n], p_i = |\\langle s_i|\\psi_{final}\\rangle|^2}  (1)\nwhere $s_i$ represents the i-th computational basis state and $p_i$ its corresponding measurement probability. Let $P_{model}$ and $P_{true}$ denote the probability distributions generated by GroverGPT and the ideal quantum simulator, respectively. We evaluate performance using:\n\u2022 Search Accuracy ($\\alpha$): Measures the model's ability to identify marked states correctly, defined as:\n$\\alpha = \\frac{|M_{model} \\cap M_{true}|}{k}$ (2)\nwhere $M_{model}$ and $M_{true}$ are the sets of k highest-probability states in $P_{model}$ and $P_{true}$, respectively.\n\u2022 Fidelity: Quantifies the overall quantum state reproduction accuracy through:\n$\\epsilon = \\frac{1}{2^n} \\sum_{i=1}^{2^n} (p^{model}_{i} - p^{true}_{i})^2$ (3)\nwhere $p^{model}_{i}$ and $p^{true}_{i}$ are probabilities from $P_{model}$ and $P_{true}$.\n\u2022 Marked Fidelity ($e_k$): Specifically evaluates the accuracy of marked state probability predictions:\n$e_k = \\frac{1}{k} \\sum_{s_i \\in M_{true}} (p^{model}_{s_i} - p^{true}_{s_i})^2$ (4)\nThese metrics provide complementary perspectives on GroverGPT's performance: $\\alpha$ assesses the model's ability to"}, {"title": "IV. CONCLUSION AND OUTLOOK", "content": "In this work, we demonstrate that an industrial-level, medium-scale LLM can be designed to simulate noiseless quantum algorithms, such as Grover's search, and potentially outperform popular general-purpose models like OpenAI's GPT-4. This is achieved through the development of GroverGPT. Additionally, we present evidence suggesting that the model can learn and generalize certain features of quantum algorithms, though its performance significantly deteriorates as the number of qubits increases. Our work offers a valuable tool for advancing research and education in quantum algorithms.\nOur work opens a new avenue for exploring the boundaries of classical simulability and quantum advantage using LLMs. This raises numerous questions for future research. For example, how well can LLMs simulate noisy quantum computers? Could they effectively model current noisy quantum systems with 100 to 1,000 qubits, and what level of error would be tolerable? Can other quantum algorithms be simulated, or might it even be possible to develop a foundational model capable of simulating quantum Turing machines? Can we simulate quantum error correction codes? Furthermore, if resources from leading LLM developers such as OpenAI, Anthropic, or xAI were available for training models specifically tailored to quantum algorithms, how many qubits and what circuit depth could we feasibly simulate? These intriguing questions remain open for future investigation."}, {"title": "APPENDIX", "content": null}, {"title": "A. Preliminaries for Quantum Computing", "content": "Quantum Turing Machine. A quantum Turing machine is a common computational model we use for universal quantum computing [3]. Here is a simplified and informal definition of a quantum Turing machine. For an n-qubit quantum Turing machine, we define a linear space H, where we call it the Hilbert space (a complex linear space equipped with an inner product), such that dim H = $2^n$, whose basis vector is the bit string denoted as $|x_0, x_1,\u00b7\u00b7\u00b7, x_{n-1}\\rangle$ where $x_i \\in \\{0,1\\}$ for $0 \\le i \\le n - 1$. Thus, the input of the machine is a description of a sequence U = $\\{U_1, U_2,\u2026\u2026,U_L\\}$ of unitary matrices on H. We call U the quantum circuit, and we call $U_a$ for $1 \\le a \\le L$ quantum gates. The output of the model is a probability distribution on the bit string $|x_0, x_1,..., x_{n-1}\\rangle$ where $x_i \\in \\{0,1\\}$ for $0 \\le i \\le n - 1$.\nThe probability distribution $p_i$ is determined by the Born rule, $P_i = |\\langle x_i|U_1U_2\u2026\u2026U_L |00, \u2026\u2026\u2026, 0\\rangle|^2$. In practice, what we receive from quantum computers are samples of the bit string exactly following the distribution pr. Due to the central limit theorem, when the number of samples is large, it is not hard to"}, {"title": "Quantum Computing and Its Key Principles", "content": "Quantum\ncomputing is operated on quantum computers or quantum devices. It leverages quantum mechanics to conduct different computing tasks. In some specific applications including encryption [1], quantum simulation [24] and quantum machine learning [25-28] etc., with a large-scale quantum computer, quantum computing can theoretically process exponentially faster speed than current classical computing.\nOne of the differences between quantum computing and classical computing lies on the basic computing unit. The classical Bit, which is the fundamental concept of classical computing, is either at state 0 or 1, while the Quantum Bit, or qubit, which is the fundamental concept of quantum computing, could stay in |0\u27e9 or |1\u27e9, or \u201cbetween\u201d these two computational basis states. It is termed the superposition:\n$|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle$, (5)\nwhere $\\alpha$ and $\\beta$ are the corresponding amplitudes for each basis state.\nBasically, quantum computing will result in the change of a single qubit or multiple qubits. This process can also be described as quantum information processing. After measurement, each qubit can only output a single bit of information. Measurement of a qubit means changing the state of every single qubit by collapsing it from its superposition of |0\u27e9 and |1\u27e9 to either |0\u27e9 or |1\u27e9 state depending on the probabilities. Measurement is one way that causes decoherence, which refers to the process which a quantum state collapses into a non-quantum state. Besides, quantum systems follow a key principle termed entanglement, which refers to the phenomenon that a qubit possess the ability to correlate its state with other qubits. Meanwhile, superposition and entanglement offer the condition for interference, which refers to the phenomenon that entangled qubits, each with multiple states, can interfere with each other, leading to amplifying or discouraging the probabilities, denoted as constructive interference and destructive interference respectively."}, {"title": "Quantum Gates and Quantum Circuits", "content": "Quantum computing relies on operations in quantum circuits, which contain reversibly elementary quantum logic gates, or so-called quantum gates. Quantum gates can be also represented as unitary operators. A unitary operator or matrix U on a Hilbert Space H indicates the following fact:\n$U^{\\dagger}U = I$, (6)\nwhere I is the identity matrix and $U^{\\dagger}$ is the adjoint or complex conjugate of the matrix U.\nWe leverage several frequently adopted quantum gates to construct the quantum circuit for implementing the Grover's"}, {"title": "quantum search algorithm", "content": "including the Hadamard gate or so-called H gate which turns a |0\u27e9 into (|0\u27e9 + |1\u27e9)/$\\sqrt{2}$ and turns |1\u27e9 into (|0\u27e9 \u2013 |1\u27e9)/$\\sqrt{2}$, single-qubit quantum NOT gate or so-called X gate, Z gate which leaves |0\u27e9 and flips the sign of |1\u27e9 or -1):\nH = $\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & 1\\\\1 & -1\\end{bmatrix}$ , X = $\\begin{bmatrix}0 & 1\\\\1 & 0\\end{bmatrix}$ , Z = $\\begin{bmatrix}1 & 0\\\\0 & -1\\end{bmatrix}$ (7)\nQuantum circuits are models for quantum computing and quantum algorithms. Basic components include quantum gates, measurements, and possibly some other actions. Fig.4 gives an example of what a quantum circuit might look like, specifically by showing the plotted circuit of Grover's searching algorithm using Qiskit."}, {"title": "Open Quantum Assembly Language (OpenQASM)", "content": "In this study, we leverage the OpenQASM [29] programming language to describe the quantum circuits and Grover's searching algorithm. OpenQASM is designed to serve as an intermediate representation to allow communication between the high-level compilers and the quantum hardware. It is implemented using python. By default, the version of the OpenQASM we adopt is Version 3.0. Below we provide an example when defining a quantum circuit that implements Grover's quantum searching algorithm under 3 qubits:"}, {"title": "B. Preliminaries for Grover's Algorithm", "content": "Grover's algorithm, introduced by Lov Grover [2], is a quantum algorithm designed to search for a specific item within an unsorted database of N items. In contrast to classical algorithms, which require an average of O(N) operations to find the target item, Grover's algorithm achieves this task in O($\\sqrt{N}$) steps, offering a quadratic speedup. This makes it a powerful tool for various applications, including database search, optimization problems [30], and cryptographic analysis [31].\nThe algorithm consists of several key components: the initial state preparation, the oracle, the diffusion operator, and the measurement. Each of these components plays a crucial role in the algorithm's ability to amplify the amplitude of the target state, which increases the probability of finding it.\nInitial State Preparation. The algorithm begins by preparing an initial state that is a uniform superposition of all possible states in the N-dimensional Hilbert space. If the N items are indexed by n-qubit states, the initial state is:\n$|\\psi_0\\rangle = \\frac{1}{\\sqrt{N}} \\sum_{x=0}^{N-1} |x\\rangle$\nThis state is prepared by applying Hadamard gates (H) to all n qubits initialized in the |0\u27e9 state:\n$H^{\\otimes n}|0\\rangle = \\frac{1}{\\sqrt{2^n}} \\sum_{x=0}^{2^n - 1} |x\\rangle$\nThe Oracle. The oracle is a quantum subroutine that marks the target state(s) by flipping their phase. For a single marked state $|x_t\\rangle$, the oracle operator O is defined as:\nO = I - 2|$x_t\\rangle\\langle x_t|$\nThis operator applies a phase flip to the target state:"}, {"title": "The optimal number of iterations k to maximize the probability of measuring a marked state is approximately", "content": "The implementation of the oracle depends on the specific problem being solved. In general, it involves encoding the target state $x_t$ using a unitary circuit that compares the input state to $x_t$ and applies a phase shift (e.g., a Z-gate) conditioned on the comparison result. For example, if $x_t$ is known, the oracle can be implemented using a controlled-Z gate, where the control qubits are those that specify $x_t$.\nThe Diffusion Operator. The diffusion operator, also known as the Grover operator, is responsible for amplifying the amplitude of the target state. It is defined as:\nD = 2|$\u03c8_0\\rangle\\langle \u03c8_0| - I\nwhere $|\u03c8_0\\rangle$ is the initial uniform superposition state and I is the identity operator. The diffusion operator can be implemented using a sequence of Hadamard gates, a multi-qubit Z-gate, and another sequence of Hadamard gates. Specifically, the diffusion operator can be written as:\nD = $H^{\\otimes n} (2|0\\rangle \\langle 0| \u2013 I)H^{\\otimes n}$\nThis operator effectively inverts the amplitudes of the quantum state to the average amplitude. The average amplitude (\u03b1) before diffusion is:\n$\\langle \\alpha \\rangle = \\frac{N - 2}{N\\sqrt{N}}$\nThe diffusion operator transforms each amplitude $\u03b1_x$ to:\n$\\beta_x = 2\\langle \\alpha \\rangle - \\alpha_x$\nFor the target state $|x_t\\rangle$:\n$\\beta_{x_t} = 2\\langle \\alpha \\rangle - 1 = \\frac{3N - 4}{N\\sqrt{N}}$\nFor other states |x\u27e9 (where x \u2260 xt):\n$\\beta_x = 2\\langle \\alpha \\rangle - \\frac{1}{\\sqrt{N}} = \\frac{N - 4}{N\\sqrt{N}}$\nRepeating the oracle and diffusion steps iteratively increases the amplitude of the target state. In the Hilbert space, the diffusion operator reflects the state vector about the average amplitude vector, which is constructive interference for the target state.\nComplexity of Grover's Algorithm. A single Grover iteration consists of applying the oracle O followed by the diffusion operator D. If the initial state is $|\u03c8_0\\rangle$, the state after k iterations is:"}, {"title": null, "content": "$k \\approx \\frac{\\pi}{4} \\frac{\\sqrt{N}}{\\sqrt{M}}$\nwhere M is the number of marked states. This formula generalizes the scenario for multiple marked states, reducing to $k \\approx \\frac{\\pi}{4} \\sqrt{N}$ when M = 1.\nThe amplitudes of the marked and unmarked states evolve with each iteration. Specifically, the amplitude of the marked states is given by:\n$A_k^{(M)} = sin((2k + 1)\u03b8)$,\nwhere \u03b8 = arcsin($\\frac{1}{\\sqrt{N}}$).\nThis evolution indicates that each iteration amplifies the amplitude of the marked states. The optimal k is chosen such that (2k + 1)\u03b8 ~ $\\frac{\\pi}{2}$, ensuring the probability of measuring a marked state is maximized. Grover's algorithm thus achieves its quadratic speedup by iteratively increasing the amplitude of the marked states with the power of quantum interference."}, {"title": "C. Preliminaries for Large Language Models", "content": "Language Models and Their Development. Language models (LMs) serves as a key method for enhancing machine language understanding, At its core, LMs maximize the probabilistic likelihood structure of word sequences, allowing for predictions of upcoming or missing words. This foundational capability supports a wide range of natural language processing (NLP) applications, including tasks like machine translation and conversational systems.\nThe recent success of pre-trained language models (PLMs) has demonstrated that increasing model size, training data volume, or computational resources often enhances their ability to perform downstream tasks. This observation, commonly referred to as the scaling law, has driven the development of large-scale models. Models like GPT and PaLM mark a major advancement, showcasing the ability to solve complex tasks and generalize from limited examples, highlighting the critical role of scaling in enhancing model performance.\nBuilding on the advancements in LLMs, Meta introduced the Llama (Large Language Model Meta AI) series, which features open-source LLMs optimized for performance and accessibility.\nLLMs basic Architecture. The Llama models are built upon the Transformer architecture, which is renowned for their self-attention mechanism and modular design. While standard Transformer models consist of both encoder and decoder stacks, Llama focuses exclusively on the Transformer decoder."}, {"title": null, "content": "Transformer architecture: Each Transformer decoder\nlayer comprises a multi-head attention mechanism to\ncapture dependencies within the sequence being gener-\nated, a feed-forward network (FFN) to enhance model\nexpressivity, and residual connections with normaliza-\ntion to improve training stability.\n\u2022 Self-Attention Mechanism: The self-attention mecha-\nnism allows the model to capture dependencies between\ntokens in the input sequence. It operates by mapping a\nquery (Q), key (K), and value (V) to an output, com-\nputed as:\nAttention(Q, K, V) = softmax($\\frac{QK^T}{\\sqrt{d_k}}$)V,\nwhere $d_k$ is the dimensionality of the keys. The query, key, and value tensors are derived from the input sequence using learned linear transformations.\n\u2022 Feed-Forward Network (FFN): The FFN in the Transformer decoder enhances the model's ability to represent complex patterns through independent non-linear transformations at each sequence position. The FFN is expressed as:\nFFN(x) = ReLU($xW_1 +b_1$)W_2 + $b_2$),\nwhere x is the input, W\u2081 and W2 are weight matrices, and $b_1$ and $b_2$ are biases.\n\u2022 Layer Normalization (LayerNorm): LayerNorm is applied within each decoder layer to stabilize and accelerate training by normalizing the input to each sub-layer. For an input x, the normalized output is computed as:\nLayerNorm(x) = $\\frac{x - \u03bc}{\u03c3} \u00b7 \u03b3 + \u03b2$ ,\nwhere u is the mean, \u03c3 is the standard deviation, \u03b3, \u03b2 are learnable scaling and shifting parameters. This component ensures that the input to each sub-layer remains well-scaled, which helps mitigate exploding or vanishing gradients in deep networks."}, {"title": "Key Improvements of Llama Models", "content": "The Llama models introduce several enhancements to the Transformer decoder to optimize both computational efficiency and expressivity for text generation tasks:\n\u2022 Grouped Query Attention (GQA): To improve the efficiency of the self-attention mechanism, Llama employs GQA, which groups multiple query heads to share the same key-value projections. In GQA, the attention computation is modified as:\nAttention($Q_g, K_g, V_g$) = softmax($\\frac{Q_gK_g^T}{\\sqrt{d_k}}$)$V_g$ ,\nwhere $Q_g$, $K_g$, and $V_g$ are grouped projections. GQA allows for fewer key-value caches during inference and"}, {"title": null, "content": "speeds up the decoding process.\n\u2022 Root Mean Square Normalization (RMSNorm): Llama employs RMSNorm instead of applying layer normalization to the output. RMSNorm computes the normalized vector as:\nRMSNorm(x) = $\\frac{x}{RMS(x)}$ , \nwhere\nRMS(x) = $\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2}$ , x \u2208 $R^d$\nx is the input vector, d is its dimensionality, and $\u03b3 \u2208 R^d$ is a learnable scaling parameter. Unlike LayerNorm, RMSNorm does not include a bias term, which reduces computational overhead.\n\u2022 SwiGLU Activation Function: Llama replaces the conventional ReLU activation function with SwiGLU to achieve a balance between computational efficiency and expressivity, which is defined as:\nSwiGLU(x) = GELU(x$W_1$ + $b_1$ )(x$W_2$+$b_2$)\nwhere $W_1, W_2 \u2208 R^{d_{in}\u00d7d_{out}}$ are weight matrices, $b_1$, $b_2 \u2208 R^{d_{out}}$ are biases, and \u2299 denotes element-wise multiplication. The Gaussian Error Linear Unit (GELU) is computed as:\nGELU(z) = z\u00b7 \u03a6(z), \u03a6(z) = $\\frac{1}{2}[1 + erf(\\frac{z}{\\sqrt{2}})]$"}, {"title": "D. Llama Pre-Training Details for Initializing GroverGPT", "content": "Pre-Training Datasets. The pre-training dataset for Llama 3 is curated from diverse sources containing knowledge up to 2023, with strict removal of PII (Personally Identifiable Information) and adult content. Web data is cleaned using custom parsers, retaining structure for math and code, and applying URL, document, and line-level de-duplication. Heuristic filters and model-based classifiers (e.g., DistilRoberta) ensure high-quality tokens by removing low-quality and repetitive content. Specialized pipelines extract math, reasoning, and code data with prompt-tuned models for STEM-specific tasks. Multilingual data is processed with FastText for language classification (176 languages) and quality-ranked using a multilingual Llama 2-based classifier. The final data mix includes 50% general knowledge, 25% math and reasoning, 17% code, and 8% multilingual data. Annealing on 40M tokens improves performance, with a 24.0% gain on GSM8k and 6.4% on MATH for the 8B model. The total token counts used in pre-training is around 15T+. Scaling law experiments guide the optimal data mix for high downstream task performance.\nPre-Training Process. The pre-training recipe for Llama is carefully designed to ensure model stability and maximize"}, {"title": null, "content": "Initial pre-training. The initial phase of training uses\nthe AdamW optimizer with a peak learning rate of\n8 \u00d7 $10^{-5}$, following an 8, 000-step linear warm-up and\ncosine decay over 1, 200, 000 steps. Batch size and se-\nquence length start at 4M tokens and 4, 096 tokens, dou-\nbling to 8M and 8, 192 tokens after 252M tokens and\nagain to 16M after 2.87T tokens. The training data mix\nis dynamically adjusted by increasing non-English data,\nupsampling mathematical datasets, adding recent web\ndata, and downsampling low-quality subsets to enhance\nmultilingual and task-specific performance.\n\u2022 Long-context pre-training. To enable Llama to process\nlong contexts of up to 128K tokens, the context length is\ngradually increased from 8K to 128K in six stages, using\napproximately 800B tokens. Successful adaptation is\nassessed by recovering short-context performance and\nsolving \"needle in a haystack\" tasks for long sequences.\n\u2022 Annealing. The final stage of pre-training anneals the\nlearning rate to 0 over the last 40M tokens, upsampling\nhigh-quality data sources and applying Polyak averaging\nto produce the final model."}, {"title": "E. GroverGPT Fine-Tuning Details", "content": "Loss Function. During the supervised fine-tuning (SFT) phase, the LLM is optimized using a standard cross-entropy loss to align its predictions with target outputs. The loss function is defined as:\n$L_{SFT} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T_i} -log P_\\theta(Y_{i,t}|Y_{i,<t}, X_i)$\nwhere N is the number of training samples, $T_i$ is the length of the target sequence for the i-th sample, $Y_{i,t}$ is the ground truth token, $Y_{i,<t}$ represents the preceding tokens, $x_i$ is the input prompt, and $P_\\theta(Y_{i,t}|Y_{i,<t}, x_i)$ is the model's predicted probability.\nThis formulation ensures that the model learns to predict target tokens accurately based on the provided context and previously predicted tokens.\nGroverGPT Prompt. We use the following prompt to fine-tune the Llama models to simulate Grover's algorithm. The prompt is provided to the Llama-3.1-8B model to generate the desired responses. The simplest version of the prompt does not include QASM instructions. Below is an example of the prompt with QASM and its corresponding question-answer pair:"}, {"title": "Simplified-QASM and Conversational Prompt", "content": "As observed in the prompt with the QASM example, the token length required for representing quantum circuits can be substantial, especially for systems with a large number of qubits. To address this challenge and facilitate training on resource-constrained hardware such as smaller GPUs, we propose reducing the token length through a process termed Simplified-QASM. For example, the sequence \"h q[0]; h q[1]; h q[2]; h q[3];\" can be compactly represented as \"h q[0:4]\", merging repetitive instructions into a concise form.\nTo further enhance the prompt's conversational nature, we append the phrase \"The answer is:\\n\" at the end of the Question section. This refinement aligns the prompt with natural language, guiding the model to generate Grover's algorithm probabilities more effectively."}]}