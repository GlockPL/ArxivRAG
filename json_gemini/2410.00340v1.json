{"title": "Sparse Attention Decomposition Applied to Circuit Tracing", "authors": ["Gabriel Franco", "Mark Crovella"], "abstract": "Many papers have shown that attention heads work in conjunction with each other to perform complex tasks. It's frequently assumed that communication between attention heads is via the addition of specific features to token residuals. In this work we seek to isolate and identify the features used to effect communication and coordination among attention heads in GPT-2 small. Our key leverage on the problem is to show that these features are very often sparsely coded in the singular vectors of attention head matrices. We characterize the dimensionality and occurrence of these signals across the attention heads in GPT-2 small when used for the Indirect Object Identification (IOI) task. The sparse encoding of signals, as provided by attention head singular vectors, allows for efficient separation of signals from the residual background and straightforward identification of communication paths between attention heads. We explore the effectiveness of this approach by tracing portions of the circuits used in the IOI task. Our traces reveal considerable detail not present in previous studies, shedding light on the nature of redundant paths present in GPT-2. And our traces go beyond previous work by identifying features used to communicate between attention heads when performing IOI.", "sections": [{"title": "Introduction", "content": "Recent work has made progress interpreting emergent algorithms used by language models in terms of circuits [21]. Much work in model interpretability views a model as a computational graph [6], and a circuit as a subgraph having a distinct function [2, 26, 14]. In language models, the computation graph is typically realized through communication between model components via the residual stream [4].\nIn this context, an important subtask is tracing a circuit [26, 12, 2] \u2013 identifying causal communication paths between model components that are significant for model function. Focusing on just communication between attention heads, one approach to circuit tracing might be to track information flow. For example, given an attention head computing a score for a particular pair of tokens, one might ask which upstream heads modify those tokens in a way that functionally changes the downstream attention head's output. A straightforward attack on this question would involve direct inspection of residuals and model component inputs and outputs at various places within the model. Unfortunately and not surprisingly, direct inspection reveals that the great majority of upstream attention heads meet this criterion. Each downstream attention head examines a pair of query and key subspaces, and most upstream heads write at least some component into each downstream head's query or key subspace. The problem this causes is that, absent interventions using counterfactual inputs, it is not clear which of those contributions are making changes that are significant in terms of model function.\nMany previous studies have approached this problem by averaging over a large set of inputs, comparing test cases with counterfactual examples, and by using interventions such as patching [29, 8]. This approach has many successes, but it can be hard to isolate specific communicating pairs of attention heads and hard to identify exactly what components of the residual are mediating the communication.\nIn this paper we explore a different strategy. We ask whether leverage can be gained on this problem by looking for low dimensional components of each upstream head's contributions. To do so, we ask: is there a change of basis"}, {"title": "Related Work", "content": "The dominant style of circuit tracing is via patching [29, 8]. That strategy has shown considerable success [26, 2, 11, 12] but is time-consuming, generally requires the creation of a counterfactual dataset to provide task-neutral activation patches, may miss alternative pathways [13, 18], and has been shown to produce indirect downstream effects that can even result in compensatory self-repair [15, 23].\nIn this work, we trace circuits using only a single forward pass, eliminating the need for counterfactuals and avoiding the problems of self-repair after patching. The authors in [5] trace circuits in a single forward pass, and argue that the approach is much faster than patching, and avoids dependence on counterfactual examples and the risk of self-repair. We derive the same benefits, but unlike that paper we leverage spectral decomposition of attention head matrices to identify the signals flowing between heads.\nLike distributed alignment search [7] we adopt the view that placing a neural representation in an alternative basis can reveal interpretable dimensions [25]. However, unlike that work, we do not require a gradient descent process to find the new basis, but rather extract it directly from attention head matrices. Likewise, using sparse autoencoders (SAEs) the authors in [9, 14] construct interpretable dimensions from internal representations; our approach is com- plementary to the use of SAEs and the relationship between the representations we extract and those obtained from SAEs is a valuable direction for further study.\nWe demonstrate circuit tracing for the IOI task in GPT-2 small, which has become a 'model organism' for tracing studies [26, 2, 5]. Like previous studies, one portion of our validation consists of recovering known circuits; however we go beyond recovery of those known circuits in a number of ways, most importantly by identifying signals used for communication between heads.\nThe use of SVD in our study is quite different from its previous application to transformers. SVD of attention matrices has been used to reduce the time and space complexity of the attention mechanism [28, 27] and improve reasoning performance [24], often leveraging a low-rank property of attention matrices. Our work does not rely on attention matrices showing low-rank properties. We use SVD as a tool to decompose the computation of attention; the leverage we obtain comes from the resulting sparsity of the terms in the attention score computation. Likewise, previous work has shown interpretability of the singular vectors of OV matrices and MLP weights \u2013 though not of QK matrices [17]. We show here evidence that it is the representations of tokens in the bases provided by the singular vectors of QK matrices that show intepretability."}, {"title": "Background", "content": "Notation. In this paper, vectors are column vectors and are shown in boldface, eg, x. Note though that GPT-2 uses row vectors for token embeddings; so when treating an individual embedding as a vector it will appear as $x^T$.\nIn the model, token embeddings are d-dimensional, there are h attention heads in each layer, and there are t layers. We define $r = \\frac{d}{h}$, which is the dimension of the spaces used for keys and queries in the attention mechanism. In GPT-2 small, $d = 768, h = 12, t = 12$, and $r = 64$.\nAttention Mechanism. The attention mechanism operates on a set of n tokens in d-dimensional embeddings: $X \\in \\mathbb{R}^{n\\times d}$. Each token $x \\in \\mathbb{R}^d$ is passed through two affine transforms given by $\\bar{x} = xW_K+b_K$, $\\bar{x}=xW_Q+b_Q$, using weight matrices $W_K,W_Q \\in \\mathbb{R}^{d\\times r}$ and offsets $b_K,b_Q \\in \\mathbb{R}^r$. Then the inner product is taken for all pairs of transformed tokens to yield attention scores. More precisely:\n$A' = (XW_Q+1b_Q)(XW_K +1b_K)^T$\n$= XW_QW_K^TX^T + XW_Qb_K^T1^T +1b_QW_K^TX^T +1b_Qb_K^T1^T$\nWe can capture (1) in a single bilinear form by making the following definitions:\n$\\Omega = \\begin{bmatrix} W_QW_K^T & W_Qb_K^T \\\\ b_QW_K^T & b_Qb_K^T \\end{bmatrix}, x = \\begin{bmatrix} X \\\\ 1 \\end{bmatrix}$"}, {"content": "Then we can rewrite the score computation (1) as\n$A'_ij = x_i^T\\Omega x_j$\nin which $x_i$ is the destination token and $x_j$ is the source token of the attention computation. To enforce masked self-attention, $A'_ij$ is set to $-\\infty$ for $i < j$. Attention scores are then normalized, for each destination (corresponding to a row in $A'$), yielding attention weights $A = Softmax(A'/\\sqrt{r})$, in which the Softmax operation is performed for each row of $A'/\\sqrt{r}$. The resulting attention weight $A_{ij}$ is the amount of attention that destination $i$ is placing on source $j$.\nTo compute the output of the attention head each input token is first passed through an affine transformation: $V = XW_V + 1b_V$ with $W_V \\in \\mathbb{R}^{d\\times r}, b_V \\in \\mathbb{R}^r$. Attention weights are then used to combine rows of V to construct the attention head's outputs: $Z = AV$. Finally each combined output is passed through another affine transformation $O = ZW_O + 1b_O$ which transforms it back into the d-dimensional embedding space. Putting it all together we have\n$O = AXW_VW_O + 1b_VW_O + 1b_O$."}, {"title": "Circuit Tracing", "content": "The approach used in this paper to trace circuits starts by decomposing the attention score A' in terms of the SVD of $\\Omega$. The matrix $\\Omega$ has size $(d+1) \\times (d+1)$, but due to its construction it has maximum rank r. We therefore work with the SVD of $\\Omega = USV^T$ in which $U \\in \\mathbb{R}^{(d+1)\\times r}, V \\in \\mathbb{R}^{(d+1)\\times r}$ and $\\Sigma\\in \\mathbb{R}^{r\\times r}$. U and V are orthonormal matrices with $U^TU = I$ and $V^TV = I$, and $\\Sigma = diag(\\sigma_0, \\sigma_1,..., \\sigma_{r-1})$ with $\\sigma_0 \\geq \\sigma_1 > \\ldots > \\sigma_{r-1} \\geq 0$. Important to our work is that the SVD of $\\Omega$ can equivalently be written as\n$\\Omega = \\sum_{k=0}^{r-1} u_k\\sigma_kV_k^T = \\sum_{k=0}^{r-1} D_k$\nin which $\\{u_k\\}$ and $\\{v_k\\}$ are orthonormal sets and each term in the sum is a rank-1 matrix having Frobenius norm $\\sigma_k$. We refer to each term $D_k$ as an orthogonal slice of $\\Omega$, since we have $D_j^T D_k = D_k^T D_j = 0$ whenever $k \\neq j$."}, {"title": "Approach", "content": "The following hypothesis drives our approach:\nHypothesis (Sparse Decomposition) When an attention head performs a task that requires detecting components in a pair of low-dimensional subspaces in its inputs $x_i$ and $x_j$, and its inputs have significant components in those subspaces, it will show large values of $x_i^Tu_k\\sigma_kv_k^Tx_j$ for a distinct subset of values of k.\nIn the remainder of this paper we show a variety of evidence that is consistent with the sparse decomposition hy- pothesis in the case of GPT-2 small. In \u00a76 we will discuss reasons why this phenomenon may arise.\nWhen the sparse decomposition hypothesis holds, we can approximate the score computed by the attention head as follows:\n$A'_{ij} \\approx \\sum_{k \\in S_{ij}} u_k\\sigma_kv_k^Tx_j = \\sum_{k \\in S_{ij}} x_i^TD_kx_j$\nwhere the number of terms in the sum (i.e., $|S_{ij}|$) is small.\nWe use $S_{ij}$ to denote the subset of values of $k$ for the token pair $(i, j)$. Besides being specific to an attention head and token pair, S also depends on the task. In this paper we do not define \u2018task' precisely; in what follows, we study only a limited set of attention head functions that are performed when generating outputs for IOI prompts in GPT-2 small. We leave the association between S and precisely-defined tasks as a fascinating direction for future work.\nOur tracing strategy constructs $A'$ using (6), i.e., using $S_{ij}$ in place of all of the singular vectors of $\\Omega$. This is akin to denoising in signal processing. When a signal has an approximately-sparse representation in a particular orthonormal basis (e.g., the Fourier basis or a wavelet basis) then removing the signal components that correspond to small coefficients is useful to suppress noise. Likewise, we find that in the orthonormal bases provided by U and V, contributions of the inputs $x_i$ and $x_j$ are typically approximately-sparse. Hence removing the dimensions with contributions summing to zero allows us to identify low-dimensional components of the inputs that are responsible for most of the attention head's output (score). We illustrate the benefits of denoising $A'$ in \u00a75.2.\nHence, to fix $S_{ij}$, we consider the individual contributions made by each orthogonal slice to $A'_{ij}$: $\\{x_i^TD_kx_j\\}_{k=0}^{r-1}$. Empirically we typically find a few large, positive terms and many others that may be positive or negative. We seek to separate terms into 'signal' and 'noise.' To do so we adopt a simple heuristic, treating noise terms as a set that, in sum, has little or no effect on the attention head score. Accordingly, we define the noise terms to be the largest set of terms whose sum is less than or equal to zero. The indices of the remaining terms constitute $S_{ij}$. Terms denoted by $S_{ij}$ are strictly positive, and are the largest positive terms. Typically the number of those terms, ie, $|S_{ij}|$, is 20 or less, often just 2 or 3. We refer to this condition where $|S_{ij}|$ is small as the sparse decomposition of attention head scores in terms of the orthogonal slices of $\\Omega$.\nGiven $S_{ij}$, we can decompose model residuals into \u2018signal' and \u2018noise' in terms of their impact on $A'_{ij}$. Define subspaces $U = Span\\{u_k | k \\in S_{ij}\\}$ and $V = Span\\{v_k| k \\in S_{ij}\\}$ and associated projectors $P_U$ and $P_V$. The denoising step separates the inputs $x_i$ and $x_j$ into:\n$\\tilde{x}_i = P_Ux_i, \\tilde{z}_i = P_U^{\\perp}x_i, \\tilde{x}_j = P_Vx_j, \\tilde{z}_j = P_V^{\\perp}x_j$,\nwhere $P_U^{\\perp} = I - P_U$ and $P_V^{\\perp} = I - P_V$. Then we have $x_i = \\tilde{x}_i + \\tilde{z}_i, x_j = \\tilde{x}_j + \\tilde{z}_j$, and\n$\\tilde{x}_i^T\\Omega\\tilde{x}_j \\approx A'_{ij} and \\tilde{z}_i^T\\Omega\\tilde{z}_j \\approx 0$,\nwhere $|S_{ij}|$ is as small as possible. Intuitively, we interpret a signal s to approximately represent a feature that is used for communication between attention heads; we present evidence in support of this interpretation below."}, {"title": "Singular Vector Tracing", "content": "We use this framework to trace circuits in GPT-2 small as follows. A prompt corresponding to the IOI task is input to GPT-2. Consider ia-th attention head at layer l generating attention score $A'_{ij}$ for source token j and destination token i. Then $S'_{ij}$ defines a set of orthogonal slices that the attention head (l, a) is using. Hence we can approximately construct $A'_{ij}$ as in (6), where we are using the SVD of $\\Omega^{l,a}$.\nWe will trace circuits causally with respect to the singular vectors of each attention head. For attention head (l, a) generating output on tokens (i, j), we identify the subspaces U and V. We then look at each attention head 'upstream'"}, {"title": "Experiments", "content": "To demonstrate the utility of singular vector tracing we apply it to a specific setting: the behavior of GPT-2 small when performing indirect object recognition (IOI). The IOI problem was introduced in [26] and that paper identified circuits that GPT-2 uses to perform the IOI task. As described in [26]: In IOI, sentences such as \u201cWhen Mary and John went to the store, John gave a drink to\u201d should be completed with \u201cMary.\u201d To be successful, the model must identify the indirect object (IO, 'Mary') and distinguish it from the subject (S, 'John') in a prompt that mentions both. Thus, a succinct measure of model performance can be obtained by comparing the output logits of the IO and S tokens. The authors identified a collection of attention heads and the token positions they attend to when performing the IOI task. We refer the reader to [26] for additional details.\nThe IOI dataset that we use consists of 256 example prompts with 106 different names. We are using the same 15 templates used in [26], with two patterns (\u2018ABBA' and 'BABA'), which refers to the order of the names appearing in the sentence (the IO name is A, and the S name is B). Prompt sizes range from 14 to 20 tokens.\nAs in [26], we focus on understanding the interaction between attention heads. We believe that extending our framework to include the contributions of MLPs is an important direction for future work. Furthermore, in the analyses in this paper we do not consider cases in which heads attend to the first token \u2013 that is, when attention head has a large value of $A_{ij}$ for $j = 0$. Because the attention weights for each target token form a probability distribution, when a destination token should not be meaningfully modified, attention heads normally put their weight on the first token. This role for token 0 has been noted in previous work [19].\nSingular vector tracing as described in \u00a74.2 can be applied at every attention head with respect to every token pair. However in our experiments we limit our analysis to cases where the attention head is primarily attending to a single source token for a given destination token. This strategy is similar to analyses in prior work [26, 2, 5]; it reduces complexity in the tracing analysis, but could be relaxed in future work. Specifically we say that an attention head is 'firing' if it places more than 50% weight on a particular source token for any given destination. This rule implies that a head can only 'fire' on one source token for each destination token. In general, we only trace upstream from attention heads that are firing on specific token pairs.\nNote that interpreting (8) as giving the actual magnitude of the change in downstream attention score overlooks processing that may affect the signal in between the upstream head and the downstream head. Previous work has shown that downstream processing can compensate for upstream ablations [15] and that some layers may remove features added by previous layers [23]. We show results in \u00a75.4 that confirm the direct causality of signals on downstream attention head outputs, but also illustrate that downstream processing can at times have a noticeable effect on model performance after signal interventions. Further, (8) ignores the impact of the layer norm. Attention head (l, b) adds its output $o^b$ to the residual $x_i$, but $x_i$ will be subjected to the layer norm before being input to attention head (l, a)."}, {"title": "Results", "content": "Throughout this and the next section, we use as our examples attention heads that figured prominently in the results of [26]. This aids interpretation and ensures we are paying attention to important components of the model.\nWe start by demonstrating the approximately-sparse nature of attention scores when decomposed via SVD. Figure 1 shows typical cases. Each plot in the figure shows results for attention head (8,6) and a single source token, destination token, and prompt. The 64 contributions made by each orthogonal slice to the attention score are shown. Red bars correspond to sets $S_{ij}$; the sum of the red bars is approximately equal to the sum of all bars, which is the attention score for this head on these inputs. Figure 1(a) corresponds to the case in which the attention score is negative (and so the attention weight would be nearly zero); Figure 1(b) corresponds to a case in which the attention score is positive (28.1), and the attention weight is large (\u2248 0.83). Note that orthogonal slices are shown in order of decreasing singular value; the effect shown is not due to a low-rank property of $\\Omega$ itself. Rather, the effect shown corresponds to sparse construction of the attention score when the inputs are encoded in the bases given by the SVD of $\\Omega$.\nWe see considerable evidence in our experiments that the orthogonal slices used by an attention head are similar to each other when the attention head is firing. Figure 2 shows examples of the $S_{ij}$ sets for four attention heads: (3, 0), (4, 11), (8, 6), and (9, 9) across 256 prompt inputs. Furthermore, the nature of these attention head's functions are evident in the sets of slices that they use. In the case of (8, 6) (an S-inhibition head), there is a set of about 6 slices that are consistently used; these are the same as the red bars in Figure 1(b). Attention head (9, 9) (a name mover head) uses a larger set, but there is still clearly a specific set of slices that frequently appear. Attention head (3, 0) (a duplicate-token head) uses a broad set of slices; this is consistent with its need to look at all the token's dimensions, since its role is to detect identical tokens. And attention head (4, 11) (a previous token head) primarily uses a single slice; this also is consistent with its role of detecting adjacent tokens, which appears to only require detecting a feature"}, {"title": "Characterizing Attention Head Behavior via SVD", "content": "Next we show that it is possible to leverage the sparsity of attention score decomposition. To illustrate this, we ask the following question: given a token that is input to a particular attention head, what similarity does it show to the outputs of upstream attention heads? In other words, could we trace some part of a circuit by simply looking upstream to see who has 'contributed' to the token?\nWe take as our examples heads (9, 9) and (10, 0), which are name mover heads. The authors in [26] find that functionally important contributors to the 'end' tokens of these heads are (7, 3), (7, 9), (8, 6), and (8, 10). The heatmaps in Figure 4 show contribution scores (computed via (8)) for two cases: the case where the signal is taken to be the entire residual x, and the case where the signal s is taken to be just its low-dimensional component as described in \u00a74.1.\nThe figure shows the strong filtering and correcting effect that results from exploiting the sparsity of attention decomposition. Figures 4(a) and (c) show that if we simply ask how much each upstream head contributes to the attention score of the downstream head, we get a very noisy answer with two problems. First, a large set of attention heads have high scores; and second, the known functional relationships from [26] are not evident. On the other hand, when we focus on Figures 4(b) and (d), we see considerable noise suppression; many heads in middle layers of the model are no longer shown as being significant. Furthermore, we see that one attention head (8, 6) stands out, and it is one of those previously identified as functionally important. And in the case of Figure 4(d), we see that other attention heads with known functional relationship (7, 3) and (7, 9), are also highlighted. In \u00a75.3 we will show many other attention head pairs with functional relationships from [26] that are recovered using the singular vector tracing strategy. In the Appendix we show a hypothetical network trace performed without using singular vectors; the resulting trace is not usable and shows little evidence of known functional relationships in the IOI circuit.\nInterpreting Signals. Detailed investigation of the interpretability of signals is beyond the scope of this paper. However we observe that in some cases signals show interpretability. As an example, we consider the name mover attention head (9, 9). For each tokens in the input to layer 9, we compute the magnitude of its residual in the V"}, {"title": "Singular Vector Trace of GPT-2 on IOI", "content": "Next we construct a singular vector trace using the concepts from \u00a74.2. We start at a particular attention head and token pair. If the head is firing on the token pair (as discussed in \u00a74.3) we obtain the subspaces U and V and associated projectors $P_U$ and $P_V$. We then look at each upstream attention head's output, and separate from it the signal it contains for the downstream head. The properly adjusted magnitude of this signal constitutes the upstream head's contribution to the downstream head's attention score via the corresponding token, as computed via (8).\nEmpirically we find that the contributions from most upstream heads are small, with only a few upstream heads making large contributions to the downstream attention score. We filter out the small contributions, which are unlikely to have significant impact on model performance. To do this for a given downstream firing, we adopt the simple rule of choosing the smallest set of upstream heads whose contributions sum to at least 70% of the sum of all contributions.\nUsing this rule, for each token we identify the upstream heads with significant contribution through that token. An edge in the resulting trace graph is defined by the upstream head, the downstream head, the two tokens on which the downstream head is firing, and the choice of which token is being written into by the upstream head. For each upstream head we then ask whether it is firing on that token as a destination. If so, the process repeats from that head and token pair. The process is presented in detail as Algorithm 1 in the Appendix.\nWe ran singular vector tracing on GPT-2 small using 256 prompts from the IOI dataset described in \u00a74.3. We started the trace at the three name mover heads (9, 6), (9, 9) and (10, 0), as they were identified as having direct effect on model performance in [26]. The resulting trace is shown in Figure 5; we refer to the graph in the figure as G. There are two kinds of edges in G: communication edges from heads to tokens, and attention edges from tokens to heads. In the figure, blue edges are toward tokens that are source tokens downstream, and red edges likewise are destinations. The width of the edge is the accumulated contribution of the edge over the 256 prompts. Darker nodes fired more often in our traces, and we have placed green borders around nodes that appeared in [26]. Edges that appear very few times (less than 65 times over the 256 prompts) are omitted.\nWe make a number of observations. First, the trace shows broad agreement with results in [26], although not all nodes in that paper appear in our graph; this may reflect differences of effect size, or differences between the input datasets. The trace shows agreement with previous results on head-to-head connections and also on the tokens through which the communication is effected. Note that in the appendix we show that further filtering this graph to just its most frequent edges yields a set of attention heads in agreement with [26] to a precision of 0.52 and recall of 0.69.\nThe trace also goes beyond previous results in a number of ways. Whereas in previous work, the upstream con- tributors to name mover source tokens were not identified, this trace shows where important features for the that (IO) token are added and that this happens very early in the model's processing. Previous work also proposed that redun- dant paths were present in GPT-2's processing for the IOI task; this trace confirms their existence and elucidates the nature of their interconnection pattern. For example, there is distinct lattice structure among nodes at layers 7, 8, and 9. (In the Appendix we isolate this lattice structure for better inspection.) We also identify highly active heads that were not discussed in previous work, including (2, 8) which attends to the ditransitive verb of the prompt and feeds"}, {"title": "Validation", "content": "We adopt a variety of strategies to validate the graph in Figure 5. To demonstrate the causal effect of each edge's communication on model performance, we intervene on individual edges; and to demonstrate that the structure of the graph itself is functionally significant, we intervene on various collections of edges simultaneously.\nEdge Validation. A communication edge in G represents the contribution $c^{l_a,l_b}_{ij}$, which is a measure of the amount that head (l, b) would change the attention score $A'_{ij}$ of head (l, a) via $s^{l_a}_{ij}$, if there were no downstream modifications. Validating an edge (l, b) \u2192 (l, a) in G involves intervening in the output of the upstream attention head (l, b) by modifying the signals (as defined in (7)) used by the downstream attention head (l, a). We define two types of interventions: global interventions, and local interventions. In a global intervention, we simply modify the signal in the output of the upstream attention head; in a local intervention, we modify the signal only at the input to the downstream attention head. Global interventions have the potential to directly affect all downstream heads, while local interventions are limited in their direct effect to only the downstream head.\nSpecifically, we define $A$ as the projection of the upstream head's output onto the associated subspace defined by $S^{l_a}_{ij}$. An edge ablation consists of subtracting $A$ from the residual; and edge boosting consists of adding $A$ to the residual. Additionally, for comparison purposes we define $A_{random}$ which consists of constructing the projection of the residual using a set of $(l, a)$'s singular vectors chosen at random from those not in $S^{l_a}_{ij}$; the size of the chosen set"}, {"title": "Discussion", "content": "The results in \u00a7\u00a75.1 and 5.2 support the sparse decomposition hypothesis. In this section we discuss some possible mechanisms behind this phenomenon. First, a motivation for decomposing attention matrices using SVD comes from the following fact:\nLemma 1 Given vectors x and y, among all rank-1 matrices having unit Frobenius norm, the matrix D that maximizes\n$x^TDy$ is $D = \\frac{xy^T}{\\|x\\| \\|y\\|}$\nThe proof is straightforward and provided in the Appendix. This suggests that model training could have the following effect. If an attention head needs to attend to particular vectors x and y, it needs to output a large value for $x^TDy$. In that case, model training could result in construction of $\\Omega$ in which one term of the SVD, say $u_k\\sigma_kv_k^T$, has $u_k \\approx x/\\|x\\|$, $v_k \\approx y/\\|y\\|$, and with $\\sigma_k$ reflecting the importance of this term in the overall computation of the attention score. As an illustration, we note that the results in \u00a75.2 and the Appendix show that in GPT-2 the singular vectors of some $\\Omega$ matrices are correlated with word features that are relevant for the IOI task.\nConsider a hypothetical case in which the sets of vectors to which the attention head needs to attend, say $\\{x_i\\}$ and $\\{y_i\\}$, happen to each form orthogonal sets. Then to achieve maximum discrimination power in distinguishing corresponding pairs, the singular vectors of $\\Omega$, that is $\\{u_k\\}$ and $\\{v_k\\}$ respectively, should be aligned with the corre- sponding vectors in $\\{x_i\\}$ and $\\{y_i\\}$. To move from vectors to features, we refer to the linear representation hypothesis [16, 10, 22] which suggests that concepts, including high-level concepts, are represented linearly in the model.\nTo move closer to realistic cases, we note that the authors in [3] make observations, based on experimental evidence and geometric considerations, about features constructed by neural models. They argue that models will tend to represent correlated feature sets in a manner such that, considered in isolation, the sets are nearly orthogonal. They term this the use of \u201clocal, almost-orthogonal bases.\u201d In our case, if an attention head is attending to vector sets $\\{x_i\\}$ and $\\{y_i\\}$ that are important when performing a specific task, then we may hypothesize that training will construct the sets to be nearly-orthogonal,\u201d meaning that cosine similarities among the vectors in each set would typically be small. In this case, the resulting sets of singular vectors $\\{u_k\\}$, $\\{v_k\\}$ are more likely to sparsely encode the $\\{x_i\\}$ and $\\{y_i\\}$ than exist in one-to-one correspondence.\nGiven the above argument, for cases where the linear representation hypothesis holds, we expect that an attention head is testing for a pair of low-dimensional subspaces in the inputs $x_i$ and $x_j$. In that case, we expect that subsets of the singular vectors of $\\Omega$ will be constructed during training so as to 'match' those subspaces. In this context, a sparse encoding allows the attention head to attend to more than r different subspaces, expanding the number of concepts that the attention head can recognize.\nNote however that attention heads have been shown to have a variety of functions, not all of which correspond to testing low-dimensional subspaces. For example, some attention heads have the role of detecting when tokens $x_i$ and $x_j$ are identical [4, 26]. In that case, we expect that $x_i$ and $x_j$ will have non-negligible inner products with most or all of the singular vectors of $\\Omega$. In fact, we see exactly this phenomenon in the case of the duplicate-token head (3, 0) as shown in Figure 2(a)."}, {"title": "Possible Mechanisms", "content": "There are a number of limitations of our study and directions for future work. First, the method as used here does not explore alternative pathways that may affect model output [13]. It also does not directly assess adaptive computations in the model [14, 23] nor does it construct minimal circuits in the sense of [26]. However, we believe that it offers an alternative toolbox that can be extended to help investigate those issues, in part by exposing the signals passing between specific pairs of attention heads.\nNext, the noise separation strategy we use is not perfect, and is perhaps too permissive. We believe that an approach that determines sets $S_{ij}$ differently may yield more precise results. For example, Figure 2 shows that certain orthogonal slices are frequently reused by certain attention heads, and a better $S'_{ij}$ identification strategy might use this observation.\nNext, the tracing strategy used herein only considers edges that make positive contributions to attention scores. However, as Figure 1 shows, certain orthogonal slices contribute strongly negative terms to the attention computation."}, {"title": "Limitations and Future Work", "content": "We have observed that these negative terms are important for determining when an attention head does not fire. The output of an attention head in general seems to depend on the balance between the few strongly positive terms and the few strongly negative terms. This is a fertile area for for investigation that could lead to deeper understanding of control signals in the model.\nIn this paper, we have restricted attention to heads that are firing (putting 50% or more attention weight on one source token). Not all attention heads work by firing on a pair of inputs. Conceptually there is no difficulty apply- ing singular"}]}