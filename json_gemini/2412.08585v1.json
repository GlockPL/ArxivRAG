{"title": "TURBOATTENTION: EFFICIENT ATTENTION APPROXIMATION FOR HIGH THROUGHPUTS LLMS", "authors": ["Hao Kang", "Srikant Bharadwaj", "James Hensman", "Tushar Krishna", "Victor R\u00fchle", "Saravan Rajmohan"], "abstract": "Large language model (LLM) inference demands significant amount of computation and memory, especially in the key attention mechanism. While techniques, such as quantization and acceleration algorithms, like FlashAttention, have improved efficiency of the overall inference, they address different aspects of the problem: quantization focuses on weight-activation operations, while FlashAttention improves execution but requires high-precision formats. Recent Key-value (KV) cache quantization reduces memory bandwidth but still needs floating-point dequantization for attention operation.\nWe present TurboAttention, a comprehensive approach to enable quantized execution of attention that simultaneously addresses both memory and computational efficiency. Our solution introduces two key innovations: FlashQ, a headwise attention quantization technique that enables both compression of KV cache and quantized execution of activation-activation multiplication, and Sparsity-based Softmax Approximation (SAS), which eliminates the need for dequantization to FP32 during exponentiation operation in attention. Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup in attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x maximum throughput over the FP16 baseline while outperforming state-of-the-art quantization and compression techniques across various datasets and models.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) (Touvron et al., 2023; Gunasekar et al., 2023; Brown et al., 2020) have excelled in tasks like natural language understanding (Joshi et al., 2017; Dodge et al., 2021) and generative text production (Hendrycks et al., 2021; Zhong et al., 2017). However, as model size increases, computational and memory demands scale correspondingly, particularly during inference. This necessitates efficient strategies to reduce memory utilization and computational complexity, thereby reducing inference latency and improving throughput critical requirements for real-time applications that demand faster user experiences.\nQuantization is among the most commonly used optimization techniques that simultaneously addresses both computational and memory constraints in LLM inference. By reducing the numerical precision of model parameters, KV cache states, and activation values, this technique enables low-precision forward pass computations while significantly reducing memory footprint. This unified approach to precision reduction offers a systematic method for optimizing inference efficiency when applied to the computational and memory intensive parts of the LLM inference.\nThe bottlenecks during LLM inference can be split into three major sections: the linear projection operations (QKV projection and FFN), the memory-intensive Key/Value (KV) cache operations, and the attention mechanism's quadratic computational and memory complexity with respect to context length. As both context lengths and model sizes scale up, the KV cache footprint expands substantially, while the attention computations become increasingly resource-intensive, leading to reduced throughput and elevated latencies, which ultimately results in degraded user experiences as well as increased costs. Previous quantization works, such as Atom (Zhao et al., 2024), QuaRot(Ashkboos et al., 2024), and Qserve (Lin et al., 2024), have primarily applied quantization techniques to linear network components (such as QKV and FFN projections), converting floating-point parameters and activations to low-bit integers to enhance memory efficiency and computational performance. On the other hand, approaches such as KIVI (Zirui Liu et al., 2023), GEAR (Kang et al.,"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "In this section, we first introduce the key aspects of the attention mechanism and state-of-the-art quantization techniques before discussing the motivation and challenges in enabling a quantized execution of attention mechanism."}, {"title": "2.1 Attention Mechanism", "content": "Multi-head attention (MHA) is a core component of the transformer architecture, with each transformer model consisting of L stacked layers. Each layer comprises two sub-modules: a multi-head attention mechanism (MHA) and a feed-forward network (FFN). Given input token embeddings $X \\in \\mathbb{R}^{n \\times d}$, the MHA computes attention in parallel across H attention heads, defined as:\n$\\text{MHA}(X) = \\text{Concat}(H^{(1)}, ..., H^{(H)})W_o$,\n$H^{(h)} = \\text{Softmax}(\\frac{Q^{(h)}K^{(h)T}}{\\sqrt{d_H}})V^{(h)}$\nwhere $Q^{(h)} = XW_q^h, K^{(h)} = XW_k^h, V^{(h)} = XW_v^h$ are the Query, Key, and Value matrices for head h. The projection matrices $W_q^h, W_k^h, W_v^h \\in \\mathbb{R}^{d \\times d_h}$ map the input embedding X to the respective attention components, and $d_h$ is typically set to d/H to balance the dimensionality across heads.\nPrefill and decoding. During generation, let the model generate $n_g$ tokens. In the initial step, the input tokens $X_0 \\in \\mathbb{R}^{n \\times d}$ are prefilled, and the Keys and Values (K, V) for each head and each layer are cached for future use. This prefill stage results in the KV caches: $K_0 = \\text{Concat}(K^{(1)}, ..., K^{(H)})$ and $V_0 = \\text{Concat}(V^{(1)}, ..., V^{(H)})$, where $K_0, V_0 \\in \\mathbb{R}^{n \\times d}$.\nAt each step t (1 \u2264 t \u2264 ng) of autoregressive decoding, the model generates a new token $x_t$, conditioned on the input and previously generated tokens. For the subsequent steps, multi-head attention (MHA) only computes the Query/Key/Value vectors ($q_t, k_t, v_t \\in \\mathbb{R}^{d}$) for the newly generated token $x_t$. These vectors are then appended to the KV cache: $K_t = K_{t-1}||k_t$ and $V_t = V_{t-1}||v_t$. The attention mechanism is then performed between $q_t$ and the updated KV cache, i.e., $K_t$ and $V_t$."}, {"title": "2.2 Attention Acceleration Techniques", "content": "The query matrix $Q \\in \\mathbb{R}^{N_q \\times d}$ and key and value matrices $K, V \\in \\mathbb{R}^{N_k \\times d}$ are inputs to the following equation which is computed independently for each head and batch instance. The output matrix $H \\in \\mathbb{R}^{N_q \\times d}$ is obtained in essentially three steps as shown in Equation 2.\n$S = \\alpha QK^T, P = \\text{softmax}(\\frac{S}{\\sqrt{d}}), H = PV.$ \nThese intermediate activations S and P are large and thus place a significant demand on memory bandwidth. FlashAttention (Dao et al., 2022) addresses this by chunking the Query, Key, and Value matrices along the token dimension and tiling the attention mechanism to accelerate the process as described below.\nFlash Attention. FlashAttention integrates the online softmax algorithm (Milakov & Gimelshein, 2018) to fuse the three operations illustrated in Equation 2. It requires only a single pass over an entire row of tokens to compute attention. This method partitions the attention output matrix H into independent output tiles, with each tile's computation being independent of the others. This approach eliminates the need for intermediate global memory reads and writes of S and P tensors. FlashAttention employs a tiling strategy, where small chunks of query, key, and value are transferred to the compute cores to first perform \u201cpartial attention\" for each pair of chunks. This is followed by normalization at the end with the help of additional values (sum and max).\nAdditionally, optimization techniques such as Ring Attention (Liu et al., 2023a), Striped Attention (Brandon et al., 2023), and Lean Attention (Sanovar et al., 2024) aim to balance computation across GPU resources while using FlashAttention's core algorithm. These techniques significantly reduce the memory bandwidth overhead during inference, especially for long-context generations. However,"}, {"title": "2.3 Progressive Quantization", "content": "Progressive quantization (PQ) uses a combination of INT8 and INT4 representations progressively, combining the computational efficiency of symmetric quantization with the ability of asymmetric quantization to represent values accurately at INT4. An overview of PQ follows.\nQuantization maps high-precision floating-point values (x) into discrete levels (Q(x)), expressed as:\n$Q(x) = \\frac{x}{s}, x = Q(x) \\cdot s + z,$\nwhere s is a scaling factor and z is a zero-point. These depend on the quantization type, symmetric (sym.) or asymmetric (asym.):\n$s = \\begin{cases} \\frac{x_{\\text{maxx}}}{2^{2bit -1}}, & \\text{sym.} \\\\ \\frac{x_{\\text{maxx}} - x_{\\text{min}}}{2^{2bit -1 - 2}}, & \\text{asym.} \\end{cases} , z = \\begin{cases} 0, & \\text{sym.} \\\\ x_{\\text{min}}, & \\text{asym.} \\end{cases}$ \nFor two quantized matrices A and B, the low-bit integer matrix multiplication is:\n$O_{ij} \\approx \\sum_k A_{ik} B_{kj} = s_a s_b \\sum_k Q(A_{ik})Q(B_{kj})$\n$+ s_a z_b \\sum_k Q(A_{ik}) + s_b z_a \\sum_k Q(B_{kj}) + z_a z_b.$\nWe see that asymmetric quantization introduces computation (the last three terms) that are not necessary in symmetric computation (where z = 0). In general, asymetric quantization leads to lower error rates but results in higher computational overhead.\nProgressive quantization proposes two stage of quantization to solve this problem. First, symmetric quantization is used to build INT8 representations for computational efficiency, avoiding the extra overhead of asymmetric quantization (the three non-zero terms above). Then, for better memory efficiency, 8-bit integers are further compressed to INT4 using asymmetric quantization.\nThe integer inference formula for PQ is:\n$O_{ij} = s_a s_b \\sum_k \\hat{Q}(A_{ik}) \\hat{Q}(B_{kj}),$\nwhere Q(A) and Q(B) denote:\n$Q(A) = \\hat{Q}(\\hat{Q}(x)) \\cdot s_{int} + z_{int},$\n$Q(B) = \\hat{Q}(\\hat{Q}(x)) \\cdot s_{int} + z_{int} .$\nHere, $s_a$ and $s_b$ are the (FP16) scales from the first INT8 quantization step. Equations 7 and 8 represent dequantization from asymmetric INT4 quantization into INT8. The scales $s_{int}$ and zero points $z_{int}$ are stored in INT8, but the majority of the data $\\hat{Q}(\\hat{Q}(x))$ are stored in INT4.\nThis approach, proposed in Qserve (Lin et al., 2024) for weight quantization, is more hardware-friendly compared to decompression or other low-bit integer inference methods, such as those in Atom (Zhao et al., 2024), LLM.int8 (Dettmers et al., 2022), or TensorRT-LLM. In our work, we extend the design of PQ to represent KV cache, with the objective of making the dequantization into the attention computations faster while keeping it compatible to attention acceleration mechanisms (such as FlashAttention)."}, {"title": "2.4 Challenges in Efficient Attention Mechanism", "content": "As we saw in Figure 1a, the attention operation can constitute up to 80% of the overall inference execution time when combined with state-of-the-art quantization techniques for other parts of transformer. This is majorly due to the high-precision execution of attention operation in state-of-the-art techniques, such as FlashAttention. As we saw in subsection 2.2, the usage of FP16 and FP32 not only adds pressure to the scratchpad and register file on the compute units of GPU, but slows down the matrix multiplications and softmax operation inside the attention operation because of the higher precision. For example, the peak performance of FP32 CUDA cores used in FlashAttention's exponentiation operation is only ~3% of the FP16 tensor performance delivered by Nvidia's A100-SXM. Thus, there is a need to enable quantized execution of the attention operation to leverage the faster low-precision tensor cores in modern GPUs. However, a naive quantization of the attention operation could lead to a loss of the model's generative performance (accuracy)."}, {"title": "3 FLASHQ: HEADWISE MIX-PRECISION PROGRESSIVE QUANTIZATION", "content": "FlashQ involves three key components that enable and significantly accelerate the quantized attention mechanism:\n1.  Blockwise Progressive Quantization. FlashAttention-compatible quantization of Key and Value for quantized execution of MatMuls.\n2.  Head-wise Mixed Precision. Compress different heads to different formats (2-bit and 4-bit) for maximum compression.\n3.  Enhanced KV cache Buffer. For dynamic management of the quantized KV cache during the decode phase, eliminating the need for recompression of KV cache."}, {"title": "3.1 Blockwise Progressive Quantization (BPQ)", "content": "As we saw in subsection 2.1, multi-head attention generates large intermediate activations, which place a significant demand on memory bandwidth. FlashAttention (Dao et al., 2022) addresses this by chunking the Query, Key, and Value matrices along the token dimension and tiling the attention mechanism to accelerate the process. FlashAttention divides the h-th head's query, key, and value matrices ($Q^{(h)}, K^{(h)},$ and $V^{(h)}$) into sub-blocks of size $T_r$ and $T_c$, denoted as $Q_r^{(h)}, K_c^{(h)},$ and $V_r^{(h)},$ for efficient tiled computation.\nTo enable a FlashAttention-compatible quantized execution of attention we propose Blockwise Progressive Quantization (BPQ). In BPQ, each of the sub-block is first compressed using 8-bit symmetric quantization. Unlike Qserve, which applies per-channel PQ to weights, we apply BPQ at a sub-block granularity via the function:\n$X^{q1} = \\text{Quant}_{sym}(X)$\nwhere $X \\in {Q_r^{(h)}, K_c^{(h)}, V_r^{(h)}}$ represents a sub-block, and Quant_{sym} denotes symmetric 8-bit quantization. Given the critical memory bottleneck posed by the KV cache, as described in subsection 2.4, we apply progressive quantization after computation to further compress the Key and Value tensors for storage.\nFurther, building on the techniques from KIVI (Liu et al., 2024) and informed by our own analysis (Figure 4), we further reduce quantization errors by compressing the Key and Value 8-bit tensors in a channel-wise manner using asymmetric quantization:\n$K^{q2} = \\text{Quant}_{asym}(K^{q1}), V^{q2} = \\text{Quant}_{asym}(V^{q1})$\nwhere $K^{q1}$ and $V^{q1}$ are groups in each channel of the key and value 8-bit tensors. This minimizes the errors caused by outliers in certain channels while keeping the tiled nature needed to make it compatible with FlashAttention's core algorithm. Since a new query vector is generated at each decoding step, compressing it further beyond the initial 8-bit quantization is unnecessary."}, {"title": "3.2 Head-wise Mixed Precision", "content": "To further enhance progressive quantization and achieve substantial memory savings in the KV cache, we explore reducing the bit-width to 2-bit. Although 4-bit KV cache quantization has shown near-lossless performance across various models, as demonstrated by Atom (Zhao et al., 2024), QuaRot (Ashkboos et al., 2024), and Qserve (Lin et al., 2024), uniformly applying 2-bit compression to all attention heads often leads to significant model performance degradation. Approaches like smooth factors (Lin et al., 2024; Xiao et al., 2024a) and offline reordering (e.g., Qserve and Atom) can improve compression accuracy but introduce additional latency overhead, making them challenging to integrate with dynamically expanding KV caches. While token-wise mixed precision could further enhance accuracy, it incurs dynamic execution overhead in CUDA kernels and is not FlashAttention-compatible, thus severely reducing hardware efficiency.\nInspired by recent work on head pruning in multi-head attention (Ge et al., 2024; Liu et al., 2023b; Rajput et al., 2024), we propose a headwise mixed-precision approach. This approach combines 2-bit and 4-bit compression, and thus achieves a favorable trade-off between accuracy and memory savings. It applies 2-bit and 4-bit BPQ for different heads based on their priority. The priority is calculated by:\n$priority(h) = gap(h) \\times std(h)$\nHere, $gap(h)$ represents the difference between the maximum and minimum values across all channels for head h, which captures the range of values in the attention mechanism. A larger gap indicates that compressing this head may introduce a larger quantization error, thus making it more important to preserve precision. $std(h)$ is the standard deviation of the channel-wise gaps within each head. This measures the variability of the gaps, where higher variability (i.e., a larger standard deviation) implies that the head's distribution is more uneven. Heads with a higher $std(h)$ are more sensitive to quantization, and thus require higher bit precision to minimize performance degradation.\nUsing this metric, we rank all heads in terms of their priority scores. Instead of using a fixed threshold, we compress the lowest-priority $n_h$ heads in each layer to 2-bit, while the remaining heads are quantized to 4-bit. The quantization strategy is thus defined as:\n$\\text{Quantization strategy} = \\begin{cases} \\text{2-bit}, & \\text{if rank(priority(h))} \\le n_h \\\\ \\text{4-bit}, & \\text{if rank(priority(h))} > n_h \\end{cases}$ \nHere, $n_h$ is the number of heads to be compressed to 2-bit, determined per layer. By ranking the heads based on their priority and applying lower-bit quantization to the least critical heads (i.e., those with the smallest priority), we achieve a balanced trade-off between memory savings and model performance."}, {"title": "3.3 Enhanced KV cache Buffer", "content": "FlashQ introduces an efficient decoding buffer to accelerate inference during long-context generation. To support integer inference in the attention mechanism, newly generated tokens are temporarily stored in a buffer B of size $n_b$ (e.g., $n_b$ = 64) and quantized using 8-bit symmetric quantization (Equation 9). This avoids the need for compression at every decoding iteration. Once the buffer reaches its capacity, FlashQ applies progressive quantization (Equation 10) to further compress the keys and values from 8 bits to lower bit-widths for improved memory efficiency.\nTo ensure stability, we employ a universal scale for 8-bit quantization and clamp outliers that exceed this range, preventing the need to re-compress previously stored tokens in the buffer when new tokens with larger values are added. This approach contrasts with previous methods like KIVI (Liu et al., 2024) and GEAR (Kang et al., 2024), which keep buffers in full precision, introducing significant memory overhead and preventing the use of integer inference in the attention mechanism."}, {"title": "4 SAS: SPARSE ACTIVATED SOFTMAX", "content": "Softmax is a key bottleneck in the attention mechanism, particularly in flash attention workflows, where tiling operations introduce additional overhead. From our experiments, we observe that softmax computation costs over 30% of the attention execution time both in prefill and decoding stages. The primary performance issue stems from frequent data type conversions between FP16 and FP32 for performing the exponentiation operation. Current GPUs are limited to single-precision floating-point (FP32) operations for exponentiation, which exacerbates this bottleneck. Previous methods, such as (Vasyltsov & Chang, 2021) and (Shim et al., 2017), either introduce significant errors that degrade performance or are unsuitable for modern large language models.\nTo mitigate these issues, we propose a softmax approximation technique based on polynomial approximation(POLY) and lookup tables(LUT), called Sparse Activated Softmax (SAS). This method divides the exponential computation into two parts: the integer and decimal components. For example, an exponent computation can be separated into integer $X_{int}$ and decimal $X_{dec}$ parts which lie in [0,1) (Detailed algorithm in Appendix B). For the integer part, we use a lookup table, which remains compact because large values in the exponential function decay quickly, allowing us to skip larger integers. For the decimal part, we employ a simple polynomial approximation.\n$e^{-x} = e^{-x_{int}} \\times e^{-x_{dec}} \\approx LUT(-x_{int}) \\times POLY(-x_{dec})$\nAs such, the approximation algorithm of SAS goes:\n$SAS(x) = \\begin{cases} 0, & x < n_r \\\\ LUT(x_{int}) \\times POLY(x_{dec}), & x \\ge n_r \\end{cases}$ \nTo optimize the computation of the exponential function $e^{-x}$, we limit the approximation range to [0, 1] and reduce the polynomial degree to less than 3 to enhance computational efficiency. Using the least squares method to determine the coefficients, our 3-degree polynomial approximation of $e^{-x}$ within this range is as follows:\n$POLY(x) = -0.1025 x^3 + 0.4626 x^2 - 0.9922 x + 0.9996$\nThis polynomial captures the essential behavior of $e^{-x}$ over [0, 1] with minimal computational overhead, as shown in Figure 5. This hybrid approach reduces computational and data type conversion overhead while preserving accuracy, effectively alleviating the exponentiation bottleneck in flash attention algorithm. The overall exponentiation operation is accelerated, as the polynomial operation can be done solely in GPU Tensor Cores in FP16 dataformat.\nFurther, from our observations, the large negative values of the attention scores (resulting from the query-key multiplication) become extremely small after softmax due to the nature of the exponential function. As such, we apply a sparsification strategy, retaining only the larger attention scores within a certain range $n_r$(e.g., 0 to -5) and setting the rest to zero after the sparse activated softmax computation to keep the look-up table small enough. This further reduces the computational cost, making SAS a practical and efficient solution for large-scale language models."}, {"title": "5 EVALUATIONS", "content": "Models and dataset We conducted an extensive evaluation of TurboAttention for generative inference using a variety of open-source pre-trained and instruction-tuned models, including LLaMA3-8B-inst (Dubey et al., 2024), Phi3-mini-inst (Abdin et al., 2024), and Qwen2.5-7B (Yang et al., 2024), on multiple generative tasks. These tasks included mathematical reasoning datasets such as GSM8k (Cobbe et al., 2021) and AQUA (Ling et al., 2017), symbolic reasoning tasks from BigBench Hard (BBH) (Suzgun et al., 2022). Given the complexity of these tasks, we use the chain-of-thought prompts created by (Fu et al., 2023) to improve reasoning, which contains 8-shot demonstrations of multi-step reasoning. With the CoT demonstrations, we have the average prefill length of GSM8k, AQUA, and BBH as 900, 1304, 1021 respectively. We then prompt the model to generate 256 tokens and extract answers from them."}, {"title": "5.2 TurboAttention Implementation", "content": "To minimize overheads, we implemented and optimized TurboAttention with OpenAI Triton(Tillet et al., 2019) kernel support as follows. First, to address the memory bottlenecks caused by frequent quantization and dequantization, we fused the QKV projection with quantization (Equation 9). Additionally, efficient dequantization from progressive quantization was integrated directly into the attention mechanism to reduce decoding latency (Equation 10).\nSecond, we implement a KV cache buffer (subsection 3.3 for the newly generated Key and Value vectors during decode. These Key/Value vectors are further compressed to INT4 and INT2 (depending on the head) every $n_b$ steps. To avoid recompression when newly generated KV tokens exceed the previous cache value range maximum, we clamp outliers to ensure only newly generated tokens are compressed to INT8. This further improves decoding efficiency by reducing unnecessary recompression.\nThird, we leveraged the Triton framework to implement INT8 matrix multiplications for the attention mechanism, thereby reducing both memory and computation bottlenecks. Finally, we implemented an efficient polynomial approximation and lookup table for SAS, significantly enhancing the overall performance of the softmax operation. For TurboAttention, we fix block size $B_c$, $B_r$, and $n_b$ to 64, SAS threshold $n_r$ to -6, and set half of the heads' KV cache to 2-bit quantization for head-wise mixed precision. Further optimization can be achieved by implementing CUDA kernels, as they can deliver higher efficiency and substantial speedup.\nFramework. For our experiments, we applied TurboAttention and the baseline methods to models available on Huggingface (Wolf et al., 2019), with our inference framework implemented in PyTorch (Paszke et al., 2019). Since our focus is on attention acceleration, all other parts of the model are maintained in FP16 unless otherwise stated.\nPrecision. In our experiments, we explore ultra-low precision quantization techniques, reporting performance results for 4-bit, 3-bit, and mixed headwise 2/4-bit quantization of the KV cache. Our findings indicate that TurboAttention achieves near-lossless performance with 4-bit KV cache compression across various tasks and models. However, consistent with findings in other KV cache compression methods, compressing to 2-bit can led to accuracy degradation. To balance accuracy and memory savings, we applied 2-bit precision to 50% of model heads, achieving a practical trade-off. Consequently, we benchmark TurboAttention with mixed precision quantization against other 3-bit baseline methods for a comprehensive comparison in 2."}, {"title": "5.3 Baseline Techniques", "content": "We compare TurboAttention against the following state-of-the-art techniques:\n\u2022 FP16 A dense model where both activations and weights are represented as 16-bit floating-point numbers. This serves as our base dense model without any attention optimizations.\n\u2022 FlashAttention An attention algorithm applying FlashAttention (Shah et al., 2024) with FP16 computation (and FP32 for exponentiation). This method is \"exact\" and does not alter accuracy but improves system efficiency by optimizing the attention mechanism.\n\u2022 KIVI (Liu et al., 2024) A near-lossless KV cache quantization method that compresses the key cache per-channel and the value cache per-token using fine-grained grouping. It stores residual tokens of length $n_0$ in full precision, and achieving 4-bit KV cache compression at its best accuracy mode.\n\u2022 GEAR(Kang et al., 2024) A concurrent KV cache quantization method that achieves 2-bit near-lossless compression by leveraging low-rank approximations to reduce quantization errors. Residual tokens of length $n_0$ are stored in full precision for error compensation. Here we use the efficiency version GEAR-L, that deploy quantiations and low rank approximation together to compress KV cache.\nIt is important to note that TurboAttention is not just a KV cache compression algorithm but rather a unified approximation algorithm for the attention mechanism. However, we compare it with KV cache compression algorithms to highlight its ability to preserve both performance and efficiency in large language models (LLMs)."}, {"title": "5.4 Evaluation Results", "content": "Generative performance on reasoning tasks. Table 2 demonstrate that TurboAttention achieves comparable or superior performance to recent KV cache compression algorithms across different models and the challenging reasoning tasks, in both 4-bit and lower-bit compression settings. Despite further compressing the query and approximating the softmax operation, TurboAttention maintains near lossless accuracy, achieving an average accuracy of 60.27% across three models and datasets, which is closely aligned with the FP16 baseline average accuracy of 61.89%. Notably, TurboAttention also delivers outstanding 3-bit performance, surpassing baseline methods while providing reduced KV cache size, and enhanced inference efficiency."}, {"title": "5.5 Efficiency Comparison", "content": "In this section, we evaluate latency(wall clock time) and maximum throughputs of TurboAttention on a single A100 GPU(80GB). We measure the attention mechanism of the model under varying batch sizes with a fixed context length of 1k, as well as under different context lengths (ranging from 4k to 32k) with a fixed batch size of 4. We separately assess the prefill and decoding phases and report the speedup ratio compared to the FlashAttention FP16/32 baseline. Figure 6 shows the latency comparison across various input settings and methods, all using the same attention mechanism configuration.\nOur results indicate that, across batch sizes from 1 to 64 and context lengths from 4k to 32k, TurboAttention consistently outperforms both FlashAttention FP16 and other KV cache compression techniques combined with Flash Attention baselines. Specifically, TurboAttention achieves a speedup of up to 1.8x for prefill and up to 1.7\u00d7 for decoding, highlighting significant improvements in softmax, matrix multiplication, and dequantization efficiency. Additionally, by compressing the KV cache to int-4/2, TurboAttention enables long-context generation up to 32k with a batch size of 4, whereas the FP16 baseline encounters out-of-memory (OOM) issues beyond a 4k context length. We also observe that due to the dequantization overhead in other KV cache compression methods, such as KIVI, these methods exhibit worse latency relative to the FP16 baseline."}, {"title": "5.6 Ablation Study", "content": "Ablation study on head wise selection methodology In this study, we aim to validate the effectiveness of our priority-based head selection strategy as detailed in subsection 3.2. We compare our approach against several baseline methods that use simpler, less adaptive metrics to drive head selection. As explored in subsection 3.2 and illustrated in Figure 4, channelwise outliers critically impact quantization error. To address this, we examine head selection strategies that incorporate channelwise value ranges as the basis for selection.\nBaseline methods include: Entropy: Selecting 2-bit heads based on the entropy of each head. Min-Max: Using the range between minimum and maximum values within heads. Variation: Evaluating variation in channelwise value ranges. The benchmarks are run on the LLaMA-3-8B-inst model with 8-shot COT prompts on the AQua dataset. Given LLaMA-3-8B-inst's 8-head configuration per key/value cache, we perform head selection across a range of 0 to 16 heads. Results, as shown in Figure 7b, indicate that our prioritized strategy significantly surpasses other selection approaches in minimizing quantization error, supporting its efficiency and robustness in managing outliers.\nAblation study on block size We evaluate the sensitivity of TurboAttention to varying block sizes (Be and Br) for the query, key, and value matrices. These block sizes are closely related to the device's SRAM capacity and have a substantial impact on system efficiency. Consequently, assessing TurboAttention's performance across different block sizes is essential to confirm its robustness and adaptability across configurations. As shown in Table 3, we tested Phi3-mini with various block sizes on the GSM8K-COT dataset using an 8-shot setup. The results demonstrate that TurboAttention maintains robustness across different block sizes, underscoring its ability to integrate seamlessly with Flash Attention across various systems while preserving model accuracy."}, {"title": "6 CONCLUSIONS", "content": "This paper introduces TurboAttention, a highly efficient compression algorithm for attention mechanisms that integrates seamlessly with attention acceleration methods like FlashAttention, achieving near loss-less performance. TurboAttention demonstrates state-of-the-art results in handling complex generative tasks, including mathematical and reasoning challenges, using 4-bit and 2-bit KV cache compression. Additionally, it provides significant performance gains, with up to 1.8x latency speedup and a 2.37x increase in maximum throughput over the FP16 baseline, marking a substantial advancement in efficient attention mechanism execution."}, {"title": "A DETAILED ALGORITHM OF TurboAttention", "content": "Algorithm 1 TurboAttention Prefill\nRequire: Matrices $Q, K, V \\in \\mathbb{R}^{N \\times d}$ in HBM, compressed KV cache $K^{q2}, V^{q2},$integer scale of KV cache $s_{K^{q2}}, s_{V^{q2}},$ floating\nscale $s_{k}, s_{v}, s_{v2},$,zero point of KV cache $z_{int}^{K^{q2}}, z_{int}^{V^{q2}} \\in \\mathbb{Z}_2, \\mathbb{Z}_2,$, block sizes $B_c, B_r$, Softmax approximate algorithm SAS.\nDivide Q into $T_r = \\lceil\\frac{N}{B_r}\\rceil$ blocks $Q_1,..., Q_{T_r}$ of size $B_r \\times d$ each, and divide $K, V$ in to $T_c=\\lceil\\frac{N}{B_c}\\rceil$ blocks $K_1,..., K_{T_c}$. and $V_1,..., V_{T_c}$, of size $B_c \\times d$ each.\nDivide the output $O \\in \\mathbb{R}^{N \\times d}$ into $T_r$ blocks $O_1, ..., O_{T_r}$ of size $B_r \\times d$ each, and divide the logsumexp $L$ into $T_r$ blocks $L_1,..., L_{T_r}$ of size $B_r$ each.\nfor 1 < i < $T_r$ do\nLoad $Q_i$ from HBM to on-chip SRAM.\nOn chip, initialize $O_{i}^{(0)} = (0)_{B_r\\times d} \\in \\mathbb{R}^{B_r\\times d}, \\ell_i^{(0)} = (0)_{B_c} \\in \\mathbb{R}^{B_c}, m_i^{(0)} = (-\\infty)_{B_r} \\in \\mathbb{R}^{B_r}$.\nfor 1 \u2264 j \u2264 $T_c$ do\nLoad $K_j, V_j$ from HBM to on-chip SRAM.\n\nOn chip, compute $\\overline{s_{Q}} = \\text{max}(abs(Q_i)), Q^{q1} = \\frac"}]}