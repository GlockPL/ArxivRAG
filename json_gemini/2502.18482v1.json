{"title": "MixLLM: Dynamic Routing in Mixed Large Language Models", "authors": ["Xinyuan Wang", "Yanchi Liu", "Wei Cheng", "Xujiang Zhao", "Zhengzhang Chen", "Wenchao Yu", "Yanjie Fu", "Haifeng Chen"], "abstract": "Large Language Models (LLMs) exhibit potential artificial generic intelligence recently, however, their usage is costly with high response latency. Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency. However, the challenges involve: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and (3) navigating a varying (e.g., new LLM addition or old LLM removal) set of LLM candidates over time. To bridge these gaps, we develop MixLLM, a dynamic contextual-bandit-based routing system for query-LLM assignment. Specifically, we first leverage query tags to enhance query embeddings for the routing task. Next, we design lightweight prediction models to estimate the response qualities and costs of queries over LLMs. We then devise a meta-decision maker to choose the query-LLM assignments to best tradeoff response quality, cost, and latency. Finally, the system benefits from continual training, allowing it to adapt to evolving queries and user feedback over time. Our extensive experiments show that MixLLM achieves the best trade-offs in response quality, cost, and latency (97.25% of GPT-4's quality at 24.18% of the cost under the time constraint).", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have exhibited abilities to understand massive texts, generate actionable knowledge, enable contextual reasoning, and innovate diverse applications (Radford et al., 2018, 2019; Brown et al., 2020; Raffel et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023). However, deploying LLMs presents unique challenges in managing computational resources, optimizing response times, and ensuring scalability."}, {"title": "Related Work", "content": "Studies on selecting the most suitable LLM include non-predictive and predictive routing systems."}, {"title": "Non-predictive Routing System", "content": "Non-predictive systems incorporate LLM inference during routing. A common approach is cascading, where smaller models are used first, switching to larger ones if needed. FrugalGPT (Chen et al., 2023) introduced three strategies to reduce cost while maintaining response quality: prompt adaptation, LLM approximation, and LLM cascade form a chain of LLMs, selecting LLMs from small to large. AutoMix (Madaan et al., 2023) introduced a similar cascading strategy, where a self-reviewer judges the answer and a meta-reviewer decides whether switching to a larger model is needed. However, in non-predictive routing systems, one query may need to be answered by several LLMs which increases both cost and resource usage."}, {"title": "Predictive Routing System", "content": "Predictive systems estimate the quality of LLM response before making routing decisions and route each query to only one LLM. These systems typically fall into categories such as classification, quality prediction, optimization, and bandit-based solutions, each offering unique strategies. Given the capability of LLMs to handle tasks across diverse domains, including medical applications (Liu et al., 2024d, 2025), bioinformatics (Ying et al., 2024; Liu et al., 2024a), material science (Hu et al., 2024b), industrial engineering (Xie et al., 2025, 2024), etc, these routing systems are crucial for optimization in real-world scenarios. Classification-based approaches predict the best LLM for a query by treating LLMs as labels. HybridLLM (Ding et al., 2024) trained a binary classifier to assign \u201ceasy\u201d queries to smaller models. ME-Switch (Liu et al., 2024c) extended to a multi-label domain classifier, improving memory and computation efficiency. Other methods like Zooter (Lu et al.,"}, {"title": "Methodology", "content": null}, {"title": "The Dynamic LLM Routing Task", "content": "We study the problem of dynamic LLM routing with streaming queries. Given queries that arrive sequentially, our goal is to assign each query to the most appropriate LLM selected from a set of candidates to trade off response quality, cost, and latency. Formally, let the set of streaming queries be: $Q = \\{q_n\\}_{n=1}^N$, and the set of LLM candidates be: $M = \\{m_l\\}_{l=1}^M$. The objective is to select the most suitable LLM $m_l$ for the query $q_n$."}, {"title": "Overview of The MixLLM Framework", "content": "Figure 2 shows that MixLLM consists of four components: (1) tag-enhanced query embedding, (2) LLM-specific prediction, (3) meta decision maker, and (4) continual learning mechanism. This framework allows MixLLM to route queries to LLMs in a dynamic system while achieving quality-cost-latency trade-offs and continual learning with a changing LLM candidate set."}, {"title": "Tag-enhanced Query Embedding via Unsupervised Fine-tuning", "content": "A query can be seen as a token sequence, thus, its embedding can be generated using a pre-trained encoder (e.g., BERT (Devlin et al., 2019)): $e_n = \\text{Encoder}(q_n)$, where $e_n$ represents the embedding of n-th query $q_n$ in a query stream. However, such general-purpose query embeddings contain too much noises and are not tailored for LLM routing. To address this limitation, we propose enhancing the encoder by introducing tag knowledge, which enriches the query embeddings and improves their effectiveness for routing tasks.\nDifferent LLMs can be proficient in different domains (e.g., Science, Legal) (Liu et al., 2024b). Using GPT-4 as an example, Figure 3 shows a clear correlation between domain and response quality. The query distribution after t-SNE dimension reduction is shown in Figure 3a, with each color representing a specific domain. Figure 3b highlights GPT-4's response quality. It is evident that GPT-4 has a higher error frequency (orange points in Figure 3b) in the \"Legal\" (red points in Figure 3a) and \"Math\" (purple points in Figure 3a) domains. These observations inspire us to develop"}, {"title": "Automated Query Tag Generation.", "content": "To prepare, we employ the InsTag (Lu et al., 2023) model to generate fine-grained tags for each query and manually cluster the tags into a set of coarse-grained domains, denoted as $D$. InsTag is an instruction tagging method designed to quantify the diversity and complexity of human instructions, and these tags contribute to model fine-tuning."}, {"title": "Unsupervised Fine-tuning of Encoder.", "content": "While the InsTag model, backed by Llama-2 13B, is too large to be used during inference, we fine-tune the BERT encoder during the training stage. We develop an unsupervised optimization objective that integrates intra-domain similarity ($L_{intra}$) and inter-domain separation ($L_{inter}$):\n$L = L_{intra} + L_{inter},$\t\t\t\t(1)\nwhere the intra-domain similarity loss encourages embeddings within the same domain cluster to be close to their center $\\mu_i$:\n$L_{intra} = \\frac{1}{\\lvert Q \\rvert}\\sum_{i=1}^{\\lvert Q \\rvert} log \\frac{exp(e_i \\cdot \\mu_j)}{\\sum_{j=1}^{\\lvert D \\rvert} exp(e_i \\cdot \\mu_j)},$\t(2)\nThe inter-domain separation loss ensures that different domain centers are distinct:\n$L_{inter} = \\frac{1}{\\lvert D \\rvert} \\sum_{j=1}^{\\lvert D \\rvert} log \\sum_{k \\neq j} exp(\\mu_j \\cdot \\mu_k).$\t\t(3)"}, {"title": "LLM-Specific Quality and Cost Prediction", "content": "Given a query embedding, we aim to predict both the response quality and financial cost for each candidate LLM on the query, so the meta decision-maker can assign the most suitable model."}, {"title": "Estimating the Response Quality of A Query-LLM Pair.", "content": "Since different LLMs have different response qualities, we learn an LLM-specific regression function for each LLM. This function estimates the response quality of the n-th query on the l-th LLM:\n$\\hat{p}_{n,l} = f_l^p(e_n; \\theta_l^p),$\\t\t\t(4)"}, {"title": "Estimating the Financial Cost of A Query-LLM Pair.", "content": "The total cost of the n-th query on the l-th LLM includes: 1) the known input cost and 2) the predicted output cost, according to typical LLM pricing policies:\n$\\hat{c}_{n,l} = len_{input} \\cdot price_{input} + \\widehat{len}_{response} \\cdot price_{response},$\\t\t(5)\ninput cost\t\t\t\toutput cost\nwhere $len_{input}$ is the prompt length of query $q_n$, and $price_{input}$ and $price_{response}$ are unit prices of input prompt and output response. The response length $\\widehat{len}_{response}$ is predicted using a similar method as the response quality predictors:\n$\\widehat{len}_{response} = f_l^{rl}(e_n; \\theta_l^{rl}),$\\t\t\t(6)"}, {"title": "Meta Decision Maker", "content": "For the n-th query $q_n$, the final decision score for each candidate LLM is determined by three factors: (1) $s_{n,l}^{trade}$, which trade-offs the predicted quality and cost; (2) $s_{n,l}^{unc}$, which accounts for potential prediction uncertainty; and (3) $s_{n,l}^{pen}$, which discourages selecting candidates with long waiting time:\n$s_{n,l} = \\alpha \\cdot s_{n,l}^{trade} + s_{n,l}^{unc} - \\beta \\cdot s_{n,l}^{pen},$\t\t\t\t(7)\nwhere $\\alpha$ and $\\beta$ control the relative importance. The willingness to pay $\\lambda$ is introduced in $s_{n,l}^{trade}$ to control the priority of quality over cost, leading to different budgets accordingly:\n$s_{n,l}^{trade} = \\frac{\\widehat{p}_{n,l}}{\\lambda + 1} - \\frac{\\widehat{c}_{n,l}}{\\lambda + 1},$\t\t\t\t\t(8)\nTo handle prediction errors, we introduce an uncertainty measurement ($s_{n,l}^{unc}$) to enhance robustness (Li et al., 2010):\n$s_{n,l}^{unc} = e_n^T A_l^{-1} e_n,$\t\t\t\t\t(9)\nwhere $A_l^{-1}$ represents the inverse covariance matrix for the l-th LLM. This measures the amount of information gathered for each candidate and adjusts the confidence of the prediction accordingly."}, {"title": "Continual Learning", "content": "To ensure effectiveness in real-world applications, we designed both offline and online training modes. The offline mode enables the model to achieve robust performance before deployment, while the online mode allows the model to continuously improve in response to changing environments and user feedback."}, {"title": "Offline Training:", "content": "Prior to deployment, we perform offline training using refined feedback from all candidate LLMs. The refined feedback includes real response quality and length, which involves updating the parameters of the predictive models:\nThe parameters $\\theta_l^p$ for the response quality predictors are updated using gradient descent:\n$\\theta_l^p := \\theta_l^p - \\eta_1 \\cdot \\nabla_{\\theta_l^p} L(\\hat{p}_{n,l}, p_{n,l}),$\t\t\t\t(12)\nSimilarly, the response length predictor parameters $\\theta_l^{rl}$ are updated as:\n$\\theta_l^{rl} := \\theta_l^{rl} - \\eta_2 \\cdot \\nabla_{\\theta_l^{rl}} L(\\widehat{len}_{response}, len_{response}),$\t\t(13)\nThe uncertainty matrices $A_l$ are updated incrementally by query embeddings:\n$A_l := A_l + e_n \\cdot e_n^T.$\t\t\t\t\t(14)\nThis update accumulates information over time, decreasing the inverse $A_l^{-1}$, which leads to low uncertainty, indicating increased confidence in predictions. Then the waiting time is adjusted based on the LLM assignment."}, {"title": "Online Training:", "content": "After deployment, the system incrementally updates predictive models and uncertainty matrices using refined single feedback from the selected LLMs.\nHowever, user feedback based on human satisfaction with the LLM service, often binary (\"good\" or \"not good\"), is challenging for training. To address this, we introduce a Dynamic Feedback Score ($s_{n,l}^{df}$) based on the contextual bandit method to capture the binary user feedback and dynamically adjust the scoring mechanism.\nThe final score for each LLM is updated as:\n$s_{n,l} = s_{n,l} + k_{n,l} \\cdot s_{n,l}^{df},$\t\t\t\t\t\t(15)\nwhere $s_{n,l}^{df}$ represents the appropriateness of the l-th LLM to answer the given query predicted by a shared 3-layer MLP network:\n$s_{n,l}^{df} = f_{df}(e_n; \\theta_{df}).$\t\t\t\t\t\t(16)\nAnd $k_{n,l}$ is the confidence factor based on the variance, to ensure the reliability of $s_{n,l}^{df}$ and prevent over-reliance on unstable predictions:\n$k_{n,l} = \\frac{1}{Var_n[s_{n,l}^{df}] + \\epsilon},$\t\t\t\t\t\t\t(17)\nwhere $\\epsilon$ is a small constant to avoid division by zero. Low variance increases $k_{n,l}$, which will enhance the importance, while high variance decreases it, which reflects instability. Then the candidate with the highest score is selected:\n$m_n^* = \\arg \\max_l (s_{n,l})$\t\t\t\t\t(18)\nSince we cannot directly supervise the network outputs with binary feedback $r_n$, we apply the Policy Gradient method (Ban et al., 2021) to update $\\theta_{df}$. The probability of selecting candidate l is:\n$\\pi(l \\vert e_n; \\theta_{df}) = \\frac{exp(s_{n,l}^{df})}{\\sum_{k=1}^M exp(s_{n,k}^{df})},$\t\t\t\t(19)\nThe goal is to maximize the expected reward:\n$J(\\theta_{df}) = E_{l \\sim \\pi(l \\vert e_n; \\theta_{df})} [r_n],$\t\t\t\t\t\t\t(20)\nwith gradient on selected candidate $m$:\n$\\nabla_{\\theta_{df}} log \\pi(m \\vert e_n; \\theta_{df}) = \\nabla_{\\theta_{df}} (s_{n,m}^{df} - log \\sum_{k=1}^M exp(s_{n,k}^{df})).$\t\t\t(21)\nThe parameters are updated as:\n$\\theta_{df} := \\theta_{df} - \\eta_3 \\cdot \\nabla_{\\theta_{df}} log \\pi(m \\vert e_n; \\theta_{df}) \\cdot r_n.$\t\t\t(22)"}, {"title": "Experiments", "content": null}, {"title": "Experimental Settings", "content": null}, {"title": "Dataset", "content": "We conduct our experiments utilizing the Router-Bench dataset (Hu et al., 2024a), which consists of 36,497 queries from 8 NLP datasets, including Chinese and English. Each query is answered by 11 different LLMs, with records of responses, as well as corresponding quality and cost metrics. Moreover, we extend the dataset with Llama 3.1 8B and 70B models 1 and add prompt and response lengths of all the queries and responses. The dataset is split into 80% training and 20% testing."}, {"title": "Baseline Algorithms", "content": "We compare MixLLM with both non-predictive and predictive baselines in our experiments. For non-predictive methods, the cascading approach tests smaller models first and switches to larger ones if needed. We extend AutoMix (Madaan et al., 2023) by ordering multiple LLMs by size, with cheaper models prioritized when sizes are equal. For predictive methods, RouteLLM (Ong et al., 2024) assigns queries to LLMs using a BERT-based multi-label classifier, while Zooter (Lu et al., 2024) is represented by an MLP-based classifier. RouterBench (Hu et al., 2024a) predicts response quality to achieve a quality-cost trade-off. Both FORC (\u0160akota et al., 2024) and OptLLM (Liu et al., 2024e) predict quality and then perform set-level optimization, while MetaLLM (Nguyen et al., 2024) uses a bandit algorithm with a quality-cost reward. For additional comparison, we also include random routing and individual LLMs.\nSince the baseline algorithms do not include online training after deployment, we only compare them with our offline training component for a fair comparison in Section 4.2, while the online training component is further evaluated in Section 4.3."}, {"title": "Evaluation Metrics", "content": "We evaluate the methods on the streaming test queries based on the quality-cost trade-off under the latency constraint. Specifically, the response quality score for each query is scaled from 0 to 1, while the query cost is measured in dollars. Any query that exceeds the maximum tolerable waiting time is assigned a quality score of 0. The total quality and total cost are calculated as the sum of quality scores and query costs for all the test queries."}, {"title": "Configurations", "content": "In our experiments, we set up a software environment consisting of Python 3.12, PyTorch 2.0, and CUDA 12.1 running on Ubuntu 18.04 LTS. Most experiments were conducted on a 12GB Titan-V GPU, while tasks involving Llama models, such as dataset extension and tag extraction, were performed on two 80GB H100 GPUs.\nAll random seeds are set to 42 for reproducibility. In Equation (7), $\\alpha$ is set to 0.01, and $\\beta$ is set to 0.1. In Equation (10), $\\gamma$ is set to 0.1.\nAs for learning rates, $\\eta_1$ and $\\eta_2$ are set to 1, reflecting the use of simple machine learning algorithms, while $\\eta_3$ is set to 0.001 due to the complexity of the neural network.\nQuery streams are configured at a rate of 100 queries per 10 seconds. The maximum tolerable waiting time $\\tau$ is set to 30 seconds, and the waiting time of LLMs will be updated every 10 seconds. The prices of input and output, the average initial time, and response speeds of different LLMs are publicly available 2. This website estimates the costs of open-source LLMs based on computational resources, including CPU, GPU, and memory usage, while API-based LLMs are priced directly using their API rates.\nAs for quality and length regressors, we use random forest (RF) for quality prediction across all LLMs, while a combination of multi-layer perceptron (MLP), RF, and K-nearest neighbors (KNN) is applied for length (cost) prediction depending on the LLM. Those predictors are lightweight. For example, the size of an MLP model is less than 2MB, so the inference and update time is shorter."}, {"title": "Overall Results", "content": "As shown in Figure 4, MixLLM consistently outperforms the baselines, delivering strong performance. For the baseline methods, response quality can decline with larger budgets since queries may exceed the latency constraint. Notably, MixLLM achieves 97.25% of GPT-4's quality at only 24.18% of the cost when $\\lambda$ is 1.4. In comparison, the best baseline method, OptLLM, reaches 96.39% of GPT-4's quality at 32.94% of the cost. However, beyond this point, OptLLM's response quality drops as many queries exceed the waiting time tolerance, while MixLLM remains stable. The same situation also happens on other baseline algorithms.\nThe Oracle result represents the most optimal routing on this dataset, balancing response quality and cost. It serves as a benchmark for the best possible assignment. In this context, a point closer to the upper left (Oracle) signifies higher quality at a lower cost. To obtain the Oracle result, all candidate LLMs are tested for each query. For each query, the LLM that meets the quality threshold and has the lowest cost is selected. While the final results reflect only the quality and cost of the selected LLM, the process of determining the Oracle result requires significant computational resources. Each single LLM provides one quality-cost point. For instance, GPT-4 demonstrates superior quality, while GPT-3.5 offers a better balance of cost and quality. The \u201cRandom\u201d routing serves as a baseline; points above and to the left of this anchor are superior in offering better quality at a lower cost.\nAutoMix struggles because multiple LLMs handle each query, quickly exhausting the budget and reaching the latency constraint. RouteLLM and Zooter fail to adjust budgets dynamically and can only provide one quality-cost point. RouterBench performs well at lower budgets but faces latency issues as budgets increase. FORC and OptLLM share the problem of ignoring some queries due to set-level optimization, affecting user experience. MetaLLM is less effective because it can't consider multiple LLMs simultaneously, underscoring the need for a multi-armed bandit approach."}, {"title": "Study on Continual Training", "content": "To enable continual training, we simulate the real-world query streams by splitting the training dataset into different ratios (Table 1) for offline and online training. For example, an 80:20 split means 80% of the data are used in offline training, while 20% of the data are used in online training. The offline training uses refined feedback across these splits. For online training, in addition to the refined feedback, user feedback is simulated by assuming the user is satisfied if the response quality exceeds 0.7 and the waiting time is less than 15 seconds.\nIn our experiments, we implemented one online test at the end of online training to demonstrate the continuing improvement of learning from and aligning with online feedback. Without loss of generality, we believe our one-time finding (online feedback can improve performance and alignment) can be generalized to recurrent tests. It is feasible to adapt our system to conduct recurrent tests at the end of each cycle in a real-world scenario."}, {"title": "Study on Tag-Enhanced Embedding", "content": "To obtain tags for the tag-enhanced encoder training, we employ InsTag (Lu et al., 2023), a Llama-"}, {"title": "Study on Latency Constraint", "content": "Theoretically, the trade-off between response quality and query cost often operates within the bounds of limited hardware resources in the real world. Effectively managing the workload on devices becomes essential. Different components, such as the CPU, GPU, memory, and bandwidth, all have their performance metrics, but these factors converge on one critical metric: query waiting time. Therefore, we employ the latency as the primary constraint.\nWe conducted a simulation to account for the latency constraint. The total time required to answer a query has two parts: 1) the initial time to begin generating and 2) the response time, which depends on the answer length. We use the average initial time for each LLM and estimate the response time by multiplying the output length by the corresponding LLM's generation speed. For closed-source LLMs, the simulation is based on API statistics. For open-source LLMs, we simulated under ideal hardware conditions, assuming sufficient memory and stable network connections to ensure optimal performance. The average initial time and response speed of different LLMs are publicly available 3."}, {"title": "Study on Adaptive Training", "content": "Each LLM in MixLLM operates independently, ensuring scalability. Adding or removing candidate LLMs does not require complete re-training, which only affects the corresponding LLM. To demonstrate this advantage, we extended the RouterBench"}, {"title": "Study on Out-of-Domain Generalization", "content": "Real-world queries often originate from new or unseen domains, presenting challenges for LLM routing systems. To evaluate the domain adaptation and generalization capabilities of MixLLM, we conducted an out-of-domain (OOD) experiment. In this setup, we simulate an OOD scenario using the domains defined by tags. We maintain an 80:20 splitting ratio, where the testing set (20% of the data) contains non-overlapping domains not present in the training set (80% of the data). This design ensures that some testing samples belong to entirely unseen domains during training."}, {"title": "Study on Different Choice Policy", "content": "During our experiments, a new question arises: Can selecting more LLMs improve performance? To explore this, we applied various selection policies, with the results presented in Figure 7.\n\"\u0422\u043e\u0440 1\", \"Top 2\", and \"Top 3\" refer to policies where the LLM(s) with the highest 1, 2, or 3 scores are selected. When multiple LLMs are chosen, the response quality reflects the best one, while costs are summed. The \"TOP 1.5\" policy introduces a dynamic adjustment, which selects the top 1 LLM when the budget is low and expands to include more LLMs as the budget increases. As illustrated in Figure 7, increasing the number of selected LLMs shifts the curve upwards and to the right. This outcome is expected because selecting more LLMs increases both cost and the likelihood of choosing the most capable model. Notably, with the same budget (red line in Figure 7), the \u201cTop 3\" policy achieves the highest response quality, even surpassing the most powerful single LLM, GPT-4, at only 20% of its cost.\nHowever, in practical scenarios, users typically seek a single, definitive answer rather than multiple options. How to select the final answer? Adding a reviewer to choose the best answer is one potential solution, but it requires additional time and resources. Given the complexity, we did not incorporate a multi-choice selection into MixLLM. It presents interesting engineering challenges, and we welcome further exploration and collaboration for those interested in addressing this problem."}, {"title": "Conclusion", "content": "We proposed MixLLM, a dynamic routing system that selects the most suitable LLM for each query by balancing response quality, cost, and latency. By enhancing query embeddings with tag knowledge and incorporating latency constraints, MixLLM effectively addresses key challenges in real-world LLM deployment. The system's adaptability, achieved through continual learning and independent prediction for each LLM, ensures efficiency as queries evolve and new models are introduced. Our results demonstrate that MixLLM optimizes resource usage while maintaining strong performance across varying budget levels."}, {"title": "Limitations", "content": "Although MixLLM presents strong performance in the experiments, some limitations are listed as follows. (1) The training process assumes access to refined feedback, including response quality and cost, which may not always be available in real world. Training-free methods could help, such as scaling laws (Ruan et al., 2024). (2) MixLLM may face challenges when routing queries from brand-new domains, commonly referred to as the out-of-domain (OOD) problem (see Section 4.7 for further details). (3) MixLLM faces challenges in practical scenarios requiring the selection of a single definitive answer from multiple LLM outputs, as discussed in Section 4.8. (4) While MixLLM considers hardware limitation through the latency constraint, more detailed dispatch strategies considering system information could further improve its practicality. (5) More complex routing tasks remain unexplored, such as hierarchical routing. This could involve first routing a query to a relevant domain, and then selecting the most suitable LLM within that domain. (6) MixLLM's performance needs to be tested in real-world applications to ensure its robustness beyond idealized environments."}]}