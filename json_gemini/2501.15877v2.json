{"title": "Boli: A dataset for understanding stuttering experience and analyzing stuttered speech", "authors": ["Ashita Batra", "Mannas Narang", "Neeraj Kumar Sharma", "Pradip K. Das"], "abstract": "There is a growing need for diverse, high-quality stuttered speech data, particularly in the context of Indian languages. This paper introduces Project Boli, a multi-lingual stuttered speech dataset designed to advance scientific understanding and technology development for individuals who stutter, particularly in India. The dataset constitutes (a) anonymized metadata (gender, age, country, mother tongue) and responses to a questionnaire about how stuttering affects their daily lives, (b) captures both read speech (using the Rainbow Passage) and spontaneous speech (through image description tasks) for each participant and (c) includes detailed annotations of five stutter types: blocks, prolongations, interjections, sound repetitions and word repetitions. We present a comprehensive analysis of the dataset, including the data collection procedure, experience summarization of people who stutter, severity assessment of stuttering events and technical validation of the collected data. The dataset is released as an open access to further speech technology development.", "sections": [{"title": "I. INTRODUCTION", "content": "Stuttering (or stammering) refers to atypical speech patterns characterized by significant uncontrollable pauses, filler words (e.g. umm, uhh, syllable or word repetitions, and other speech disturbances. Table I details these atypicalities, while Figure 1 presents spectrographic visualizations of some examples.\nStuttering affects approximately 12 million individuals in India [1]. Despite the recent proliferation of voice-based AI assistants like Cortana, Siri, Alexa and Google Assistant, these technologies are significantly challenged when processing and recognizing stuttered speech [2]-[5]. While several datasets have been created in this field, there remains a lack of balanced classes, word-level transcriptions and speaker information re- lated to stuttering experiences.\nExisting research on stutter analysis focuses primarily on the following datasets (tabulated in II): UCLASS [6]: A publicly available clinical English dataset from the University College of London (UCL), focusing on stuttered speech in children aged 7-17 years. It comprises 1 hour of audio recordings, with 138 recordings in release 1, containing annotations for 25 children (2 female, 23 male). Studies using this dataset have employed spectrograms as features and trained Bi-LSTMS for multi-class classification of different stutter types [13]. This work was extended in [9], replacing residual layers with squeeze & excitation (SE) layers and an attention mechanism.\nKSoF [7]: A German speech therapy-based stuttered speech dataset containing 214 recordings from 37 individuals. Sep- 28k [8]: The largest publicly available English stutter dataset, comprising 28,177 audio files (\u2248 3 seconds each) from natural conversations in podcasts. It provides file-level annotations for various stutter types: prolongation (PR), interjection (IN), sound repetition (SR), word repetition (WR) and block (B). FluencyBank [10]: An interview-based dataset of 32 individ- uals (\u2248 3.5 hours) with labeling similar to the Sep-28k dataset. Recent additions to stutter datasets include a Mandarin corpus [11], which is twice the size of Sep-28k, and a syllable-level stutter dataset in Kannada [12]. Additionally, LibriStutter [9] is a publicly available, synthetically generated English dataset derived from the LibriSpeech ASR corpus, containing time-aligned transcriptions from 20 hours of audio data (50 individuals: 23 male, 27 female).\nThis paper introduces the Boli dataset, documenting de- mographic information, experiential data, and read and spon- taneous speech recordings from individuals who stutter. The Hindi word Boli means a person's unique speaking style and hence is considered an appropriate name for this dataset. The dataset was collected through crowd-sourcing\u00b9 and is manually curated and validated. Section II documents the data collection and curation procedures. Section III presents the validation of the dataset for the detection of stuttering from speech recordings. SectionIV presents a summary of the experiential data collected using a questionnaire. Section V presents the conclusion."}, {"title": "II. DATA COLLECTION AND CURATION", "content": "Data are collected from participants in a crowd-sourcing manner, through a custom-designed website for this task. No person identification data is collected. A participant first fills the demographic information (namely, age, gender, country,\ncity and mother tongue), and then completes a questionnaire on experiential information in relation to stuttering. The questionnaire is composed of 25 multiple choice questions (MCQs). Subsequently, the participant's speech is collected as audio files corresponding to a few (fixed) sentences from the widely used Rainbow passage (in English and also translated to their respective mother tongues). As a follow-up to this recording, the participant also describes an image (in English and their mother tongue) and this is stored as spontaneous speech audio recording. Participants were encouraged to relax, take time and not produce artificial stutter. In total, as of 05- Sep-2024, 67 individuals who stutter (in the age group 17-48 years) participated in the data collection.\nManual annotation of stuttered speech recordings is a laborious task due to variations in speaking style and duration across stutter types, necessitating repeated listening of audio files. Consequently, most stuttered speech datasets provide only file- level stutter-type annotations. In contrast, the Boli dataset offers word-level annotations, including specific timestamps and stutter types. After manual analysis of 67 participants data, 28 were identified as containing stutter (25 male and 3 fe-"}, {"title": "III. STUTTER-TYPE CLASSIFICATION", "content": "To validate the utility of the audio dataset, we analyzed the performance of stutter-type classification by applying a variety of machine learning models on its audio files. We do not aim to achieve state-of-the-art results. Instead, we show the effect of class imbalance (and balancing) and performance comparison amongst traditional machine learning and deep learning-based classification models. Each audio file had a sampling rate of 16 kHz. As features, we used the mel-frequency cepstral coefficients (MFCCs). Specifically, 13 dimensional MFCCS were extracted which were averaged across frames and then fed as input vector, with 25 ms temporal window and 10 ms hop duration, resulting in \u2248 465 frames per file. We use the machine learning models: random forest (RF), support vector machine (SVM), LSTM, and BiLSTM. The classification methodology is shown in Figure 2. Owing to the small size of the Boli dataset, we have focused on performing cross- dataset evaluation, that is, training on Sep-28k and testing on the Boli dataset (English utterances only). The results are shown in Table V. Our experiments maintain consistency in class types and duration (\u2248 5 s) to help the model learn characteristics more efficiently. As the Sep-28k dataset is imbalanced in relation to stutter-types, we have used hybrid sampling before training to balance the classes by combining up-sampling and down-sampling. This takes the average of minority and majority samples, computed by dividing these samples by two. It is computed as follows:\n$N_{avg} = \\frac{N_1 + N_2}{2}$\nwhere N\u2081 is the number of samples in the minority class and N2 is the number of samples in the majority class. Balancing classes prevents the model bias towards the majority class, as shown in Table V. Comparative analysis between an imbalanced and balanced dataset has been demonstrated using different learning techniques, namely SVM, RF, LSTM and BiLSTM. Table V indicates that the RF best captures the characteristics of all stutter classes. Furthermore, statis- tical technique, i.e., Fl-score, is used to assess the model's effectiveness.\nIn another analysis, we compared two ASR models, namely Wav2Vec2.0 [14] and Whisper [15], to assess their effective- ness in capturing stuttered speech by calculating the word error rate (WER) for English audio recordings. We evaluated ASR for two settings: concatenated speech (grouping utter- ances from all participants), ASR evaluation, and speaker- wise speech evaluation (averaging the performance obtained across participants). The results are shown in Table VI. On manually verifying the ASR transcriptions, it was found that word repetition (WR) stutter type was well identified by both models."}, {"title": "IV. QUESTIONNAIRE DATA", "content": "From the questionnaire responses of 67 participants, it was observed that language plays a crucial role in stuttering. Most participants reported difficulties with stop consonants (and stop consonant clusters or blends) such as /p/, /t/, /k/,/str/,/br/ and /pl/ (see Figure 3). Many use speech therapy techniques to manage their stuttering and experience similar levels of stuttering in their mother tongue. In general, most have un- dergone speech therapy and feel hesitant to speak in front of crowds, often experiencing facial muscle tension. Stuttering is more pronounced when speaking to unfamiliar people, leading some to either minimize their stutter or avoid the situation, while only a few disclose it upfront. Singing, however, tends to reduce stuttering, which participants attribute to the rhythm and timing of music, as well as different breathing techniques that help alleviate pressure and anxiety. Additionally, many report being able to anticipate when they might stutter."}, {"title": "V. CONCLUSION", "content": "We present the Boli data set to facilitate the development of technology for people who stutter. It includes both read and spontaneous speech with manual word-level annotations. Every file contains a single stuttered word along with a few non-stuttered words. Additionally, we collected individuals who stutter life experiences through a questionnaire to cross- validate severity, intelligibility, and identifying common stut- tered words, syllables across individuals. Despite its small size, the data set includes various types of stutter in both read and spontaneous modes, which is rare in the Indian speech context. This can help improve the performance of the ASR model in multiple languages when the data set expands."}]}