{"title": "Continual Deep Reinforcement Learning to Prevent Catastrophic Forgetting in Jamming Mitigation", "authors": ["Kemal Davaslioglu", "Sastry Kompella", "Tugba Erpek", "Yalin E. Sagduyu"], "abstract": "Deep Reinforcement Learning (DRL) has been highly effective in learning from and adapting to RF environments and thus detecting and mitigating jamming effects to facilitate reliable wireless communications. However, traditional DRL methods are susceptible to catastrophic forgetting (namely forgetting old tasks when learning new ones), especially in dynamic wireless environments where jammer patterns change over time. This paper considers an anti-jamming system and addresses the challenge of catastrophic forgetting in DRL applied to jammer detection and mitigation. First, we demonstrate the impact of catastrophic forgetting in DRL when applied to jammer detection and mitigation tasks, where the network forgets previously learned jammer patterns while adapting to new ones. This catastrophic interference undermines the effectiveness of the system, particularly in scenarios where the environment is non-stationary. We present a method that enables the network to retain knowledge of old jammer patterns while learning to handle new ones. Our approach substantially reduces catastrophic forgetting, allowing the anti-jamming system to learn new tasks without compromising its ability to perform previously learned tasks effectively. Furthermore, we introduce a systematic methodology for sequentially learning tasks in the anti-jamming framework. By leveraging continual DRL techniques based on PackNet, we achieve superior anti-jamming performance compared to standard DRL methods. Our proposed approach not only addresses catastrophic forgetting but also enhances the adaptability and robustness of the system in dynamic jamming environments. We demonstrate the efficacy of our method in preserving knowledge of past jammer patterns, learning new tasks efficiently, and achieving superior anti-jamming performance compared to traditional DRL approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "The proliferation of wireless communication systems has been accompanied by an escalating threat landscape, particularly in the form of jamming attacks. Due to the open and shared nature of wireless medium, jamming, an intentional interference with wireless signals, poses a significant threat to the reliability, efficiency, and security of wireless networks. It disrupts the normal functioning of communication systems, leading to degraded service quality or complete denial of service. The dynamic and complex nature of wireless channels, coupled with the evolving sophistication of jamming techniques, necessitates advanced solutions for jammer detection and mitigation [1]-[3].\nMachine learning (ML) offers a promising avenue for addressing these challenges due to its ability to learn and model complex patterns and behaviors. Specifically, the application of ML for anti-jamming allows for an adaptive and intelligent approach to counteract jamming effects. This adaptability is crucial in dealing with the variability of channel conditions and the characteristics of jammers, which traditional rule-based systems may fail to effectively counter [4]\u2013[7].\nAmong various ML techniques, reinforcement learning (RL) stands out for its potential in anti-jamming. RL differs from other machine learning approaches by learning optimal behaviors through interactions with the environment, rather than relying on a pre-labeled dataset. This feature is particularly advantageous for anti-jamming strategies, where the environment (i.e., the wireless channel and the jammer's behavior) is dynamic and unpredictable. RL enables the system to learn and adapt to jamming effects on the fly, offering a robust solution for real-time detection and mitigation of jammers, even in the absence of supervised training data that may not be available for zero-day jammer threats [7]-[9].\nDeep reinforcement learning (DRL), combining deep learning and RL, further enhances the capability to deal with the complexities of anti-jamming [10]\u2013[13]. By leveraging deep neural networks, DRL can model highly complex strategies and patterns, outperforming traditional RL in environments with vast state and action spaces such as those encountered in wireless communication systems. This makes DRL an ideal candidate for developing sophisticated anti-jamming strategies that can adapt to a wide range of jamming scenarios.\nHowever, the application of DRL in dynamic environments like wireless communications is not without challenges. A significant issue is catastrophic forgetting (catastrophic interference), a phenomenon where ML models and RL mechanisms forget previously learned tasks upon learning new tasks [14]. Catastrophic forgetting has been demonstrated for signal classification in wireless systems [15], where a receiver may learn to classify new types of signals (such as new modulations) while forgetting old ones using elastic weight consolidation (EWC) [14]. Continual learning with EWC has been also applied to wireless systems in the context of synchronization in digital twins over wireless networks [16]. Catastrophic forgetting is particularly problematic in anti-jamming applications, where the ability to remember previously encountered jammer strategies and adapt to new jammer patterns is crucial for sustained system performance.\nTo address this critical challenge, we propose the use of progressive neural networks such as PackNet as a solution"}, {"title": "II. SYSTEM MODEL", "content": "We provide a succinct overview of the DRL agents utilized in this paper, namely, the Deep Q-network (DQN) [19] and Soft Actor-Critic (SAC) [20] algorithms. The DQN combines Q-learning with deep neural networks to approximate and optimize the Q-values. DQN enables agents to make decisions in environments with high-dimensional state spaces. The goal of DRL agent is to interact with the environment and select actions in a way that maximizes its future rewards. The future rewards of the agent are discounted by a factor of \u03b3 per time-stamp where \u03b3\u2208 (0,1] and the future discounted return at time t, $R_{t_0}$, is defined as\n$R_{t_0}=\\sum_{t'=t_0}^{\\infty} \\gamma^{t'-t_0}r_{t'},$ (1)\nwhere $r_t$ is the reward at time t. The optimal action-value function, Q*(s, a) can be defined as the maximum expected return achievable by following any strategy after seeing some sequence of states s and taking some actions a. The stochastic policy \u03c0 learns to map sequences to actions or distributions over actions, which can be represented as\n$Q^*(s, a) = \\underset{\\pi}{max} E [R_t | s_t = s, a_t = \\alpha, \\pi],$ (2)\nwhere st and at are the state visited and the action taken at time t, respectively. The set of all possible states is shown by S, and at is selected from some set of possible actions A.\nSAC introduces a maximum entropy framework, promoting exploration and enabling better handling of stochastic environments by optimizing both the policy and the value function. The SAC framework extends the standard reinforcement learning objective of maximizing the return by simultaneously maximizing the entropy of the agent's policy, which can be expressed as\n$\\pi^* = \\underset{\\pi}{arg max} E_\\pi [\\sum_{t=0}^{\\infty} \\gamma^t (r(s_t, a_t) + \\alpha H (\\pi (\\cdot | S_t)))],$ (3)"}, {"title": "A. Scenarios", "content": "For jamming detection and mitigation, we define different scenarios, each presenting increasing levels of difficulty.\nScenario 1: In the first scenario, we define Environment 1 (Env 1) that considers a linear sweeping jamming pattern where the jammer selects one of N channels and transmits with full power only on this channel. In the next time instant, the jammer switches to the next channel and again transmits at full power only in this channel. This process is repeated until all N channels are jammed in N time instants. The goal of the agent is to learn the jamming pattern and select a next time slot that is interference-free to mitigate the adverse effects of interference. For this scenario, we identify the following states, actions, rewards, and game over conditions:\nStates: The channel occupancy of all N channels in the current time slot, i.e., wideband sensing.\nAction: Agent selects the channel to be used for the next time slot.\nReward: The agent receives a +1 reward if there is no collision with interferer; otherwise, -1 penalty is incurred.\nGame over condition: Each epoch runs until the agent makes three collisions in the last ten time slots or maximum number of steps is reached.\nScenario 2: In this scenario, the jamming behavior changes from the one in Env 1 to a new jamming pattern where the jammer uses a non-uniform power allocation across different number of channels that are selected randomly. We refer to this environment as Env 2. In this scenario, as the jammer can interfere with multiple channels, we need to update the action, reward, and game over condition definitions. For the actions, the agent can select a channel to transmit and a Modulation Coding Scheme (MCS) to adapt its waveform. Also, we update the reward definition from a simple \u00b11 to a more elaborate spectral efficiency, that is typically represented in bits/sec/Hz. As reinforcement learning environments typically use positive rewards to encourage behavior and negative ones for the negative feedback, we apply standard normalization the"}, {"title": "III. CONTINUAL LEARNING FRAMEWORK", "content": "In this section, we describe the parameter isolation approach that we employed for anti-jamming. This approach involves several key steps:\nFirst, for task n, Tn, we train the network while keeping the parameters $\u03b8_{1:n\u22121}$ associated with previous tasks frozen. Following the training phase, we proceed to pruning, wherein a fraction of the network weights are set to zero. This is accomplished by assigning a binary mask to allocate a subset of parameters. Rather than randomly selecting weights for pruning, we sort the weights in each layer (convolutional and/or fully connected) based on their absolute values, indicating their importance. We then discard either the lowest 50% or 75%, following the suggested numbers in [17], [22].\nTo maintain simplicity, we adopt a one-shot pruning method, although incremental pruning has shown promise for improved results [23]. Notably, extensive pruning, especially with high pruning ratios, can lead to an immediate performance drop due to significant changes in network connectivity [17]. To mitigate this, we perform a small number of retraining steps where we retrain the most crucial parameters \u03b8n while keeping the parameters of previous tasks $\u03b8_{1:n\u22121}$ masked out. This ensures that the performance on task Tn is preserved during inference, utilizing only unmasked parameters $M_{1:n}$.\nAfter a round of pruning and re-training, we achieve a network with sparse filters and minimal performance degradation on task Tn. Importantly, during the pruning step, we only remove weights associated with the current task, leaving those from previous tasks untouched. This ensures that introducing a new task does not compromise the performance of prior tasks."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "In our numerical evaluations, we used the ns-3 gym environment. Ns-3 [24] is an open-source discrete-event simulator for network systems. For many RL applications, OpenAI's Gym [25] is used to support the development of RL agents for a variety of applications ranging from playing video games like Pong or Pinball to robotics applications. Due to the Gym's easy interface, it is commonly used by different ML frameworks. The ns-3 simulator framework is integrated with the Gym using the ns3-gym environment [26]. We have incorporated different jamming strategies by expanding the sweeping jammer code that was proposed in the ns3-gym library and making our adaptive jamming pattern module to develop jammers with different jamming behaviors.\nIn Scenario 1, we train two DRL agents using the DQN and SAC approaches described in Section II. We measure the average reward obtained by the agents over 450 epochs. We repeat this process five times with different seeds to ensure robustness and reliability of the results. Monitoring the average reward provides a general indication of the performance of the DRL agents. Figs. 1 and 2 demonstrate the mean reward of DQN and SAC agents at different epochs averaged over five seeds, respectively. These results have demonstrated that both agents in Scenario 1 are very successful, mostly due to the fact that the jamming pattern is very predictable. For example, the DQN agent quickly learns the jamming pattern and achieves more than 95% success probability in selecting the channel without interference. SAC agent is able to achieve an average reward with a 100% success rate which indicates it was able to select a channel without any collision over all five runs. Furthermore, it took around only three epochs for the SAC agent to fully learn the jamming pattern and avoid the interference."}, {"title": "B. Continual Learning Scenarios", "content": "In the continual learning scenarios, we adopt the DQN agent due to its simplicity and similar performance. To measure the continual learning capabilities, we consider three different baseline initializations which are described as follows:\n1) No pretraining: DRL agent resets its coefficients, empties its buffer, and starts to learn the new environment with new observations.\n2) Pretrained: DRL agent uses the weights and biases learned in previous task and updates the network with new observations in the new environment.\n3) PackNet: DRL agent uses the weights and biases learned in previous task, applies pruning, and finetunes the network with limited observations.\nAll three DRL agents have the exact same neural network architecture for a fair assessment, and they are trained using the same number of epochs, learning rates, and set of seeds for random number generation. We implemented a fully connected feedforward neural network with 6 layers, where the hidden layers have 256 neurons in each layer and Rectifying Linear Unit (ReLU) activations are used in between layers. The AdamW optimizer [27] is used with a learning rate of 0.01.\n1) Scenario 2: In this scenario, No pretraining DRL agent starts fresh to learn the Env 2. Pretrained DRL agent uses the parameters learned in Env 1 and adapts to Env 2 by interacting with the new environment. PackNet DRL agent applies the continual learning framework described in Section III with pruning. In this scenario, we can see that all agents perform similarly and rapidly learn the new jamming behavior in Env 2. PackNet DRL Agent slightly provides better performance in Env 2 compared to the other two agents.\n2) Scenario 3A: In this scenario, the jammer in Scenario 2 changes its behavior back to the one in Env 1. For all three DRL agents, the network parameters at the end of Scenario 2 are frozen when we evaluate them in this scenario. Ideally, the Pretrained DQN agent has learned the"}, {"title": "C. Scenario 3B", "content": "In this scenario, the jammer adapts a new behavior, labeled as Env 3, where it integrates its jamming tactics from both Env 1 and Env 2. This scenario is designed to highlight the sequential task learning capabilities of the agents. We observe that all the agents achieve similar performance and converge to the same level given enough training epochs. However, the PackNet method approaches this point 20-40 epochs faster than both methods."}, {"title": "V. CONCLUSION", "content": "In this paper, we study the challenge of catastrophic forgetting for anti-jamming systems, where DRL is applied to jammer detection and mitigation tasks within dynamic RF environments. Traditional DRL methods struggle with retaining knowledge of previously learned jammer patterns while adapting to new ones, thereby compromising system effectiveness, particularly in non-stationary environments. To address this issue, we studied the parameter isolation method to mitigate catastrophic forgetting. Our approach enables the network to preserve knowledge of old jammer patterns while effectively learning to handle new ones, thus significantly reducing interference and enhancing system performance. Additionally, we introduced a systematic approach for sequentially learning tasks in the anti-jamming framework, leveraging continual DRL techniques based on PackNet. We demonstrated the effectiveness of our approach in preserving past knowledge, efficiently learning new tasks, and achieving superior anti-jamming performance compared to traditional DRL approaches. This approach contributes to the advancement of anti-jamming systems by improving adaptability and robustness of wireless communication systems in dynamic jamming environments."}]}