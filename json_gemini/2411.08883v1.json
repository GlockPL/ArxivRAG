{"title": "KisanQRS: A Deep Learning-based Automated Query-Response System for Agricultural Decision-Making", "authors": ["Mohammad Zia Ur Rehman", "Devraj Raghuvanshi", "Nagendra Kumar"], "abstract": "Delivering prompt information and guidance to farmers is critical in agricultural decision-making. Farmers' helpline centres are heavily reliant on the expertise and availability of call centre agents, leading to inconsistent quality and delayed responses. To this end, this article presents Kisan Query Response System (KisanQRS), a Deep Learning-based robust query-response framework for the agriculture sector. KisanQRS integrates semantic and lexical similarities of farmers' queries and employs a rapid threshold-based clustering method. The clustering algorithm is based on a linear search technique to iterate through all queries and organize them into clusters according to their similarity. For query mapping, LSTM is found to be the optimal method. Our proposed answer retrieval method clusters candidate answers for a crop, ranks these answer clusters based on the number of answers in a cluster, and selects the leader of each cluster. The dataset used in our analysis consists of a subset of 34 million call logs from the Kisan Call Centre (KCC), operated under the Government of India. We evaluated the performance of the query mapping module on the data of five major states of India with 3,00,000 samples and the quantifiable outcomes demonstrate that KisanQRS significantly outperforms traditional techniques by achieving 96.58% top F1-score for a state. The answer retrieval module is evaluated on 10,000 samples and it achieves a competitive NDCG score of 96.20%. KisanQRS is useful in enabling farmers to make informed decisions about their farming practices by providing quick and pertinent responses to their queries.", "sections": [{"title": "1. Introduction", "content": "Developing nations are working towards utilizing advancements in technology to gain insights into the challenges faced by farmers. The agricultural sector faces numerous challenges, such as unpredictable weather conditions, poor infrastructure, and pest infestation which often lead to poor agricultural yield for farmers (Luo et al., 2023). These challenges are compounded by the lack of timely access to information which further hinders the sector's growth. One potential strategy to overcome these challenges is to furnish timely information and guidance to farmers. Keeping this in mind, policymakers in India came up with the idea of Kisan Call Centres (KCCs). These farmers' helpline centres were established by the Government of India to provide agriculture-related information and advisory services to farmers. These centres receive thousands of queries every day from farmers seeking guidance on various issues related to farming. The KCCs have become a crucial source of information for farmers, providing them with advice on crop cultivation, pest, and disease management, market prices, and government schemes (Chachra et al., 2020; Raja and Naika, 2022).\nAlthough the KCCs have been successful in providing information to farmers, the effectiveness of query resolution relies heavily on the expertise and accessibility of call centre agents. This can result in delays in responding to queries and a lack of consistency in the quality of responses. Additionally, the KCCs receive a large volume of queries, which can be overwhelming for call centre agents. An automated query-response system can tackle these challenges by providing quick and accurate responses to farmers' queries using Natural Language Processing (NLP) techniques. The massive call logs from KCC can serve as a good knowledge base for such a query-response system. Several approaches have been proposed to develop an automated system that can provide answers to farmers' queries by training on the KCC dataset\u00b9. However, none of these approaches have utilized state-of-the-art artificial intelligence-based techniques to effectively solve this problem. Furthermore, existing systems that utilize traditional techniques such as Bag of Words (BoW), and Term Frequency-Inverse Document Frequency (TF-IDF) to calculate cosine similarities between sentence vectors fail to account for the semantic meaning of sentences. A few approaches collected agriculture-related information through social media such as Twitter and Facebook to solve farming problems (Zipper, 2018)."}, {"title": "1.1. Knowledge-based Methods", "content": "These methods use structured knowledge representation frameworks, such as ontologies, to represent the relationships between concepts in a domain. Knowledge-based methods rely on domain-specific knowledge bases and reasoning mechanisms to generate answers to questions. Saxena et al. (Saxena et al., 2020) uses knowledge base embeddings to encode information about the entities and relations in the knowledge graph. The paper suggests that incorporating knowledge base embeddings can significantly improve the performance of multi-hop question answering over knowledge graphs. Devi et al. (Devi and Dua, 2017) presents a question-answering system that is designed specifically for the agriculture domain. This paper proposes a system that uses ontologies to represent the knowledge required to answer questions in the agriculture domain. The system is designed to handle both factoid and descriptive questions. Deepa et al. (Deepa and Vigneshwari, 2022) proposes a method for automatically constructing an ontology for the agriculture domain using part of speech tags and the Jaccard similarity method."}, {"title": "1.2. Machine Learning and Deep Learning-based Methods", "content": "ML methods use algorithms and statistical models to learn patterns and relationships in large datasets and use this knowledge to generate answers to questions. Sarrouti et al. (Sarrouti and El Alaoui, 2017) and Yen et al. (Yen et al., 2013) propose a Machine Learning-based approach for question-answering that uses a variety of features such as question length, presence of specific keywords, and syntactic features to classify the question type. Zin Oo et al. (Oo et al., 2021) proposes a two-step approach for classifying questions that involve defining the tag of each word in the question and then classifying the tags using various classification algorithms such as K-NN, Naive Bayes, Decision Tree, Random Forest, SVM, and XGBoost. The limitation of the aforementioned works is that they utilize traditional NLP techniques to extract text features for question categorization. The traditional approaches offer query features that lack contextual relevance. To this end, the KisanQRS utilizes a transformer-based encoder to generate contextual features and then combines their pairwise cosine similarity with lexical similarity to assign cluster labels to queries resulting in high-quality query clusters.\nDL techniques are extensively utilized for a diverse range of NLP tasks, including text categorization (Minaee et al., 2021), identification of named entities (Li et al., 2020), and generation of word embeddings (Wu et al., 2022; Wang et al., 2020). Deep Learning-based question-answering (QA) methods have shown great promise in recent years and have been used to build various question-answering systems, including those used in search engines, customer support chatbots, and virtual assistants (Hao et al., 2022). McCreery et al. (McCreery et al., 2020) perform double finetuning of BERT (Devlin et al., 2019) for the objective of identifying medical question similarity. This approach serves as an effective intermediate task, but the substantial computational overhead of BERT renders it inappropriate for performing semantic similarity searches and unsupervised tasks such as clustering. Sakata et al. (Sakata et al., 2019) proposes an FAQ retrieval system that utilizes both query-question similarity and BERT-based query-answer relevance. For similarity computation between a user query and a question from the dataset, they use the TSUBAKI (Shinzato et al., 2012) method, which relies more heavily on the syntactic structure of the text, which is also not as effective at capturing contextual relationships between words in a query as recent transformer-based models. Arora et al. (Arora et al., 2020) gives an approach for a conversational chatbot for agricultural queries. They use a sequence-to-sequence model to build the chatbot using LSTM and GRU. Bi et al. (Bi et al., 2021), Chen et al. (Chen et al., 2022), and Zhu et al. (Zhu et al., 2023) explore the utility of attention mechanism with DL techniques for the question-answering system in different domains. Kim et al. (Kim et al.,"}, {"title": "1. Problem 1: Query Clustering", "content": "Our first objective is to cluster the queries in Q into semantically as well as lexically similar groups, forming the set of query clusters $C_{query} = \\{\\alpha_k\\}_{k=1}^M$. Here, \u03b1k represents the $k^{th}$ query cluster which is a set of similar queries, and M represents the total number of query clusters formed. It is necessary to compare different clustering techniques in order to determine which one is providing superior outcomes."}, {"title": "2. Problem 2: Query Mapping", "content": "Our second objective is to map $q_{user}$ to the most relevant cluster \u03b1k such that the queries in \u03b1k are semantically similar to $q_{user}$. This problem can be approached as a supervised learning task, where the model is trained on a labeled dataset of queries and their corresponding"}, {"title": "3. Problem 3: Answer Retrieval", "content": "Once $q_{user}$ has been mapped to \u03b1k, our third objective is to retrieve the top-K answers from $\u03b2_k$ that are most relevant to $q_{user}$ based on a specific ranking criterion, where $\u03b2_k$ is the set of answers for all queries $q_i$ such that $q_i \\in \u03b1_k$. These retrieved answers should be for the crop mentioned in $q_{user}$, if one is specified. This objective requires us to develop a method for ranking the answers based on their relevance to the query and the crop."}, {"title": "2. Materials and Methods", "content": "This section outlines the proposed methodology and experimental details of KisanQRS. The methodology consists of four phases: Data Preprocessing, Query Clustering, Model Training, and Answer Retrieval and Ranking. Subsequently, we discuss the experimental details which include the hyperparameter and model details, evaluation metrics for different modules of KisanQRS, and the description of the dataset."}, {"title": "2.1. Methodology", "content": "A general end-to-end schematic representation of KisanQRS is shown in Figure-1. First, the queries are grouped into clusters according to their semantic and token similarities by the query clustering module. Next, a query mapping module is trained to map any new queries to the most fitting cluster. Finally, potential responses are grouped into clusters and the top-K most pertinent answers are selected by the answer retrieval and ranking module to serve as the conclusive answers to the farmer's query. A detailed discussion of the phases and architectures of different modules is given in the subsequent sections."}, {"title": "2.1.1. Data Preprocessing", "content": "This subsection discusses the data preprocessing steps that are essential for effective feature extraction. The section is divided into two main parts: data cleaning and data transformation.\n1. Data Cleaning: In this step, entries that contain empty queries $q_i$ or answers $a_i$ are removed from the dataset. Then, all instances of special characters including commas, hyphens, semicolons, and extra white spaces, are purged$\\forall q_i \\in Q$. Lastly, any duplicate rows present in the dataset are eliminated."}, {"title": "2. Data Transformation", "content": "Data transformation is the process of modifying the original data to make it more suitable for analysis or modeling. Queries that ask for real-time data, such as market rates or weather conditions are removed from Q as it is not possible to determine their answers based on past data. In addition, if any query $q_i \\in Q$ contains a token $c \\in Crops$, then c is removed from $q_i$ to generate similar feature vectors for similar queries, resulting in the formation of better clusters. For example, if we have queries such as fertilizer dose for onion and fertilizer dose for tomato, the tokens onion and tomato are removed from these queries, respectively, resulting in both queries being reduced to fertilizer dose after preprocessing. This increases the likelihood of both queries being grouped into the same cluster by the clustering technique, resulting in the formation of a cluster $\u03b1_k$ containing queries for the fertilizer dose of multiple crops. Thus, when a user asks a query about the fertilizer dose for a specific crop, $q_{user}$ can be mapped to $\u03b1_k$ first, and all queries related to the crop mentioned in $q_i$ can be filtered from $\u03b1_k$. This approach prevents the formation of redundant clusters, which could result in lower performance of the supervised learning method due to multiple clusters addressing the same problem for different crops. We develop two versions of Q, $Q_{jaccard}$ and $Q_{sbert}$. In $Q_{jaccard}$, we remove stopwords and apply stemming. However, for $Q_{sbert}$, we retain the complete context as transformer-based embedding models require it to generate accurate embeddings."}, {"title": "2.1.2. Query Clustering", "content": "Numerous entries in the dataset have semantically and lexically similar queries, so it is imperative to cluster such queries. Clusters generated in this module serve as the target labels for the DL model for query mapping. The query clustering module has two main components: similarity"}, {"title": "1. Similarity Matrix Generation", "content": "The similarity between two queries is measured as the weighted sum of cosine similarity between their contextual embeddings and token-wise similarity using the Jaccard Index (Jaccard, 1901), as shown in Equation-1.\n$\\text{sim}(q_i, q_j) = \\lambda (\\text{sim}_{emb}) + (1 - \\lambda) \\text{sim}_{token}$ (1)\nHere, $\\text{sim}(q_i, q_j)$ denotes the similarity score between query $q_i$ and query $q_j$, $\\text{sim}_{emb}$ denotes cosine similarity between the contextual embeddings of query $q_i$ and $q_j$, $\\text{sim}_{token}$ denotes token-wise similarity and \u03bb denotes the weight given to embedding similarity. We conduct experiments using different values of \u03bb, ranging from 0 to 1 with increments of 0.01. Empirical analysis indicates that the best results are achieved when \u03bb is between 0.78 and 0.81. These results are the same for any value between 0.78 to 0.81. So we select 0.8"}, {"title": "2. Query Cluster Formation", "content": "We implement a threshold-based clustering technique to group queries into clusters. Algorithm-1 represents an implementation of our threshold-based"}, {"title": "2.1.3. Model Training", "content": "A DL model is trained with the supervised multi-class classification objective for query mapping. Query mapping is the process of assigning an accurate cluster label to a farmer's query. In this task, the query embeddings extracted using a transformer encoder serve as the input, while the cluster labels act as the target. We evaluate multiple models to identify the most suitable architecture for our task. Among the different options considered, the LSTM model consistently demonstrates superior performance and exhibits the ability to effectively capture temporal dependencies in the data. As a result, we select LSTM for the query mapping module. The detailed comparison and performance analysis of other models explored during the experimentation can be found in Section-3.2.1."}, {"title": "2.1.4. Answer Retrieval and Ranking", "content": "This task is divided into two submodules: query mapping module, and answer retrieval module as shown in Figure-3."}, {"title": "(a) Query Mapping", "content": "The user query $q_{user}$ is first preprocessed, then the contextual features are extracted from $q_{user}$ which serve as an input to the LSTM model for inference."}, {"title": "i. Preprocessing User Query", "content": "In this phase, special characters are removed from q and crop detection is performed, in which let's say a crop $c \\in Crops$ is detected i.e. $\\exists c \\in N\\text{-grams}(q_{user}) : c \\in Crops$, which is also removed from $q_{user}$ due to the reason explained in Section-2.1.1."}, {"title": "ii. Feature Extraction and Inference", "content": "From the preprocessed query, the contextual features are extracted using the SBERT encoder. The features are then served as an input to the LSTM model for inference. Let's say that $q_{user}$ is mapped to cluster $\u03b1_k = \\{q_0^k, q_1^k, q_2^k, ..., q_{n-1}^k\\}$ by the model. The query clusters are formed such that each query $q_i^k$ has a similar meaning as well as a lexical structure similar to the rest of the queries in $\u03b1_k$, which means if $q_{user}$ has been mapped to $\u03b1_k$, then each answer $a_i^k$ will be a correct answer for $q_{user}$ such that $crop(a_i^k) = c$.\nHere, $q_i^k$ denotes a query of cluster $\u03b1_k$, where x is used to index the queries or their corresponding answers and n is the total number of queries in $\u03b1_k$. For each query $q_i^k$, the corresponding answer is denoted as $a_i^k$ and $\u03b2_k$ denotes the set $\\{a_0^k, a_1^k, a_2^k, ..., a_{n-1}^k\\}$."}, {"title": "(b) Answer Retrieval", "content": "Given a user query $q_{user}$ and the cluster to which it is mapped ($\u03b1_k$), the next task is to retrieve top-K answers from the set $\u03b2_k$. This task is divided into three phases which are elaborated below:"}, {"title": "i. Candidate Answer Selection", "content": "Once the query has been mapped to the cluster \u03b1k, some answers $a_i^k$ for the queries in \u03b1k might not be for the crop c that the user is querying for. Out of all these candidate answers A in the predicted cluster \u03b1k, only the candidate answers containing the crop c (which is detected during the preprocessing stage) are selected which form the set $B_k$.\nIf the user has not mentioned any crop in the query, then the filtering on the basis of the crop is not performed and all the answers in $\u03b2_k$ are taken into account, i.e. $B = \u03b2_k$ for this case."}, {"title": "ii. Similarity Matrix Generation and Clustering", "content": "Within the set $\u03b2_k$, certain answers exhibit strong semantic similarity with each other. So, in order to extract a single answer from these groups of similar answers, we first perform clustering $\\forall a \\in B$ to obtain a set of answer clusters $C_k = \\{Y_x\\}_{x=1}^P$, where P denotes the total number of answer clusters formed. For making answer clusters, we first generate the similarity matrix for all the answers $a \\in \u03b2$. The similarity between two answers a\u2081 and a2 is measured as the average of character similarity and token-wise similarity between a\u2081 and a2 as shown in Equation-4.\n$\\text{sim}(a_1, a_2) = \\frac{\\text{sim}_{char}(a_1, a_2) + \\text{sim}_{token}(a_1, a_2)}{2}$ (4)\nHere, $\\text{sim}_{char}$ denotes the character-wise similarity between a\u2081 and a2 and $\\text{sim}_{token}$"}, {"title": "iii. Ranking top-K Answers", "content": "To select the top-K answers from \u03b2, we first rank the answer clusters where the ranking of answer cluster $Y_i$ is determined by its size $|Y_i|$ in comparison to other answer clusters. The larger the size, the higher the rank of $Y_i$. Subsequently, from each answer cluster, a leader answer is chosen. As the answers in $Y_i$ are expected to be similar due to being part of the same cluster, the leader $L_i$ of $Y_i$ is chosen based on the answer with the most keywords coverage. The leader conveys the most comprehensive information among all the answers in $Y_i$. Finally, the top-K leaders are returned based on the ranking of answer clusters. Algorithm-2 gives steps for answer retrieval and ranking."}, {"title": "2.2. Experimental Details", "content": "In this section, we discuss the evaluation metrics utilized for assessing the performance of KisanQRS, along with comprehensive information regarding the dataset employed in our experiments."}, {"title": "2.2.1. Evaluation Metrics", "content": "In this section, we discuss the evaluation metrics for different modules of KisanQRS: Query Clustering, Query Mapping, and Answer Retrieval and Ranking."}, {"title": "(a) Evaluation Metrics for Clustering Method", "content": "We use Silhouette Score, Calinski-Harabasz (CH) Index, and Davies-Bouldin (DB) Index to evaluate the performance of our clustering method. Their brief description is given as follows:"}, {"title": "i. Silhouette Score", "content": "This metric is utilized in cluster analysis for assessing the quality of the formed clusters. Consider $a_i$ as the mean distance between the $i^{th}$ data point and all other data points that belong to the same cluster. Similarly, let $b_i$ refer to the mean distance between the $i^{th}$ data point and all the data points present in the closest cluster (i.e., the cluster with the minimum average distance to the given data point), then Silhouette score, $s_i$, for a single data point is as follows:\n$s_i = \\frac{b_i - a_i}{\\text{max}\\{a_i, b_i\\}}$ (5)\nIf the Silhouette score is high, it implies that a data point fits well in its assigned cluster but not in neighboring clusters. Conversely, a low score indicates the opposite, that is, a data point may be a better fit for a neighboring cluster than its assigned cluster. The overall Silhouette Score for the clustering solution is calculated as the average Silhouette score of all data points in all clusters."}, {"title": "ii. Calinski-Harabasz Index", "content": "It measures the ratio of between-cluster dispersion and within-cluster dispersion, where higher scores indicate better clustering results. The mathematical formula for CH Index is:\n$CH(n) = \\frac{\\text{Tr}(B)}{\\text{Tr}(W)} \\times \\frac{N-n}{n-1}$ (6)\nHere, n represents the number of clusters, N represents the total number of data points, W represents the within-cluster dispersion matrix (also known as the sum of squared errors within each cluster), and B represents the between-cluster dispersion matrix (also known as the sum of squared errors between each cluster). Tr denotes the trace of a matrix, which is the sum of its diagonal elements."}, {"title": "iii. Davies-Bouldin Index", "content": "It is used to evaluate clustering quality by calculating the average similarity between each cluster and the cluster that is most similar to it. Its objective is to minimize the intra-cluster distance while maximizing the inter-cluster distance. The mathematical formula for the Davies-Bouldin index can be expressed as:\n$DB = \\frac{1}{n} \\sum_{i=1}^{n} \\underset{j \\neq i}{\\text{max}}(\\frac{s_i + s_j}{d(c_i, c_j)})$ (7)\nHere, n refers to the number of clusters, $c_i$ represents the centroid of the $i^{th}$ cluster, $s_i$ denotes the average distance between each point in the $i^{th}$ cluster and its centroid, and $d(c_i, c_j)$ is the distance between the centroids of the $i^{th}$ and $j^{th}$ clusters. The Davies-Bouldin index ranges from 0 to infinity, where a score of 0 indicates perfect clustering, while a higher score indicates poorer clustering."}, {"title": "(b) Evaluation Metrics for Query Mapping Module", "content": "Two of sets of investigations are conducted for the query mapping module. A detailed discussion of these experiments is presented in the next section, i.e. Section-3.2. In this section, we discuss the different evaluation metrics used for the query mapping module. Precision, Recall, F1-score, and Accuracy are used as evaluation metrics for the query mapping module.\nIf TP, TN, FP, and FN denote the true positives, true negatives, false positives, and false negatives respectively, then:\n$\\text{Precision} = \\frac{TP}{TP + FP}$ (8)\n$\\text{Recall} = \\frac{TP}{TP + FN}$ (9)\n$\\text{F1 score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ (10)"}, {"title": "Weighted recall and accuracy", "content": "As we have a multi-class classification problem, we use the weighted average of precision, recall, and F1-score for evaluation by computing the average of binary metrics weighted by the number of samples of each class as shown in Equation-11.\n$\\text{weighted_average}(X_l) = \\frac{1}{\\sum_{l=1}^{L} n_l} \\sum_{l=1}^{L} n_l X_l$ (11)\nHere $X_l$ denotes an evaluation metric for class l, $n_l$ denotes the number of samples for class l, L is the set of classes, and l is a single class such that l \u2208 L.\nAccuracy is calculated by dividing the number of samples that are correctly classified by the total number of samples in the dataset. This metric provides an overall measure of how well the model is able to predict the correct class for each instance.\n$\\text{Accuracy} = \\frac{\\text{Number of correctly classified samples}}{\\text{Total number of samples}}$ (12)\nWeighted recall and accuracy used for KisanQRS come out to be equal mathematically, which is also visible in the results. We choose to keep both in the results for clarity."}, {"title": "(c) Evaluation Metrics for Answer Retrieval and Ranking", "content": "We use the Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP) metrics to evaluate the performance of our retrieval system. Their brief description is given as follows:"}, {"title": "i. NDCG", "content": "NDCG is a popular evaluation metric because it takes into account both the relevance of the items returned and their order in the ranking. This makes it a suitable metric for comparing the performance of different retrieval systems.\n$\\text{DCG} = \\sum_{i=1}^{n} \\frac{2^{\\text{rel}_i} - 1}{\\log_2(i + 1)}$ (13)\n$\\text{IDCG} = \\sum_{j=1}^{n} \\frac{2^{\\text{rel}_j} - 1}{\\log_2(j + 1)}$ (14)\n$\\text{NDCG} = \\frac{\\text{DCG}}{\\text{IDCG}}$ (15)\nHere, $rel_i$ is the relevance score of the $i^{th}$ item in the ranking, i is the position of the $i^{th}$ item in the ranking, starting from 1, n is the total number of items in the ranking, DCG is the Discounted Cumulative Gain, which is the sum of the discounted relevance scores of the items in the ranking, $rel_j$ is the relevance score of the $j^{th}$ item in the ideal ranking, j is the position of the $j^{th}$ item in the ideal ranking, starting from 1, IDCG is the Ideal Discounted Cumulative Gain, which is the maximum possible DCG value for a given ranking."}, {"title": "ii. MAP", "content": "MAP is used in information retrieval to evaluate the performance of a model in retrieving relevant items from a large dataset. It provides an overall estimate of a model's accuracy at ranking a set of relevant items higher in the"}, {"title": "MAP", "content": "list of predicted items than the non-relevant items. The formula for MAP is given as follows:\n$\\text{MAP} = \\frac{1}{N} \\sum_{i=1}^{N} AP_i$ (16)\nwhere N is the number of relevant items in the ground truth dataset, and $AP_i$ is the average precision of the $i^{th}$ relevant item. And the formula for average precision (AP) is:\n$AP_i = \\frac{1}{R_i} \\sum_{k=1}^{R_i} P(k) \\cdot [rel(k)=1]$ (17)\nwhere, $R_i$ is the number of retrieved items for the $i^{th}$ relevant item, P(k) is the precision at the kth retrieval, rel(k) is the relevance of the item retrieved at the $k^{th}$ position, with rel(k) = 1 for relevant items and rel(k) = 0 for non-relevant items."}, {"title": "2.2.2. Dataset", "content": "The dataset used for the evaluation of KisanQRS is collected from the KCC call logs repository. This repository contains records from 2006 to till date, totaling 34 million. A text record is kept of every query call made by farmers, including both, the information they request (query) and the information that is provided to them (response). For our experiments, we take data from QueryText, KccAns, and Crop attributes from 2018 to 2020 with the sample sizes mentioned in Table-4."}, {"title": "3. Results of the Study", "content": "In this section, we present the results of all three modules of KisanQRS: Clustering module, Query Mapping module, and Answer Retrieval module."}, {"title": "3.1. Clustering Results", "content": "In this subsection, we discuss the performance of different clustering algorithms on the basis of three evaluation metrics described in the previous section followed by a discussion about the runtime of these algorithms. Results are shown in Table-5, where \u2191 for a metric represents that a higher score is better, and \u2193 denotes the lower, the better. The clustering algorithm of KisanQRS achieves a Silhouette score of 0.82 outperforming K-Means, Agglomerative, and DBSCAN, which represents that the query clusters are well-defined, with high intra-cluster similarity and low inter-cluster similarity. A high CH-index score of 4125.52 implies that the clusters are compact and well-separated, and a lower DB-Index score of 0.50 indicates less overlap and better-defined clusters.\nThe improved clustering performance of our method is due to the fact that it considers both semantic and token-wise similarity between queries to form corresponding clusters without any constraints on the total number of clusters. K-Means partitions the dataset into a pre-defined number of clusters. It assumes that the data can be represented as spherical clusters with similar variances but it does not consider density or connectivity among data points. Hence, it struggles with clusters of varying sizes and shapes. As for DBSCAN, it is not suited for high-dimensional data as its clustering becomes less effective in higher-dimensional spaces. Agglomerative clustering is sensitive to the choice of distance metric and linkage criterion used to measure similarity between clusters. Different combinations of metrics and criteria lead to different clustering results. Overall the clustering method used for KisanQRS makes well-quality clusters that help to train the query-mapping module."}, {"title": "3.2. Query Mapping Results", "content": "First, we conduct experiments to select the most suitable DL or ML model for the query mapping module. Subsequently, we evaluate the performance of our query mapping module by comparing its performance with other techniques."}, {"title": "3.2.1. Model Selection Results for Query Mapping", "content": "The experiments are conducted for five models out of which three are DL models: Multi-layer Perceptron (MLP), LSTM and Gated Recurrent Unit (GRU), and two are ML models: Support Vector Machine (SVM) and Logistic Regression (LR). Each of these models is tested for different input feature vectors, the set of which includes SBERT, Word2vec, GloVe, BoW, and TF-IDF. The performance of the models is assessed by employing accuracy, weighted precision, weighted recall, and weighted F1-score, which are widely used metrics for evaluating the performance of multi-class classification models. The results for these metrics are presented in Table-6 and show that LSTM with SBERT is performing better as compared to other models. The best accuracy of 97.12%, F1-score of 96.52%, and recall of 97.12% are achieved using the combination of LSTM and SBERT, while the highest recall is achieved using the combination of LSTM and Word2vec.\nThe superior performance of LSTM with SBERT can be attributed to several factors. First, SBERT is based on BERT architecture which is a transformer-based model. Transformer-based models consider the context of a word while extracting embeddings, whereas other feature extractors used in the comparison are context-independent methods. Secondly, DL models such as LSTM, GRU, and MLP excel at learning intricate patterns and representations from input feature vectors. Specifically, LSTM is adept at modeling sequential dependencies and capturing long-term contextual relationships. However, the dataset is also important for the selection of the appropriate method for the specific task. In the KCC dataset, many queries are in the form of short phrases where only the keywords convey the meaning of the query. For this reason, BoW, which is built around the unique words in the entire corpus, nearly matches the performance of SBERT. Yet, LSTM with SBERT provides slightly better performance than other combinations due to their aforementioned advantages. Hence, the LSTM with SBERT features is chosen for the query mapping module."}, {"title": "3.2.2. Evaluation Results of Query Mapping Module", "content": "The performance of the query mapping module of KisanQRS is evaluated on five different states' data, namely Andhra Pradesh, Madhya Pradesh, Maharashtra, Tamil Nadu, and Uttar Pradesh. Since the performance of the query mapping module is affected by the quality of clusters formed as the cluster labels eventually serve as the target labels for the LSTM-based query mapping module, the scope of our experiments includes K-Means, Agglomerative, DBSCAN, and proposed KisanQRS clustering algorithm for three thresh values; 0.80, 0,90 and 0.95. A higher value of thresh, like 0.99, produces a large number of clusters, with each cluster containing identical queries, which eliminates the queries having the same semantic meaning but different lexical structures to be placed in the same cluster. In contrast, a lower thresh value may result in clusters containing queries that are not semantically similar. Therefore, a moderate thresh"}, {"title": "3.3. Answer Retrieval Results", "content": "Results for answer retrieval are taken on the set of answers which are scored on a scale of 0-10, i.e. $rel_i \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\}$, where 10 denotes \u2018highly relevant' and"}]}