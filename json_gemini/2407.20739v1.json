{"title": "Architectural Influence on Variational Quantum Circuits in Multi-Agent Reinforcement Learning: Evolutionary Strategies for Optimization", "authors": ["Michael K\u00f6lle", "Karola Schneider", "Sabrina Egger", "Felix Topp", "Thomy Phan", "Philipp Altmann", "Jonas N\u00fc\u00dflein", "Claudia Linnhoff-Popien"], "abstract": "In recent years, Multi-Agent Reinforcement Learning (MARL) has found application in numerous areas of science and industry, such as autonomous driving, telecommunications, and global health. Nevertheless, MARL suffers from, for instance, an exponential growth of dimensions. Inherent properties of quantum mechanics help to overcome these limitations, e.g., by significantly reducing the number of trainable parameters. Previous studies have developed an approach that uses gradient-free quantum Reinforcement Learning and evolutionary optimization for variational quantum circuits (VQCs) to reduce the trainable parameters and avoid barren plateaus as well as vanishing gradients. This leads to a significantly better performance of VQCs compared to classical neural networks with a similar number of trainable parameters and a reduction in the number of parameters by more than 97% compared to similarly good neural networks. We extend an approach of K\u00f6lle et al. by proposing a Gate-Based, a Layer-Based, and a Prototype-Based concept to mutate and recombine VQCs. Our results show the best performance for mutation-only strategies and the Gate-Based approach. In particular, we observe a significantly better score, higher total and own collected coins, as well as a superior own coin rate for the best agent when evaluated in the Coin Game environment.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) is currently on everyone's lips. This is due to the fact that it is being used to find innovative solutions in many areas. Al is now a player in industry, healthcare, transportation, and education, for example, as it contributes to the progress of many current technologies [16].\nBut not only single-agent settings are needed; Multi-Agent Systems (MAS) are also central elements in these application areas. Although these agents are"}, {"title": "2 Preliminaries", "content": "In this section, we provide foundational concepts necessary for understanding the multi-agent setting and the methodologies employed in our research. We begin by discussing the framework of Markov games, which underpins the interactions among multiple agents. This is followed by an overview of independent learning in MARL, highlighting the challenges and dynamics involved. Next, we delve into the specifics of evolutionary optimization and its application in MARL, drawing inspiration from natural selection to improve agent performance. Finally, we cover the basics of quantum computing and VQCs, which are integral to our approach."}, {"title": "2.1 Multi-Agent Setting", "content": "We focus on Markov games $M = (D, S, A, P, R)$, where $D = {1, ..., N}$ represents a set of agents $i$, $S$ is a set of states $s_t$ at time step $t$, and $A = (A_1, ..., A_N)$ is the set of joint actions $a_t = (a_{t,i})_{i \\in D}$. The transition probability is denoted as $P(s_{t+1} | s_t, a_t)$, and the joint reward is $(r_{t,1},...,r_{t,n}) = R(s_t,a_t) \\in R$. The action selection probability for agent $i$ is represented by the individual policy $\\pi_i(a_{t,i} | s_t)$.\nThe policy $\\pi_i$ is typically evaluated using a value function $V_i^{\\pi}(s_t) = E_{\\pi}[G_{t,i}|s_t]$ for all $s_t \\in S$, where $G_{t,i} = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k,i}$ represents the individual and discounted return of agent $i \\in D$ with a discount factor $\\gamma \\in [0,1)$, and $\\pi = (\\pi_1,..., \\pi_N)$ is the joint policy of the multi-agent system (MAS). The goal for agent $i$ is to find the best response $\\pi_i^*$ with $V_i^* = \\max_{\\pi_i} V_i^{\\pi_i, \\pi_{-i}}$ for all $s_t \\in S$, where $\\pi_{-i}$ denotes the joint policy excluding agent $i$.\nWe define the efficiency of a MAS or utilitarian metric (U) by the sum of all individual rewards until time step $T$:\n$U = \\sum_{i \\in D} R_i, \\qquad (1)$\nwhere $R_i = \\sum_{t=0}^{T-1} r_{t,i}$ is the undiscounted return or the sum of rewards for agent $i$ starting from the initial state $s_0$."}, {"title": "2.2 Multi-Agent Reinforcement Learning", "content": "In our research, we focus on independent learning, where each agent $i$ optimizes its individual policy $\\pi_i$ based on its own information, such as $a_{i,j}$ and $r_{i,j}$, using reinforcement learning (RL) techniques. For instance, we employ evolutionary optimization as described in Section 2.3 on Evolutionary Optimization.\nIndependent learning introduces non-stationarity because the agents adapt simultaneously, continuously altering the environment dynamics from each agent's perspective [21], [18], [14]. This non-stationarity can lead to the development of overly greedy and exploitative policies, causing agents to defect from cooperative behavior [19], [8]."}, {"title": "2.3 Evolutionary Optimization", "content": "Inspired by the process of natural selection, evolutionary optimization has demonstrated efficacy in solving complex problems where traditional methods may fall short [33]. This approach utilizes a population of individuals, each randomly generated with a unique set of parameters. These individuals are assessed based on a fitness function that evaluates their performance on the given problem. The most fit individuals are then selected for reproduction, with their parameters recombined and mutated to create a new population for the next generation [7].\nEvolutionary optimization techniques, such as genetic algorithms [15], have been successfully applied in various fields. These include the optimization of neural networks and interactive recommendation tasks [6], [10]. Moreover, these methods have been employed to address a broad spectrum of challenges, from designing quantum circuit architectures to optimizing complex real-world designs [22], [2]."}, {"title": "2.4 Quantum Computing", "content": "Quantum computing is an emerging field in computer science that leverages the principles of quantum mechanics to process information. Unlike classical computers, which use bits to store and process data, quantum computers use quantum bits, or qubits, which can exist in multiple states simultaneously due to the principle of superposition [34].\nA qubit's state, denoted as $|\\Psi\\rangle$, can be expressed as a linear combination of the basis states $|0\\rangle$ and $|1\\rangle$:\n$|\\Psi\\rangle = \\alpha |0\\rangle + \\beta |1\\rangle, \\qquad (2)$\nwhere $\\alpha$ and $\\beta$ are complex coefficients that satisfy the normalization condition:\n$|\\alpha|^2 + |\\beta|^2 = 1. \\qquad (3)$\nUpon measurement, a qubit in the superposition state $\\alpha |0\\rangle + \\beta |1\\rangle$ collapses to one of the basis states, $|0\\rangle$ or $|1\\rangle$, with probabilities determined by the magnitudes of the coefficients, $|\\alpha|^2$ and $|\\beta|^2$, respectively. This process marks the transition from a quantum superposition to a definite classical state, where the observable's value is precisely known [26].\nMoreover, multiple qubits can be entangled, creating strong correlations between them. Entanglement allows the qubits to be interconnected in such a way that the state of one qubit directly influences the state of another, regardless of the distance separating them. This property is pivotal for the enhanced computational power of quantum systems, enabling them to solve complex problems more efficiently than classical computers."}, {"title": "2.5 Variational Quantum Circuits", "content": "Variational Quantum Circuits (VQCs), also known as parameterized quantum circuits, are quantum algorithms designed as function approximators that are"}, {"title": "3 Related Work", "content": "Our work extends the approach for using evolutionary optimization for quantum Reinforcement Learning using VQCs by K\u00f6lle et al. [17]. They, in turn, were inspired by Chen et al. [4] who showed that it is possible to reduce parameter requirement in comparison to classical RL when leveraging VQCs for Q-value function approximation in Deep Q-Learning. K\u00f6lle et al. [17] then developed three gradient-free genetic variations of VQCs with MAQRL applying evolutionary optimization. They compared the results of their agents with Neural Networks in the Coin Game [20], finding similar accomplishments with more than 97% less parameters within the VQC.\nAnother approach to Quantum Multi-Agent Reinforcement Learning is leveraging Quantum Boltzmann Machines (QBM) [25], [24]. This strategy builds on an existing methodology where QBM outperforms classical Deep RL in terms of convergence speed, measured by the number of required time steps while using Q-value approximation. The results suggest enhanced stability in learning and the agents' ability to attain optimal strategies in grid domains. Similarities to the method employed here lie in the utilization of Q-values and grid domains as testing environments.\nYun et al. [35] employ Quantum Multi-Agent Reinforcement Learning using centralized learning and decentralized execution to avoid the challenges of the NISQ era and the non-stationary properties from classic MARL. This approach achieves a superior overall reward in the tested environments compared to classical methodologies using less parameters."}, {"title": "4 Approach", "content": "In this section we describe an idea of optimizing a $\\theta$ parameterized agent using the evolutionary approach by K\u00f6lle et al. [17]. In the next step, we extend this concept by applying architectural changes and parameter adjustments to explore the impact of circuit architecture on the performance.\nK\u00f6lle et al. [17] were inspired by Chen et al. [4], but in contrast, they use a more general Multi-Agent setup to optimize the utilitarian metric $U$ (Eq. 1).\nFor maximizing the fitness function, they use a population $P$, which consists of $\\eta$ randomly initialized agents. To fully match the quantum circuits' parameter space, the agents are parameterized by $\\theta \\in [-\\pi, \\pi)$."}, {"title": "4.1 Layer-Based Concept", "content": "The Layer-Based concept is inspired by Strongly Entangling Layers [30,11], where all qubits are entangled. This is realized by applying a CNOT Gate to every qubit $q_n$ and selecting qubit $q_{n-1 \\mod n}$ as the target qubit. Then $R_x$, $R_y$ and $R_z$ rotation gates are performed for every qubit. In contrast to [17], during the evolutionary optimization process the number of layers can increase and decrease due to mutation and recombination. Here, the evolutionary algorithm creates new circuits by recombining the initial layers of one circuit with the final layers of another. Mutation alters the circuit's design by either adding or removing entire layers, thereby preserving the overall layer structure. During tests, we found that the evolutionary optimization process works best when starting with circuits that consist of only one layer."}, {"title": "4.2 Gate-Based Concept", "content": "When applying the Gate-Based concept, we eliminate the organization of gates into layers. Instead, the circuit is built by applying a specific number of randomly sampled gates to randomly chosen qubits. These gates are selected from"}, {"title": "4.3 Prototype-Based Concept", "content": "The Prototype-Based concept merges the Layer-Based and Gate-Based approaches by creating a circuit with repeated layers, while manipulations occur at the gate level. One layer of this circuit is constructed similarly to a Gate-Based Circuit, and the resulting gate arrangement, called the prototype, is then repeated across all layers of the circuit. The evolutionary processes applied to the prototype work the same as for the Gate-Based approach. This approach ensures, that the gate composition remains consistent for all layers.\nAs we change the circuit architecture, we need to adjust some parameters for the initialization of VQCs: Layer-Based circuits have a specific number of initial layers $\\nu$, Gate-Based circuits have a specific number of initial gates $\\chi$, and Prototype-Based circuits have a specific number of initial layers $\\nu$ as well as a specific number of initial gates per layer $\\chi$. This means when constructing a Gate-Based or a Prototype-Based circuit, we have to randomly select $\\chi$ gates from our predefined gate set ($R_x,R_y,R_z$,CNOT) and place them on $\\chi$ randomly"}, {"title": "4.4 Evolutionary Algorithm", "content": "Like in K\u00f6lle et al. [17], for all of the three aforementioned concepts, two different strategies for creating the next generation can be used: (1) recombination and mutation, where new individuals are formed by recombining selected agents from the existing population. Following this, they are mutated based on the mutation rate. (2) mutation only, where new individuals are created by mutating selected agents from the current population. Both approaches lead to a new generation of population size $n$, including the elite agent from the previous generation. Algorithm 2 shows the employed EA, that extends the approach by K\u00f6lle et al. [17] by applying architectural changes and Fig. 3 depicts the training process. In detail, this works as follows:\nWe use tournament selection to choose individuals from the population. This process starts by randomly picking $\\tau$ individuals from the population to form a tournament. The fitness of each individual in the tournament is then compared, and the one with the highest fitness is selected [32]. By increasing the size of the tournament, the likelihood of selecting high-performing agents rises, thereby boosting the selection pressure [7].\nWe implement elitism by carrying the unchanged fittest agent into the next generation [4]. This strategy ensures that advantageous traits are preserved, preventing a decline in performance and promoting continuous improvement within the population [12]."}, {"title": "5 Experimental Setup", "content": "In this section, we describe details about our experimental setup we use to evaluate the evolutionary algorithm introduced in Section 4. We apply the aforementioned strategies for creating different architectural circuits. The following parts include information about our game environment, baselines, metrics, and hyperparameters we use for training."}, {"title": "5.1 Coin Game Environment", "content": "We use the Coin Game [20] environment in a 3x3 gridworld version as it has a comparatively small observation space. The use of the short evaluation environment is due to the fact that we are currently in the Noisy Intermediate-Scale Quantum (NISQ) era, where we can only simulate a restricted amount of qubits, thus limiting the data we can embed into a quantum circuit [28].\nIn the Coin Game, both the Red and Blue agents aim for collecting coins, with a single coin placed on a free cell in the grid. The color of the coin always corresponds to one of the agents' colors. An example state of this well-known sequential game that is suited for evaluating RL strategies is illustrated in Fig. 4.\nWhen an agent occupies the same position as a coin, the coin is collected. Once a coin is collected, a new one appears at a random location that is not occupied by an agent, and it is again either red or blue. Each game of the Coin Game lasts for 50 steps, with each agent getting 25 turns. The goal is to maximize the agents' rewards.\nThe Coin Game can be played in both competitive and cooperative modes. In the cooperative mode, the agent that collects a coin receives an reward of +1. Conversely, the second agent's reward decreases by -2 if the first agent collects a coin of their color. Therefore, considering the total reward, collecting one's own coin results in a common reward of +1, while collecting an opponent's coin"}, {"title": "5.2 Baselines", "content": "VQCs serve as an alternative to classical neural networks in agent design. In the first part of the evaluation, neural networks are used as agents because they function as general approximators. The basic neural network consists of 2-layers for this purpose. The first layer maps the input observations to a variable number of hidden units $x$, and the second layer connects these hidden units to the number of possible actions. This configuration yields the individual Q-values for each action, similar to the VQC approach. To prevent the selection of illegal actions, the Q-values are adjusted using an action mask. Our focus is on this specific neural network with varying hidden unit counts, so that the network can be modified in many ways to affect the outcomes.\nIn the second part of our evaluation we use a static baseline, which consists of a 8 layers of the Layer-Based approach where we only change the parameters and not the architecture. For the baseline, we run a mutation-only strategy with parameter mutation power $\\sigma_p=0.01$."}, {"title": "5.3 Metrics", "content": "We evaluate our experiments in the Coin Game environment using three metrics: Score, Total Collected Coins, and Own Coin Rate. In our study, the agents play solely against themselves to simplify the evaluation of their performance.\n1.  Score ($S_n$): This metric consists of the undiscounted individual rewards $r_{t,i}$ accumulated over all agents until timestep $T\\in$ {0..49}. It is calculated as:\n$S_n = \\sum_{i\\in{0,1}} \\sum_{t=0}^{T-1} r_{t,i} \\qquad (5)$\nHere, $i$ represents the agent and $n$ (ranging from 0 to 199) denotes the generation, averaged over five seeds. This score provides a comprehensive indicator of the agents' performance in the Coin Game environment.\n2.  Total Coins Collected (TCn): This metric tracks the total number of coins collected $c_{t,i}$ by all agents until timestep $T\\in$ {0..49}:\n$TC_n = \\sum_{i\\in{0,1}} \\sum_{t=0}^{T-1} c_{t,i} \\qquad (6)$\nSimilar to the Score metric, $i$ represents the agent and $n\\in$ {0..199} the generation, averaged over five seeds. This metric helps in understanding the total collection performance of the agents.\n3.  Own Coins Collected (OCn): This metric is the sum of all collected coins that match the agent's own color $O_{t,i}$ until timestep $T\\in$ {0..49} accumulated over all agents:\n$OC_n = \\sum_{i\\in{0,1}} \\sum_{t=0}^{T-1} O_{t,i} \\qquad (7)$\nAgain, similar to the previous metrics, $i$ represents the agents and $n \\in$ {0..199} the generation, averaged over five seeds.\n4.  Own Coin Rate (OCRn): Comparing the Own Coins Collected metric with the Total Coins Collected allows us to gauge the level of cooperation achieved, by determining:\n$OCR_n = \\sum_{i\\in{0,1}} \\frac{O_{t,i}}{c_{t,i}} \\qquad (8)$\nThis approach provides a detailed evaluation of the agents' behavior and their ability to cooperate within the Coin Game environment."}, {"title": "5.4 Training and Hyperparameters", "content": "For all our experiments in the Coin Game environment, we train the agents over $\\mu=200$ generations with a population size of $\\eta=250$. Multiple executions have"}, {"title": "6 Results", "content": "In this section, we present the results of our experiments in the Coin Game environment. We tested all approaches with mutation and recombination combined and mutation only. For our first experiments, we additionally tested two classical neural networks. They consist out of two hidden layers, one with size 64 x 64 and the second one with size 3 x 4. We choose this size to gain better comparability"}, {"title": "6.1 MAQRL in Coin Game", "content": "Comparing Generational Evolution Strategies Our goal is to assess the impact of different generational evolution strategies. First, we compare the performance of a mutation-only strategy (Mu) with two combined strategies involving both mutation and recombination. The first combined approach uses a random-point crossover recombination strategy (RaReMu), where crossover occurs at a randomly chosen point in the parameter vector. The second combined strategy employs a layer-wise crossover (LaReMu), selecting a random layer and applying the crossover after the last parameter of that layer. For all strategies, the mutation power $\\sigma$ is fixed at 0.01.\nFig. 5 illustrates that the mutation-only strategy outperforms the combined strategies. The mutation-only approach begins with an average reward of 5, dips slightly below 4 by the 17th generation, and then steadily increases, stabilizing around a score of 7 by the 140th generation. In contrast, the layer-wise recombination strategy starts at 3.3, rapidly ascends until the 30th generation, and then stabilizes, reaching an average reward of 6 by the 123rd generation with fluctuations thereafter. The random crossover strategy starts near the mutation-only strategy at 4.7, declines to 3 by the 17th generation, then climbs steadily until the 131st generation, reaching a score of 6 but eventually settling around 5.5, making it the least effective of the three methods.\nWe also evaluated the average number of coins collected during the experiments. The mutation-only strategy consistently collects more coins, as shown in Fig. 6a. While there are periods where all strategies have nearly identical coin counts, the mutation-only strategy generally leads, with up to a 2-coin gap at times. This is reflected in the coin rate (Fig. 6c), where the mutation-only strategy shows a steady increase over generations, indicating enhanced agent cooperation in the testing environment.\nThe comparison of both strategies lead to the result, that the mutation-only strategy consistently achieves the highest coin rate and collects the most coins, aligning well with our goal of maximizing the reward. The layer-wise recombination strategy shows an initial surge in performance, correlating with an increase in collected coins and coin rate, but its advantage diminishes after the 90th generation. For the random crossover strategy we observe that it collects"}, {"title": "6.2 Architectural Variations", "content": "In the second part of our results we examine how different architecture strategies during the evolution process influence the performance of the agents. Therefore, we investigate the efficiency of recombination and mutation, and we compare the Gate-Based, Layer-Based and Prototype-Based approaches against each other looking at the score, coin and gate count metrics. We conclude the results section with a look at the performance of a static baseline (i.e. an unchanged circuit architecture throughout the whole evolution process) compared to architectural evolution.\nAssessment of Evolutionary Strategies: ReMu vs. Mu In this section, we evaluate the effectiveness of two evolutionary strategies for generating successive generations: recombination combined with mutation (ReMu) and mutation-only (Mu). The goal is to understand the performance of these strategies to optimize the evolutionary process in future experiments. As the performance trends of recombination versus mutation-only are consistent across different approaches, in our analysis we focus on the Gate-Based approach.\nOur findings reveal a significant difference in the performance of the ReMu and Mu strategies. As illustrated in Fig. 11, the recombination approach failed to yield good solution candidates. The best score achieved by the ReMu strategy declined from 4.5 to 1.3 by the 170th generation, with a minor recovery to just above 2, resulting in an overall decline of more than 2 points. In contrast, the best run of the mutation-only approach showed a steady and consistent improvement, with scores rising from 4.5 to almost 12 by generation 150.\nFor the ReMu strategy, the gap between the average score and the best score diminished over time, indicating a convergence towards lower-quality solutions. By the 125th generation, the best score was only about 0.6 points above the average. This suggests that the recombination method might introduce complexities or incompatibilities that hinder the evolutionary process, leading to suboptimal solutions and reduced diversity.\nIn contrast, the consistent upward trajectory in performance of the mutation-only strategy indicates a more efficient navigation of the solution space, leading to the discovery of higher-quality solutions.\nBased on these findings, we conclude that the mutation-only strategy is more effective for our purposes. The ReMu strategy's introduction of complexities appears to hinder the evolutionary process, leading to less optimal solutions. Therefore, we will exclusively employ the mutation-only strategy in future experiments to maximize the potential for discovering optimal solutions.\nComparative Analysis of Architectural Evolution Strategies Now, we evaluate the performance of three Variational Quantum Circuit architecture structures and their respective evolutionary strategies: Layer-Based, Gate-Based, and Prototype-Based. Each approach presents a unique design for the variational layers and employs distinct architectural changes within the Evolutionary Algorithm (EA)."}, {"title": "7 Conclusion", "content": "At the moment, MAQRL suffers from barren-plateaus and vanishing gradients due to their gradient based training [4], [9]. K\u00f6lle et al. [17] extended an evolutionary optimization process by Chen et al. [4] to a Multi-Agent setting and used different evolutionary strategies. They proposed three approaches: layer-wise crossover with mutation, random crossover with mutation and mutation"}]}