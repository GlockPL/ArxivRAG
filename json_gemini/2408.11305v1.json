{"title": "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "authors": ["Xiangyu Zhao", "Yuehan Zhang", "Wenlong Zhang", "Xiao-Ming Wu"], "abstract": "The fashion domain encompasses a variety of real-world multimodal tasks, including multimodal retrieval and multimodal generation. The rapid advancements in artificial intelligence generated content, particularly in technologies like large language models for text generation and diffusion models for visual generation, have sparked widespread research interest in applying these multimodal models in the fashion domain. However, tasks involving embeddings, such as image-to-text or text-to-image retrieval, have been largely overlooked from this perspective due to the diverse nature of the multimodal fashion domain. And current research on multi-task single models lack focus on image generation. In this work, we present UniFashion, a unified framework that simultaneously tackles the challenges of multimodal generation and retrieval tasks within the fashion domain, integrating image generation with retrieval tasks and text generation tasks. UniFashion unifies embedding and generative tasks by integrating a diffusion model and LLM, enabling controllable and high-fidelity generation. Our model significantly outperforms previous single-task state-of-the-art models across diverse fashion tasks, and can be readily adapted to manage complex vision-language tasks. This work demonstrates the potential learning synergy between multimodal generation and retrieval, offering a promising direction for future research in the fashion domain. The source code is available at https://github.com/xiangyu-mm/UniFashion.", "sections": [{"title": "1 Introduction", "content": "The fashion domain presents a range of real-world multimodal tasks, encompassing multimodal retrieval (Gao et al., 2020; Wu et al., 2021; Bai et al., 2023) and multimodal generation (Yang et al., 2020) tasks. Such tasks have been utilized in diverse e-commerce scenarios to enhance product discoverability, seller-buyer interaction, and customer conversion rates after catalog browsing (Han et al., 2023; Zhuge et al., 2021). The remarkable progress in the field of artificial intelligence generated content (AIGC), particularly in technologies like large language models (LLMs) (Chiang et al., 2023; Touvron et al., 2023; Brown et al., 2020) for text generation and diffusion models (Rombach et al., 2022; Nichol et al., 2022; Saharia et al., 2022) for visual generation, has sparked widespread research interest in applying these multimodal models in the fashion domain.\nMultimodal large language models (Liu et al., 2023a; Dai et al., 2023; Dong et al., 2023) (MLLMs) seem to emerge as a promising direction for a single multi-task model. However, due to the heterogeneous nature of the multimodal fashion tasks (Han et al., 2023), existing MLLMs lack the capability to be directly applied to the fashion domain, such as embedding ability. For example, in the fashion domain, retrieval tasks that rely on embedding ability, like image-to-text or text-to-image retrieval, have been largely neglected from this aspect. Furthermore, MLLMs lack the ability to solve composed image retrieval (CIR) (Liu et al., 2021; Baldrati et al., 2022) task, which composes the reference image and related caption into a joint embedding to calculate similarities with the candidate images and is particularly relevant in fashion recommendation systems (Han et al., 2017).\nDrawing inspiration from GRIT (Muennighoff et al., 2024), which successfully combined embedding and generative tasks into a unified model for text-centric applications and showed improved embedding performance through the addition of a generative objective, it becomes clear that investigating task correlations and integrating embedding with generative models in the fashion domain is is both necessary and promising.\nWhile previous works (Han et al., 2023; Zhuge et al., 2021) in the fashion domain have also proposed using a single model for solving multiple"}, {"title": "2 Preliminaries and Related Works", "content": "Fashion tasks encompass a range of image and language manipulations, including cross-modal retrieval, composed image retrieval, fashion image captioning and generation, etc. The representative tasks can be briefly divided into the following two groups:\nFashion Retrieval generally consists of Cross-Modal Retrieval (CMR) (Ma et al., 2022; Rostamzadeh et al., 2018) and composed image retrieval (CIR) tasks (Baldrati et al., 2023a; Bai et al., 2023). CMR requests to efficiently retrieve the most matched image/sentence from a large candidate pool D given a text/image query. CIR is a special type of image retrieval with a multimodal query (a combination of a reference image and a modifying text) matched against a set of images. It retrieves a target image from a vast image database based on a reference image and a text description detailing changes to be applied to the reference image. In this scenario, a query pair p = {IR,t} is provided, where IR is the reference image and t is the text describing the desired modifications. The challenge for this task is to accurately identify the target image IT that best matches the query among all potential candidates in the image corpus D.\nFashion Generation consists of Fashion Image Captioning (FIC) and Fashion Image Generation (FIG). FIC (Yang et al., 2020) aims to generate a descriptive caption for a product based on the visual and/or textual information provided in the input. FIG aims to generate images based on the multimodal input, such as try-on (Choi et al., 2021; Gou et al., 2023) and fashion design (Baldrati et al., 2023b)."}, {"title": "2.2 Multimodal Language Models", "content": "Recent research has witnessed a surge of interest in multimodal LLMs, including collaborative models (Wu et al., 2023; Yang et al., 2023b; Shen et al., 2023) and end-to-end methods (Alayrac et al., 2022; Zhao et al., 2023; Li et al., 2022; Bao et al., 2021; Wang et al., 2022b,a,a). More recently, some works also explore training LLMs with parameter-efficient tuning (Li et al., 2023b; Zhang et al., 2023b) and instruction tuning (Dai et al., 2023; Liu et al., 2023a; Ye et al., 2023; Zhu et al., 2023a; Li et al., 2023a). They only focus on generation tasks, while UniFashion is built upon a unified framework that enables both retrieval and generation tasks."}, {"title": "2.3 Diffusion Models", "content": "Diffusion generative models (Rombach et al., 2022; Ramesh et al., 2021; Nichol et al., 2022; Ruiz et al., 2023) have achieved strong results in text conditioned image generation works. Among contemporary works that aim to condition pretrained latent diffusion models, ControlNet (Zhang et al., 2023a) proposes to extend the Stable Diffusion model with an additional trainable copy part for conditioning input. In this work, we focus on the fashion domain and propose a unified framework that can leverage latent diffusion models that directly exploit the conditioning of textual sentences and other modalities such as human body poses and garment sketches."}, {"title": "2.4 Problem Formulation", "content": "Existing fashion image retrieval and generation methods are typically designed for specific tasks, which inherently restricts their applicability to the various task forms and input/output forms in the fashion domain. To train a unified model that can handle multiple fashion tasks, our approach introduces a versatile framework capable of handling multiple fashion tasks by aligning the multimodal representation into the LLM and the diffusion model. This innovative strategy enhances the model's adaptability, and it can be represented as:\n$I_{out}, T_{out} = F_{TRet}, T_{Gen} (I_{in}, T_{in}; \\Theta)$,                                        (1)\nwhere F represents the unified model parameterized by \u0398, it consists of retrieval module T_{ret} and generative module T_{Gen}."}, {"title": "3 Proposed Model: UniFashion", "content": "In this section, we introduce the UniFashion to unify the fashion retrieval and generation tasks into a single model. By combining retrieval and generative modules, the proposed UniFashion employs a two-stage training strategy to capture relatedness between image and language information. Consequently, it can seamlessly switch between two operational modes for cross-modal tasks and composed modal tasks."}, {"title": "3.1 Phase 1: Cross-modal Pre-training", "content": "In the first stage, we conduct pre-training on the retrieval and generation modules to equip the Large Language Model (LLM) and diffusion model with strong cross-modal fashion representation capabilities for the next phase."}, {"title": "3.1.1 Cross-modal Retrieval", "content": "For cross-modal retrieval tasks, given a batch of image caption pairs p = {I, C'}, we first calculate their unimodal representations using an independent method. In particular, we adopt a lightweight Querying Transformer, i.e., Q-Former in BLIP-2 (Li et al., 2023b), to encode the multimodal inputs, as it is effective in bridging the modality gap. To avoid information leaks, we employ a unimodal self-attention mask (Li et al., 2023b), where the queries and text are not allowed to see each other:\n$Z_1 = Q-Former(I, q)$,\n$Z_c = Q-Former(C)$.                            (2)\nwhere the output sequence Z_I is the encoding result of an initialized learnable query q with the input image and Z_c is the encoded caption, which contains the embedding of the output of the [CLS] token ecls, which is a representation of the input caption text. Since Z_I contains multiple output embeddings (one from each query), we first compute the pairwise similarity between each query output and ecls, and then select the highest one as the image-text similarity. In our experiments, we employ 32 queries in q, with each query having a dimension of 768, which is the same as the hidden dimension of the Q-Former. For cross-modal learning objective, we leverage the Image-Text Contrastive Learning (ITC) and Image-Text Matching (ITM) method. The first loss term is image-text contrastive loss, which has been widely adopted in existing text-to-image retrieval models. Specifically, the image-text contrastive loss is defined as:\n$L_{ITC}(X, Y) = - \\frac{1}{B} \\sum_{i=1}^{B} log \\frac{exp[X(X_i^T Y_i)]}{\\sum_{j=1}^{B} exp[(X_i^T Y_j)]}$,                                (3)\nwhere X is a learnable temperature parameter. ITM aims to learn fine-grained alignment between image and text representation. It is a binary classification task where the model is asked to predict"}, {"title": "3.1.2 Cross-modal Generation", "content": "As depicted in Fig. 2, after the learnable queries q pass through the multimodal encoder, they are capable of integrating the visual information with textual guidance. However, in Section 3.1.1, we did not specify a learning target for q. Empirically, the q that has been merged with the reference image and edited text information should be equivalent to the encoding of the target image. This implies that we should be able to reconstruct the target image and its caption based on q. In this section, we will employ generative objectives to improve the representation of augmented q.\nIn the first stage, we connect the Q-Former (equipped with a frozen image encoder) to a Large Language Model (LLM) to harness the LLM's prowess in language generation, and to a diffusion model to exploit its image generation capabilities. Notably, we exclusively train the model using image-text pairs throughout this process. As depicted in Figure 2, we employ a Task Specific Adapter (TSA) layer to linearly project the output query embeddings q to match the dimensionality of the embeddings used by the LLM and diffusion model. In this stage, we freeze the parameters of the Q-Former and fine-tune only the adapter layers, connecting LLM and diffusion models. This approach allows us to develop a discriminative model that can evaluate whether queries q can generate the target image and its corresponding caption.\nTarget caption generation. The adapter layer is placed before the LLM to map the output of Q-Former to the text embedding space of the LLM. To synchronize the space of Q-Former with that of the LLM, we propose to use the image-grounded text generation (ITG) objective to drive the model to generate texts based on the input image by computing the auto-regressive loss:\n$L_{ITG} = -\\frac{1}{L} \\sum_{l=1}^{L} log p_{\\phi}(w_l|w^{l<1}, f_{\\phi} (q))$,                              (6)\nwhere $w^g = (w_1, ..., w_L)$ represents the groundtruth caption of image I with length L, q = Q-Former(I, q), denotes the LLM's parameters, and denotes the text adapter layers' parameters.\nTarget image generation. In the first stage, our task also aims to reconstruct the image I_T from q. As in standard latent diffusion models, given an encoded input x, the proposed denoising network is trained to predict the noise stochastically added to x. The corresponding objective function can be specified as:\n$L_{q2I} = E_{\\epsilon v, x0}[||\\epsilon - \\epsilon_{\\theta} (x_t x, f_{\\psi} (q), t_x) ||^2]$,                          (7)\nwhere denotes the u-net models' parameters and denotes the image adapter layers' parameters. The overall loss in the first stage can be expressed:\n$L_{ph1} = L_{cross} + L_{ITG} + L_{q2I}$.                                        (8)\nAfter the first training stage, we can leverage the LLM and diffusion model as discriminators to guide the generation of composed queries."}, {"title": "3.2 Phase 2: Composed Multimodal Fine-tuning", "content": "In this phase, the inputs are reference image and guidance text, and we fine-tune the model for composed multimodal retrieval and generation tasks."}, {"title": "3.2.1 Composed Image Retrieval", "content": "For CIR task, the target image I_T generally encompasses the removal of objects and the modification of attributes in the reference image. To solve this problem, as depicted in Fig. 2, the multimodal encoder is utilized to extract features from the reference image and the guide text. It joint embeds the given pair p = {I_R, t} in a sequential output. Specifically, a set of learnable queries q concatenated with text guidance t is introduced to interact with the features of the reference image. Finally, the output of Q-Former is the multimodal synthetic prompt Z_R. We use a bi-directional self-attention mask, similar to the one used in BLIP2 (Li et al., 2023b), where all queries and texts can attend to each other. The output query embeddings Z_R thus capture multimodal information:\n$Z_R = Q-Former(I_R, t, q_R)$,\n$Z_T = Q-Former(I_T, q_T)$.                           (9)\nNoting that the output sequence Z_R consists of learnable queries q and encoded text guidance t,"}, {"title": "3.2.2 Composed Multimodal Generation", "content": "For these generation tasks, we freeze the LLM parameters and tune the parameters of the task-specific adapters, the diffusion model, and the Q-Former. The loss function for the target image's caption generation is formulated in a way that is similar to Eq. 6:\n$L_{ITG} = - \\frac{1}{L} \\sum_{l=1}^{L} log p_{\\phi}(w_l|w^{l<1}, f_{\\psi}(q_R))$,                      (11)\nThe loss function for the target image generation is formulated in a way that is similar to Eq. 7:\n$L_{q2I} = E_{\\epsilon v, x0}[||\\epsilon^r - \\epsilon_{\\theta} (x_t x, f_{\\psi}(q_R), t^r) ||^2]$,                  (12)\nThe overall loss in the second stage can be expressed as:\n$L_{stage2} = L_{cir} + L_{ITG} + L_{q2I}$.                                     (13)"}, {"title": "4 Experiments", "content": "We compare our models with previous state-of-the-art methods on each task. For extensive and fair comparisons, all prior competitors are based on large-scale pre-trained models."}, {"title": "4.1 Experimental Setup", "content": "We initialize the multimodal encoder from BLIP2's Q-Former and MLLM from LLaVA-1.5. As for the diffusion module, following, StableVITON, we inherit the autoencoder and the denoising U-Net of the Stable Diffusion v1.4. We initialize the weights of the U-Net from the Paint-by-Example and for more refined person texture, we utilized a VAE fine-tuned on the VITONHD dataset from Stable-VITON.  For cross-modal retrieval, we evaluated UniFashion on FashionGen validation set. For the image captioning task, UniFashion is evaluated in the FashionGen dataset. For the composed image retrieval task, we evaluated the Fashion-IQ validation set. To maintain consistency with previous work, for the composed image generation task, we fine-tuned UniFashion and evaluated it on the VITON-HD and MGD datasets."}, {"title": "4.2 Evaluation Methods", "content": "Cross-modal retrieval evaluation: We consider both image-to-text retrieval and text-to-image retrieval with random 100 protocols used by previous methods. 100 candidates are randomly sampled from the same category to construct a retrieval database. The goal is to locate the positive match depicting the same garment instance from these 100 same-category negative matches. We utilize Recall@K as the evaluation metric, which reflects the percentage of queries whose true target ranked within the top K candidates.\nFashion image captioning evaluation: For evaluating the performance of caption generation, we utilize BLEU-4, METEOR, ROUGE-L, and CIDEr as metrics.\nComposed fashion image retrieval evaluation: We compare our UniFashion with CIR methods and the FAME-ViL model of V + L that is oriented towards fashion in the original protocol used by Fashion-IQ. For this task, we also utilize Recall@K as the evaluation metric.\nComposed fashion image generation evaluation: We compare our UniFashion with try-on methods on VITON-HD dataset and fashion design works on MGD dataset. To evaluate the quality of image"}, {"title": "4.3 Comparative Analysis of Baselines and Our Method", "content": "UniFashion performs better compared to baselines in all data sets. presents the evaluation results for each baseline and our models in Fashion- Gen data sets for cross-modal retrieval. UniFashion outperforms most of the baseline models on both the text-to-image and image-to-text tasks. Following FAME-ViL, we also adopt a more challenging and practical protocol that conducts retrieval on the entire product set, which is in line with actual product retrieval scenarios. , we performed a comparison between our UniFashion and other baselines on the FashionGen dataset for the image captioning task. By integrating the powerful generative ability of the LLM, our model performed significantly better than the traditional multimodal models in this task. , we conducted a comparison between our UniFashion and CIR-specialist methods. Our findings are in line with those of Tab. 1.\nAfter fine-tuning UniFashion on different modal input composed image generation/editing"}, {"title": "4.4 Ablation Study", "content": "Our model completes the multimodal composed tasks in more aspects. , we also carry out ablation studies on different retrieval methods. Since UniFashion is capable of generating captions, for the CIR task, we initially utilize UniFashion to generate the captions of candidate images and then conduct the image retrieval task (denoted as UniFashion w/o cap) and the caption retrieval task (denoted as UniFashion w/o img). We find that our single-task variant has already achieved superior performance in the relevant field. Furthermore, due to the generative ability of our model, the pre- generated candidate library optimizes the model's performance in this task.  We researched the impact of different mod-"}, {"title": "5 Conclusion", "content": "We have introduced UniFashion, a unified framework that addresses the challenges in multimodal generation and retrieval tasks within the fashion domain. By unifying embedding and generative tasks with a diffusion model and LLM, UniFashion enables controllable and high-fidelity generation, significantly outperforming previous single-task state-of-the-art models across diverse fashion tasks. Our model's ability to readily adapt to manage complex vision-language tasks demonstrates its potential for enhancing various e-commerce scenarios and fashion-related applications. The findings of this study highlight the importance of exploring the potential learning synergy between multimodal"}, {"title": "6 Limitations", "content": "This section aims to highlight the limitations of our work and provide further insight into the research in this area. Our model relies on diffusion for multimodal interaction, which means that the composed image generation processes may take longer. In our experiments, we tested the performance of our model on one A100 (80G) GPU. During inference, using 1000 examples from VITON-HD dataset, UniFashion took approximately 3.15 seconds for each image generation. We believe it would be beneficial to explore more efficient sampling methods, such as DPM-Solver++ (Lu et al., 2022), to improve the overall efficiency of UniFashion."}, {"title": "A Basics of Diffusion Models", "content": "After the initial proposal of diffusion models by (Sohl-Dickstein et al., 2015), they have demonstrated remarkable capacity for generating high-quality and diverse data. DDPM (Ho et al., 2020) connects diffusion and score matching models through a noise prediction formulation, while DDIM (Song et al., 2020) proposes an implicit generative model that generates deterministic samples from latent variables.\nGiven a data point sampled from a real data distribution x_0 \\in q(x), during forward diffusion, x_0 is gradually \"corrupted\u201d at each step t by adding Gaussian noise to the output of step t-1. It produces a sequence of noisy samples x_1, ..., x_T. Each step is controlled by:\nStable Diffusion Model. In the field of image generation, diffusion models operate by progressively denoising a random variable that is sampled from a Gaussian distribution. Latent diffusion models (LDMs) operate in the latent space of a pre-trained autoencoder achieving higher computational efficiency while preserving the generation quality. Stable diffusion model is composed of an autoencoder with an encoder E and a decoder D, a conditional U-Net denoising model \\epsilon_{\\theta}, and a CLIP-based text encoder. With the fixed encoder E, an input image x is first transformed to a lower-dimensional latent space z_0 = E(x). The decoder D performs the opposite operation, decoding z_0 into the pixel space. When considering a latent variable z and its noisy counterpart z_t, which is obtained by incrementally adding noises to z over t steps, the latent diffusion models are designed to train the $\\epsilon_{\\theta}(\\cdot)$ to predict the added noise $\\epsilon$ using a standard mean squared error loss:\n$L := E_{z,\\epsilon,t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t)||^2]$.                                     (14)\nMultimodal Conditional Generation. In the context of our current work, we have a particular focus on the pre-trained multimodal latent diffusion models. For a multimodal conditional generation, given a target image x_0, in addition to the textual information, the input condition y_0 also contains other constraints such as . The aim is to model the conditional data distribution q(x_0|y_0), where y_0 contains different modalities prompts. The conditioning mechanism is implemented by first encoding conditional information, then the denoising network \\epsilon_{\\theta} conditions on y_0 via cross-attention."}, {"title": "B Datasets", "content": "We test the effectiveness of UniFashion by experimenting on different tasks including fashion image captioning, cross-modal retrieval, composed image retrieval and composed image generation.\nWe use the FashionGen and FshaionIQ datasets for retrieval tasks. Fashion-Gen contains 68k fashion products accompanied by text descriptions.  We use the FashionGen dataset.  , we draw inspiration from the success of recent MLLM models such as LLaVA in text-annotation tasks, and propose leveraging LLaVA 1.5 to semi-automatically annotate the dataset.  To avoid overfitting to the specific task and counteract the model's inclination to generate excessively short outputs, we have devised specific instructions,"}, {"title": "C Implementation Details", "content": "LLM During the first phase, due to the flexibility brought by the modular architectural design of BLIP-2, we are able to adapt the model to a broad spectrum of LLMs. We designed different instructions."}]}